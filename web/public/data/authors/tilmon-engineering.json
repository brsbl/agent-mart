{
  "author": {
    "id": "tilmon-engineering",
    "display_name": "Tilmon Engineering",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/88894744?v=4",
    "url": "https://github.com/tilmon-engineering",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 2,
      "total_commands": 14,
      "total_skills": 29,
      "total_stars": 2,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "tilmon-eng-skills",
      "version": "1.0.0",
      "description": "Tilmon Engineering institutional knowledge marketplace - AI agent skills for analysis, security, customer service, and knowledge management",
      "owner_info": {
        "name": "Tilmon Engineering",
        "email": "team@tilmonengineering.com"
      },
      "keywords": [],
      "repo_full_name": "tilmon-engineering/claude-skills",
      "repo_url": "https://github.com/tilmon-engineering/claude-skills",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 2,
        "forks": 0,
        "pushed_at": "2026-01-03T19:37:48Z",
        "created_at": "2025-12-12T15:12:26Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1374
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 618
        },
        {
          "path": "plugins/autonomy/README.md",
          "type": "blob",
          "size": 34403
        },
        {
          "path": "plugins/autonomy/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/agents/branch-analyzer.md",
          "type": "blob",
          "size": 8538
        },
        {
          "path": "plugins/autonomy/agents/journal-reader.md",
          "type": "blob",
          "size": 2554
        },
        {
          "path": "plugins/autonomy/agents/journal-summarizer.md",
          "type": "blob",
          "size": 3668
        },
        {
          "path": "plugins/autonomy/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/commands/analyze-branch.md",
          "type": "blob",
          "size": 1380
        },
        {
          "path": "plugins/autonomy/commands/branch-status.md",
          "type": "blob",
          "size": 1497
        },
        {
          "path": "plugins/autonomy/commands/checkpoint-iteration.md",
          "type": "blob",
          "size": 726
        },
        {
          "path": "plugins/autonomy/commands/compare-branches.md",
          "type": "blob",
          "size": 1562
        },
        {
          "path": "plugins/autonomy/commands/create-goal.md",
          "type": "blob",
          "size": 572
        },
        {
          "path": "plugins/autonomy/commands/end-iteration.md",
          "type": "blob",
          "size": 612
        },
        {
          "path": "plugins/autonomy/commands/fork-iteration.md",
          "type": "blob",
          "size": 1428
        },
        {
          "path": "plugins/autonomy/commands/fork-worktree.md",
          "type": "blob",
          "size": 2174
        },
        {
          "path": "plugins/autonomy/commands/list-branches.md",
          "type": "blob",
          "size": 1339
        },
        {
          "path": "plugins/autonomy/commands/list-worktrees.md",
          "type": "blob",
          "size": 1284
        },
        {
          "path": "plugins/autonomy/commands/remove-worktree.md",
          "type": "blob",
          "size": 1754
        },
        {
          "path": "plugins/autonomy/commands/review-progress.md",
          "type": "blob",
          "size": 542
        },
        {
          "path": "plugins/autonomy/commands/slime.md",
          "type": "blob",
          "size": 1326
        },
        {
          "path": "plugins/autonomy/commands/start-iteration.md",
          "type": "blob",
          "size": 593
        },
        {
          "path": "plugins/autonomy/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/analyzing-branch-status",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/analyzing-branch-status/SKILL.md",
          "type": "blob",
          "size": 7475
        },
        {
          "path": "plugins/autonomy/skills/analyzing-branches",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/analyzing-branches/SKILL.md",
          "type": "blob",
          "size": 10569
        },
        {
          "path": "plugins/autonomy/skills/checkpointing-an-iteration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/checkpointing-an-iteration/SKILL.md",
          "type": "blob",
          "size": 6750
        },
        {
          "path": "plugins/autonomy/skills/comparing-branches",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/comparing-branches/SKILL.md",
          "type": "blob",
          "size": 10275
        },
        {
          "path": "plugins/autonomy/skills/creating-a-goal",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/creating-a-goal/SKILL.md",
          "type": "blob",
          "size": 6186
        },
        {
          "path": "plugins/autonomy/skills/ending-an-iteration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/ending-an-iteration/SKILL.md",
          "type": "blob",
          "size": 17528
        },
        {
          "path": "plugins/autonomy/skills/forking-iteration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/forking-iteration/SKILL.md",
          "type": "blob",
          "size": 7116
        },
        {
          "path": "plugins/autonomy/skills/forking-worktree",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/forking-worktree/SKILL.md",
          "type": "blob",
          "size": 11405
        },
        {
          "path": "plugins/autonomy/skills/listing-branches",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/listing-branches/SKILL.md",
          "type": "blob",
          "size": 5876
        },
        {
          "path": "plugins/autonomy/skills/listing-worktrees",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/listing-worktrees/SKILL.md",
          "type": "blob",
          "size": 8333
        },
        {
          "path": "plugins/autonomy/skills/removing-worktree",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/removing-worktree/SKILL.md",
          "type": "blob",
          "size": 9370
        },
        {
          "path": "plugins/autonomy/skills/reviewing-progress",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/reviewing-progress/SKILL.md",
          "type": "blob",
          "size": 7071
        },
        {
          "path": "plugins/autonomy/skills/slime-strategy",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/slime-strategy/SKILL.md",
          "type": "blob",
          "size": 13819
        },
        {
          "path": "plugins/autonomy/skills/starting-an-iteration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/autonomy/skills/starting-an-iteration/SKILL.md",
          "type": "blob",
          "size": 7519
        },
        {
          "path": "plugins/datapeeker",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 670
        },
        {
          "path": "plugins/datapeeker/README.md",
          "type": "blob",
          "size": 9035
        },
        {
          "path": "plugins/datapeeker/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/agents/analyze-transcript.md",
          "type": "blob",
          "size": 5021
        },
        {
          "path": "plugins/datapeeker/agents/categorize-free-text.md",
          "type": "blob",
          "size": 4847
        },
        {
          "path": "plugins/datapeeker/agents/detect-exact-duplicates.md",
          "type": "blob",
          "size": 2534
        },
        {
          "path": "plugins/datapeeker/agents/detect-foreign-keys.md",
          "type": "blob",
          "size": 10366
        },
        {
          "path": "plugins/datapeeker/agents/detect-near-duplicates.md",
          "type": "blob",
          "size": 4215
        },
        {
          "path": "plugins/datapeeker/agents/detect-outliers.md",
          "type": "blob",
          "size": 5105
        },
        {
          "path": "plugins/datapeeker/agents/extract-supporting-quotes.md",
          "type": "blob",
          "size": 9810
        },
        {
          "path": "plugins/datapeeker/agents/generate-initial-codes.md",
          "type": "blob",
          "size": 7269
        },
        {
          "path": "plugins/datapeeker/agents/identify-themes.md",
          "type": "blob",
          "size": 9575
        },
        {
          "path": "plugins/datapeeker/agents/intercoder-reliability-check.md",
          "type": "blob",
          "size": 7810
        },
        {
          "path": "plugins/datapeeker/agents/market-researcher.md",
          "type": "blob",
          "size": 5985
        },
        {
          "path": "plugins/datapeeker/agents/quality-assessment.md",
          "type": "blob",
          "size": 3445
        },
        {
          "path": "plugins/datapeeker/agents/search-disconfirming-evidence.md",
          "type": "blob",
          "size": 13552
        },
        {
          "path": "plugins/datapeeker/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/CLAUDE.md",
          "type": "blob",
          "size": 11980
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/SKILL.md",
          "type": "blob",
          "size": 38723
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-1.md",
          "type": "blob",
          "size": 2704
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-2.md",
          "type": "blob",
          "size": 5919
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-3.md",
          "type": "blob",
          "size": 3288
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-4.md",
          "type": "blob",
          "size": 4139
        },
        {
          "path": "plugins/datapeeker/skills/cleaning-data/templates/phase-5.md",
          "type": "blob",
          "size": 5102
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/SKILL.md",
          "type": "blob",
          "size": 16428
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-1.md",
          "type": "blob",
          "size": 4025
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-2.md",
          "type": "blob",
          "size": 3983
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-3.md",
          "type": "blob",
          "size": 4623
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-4.md",
          "type": "blob",
          "size": 7326
        },
        {
          "path": "plugins/datapeeker/skills/comparative-analysis/templates/phase-5.md",
          "type": "blob",
          "size": 8521
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/SKILL.md",
          "type": "blob",
          "size": 21051
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/formats",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/formats/graphviz.md",
          "type": "blob",
          "size": 13661
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/formats/mermaid.md",
          "type": "blob",
          "size": 9060
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/formats/vega-lite.md",
          "type": "blob",
          "size": 14492
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/image-formats.md",
          "type": "blob",
          "size": 12930
        },
        {
          "path": "plugins/datapeeker/skills/creating-visualizations/terminal-formats.md",
          "type": "blob",
          "size": 17354
        },
        {
          "path": "plugins/datapeeker/skills/detect-foreign-keys",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/detect-foreign-keys/SKILL.md",
          "type": "blob",
          "size": 18266
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/SKILL.md",
          "type": "blob",
          "size": 15736
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/01-data-familiarization.md",
          "type": "blob",
          "size": 3254
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/02-temporal-patterns.md",
          "type": "blob",
          "size": 2768
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/03-segmentation-patterns.md",
          "type": "blob",
          "size": 2154
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/04-relationship-patterns.md",
          "type": "blob",
          "size": 1666
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/05-anomaly-investigation.md",
          "type": "blob",
          "size": 2724
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/06-insights.md",
          "type": "blob",
          "size": 3825
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/07-next-questions.md",
          "type": "blob",
          "size": 3550
        },
        {
          "path": "plugins/datapeeker/skills/exploratory-analysis/templates/overview-summary.md",
          "type": "blob",
          "size": 931
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/SKILL.md",
          "type": "blob",
          "size": 20501
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-1-question-decomposition.md",
          "type": "blob",
          "size": 2276
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-2-data-discovery.md",
          "type": "blob",
          "size": 1984
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-3-sub-question.md",
          "type": "blob",
          "size": 2619
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-4-synthesis.md",
          "type": "blob",
          "size": 3091
        },
        {
          "path": "plugins/datapeeker/skills/guided-investigation/templates/phase-5-conclusions.md",
          "type": "blob",
          "size": 5192
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/SKILL.md",
          "type": "blob",
          "size": 22900
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/overview-summary.md",
          "type": "blob",
          "size": 251
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-1.md",
          "type": "blob",
          "size": 843
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-2.md",
          "type": "blob",
          "size": 1380
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-3-query.md",
          "type": "blob",
          "size": 961
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-4.md",
          "type": "blob",
          "size": 2048
        },
        {
          "path": "plugins/datapeeker/skills/hypothesis-testing/templates/phase-5.md",
          "type": "blob",
          "size": 2081
        },
        {
          "path": "plugins/datapeeker/skills/importing-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/SKILL.md",
          "type": "blob",
          "size": 20141
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-1.md",
          "type": "blob",
          "size": 802
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-2.md",
          "type": "blob",
          "size": 1301
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-3.md",
          "type": "blob",
          "size": 2120
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-4.md",
          "type": "blob",
          "size": 2092
        },
        {
          "path": "plugins/datapeeker/skills/importing-data/templates/phase-5.md",
          "type": "blob",
          "size": 4149
        },
        {
          "path": "plugins/datapeeker/skills/interpreting-results",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/interpreting-results/SKILL.md",
          "type": "blob",
          "size": 23407
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/SKILL.md",
          "type": "blob",
          "size": 56518
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/01-discovery.md",
          "type": "blob",
          "size": 3800
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/02-hypothesis-generation.md",
          "type": "blob",
          "size": 5156
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/03-prioritization.md",
          "type": "blob",
          "size": 5312
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/04-experiment-tracker.md",
          "type": "blob",
          "size": 2950
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/05-synthesis.md",
          "type": "blob",
          "size": 4248
        },
        {
          "path": "plugins/datapeeker/skills/marketing-experimentation/templates/06-iteration-plan.md",
          "type": "blob",
          "size": 6694
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/SKILL.md",
          "type": "blob",
          "size": 20986
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/formats",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/formats/citations.md",
          "type": "blob",
          "size": 18777
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/formats/reproducibility.md",
          "type": "blob",
          "size": 20302
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/frameworks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/frameworks/3-paragraph-essay.md",
          "type": "blob",
          "size": 20920
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/frameworks/narrative-structure.md",
          "type": "blob",
          "size": 21276
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/tools/marp.md",
          "type": "blob",
          "size": 19294
        },
        {
          "path": "plugins/datapeeker/skills/presenting-data/tools/pandoc.md",
          "type": "blob",
          "size": 25147
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/SKILL.md",
          "type": "blob",
          "size": 21563
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/focus-groups",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/focus-groups/phase-1-facilitator-guide.md",
          "type": "blob",
          "size": 15733
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/focus-groups/phase-2-session-execution.md",
          "type": "blob",
          "size": 16673
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/interviews",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/interviews/phase-1-interview-guide.md",
          "type": "blob",
          "size": 8123
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/interviews/phase-2-interview-execution.md",
          "type": "blob",
          "size": 13309
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/observations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/observations/phase-1-observation-protocol.md",
          "type": "blob",
          "size": 15440
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/observations/phase-2-field-work.md",
          "type": "blob",
          "size": 17751
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/overview-summary.md",
          "type": "blob",
          "size": 2143
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/phase-3-familiarization.md",
          "type": "blob",
          "size": 3171
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/phase-4-coding.md",
          "type": "blob",
          "size": 6019
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/phase-5-themes.md",
          "type": "blob",
          "size": 4457
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/phase-6-reporting.md",
          "type": "blob",
          "size": 6852
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/surveys",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/surveys/phase-1-survey-design.md",
          "type": "blob",
          "size": 13163
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/templates/surveys/phase-2-survey-distribution.md",
          "type": "blob",
          "size": 11909
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/CLAUDE.md",
          "type": "blob",
          "size": 8887
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/baseline-results.md",
          "type": "blob",
          "size": 7319
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/green-results.md",
          "type": "blob",
          "size": 10237
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/rationalization-patterns.md",
          "type": "blob",
          "size": 7946
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/scenario-1-skip-bias-documentation.md",
          "type": "blob",
          "size": 2260
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/scenario-2-skip-intercoder-reliability.md",
          "type": "blob",
          "size": 2333
        },
        {
          "path": "plugins/datapeeker/skills/qualitative-research/tests/scenario-3-skip-disconfirming-evidence.md",
          "type": "blob",
          "size": 2795
        },
        {
          "path": "plugins/datapeeker/skills/understanding-data",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/understanding-data/SKILL.md",
          "type": "blob",
          "size": 11528
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/SKILL.md",
          "type": "blob",
          "size": 5886
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/exploring-schema.md",
          "type": "blob",
          "size": 6110
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/formatting-output.md",
          "type": "blob",
          "size": 9615
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/importing-data.md",
          "type": "blob",
          "size": 11221
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/invoking-cli.md",
          "type": "blob",
          "size": 11413
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/optimizing-performance.md",
          "type": "blob",
          "size": 9800
        },
        {
          "path": "plugins/datapeeker/skills/using-sqlite/writing-queries.md",
          "type": "blob",
          "size": 9637
        },
        {
          "path": "plugins/datapeeker/skills/writing-queries",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/skills/writing-queries/SKILL.md",
          "type": "blob",
          "size": 24214
        },
        {
          "path": "plugins/datapeeker/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/templates/_template",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/datapeeker/templates/_template/README.md",
          "type": "blob",
          "size": 2134
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"tilmon-eng-skills\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Tilmon Engineering institutional knowledge marketplace - AI agent skills for analysis, security, customer service, and knowledge management\",\n  \"owner\": {\n    \"name\": \"Tilmon Engineering\",\n    \"email\": \"team@tilmonengineering.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"datapeeker\",\n      \"description\": \"Structured research methods for AI agents - hypothesis testing, exploratory analysis, comparative analysis, and qualitative research\",\n      \"version\": \"1.0.0\",\n      \"source\": \"./plugins/datapeeker\",\n      \"author\": {\n        \"name\": \"Tilmon Engineering\",\n        \"email\": \"team@tilmonengineering.com\"\n      },\n      \"keywords\": [\"data-analysis\", \"research-methods\", \"hypothesis-testing\", \"qualitative-research\"]\n    },\n    {\n      \"name\": \"autonomy\",\n      \"description\": \"Enable AI agents to iteratively self-direct in pursuit of open-ended goals with state continuity across conversations through iteration journals\",\n      \"version\": \"1.0.0\",\n      \"source\": \"./plugins/autonomy\",\n      \"author\": {\n        \"name\": \"Tilmon Engineering\",\n        \"email\": \"team@tilmonengineering.com\"\n      },\n      \"keywords\": [\"autonomy\", \"iteration\", \"goal-pursuit\", \"state-continuity\", \"journal\", \"open-ended-goals\"]\n    }\n  ]\n}\n",
        "plugins/autonomy/.claude-plugin/plugin.json": "{\n  \"name\": \"autonomy\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Enable AI agents to iteratively self-direct in pursuit of open-ended goals with state continuity across conversations through iteration journals\",\n  \"author\": {\n    \"name\": \"Tilmon Engineering\",\n    \"email\": \"team@tilmonengineering.com\"\n  },\n  \"homepage\": \"https://github.com/tilmon-engineering/tilmon-eng-skills\",\n  \"repository\": \"https://github.com/tilmon-engineering/tilmon-eng-skills\",\n  \"license\": \"UNLICENSED\",\n  \"keywords\": [\n    \"autonomy\",\n    \"iteration\",\n    \"goal-pursuit\",\n    \"state-continuity\",\n    \"journal\",\n    \"open-ended-goals\"\n  ]\n}\n",
        "plugins/autonomy/README.md": "# Autonomy Plugin\n\nEnable AI agents to iteratively self-direct in pursuit of open-ended goals with state continuity across conversations through iteration journals.\n\n## Overview\n\n**Autonomy** is for goals that never truly end - ongoing optimization, continuous improvement, and iterative exploration. Unlike closed goals with a definition of done, autonomy goals are about making incremental progress toward an ever-evolving target.\n\n**Example open-ended goals:**\n- Maximize monthly recurring revenue\n- Improve developer productivity\n- Reduce customer churn\n- Optimize application performance\n- Grow monthly active users\n\n**Not for:** Closed goals with clear completion criteria (use ed3d-superpowers workflow instead)\n\n## Core Concept: Iteration Journals\n\nEach conversation is one **iteration**. The plugin maintains state across iterations through:\n\n1. **goal.md** - Stable goal definition and current metrics\n2. **iteration-NNNN-YYYY-MM-DD.md** - Journal entry for each iteration (4-digit numbering)\n3. **summary.md** - Condensed history of older iterations\n\nIteration journals are automatically committed to git with tags for easy navigation.\n\nThis creates a system of record that enables the agent to understand where it's been and where it's going.\n\n## Quick Start\n\n**Standard workflow:**\n```bash\n# Create a new goal (one-time setup)\n/create-goal\n\n# Start first iteration\n/start-iteration\n\n# Work on the goal using any skills/workflows\n# ... do work ...\n\n# Save progress mid-iteration if needed\n/checkpoint-iteration\n\n# End iteration when done\n/end-iteration\n\n# Resume in next conversation\n/start-iteration\n\n# Check progress anytime\n/review-progress\n\n# Analyze another branch for insights\n/analyze-branch experiment-a \"what we tried\"\n```\n\n**Slime mold strategy workflow (parallel exploration):**\n```bash\n# One-command setup for parallel exploration\n/slime\n\n# Start first real iteration\n/start-iteration\n\n# Work on the goal\n# ... do work ...\n\n# End iteration\n/end-iteration\n\n# Fork new exploration branch to try different approach\n/fork-iteration alternative-approach\n\n# Work on alternative branch\n/start-iteration\n# ... do work ...\n/end-iteration\n\n# Compare branches to see different outcomes\n/compare-branches [goal-name] alternative-approach\n\n# Extract insights from other branch\n/analyze-branch alternative-approach \"optimization techniques\"\n\n# List all exploration branches\n/list-branches recent\n```\n\n**Parallel agent workflow (with worktrees):**\n```bash\n# Terminal 1: Create first experiment with isolated worktree\n/fork-worktree experiment-pricing-a\ncd .worktrees/autonomy/experiment-pricing-a\n/start-iteration\n# Agent 1 works on usage-based pricing approach\n# ... do work ...\n/end-iteration\n\n# Terminal 2: Create second experiment (while agent 1 is still working)\n/fork-worktree experiment-pricing-b\ncd .worktrees/autonomy/experiment-pricing-b\n/start-iteration\n# Agent 2 works on flat enterprise pricing approach\n# ... do work ...\n/end-iteration\n\n# Terminal 3: Create third experiment\n/fork-worktree experiment-hybrid\ncd .worktrees/autonomy/experiment-hybrid\n/start-iteration\n# Agent 3 explores hybrid approach\n# ... do work ...\n/end-iteration\n\n# From any location: analyze and compare\n/list-worktrees                                    # See active worktrees\n/list-branches                                     # See all branches\n/compare-branches experiment-pricing-a experiment-pricing-b\n/analyze-branch experiment-pricing-a \"successful strategies\"\n\n# Clean up worktrees when done (branches persist)\n/remove-worktree experiment-pricing-a\n/remove-worktree experiment-pricing-b\n# experiment-hybrid continues in its worktree\n```\n\n**When to use worktrees:**\n- Running multiple Claude agents in parallel on different autonomy branches\n- Each agent needs isolated working directory\n- Testing multiple approaches simultaneously\n\n**When NOT to use worktrees:**\n- Single agent workflow (use `/fork-iteration` instead)\n- Working sequentially on different branches (just git checkout)\n\n## Commands\n\n### `/create-goal`\n\nOne-time setup for a new open-ended goal.\n\n**Process:**\n- Prompts for goal statement and metrics\n- Creates `autonomy/[goal-name]/` directory\n- Writes goal.md with goal definition\n\n**When to use:** Before starting your first iteration on a new goal\n\n### `/slime`\n\nOne-command setup for the \"slime mold strategy\" - parallel exploration using autonomy branches as a genetic algorithm.\n\n**If no goal exists yet (full setup):**\n- Invokes `/create-goal` to define goal and create `autonomy/[goal-name]/` directory\n- Creates/updates `autonomy/CLAUDE.md` with slime mold strategy documentation\n- Invokes `/fork-iteration` to create initial branch `autonomy/[goal-name]`\n- Creates `iteration-0000-YYYY-MM-DD.md` as baseline setup journal\n- Makes git commit with tag `autonomy/[goal-name]/iteration-0000`\n\n**If goal already exists (idempotent):**\n- Updates `autonomy/CLAUDE.md` to ensure slime mold strategy description is current\n- Skips goal creation, branching, and iteration 0000\n\n**What is the slime mold strategy?**\n\nLike a slime mold organism extending multiple tendrils to find optimal paths, this strategy maintains parallel autonomy branches exploring different approaches to the same goal. Branches are NOT competingâ€”they cooperate as part of the same organism. Use `/analyze-branch` to extract insights from other branches and incorporate them into your current work. Use `/compare-branches` to understand how different approaches diverged. The goal is to explore the solution space thoroughly and cross-pollinate insights across branches.\n\n**After running `/slime`:**\n- Run `/start-iteration` to begin iteration 0001 (first real work iteration)\n- Use `/fork-iteration <strategy-name>` to create additional exploration branches\n- Use `/analyze-branch <branch> <search>` to cross-pollinate learnings\n- Use `/compare-branches <branch-a> <branch-b>` to compare approaches\n\n**When to use:** When adopting parallel exploration strategy with multiple autonomy branches\n\n### `/start-iteration`\n\nBegin a new iteration for an existing open-ended goal.\n\n**First iteration:**\n- Creates initial journal file with iteration intention\n- Prompts: \"What do you want to accomplish this iteration?\"\n\n**Subsequent iterations:**\n- Loads context from last 3-5 iterations\n- Summarizes older iterations if >5 exist\n- Presents current state, blockers, and next steps\n\n**Output:** Ready to continue working toward goal\n\n### `/checkpoint-iteration`\n\nSave current progress mid-iteration before context compaction.\n\n**Process:**\n- Updates journal Work Performed section with progress so far\n- Merges with any previous checkpoint content\n- Preserves state against potential context loss\n\n**When to use:** During long conversations or before risky operations\n\n### `/end-iteration`\n\nConclude current iteration and write journal entry.\n\n**Process:**\n- Reviews conversation for skills used, decisions made\n- Documents artifacts created and blockers encountered\n- Captures reasoning and strategy changes\n- Completes journal entry (updates existing file from start-iteration)\n- Updates summary.md every 5 iterations\n- **Commits journal to git** with tag `autonomy/iteration-NNNN`\n\n**Output:** Journal file completed and committed, ready for next iteration\n\n### `/review-progress`\n\nAssess progress toward goal across all iterations.\n\n**Process:**\n- Reads all journal entries (or summary + recent)\n- Analyzes metrics, trends, patterns\n- Identifies what's working and what's not\n- Provides honest assessment with recommendations\n\n**Output:** Comprehensive progress report\n\n### `/analyze-branch [branch-name] [search-criteria]`\n\nAnalyze another branch's iteration journals to extract findings and insights.\n\n**Process:**\n- Uses `git merge-base` to find where branches diverged\n- Extracts iteration journals from divergent commits on target branch\n- Searches for relevant content based on search criteria\n- Produces markdown report with findings, decisions, and lessons\n\n**Arguments:**\n- `branch-name` - Git branch to analyze (e.g., \"experiment-a\")\n- `search-criteria` - Free-text description of what to look for (e.g., \"pricing experiments\")\n\n**Output:** Markdown report ready for inclusion in current journal's \"External Context Gathered\" section\n\n**Use cases:**\n- Review work from parallel experiment branches\n- Extract lessons from concluded experiments\n- Compare approaches across different branches\n- Find salvageable ideas from any branch\n\n**Example:**\n```bash\n/analyze-branch experiment-a \"API optimization and performance improvements\"\n```\n\n## Branch Management Commands\n\nThe autonomy plugin includes commands for managing exploration branches in the \"slime mold strategy\" workflow. These commands only operate on `autonomy/*` branches.\n\n### `/list-branches [optional-query]`\n\nInventory all autonomy branches with flexible sorting, grouping, and filtering.\n\n**Arguments:**\n- `optional-query` - Free-text description of how to sort, group, and what information to display\n- If no query provided, defaults to: sort by most recent update, show all branches\n\n**Process:**\n- Finds all `autonomy/*` branches via git\n- Reads latest journal commit from each branch (commit messages, not files)\n- Dispatches branch-analyzer agent to generate Python script for computational analysis\n- Produces formatted markdown table with requested information\n\n**Examples:**\n```bash\n/list-branches\n/list-branches sort by most recent, show only active\n/list-branches group by status, show metrics\n/list-branches show branches updated in last 30 days\n```\n\n**Output:** Markdown table showing branch name, latest iteration, last update, status, metrics, and next steps\n\n### `/fork-iteration [iteration] <strategy-name>`\n\nCreate new autonomy branch forked from current commit or specific past iteration.\n\n**Arguments:**\n- `iteration` (optional) - Iteration number (NNNN format) to fork from. If provided, searches current branch history for matching iteration tag. If omitted, forks from current HEAD.\n- `strategy-name` (required) - Name for new branch (kebab-case). Will become `autonomy/<strategy-name>`.\n\n**Process:**\n- Resolves fork point (iteration tag or current HEAD)\n- Validates fork point exists\n- Checks out fork point\n- Creates new branch `autonomy/<strategy-name>`\n- Reports success with next steps\n\n**Examples:**\n```bash\n# Fork from current commit\n/fork-iteration experiment-b\n\n# Fork from specific iteration in current branch history\n/fork-iteration 0015 experiment-b\n\n# Bootstrap autonomy workflow from non-autonomy branch\ngit checkout main\n/fork-iteration initial-strategy\n```\n\n**Output:** Branch created message with next steps (`/start-iteration` to begin work)\n\n**Note:** This creates the branch but does NOT start an iteration. Iteration numbering is handled by `/start-iteration`.\n\n### `/branch-status <branch-name>`\n\nDetailed status report for single autonomy branch.\n\n**Arguments:**\n- `branch-name` (required) - Branch name to analyze. `autonomy/` prefix optional (added automatically if missing).\n\n**Process:**\n- Normalizes branch name (adds `autonomy/` prefix if missing)\n- Validates branch exists\n- Dispatches branch-analyzer agent to read all journal commits on branch\n- Agent generates Python script to analyze iteration timeline, metrics progression, status changes\n- Produces comprehensive report\n\n**Example:**\n```bash\n/branch-status experiment-a\n/branch-status autonomy/experiment-b\n```\n\n**Output:**\n- Complete iteration timeline with dates and status\n- Metrics progression over time (if applicable)\n- Blocker history\n- Current state assessment\n- Recommended next actions\n\n### `/compare-branches <branch-a> <branch-b>`\n\nCompare two autonomy branches to show different approaches and outcomes.\n\n**Arguments:**\n- `branch-a` (required) - First branch name. `autonomy/` prefix optional.\n- `branch-b` (required) - Second branch name. `autonomy/` prefix optional.\n\n**Process:**\n- Normalizes both branch names\n- Validates both branches exist\n- Uses `git merge-base` to find where branches diverged\n- Dispatches branch-analyzer agent for comparative analysis\n- Agent generates Python script for comparison\n- Produces report showing differences\n\n**Example:**\n```bash\n/compare-branches experiment-a experiment-b\n/compare-branches autonomy/usage-pricing autonomy/flat-enterprise\n```\n\n**Output:**\n- Where branches diverged (common ancestor)\n- Iteration timeline comparison\n- Metrics trajectories (if applicable)\n- Status comparison\n- Different decisions made\n- Outcomes on each branch\n- Cross-branch learning insights\n\n### `/fork-worktree [iteration] <strategy-name>`\n\nCreate new autonomy branch with dedicated worktree for parallel agent workflows.\n\n**Arguments:**\n- `iteration` (optional) - Iteration number to fork from (same as `/fork-iteration`)\n- `strategy-name` (required) - Name for branch and worktree (kebab-case)\n\n**Process:**\n- Detects repository root (works from main repo or within other worktrees)\n- Resolves fork point (iteration tag or current HEAD)\n- Creates branch `autonomy/<strategy-name>` with worktree at `.worktrees/autonomy/<strategy-name>/`\n- All worktrees created at repository root level (no nested worktrees)\n\n**Example:**\n```bash\n# Fork from current commit\n/fork-worktree experiment-b\n\n# Fork from specific iteration\n/fork-worktree 0015 experiment-c\n\n# Fork from within another worktree (creates sibling, not nested)\ncd .worktrees/autonomy/experiment-a\n/fork-worktree experiment-d  # Creates .worktrees/autonomy/experiment-d/ at root\n```\n\n**Next steps:**\n```bash\ncd .worktrees/autonomy/<strategy-name>\n/start-iteration\n```\n\n**Difference from `/fork-iteration`:**\n- `/fork-iteration`: Creates branch in current working directory\n- `/fork-worktree`: Creates branch + isolated worktree for parallel work\n- Use worktrees for running multiple agents simultaneously\n\n### `/remove-worktree [--force] <strategy-name>`\n\nSafely remove autonomy worktree while preserving branch and history.\n\n**Arguments:**\n- `--force` (optional) - Skip uncommitted changes check and force removal\n- `strategy-name` (required) - Worktree to remove (without `autonomy/` prefix)\n\n**Process:**\n- Detects repository root\n- Validates worktree exists\n- Checks for uncommitted changes (unless `--force`)\n- Removes worktree directory\n- Prunes git metadata\n\n**Example:**\n```bash\n# Safe removal (fails if uncommitted changes)\n/remove-worktree experiment-b\n\n# Force removal (discards uncommitted changes)\n/remove-worktree --force experiment-b\n```\n\n**What gets removed:**\n- Worktree directory `.worktrees/autonomy/<strategy-name>/`\n- Git worktree metadata\n\n**What persists:**\n- Branch `autonomy/<strategy-name>` and all commits\n- All iteration tags\n- Git history (can checkout branch later or create new worktree)\n\n### `/list-worktrees`\n\nList all autonomy worktrees with status and location.\n\n**Process:**\n- Finds all git worktrees\n- Filters to autonomy worktrees (`.worktrees/autonomy/`)\n- Displays formatted table\n\n**Example output:**\n```\nAutonomy Worktrees:\n\nBranch                    Path                                      HEAD       Locked\nautonomy/experiment-a     .worktrees/autonomy/experiment-a          a1b2c3d\nautonomy/experiment-b     .worktrees/autonomy/experiment-b          d4e5f6g    ðŸ”’\n\nTotal: 2 autonomy worktrees\n```\n\n**Note:** This lists worktrees only, not all branches. Use `/list-branches` to see all autonomy branches (including those without worktrees).\n\n## Directory Structure\n\n**Main repository structure:**\n```\nautonomy/[goal-name]/\nâ”œâ”€â”€ goal.md                          # Goal definition and metrics\nâ”œâ”€â”€ summary.md                       # Condensed history (updated every 5 iterations)\nâ”œâ”€â”€ iteration-0001-YYYY-MM-DD.md    # First iteration journal\nâ”œâ”€â”€ iteration-0002-YYYY-MM-DD.md    # Second iteration journal\nâ””â”€â”€ iteration-NNNN-YYYY-MM-DD.md    # Nth iteration journal (4-digit numbering)\n```\n\n**With worktrees (for parallel agent workflows):**\n```\nproject-root/\nâ”œâ”€â”€ .git/                            # Git repository metadata\nâ”œâ”€â”€ .worktrees/                      # Worktree container (gitignored)\nâ”‚   â””â”€â”€ autonomy/                    # Autonomy-specific worktrees\nâ”‚       â”œâ”€â”€ experiment-a/            # Worktree for autonomy/experiment-a\nâ”‚       â”‚   â”œâ”€â”€ autonomy/\nâ”‚       â”‚   â”‚   â””â”€â”€ goal-name/\nâ”‚       â”‚   â”‚       â”œâ”€â”€ goal.md\nâ”‚       â”‚   â”‚       â””â”€â”€ iteration-0001-YYYY-MM-DD.md\nâ”‚       â”‚   â””â”€â”€ [other project files]\nâ”‚       â””â”€â”€ experiment-b/            # Worktree for autonomy/experiment-b\nâ”‚           â””â”€â”€ autonomy/\nâ”‚               â””â”€â”€ goal-name/\nâ”‚                   â””â”€â”€ iteration-0001-YYYY-MM-DD.md\nâ””â”€â”€ autonomy/                        # Main repo autonomy work (optional)\n    â””â”€â”€ goal-name/\n```\n\n**Notes:**\n- Iteration numbering: 4-digit zero-padded format (0001-9999) supports up to 9999 iterations\n- Each worktree is a complete working copy with own `autonomy/` directory\n- All worktrees share same `.git` directory (commits visible across all worktrees)\n- Add `.worktrees/` to `.gitignore` to prevent committing worktree directories\n\n## Iteration Journal Format\n\nEach iteration journal documents:\n\n### Beginning State\nWhat was the state when iteration started?\n- Current progress\n- Known blockers\n- Open questions\n\n### Work Performed\nWhat happened during this iteration?\n- **Skills & Workflows Used** - Which methodologies were applied\n- **Key Decisions Made** - Major choices with rationale\n- **Artifacts Created/Modified** - Files, commits, PRs\n- **External Context Gathered** - Research, feedback, docs\n- **Reasoning & Strategy Changes** - Why approach evolved\n- **Blockers Encountered** - What's preventing progress\n- **Open Questions** - What needs resolution\n\n### Ending State\nWhat is the state now?\n- Progress made\n- Updated metrics\n- Recommended next steps\n\n### Iteration Metadata\nExecution details:\n- Context usage\n- Checkpoint count\n- Suggested next action\n\n## Git Integration\n\nAutonomy automatically commits journal files to git when ending an iteration:\n\n**What gets committed:**\n- `iteration-NNNN-YYYY-MM-DD.md` (always)\n- `summary.md` (when updated - every 5 iterations)\n\n**Commit format (enhanced for branch management):**\n```\njournal: [goal-name] iteration NNNN\n\n[2-3 line summary of work completed]\n\n## Journal Summary\n\n[4-6 sentence summary of iteration - what happened, what was learned, what changed]\n\n## Iteration Metadata\n\nStatus: [active|blocked|concluded|dead-end]\nMetrics: [quantitative metrics or \"None\"]\nBlockers: [critical blockers or \"None\"]\nNext: [next iteration intention]\n\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n\n**Enhanced commit metadata:**\n- **Status** - Determined from Ending State: active (default), blocked (waiting on dependency), concluded (goal achieved), dead-end (approach invalidated)\n- **Metrics** - Quantitative progress indicators extracted from Ending State (e.g., \"MRR: $62k (+12%)\")\n- **Blockers** - Summary of current blockers from journal\n- **Next** - Next iteration intention/recommended action\n\nThis metadata enables branch management commands to summarize branches without reading journal files.\n\n**Git tag (branch-aware):**\n- On autonomy branches: `autonomy/<branch-name>/iteration-NNNN` (e.g., `autonomy/experiment-a/iteration-0015`)\n- On non-autonomy branches: `autonomy/<goal-name>/iteration-NNNN` (backward compatibility)\n- Each branch has its own iteration namespace\n- No tag collisions when multiple branches have same iteration number\n\n**Branch behavior:**\n- Commits to current branch (does NOT create new branch)\n- Respects your branching strategy\n- You control when to merge/push\n\n**Tag benefits:**\n- Navigate to iteration on specific branch: `git checkout autonomy/experiment-a/iteration-0042`\n- List all iterations on a branch: `git tag -l 'autonomy/experiment-a/*'`\n- List all autonomy iterations: `git tag -l 'autonomy/*/*'`\n- CI/CD integration: Trigger on `autonomy/*/iteration-*` tags\n- Immutable history markers\n- Enables branch management queries without reading journal files\n\n**Error handling:**\n- If git operations fail, iteration still completes\n- Journal is always written (critical for continuity)\n- Manual commit commands provided if needed\n- Graceful degradation ensures no data loss\n\n## Skills\n\n### `starting-an-iteration`\n\nInvoked by `/start-iteration` command. Handles:\n- Goal detection and setup\n- Context loading from previous iterations\n- Agent delegation for journal reading and summarization\n- State presentation to user\n\n### `ending-an-iteration`\n\nInvoked by `/end-iteration` command. Handles:\n- Conversation review for key information\n- Comprehensive journal entry completion\n- Summary updates (every 5 iterations)\n- Git commit and tagging (automatic)\n- Completion announcement\n\n### `checkpointing-an-iteration`\n\nInvoked by `/checkpoint-iteration` command. Handles:\n- Mid-iteration progress capture\n- Incremental journal updates\n- Context preservation before compaction\n\n### `creating-a-goal`\n\nInvoked by `/create-goal` command. Handles:\n- Goal statement collection\n- Directory structure creation\n- goal.md initialization\n\n### `slime-strategy`\n\nInvoked by `/slime` command. Handles:\n- Checking if autonomy workflow already exists\n- Orchestrating creating-a-goal skill for new goals\n- Creating/updating autonomy/CLAUDE.md with slime mold strategy documentation\n- Orchestrating forking-iteration skill to create initial branch\n- Creating iteration-0000 baseline journal\n- Git commit and branch-aware tagging\n- Idempotent behavior (only updates CLAUDE.md if goal exists)\n\n### `reviewing-progress`\n\nInvoked by `/review-progress` command. Handles:\n- Full history reading\n- Progress analysis and trend identification\n- Honest assessment with recommendations\n- Strategic guidance\n\n### `analyzing-branches`\n\nInvoked by `/analyze-branch` command. Handles:\n- Git merge-base operations to find divergence point\n- Extraction of iteration journals from target branch\n- Content searching based on user criteria\n- Report generation for cross-branch learning\n\n### `listing-branches`\n\nInvoked by `/list-branches` command. Handles:\n- Parsing user's query for sorting/grouping/filtering requirements\n- Dispatching branch-analyzer agent with query\n- Presenting formatted markdown table of all autonomy branches\n\n### `forking-iteration`\n\nInvoked by `/fork-iteration` command. Handles:\n- Parsing and validating strategy name\n- Resolving fork point (iteration tag or HEAD)\n- Checking out fork point and creating new autonomy branch\n- Direct git operations (no agent needed)\n\n### `analyzing-branch-status`\n\nInvoked by `/branch-status` command. Handles:\n- Normalizing and validating branch name\n- Dispatching branch-analyzer agent for comprehensive analysis\n- Presenting detailed status report for single branch\n\n### `comparing-branches`\n\nInvoked by `/compare-branches` command. Handles:\n- Normalizing and validating both branch names\n- Using git merge-base to find divergence point\n- Dispatching branch-analyzer agent for comparative analysis\n- Presenting comparison report\n\n## Agents\n\n### `journal-reader` (Haiku)\n\nReads and structures recent journal entries (last 3-5) for context loading. Extracts:\n- Current state\n- Open blockers and questions\n- Recent progress\n- Key metrics\n- Recommended next steps\n\n### `journal-summarizer` (Haiku)\n\nCondenses older iterations into summary.md. Preserves:\n- Major milestones\n- Persistent blockers\n- Strategic pivots\n- Key learnings\n- Metric trends\n\n### `branch-analyzer` (Haiku)\n\nAnalyzes autonomy branches via git operations and computational methods. Used by list-branches, branch-status, and compare-branches commands. Handles:\n- Reading git log data from autonomy branches\n- Parsing enhanced commit message metadata (status, metrics, blockers, next steps)\n- Generating Python scripts for computational analysis (sorting, filtering, grouping, comparison)\n- Executing Python scripts for precise results (never \"eyeball it\")\n- Formatting output as markdown tables and reports\n- Read-only operations (never checks out branches or modifies files)\n\n## Integration with Other Plugins\n\nAutonomy automatically detects and logs skills from any plugin:\n- Scans conversation for `Skill` tool invocations\n- Records skill name and observed purpose\n- No hardcoded plugin list - works with current and future plugins\n\n## Context Loading Strategy\n\n**Iterations 1-5:**\n- Load all iterations in detail via journal-reader\n\n**Iterations 6+:**\n- Load last 3-5 iterations in detail\n- Sub-agent reads and summarizes older iterations\n- Balance between context and performance\n\n**Summary updates:**\n- Automatically triggered every 5 iterations (5, 10, 15, etc.)\n- Keeps summary.md current without manual updates\n\n## Goal Lifecycle\n\nGoals are **append-only** - you can always add new iterations:\n- No \"complete\" state (goals are open-ended)\n- Can pause by not starting new iterations\n- Can resume anytime, even after long gaps\n- Mark as \"Paused\" in goal.md if desired\n\n## Advanced Workflow: Exploratory Branching\n\n### The Slime Mold Strategy\n\nGit branches become **exploratory tendrils** searching for solutions. Instead of a linear progression toward a single solution, you spawn multiple independent branches that explore different approaches simultaneously.\n\n**Key Concept:** Like a slime mold extending tendrils to search for resources, each branch is an autonomous experiment. Some tendrils find nutrients and thrive. Others reach dead ends and stop. Living branches can **reincorporate discoveries** from other branches using `/analyze-branch`.\n\n### How It Works\n\n**1. Initial Setup**\n\n```bash\n# Create goal on main branch\ngit checkout -b main\n/create-goal  # Goal: \"Find optimal solution to X\"\ngit add autonomy/\ngit commit -m \"initial commit: autonomy goal setup\"\n\n# Main branch stays at initial commit\n# All experiments happen on separate branches\n```\n\n**2. Spawn Experiment Branches**\n\n```bash\n# Start first exploration path using branch management\n/fork-iteration experiment-pricing-model-a\n/start-iteration\n# Intention: \"Explore usage-based pricing with tiered caps\"\n# ... work on this approach ...\n/end-iteration\n\n# Start parallel exploration path\n/fork-iteration experiment-pricing-model-b\n/start-iteration\n# Intention: \"Explore flat enterprise pricing with seat limits\"\n# ... work on this approach ...\n/end-iteration\n\n# List all branches to see status\n/list-branches\n```\n\n**3. Branches Evolve Independently**\n\nEach branch has its own iteration history:\n- `experiment-pricing-model-a`: Iterations 0001-0015\n- `experiment-pricing-model-b`: Iterations 0001-0023\n- `experiment-hybrid-approach`: Iterations 0001-0008\n\n**No merging required.** Branches are independent experiments, not feature branches.\n\n**4. Learn Across Branches**\n\nWhen one branch discovers something useful:\n\n```bash\n# On experiment-pricing-model-b (the successful path)\n/analyze-branch experiment-pricing-model-a \"usage-based billing experiments\"\n\n# Agent extracts:\n# - What worked: Real-time usage tracking implementation\n# - What failed: Tiered caps confused users\n# - Good ideas: API rate limiting patterns from iteration 0012\n```\n\nCopy relevant findings into current iteration's \"External Context Gathered\" section. The living branch **reincorporates** discoveries from other tendrils.\n\n**5. Branches Reach Conclusions**\n\nSome branches succeed and continue indefinitely. Others reach natural conclusions:\n\n```bash\n# On experiment-pricing-model-a\n/end-iteration\n# Ending State: \"Dead end. Tiered caps add complexity without value.\n#                Users prefer simple per-unit pricing.\n#                Abandoning this approach.\"\n```\n\nBranch stops. Remains in git history for future analysis.\n\n**6. New Branches Spawn from Insights**\n\n```bash\n# Compare approaches to inform new direction\n/compare-branches experiment-pricing-model-a experiment-pricing-model-b\n\n# Fork new branch from iteration 0015 (divergence point)\n/fork-iteration 0015 experiment-simple-per-unit\n/start-iteration\n# Intention: \"Pure per-unit pricing. Incorporating usage tracking\n#             from experiment-a and simplicity from experiment-b.\"\n\n# Extract specific learnings from each branch\n/analyze-branch experiment-pricing-model-a \"usage tracking implementation\"\n/analyze-branch experiment-pricing-model-b \"simple pricing communication\"\n```\n\n### Workflow Characteristics\n\n**No canonical \"main\" branch:**\n- `main` may just be the initial commit\n- All work happens on experiment branches\n- No branch is \"more correct\" than others\n\n**Branches are peers, not hierarchical:**\n- experiment-a isn't a \"feature branch\" of main\n- experiment-b doesn't \"merge into\" experiment-a\n- Each branch is an autonomous exploration path\n\n**Natural selection through iteration:**\n- Productive branches accumulate iterations\n- Dead-end branches stop naturally\n- No manual cleanup required - git history preserves everything\n\n**Cross-pollination via `/analyze-branch`:**\n- Living branches extract insights from others (living or dead)\n- Ideas propagate across independent lineages\n- Best patterns emerge organically\n\n### Example: Three-Branch Evolution\n\n**Timeline:**\n\n```\nmain (iteration 0, initial commit)\n  â”‚\n  â”œâ”€ experiment-redis-cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (iterations 1-8, dead end)\n  â”‚\n  â”œâ”€ experiment-cdn-strategy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬ (iterations 1-15, ongoing)\n  â”‚                                        â”‚\n  â”‚                                        â””â”€ experiment-hybrid-caching â”€ (iterations 1-3, new)\n  â”‚\n  â””â”€ experiment-edge-computing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (iterations 1-20, promising)\n```\n\n**Branch statuses:**\n- **experiment-redis-cache**: Dead end (iteration 8 concluded \"too expensive at scale\")\n- **experiment-cdn-strategy**: Active, moderate progress\n- **experiment-edge-computing**: Active, showing strong results\n- **experiment-hybrid-caching**: New fork combining CDN + edge insights\n\n**Cross-branch learning:**\n\nOn `experiment-hybrid-caching`:\n```bash\n/analyze-branch experiment-redis-cache \"caching invalidation patterns\"\n/analyze-branch experiment-edge-computing \"latency optimization techniques\"\n```\n\n### When to Use This Workflow\n\n**Good for:**\n- Unclear solution space (multiple valid approaches)\n- High experimentation cost (want to explore in parallel)\n- Long-term R&D (months or years of iteration)\n- Learning what works through trial and error\n- Problems where the \"right\" answer isn't obvious\n\n**Not good for:**\n- Well-defined tasks with known solutions\n- Short-term projects (overhead not worth it)\n- When you need a single canonical answer quickly\n\n### Git Repository as Exploration Map\n\nYour git history becomes a **map of explored territory**:\n\n```bash\n# List all autonomy branches with status\n/list-branches\n\n# See detailed status of specific branch\n/branch-status experiment-a\n\n# Compare two branches\n/compare-branches experiment-a experiment-b\n\n# List all iteration tags across all branches\ngit tag -l 'autonomy/*/*'\n\n# List iterations on specific branch\ngit tag -l 'autonomy/experiment-a/*'\n\n# View branch divergence points\ngit log --graph --oneline --all\n\n# Jump to any iteration on any branch\ngit checkout autonomy/experiment-a/iteration-0042\n```\n\nThe repository preserves the **entire exploration**, not just the final answer. Future work can mine this history for insights using branch management commands.\n\n## Best Practices\n\n### When to Start Iteration\n- Beginning of new conversation session\n- After long break from goal\n- When context from previous work is needed\n\n### When to End Iteration\n- Natural stopping point (subtask complete)\n- Blocked on external dependency\n- Context limit approaching\n- Good handoff point for future iteration\n\n### Review Frequency\n- Every 5 iterations (automatic checkpoint)\n- Before major strategic decisions\n- When feeling uncertain about direction\n- If progress seems stalled\n\n### Journal Quality\n- Be comprehensive - future iterations depend on it\n- Document rationale, not just actions\n- Capture all blockers, even minor ones\n- Specific next steps, not vague \"continue working\"\n\n## Examples\n\n### Example: Maximize MRR Goal\n\n**Directory:** `autonomy/maximize-monthly-recurring-revenue/`\n\n**Iteration 1:** Research competitor pricing, analyze current metrics\n**Iteration 2:** Design new pricing tiers based on research\n**Iteration 3:** Implement usage-based billing\n**Iteration 4:** Run A/B test on pricing page\n**Iteration 5:** Analyze results, update summary.md\n\n**Progress:** MRR increased from $45k to $62k (+37.8%) over 5 iterations\n\n### Example: Reduce Customer Churn Goal\n\n**Directory:** `autonomy/reduce-customer-churn/`\n\n**Iteration 1:** Analyze churn data, identify patterns\n**Iteration 2:** Interview churned customers\n**Iteration 3:** Implement email engagement campaign\n**Iteration 4:** Build onboarding improvements\n**Iteration 5:** Measure impact, update summary.md\n\n**Progress:** Churn reduced from 13% to 8% over 5 iterations\n\n## Troubleshooting\n\n### \"No goal found\" when starting iteration\n- First time: Normal, will prompt for goal creation\n- Expected goal: Check if `autonomy/[goal-name]/goal.md` exists\n- Wrong directory: Navigate to project root\n\n### Journal files out of order\n- Plugin expects sequential numbering (0001, 0002, 0003...)\n- Missing numbers are okay (will skip gaps)\n- Duplicate numbers: Rename manually to fix\n\n### Git commit failed\n- Journal is still written (iteration completes normally)\n- Manual commands provided in output\n- Common causes: not in git repo, detached HEAD, permissions\n- Fix underlying issue and commit manually if needed\n\n### Summary not updating\n- Check if iteration count % 5 == 0 (5, 10, 15...)\n- Verify journal-summarizer agent ran successfully\n- Manually trigger: Can run `/review-progress` to regenerate\n\n### Context loading too slow\n- Normal for many iterations (>20)\n- Summary.md reduces load time\n- Can manually review fewer iterations by editing skill\n\n## Development\n\nSee [creating-a-plugin](../../ed3d-extending-claude/skills/creating-a-plugin/) skill for plugin development guidance.\n\n**Testing locally:**\n```bash\n/plugin install file:///absolute/path/to/autonomy\n/plugin reload\n/start-iteration\n```\n\n## License\n\nUNLICENSED - Internal use only\n\n## Author\n\nTilmon Engineering\n- Email: team@tilmonengineering.com\n- Repository: https://github.com/tilmon-engineering/tilmon-eng-skills\n",
        "plugins/autonomy/agents/branch-analyzer.md": "---\nname: branch-analyzer\ndescription: Use when analyzing autonomy branches via git log, generating Python scripts for computational analysis of iteration data, and producing formatted reports for list-branches, branch-status, or compare-branches operations\ntools: Read, Bash, Write\nmodel: haiku\n---\n\n# Branch Analyzer Agent\n\nYou are a specialized agent for analyzing autonomy branches using git operations and computational methods.\n\n## Your Responsibilities\n\n1. **Read git data** - Execute git commands to read branch and commit information\n2. **Parse commit messages** - Extract structured metadata from journal commit messages\n3. **Generate Python scripts** - Create Python code for computational analysis (sorting, grouping, filtering, comparison)\n4. **Execute analysis** - Run Python scripts to produce precise results\n5. **Format output** - Return markdown reports with tables, timelines, and insights\n\n## Tools Available\n\n- **Bash** - Execute git commands and Python scripts\n- **Write** - Create temporary Python scripts for analysis\n- **Read** - Read files if needed (though primarily use git commands)\n\n## Core Workflow\n\n### 1. Understanding the Task\n\nYou will receive one of these task types:\n\n**A) List branches** - Inventory all autonomy branches\n**B) Branch status** - Analyze single branch in detail\n**C) Compare branches** - Compare two branches\n\nThe prompt will specify:\n- Task type (list/status/compare)\n- Branch name(s) if applicable\n- User's query for sorting/grouping/filtering (for list task)\n- Specific analysis requirements\n\n### 2. Reading Git Data\n\n**For list branches:**\n```bash\n# Get all autonomy branches\ngit branch -a | grep 'autonomy/'\n\n# For each branch, get latest journal commit\nfor branch in $branches; do\n  # Find most recent journal commit\n  git log \"$branch\" --oneline | grep '^[a-f0-9]* journal:' | head -1\n\n  # Get full commit message\n  commit_hash=$(...)\n  git log -1 \"$commit_hash\" --format='%B'\ndone\n```\n\n**For branch status:**\n```bash\n# Get all journal commits on branch\ngit log \"$branch\" --oneline | grep '^[a-f0-9]* journal:'\n\n# For each commit, get full message\nfor commit in $commits; do\n  git log -1 \"$commit\" --format='%B'\ndone\n```\n\n**For compare branches:**\n```bash\n# Find common ancestor\nmerge_base=$(git merge-base \"$branch_a\" \"$branch_b\")\n\n# Get commits on branch A since divergence\ngit log \"$merge_base..$branch_a\" --oneline | grep 'journal:'\n\n# Get commits on branch B since divergence\ngit log \"$merge_base..$branch_b\" --oneline | grep 'journal:'\n\n# Read full messages for each\n```\n\n### 3. Parsing Commit Messages\n\nJournal commit messages follow this format:\n\n```\njournal: [goal-name] iteration NNNN\n\n[2-3 line summary]\n\n## Journal Summary\n\n[4-6 sentence summary of iteration]\n\n## Iteration Metadata\n\nStatus: [active|blocked|concluded|dead-end]\nMetrics: [metrics or \"None\"]\nBlockers: [blockers or \"None\"]\nNext: [next iteration intention]\n\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>\n```\n\n**Extract:**\n- Iteration number (NNNN from first line)\n- Date (from `git log --format='%ai'`)\n- Status (from \"Status:\" line)\n- Metrics (from \"Metrics:\" line)\n- Blockers (from \"Blockers:\" line)\n- Next steps (from \"Next:\" line)\n- Summary (2-3 line summary section)\n\n**Handle variations:**\n- Older commits may not have ## Journal Summary or ## Iteration Metadata sections\n- If metadata missing, extract what you can from summary\n- Default status to \"unknown\" if not specified\n\n### 4. Generating Python Scripts\n\n**Critical: Always use Python scripts for analysis. Never \"eyeball it\".**\n\nCreate temporary Python script for computational analysis:\n\n```python\n#!/usr/bin/env python3\nimport subprocess\nimport re\nfrom datetime import datetime\n\n# Data structure for commits\ncommits = []\n\n# Parse git data\n# ... (read from git commands)\n\n# Extract metadata with regex\nfor commit in raw_commits:\n    iteration_match = re.search(r'iteration (\\d{4})', commit)\n    status_match = re.search(r'Status: (\\w+)', commit)\n    metrics_match = re.search(r'Metrics: (.+)', commit)\n    # ...\n\n    commits.append({\n        'iteration': iteration_match.group(1) if iteration_match else None,\n        'status': status_match.group(1) if status_match else 'unknown',\n        'metrics': metrics_match.group(1) if metrics_match else 'None',\n        # ...\n    })\n\n# Apply sorting (if list task)\nif sort_by == 'recent':\n    commits.sort(key=lambda x: x['date'], reverse=True)\nelif sort_by == 'iteration':\n    commits.sort(key=lambda x: x['iteration'])\n\n# Apply grouping (if requested)\nif group_by == 'status':\n    groups = {}\n    for commit in commits:\n        status = commit['status']\n        if status not in groups:\n            groups[status] = []\n        groups[status].append(commit)\n\n# Apply filtering (if requested)\nif filter_active_only:\n    commits = [c for c in commits if c['status'] == 'active']\n\n# Generate markdown output\nprint(\"# Branch Analysis\\n\")\nprint(f\"Showing {len(commits)} results\\n\")\nprint(\"| Branch | Iteration | Date | Status | Metrics |\")\nprint(\"|--------|-----------|------|--------|---------|\")\nfor commit in commits:\n    print(f\"| {commit['branch']} | {commit['iteration']} | {commit['date']} | {commit['status']} | {commit['metrics']} |\")\n```\n\n**Save script:**\n```bash\n# Write Python script to temporary file\nwrite_file=\"/tmp/analyze_branches_$$.py\"\n\n# Make executable\nchmod +x \"$write_file\"\n\n# Execute\npython3 \"$write_file\"\n\n# Clean up\nrm \"$write_file\"\n```\n\n### 5. Formatting Output\n\nOutput markdown formatted for the user:\n\n**Tables:**\n- Use pipe syntax for markdown tables\n- Include headers\n- Align columns reasonably\n\n**Sections:**\n- Use ## headers for major sections\n- Use ### for subsections\n- Include horizontal rules `---` for visual separation\n\n**Lists:**\n- Use bullet points for insights\n- Use numbered lists for recommendations\n- Keep items concise (1-2 sentences)\n\n**Code blocks:**\n- Use ```markdown or ```bash for examples\n- Not needed in analysis output, only in documentation\n\n## Important Guidelines\n\n### Computational Methods Required\n\n**You MUST:**\n- Generate Python scripts for ALL analysis\n- Use regex for parsing commit messages\n- Use Python data structures for sorting/grouping\n- Execute scripts to get precise results\n\n**You MUST NOT:**\n- Manually count iterations\n- \"Eyeball\" which branch is most recent\n- Guess at groupings\n- Skip Python script generation for \"simple\" tasks\n\n### Read-Only Operations\n\n**You MUST:**\n- Use git log to read commits\n- Parse commit messages from git output\n- Keep all operations read-only\n\n**You MUST NOT:**\n- Checkout any branches\n- Modify any files\n- Create commits or tags\n- Change git state in any way\n\n### Handle Missing Data\n\nCommit messages may be incomplete:\n- Older commits may lack metadata sections\n- Some commits may have \"None\" for metrics\n- Extract what exists, don't fabricate data\n- Report \"unknown\" or \"None\" honestly\n\n### Error Handling\n\nIf git commands fail:\n```bash\nif [ $? -ne 0 ]; then\n  echo \"Error: Git command failed\"\n  echo \"Command: $command\"\n  echo \"Output: $output\"\n  exit 1\nfi\n```\n\nReturn clear error messages to user.\n\n## Example Tasks\n\n### Task: List all branches sorted by recency\n\n1. Run: `git branch -a | grep 'autonomy/'`\n2. For each branch, find latest journal commit\n3. Parse commit date and metadata\n4. Generate Python script to sort by date\n5. Execute script\n6. Output markdown table\n\n### Task: Analyze branch \"experiment-a\"\n\n1. Run: `git log autonomy/experiment-a --oneline | grep 'journal:'`\n2. Get full message for each commit\n3. Parse all metadata\n4. Generate Python script to analyze timeline, metrics, blockers\n5. Execute script\n6. Output comprehensive report with multiple sections\n\n### Task: Compare branches \"experiment-a\" and \"experiment-b\"\n\n1. Run: `git merge-base autonomy/experiment-a autonomy/experiment-b`\n2. Get commits on each branch since divergence\n3. Parse metadata from both branches\n4. Generate Python script for comparative analysis\n5. Execute script\n6. Output comparison report\n\n## Success Criteria\n\nYour output is successful if:\n- âœ“ All data extracted via git commands\n- âœ“ Python scripts used for analysis\n- âœ“ Results are computationally precise\n- âœ“ Output is well-formatted markdown\n- âœ“ Missing data handled gracefully\n- âœ“ No git state modifications\n- âœ“ Clear error messages if issues occur\n\nYour output has FAILED if:\n- âœ— Manual counting or \"eyeballing\"\n- âœ— Checking out branches\n- âœ— Modifying files or git state\n- âœ— Incomplete error handling\n- âœ— Fabricated data for missing fields\n",
        "plugins/autonomy/agents/journal-reader.md": "---\nname: journal-reader\ndescription: Use when starting an iteration to read and structure recent journal entries (last 3-5 iterations) for context loading\ntools: Read, Glob\nmodel: haiku\n---\n\n# Journal Reader Agent\n\nYou are a specialized agent for reading and structuring iteration journal files to provide context for the main conversation.\n\n**Model:** Haiku (fast, efficient text processing)\n**Used by:** `starting-an-iteration` and `reviewing-progress` skills\n\n## Your Responsibilities\n\n1. **Locate journal files** for the specified goal\n2. **Read recent iterations** (typically last 3-5 files)\n3. **Extract key information**:\n   - Current state of the goal\n   - Open blockers and questions\n   - Recent decisions and rationale\n   - Artifacts created\n   - Skills and workflows used\n4. **Structure findings** in a clear, scannable format\n\n## Workflow\n\n1. **Find journal directory**: Use Glob to locate `autonomy/[goal-name]/`\n2. **Identify recent iterations**: List iteration files, sort by number, select last N\n3. **Read each iteration**: Use Read to load journal content\n4. **Extract structured data**:\n   - Parse Beginning State, Work Performed, Ending State sections\n   - Collect blockers, open questions, artifacts\n   - Note skills used and key decisions\n5. **Present summary**: Return structured findings with clear sections\n\n## Input Expected\n\nYou will receive:\n- Goal name (kebab-case identifier)\n- Number of iterations to read (typically 3-5)\n- Optional: specific sections to focus on\n\n## Output Format\n\nReturn findings in this structure:\n\n```markdown\n## Context from Iterations [N-M]\n\n### Current State\n[Most recent ending state from latest iteration]\n\n### Open Blockers\n- [Blocker 1 from previous iterations]\n- [Blocker 2]\n\n### Open Questions\n- [Question 1]\n- [Question 2]\n\n### Recent Progress\n- Iteration N: [Summary of work]\n- Iteration N-1: [Summary of work]\n- Iteration N-2: [Summary of work]\n\n### Key Metrics (if tracked)\n- [Metric name]: [Current value] (was [previous value])\n\n### Artifacts Created Recently\n- [File paths or PRs from recent iterations]\n\n### Skills & Workflows Recently Used\n- [Skill names and purposes]\n\n### Recommended Next Steps\n[From most recent iteration's suggestions]\n```\n\n## Important Notes\n\n- **Be concise**: Main agent has limited context\n- **Prioritize recent state**: Latest iteration is most important\n- **Surface blockers**: Always highlight what's preventing progress\n- **Note patterns**: If same blocker appears multiple times, emphasize it\n- **Don't infer**: Report only what's explicitly in journals\n",
        "plugins/autonomy/agents/journal-summarizer.md": "---\nname: journal-summarizer\ndescription: Use when starting an iteration with >5 previous iterations to condense older journal entries into a summary document\ntools: Read, Write, Glob\nmodel: haiku\n---\n\n# Journal Summarizer Agent\n\nYou are a specialized agent for condensing older iteration journals into summary documents to preserve key information while reducing context usage.\n\n**Model:** Haiku (fast, efficient text summarization)\n**Used by:** `starting-an-iteration` skill (when >5 iterations exist)\n\n## Your Responsibilities\n\n1. **Read older iterations** (iterations beyond the recent 3-5)\n2. **Extract and condense** major themes, decisions, and progress\n3. **Preserve critical information**:\n   - Major milestones and achievements\n   - Persistent blockers (appearing multiple times)\n   - Strategic pivots and why they occurred\n   - Key learnings and insights\n   - Metric trends over time\n4. **Write/update** `summary.md` file\n\n## Workflow\n\n1. **Identify iterations to summarize**: Typically iterations 1 through (N-5)\n2. **Read all older iterations**: Use Read for each file\n3. **Extract themes**:\n   - Group related work across iterations\n   - Track metric changes over time\n   - Identify patterns in blockers\n   - Note major strategic decisions\n4. **Condense findings**: Create summary organized by theme, not chronology\n5. **Write summary file**: Update `autonomy/[goal-name]/summary.md`\n\n## Input Expected\n\nYou will receive:\n- Goal name (kebab-case identifier)\n- Range of iterations to summarize (e.g., \"iterations 1-10\")\n- Current summary.md content (if exists, for updating)\n\n## Output Format\n\nWrite to `summary.md` in this structure:\n\n```markdown\n# Summary: [Goal Name]\n\n**Last Updated:** [Date] after iteration [N]\n**Iterations Covered:** 1-[N]\n\n## Goal Overview\n[Goal statement and success criteria]\n\n## Progress Overview\n- **Starting state** (Iteration 1): [Initial metrics/state]\n- **Current state** (Iteration N): [Current metrics/state]\n- **Net progress**: [Key improvements]\n\n## Major Initiatives Completed\n1. **[Initiative name]** (Iterations X-Y): [What was done and outcome]\n2. **[Initiative name]** (Iterations A-B): [What was done and outcome]\n\n## Persistent Blockers\n- **[Blocker name]**: Appeared in iterations [list], [current status]\n- **[Blocker name]**: [Description and impact]\n\n## Key Learnings & Insights\n- [Learning 1]: [Why it matters]\n- [Learning 2]: [How it influenced strategy]\n\n## Strategic Pivots\n1. **Iteration X**: [What changed and why]\n2. **Iteration Y**: [What changed and why]\n\n## Metric Trends (if applicable)\n| Metric | Iteration 1 | Iteration 5 | Iteration 10 | Current |\n|--------|-------------|-------------|--------------|---------|\n| [Name] | [Value]     | [Value]     | [Value]      | [Value] |\n\n## Skills & Workflows Most Used\n- [Skill name]: Used in iterations [list], for [purpose]\n- [Workflow name]: Key method for [task type]\n\n## Current Direction\n[Based on latest iterations, where is the goal heading?]\n```\n\n## Updating Existing Summary\n\nIf `summary.md` already exists:\n- Read current content\n- Identify which iterations it covers (from \"Last Updated\" line)\n- Append new iterations to existing summary\n- Update metrics, add new learnings, note new blockers\n- Preserve all existing information\n\n## Important Notes\n\n- **Preserve context**: Don't lose critical information in compression\n- **Highlight patterns**: Multi-iteration trends are important\n- **Track metrics**: If goal has measurable targets, show progression\n- **Be factual**: Don't interpret or infer beyond what journals state\n- **Maintain chronology**: Note when major changes occurred\n- **Update summary.md**: Write findings directly to file\n",
        "plugins/autonomy/commands/analyze-branch.md": "---\ndescription: Analyze another branch to extract findings, decisions, and insights\nallowed-tools: Skill, Read, Glob, Bash, Task, AskUserQuestion\nmodel: sonnet\nargument-hint: \"[branch-name] [search-description]\"\n---\n\n# Analyze Branch\n\nAnalyze another branch's iteration journals to extract key findings, decisions, and insights.\n\nYou must invoke the `analyzing-branches` skill to perform the analysis.\n\nUse the Skill tool:\n```\nskill: \"autonomy:analyzing-branches\"\nargs: \"[branch-name] [search-description]\"\n```\n\n**Arguments:**\n- `branch-name` - The git branch to analyze (e.g., \"experiment-a\")\n- `search-description` - Free-text description of what to look for (e.g., \"pricing experiments and revenue optimization\")\n\nThe skill will:\n1. Use `git merge-base` to find where the branches diverged\n2. Extract iteration journals from the divergent commits\n3. Search for relevant content based on your criteria\n4. Produce a markdown report ready for inclusion in your journal\n\n**Use cases:**\n- Review work from parallel experiment branch\n- Extract lessons learned from concluded experiments\n- Compare approaches across different branches\n- Salvage good ideas from any branch\n\n**Example:**\n```\n/analyze-branch experiment-a \"API optimization attempts and performance improvements\"\n```\n\nThe resulting report can be copied into your current iteration's \"External Context Gathered\" section.\n",
        "plugins/autonomy/commands/branch-status.md": "---\ndescription: Detailed status report for single autonomy branch\nallowed-tools: Skill\nmodel: sonnet\nargument-hint: \"<branch-name>\"\n---\n\n# Branch Status\n\nProvide comprehensive status report for a single autonomy branch showing complete iteration history, metrics progression, and current state.\n\nYou must invoke the `analyzing-branch-status` skill to perform the analysis.\n\nUse the Skill tool:\n```\nskill: \"autonomy:analyzing-branch-status\"\nargs: \"<branch-name>\"\n```\n\n**Arguments:**\n- `branch-name` (required) - Branch name to analyze. `autonomy/` prefix optional (will be added automatically if missing).\n\n**The skill will:**\n1. Normalize branch name (add `autonomy/` prefix if missing)\n2. Validate branch exists\n3. Dispatch branch-analyzer agent to read all commits on branch\n4. Agent generates Python script to analyze iteration timeline, metrics progression, status changes\n5. Produce comprehensive report with:\n   - Complete iteration list with dates and status\n   - Metrics over time (if applicable)\n   - Blocker history\n   - Current state and recommendations\n\n**Example usage:**\n```\n/branch-status experiment-a\n/branch-status autonomy/experiment-b\n```\n\n**Output includes:**\n- Iteration timeline (all iterations from first to latest)\n- Status changes over time\n- Metrics progression\n- Blocker history\n- Current state assessment\n- Recommended next actions\n\n**Note:** This command only operates on `autonomy/*` branches. For general iteration review on current branch, use `/review-progress`.\n",
        "plugins/autonomy/commands/checkpoint-iteration.md": "---\ndescription: Save current iteration progress before conversation compaction or as interim checkpoint\nallowed-tools: Skill, Read, Write, Edit\nmodel: sonnet\n---\n\n# Checkpoint Iteration\n\nYou must invoke the `checkpointing-an-iteration` skill to save current progress.\n\nUse the Skill tool:\n```\nskill: \"autonomy:checkpointing-an-iteration\"\n```\n\nThe skill will:\n1. Review conversation since iteration started\n2. Update journal entry with current work performed\n3. Capture decisions, artifacts, blockers so far\n4. Preserve state before potential context compaction\n\nUse this when:\n- Conversation getting long, compaction imminent\n- Reached natural pause point mid-iteration\n- Want to preserve important context discovered so far\n",
        "plugins/autonomy/commands/compare-branches.md": "---\ndescription: Compare two autonomy branches to show different approaches and outcomes\nallowed-tools: Skill\nmodel: sonnet\nargument-hint: \"<branch-a> <branch-b>\"\n---\n\n# Compare Branches\n\nCompare two autonomy branches to show differences in approaches, iterations, metrics, and outcomes.\n\nYou must invoke the `comparing-branches` skill to perform the comparison.\n\nUse the Skill tool:\n```\nskill: \"autonomy:comparing-branches\"\nargs: \"<branch-a> <branch-b>\"\n```\n\n**Arguments:**\n- `branch-a` (required) - First branch name. `autonomy/` prefix optional.\n- `branch-b` (required) - Second branch name. `autonomy/` prefix optional.\n\n**The skill will:**\n1. Normalize both branch names (add `autonomy/` prefix if missing)\n2. Validate both branches exist\n3. Use `git merge-base` to find where branches diverged\n4. Dispatch branch-analyzer agent to compare approaches\n5. Agent generates Python script for comparative analysis\n6. Produce report showing:\n   - Where branches diverged\n   - Iteration counts on each branch since divergence\n   - Comparative metrics (if applicable)\n   - Different approaches taken\n   - Outcomes on each branch\n\n**Example usage:**\n```\n/compare-branches experiment-a experiment-b\n/compare-branches autonomy/usage-pricing autonomy/flat-enterprise\n```\n\n**Output includes:**\n- Divergence point (common ancestor)\n- Iteration timeline comparison\n- Metrics trajectories (if applicable)\n- Status comparison\n- Different decisions made\n- Outcomes on each branch\n- Insights and recommendations\n\n**Note:** This command only operates on `autonomy/*` branches.\n",
        "plugins/autonomy/commands/create-goal.md": "---\ndescription: Create a new open-ended goal for autonomy tracking\nallowed-tools: Skill, Read, Write, Glob, Bash\nmodel: sonnet\n---\n\n# Create Goal\n\nYou must invoke the `creating-a-goal` skill to set up a new open-ended goal.\n\nUse the Skill tool:\n```\nskill: \"autonomy:creating-a-goal\"\n```\n\nThe skill will:\n1. Prompt for goal statement and success criteria\n2. Generate goal directory name\n3. Create autonomy/[goal-name]/ directory structure\n4. Write goal.md with goal definition\n5. Prepare for first iteration\n\nAfter creating the goal, use `/start-iteration` to begin work.\n",
        "plugins/autonomy/commands/end-iteration.md": "---\ndescription: End the current iteration, writing journal entry and summarizing work completed\nallowed-tools: Skill, Read, Write, Glob, Bash, Task, TodoWrite\nmodel: sonnet\n---\n\n# End Iteration\n\nYou must invoke the `ending-an-iteration` skill to finalize the current iteration.\n\nUse the Skill tool:\n```\nskill: \"autonomy:ending-an-iteration\"\n```\n\nThe skill will:\n1. Review the conversation to identify skills used, decisions made, and artifacts created\n2. Write a comprehensive journal entry documenting this iteration\n3. Update summary if needed (every 5 iterations)\n4. Prepare the state for the next iteration\n",
        "plugins/autonomy/commands/fork-iteration.md": "---\ndescription: Create new autonomy branch forked from current commit or specific iteration\nallowed-tools: Skill, AskUserQuestion, Bash\nmodel: sonnet\nargument-hint: \"[iteration] <strategy-name>\"\n---\n\n# Fork Iteration\n\nCreate a new autonomy branch forked from current commit or any iteration tag in git history.\n\nYou must invoke the `forking-iteration` skill to perform the fork.\n\nUse the Skill tool:\n```\nskill: \"autonomy:forking-iteration\"\nargs: \"[iteration] <strategy-name>\"\n```\n\n**Arguments:**\n- `iteration` (optional) - Iteration number (NNNN format) to fork from. If provided, searches backward in current branch history for matching iteration tag. If omitted, forks from current HEAD.\n- `strategy-name` (required) - Name for new branch (kebab-case). Will become `autonomy/<strategy-name>`.\n\n**The skill will:**\n1. Resolve fork point (iteration tag or current HEAD)\n2. Validate fork point exists\n3. Checkout fork point\n4. Create new branch `autonomy/<strategy-name>`\n5. Report success with next steps\n\n**Example usage:**\n```\n# Fork from current commit\n/fork-iteration experiment-b\n\n# Fork from specific iteration in current branch history\n/fork-iteration 0015 experiment-b\n\n# Bootstrap autonomy workflow from non-autonomy branch\ngit checkout main\n/fork-iteration initial-strategy\n```\n\n**Note:** This creates the branch but does NOT start an iteration. After forking, run `/start-iteration` to begin work on the new branch.\n",
        "plugins/autonomy/commands/fork-worktree.md": "---\ndescription: Create new autonomy branch with dedicated worktree for parallel agent workflows\nallowed-tools: Skill, AskUserQuestion, Bash\nmodel: sonnet\nargument-hint: \"[iteration] <strategy-name>\"\n---\n\n# Fork Worktree\n\nCreate a new autonomy branch with a dedicated worktree for running parallel Claude agents on different branches simultaneously.\n\nYou must invoke the `forking-worktree` skill to perform the fork.\n\nUse the Skill tool:\n```\nskill: \"autonomy:forking-worktree\"\nargs: \"[iteration] <strategy-name>\"\n```\n\n**Arguments:**\n- `iteration` (optional) - Iteration number (NNNN format) to fork from. If provided, searches backward in current branch history for matching iteration tag. If omitted, forks from current HEAD.\n- `strategy-name` (required) - Name for new branch and worktree (kebab-case). Will become `autonomy/<strategy-name>` branch and `.worktrees/autonomy/<strategy-name>/` directory.\n\n**The skill will:**\n1. Detect repository root (works from main repo or within worktrees)\n2. Resolve fork point (iteration tag or current HEAD)\n3. Validate fork point exists and worktree path is available\n4. Create new branch `autonomy/<strategy-name>` with worktree at `.worktrees/autonomy/<strategy-name>/`\n5. Report success with navigation instructions\n\n**Example usage:**\n```\n# Fork from current commit (works from anywhere)\n/fork-worktree experiment-b\n\n# Fork from specific iteration in current branch history\n/fork-worktree 0015 experiment-b\n\n# Fork from within another worktree (creates sibling worktree, not nested)\ncd .worktrees/autonomy/experiment-a\n/fork-worktree experiment-c  # Creates .worktrees/autonomy/experiment-c/ at root level\n```\n\n**Note:** This creates the branch AND worktree but does NOT start an iteration. After forking, navigate to the worktree directory and run `/start-iteration` to begin work:\n```bash\ncd .worktrees/autonomy/<strategy-name>\n/start-iteration\n```\n\n**Difference from `/fork-iteration`:**\n- `/fork-iteration` creates branch in current working directory (main repo)\n- `/fork-worktree` creates branch + isolated worktree directory for parallel work\n- Use worktrees when running multiple agents in parallel on different branches\n",
        "plugins/autonomy/commands/list-branches.md": "---\ndescription: Inventory autonomy branches with flexible sorting and grouping\nallowed-tools: Skill\nmodel: sonnet\nargument-hint: \"[optional-query]\"\n---\n\n# List Branches\n\nDisplay inventory of all autonomy branches with user-specified sorting, grouping, and information display.\n\nYou must invoke the `listing-branches` skill to perform the analysis.\n\nUse the Skill tool:\n```\nskill: \"autonomy:listing-branches\"\nargs: \"[optional-query]\"\n```\n\n**Arguments:**\n- `optional-query` - Free-text description of how to sort, group, and what information to display (e.g., \"sort by most recent, group by status\", \"show branches updated in last 30 days with metrics\")\n- If no query provided, defaults to: sort by most recent update, show all branches\n\n**The skill will:**\n1. Parse user's query to understand desired sorting/grouping/information\n2. Dispatch branch-analyzer agent to read git data\n3. Agent generates Python script for computational analysis (never \"eyeball it\")\n4. Produce formatted markdown table with requested information\n\n**Example queries:**\n```\n/list-branches\n/list-branches sort by most recent, show only active\n/list-branches group by status, show metrics\n/list-branches show branches updated in last 30 days\n```\n\n**Note:** This command only operates on `autonomy/*` branches. For general iteration review, use `/review-progress`.\n",
        "plugins/autonomy/commands/list-worktrees.md": "---\ndescription: List all autonomy worktrees with their status and location\nallowed-tools: Skill, Bash\nmodel: sonnet\nargument-hint: \"\"\n---\n\n# List Worktrees\n\nList all autonomy worktrees showing their branch, location, and current HEAD commit.\n\nYou must invoke the `listing-worktrees` skill to perform the listing.\n\nUse the Skill tool:\n```\nskill: \"autonomy:listing-worktrees\"\nargs: \"\"\n```\n\n**The skill will:**\n1. Find all git worktrees in the repository\n2. Filter to autonomy worktrees (in `.worktrees/autonomy/`)\n3. Display formatted table with branch, path, HEAD commit, and lock status\n4. Provide navigation hints\n\n**Example output:**\n```\nAutonomy Worktrees:\n\nBranch                    Path                                      HEAD       Locked\nautonomy/experiment-a     .worktrees/autonomy/experiment-a          a1b2c3d\nautonomy/experiment-b     .worktrees/autonomy/experiment-b          d4e5f6g    ðŸ”’\nautonomy/cdn-optimize     .worktrees/autonomy/cdn-optimize          h7i8j9k\n\nTotal: 3 autonomy worktrees\n\nTo navigate to a worktree:\n  cd .worktrees/autonomy/<strategy-name>\n\nTo remove a worktree:\n  /remove-worktree <strategy-name>\n```\n\n**Note:** This only lists worktrees, not branches. To see all autonomy branches (including those without worktrees), use `/list-branches`.\n",
        "plugins/autonomy/commands/remove-worktree.md": "---\ndescription: Safely remove an autonomy worktree while preserving the branch and its history\nallowed-tools: Skill, AskUserQuestion, Bash\nmodel: sonnet\nargument-hint: \"[--force] <strategy-name>\"\n---\n\n# Remove Worktree\n\nSafely remove an autonomy worktree directory while preserving the autonomy branch, commits, and iteration tags.\n\nYou must invoke the `removing-worktree` skill to perform the removal.\n\nUse the Skill tool:\n```\nskill: \"autonomy:removing-worktree\"\nargs: \"[--force] <strategy-name>\"\n```\n\n**Arguments:**\n- `--force` (optional) - Skip uncommitted changes check and force removal\n- `strategy-name` (required) - Name of worktree to remove (without `autonomy/` prefix)\n\n**The skill will:**\n1. Detect repository root (works from anywhere)\n2. Validate worktree exists at `.worktrees/autonomy/<strategy-name>/`\n3. Check for uncommitted changes (unless `--force`)\n4. Remove worktree directory\n5. Prune git worktree metadata\n6. Report success\n\n**Example usage:**\n```\n# Safe removal (fails if uncommitted changes)\n/remove-worktree experiment-b\n\n# Force removal (discards uncommitted changes)\n/remove-worktree --force experiment-b\n\n# Remove from within another worktree\ncd .worktrees/autonomy/experiment-a\n/remove-worktree experiment-b  # Removes sibling worktree\n```\n\n**What gets removed:**\n- Worktree directory: `.worktrees/autonomy/<strategy-name>/`\n- Git worktree metadata\n\n**What persists:**\n- Branch `autonomy/<strategy-name>` and all commits\n- All iteration tags (`autonomy/<strategy-name>/iteration-NNNN`)\n- Git history and journal commits\n- Can checkout branch later or create new worktree for it\n\n**Manual removal:**\nIf automated removal fails:\n```bash\ngit worktree remove --force .worktrees/autonomy/<strategy-name>\ngit worktree prune\n```\n",
        "plugins/autonomy/commands/review-progress.md": "---\ndescription: Review progress toward an open-ended goal by reading iteration journals\nallowed-tools: Skill, Read, Glob, Task, TodoWrite\nmodel: sonnet\n---\n\n# Review Progress\n\nYou must invoke the `reviewing-progress` skill to assess progress toward the current goal.\n\nUse the Skill tool:\n```\nskill: \"autonomy:reviewing-progress\"\n```\n\nThe skill will:\n1. Load and summarize all iterations for the goal\n2. Present progress metrics and completed work\n3. Identify current blockers and open questions\n4. Suggest next steps for continuing the goal\n",
        "plugins/autonomy/commands/slime.md": "---\ndescription: Set up slime mold strategy for parallel exploration with autonomy branches\nallowed-tools: Skill\nmodel: sonnet\n---\n\n# Slime Mold Strategy Setup\n\nYou must invoke the `slime-strategy` skill to set up the complete slime mold exploration workflow.\n\nUse the Skill tool:\n```\nskill: \"autonomy:slime-strategy\"\n```\n\n**What this command does:**\n\nIf no autonomy goal exists yet:\n1. Uses `creating-a-goal` skill to define goal and set up `autonomy/[goal-name]/` directory\n2. Creates/updates `autonomy/CLAUDE.md` with slime mold strategy documentation\n3. Uses `forking-iteration` skill to create initial `autonomy/[goal-name]` branch\n4. Creates `iteration-0000-YYYY-MM-DD.md` as baseline setup journal\n5. Makes git commit with tag `autonomy/[goal-name]/iteration-0000`\n\nIf autonomy goal already exists:\n- Updates `autonomy/CLAUDE.md` to ensure slime mold strategy documentation is current\n- Skips goal creation, branching, and iteration 0000 (idempotent behavior)\n\n**After running `/slime`:**\n- Use `/start-iteration` to begin iteration 0001 (first real work iteration)\n- Use `/fork-iteration <strategy-name>` to create additional exploration branches\n- Use `/analyze-branch <branch> <search>` to cross-pollinate learnings between branches\n- Use `/compare-branches <branch-a> <branch-b>` to understand divergent approaches\n",
        "plugins/autonomy/commands/start-iteration.md": "---\ndescription: Start a new iteration for an open-ended goal, loading context from previous iterations\nallowed-tools: Skill, Read, Write, Glob, Bash, Task, TodoWrite\nmodel: sonnet\n---\n\n# Start Iteration\n\nYou must invoke the `starting-an-iteration` skill to begin a new iteration for an open-ended goal.\n\nUse the Skill tool:\n```\nskill: \"autonomy:starting-an-iteration\"\n```\n\nThe skill will:\n1. Detect if a goal already exists or prompt to create a new one\n2. Load context from recent iterations\n3. Present the current state and open questions\n4. Prepare you to continue working toward the goal\n",
        "plugins/autonomy/skills/analyzing-branch-status/SKILL.md": "---\nname: analyzing-branch-status\ndescription: Use when user wants detailed status report for single autonomy branch including iteration timeline and metrics progression\n---\n\n# Analyzing Branch Status\n\n## Overview\n\nProvide comprehensive status report for a single autonomy branch by analyzing all journal commits and extracting timeline, metrics, and state evolution.\n\n**Core principle:** Dispatch branch-analyzer agent for computational analysis. Never manually review commits.\n\n## When to Use\n\nUse this skill when:\n- User runs `/branch-status` command\n- User wants deep dive into one branch\n- User wants to see metrics progression over time\n- User wants blocker history for a branch\n\n**DO NOT use for:**\n- Listing all branches (use listing-branches instead)\n- Comparing two branches (use comparing-branches instead)\n- Current branch review (use reviewing-progress instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse and validate | Normalize branch name, check exists | Bash |\n| 2. Dispatch agent | Send branch to branch-analyzer | Task |\n| 3. Present report | Display comprehensive status | Direct output |\n\n## Process\n\n### Step 1: Parse and Validate Branch Name\n\nNormalize and validate the branch name:\n\n**Normalize:**\n```bash\n# If user provided name without autonomy/ prefix, add it\nif [[ \"$branch_name\" != autonomy/* ]]; then\n  branch_name=\"autonomy/$branch_name\"\nfi\n```\n\n**Validate exists:**\n```bash\n# Check if branch exists (local or remote)\nif ! git branch -a | grep -q \"$branch_name\\$\"; then\n  echo \"Error: Branch '$branch_name' not found among autonomy branches.\"\n  echo \"\"\n  echo \"Available autonomy branches:\"\n  git branch -a | grep 'autonomy/' | sed 's/^..//; s/ -> .*//'\n  echo \"\"\n  echo \"Run '/list-branches' to see all autonomy branches.\"\n  exit 1\nfi\n```\n\n**Validate is autonomy branch:**\n```bash\n# Verify it's an autonomy branch\nif [[ \"$branch_name\" != autonomy/* ]]; then\n  echo \"Error: Branch '$branch_name' is not an autonomy branch.\"\n  echo \"\"\n  echo \"These commands only operate on autonomy/* branches.\"\n  echo \"\"\n  echo \"To analyze this branch's iterations:\"\n  echo \"- Run '/review-progress' (works on any branch)\"\n  echo \"\"\n  echo \"To convert to autonomy workflow:\"\n  echo \"- Run '/fork-iteration <strategy-name>' to create autonomy branch from current state\"\n  exit 1\nfi\n```\n\n### Step 2: Dispatch Branch-Analyzer Agent\n\nDispatch the `branch-analyzer` agent with detailed instructions:\n\n```bash\nTask tool with subagent_type: \"autonomy:branch-analyzer\"\nModel: haiku\nPrompt: \"Analyze autonomy branch '$branch_name' and provide comprehensive status report.\n\nTasks:\n1. Read all journal commits on branch (commits starting with 'journal: ')\n2. Parse each commit message for:\n   - Iteration number\n   - Date\n   - Status (active/blocked/concluded/dead-end)\n   - Metrics\n   - Blockers\n   - Next steps\n3. Generate Python script to analyze:\n   - Complete iteration timeline (chronological)\n   - Status changes over time\n   - Metrics progression (if metrics exist)\n   - Blocker history\n   - Current state from most recent commit\n4. Execute Python script\n5. Output comprehensive markdown report\n\nUse computational methods (Python scripts), do not eyeball the analysis.\n\nReport format:\n- Iteration Timeline section\n- Metrics Over Time section (if metrics exist)\n- Status Evolution section\n- Blocker History section\n- Current State and Recommendations section\"\n```\n\n**Agent will:**\n1. List all commits on branch: `git log autonomy/<branch-name>`\n2. Filter for journal commits (start with \"journal: \")\n3. Parse each commit message metadata\n4. Generate Python script for analysis\n5. Execute script to produce timeline, metrics, blockers\n6. Return formatted markdown report\n\n### Step 3: Present Report\n\nDisplay agent's comprehensive report to user.\n\n**Example output format:**\n```markdown\n# Branch Status: autonomy/experiment-a\n\n**Current Status:** blocked\n**Latest Iteration:** 0028\n**Last Updated:** 2026-01-02\n**Total Iterations:** 28\n\n---\n\n## Iteration Timeline\n\n| Iteration | Date | Status | Summary |\n|-----------|------|--------|---------|\n| 0001 | 2025-11-15 | active | Initial setup of usage-based pricing model |\n| 0002 | 2025-11-16 | active | Implemented tier calculations |\n| ... | ... | ... | ... |\n| 0027 | 2026-01-01 | active | Stripe API integration progress |\n| 0028 | 2026-01-02 | blocked | Awaiting Stripe webhook documentation |\n\n---\n\n## Metrics Over Time\n\nMRR progression:\n- Iteration 0001: $45k (baseline)\n- Iteration 0010: $52k (+15.6%)\n- Iteration 0020: $58k (+28.9%)\n- Iteration 0028: $62k (+37.8%)\n\nBuild time:\n- Iteration 0015: 5.2min (baseline)\n- Iteration 0028: 3.2min (-38.5%)\n\n---\n\n## Status Evolution\n\n- Iterations 0001-0027: active (normal progression)\n- Iteration 0028: blocked (current)\n\n---\n\n## Blocker History\n\n**Current Blockers (Iteration 0028):**\n- Stripe webhook integration unclear: need updated API docs\n- Finance team approval pending for pricing structure\n\n**Resolved Blockers:**\n- Iteration 0015: Build performance (resolved at 0016)\n- Iteration 0022: User feedback collection (resolved at 0024)\n\n---\n\n## Current State and Recommendations\n\n**Where we are:**\nBranch has made substantial progress over 28 iterations. MRR increased 37.8%, build time reduced 38.5%. Currently blocked on external dependencies.\n\n**Recommended actions:**\n1. Escalate Stripe API documentation request\n2. Schedule finance team review meeting\n3. Consider parallel work on pricing page UI while blocked\n4. Review iteration 0027 for alternative integration approaches\n\n**Branch health:** Active exploration, currently blocked but making good progress\n```\n\n## Important Notes\n\n### Only Autonomy Branches\n\nThis skill ONLY analyzes `autonomy/*` branches:\n- Validates branch has `autonomy/` prefix\n- Will not analyze non-autonomy branches\n- For general iteration review, user should use `/review-progress`\n\n### Computational Analysis Required\n\n**DO NOT:**\n- Manually read through commits\n- \"Eyeball\" metrics progression\n- Guess at patterns or trends\n\n**DO:**\n- Dispatch branch-analyzer agent\n- Let agent generate Python scripts\n- Use computational methods for precision\n\n### Read-Only Operations\n\nAll analysis happens via git commands:\n- Never checkout the branch\n- Read commits via `git log <branch-name>`\n- Branch-analyzer uses read-only operations\n- No modifications to any files\n\n### Metrics May Not Exist\n\nNot all goals have quantitative metrics:\n- Some goals are qualitative\n- Metrics section may be \"None\" in commit messages\n- Report should handle missing metrics gracefully\n- Don't force metrics where they don't exist\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll read the journal files to get status\" | NO. Read commit messages via git log. Don't checkout branch. |\n| \"Only 10 iterations, I can review manually\" | NO. Always dispatch branch-analyzer for computational analysis. |\n| \"Branch is on remote, I can't analyze it\" | YES YOU CAN. Use git log origin/branch-name to read commits. |\n| \"No metrics in some commits, report is incomplete\" | OK. Not all iterations have metrics. Report what exists. |\n| \"I'll checkout branch to read latest journal\" | NO. Read commit message via git log. Never checkout. |\n\n## After Analyzing\n\nOnce analysis is complete:\n- Report displayed to user\n- No files created or modified\n- User can fork from any iteration: `/fork-iteration <iteration> <strategy-name>`\n- User can compare with another branch: `/compare-branches <branch-a> <branch-b>`\n",
        "plugins/autonomy/skills/analyzing-branches/SKILL.md": "---\nname: analyzing-branches\ndescription: Use when analyzing another branch's iteration journals to extract findings, decisions, and insights from divergent work\n---\n\n# Analyzing Branches\n\n## Overview\n\nAnalyze another branch's iteration journals to extract key findings, decisions, and insights from work that diverged from the current branch.\n\n**Core principle:** Learn from parallel explorations. Extract valuable insights from any branch, regardless of success or failure.\n\n## When to Use\n\nUse this skill when:\n- User runs `/analyze-branch` command\n- Want to review work from parallel experiment branch\n- Need to extract lessons from concluded experiments\n- Comparing approaches across different branches\n- Looking for salvageable ideas from any branch\n\n**DO NOT use for:**\n- Analyzing current branch history (use `/review-progress` instead)\n- Comparing file changes (use `git diff` directly)\n- When branches have no autonomy iterations\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse arguments | Extract branch name and search criteria | Manual |\n| 2. Find divergence | Use git merge-base to find common ancestor | Bash |\n| 3. Extract iterations | Get autonomy tags from divergent commits | Bash |\n| 4. Read journals | Read iteration files from target branch | Bash, Read |\n| 5. Search and filter | Find relevant content based on criteria | Manual |\n| 6. Generate report | Create markdown summary | Direct output |\n\n## Process\n\n### Step 1: Parse Arguments\n\nExtract branch name and search criteria from command arguments:\n\n```\nArguments format: \"[branch-name] [search-description]\"\nExample: \"experiment-a pricing experiments and revenue optimization\"\n```\n\n**Parse:**\n- First argument: Branch name (e.g., \"experiment-a\")\n- Remaining arguments: Search criteria as free text\n\n**Verify goal exists:**\n```bash\n# Use Glob to find goal\npattern: \"autonomy/*/goal.md\"\n```\n\nIf no goal found:\n```\n\"No autonomy goal found in this project. Branch analysis requires an autonomy goal.\"\n```\nStop here.\n\nExtract goal name from path for use in later steps.\n\n### Step 2: Find Divergence Point\n\nUse git to find where branches diverged:\n\n```bash\n# Get current branch name\ncurrent_branch=$(git branch --show-current)\n\n# Find common ancestor between current and target branch\nmerge_base=$(git merge-base \"$current_branch\" \"$target_branch\" 2>&1)\n\n# Check if command succeeded\nif [ $? -ne 0 ]; then\n  # Error handling - see Step 2a\nfi\n```\n\n**Step 2a: Handle Git Errors**\n\nIf `git merge-base` fails, determine cause and prompt user:\n\n**Error: Branch not found**\n```markdown\nError: Branch '$target_branch' does not exist.\n\nAvailable branches:\n$(git branch -a | sed 's/^..//; s/ -> .*//')\n\nPlease verify the branch name and try again.\n```\n\n**Error: No common ancestor**\n```markdown\nWarning: Cannot find common ancestor between current branch and '$target_branch'.\n\nThis may mean:\n- Branches have completely independent histories\n- Branch was rebased and history was rewritten\n\nOptions:\n1. Analyze entire branch history (may include duplicate work)\n2. Specify iteration range manually\n3. Specify common ancestor commit manually\n\nWhich would you like?\n```\n\nUse AskUserQuestion to let user choose:\n- Option 1: Set `merge_base=\"\"` and analyze all iterations\n- Option 2: Prompt for iteration range, skip git operations\n- Option 3: Prompt for commit hash, use as merge_base\n\n### Step 3: Extract Divergent Iterations\n\nFind autonomy iteration tags on target branch after divergence:\n\n```bash\n# Get all autonomy iteration tags on target branch\nif [ -n \"$merge_base\" ]; then\n  # Analyze only divergent work\n  commit_range=\"$merge_base..$target_branch\"\nelse\n  # Analyze entire branch (fallback)\n  commit_range=\"$target_branch\"\nfi\n\n# Extract iteration numbers from tags\niterations=$(git log \"$commit_range\" \\\n  --pretty=format:\"%D\" \\\n  | tr ',' '\\n' \\\n  | grep \"tag: autonomy/iteration-\" \\\n  | sed 's/.*autonomy\\/iteration-//' \\\n  | sort -n)\n\n# Count iterations found\niteration_count=$(echo \"$iterations\" | wc -w)\n```\n\n**If no iterations found:**\n```markdown\nNo autonomy iteration tags found on branch '$target_branch' after divergence.\n\nPossible reasons:\n- Branch hasn't used autonomy plugin\n- All iterations are before the divergence point\n- Iterations exist but weren't committed with git integration\n\nWould you like to:\n1. Analyze entire branch (all iterations)\n2. Specify iteration range manually\n```\n\nUse AskUserQuestion for recovery.\n\n### Step 4: Read Journal Files\n\nFor each iteration found, read the journal content:\n\n```bash\n# For each iteration number\nfor iter in $iterations; do\n  # Find journal file with this iteration number\n  # Format: iteration-NNNN-YYYY-MM-DD.md\n  journal_file=$(git ls-tree -r --name-only \"$target_branch\" \\\n    \"autonomy/$goal_name/\" \\\n    | grep \"iteration-$(printf '%04d' $iter)-\")\n\n  if [ -n \"$journal_file\" ]; then\n    # Read file content from target branch\n    journal_content=$(git show \"$target_branch:$journal_file\")\n\n    # Store for analysis in next step\n  fi\ndone\n```\n\n**Build iteration data structure:**\nFor each iteration, extract:\n- Iteration number\n- Date (from filename)\n- Beginning State section\n- Iteration Intention section\n- Work Performed subsections\n- Ending State section\n- Full content for searching\n\n### Step 5: Search and Filter\n\nApply search criteria to find relevant iterations and content:\n\n**Search strategy:**\n```bash\n# Convert search criteria to grep pattern\n# Example: \"pricing experiments\" â†’ \"pricing|experiments\"\nsearch_pattern=$(echo \"$search_criteria\" | tr ' ' '|')\n\n# For each iteration's content\n# Score by relevance (number of search term matches)\n# Prioritize sections: Key Decisions, Ending State, Reasoning & Strategy\n```\n\n**Extract relevant sections:**\n- **Key Findings:** Extract from Ending State + Work Performed\n- **Decisions:** Extract from Key Decisions Made subsection\n- **Ideas/Experiments:** Extract from Reasoning & Strategy Changes\n- **Outcomes:** Extract from Ending State\n\n**If search finds nothing:**\n```markdown\nNo iterations matched search criteria: \"$search_criteria\"\n\nAnalyzed $iteration_count iterations on '$target_branch'.\n\nWould you like to:\n1. Broaden search (show all iterations)\n2. Refine search criteria\n```\n\nUse AskUserQuestion to offer alternatives.\n\n### Step 6: Generate Report\n\nProduce markdown report formatted for journal inclusion:\n\n```markdown\n## Analysis of Branch: $target_branch\n\n**Analyzed range:** Iterations $first_iter-$last_iter (diverged from iteration $divergence_iter)\n**Search focus:** $search_criteria\n**Common ancestor:** $merge_base\n**Branch status:** [Active/Merged/Concluded - from latest iteration if stated]\n\n---\n\n### Key Findings\n\n[For each significant finding extracted from journals:]\n- **Iteration NNNN:** [Finding or insight]\n  - **Context:** [What problem was being addressed]\n  - **Approach taken:** [How it was implemented or explored]\n  - **Outcome:** [Results from Ending State]\n  - **Reference:** `$target_branch @ iteration-NNNN`\n\n---\n\n### Decisions and Rationale\n\n[For each major decision from Key Decisions Made:]\n- **Iteration NNNN:** [Decision]\n  - **Rationale:** [Why this choice was made]\n  - **Outcome:** [Result - success/failure/inconclusive/ongoing]\n  - **Takeaway:** [What this tells us]\n\n---\n\n### Ideas and Experiments\n\n[For each experimental approach from Reasoning & Strategy Changes:]\n- **Iteration NNNN:** [What was tried]\n  - **Results:** [What was learned]\n  - **Status:** [Validated/Invalidated/Needs more testing/Inconclusive]\n  - **Applicability:** [Could this apply to current branch?]\n\n---\n\n### Timeline of Branch Activity\n\n| Iteration | Date | Summary | Status |\n|-----------|------|---------|--------|\n| NNNN | YYYY-MM-DD | [From Iteration Intention or Ending State] | [From Ending State] |\n| NNNN | YYYY-MM-DD | [Summary] | [Status] |\n\n---\n\n**Analysis Summary:** [Neutral assessment of what was learned, key patterns, notable outcomes]\n```\n\n**Output to user:**\nDisplay the complete report. User can copy relevant sections into their current journal's \"External Context Gathered\" section.\n\n## Important Notes\n\n### Neutral Framing\n\n**Do NOT assume branch status:**\n- Branch may be active, merged, concluded, or abandoned\n- Don't label as \"failed\" unless journal explicitly states that\n- Let the journal entries speak for themselves\n- Use neutral language: \"findings\", \"outcomes\", not \"failures\"\n\n### Git Safety\n\n**Read-only operations:**\n- Never checkout or switch branches\n- Use `git show` to read historical files\n- Don't modify target branch\n- Don't create commits\n\n### Search Flexibility\n\n**Interpreting search criteria:**\n- Free-text is intentionally loose\n- Don't require exact matches\n- Look for semantic relevance\n- Prioritize iteration sections mentioned in criteria\n\n**If criteria is very specific:**\n- \"pricing tier experiments\" â†’ Focus on those specific words\n- \"revenue optimization\" â†’ Include related terms (profit, MRR, monetization)\n\n**If criteria is broad:**\n- \"general findings\" â†’ Show all significant content\n- \"lessons learned\" â†’ Focus on Ending State assessments\n\n### Report Quality\n\n**Characteristics of good report:**\n- **Self-contained:** Readable without accessing original branch\n- **Actionable:** Clear what to do with findings\n- **Referenced:** Links to specific iterations for deep dives\n- **Concise:** Extract signal, avoid noise\n- **Balanced:** Show successes and failures equally\n\n### Performance Considerations\n\n**For branches with many iterations:**\n- Reading 50+ iterations may be slow\n- Consider limiting to most recent N iterations if search is broad\n- User can refine criteria to narrow scope\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"Branch is abandoned, so it failed\" | NO. Don't assume status. Read journals for actual outcomes. |\n| \"I'll modify the target branch\" | NO. Read-only operations only. Never checkout or modify target. |\n| \"Search found nothing, report empty\" | NO. Offer to broaden search or show all iterations. |\n| \"I'll analyze current branch\" | NO. Use /review-progress for current branch analysis. |\n| \"No common ancestor means I should fail\" | NO. Offer alternatives: analyze all, manual range, manual ancestor. |\n| \"Report should only include successes\" | NO. Balanced view - show what worked AND what didn't. |\n\n## After Analyzing\n\nOnce analysis is complete:\n- Report displayed to user\n- User copies relevant sections into current journal\n- No files created or modified by this skill\n- Can analyze multiple branches in same conversation\n- Skill usage logged in journal's \"Skills & Workflows Used\" section\n",
        "plugins/autonomy/skills/checkpointing-an-iteration/SKILL.md": "---\nname: checkpointing-an-iteration\ndescription: Use when saving current iteration progress mid-conversation, before context compaction, or at interim pause points\n---\n\n# Checkpointing an Iteration\n\n## Overview\n\nSave current iteration progress by updating the journal entry with work performed so far, preserving state before potential context loss.\n\n**Core principle:** Protect important context from compaction. Update journal incrementally during long conversations.\n\n## When to Use\n\nUse this skill when:\n- User runs `/checkpoint-iteration` command\n- Conversation is getting long, compaction may be imminent\n- Reached natural pause point mid-iteration\n- Discovered important context that must be preserved\n- Want to save progress before taking risky action\n\n**DO NOT use for:**\n- Ending the iteration (use ending-an-iteration instead)\n- Starting iteration (use starting-an-iteration instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Find journal file | Locate current iteration journal | Glob |\n| 2. Review conversation | Extract work performed so far | Manual review |\n| 3. Read current journal | Get existing content | Read |\n| 4. Update Work Performed | Append new findings | Edit |\n| 5. Announce checkpoint | Confirm save | Direct output |\n\n## Process\n\n### Step 1: Find Journal File\n\nLocate the current iteration's journal:\n\n```bash\n# Use Glob to find journal files\npattern: \"autonomy/*/iteration-*.md\"\n```\n\nSort by filename (iteration number) and identify the most recent one. This should be today's date or recent.\n\n**If no journal file found:**\n```\n\"No active iteration journal found.\n\nThis usually means `/start-iteration` wasn't run yet. Start an iteration first before checkpointing.\"\n```\nStop here.\n\n**If journal file found:**\nExtract the full path and proceed.\n\n### Step 2: Review Conversation\n\nReview the conversation since iteration started to extract:\n\n**Skills & Workflows Used (so far):**\n- Scan for `<invoke name=\"Skill\">` tool calls\n- Note which skills used and for what purpose\n\n**Key Decisions Made (so far):**\n- Identify major choices and rationale\n- Note alternatives considered\n\n**Artifacts Created/Modified (so far):**\n- Files created or changed\n- Git commits made\n- Pull requests opened\n\n**External Context Gathered (so far):**\n- Web research findings\n- User feedback\n- Documentation consulted\n\n**Reasoning & Strategy Changes (so far):**\n- Why certain approaches chosen\n- Where strategy pivoted\n\n**Blockers Encountered (so far):**\n- What's preventing progress\n- Dependencies identified\n\n**Open Questions (so far):**\n- What needs resolution\n- Decisions deferred\n\n### Step 3: Read Current Journal\n\nRead the existing journal file to see what's already documented:\n\n```bash\n# Use Read tool\nfile: \"autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\"\n```\n\nThe journal will have:\n- **Metadata header** (# Iteration NNNN - YYYY-MM-DD)\n- **Beginning State** section (from starting-an-iteration)\n- **Iteration Intention** section (from starting-an-iteration)\n- **Work Performed** section (may be partially filled from previous checkpoint, or empty)\n- **Ending State** section (will be empty - that's for ending-an-iteration)\n- **Iteration Metadata** section (will be empty - that's for ending-an-iteration)\n\n### Step 4: Update Work Performed Section\n\nUpdate the \"Work Performed\" section with current findings from Step 2.\n\n**If Work Performed section is empty:**\nReplace the empty section with full content:\n\n```markdown\n## Work Performed\n\n### Skills & Workflows Used\n[From Step 2 review]\n\n### Key Decisions Made\n[From Step 2 review]\n\n### Artifacts Created/Modified\n[From Step 2 review]\n\n### External Context Gathered\n[From Step 2 review]\n\n### Reasoning & Strategy Changes\n[From Step 2 review]\n\n### Blockers Encountered\n[From Step 2 review]\n\n### Open Questions\n[From Step 2 review]\n```\n\n**If Work Performed section already has content:**\nMerge new findings with existing:\n- Add new skills to \"Skills & Workflows Used\" list\n- Add new decisions to \"Key Decisions Made\"\n- Add new artifacts to \"Artifacts Created/Modified\"\n- Append new findings to other sections\n- Preserve all existing content\n\nUse Edit tool to update the file.\n\n### Step 5: Announce Checkpoint\n\nReport to user:\n\n```markdown\n**Checkpoint saved for iteration [N]**\n\nJournal updated: `autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md`\n\n## Checkpoint Summary\n- **Skills used:** [Count] skills/workflows\n- **Decisions made:** [Count] key decisions\n- **Artifacts created:** [Count] files/commits\n- **Blockers:** [Count] blockers identified\n- **Open questions:** [Count] questions pending\n\nContext preserved. Safe to continue or compact conversation.\n\n---\n\nTo finalize this iteration, use `/end-iteration` when ready.\n```\n\n## Important Notes\n\n### Checkpoint vs. End Iteration\n\n**Checkpoint:**\n- Updates journal mid-iteration\n- Iteration continues after checkpoint\n- Can checkpoint multiple times per iteration\n- Doesn't update summary.md\n- Doesn't finalize \"Ending State\"\n\n**End Iteration:**\n- Finalizes journal entry\n- Iteration concludes\n- Writes final \"Ending State\"\n- Updates summary.md if needed (every 5 iterations)\n- Conversation ends or new iteration starts\n\n### Multiple Checkpoints\n\nIt's fine to checkpoint multiple times:\n- Each checkpoint merges with previous content\n- Later checkpoints add to earlier ones\n- All information accumulated in journal\n\n### What Gets Preserved\n\nCheckpointing preserves:\n- Work done so far\n- Decisions and reasoning\n- Blockers discovered\n- Context gathered\n\nCheckpointing does NOT capture:\n- Future plans (that's in \"Ending State\" at iteration end)\n- Final assessment (that's for ending-an-iteration)\n- Complete iteration story (still in progress)\n\n### Merging Strategy\n\nWhen updating Work Performed with existing content:\n- **Append** new items to lists (don't overwrite)\n- **Preserve** all existing information\n- **Deduplicate** if same item mentioned twice\n- **Maintain** chronological order within sections\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll create new journal file for checkpoint\" | NO. Update existing journal from starting-an-iteration. |\n| \"I'll overwrite existing Work Performed section\" | NO. Merge new content with existing. |\n| \"I'll write Ending State during checkpoint\" | NO. That's only for ending-an-iteration. |\n| \"Checkpoint means iteration is over\" | NO. Iteration continues after checkpoint. |\n| \"I'll skip checkpoint if conversation isn't long\" | NO. Checkpoint anytime user requests it. |\n\n## After Checkpointing\n\nOnce checkpoint is saved:\n- Journal updated with current progress\n- Context preserved against compaction\n- Iteration continues normally\n- Can checkpoint again later if needed\n- When ready to conclude, use `/end-iteration`\n",
        "plugins/autonomy/skills/comparing-branches/SKILL.md": "---\nname: comparing-branches\ndescription: Use when user wants to compare two autonomy branches to see different approaches, metrics, and outcomes\n---\n\n# Comparing Branches\n\n## Overview\n\nCompare two autonomy branches to show where they diverged, how their approaches differ, and what outcomes each achieved.\n\n**Core principle:** Use git merge-base to find divergence, then dispatch branch-analyzer for computational comparison.\n\n## When to Use\n\nUse this skill when:\n- User runs `/compare-branches` command\n- User wants to see differences between two exploration branches\n- User wants to understand alternative approaches\n- User wants to compare metrics/outcomes across branches\n\n**DO NOT use for:**\n- Analyzing single branch (use analyzing-branch-status instead)\n- Listing all branches (use listing-branches instead)\n- Current branch review (use reviewing-progress instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse and validate | Normalize branch names, check both exist | Bash |\n| 2. Find divergence | Use git merge-base to find common ancestor | Bash |\n| 3. Dispatch agent | Send both branches to branch-analyzer | Task |\n| 4. Present comparison | Display comparative report | Direct output |\n\n## Process\n\n### Step 1: Parse and Validate Branch Names\n\nExtract and normalize both branch names:\n\n**Parse arguments:**\n```\nargs = \"<branch-a> <branch-b>\"\nSplit on whitespace:\n  branch_a = first word\n  branch_b = second word\n\nIf not exactly 2 words:\n  Error: \"Usage: /compare-branches <branch-a> <branch-b>\"\n```\n\n**Normalize:**\n```bash\n# Add autonomy/ prefix if missing\nif [[ \"$branch_a\" != autonomy/* ]]; then\n  branch_a=\"autonomy/$branch_a\"\nfi\n\nif [[ \"$branch_b\" != autonomy/* ]]; then\n  branch_b=\"autonomy/$branch_b\"\nfi\n```\n\n**Validate both exist:**\n```bash\n# Check branch A\nif ! git branch -a | grep -q \"$branch_a\\$\"; then\n  echo \"Error: Branch '$branch_a' not found.\"\n  echo \"\"\n  echo \"Available autonomy branches:\"\n  git branch -a | grep 'autonomy/' | sed 's/^..//; s/ -> .*//'\n  exit 1\nfi\n\n# Check branch B\nif ! git branch -a | grep -q \"$branch_b\\$\"; then\n  echo \"Error: Branch '$branch_b' not found.\"\n  echo \"\"\n  echo \"Available autonomy branches:\"\n  git branch -a | grep 'autonomy/' | sed 's/^..//; s/ -> .*//'\n  exit 1\nfi\n```\n\n**Validate both are autonomy branches:**\n```bash\nfor branch in \"$branch_a\" \"$branch_b\"; do\n  if [[ \"$branch\" != autonomy/* ]]; then\n    echo \"Error: Branch '$branch' is not an autonomy branch.\"\n    echo \"\"\n    echo \"These commands only operate on autonomy/* branches.\"\n    exit 1\n  fi\ndone\n```\n\n### Step 2: Find Divergence Point\n\nUse git to find where the branches diverged:\n\n```bash\n# Find common ancestor\nmerge_base=$(git merge-base \"$branch_a\" \"$branch_b\" 2>&1)\n\n# Check if command succeeded\nif [ $? -ne 0 ]; then\n  echo \"Error: Cannot find common ancestor between '$branch_a' and '$branch_b'.\"\n  echo \"\"\n  echo \"This may mean:\"\n  echo \"- Branches have completely independent histories\"\n  echo \"- Branch was rebased and history was rewritten\"\n  echo \"\"\n  echo \"Git error: $merge_base\"\n  exit 1\nfi\n\n# Get divergence info\ndivergence_date=$(git log -1 --format='%ai' \"$merge_base\")\ndivergence_short=$(git rev-parse --short \"$merge_base\")\n```\n\n**Find iteration at divergence (if it exists):**\n```bash\n# Check if divergence point has an iteration tag\ndivergence_tag=$(git tag --points-at \"$merge_base\" | grep 'iteration-')\nif [ -n \"$divergence_tag\" ]; then\n  divergence_iteration=$(echo \"$divergence_tag\" | sed 's/.*iteration-//')\nelse\n  divergence_iteration=\"(no iteration tag at divergence)\"\nfi\n```\n\n### Step 3: Dispatch Branch-Analyzer Agent\n\nDispatch the `branch-analyzer` agent with comparison instructions:\n\n```bash\nTask tool with subagent_type: \"autonomy:branch-analyzer\"\nModel: haiku\nPrompt: \"Compare two autonomy branches and show differences in approaches and outcomes.\n\nBranches:\n- Branch A: $branch_a\n- Branch B: $branch_b\n\nDivergence point: $merge_base ($divergence_short)\nDivergence date: $divergence_date\nDivergence iteration: $divergence_iteration\n\nTasks:\n1. Read all journal commits on branch A since divergence\n2. Read all journal commits on branch B since divergence\n3. Parse each commit message for: iteration, date, status, metrics, blockers, next steps\n4. Generate Python script to compare:\n   - Iteration counts on each branch\n   - Status patterns (how often active/blocked/etc)\n   - Metrics trajectories (if metrics exist)\n   - Different decisions/approaches mentioned\n   - Outcomes on each branch\n5. Execute Python script\n6. Output comparative markdown report\n\nUse computational methods (Python scripts), do not eyeball the comparison.\n\nReport format:\n- Divergence Information section\n- Iteration Comparison section\n- Metrics Comparison section (if metrics exist)\n- Approach Differences section\n- Outcomes and Status section\n- Insights and Recommendations section\"\n```\n\n**Agent will:**\n1. Get commit range for each branch since divergence\n2. Read journal commits from both ranges\n3. Parse metadata from commit messages\n4. Generate Python script for comparison\n5. Execute script to produce comparative analysis\n6. Return formatted markdown report\n\n### Step 4: Present Comparison Report\n\nDisplay agent's comparative report to user.\n\n**Example output format:**\n```markdown\n# Branch Comparison\n\n**Branch A:** autonomy/experiment-a\n**Branch B:** autonomy/experiment-b\n\n---\n\n## Divergence Information\n\n**Common ancestor:** abc123f\n**Divergence date:** 2025-12-15\n**Divergence iteration:** 0015\n\nBranches have been exploring different approaches for 18 days.\n\n---\n\n## Iteration Comparison\n\n| Branch | Iterations Since Divergence | Current Iteration | Latest Update |\n|--------|----------------------------|-------------------|---------------|\n| experiment-a | 13 (0016-0028) | 0028 | 2026-01-02 |\n| experiment-b | 8 (0016-0023) | 0023 | 2025-12-28 |\n\n**Observation:** Branch A has progressed more iterations but is currently blocked. Branch B has fewer iterations but is active.\n\n---\n\n## Metrics Comparison\n\n### MRR Trajectory\n\n- **experiment-a:** $45k â†’ $62k (+37.8%)\n  - Faster growth, reached $62k at iteration 0028\n- **experiment-b:** $45k â†’ $58k (+28.9%)\n  - Steady growth, reached $58k at iteration 0023\n\n### Build Time\n\n- **experiment-a:** 5.2min â†’ 3.2min (-38.5%)\n  - Significant optimization focus\n- **experiment-b:** 5.2min â†’ 4.8min (-7.7%)\n  - Minor improvements only\n\n---\n\n## Approach Differences\n\n### Branch A (experiment-a): Usage-based pricing\n- Implemented tiered usage model\n- Real-time usage tracking\n- API rate limiting integration\n- **Current status:** Blocked on Stripe API integration\n\n### Branch B (experiment-b): Flat enterprise pricing\n- Fixed pricing tiers (Startup/Growth/Enterprise)\n- Annual commitment discounts\n- Sales-assisted onboarding\n- **Current status:** Active, implementing pricing page UI\n\n---\n\n## Outcomes and Status\n\n| Branch | Current Status | Key Achievement | Main Blocker |\n|--------|----------------|-----------------|--------------|\n| experiment-a | blocked | Higher MRR growth (+37.8%) | Stripe webhook docs |\n| experiment-b | active | Simpler implementation | None currently |\n\n---\n\n## Insights and Recommendations\n\n**What worked well:**\n- **experiment-a:** Usage-based model drove higher revenue but added complexity\n- **experiment-b:** Flat pricing is simpler to implement and maintain\n\n**What didn't work:**\n- **experiment-a:** Dependency on external Stripe API caused blocking\n- **experiment-b:** Slower revenue growth compared to usage model\n\n**Cross-branch learning:**\n- experiment-b could adopt experiment-a's build optimizations (-38.5% improvement)\n- experiment-a could simplify by borrowing experiment-b's pricing page approach\n- Consider hybrid: flat base + usage overage (best of both)\n\n**Recommendations:**\n1. If revenue growth is priority: Continue experiment-a, resolve Stripe blocker\n2. If speed to market is priority: Continue experiment-b, ship simple version\n3. If unsure: Fork new branch from divergence point implementing hybrid approach\n```\n\n## Important Notes\n\n### Only Autonomy Branches\n\nThis skill ONLY compares `autonomy/*` branches:\n- Validates both branches have `autonomy/` prefix\n- Will not compare non-autonomy branches\n- Branches must share common ancestor (git merge-base succeeds)\n\n### Computational Comparison Required\n\n**DO NOT:**\n- Manually compare commits\n- \"Eyeball\" which branch is better\n- Guess at metrics differences\n\n**DO:**\n- Dispatch branch-analyzer agent\n- Let agent generate Python scripts\n- Use computational methods for precision\n\n### Read-Only Operations\n\nAll comparison happens via git commands:\n- Never checkout either branch\n- Read commits via `git log <branch>` for each\n- Branch-analyzer uses read-only operations\n- No modifications to any files\n\n### No Value Judgments\n\n**DO NOT:**\n- Declare one branch \"better\" or \"worse\"\n- Recommend abandoning a branch\n- Make strategic decisions for user\n\n**DO:**\n- Present objective comparison\n- Note trade-offs\n- Suggest cross-branch learning opportunities\n- Let user decide which approach to continue\n\n### Branches May Have No Metrics\n\n- Not all branches track quantitative metrics\n- Metrics section may be \"None\" in commit messages\n- Report should handle missing metrics gracefully\n- Can still compare on iterations, status, decisions\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll declare branch A is better\" | NO. Present objective comparison, let user decide. |\n| \"I'll recommend merging branches\" | NO. Autonomy branches never merge. Only cross-learning via /analyze-branch. |\n| \"Only 5 iterations different, I can compare manually\" | NO. Always dispatch branch-analyzer for computational analysis. |\n| \"Branches have no common ancestor, I'll error\" | CORRECT. This is a real error case - branches are independent. |\n| \"I'll checkout branches to compare journals\" | NO. Read commit messages via git log. Never checkout. |\n\n## After Comparing\n\nOnce comparison is complete:\n- Report displayed to user\n- No files created or modified\n- User can fork new branch from divergence point: `/fork-iteration <iteration> <strategy-name>`\n- User can analyze either branch individually: `/branch-status <branch-name>`\n- User can use `/analyze-branch` to extract specific learnings from one branch for use in another\n",
        "plugins/autonomy/skills/creating-a-goal/SKILL.md": "---\nname: creating-a-goal\ndescription: Use when setting up a new open-ended goal for autonomy tracking, before starting the first iteration\n---\n\n# Creating a Goal\n\n## Overview\n\nSet up a new open-ended goal by creating the directory structure, writing the goal definition, and preparing for iteration tracking.\n\n**Core principle:** One-time setup. Run this once per goal, then use starting-an-iteration for all subsequent work.\n\n## When to Use\n\nUse this skill when:\n- User runs `/create-goal` command\n- Starting a brand new open-ended goal\n- No existing autonomy goal directory exists\n\n**DO NOT use for:**\n- Continuing existing goal (use starting-an-iteration instead)\n- Closed goals with definition of done (use ed3d-superpowers workflow)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Verify no existing goal | Check for autonomy directory | Glob |\n| 2. Get goal details | Prompt user for goal statement | User interaction |\n| 3. Generate directory name | Convert to kebab-case | Manual |\n| 4. Create structure | Make directory | Bash |\n| 5. Write goal.md | Document goal definition | Write |\n| 6. Announce ready | Tell user to run /start-iteration | Direct output |\n\n## Process\n\n### Step 1: Verify No Existing Goal\n\nCheck that no goal already exists:\n\n```bash\n# Use Glob to check\npattern: \"autonomy/*/goal.md\"\n```\n\n**If goal.md found:**\n```\n\"An autonomy goal already exists: [goal-name]\nUse '/start-iteration' to continue working on this goal.\nIf you want to create a different goal, first archive or remove the existing one.\"\n```\nStop here - don't create duplicate goals.\n\n**If no goal found:**\nProceed to Step 2.\n\n### Step 2: Get Goal Details from User\n\nPrompt user for goal information:\n\n```\n\"What open-ended goal would you like to pursue?\n\nThis should be a goal that never truly completes - ongoing optimization, continuous improvement, or iterative exploration.\n\nExamples:\n- Maximize monthly recurring revenue\n- Improve developer productivity\n- Reduce customer churn\n- Optimize application performance\n\nYour goal:\"\n```\n\n**User provides goal statement.**\n\nThen ask about success criteria:\n\n```\n\"What metrics or indicators should we track for this goal?\n\nSince this is open-ended, there's no 'done' state, but we can track progress.\n\nExamples:\n- MRR: Starting at $45k/month\n- Churn rate: Currently 13%\n- Build time: Currently 5 minutes\n\nYour metrics (or 'none' if not applicable):\"\n```\n\n**User provides metrics or indicates none.**\n\n### Step 3: Generate Directory Name\n\nConvert goal statement to kebab-case:\n\n**Rules:**\n- Lowercase all characters\n- Replace spaces with hyphens\n- Remove special characters\n- Keep concise (max 50 chars)\n\n**Examples:**\n- \"Maximize monthly recurring revenue\" â†’ `maximize-monthly-recurring-revenue`\n- \"Improve developer productivity\" â†’ `improve-developer-productivity`\n- \"Reduce customer churn by 50%\" â†’ `reduce-customer-churn`\n\n### Step 4: Create Directory Structure\n\nCreate the goal directory:\n\n```bash\n# Use Bash to create directory\nmkdir -p autonomy/[goal-name]\n```\n\n### Step 5: Write goal.md\n\nCreate the goal definition file:\n\n```markdown\n# Goal: [Original goal statement]\n\n## Goal Statement\n[Full description from user]\n\n## Success Criteria\n[Open-ended - note that this has no completion state]\n\n## Metrics to Track\n[If user provided metrics:]\n- [Metric 1]: [Starting value]\n- [Metric 2]: [Starting value]\n\n[If no metrics:]\n- No specific metrics defined\n- Progress will be qualitative\n\n## Current Status\nActive - Ready for iteration 1\n\n## Started\n[Current date: YYYY-MM-DD]\n\n## Notes\n[Any additional context from user]\n```\n\nWrite this file to: `autonomy/[goal-name]/goal.md`\n\n### Step 6: Announce Ready\n\nInform user that goal is created:\n\n```markdown\n**Goal created: [goal-name]**\n\nDirectory: `autonomy/[goal-name]/`\nGoal definition: `autonomy/[goal-name]/goal.md`\n\n---\n\n**Next step:** Run `/start-iteration` to begin the first iteration.\n\nThis will:\n- Create iteration-0001-[today's date].md\n- Set up workspace for working toward the goal\n- Track progress in iteration journals\n```\n\n## Important Notes\n\n### Single Goal Per Project\n\nCurrently, autonomy supports one goal per project directory:\n- If goal.md exists, creation fails\n- User must archive/remove existing goal to create new one\n- Future enhancement could support multiple goals\n\n### Goal Naming\n\nKeep directory names:\n- **Descriptive** but **concise**\n- **Stable** (won't change over time)\n- **Filesystem-safe** (kebab-case, no special chars)\n\n### Open-Ended Requirement\n\nVerify goal is truly open-ended:\n\n**Good examples:**\n- \"Maximize X\" - always can improve\n- \"Reduce Y\" - continuous optimization\n- \"Improve Z\" - never fully complete\n\n**Bad examples:**\n- \"Build authentication system\" - has done state (use ed3d-superpowers)\n- \"Fix bug #123\" - has done state\n- \"Write documentation\" - has done state\n\nIf user suggests closed goal, guide them:\n```\n\"This goal seems to have a clear completion state. For goals with a definition of 'done',\nconsider using the ed3d-superpowers development workflow instead.\n\nAutonomy is designed for never-ending optimization and improvement goals.\n\nWould you like to reframe this as an open-ended goal, or would a different workflow be better?\"\n```\n\n### Metrics Are Optional\n\nNot all open-ended goals have quantifiable metrics:\n- \"Improve code quality\" - may be qualitative\n- \"Increase team knowledge\" - hard to measure\n- \"Enhance user experience\" - subjective\n\nAccept \"none\" or \"qualitative\" as valid answers.\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll create goal even though one exists\" | NO. One goal per project. Check first. |\n| \"User goal is closed-ended but I'll create anyway\" | NO. Guide to appropriate workflow. |\n| \"I'll skip metrics prompt to save time\" | NO. Always ask - user may have important metrics. |\n| \"Directory name can have spaces\" | NO. Use kebab-case only. |\n| \"I'll start first iteration automatically\" | NO. Let user run /start-iteration when ready. |\n\n## After Creating\n\nOnce goal is created:\n- goal.md exists with definition\n- Directory ready for iteration journals\n- User runs `/start-iteration` to begin work\n- Iteration 1 will create first journal entry\n",
        "plugins/autonomy/skills/ending-an-iteration/SKILL.md": "---\nname: ending-an-iteration\ndescription: Use when concluding work on an open-ended goal to write iteration journal entry documenting work performed, decisions made, and state changes\n---\n\n# Ending an Iteration\n\n## Overview\n\nConclude the current iteration by reviewing the conversation, documenting what happened, and preparing state for the next iteration.\n\n**Core principle:** Comprehensive journaling enables continuity. Future iterations depend on accurate state capture.\n\n## When to Use\n\nUse this skill when:\n- User runs `/end-iteration` command\n- Agent suggests natural stopping point and user confirms\n- Context limit approaching and good time to pause\n- Subtask complete and ready to hand off to next iteration\n\n**DO NOT use for:**\n- Middle of active work (finish current subtask first)\n- Before resolving critical blockers (unless blocked externally)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Review conversation | Extract skills, decisions, artifacts | Manual review |\n| 2. Identify stopping point | Why is this iteration ending? | User confirmation |\n| 3. Complete journal entry | Update existing journal file | Read, Edit |\n| 4. Update summary | If iteration 5, 10, 15, etc. | Task (journal-summarizer) |\n| 5a. Git commit and tag | Commit journal to current branch | Bash |\n| 5b. Announce completion | Confirm next steps | Direct output |\n\n## Process\n\n### Step 1: Review Conversation\n\nReview the full conversation to extract key information:\n\n**Skills & Workflows Used:**\n- Scan conversation for `<invoke name=\"Skill\">` tool calls\n- Note which skills were used and for what purpose\n- Generic detection - works with any plugin skills\n\nExample findings:\n```markdown\n### Skills & Workflows Used\n- `brainstorming`: Designed pricing tier alternatives\n- `hypothesis-testing`: Validated annual billing preference\n- `internet-researcher`: Found competitor pricing data\n```\n\n**Key Decisions Made:**\n- Identify major choices: architecture, strategy, approach\n- Note rationale: why was this chosen?\n- Record alternatives considered\n\nExample:\n```markdown\n### Key Decisions Made\n- **Decision:** Implement usage-based pricing tier\n  **Rationale:** Research showed power users want to scale gradually\n  **Alternatives:** Flat enterprise tier (rejected: less flexible)\n```\n\n**Artifacts Created/Modified:**\n- Files created or changed (use git status/diff if applicable)\n- Git commits made during iteration\n- Pull requests opened\n- Documentation written\n\nExample:\n```markdown\n### Artifacts Created/Modified\n- `src/pricing/usage-tier.ts`: New usage-based pricing calculator\n- `docs/pricing-strategy.md`: Documented pricing decision rationale\n- Git commit: abc123f \"Add usage-based pricing tier\"\n```\n\n**External Context Gathered:**\n- Web research findings\n- User feedback received\n- Documentation consulted\n- Competitor analysis\n\nExample:\n```markdown\n### External Context Gathered\n- Research: Competitor X charges $0.10/unit, Competitor Y charges $0.15/unit\n- User feedback: \"Annual discount of 20% drives conversions\"\n- Documentation: Stripe API supports usage-based billing natively\n```\n\n**Reasoning & Strategy Changes:**\n- Why certain approaches were chosen\n- What alternatives were explored\n- Where strategy pivoted and why\n\nExample:\n```markdown\n### Reasoning & Strategy Changes\n- Initially planned flat enterprise tier\n- Research showed power users prefer scaling gradually\n- Pivoted to usage-based model to reduce friction\n- This aligns with SaaS best practices for 2026\n```\n\n**Blockers Encountered:**\n- What's preventing progress?\n- Dependencies on external factors\n- Questions needing answers\n\nExample:\n```markdown\n### Blockers Encountered\n- Stripe webhook integration unclear: need to consult API docs\n- Finance team needs to approve pricing before launch\n- Usage tracking infrastructure not yet built (dependency)\n```\n\n**Open Questions:**\n- What needs to be resolved next?\n- Decisions deferred to future iterations\n- Unknowns requiring investigation\n\nExample:\n```markdown\n### Open Questions\n- Should we offer annual discount on usage-based tier?\n- What's the right per-unit price point?\n- Do we need a minimum monthly commit?\n```\n\n### Step 2: Identify Stopping Point\n\nDetermine why this iteration is ending:\n\n**Ask user to confirm:**\n```\n\"Suggested stopping point: [reason]. Should we end this iteration?\"\n\nReasons:\n- Subtask complete: [what was finished]\n- Blocked on: [external dependency]\n- Context approaching limit\n- Natural break point: [why this makes sense]\n```\n\n**User confirms** â†’ Proceed to write journal\n\n**User wants to continue** â†’ Return to work, don't end iteration yet\n\n### Step 3: Complete Journal Entry\n\nThe journal file was created by starting-an-iteration. Now complete it:\n\n1. **Find current journal file:**\n   ```bash\n   # Use Glob to find iteration files\n   pattern: \"autonomy/*/iteration-*.md\"\n   ```\n\n   Sort by iteration number and identify the most recent (should be today's date).\n\n2. **Read existing journal:**\n   ```bash\n   # Use Read to load current content\n   file: \"autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\"\n   ```\n\n   Journal will have:\n   - Beginning State (from starting-an-iteration)\n   - Iteration Intention (from starting-an-iteration)\n   - Work Performed (may be filled by checkpointing, or still empty)\n   - Ending State (empty - we'll fill this)\n   - Iteration Metadata (empty - we'll fill this)\n\n3. **Update Work Performed section:**\n\n   Using findings from Step 1, update the Work Performed subsections.\n\n   **If Work Performed is empty:**\n   Replace entire section with Step 1 findings.\n\n   **If Work Performed has content (from checkpointing):**\n   Merge Step 1 findings with existing:\n   - Add any new skills/workflows used since checkpoint\n   - Add any new decisions made since checkpoint\n   - Add new artifacts created since checkpoint\n   - Append new context gathered since checkpoint\n   - Note any additional blockers or questions\n   - Preserve all existing checkpoint content\n\n4. **Fill Ending State section:**\n   ```markdown\n   ## Ending State\n   [What is the state NOW at iteration end?]\n   - Progress made during this iteration\n   - What's complete vs. what remains\n   - Updated metrics (if applicable)\n   - How well did we achieve the iteration intention?\n   - Recommended next steps for iteration N+1\n   ```\n\n5. **Fill Iteration Metadata section:**\n   ```markdown\n   ## Iteration Metadata\n   - Context usage: [Note if approaching limits, if compaction occurred]\n   - Checkpoints: [How many times was checkpoint-iteration used?]\n   - Suggested next action: [What should iteration N+1 work on?]\n   ```\n\n6. **Update file using Edit tool:**\n   - Replace Work Performed section (merge with existing)\n   - Replace Ending State section (fill in)\n   - Replace Iteration Metadata section (fill in)\n   - Preserve Beginning State and Iteration Intention sections\n\n### Step 4: Update Summary (if needed)\n\nCheck if summary update is needed:\n\n**Update summary every 5 iterations:** 5, 10, 15, 20, etc.\n\n```\nIf current iteration number % 5 == 0:\n   Dispatch journal-summarizer agent to update summary.md\n```\n\n**Dispatch journal-summarizer:**\n```\nTask tool with subagent_type: \"autonomy:journal-summarizer\"\nPrompt: \"Update summary.md for goal '[goal-name]' to include iterations 1-[N].\n        Previous summary covered iterations 1-[N-5].\n        Add new learnings, update metrics, note new blockers.\"\nModel: haiku\n```\n\n**Wait for agent** to update summary.md\n\n### Step 5a: Git Commit and Tag\n\nAfter journal is complete and summary is updated (if needed), commit to git:\n\n1. **Extract iteration metadata:**\n   - Goal name from journal path: `autonomy/[goal-name]/`\n   - Iteration number from filename: `iteration-NNNN-YYYY-MM-DD.md`\n   - Extract 2-3 line summary from Ending State section\n\n2. **Determine iteration status:**\n\n   Analyze Ending State section to determine status:\n   - Look for explicit status indicators: \"This is a dead end\", \"Approach invalidated\", \"Successfully completed\", \"Blocked by\", etc.\n   - **active** (default) - Normal progression, work continuing\n   - **blocked** - Contains phrases like \"blocked by\", \"waiting on\", \"cannot proceed until\"\n   - **concluded** - Contains phrases like \"successfully completed\", \"goal achieved\", \"experiment succeeded\"\n   - **dead-end** - Contains phrases like \"dead end\", \"not working\", \"abandoning approach\", \"invalidated\"\n\n3. **Extract quantitative metrics:**\n\n   Parse Ending State for metrics with numbers:\n   - Look for patterns like \"MRR: $62k (+12%)\", \"Build time: 3.2min (-40%)\", \"Churn: 8% (from 13%)\"\n   - Collect all quantitative progress indicators\n   - Format as single line: `Metrics: [metric1], [metric2], ...` or \"None\" if no metrics\n\n4. **Summarize journal content:**\n\n   Create 4-6 sentence summary from Work Performed and Ending State:\n   - What was accomplished this iteration\n   - Key decisions made and rationale\n   - Major learnings or discoveries\n   - How this iteration moved toward the goal\n   - Be substantive but concise - this is for git log readers\n\n5. **Build enhanced commit message:**\n   ```\n   journal: [goal-name] iteration NNNN\n\n   [2-3 line summary from Ending State - as before]\n\n   ## Journal Summary\n\n   [4-6 sentence summary from step 4 above - what happened, what was learned, what changed]\n\n   ## Iteration Metadata\n\n   Status: [active|blocked|concluded|dead-end]\n   Metrics: [quantitative metrics from step 3, or \"None\"]\n   Blockers: [summary from Blockers Encountered section, or \"None\"]\n   Next: [next iteration intention from Iteration Metadata section]\n\n   ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\n   Co-Authored-By: Claude <noreply@anthropic.com>\n   ```\n\n6. **Stage files:**\n   ```bash\n   # Always stage journal file\n   git add autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\n\n   # If summary was updated this iteration (iteration % 5 == 0)\n   git add autonomy/[goal-name]/summary.md\n   ```\n\n7. **Create commit:**\n   ```bash\n   # Use heredoc for multi-line message with enhanced format\n   git commit -m \"$(cat <<'EOF'\n   journal: [goal-name] iteration NNNN\n\n   [2-3 line summary from Ending State]\n\n   ## Journal Summary\n\n   [4-6 sentence summary of iteration]\n\n   ## Iteration Metadata\n\n   Status: [active|blocked|concluded|dead-end]\n   Metrics: [metrics or \"None\"]\n   Blockers: [blockers or \"None\"]\n   Next: [next iteration intention]\n\n   ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\n   Co-Authored-By: Claude <noreply@anthropic.com>\n   EOF\n   )\"\n   ```\n\n8. **Create annotated tag with branch-aware naming:**\n   ```bash\n   # Get current branch name\n   current_branch=$(git branch --show-current)\n\n   # Extract strategy name from branch\n   # For autonomy branches: \"autonomy/experiment-a\" â†’ \"experiment-a\"\n   # For non-autonomy branches: use goal name as strategy\n   if [[ \"$current_branch\" =~ ^autonomy/ ]]; then\n     strategy_name=${current_branch#autonomy/}\n   else\n     # Use goal name from journal path\n     strategy_name=\"[goal-name]\"\n   fi\n\n   # Tag format: autonomy/<strategy-name>/iteration-NNNN (4 digits, zero-padded)\n   git tag -a \"autonomy/${strategy_name}/iteration-$(printf '%04d' NNNN)\" \\\n     -m \"journal: [goal-name] iteration NNNN\"\n   ```\n\n9. **Handle errors gracefully:**\n   - If git operations fail (not a repo, detached HEAD, permissions, etc.):\n     - Capture error message\n     - Continue to Step 5b anyway (journal is written, that's critical)\n     - Report warning to user with manual commands\n\n**Error reporting format:**\n```markdown\nâš ï¸ **Git Integration Warning:**\nFailed to commit journal: [error message]\n\nYou can manually commit with:\n  git add autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\n  git commit -m \"journal: [goal-name] iteration NNNN ...\"\n  git tag -a \"autonomy/[strategy-name]/iteration-NNNN\" -m \"journal: [goal-name] iteration NNNN\"\n```\n\n**Success indicator:**\n- If git operations succeed, note success for Step 5b announcement\n- Tag `autonomy/[strategy-name]/iteration-NNNN` marks this iteration in git history\n\n### Step 5b: Announce Completion\n\nReport to user with git status:\n\n**If git operations succeeded:**\n```markdown\n**Iteration [N] complete for goal: [goal-name]**\n\nâœ“ Journal committed and tagged: `autonomy/[strategy-name]/iteration-NNNN`\n\nJournal entry: `autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md`\nBranch: [current-branch-name]\nStatus: [active|blocked|concluded|dead-end]\n\n## Summary of This Iteration\n- **Work completed:** [Brief summary]\n- **Key decisions:** [Major choices made]\n- **Blockers:** [What's preventing progress]\n- **Next steps:** [Recommended for iteration N+1]\n\n---\n\n**Ready to resume in next conversation with `/start-iteration`**\n```\n\n**If git operations failed:**\n```markdown\n**Iteration [N] complete for goal: [goal-name]**\n\nJournal entry written: `autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md`\n\nâš ï¸ **Git Integration Warning:**\nFailed to commit journal: [error message]\n\nYou can manually commit with:\n  git add autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md\n  git commit -m \"journal: [goal-name] iteration NNNN ...\"\n  git tag -a \"autonomy/[strategy-name]/iteration-NNNN\" -m \"journal: [goal-name] iteration NNNN\"\n\n## Summary of This Iteration\n- **Work completed:** [Brief summary]\n- **Key decisions:** [Major choices made]\n- **Blockers:** [What's preventing progress]\n- **Next steps:** [Recommended for iteration N+1]\n\n---\n\n**Ready to resume in next conversation with `/start-iteration`**\n```\n\n## Important Notes\n\n### Journal File Already Exists\n\nThe journal file was created by starting-an-iteration with:\n- Beginning State (from context loading)\n- Iteration Intention (from user input)\n- Empty Work Performed sections (to be filled)\n\nYour job is to **complete** the journal, not create it from scratch.\n\n### Merging with Checkpoint Content\n\nIf user ran `/checkpoint-iteration` during the iteration:\n- Work Performed section will have partial content\n- Merge your Step 1 findings with existing content\n- Add new information discovered since checkpoint\n- Don't overwrite or lose checkpoint data\n\n### Comprehensive Documentation\n\n**Critical:** Future iterations depend entirely on this journal. Include:\n- Enough detail that iteration N+1 can pick up seamlessly\n- Rationale for decisions (not just what, but why)\n- All blockers, even minor ones\n- Specific next steps, not vague \"continue working\"\n\n### Date in Filename\n\nAlways use today's date (YYYY-MM-DD format):\n```bash\n# Get current date\ndate +%Y-%m-%d\n```\n\n### Don't Skip Sections\n\nEven if a section is empty, include it with a note:\n```markdown\n### Blockers Encountered\nNone - iteration progressed smoothly\n```\n\nThis shows the section was considered, not forgotten.\n\n### Git Integration\n\n**Automatic commits to current branch:**\n- Journal file is committed to whatever branch you're currently on\n- Does NOT create new branch or switch branches\n- Uses enhanced commit message format with:\n  - Brief summary (2-3 lines)\n  - Journal Summary section (4-6 sentences)\n  - Iteration Metadata section (status, metrics, blockers, next steps)\n- Tags commit as `autonomy/<strategy-name>/iteration-NNNN` for branch-aware navigation\n\n**Branch-aware tagging:**\n- On autonomy branches: Tag uses branch name (e.g., `autonomy/experiment-a/iteration-0015`)\n- On non-autonomy branches: Tag uses goal name (backward compatibility)\n- Each branch has its own iteration namespace\n- No tag collisions when multiple branches have same iteration number\n\n**Error handling:**\n- Git failures do NOT block iteration completion\n- Journal is always written, even if commit fails\n- User receives manual commands if git fails\n- Graceful degradation ensures state is preserved\n\n**What gets committed:**\n- Always: `iteration-NNNN-YYYY-MM-DD.md` journal file\n- Sometimes: `summary.md` (if iteration % 5 == 0)\n- Never: Other files in working directory\n\n**Tag benefits:**\n- Navigate to iteration on specific branch: `git checkout autonomy/experiment-a/iteration-0042`\n- List all iterations on a branch: `git tag -l 'autonomy/experiment-a/*'`\n- List all autonomy iterations: `git tag -l 'autonomy/*/*'`\n- Immutable history markers for CI/CD integration\n- Enables branch management commands to query git log for metadata\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll create new journal file instead of updating existing\" | NO. Journal was created by starting-an-iteration. Update it with Edit tool. |\n| \"I'll overwrite Work Performed that has checkpoint content\" | NO. Merge with existing. Preserve checkpoint data. |\n| \"Journal is too detailed, I'll abbreviate\" | NO. Be comprehensive. Future iterations need full context. |\n| \"No blockers this time, I'll skip that section\" | NO. Write \"None\" to show you checked. |\n| \"Summary update can wait\" | NO. If iteration % 5 == 0, update summary. |\n| \"I'll skip comparing against iteration intention\" | NO. Ending State should assess how well intention was achieved. |\n| \"Git commit failed, so iteration failed\" | NO. Journal writing is critical, git commit is helpful. Warn about git error but complete iteration. |\n| \"I'll create a new branch for the commit\" | NO. Commit to current branch. User controls branching strategy. |\n\n## After Ending\n\nOnce iteration is ended:\n- Journal file completed with Ending State and Iteration Metadata\n- Summary updated if needed (every 5 iterations)\n- Git commit created and tagged as `autonomy/iteration-NNNN` (if git operations succeeded)\n- Full iteration story captured for next iteration\n- Can start new iteration anytime with `/start-iteration`\n",
        "plugins/autonomy/skills/forking-iteration/SKILL.md": "---\nname: forking-iteration\ndescription: Use when user wants to create new autonomy branch from current commit or specific past iteration\n---\n\n# Forking Iteration\n\n## Overview\n\nCreate new autonomy branch forked from current commit or any iteration tag in git history. Enables \"slime mold strategy\" of parallel exploration.\n\n**Core principle:** Branch creation is independent of iteration management. Fork creates branch, start-iteration begins work.\n\n## When to Use\n\nUse this skill when:\n- User runs `/fork-iteration` command\n- User wants to create new exploration branch\n- User wants to try alternative approach from past iteration\n- User wants to bootstrap autonomy workflow from non-autonomy branch\n\n**DO NOT use for:**\n- Creating non-autonomy branches (use git directly)\n- Starting iterations (use starting-an-iteration instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse arguments | Extract iteration (optional) and strategy-name | Manual |\n| 2. Validate strategy-name | Check kebab-case, not already exists | Bash |\n| 3. Resolve fork point | Find iteration tag or use HEAD | Bash |\n| 4. Create branch | Checkout fork point, create autonomy branch | Bash |\n| 5. Report success | Confirm creation with next steps | Direct output |\n\n## Process\n\n### Step 1: Parse and Validate Arguments\n\nExtract arguments from command args:\n\n**Format:** `[iteration] <strategy-name>`\n\n**Parse:**\n```\nIf args contains only one word:\n  iteration = None\n  strategy_name = args\nElse if args contains two words:\n  iteration = first word\n  strategy_name = second word\nElse:\n  Error: \"Invalid arguments. Usage: /fork-iteration [iteration] <strategy-name>\"\n```\n\n**Validate strategy-name:**\n- Must be kebab-case (lowercase, hyphens, no special chars)\n- Must not be empty\n- Should be descriptive (warn if too generic like \"test\" or \"new\")\n\n**Normalize strategy-name:**\n```bash\n# Convert to lowercase, replace invalid chars with hyphens\nstrategy_name=$(echo \"$strategy_name\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]-' '-' | sed 's/^-*//; s/-*$//')\n```\n\n### Step 2: Check if Branch Already Exists\n\nValidate that `autonomy/<strategy-name>` doesn't already exist:\n\n```bash\n# Check for existing branch (local or remote)\nif git branch -a | grep -q \"autonomy/$strategy_name\\$\"; then\n  # Error: branch exists\nfi\n```\n\n**If exists:**\n```markdown\nError: Branch 'autonomy/<strategy-name>' already exists.\n\nTo work on existing branch:\n  git checkout autonomy/<strategy-name>\n\nTo create with different name:\n  /fork-iteration [iteration] <different-name>\n\nAvailable autonomy branches:\n[List from git branch -a | grep autonomy/]\n```\n\nStop here if branch exists.\n\n### Step 3: Resolve Fork Point\n\nDetermine what commit to fork from:\n\n**If iteration specified:**\n```bash\n# Search for iteration tag in current branch history\nmatching_tags=$(git tag --merged HEAD | grep \"iteration-$(printf '%04d' $iteration)\\$\")\n\ntag_count=$(echo \"$matching_tags\" | wc -l)\n\nif [ \"$tag_count\" -eq 0 ]; then\n  # Error: iteration not found\n  echo \"Error: Iteration $iteration not found in current branch history.\"\n  echo \"\"\n  echo \"Most recent iterations in current branch:\"\n  git tag --merged HEAD | grep 'iteration-' | tail -5\n  exit 1\nelif [ \"$tag_count\" -eq 1 ]; then\n  # Use the tag\n  fork_point=\"$matching_tags\"\nelse\n  # Multiple matches (shouldn't happen with branch-namespaced tags, but handle it)\n  echo \"Multiple iteration tags found for iteration $iteration:\"\n  echo \"$matching_tags\"\n  echo \"\"\n  # Use AskUserQuestion to let user choose\n  exit 1\nfi\n```\n\n**If iteration NOT specified:**\n```bash\n# Fork from current HEAD\nfork_point=$(git rev-parse HEAD)\nfork_description=\"current commit (HEAD)\"\n```\n\n**Validate fork point exists:**\n```bash\nif ! git rev-parse \"$fork_point\" >/dev/null 2>&1; then\n  echo \"Error: Fork point '$fork_point' not found in git history.\"\n  exit 1\nfi\n```\n\n### Step 4: Create New Branch\n\nCheckout fork point and create new autonomy branch:\n\n```bash\n# Checkout fork point (detached HEAD)\ngit checkout \"$fork_point\"\n\n# Create and switch to new branch\ngit checkout -b \"autonomy/$strategy_name\"\n\n# Confirm creation\ncurrent_branch=$(git branch --show-current)\nif [ \"$current_branch\" != \"autonomy/$strategy_name\" ]; then\n  echo \"Error: Failed to create branch 'autonomy/$strategy_name'\"\n  exit 1\nfi\n```\n\n**On success:**\n- Branch created and checked out\n- Working directory contains files from fork point\n- Ready for `/start-iteration`\n\n### Step 5: Report Success\n\nAnnounce successful branch creation:\n\n```markdown\nâœ“ Branch `autonomy/<strategy-name>` created\n\nForked from: [tag or commit hash]\nCurrent branch: autonomy/<strategy-name>\n\nNext step: Run `/start-iteration` to begin work on this branch.\n```\n\n## Important Notes\n\n### Branch Creation Only\n\n**This skill ONLY creates branches:**\n- Does NOT start iterations\n- Does NOT modify journal files\n- Does NOT determine iteration numbering\n- `start-iteration` will handle iteration logic when user runs it\n\n### Works from Any Starting Point\n\nCan fork from:\n- Autonomy branches (e.g., `autonomy/experiment-a`)\n- Non-autonomy branches (e.g., `main`, `develop`)\n- Detached HEAD\n- Any commit with autonomy iteration tags\n\n### No Merge Provisions\n\n**Autonomy branches NEVER merge:**\n- Each branch is independent exploration\n- Branches learn from each other via `/analyze-branch` (read-only)\n- No git merge operations\n- No rebase operations\n- Branches are peers, not hierarchical\n\n### Strategy Name Matters\n\nBranch name becomes identity:\n- Used in tag names: `autonomy/<strategy-name>/iteration-NNNN`\n- Appears in branch listings\n- Should describe exploration direction\n- Can't easily change later (would require tag renaming)\n\n**Good names:**\n- `usage-based-pricing`\n- `cdn-optimization`\n- `react-migration`\n\n**Bad names:**\n- `test` (too generic)\n- `new` (not descriptive)\n- `experiment1` (meaningless)\n\n### Iteration Numbering Continuity\n\nWhen `start-iteration` runs on new branch:\n- If forked from iteration tag: continues numbering (fork from 0015 â†’ next is 0016)\n- If forked from non-iteration commit: starts at 0001 (if goal exists)\n- If no goal exists: user must run `/create-goal` first\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll also start the iteration in this skill\" | NO. Branch creation is separate. User runs /start-iteration when ready. |\n| \"User wants to fork, I'll check out the branch for them\" | Already doing this. Checkout happens during creation. |\n| \"Branch name has spaces, I'll keep them\" | NO. Normalize to kebab-case. |\n| \"I'll search all branches for iteration tag\" | NO. Only search tags reachable from current HEAD (git tag --merged HEAD). |\n| \"Branch exists, I'll overwrite it\" | NO. Error and show user how to work with existing branch. |\n| \"No goal found, I'll create one\" | NO. User must run /create-goal explicitly. |\n\n## After Forking\n\nOnce branch is created:\n- User is on new `autonomy/<strategy-name>` branch\n- Working directory contains files from fork point\n- No iterations started yet\n- User runs `/start-iteration` to begin work\n- Iteration numbering determined by start-iteration skill\n",
        "plugins/autonomy/skills/forking-worktree/SKILL.md": "---\nname: forking-worktree\ndescription: Use when user wants to create new autonomy branch with dedicated worktree for parallel agent workflows\n---\n\n# Forking Worktree\n\n## Overview\n\nCreate new autonomy branch with dedicated worktree for running parallel Claude agents on different branches simultaneously. All worktrees created at repository root level (`.worktrees/autonomy/`), regardless of where command is invoked.\n\n**Core principle:** Worktrees represent branches, not iterations. Multiple iterations happen within same worktree. Worktrees enable parallel exploration with isolated working directories.\n\n## When to Use\n\nUse this skill when:\n- User runs `/fork-worktree` command\n- User wants to run multiple agents in parallel on different autonomy branches\n- User wants isolated working directory for new exploration branch\n- User wants to create autonomy branch without disturbing current working directory\n\n**DO NOT use for:**\n- Creating branches without worktrees (use forking-iteration instead)\n- Creating non-autonomy branches (use git directly)\n- Starting iterations (use starting-an-iteration instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse arguments | Extract iteration (optional) and strategy-name | Manual |\n| 2. Detect repository root | Find git root from anywhere (main repo or worktree) | Bash |\n| 3. Validate | Check strategy-name, branch, and worktree path availability | Bash |\n| 4. Resolve fork point | Find iteration tag or use HEAD | Bash |\n| 5. Create worktree | Create branch + worktree at root level | Bash |\n| 6. Report success | Confirm creation with navigation instructions | Direct output |\n\n## Process\n\n### Step 1: Parse and Validate Arguments\n\nExtract arguments from command args:\n\n**Format:** `[iteration] <strategy-name>`\n\n**Parse:**\n```\nIf args contains only one word:\n  iteration = None\n  strategy_name = args\nElse if args contains two words:\n  iteration = first word\n  strategy_name = second word\nElse:\n  Error: \"Invalid arguments. Usage: /fork-worktree [iteration] <strategy-name>\"\n```\n\n**Validate strategy-name:**\n- Must be kebab-case (lowercase, hyphens, no special chars)\n- Must not be empty\n- Should be descriptive (warn if too generic like \"test\" or \"new\")\n\n**Normalize strategy-name:**\n```bash\n# Convert to lowercase, replace invalid chars with hyphens\nstrategy_name=$(echo \"$strategy_name\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]-' '-' | sed 's/^-*//; s/-*$//')\n```\n\n### Step 2: Detect Repository Root\n\nFind the git repository root, regardless of whether we're in main repo or inside a worktree:\n\n```bash\n# Get common git directory (works from anywhere, including worktrees)\ngit_common_dir=$(git rev-parse --git-common-dir)\n\n# Repository root is parent of .git directory\n# For main repo: .git is directory -> parent is repo root\n# For worktree: .git is file pointing to .git/worktrees/X -> common-dir points back to main .git\nif [ -d \"$git_common_dir\" ]; then\n  repo_root=$(cd \"$git_common_dir/..\" && pwd)\nelse\n  echo \"Error: Unable to determine repository root\"\n  exit 1\nfi\n```\n\n**Why this matters:**\n- Ensures all worktrees created at `.worktrees/autonomy/` relative to repo root\n- Prevents nested worktrees (`.worktrees/autonomy/experiment-a/.worktrees/...`)\n- Works whether invoked from main repo or from within another worktree\n\n### Step 3: Validate Branch and Worktree Availability\n\nCheck that we can create the new branch and worktree:\n\n**Check branch doesn't exist:**\n```bash\nif git branch -a | grep -q \"autonomy/$strategy_name\\$\"; then\n  echo \"Error: Branch 'autonomy/$strategy_name' already exists.\"\n  echo \"\"\n  echo \"To work on existing branch:\"\n  echo \"  git checkout autonomy/$strategy_name\"\n  echo \"\"\n  echo \"To create worktree for existing branch:\"\n  echo \"  cd $repo_root\"\n  echo \"  git worktree add .worktrees/autonomy/$strategy_name autonomy/$strategy_name\"\n  echo \"\"\n  echo \"Available autonomy branches:\"\n  git branch -a | grep autonomy/\n  exit 1\nfi\n```\n\n**Check worktree path doesn't exist:**\n```bash\nworktree_path=\"$repo_root/.worktrees/autonomy/$strategy_name\"\n\nif [ -e \"$worktree_path\" ]; then\n  echo \"Error: Worktree directory already exists: $worktree_path\"\n  echo \"\"\n  echo \"Remove it first:\"\n  echo \"  /remove-worktree $strategy_name\"\n  echo \"Or:\"\n  echo \"  git worktree remove .worktrees/autonomy/$strategy_name\"\n  exit 1\nfi\n```\n\n**Create parent directory if needed:**\n```bash\nmkdir -p \"$repo_root/.worktrees/autonomy\"\n```\n\n### Step 4: Resolve Fork Point\n\nDetermine what commit to fork from (same logic as forking-iteration):\n\n**If iteration specified:**\n```bash\n# Search for iteration tag in current branch history\nmatching_tags=$(git tag --merged HEAD | grep \"iteration-$(printf '%04d' $iteration)\\$\")\n\ntag_count=$(echo \"$matching_tags\" | grep -c '^' || echo \"0\")\n\nif [ \"$tag_count\" -eq 0 ]; then\n  echo \"Error: Iteration $iteration not found in current branch history.\"\n  echo \"\"\n  echo \"Most recent iterations in current branch:\"\n  git tag --merged HEAD | grep 'iteration-' | tail -5\n  exit 1\nelif [ \"$tag_count\" -eq 1 ]; then\n  fork_point=\"$matching_tags\"\n  fork_description=\"iteration $iteration (tag: $matching_tags)\"\nelse\n  # Multiple matches (shouldn't happen with branch-namespaced tags)\n  echo \"Multiple iteration tags found for iteration $iteration:\"\n  echo \"$matching_tags\"\n  echo \"\"\n  echo \"Please specify which tag to fork from.\"\n  exit 1\nfi\n```\n\n**If iteration NOT specified:**\n```bash\n# Fork from current HEAD\nfork_point=$(git rev-parse HEAD)\nfork_description=\"current commit ($(git rev-parse --short HEAD))\"\n```\n\n**Validate fork point exists:**\n```bash\nif ! git rev-parse \"$fork_point\" >/dev/null 2>&1; then\n  echo \"Error: Fork point '$fork_point' not found in git history.\"\n  exit 1\nfi\n```\n\n### Step 5: Create Worktree\n\nCreate the branch and worktree in a single git command:\n\n```bash\n# Navigate to repository root (important for relative path resolution)\ncd \"$repo_root\"\n\n# Create branch + worktree together\n# -b creates new branch\n# path is relative to repo root: .worktrees/autonomy/<strategy-name>\n# fork_point is where to create the branch from\ngit worktree add -b \"autonomy/$strategy_name\" \\\n  \".worktrees/autonomy/$strategy_name\" \\\n  \"$fork_point\"\n\n# Verify worktree created successfully\nif [ ! -d \"$worktree_path\" ]; then\n  echo \"Error: Failed to create worktree at $worktree_path\"\n  exit 1\nfi\n\n# Verify branch exists\nif ! git branch -a | grep -q \"autonomy/$strategy_name\\$\"; then\n  echo \"Error: Failed to create branch 'autonomy/$strategy_name'\"\n  exit 1\nfi\n```\n\n**What this does:**\n- Creates new branch `autonomy/<strategy-name>` at fork point\n- Creates worktree directory at `.worktrees/autonomy/<strategy-name>/`\n- Checks out the new branch in the worktree (not in current directory)\n- Current directory remains unchanged\n\n### Step 6: Report Success\n\nAnnounce successful worktree creation with navigation instructions:\n\n```markdown\nâœ“ Worktree created successfully\n\nBranch: autonomy/<strategy-name>\nWorktree: .worktrees/autonomy/<strategy-name>/\nForked from: [fork_description]\n\nNext steps:\n1. Navigate to worktree:\n   cd .worktrees/autonomy/<strategy-name>\n\n2. Start iteration:\n   /start-iteration\n\nThe worktree is an isolated working directory where you can work on this branch independently of other branches.\n```\n\n**Calculate relative path for convenience:**\n```bash\n# If we're already in a worktree, provide sibling navigation\ncurrent_dir=$(pwd)\nif [[ \"$current_dir\" == *\"/.worktrees/autonomy/\"* ]]; then\n  # In a worktree, provide relative navigation to sibling\n  echo \"From your current location:\"\n  echo \"  cd ../$strategy_name\"\nfi\n```\n\n## Important Notes\n\n### Repository Root Detection\n\n**Works from anywhere:**\n- Main repository working directory\n- Inside any worktree (including deeply nested work)\n- Detached HEAD state\n- Any subdirectory\n\n**All worktrees created at same level:**\n```\nrepo-root/\nâ”œâ”€â”€ .git/\nâ”œâ”€â”€ .worktrees/\nâ”‚   â””â”€â”€ autonomy/\nâ”‚       â”œâ”€â”€ experiment-a/    # Created from main repo\nâ”‚       â”œâ”€â”€ experiment-b/    # Created from experiment-a worktree\nâ”‚       â””â”€â”€ experiment-c/    # Created from experiment-b worktree\n```\n\n### Branch and Worktree Lifecycle\n\n**Branch persists after worktree removal:**\n- Removing worktree (via `/remove-worktree`) deletes directory\n- Branch `autonomy/<strategy-name>` and all commits persist\n- Can create new worktree for same branch later\n- All iteration tags remain in git history\n\n**One branch per worktree:**\n- Git enforces: branch can only be checked out in one worktree at a time\n- Attempting to create worktree for already-checked-out branch fails\n- This is a feature, not a bug (prevents conflicting changes)\n\n### Worktrees Are Optional\n\n**Existing workflows unchanged:**\n- `/fork-iteration` still creates branches without worktrees\n- All autonomy skills work in both main repo and worktrees\n- Worktrees are enhancement for parallel agent workflows\n- Use worktrees when beneficial, ignore when not needed\n\n### Isolation and Independence\n\n**Each worktree is independent:**\n- Separate working directory with own file state\n- Can have uncommitted changes independent of other worktrees\n- Can be at different commits (though shares branch history)\n- Perfect for running multiple Claude agents in parallel\n\n**Shared git metadata:**\n- All commits immediately visible across all worktrees\n- Tags created in one worktree visible in all worktrees\n- Branch operations (create, delete) affect all worktrees\n- `.git` directory is shared\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"Create worktree in current directory\" | NO. Always create at repo root `.worktrees/autonomy/` regardless of where invoked. |\n| \"I'll use relative path .worktrees/...\" | YES, but must cd to repo_root first. Path is relative to repo root. |\n| \"Branch exists, I'll create worktree for it\" | NO. This skill creates NEW branches. Existing branch = error. |\n| \"I'll also start the iteration\" | NO. Worktree creation is separate. User navigates and runs /start-iteration. |\n| \"I'll checkout the branch in current directory\" | NO. Branch checked out in worktree, current directory unchanged. |\n| \"Nested worktrees are fine\" | NO. All worktrees must be at root level `.worktrees/autonomy/`. |\n| \"I'll use git rev-parse --show-toplevel\" | NO. That gives worktree root, not repo root. Use --git-common-dir. |\n\n## After Creating Worktree\n\nOnce worktree is created:\n- New directory exists at `.worktrees/autonomy/<strategy-name>/`\n- Branch `autonomy/<strategy-name>` checked out in that directory\n- Current working directory unchanged (still in main repo or original worktree)\n- User must navigate to worktree: `cd .worktrees/autonomy/<strategy-name>`\n- User runs `/start-iteration` to begin work\n- All autonomy skills work normally in worktree\n\n## Integration with Parallel Agents\n\n**Typical workflow:**\n```bash\n# Terminal 1: First agent\n/fork-worktree experiment-a\ncd .worktrees/autonomy/experiment-a\n/start-iteration\n# Agent 1 works here\n\n# Terminal 2: Second agent (can run while agent 1 is working)\n/fork-worktree experiment-b\ncd .worktrees/autonomy/experiment-b\n/start-iteration\n# Agent 2 works here independently\n\n# Terminal 3: Analysis (from anywhere)\n/list-branches\n/compare-branches experiment-a experiment-b\n```\n\nEach agent has:\n- Isolated working directory\n- Independent iteration journals\n- Separate uncommitted changes\n- Shared git history and tags\n",
        "plugins/autonomy/skills/listing-branches/SKILL.md": "---\nname: listing-branches\ndescription: Use when user wants to inventory autonomy branches with custom sorting, grouping, or filtering\n---\n\n# Listing Branches\n\n## Overview\n\nDisplay inventory of all autonomy branches with user-specified sorting, grouping, and information display using computational analysis.\n\n**Core principle:** Use branch-analyzer agent with Python scripts for precise analysis. Never \"eyeball it\".\n\n## When to Use\n\nUse this skill when:\n- User runs `/list-branches` command\n- User wants to see all autonomy branches\n- User wants custom sorting or grouping of branches\n- User wants to filter branches by criteria\n\n**DO NOT use for:**\n- Analyzing single branch (use analyzing-branch-status instead)\n- Comparing two branches (use comparing-branches instead)\n- General iteration review on current branch (use reviewing-progress instead)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse query | Extract sorting/grouping/filter requirements | Manual |\n| 2. Dispatch agent | Send query to branch-analyzer | Task |\n| 3. Format results | Present markdown table to user | Direct output |\n\n## Process\n\n### Step 1: Parse User Query\n\nExtract requirements from user's query (if provided):\n\n**Default (no query):**\n- Sort by: most recent update (git log date)\n- Grouping: none (flat list)\n- Show: branch name, latest iteration, last update, status\n\n**Parse user query for:**\n- **Sorting:** \"sort by [criterion]\" â†’ most recent, alphabetical, iteration count, status\n- **Grouping:** \"group by [field]\" â†’ status, date range, metric value\n- **Filtering:** \"show only [criteria]\" â†’ active, blocked, updated since date\n- **Information:** \"with [fields]\" â†’ metrics, blockers, next steps\n\n**Example queries:**\n```\n\"sort by most recent, show only active\"\n  â†’ Sort: recency, Filter: status=active, Show: default fields\n\n\"group by status, show metrics\"\n  â†’ Group: status, Sort: recency within groups, Show: + metrics\n\n\"show branches updated in last 30 days\"\n  â†’ Filter: date > (today - 30 days), Sort: recency, Show: default\n```\n\n### Step 2: Dispatch Branch-Analyzer Agent\n\nDispatch the `branch-analyzer` agent with detailed instructions:\n\n```bash\nTask tool with subagent_type: \"autonomy:branch-analyzer\"\nModel: haiku\nPrompt: \"List all autonomy branches and analyze their status.\n\nUser query: [user's query or 'default: sort by most recent']\n\nRequirements:\n- Find all branches matching 'autonomy/*'\n- For each branch, find most recent journal commit (starts with 'journal: ')\n- Parse commit message for: status, metrics, blockers, next steps\n- [Apply sorting: {criterion}]\n- [Apply grouping: {field}]\n- [Apply filtering: {criteria}]\n- Generate Python script to process data\n- Output markdown table with columns: [requested fields]\n\nUse computational methods (Python scripts), do not eyeball the analysis.\"\n```\n\n**Agent will:**\n1. Run `git branch -a | grep 'autonomy/'` to list all autonomy branches\n2. For each branch, find most recent journal commit\n3. Parse commit message metadata (Status, Metrics, Blockers, Next)\n4. Generate Python script to sort/group/filter\n5. Execute Python script\n6. Return formatted markdown table\n\n### Step 3: Present Results\n\nDisplay agent's output to user.\n\n**Example output format:**\n```markdown\n# Autonomy Branches\n\nShowing 3 branches (sorted by most recent update)\n\n| Branch | Latest Iteration | Last Updated | Status | Metrics | Next |\n|--------|------------------|--------------|--------|---------|------|\n| experiment-a | 0028 | 2026-01-02 | blocked | MRR: $62k (+12%) | Resolve Stripe API integration |\n| experiment-b | 0015 | 2025-12-28 | active | Build: 3.2min (-40%) | Implement checkout flow |\n| initial-strategy | 0042 | 2025-12-15 | concluded | Churn: 8% (from 13%) | Goal achieved |\n```\n\n**If no autonomy branches found:**\n```markdown\nNo autonomy branches found.\n\nTo create your first autonomy branch:\n1. Run `/create-goal` to set up an open-ended goal\n2. Run `/fork-iteration <strategy-name>` to create autonomy branch\n3. Run `/start-iteration` to begin work\n```\n\n## Important Notes\n\n### Only Autonomy Branches\n\nThis skill ONLY operates on `autonomy/*` branches:\n- Filters for branches with `autonomy/` prefix\n- Will not show non-autonomy branches (e.g., `main`, `develop`)\n- For general iteration review, user should use `/review-progress`\n\n### Computational Analysis Required\n\n**DO NOT:**\n- Manually count or sort branches\n- \"Eyeball\" which branches are active\n- Guess at groupings or filters\n\n**DO:**\n- Dispatch branch-analyzer agent\n- Let agent generate Python scripts\n- Use computational methods for precision\n\n### Flexible Query Parsing\n\nUser queries are free-text and flexible:\n- Don't require exact syntax\n- Interpret intent from natural language\n- Ask via AskUserQuestion if query is ambiguous\n- Default to sensible behavior if unclear\n\n### No Branch Checkout Required\n\nAll analysis happens via git commands:\n- Never checkout branches\n- Read commit messages via `git log <branch>`\n- Branch-analyzer uses read-only operations\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll manually list branches from git branch output\" | NO. Dispatch branch-analyzer agent for computational analysis. |\n| \"Only 3 branches, I can eyeball the sorting\" | NO. Always use Python scripts for precision. |\n| \"User query is unclear, I'll guess\" | NO. Use AskUserQuestion to clarify if ambiguous. |\n| \"I'll check out each branch to read journals\" | NO. Use git log to read commit messages without checkout. |\n| \"Non-autonomy branch appeared, I'll include it\" | NO. Only autonomy/* branches. Strict filtering. |\n\n## After Listing\n\nOnce branches are listed:\n- Results displayed to user\n- No files created or modified\n- User can drill into specific branch with `/branch-status <branch-name>`\n- User can fork from any listed iteration with `/fork-iteration <iteration> <strategy-name>`\n",
        "plugins/autonomy/skills/listing-worktrees/SKILL.md": "---\nname: listing-worktrees\ndescription: Use when user wants to see all autonomy worktrees with their status\n---\n\n# Listing Worktrees\n\n## Overview\n\nList all autonomy worktrees in the repository, showing their branch, location, HEAD commit, and lock status. Helps users navigate and manage multiple parallel worktrees.\n\n**Core principle:** Worktrees are working directories, not branches. This lists checked-out worktrees only, not all autonomy branches.\n\n## When to Use\n\nUse this skill when:\n- User runs `/list-worktrees` command\n- User wants to see which autonomy branches have active worktrees\n- User needs to find worktree paths for navigation\n- User wants to identify locked or stale worktrees\n\n**DO NOT use for:**\n- Listing all autonomy branches (use `/list-branches` instead)\n- Analyzing branch status or progress (use `/branch-status`)\n- Comparing branches (use `/compare-branches`)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Get all worktrees | Run git worktree list | Bash |\n| 2. Filter to autonomy | Keep only .worktrees/autonomy/ paths | Bash |\n| 3. Format output | Create table with branch, path, HEAD, lock status | Manual |\n| 4. Provide guidance | Add navigation and management hints | Direct output |\n\n## Process\n\n### Step 1: Get All Worktrees\n\nRetrieve complete worktree list from git:\n\n```bash\n# Get porcelain format for machine parsing\nworktree_list=$(git worktree list --porcelain)\n\n# Format:\n# worktree /path/to/worktree\n# HEAD <commit-hash>\n# branch refs/heads/<branch-name>\n#\n# worktree /path/to/another\n# ...\n```\n\n**Parse porcelain output:**\n```bash\n# Extract worktree information\n# Each worktree is separated by blank line\n# Fields: worktree, HEAD, branch, detached, locked\n\n# Example parsing with awk:\ngit worktree list --porcelain | awk '\n  /^worktree / { path = substr($0, 10) }\n  /^HEAD / { head = substr($0, 6); head_short = substr(head, 1, 7) }\n  /^branch / { branch = substr($0, 8); gsub(\"refs/heads/\", \"\", branch) }\n  /^locked/ { locked = \"yes\" }\n  /^$/ {\n    if (path != \"\") {\n      print path \"|\" head_short \"|\" branch \"|\" locked\n      path = \"\"; head = \"\"; head_short = \"\"; branch = \"\"; locked = \"no\"\n    }\n  }\n  END {\n    if (path != \"\") {\n      print path \"|\" head_short \"|\" branch \"|\" locked\n    }\n  }\n'\n```\n\n### Step 2: Filter to Autonomy Worktrees\n\nKeep only worktrees in `.worktrees/autonomy/`:\n\n```bash\n# Filter parsed output to autonomy worktrees only\nautonomy_worktrees=$(echo \"$all_worktrees\" | grep \"\\.worktrees/autonomy/\")\n\n# Count autonomy worktrees\ncount=$(echo \"$autonomy_worktrees\" | grep -c \"^\" || echo \"0\")\n\nif [ \"$count\" -eq 0 ]; then\n  echo \"No autonomy worktrees found.\"\n  echo \"\"\n  echo \"Autonomy branches exist in main repository or without worktrees.\"\n  echo \"\"\n  echo \"To create a worktree:\"\n  echo \"  /fork-worktree <strategy-name>\"\n  echo \"\"\n  echo \"To see all autonomy branches:\"\n  echo \"  /list-branches\"\n  exit 0\nfi\n```\n\n### Step 3: Format Output Table\n\nCreate readable table with worktree information:\n\n```markdown\nAutonomy Worktrees:\n\nBranch                    Path                                      HEAD       Locked\nautonomy/experiment-a     .worktrees/autonomy/experiment-a          a1b2c3d\nautonomy/experiment-b     .worktrees/autonomy/experiment-b          d4e5f6g    ðŸ”’\nautonomy/cdn-optimize     .worktrees/autonomy/cdn-optimize          h7i8j9k\n\nTotal: 3 autonomy worktrees\n```\n\n**Column specifications:**\n- **Branch**: Full branch name (`autonomy/<strategy-name>`)\n- **Path**: Relative path from repository root\n- **HEAD**: Short commit hash (7 chars)\n- **Locked**: ðŸ”’ if locked, empty otherwise\n\n**Implementation:**\n```bash\n# Print header\nprintf \"%-25s %-41s %-10s %s\\n\" \"Branch\" \"Path\" \"HEAD\" \"Locked\"\n\n# Print each worktree\necho \"$autonomy_worktrees\" | while IFS='|' read -r path head branch locked; do\n  # Make path relative to repo root if absolute\n  rel_path=$(realpath --relative-to=\"$repo_root\" \"$path\" 2>/dev/null || echo \"$path\")\n\n  # Lock indicator\n  lock_icon=\"\"\n  if [ \"$locked\" = \"yes\" ]; then\n    lock_icon=\"ðŸ”’\"\n  fi\n\n  # Print row\n  printf \"%-25s %-41s %-10s %s\\n\" \"$branch\" \"$rel_path\" \"$head\" \"$lock_icon\"\ndone\n```\n\n### Step 4: Provide Guidance\n\nAdd helpful navigation and management instructions:\n\n```markdown\nTo navigate to a worktree:\n  cd .worktrees/autonomy/<strategy-name>\n\nTo remove a worktree:\n  /remove-worktree <strategy-name>\n\nTo create a new worktree:\n  /fork-worktree <strategy-name>\n\nNote: This lists worktrees only. To see all autonomy branches (including those without worktrees):\n  /list-branches\n```\n\n**Additional context:**\n```markdown\nLocked worktrees (ðŸ”’):\n- Cannot be removed without unlocking\n- To unlock: git worktree unlock .worktrees/autonomy/<strategy-name>\n```\n\n## Important Notes\n\n### Worktrees vs Branches\n\n**This command shows worktrees, not branches:**\n- A branch may exist without a worktree (created via `/fork-iteration`)\n- A branch with worktree can also be checked out in main repo (though git prevents this)\n- Use `/list-branches` to see all autonomy branches regardless of worktrees\n\n**Example scenario:**\n```\nBranches in repo:\n- autonomy/experiment-a (has worktree)\n- autonomy/experiment-b (has worktree)\n- autonomy/experiment-c (no worktree, created via /fork-iteration)\n\n/list-worktrees shows: experiment-a, experiment-b\n/list-branches shows: experiment-a, experiment-b, experiment-c\n```\n\n### Main Worktree\n\nThe main repository working directory is technically a worktree, but NOT listed here:\n- We filter to `.worktrees/autonomy/` only\n- Main repo worktree is not part of parallel agent workflow\n- Focus on dedicated worktrees for clarity\n\n### Locked Worktrees\n\nWorktrees can be locked to prevent accidental removal:\n\n```bash\n# Lock a worktree\ngit worktree lock .worktrees/autonomy/<strategy-name>\n\n# Lock with reason\ngit worktree lock --reason \"Long-running experiment\" .worktrees/autonomy/<strategy-name>\n\n# Unlock\ngit worktree unlock .worktrees/autonomy/<strategy-name>\n```\n\n**When locked:**\n- `/remove-worktree` fails\n- Must unlock manually before removal\n- Useful for protecting important worktrees\n\n### Relative Paths\n\nPaths displayed are relative to repository root:\n- `.worktrees/autonomy/experiment-a` (not absolute `/home/user/repo/.worktrees/...`)\n- Easier to copy-paste for `cd` commands\n- Consistent regardless of where command invoked\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"This shows all autonomy branches\" | NO. Only branches with worktrees. Use /list-branches for all branches. |\n| \"Main repo appears in list\" | NO. Only dedicated worktrees in .worktrees/autonomy/. |\n| \"I can see iteration progress here\" | NO. This is just worktree metadata. Use /branch-status for iteration progress. |\n| \"I'll parse git worktree list directly\" | YES, but use --porcelain for reliable parsing. |\n| \"All worktrees are unlocked\" | NO. Check locked field; some may be locked. |\n\n## After Listing Worktrees\n\nUser can:\n- Navigate to worktree: `cd .worktrees/autonomy/<strategy-name>`\n- Remove worktree: `/remove-worktree <strategy-name>`\n- View branch status: `/branch-status <strategy-name>`\n- Compare worktrees: `/compare-branches <strategy-a> <strategy-b>`\n\n## Integration with Other Commands\n\n**Relationship to other autonomy commands:**\n\n| Command | Shows | Purpose |\n|---------|-------|---------|\n| `/list-worktrees` | Worktrees only | Navigate to worktree directories |\n| `/list-branches` | All autonomy branches | See all exploration branches |\n| `/branch-status` | Single branch details | Iteration progress and status |\n| `/compare-branches` | Two branches comparison | Compare different approaches |\n\n**Typical workflow:**\n```bash\n# See which worktrees exist\n/list-worktrees\n\n# Navigate to one\ncd .worktrees/autonomy/experiment-a\n\n# Work on iteration\n/start-iteration\n# ... work ...\n/end-iteration\n\n# Check all branches (including ones without worktrees)\n/list-branches\n\n# Clean up worktree when done\ncd <repo-root>\n/remove-worktree experiment-a\n```\n\n## Empty State\n\nWhen no autonomy worktrees exist:\n```markdown\nNo autonomy worktrees found.\n\nAutonomy branches may exist in main repository or without worktrees.\n\nTo create a worktree:\n  /fork-worktree <strategy-name>\n\nTo see all autonomy branches:\n  /list-branches\n\nTo work on autonomy branch in main repo:\n  /fork-iteration <strategy-name>\n  /start-iteration\n```\n",
        "plugins/autonomy/skills/removing-worktree/SKILL.md": "---\nname: removing-worktree\ndescription: Use when user wants to safely remove an autonomy worktree while preserving the branch\n---\n\n# Removing Worktree\n\n## Overview\n\nSafely remove an autonomy worktree directory while preserving the autonomy branch, all commits, and iteration history. Only removes the working directory; git history remains intact.\n\n**Core principle:** Worktree removal is destructive for working directory but non-destructive for git history. Branch and all commits persist.\n\n## When to Use\n\nUse this skill when:\n- User runs `/remove-worktree` command\n- User finished with worktree and wants to clean up disk space\n- User wants to recreate worktree from scratch\n- User made mistakes in worktree and wants fresh start\n\n**DO NOT use for:**\n- Deleting branches (use git directly)\n- Deleting iterations or journals (those are committed, can't be removed this way)\n- Cleaning up non-autonomy worktrees\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Parse arguments | Extract --force flag and strategy-name | Manual |\n| 2. Detect repository root | Find git root from anywhere | Bash |\n| 3. Validate worktree exists | Check .worktrees/autonomy/<strategy-name>/ exists | Bash |\n| 4. Safety checks | Check uncommitted changes (unless --force) | Bash |\n| 5. Remove worktree | git worktree remove | Bash |\n| 6. Cleanup metadata | git worktree prune | Bash |\n| 7. Report success | Confirm removal, note branch persists | Direct output |\n\n## Process\n\n### Step 1: Parse Arguments\n\nExtract arguments from command args:\n\n**Format:** `[--force] <strategy-name>`\n\n**Parse:**\n```\nforce = false\nstrategy_name = \"\"\n\nfor arg in args:\n  if arg == \"--force\":\n    force = true\n  else:\n    if strategy_name != \"\":\n      Error: \"Multiple strategy names provided. Usage: /remove-worktree [--force] <strategy-name>\"\n    strategy_name = arg\n\nif strategy_name == \"\":\n  Error: \"Strategy name required. Usage: /remove-worktree [--force] <strategy-name>\"\n```\n\n**Normalize strategy-name:**\n```bash\n# Remove 'autonomy/' prefix if user included it\nstrategy_name=$(echo \"$strategy_name\" | sed 's/^autonomy\\///')\n```\n\n### Step 2: Detect Repository Root\n\nFind the git repository root (same logic as forking-worktree):\n\n```bash\n# Get common git directory\ngit_common_dir=$(git rev-parse --git-common-dir)\n\n# Repository root is parent of .git directory\nif [ -d \"$git_common_dir\" ]; then\n  repo_root=$(cd \"$git_common_dir/..\" && pwd)\nelse\n  echo \"Error: Unable to determine repository root\"\n  exit 1\nfi\n```\n\n### Step 3: Validate Worktree Exists\n\nCheck that the worktree actually exists:\n\n```bash\nworktree_path=\"$repo_root/.worktrees/autonomy/$strategy_name\"\n\nif [ ! -d \"$worktree_path\" ]; then\n  echo \"Error: Worktree not found: $worktree_path\"\n  echo \"\"\n  echo \"Available autonomy worktrees:\"\n  if [ -d \"$repo_root/.worktrees/autonomy\" ]; then\n    ls -1 \"$repo_root/.worktrees/autonomy\"\n  else\n    echo \"  (none)\"\n  fi\n  echo \"\"\n  echo \"To see all worktrees: /list-worktrees\"\n  exit 1\nfi\n```\n\n### Step 4: Safety Checks\n\nCheck for uncommitted changes (unless `--force`):\n\n```bash\nif [ \"$force\" = false ]; then\n  # Change to worktree directory to check status\n  cd \"$worktree_path\"\n\n  # Check for uncommitted changes\n  if ! git diff-index --quiet HEAD -- 2>/dev/null; then\n    echo \"Error: Worktree has uncommitted changes\"\n    echo \"\"\n    echo \"Uncommitted changes in $worktree_path:\"\n    git status --short\n    echo \"\"\n    echo \"Options:\"\n    echo \"  1. Commit changes:\"\n    echo \"     cd $worktree_path\"\n    echo \"     /end-iteration  # or git commit\"\n    echo \"\"\n    echo \"  2. Force removal (discards changes):\"\n    echo \"     /remove-worktree --force $strategy_name\"\n    exit 1\n  fi\n\n  # Check for untracked files (warn but allow)\n  untracked_count=$(git ls-files --others --exclude-standard | wc -l)\n  if [ \"$untracked_count\" -gt 0 ]; then\n    echo \"Warning: Worktree has $untracked_count untracked file(s)\"\n    echo \"\"\n    git ls-files --others --exclude-standard | head -10\n    echo \"\"\n    echo \"Proceeding with removal. Untracked files will be deleted.\"\n    echo \"\"\n  fi\nfi\n```\n\n### Step 5: Remove Worktree\n\nRemove the worktree using git:\n\n```bash\n# Return to repo root for git worktree commands\ncd \"$repo_root\"\n\n# Remove worktree\nif [ \"$force\" = true ]; then\n  git worktree remove --force \".worktrees/autonomy/$strategy_name\"\nelse\n  git worktree remove \".worktrees/autonomy/$strategy_name\"\nfi\n\n# Check if removal succeeded\nif [ $? -ne 0 ]; then\n  echo \"Error: git worktree remove failed\"\n  echo \"\"\n  echo \"Manual removal command:\"\n  echo \"  cd $repo_root\"\n  echo \"  git worktree remove --force .worktrees/autonomy/$strategy_name\"\n  echo \"  git worktree prune\"\n  exit 1\nfi\n```\n\n### Step 6: Cleanup Metadata\n\nPrune stale worktree metadata:\n\n```bash\n# Clean up any stale administrative files\ngit worktree prune\n\n# Verify worktree removed from git's list\nif git worktree list | grep -q \".worktrees/autonomy/$strategy_name\"; then\n  echo \"Warning: Worktree still appears in git worktree list\"\n  echo \"Manual cleanup may be needed:\"\n  echo \"  git worktree prune --verbose\"\nfi\n```\n\n### Step 7: Report Success\n\nAnnounce successful removal with important notes:\n\n```markdown\nâœ“ Worktree removed successfully\n\nRemoved: .worktrees/autonomy/<strategy-name>/\n\nBranch preserved:\n- Branch: autonomy/<strategy-name>\n- All commits and iteration tags remain in git history\n- Can checkout branch later: git checkout autonomy/<strategy-name>\n- Can create new worktree: /fork-worktree <strategy-name>  # (will fail if branch exists, use different name or delete branch first)\n\nTo delete the branch entirely:\n  git branch -d autonomy/<strategy-name>  # Safe delete (only if merged)\n  git branch -D autonomy/<strategy-name>  # Force delete\n```\n\n**If force was used:**\n```markdown\nâš ï¸  Force removal completed\n\nAny uncommitted changes in the worktree were discarded.\n```\n\n## Important Notes\n\n### Destructive vs Non-Destructive\n\n**What gets destroyed (non-recoverable):**\n- Worktree working directory (`.worktrees/autonomy/<strategy-name>/`)\n- Any uncommitted changes in that directory\n- Any untracked files in that directory\n\n**What persists (safe):**\n- Branch `autonomy/<strategy-name>` (can still git checkout)\n- All committed iteration journals\n- All iteration tags (`autonomy/<strategy-name>/iteration-NNNN`)\n- All git history and commits\n\n### Branch Still Exists\n\nAfter worktree removal:\n- Branch is \"unlocked\" (can be checked out elsewhere)\n- All commits remain in git history\n- Can checkout in main repo: `git checkout autonomy/<strategy-name>`\n- Can create new worktree for same branch (but this skill creates new branch, not for existing)\n- To work with existing branch in worktree: manual git command needed\n\n### Force Flag Behavior\n\n**Without `--force`:**\n- Checks for uncommitted changes (fails if found)\n- Warns about untracked files (but proceeds)\n- Safe for preserving work\n\n**With `--force`:**\n- Skips all safety checks\n- Discards uncommitted changes\n- Deletes untracked files\n- Use when worktree is broken or changes are intentional throwaways\n\n### Cannot Remove Current Worktree\n\nGit prevents removing the worktree you're currently in:\n\n```bash\n# This fails:\ncd .worktrees/autonomy/experiment-a\n/remove-worktree experiment-a\n# Error: Cannot remove current working tree\n```\n\n**Solution:** Navigate to different directory first:\n```bash\ncd \"$repo_root\"  # Or any other directory\n/remove-worktree experiment-a\n```\n\n### Locked Worktrees\n\nIf worktree was locked (via `git worktree lock`):\n- Removal fails with error\n- Must unlock first: `git worktree unlock .worktrees/autonomy/<strategy-name>`\n- Then retry removal\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"This deletes the branch\" | NO. Only removes worktree directory. Branch persists. |\n| \"This deletes iteration journals\" | NO. Journals are committed. They persist in git history. |\n| \"I can remove worktree I'm currently in\" | NO. Must navigate away first. |\n| \"Force is always safe\" | NO. Force discards uncommitted work. Use carefully. |\n| \"After removal, branch is gone\" | NO. Branch still exists. Can checkout or delete separately. |\n| \"I'll also clean up the branch\" | NO. Worktree removal is separate from branch deletion. |\n\n## After Removing Worktree\n\nOnce worktree is removed:\n- Directory `.worktrees/autonomy/<strategy-name>/` no longer exists\n- Branch `autonomy/<strategy-name>` still exists\n- All iteration journals in git history\n- All tags preserved\n- Branch can be checked out in main repo or new worktree created for it\n- Disk space freed\n\n## Manual Recovery\n\nIf automated removal fails or worktree is in broken state:\n\n```bash\n# Manual removal commands\ncd <repo-root>\n\n# Force remove worktree\ngit worktree remove --force .worktrees/autonomy/<strategy-name>\n\n# If that fails, manually delete directory then prune\nrm -rf .worktrees/autonomy/<strategy-name>\ngit worktree prune\n\n# Verify cleanup\ngit worktree list\n```\n\n## Relationship to Branch Deletion\n\n**Worktree removal â‰  Branch deletion**\n\nAfter removing worktree:\n```bash\n# Branch still exists\ngit branch -a | grep autonomy/<strategy-name>\n# autonomy/<strategy-name>\n\n# To also delete branch:\ngit branch -D autonomy/<strategy-name>\n\n# This removes branch and all its commits from local repo\n# (Tags remain unless explicitly deleted)\n```\n\nUse worktree removal for cleanup, branch deletion for ending exploration.\n",
        "plugins/autonomy/skills/reviewing-progress/SKILL.md": "---\nname: reviewing-progress\ndescription: Use when user wants to assess progress toward an open-ended goal by reading and summarizing all iteration journals\n---\n\n# Reviewing Progress\n\n## Overview\n\nAssess progress toward an open-ended goal by reading iteration journals, summarizing achievements, identifying patterns, and suggesting next steps.\n\n**Core principle:** Regular review reveals trends, validates strategy, and informs future direction.\n\n## When to Use\n\nUse this skill when:\n- User runs `/review-progress` command\n- User asks \"how are we doing?\" or \"what's our progress?\"\n- Need to assess if current strategy is working\n- Deciding whether to pivot approach\n- Preparing report on goal status\n\n**Can be used:**\n- Mid-iteration (doesn't end current iteration)\n- Standalone (no active iteration required)\n- Anytime user wants status check\n\n## Quick Reference\n\n| Step | Action | Tool/Agent |\n|------|--------|------------|\n| 1. Locate goal | Find autonomy directory | Glob |\n| 2. Load summary | Read summary.md if exists | Read |\n| 3. Load recent iterations | Get last 3-5 iterations | Task (journal-reader) |\n| 4. Analyze progress | Identify trends, metrics, patterns | Manual analysis |\n| 5. Present report | Structured progress summary | Direct output |\n\n## Process\n\n### Step 1: Locate Goal\n\nFind the goal directory:\n\n```bash\n# Use Glob to find goal\npattern: \"autonomy/*/goal.md\"\n```\n\n**If multiple goals found:**\n- List goals and ask user which to review\n- If only one goal, proceed with that one\n\n**If no goal found:**\n```\n\"No autonomy goal found in this project. Use `/start-iteration` to begin tracking an open-ended goal.\"\n```\n\n### Step 2: Load Summary (if exists)\n\nCheck for summary.md:\n\n```bash\n# Use Read to load summary\nfile: \"autonomy/[goal-name]/summary.md\"\n```\n\n**If summary exists:**\n- Read full content\n- Note which iterations it covers\n- Use as foundation for report\n\n**If summary doesn't exist:**\n- Will rely on reading iterations directly\n\n### Step 3: Load Recent Iterations\n\nDispatch journal-reader to load recent context:\n\n```\nTask tool with subagent_type: \"autonomy:journal-reader\"\nPrompt: \"Read all iteration files for goal '[goal-name]' (or last 5-10 if many exist).\n        Extract:\n        - Progress timeline\n        - Completed work by iteration\n        - Persistent blockers\n        - Metric trends\n        - Skills most frequently used\n        - Strategic pivots\"\nModel: haiku\n```\n\nWait for agent response with structured findings.\n\n### Step 4: Analyze Progress\n\nSynthesize information from summary and journal-reader:\n\n**Calculate metrics:**\n- If goal tracks metrics: Starting value â†’ Current value â†’ % change\n- Iteration count: How many iterations completed?\n- Time elapsed: First iteration date â†’ Latest iteration date\n\n**Identify patterns:**\n- Which types of work have been most common?\n- Are there recurring blockers?\n- Has strategy evolved or stayed consistent?\n- Which skills/workflows used most frequently?\n\n**Assess trajectory:**\n- Is progress accelerating, steady, or slowing?\n- Are recent iterations more or less productive?\n- Is current approach working?\n\n**Flag concerns:**\n- Blockers appearing in multiple iterations (persistent issues)\n- Metrics stagnating or declining\n- Lack of clear recent progress\n- Strategy thrashing (frequent pivots without validation)\n\n### Step 5: Present Report\n\nGenerate comprehensive progress report:\n\n```markdown\n# Progress Report: [Goal Name]\n\n**Report Date:** [Today's date]\n**Goal Status:** Active\n**Iterations Completed:** [N]\n**Time Elapsed:** [First date] - [Latest date] ([X days/weeks/months])\n\n---\n\n## Goal Statement\n[From goal.md]\n\n---\n\n## Progress Overview\n\n### Key Metrics\n| Metric | Starting | Current | Change |\n|--------|----------|---------|--------|\n| [Metric 1] | [Value] | [Value] | [+X% or -Y%] |\n| [Metric 2] | [Value] | [Value] | [+X% or -Y%] |\n\n### Timeline of Major Work\n- **Iteration 1:** [Summary of work]\n- **Iteration 2:** [Summary of work]\n- **Iteration N:** [Summary of work]\n\n### Completed Initiatives\nâœ… [Initiative 1]: [Outcome]\nâœ… [Initiative 2]: [Outcome]\n\n### In Progress\nðŸš§ [Initiative 3]: [Current state]\n\n---\n\n## Current State\n\n### What's Working Well\n- [Positive pattern 1]\n- [Positive pattern 2]\n\n### Current Blockers\n- **[Blocker 1]:** [First appeared iteration X, still unresolved]\n- **[Blocker 2]:** [Description and impact]\n\n### Open Questions\n- [Question 1]\n- [Question 2]\n\n---\n\n## Analysis\n\n### Strategic Evolution\n[How has the approach changed over time?]\n- Iteration 1-3: [Initial strategy]\n- Iteration 4-6: [Pivot or continuation]\n- Current: [Where we are now]\n\n### Skills & Methods Most Used\n- **[Skill/workflow]:** Used in [X] iterations for [purpose]\n- **[Skill/workflow]:** Used in [Y] iterations for [purpose]\n\n### Effectiveness Assessment\n[Is the current approach working?]\n- **Strengths:** [What's effective]\n- **Weaknesses:** [What's not working]\n- **Opportunities:** [What could be explored next]\n\n---\n\n## Recommendations\n\n### Immediate Next Steps\n1. [Specific action based on current state]\n2. [Specific action based on blockers]\n\n### Strategic Considerations\n- [Consider pivot if X]\n- [Double down on Y because Z]\n- [Investigate new approach for A]\n\n### Health Check\n[Overall assessment: Is goal on track? Should strategy change? Is this goal still valuable?]\n\n---\n\n**This is an open-ended goal - continuous iteration and optimization expected.**\n```\n\n## Important Notes\n\n### Mid-Iteration Review\n\nIf reviewing during active iteration:\n- Include current iteration work in report\n- Note that current iteration is in progress\n- Don't write journal entry (that's ending-an-iteration's job)\n\n### Standalone Review\n\nIf no iteration active:\n- Report based solely on journal history\n- Suggest starting new iteration if next steps are clear\n\n### Frequency Recommendations\n\nSuggest reviewing progress:\n- Every 5 iterations (natural checkpoint)\n- When considering strategy pivot\n- Before major decisions\n- If feeling lost or uncertain\n\n### Honesty in Assessment\n\n**Be honest:**\n- If metrics aren't improving, say so\n- If approach isn't working, acknowledge it\n- If goal seems stalled, flag it\n- Don't sugarcoat to avoid disappointing user\n\n**Constructive:**\n- Always include specific next steps\n- Suggest concrete changes if needed\n- Identify what IS working to preserve\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"Progress looks good\" without data | NO. Use specific metrics and evidence. |\n| \"I'll review just the last iteration\" | NO. Look at full history for patterns. |\n| \"No need to flag concerns\" | NO. Honest assessment helps course-correct. |\n| \"I'll skip analysis and just list work done\" | NO. Synthesis and insights are the value. |\n| \"Goal is open-ended so no progress metrics\" | NO. Even open goals have measurable indicators. |\n\n## After Reviewing\n\nOnce review is complete:\n- User has clear picture of progress\n- Can decide: continue current path, pivot strategy, or pause goal\n- Next iteration can incorporate review insights\n- May identify need to update goal.md with new success criteria\n",
        "plugins/autonomy/skills/slime-strategy/SKILL.md": "---\nname: slime-strategy\ndescription: Use when user wants to set up slime mold exploration strategy with parallel autonomy branches for genetic algorithm approach to problem-solving\n---\n\n# Slime Mold Strategy Setup\n\n## Overview\n\nSet up the complete \"slime mold strategy\" workflow for exploring problem spaces through parallel autonomy branches that cooperate like a genetic algorithm.\n\n**Core principle:** Initialize autonomy workflow with slime mold philosophy: multiple parallel branches exploring different approaches, cooperating not competing, cross-pollinating insights.\n\n## When to Use\n\nUse this skill when:\n- User runs `/slime` command\n- User wants to adopt slime mold strategy for exploration\n- Setting up parallel branch exploration with genetic algorithm approach\n\n**DO NOT use for:**\n- Adding single goal without branch strategy (use `/create-goal`)\n- Creating additional branches in existing workflow (use `/fork-iteration`)\n- Analyzing existing branches (use `/branch-status` or `/compare-branches`)\n\n## Quick Reference\n\n| Step | Action | Tool |\n|------|--------|------|\n| 1. Check state | Look for existing autonomy goal | Glob |\n| 2a. Create goal | Invoke creating-a-goal skill if needed | Skill |\n| 2b. Update only | Skip to CLAUDE.md if goal exists | - |\n| 3. Update CLAUDE.md | Replace slime mold section or create file | Read, Write |\n| 4. Create branch | Invoke forking-iteration with goal name | Skill |\n| 5. Initialize iteration | Create iteration-0000 journal file | Write |\n| 6. Git commit | Commit with enhanced format and tag | Bash |\n\n## Process\n\n### Step 1: Check Existing State\n\nFirst, determine if autonomy workflow already exists:\n\n```bash\n# Look for any existing goal.md files\nexisting_goals=$(find autonomy -name \"goal.md\" 2>/dev/null)\n\nif [ -n \"$existing_goals\" ]; then\n  mode=\"update-only\"\n  echo \"Autonomy workflow already exists. Will update CLAUDE.md only.\"\n\n  # Extract goal name from first goal found\n  goal_dir=$(dirname \"$existing_goals\" | head -1)\n  goal_name=$(basename \"$goal_dir\")\nelse\n  mode=\"full-setup\"\n  echo \"No existing autonomy goal found. Will perform full setup.\"\nfi\n```\n\n**Validation:**\n- If multiple goals found, use first one alphabetically\n- Warn user: \"Multiple goals detected, using [goal-name]\"\n\n### Step 2: Create Goal (full-setup mode only)\n\nIf `mode=\"full-setup\"`, invoke the creating-a-goal skill:\n\n```bash\n# Only run if full-setup mode\nif [ \"$mode\" = \"full-setup\" ]; then\n  # Invoke creating-a-goal skill using Skill tool\n  skill: \"autonomy:creating-a-goal\"\n\n  # After skill completes, extract goal name\n  goal_dir=$(find autonomy -name \"goal.md\" -exec dirname {} \\; | head -1)\n  goal_name=$(basename \"$goal_dir\")\n\n  echo \"Goal created: $goal_name\"\nfi\n```\n\n**Error Handling:**\n- If creating-a-goal fails or user cancels\n- Abort entire workflow\n- Show: \"Goal creation cancelled. Slime mold setup aborted.\"\n- Don't proceed to CLAUDE.md or iteration 0000\n\n### Step 3: Create/Update autonomy/CLAUDE.md\n\nEnsure `autonomy/CLAUDE.md` exists and contains current slime mold strategy documentation.\n\n**CLAUDE.md Content (exact text to use):**\n\n```markdown\n# Slime Mold Strategy\n\nThis project uses the \"slime mold strategy\" for exploring the problem space. Like a slime mold organism that extends multiple tendrils to find optimal paths, we maintain parallel autonomy branches that explore different approaches to the same goal. These branches represent a genetic algorithm: each branch is an independent experiment testing different hypotheses, strategies, or implementations.\n\nCrucially, these branches are NOT competingâ€”they are cooperating. All branches are part of the same organism pursuing the same goal. When branches encounter each other or when you discover useful insights in one branch, use `/analyze-branch <branch-name> <search-description>` to extract \"genetic material\" (learnings, patterns, solutions) and incorporate them into your current branch. Use `/compare-branches <branch-a> <branch-b>` to understand how different approaches diverged and what outcomes each achieved. Use `/list-branches` to inventory all active exploration tendrils. When ready to fork a new experimental direction, use `/fork-iteration [iteration] <strategy-name>` to create a new tendril from any point in history.\n\nThe goal is not to find the single \"best\" branch, but to explore the solution space thoroughly and cross-pollinate insights across branches, allowing the organism as a whole to discover optimal solutions through parallel exploration and cooperation.\n```\n\n**Implementation:**\n\n```bash\n# Check if autonomy/CLAUDE.md exists\nif [ -f \"autonomy/CLAUDE.md\" ]; then\n  # Read existing content\n  existing_content=$(cat autonomy/CLAUDE.md)\n\n  # Check if slime mold section exists\n  if echo \"$existing_content\" | grep -q \"# Slime Mold Strategy\"; then\n    echo \"Updating existing slime mold section in CLAUDE.md\"\n\n    # Strategy: Replace section from \"# Slime Mold Strategy\" to next \"# \" heading or EOF\n    # Use Read tool to get current content\n    # Use Edit tool to replace the section\n\n    # Read current file\n    # Find start of \"# Slime Mold Strategy\" section\n    # Find end (next # heading or EOF)\n    # Replace that section with new content\n  else\n    echo \"Appending slime mold section to existing CLAUDE.md\"\n\n    # Append new section to end of file\n    # Use Edit tool to add content at end\n  fi\nelse\n  echo \"Creating new autonomy/CLAUDE.md with slime mold section\"\n\n  # Create new file with slime mold section\n  # Use Write tool to create file\nfi\n```\n\n**Write/Edit the slime mold section:**\n\nUse Write tool if creating new file, or Edit tool if updating existing file.\n\nThe section content is the markdown block shown above under \"CLAUDE.md Content\".\n\n### Step 4: Create Initial Branch (full-setup mode only)\n\nIf `mode=\"full-setup\"`, create the initial autonomy branch:\n\n```bash\n# Only run if full-setup mode\nif [ \"$mode\" = \"full-setup\" ]; then\n  # Invoke forking-iteration skill with goal-name as strategy-name\n  # No iteration number specified = fork from current HEAD\n\n  skill: \"autonomy:forking-iteration\"\n  args: \"$goal_name\"\n\n  # This creates branch: autonomy/[goal-name]\n  # And checks out that branch\n\n  echo \"Created and switched to branch: autonomy/$goal_name\"\nfi\n```\n\n**Error Handling:**\n- If forking-iteration fails (e.g., branch already exists)\n- Show error from skill\n- Don't create iteration 0000\n- Suggest: \"Use /fork-iteration <different-name> to create branch with different name\"\n\n### Step 5: Create Iteration 0000 (full-setup mode only)\n\nIf `mode=\"full-setup\"`, create the baseline setup iteration:\n\n```bash\n# Only run if full-setup mode\nif [ \"$mode\" = \"full-setup\" ]; then\n  # Get current date in YYYY-MM-DD format\n  current_date=$(date +%Y-%m-%d)\n\n  # Create iteration-0000 file\n  iteration_file=\"autonomy/$goal_name/iteration-0000-$current_date.md\"\n\n  # Use Write tool to create file with this content:\nfi\n```\n\n**Iteration 0000 Content:**\n\n```markdown\n# Iteration 0000 - YYYY-MM-DD\n\n## Beginning State\n\nNo previous iterations. This is the initial setup for the slime mold exploration strategy.\n\n**Baseline Metrics:**\n[Extract from goal.md metrics section, or \"None established yet\" if no metrics defined]\n\n## Iteration Intention\n\nEstablish the foundation for slime mold strategy exploration:\n- Define goal and success criteria\n- Create initial autonomy branch\n- Document exploration approach\n- Prepare for iteration 0001 to begin actual work\n\n## Work Performed\n\n### Setup Completed\n- Created goal: [Extract goal statement from goal.md]\n- Established autonomy/CLAUDE.md with slime mold strategy documentation\n- Created initial branch: autonomy/[goal-name]\n- Initialized iteration tracking system\n\n### Slime Mold Strategy Adopted\n- Multiple parallel branches will explore different approaches\n- Branches cooperate via /analyze-branch for cross-pollination\n- Use /fork-iteration to create new exploration tendrils\n- Use /compare-branches to understand divergent paths\n\n## Ending State\n\nAutonomy workflow initialized. Ready to begin iteration 0001 with actual exploration work. Use `/start-iteration` to begin.\n\n**Recommended next action:** Run `/start-iteration` to begin first real iteration of exploration.\n```\n\n**Implementation:**\n\nRead `autonomy/[goal-name]/goal.md` to extract:\n- Goal statement (from first paragraph or \"## Goal\" section)\n- Metrics (from \"## Metrics\" or \"Success Criteria\" section)\n\nUse Write tool to create `iteration-0000-YYYY-MM-DD.md` with content above, substituting:\n- `YYYY-MM-DD` with current date\n- `[goal-name]` with actual goal name\n- Goal statement and metrics from goal.md\n\n### Step 6: Git Commit and Tag (full-setup mode only)\n\nIf `mode=\"full-setup\"`, commit iteration 0000 with enhanced format:\n\n```bash\n# Only run if full-setup mode\nif [ \"$mode\" = \"full-setup\" ]; then\n  # Stage the iteration file and CLAUDE.md\n  git add \"autonomy/$goal_name/iteration-0000-$current_date.md\"\n  git add \"autonomy/CLAUDE.md\"\n\n  # Create enhanced commit message\n  git commit -m \"$(cat <<'EOF'\njournal: $goal_name iteration 0000\n\nEstablished slime mold exploration strategy with initial autonomy branch.\n\n## Journal Summary\n\nCreated goal definition for $goal_name, initialized slime mold strategy documentation in CLAUDE.md, created initial autonomy/$goal_name branch, and established iteration 0000 as baseline. Workflow ready for iteration 0001 to begin actual exploration work.\n\n## Iteration Metadata\n\nStatus: active\nMetrics: None (baseline setup)\nBlockers: None\nNext: Run /start-iteration to begin iteration 0001\n\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\nEOF\n)\"\n\n  # Create branch-aware tag\n  git tag -a \"autonomy/$goal_name/iteration-0000\" \\\n    -m \"journal: $goal_name iteration 0000\"\n\n  echo \"Created git commit and tag: autonomy/$goal_name/iteration-0000\"\nfi\n```\n\n**Commit Message Format:**\n- First line: `journal: [goal-name] iteration 0000`\n- Blank line\n- 2-3 line summary\n- `## Journal Summary` section (4-6 sentences)\n- `## Iteration Metadata` section (Status, Metrics, Blockers, Next)\n- Claude Code attribution\n\n**Tag Format:**\n- `autonomy/[goal-name]/iteration-0000`\n- Matches branch-aware tagging from ending-an-iteration skill\n\n### Step 7: Final Messages\n\nDisplay completion message based on mode:\n\n**Full-setup mode:**\n```\nSlime mold strategy initialized successfully!\n\nâœ“ Goal created: [goal-name]\nâœ“ CLAUDE.md documented slime mold strategy\nâœ“ Branch created: autonomy/[goal-name]\nâœ“ Iteration 0000 established as baseline\nâœ“ Git commit and tag: autonomy/[goal-name]/iteration-0000\n\nNext steps:\n1. Run /start-iteration to begin iteration 0001\n2. Use /fork-iteration <strategy-name> to create additional exploration branches\n3. Use /analyze-branch to cross-pollinate insights between branches\n4. Use /compare-branches to understand divergent approaches\n```\n\n**Update-only mode:**\n```\nSlime mold strategy documentation updated!\n\nâœ“ CLAUDE.md updated with current slime mold strategy description\n\nYour autonomy workflow is ready. Use these commands:\n- /fork-iteration <strategy-name> - Create new exploration branch\n- /list-branches - Inventory all autonomy branches\n- /compare-branches <a> <b> - Compare two exploration paths\n- /analyze-branch <branch> <search> - Extract insights from another branch\n```\n\n## Important Notes\n\n### Git Repository Required\n\nThis skill requires a git repository:\n- Check with `git rev-parse --git-dir` before starting\n- If not in git repo, show error: \"This command requires a git repository. Run 'git init' first.\"\n- Abort workflow if git not available\n\n### Idempotent Behavior\n\nThe skill is designed to be idempotent:\n- Safe to run multiple times\n- If goal exists: Only updates CLAUDE.md (doesn't recreate goal or iteration 0000)\n- CLAUDE.md section replacement ensures documentation stays current\n- Use this to update slime mold description as concept evolves\n\n### CLAUDE.md Location\n\n- File created at `autonomy/CLAUDE.md` (working directory, not plugin directory)\n- At same level as `autonomy/[goal-name]/` directories\n- Provides project-level context for slime mold strategy\n\n### Iteration 0000 is Special\n\n- Iteration 0000 is baseline setup only\n- First real work iteration is 0001 (via `/start-iteration`)\n- Don't use `starting-an-iteration` skill for iteration 0000\n- Manually create the file with setup-specific content\n\n### Branch Name = Goal Name\n\n- Initial branch always named `autonomy/[goal-name]`\n- Matches goal directory name for consistency\n- Additional branches via `/fork-iteration` can use different strategy names\n- Example: goal \"improve-performance\" â†’ branch \"autonomy/improve-performance\"\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll use starting-an-iteration for iteration 0000\" | NO. Manually create iteration 0000 with setup-specific content. |\n| \"I'll skip CLAUDE.md in update-only mode\" | NO. Always update CLAUDE.md to keep description current. |\n| \"I'll append to CLAUDE.md even if section exists\" | NO. Replace existing slime mold section to keep it current. |\n| \"I'll create iteration 0001 after 0000\" | NO. User runs /start-iteration for 0001. This skill only creates 0000. |\n| \"I'll use different branch name than goal name\" | NO. Initial branch must be autonomy/[goal-name]. Use /fork-iteration for other names. |\n| \"I'll skip git commit if user doesn't want it\" | NO. Git integration is core to autonomy workflow. Always commit. |\n\n## After Setup\n\nOnce `/slime` completes:\n- User should run `/start-iteration` to begin iteration 0001\n- User can run `/fork-iteration <strategy-name>` to create additional branches\n- User can use `/analyze-branch` for cross-pollination\n- User can use `/compare-branches` to understand divergence\n\nThe slime mold strategy is now active and documented in `autonomy/CLAUDE.md`.\n",
        "plugins/autonomy/skills/starting-an-iteration/SKILL.md": "---\nname: starting-an-iteration\ndescription: Use when beginning a new conversation to work on an open-ended goal, loading context from previous iterations through iteration journals\n---\n\n# Starting an Iteration\n\n## Overview\n\nBegin a new iteration for an open-ended goal by loading context from previous iterations, setting up the workspace, and preparing to continue progress.\n\n**Core principle:** Each conversation is one iteration. Load recent state, understand where you left off, continue the journey.\n\n## When to Use\n\nUse this skill when:\n- User runs `/start-iteration` command\n- Beginning work on an ongoing open-ended goal\n- Need to load context from previous conversation sessions\n\n**DO NOT use for:**\n- Closed goals with definition of done (use ed3d-superpowers workflow instead)\n- First-time brainstorming (use brainstorming skill first)\n- One-off tasks that don't need iteration tracking\n\n## Quick Reference\n\n| Step | Action | Tool/Agent |\n|------|--------|------------|\n| 1. Detect goal | Check for autonomy directory | Glob |\n| 2. Load recent context | Read last 3-5 iterations | Task (journal-reader) |\n| 3. Load older context | Summarize if >5 iterations | Task (journal-summarizer) |\n| 4. Present state | Show context to user | Direct output |\n| 5. Create journal | Write initial journal entry | Write |\n| 6. Ready to work | Create TodoWrite tracker | TodoWrite |\n\n## Process\n\n### Step 1: Detect Existing Goal\n\nCheck if a goal already exists in the current project:\n\n```bash\n# Use Glob to check for autonomy directory\npattern: \"autonomy/*/goal.md\"\n```\n\n**If goal.md found:**\n- Extract goal name from directory path\n- Proceed to Step 2 (Load Recent Context)\n\n**If no goal found:**\n```\n\"No autonomy goal found in this project.\n\nUse `/create-goal` to set up a new open-ended goal first, then run `/start-iteration` to begin work.\"\n```\nStop here - cannot start iteration without a goal.\n\n### Step 2: Load Recent Context\n\nFor existing goals, load context from recent iterations:\n\n1. **Count iterations:**\n   ```bash\n   # Use Glob to find iteration files\n   pattern: \"autonomy/[goal-name]/iteration-*.md\"\n   ```\n\n2. **Determine iteration number:**\n   - Count existing files: N\n   - This iteration will be: N+1\n\n3. **Dispatch journal-reader agent:**\n   ```\n   Task tool with subagent_type: \"autonomy:journal-reader\"\n   Prompt: \"Read last 3-5 iterations for goal '[goal-name]' and extract:\n           - Current state\n           - Open blockers and questions\n           - Recent progress\n           - Key metrics\n           - Recommended next steps\"\n   Model: haiku\n   ```\n\n4. **Wait for agent response** with structured context\n\n### Step 3: Load Older Context (if >5 iterations)\n\nIf more than 5 iterations exist, summarize older ones:\n\n1. **Check if summary.md exists:**\n   ```bash\n   # Use Read to check\n   file: \"autonomy/[goal-name]/summary.md\"\n   ```\n\n2. **Determine what needs summarizing:**\n   - If summary.md exists: Check which iterations it covers\n   - If summary.md missing OR outdated: Summarize iterations 1 through (N-5)\n\n3. **Dispatch journal-summarizer agent:**\n   ```\n   Task tool with subagent_type: \"autonomy:journal-summarizer\"\n   Prompt: \"Summarize iterations [range] for goal '[goal-name]'.\n           Extract major initiatives, persistent blockers, key learnings,\n           strategic pivots, and metric trends. Update summary.md.\"\n   Model: haiku\n   ```\n\n4. **Wait for agent to write summary.md**\n\n### Step 4: Present State to User\n\nCombine findings from journal-reader (and summary if applicable):\n\n```markdown\n**Iteration [N] started for goal: [goal-name]**\n\n## Current State\n[From journal-reader: most recent ending state]\n\n## Recent Progress (Last 3-5 Iterations)\n[Condensed summary of what was accomplished]\n\n## Open Blockers\n- [Blocker 1]\n- [Blocker 2]\n\n## Open Questions\n- [Question 1]\n- [Question 2]\n\n## Key Metrics\n- [Metric]: [Current value] (was [previous value])\n\n## Recommended Next Steps\n[From previous iteration's suggestions]\n\n---\n\n**Ready to continue. What should we work on this iteration?**\n```\n\n### Step 5: Create Initial Journal Entry\n\nBefore beginning work, create the journal file for this iteration:\n\n1. **Determine iteration number and filename:**\n   - Count: N existing iterations\n   - This iteration: N+1\n   - Filename: `iteration-NNNN-YYYY-MM-DD.md` (today's date)\n\n2. **Prompt user for iteration intention:**\n   ```\n   \"What do you want to accomplish this iteration?\n\n   This helps keep us honest about the goal. We'll check against this intention when ending the iteration.\"\n   ```\n\n3. **Write initial journal entry:**\n\n```markdown\n# Iteration NNNN - YYYY-MM-DD\n\n## Beginning State\n[From journal-reader output or user context:\n- Current progress summary\n- Known blockers from previous iteration\n- Open questions being addressed\n- Current metric values if tracked]\n\n## Iteration Intention\n[User's stated intention for this iteration]\n\n## Work Performed\n\n### Skills & Workflows Used\n[Will be filled during checkpoint or ending]\n\n### Key Decisions Made\n[Will be filled during checkpoint or ending]\n\n### Artifacts Created/Modified\n[Will be filled during checkpoint or ending]\n\n### External Context Gathered\n[Will be filled during checkpoint or ending]\n\n### Reasoning & Strategy Changes\n[Will be filled during checkpoint or ending]\n\n### Blockers Encountered\n[Will be filled during checkpoint or ending]\n\n### Open Questions\n[Will be filled during checkpoint or ending]\n\n## Ending State\n[Will be filled when ending iteration]\n\n## Iteration Metadata\n[Will be filled when ending iteration]\n```\n\n4. **Write file** to: `autonomy/[goal-name]/iteration-NNNN-YYYY-MM-DD.md`\n\n### Step 6: Ready to Work\n\n1. **Create TodoWrite tracker** for current iteration work:\n   ```\n   TodoWrite with todos:\n   - [First task based on next steps or user intention]\n   - [Additional tasks as identified]\n   ```\n\n2. **Begin work** on the goal\n\n## Important Notes\n\n### Iteration Numbering\n\n- Use 4-digit zero-padded numbers: `0001`, `0002`, ..., `0999`, `1000`\n- Format: `iteration-NNNN-YYYY-MM-DD.md`\n- Date is when iteration occurred (today)\n\n### Context Loading Strategy\n\n- **Iterations 1-5:** Load all in detail with journal-reader\n- **Iterations 6+:** Load last 3-5 in detail, older iterations via summary\n- **Update summary** every 5 iterations (5, 10, 15, 20, etc.)\n\n### Agent Delegation\n\n**Always use Task tool** to dispatch journal-reader and journal-summarizer:\n- Prevents context pollution in main conversation\n- Uses Haiku model for efficiency\n- Returns structured findings\n\n**Never read journals yourself** - always delegate to agents.\n\n## Common Mistakes\n\n| Mistake | Reality |\n|---------|---------|\n| \"I'll quickly read the journals myself\" | NO. Always dispatch agents. Journals can be large. |\n| \"User goal seems closed-ended, but I'll use autonomy anyway\" | NO. Autonomy is for open-ended goals only. |\n| \"I'll skip loading older iterations to save time\" | NO. Context is critical. Always load per strategy. |\n| \"I'll create summary.md myself\" | NO. Dispatch journal-summarizer agent. |\n| \"Goal exists, so I'll skip asking user what to work on\" | NO. Always present state and ask for direction. |\n\n## After Starting\n\nOnce iteration is started:\n- Journal file created with Beginning State and Intention\n- Work normally on the goal using appropriate skills and workflows\n- Track progress mentally or with TodoWrite\n- Use `/checkpoint-iteration` to save progress before compaction or at interim points\n- When ready to conclude, use `/end-iteration` to finalize journal entry\n",
        "plugins/datapeeker/.claude-plugin/plugin.json": "{\n  \"name\": \"datapeeker\",\n  \"description\": \"Structured research methods for AI agents - hypothesis testing, exploratory analysis, comparative analysis, and qualitative research\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Tilmon Engineering\",\n    \"email\": \"team@tilmonengineering.com\"\n  },\n  \"homepage\": \"https://github.com/tilmon-engineering/tilmon-eng-skills\",\n  \"repository\": \"https://github.com/tilmon-engineering/tilmon-eng-skills\",\n  \"license\": \"UNLICENSED\",\n  \"keywords\": [\n    \"data-analysis\",\n    \"research-methods\",\n    \"hypothesis-testing\",\n    \"exploratory-analysis\",\n    \"comparative-analysis\",\n    \"qualitative-research\",\n    \"market-research\"\n  ]\n}\n",
        "plugins/datapeeker/README.md": "# DataPeeker Plugin\n\nStructured research methods for AI agents conducting rigorous, reproducible data analysis and qualitative research.\n\n## Overview\n\nDataPeeker provides a comprehensive toolkit for systematic research, emphasizing intellectual honesty, reproducibility, and evidence-based conclusions. It follows a two-tier architecture combining component skills (specialized tasks) with process skills (workflow orchestration).\n\n## Skills Included\n\n### Process Skills (Workflow Orchestration)\n\nThese skills guide complete analytical investigations from start to finish:\n\n**`exploratory-analysis`** - Discover patterns in unfamiliar data\n- When to use: Initial investigation with no specific hypothesis\n- Phases: Understanding â†’ Profiling â†’ Pattern discovery â†’ Synthesis\n- Output: Insights, anomalies, and questions for deeper investigation\n\n**`hypothesis-testing`** - Rigorously test specific hypotheses\n- When to use: Validating specific claims or theories\n- Phases: Hypothesis formulation â†’ Test design â†’ Analysis â†’ Interpretation\n- Output: Statistical evidence supporting or contradicting hypothesis\n\n**`comparative-analysis`** - Compare segments, cohorts, or time periods\n- When to use: Understanding differences between groups\n- Phases: Definition â†’ Measurement â†’ Comparison â†’ Explanation\n- Output: Fair comparisons with context for differences\n\n**`guided-investigation`** - Investigate open-ended business questions\n- When to use: Answering \"why did X happen?\" or \"what's driving Y?\"\n- Phases: Question decomposition â†’ Mapping â†’ Incremental investigation â†’ Synthesis\n- Output: Evidence-based answers to complex questions\n\n**`marketing-experimentation`** - Validate marketing concepts through experimental cycles\n- When to use: Testing marketing ideas, business concepts, campaign strategies\n- Phases: Discovery â†’ Hypothesis generation â†’ Prioritization â†’ Coordination â†’ Synthesis â†’ Iteration\n- Output: Validated concepts with evidence, iteration plans for next cycle\n\n**`qualitative-research`** - Conduct rigorous qualitative research\n- When to use: Customer discovery, user research, surveys, focus groups, observations\n- Phases: Design â†’ Data collection â†’ Familiarization â†’ Coding â†’ Themes â†’ Reporting\n- Output: Thematic analysis with bias prevention and intercoder reliability\n\n### Component Skills (Specialized Tasks)\n\n#### Data Preparation\n\n**`importing-data`** - Transform CSV files into SQLite with proper schema\n- MANDATORY first step before any analysis\n- Output: `raw_*` table + quality report\n\n**`cleaning-data`** - Address data quality issues\n- MANDATORY after importing-data\n- Output: `clean_*` table + verification report\n\n#### Analysis Support\n\n**`understanding-data`** - Systematic data profiling and exploration\n- Used by process skills to examine datasets\n- Capabilities: Schema inspection, distributions, relationships\n\n**`detect-foreign-keys`** - Identify foreign key relationships between tables\n- Useful for understanding multi-table databases\n- Validates referential integrity, finds orphaned records\n\n**`writing-queries`** - Construct correct, efficient SQL queries\n- Translates analytical questions to SQL\n- Includes optimization patterns and common pitfalls\n\n**`interpreting-results`** - Analyze query results with intellectual honesty\n- CRITICAL for avoiding false conclusions\n- Assesses confidence, identifies biases, considers alternatives\n\n**`creating-visualizations`** - Generate effective visualizations\n- Terminal-based and image-based formats\n- Mermaid diagrams, Vega-Lite charts, ASCII art\n\n**`presenting-data`** - Create data-driven presentations and whitepapers\n- Uses marp and pandoc for professional output\n- Includes citations and reproducibility documentation\n\n**`using-sqlite`** - Task-oriented SQLite CLI guidance\n- Querying, importing, schema exploration, optimization\n- Essential for data analysis workflows\n\n## Agents Included\n\nDataPeeker uses specialized Haiku sub-agents to prevent context pollution during data operations:\n\n### Data Quality Agents\n\n**`quality-assessment`** - Comprehensive quality profiling\n- Detects NULL percentages, duplicates, outliers\n- Identifies free text columns needing categorization\n\n**`detect-exact-duplicates`** - Find identical records\n- Returns duplicate groups with counts and examples\n\n**`detect-near-duplicates`** - Fuzzy matching for similar records\n- Uses fuzzy string matching on key text columns\n\n**`detect-outliers`** - Statistical outlier detection\n- MAD (Median Absolute Deviation) with 3 MAD threshold\n\n**`categorize-free-text`** - Semantic grouping of free text values\n- Proposes categorical groupings with complete mappings\n\n**`detect-foreign-keys`** - Identify foreign key relationships\n- Heuristics, value overlap analysis, referential integrity checks\n\n### Qualitative Research Agents\n\n**`analyze-transcript`** - Extract structured data from interview/focus group transcripts\n- Identifies speakers, codes responses, maintains context\n\n**`generate-initial-codes`** - Generate initial codes from qualitative data\n- Inductive coding from interview transcripts, observations\n\n**`identify-themes`** - Identify themes from coded qualitative data\n- Pattern recognition across codes, thematic grouping\n\n**`extract-supporting-quotes`** - Extract supporting quotes for themes\n- Representative quotes demonstrating each theme\n\n**`intercoder-reliability-check`** - Verify intercoder reliability\n- Prevents single-coder bias, ensures reproducibility\n\n**`search-disconfirming-evidence`** - Find evidence contradicting current conclusions\n- MANDATORY for preventing confirmation bias\n\n### Market Research Agent\n\n**`market-researcher`** - Validate marketing concepts via internet research\n- Market demand signals, similar solutions, audience needs\n- Validation evidence for marketing experimentation\n\n## Getting Started\n\n### Prerequisites\n\nDataPeeker requires:\n- SQLite 3 for data storage\n- Python 3 for analysis scripts\n- Claude Code with plugin support\n\n### Basic Workflow\n\n```bash\n# 1. Import your data\nUse importing-data skill with CSV file path\n\n# 2. Clean your data (MANDATORY)\nUse cleaning-data skill with raw_* table\n\n# 3. Choose appropriate process skill:\n- exploratory-analysis: Discover patterns\n- hypothesis-testing: Validate claims\n- comparative-analysis: Compare groups\n- guided-investigation: Answer \"why\" questions\n- marketing-experimentation: Test marketing concepts\n- qualitative-research: Analyze interviews/surveys\n```\n\n### Example: Exploratory Analysis\n\n```\n1. Start with CSV data\n2. /importing-data\n   â†’ Creates raw_sales table\n   â†’ Documents quality issues\n\n3. /cleaning-data\n   â†’ Creates clean_sales table\n   â†’ Verifies data quality\n\n4. /exploratory-analysis\n   â†’ Discovers seasonal patterns\n   â†’ Identifies top segments\n   â†’ Suggests hypotheses to test\n```\n\n## Key Principles\n\n1. **Data Quality First** - Never skip cleaning-data, even if data looks clean\n2. **Document Everything** - Every decision captured in numbered markdown files\n3. **Intellectual Honesty** - Interpret results skeptically, acknowledge limitations\n4. **Reproducibility** - Others should be able to follow your exact analytical path\n5. **Context Management** - Use sub-agents for data operations, keep main context analytical\n\n## Documentation\n\nAll process skills create numbered markdown files (01-phase-name.md, 02-phase-name.md, etc.) documenting:\n- Decisions made during analysis\n- Queries executed\n- Results observed\n- Interpretations with confidence levels\n- Next steps and questions\n\nThese files are git-committable and serve as complete audit trail.\n\n## Architecture\n\nDataPeeker follows a two-tier architecture:\n\n**Tier 1: Component Skills**\n- Specialized, focused capabilities\n- Referenced by process skills\n- Can be used independently when needed\n\n**Tier 2: Process Skills**\n- Workflow orchestrators\n- Guide complete investigations\n- Call component skills as needed\n- Ensure systematic methodology\n\n## Advanced Usage\n\n### Custom Workflows\n\nComponent skills can be combined for custom workflows:\n\n```\n1. importing-data â†’ raw_* table\n2. cleaning-data â†’ clean_* table\n3. understanding-data â†’ profile dataset\n4. writing-queries â†’ custom analysis\n5. interpreting-results â†’ draw conclusions\n6. creating-visualizations â†’ communicate findings\n```\n\n### Multi-Table Analysis\n\nWhen working with multiple related tables:\n\n```\n1. Import each table separately\n2. Use detect-foreign-keys to understand relationships\n3. Clean each table\n4. Use process skills that join tables as needed\n```\n\n## Contributing\n\nTo improve DataPeeker skills:\n\n1. Document gaps, ambiguities, or inefficiencies discovered during use\n2. Propose skill updates to maintain quality\n3. Test changes with realistic scenarios\n4. Update skill dependencies after modifications\n\n## Version\n\n**Current version:** 1.0.0\n\n## License\n\nUNLICENSED - Internal use for Tilmon Engineering\n\n## Contact\n\n**Tilmon Engineering**\nEmail: team@tilmonengineering.com\nRepository: https://github.com/tilmon-engineering/tilmon-eng-skills\n",
        "plugins/datapeeker/agents/analyze-transcript.md": "# analyze-transcript Agent\n\n## Purpose\n\nSummarize interview transcripts or field notes, extract key quotes, and identify initial observations to prevent context pollution in the main analysis session.\n\n**Model:** Haiku (fast, efficient, cost-effective)\n\n**Used by:** qualitative-research skill, Phase 3 (Data Familiarization)\n\n## When to Use\n\nUse this agent when:\n- Dataset contains 10+ interview transcripts\n- Transcripts are lengthy (>2000 words each)\n- Main agent context would be polluted by reading all raw data\n- Need structured summaries for systematic review\n\n## Input\n\n**One or more transcript files** from `raw-data/` directory:\n- `transcript-001.md` through `transcript-NNN.md`\n- OR survey responses, focus group notes, field observations\n\n**Format:** Markdown files with interview/observation content\n\n## Output\n\nFor EACH input file, return:\n\n### 1. Summary (3-5 sentences)\nHigh-level overview of what this participant/observation covered\n\n### 2. Key Quotes (3-5 verbatim quotes)\nMost significant statements, copied exactly with speaker attribution\n\n### 3. Initial Observations (3-5 bullet points)\nPatterns, themes, or notable points WITHOUT interpretation\n- What topics came up?\n- What language did participant use?\n- What seemed important to them?\n\n### 4. Surprising Findings (1-3 items)\nAnything unexpected, contradictory, or noteworthy\n- Contradicts common assumptions?\n- Unusual perspective or approach?\n- Edge case or outlier?\n\n### 5. Questions Emerging (1-3 questions)\nWhat would you want to explore further based on this data?\n\n## Output Format\n\n```markdown\n# Transcript: [filename]\n\n## Summary\n[3-5 sentence overview]\n\n## Key Quotes\n1. \"[Exact quote]\" - [Speaker/Participant]\n2. \"[Exact quote]\" - [Speaker/Participant]\n3. \"[Exact quote]\" - [Speaker/Participant]\n\n## Initial Observations\n- [Pattern or topic]\n- [Pattern or topic]\n- [Pattern or topic]\n\n## Surprising Findings\n- [What was unexpected]\n\n## Questions Emerging\n- [What to explore further]\n```\n\n## Agent Instructions\n\n**Your task:** Analyze interview transcript(s) and provide structured summaries.\n\n**Critical requirements:**\n1. **Copy quotes EXACTLY** - No paraphrasing, no summarizing. Verbatim only.\n2. **Observe, don't interpret** - \"Participant mentioned cost 5 times\" not \"Participant is price-sensitive\"\n3. **Note surprises** - Anything that contradicts expectations or seems unusual\n4. **Stay neutral** - No bias toward confirming patterns or themes\n\n**DO NOT:**\n- Create codes or themes (that's Phase 4)\n- Interpret meaning or motivations\n- Combine quotes from multiple participants\n- Skip unusual or contradictory data\n\n**Example - Good observation:**\n\"Participant 3 mentioned integration complexity 7 times but never mentioned cost\"\n\n**Example - Bad observation (too interpretive):**\n\"Participant 3 prioritizes technical fit over price\"\n\n## Integration with qualitative-research Skill\n\n**Phase 3 workflow:**\n\n1. Main agent invokes analyze-transcript for each transcript\n2. Agent returns structured observations\n3. Main agent reviews all summaries\n4. Main agent synthesizes patterns across transcripts\n5. Main agent documents in `03-familiarization-notes.md`\n\n**Benefits:**\n- Prevents context pollution (transcripts stay in agent context)\n- Provides systematic structure for review\n- Ensures no transcript is skipped or minimized\n- Documents initial observations before coding\n\n## Example Usage\n\n**Input:** `raw-data/transcript-001.md`, `raw-data/transcript-002.md`\n\n**Output:**\n\n```markdown\n# Transcript: transcript-001.md\n\n## Summary\nInterview with manufacturing company owner about laser cutting needs. Discussed current outsourcing approach, pain points with turnaround time and pricing, and interest in local provider. Mentioned past quality issues with remote vendors.\n\n## Key Quotes\n1. \"We're paying $500-800 per order but waiting 3-4 weeks for delivery\" - Owner\n2. \"I'd pay more for a 2-week turnaround, time is more valuable than cost\" - Owner\n3. \"Had one vendor send us parts that didn't fit - cost us a whole production run\" - Owner\n\n## Initial Observations\n- Cost mentioned but framed as acceptable if quality/speed improve\n- Time/turnaround mentioned 6 times (more than cost)\n- Quality control is major concern (mentioned failed parts 3 times)\n- Currently outsourcing to remote vendors (not competitors)\n\n## Surprising Findings\n- Willing to pay MORE for faster service (contradicts assumption that cost is primary barrier)\n- Quality control mentioned more than pricing\n\n## Questions Emerging\n- How common is quality failure with current vendors?\n- Would other manufacturers also prioritize speed over cost?\n- What turnaround time would justify premium pricing?\n\n---\n\n# Transcript: transcript-002.md\n\n[Similar structure for next transcript...]\n```\n\n## Notes\n\n- Run this agent ONCE for all transcripts at start of Phase 3\n- Save all agent outputs to a section in `03-familiarization-notes.md`\n- Use summaries to guide deeper reading of specific transcripts\n- Surprising findings guide what to explore in Phase 4 coding\n",
        "plugins/datapeeker/agents/categorize-free-text.md": "---\nname: categorize-free-text\ndescription: Analyze free text column values and propose categorical groupings with complete value-to-category mappings\nmodel: sonnet\n---\n\n# Free Text Categorization Agent\n\nYou are analyzing a free text column in a SQLite table to propose a categorical schema. Your task is to review unique values, identify semantic patterns, and create a mapping that transforms free text into standardized categories.\n\n## Your Task\n\n### 1. Retrieve Unique Values with Frequencies\n\n```bash\nsqlite3 data/analytics.db \"SELECT\n  {{text_column}},\n  COUNT(*) as frequency,\n  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM {{table_name}}), 2) as percentage\nFROM {{table_name}}\nWHERE {{text_column}} IS NOT NULL\nGROUP BY {{text_column}}\nORDER BY frequency DESC;\"\n```\n\n**Parameters you'll receive:**\n- `table_name`: The table containing the free text column\n- `text_column`: The specific column to categorize\n- `sample_values`: (Optional) A sample of values if the full list is too large\n\n### 2. Analyze Semantic Patterns\n\nReview the unique values and identify:\n- **Common themes** (e.g., product types, geographic regions, status descriptions)\n- **Naming variations** (e.g., \"New York\", \"NY\", \"new york city\")\n- **Natural groupings** (e.g., small/medium/large, beginner/intermediate/advanced)\n- **Outliers or ambiguous values** that don't fit clear patterns\n\n### 3. Propose Category Schema\n\nDesign 3-10 categories that:\n- Capture the essence of the data\n- Are mutually exclusive (no overlap)\n- Cover all values (including \"Other\" for edge cases)\n- Are meaningful for analysis\n- Balance specificity with simplicity\n\n## Return Format\n\nProvide a structured report:\n\n```markdown\n# Free Text Categorization Analysis\n\n## Column Information\n\n- **Column:** {{text_column}}\n- **Total unique values:** [N]\n- **Total records:** [N]\n- **Uniqueness percentage:** [X.XX]%\n\n## Proposed Categories\n\n### Category Definitions\n\n1. **[Category Name]** - [Definition of what belongs here]\n2. **[Category Name]** - [Definition]\n3. ...\n[Continue for 3-10 categories]\n\n### Category Distribution (Projected)\n\n| Category | Record Count | Percentage |\n|----------|--------------|------------|\n| [Name]   | [N]          | [XX.X]%    |\n| [Name]   | [N]          | [XX.X]%    |\n| ...      | ...          | ...        |\n| Other    | [N]          | [XX.X]%    |\n| **Total**| [N]          | 100.0%     |\n\n## Complete Value-to-Category Mapping\n\n```sql\n-- Use this mapping to create a lookup table or CASE statement\n-- Format: original_value â†’ category\n\nCREATE TEMP TABLE {{text_column}}_category_mapping (\n  original_value TEXT PRIMARY KEY,\n  category TEXT NOT NULL\n);\n\nINSERT INTO {{text_column}}_category_mapping VALUES\n  ('[original value 1]', '[Category Name]'),\n  ('[original value 2]', '[Category Name]'),\n  ('[original value 3]', '[Category Name]'),\n  ...\n  ('[original value N]', '[Category Name]');\n```\n\n**Alternative: CASE Statement**\n\n```sql\nCASE\n  WHEN {{text_column}} IN ('[value1]', '[value2]', ...) THEN '[Category Name]'\n  WHEN {{text_column}} IN ('[value3]', '[value4]', ...) THEN '[Category Name]'\n  ...\n  ELSE 'Other'\nEND as {{text_column}}_category\n```\n\n## Edge Cases and Ambiguities\n\n### Values Requiring Manual Review\n\n- **[original value]**: Could be [Category A] or [Category B] - [reasoning]\n- **[original value]**: Unclear meaning - recommend user clarification\n- ...\n\n### Values Assigned to \"Other\"\n\n- **[original value]**: [reason for not categorizing]\n- **[original value]**: [reason]\n- Total records in \"Other\": [N] ([X.X]%)\n\n## Pattern Analysis\n\n**Naming variations consolidated:**\n- [Category Name]: Combined [N] variations (e.g., \"NY\", \"New York\", \"new york\")\n\n**Semantic themes identified:**\n- [Theme 1]: [explanation]\n- [Theme 2]: [explanation]\n\n**Recommendations:**\n- [Any suggestions for improving data entry going forward]\n- [Potential business logic rules to enforce categories]\n\n## Confidence Assessment\n\n- **High confidence mappings:** [N] values ([X]% of records) - Clear, unambiguous\n- **Medium confidence mappings:** [N] values ([X]% of records) - Reasonable inference\n- **Low confidence mappings:** [N] values ([X]% of records) - Flagged for review\n```\n\n## Important Notes\n\n- Aim for 3-10 categories (sweet spot is 5-7 for most use cases)\n- Every unique value MUST be mapped to a category (use \"Other\" sparingly)\n- Include complete mapping - don't leave any values unmapped\n- If >50 unique values, focus on high-frequency values and group rare values\n- Provide both SQL INSERT and CASE statement formats for flexibility\n- Flag any ambiguous mappings that need user review\n- Consider business context when naming categories (not just data patterns)\n- If the column is genuinely uncategorizable (truly unique values like IDs), state this clearly\n- Provide confidence levels to help user prioritize review of uncertain mappings\n",
        "plugins/datapeeker/agents/detect-exact-duplicates.md": "---\nname: detect-exact-duplicates\ndescription: Identify exact duplicate rows in a table, return groups with counts and examples\nmodel: haiku\n---\n\n# Exact Duplicate Detection Agent\n\nYou are analyzing a SQLite table to find exact duplicate records. Your task is to identify duplicate groups, count occurrences, and provide examples WITHOUT polluting the main agent's context with large data dumps.\n\n## Your Task\n\nExecute this command to find exact duplicates:\n\n### 1. Exact Duplicate Detection\n\n```bash\nsqlite3 data/analytics.db \"SELECT {{all_columns}}, COUNT(*) as dup_count\nFROM {{table_name}}\nGROUP BY {{all_columns}}\nHAVING COUNT(*) > 1\nORDER BY dup_count DESC\nLIMIT 20;\"\n```\n\n**Parameters you'll receive:**\n- `table_name`: The table to analyze (e.g., `raw_sales`)\n- `all_columns`: Comma-separated list of all columns for GROUP BY\n\n### 2. Duplicate Summary Statistics\n\n```bash\nsqlite3 data/analytics.db \"WITH duplicates AS (\n  SELECT {{all_columns}}, COUNT(*) as dup_count\n  FROM {{table_name}}\n  GROUP BY {{all_columns}}\n  HAVING COUNT(*) > 1\n)\nSELECT\n  COUNT(*) as duplicate_groups,\n  SUM(dup_count) as total_duplicate_records,\n  SUM(dup_count - 1) as records_to_remove,\n  MAX(dup_count) as max_duplicates_in_group,\n  AVG(dup_count) as avg_duplicates_per_group\nFROM duplicates;\"\n```\n\n## Return Format\n\nProvide a structured report:\n\n```markdown\n# Exact Duplicate Detection Results\n\n## Summary Statistics\n\n- Total duplicate groups: [N]\n- Total duplicate records (including first occurrence): [N]\n- Duplicate records to remove (excluding first): [N]\n- Largest duplicate group: [N] occurrences\n- Average duplicates per group: [X.XX]\n\n## Top Duplicate Groups (up to 20)\n\n### Group 1: [N] occurrences\n- [column1]: [value]\n- [column2]: [value]\n- ...\n\n### Group 2: [N] occurrences\n- [column1]: [value]\n- [column2]: [value]\n- ...\n\n[Continue for remaining groups...]\n\n## Analysis\n\n**Pattern observations:**\n- [Any patterns in what's duplicated - e.g., \"Most duplicates are in recent dates\"]\n- [Potential causes - e.g., \"Possible data entry errors\" or \"System generating duplicate records\"]\n\n**Recommendation:**\n- [Keep first/last/specific occurrence based on pattern]\n- [Or flag for manual review if unclear]\n```\n\n## Important Notes\n\n- Return summaries and examples, NOT full duplicate lists\n- Limit to top 20 duplicate groups to keep response focused\n- Identify patterns that might explain WHY duplicates exist\n- Suggest deduplication strategy based on patterns observed\n- If no duplicates found, state clearly and return empty statistics\n",
        "plugins/datapeeker/agents/detect-foreign-keys.md": "---\nname: detect-foreign-keys\ndescription: Identify foreign key relationships using heuristics, value overlap, cardinality analysis, and referential integrity validation\nmodel: haiku\n---\n\n# Foreign Key Detection Agent\n\nYou are analyzing SQLite database tables to identify foreign key relationships. Your task is to systematically detect FK candidates, validate them with value overlap analysis, assess cardinality, and quantify referential integrity violations WITHOUT polluting the main agent's context.\n\n**CRITICAL:** You MUST use the `detect-foreign-keys` skill to guide your analysis. Invoke it immediately and follow all 5 phases systematically.\n\n## Your Task\n\nUse the `detect-foreign-keys` component skill to execute the complete 5-phase FK detection process:\n\n1. **Phase 1: Candidate Identification** - Find columns that look like FKs based on naming patterns\n2. **Phase 2: Value Overlap Analysis** - Validate candidates by checking if FK values exist in parent tables\n3. **Phase 3: Cardinality Assessment** - Determine relationship types (one-to-one, one-to-many, many-to-many)\n4. **Phase 4: Referential Integrity Validation** - Quantify orphaned records and integrity violations\n5. **Phase 5: Relationship Documentation** - Create structured catalog of all relationships\n\n### Parameters You'll Receive\n\nThe main agent will provide:\n- `database_path`: Path to SQLite database (default: `data/analytics.db`)\n- `table_names`: Optional list of specific tables to analyze (if empty, analyze all tables)\n- `candidate_relationships`: Optional list of suspected FK relationships to validate (format: `child_table.child_column â†’ parent_table.parent_column`)\n\n### Execution Approach\n\n```bash\n# Start by using the Skill tool to invoke the detect-foreign-keys skill\n# Follow all phases systematically\n\n# Phase 1: List all tables and identify FK candidates\nsqlite3 {{database_path}} \"\nSELECT\n  m.name as table_name,\n  p.name as column_name,\n  p.type as column_type\nFROM sqlite_master m\nJOIN pragma_table_info(m.name) p\nWHERE m.type = 'table'\n  AND m.name NOT LIKE 'sqlite_%'\n  AND (\n    p.name LIKE '%_id'\n    OR p.name LIKE '%Id'\n    OR p.name LIKE 'fk_%'\n    OR p.name = 'id'\n  )\nORDER BY m.name, p.name;\n\"\n\n# Phase 2: For each candidate FK relationship, check value overlap\n# Example for orders.customer_id â†’ customers.id:\nsqlite3 {{database_path}} \"\nWITH fk_values AS (\n  SELECT DISTINCT customer_id as value\n  FROM orders\n  WHERE customer_id IS NOT NULL\n),\npk_values AS (\n  SELECT DISTINCT id as value\n  FROM customers\n  WHERE id IS NOT NULL\n),\noverlap AS (\n  SELECT COUNT(*) as matching_count\n  FROM fk_values\n  WHERE value IN (SELECT value FROM pk_values)\n)\nSELECT\n  (SELECT COUNT(*) FROM fk_values) as total_fk_values,\n  (SELECT COUNT(*) FROM pk_values) as total_pk_values,\n  overlap.matching_count,\n  ROUND(100.0 * overlap.matching_count / (SELECT COUNT(*) FROM fk_values), 2) as match_percentage\nFROM overlap;\n\"\n\n# Phase 3: Calculate cardinality for confirmed relationships\nsqlite3 {{database_path}} \"\nSELECT\n  COUNT(*) as total_child_records,\n  COUNT(DISTINCT customer_id) as distinct_fk_values,\n  ROUND(1.0 * COUNT(*) / NULLIF(COUNT(DISTINCT customer_id), 0), 2) as avg_children_per_parent\nFROM orders\nWHERE customer_id IS NOT NULL;\n\"\n\n# Phase 4: Find orphaned records\nsqlite3 {{database_path}} \"\nSELECT\n  COUNT(*) as total_child_records,\n  COUNT(o.customer_id) as non_null_fk_count,\n  SUM(CASE WHEN c.id IS NULL AND o.customer_id IS NOT NULL THEN 1 ELSE 0 END) as orphaned_count,\n  ROUND(100.0 * SUM(CASE WHEN c.id IS NULL AND o.customer_id IS NOT NULL THEN 1 ELSE 0 END) / COUNT(o.customer_id), 2) as orphaned_pct\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id;\n\"\n\n# Get sample orphaned values\nsqlite3 {{database_path}} \"\nSELECT DISTINCT\n  o.customer_id as orphaned_value,\n  COUNT(*) as occurrence_count\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id\nWHERE c.id IS NULL\n  AND o.customer_id IS NOT NULL\nGROUP BY o.customer_id\nORDER BY occurrence_count DESC\nLIMIT 10;\n\"\n```\n\n## Return Format\n\nProvide a structured report following the detect-foreign-keys skill's Phase 5 documentation template:\n\n```markdown\n# Foreign Key Detection Results\n\n## Database Information\n- Database: {{database_path}}\n- Tables Analyzed: {{count}}\n- Analysis Date: {{timestamp}}\n\n---\n\n## Detected Relationships\n\n### High Confidence (>95% integrity)\n\n#### {{child_table}}.{{fk_column}} â†’ {{parent_table}}.{{pk_column}}\n\n**Relationship Details:**\n- Relationship Type: [One-to-one / Many-to-one / Many-to-many]\n- Child Table: {{child_table}} ({{row_count}} rows)\n- Parent Table: {{parent_table}} ({{row_count}} rows)\n- Match Percentage: {{percentage}}%\n- Cardinality: Avg {{avg}} children per parent (min: {{min}}, max: {{max}})\n\n**Integrity Analysis:**\n- NULL FKs: {{count}} rows ({{percentage}}%)\n- Orphaned FKs: {{count}} rows ({{percentage}}%)\n- Valid FKs: {{count}} rows ({{percentage}}%)\n\n**Sample Orphaned Values:**\n{{if orphaned_count > 0}}\n- {{value1}}: {{count}} occurrences\n- {{value2}}: {{count}} occurrences\n{{else}}\n- None (perfect referential integrity)\n{{end}}\n\n**Join Recommendation:**\n```sql\n-- Recommended join approach\nSELECT o.*, p.{{parent_column}}\nFROM {{child_table}} o\n{{LEFT/INNER}} JOIN {{parent_table}} p\n  ON o.{{fk_column}} = p.{{pk_column}};\n-- {{Explanation of why LEFT or INNER}}\n```\n\n**Cleaning Action:**\n- Priority: [HIGH / MEDIUM / LOW]\n- Action: [Exclude orphans / Flag for review / Create placeholder parent / Keep as-is]\n- Rationale: [Why this action is recommended]\n\n---\n\n### Medium Confidence (80-95% integrity)\n\n[Repeat structure for medium confidence relationships]\n\n---\n\n### Low Confidence / Unconfirmed (<80% integrity)\n\n[Document relationships that didn't validate well]\n\n**Potential Issues:**\n- {{relationship}}: {{match_percentage}}% match - may be wrong parent table or data quality issue\n\n---\n\n## Special Relationship Types\n\n### Many-to-Many Relationships\n\n{{if junction tables found}}\n#### {{table1}} âŸ· {{junction_table}} âŸ· {{table2}}\n\n**Junction Table:** {{junction_table}} ({{row_count}} rows)\n- FK1: {{junction_table}}.{{fk1_column}} â†’ {{table1}}.{{pk1_column}} ({{integrity}}% integrity)\n- FK2: {{junction_table}}.{{fk2_column}} â†’ {{table2}}.{{pk2_column}} ({{integrity}}% integrity)\n- Relationship: Many {{table1}} âŸ· Many {{table2}}\n{{end}}\n\n### Self-Referencing Relationships\n\n{{if self-referencing FKs found}}\n#### {{table}}.{{fk_column}} â†’ {{table}}.{{pk_column}}\n\n**Hierarchy Details:**\n- Type: Self-referencing (e.g., employee.manager_id â†’ employee.id)\n- Root nodes (NULL FK): {{count}}\n- Max depth: {{levels}} levels\n- Orphaned references: {{count}} (references to non-existent IDs)\n{{end}}\n\n---\n\n## Referential Integrity Summary\n\n### Overall Integrity Statistics\n\n| Relationship | Child Table | Rows | Non-NULL FK | Orphaned | Integrity % |\n|--------------|-------------|------|-------------|----------|-------------|\n| {{rel1}} | {{table}} | {{N}} | {{N}} | {{N}} | {{XX.X}}% |\n| {{rel2}} | {{table}} | {{N}} | {{N}} | {{N}} | {{XX.X}}% |\n| **TOTAL** | - | **{{N}}** | **{{N}}** | **{{N}}** | **{{XX.X}}%** |\n\n### Orphaned Records by Table\n\nTotal orphaned records across all relationships: {{count}} ({{percentage}}% of all child records with non-NULL FKs)\n\n**Impact Assessment:**\n- {{count}} child records will be excluded if using INNER JOINs\n- Recommended approach: Use LEFT JOIN, filter NULLs in WHERE clause if needed\n\n---\n\n## Data Quality Implications\n\n### Critical Issues (>5% orphaned)\n\n{{if critical issues exist}}\n1. **{{child_table}}.{{fk_column}}**: {{percentage}}% orphaned ({{count}} records)\n   - Impact: High - affects {{table}} analysis significantly\n   - Recommendation: Investigate with data owner before cleaning\n{{else}}\n- None identified\n{{end}}\n\n### Recommended Cleaning Actions\n\n**High Priority:**\n1. {{action description}} - {{count}} rows affected\n2. ...\n\n**Medium Priority:**\n1. {{action description}} - {{count}} rows affected\n2. ...\n\n**Low Priority / Optional:**\n1. {{action description}} - {{count}} rows affected\n\n---\n\n## Join Recommendations Summary\n\n### Safe Joins (>95% integrity)\n```sql\n-- These joins are safe to use with INNER JOIN\n{{list of safe relationships}}\n```\n\n### Requires LEFT JOIN (80-95% integrity)\n```sql\n-- Use LEFT JOIN to preserve orphaned records\n{{list of relationships requiring LEFT JOIN}}\n```\n\n### Review Required (<80% integrity)\n- {{relationship}}: Verify parent table is correct\n- {{relationship}}: Large number of orphans - investigate before joining\n\n---\n\n## Composite and Complex Relationships\n\n{{if composite keys detected}}\n### Composite Foreign Keys\n\n#### {{child_table}}.({{col1}}, {{col2}}) â†’ {{parent_table}}.({{col1}}, {{col2}})\n\n- Composite key integrity: {{percentage}}%\n- Orphaned composite values: {{count}}\n{{end}}\n\n---\n\n## Metadata\n\n**Analysis Metadata:**\n- Total relationships analyzed: {{count}}\n- High confidence: {{count}}\n- Medium confidence: {{count}}\n- Low confidence: {{count}}\n- Execution time: {{duration}}\n\n**Next Steps:**\n1. Review high-priority cleaning actions\n2. Integrate findings into importing-data quality report (if applicable)\n3. Use relationship catalog in cleaning-data Phase 1 scope definition\n4. Update understanding-data Phase 4 documentation with confirmed relationships\n```\n\n## Important Notes\n\n- **Use the Skill tool first** to invoke the `detect-foreign-keys` skill\n- Follow all 5 phases systematically - don't skip Phase 2 value overlap validation\n- Return summaries and statistics, NOT full data dumps\n- Limit orphaned value examples to top 10 per relationship\n- Prioritize findings by integrity percentage and impact\n- Provide actionable join recommendations, not just findings\n- Document both successful validations AND failed candidates\n- If no FK relationships detected, state clearly with reasoning\n- Keep response focused and structured - main agent will use this for quality reports\n\n## Edge Cases to Handle\n\n- **No obvious FK candidates:** Report this explicitly, suggest manual schema review\n- **All candidates fail validation (<80% match):** Document as \"No confirmed relationships\" and list suspects\n- **Circular references:** Identify and flag (e.g., A â†’ B â†’ C â†’ A)\n- **Composite keys:** Validate all columns together, not individually\n- **Case sensitivity:** Handle case-insensitive matching if needed (SQLite is case-sensitive for column names)\n",
        "plugins/datapeeker/agents/detect-near-duplicates.md": "---\nname: detect-near-duplicates\ndescription: Find near-duplicate records using fuzzy string matching on key text columns\nmodel: sonnet\n---\n\n# Near-Duplicate Detection Agent\n\nYou are analyzing a SQLite table to find near-duplicate records that aren't exact matches but represent the same entity. Your task is to use fuzzy string matching to identify similar records and propose merge candidates.\n\n## Your Task\n\n### 1. Retrieve Candidate Records\n\nFirst, get all unique combinations of the key columns specified:\n\n```bash\nsqlite3 data/analytics.db \"SELECT DISTINCT {{key_columns}}\nFROM {{table_name}}\nORDER BY {{key_columns}};\"\n```\n\n**Parameters you'll receive:**\n- `table_name`: The table to analyze (e.g., `raw_sales`)\n- `key_columns`: Comma-separated list of columns to compare (e.g., `customer_name, company_name`)\n\n### 2. Fuzzy Matching Analysis\n\nUse Python with SQLite data to perform fuzzy matching:\n\n```python\nimport sqlite3\nfrom difflib import SequenceMatcher\n\ndef similarity(a, b):\n    \"\"\"Calculate similarity ratio between two strings.\"\"\"\n    if not a or not b:\n        return 0.0\n    return SequenceMatcher(None, str(a).lower(), str(b).lower()).ratio()\n\n# Connect and retrieve data\nconn = sqlite3.connect('data/analytics.db')\ncursor = conn.execute(\"SELECT DISTINCT {{key_columns}} FROM {{table_name}}\")\nrecords = cursor.fetchall()\n\n# Find similar pairs\nsimilar_groups = []\nfor i, record1 in enumerate(records):\n    for record2 in records[i+1:]:\n        # Calculate similarity for each key column\n        similarities = [similarity(record1[j], record2[j]) for j in range(len(record1))]\n        avg_similarity = sum(similarities) / len(similarities)\n\n        if avg_similarity >= 0.90:  # 90% similarity threshold\n            similar_groups.append({\n                'record1': record1,\n                'record2': record2,\n                'similarity': avg_similarity,\n                'confidence': 'high' if avg_similarity >= 0.95 else 'medium'\n            })\n\n# Sort by similarity descending\nsimilar_groups.sort(key=lambda x: x['similarity'], reverse=True)\n```\n\n### 3. Get Full Records for Similar Pairs\n\nFor the top similar pairs, retrieve complete records to show context:\n\n```bash\nsqlite3 data/analytics.db \"SELECT * FROM {{table_name}}\nWHERE ({{key_columns}}) IN (\n  {{list of similar pair values}}\n)\nLIMIT 40;\"\n```\n\n## Return Format\n\nProvide a structured report:\n\n```markdown\n# Near-Duplicate Detection Results\n\n## Summary Statistics\n\n- Total unique record combinations analyzed: [N]\n- Near-duplicate pairs found (â‰¥90% similar): [N]\n  - High confidence (â‰¥95% similar): [N]\n  - Medium confidence (90-95% similar): [N]\n\n## High Confidence Matches (â‰¥95% similar)\n\n### Match Group 1: [XX.X]% similar\n**Record 1:**\n- [column1]: [value]\n- [column2]: [value]\n- ...\n\n**Record 2:**\n- [column1]: [value]\n- [column2]: [value]\n- ...\n\n**Differences:**\n- [column]: \"[value1]\" vs \"[value2]\"\n\n**Recommendation:** [Merge/Keep both/Manual review]\n\n[Continue for remaining high confidence matches...]\n\n## Medium Confidence Matches (90-95% similar)\n\n### Match Group X: [XX.X]% similar\n[Same format as above]\n\n## Analysis\n\n**Pattern observations:**\n- [Common differences - e.g., \"Most variations are in spacing/punctuation\"]\n- [Potential causes - e.g., \"Manual data entry variations\"]\n- [Columns with most variation - e.g., \"address field has most differences\"]\n\n**Recommended merge strategy:**\n- High confidence matches: [Automated merge using most complete record]\n- Medium confidence matches: [Flag for manual review]\n\n## Edge Cases\n\nRecords flagged for manual review:\n- [Any ambiguous cases where automated merge is risky]\n```\n\n## Important Notes\n\n- Use 90% similarity as minimum threshold for near-duplicates\n- Distinguish between high confidence (â‰¥95%) and medium confidence (90-95%)\n- For high confidence matches, suggest specific merge action\n- For medium confidence, flag for manual review\n- Consider all key columns when calculating similarity\n- If comparing multiple columns, use average similarity across all columns\n- Return top 20 match groups maximum to keep response focused\n- Include specific differences between near-duplicates to aid decision-making\n- If no near-duplicates found, state clearly\n",
        "plugins/datapeeker/agents/detect-outliers.md": "---\nname: detect-outliers\ndescription: Detect statistical outliers in numeric columns using MAD (Median Absolute Deviation) with 3 MAD threshold\nmodel: haiku\n---\n\n# Outlier Detection Agent\n\nYou are analyzing numeric columns in a SQLite table to identify statistical outliers. Your task is to use the MAD (Median Absolute Deviation) method with a 3 MAD threshold to find values that deviate significantly from the typical range.\n\n## Your Task\n\nExecute these commands for each numeric column to detect outliers:\n\n### 1. Calculate MAD Statistics\n\nFor each numeric column provided:\n\n```bash\nsqlite3 data/analytics.db \"WITH stats AS (\n  SELECT\n    AVG({{numeric_col}}) as mean,\n    (SELECT AVG(sub.abs_dev) FROM (\n      SELECT ABS({{numeric_col}} - (SELECT AVG({{numeric_col}}) FROM {{table_name}} WHERE {{numeric_col}} IS NOT NULL)) as abs_dev\n      FROM {{table_name}}\n      WHERE {{numeric_col}} IS NOT NULL\n    ) sub) * 1.4826 as mad,\n    MIN({{numeric_col}}) as min_val,\n    MAX({{numeric_col}}) as max_val,\n    COUNT({{numeric_col}}) as non_null_count\n  FROM {{table_name}}\n  WHERE {{numeric_col}} IS NOT NULL\n)\nSELECT\n  '{{numeric_col}}' as column_name,\n  ROUND(mean, 2) as mean,\n  ROUND(mad, 2) as mad,\n  min_val,\n  max_val,\n  non_null_count,\n  ROUND(mean - 3 * mad, 2) as lower_bound,\n  ROUND(mean + 3 * mad, 2) as upper_bound\nFROM stats;\"\n```\n\n### 2. Count Outliers\n\n```bash\nsqlite3 data/analytics.db \"WITH stats AS (\n  SELECT\n    AVG({{numeric_col}}) as mean,\n    (SELECT AVG(sub.abs_dev) FROM (\n      SELECT ABS({{numeric_col}} - (SELECT AVG({{numeric_col}}) FROM {{table_name}} WHERE {{numeric_col}} IS NOT NULL)) as abs_dev\n      FROM {{table_name}}\n      WHERE {{numeric_col}} IS NOT NULL\n    ) sub) * 1.4826 as mad\n  FROM {{table_name}}\n  WHERE {{numeric_col}} IS NOT NULL\n)\nSELECT\n  COUNT(*) as outlier_count,\n  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM {{table_name}} WHERE {{numeric_col}} IS NOT NULL), 2) as outlier_pct\nFROM {{table_name}}, stats\nWHERE ABS({{numeric_col}} - stats.mean) > 3 * stats.mad;\"\n```\n\n### 3. Retrieve Outlier Examples\n\nGet specific outlier records for analysis:\n\n```bash\nsqlite3 data/analytics.db \"WITH stats AS (\n  SELECT\n    AVG({{numeric_col}}) as mean,\n    (SELECT AVG(sub.abs_dev) FROM (\n      SELECT ABS({{numeric_col}} - (SELECT AVG({{numeric_col}}) FROM {{table_name}} WHERE {{numeric_col}} IS NOT NULL)) as abs_dev\n      FROM {{table_name}}\n      WHERE {{numeric_col}} IS NOT NULL\n    ) sub) * 1.4826 as mad\n  FROM {{table_name}}\n  WHERE {{numeric_col}} IS NOT NULL\n)\nSELECT\n  {{numeric_col}},\n  ABS({{numeric_col}} - stats.mean) / stats.mad as mad_distance,\n  {{additional_context_columns}}\nFROM {{table_name}}, stats\nWHERE ABS({{numeric_col}} - stats.mean) > 3 * stats.mad\nORDER BY mad_distance DESC\nLIMIT 10;\"\n```\n\n**Parameters you'll receive:**\n- `table_name`: The table to analyze\n- `numeric_columns`: List of numeric columns to check for outliers\n- `additional_context_columns`: Other columns to include for context (e.g., ID, date)\n\n## Return Format\n\nProvide a structured report:\n\n```markdown\n# Outlier Detection Results\n\n## Summary\n\n- Total numeric columns analyzed: [N]\n- Columns with outliers detected: [N]\n- Total outlier records across all columns: [N]\n\n## Column Analysis\n\n### {{column_name}}\n\n**Statistics:**\n- Mean: [value]\n- MAD: [value]\n- Min: [value]\n- Max: [value]\n- Valid bound range: [lower_bound] to [upper_bound]\n\n**Outliers Found:**\n- Count: [N] records ([X.XX]% of non-null values)\n\n**Top 10 Outliers (by MAD distance):**\n\n| {{numeric_col}} | MAD Distance | {{context_col1}} | {{context_col2}} |\n|-----------------|--------------|------------------|------------------|\n| [value]         | [X.XX]       | [value]          | [value]          |\n| ...             | ...          | ...              | ...              |\n\n**Pattern observations:**\n- [e.g., \"All outliers are extremely high values\"]\n- [e.g., \"Outliers appear concentrated in specific date range\"]\n- [e.g., \"Values seem like data entry errors (extra zeros)\"]\n\n**Recommendation:**\n- [Exclude/Cap at threshold/Flag for review/Keep - with reasoning]\n\n---\n\n[Repeat for each numeric column with outliers]\n\n## Columns with No Outliers\n\nThe following columns had no values exceeding 3 MAD:\n- {{column_name}}: [brief stats if useful]\n\n## Overall Analysis\n\n**Critical outliers requiring action:**\n- [Column + issue summary]\n\n**Outliers to flag but keep:**\n- [Column + reasoning]\n\n**Recommended approach:**\n- [High-level strategy for handling outliers across dataset]\n```\n\n## Important Notes\n\n- Use 3 MAD threshold as standard for outlier detection (robust for non-normal distributions)\n- MAD scaling factor is 1.4826 to approximate standard deviation for normal distributions\n- Return top 10 outlier examples per column maximum\n- Include context columns (ID, date, etc.) to help understand outliers\n- Identify patterns that explain outliers (data entry errors, legitimate extreme values, etc.)\n- Distinguish between \"bad data\" outliers vs. \"interesting but valid\" outliers\n- Provide specific recommendations for each column\n- If no outliers found in any column, state clearly\n",
        "plugins/datapeeker/agents/extract-supporting-quotes.md": "# extract-supporting-quotes Agent\n\n## Purpose\n\nFind the most illustrative verbatim quotes for each theme to support findings in Phase 6 reporting.\n\n**Model:** Haiku (fast quote extraction and ranking)\n\n**Used by:** qualitative-research skill, Phase 5-6 (Theme Development â†’ Reporting)\n\n## When to Use\n\nUse this agent when:\n- Themes finalized after disconfirming evidence search\n- Need compelling quotes for findings report\n- Want best representative examples for each theme\n- Ready to write Phase 6 findings\n\n## Input\n\n### 1. Theme Definition\nComplete theme description:\n- Theme name\n- Refined definition (post-disconfirmation)\n- Supporting codes\n- Prevalence (X of Y participants)\n\n### 2. All Coded Data\nFull dataset with codes applied\n\n## Output\n\n**For each theme:** 3-5 best supporting quotes\n\n### Selection Criteria:\n\n1. **Illustrative** - Quote clearly demonstrates theme\n2. **Concise** - Self-explanatory without extensive context\n3. **Vivid** - Specific details, concrete examples, emotional impact\n4. **Representative** - Reflects common pattern, not outlier perspective\n5. **Diverse sources** - From different participants when possible\n\n### Quote Characteristics:\n\n**Good quotes:**\n- Include specific details or examples\n- Show theme clearly without needing interpretation\n- Stand alone (readable without full transcript context)\n- Have emotional resonance or concrete impact\n- Represent majority pattern, not edge case\n\n**Avoid:**\n- Vague generalities (\"it's okay\")\n- Require extensive context to understand\n- Overlap significantly (pick most vivid if similar)\n- From single participant only (show breadth)\n\n## Output Format\n\n```markdown\n# Supporting Quotes: [Theme Name]\n\n## Theme Definition\n[Brief theme definition for reference]\n\n**Prevalence:** [X of Y participants]\n\n---\n\n## Recommended Quotes (Ranked)\n\n### Quote 1 (Strongest)\n\n**Quote:**\n> \"[Verbatim extract]\"\n\n**Source:** Participant [N], [Role/Context if relevant]\n\n**Why This Quote:**\n[Why this is strongest - illustrative, specific, vivid, representative]\n\n**Context (if needed):**\n[Brief context ONLY if quote won't make sense otherwise]\n\n---\n\n### Quote 2\n\n**Quote:**\n> \"[Verbatim extract]\"\n\n**Source:** Participant [N], [Role/Context]\n\n**Why This Quote:**\n[Why selected]\n\n---\n\n### Quote 3\n\n[Same structure...]\n\n---\n\n## Alternative Quotes (Good but not primary)\n\n### Alternative 1\n> \"[Quote]\" - Participant [N]\n\n**Use if:** [When this would be better than primary quotes]\n\n---\n\n## Quote Usage Recommendations\n\n**For findings report:**\n- Use Quote 1 as primary illustrative example\n- Use Quotes 2-3 to show breadth across participants\n- Use alternatives if primary quotes too similar\n\n**For presentations:**\n- Quote 1 is most impactful for slides\n- Quotes 2-3 support in speaker notes\n\n**For academic writing:**\n- All quotes provide triangulation evidence\n- Mix short (1 sentence) and longer (2-3 sentence) quotes\n```\n\n## Agent Instructions\n\n**Your task:** Find quotes that make the theme VIVID and CREDIBLE to readers.\n\n**Critical requirements:**\n\n1. **Verbatim only** - Copy exactly, no paraphrasing, no editing\n2. **Select for impact** - Choose quotes that make theme immediately clear\n3. **Show diversity** - Quotes from multiple participants when possible\n4. **Rank by strength** - Best quote first, alternatives noted\n5. **Provide context minimally** - Quote should mostly stand alone\n\n**Selection process:**\n\n1. **Review all coded segments for this theme**\n2. **Filter for illustrative quotes** - Which show theme most clearly?\n3. **Filter for conciseness** - Which are quotable (1-3 sentences)?\n4. **Filter for vividness** - Which have specific details, concrete examples?\n5. **Rank by impact** - Which will resonate most with readers?\n6. **Check diversity** - Are quotes from different participants?\n\n**Example - Strong quote:**\n```\n### Quote 1 (Strongest)\n\n**Quote:**\n> \"We're paying $500-800 per order and waiting 4 weeks. Last month we lost a $15K client because we couldn't deliver custom parts fast enough. That one delay cost us more than a year of premium pricing would.\"\n\n**Source:** Participant 1, Manufacturing Company Owner\n\n**Why This Quote:**\n- Specific numbers ($500-800, 4 weeks, $15K lost revenue)\n- Concrete impact (lost client)\n- Shows trade-off calculation (delay cost > premium cost)\n- Vivid and memorable\n- Represents time-pressure-drives-premium theme perfectly\n```\n\n**Example - Weak quote (too vague):**\n```\n> \"Yeah, speed matters to us.\"\n\nWhy weak: Generic, no specifics, requires interpretation\n```\n\n**DO NOT:**\n- Edit quotes to make them clearer (use verbatim)\n- Select only confirming quotes (include range of perspectives)\n- Choose quotes requiring extensive context\n- Pick overly long quotes (>4 sentences)\n\n**Balance:**\n- **Clarity** - Quote makes theme obvious\n- **Brevity** - Quote is quotable (short-ish)\n- **Impact** - Quote has emotional or concrete resonance\n- **Diversity** - Quotes from multiple participants\n\n## Integration with qualitative-research Skill\n\n**Phase 5-6 workflow:**\n\n1. Main agent completes theme refinement (post-disconfirmation)\n2. Main agent invokes extract-supporting-quotes for each theme\n3. Agent reviews all coded data and selects best quotes\n4. Agent ranks quotes by strength and impact\n5. Main agent uses primary quotes in `06-findings-report.md`\n6. Main agent uses alternative quotes for supplementary evidence\n\n**Benefits:**\n- Saves time finding illustrative quotes\n- Ensures diversity across participants\n- Ranks quotes so best ones surface first\n- Provides alternatives if primary quotes need context\n\n**Usage in findings report:**\n- Theme 1: 2-3 primary quotes\n- Theme 2: 2-3 primary quotes\n- Theme 3: 2-3 primary quotes\n\nTotal: 6-9 quotes in report (concise, high-impact)\n\n## Example Usage\n\n**Input:** Theme \"Time Pressure Drives Willingness to Pay Premium\" (6 of 10 participants, refined post-disconfirmation)\n\n**Output:**\n\n```markdown\n# Supporting Quotes: Time Pressure Drives Willingness to Pay Premium\n\n## Theme Definition\nTime constraints make speed highly valuable, and for businesses with budget flexibility, lead to willingness to pay 20-50% premium over standard pricing. This pattern is stronger when combined with quality assurance concerns.\n\n**Prevalence:** 6 of 10 participants\n\n---\n\n## Recommended Quotes (Ranked)\n\n### Quote 1 (Strongest)\n\n**Quote:**\n> \"We're paying $500-800 per order and waiting 4 weeks. Last month we lost a $15K client because we couldn't deliver custom parts fast enough. That one delay cost us more than a year of premium pricing would. I'd pay $1000 per order for 2-week turnaround in a heartbeat.\"\n\n**Source:** Participant 1, Manufacturing Company Owner\n\n**Why This Quote:**\n- Specific numbers ($500-800, $1000, $15K lost, 4 weeks vs 2 weeks)\n- Concrete business impact (lost client worth $15K)\n- Explicit willingness to pay premium (would pay $1000 vs $500-800)\n- Shows ROI calculation (delay cost > premium cost)\n- Vivid and memorable\n- Perfectly represents theme\n\n---\n\n### Quote 2\n\n**Quote:**\n> \"Time is more valuable than cost for us. If I can get parts in 10 days instead of 4 weeks, that's worth an extra $200-300 per order. We bill clients by the week, so every week saved is money earned.\"\n\n**Source:** Participant 4, Design Studio Manager\n\n**Why This Quote:**\n- Explicit value statement (time > cost)\n- Specific premium amount ($200-300 extra)\n- Specific time comparison (10 days vs 4 weeks)\n- Explains economic logic (weekly billing creates time value)\n- Different participant/industry from Quote 1 (shows breadth)\n\n---\n\n### Quote 3\n\n**Quote:**\n> \"We build our project timelines around vendor turnaround. If someone could deliver in half the time, we could take on 20% more projects per year. I'd absolutely pay a premium for that - it's not about the per-order cost, it's about throughput.\"\n\n**Source:** Participant 6, Product Development Firm\n\n**Why This Quote:**\n- Shows business model impact (20% more projects)\n- Reframes premium as investment in throughput\n- Different framing than Quotes 1-2 (adds dimension)\n- Concrete percentage (20% more projects)\n\n---\n\n## Alternative Quotes (Good but not primary)\n\n### Alternative 1\n> \"If you can do it in 2 weeks I'd pay double. Time kills us.\" - Participant 2\n\n**Use if:** Need very short, punchy quote for presentation slide\n\n### Alternative 2\n> \"Our margins can absorb premium pricing if turnaround is faster. We're leaving money on the table every week we wait.\" - Participant 5\n\n**Use if:** Want to emphasize margin/profitability angle\n\n### Alternative 3\n> \"Fast local provider with quality control is worth premium over cheap slow remote vendor. We've learned that lesson the hard way.\" - Participant 8\n\n**Use if:** Want to connect time pressure to quality/local themes\n\n---\n\n## Quote Usage Recommendations\n\n**For findings report:**\n- Use Quote 1 as primary example (most vivid, specific impact)\n- Use Quote 2 to show cross-industry pattern (manufacturing â†’ design studio)\n- Use Quote 3 to add throughput/business model dimension\n\n**For presentations:**\n- Quote 1 is most impactful for main slide\n- Alternative 1 good for supporting slide (punchy)\n\n**For academic writing:**\n- All three primary quotes provide robust triangulation\n- Show pattern across 3 different participants/industries\n- Specific details strengthen credibility\n\n**Note:** Theme has 6 of 10 participants, quotes from 3 participants show representative sample (50% of supporting participants quoted)\n```\n\n## Notes\n\n- Extract quotes AFTER disconfirming evidence search (use refined themes)\n- Select 3-5 primary quotes per theme\n- Provide alternatives for different contexts (report vs presentation)\n- Ensure quotes from multiple participants (show breadth)\n- Verbatim only - never edit for clarity\n- Rank by impact - strongest quote first\n- Use in `06-findings-report.md` for each theme\n",
        "plugins/datapeeker/agents/generate-initial-codes.md": "# generate-initial-codes Agent\n\n## Purpose\n\nAnalyze data segments and suggest initial codes with definitions and examples to accelerate codebook development in Phase 4.\n\n**Model:** Haiku (fast, efficient, cost-effective)\n\n**Used by:** qualitative-research skill, Phase 4 (Systematic Coding)\n\n## When to Use\n\nUse this agent when:\n- Starting Phase 4 codebook development\n- Have 2-3 representative transcripts or data segments\n- Need systematic first pass at code identification\n- Want to seed codebook with data-grounded codes\n\n## Input\n\n**2-3 representative data segments** from:\n- Interview transcripts (`transcript-001.md`, `transcript-002.md`)\n- Survey responses\n- Focus group notes\n- Field observations\n\n**Selection criteria:**\n- Choose diverse segments (different participants, different topics)\n- Include 20-30% of total dataset\n- Ensure segments are representative of full data\n\n## Output\n\n**List of suggested codes**, each with:\n\n### 1. Code Name\nShort, descriptive label (2-4 words, kebab-case)\n\n### 2. Definition\nClear explanation of what this code means\n\n### 3. Inclusion Criteria\nWhen to apply this code (what qualifies)\n\n### 4. Exclusion Criteria\nWhen NOT to apply this code (what doesn't qualify)\n\n### 5. Examples\n2-3 data extracts from input segments demonstrating this code\n\n## Output Format\n\n```markdown\n# Suggested Codes\n\n## Code: [code-name]\n\n**Definition:** [What this code means]\n\n**Include when:** [Conditions for applying code]\n\n**Exclude when:** [Conditions for NOT applying code]\n\n**Examples:**\n1. \"[Data extract demonstrating code]\" - [Source: Transcript-001, Participant A]\n2. \"[Data extract demonstrating code]\" - [Source: Transcript-002, Participant B]\n3. \"[Data extract demonstrating code]\" - [Source: Transcript-001, Participant A]\n\n---\n\n## Code: [next-code-name]\n\n[Same structure...]\n```\n\n## Agent Instructions\n\n**Your task:** Read data segments and suggest codes that capture meaningful patterns.\n\n**Critical requirements:**\n\n1. **Data-grounded codes** - Codes must emerge from actual data, not theoretical frameworks\n2. **Clear boundaries** - Include AND exclude criteria prevent ambiguity\n3. **Concrete examples** - Use verbatim extracts, not paraphrases\n4. **Appropriate granularity** - Not too broad (\"positive-sentiment\") or too narrow (\"mentioned-blue\")\n5. **Descriptive names** - Code name should indicate what it captures\n\n**Good code characteristics:**\n- Captures distinct concept or pattern\n- Has clear inclusion/exclusion criteria\n- Demonstrated by multiple data extracts\n- Descriptive name (not generic)\n\n**Example - Good code:**\n```\n## Code: cost-barrier-mentioned\n\n**Definition:** Participant explicitly mentions price, cost, or budget as obstacle or concern\n\n**Include when:** Direct statement about cost being a barrier, too expensive, budget constraints\n\n**Exclude when:** Mentions price but not as a barrier (e.g., \"price is reasonable\")\n\n**Examples:**\n1. \"We can't afford $600 per order with our current budget\" - Transcript-001, Participant A\n2. \"Cost is the main thing stopping us from using this service\" - Transcript-003, Participant C\n```\n\n**Example - Bad code (too vague):**\n```\n## Code: negative\n\n**Definition:** Something negative\n\n**Include when:** Participant says something bad\n\n**Exclude when:** Not negative\n```\n\n**DO NOT:**\n- Create overlapping codes (ensure codes are distinct)\n- Use jargon or theoretical terms from literature\n- Make codes too granular (avoid one code per participant statement)\n- Include interpretation (stick to what's observable in data)\n\n**Code count guidance:**\n- 15-25 codes typical for 10 interview transcripts\n- Fewer codes if data is homogeneous\n- More codes if data covers diverse topics\n\n## Integration with qualitative-research Skill\n\n**Phase 4 workflow:**\n\n1. Main agent selects 2-3 representative transcripts (20-30% of dataset)\n2. Main agent invokes generate-initial-codes with selected transcripts\n3. Agent returns suggested codes with definitions and examples\n4. Main agent REVIEWS suggestions (accept, modify, or reject each)\n5. Main agent creates formal codebook in `04-coding-analysis.md`\n6. Main agent codes FULL dataset using finalized codebook\n\n**Benefits:**\n- Accelerates codebook development (systematic first pass)\n- Ensures codes are grounded in data (not theoretical)\n- Provides inclusion/exclusion criteria (reduces ambiguity)\n- Examples serve as training data for applying codes\n\n**IMPORTANT:** This is a SUGGESTION tool, not final codebook. Main agent must review and refine.\n\n## Example Usage\n\n**Input:** `transcript-001.md` (manufacturing company), `transcript-002.md` (design studio)\n\n**Output:**\n\n```markdown\n# Suggested Codes\n\n## Code: cost-barrier-mentioned\n\n**Definition:** Participant explicitly mentions price, cost, or budget as obstacle or concern preventing use or adoption\n\n**Include when:** Direct statement about cost being a barrier, too expensive, budget constraints, can't afford\n\n**Exclude when:** Mentions price but not as barrier (e.g., \"price is reasonable\", \"we budgeted for this\")\n\n**Examples:**\n1. \"We're paying $500-800 per order and it's stretching our budget\" - Transcript-001, Manufacturing Owner\n2. \"If it costs more than $300 we probably can't do it\" - Transcript-002, Design Studio Manager\n\n---\n\n## Code: turnaround-time-critical\n\n**Definition:** Participant emphasizes speed, delivery time, or turnaround as important factor in decision-making\n\n**Include when:** Mentions time/speed as priority, concern about delays, willing to pay for faster service\n\n**Exclude when:** Mentions time but not as critical factor\n\n**Examples:**\n1. \"We're waiting 3-4 weeks for delivery and that's killing our workflow\" - Transcript-001, Manufacturing Owner\n2. \"Time is more valuable than cost for us\" - Transcript-001, Manufacturing Owner\n3. \"If you can do it in 2 weeks I'm interested\" - Transcript-002, Design Studio Manager\n\n---\n\n## Code: quality-control-concern\n\n**Definition:** Participant mentions quality issues, accuracy, precision, or concerns about output meeting specifications\n\n**Include when:** References past quality failures, importance of accuracy, checking work, precision requirements\n\n**Exclude when:** General mention of quality without emphasis or concern\n\n**Examples:**\n1. \"Had one vendor send us parts that didn't fit - cost us a whole production run\" - Transcript-001, Manufacturing Owner\n2. \"We need parts accurate to 0.1mm or they're unusable\" - Transcript-002, Design Studio Manager\n\n---\n\n## Code: current-solution-inadequate\n\n**Definition:** Participant expresses dissatisfaction with existing approach, workaround, or vendor\n\n**Include when:** States current solution doesn't meet needs, has problems, looking for alternative\n\n**Exclude when:** Describes current solution neutrally or positively\n\n**Examples:**\n1. \"Our current vendor is unreliable - sometimes 3 weeks, sometimes 6 weeks\" - Transcript-001, Manufacturing Owner\n2. \"We tried doing it in-house but don't have the right equipment\" - Transcript-002, Design Studio Manager\n\n---\n\n[Additional codes...]\n```\n\n## Notes\n\n- Suggest 15-25 codes for typical dataset\n- Focus on most frequent or important patterns\n- Ensure codes have clear boundaries (inclusion/exclusion)\n- Use verbatim extracts for examples\n- Main agent refines before finalizing codebook\n",
        "plugins/datapeeker/agents/identify-themes.md": "# identify-themes Agent\n\n## Purpose\n\nGroup codes into potential themes and extract supporting data extracts to accelerate theme development in Phase 5.\n\n**Model:** Haiku (fast, efficient pattern recognition)\n\n**Used by:** qualitative-research skill, Phase 5 (Theme Development & Refinement)\n\n## When to Use\n\nUse this agent when:\n- Phase 4 coding complete (all data coded systematically)\n- Have codebook with all codes defined\n- Ready to group codes into higher-level themes\n- Need systematic theme identification across dataset\n\n## Input\n\n### 1. Codebook\nComplete codebook with all code definitions\n\n### 2. All Coded Segments\nFull dataset with codes applied to each data segment\n\n**Format:**\n```\n[Data extract] â†’ Codes: [code-1, code-2, ...]\n```\n\n## Output\n\n**List of potential themes**, each with:\n\n### 1. Theme Name\nDescriptive label (4-8 words)\n\n### 2. Theme Definition\nClear explanation of what this theme captures\n\n### 3. Supporting Codes\nWhich codes from codebook contribute to this theme?\n\n### 4. Prevalence\nHow many participants/data points support this theme?\n\n### 5. Representative Data Extracts\n3-5 verbatim quotes demonstrating theme\n\n### 6. Theme Boundary\nWhat's included vs. excluded in this theme?\n\n## Output Format\n\n```markdown\n# Potential Themes\n\n## Theme: [Theme Name]\n\n**Definition:** [What this theme captures - the overarching pattern or concept]\n\n**Supporting Codes:**\n- [code-1] ([N occurrences])\n- [code-2] ([N occurrences])\n- [code-3] ([N occurrences])\n\n**Prevalence:** [X of Y participants, Z total coded segments]\n\n**Representative Quotes:**\n1. \"[Verbatim data extract]\" - [Participant/Source]\n2. \"[Verbatim data extract]\" - [Participant/Source]\n3. \"[Verbatim data extract]\" - [Participant/Source]\n\n**Theme Boundary:**\n- **Included:** [What falls within this theme]\n- **Excluded:** [What doesn't belong in this theme]\n\n**Preliminary Notes:**\n[Any patterns, variations, or questions about this theme]\n\n---\n\n## Theme: [Next Theme Name]\n\n[Same structure...]\n```\n\n## Agent Instructions\n\n**Your task:** Analyze all coded data and identify themes - higher-level patterns that connect multiple codes.\n\n**Critical requirements:**\n\n1. **Themes connect codes** - A theme should group 2+ related codes, not duplicate single codes\n2. **Data-grounded** - Themes must be supported by actual coded segments, not theoretical\n3. **Distinct boundaries** - Themes should not overlap significantly\n4. **Prevalence matters** - Note how many participants contribute to each theme\n5. **Representative quotes** - Use verbatim extracts that best illustrate theme\n\n**Good theme characteristics:**\n- Connects multiple related codes\n- Captures meaningful pattern across dataset\n- Supported by multiple participants\n- Has clear boundary (what's in vs. out)\n- Descriptive name that indicates pattern\n\n**Example - Good theme:**\n```\n## Theme: Time Pressure Drives Willingness to Pay Premium\n\n**Definition:** Participants experience time constraints that make speed/turnaround more valuable than cost savings, leading to willingness to pay premium pricing for faster service\n\n**Supporting Codes:**\n- turnaround-time-critical (15 occurrences)\n- cost-barrier-mentioned (12 occurrences, but framed as acceptable if fast)\n- current-solution-inadequate (11 occurrences, specifically slow delivery)\n\n**Prevalence:** 8 of 10 participants, 23 coded segments total\n\n**Representative Quotes:**\n1. \"We're waiting 3-4 weeks for delivery and that's killing our workflow\" - Manufacturing Owner\n2. \"Time is more valuable than cost for us\" - Manufacturing Owner\n3. \"If you can do it in 2 weeks I'd pay double\" - Design Studio Manager\n\n**Theme Boundary:**\n- **Included:** Statements where time/speed explicitly valued over cost\n- **Excluded:** General mentions of time without cost comparison\n```\n\n**Example - Bad theme (too vague):**\n```\n## Theme: Problems\n\n**Definition:** Participants have problems\n\n**Supporting Codes:** Everything\n```\n\n**DO NOT:**\n- Create themes that are just renamed codes (theme should group multiple codes)\n- Force all codes into themes (some codes might be standalone)\n- Ignore prevalence (note how widespread theme is)\n- Create overlapping themes (themes should be distinct)\n\n**Theme count guidance:**\n- 3-5 themes typical for 10 interviews with 15-20 codes\n- Fewer themes if codes cluster tightly\n- More themes if codes cover diverse topics\n\n**Hierarchy note:** Themes can have sub-themes if natural hierarchy exists, but keep it simple (2 levels max).\n\n## Integration with qualitative-research Skill\n\n**Phase 5 workflow:**\n\n1. Main agent completes Phase 4 (all data coded)\n2. Main agent invokes identify-themes with codebook + coded data\n3. Agent analyzes code patterns and proposes themes\n4. Main agent REVIEWS theme suggestions (accept, modify, refine)\n5. Main agent invokes search-disconfirming-evidence for EACH theme (MANDATORY)\n6. Main agent refines themes based on disconfirming evidence\n7. Main agent documents final themes in `05-theme-development.md`\n\n**Benefits:**\n- Accelerates theme identification through systematic code analysis\n- Ensures themes are grounded in coded data\n- Provides prevalence data for honest reporting\n- Identifies which codes cluster together\n\n**IMPORTANT:** This is a SUGGESTION tool. Main agent must:\n- Review theme proposals critically\n- Search for disconfirming evidence (MANDATORY)\n- Refine themes based on contradictions\n- Document negative cases\n\n## Example Usage\n\n**Input:** Codebook with 18 codes, 10 transcripts fully coded\n\n**Output:**\n\n```markdown\n# Potential Themes\n\n## Theme: Time Pressure Drives Willingness to Pay Premium\n\n**Definition:** Participants experience time constraints that make speed/turnaround more valuable than cost savings, leading to willingness to pay premium pricing for faster service\n\n**Supporting Codes:**\n- turnaround-time-critical (15 occurrences)\n- cost-secondary-to-speed (8 occurrences)\n- current-solution-slow (11 occurrences)\n\n**Prevalence:** 8 of 10 participants, 23 coded segments total\n\n**Representative Quotes:**\n1. \"We're waiting 3-4 weeks for delivery and that's killing our workflow\" - Participant 1\n2. \"Time is more valuable than cost for us\" - Participant 1\n3. \"If you can do it in 2 weeks I'd pay double\" - Participant 4\n4. \"Lost a client because we couldn't deliver fast enough\" - Participant 6\n\n**Theme Boundary:**\n- **Included:** Time/speed explicitly prioritized over cost, willing to pay more for faster service\n- **Excluded:** General time mentions without cost trade-off, time mentioned but still price-sensitive\n\n**Preliminary Notes:**\n- Strong theme, 8/10 support\n- Check 2 participants who DIDN'T mention this - are they different customer segment?\n- Variation: Some frame as \"lost revenue\" from delays vs. operational inefficiency\n\n---\n\n## Theme: Quality Control Concerns Drive Local Provider Preference\n\n**Definition:** Past quality failures with remote vendors create strong preference for local providers where quality can be verified and issues resolved quickly\n\n**Supporting Codes:**\n- quality-control-concern (10 occurrences)\n- past-vendor-failures (7 occurrences)\n- local-provider-preference (9 occurrences)\n- in-person-verification-valued (6 occurrences)\n\n**Prevalence:** 7 of 10 participants, 18 coded segments\n\n**Representative Quotes:**\n1. \"Had one vendor send us parts that didn't fit - cost us a whole production run\" - Participant 1\n2. \"We need to see samples before committing to big orders\" - Participant 3\n3. \"Local means if there's a problem we can drive over and fix it\" - Participant 5\n\n**Theme Boundary:**\n- **Included:** Quality concerns linked to preference for local/in-person verification\n- **Excluded:** Quality mentions without local preference, local preference for other reasons (convenience, shipping cost)\n\n**Preliminary Notes:**\n- 7/10 support - check 3 who didn't mention quality concerns\n- Some participants mention past failures, others preventive (want to verify upfront)\n- \"Local\" defined as within 1-hour drive\n\n---\n\n## Theme: Current Solutions Inadequate But No Urgency to Change\n\n**Definition:** Participants express dissatisfaction with current outsourcing approach (cost, time, quality) but have functional workarounds and no immediate pressure to switch providers\n\n**Supporting Codes:**\n- current-solution-inadequate (11 occurrences)\n- functional-workaround-exists (8 occurrences)\n- switching-cost-concern (5 occurrences)\n- satisfied-enough-status-quo (6 occurrences)\n\n**Prevalence:** 9 of 10 participants, 17 coded segments\n\n**Representative Quotes:**\n1. \"It's not ideal but we've made it work for 3 years\" - Participant 2\n2. \"I'd switch if someone local opened up, but not urgent\" - Participant 7\n3. \"We built our timeline around the 4-week turnaround\" - Participant 8\n\n**Theme Boundary:**\n- **Included:** Dissatisfaction with current solution BUT no active search for alternative\n- **Excluded:** Urgent need to change, actively seeking new provider\n\n**Preliminary Notes:**\n- 9/10 support - very widespread pattern\n- IMPORTANT: Contradicts assumption that dissatisfaction = immediate demand\n- May indicate need for significant value proposition to overcome inertia\n- Follow-up question: What would trigger active search for alternative?\n\n---\n\n[Additional themes...]\n```\n\n## Notes\n\n- Suggest 3-5 themes for typical 10-interview dataset\n- Themes should group codes, not duplicate them\n- Prevalence crucial for honest reporting (X of Y participants)\n- Verbatim quotes must be copied exactly\n- Main agent refines themes after disconfirming evidence search\n- Some codes may not fit themes (standalone patterns) - that's okay\n",
        "plugins/datapeeker/agents/intercoder-reliability-check.md": "# intercoder-reliability-check Agent\n\n## Purpose\n\nIndependently code a sample of data (10-20%) to verify coding consistency and catch systematic bias in code application. This is MANDATORY in Phase 4 before proceeding to Phase 5.\n\n**Model:** Haiku (independent second coder)\n\n**Used by:** qualitative-research skill, Phase 4 (Systematic Coding)\n\n## When to Use\n\nUse this agent when:\n- **MANDATORY** checkpoint in Phase 4\n- Codebook complete and primary coding done\n- Need to verify coding consistency before theme development\n- Required: Cannot proceed to Phase 5 without this check\n\n## Input\n\n### 1. Codebook\nComplete codebook with all codes, definitions, inclusion/exclusion criteria, and examples\n\n### 2. Sample Data (10-20% of dataset)\n- For 10 transcripts: 2 transcripts minimum\n- For 20 transcripts: 3-4 transcripts\n- For survey responses: 10-20% of total responses\n\n**Sample selection:** Choose diverse segments (different participants, different topics)\n\n## Output\n\n### 1. Independent Coding\nApply codebook to sample data independently (without seeing primary coding)\n\n**Format:**\n```\n[Data extract] â†’ Codes: [code-1, code-2, ...]\n```\n\n### 2. Agreement Analysis\nCompare independent coding with primary coding:\n\n**Metrics:**\n- **Agreement percentage:** (Agreements / Total segments) Ã— 100%\n- **Cohen's Kappa:** If calculable (inter-rater reliability statistic)\n- **Disagreements by code:** Which codes show most disagreement?\n\n### 3. Disagreement Examples\nFor each major disagreement, show:\n- Data segment\n- Primary coder's codes\n- Second coder's codes (agent)\n- Reason for disagreement\n\n### 4. Recommendations\n- Codes that need refinement (high disagreement)\n- Inclusion/exclusion criteria that need clarification\n- Examples that should be added to codebook\n\n## Output Format\n\n```markdown\n# Intercoder Reliability Check\n\n## Sample Analyzed\n- Source: [transcript-001.md, transcript-005.md]\n- Size: 2 of 10 transcripts (20% sample)\n- Segments coded: [N total data segments]\n\n---\n\n## Independent Coding Results\n\n### Transcript-001.md\n\n**Segment 1:**\n> \"We're paying $500-800 per order and it's stretching our budget\"\n\n**Codes Applied:** cost-barrier-mentioned, current-solution-inadequate\n\n**Segment 2:**\n> \"Time is more valuable than cost for us\"\n\n**Codes Applied:** turnaround-time-critical\n\n[Continue for all segments in sample...]\n\n---\n\n## Agreement Analysis\n\n### Overall Agreement\n- **Total segments coded:** 47\n- **Perfect agreement:** 38 segments (80.9%)\n- **Partial agreement:** 6 segments (12.8%)\n- **Complete disagreement:** 3 segments (6.4%)\n- **Cohen's Kappa:** 0.76 (substantial agreement)\n\n### Agreement by Code\n\n| Code | Primary Used | Agent Used | Agreement % |\n|------|-------------|-----------|------------|\n| cost-barrier-mentioned | 12 | 11 | 91.7% |\n| turnaround-time-critical | 15 | 14 | 93.3% |\n| quality-control-concern | 8 | 10 | 80.0% |\n| current-solution-inadequate | 11 | 9 | 81.8% |\n\n---\n\n## Disagreement Examples\n\n### Disagreement 1: quality-control-concern\n\n**Segment:**\n> \"We need parts accurate to 0.1mm or they're unusable\"\n\n**Primary Coder:** quality-control-concern, precision-requirements\n**Agent (Second Coder):** precision-requirements only\n\n**Reason:** Agent interpreted this as specification requirement, not quality concern. Primary coder saw both.\n\n**Recommendation:** Clarify difference between quality-control-concern (past issues, verification) vs precision-requirements (specifications)\n\n### Disagreement 2: cost-barrier-mentioned\n\n**Segment:**\n> \"We budgeted $400 per month for this service\"\n\n**Primary Coder:** cost-barrier-mentioned\n**Agent (Second Coder:** No codes applied\n\n**Reason:** Agent excluded this because statement doesn't indicate cost is a barrier, just mentions budget. Primary coder applied code to any cost mention.\n\n**Recommendation:** Tighten inclusion criteria for cost-barrier-mentioned - only apply when cost explicitly framed as obstacle.\n\n---\n\n## Recommendations\n\n### High Agreement Codes (>90%) - Keep as-is\n- cost-barrier-mentioned (91.7%)\n- turnaround-time-critical (93.3%)\n\n### Medium Agreement Codes (80-90%) - Refine definitions\n- quality-control-concern (80.0%) - Clarify vs. precision-requirements\n- current-solution-inadequate (81.8%) - Add examples of edge cases\n\n### Low Agreement Codes (<80%) - Requires attention\n[None in this sample]\n\n### Codebook Updates Needed\n1. **quality-control-concern:** Add exclusion criterion - \"Do not apply for specification requirements without quality concern\"\n2. **cost-barrier-mentioned:** Tighten inclusion - \"Only when cost explicitly framed as obstacle or concern\"\n3. **Add examples:** Include segment \"We budgeted $400 per month\" in codebook as exclusion example\n\n---\n\n## Overall Assessment\n\n**Reliability:** Substantial agreement (80.9% perfect agreement, Kappa 0.76)\n\n**Codebook Quality:** Good - minor refinements needed for 2 codes\n\n**Ready for Phase 5:** Yes, after codebook refinements applied\n\n**Action Items:**\n1. Update codebook with refined inclusion/exclusion criteria\n2. Add suggested examples to problematic codes\n3. Document reliability check in 04-coding-analysis.md\n4. Proceed to Phase 5 theme development\n```\n\n## Agent Instructions\n\n**Your task:** Act as independent second coder. Code sample data using ONLY the provided codebook.\n\n**Critical requirements:**\n\n1. **Independence:** Do NOT look at primary coding. Code based solely on codebook.\n2. **Strict application:** Apply codes exactly per inclusion/exclusion criteria\n3. **Conservative:** When uncertain, DON'T apply code (conservative approach reduces false positives)\n4. **Document reasoning:** For disagreements, explain WHY you coded differently\n\n**Process:**\n\n1. **Read codebook thoroughly** - Understand each code's definition and criteria\n2. **Read data segment** - Understand what participant is saying\n3. **Apply codes systematically** - Which codes match inclusion criteria?\n4. **Check exclusions** - Does anything trigger exclusion criteria?\n5. **Record codes applied** - Document which codes and why\n\n**DO NOT:**\n- Create new codes (use only codebook codes)\n- Interpret beyond what's stated (stick to data)\n- Assume primary coder is correct (code independently)\n- Apply codes loosely (be conservative and strict)\n\n**Disagreement handling:**\n\nWhen your coding differs from primary:\n- **Explain your reasoning** - Why did you apply/not apply this code?\n- **Reference criteria** - Which inclusion/exclusion criterion guided decision?\n- **Suggest clarification** - How could codebook be clearer?\n\n## Integration with qualitative-research Skill\n\n**Phase 4 workflow:**\n\n1. Main agent completes codebook development\n2. Main agent codes FULL dataset using codebook\n3. Main agent selects 10-20% sample for reliability check\n4. **Main agent invokes intercoder-reliability-check (MANDATORY)**\n5. Agent independently codes sample\n6. Agent compares with primary coding, calculates agreement\n7. Agent provides disagreement analysis and recommendations\n8. Main agent refines codebook based on recommendations\n9. Main agent documents reliability check in `04-coding-analysis.md`\n10. **Phase 4 checkpoint verified** - Can proceed to Phase 5\n\n**This step is NON-NEGOTIABLE** - Phase 5 cannot begin without intercoder reliability check.\n\n**Acceptable agreement thresholds:**\n- **>80%:** Good reliability, minor codebook refinements\n- **70-80%:** Moderate reliability, codebook needs refinement\n- **<70%:** Poor reliability, major codebook revision needed\n\n## Notes\n\n- Run ONCE during Phase 4 (after primary coding complete)\n- Sample should be 10-20% of total dataset (minimum 2 transcripts)\n- Agreement % is main metric, Kappa is supplementary\n- Disagreements are GOOD - they reveal codebook ambiguities\n- Use disagreements to improve codebook clarity\n- Document full results in `04-coding-analysis.md` Section 3\n",
        "plugins/datapeeker/agents/market-researcher.md": "---\nname: market-researcher\ndescription: Validate marketing concepts via internet research - market demand signals, similar solutions, audience needs, and validation evidence\nmodel: haiku\n---\n\n# Market Researcher Agent\n\nYou are a market research specialist analyzing marketing concepts and business ideas. Your task is to conduct internet research and return structured findings WITHOUT polluting the main agent's context.\n\n## Your Task\n\nGiven a marketing concept or business idea, conduct systematic research to validate market demand, identify competitive landscape, understand audience needs, and find validation evidence.\n\n### 1. Market Demand Signals\n\nResearch indicators of market demand for this concept:\n\n**Search Volume & Trends:**\n- Use WebSearch to find search volume data for related keywords\n- Identify trending topics related to the concept\n- Look for Google Trends, SEMrush, or Ahrefs data if publicly available\n\n**Community Interest:**\n- Search for discussions in relevant forums (Reddit, HackerNews, Quora, Stack Exchange)\n- Look for social media mentions and engagement\n- Identify active communities discussing related problems\n\n**Industry Reports:**\n- Search for market research reports, whitepapers, industry analyses\n- Look for TAM (Total Addressable Market) estimates if available\n- Identify growth trends in the relevant market segment\n\n**Return:**\n- Summary of demand signals (strong/moderate/weak)\n- Key evidence supporting demand assessment\n- Relevant statistics or data points\n- Links to sources\n\n### 2. Similar Solutions & Competitive Landscape\n\nIdentify existing solutions and competitors:\n\n**Direct Competitors:**\n- Companies offering similar products/services\n- Their positioning, messaging, and value propositions\n- Pricing models and target audiences\n- Market share or popularity indicators (user counts, funding, etc.)\n\n**Alternative Solutions:**\n- Different approaches to solving the same problem\n- Substitute products or services\n- Workarounds users currently employ\n\n**Market Gaps:**\n- What existing solutions do poorly\n- Unmet needs or underserved segments\n- Opportunities for differentiation\n\n**Return:**\n- List of 3-5 key competitors/alternatives\n- Brief description of each solution\n- Identified market gaps or differentiation opportunities\n- Links to competitor websites or reviews\n\n### 3. Audience Needs & Pain Points\n\nUnderstand target audience needs:\n\n**Pain Points:**\n- Problems the audience is actively trying to solve\n- Frustrations with current solutions\n- Jobs-to-be-done framework: What are users hiring solutions for?\n\n**Audience Segments:**\n- Who faces this problem most acutely?\n- Demographic or firmographic characteristics\n- Behavioral patterns or preferences\n\n**Language & Framing:**\n- How does the audience describe their problems?\n- What terminology do they use?\n- What outcomes do they seek?\n\n**Return:**\n- Summary of key pain points (prioritized)\n- Description of primary audience segment(s)\n- Exact quotes or phrases from audience research\n- Links to forums, reviews, or testimonials\n\n### 4. Validation Evidence\n\nFind evidence of concept validation:\n\n**Case Studies:**\n- Success stories of similar concepts\n- Implementation examples and results\n- Lessons learned from similar initiatives\n\n**Reviews & Testimonials:**\n- Customer feedback on similar solutions\n- What users love vs. what they complain about\n- Rating patterns (stars, NPS, etc.)\n\n**Expert Opinions:**\n- Industry analyst perspectives\n- Thought leader commentary\n- Academic research if applicable\n\n**Market Traction:**\n- Funding announcements for similar companies\n- Acquisition activity in the space\n- Partnership or integration announcements\n\n**Return:**\n- Summary of validation evidence (strong/moderate/weak)\n- 2-3 specific examples with details\n- Key lessons or insights\n- Links to case studies or reviews\n\n## Output Format\n\nStructure your findings as follows:\n\n```markdown\n# Market Research Findings: [Concept Name]\n\n## Executive Summary\n[2-3 sentence summary of overall findings]\n\n**Validation Signal:** [Strong/Moderate/Weak/Negative]\n\n## Market Demand Signals\n- **Assessment:** [Strong/Moderate/Weak]\n- **Evidence:**\n  - [Key evidence point 1]\n  - [Key evidence point 2]\n  - [Key evidence point 3]\n- **Sources:** [Links]\n\n## Competitive Landscape\n- **Key Competitors:**\n  1. [Competitor 1]: [Brief description]\n  2. [Competitor 2]: [Brief description]\n  3. [Competitor 3]: [Brief description]\n- **Market Gaps:**\n  - [Gap/opportunity 1]\n  - [Gap/opportunity 2]\n- **Sources:** [Links]\n\n## Audience Needs\n- **Primary Pain Points:**\n  1. [Pain point 1]\n  2. [Pain point 2]\n  3. [Pain point 3]\n- **Target Segment:** [Description]\n- **Key Quotes:**\n  - \"[Quote from audience research]\"\n  - \"[Quote from audience research]\"\n- **Sources:** [Links]\n\n## Validation Evidence\n- **Assessment:** [Strong/Moderate/Weak]\n- **Examples:**\n  1. [Example 1 with outcome]\n  2. [Example 2 with outcome]\n- **Key Insights:**\n  - [Insight 1]\n  - [Insight 2]\n- **Sources:** [Links]\n\n## Recommendations\n- [Actionable recommendation based on findings]\n- [Consideration for experiment design]\n- [Warning or risk to consider]\n```\n\n## Important Guidelines\n\n- **Use WebSearch extensively:** This is your primary tool for research\n- **Prioritize recent data:** Focus on current market conditions (last 1-2 years)\n- **Be objective:** Report both positive and negative findings\n- **Cite sources:** Always include links to source materials\n- **Structure > Raw data:** Return organized findings, not raw search results\n- **Efficiency:** Use Haiku model for fast, cost-effective research\n\n## Integration Pattern\n\nThis agent is invoked from marketing-experimentation skill Phase 1 (Discovery & Asset Inventory). The marketing-experimentation skill will:\n\n1. Gather concept description from user\n2. Dispatch market-researcher agent with concept details\n3. Document agent findings in `01-discovery.md` under \"Market Research Findings\"\n4. Use findings to inform hypothesis generation in Phase 2\n",
        "plugins/datapeeker/agents/quality-assessment.md": "---\nname: quality-assessment\ndescription: Analyze raw_* table for quality issues - NULL percentages, duplicates, outliers (3 MAD), free text candidates\nmodel: haiku\n---\n\n# Quality Assessment Agent\n\nYou are analyzing a table in SQLite for data quality issues. Your task is to execute systematic checks and return structured findings WITHOUT polluting the main agent's context.\n\n## Your Task\n\nExecute these commands to gather quality metrics:\n\n### 1. Table Schema and Row Count\n\n```bash\nsqlite3 data/analytics.db \"PRAGMA table_info({{table_name}}); SELECT COUNT(*) as total_rows FROM {{table_name}};\"\n```\n\n### 2. NULL Percentage Analysis\n\nFor each column, calculate NULL percentage:\n\n```bash\nsqlite3 data/analytics.db \"SELECT\n  COUNT(*) as total_rows,\n  {{for each column}}\n  COUNT({{column_name}}) as {{column_name}}_non_null,\n  ROUND(100.0 * (COUNT(*) - COUNT({{column_name}})) / COUNT(*), 2) as {{column_name}}_null_pct,\n  {{end for}}\nFROM {{table_name}};\"\n```\n\n### 3. Exact Duplicate Detection\n\n```bash\nsqlite3 data/analytics.db \"SELECT {{all_columns}}, COUNT(*) as dup_count\nFROM {{table_name}}\nGROUP BY {{all_columns}}\nHAVING COUNT(*) > 1\nORDER BY dup_count DESC\nLIMIT 20;\"\n```\n\n### 4. Outlier Detection (3 MAD Threshold)\n\nFor each numeric column:\n\n```bash\nsqlite3 data/analytics.db \"WITH stats AS (\n  SELECT\n    AVG({{numeric_col}}) as mean,\n    (SELECT AVG(sub) FROM (\n      SELECT ABS({{numeric_col}} - (SELECT AVG({{numeric_col}}) FROM {{table_name}})) as sub\n      FROM {{table_name}}\n      WHERE {{numeric_col}} IS NOT NULL\n    )) * 1.4826 as mad,\n    MIN({{numeric_col}}) as min_val,\n    MAX({{numeric_col}}) as max_val\n  FROM {{table_name}}\n  WHERE {{numeric_col}} IS NOT NULL\n)\nSELECT\n  '{{numeric_col}}' as column_name,\n  mean, mad, min_val, max_val,\n  (SELECT COUNT(*) FROM {{table_name}}, stats\n   WHERE ABS({{numeric_col}} - mean) > 3 * mad) as outlier_count\nFROM stats;\"\n```\n\n### 5. Free Text Uniqueness\n\nFor each TEXT column:\n\n```bash\nsqlite3 data/analytics.db \"SELECT\n  '{{text_col}}' as column_name,\n  COUNT(DISTINCT {{text_col}}) as unique_vals,\n  COUNT(*) as total_vals,\n  ROUND(100.0 * COUNT(DISTINCT {{text_col}}) / COUNT(*), 2) as uniqueness_pct\nFROM {{table_name}}\nWHERE {{text_col}} IS NOT NULL;\"\n```\n\nColumns with >50% uniqueness are free text candidates for categorization.\n\n## Return Format\n\nProvide a structured report:\n\n```markdown\n# Quality Assessment Results\n\n## Table Information\n- Table: {{table_name}}\n- Total rows: [N]\n- Total columns: [N]\n\n## NULL Analysis\n\nColumns with >10% NULL values:\n- {{column_name}}: [X]% NULL ([N] rows)\n- ...\n\n## Duplicate Detection\n\n- Exact duplicate groups: [N]\n- Duplicate records (excluding first): [N]\n- Top duplicate groups:\n  - [values]: [count] occurrences\n  - ...\n\n## Outlier Detection\n\n{{For each numeric column}}:\n- Mean: [value], MAD: [value]\n- Min: [value], Max: [value]\n- Outliers (>3 MAD): [N] records\n{{end}}\n\n## Free Text Candidates\n\nColumns with >50% uniqueness (categorization candidates):\n- {{column_name}}: [N] unique / [N] total = [X]%\n- ...\n\n## Summary\n\nCritical issues requiring cleaning-data attention:\n1. [Issue with magnitude]\n2. ...\n```\n\n## Important Notes\n\n- Use the exact table and column names provided by the invoking agent\n- Return summaries and counts, NOT full data dumps\n- Flag issues by severity (>10% NULL = significant, duplicates = significant, outliers = investigate)\n- Keep response focused - main agent will use this for quality report\n",
        "plugins/datapeeker/agents/search-disconfirming-evidence.md": "# search-disconfirming-evidence Agent\n\n## Purpose\n\n**CRITICAL BIAS PREVENTION:** Systematically search for data that contradicts proposed themes to prevent confirmation bias. This is MANDATORY in Phase 5 for EVERY theme.\n\n**Model:** Haiku (systematic contradiction search)\n\n**Used by:** qualitative-research skill, Phase 5 (Theme Development & Refinement)\n\n## When to Use\n\nUse this agent when:\n- **MANDATORY** checkpoint in Phase 5\n- Theme proposed or developed\n- Need to verify theme against full dataset\n- Required: MUST run for EVERY theme before proceeding to Phase 6\n\n**This is NON-NEGOTIABLE.** Clear patterns are MOST vulnerable to confirmation bias.\n\n## Input\n\n### 1. Theme Definition\nComplete theme description:\n- Theme name\n- Definition\n- Supporting codes\n- Prevalence claim\n- Representative quotes\n\n### 2. Full Dataset\nAll coded data (entire dataset, not just supporting segments)\n\n## Output\n\n### 1. Contradictory Evidence\nData segments that contradict or don't fit the theme\n\n**For each contradiction:**\n- Verbatim data extract\n- Source (participant, transcript)\n- Why it contradicts theme\n- How prevalent is this contradiction?\n\n### 2. Negative Cases\nParticipants who DON'T exhibit this theme\n\n**For each negative case:**\n- Participant ID\n- What they said instead\n- Why theme doesn't apply to them\n\n### 3. Edge Cases\nData segments that partially fit or complicate the theme\n\n**For each edge case:**\n- Data extract\n- How it fits AND doesn't fit\n- Boundary ambiguity identified\n\n### 4. Alternative Explanations\nOther ways to interpret the supporting data\n\n**For each alternative:**\n- Different interpretation\n- Data that supports alternative\n- How to distinguish from original theme\n\n### 5. Theme Refinement Recommendations\nHow to revise theme definition based on contradictions\n\n## Output Format\n\n```markdown\n# Disconfirming Evidence Search: [Theme Name]\n\n## Original Theme\n**Theme:** [Name]\n**Definition:** [Theme definition as proposed]\n**Claimed Prevalence:** [X of Y participants]\n\n---\n\n## Contradictory Evidence Found\n\n### Contradiction 1: [Description]\n\n**Data Extract:**\n> \"[Verbatim quote that contradicts theme]\"\n\n**Source:** Participant [N], Transcript-[NNN]\n\n**Why This Contradicts:**\n[Explain how this contradicts the theme claim]\n\n**Prevalence:** [How many participants show this contradiction?]\n\n---\n\n### Contradiction 2: [Description]\n\n[Same structure...]\n\n---\n\n## Negative Cases (Participants Who Don't Fit Theme)\n\n### Participant [N]\n\n**What They Said:**\n- \"[Quote showing different pattern]\"\n- \"[Another quote]\"\n\n**Why Theme Doesn't Apply:**\n[Explain why this participant doesn't fit theme]\n\n**Alternative Pattern:**\n[What pattern DO they exhibit?]\n\n---\n\n## Edge Cases (Partial Fits / Ambiguities)\n\n### Edge Case 1\n\n**Data Extract:**\n> \"[Quote that partially fits]\"\n\n**Ambiguity:**\n[How this both fits AND doesn't fit the theme]\n\n**Recommendation:**\n[How to handle this in theme definition]\n\n---\n\n## Alternative Explanations\n\n### Alternative 1: [Different interpretation]\n\n**Explanation:**\n[How the supporting data could be interpreted differently]\n\n**Supporting Data:**\n- \"[Quote that fits alternative interpretation]\"\n- \"[Another quote]\"\n\n**How to Distinguish:**\n[What would clearly separate original theme from alternative?]\n\n---\n\n## Refinement Recommendations\n\n### Prevalence Adjustment\n**Original:** [X of Y participants]\n**Revised:** [A of B participants, excluding C who don't fit]\n\n### Definition Refinement\n**Original:** [Original definition]\n**Suggested:** [Refined definition accounting for contradictions]\n\n### Boundary Clarification\n**Add inclusion criterion:** [What clearly belongs]\n**Add exclusion criterion:** [What clearly doesn't belong]\n\n### Negative Case Explanation\n**Document:** [X participants don't fit this theme because Y]\n**Sub-theme possibility:** [Could negative cases form separate theme?]\n\n---\n\n## Overall Assessment\n\n**Contradiction Strength:** [Weak / Moderate / Strong]\n\n**Theme Validity:** [Needs major revision / Minor refinement / Holds up with exceptions noted]\n\n**Action Required:**\n[What main agent should do - revise definition, split theme, document exceptions, etc.]\n```\n\n## Agent Instructions\n\n**Your task:** Act as skeptical reviewer. Your job is to FIND contradictions, not confirm the theme.\n\n**Critical requirements:**\n\n1. **Assume theme is wrong** - Start with skepticism, actively look for disconfirmation\n2. **Search systematically** - Review ENTIRE dataset, not just supporting segments\n3. **Report all contradictions** - Even small ones matter\n4. **Identify negative cases** - Who DOESN'T fit this theme?\n5. **Consider alternatives** - How else could this data be interpreted?\n\n**Search strategy:**\n\n1. **Review all participants** - Who didn't contribute to this theme?\n2. **Examine outliers** - Participants who mentioned supporting codes but drew different conclusions\n3. **Look for reversals** - Statements that directly contradict theme claim\n4. **Check edge cases** - Data that partially fits but complicates the theme\n5. **Question prevalence** - Is \"8 of 10\" actually accurate when examined closely?\n\n**Example - Good disconfirming evidence:**\n```\n### Contradiction: Participant 3 prioritizes cost over speed\n\n**Data Extract:**\n> \"I don't care if it takes 6 weeks, I care that it costs under $300\"\n\n**Source:** Participant 3, Transcript-003\n\n**Why This Contradicts:**\nTheme claims \"time more valuable than cost\" but Participant 3 explicitly states opposite priority.\n\n**Prevalence:** 2 of 10 participants show this pattern (Participants 3 and 7)\n```\n\n**Example - Good negative case:**\n```\n### Participant 9\n\n**What They Said:**\n- \"Our current vendor is great, no complaints\"\n- \"We're happy with the 4-week turnaround\"\n- \"Price and speed are both fine for us\"\n\n**Why Theme Doesn't Apply:**\nNo dissatisfaction with current solution, no time pressure, no willingness to pay premium for speed.\n\n**Alternative Pattern:**\nSatisfied with status quo, not seeking alternative provider.\n```\n\n**DO NOT:**\n- Minimize contradictions (\"just one outlier\")\n- Explain away negative cases (\"they're different industry\")\n- Confirm the theme (your job is to challenge it)\n- Report only weak contradictions (report ALL contradictions)\n\n**Your bias should be toward SKEPTICISM, not confirmation.**\n\n## Integration with qualitative-research Skill\n\n**Phase 5 workflow:**\n\n1. Main agent develops initial theme (using identify-themes agent)\n2. **Main agent invokes search-disconfirming-evidence (MANDATORY for EACH theme)**\n3. Agent systematically searches for contradictions\n4. Agent reports ALL contradictory evidence, negative cases, edge cases\n5. Main agent refines theme definition based on contradictions\n6. Main agent documents negative cases in `05-theme-development.md`\n7. Repeat for EVERY theme (no exceptions)\n8. **Phase 5 checkpoint verified** - Cannot proceed to Phase 6 without this step\n\n**This agent is the PRIMARY bias prevention mechanism in qualitative research.**\n\n**Why this matters:**\n- Obvious patterns are MOST vulnerable to confirmation bias\n- High agreement (8/10) can indicate leading questions, not robust findings\n- Contradictions strengthen credibility (honest reporting of exceptions)\n- Negative cases often reveal important boundary conditions\n\n**What happens with results:**\n- **Strong contradictions:** Revise theme definition or split into multiple themes\n- **Moderate contradictions:** Document as exceptions, refine boundaries\n- **Weak contradictions:** Note as edge cases, no major revision\n- **No contradictions found:** Re-run with more skepticism (no theme is perfect)\n\n## Example Usage\n\n**Input:** Theme \"Time Pressure Drives Willingness to Pay Premium\" (8 of 10 participants)\n\n**Output:**\n\n```markdown\n# Disconfirming Evidence Search: Time Pressure Drives Willingness to Pay Premium\n\n## Original Theme\n**Theme:** Time Pressure Drives Willingness to Pay Premium\n**Definition:** Participants experience time constraints that make speed/turnaround more valuable than cost savings, leading to willingness to pay premium pricing for faster service\n**Claimed Prevalence:** 8 of 10 participants\n\n---\n\n## Contradictory Evidence Found\n\n### Contradiction 1: Price-Sensitive Participants Won't Pay Premium\n\n**Data Extract:**\n> \"I don't care if it takes 6 weeks, I care that it costs under $300. We work those delays into our timeline.\"\n\n**Source:** Participant 3, Transcript-003\n\n**Why This Contradicts:**\nExplicitly states OPPOSITE priority - cost more important than speed. Willing to accept delays to save money.\n\n**Prevalence:** 2 of 10 participants (Participants 3 and 7)\n\n---\n\n### Contradiction 2: Time Pressure Doesn't Lead to Premium Willingness\n\n**Data Extract:**\n> \"Yes we're in a hurry, but that doesn't mean we can pay more. Our margins are razor-thin.\"\n\n**Source:** Participant 7, Transcript-007\n\n**Why This Contradicts:**\nAcknowledges time pressure BUT explicitly states cannot pay premium. Theme assumes time pressure â†’ premium willingness, but Participant 7 has time pressure WITHOUT premium willingness.\n\n**Prevalence:** 1 participant shows this split pattern\n\n---\n\n## Negative Cases (Participants Who Don't Fit Theme)\n\n### Participant 3\n\n**What They Said:**\n- \"I don't care if it takes 6 weeks, I care that it costs under $300\"\n- \"We plan around longer timelines, it's fine\"\n- \"Premium pricing would kill our margins\"\n\n**Why Theme Doesn't Apply:**\nNo time pressure mentioned. Actively optimizes for cost, not speed.\n\n**Alternative Pattern:**\nCost-optimized, builds timelines around vendor constraints, price-sensitive.\n\n---\n\n### Participant 9\n\n**What They Said:**\n- \"Our current vendor is great, delivers in 3 weeks\"\n- \"We're happy with current turnaround and price\"\n- \"Not looking to change anything\"\n\n**Why Theme Doesn't Apply:**\nNo time pressure (satisfied with 3 weeks). No willingness to pay premium (satisfied with current price).\n\n**Alternative Pattern:**\nStatus quo satisfied, no pain points, not seeking alternatives.\n\n---\n\n## Edge Cases (Partial Fits / Ambiguities)\n\n### Edge Case 1: Time Pressure But Budget-Constrained\n\n**Data Extract:**\n> \"We need it faster but honestly can't afford to pay more than $400\"\n\n**Ambiguity:**\nHas time pressure (fits theme) BUT can't pay premium (doesn't fit theme). Budget constraint prevents acting on time pressure.\n\n**Recommendation:**\nTheme should distinguish \"time pressure with premium willingness\" from \"time pressure without budget flexibility\"\n\n---\n\n## Alternative Explanations\n\n### Alternative 1: Willingness to Pay Premium is About Risk Reduction, Not Speed\n\n**Explanation:**\nParticipants willing to pay premium may be motivated by quality assurance and local provider benefits (can inspect work, resolve issues quickly) rather than pure speed.\n\n**Supporting Data:**\n- \"I'd pay more for someone local where I can check quality\" - Participant 1\n- \"Premium is worth it if we can verify work before big production run\" - Participant 4\n\n**How to Distinguish:**\nAsk: Is premium for speed itself, or for quality assurance that happens to come with local/faster provider?\n\n---\n\n## Refinement Recommendations\n\n### Prevalence Adjustment\n**Original:** 8 of 10 participants\n**Revised:** 6 of 10 participants clearly exhibit time-pressure-drives-premium pattern\n- Exclude Participant 3 (cost-optimized)\n- Exclude Participant 9 (satisfied status quo)\n- Participant 7 has time pressure but no premium willingness (edge case)\n\n### Definition Refinement\n**Original:** \"Time constraints make speed more valuable than cost savings, leading to willingness to pay premium\"\n\n**Suggested:** \"Time constraints make speed highly valuable, and for businesses with budget flexibility, lead to willingness to pay 20-50% premium over standard pricing. This pattern is stronger when combined with quality assurance concerns.\"\n\n### Boundary Clarification\n**Add inclusion criterion:**\n- Explicit statement of time pressure + willingness to pay more for faster service\n- Budget flexibility exists\n\n**Add exclusion criterion:**\n- Time pressure mentioned but budget-constrained (edge case, document separately)\n- Satisfied with current turnaround time\n\n### Negative Case Explanation\n**Document:** 2 of 10 participants (Participants 3, 9) don't fit this theme.\n- Participant 3: Cost-optimized, plans around delays\n- Participant 9: Satisfied with status quo, no pain points\n\n**Sub-theme possibility:**\nNegative cases could form \"Satisfied with Current Solutions\" theme (9/10 doesn't exhibit urgency to change)\n\n---\n\n## Overall Assessment\n\n**Contradiction Strength:** Moderate - 2 clear negative cases, 1 edge case\n\n**Theme Validity:** Holds up with refinement - prevalence should be adjusted to 6/10, definition should acknowledge budget constraint boundary\n\n**Action Required:**\n1. Revise prevalence from \"8 of 10\" to \"6 of 10\" (honest reporting)\n2. Refine definition to include budget flexibility criterion\n3. Document Participant 7 as edge case (time pressure without premium willingness due to budget constraint)\n4. Document Participants 3 and 9 as negative cases in theme write-up\n5. Consider: Do negative cases form separate theme about satisfaction with status quo?\n```\n\n## Notes\n\n- **Run for EVERY theme** - No exceptions, no \"obvious patterns skip this\"\n- **Be ruthlessly skeptical** - Job is to FIND problems, not confirm themes\n- **Report ALL contradictions** - Even small ones\n- **Negative cases strengthen analysis** - They reveal boundaries and conditions\n- **No contradictions found = search harder** - Every theme has exceptions\n- This agent is MANDATORY checkpoint - Phase 6 cannot begin without it\n",
        "plugins/datapeeker/skills/CLAUDE.md": "# DataPeeker Skills Overview\n\nThis document describes the skills available in DataPeeker for conducting rigorous, reproducible data analysis.\n\n## Skill Architecture\n\nDataPeeker uses a **two-tier skill architecture**:\n\n1. **Component Skills**: Specialized skills that handle specific analytical tasks\n2. **Process Skills**: Workflow orchestrators that guide complete analytical investigations\n\n## Data Preparation Pipeline\n\n**IMPORTANT**: Before using any process skill, data MUST flow through this pipeline:\n\n```\nRaw CSV Files â†’ [importing-data] â†’ raw_* tables â†’ [cleaning-data] â†’ clean_* tables â†’ Process Skills\n```\n\n### Why This Matters\n\n- **Quality first**: Process skills assume clean, validated data\n- **Traceability**: Every transformation is documented in numbered markdown files\n- **Reproducibility**: Cleaning decisions are preserved for audit and replication\n- **Context efficiency**: Sub-agents handle data profiling to keep analysis sessions focused\n\n## Component Skills\n\n### Data Preparation\n\n**`importing-data`** - Transform CSV files into SQLite with proper schema and types\n- **When to use**: Starting analysis with new data sources\n- **Input**: CSV file path\n- **Output**: `raw_*` table + quality report + documentation\n- **Phases**: CSV profiling â†’ schema design â†’ standardization â†’ import â†’ quality assessment\n- **Location**: `.claude/skills/importing-data/SKILL.md`\n\n**`cleaning-data`** - Address data quality issues and prepare analysis-ready tables\n- **When to use**: After importing-data completes (MANDATORY before analysis)\n- **Input**: `raw_*` table + quality report\n- **Output**: `clean_*` table + verification report + documentation\n- **Phases**: Quality review â†’ issue detection â†’ strategy design â†’ execution â†’ verification\n- **Location**: `.claude/skills/cleaning-data/SKILL.md`\n\n### Analysis Support\n\n**`understanding-data`** - Systematic data profiling and exploration\n- **When to use**: Beginning investigation of any dataset\n- **Capabilities**: Schema inspection, distributions, relationships, quality checks\n- **Location**: `.claude/skills/understanding-data/SKILL.md`\n\n**`detect-foreign-keys`** - Identify and validate foreign key relationships between tables\n- **When to use**: Multiple tables with undocumented relationships, validating referential integrity\n- **Capabilities**: FK candidate identification, value overlap analysis, cardinality assessment, orphaned record detection\n- **Location**: `.claude/skills/detect-foreign-keys/SKILL.md`\n\n**`writing-queries`** - Construct correct, efficient SQL queries\n- **When to use**: Translating analytical questions to SQL\n- **Capabilities**: Query patterns, optimization, common pitfalls\n- **Location**: `.claude/skills/writing-queries/SKILL.md`\n\n**`interpreting-results`** - Analyze query results with intellectual honesty\n- **When to use**: After executing queries, before drawing conclusions\n- **Capabilities**: Statistical interpretation, bias detection, confidence assessment\n- **Location**: `.claude/skills/interpreting-results/SKILL.md`\n\n**`creating-visualizations`** - Generate effective text-based visualizations\n- **When to use**: Communicating findings from analysis\n- **Capabilities**: Chart selection, ASCII art, markdown tables\n- **Location**: `.claude/skills/creating-visualizations/SKILL.md`\n\n## Process Skills\n\n**`exploratory-analysis`** - Discover patterns in unfamiliar data\n- **When to use**: Initial investigation, no specific hypothesis\n- **Phases**: Understanding â†’ profiling â†’ pattern discovery â†’ synthesis\n- **Prerequisites**: Data imported and cleaned\n- **Location**: `.claude/skills/exploratory-analysis/SKILL.md`\n\n**`guided-investigation`** - Investigate open-ended business questions\n- **When to use**: Answering \"why did X happen?\" or \"what's driving Y?\"\n- **Phases**: Question decomposition â†’ mapping â†’ incremental investigation â†’ synthesis\n- **Prerequisites**: Data imported and cleaned\n- **Location**: `.claude/skills/guided-investigation/SKILL.md`\n\n**`hypothesis-testing`** - Rigorously test specific hypotheses\n- **When to use**: Validating specific claims or theories\n- **Phases**: Hypothesis formulation â†’ test design â†’ analysis â†’ interpretation\n- **Prerequisites**: Data imported and cleaned\n- **Location**: `.claude/skills/hypothesis-testing/SKILL.md`\n\n**`comparative-analysis`** - Compare segments, cohorts, or time periods\n- **When to use**: Understanding differences between groups\n- **Phases**: Definition â†’ measurement â†’ comparison â†’ explanation\n- **Prerequisites**: Data imported and cleaned\n- **Location**: `.claude/skills/comparative-analysis/SKILL.md`\n\n**`marketing-experimentation`** - Validate marketing concepts through rigorous experimental cycles\n- **When to use**: Testing marketing ideas, business concepts, or campaign strategies\n- **Phases**: Discovery â†’ Hypothesis Generation â†’ Prioritization â†’ Experiment Coordination â†’ Synthesis â†’ Iteration Planning\n- **Prerequisites**: None initially (qualitative market research), data needs emerge from experiments\n- **Location**: `.claude/skills/marketing-experimentation/SKILL.md`\n\n## Workflow Patterns\n\n### Starting a New Analysis\n\n```\n1. Use `importing-data` skill\n   â”œâ”€ Input: CSV file path\n   â”œâ”€ Output: raw_* table + 05-quality-report.md\n   â””â”€ Documents: 01-csv-profile.md through 05-quality-report.md\n\n2. Use `cleaning-data` skill (MANDATORY)\n   â”œâ”€ Input: raw_* table + quality report\n   â”œâ”€ Output: clean_* table + verification report\n   â””â”€ Documents: 01-cleaning-scope.md through 05-verification-report.md\n\n3. Choose appropriate process skill\n   â”œâ”€ Exploratory: No specific question, discover patterns\n   â”œâ”€ Guided Investigation: Answer open-ended \"why\" questions\n   â”œâ”€ Hypothesis Testing: Validate specific claims\n   â””â”€ Comparative Analysis: Understand group differences\n```\n\n### During Analysis\n\nAll process skills follow this pattern:\n\n1. **Phase-based workflow**: Each process has 4-5 phases with clear deliverables\n2. **Numbered markdown files**: Every phase produces a documented artifact\n3. **Checkpoints**: Mandatory validation before proceeding to next phase\n4. **Component skill integration**: Reference understanding-data, writing-queries, etc. as needed\n\n### After Analysis\n\n- Numbered markdown files create complete audit trail\n- Git-committable artifacts enable reproducibility\n- Quality reports document data lineage and transformations\n\n## Sub-Agents\n\nDataPeeker uses specialized sub-agents to prevent context pollution during data operations:\n\n**Data Quality Agents** (used by importing-data and cleaning-data):\n- `quality-assessment` - Comprehensive quality profiling\n- `detect-exact-duplicates` - Find identical records\n- `detect-near-duplicates` - Fuzzy matching for similar records\n- `detect-outliers` - MAD-based statistical outlier detection\n- `categorize-free-text` - Semantic grouping of free text values\n- `detect-foreign-keys` - Identify foreign key relationships and validate referential integrity\n\n**Location**: `.claude/agents/[agent-name].md`\n\n## Key Principles\n\n1. **Data Quality First**: Never skip cleaning-data, even if data looks clean\n2. **Document Everything**: Every decision captured in numbered markdown files\n3. **Intellectual Honesty**: Interpret results skeptically, acknowledge limitations\n4. **Reproducibility**: Others should be able to follow your exact analytical path\n5. **Context Management**: Use sub-agents for data operations, keep main context analytical\n\n## Skill Development Guidelines\n\n### Template Organization\n\n**CRITICAL**: Never embed full markdown templates inline within skill files.\n\n**Why this matters:**\n- Nested markdown templates create unreadable skill documentation\n- Inline templates make skills difficult to maintain and update\n- Large embedded templates obscure the skill's actual workflow logic\n- Template changes require editing massive skill files instead of focused template files\n\n**Best Practice:**\n\nâœ… **DO**: Create separate template files in a `templates/` directory\n```markdown\nCreate `analysis/[session-name]/01-filename.md` with: ./templates/phase-1.md\n```\n\n**Directory structure:**\n```\n.claude/skills/[skill-name]/\nâ”œâ”€â”€ SKILL.md\nâ””â”€â”€ templates/\n    â”œâ”€â”€ phase-1.md\n    â”œâ”€â”€ phase-2.md\n    â””â”€â”€ phase-3.md\n```\n\nâŒ **DON'T**: Embed full file templates inline\n```markdown\nCreate `analysis/[session-name]/01-filename.md`:\n\n```markdown\n# Long Template Content\n[50+ lines of template markdown]\n```\n```\n\n**When inline snippets are acceptable:**\n- Very short examples (< 5 lines)\n- Code snippets demonstrating syntax\n- Brief format examples\n\n**When separate template files are required:**\n- Any template describing the full contents of a file to be written\n- Templates with > 10 lines of content\n- Templates that will be referenced by multiple skills\n- Templates that define documentation structure\n\n**Example from DataPeeker:**\n- All component skills (importing-data, cleaning-data) use separate template files\n- All process skills (exploratory-analysis, guided-investigation, hypothesis-testing, comparative-analysis) use separate template files\n- This pattern reduced some skill files by up to 71%, making them dramatically more readable\n\n## Common Anti-Patterns\n\nâŒ **Skipping cleaning-data**: \"The data looks clean enough\"\n- **Why it fails**: Quality issues emerge during analysis, invalidating conclusions\n- **Correct approach**: Always run cleaning-data, even if it finds no issues\n\nâŒ **Analyzing data in main context**: Directly querying tables during skill execution\n- **Why it fails**: Pollutes context with irrelevant records, reduces analysis effectiveness\n- **Correct approach**: Delegate all data retrieval to sub-agents\n\nâŒ **Jumping to conclusions**: Seeing pattern and immediately explaining it\n- **Why it fails**: Misses confounding factors, produces false insights\n- **Correct approach**: Use interpreting-results skill to assess confidence and alternatives\n\nâŒ **Skipping documentation**: \"I'll remember the decisions\"\n- **Why it fails**: Decisions are forgotten, analysis becomes irreproducible\n- **Correct approach**: Write numbered markdown files for every phase\n\n## Getting Started\n\n**New to DataPeeker?** Start here:\n\n1. Read `importing-data` skill to understand data ingestion\n2. Read `cleaning-data` skill to understand quality workflow\n3. Read `exploratory-analysis` to understand process skill structure\n4. Try a simple analysis end-to-end: import â†’ clean â†’ explore\n\n**Have existing analysis?** Ensure:\n- âœ… Data was imported via `importing-data` (or equivalent documented process)\n- âœ… Data was cleaned via `cleaning-data` (or equivalent quality validation)\n- âœ… All transformations are documented in numbered markdown files\n\n## Skill Evolution\n\nSkills are continuously improved based on usage. When you discover:\n- **Gaps**: Missing guidance for specific scenarios\n- **Ambiguities**: Instructions that are unclear\n- **Inefficiencies**: Redundant or unnecessary steps\n\nDocument findings and propose skill updates to maintain quality.\n\n## Skill Dependencies\n\nReview @skill-dependencies.mermaid\n\n**CRITICAL: Maintaining Skill Dependencies**\n\nâš ï¸ **MANDATORY STEP AFTER ANY SKILL MODIFICATION** âš ï¸\n\nWhenever you create, update, or modify ANY skill in this directory, you **MUST**:\n\n1. Run the analysis script:\n   ```bash\n   python3 ./.claude/skills/analyze-skill-dependencies.py\n   ```\n\n2. Review the generated `.claude/skills/skill-dependencies.mermaid` file\n\n3. Copy the contents and update the Mermaid diagram below with the new findings\n\n**NEVER skip this step.** The dependency diagram is critical for understanding skill relationships and must remain accurate. An outdated diagram leads to:\n- Missing prerequisite skills during analysis\n- Incorrect workflow planning\n- Broken skill orchestration\n- Wasted time debugging circular dependencies\n\nIf you update a skill and do not run this script, you have failed to complete the task.\n",
        "plugins/datapeeker/skills/cleaning-data/SKILL.md": "---\nname: cleaning-data\ndescription: Systematic data quality remediation - detect duplicates/outliers/inconsistencies, design cleaning strategy, execute transformations, verify results (component skill for DataPeeker analysis sessions)\n---\n\n# Cleaning Data - Component Skill\n\n## Purpose\n\nUse this skill when:\n- Have completed the `importing-data` skill with quality report generated\n- Need to address data quality issues before analysis (duplicates, outliers, NULL handling, free text categorization)\n- Want systematic approach to cleaning decisions with documented rationale\n- Need to create clean tables ready for process skills (exploratory-analysis, guided-investigation, etc.)\n- Following DataPeeker principle: **cleaning is ALWAYS mandatory** even if minimal issues found\n\nThis skill is a **prerequisite** for all DataPeeker analysis workflows and consumes the quality report from importing-data.\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have completed the `importing-data` skill successfully\n2. Have access to `05-quality-report.md` generated by importing-data\n3. Have `raw_*` table(s) in `data/analytics.db`\n4. Be familiar with basic SQLite for validation queries\n5. Understand data quality concepts: duplicates, outliers, NULL handling, categorical standardization\n\n## Data Cleaning Process\n\nCreate a TodoWrite checklist for the 5-phase data cleaning process:\n\n```\nPhase 1: Quality Report Review - pending\nPhase 2: Issue Detection (Agent-Delegated) - pending\nPhase 3: Cleaning Strategy Design - pending\nPhase 4: Cleaning Execution - pending\nPhase 5: Verification & Documentation - pending\n```\n\nMark each phase as you complete it. Document all findings in numbered markdown files (`01-cleaning-scope.md` through `05-verification-report.md`) within your analysis workspace directory.\n\n---\n\n## Phase 1: Quality Report Review\n\n**Goal:** Read quality report from importing-data, understand detected issues, prioritize for cleaning based on impact and severity.\n\n### Read Quality Report from importing-data\n\n**Locate the quality report:**\n```\nanalysis/[session-name]/05-quality-report.md\n```\n\nThis report (generated by importing-data Phase 5) contains:\n- Table schema and row counts\n- NULL percentages per column\n- Duplicate counts and examples\n- Outlier flags (3 MAD threshold) per numeric column\n- Free text candidates (columns with >50% uniqueness)\n- Summary of quality concerns\n\n**Extract key information:**\n- Which columns have >10% NULLs?\n- How many duplicate rows exist (exact duplicates)?\n- Which numeric columns have outliers?\n- Which text columns need categorization?\n\n**Document:** Summarize findings from quality report.\n\n### Prioritize Issues Using Framework\n\n**Issue Prioritization Matrix:**\n\nEvaluate each issue on three dimensions:\n\n**1. Impact (% of rows/columns affected)**\n- **High:** >10% of rows affected\n- **Medium:** 1-10% of rows affected\n- **Low:** <1% of rows affected\n\n**2. Severity (effect on analysis validity)**\n- **Critical:** Makes analysis invalid or misleading (e.g., key column >50% NULL)\n- **Significant:** Reduces data quality for important columns (e.g., duplicates, inconsistent categories)\n- **Minor:** Affects edge cases only (e.g., outliers that are legitimate)\n\n**3. Effort (complexity to resolve)**\n- **Low:** Simple removal, exclusion, or standardization (1-2 SQL queries)\n- **Medium:** Requires sub-agent for categorization or pattern analysis (3-5 queries)\n- **High:** Complex deduplication, manual review, or domain expertise needed (>5 queries)\n\n**Combine dimensions to assign priority:**\n\n| Impact | Severity  | Effort | Priority     | Action Timing |\n|--------|-----------|--------|--------------|---------------|\n| High   | Critical  | Any    | **CRITICAL** | Must address  |\n| High   | Significant| Low/Med| **HIGH**     | Must address  |\n| Medium | Critical  | Low/Med| **HIGH**     | Must address  |\n| Any    | Any       | High   | **MEDIUM**   | Address if time permits |\n| Low    | Minor     | Any    | **LOW**      | Document, may skip |\n\n**Document:** Create prioritized issue table in `01-cleaning-scope.md`.\n\n### Define Cleaning Scope and Objectives\n\nCreate `analysis/[session-name]/01-cleaning-scope.md` with: ./templates/phase-1.md\n\n**CHECKPOINT:** Before proceeding to Phase 2, you MUST have:\n- [ ] Read and understood `05-quality-report.md` from importing-data\n- [ ] Extracted all detected issues (NULLs, duplicates, outliers, free text, FK orphans)\n- [ ] Applied prioritization matrix (impact Ã— severity Ã— effort)\n- [ ] Reviewed FK relationships and orphaned records (if multiple tables)\n- [ ] Defined cleaning objectives with success criteria\n- [ ] `01-cleaning-scope.md` created with all sections filled\n\n---\n\n## Phase 2: Issue Detection (Agent-Delegated)\n\n**Goal:** Deep-dive investigation of prioritized data quality issues using sub-agents to prevent context pollution.\n\n**CRITICAL:** This phase MUST use sub-agent delegation. DO NOT analyze data in main agent context.\n\n### Detection 1: Duplicate Records\n\n**Use dedicated duplicate detection agents**\n\n**For exact duplicates:**\n\nInvoke the `detect-exact-duplicates` agent:\n\n```\nTask tool with agent: detect-exact-duplicates\nParameters:\n- table_name: raw_[actual_table_name]\n- key_columns: [columns that define uniqueness, from Phase 1 scope]\n```\n\n**For near-duplicates (fuzzy matching):**\n\nInvoke the `detect-near-duplicates` agent:\n\n```\nTask tool with agent: detect-near-duplicates\nParameters:\n- table_name: raw_[actual_table_name]\n- text_column: [specific text column flagged in Phase 1]\n```\n\nRepeat for each text column requiring fuzzy matching.\n\n**Document findings in `02-detected-issues.md` using template below.**\n\n---\n\n### Detection 2: Outliers (MAD-Based)\n\n**Use dedicated outlier detection agent**\n\nFor each numeric column flagged in Phase 1:\n\nInvoke the `detect-outliers` agent:\n\n```\nTask tool with agent: detect-outliers\nParameters:\n- table_name: raw_[actual_table_name]\n- numeric_col: [specific numeric column from Phase 1 scope]\n```\n\nRepeat for each numeric column requiring outlier analysis.\n\n**Document findings in `02-detected-issues.md` using template below.**\n\n---\n\n### Detection 4: Referential Integrity Validation\n\n**If multiple tables exist with FK relationships identified in Phase 1:**\n\n**Use dedicated FK validation agent**\n\nFor each FK relationship flagged in Phase 1:\n\nInvoke the `detect-foreign-keys` agent (focused validation mode):\n\n```\nTask tool with agent: detect-foreign-keys\nParameters:\n- database_path: data/analytics.db\n- child_table: [specific child table]\n- child_column: [FK column]\n- parent_table: [specific parent table]\n- parent_column: [PK column]\n```\n\nThe agent will:\n- Confirm value overlap percentage (validate Phase 1 findings)\n- Identify specific orphaned record IDs\n- Assess orphan patterns (recent vs old, specific categories, etc.)\n- Quantify impact on analysis (% of records affected in joins)\n\n**If single table:** Skip this detection, document \"N/A - Single table\" in detected issues report.\n\n**Document findings in `02-detected-issues.md` using template below.**\n\n---\n\n### Review Sub-Agent Findings\n\nAfter all sub-agents return findings:\n\n**For duplicates:**\n- Are exact duplicates truly identical (all columns match)?\n- Are near-duplicates legitimate variations or data entry errors?\n- Which duplicate groups should be merged vs kept separate?\n\n**For outliers:**\n- Are outliers data errors or legitimate extreme values?\n- Do outliers follow any pattern (seasonal, geographic, product-specific)?\n- Which outliers should be excluded vs capped vs flagged?\n\n**For FK orphans (if multiple tables):**\n- Are orphaned records recent (may resolve soon) or old (permanent issue)?\n- Do orphans follow a pattern (specific categories, time periods)?\n- Can orphans be matched to parent records through fuzzy matching?\n- Should orphans be excluded, flagged, or have placeholder parents created?\n\n**Document:** Observations and preliminary decisions for Strategy Phase.\n\n### Create Detected Issues Report\n\nCreate `analysis/[session-name]/02-detected-issues.md` with: ./templates/phase-2.md\n\n**CHECKPOINT:** Before proceeding to Phase 3, you MUST have:\n- [ ] Delegated duplicate detection to sub-agent (exact and near-duplicates)\n- [ ] Delegated outlier detection to sub-agent (MAD-based, all flagged columns)\n- [ ] Delegated FK validation to sub-agent (if multiple tables with relationships)\n- [ ] Reviewed all sub-agent findings and documented observations\n- [ ] Created `02-detected-issues.md` with all sections filled (including FK orphans if applicable)\n- [ ] Identified specific records/issues for Phase 3 strategy decisions\n- [ ] Listed implications for Phase 3 (what user needs to decide)\n\n---\n\n## Phase 3: Cleaning Strategy Design\n\n**Goal:** Design cleaning approach for each detected issue type, present options with trade-offs, get user confirmation before execution.\n\n### Review Detected Issues from Phase 2\n\nFrom `02-detected-issues.md`, identify all issue types requiring decisions:\n- Exact duplicates: [count]\n- Near-duplicates: [count]\n- Outliers per column: [counts]\n- Free text categorization: [columns]\n\n### Decision Framework: Duplicates\n\n**For exact duplicates, choose ONE approach:**\n\n**Option A: Keep First Occurrence**\n- **Method:** ORDER BY rowid, keep lowest rowid per duplicate group\n- **Pros:** Simple, deterministic, fast\n- **Cons:** First may not be most complete/accurate\n- **Use when:** No quality difference between duplicates\n\n**Option B: Keep Most Complete**\n- **Method:** Rank by completeness (fewest NULLs), keep best per group\n- **Pros:** Preserves maximum information\n- **Cons:** More complex, requires completeness scoring\n- **Use when:** Duplicates have varying data quality\n\n**Option C: Merge Records**\n- **Method:** Combine non-NULL values from all duplicates\n- **Pros:** No data loss\n- **Cons:** Complex, may create inconsistencies\n- **Use when:** Duplicates have complementary information\n\n**For near-duplicates, choose ONE approach:**\n\n**Option A: Auto-Merge High Confidence (>95%)**\n- **Method:** Apply fuzzy matching agent's high confidence mappings automatically\n- **Pros:** Efficient, addresses most obvious issues\n- **Cons:** Small risk of incorrect merges\n- **Use when:** Trust fuzzy matching agent's assessment\n\n**Option B: Manual Review All**\n- **Method:** Review every near-duplicate group before merging\n- **Pros:** Zero incorrect merges\n- **Cons:** Time-consuming\n- **Use when:** Data quality is critical\n\n**Option C: Skip Near-Duplicates**\n- **Method:** Only handle exact duplicates, leave fuzzy matches as-is\n- **Pros:** Conservative, no risk\n- **Cons:** Misses data quality improvements\n- **Use when:** Near-duplicates are legitimate variations\n\n**Document chosen approach in `03-cleaning-strategy.md`**\n\n---\n\n### Decision Framework: Outliers\n\n**For each numeric column with outliers, choose ONE approach:**\n\n**Option A: Exclude Outliers**\n- **Method:** Filter out rows where value > 3 MAD from median\n- **Pros:** Clean dataset, no extreme values skewing analysis\n- **Cons:** Data loss, may exclude legitimate extremes\n- **Use when:** Outliers are clearly data errors\n\n**Option B: Cap at Threshold**\n- **Method:** Set outliers to 3 MAD threshold (winsorization)\n- **Pros:** Preserves row count, reduces extreme influence\n- **Cons:** Distorts actual values\n- **Use when:** Want to preserve rows but limit extreme influence\n\n**Option C: Flag and Keep**\n- **Method:** Add outlier_flag column, keep all data\n- **Pros:** No data loss, analysts can filter if needed\n- **Cons:** Outliers may still skew analysis if not filtered\n- **Use when:** Outliers might be legitimate, need analyst judgment\n\n**Option D: Keep As-Is**\n- **Method:** No transformation\n- **Pros:** Preserves true data\n- **Cons:** Extremes may dominate analysis\n- **Use when:** Outliers are legitimate (VIPs, seasonal spikes)\n\n**Document chosen approach per column in `03-cleaning-strategy.md`**\n\n---\n\n### Decision Framework: Free Text Categorization\n\n**If free text columns flagged in Phase 2:**\n\n**Step 1: Invoke categorization agent**\n\n```\nTask tool with agent: categorize-free-text\nParameters:\n- column_name: [specific text column]\n- unique_values: [list from Phase 2 detection]\n- business_context: [optional context about what values represent]\n```\n\n**Step 2: Review agent's proposed categories**\n\nAgent will return:\n- Proposed category schema (3-10 categories)\n- Value mappings with confidence levels\n- Ambiguous/uncategorizable values flagged\n\n**Step 3: Choose categorization approach:**\n\n**Option A: Accept Agent Proposal**\n- **Method:** Use agent's categories and mappings as-is\n- **Pros:** Fast, leverages semantic analysis\n- **Cons:** May miss business context\n- **Use when:** Agent's categories make sense for analysis\n\n**Option B: Modify Categories**\n- **Method:** Adjust agent's proposal (rename, merge, split categories)\n- **Pros:** Incorporates business knowledge\n- **Cons:** Requires manual refinement\n- **Use when:** Agent's categories are close but need tweaking\n\n**Option C: Manual Categorization**\n- **Method:** Define categories and mappings from scratch\n- **Pros:** Full control, perfect fit for business needs\n- **Cons:** Time-consuming\n- **Use when:** Agent's proposal doesn't fit business model\n\n**Option D: Keep As-Is**\n- **Method:** No categorization\n- **Pros:** Preserves original data\n- **Cons:** High-cardinality text column harder to analyze\n- **Use when:** Free text values are inherently unique (IDs, descriptions)\n\n**Document chosen approach in `03-cleaning-strategy.md`**\n\n---\n\n### Decision Framework: Referential Integrity (If Multiple Tables)\n\n**For FK orphaned records identified in Phase 2:**\n\nFor each FK relationship with orphaned records, choose ONE approach:\n\n**Option A: Exclude Orphaned Records**\n- **Method:** Filter out child records where FK doesn't match any parent PK\n- **Pros:** Clean referential integrity, INNER JOINs work correctly\n- **Cons:** Data loss\n- **Use when:** Orphaned records are data errors with no business value\n\n**Option B: Preserve with NULL**\n- **Method:** Set orphaned FK values to NULL (retain child records)\n- **Pros:** Preserves child row count, makes orphans explicit\n- **Cons:** Loses relationship information, NULL handling required in queries\n- **Use when:** Child records have value even without parent context\n\n**Option C: Flag and Keep**\n- **Method:** Add `[fk_column]_orphan_flag` column, keep original FK value\n- **Pros:** No data loss, analysts can filter as needed\n- **Cons:** Referential integrity violated until analyst filters\n- **Use when:** Need investigation before deciding, orphans may resolve\n\n**Option D: Create Placeholder Parent**\n- **Method:** Insert synthetic parent record (e.g., id=-1, name=\"Unknown\"), map orphans to it\n- **Pros:** Preserves referential integrity AND child rows, INNER JOINs work\n- **Cons:** Creates synthetic data, may skew parent-level aggregations\n- **Use when:** JOINs required but can't lose child records (e.g., orders with unknown customer)\n\n**Document chosen approach per FK relationship in `03-cleaning-strategy.md`**\n\n---\n\n### Decision Framework: Business Rules (Optional)\n\n**If business rules were defined in Phase 1 scope:**\n\nFor each rule, choose approach for violations:\n\n**Option A: Exclude Violating Records**\n- **Method:** Filter out rows that fail validation\n- **Pros:** Clean dataset, only valid data\n- **Cons:** Data loss\n- **Use when:** Invalid data cannot be corrected\n\n**Option B: Cap/Coerce to Valid Range**\n- **Method:** Adjust values to meet constraints\n- **Pros:** Preserves rows\n- **Cons:** Changes actual data\n- **Use when:** Violations are minor (e.g., age 150 â†’ cap at 120)\n\n**Option C: Flag and Keep**\n- **Method:** Add validation_flag column\n- **Pros:** No data loss, transparent\n- **Cons:** Invalid data present\n- **Use when:** Need to investigate violations before deciding\n\n**Document chosen approach per rule in `03-cleaning-strategy.md`**\n\n---\n\n### Create Cleaning Strategy Document\n\nCreate `analysis/[session-name]/03-cleaning-strategy.md` with: ./templates/phase-3.md\n\n**CHECKPOINT:** Before proceeding to Phase 4, you MUST have:\n- [ ] Reviewed all detected issues from Phase 2\n- [ ] Chosen approach for duplicates (exact and near)\n- [ ] Chosen approach for outliers (per numeric column)\n- [ ] Reviewed free text categorization agent proposal (if applicable)\n- [ ] Chosen approach for free text categorization\n- [ ] Chosen approach for FK orphans (if multiple tables with relationships)\n- [ ] Defined business rule handling (if applicable)\n- [ ] User confirmed all strategies via checkpoint review\n- [ ] `03-cleaning-strategy.md` created with all decisions documented\n\n---\n\n## Phase 4: Cleaning Execution\n\n**Goal:** Execute approved cleaning strategies, create clean_* tables, track all exclusions and transformations.\n\n**CRITICAL:** All transformations use CREATE TABLE AS SELECT pattern. Keep raw_* tables intact.\n\n### Transformation 1: Remove Duplicates\n\n**Based on Strategy from Phase 3:**\n\n**For Exact Duplicates (Keep First approach):**\n\n```sql\n-- Create clean table without exact duplicates\nCREATE TABLE clean_[table_name] AS\nWITH ranked_records AS (\n  SELECT *,\n    ROW_NUMBER() OVER (\n      PARTITION BY [key_columns]\n      ORDER BY rowid\n    ) as rn\n  FROM raw_[table_name]\n)\nSELECT [all_columns]  -- Exclude rn column\nFROM ranked_records\nWHERE rn = 1;\n```\n\n**For Exact Duplicates (Keep Most Complete approach):**\n\n```sql\n-- Create clean table keeping most complete record per duplicate group\nCREATE TABLE clean_[table_name] AS\nWITH completeness_scored AS (\n  SELECT *,\n    ([count non-NULL columns formula]) as completeness_score,\n    ROW_NUMBER() OVER (\n      PARTITION BY [key_columns]\n      ORDER BY completeness_score DESC, rowid\n    ) as rn\n  FROM raw_[table_name]\n)\nSELECT [all_columns]  -- Exclude rn and completeness_score\nFROM completeness_scored\nWHERE rn = 1;\n```\n\n**For Near-Duplicates (Auto-Merge High Confidence approach):**\n\n```sql\n-- Create mapping table from categorization agent\nCREATE TABLE [column]_near_dup_mapping AS\nSELECT\n  original_value,\n  canonical_value\nFROM (VALUES\n  ('[value1]', '[canonical]'),\n  ('[value2]', '[canonical]'),\n  ...\n) AS mapping(original_value, canonical_value);\n\n-- Apply mapping\nUPDATE clean_[table_name]\nSET [text_column] = (\n  SELECT canonical_value\n  FROM [column]_near_dup_mapping\n  WHERE original_value = [text_column]\n)\nWHERE [text_column] IN (SELECT original_value FROM [column]_near_dup_mapping);\n```\n\n**Verification:**\n\n```sql\n-- Verify duplicate removal\nSELECT COUNT(*) as clean_count FROM clean_[table_name];\nSELECT COUNT(*) - COUNT(DISTINCT [key_columns]) as remaining_dups FROM clean_[table_name];\n-- Expected: 0 remaining duplicates\n\n-- Exclusion count\nSELECT [raw_count] - [clean_count] as excluded_duplicates;\n```\n\n**Document in `04-cleaning-execution.md`:**\n- SQL executed\n- Before count: [N] rows\n- After count: [N] rows\n- Duplicates removed: [N] rows ([X]%)\n\n---\n\n### Transformation 2: Handle Outliers\n\n**Based on Strategy from Phase 3:**\n\n**For Outliers (Exclude approach):**\n\n```sql\n-- Calculate thresholds\nWITH stats AS (\n  SELECT\n    [median_calculation] as median,\n    [mad_calculation] * 1.4826 as mad\n  FROM raw_[table_name]\n  WHERE [numeric_col] IS NOT NULL\n)\n-- Create clean table excluding outliers\nCREATE TABLE clean_[table_name] AS\nSELECT r.*\nFROM raw_[table_name] r\nCROSS JOIN stats s\nWHERE ABS(r.[numeric_col] - s.median) <= 3 * s.mad\n   OR r.[numeric_col] IS NULL;  -- Keep NULLs\n```\n\n**For Outliers (Cap at Threshold approach - Winsorization):**\n\n```sql\nWITH stats AS (\n  SELECT\n    [median_calculation] as median,\n    [mad_calculation] * 1.4826 as mad\n  FROM raw_[table_name]\n  WHERE [numeric_col] IS NOT NULL\n)\nCREATE TABLE clean_[table_name] AS\nSELECT\n  [other_columns],\n  CASE\n    WHEN [numeric_col] > median + 3 * mad THEN median + 3 * mad\n    WHEN [numeric_col] < median - 3 * mad THEN median - 3 * mad\n    ELSE [numeric_col]\n  END as [numeric_col]\nFROM raw_[table_name]\nCROSS JOIN stats;\n```\n\n**For Outliers (Flag and Keep approach):**\n\n```sql\nWITH stats AS (\n  SELECT [median_calculation] as median, [mad_calculation] * 1.4826 as mad\n  FROM raw_[table_name]\n  WHERE [numeric_col] IS NOT NULL\n)\nCREATE TABLE clean_[table_name] AS\nSELECT\n  r.*,\n  CASE\n    WHEN ABS(r.[numeric_col] - s.median) > 3 * s.mad THEN 1\n    ELSE 0\n  END as [numeric_col]_outlier_flag\nFROM raw_[table_name] r\nCROSS JOIN stats s;\n```\n\n**Verification:**\n\n```sql\n-- Verify outlier handling\nSELECT COUNT(*) as clean_count FROM clean_[table_name];\n\n-- For Exclude approach: check no outliers remain\nWITH stats AS (...)\nSELECT COUNT(*) as remaining_outliers\nFROM clean_[table_name], stats\nWHERE ABS([numeric_col] - median) > 3 * mad;\n-- Expected: 0\n\n-- For Cap approach: check values at thresholds\nSELECT MIN([numeric_col]), MAX([numeric_col]) FROM clean_[table_name];\n\n-- For Flag approach: check flag distribution\nSELECT [numeric_col]_outlier_flag, COUNT(*)\nFROM clean_[table_name]\nGROUP BY [numeric_col]_outlier_flag;\n```\n\n**Document in `04-cleaning-execution.md`:**\n- Approach used per column\n- SQL executed\n- Before/after row counts (if excluding)\n- Outliers affected: [N] rows ([X]%)\n\n---\n\n### Transformation 3: Categorize Free Text\n\n**Based on Strategy from Phase 3:**\n\n**If using agent's proposed categories:**\n\n```sql\n-- Create category mapping from agent output\nCREATE TABLE [column]_category_mapping AS\nVALUES\n  ('[value]', '[Category 1]'),\n  ('[value]', '[Category 1]'),\n  ('[value]', '[Category 2]'),\n  ...\n) AS mapping(original_value, category);\n\n-- Apply categorization\nCREATE TABLE clean_[table_name] AS\nSELECT\n  r.[other_columns],\n  COALESCE(m.category, 'Other') as [column]_category\nFROM raw_[table_name] r\nLEFT JOIN [column]_category_mapping m\n  ON r.[text_column] = m.original_value;\n```\n\n**If handling uncategorizable values:**\n\n```sql\n-- Option A: Exclude uncategorizable\nCREATE TABLE clean_[table_name] AS\nSELECT\n  r.*,\n  m.category as [column]_category\nFROM raw_[table_name] r\nINNER JOIN [column]_category_mapping m\n  ON r.[text_column] = m.original_value;\n-- INNER JOIN excludes unmapped values\n\n-- Option B: Map to \"Other\" category (shown in previous query with COALESCE)\n```\n\n**Verification:**\n\n```sql\n-- Verify categorization coverage\nSELECT\n  [column]_category,\n  COUNT(*) as count,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct\nFROM clean_[table_name]\nGROUP BY [column]_category\nORDER BY count DESC;\n\n-- Check for unmapped values (if kept)\nSELECT COUNT(*) as unmapped\nFROM clean_[table_name]\nWHERE [column]_category IS NULL OR [column]_category = 'Other';\n```\n\n**Document in `04-cleaning-execution.md`:**\n- Category schema used\n- SQL executed\n- Distribution by category\n- Unmapped/excluded values: [N] rows ([X]%)\n\n---\n\n### Transformation 4: Business Rule Validation (if applicable)\n\n**Based on Strategy from Phase 3:**\n\n```sql\n-- Exclude rows violating business rules\nCREATE TABLE clean_[table_name] AS\nSELECT *\nFROM raw_[table_name]\nWHERE [rule_1_validation]\n  AND [rule_2_validation]\n  ...;\n\n-- Example rules:\n-- WHERE age BETWEEN 0 AND 120\n-- AND amount > 0\n-- AND date BETWEEN '2020-01-01' AND '2025-12-31'\n```\n\n**Verification:**\n\n```sql\n-- Verify no violations remain\nSELECT COUNT(*) as violations\nFROM clean_[table_name]\nWHERE NOT ([rule_1_validation] AND [rule_2_validation] ...);\n-- Expected: 0\n```\n\n**Document in `04-cleaning-execution.md`:**\n- Rules enforced\n- SQL executed\n- Violations excluded: [N] rows ([X]%)\n\n---\n\n### Transformation 5: Referential Integrity Enforcement (if multiple tables)\n\n**Based on Strategy from Phase 3:**\n\n**For Orphaned Records (Exclude approach):**\n\n```sql\n-- Remove orphaned child records (Option A from Phase 3)\nCREATE TABLE clean_child_table AS\nSELECT c.*\nFROM raw_child_table c\nINNER JOIN raw_parent_table p ON c.fk_column = p.pk_column;\n-- INNER JOIN automatically excludes orphans\n```\n\n**For Orphaned Records (Preserve with NULL approach):**\n\n```sql\n-- Set orphaned FK values to NULL (Option B from Phase 3)\nCREATE TABLE clean_child_table AS\nSELECT\n  c.*,\n  CASE\n    WHEN p.pk_column IS NULL THEN NULL\n    ELSE c.fk_column\n  END as fk_column\nFROM raw_child_table c\nLEFT JOIN raw_parent_table p ON c.fk_column = p.pk_column;\n```\n\n**For Orphaned Records (Flag and Keep approach):**\n\n```sql\n-- Add orphan flag column (Option C from Phase 3)\nCREATE TABLE clean_child_table AS\nSELECT\n  c.*,\n  CASE\n    WHEN p.pk_column IS NULL AND c.fk_column IS NOT NULL THEN 1\n    ELSE 0\n  END as fk_column_orphan_flag\nFROM raw_child_table c\nLEFT JOIN raw_parent_table p ON c.fk_column = p.pk_column;\n```\n\n**For Orphaned Records (Create Placeholder Parent approach):**\n\n```sql\n-- Step 1: Create placeholder parent record (Option D from Phase 3)\nINSERT INTO raw_parent_table (pk_column, name, other_fields)\nVALUES (-1, 'Unknown', NULL, ...);\n\n-- Step 2: Remap orphans to placeholder\nCREATE TABLE clean_child_table AS\nSELECT\n  c.*,\n  CASE\n    WHEN p.pk_column IS NULL THEN -1\n    ELSE c.fk_column\n  END as fk_column\nFROM raw_child_table c\nLEFT JOIN raw_parent_table p ON c.fk_column = p.pk_column;\n```\n\n**Verification:**\n\n```sql\n-- Verify no orphans remain (for Exclude approach)\nSELECT COUNT(*) as orphans\nFROM clean_child_table c\nLEFT JOIN clean_parent_table p ON c.fk_column = p.pk_column\nWHERE p.pk_column IS NULL AND c.fk_column IS NOT NULL;\n-- Expected: 0\n\n-- Verify NULL remapping (for Preserve with NULL approach)\nSELECT COUNT(*) as nulled_fks\nFROM clean_child_table\nWHERE fk_column IS NULL;\n-- Expected: [count of orphans from Phase 2]\n\n-- Verify flag accuracy (for Flag and Keep approach)\nSELECT fk_column_orphan_flag, COUNT(*)\nFROM clean_child_table\nGROUP BY fk_column_orphan_flag;\n-- Expected: flag=1 count matches orphan count from Phase 2\n```\n\n**Document in `04-cleaning-execution.md`:**\n- FK relationship handled\n- Approach used (Exclude/Preserve/Flag/Placeholder)\n- SQL executed\n- Orphans affected: [N] rows ([X]%)\n- JOIN behavior after transformation\n\n[If single table: \"N/A - Single table analysis\"]\n\n---\n\n### Combined Transformation Approach\n\n**If multiple transformations needed, use CTE chain:**\n\n```sql\nCREATE TABLE clean_[table_name] AS\nWITH\n-- Step 1: Remove duplicates\ndeduped AS (\n  SELECT *, ROW_NUMBER() OVER (PARTITION BY [key_cols] ORDER BY rowid) as rn\n  FROM raw_[table_name]\n),\nno_dups AS (\n  SELECT [all_columns] FROM deduped WHERE rn = 1\n),\n-- Step 2: Handle outliers\noutliers_removed AS (\n  SELECT d.*\n  FROM no_dups d\n  CROSS JOIN (SELECT [median], [mad] FROM ...) stats\n  WHERE ABS(d.[numeric_col] - stats.median) <= 3 * stats.mad\n),\n-- Step 3: Apply categorization\ncategorized AS (\n  SELECT\n    o.*,\n    COALESCE(m.category, 'Other') as [column]_category\n  FROM outliers_removed o\n  LEFT JOIN [column]_category_mapping m ON o.[text_col] = m.original_value\n),\n-- Step 4: Enforce business rules\nfinal AS (\n  SELECT *\n  FROM categorized\n  WHERE [rule_validations]\n)\nSELECT * FROM final;\n```\n\n**Verification of combined transformations:**\n\n```sql\n-- Row count reconciliation\nSELECT\n  (SELECT COUNT(*) FROM raw_[table_name]) as raw_count,\n  (SELECT COUNT(*) FROM clean_[table_name]) as clean_count,\n  (SELECT COUNT(*) FROM raw_[table_name]) - (SELECT COUNT(*) FROM clean_[table_name]) as total_excluded;\n```\n\n---\n\n### Create Cleaning Execution Log\n\nCreate `analysis/[session-name]/04-cleaning-execution.md` with: ./templates/phase-4.md\n\n**CHECKPOINT:** Before proceeding to Phase 5, you MUST have:\n- [ ] Executed all transformations from Phase 3 strategy\n- [ ] Created clean_[table_name] table in data/analytics.db\n- [ ] Verified each transformation with specific checks\n- [ ] Reconciled row counts (raw = clean + exclusions)\n- [ ] Documented all exclusions with reasons and counts\n- [ ] Spot-checked sample records before/after\n- [ ] `04-cleaning-execution.md` created with all results documented\n\n---\n\n## Phase 5: Verification & Documentation\n\n**Goal:** Validate cleaning results, quantify quality improvements, document complete audit trail from raw to clean.\n\n### Verify Row Count Reconciliation\n\n**Critical validation - MUST match exactly:**\n\n```sql\n-- Count raw table\nSELECT COUNT(*) as raw_count FROM raw_[table_name];\n\n-- Count clean table\nSELECT COUNT(*) as clean_count FROM clean_[table_name];\n\n-- Calculate exclusions from Phase 4 log\n-- Expected: raw_count = clean_count + total_exclusions\n```\n\n**Document:** Confirm reconciliation passes. If mismatch, investigate before proceeding.\n\n---\n\n### Verify Transformation Results\n\n**For each transformation applied in Phase 4:**\n\n**Duplicate Removal Verification:**\n\n```sql\n-- Confirm no duplicates remain\nSELECT [key_columns], COUNT(*) as occurrences\nFROM clean_[table_name]\nGROUP BY [key_columns]\nHAVING COUNT(*) > 1;\n-- Expected: 0 rows returned\n```\n\n**Outlier Handling Verification:**\n\n```sql\n-- For Exclude approach: confirm no outliers remain\nWITH stats AS (\n  SELECT [median], [mad] FROM ...\n)\nSELECT COUNT(*) as remaining_outliers\nFROM clean_[table_name], stats\nWHERE ABS([numeric_col] - median) > 3 * mad;\n-- Expected: 0 rows\n\n-- For Cap approach: confirm values at thresholds\nSELECT MIN([numeric_col]) as min_val, MAX([numeric_col]) as max_val\nFROM clean_[table_name];\n-- Expected: min >= (median - 3*MAD), max <= (median + 3*MAD)\n\n-- For Flag approach: check flag accuracy\nSELECT [numeric_col]_outlier_flag, COUNT(*)\nFROM clean_[table_name]\nGROUP BY [numeric_col]_outlier_flag;\n-- Expected: distribution matches Phase 4 execution log\n```\n\n**Free Text Categorization Verification:**\n\n```sql\n-- Confirm all values categorized\nSELECT COUNT(*) as uncategorized\nFROM clean_[table_name]\nWHERE [column]_category IS NULL;\n-- Expected: 0 (unless \"keep uncategorized\" was strategy)\n\n-- Verify category distribution\nSELECT [column]_category, COUNT(*) as count,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct\nFROM clean_[table_name]\nGROUP BY [column]_category\nORDER BY count DESC;\n-- Expected: matches Phase 4 execution results\n```\n\n**Business Rule Verification (if applicable):**\n\n```sql\n-- Confirm no violations remain\nSELECT COUNT(*) as violations\nFROM clean_[table_name]\nWHERE NOT ([rule_1_condition] AND [rule_2_condition] ...);\n-- Expected: 0 rows\n```\n\n**Document all verification results:**\n- âœ“ Pass: Expected result confirmed\n- âœ— Fail: Unexpected result, requires investigation\n\n---\n\n### Compare Data Quality Metrics\n\n**Before vs After comparison:**\n\n```sql\n-- Completeness comparison\nSELECT\n  'raw' as table_name,\n  COUNT(*) as total_rows,\n  COUNT([col1]) as [col1]_non_null,\n  ROUND(100.0 * COUNT([col1]) / COUNT(*), 2) as [col1]_completeness_pct\nFROM raw_[table_name]\nUNION ALL\nSELECT\n  'clean' as table_name,\n  COUNT(*),\n  COUNT([col1]),\n  ROUND(100.0 * COUNT([col1]) / COUNT(*), 2)\nFROM clean_[table_name];\n```\n\n**Create quality improvement table:**\n\n| Metric | Raw Table | Clean Table | Improvement |\n|--------|-----------|-------------|-------------|\n| Total rows | [N] | [N] | -[X]% (exclusions) |\n| Completeness ([col1]) | [X]% | [X]% | +[X] pct points |\n| Duplicate groups | [N] | 0 | -[N] (100%) |\n| Outliers ([col2]) | [N] | 0 | -[N] (100%) |\n| Free text unique values | [N] | [N categories] | -[X]% (categorization) |\n\n**Document:** Quality improvements quantified with specific deltas.\n\n---\n\n### Spot Check Sample Records\n\n**Select representative samples to manually verify:**\n\n```sql\n-- Records that were in duplicate groups\nSELECT * FROM clean_[table_name] WHERE rowid IN ([IDs from Phase 2]);\n-- Verify: correct record kept per strategy\n\n-- Records with outliers (if flagged/capped, not excluded)\nSELECT * FROM clean_[table_name] WHERE [numeric_col]_outlier_flag = 1;\n-- Verify: flag accurate, values capped if applicable\n\n-- Records with categorized free text\nSELECT [original_col], [col]_category FROM clean_[table_name] LIMIT 20;\n-- Verify: categories make sense, mapping correct\n```\n\n**Document:** Manual verification confirms automated transformations worked correctly.\n\n---\n\n### Assess Limitations and Confidence\n\n**Document what this cleaning did NOT address:**\n\n- **Scope limitations:** [Issues identified but not addressed - e.g., \"Date range not validated\"]\n- **Data coverage:** [Time periods, geographies, categories not covered]\n- **Assumptions made:** [Business rules assumed without domain validation]\n- **Edge cases:** [Unusual values handled a specific way]\n\n**Confidence assessment:**\n\n- **High confidence:** [Transformations with clear validation - e.g., \"Exact duplicate removal\"]\n- **Medium confidence:** [Transformations with some subjectivity - e.g., \"Free text categorization\"]\n- **Low confidence / Needs review:** [Transformations requiring domain expertise - e.g., \"Outliers might be legitimate\"]\n\n**Document:** Honest assessment of what was cleaned and what wasn't, with confidence levels.\n\n---\n\n### Create Verification Report\n\nCreate `analysis/[session-name]/05-verification-report.md` with: ./templates/phase-5.md\n\n**CHECKPOINT:** Before concluding cleaning-data skill, you MUST have:\n- [ ] Verified row count reconciliation (raw = clean + exclusions)\n- [ ] Validated all transformations with specific queries\n- [ ] Quantified quality improvements with before/after metrics\n- [ ] Spot-checked sample records manually\n- [ ] Documented limitations and assumptions honestly\n- [ ] Assessed confidence level for each transformation\n- [ ] Created complete exclusion accounting table\n- [ ] `05-verification-report.md` created with all sections filled\n- [ ] clean_* table ready for analysis process skills\n\n---\n\n## Common Rationalizations\n\n### \"The data looks clean after Phase 2, I can skip Phase 3 strategy design\"\n**Why this is wrong:** Detecting issues isn't the same as deciding how to fix them. Different approaches (exclude vs cap vs flag) have different analytical implications.\n\n**Do instead:** Always complete Phase 3 with explicit decision frameworks. Document why you chose each approach with user confirmation.\n\n### \"I'll just exclude all outliers automatically, no need to review them\"\n**Why this is wrong:** Some outliers are legitimate (VIP customers, seasonal spikes, rare events). Automatic exclusion loses valuable data.\n\n**Do instead:** Complete Phase 2 detection with agent analysis. Review patterns in Phase 3. Choose approach based on business context, not just statistical threshold.\n\n### \"The fuzzy matching agent found near-duplicates, I'll merge them all\"\n**Why this is wrong:** 90-95% similarity doesn't mean identical. \"John Smith\" vs \"John Smyth\" might be the same person OR two different people.\n\n**Do instead:** Review confidence levels in Phase 3. Auto-merge only high confidence (>95%). Manual review medium confidence. Document decisions.\n\n### \"I don't need to document exclusions, I can remember what I removed\"\n**Why this is wrong:** Undocumented exclusions break audit trail. When results are questioned, you can't explain what data was excluded or why.\n\n**Do instead:** Complete Phase 4 execution log with exclusion summary table. Document every excluded record with reason and count. Reconcile in Phase 5.\n\n### \"Verification is just running the same queries again, waste of time\"\n**Why this is wrong:** Phase 5 verification checks RESULTS, not execution. Queries can run without errors but produce wrong results (logic bugs, wrong thresholds, incorrect mappings).\n\n**Do instead:** Always complete Phase 5 with before/after comparisons, spot checks, and manual inspection. Verification catches transformation bugs.\n\n### \"I found one issue, fixed it, done with cleaning\"\n**Why this is wrong:** Data quality issues cluster. If you found duplicates, likely also have outliers, NULLs, and inconsistencies. One fix doesn't make data \"clean\".\n\n**Do instead:** Complete all 5 phases systematically. Phase 2 detects ALL issue types. Address all prioritized issues in Phases 3-4.\n\n### \"The clean table has fewer rows, that's proof it's better\"\n**Why this is wrong:** Smaller isn't always better. Excluding 50% of data might remove all the interesting variation. Quality â‰  quantity reduction.\n\n**Do instead:** Complete Phase 5 with quality improvement quantification. Measure completeness, consistency, validity improvements - not just row count reduction.\n\n### \"I'll categorize free text myself, faster than using the agent\"\n**Why this is wrong:** Manual categorization is inconsistent, misses patterns, and pollutes main agent context with hundreds of unique values.\n\n**Do instead:** Always delegate free text analysis to categorize-free-text agent in Phase 3. Agent provides structured mapping with confidence levels. Review and adjust if needed.\n\n### \"Business rules failed for 2%, I'll just delete those rows and move on\"\n**Why this is wrong:** 2% violations might indicate systematic data quality issue (bad data entry, import error, logic flaw). Deleting hides the problem.\n\n**Do instead:** Investigate violations in Phase 2. Document why they violate rules in Phase 3. Consider whether to exclude, correct, or flag in strategy. Document pattern in Phase 5.\n\n### \"Phase 5 validation passed, I'm done - no need to document limitations\"\n**Why this is wrong:** All cleaning has limitations and assumptions. Pretending otherwise misleads analysts who use the clean data.\n\n**Do instead:** Complete Phase 5 limitations section honestly. Document what was NOT cleaned, assumptions made, edge cases, confidence levels. Transparency builds trust.\n\n---\n\n## Summary\n\nThis skill ensures systematic, documented data cleaning with quality validation by:\n\n1. **Prioritized scope definition:** Read quality report from importing-data, apply impact Ã— severity Ã— effort framework - ensures high-value issues addressed first, not random fixes.\n\n2. **Structured decision-making:** Present options with trade-offs for duplicates, outliers, free text, business rules - gets user confirmation before execution, prevents undocumented assumptions.\n\n3. **Agent-delegated detection:** Use dedicated sub-agents (detect-exact-duplicates, detect-near-duplicates, detect-outliers, categorize-free-text) - prevents context pollution while ensuring thorough analysis.\n\n4. **Explicit strategy approval:** Document chosen approach per issue type in Phase 3 with rationale - creates decision audit trail, enables strategy review if results questioned.\n\n5. **Transformation transparency:** Execute cleaning with CREATE TABLE AS SELECT, preserve raw_* tables, track all exclusions - maintains complete audit trail from raw to clean.\n\n6. **Rigorous verification:** Validate transformations, quantify quality improvements, spot-check samples, document limitations - ensures clean data is actually clean and limitations are known.\n\nFollow this process and you'll create well-documented clean tables with validated quality improvements, complete audit trail from raw data to analysis-ready data, and honest assessment of what was cleaned and what limitations remain.\n\n---\n",
        "plugins/datapeeker/skills/cleaning-data/templates/phase-1.md": "# Cleaning Scope: [dataset_name]\n\n## Data Source\n- **Source Skill:** importing-data\n- **Raw Table:** raw_[table_name]\n- **Quality Report Reference:** 05-quality-report.md (from importing-data)\n- **Rows in raw table:** [N]\n\n## Quality Issues Summary\n\n[Extracted from 05-quality-report.md:]\n\n### Completeness Issues\n- **Columns with >10% NULLs:**\n  - [column_name]: [X]% NULL ([N] rows)\n  - [column_name]: [X]% NULL ([N] rows)\n\n### Duplicate Issues\n- **Exact duplicates:** [N] rows ([X]% of total)\n- **Near-duplicates:** [Estimated count if flagged]\n\n### Outlier Issues\n- **[Numeric column]:** [N] outliers (>3 MAD)\n- **[Numeric column]:** [N] outliers (>3 MAD)\n\n### Free Text Issues\n- **[Text column]:** [N] unique values ([X]% uniqueness) - categorization candidate\n- **[Text column]:** [N] unique values ([X]% uniqueness) - categorization candidate\n\n### Referential Integrity Issues\n\n[If multiple tables analyzed in importing-data:]\n\n- **[child_table].[fk_column] â†’ [parent_table].[pk_column]:**\n  - Orphaned records: [N] ([X]% of child table)\n  - Match confidence: [High/Medium/Low] ([XX]% integrity)\n  - Relationship type: [One-to-one / Many-to-one / Many-to-many]\n  - Recommended action: [Exclude orphans / Flag for review / Preserve with NULL / Create placeholder parent]\n\n[If single table: \"N/A - Single table analysis\"]\n\n## Issue Prioritization\n\n| Issue | Impact | Severity | Effort | Priority | Rationale |\n|-------|--------|----------|--------|----------|-----------|\n| [Issue description] | High | Critical | Low | **CRITICAL** | [Why this matters] |\n| [Issue description] | High | Significant | Medium | **HIGH** | [Why this matters] |\n| [Issue description] | Medium | Significant | Low | **HIGH** | [Why this matters] |\n| [Issue description] | Low | Minor | Low | **LOW** | [Can skip or document] |\n\n## Cleaning Objectives\n\nBased on prioritization, the cleaning objectives are:\n\n1. **Address CRITICAL issues:** [List specific actions]\n   - Expected impact: [% of rows affected, data quality improvement]\n\n2. **Address HIGH priority issues:** [List specific actions]\n   - Expected impact: [% of rows affected, data quality improvement]\n\n3. **Document MEDIUM/LOW priority issues:** [List what won't be addressed and why]\n   - Rationale: [Effort vs benefit trade-off]\n\n## Success Criteria\n\nAfter cleaning, the clean_[table_name] table MUST meet:\n- [ ] All CRITICAL issues resolved\n- [ ] All HIGH priority issues resolved\n- [ ] All transformations documented with before/after counts\n- [ ] All exclusions documented with rationale\n- [ ] Verification queries confirm success\n\n## Next Steps\nProceed to Phase 2: Issue Detection (Agent-Delegated) for deep-dive investigation of prioritized issues.\n",
        "plugins/datapeeker/skills/cleaning-data/templates/phase-2.md": "# Detected Issues Report: [dataset_name]\n\n## Objective\nDeep-dive investigation of data quality issues identified in Phase 1 scope using sub-agent delegation.\n\n## Detection 1: Duplicate Records\n\n### Exact Duplicates\n\n**Detection Method:** GROUP BY all key columns, COUNT(*) > 1\n\n**Summary:**\n- **Total duplicate groups:** [N]\n- **Total duplicate records:** [N] (excludes first occurrence in each group)\n- **Percentage of dataset:** [X%]\n\n**Top Duplicate Groups:**\n\n| Group # | Key Values | Occurrence Count | Example IDs |\n|---------|------------|------------------|-------------|\n| 1 | [values] | [N] | [IDs] |\n| 2 | [values] | [N] | [IDs] |\n| ... | ... | ... | ... |\n\n**Analysis:**\n[Are these true duplicates or expected repeated values? Pattern observations?]\n\n### Near-Duplicates (Fuzzy Matching)\n\n**Detection Method:** Python fuzzy string matching (>90% similarity threshold)\n\n**Summary:**\n- **Fuzzy groups found:** [N]\n- **High confidence (>95%):** [N groups]\n- **Medium confidence (90-95%):** [N groups]\n\n**High Confidence Groups (Likely Merge Candidates):**\n\n| Group # | Values | Similarity | IDs | Recommended Action |\n|---------|--------|------------|-----|---------------------|\n| 1 | \"[value1]\" vs \"[value2]\" | 97% | [IDs] | Merge to \"[canonical]\" |\n| 2 | \"[value1]\" vs \"[value2]\" | 95% | [IDs] | Merge to \"[canonical]\" |\n\n**Medium Confidence Groups (Manual Review Needed):**\n\n| Group # | Values | Similarity | IDs | Recommended Action |\n|---------|--------|------------|-----|---------------------|\n| 1 | \"[value1]\" vs \"[value2]\" | 92% | [IDs] | Review - may be distinct |\n\n**Analysis:**\n[Which fuzzy groups should be merged? Which need manual review?]\n\n---\n\n## Detection 2: Outliers (MAD-Based)\n\n### [Numeric Column 1]\n\n**Statistics:**\n- **Median:** [value]\n- **MAD:** [value]\n- **Min:** [value]\n- **Max:** [value]\n- **3 MAD threshold:** [value]\n\n**Outlier Summary:**\n- **Total outliers (>3 MAD):** [N]\n- **Extreme (>5 MAD):** [N]\n- **Significant (3-5 MAD):** [N]\n\n**Top Outlier Examples:**\n\n| Row ID | Value | MAD Distance | Context/Pattern |\n|--------|-------|--------------|-----------------|\n| [ID] | [value] | [X] MAD | [e.g., \"Dec 2024 spike\"] |\n| [ID] | [value] | [X] MAD | [context] |\n\n**Analysis:**\n[Are these data errors or legitimate? Pattern observed? Recommended action?]\n\n[Repeat for each numeric column flagged in Phase 1]\n\n---\n\n## Detection 3: Free Text Issues\n\n[If free text columns were in scope:]\n\n### [Text Column Name]\n\n**Unique Values:** [N]\n**Total Values:** [N]\n**Uniqueness:** [X%]\n\n**Categorization Assessment:**\n[Can values be grouped into categories? Natural groupings identified?]\n\n[If sub-agent analyzed free text for categorization, include findings here]\n\n---\n\n## Detection 4: Referential Integrity Violations\n\n[If multiple tables with FK relationships:]\n\n### [child_table].[fk_column] â†’ [parent_table].[pk_column]\n\n**Relationship Confirmation:**\n- Match percentage: [XX.X]% (from importing-data: [XX.X]%)\n- Relationship type: [One-to-one / Many-to-one / Many-to-many]\n- Cardinality: Avg [X.X] children per parent\n\n**Orphaned Records:**\n- Total orphaned: [N] rows ([X]% of child table)\n- Sample orphaned values:\n  - [value1]: [count] occurrences\n  - [value2]: [count] occurrences\n\n**Orphan Pattern Analysis:**\n- **Recency:** [Most orphans are <recent/old>, detailed breakdown:]\n  - Last 7 days: [N] orphans\n  - Last 30 days: [N] orphans\n  - Older than 90 days: [N] orphans\n- **Pattern observed:** [e.g., \"All orphans are from Q1 2024\", \"No clear pattern\"]\n- **Potential cause:** [e.g., \"Parent records deleted\", \"Data entry lag\", \"Import order issue\"]\n\n**Impact Assessment:**\n- **INNER JOIN impact:** [N] child records excluded ([X]% of total)\n- **LEFT JOIN impact:** [N] child records will have NULL parent fields\n- **Analysis affected:** [Which queries/analyses depend on this relationship?]\n\n**Recommended Action:**\n- [Exclude orphans / Flag for review / Preserve with NULL / Create placeholder parent]\n- Rationale: [Why this approach based on pattern analysis]\n\n[Repeat for each FK relationship requiring attention]\n\n[If single table: \"N/A - Single table analysis\"]\n\n---\n\n## Cross-Issue Analysis\n\n### Records with Multiple Issues\n\n[Do any records have BOTH duplicates AND outliers? List overlaps.]\n\n| Row ID | Issues | Impact |\n|--------|--------|--------|\n| [ID] | Duplicate + Outlier | [Decision needed] |\n\n---\n\n## Summary of All Findings\n\n### Issue Counts by Type\n\n| Issue Type | Count | % of Dataset | Severity Assessment |\n|-----------|-------|--------------|---------------------|\n| Exact Duplicates | [N] | [X%] | [High/Medium/Low] |\n| Near-Duplicates | [N] | [X%] | [High/Medium/Low] |\n| Outliers ([col]) | [N] | [X%] | [High/Medium/Low] |\n| Free Text | [N unique] | [X%] | [High/Medium/Low] |\n| FK Orphans ([relationship]) | [N] | [X%] | [High/Medium/Low] |\n\n### Records Flagged for Manual Review\n\n**High Priority Review:**\n- [Row IDs and reasons]\n\n**Medium Priority Review:**\n- [Row IDs and reasons]\n\n---\n\n## Implications for Phase 3 (Strategy Design)\n\nBased on detected issues, Phase 3 must address:\n\n1. **Duplicate Strategy:**\n   - Exact duplicates: [Keep first / Keep most complete / Merge approach]\n   - Near-duplicates: [Auto-merge high confidence / Manual review medium confidence]\n\n2. **Outlier Strategy:**\n   - [Column]: [Exclude / Cap at threshold / Flag for review / Keep as-is]\n   - [Column]: [approach]\n\n3. **Free Text Strategy:**\n   - [Column]: [Categorization schema / Manual mapping / Keep as-is]\n\n4. **FK Orphan Strategy (if multiple tables):**\n   - [Relationship]: [Exclude orphans / Preserve with NULL / Flag for review / Create placeholder parent]\n   - [Relationship]: [approach]\n\n5. **Records to Exclude:**\n   - [List specific IDs or criteria for exclusion]\n\n6. **User Decisions Needed:**\n   - [Which issues require user input in Phase 3?]\n\n## Next Steps\nProceed to Phase 3: Cleaning Strategy Design with user confirmation on approaches.\n",
        "plugins/datapeeker/skills/cleaning-data/templates/phase-3.md": "# Cleaning Strategy: [dataset_name]\n\n## Objective\nDefine cleaning approach for all detected issues, with user-approved decisions for execution in Phase 4.\n\n## Issue Summary from Phase 2\n\n[Brief recap of issues detected]\n\n- Exact duplicates: [N]\n- Near-duplicates: [N]\n- Outliers: [N per column]\n- Free text columns: [N]\n- Business rule violations: [N if applicable]\n\n---\n\n## Strategy 1: Duplicate Handling\n\n### Exact Duplicates\n**Chosen Approach:** [Option A/B/C]\n\n**Rationale:** [Why this approach fits the data and use case]\n\n**Implementation:**\n- [Specific steps for execution]\n- Expected exclusions: [N] rows ([X]%)\n\n### Near-Duplicates\n**Chosen Approach:** [Option A/B/C]\n\n**Rationale:** [Why this approach]\n\n**Implementation:**\n- [Specific steps, including which groups to merge]\n- Expected exclusions/merges: [N] rows\n\n---\n\n## Strategy 2: Outlier Handling\n\n### [Numeric Column 1]\n**Chosen Approach:** [Option A/B/C/D]\n\n**Rationale:** [Why this approach for this column]\n\n**Threshold:** [3 MAD or other value]\n\n**Implementation:**\n- [Specific SQL approach]\n- Expected exclusions/caps: [N] rows ([X]%)\n\n[Repeat for each numeric column]\n\n---\n\n## Strategy 3: Free Text Categorization\n\n### [Text Column 1]\n**Chosen Approach:** [Option A/B/C/D]\n\n**Agent Proposal Review:**\n- Categories proposed: [N]\n- High confidence mappings: [N]\n- Manual review needed: [N]\n\n**Final Category Schema:**\n\n| Category | Definition | Value Count |\n|----------|------------|-------------|\n| [Cat 1] | [Definition] | [N] |\n| [Cat 2] | [Definition] | [N] |\n| Other | [Uncategorized] | [N] |\n\n**Implementation:**\n- [SQL CASE statement or mapping table approach]\n- Ambiguous values: [How handled]\n\n[Repeat for each text column]\n\n---\n\n## Strategy 4: Referential Integrity (if multiple tables)\n\n### [Relationship 1]: [child_table].[fk_column] â†’ [parent_table].[pk_column]\n\n**Orphaned Records:** [N] ([X]% of child table)\n\n**Chosen Approach:** [Option A/B/C/D]\n\n**Rationale:** [Why this approach for this relationship]\n\n**Implementation:**\n- [Specific SQL approach - filter, NULL update, flag addition, or placeholder insertion]\n- Expected exclusions: [N] rows ([X]%) [if Option A]\n- JOIN behavior impact: [How this affects downstream queries]\n\n**Verification:**\n- [How to confirm approach worked correctly]\n\n[Repeat for each FK relationship]\n\n[If single table: \"N/A - Single table analysis\"]\n\n---\n\n## Strategy 5: Business Rules (if applicable)\n\n### Rule 1: [Rule Name]\n**Validation:** [Constraint description]\n\n**Violations Found:** [N] ([X]%)\n\n**Chosen Approach:** [Option A/B/C]\n\n**Implementation:** [How violations will be handled]\n\n[Repeat for each rule]\n\n---\n\n## Exclusion Projections\n\n**Estimated total exclusions:**\n\n| Reason | Count | % of Dataset |\n|--------|-------|--------------|\n| Duplicates | [N] | [X]% |\n| Outliers | [N] | [X]% |\n| FK orphans (if excluded) | [N] | [X]% |\n| Business rule violations | [N] | [X]% |\n| Uncategorizable free text | [N] | [X]% |\n| **TOTAL** | **[N]** | **[X]%** |\n\n**Expected clean table size:** [raw count] - [exclusions] = [clean count] rows\n\n---\n\n## User Confirmation\n\n- **Date:** [Timestamp]\n- **Confirmed by:** [User]\n- **Modifications from agent proposals:** [None / List changes]\n\n## Next Steps\nProceed to Phase 4: Cleaning Execution with approved strategies.\n",
        "plugins/datapeeker/skills/cleaning-data/templates/phase-4.md": "# Cleaning Execution Log: [dataset_name]\n\n## Objective\nExecute approved cleaning strategies from Phase 3 and create clean_[table_name] ready for analysis.\n\n## Execution Summary\n\n**Start time:** [timestamp]\n**End time:** [timestamp]\n**Duration:** [seconds]\n\n---\n\n## Transformation 1: Duplicate Removal\n\n### Strategy\n[Approach chosen in Phase 3]\n\n### SQL Executed\n\n```sql\n[Actual SQL run]\n```\n\n### Results\n- **Raw table rows:** [N]\n- **Duplicates identified:** [N]\n- **Duplicates removed:** [N]\n- **Rows after deduplication:** [N]\n\n### Verification\n\n```sql\n[Verification query]\n```\n\n**Result:** [Pass/Fail - if fail, explain]\n\n### Spot Checks\n\nChecked duplicate group [example]:\n- Before: [N] occurrences\n- After: 1 occurrence (kept [which one])\n- âœ“ Correct\n\n---\n\n## Transformation 2: Outlier Handling\n\n### Strategy\n[Approach chosen in Phase 3]\n\n### [Numeric Column 1]\n\n**SQL Executed:**\n\n```sql\n[Actual SQL]\n```\n\n**Statistics:**\n- Median: [value]\n- MAD: [value]\n- 3 MAD threshold: [value]\n- Outliers identified: [N]\n\n**Results:**\n- Outliers [excluded/capped/flagged]: [N]\n- Rows after transformation: [N]\n\n**Verification:**\n\n```sql\n[Verification query]\n```\n\n**Result:** [Pass/Fail]\n\n[Repeat for each numeric column]\n\n---\n\n## Transformation 3: Free Text Categorization\n\n### Strategy\n[Approach chosen in Phase 3]\n\n### [Text Column 1]\n\n**Category Schema:**\n\n| Category | Definition | Value Count |\n|----------|------------|-------------|\n| [Cat 1] | [Def] | [N] |\n| [Cat 2] | [Def] | [N] |\n| Other | [Uncategorized] | [N] |\n\n**SQL Executed:**\n\n```sql\n[Mapping table creation]\n[Categorization application]\n```\n\n**Results:**\n\n| Category | Count | % |\n|----------|-------|---|\n| [Cat 1] | [N] | [X]% |\n| [Cat 2] | [N] | [X]% |\n| Other | [N] | [X]% |\n\n**Uncategorizable values excluded:** [N]\n\n**Verification:**\n\n```sql\n[Category distribution check]\n```\n\n**Result:** [Distribution matches expectations]\n\n[Repeat for each text column]\n\n---\n\n## Transformation 4: Business Rule Enforcement\n\n[If applicable]\n\n### Rules Applied\n1. [Rule 1]: [Description]\n2. [Rule 2]: [Description]\n\n**SQL Executed:**\n\n```sql\n[Validation SQL]\n```\n\n**Results:**\n- Violations found: [N]\n- Violations excluded: [N]\n\n---\n\n## Exclusion Summary\n\n**Total Exclusions by Reason:**\n\n| Reason | Count | % of Raw Dataset |\n|--------|-------|------------------|\n| Duplicates | [N] | [X]% |\n| Outliers ([col1]) | [N] | [X]% |\n| Outliers ([col2]) | [N] | [X]% |\n| FK orphans (if excluded) | [N] | [X]% |\n| Uncategorizable free text | [N] | [X]% |\n| Business rule violations | [N] | [X]% |\n| **TOTAL EXCLUDED** | **[N]** | **[X]%** |\n\n### Row Count Reconciliation\n\n```\nRaw table (raw_[table]):        [N] rows\nExclusions (all reasons):       [N] rows\nClean table (clean_[table]):    [N] rows\n\nVerification: [N] - [N] = [N] âœ“\n```\n\n---\n\n## Clean Table Schema\n\n```sql\nPRAGMA table_info(clean_[table_name]);\n```\n\n**Result:**\n```\n[Paste schema showing columns, including any new columns like category fields or flags]\n```\n\n---\n\n## Before/After Comparison\n\n### Data Quality Metrics\n\n| Metric | Raw Table | Clean Table | Improvement |\n|--------|-----------|-------------|-------------|\n| Total rows | [N] | [N] | -[X]% |\n| Duplicate groups | [N] | 0 | 100% |\n| NULL % in [col] | [X]% | [X]% | [improvement] |\n| Outliers in [col] | [N] | 0 | 100% |\n| Free text unique values | [N] | [N categories] | [reduction] |\n\n### Sample Records\n\n**Before (raw_[table]):**\n```sql\nSELECT * FROM raw_[table] LIMIT 3;\n```\n\n**Result:**\n```\n[Paste sample showing issues]\n```\n\n**After (clean_[table]):**\n```sql\nSELECT * FROM clean_[table] LIMIT 3;\n```\n\n**Result:**\n```\n[Paste sample showing cleaned data]\n```\n\n---\n\n## Issues Encountered\n\n[Document any problems during execution and how resolved]\n\nExample:\n- Issue: Category mapping had typo in value\n- Resolution: Corrected mapping table, re-ran categorization\n- Impact: [describe]\n\n---\n\n## Clean Table Status\n\n- **Table name:** clean_[table_name]\n- **Location:** data/analytics.db\n- **Row count:** [N]\n- **Ready for analysis:** âœ“ Yes / âœ— No (if no, explain)\n\n## Next Steps\nProceed to Phase 5: Verification & Documentation to validate cleaning results.\n",
        "plugins/datapeeker/skills/cleaning-data/templates/phase-5.md": "# Verification Report: [dataset_name]\n\n## Objective\nValidate cleaning transformations, quantify quality improvements, document complete audit trail.\n\n## Executive Summary\n- Raw table: [N] rows\n- Clean table: [N] rows\n- Total exclusions: [N] ([X]%)\n- Verification status: âœ“ All validations passed\n\n## Row Count Reconciliation\n\n**Critical validation - MUST match exactly:**\n\n```sql\n-- Count raw table\nSELECT COUNT(*) as raw_count FROM raw_[table_name];\n-- Result: [N]\n\n-- Count clean table\nSELECT COUNT(*) as clean_count FROM clean_[table_name];\n-- Result: [N]\n\n-- Calculate exclusions from Phase 4 log\n-- Expected: raw_count = clean_count + total_exclusions\n```\n\n**Reconciliation Status:** âœ“ Pass / âœ— Fail - [Explanation if fail]\n\n## Transformation Verification\n\n### Duplicate Removal Verification\n\n```sql\n-- Confirm no duplicates remain\nSELECT [key_columns], COUNT(*) as occurrences\nFROM clean_[table_name]\nGROUP BY [key_columns]\nHAVING COUNT(*) > 1;\n-- Expected: 0 rows returned\n```\n\n**Result:** [Pass/Fail with details]\n\n### Outlier Handling Verification\n\n```sql\n-- For Exclude approach: confirm no outliers remain\nWITH stats AS ([median/MAD calculation])\nSELECT COUNT(*) as remaining_outliers\nFROM clean_[table_name], stats\nWHERE ABS([numeric_col] - median) > 3 * mad;\n-- Expected: 0 rows\n```\n\n**Result:** [Pass/Fail with details]\n\n### Free Text Categorization Verification\n\n```sql\n-- Confirm all values categorized\nSELECT COUNT(*) as uncategorized\nFROM clean_[table_name]\nWHERE [column]_category IS NULL;\n-- Expected: 0 (unless \"keep uncategorized\" was strategy)\n\n-- Verify category distribution\nSELECT [column]_category, COUNT(*) as count,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct\nFROM clean_[table_name]\nGROUP BY [column]_category\nORDER BY count DESC;\n```\n\n**Result:** [Distribution matches Phase 4 expectations]\n\n### Business Rule Verification (if applicable)\n\n```sql\n-- Confirm no violations remain\nSELECT COUNT(*) as violations\nFROM clean_[table_name]\nWHERE NOT ([rule_1_condition] AND [rule_2_condition] ...);\n-- Expected: 0 rows\n```\n\n**Result:** [Pass/Fail with details]\n\n## Data Quality Improvements\n\n### Before vs After Comparison\n\n| Metric | Raw Table | Clean Table | Improvement |\n|--------|-----------|-------------|-------------|\n| Total rows | [N] | [N] | -[X]% (exclusions) |\n| Completeness ([col1]) | [X]% | [X]% | +[X] pct points |\n| Duplicate groups | [N] | 0 | -[N] (100%) |\n| Outliers ([col2]) | [N] | 0 | -[N] (100%) |\n| Free text unique values | [N] | [N categories] | -[X]% (categorization) |\n\n## Exclusion Accounting\n\n**Complete exclusion summary from Phase 4:**\n\n| Reason | Count | % of Raw Dataset |\n|--------|-------|------------------|\n| Duplicates | [N] | [X]% |\n| Outliers ([col1]) | [N] | [X]% |\n| Outliers ([col2]) | [N] | [X]% |\n| FK orphans (if excluded) | [N] | [X]% |\n| Uncategorizable free text | [N] | [X]% |\n| Business rule violations | [N] | [X]% |\n| **TOTAL EXCLUDED** | **[N]** | **[X]%** |\n\n## Spot Check Sample Records\n\n### Records from Duplicate Groups\n\n```sql\nSELECT * FROM clean_[table_name] WHERE rowid IN ([IDs from Phase 2]);\n```\n\n**Verification:** [Correct record kept per strategy - details]\n\n### Records with Outliers (if flagged/capped)\n\n```sql\nSELECT * FROM clean_[table_name] WHERE [numeric_col]_outlier_flag = 1;\n```\n\n**Verification:** [Flag accurate, values capped if applicable - details]\n\n### Records with Categorized Free Text\n\n```sql\nSELECT [original_col], [col]_category FROM clean_[table_name] LIMIT 20;\n```\n\n**Verification:** [Categories make sense, mapping correct - details]\n\n## Confidence Assessment\n\n### High Confidence Transformations\n- [Transformation]: [Why high confidence]\n- Example: \"Exact duplicate removal\" - Clear validation, deterministic\n\n### Medium Confidence Transformations\n- [Transformation]: [Why medium confidence, some subjectivity]\n- Example: \"Free text categorization\" - Agent-proposed with review\n\n### Low Confidence / Needs Review\n- [Transformation]: [Why needs domain expertise review]\n- Example: \"Outliers might be legitimate seasonal patterns\"\n\n## Limitations\n\n**Scope limitations - What this cleaning did NOT address:**\n- [Issues identified but not addressed]: [Reason not addressed]\n- Example: \"Date range not validated - requires domain knowledge\"\n\n**Data coverage:**\n- Time periods covered: [Range]\n- Geographies/categories not covered: [List if applicable]\n\n**Assumptions made:**\n- [Business rule assumed]: [Without domain validation]\n- Example: \"Assumed age >120 is invalid - may not apply to historical data\"\n\n**Edge cases:**\n- [Unusual value handling]: [How handled]\n- Example: \"NULL FKs preserved - may need review based on analysis needs\"\n\n## Clean Table Readiness\n\n**Status:** âœ“ clean_[table_name] ready for analysis / âœ— Not ready - [Issues to resolve]\n\n**Table details:**\n- Location: data/analytics.db\n- Row count: [N]\n- Columns: [N] (including [N] new category/flag columns)\n\n**Next steps:**\n- Proceed to analysis process skills (exploratory-analysis, guided-investigation, etc.)\n- Use this verification report to understand data transformations and limitations\n",
        "plugins/datapeeker/skills/comparative-analysis/SKILL.md": "---\nname: comparative-analysis\ndescription: Systematic comparison of segments, cohorts, or time periods - ensure fair apples-to-apples comparisons, identify meaningful differences, explain WHY differences exist\n---\n\n# Comparative Analysis Process\n\n## Overview\n\nThis skill guides you through systematic comparison of two or more groups, segments, cohorts, or time periods. Unlike exploratory-analysis (where you discover patterns) or guided-investigation (where you answer broad questions), comparative analysis helps you **rigorously compare** specific groups and **explain** why they differ.\n\nComparative analysis is appropriate when:\n- You need to compare performance between specific segments (regions, products, customer cohorts)\n- You want to understand how groups differ across multiple dimensions\n- You're evaluating changes before/after an intervention or between time periods\n- The user asks \"How does X compare to Y?\" or \"What's different about segment A vs B?\"\n- You need to make fair, apples-to-apples comparisons with controls for confounding factors\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have data imported into SQLite database using the `importing-data` skill\n2. Have data quality validated and cleaned using the `cleaning-data` skill (MANDATORY - never skip)\n3. Have created an analysis workspace (`just start-analysis comparative-analysis <name>`)\n4. Have clearly defined what you're comparing (user must specify comparison groups)\n5. Be familiar with the component skills:\n   - `understanding-data` - for data profiling\n   - `writing-queries` - for SQL query construction\n   - `interpreting-results` - for result analysis\n   - `creating-visualizations` - for text-based visualizations\n\n## Mandatory Process Structure\n\nYou MUST use TodoWrite to track progress through all 5 phases. Create todos at the start:\n\n```markdown\n- Phase 1: Define Comparison - pending\n- Phase 2: Segment Definition - pending\n- Phase 3: Metric Comparison - pending\n- Phase 4: Difference Explanation - pending\n- Phase 5: Conclusions and Recommendations - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n---\n\n## Phase 1: Define Comparison\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Clarified what groups/segments/periods are being compared\n- [ ] Identified the comparison objective (what question does this answer?)\n- [ ] Determined comparison type (segments, cohorts, time periods, before/after)\n- [ ] Established what metrics will be compared\n- [ ] Documented the comparison framework\n- [ ] Saved to `01 - comparison-definition.md`\n\n### Instructions\n\n1. **Clarify the comparison goal with the user**\n\nAsk clarifying questions:\n- What specific groups/segments do you want to compare?\n- What's the purpose of this comparison? What decision will it inform?\n- What differences would be meaningful or actionable?\n- Are there specific metrics you care about most?\n- What time period should the comparison cover?\n\n2. **Define the comparison framework**\n\nCreate `analysis/[session-name]/01-comparison-definition.md` with: ./templates/phase-1.md\n\n3. **Get user confirmation before proceeding**\n   - Review comparison definition with user\n   - Confirm groups are defined correctly\n   - Verify metrics align with user's goals\n   - Adjust framework if needed\n\n**Common Rationalization:** \"The comparison is obvious, I'll just start querying\"\n**Reality:** Without explicit definition, you'll make unstated assumptions about what \"fair\" means.\n\n**Common Rationalization:** \"I'll compare everything and see what's different\"\n**Reality:** Unfocused comparison produces noise. Define specific metrics and materiality thresholds.\n\n---\n\n## Phase 2: Segment Definition\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Verified that comparison groups exist in the data\n- [ ] Validated group definitions with actual queries\n- [ ] Checked for data quality issues within each group\n- [ ] Documented group characteristics and sample sizes\n- [ ] Confirmed groups are comparable (similar data quality, coverage)\n- [ ] Saved to `02 - segment-definition.md`\n\n### Instructions\n\n1. **Validate that groups exist and are well-defined**\n\nCreate `analysis/[session-name]/02-segment-definition.md` with: ./templates/phase-2.md\n\n2. **Handle segment definition issues**\n\nIf groups are not comparable:\n- **Sample size too small:** Consider combining groups, expanding time window, or adjusting comparison\n- **Different time periods:** Either align periods or explicitly control for temporal effects\n- **Data quality differs:** Document the difference and consider if it invalidates comparison\n- **Overlapping groups:** Redefine to ensure groups are mutually exclusive\n\n3. **Document any exclusions or adjustments**\n\nIf you exclude outliers, filter dates, or adjust definitions, document clearly:\n```markdown\n## Adjustments Made for Fair Comparison\n\n1. **Outlier handling:** Excluded 8 transactions >$50,000 as data entry errors (confirmed with field validation)\n2. **Date alignment:** Limited both groups to Feb 1 - Apr 30 to match shorter group's coverage\n3. **Null handling:** Excluded 127 transactions with NULL customer_id from both groups\n```\n\n**Common Rationalization:** \"The groups look fine, I'll skip validation\"\n**Reality:** Unstated data quality issues or sample size problems will invalidate your comparison.\n\n**Common Rationalization:** \"Sample sizes are different but that's okay\"\n**Reality:** Large sample size differences require per-capita normalization. Raw totals are misleading.\n\n---\n\n## Phase 3: Metric Comparison\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Calculated all primary metrics for each group\n- [ ] Calculated all secondary metrics for each group\n- [ ] Computed differences (absolute and percentage) between groups\n- [ ] Created comparison visualizations (tables, charts)\n- [ ] Identified which metrics differ meaningfully (per materiality threshold)\n- [ ] Documented all comparisons with queries and results\n- [ ] Saved to `03 - metric-comparison.md`\n\n### Instructions\n\n1. **Compare groups systematically across all metrics**\n\nCreate `analysis/[session-name]/03-metric-comparison.md` with: ./templates/phase-3.md\n\n2. **Be rigorous about normalization**\n\nCompare apples-to-apples:\n- Use per-customer or per-day metrics, not raw totals (unless groups are identical size)\n- Calculate percentages within group (e.g., % of group's revenue by category)\n- Use rate metrics (transactions per customer, revenue per transaction)\n\nWrong: \"Northeast has $458K revenue vs Southeast's $392K\"\nRight: \"Northeast has $161/customer vs Southeast's $150/customer (7.5% higher)\"\n\n3. **Compute statistical and practical significance**\n\n**Statistical significance:** Is difference larger than random variation would explain?\n- With large samples (>1000), small differences can be statistically significant\n- Document sample sizes to provide context\n\n**Practical significance:** Is difference large enough to matter for decisions?\n- Use materiality threshold from Phase 1\n- 5% difference in revenue might be huge for a billion-dollar company, trivial for a startup\n\n4. **Use visualization to make differences clear**\n\n- Side-by-side tables with difference columns\n- ASCII bar charts showing relative magnitudes\n- Percentage difference callouts\n\n**Common Rationalization:** \"I'll just show the raw numbers and let the user interpret\"\n**Reality:** Your job is interpretation. Show differences clearly and explain what they mean.\n\n**Common Rationalization:** \"Northeast has more revenue, that's the answer\"\n**Reality:** Explain WHY - more customers? Higher spend per customer? Different product mix? Dig deeper.\n\n**Common Rationalization:** \"These differences are statistically significant, so they matter\"\n**Reality:** Statistical significance â‰  practical significance. A 1% difference might be \"significant\" but not meaningful.\n\n---\n\n## Phase 4: Difference Explanation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Identified the 2-3 most important differences from Phase 3\n- [ ] For each difference, investigated potential causes\n- [ ] Analyzed confounding factors\n- [ ] Ruled out alternative explanations where possible\n- [ ] Quantified relative contribution of different factors\n- [ ] Documented explanation analysis with supporting queries\n- [ ] Saved to `04 - difference-explanation.md`\n\n### Instructions\n\n1. **Focus on the most meaningful differences**\n\nDon't try to explain every small difference. Focus on:\n- Differences that exceed materiality threshold\n- Differences that are surprising or counter-intuitive\n- Differences that inform the comparison objective\n\n2. **Investigate WHY differences exist**\n\nCreate `analysis/[session-name]/04-difference-explanation.md` with: ./templates/phase-4.md\n\n3. **Be intellectually honest about causation**\n\nComparative analysis shows WHAT differs, not always WHY:\n- With observational data, you usually can't prove causation\n- Multiple explanations may fit the data\n- Acknowledge uncertainty and alternative explanations\n\n4. **Decompose complex differences**\n\nWhen metrics differ, break them into components:\n- Revenue = Customers Ã— Revenue per Customer\n- Revenue per Customer = Transactions per Customer Ã— Revenue per Transaction\n- Identify which component drives the overall difference\n\n**Common Rationalization:** \"I found the difference, that's enough\"\n**Reality:** Finding the difference is half the job. Explaining WHY is equally important.\n\n**Common Rationalization:** \"This factor correlates with the difference, so it's the cause\"\n**Reality:** Correlation â‰  causation. Multiple factors may correlate. Be cautious about causal claims.\n\n**Common Rationalization:** \"I'll ignore confounds since I can't measure them\"\n**Reality:** Acknowledge unmeasured confounds explicitly. They limit your conclusions but shouldn't be ignored.\n\n---\n\n## Phase 5: Conclusions and Recommendations\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Summarized key differences and their magnitudes\n- [ ] Explained most likely drivers of differences\n- [ ] Assessed confidence in findings\n- [ ] Identified actionable insights\n- [ ] Made specific recommendations based on comparison\n- [ ] Documented limitations and caveats\n- [ ] Saved to `05 - conclusions-and-recommendations.md`\n- [ ] Updated `00 - overview.md` with comparison summary\n\n### Instructions\n\n1. **Synthesize findings into clear conclusions**\n\nCreate `analysis/[session-name]/05-conclusions-and-recommendations.md` with: ./templates/phase-5.md\n\n2. **Update overview document**\n\nUpdate: `00 - overview.md`\n\nAdd at the end:\n\n```markdown\n## Comparison Summary\n\n**Groups Compared:** [Groups]\n\n**Time Period:** [Date range]\n\n**Comparison Completed:** [Date]\n\n---\n\n## Key Differences Identified\n\n1. **[Difference 1]:** [Brief description with magnitude]\n   - Driver: [Primary explanation]\n   - Confidence: [High/Medium/Low]\n\n2. **[Difference 2]:** [Brief description with magnitude]\n   - Driver: [Primary explanation]\n   - Confidence: [High/Medium/Low]\n\n3. **[Difference 3]:** [Brief description with magnitude]\n   - Driver: [Primary explanation]\n   - Confidence: [High/Medium/Low]\n\n---\n\n## Top Recommendations\n\n1. **[Recommendation 1]:** [One sentence]\n   - Expected impact: [Magnitude]\n\n2. **[Recommendation 2]:** [One sentence]\n   - Expected impact: [Magnitude]\n\n---\n\n## File Index\n\n- 01 - Comparison Definition\n- 02 - Segment Definition\n- 03 - Metric Comparison\n- 04 - Difference Explanation\n- 05 - Conclusions and Recommendations\n```\n\n3. **Communicate findings to user**\n\nPresent conclusions clearly:\n- Lead with directional summary (which group \"wins\" and why)\n- Quantify key differences with specific numbers\n- Explain drivers of differences\n- Acknowledge uncertainty and limitations\n- Provide actionable recommendations\n- Suggest follow-up questions\n\n**Common Rationalization:** \"I'll just present all the numbers and let the user draw conclusions\"\n**Reality:** Your job is to interpret and synthesize. Provide clear conclusions, not just data dumps.\n\n**Common Rationalization:** \"I'm 100% confident in these conclusions\"\n**Reality:** Be honest about confidence levels and limitations. Overconfidence undermines credibility.\n\n**Common Rationalization:** \"Comparison complete, no follow-up needed\"\n**Reality:** Comparisons often raise more questions than they answer. Identify high-value follow-up investigations.\n\n---\n\n## Common Rationalizations\n\n### \"The groups are obviously different, I'll skip formal definition\"\n**Why this is wrong:** \"Obvious\" differences often have unstated assumptions. Explicit definition prevents misunderstandings and ensures you're answering the right question.\n\n**Do instead:** Complete Phase 1 fully. Define groups, metrics, and materiality thresholds explicitly.\n\n### \"Sample sizes look fine, I'll skip validation\"\n**Why this is wrong:** Sample size is only one aspect. Data quality, temporal alignment, and outliers can invalidate comparisons even with large samples.\n\n**Do instead:** Validate groups thoroughly in Phase 2. Check quality, coverage, and comparability.\n\n### \"I'll just compare raw totals\"\n**Why this is wrong:** Raw totals are misleading when groups have different sizes. $500K vs $400K tells you nothing if one group is 2x the size of the other.\n\n**Do instead:** Normalize by customers, days, or transactions. Compare per-capita or rate metrics.\n\n### \"Northeast has higher revenue, that's the finding\"\n**Why this is wrong:** Stating WHAT differs without explaining WHY provides limited value. The explanation is where actionable insights live.\n\n**Do instead:** Decompose differences. Explain whether higher revenue comes from more customers, higher per-customer value, different mix, etc.\n\n### \"These groups are different, so one must be better\"\n**Why this is wrong:** Different doesn't mean better. Context matters. Lower-revenue group might serve a different market, have different goals, or optimize for different metrics.\n\n**Do instead:** Interpret differences in context. Consider whether \"better\" is even the right framing.\n\n### \"I found a correlation, so that's the cause\"\n**Why this is wrong:** Correlation â‰  causation. Many factors correlate with outcomes without causing them.\n\n**Do instead:** Be cautious with causal language. Say \"associated with\" or \"correlated with\" rather than \"caused by\" unless you have experimental evidence.\n\n### \"I'll ignore the confounds I can't measure\"\n**Why this is wrong:** Unmeasured confounds don't disappear by ignoring them. They limit what you can conclude.\n\n**Do instead:** Explicitly acknowledge unmeasured confounds in Phase 4. Explain how they limit causal interpretation.\n\n### \"I'll recommend that Southeast copy Northeast exactly\"\n**Why this is wrong:** You don't know if differences are due to replicable practices or immutable characteristics (market size, demographics, etc.).\n\n**Do instead:** Recommend further investigation to understand whether differences are actionable or structural.\n\n### \"This comparison answered the question completely\"\n**Why this is wrong:** Comparisons typically reveal new questions about root causes, generalizability, and interventions.\n\n**Do instead:** Identify high-value follow-up questions in Phase 5. Guide next investigations.\n\n### \"Statistical significance means it matters\"\n**Why this is wrong:** With large samples, tiny differences can be statistically significant but practically meaningless.\n\n**Do instead:** Focus on practical significance (materiality threshold). A 1% difference might be \"significant\" but not meaningful.\n\n---\n\n## Summary\n\nThis skill ensures rigorous, fair comparisons by:\n\n1. **Defining comparisons explicitly:** Clear groups, metrics, and materiality thresholds prevent unstated assumptions\n2. **Validating segment quality:** Ensuring groups are comparable prevents invalid comparisons\n3. **Comparing systematically:** Multi-metric analysis reveals patterns that single metrics miss\n4. **Explaining differences:** Understanding WHY groups differ is as important as knowing WHAT differs\n5. **Acknowledging limitations:** Honest assessment of confounds and causation builds credibility\n6. **Providing actionable insights:** Converting findings into recommendations and follow-up questions\n\nFollow this process and you'll deliver fair, rigorous comparisons that explain not just WHAT differs but WHY, identify actionable opportunities, and guide follow-up investigations.\n",
        "plugins/datapeeker/skills/comparative-analysis/templates/phase-1.md": "# Comparison Definition\n\n## Comparison Objective\n\n[Clear statement of what you're comparing and why]\n\nExample: \"Compare sales performance between Northeast and Southeast regions to understand regional differences and identify growth opportunities.\"\n\n## Comparison Type\n\n**Type:** [Select one: Segment Comparison / Cohort Comparison / Time Period Comparison / Before-After Comparison]\n\n**Explanation:** [Brief description of comparison type]\n\nExamples:\n- **Segment Comparison:** Comparing naturally-occurring groups (regions, product categories, customer tiers)\n- **Cohort Comparison:** Comparing groups defined by shared timing (signup month, first purchase year)\n- **Time Period Comparison:** Comparing same metrics across different time ranges (Q1 vs Q2, 2024 vs 2025)\n- **Before-After Comparison:** Comparing metrics before and after an event/intervention (pre-launch vs post-launch)\n\n## Groups Being Compared\n\n### Group 1: [Name/Description]\n**Definition:** [How this group is defined in the data]\n**Size/Scope:** [Expected magnitude - approximate # of records, customers, transactions]\n**Time Period:** [What time range for this group]\n\nExample:\n### Group 1: Northeast Region\n**Definition:** Customers where `region = 'Northeast'`\n**Size/Scope:** ~15,000 transactions from approximately 2,500 customers\n**Time Period:** January 2024 - March 2024 (Q1)\n\n### Group 2: [Name/Description]\n**Definition:** [How this group is defined]\n**Size/Scope:** [Expected magnitude]\n**Time Period:** [What time range]\n\n### Group 3: [If comparing more than 2 groups]\n...\n\n## Comparison Metrics\n\n[What specific metrics will be compared across groups?]\n\n**Primary Metrics:**\n1. **[Metric name]:** [Definition, why it matters]\n2. **[Metric name]:** [Definition, why it matters]\n3. **[Metric name]:** [Definition, why it matters]\n\nExample:\n**Primary Metrics:**\n1. **Total Revenue:** Total sales dollars - measures overall volume\n2. **Average Transaction Size:** Revenue per transaction - measures customer purchasing behavior\n3. **Customer Count:** Unique customers - measures market penetration\n4. **Transactions per Customer:** Average purchases per customer - measures engagement/loyalty\n\n**Secondary Metrics:**\n[Additional metrics that provide context]\n\n1. **[Metric name]:** [Why this is useful context]\n2. **[Metric name]:** [Why this is useful context]\n\nExample:\n**Secondary Metrics:**\n1. **Product Mix:** Distribution of sales across categories - helps explain revenue differences\n2. **Temporal Distribution:** Day-of-week patterns - checks for operational differences\n\n## Fair Comparison Principles\n\n[What controls or adjustments are needed for fair comparison?]\n\n**Potential Confounds to Address:**\n1. **[Confound]:** [How to control for this]\n2. **[Confound]:** [How to control for this]\n\nExample:\n**Potential Confounds to Address:**\n1. **Time period differences:** Ensure both regions use same date range\n2. **Sample size:** Normalize metrics (per customer, per day) rather than raw totals\n3. **Seasonality:** If comparing different time periods, control for seasonal effects\n4. **Operational differences:** Check store hours, staffing levels if available\n\n**What constitutes \"different enough to matter\":**\n- [Define materiality threshold]\n\nExample: \"Difference of >10% in primary metrics or >20% in secondary metrics will be considered meaningful\"\n\n## Success Criteria\n\n[What would make this comparison complete and useful?]\n\nExample:\n- Identify which metrics differ significantly (>10%) between regions\n- Quantify the magnitude of key differences\n- Explain the top 2-3 drivers of differences\n- Provide actionable recommendations based on findings\n\n## Questions to Answer\n\n[Specific questions this comparison should answer]\n\n1. [Question 1]\n2. [Question 2]\n3. [Question 3]\n\nExample:\n1. Which region has higher revenue, and by how much?\n2. Is revenue difference due to more customers, higher transaction size, or both?\n3. Do regions differ in product preferences or purchasing patterns?\n4. What explains the observed differences?\n",
        "plugins/datapeeker/skills/comparative-analysis/templates/phase-2.md": "# Segment Definition and Validation\n\n## Objective\n\nVerify that comparison groups are well-defined in the data, have adequate sample sizes, and are suitable for comparison.\n\n---\n\n## Group 1: [Name]\n\n### Definition Query\n\n[Query that identifies this group]\n\n```sql\n-- Example: Northeast region definition\nSELECT DISTINCT region\nFROM sales\nWHERE region IS NOT NULL\nORDER BY region;\n\n-- Verify 'Northeast' exists\n```\n\n**Results:** [Paste results]\n\n**Validation:** [Confirm group identifier exists and is spelled consistently]\n\n### Group Characteristics\n\n```sql\n-- Profile Group 1\nSELECT\n  COUNT(*) as total_transactions,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  COUNT(DISTINCT product_id) as unique_products,\n  MIN(transaction_date) as earliest_date,\n  MAX(transaction_date) as latest_date,\n  ROUND(SUM(amount), 2) as total_revenue,\n  ROUND(AVG(amount), 2) as avg_transaction\nFROM sales\nWHERE region = 'Northeast'\n  AND transaction_date BETWEEN '2024-01-01' AND '2024-03-31';\n```\n\n**Results:** [Paste results]\n\n**Profile:**\n- **Sample Size:** [Number of transactions, customers]\n- **Temporal Coverage:** [Date range, any gaps?]\n- **Revenue Scale:** [Total and average]\n- **Product Diversity:** [Number of unique products]\n\n### Data Quality Check\n\n```sql\n-- Check for NULL values or anomalies in Group 1\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(customer_id) as non_null_customers,\n  COUNT(amount) as non_null_amounts,\n  COUNT(CASE WHEN amount < 0 THEN 1 END) as negative_amounts,\n  COUNT(CASE WHEN amount = 0 THEN 1 END) as zero_amounts,\n  COUNT(CASE WHEN amount > 10000 THEN 1 END) as extreme_amounts\nFROM sales\nWHERE region = 'Northeast'\n  AND transaction_date BETWEEN '2024-01-01' AND '2024-03-31';\n```\n\n**Results:** [Paste results]\n\n**Quality Assessment:**\n- Completeness: [% of records with required fields]\n- Anomalies: [Any concerning patterns?]\n- Outliers: [How many extreme values? Should they be excluded?]\n\n---\n\n## Group 2: [Name]\n\n[Repeat same structure as Group 1]\n\n### Definition Query\n...\n\n### Group Characteristics\n...\n\n### Data Quality Check\n...\n\n---\n\n## Group 3: [If applicable]\n\n[Repeat structure]\n\n---\n\n## Comparability Assessment\n\n### Sample Size Comparison\n\n| Group | Transactions | Customers | Date Range | Days of Data |\n|-------|--------------|-----------|------------|--------------|\n| [Group 1] | [count] | [count] | [range] | [days] |\n| [Group 2] | [count] | [count] | [range] | [days] |\n\n**Assessment:** [Are sample sizes adequate? Are they comparable? Any concerns?]\n\nExample: \"Both groups have >10,000 transactions and >2,000 customers. Sample sizes are adequate for comparison. Date ranges are identical (90 days), ensuring temporal comparability.\"\n\n### Data Quality Comparison\n\n| Group | Completeness % | Anomalies | Outliers |\n|-------|----------------|-----------|----------|\n| [Group 1] | [%] | [count/issues] | [count] |\n| [Group 2] | [%] | [count/issues] | [count] |\n\n**Assessment:** [Is data quality similar across groups? Any quality differences that could bias comparison?]\n\nExample: \"Data quality is comparable. Both groups have >99% completeness. Group 1 has 3 extreme values (>$10K), Group 2 has 5. Will include all data but note outlier sensitivity in analysis.\"\n\n### Temporal Alignment\n\n[Are time periods aligned? Any seasonal or calendar effects to consider?]\n\nExample: \"Both groups cover Q1 2024 (Jan 1 - Mar 31), so seasonality is controlled. Both include same number of weekends (13) and no major holidays affect one region more than the other.\"\n\n### Fair Comparison Checklist\n\n- [ ] Groups are clearly defined and non-overlapping\n- [ ] Sample sizes are adequate (typically >1000 transactions or >100 customers per group)\n- [ ] Time periods are aligned (same date range or controlled for seasonality)\n- [ ] Data quality is comparable across groups\n- [ ] Outliers are handled consistently\n- [ ] No obvious confounds that would make comparison unfair\n\n**Proceed to comparison?** [Yes/No - if No, explain what needs to be fixed]\n",
        "plugins/datapeeker/skills/comparative-analysis/templates/phase-3.md": "# Metric Comparison\n\n## Objective\n\nCompare all defined metrics across groups to identify meaningful differences.\n\n**Materiality Threshold:** [Restate from Phase 1]\n\nExample: \"Differences >10% in primary metrics or >20% in secondary metrics are considered meaningful\"\n\n---\n\n## Primary Metric 1: [Metric Name]\n\n### Rationale\n\n[Why this metric matters for the comparison]\n\n### Query\n\n```sql\n-- Calculate [metric] for each group\nSELECT\n  region as group_name,\n  COUNT(*) as transaction_count,\n  ROUND(SUM(amount), 2) as total_revenue,\n  ROUND(AVG(amount), 2) as avg_transaction_size,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  ROUND(SUM(amount) / COUNT(DISTINCT customer_id), 2) as revenue_per_customer\nFROM sales\nWHERE region IN ('Northeast', 'Southeast')\n  AND transaction_date BETWEEN '2024-01-01' AND '2024-03-31'\nGROUP BY region\nORDER BY total_revenue DESC;\n```\n\n### Results\n\n[Paste actual results]\n\n### Comparison Table\n\n| Metric | Northeast | Southeast | Difference | % Difference |\n|--------|-----------|-----------|------------|--------------|\n| Total Revenue | $458,920 | $392,140 | +$66,780 | +17.0% |\n| Avg Transaction | $94.50 | $89.25 | +$5.25 | +5.9% |\n| Customers | 2,847 | 2,615 | +232 | +8.9% |\n| Revenue/Customer | $161.20 | $149.98 | +$11.22 | +7.5% |\n\n### Observations\n\n**Magnitude:** [Describe the difference with specific numbers]\n\nExample: \"Northeast has 17% higher total revenue ($66,780 more), driven by both more customers (+8.9%) and slightly higher revenue per customer (+7.5%).\"\n\n**Materiality:** [Meets threshold? Meaningful?]\n\nExample: \"Total revenue difference (17%) exceeds materiality threshold (10%). This is a meaningful difference worthy of explanation.\"\n\n**Pattern:** [What does this tell us?]\n\nExample: \"Northeast advantage is multi-factorial: more customers AND higher per-customer value. Not just volume-driven.\"\n\n### Visualization\n\n[Use creating-visualizations skill for clarity]\n\n```\nGroup          Total Revenue\nNortheast      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ $458,920\nSoutheast      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ $392,140\n                                    (83% of NE)\n```\n\n---\n\n## Primary Metric 2: [Next Metric]\n\n[Repeat structure for each primary metric]\n\n---\n\n## Secondary Metrics\n\n[Compare all secondary metrics using same structure]\n\n### Product Mix Comparison\n\n```sql\nSELECT\n  region,\n  product_category,\n  COUNT(*) as transactions,\n  ROUND(SUM(amount), 2) as revenue,\n  ROUND(100.0 * SUM(amount) / SUM(SUM(amount)) OVER (PARTITION BY region), 1) as pct_of_region_revenue\nFROM sales\n  JOIN products ON sales.product_id = products.id\nWHERE region IN ('Northeast', 'Southeast')\n  AND transaction_date BETWEEN '2024-01-01' AND '2024-03-31'\nGROUP BY region, product_category\nORDER BY region, revenue DESC;\n```\n\n**Results:** [Paste results]\n\n**Comparison:**\n\n| Category | NE Revenue | NE % | SE Revenue | SE % | Difference |\n|----------|------------|------|------------|------|------------|\n| Electronics | $182,450 | 39.7% | $145,200 | 37.0% | +2.7pp |\n| Apparel | $135,890 | 29.6% | $128,340 | 32.7% | -3.1pp |\n| Home Goods | $98,120 | 21.4% | $75,890 | 19.4% | +2.0pp |\n| Other | $42,460 | 9.3% | $42,710 | 10.9% | -1.6pp |\n\n**Observations:** [What product mix differences exist?]\n\n---\n\n## Metrics Summary\n\n### Meaningful Differences (Exceed Materiality Threshold)\n\n[List all metrics where difference meets threshold]\n\n1. **Total Revenue:** Northeast +17.0% (meaningful)\n2. **Revenue per Customer:** Northeast +7.5% (below threshold but notable)\n3. **[Metric]:** [Group] [+/- %] ([meaningful/not meaningful])\n\n### Similar Metrics (Below Materiality Threshold)\n\n[List metrics that are similar between groups]\n\n1. **Average Transaction Size:** Only 5.9% difference (below 10% threshold)\n2. **[Metric]:** [% difference] (groups are similar on this dimension)\n\n### Directional Summary\n\n[High-level summary of how groups compare]\n\nExample:\n\"Northeast outperforms Southeast on volume metrics (customers, transactions, total revenue) by 8-17%. Per-customer and per-transaction metrics show smaller differences (6-8%), suggesting advantage is primarily scale-driven rather than fundamental behavior differences. Product mix is similar, with minor preferences for Electronics and Home Goods in Northeast.\"\n\n### Comparison Visualization\n\n[Overall summary table or chart]\n\n| Dimension | Winner | Magnitude |\n|-----------|--------|-----------|\n| Total Revenue | Northeast | +17% |\n| Customer Base | Northeast | +9% |\n| Transaction Size | Northeast | +6% |\n| Product Mix | Similar | <3pp diff |\n| Engagement | Similar | <5% diff |\n",
        "plugins/datapeeker/skills/comparative-analysis/templates/phase-4.md": "# Explaining the Differences\n\n## Differences to Explain\n\n[List the key differences identified in Phase 3 that need explanation]\n\n1. **[Difference 1]:** [Brief description with magnitude]\n2. **[Difference 2]:** [Brief description with magnitude]\n3. **[Difference 3]:** [Brief description with magnitude]\n\nExample:\n1. **Total Revenue:** Northeast 17% higher than Southeast ($66,780 difference)\n2. **Customer Count:** Northeast 9% more customers than Southeast (232 customers)\n3. **Product Mix:** Northeast skews 3pp more toward Electronics\n\n---\n\n## Difference 1: [Description]\n\n### The Observed Difference\n\n[Restate the finding from Phase 3]\n\nExample: \"Northeast region has 17% higher total revenue than Southeast ($458,920 vs $392,140)\"\n\n### Hypotheses for Why This Difference Exists\n\n[List plausible explanations to investigate]\n\n1. **[Hypothesis 1]:** [Explanation]\n2. **[Hypothesis 2]:** [Explanation]\n3. **[Hypothesis 3]:** [Explanation]\n\nExample:\n1. **Market size:** Northeast serves larger population / more potential customers\n2. **Customer behavior:** Northeast customers purchase more frequently or at higher value\n3. **Operational:** Northeast has more stores, longer hours, or better staffing\n4. **Product availability:** Northeast carries different/better product mix\n5. **Pricing:** Northeast has different pricing that drives more revenue\n\n### Investigation: Hypothesis 1 - [Description]\n\n**How to test:** [What query or analysis would confirm/refute this?]\n\n```sql\n-- Example: Test if customer count difference explains revenue difference\nSELECT\n  region,\n  COUNT(DISTINCT customer_id) as customers,\n  ROUND(SUM(amount), 2) as revenue,\n  ROUND(SUM(amount) / COUNT(DISTINCT customer_id), 2) as revenue_per_customer\nFROM sales\nWHERE region IN ('Northeast', 'Southeast')\n  AND transaction_date BETWEEN '2024-01-01' AND '2024-03-31'\nGROUP BY region;\n```\n\n**Results:** [Paste results]\n\n**Analysis:**\n\n[Evaluate the hypothesis]\n\nExample:\n\"Northeast has 232 more customers (8.9% more). If customer count were the ONLY difference, and per-customer revenue were equal, revenue difference would be:\n- Expected: 8.9% revenue advantage\n- Actual: 17.0% revenue advantage\n- **Conclusion:** Customer count explains ~half the revenue gap. Other factors must explain the remaining 8pp.\"\n\n**Verdict:** [Fully explains / Partially explains / Does not explain]\n\n### Investigation: Hypothesis 2 - [Next hypothesis]\n\n[Repeat structure]\n\n### Investigation: Hypothesis 3 - [Next hypothesis]\n\n[Repeat structure]\n\n---\n\n## Difference Decomposition: [Metric]\n\n[Break down the difference into component factors]\n\n### Mathematical Decomposition\n\n[If applicable, show how metric can be broken into factors]\n\nExample:\n```\nTotal Revenue = Customers Ã— Revenue per Customer\nTotal Revenue = Customers Ã— Transactions per Customer Ã— Revenue per Transaction\n\nNortheast revenue advantage:\n- Customer count effect: +8.9% (232 more customers)\n- Revenue per customer effect: +7.5% ($11.22 more per customer)\n- Combined multiplicative effect: ~17%\n\nConclusion: Both volume (more customers) AND value (higher per-customer) drive the difference\n```\n\n### Factor Contribution\n\n[Quantify relative importance of factors]\n\nExample:\n| Factor | Contribution to Revenue Gap |\n|--------|----------------------------|\n| More customers | ~50% of gap |\n| Higher revenue/customer | ~45% of gap |\n| Interaction effect | ~5% of gap |\n\n**Interpretation:** [What does this mean?]\n\nExample: \"Northeast advantage is roughly equally split between having more customers (volume) and extracting more value from each customer (intensity). This is a dual advantage.\"\n\n---\n\n## Difference 2: [Next Difference]\n\n[Repeat full structure for next major difference]\n\n---\n\n## Confound Analysis\n\n### Potential Confounds Checked\n\n[List factors that could create spurious differences - revisit Phase 1 confounds]\n\n1. **[Confound]:** [How you checked for this]\n   - **Result:** [Found to be issue / Not an issue]\n\nExample:\n1. **Time period alignment:** Checked that both regions use identical date range (2024-01-01 to 2024-03-31)\n   - **Result:** Confirmed aligned - not a confound\n\n2. **Seasonality:** Checked if regions have different seasonal patterns\n   - Query: Compared month-over-month growth for both regions\n   - **Result:** Similar seasonal patterns - not a confound\n\n3. **Holidays/Events:** Checked if regions had different holiday coverage\n   - Analysis: Both regions include same holidays (New Year, Valentine's, etc.)\n   - **Result:** Equal holiday exposure - not a confound\n\n4. **Data quality:** Checked if one region has more NULL values or data issues\n   - Reviewed Phase 2 quality checks\n   - **Result:** Both regions >99% complete - not a confound\n\n### Confounds That Could Not Be Ruled Out\n\n[Factors that might affect comparison but can't be tested with available data]\n\nExample:\n1. **Store count:** No data on number of stores per region - Northeast might have more locations\n2. **Operating hours:** No data on hours - differences could be operational rather than demand-based\n3. **Marketing spend:** No data on regional marketing - could drive customer acquisition differences\n4. **Competitive intensity:** No data on competitors - one region might have less competition\n\n**Impact on conclusions:** [How does this limit your confidence?]\n\nExample: \"Cannot determine if Northeast advantage stems from market characteristics (larger market, less competition) or operational excellence (better execution). Causal interpretation is limited.\"\n\n---\n\n## Explanation Summary\n\n### Key Findings\n\n[Synthesize what you learned about WHY differences exist]\n\n1. **[Finding 1]:** [Explanation with supporting evidence]\n2. **[Finding 2]:** [Explanation with supporting evidence]\n3. **[Finding 3]:** [Explanation with supporting evidence]\n\nExample:\n1. **Dual driver model:** Northeast revenue advantage comes from both more customers (+9%) AND higher value per customer (+7.5%)\n2. **Product preference:** Northeast's 3pp skew toward Electronics (higher margin category) contributes to revenue/customer advantage\n3. **Engagement similarity:** Transaction frequency is similar across regions - difference is not about customer engagement\n\n### Causal vs Correlational\n\n**What we can say with confidence:**\n- [Patterns that are clearly demonstrated]\n\n**What remains uncertain:**\n- [Causal questions that can't be answered with available data]\n\nExample:\n**What we can say with confidence:**\n- Northeast generates 17% more revenue through both volume and value advantages\n- Product mix contributes modestly to per-customer revenue difference\n- Customer engagement (transactions per customer) is similar\n\n**What remains uncertain:**\n- Whether Northeast advantage is due to market size, operations, or competitive dynamics\n- Whether Southeast could achieve Northeast performance levels with operational changes\n- Root cause of customer count difference (market size vs acquisition effectiveness)\n\n### Alternative Explanations Not Ruled Out\n\n[Acknowledge what else could explain the patterns]\n\nExample:\n1. Northeast might serve fundamentally different customer demographics (can't test without demographic data)\n2. Regions might have different store footprints affecting convenience (no store location data)\n3. Historical or seasonal factors beyond the Q1 window analyzed (limited to 3 months)\n",
        "plugins/datapeeker/skills/comparative-analysis/templates/phase-5.md": "# Conclusions and Recommendations\n\n## Comparison Summary\n\n**Groups Compared:** [List groups]\n\n**Time Period:** [Date range]\n\n**Sample Sizes:** [Transactions, customers per group]\n\nExample:\n**Groups Compared:** Northeast vs Southeast regions\n\n**Time Period:** Q1 2024 (January 1 - March 31)\n\n**Sample Sizes:** Northeast: 4,857 transactions, 2,847 customers | Southeast: 4,393 transactions, 2,615 customers\n\n---\n\n## Key Findings\n\n### Finding 1: [Concise finding statement]\n\n**Magnitude:** [Quantify the difference]\n\n**Explanation:** [Why this difference exists]\n\n**Confidence:** [High / Medium / Low]\n\nExample:\n### Finding 1: Northeast generates 17% more revenue than Southeast\n\n**Magnitude:** $66,780 higher revenue in Q1 ($458,920 vs $392,140)\n\n**Explanation:** Revenue advantage stems from two factors:\n1. Larger customer base (+232 customers, +8.9%)\n2. Higher revenue per customer (+$11.22, +7.5%)\n\nProduct mix contributes modestly - Northeast's 3pp preference for Electronics adds ~$8K to the gap.\n\n**Confidence:** High - robust sample sizes, consistent across metrics, confounds controlled\n\n### Finding 2: [Next finding]\n\n[Repeat structure]\n\n### Finding 3: [Next finding]\n\n[Repeat structure]\n\n---\n\n## Directional Summary\n\n[High-level takeaway from the comparison]\n\nExample:\n\"Northeast outperforms Southeast on nearly all metrics, with advantages concentrated in customer acquisition (9% more customers) and per-customer value (8% higher). The regions show similar customer engagement patterns (transactions per customer within 5%), suggesting behavioral similarity despite outcome differences. This points to market or operational factors rather than customer behavior as drivers of regional performance gap.\"\n\n---\n\n## Confidence Assessment\n\n### High Confidence Conclusions\n\n[What you can state with strong evidence]\n\n1. [Conclusion with reasoning]\n2. [Conclusion with reasoning]\n\nExample:\n1. **Northeast revenue advantage is real and substantial (17%):** Based on 90 days of data, >9,000 transactions, robust across multiple metrics\n2. **Advantage is dual-source (volume + value):** Mathematical decomposition clearly shows both factors contribute ~equally\n\n### Medium Confidence Conclusions\n\n[What is likely but has some uncertainty]\n\n1. [Conclusion with caveat]\n2. [Conclusion with caveat]\n\nExample:\n1. **Product mix contributes modestly:** Electronics preference explains ~$8K of $67K gap, but product-level data has some attribution uncertainty\n2. **Regions have similar customer engagement:** Similar transaction frequency, but limited to 3-month window\n\n### Low Confidence / Uncertain\n\n[What remains unclear or speculative]\n\n1. [Uncertainty with explanation]\n2. [Uncertainty with explanation]\n\nExample:\n1. **Root cause of customer count difference:** Could be market size, acquisition effectiveness, or historical factors - cannot determine from available data\n2. **Sustainability of patterns:** Only 3 months of data - unclear if patterns hold year-round\n\n---\n\n## Limitations and Caveats\n\n### Data Limitations\n\n[What data constraints limit conclusions?]\n\n1. **[Limitation]:** [Impact on analysis]\n2. **[Limitation]:** [Impact on analysis]\n\nExample:\n1. **No operational data:** Cannot determine if differences stem from store count, hours, or staffing\n2. **Limited time window:** 3 months may not capture full seasonal patterns or year-round dynamics\n3. **No customer demographics:** Cannot control for population characteristics that might differ by region\n\n### Analytical Limitations\n\n[What methodological constraints exist?]\n\n1. **[Limitation]:** [Impact]\n2. **[Limitation]:** [Impact]\n\nExample:\n1. **Observational data only:** Cannot make causal claims about WHY regions differ\n2. **No statistical testing:** Differences described but not tested for statistical significance\n3. **Potential unmeasured confounds:** Store characteristics, competitive landscape, marketing spend not available\n\n### Scope Limitations\n\n[What was excluded from comparison?]\n\nExample:\n1. **Costs not included:** Compared revenue only, not profitability\n2. **Other regions excluded:** Binary comparison (NE vs SE) - West and Midwest not analyzed\n3. **Customer lifetime value:** Single quarter snapshot, not long-term value\n\n---\n\n## Actionable Insights\n\n[What can be done with these findings?]\n\n### Insight 1: [Actionable statement]\n\n**The Opportunity:** [What this insight suggests is possible]\n\n**Why It Matters:** [Business impact or value]\n\n**Supporting Evidence:** [Reference to findings]\n\nExample:\n### Insight 1: Southeast has potential to close 17% revenue gap with Northeast\n\n**The Opportunity:** If Southeast could achieve Northeast's performance levels, it would generate additional $66K per quarter ($265K annually)\n\n**Why It Matters:** Material revenue growth opportunity (~17% increase) from existing operations\n\n**Supporting Evidence:** Northeast demonstrates that $161/customer revenue is achievable; Southeast currently at $150/customer\n\n### Insight 2: [Next insight]\n\n[Repeat structure]\n\n---\n\n## Recommendations\n\n### Priority 1: [High-priority recommendation]\n\n**Recommendation:** [Specific action to take]\n\n**Rationale:** [Why this matters, based on comparison findings]\n\n**Expected Impact:** [What change would this drive]\n\n**Implementation:** [How to do this]\n\n**Data/Analysis Needed:** [What additional investigation would help]\n\nExample:\n### Priority 1: Investigate root cause of customer count difference\n\n**Recommendation:** Conduct deep-dive analysis to understand why Northeast has 9% more customers than Southeast\n\n**Rationale:** Customer acquisition accounts for ~50% of revenue gap. Understanding whether this stems from market size, operational factors, or acquisition effectiveness is critical to determining if Southeast can close the gap.\n\n**Expected Impact:** If gap is operational/tactical (not market size), could guide strategies to boost Southeast customer base by up to 232 customers (~$36K quarterly revenue)\n\n**Implementation:**\n1. Gather data on: store count, market population, marketing spend, competitive intensity per region\n2. Use guided-investigation skill to answer: \"Why does Northeast have more customers?\"\n3. If operational factors identified, pilot interventions in Southeast to test effectiveness\n\n**Data/Analysis Needed:**\n- Store locations and square footage by region\n- Regional population and demographics\n- Marketing spend by region\n- Customer acquisition cost and channel mix\n\n### Priority 2: [Next recommendation]\n\n[Repeat structure]\n\n### Priority 3: [Next recommendation]\n\n[Repeat structure]\n\n---\n\n## Follow-Up Questions\n\n[What questions does this comparison raise for future investigation?]\n\n1. **[Question]:** [Why this matters]\n   - **Suggested process:** [hypothesis-testing / guided-investigation / comparative-analysis / exploratory-analysis]\n   - **Data required:** [What you'd need]\n\nExample:\n1. **Why does Northeast have more customers?**\n   - **Suggested process:** `guided-investigation` - decompose into market, operational, and acquisition factors\n   - **Data required:** Store count, market demographics, marketing spend, acquisition channels\n\n2. **Can Southeast replicate Northeast's product mix?**\n   - **Suggested process:** `hypothesis-testing` - test if shifting SE toward Electronics would improve revenue/customer\n   - **Data required:** Product availability by region, Electronics demand elasticity\n\n3. **Do patterns hold year-round?**\n   - **Suggested process:** `comparative-analysis` - repeat comparison for Q2, Q3, Q4 to check consistency\n   - **Data required:** Full year of transaction data\n\n---\n\n## Comparison Validity\n\n[Final assessment of how fair and valid the comparison was]\n\n**Fair Comparison Checklist:**\n- [x] Groups clearly defined and non-overlapping\n- [x] Time periods aligned (both Q1 2024)\n- [x] Sample sizes adequate (>2,500 customers per group)\n- [x] Data quality comparable (>99% complete both groups)\n- [x] Known confounds controlled (time, seasonality, data quality)\n- [ ] All potential confounds addressed (operational factors unknown)\n\n**Overall Validity:** [High / Medium / Low]\n\n**Reasoning:** [Explain validity assessment]\n\nExample:\n**Overall Validity:** Medium-High\n\n**Reasoning:** Comparison is well-controlled for temporal and data quality factors. Sample sizes are robust. Primary limitation is inability to control for operational differences (store count, hours, staffing) which may explain some or all of observed differences. Comparison validly describes WHAT differs but has limited ability to determine WHY.\n",
        "plugins/datapeeker/skills/creating-visualizations/SKILL.md": "---\nname: creating-visualizations\ndescription: Component skill for creating effective visualizations (terminal-based and image-based) in DataPeeker analysis sessions\n---\n\n# Creating Visualizations\n\n## Purpose\n\nThis component skill guides creation of clear, effective visualizations for analytics documentation. Use it when:\n- Presenting query results in a more visual format\n- Need to reveal patterns that are hard to see in raw numbers\n- Creating reports or documentation that will be read by stakeholders\n- Documenting data workflows, lineage, or database schemas\n- Referenced by process skills requiring data visualization\n\n**Supports two approaches:**\n- **Terminal-based** (plotext, sparklines, etc.) - For interactive analysis\n- **Image-based** (Kroki: Mermaid, GraphViz, Vega-Lite) - For reports and complex diagrams\n\n## Prerequisites\n\n- Query results obtained and interpreted\n- Understanding of patterns to highlight (use `interpreting-results` skill)\n- Analysis documented in markdown files\n- Clear communication goal for the visualization\n\n## Visualization Creation Process\n\nCreate a TodoWrite checklist for the 4-phase visualization process:\n\n```\nPhase 1: Choose Visualization Type\nPhase 2: Structure Data for Display\nPhase 3: Create Visualization\nPhase 4: Annotate with Context\n```\n\nMark each phase as you complete it. Include visualizations in numbered markdown files alongside queries and interpretations.\n\n---\n\n## Phase 1: Choose Visualization Type\n\n**Goal:** Select the right visualization format for your data and communication goal.\n\n### Visualization Selection Decision Tree\n\nAsk these questions in order:\n\n**1. What type of data am I visualizing?**\n\n- **Single summary statistic** â†’ Callout box or highlighted metric\n- **List of values** â†’ Table or ranked list\n- **Distribution across categories** â†’ Bar chart (ASCII or markdown)\n- **Time series** â†’ Line chart (sparkline) or time table\n- **Comparison between groups** â†’ Side-by-side table or grouped bars\n- **Part-to-whole relationship** â†’ Percentage table or ASCII pie chart\n- **Correlation or relationship** â†’ Scatter (character plot) or correlation matrix\n\n**2. What is my primary communication goal?**\n\n- **Show exact values** â†’ Table with clear formatting\n- **Show relative magnitudes** â†’ Bar chart or ranked list\n- **Show trends over time** â†’ Sparkline or time series table\n- **Show distribution shape** â†’ Histogram (ASCII)\n- **Show ranking** â†’ Ordered list or horizontal bars\n- **Show proportions** â†’ Percentage table with bars\n\n**3. How many data points?**\n\n- **1-5 values** â†’ Callout boxes or simple list\n- **6-20 values** â†’ Table or bar chart\n- **21-50 values** â†’ Grouped table or histogram\n- **50+ values** â†’ Summary statistics + histogram, or top/bottom N\n\n**4. Who is the audience?**\n\n- **Technical analysts** â†’ Full tables with precision\n- **Business stakeholders** â†’ Simplified visuals with key takeaways\n- **Mixed audience** â†’ Visual summary + detailed table\n\n### Available Visualization Types\n\n**DataPeeker supports two complementary approaches:**\n\n#### Terminal-Based Formats (Primary for analysis):\n1. **Markdown Tables** - Structured data with alignment\n2. **ASCII Bar Charts** - Visual magnitude comparison (plotext, termgraph)\n3. **Sparklines** - Compact trend indicators (sparklines library)\n4. **ASCII Histograms** - Distribution visualization (plotext)\n5. **Callout Boxes** - Highlighting key metrics\n6. **Ranked Lists** - Ordered items with context\n7. **Comparison Tables** - Side-by-side metrics\n8. **Line Plots** - Time series (plotext, asciichartpy)\n\n#### Image-Based Formats (For reports and complex diagrams):\n1. **Mermaid** - Flowcharts, Gantt charts, workflows\n2. **GraphViz** - Network graphs, data lineage, hierarchies\n3. **Vega-Lite** - Statistical charts (bar, line, scatter)\n4. **ERD/DBML** - Database schemas\n\n**Choose based on:**\n- What pattern you want to communicate\n- Where the output will be viewed (terminal vs report)\n- Complexity of the visualization needed\n\n---\n\n## Phase 2: Structure Data for Display\n\n**Goal:** Organize and format data for effective visualization.\n\n### Data Preparation Checklist\n\nBefore creating visualization:\n\n**1. Sort appropriately:**\n```markdown\nFor ranked data:\n- Sort by the metric you want to emphasize (descending for \"top N\")\n- Consider: Alphabetical only if order doesn't matter\n\nFor time series:\n- Sort chronologically (oldest to newest, or newest first if recent matters)\n\nFor categorical:\n- Sort by frequency, magnitude, or logical grouping\n- Avoid: Random or database-default ordering\n```\n\n**2. Round to appropriate precision:**\n```markdown\nExamples:\n- Revenue: Round to thousands or whole dollars (not $1,234.56789)\n- Percentages: 1-2 decimal places (14.3%, not 14.285714%)\n- Counts: Whole numbers only (1,234 not 1234.0)\n- Ratios: 2-3 significant figures (2.4x not 2.3567x)\n\nRule: Show precision that matches the certainty of your data\n```\n\n**3. Add calculated columns:**\n```markdown\nUseful additions:\n- Percentage of total\n- Difference from average/baseline\n- Rank or percentile\n- Running totals or moving averages\n- Year-over-year change\n```\n\n**4. Consider grouping:**\n```markdown\nFor large datasets:\n- Show Top N + \"Other\" row\n- Group by logical categories\n- Use ranges/buckets for continuous data\n- Separate outliers from main distribution\n```\n\n**5. Format for readability:**\n```markdown\nBest practices:\n- Add thousand separators (1,234 not 1234)\n- Use consistent decimal places within columns\n- Align numbers right, text left\n- Include units in headers ($, %, units)\n```\n\n---\n\n## Phase 3: Create Visualization\n\n**Goal:** Build the actual visualization using appropriate format and tools.\n\n### Two Visualization Approaches\n\nDataPeeker supports two complementary visualization approaches:\n\n#### 1. Terminal-Based Visualizations (Primary)\n\n**Use for:**\n- Interactive terminal/Jupyter notebook analysis\n- Quick data exploration\n- Markdown documentation that stays in terminal\n- Fast iteration without external dependencies\n\n**Available formats:**\n1. **Markdown Tables** - Structured data with multiple columns, exact values\n2. **ASCII Bar Charts** - Visual magnitude comparison, relative sizes\n3. **Sparklines** - Compact trend indicators with Unicode characters\n4. **ASCII Histograms** - Distribution visualization, shape and spread\n5. **Callout Boxes** - Highlighting key metrics or insights\n6. **Ranked Lists** - Top/bottom N items with narrative context\n7. **Comparison Tables** - Side-by-side metrics across segments or time\n8. **Line Plots** - Time series and trends\n\n**â†’ See [terminal-formats.md](./terminal-formats.md) for implementation**\n\n#### 2. Image-Based Visualizations (via Kroki)\n\n**Use for:**\n- Reports and presentations (embedded images)\n- Complex diagrams (workflows, data lineage, relationships)\n- Database schemas and architecture\n- Documentation that needs to be viewed outside terminal\n- High-quality charts for stakeholder communication\n\n**Available formats:**\n1. **Mermaid** - Flowcharts, Gantt charts, sequence diagrams\n2. **GraphViz** - Network graphs, data lineage, hierarchies\n3. **Vega-Lite** - Statistical charts (bar, line, scatter, histograms)\n4. **D2** - Modern diagrams, architecture, data models\n5. **ERD/DBML** - Database schemas and relationships\n\n**â†’ See [image-formats.md](./image-formats.md) for implementation**\n\n### Choosing Between Terminal and Image Formats\n\n**Use Terminal formats when:**\n- Working interactively in analysis session\n- Output stays in markdown/terminal\n- Quick iteration and exploration\n- Simple charts and tables\n\n**Use Image formats when:**\n- Creating final reports or presentations\n- Visualizing complex relationships (data lineage, workflows)\n- Documenting database schemas\n- Output needs to be embedded in documents/web\n- Audience views outside terminal environment\n\n**Can use both:**\n- Terminal for exploration â†’ Image for final report\n- Tables (terminal) + Diagrams (image) in same document\n\n### âš ï¸ CRITICAL: Tool Usage Requirements\n\n**MANDATORY:** All visualizations (bar charts, line plots, histograms, sparklines, scatter plots) **MUST** use established visualization tools. **NEVER create these manually.**\n\n**âœ… ALLOWED - Manual Creation:**\n- Markdown tables with exact values\n- Callout boxes and formatted text\n- Ranked lists with exact numbers\n\n**âŒ PROHIBITED - Manual Creation:**\n- Bar charts (no manual â–ˆ characters)\n- Line plots or time series (no manual * or - characters)\n- Histograms\n- Sparklines (no manual â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ characters)\n- Any visualization requiring scaling or positioning\n\n### Implementation Details\n\n**ðŸ“„ For visualization implementations, use these guides:**\n\n#### Terminal-Based Visualizations\n**[terminal-formats.md](./terminal-formats.md)**\n\nThis document provides:\n- **Mandatory tool usage principles** (read this first!)\n- **Quick Start guide** with tool installation (plotext, asciichartpy, termgraph, sparklines)\n- **Complete code examples** for each visualization type using proper tools\n- **SQLite integration examples** for generating visualizations from query results\n\n**The rule:** If it visualizes relative magnitudes, trends, or distributions â†’ USE A TOOL. If it's exact numbers in a table â†’ Manual creation is fine.\n\n#### Image-Based Visualizations\n**[image-formats.md](./image-formats.md)**\n\nThis document provides:\n- **Kroki overview** - Unified API for generating diagrams from text\n- **Quick Start guide** with Python examples and API usage\n- **Format selection guide** - When to use Mermaid vs GraphViz vs Vega-Lite\n- **Complete implementation guides** for each format in `formats/` directory:\n  - [Mermaid](./formats/mermaid.md) - Flowcharts, Gantt, sequences\n  - [GraphViz](./formats/graphviz.md) - Network graphs, data lineage\n  - [Vega-Lite](./formats/vega-lite.md) - Statistical charts\n- **DataPeeker integration examples** - Visualizing data workflows and schemas\n\n---\n\n## Phase 4: Annotate with Context\n\n**Goal:** Add context and guidance so visualization is self-explanatory.\n\n### Annotation Checklist\n\nEvery visualization should include:\n\n**1. Title/Caption:**\n```markdown\n## [Clear, descriptive title that states what is being shown]\n\nExample:\nâœ“ Good: \"Monthly Revenue by Product Category (Jan-Dec 2024)\"\nâœ— Bad: \"Revenue Chart\"\n```\n\n**2. Data source and date:**\n```markdown\n**Data source:** analytics.db, orders table\n**Time period:** Q4 2024 (Oct 1 - Dec 31)\n**Last updated:** 2025-11-18\n```\n\n**3. Key takeaway (above or below visualization):**\n```markdown\n**Key Finding:** Electronics drove 42.5% of Q4 revenue despite representing\nonly 15% of order volume, indicating premium product performance.\n```\n\n**4. Units and scale:**\n```markdown\n- Include $ or % symbols\n- Clarify if values are in thousands: ($000s)\n- Note if values are indexed or normalized\n- Specify timezone for timestamps\n```\n\n**5. Context for interpretation:**\n```markdown\n**Context notes:**\n- Q4 includes Black Friday/Cyber Monday (Nov 24-27)\n- New product line launched Oct 15, affecting Electronics category\n- Shipping delays in December may have suppressed orders\n```\n\n**6. Limitations and caveats:**\n```markdown\n**Caveats:**\n- Data excludes returns and cancellations\n- International orders converted to USD at average quarterly exchange rate\n- First week of October had incomplete data due to system migration\n```\n\n**7. What to look for:**\n```markdown\n**What to notice:**\n- Electronics peak in November (holiday season)\n- Clothing shows consistent decline (investigate seasonality)\n- Sports category smallest but growing fastest (+45% QoQ)\n```\n\n\n## Visualization Best Practices\n\n### DO:\n\n1. **Choose format based on communication goal, not convenience**\n   - Ask: \"What do I want the reader to notice first?\"\n   - Match visualization to insight you're highlighting\n\n2. **Make visualizations self-contained**\n   - Reader should understand without reading entire document\n   - Include title, units, source, key takeaway\n\n3. **Use consistent formatting within analysis**\n   - Same bar width for all bar charts\n   - Same precision for similar metrics\n   - Consistent color/symbol conventions (if using)\n\n4. **Highlight what matters**\n   - Use **bold** for most important values\n   - Put key finding at top or bottom\n   - Add ðŸ”¥, âš ï¸, âœ“ symbols sparingly for emphasis\n\n5. **Test readability**\n   - View in markdown preview (not just raw markdown)\n   - Check alignment and spacing\n   - Ensure visualization works in different font sizes\n\n6. **Layer detail progressively**\n   - Summary visualization first (bar chart, key metrics)\n   - Detailed table second (full data)\n   - Technical notes third (methodology, caveats)\n\n7. **Combine formats when helpful**\n   - Bar chart + exact values table\n   - Sparkline + summary statistics\n   - Visualization + narrative interpretation\n\n### DON'T:\n\n1. **Don't create visualizations for their own sake**\n   - If a simple table is clearer, use the table\n   - Visualization should reveal patterns, not obscure them\n\n2. **Don't use excessive precision**\n   - Revenue in dollars, not cents ($1,234 not $1,234.56)\n   - Percentages to 1 decimal place (14.3% not 14.285714%)\n\n3. **Don't hide important caveats**\n   - Data quality issues must be visible\n   - Exclusions and filters must be noted\n   - Sample size and time period must be clear\n\n4. **Don't use misleading scales**\n   - Bar charts should start at zero (not truncated y-axis)\n   - Be explicit if using non-zero baseline\n\n5. **Don't over-format**\n   - Too many symbols/colors creates visual noise\n   - Keep it simple and professional\n\n6. **Don't assume reader knows context**\n   - Define abbreviations\n   - Explain what metrics mean\n   - Note if using non-standard calculations\n\n7. **Don't forget the \"so what?\"**\n   - Every visualization needs an interpretation\n   - State implications, not just observations\n\n---\n\n## Common Visualization Patterns\n\n### Pattern 1: Before/After Comparison\n\n```markdown\n## Impact of Pricing Change (Oct 15, 2024)\n\n### Before Pricing Change (Oct 1-14)\n- Average Order Value: **$145.67**\n- Daily Orders: **234**\n- Daily Revenue: **$34,087**\n\n### After Pricing Change (Oct 15-31)\n- Average Order Value: **$127.23** (â†“ $18.44, -12.7%)\n- Daily Orders: **289** (â†‘ 55, +23.5%)\n- Daily Revenue: **$36,769** (â†‘ $2,682, +7.9%)\n\n**Net effect:** Lower prices increased volume enough to grow total revenue.\n```\n\n### Pattern 2: Distribution Summary\n\n**âš ï¸ Use plotext to create histograms - DO NOT create manually**\n\nShow distribution with summary statistics:\n\n```python\nimport plotext as plt\nimport statistics\n\n# Customer LTV values from query\nltv_values = [423, 687, 892, 2145, ...]  # Your data\n\nplt.hist(ltv_values, bins=7)\nplt.title('Customer Lifetime Value Distribution')\nplt.xlabel('Customer LTV ($)')\nplt.ylabel('Number of Customers')\nplt.show()\n\n# Show summary statistics\nprint(f\"\\nSummary Statistics:\")\nprint(f\"Median LTV: ${statistics.median(ltv_values):,.0f}\")\nprint(f\"Mean LTV: ${statistics.mean(ltv_values):,.0f}\")\nprint(f\"75th percentile: ${statistics.quantiles(ltv_values, n=4)[2]:,.0f}\")\n```\n\n**See terminal-formats.md Format 4 for complete histogram examples.**\n\n### Pattern 3: Segmentation Analysis\n\n**âœ… Tables are fine for exact values, use plotext/termgraph for visual breakdown**\n\n```markdown\n## Customer Segmentation by Purchase Behavior\n\n| Segment         | Customers | Avg Orders | Avg LTV | % of Revenue | Strategy      |\n|:----------------|----------:|-----------:|--------:|-------------:|:--------------|\n| **Champions**   |       234 |       18.3 |  $2,145 |        18.2% | VIP treatment |\n| **Loyal**       |     1,456 |        8.7 |    $892 |        47.3% | Retain & grow |\n| **Potential**   |     3,678 |        2.4 |    $287 |        38.5% | Nurture       |\n| **At Risk**     |       892 |        1.2 |    $156 |         5.1% | Win-back      |\n| **Lost**        |     2,134 |        1.0 |     $87 |         6.8% | Low priority  |\n\n**Key insight:** Top two segments (Champions + Loyal) are only 18% of customer\nbase but generate 66% of revenue. These 1,690 customers should receive majority\nof retention investment.\n```\n\n**For visual breakdown, use plotext:**\n```python\nimport plotext as plt\n\nsegments = ['Champions', 'Loyal', 'Potential', 'At Risk', 'Lost']\nrevenue = [501030, 1299552, 1055586, 139152, 185658]\n\nplt.simple_bar(segments, revenue, title='Revenue by Customer Segment')\nplt.xlabel('Segment')\nplt.ylabel('Revenue ($)')\nplt.show()\n```\n\n**See terminal-formats.md Format 2 for complete bar chart examples.**\n\n### Pattern 4: Time Series with Annotations\n\n**âš ï¸ Use plotext or asciichartpy - DO NOT create manually**\n\n```python\nimport plotext as plt\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nrevenue = [1.0, 1.1, 1.2, 1.3, 1.4, 1.5,\n           1.5, 1.6, 1.7, 1.7, 1.9, 2.0]  # Revenue in millions\n\nplt.plot(months, revenue)\nplt.title('Monthly Revenue Trend with Key Events')\nplt.xlabel('Month')\nplt.ylabel('Revenue ($M)')\nplt.show()\n\nprint(\"\\nKey Events:\")\nprint(\"- Oct 1: Q4 begins, seasonal uptick expected\")\nprint(\"- Oct 15: Pricing change (-10% on popular items)\")\nprint(\"- Nov 1: New product line launched (premium segment)\")\nprint(\"- Nov 24-27: Black Friday/Cyber Monday surge\")\nprint(\"\\nAnalysis: Revenue growth accelerated after new product launch (Nov),\")\nprint(\"suggesting demand for premium options. Pricing change impact unclear due to\")\nprint(\"seasonal overlap.\")\n```\n\n**See terminal-formats.md Format 8 for complete line plot examples.**\n\n### Pattern 5: Funnel Analysis\n\n**âœ… Tables for exact values, use plotext for visualization**\n\n```markdown\n## Purchase Funnel Conversion Rates\n\n| Step              | Count   | Conversion | Drop-off | Notes |\n|:------------------|--------:|-----------:|---------:|:------|\n| 1. Site Visitors  | 100,000 |     100.0% |        â€” |       |\n| 2. Product Viewers|  45,000 |      45.0% |    55.0% | High bounce rate |\n| 3. Add to Cart    |  12,000 |      26.7% |    73.3% |       |\n| 4. Begin Checkout |   8,500 |      70.8% |    29.2% | Cart abandonment |\n| 5. Complete       |   3,200 |      37.6% |    62.4% | Payment issues? |\n\n**Overall Conversion:** 3.2%\n\n**Problem areas:**\n1. **Bounce rate (55%):** Half of visitors leave without viewing products\n   - Action: Improve landing page, clearer value proposition\n\n2. **Cart abandonment (29%):** Losing 3,500 potential customers at checkout\n   - Action: Simplify checkout, add progress indicator\n\n3. **Checkout failure (62%):** Massive drop-off at payment\n   - Action: URGENT â€” investigate payment gateway, error messages\n\n**Quick win:** Fixing checkout issues could 2.6x conversion (3.2% â†’ 8.4%)\n```\n\n**For funnel visualization, use plotext:**\n```python\nimport plotext as plt\n\nsteps = ['Visitors', 'Viewers', 'Cart', 'Checkout', 'Purchase']\ncounts = [100000, 45000, 12000, 8500, 3200]\n\nplt.simple_bar(steps, counts, title='Purchase Funnel')\nplt.xlabel('Funnel Step')\nplt.ylabel('Count')\nplt.show()\n```\n\n**See terminal-formats.md Format 2 for complete bar chart examples.**\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill with:\n\n```markdown\nUse the `creating-visualizations` component skill to present query results\nvisually, making patterns and insights more accessible to stakeholders.\n```\n\nWhen creating visualizations during analysis:\n1. Choose format based on communication goal (Phase 1)\n2. Structure data for clarity (Phase 2)\n3. Build visualization with appropriate text format (Phase 3)\n4. Annotate with context and interpretation (Phase 4)\n\nThis ensures analysis outputs are not just technically correct but also\neffectively communicated and actionable.\n\n---\n\n## When to Visualize\n\n**Visualize when:**\n- Pattern is easier to see visually than in raw numbers\n- Presenting to stakeholders who need quick understanding\n- Comparing multiple segments, time periods, or metrics\n- Distribution shape matters (histograms)\n- Trend direction matters (sparklines, time series)\n\n**Use tables when:**\n- Exact values are critical\n- Reader needs to reference specific numbers\n- Data is already structured and scannable\n- Audience is technical and prefers precision\n\n**Use both when:**\n- Visualization reveals pattern, table provides detail\n- Different audiences (executive summary + appendix)\n- Building progressive disclosure (overview â†’ detail)\n\n---\n\n## Quality Checklist\n\nBefore finalizing any visualization, verify:\n\n- [ ] Visualization has clear, descriptive title\n- [ ] Units are labeled ($ , %, etc.)\n- [ ] Data source and time period documented\n- [ ] Key takeaway stated explicitly\n- [ ] Appropriate precision (not over-rounded or over-precise)\n- [ ] Scale is appropriate (bars from zero, etc.)\n- [ ] Annotations explain what to notice\n- [ ] Caveats and limitations noted\n- [ ] Visualization renders correctly in markdown preview\n- [ ] Numbers match source query results\n- [ ] Format matches communication goal\n- [ ] Audience can understand without additional context\n\n**If any checklist item fails, revise before including in analysis.**\n",
        "plugins/datapeeker/skills/creating-visualizations/formats/graphviz.md": "# GraphViz (DOT) Diagrams via Kroki\n\nGraphViz uses the DOT language to create network graphs, hierarchies, and relationship diagrams with powerful automatic layout algorithms. Excellent for data lineage and complex relationships.\n\n---\n\n## When to Use GraphViz\n\n**Best for:**\n- Data lineage and provenance\n- Complex network relationships\n- Hierarchical structures\n- Dependency graphs\n- System architecture\n\n**Advantages:**\n- Powerful automatic layout algorithms\n- Handles complex graphs with many nodes\n- Fine-grained control over appearance\n- Industry-standard DOT language\n\n---\n\n## Quick Start\n\n```python\nfrom kroki_client import KrokiClient\n\nclient = KrokiClient()\n\ndiagram = \"\"\"\ndigraph G {\n    rankdir=LR;\n\n    raw_data [label=\"Raw CSV\" shape=cylinder];\n    clean_data [label=\"Clean Data\" shape=cylinder];\n    analysis [label=\"Analysis\" shape=box];\n    report [label=\"Report\" shape=note];\n\n    raw_data -> clean_data [label=\"cleaning\"];\n    clean_data -> analysis [label=\"query\"];\n    analysis -> report [label=\"generate\"];\n}\n\"\"\"\n\nclient.save(diagram, 'graphviz', 'lineage.svg')\n```\n\n---\n\n## Graph Types\n\n### 1. Directed Graphs (digraph)\n\n**Use for:** Data flow, dependencies, lineage\n\n```python\nlineage = \"\"\"\ndigraph DataLineage {\n    rankdir=LR;\n    node [shape=box, style=rounded];\n\n    // Data sources\n    csv1 [label=\"sales.csv\"];\n    csv2 [label=\"customers.csv\"];\n    csv3 [label=\"products.csv\"];\n\n    // Raw tables\n    raw_sales [label=\"raw_sales\"];\n    raw_customers [label=\"raw_customers\"];\n    raw_products [label=\"raw_products\"];\n\n    // Clean tables\n    clean_sales [label=\"clean_sales\"];\n    clean_customers [label=\"clean_customers\"];\n    clean_products [label=\"clean_products\"];\n\n    // Analysis\n    revenue_analysis [label=\"Revenue Analysis\"];\n\n    // Connections\n    csv1 -> raw_sales [label=\"import\"];\n    csv2 -> raw_customers [label=\"import\"];\n    csv3 -> raw_products [label=\"import\"];\n\n    raw_sales -> clean_sales [label=\"clean\"];\n    raw_customers -> clean_customers [label=\"clean\"];\n    raw_products -> clean_products [label=\"clean\"];\n\n    clean_sales -> revenue_analysis;\n    clean_customers -> revenue_analysis;\n    clean_products -> revenue_analysis;\n}\n\"\"\"\n\nclient.save(lineage, 'graphviz', 'data-lineage.svg')\n```\n\n### 2. Undirected Graphs (graph)\n\n**Use for:** Relationships, correlations\n\n```python\ncorrelations = \"\"\"\ngraph Correlations {\n    node [shape=ellipse];\n\n    Revenue -- CustomerCount [label=\"r=0.85\"];\n    Revenue -- AvgOrderValue [label=\"r=0.72\"];\n    Revenue -- MarketingSpend [label=\"r=0.65\"];\n\n    CustomerCount -- AvgOrderValue [label=\"r=0.23\"];\n    CustomerCount -- MarketingSpend [label=\"r=0.78\"];\n\n    AvgOrderValue -- MarketingSpend [label=\"r=0.45\"];\n}\n\"\"\"\n\nclient.save(correlations, 'graphviz', 'correlations.svg')\n```\n\n---\n\n## Layout Directions\n\n```python\n# Left to Right (best for pipelines)\n\"digraph { rankdir=LR; A -> B -> C; }\"\n\n# Top to Bottom (best for hierarchies)\n\"digraph { rankdir=TB; A -> B -> C; }\"\n\n# Bottom to Top\n\"digraph { rankdir=BT; A -> B -> C; }\"\n\n# Right to Left\n\"digraph { rankdir=RL; A -> B -> C; }\"\n```\n\n---\n\n## Node Shapes\n\n```python\nshapes = \"\"\"\ndigraph Shapes {\n    rankdir=LR;\n\n    a [label=\"Box\" shape=box];\n    b [label=\"Rounded\" shape=box, style=rounded];\n    c [label=\"Circle\" shape=circle];\n    d [label=\"Ellipse\" shape=ellipse];\n    e [label=\"Database\" shape=cylinder];\n    f [label=\"Document\" shape=note];\n    g [label=\"Diamond\" shape=diamond];\n    h [label=\"Folder\" shape=folder];\n    i [label=\"Component\" shape=component];\n\n    a -> b -> c -> d -> e -> f -> g -> h -> i;\n}\n\"\"\"\n```\n\n**Common shapes for data visualization:**\n- `cylinder` - Databases, data stores\n- `box` - Processes, transformations\n- `note` - Reports, documents\n- `folder` - Directories, collections\n- `diamond` - Decisions, conditions\n\n---\n\n## Edge Styles\n\n```python\nedges = \"\"\"\ndigraph Edges {\n    A -> B [label=\"solid\"];\n    B -> C [label=\"dashed\", style=dashed];\n    C -> D [label=\"dotted\", style=dotted];\n    D -> E [label=\"bold\", style=bold];\n\n    E -> F [label=\"colored\", color=red];\n    F -> G [label=\"thick\", penwidth=3];\n\n    G -> H [arrowhead=normal];\n    H -> I [arrowhead=diamond];\n    I -> J [arrowhead=dot];\n}\n\"\"\"\n```\n\n---\n\n## Clusters (Subgraphs)\n\nGroup related nodes together:\n\n```python\nclustered = \"\"\"\ndigraph Pipeline {\n    rankdir=LR;\n\n    subgraph cluster_ingestion {\n        label=\"Data Ingestion\";\n        style=filled;\n        color=lightgrey;\n\n        csv1 [label=\"sales.csv\"];\n        csv2 [label=\"customers.csv\"];\n        import [label=\"Import Process\"];\n\n        csv1 -> import;\n        csv2 -> import;\n    }\n\n    subgraph cluster_storage {\n        label=\"Data Storage\";\n        style=filled;\n        color=lightblue;\n\n        raw [label=\"raw_* tables\"];\n        clean [label=\"clean_* tables\"];\n\n        raw -> clean [label=\"cleaning\"];\n    }\n\n    subgraph cluster_analysis {\n        label=\"Analysis\";\n        style=filled;\n        color=lightgreen;\n\n        explore [label=\"Exploratory\"];\n        stats [label=\"Statistical\"];\n        viz [label=\"Visualization\"];\n\n        explore -> stats -> viz;\n    }\n\n    import -> raw;\n    clean -> explore;\n}\n\"\"\"\n\nclient.save(clustered, 'graphviz', 'pipeline-clustered.svg')\n```\n\n---\n\n## DataPeeker Examples\n\n### Example 1: Complete Data Pipeline\n\n```python\ncomplete_pipeline = \"\"\"\ndigraph CompletePipeline {\n    rankdir=TB;\n    node [style=rounded];\n\n    // Source layer\n    subgraph cluster_sources {\n        label=\"Data Sources\";\n        style=filled;\n        color=\"#E8F4F8\";\n\n        s1 [label=\"sales_2024.csv\" shape=note];\n        s2 [label=\"customers.csv\" shape=note];\n        s3 [label=\"products.csv\" shape=note];\n    }\n\n    // Import layer\n    import [label=\"Import Script\\n(Python)\" shape=box];\n\n    // Raw storage\n    subgraph cluster_raw {\n        label=\"Raw Tables\";\n        style=filled;\n        color=\"#FFF4E6\";\n\n        r1 [label=\"raw_sales\" shape=cylinder];\n        r2 [label=\"raw_customers\" shape=cylinder];\n        r3 [label=\"raw_products\" shape=cylinder];\n    }\n\n    // Quality checks\n    quality [label=\"Quality\\nValidation\" shape=diamond];\n    issues [label=\"Quality\\nReport\" shape=note];\n\n    // Clean storage\n    subgraph cluster_clean {\n        label=\"Clean Tables\";\n        style=filled;\n        color=\"#E8F5E9\";\n\n        c1 [label=\"clean_sales\" shape=cylinder];\n        c2 [label=\"clean_customers\" shape=cylinder];\n        c3 [label=\"clean_products\" shape=cylinder];\n    }\n\n    // Analysis\n    analysis [label=\"Revenue\\nAnalysis\" shape=box];\n    report [label=\"Q4 Report\" shape=note];\n\n    // Connections\n    s1 -> import;\n    s2 -> import;\n    s3 -> import;\n\n    import -> r1;\n    import -> r2;\n    import -> r3;\n\n    r1 -> quality;\n    r2 -> quality;\n    r3 -> quality;\n\n    quality -> issues [label=\"if issues\"];\n    quality -> c1 [label=\"if clean\"];\n    quality -> c2 [label=\"if clean\"];\n    quality -> c3 [label=\"if clean\"];\n\n    c1 -> analysis;\n    c2 -> analysis;\n    c3 -> analysis;\n\n    analysis -> report;\n}\n\"\"\"\n\nclient.save(complete_pipeline, 'graphviz', 'complete-pipeline.svg')\n```\n\n### Example 2: Data Lineage with Transformations\n\n```python\nlineage_detailed = \"\"\"\ndigraph DataLineage {\n    rankdir=LR;\n\n    // Style definitions\n    node [style=rounded, fontname=\"Arial\"];\n    edge [fontname=\"Arial\", fontsize=10];\n\n    // Sources\n    raw_orders [label=\"raw_orders\\n(1.2M rows)\" shape=cylinder, fillcolor=\"#FFE4E1\", style=\"rounded,filled\"];\n\n    // Transformations\n    dedup [label=\"Remove\\nDuplicates\\n(-234 rows)\" shape=box];\n    filter_nulls [label=\"Filter NULLs\\n(-1,456 rows)\" shape=box];\n    standardize [label=\"Standardize\\nFormats\" shape=box];\n    validate [label=\"Validate\\nRanges\" shape=box];\n\n    // Clean data\n    clean_orders [label=\"clean_orders\\n(1,198,310 rows)\" shape=cylinder, fillcolor=\"#90EE90\", style=\"rounded,filled\"];\n\n    // Analysis views\n    monthly_rev [label=\"monthly_revenue\\n(view)\" shape=folder];\n    customer_ltv [label=\"customer_ltv\\n(view)\" shape=folder];\n\n    // Reports\n    report1 [label=\"Revenue\\nReport\" shape=note];\n    report2 [label=\"Customer\\nAnalysis\" shape=note];\n\n    // Flow\n    raw_orders -> dedup -> filter_nulls -> standardize -> validate -> clean_orders;\n\n    clean_orders -> monthly_rev -> report1;\n    clean_orders -> customer_ltv -> report2;\n}\n\"\"\"\n\nclient.save(lineage_detailed, 'graphviz', 'lineage-detailed.svg')\n```\n\n### Example 3: Analysis Dependencies\n\n```python\ndependencies = \"\"\"\ndigraph AnalysisDeps {\n    rankdir=TB;\n\n    // Data sources\n    node [shape=cylinder, style=filled, fillcolor=\"#E8F4F8\"];\n    orders;\n    customers;\n    products;\n\n    // Analysis scripts\n    node [shape=box, style=rounded, fillcolor=\"#FFF4E6\"];\n    revenue_calc [label=\"01_revenue_calc.py\"];\n    cohort_analysis [label=\"02_cohort_analysis.py\"];\n    churn_pred [label=\"03_churn_prediction.py\"];\n\n    // Outputs\n    node [shape=note, style=filled, fillcolor=\"#E8F5E9\"];\n    revenue_report [label=\"revenue_report.md\"];\n    cohort_viz [label=\"cohort_viz.png\"];\n    churn_model [label=\"churn_model.pkl\"];\n\n    // Dependencies\n    orders -> revenue_calc;\n    customers -> revenue_calc;\n    products -> revenue_calc;\n    revenue_calc -> revenue_report;\n\n    orders -> cohort_analysis;\n    customers -> cohort_analysis;\n    cohort_analysis -> cohort_viz;\n\n    orders -> churn_pred;\n    customers -> churn_pred;\n    cohort_analysis -> churn_pred [style=dashed, label=\"uses\"];\n    churn_pred -> churn_model;\n}\n\"\"\"\n\nclient.save(dependencies, 'graphviz', 'analysis-deps.svg')\n```\n\n### Example 4: Database Schema Relationships\n\n```python\nschema = \"\"\"\ndigraph Schema {\n    rankdir=TB;\n    node [shape=record, style=rounded];\n\n    users [label=\"{users|+ id : int\\\\l+ email : varchar\\\\l+ name : varchar\\\\l+ created_at : timestamp\\\\l}\"];\n\n    orders [label=\"{orders|+ id : int\\\\l+ user_id : int (FK)\\\\l+ order_date : date\\\\l+ total : decimal\\\\l+ status : varchar\\\\l}\"];\n\n    order_items [label=\"{order_items|+ id : int\\\\l+ order_id : int (FK)\\\\l+ product_id : int (FK)\\\\l+ quantity : int\\\\l+ price : decimal\\\\l}\"];\n\n    products [label=\"{products|+ id : int\\\\l+ name : varchar\\\\l+ price : decimal\\\\l+ category : varchar\\\\l}\"];\n\n    users -> orders [label=\"1:N\"];\n    orders -> order_items [label=\"1:N\"];\n    products -> order_items [label=\"1:N\"];\n}\n\"\"\"\n\nclient.save(schema, 'graphviz', 'schema-graph.svg')\n```\n\n---\n\n## Colors and Styling\n\n### Color Schemes\n\n```python\ncolored = \"\"\"\ndigraph Colored {\n    rankdir=LR;\n\n    // Color by type\n    source [fillcolor=\"#E8F4F8\", style=filled, label=\"Data Source\"];\n    process [fillcolor=\"#FFF4E6\", style=filled, label=\"Processing\"];\n    storage [fillcolor=\"#E8F5E9\", style=filled, label=\"Storage\"];\n    output [fillcolor=\"#FFE4E1\", style=filled, label=\"Output\"];\n\n    source -> process -> storage -> output;\n}\n\"\"\"\n```\n\n### Common Color Codes\n\n- `#E8F4F8` - Light blue (sources)\n- `#FFF4E6` - Light orange (processing)\n- `#E8F5E9` - Light green (success/clean)\n- `#FFE4E1` - Light red (issues/outputs)\n- `#F3E5F5` - Light purple (analysis)\n\n---\n\n## Best Practices\n\n### 1. Use Meaningful Node IDs\n\n```python\n# âœ… Good: Descriptive IDs\n\"\"\"\ndigraph {\n    raw_sales -> clean_sales -> revenue_analysis;\n}\n\"\"\"\n\n# âŒ Avoid: Generic IDs\n\"\"\"\ndigraph {\n    n1 -> n2 -> n3;\n}\n\"\"\"\n```\n\n### 2. Group Related Nodes with Clusters\n\n```python\n# âœ… Good: Organized with clusters\n\"\"\"\ndigraph {\n    subgraph cluster_sources {\n        label=\"Sources\";\n        csv1; csv2;\n    }\n    subgraph cluster_storage {\n        label=\"Storage\";\n        raw; clean;\n    }\n}\n\"\"\"\n```\n\n### 3. Use Appropriate Layout Direction\n\n```python\n# Pipeline flow: Left to Right\n\"digraph { rankdir=LR; ... }\"\n\n# Hierarchy: Top to Bottom\n\"digraph { rankdir=TB; ... }\"\n```\n\n### 4. Add Labels to Edges for Clarity\n\n```python\n# âœ… Good: Labeled relationships\n\"\"\"\ndigraph {\n    A -> B [label=\"transform\"];\n    B -> C [label=\"validate\"];\n}\n\"\"\"\n```\n\n### 5. Use HTML-Like Labels for Rich Formatting\n\n```python\nhtml_labels = \"\"\"\ndigraph {\n    node [shape=none];\n\n    table1 [label=<\n        <TABLE BORDER=\"0\" CELLBORDER=\"1\" CELLSPACING=\"0\">\n        <TR><TD BGCOLOR=\"lightblue\"><B>clean_orders</B></TD></TR>\n        <TR><TD ALIGN=\"LEFT\">id: int</TD></TR>\n        <TR><TD ALIGN=\"LEFT\">user_id: int</TD></TR>\n        <TR><TD ALIGN=\"LEFT\">total: decimal</TD></TR>\n        </TABLE>\n    >];\n}\n\"\"\"\n```\n\n---\n\n## Advanced Features\n\n### Rank Constraints\n\nForce nodes to same rank (horizontal alignment):\n\n```python\nsame_rank = \"\"\"\ndigraph {\n    rankdir=TB;\n\n    {rank=same; A; B; C;}\n    {rank=same; D; E;}\n\n    A -> D;\n    B -> E;\n    C -> E;\n}\n\"\"\"\n```\n\n### Port Connections\n\nConnect to specific sides of nodes:\n\n```python\nports = \"\"\"\ndigraph {\n    A -> B:n;  // Connect to north (top)\n    B:s -> C;  // Connect from south (bottom)\n    C:e -> D:w;  // East to west\n}\n\"\"\"\n```\n\n---\n\n## Troubleshooting\n\n### Issue: Overlapping Nodes\n\n**Solution:** Use `nodesep` and `ranksep`:\n\n```python\nspaced = \"\"\"\ndigraph {\n    nodesep=1.0;  // Horizontal spacing\n    ranksep=1.0;  // Vertical spacing\n\n    A -> B -> C;\n}\n\"\"\"\n```\n\n### Issue: Long Edge Crossings\n\n**Solution:** Adjust rank direction or use clusters:\n\n```python\n# Try different directions\nrankdir=LR  # vs TB, BT, RL\n```\n\n### Issue: Text Overflow in Nodes\n\n**Solution:** Use `\\\\l` for left-aligned multi-line text:\n\n```python\nmultiline = \"\"\"\ndigraph {\n    A [label=\"Line 1\\\\lLine 2\\\\lLine 3\\\\l\"];\n}\n\"\"\"\n```\n\n---\n\n## References\n\n- [GraphViz Official Docs](https://graphviz.org/)\n- [DOT Language Guide](https://graphviz.org/doc/info/lang.html)\n- [Node Shapes Gallery](https://graphviz.org/doc/info/shapes.html)\n- [Color Names](https://graphviz.org/doc/info/colors.html)\n- [Kroki GraphViz Examples](https://kroki.io/examples.html#graphviz)\n",
        "plugins/datapeeker/skills/creating-visualizations/formats/mermaid.md": "# Mermaid Diagrams via Kroki\n\nMermaid is a simple text-based diagramming language perfect for flowcharts, sequence diagrams, Gantt charts, and timelines. Great for documenting data workflows and analysis processes.\n\n---\n\n## When to Use Mermaid\n\n**Best for:**\n- Data processing workflows\n- Analysis pipelines\n- Project timelines (Gantt charts)\n- Sequence of operations\n- State machines\n\n**Advantages:**\n- Simple, readable syntax\n- Documentation-friendly (looks good as text)\n- Wide adoption in markdown tools\n- Multiple diagram types in one syntax\n\n---\n\n## Quick Start\n\n```python\nfrom kroki_client import KrokiClient\n\nclient = KrokiClient()\n\ndiagram = \"\"\"\ngraph TD\n    A[Raw Data] --> B[Data Cleaning]\n    B --> C[Analysis]\n    C --> D[Visualization]\n    D --> E[Report]\n\"\"\"\n\nclient.save(diagram, 'mermaid', 'workflow.svg')\n```\n\n---\n\n## Diagram Types\n\n### 1. Flowcharts (graph)\n\n**Use for:** Data workflows, processing pipelines\n\n```python\nworkflow = \"\"\"\ngraph LR\n    A[(Raw CSV)] --> B[Import to SQLite]\n    B --> C[Data Cleaning]\n    C --> D[Quality Checks]\n    D --> E{All Checks Pass?}\n    E -->|Yes| F[Analysis]\n    E -->|No| G[Manual Review]\n    G --> C\n    F --> H[Report]\n\"\"\"\n\nclient.save(workflow, 'mermaid', 'data-pipeline.svg')\n```\n\n**Syntax:**\n- `graph TD` - Top to bottom\n- `graph LR` - Left to right\n- `graph RL` - Right to left\n- `graph BT` - Bottom to top\n\n**Node shapes:**\n- `A[Rectangle]` - Standard box\n- `B([Rounded])` - Rounded box\n- `C[(Database)]` - Cylinder (database)\n- `D{Decision}` - Diamond\n- `E((Circle))` - Circle\n- `F>Flag]` - Flag shape\n\n**Arrows:**\n- `-->` - Solid arrow\n- `-.->` - Dotted arrow\n- `==>` - Thick arrow\n- `--text-->` - Arrow with label\n\n### 2. Sequence Diagrams\n\n**Use for:** Data flow between systems, API interactions\n\n```python\nsequence = \"\"\"\nsequenceDiagram\n    participant User\n    participant API\n    participant DB\n    participant Cache\n\n    User->>API: Request data\n    API->>Cache: Check cache\n    alt Cache hit\n        Cache-->>API: Return cached data\n    else Cache miss\n        API->>DB: Query database\n        DB-->>API: Return results\n        API->>Cache: Store in cache\n    end\n    API-->>User: Return data\n\"\"\"\n\nclient.save(sequence, 'mermaid', 'data-flow.svg')\n```\n\n### 3. Gantt Charts\n\n**Use for:** Analysis project timelines, sprint planning\n\n```python\ngantt_chart = \"\"\"\ngantt\n    title Data Analysis Project Timeline\n    dateFormat YYYY-MM-DD\n\n    section Data Collection\n    Gather Requirements   :a1, 2024-01-01, 5d\n    Import Raw Data       :a2, after a1, 3d\n\n    section Data Prep\n    Data Cleaning         :b1, after a2, 7d\n    Quality Validation    :b2, after b1, 2d\n\n    section Analysis\n    Exploratory Analysis  :c1, after b2, 10d\n    Statistical Tests     :c2, after c1, 5d\n    Visualization         :c3, after c2, 3d\n\n    section Reporting\n    Draft Report          :d1, after c3, 5d\n    Review & Revisions    :d2, after d1, 3d\n    Final Presentation    :milestone, after d2, 1d\n\"\"\"\n\nclient.save(gantt_chart, 'mermaid', 'timeline.svg')\n```\n\n### 4. State Diagrams\n\n**Use for:** Data state transitions, workflow states\n\n```python\nstate_diagram = \"\"\"\nstateDiagram-v2\n    [*] --> Raw\n    Raw --> Cleaning: Import\n    Cleaning --> Validated: Quality Checks\n    Validated --> Analyzed: Analysis\n    Analyzed --> Reported: Visualization\n    Reported --> [*]\n\n    Cleaning --> Raw: Validation Failed\n    Validated --> Cleaning: Issues Found\n\"\"\"\n\nclient.save(state_diagram, 'mermaid', 'data-states.svg')\n```\n\n### 5. Entity Relationship Diagrams (Simple)\n\n**Use for:** Basic database schemas\n\n```python\nerd = \"\"\"\nerDiagram\n    CUSTOMER ||--o{ ORDER : places\n    ORDER ||--|{ LINE-ITEM : contains\n    PRODUCT ||--o{ LINE-ITEM : \"ordered in\"\n\n    CUSTOMER {\n        int id PK\n        string email\n        string name\n    }\n\n    ORDER {\n        int id PK\n        int customer_id FK\n        date order_date\n        decimal total\n    }\n\n    LINE-ITEM {\n        int order_id FK\n        int product_id FK\n        int quantity\n        decimal price\n    }\n\n    PRODUCT {\n        int id PK\n        string name\n        decimal price\n    }\n\"\"\"\n\nclient.save(erd, 'mermaid', 'schema.svg')\n```\n\n**Cardinality notation:**\n- `||--||` - One to one\n- `||--o{` - One to many\n- `}o--o{` - Many to many\n\n---\n\n## DataPeeker Examples\n\n### Example 1: Complete Analysis Workflow\n\n```python\nanalysis_workflow = \"\"\"\ngraph TD\n    Start[Start Analysis] --> A1[Load CSV Files]\n    A1 --> A2[Import to raw_* tables]\n\n    A2 --> B1{Data Quality OK?}\n    B1 -->|No| B2[Manual Cleaning]\n    B2 --> A2\n    B1 -->|Yes| B3[Create clean_* tables]\n\n    B3 --> C1[Exploratory Analysis]\n    C1 --> C2[Hypothesis Formation]\n    C2 --> C3[Statistical Testing]\n\n    C3 --> D1{Significant Results?}\n    D1 -->|Yes| D2[Create Visualizations]\n    D1 -->|No| D3[Adjust Hypothesis]\n    D3 --> C2\n\n    D2 --> E1[Draft Report]\n    E1 --> E2[Peer Review]\n    E2 --> E3[Final Report]\n    E3 --> End[Analysis Complete]\n\n    style Start fill:#90EE90\n    style End fill:#FFB6C1\n    style B1 fill:#FFE4B5\n    style D1 fill:#FFE4B5\n\"\"\"\n\nclient.save(analysis_workflow, 'mermaid', 'full-workflow.svg')\n```\n\n### Example 2: Data Quality Pipeline\n\n```python\nquality_pipeline = \"\"\"\ngraph LR\n    A[(Raw CSV)] --> B[Profile Data]\n    B --> C[Detect Duplicates]\n    B --> D[Detect Outliers]\n    B --> E[Check NULL %]\n\n    C --> F{Issues Found?}\n    D --> F\n    E --> F\n\n    F -->|Yes| G[Generate Quality Report]\n    F -->|No| H[Mark as Clean]\n\n    G --> I[Manual Review Required]\n    H --> J[Ready for Analysis]\n\n    style A fill:#E8F4F8\n    style I fill:#FFE4E1\n    style J fill:#90EE90\n\"\"\"\n\nclient.save(quality_pipeline, 'mermaid', 'quality-pipeline.svg')\n```\n\n### Example 3: Analysis Sprint Timeline\n\n```python\nsprint = \"\"\"\ngantt\n    title Q4 Revenue Analysis Sprint\n    dateFormat YYYY-MM-DD\n\n    section Week 1\n    Import sales data       :2024-10-01, 2d\n    Import customer data    :2024-10-01, 2d\n    Data profiling          :2024-10-03, 2d\n\n    section Week 2\n    Data cleaning           :2024-10-07, 3d\n    Quality validation      :2024-10-10, 2d\n\n    section Week 3\n    Exploratory analysis    :2024-10-14, 5d\n\n    section Week 4\n    Statistical analysis    :2024-10-21, 3d\n    Visualizations          :2024-10-24, 2d\n\n    section Week 5\n    Report draft            :2024-10-28, 3d\n    Review                  :2024-10-31, 2d\n\n    section Milestones\n    Analysis kickoff        :milestone, 2024-10-01, 0d\n    Data ready              :milestone, 2024-10-12, 0d\n    Analysis complete       :milestone, 2024-10-26, 0d\n    Report final            :milestone, 2024-11-02, 0d\n\"\"\"\n\nclient.save(sprint, 'mermaid', 'sprint-timeline.svg')\n```\n\n---\n\n## Styling\n\n### Colors and Themes\n\n```python\nstyled = \"\"\"\ngraph TD\n    A[Start] --> B[Process 1]\n    B --> C[Process 2]\n    C --> D[End]\n\n    style A fill:#90EE90,stroke:#228B22,stroke-width:2px\n    style D fill:#FFB6C1,stroke:#C71585,stroke-width:2px\n    style B fill:#87CEEB,stroke:#4682B4\n    style C fill:#87CEEB,stroke:#4682B4\n\"\"\"\n```\n\n### Class Definitions\n\n```python\nwith_classes = \"\"\"\ngraph TD\n    A[Import Data]:::import --> B[Clean Data]:::process\n    B --> C[Analyze]:::process\n    C --> D[Report]:::output\n\n    classDef import fill:#E8F4F8,stroke:#0077BE\n    classDef process fill:#FFF4E6,stroke:#FF8C00\n    classDef output fill:#E8F5E9,stroke:#4CAF50\n\"\"\"\n```\n\n---\n\n## Best Practices\n\n### 1. Keep It Simple\n\nMermaid is best for high-level overviews. For complex diagrams, consider GraphViz.\n\n```python\n# âœ… Good: Clear, focused\nsimple = \"\"\"\ngraph TD\n    A[Raw Data] --> B[Clean Data]\n    B --> C[Analysis]\n\"\"\"\n\n# âŒ Avoid: Too many nodes\ncomplex = \"\"\"\ngraph TD\n    A1 --> A2 --> A3 --> A4 --> ...\n    # (20+ nodes becomes hard to read)\n\"\"\"\n```\n\n### 2. Use Descriptive Labels\n\n```python\n# âœ… Good: Clear purpose\nclear = \"\"\"\ngraph TD\n    A[Import CSV Files] --> B[Validate Schema]\n\"\"\"\n\n# âŒ Avoid: Cryptic labels\ncryptic = \"\"\"\ngraph TD\n    A[Step1] --> B[Step2]\n\"\"\"\n```\n\n### 3. Direction Matters\n\nChoose direction based on content:\n\n```python\n# Process flow: Left to right\nprocess = \"graph LR\\n    A --> B --> C\"\n\n# Hierarchy: Top to bottom\nhierarchy = \"graph TD\\n    A --> B\\n    A --> C\"\n```\n\n### 4. Use Subgraphs for Grouping\n\n```python\ngrouped = \"\"\"\ngraph TD\n    subgraph \"Data Ingestion\"\n        A[Load CSV] --> B[Import to DB]\n    end\n\n    subgraph \"Data Preparation\"\n        C[Clean Data] --> D[Validate]\n    end\n\n    B --> C\n    D --> E[Analysis]\n\"\"\"\n```\n\n---\n\n## Troubleshooting\n\n### Issue: Syntax Error\n\n**Common mistakes:**\n- Missing quotes around labels with special characters\n- Incorrect arrow syntax (`->` instead of `-->`)\n- Unclosed brackets\n\n```python\n# âŒ Wrong\n\"A[My Label] -> B\"\n\n# âœ… Correct\n\"A[My Label] --> B\"\n```\n\n### Issue: Layout Problems\n\nMermaid auto-layouts diagrams. For fine control, use GraphViz instead.\n\n---\n\n## References\n\n- [Mermaid Official Docs](https://mermaid.js.org/)\n- [Mermaid Live Editor](https://mermaid.live/)\n- [Syntax Reference](https://mermaid.js.org/intro/syntax-reference.html)\n- [Kroki Mermaid Examples](https://kroki.io/examples.html#mermaid)\n",
        "plugins/datapeeker/skills/creating-visualizations/formats/vega-lite.md": "# Vega-Lite Charts via Kroki\n\nVega-Lite is a high-level grammar for creating statistical visualizations. Perfect for bar charts, line plots, scatter plots, and distributions from data.\n\n---\n\n## When to Use Vega-Lite\n\n**Best for:**\n- Statistical charts (bar, line, scatter, histograms)\n- Data distributions and aggregations\n- Multi-series comparisons\n- Interactive visualizations (when exported to HTML)\n\n**Advantages:**\n- Declarative JSON specification\n- Built-in data transformations\n- Statistical aggregations (sum, mean, count, etc.)\n- Professional statistical graphics\n\n**Note:** For quick terminal visualizations, use plotext. For embedded reports/presentations, use Vega-Lite via Kroki.\n\n---\n\n## Quick Start\n\n```python\nfrom kroki_client import KrokiClient\nimport json\n\nclient = KrokiClient()\n\n# Vega-Lite uses JSON specification\nchart = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"description\": \"Simple bar chart\",\n    \"data\": {\n        \"values\": [\n            {\"category\": \"Electronics\", \"revenue\": 345678},\n            {\"category\": \"Home & Garden\", \"revenue\": 298432},\n            {\"category\": \"Clothing\", \"revenue\": 234567}\n        ]\n    },\n    \"mark\": \"bar\",\n    \"encoding\": {\n        \"x\": {\"field\": \"category\", \"type\": \"nominal\"},\n        \"y\": {\"field\": \"revenue\", \"type\": \"quantitative\"}\n    }\n}\n\n# Convert to JSON string\nchart_json = json.dumps(chart)\n\nclient.save(chart_json, 'vegalite', 'revenue-chart.svg')\n```\n\n---\n\n## Chart Types\n\n### 1. Bar Charts\n\n```python\nbar_chart = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"Q4 Revenue by Category\",\n    \"data\": {\n        \"values\": [\n            {\"category\": \"Electronics\", \"revenue\": 345678},\n            {\"category\": \"Home\", \"revenue\": 298432},\n            {\"category\": \"Clothing\", \"revenue\": 234567},\n            {\"category\": \"Sports\", \"revenue\": 134234},\n            {\"category\": \"Toys\", \"revenue\": 56789}\n        ]\n    },\n    \"mark\": \"bar\",\n    \"encoding\": {\n        \"x\": {\"field\": \"category\", \"type\": \"nominal\", \"title\": \"Category\"},\n        \"y\": {\"field\": \"revenue\", \"type\": \"quantitative\", \"title\": \"Revenue ($)\"},\n        \"color\": {\"field\": \"category\", \"type\": \"nominal\", \"legend\": None}\n    }\n}\n\nclient.save(json.dumps(bar_chart), 'vegalite', 'bar-chart.svg')\n```\n\n**Horizontal bars:**\n\n```python\nhorizontal = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"data\": {\"values\": [...]},\n    \"mark\": \"bar\",\n    \"encoding\": {\n        \"y\": {\"field\": \"category\", \"type\": \"nominal\"},  # Swap x/y\n        \"x\": {\"field\": \"revenue\", \"type\": \"quantitative\"}\n    }\n}\n```\n\n### 2. Line Charts\n\n```python\nline_chart = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"Monthly Revenue Trend\",\n    \"data\": {\n        \"values\": [\n            {\"month\": \"2024-01\", \"revenue\": 120000},\n            {\"month\": \"2024-02\", \"revenue\": 135000},\n            {\"month\": \"2024-03\", \"revenue\": 142000},\n            {\"month\": \"2024-04\", \"revenue\": 138000},\n            {\"month\": \"2024-05\", \"revenue\": 155000},\n            {\"month\": \"2024-06\", \"revenue\": 168000}\n        ]\n    },\n    \"mark\": {\"type\": \"line\", \"point\": True},\n    \"encoding\": {\n        \"x\": {\"field\": \"month\", \"type\": \"temporal\", \"title\": \"Month\"},\n        \"y\": {\"field\": \"revenue\", \"type\": \"quantitative\", \"title\": \"Revenue ($)\"}\n    }\n}\n\nclient.save(json.dumps(line_chart), 'vegalite', 'line-chart.svg')\n```\n\n**Multiple series:**\n\n```python\nmulti_series = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"Revenue Comparison: 2023 vs 2024\",\n    \"data\": {\n        \"values\": [\n            {\"month\": \"Jan\", \"year\": \"2023\", \"revenue\": 120000},\n            {\"month\": \"Jan\", \"year\": \"2024\", \"revenue\": 140000},\n            {\"month\": \"Feb\", \"year\": \"2023\", \"revenue\": 130000},\n            {\"month\": \"Feb\", \"year\": \"2024\", \"revenue\": 155000},\n            # ... more months\n        ]\n    },\n    \"mark\": {\"type\": \"line\", \"point\": True},\n    \"encoding\": {\n        \"x\": {\"field\": \"month\", \"type\": \"ordinal\"},\n        \"y\": {\"field\": \"revenue\", \"type\": \"quantitative\"},\n        \"color\": {\"field\": \"year\", \"type\": \"nominal\"}\n    }\n}\n```\n\n### 3. Scatter Plots\n\n```python\nscatter = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"Order Value vs Shipping Speed\",\n    \"data\": {\n        \"values\": [\n            {\"shipping_days\": 1, \"order_value\": 245},\n            {\"shipping_days\": 1, \"order_value\": 312},\n            {\"shipping_days\": 2, \"order_value\": 156},\n            {\"shipping_days\": 2, \"order_value\": 198},\n            {\"shipping_days\": 3, \"order_value\": 89},\n            {\"shipping_days\": 5, \"order_value\": 45}\n            # ... more data points\n        ]\n    },\n    \"mark\": \"point\",\n    \"encoding\": {\n        \"x\": {\"field\": \"shipping_days\", \"type\": \"quantitative\", \"title\": \"Shipping Speed (days)\"},\n        \"y\": {\"field\": \"order_value\", \"type\": \"quantitative\", \"title\": \"Order Value ($)\"}\n    }\n}\n\nclient.save(json.dumps(scatter), 'vegalite', 'scatter.svg')\n```\n\n### 4. Histograms\n\n```python\nhistogram = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"Order Value Distribution\",\n    \"data\": {\n        \"values\": [\n            {\"order_value\": 23}, {\"order_value\": 45}, {\"order_value\": 67},\n            {\"order_value\": 89}, {\"order_value\": 34}, {\"order_value\": 56},\n            # ... many more values\n        ]\n    },\n    \"mark\": \"bar\",\n    \"encoding\": {\n        \"x\": {\n            \"field\": \"order_value\",\n            \"type\": \"quantitative\",\n            \"bin\": {\"maxbins\": 10},\n            \"title\": \"Order Value ($)\"\n        },\n        \"y\": {\n            \"aggregate\": \"count\",\n            \"type\": \"quantitative\",\n            \"title\": \"Frequency\"\n        }\n    }\n}\n\nclient.save(json.dumps(histogram), 'vegalite', 'histogram.svg')\n```\n\n### 5. Grouped Bar Charts\n\n```python\ngrouped = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"Revenue by Category and Quarter\",\n    \"data\": {\n        \"values\": [\n            {\"category\": \"Electronics\", \"quarter\": \"Q1\", \"revenue\": 85000},\n            {\"category\": \"Electronics\", \"quarter\": \"Q2\", \"revenue\": 92000},\n            {\"category\": \"Home\", \"quarter\": \"Q1\", \"revenue\": 72000},\n            {\"category\": \"Home\", \"quarter\": \"Q2\", \"revenue\": 78000},\n            # ... more combinations\n        ]\n    },\n    \"mark\": \"bar\",\n    \"encoding\": {\n        \"x\": {\"field\": \"category\", \"type\": \"nominal\"},\n        \"y\": {\"field\": \"revenue\", \"type\": \"quantitative\"},\n        \"color\": {\"field\": \"quarter\", \"type\": \"nominal\"},\n        \"xOffset\": {\"field\": \"quarter\"}\n    }\n}\n```\n\n---\n\n## Data Transformations\n\nVega-Lite includes powerful built-in transformations:\n\n### Aggregation\n\n```python\naggregated = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"Average Order Value by Category\",\n    \"data\": {\n        \"values\": [\n            {\"category\": \"Electronics\", \"order_value\": 245},\n            {\"category\": \"Electronics\", \"order_value\": 312},\n            {\"category\": \"Home\", \"order_value\": 89},\n            {\"category\": \"Home\", \"order_value\": 145},\n            # ... more orders\n        ]\n    },\n    \"mark\": \"bar\",\n    \"encoding\": {\n        \"x\": {\"field\": \"category\", \"type\": \"nominal\"},\n        \"y\": {\"aggregate\": \"mean\", \"field\": \"order_value\", \"type\": \"quantitative\", \"title\": \"Avg Order Value\"}\n    }\n}\n```\n\n**Available aggregations:**\n- `count` - Count of records\n- `sum` - Sum of values\n- `mean` - Average\n- `median` - Median value\n- `min` / `max` - Min/max values\n- `stdev` - Standard deviation\n\n### Filtering\n\n```python\nfiltered = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"data\": {\"values\": [...]},\n    \"transform\": [\n        {\"filter\": \"datum.revenue > 100000\"}\n    ],\n    \"mark\": \"bar\",\n    \"encoding\": {...}\n}\n```\n\n### Calculate New Fields\n\n```python\ncalculated = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"data\": {\"values\": [...]},\n    \"transform\": [\n        {\"calculate\": \"datum.revenue / datum.orders\", \"as\": \"avg_order_value\"}\n    ],\n    \"mark\": \"bar\",\n    \"encoding\": {\n        \"x\": {\"field\": \"category\"},\n        \"y\": {\"field\": \"avg_order_value\"}\n    }\n}\n```\n\n---\n\n## DataPeeker Examples\n\n### Example 1: Revenue Analysis Dashboard\n\n```python\nimport sqlite3\nimport json\n\n# Query data from SQLite\nconn = sqlite3.connect('analysis.db')\ncursor = conn.execute(\"\"\"\n    SELECT\n        strftime('%Y-%m', order_date) as month,\n        SUM(total) as revenue\n    FROM clean_orders\n    WHERE order_date >= '2024-01-01'\n    GROUP BY month\n    ORDER BY month\n\"\"\")\n\ndata = [{\"month\": row[0], \"revenue\": row[1]} for row in cursor.fetchall()]\n\n# Create Vega-Lite spec\nchart = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"2024 Monthly Revenue\",\n    \"width\": 600,\n    \"height\": 400,\n    \"data\": {\"values\": data},\n    \"mark\": {\"type\": \"line\", \"point\": True, \"tooltip\": True},\n    \"encoding\": {\n        \"x\": {\"field\": \"month\", \"type\": \"temporal\", \"title\": \"Month\"},\n        \"y\": {\"field\": \"revenue\", \"type\": \"quantitative\", \"title\": \"Revenue ($)\",\n              \"axis\": {\"format\": \"$,.0f\"}}\n    }\n}\n\nclient.save(json.dumps(chart), 'vegalite', 'monthly-revenue.svg')\n```\n\n### Example 2: Category Performance Comparison\n\n```python\n# Query data\ncursor = conn.execute(\"\"\"\n    SELECT\n        p.category,\n        COUNT(DISTINCT o.id) as orders,\n        SUM(oi.quantity * oi.price) as revenue\n    FROM clean_orders o\n    JOIN order_items oi ON o.id = oi.order_id\n    JOIN products p ON oi.product_id = p.id\n    GROUP BY p.category\n    ORDER BY revenue DESC\n\"\"\")\n\ndata = [{\"category\": row[0], \"orders\": row[1], \"revenue\": row[2]} for row in cursor]\n\n# Create layered chart (bars + line)\nchart = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"Category Performance: Orders vs Revenue\",\n    \"width\": 600,\n    \"height\": 400,\n    \"data\": {\"values\": data},\n    \"encoding\": {\"x\": {\"field\": \"category\", \"type\": \"nominal\", \"title\": \"Category\"}},\n    \"layer\": [\n        {\n            \"mark\": \"bar\",\n            \"encoding\": {\n                \"y\": {\"field\": \"revenue\", \"type\": \"quantitative\", \"title\": \"Revenue ($)\"},\n                \"color\": {\"value\": \"#4682B4\"}\n            }\n        },\n        {\n            \"mark\": {\"type\": \"line\", \"color\": \"#FF6347\", \"point\": True},\n            \"encoding\": {\n                \"y\": {\"field\": \"orders\", \"type\": \"quantitative\", \"title\": \"Orders\"},\n                \"color\": {\"value\": \"#FF6347\"}\n            }\n        }\n    ],\n    \"resolve\": {\"scale\": {\"y\": \"independent\"}}\n}\n\nclient.save(json.dumps(chart), 'vegalite', 'category-performance.svg')\n```\n\n### Example 3: Customer Segmentation\n\n```python\n# Query customer LTV distribution\ncursor = conn.execute(\"\"\"\n    SELECT\n        customer_id,\n        SUM(total) as lifetime_value\n    FROM clean_orders\n    GROUP BY customer_id\n\"\"\")\n\ndata = [{\"ltv\": row[1]} for row in cursor]\n\n# Create histogram with annotations\nchart = {\n    \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n    \"title\": \"Customer Lifetime Value Distribution\",\n    \"width\": 600,\n    \"height\": 400,\n    \"data\": {\"values\": data},\n    \"layer\": [\n        {\n            \"mark\": \"bar\",\n            \"encoding\": {\n                \"x\": {\n                    \"field\": \"ltv\",\n                    \"type\": \"quantitative\",\n                    \"bin\": {\"maxbins\": 20},\n                    \"title\": \"Lifetime Value ($)\"\n                },\n                \"y\": {\n                    \"aggregate\": \"count\",\n                    \"type\": \"quantitative\",\n                    \"title\": \"Number of Customers\"\n                }\n            }\n        },\n        {\n            \"mark\": {\"type\": \"rule\", \"color\": \"red\", \"strokeWidth\": 2},\n            \"encoding\": {\n                \"x\": {\"aggregate\": \"median\", \"field\": \"ltv\"}\n            }\n        }\n    ]\n}\n\nclient.save(json.dumps(chart), 'vegalite', 'ltv-distribution.svg')\n```\n\n---\n\n## Best Practices\n\n### 1. Use Appropriate Mark Types\n\n```python\n# âœ… Good: Right mark for data type\nbar_chart = {\"mark\": \"bar\", ...}  # Categorical data\nline_chart = {\"mark\": \"line\", ...}  # Time series\nscatter = {\"mark\": \"point\", ...}  # Relationships\n\n# âŒ Avoid: Wrong mark type\n# Don't use line chart for categorical data\n```\n\n### 2. Add Titles and Axis Labels\n\n```python\n# âœ… Good: Clear labels\nchart = {\n    \"title\": \"Q4 2024 Revenue by Category\",\n    \"encoding\": {\n        \"x\": {\"field\": \"category\", \"title\": \"Product Category\"},\n        \"y\": {\"field\": \"revenue\", \"title\": \"Revenue (USD)\"}\n    }\n}\n```\n\n### 3. Format Numbers Appropriately\n\n```python\n# Add formatting to axis\n\"y\": {\n    \"field\": \"revenue\",\n    \"type\": \"quantitative\",\n    \"axis\": {\"format\": \"$,.0f\"}  # Currency with commas\n}\n```\n\n**Format codes:**\n- `$,.0f` - Currency: $1,234\n- `.2f` - Decimal: 12.34\n- `.1%` - Percentage: 45.2%\n\n### 4. Use Data from SQLite\n\n```python\ndef create_chart_from_query(query, x_field, y_field, title):\n    \"\"\"Generate Vega-Lite chart from SQL query.\"\"\"\n\n    conn = sqlite3.connect('analysis.db')\n    cursor = conn.execute(query)\n    columns = [desc[0] for desc in cursor.description]\n    data = [dict(zip(columns, row)) for row in cursor.fetchall()]\n\n    chart = {\n        \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n        \"title\": title,\n        \"data\": {\"values\": data},\n        \"mark\": \"bar\",\n        \"encoding\": {\n            \"x\": {\"field\": x_field, \"type\": \"nominal\"},\n            \"y\": {\"field\": y_field, \"type\": \"quantitative\"}\n        }\n    }\n\n    return json.dumps(chart)\n```\n\n---\n\n## Troubleshooting\n\n### Issue: Data Not Displaying\n\n**Check data format:**\n```python\n# âœ… Correct: Array of objects\n{\"values\": [{\"x\": 1, \"y\": 2}, {\"x\": 2, \"y\": 4}]}\n\n# âŒ Wrong: Flat array\n{\"values\": [1, 2, 3, 4]}\n```\n\n### Issue: Type Mismatch Errors\n\n**Ensure correct field types:**\n- `nominal` - Categories (strings)\n- `ordinal` - Ordered categories\n- `quantitative` - Numbers\n- `temporal` - Dates/times\n\n### Issue: Chart Too Small\n\n**Adjust dimensions:**\n```python\nchart = {\n    \"width\": 800,\n    \"height\": 600,\n    ...\n}\n```\n\n---\n\n## References\n\n- [Vega-Lite Documentation](https://vega.github.io/vega-lite/)\n- [Example Gallery](https://vega.github.io/vega-lite/examples/)\n- [Data Transform Reference](https://vega.github.io/vega-lite/docs/transform.html)\n- [Mark Types](https://vega.github.io/vega-lite/docs/mark.html)\n- [Kroki Vega-Lite Examples](https://kroki.io/examples.html#vegalite)\n",
        "plugins/datapeeker/skills/creating-visualizations/image-formats.md": "# Image-Based Visualization Formats with Kroki\n\nThis document provides guidance for creating image-based diagrams and visualizations using Kroki, complementing the terminal-based visualizations described in `terminal-formats.md`.\n\n---\n\n## When to Use Image-Based Visualizations\n\n**Use image formats (Kroki) when:**\n- Creating diagrams that need to be embedded in reports/presentations\n- Visualizing complex relationships, hierarchies, or networks\n- Documenting data lineage, schemas, or workflows\n- Creating flowcharts, entity-relationship diagrams, or architecture diagrams\n- Output will be viewed outside the terminal (web, PDF, documentation)\n\n**Use terminal formats (plotext, etc.) when:**\n- Working interactively in terminal/Jupyter notebooks\n- Quick data exploration and analysis\n- Output stays in markdown documentation\n- Fast iteration without external dependencies\n\n---\n\n## What is Kroki?\n\n**Kroki** is a unified diagram generation API that creates diagrams from textual descriptions. It supports 25+ diagram formats through a single HTTP interface.\n\n**Key Features:**\n- **Free & Open Source** (MIT licensed)\n- **No installation required** for individual diagram tools\n- **Multiple output formats**: SVG, PNG, PDF\n- **Public service** (kroki.io) or **self-hosted** (Docker)\n- **Simple HTTP API** - Generate diagrams via GET or POST requests\n\n### How Kroki Works\n\n```\nText Description â†’ HTTP Request â†’ Kroki API â†’ Rendered Diagram (SVG/PNG/PDF)\n```\n\n**Example workflow:**\n1. Write diagram description in format-specific syntax (Mermaid, GraphViz, etc.)\n2. Send to Kroki via HTTP (GET with URL encoding or POST with plain text)\n3. Receive rendered diagram as image\n4. Embed in documentation or save to file\n\n---\n\n## Supported Data Visualization Formats\n\nKroki supports 25+ formats. For **data analysis and visualization**, these are most relevant:\n\n| Format | Best For | Use Cases | Guide |\n|--------|----------|-----------|-------|\n| **Vega-Lite** | Statistical charts | Bar charts, line plots, scatter plots, distributions | [formats/vega-lite.md](./formats/vega-lite.md) |\n| **Mermaid** | Flowcharts, timelines | Data workflows, Gantt charts, sequence diagrams | [formats/mermaid.md](./formats/mermaid.md) |\n| **GraphViz (DOT)** | Network graphs | Data lineage, relationships, hierarchies | [formats/graphviz.md](./formats/graphviz.md) |\n| **D2** | Modern diagrams | Architecture, data models, SQL tables | [formats/d2.md](./formats/d2.md) |\n| **ERD** | Database schemas | Entity-relationship diagrams | [formats/erd.md](./formats/erd.md) |\n| **DBML** | Database docs | Complete database documentation with details | [formats/dbml.md](./formats/dbml.md) |\n\n**Quick Selector:**\n- **Charts/plots** â†’ Vega-Lite\n- **Workflows/processes** â†’ Mermaid\n- **Relationships/networks** â†’ GraphViz\n- **Data models** â†’ D2 or ERD\n- **Database schemas** â†’ ERD or DBML\n\n---\n\n## Quick Start\n\n### Using Kroki Public Service\n\n**Generate diagram via Python:**\n\n```python\nimport requests\nimport base64\nimport zlib\n\ndef generate_diagram(diagram_text, diagram_type='mermaid', output_format='svg'):\n    \"\"\"Generate diagram using Kroki public API.\"\"\"\n\n    # Compress and encode (for GET requests)\n    compressed = zlib.compress(diagram_text.encode('utf-8'), 9)\n    encoded = base64.urlsafe_b64encode(compressed).decode('ascii')\n\n    # Kroki endpoint\n    url = f\"https://kroki.io/{diagram_type}/{output_format}/{encoded}\"\n\n    response = requests.get(url)\n    return response.content\n\n# Example: Simple Mermaid flowchart\ndiagram = \"\"\"\ngraph TD\n    A[Raw Data] --> B[Clean Data]\n    B --> C[Analysis]\n    C --> D[Report]\n\"\"\"\n\nsvg_output = generate_diagram(diagram, 'mermaid', 'svg')\n\n# Save to file\nwith open('workflow.svg', 'wb') as f:\n    f.write(svg_output)\n```\n\n### Alternative: POST Request (Simpler)\n\n```python\nimport requests\n\ndef generate_diagram_post(diagram_text, diagram_type='mermaid', output_format='svg'):\n    \"\"\"Generate diagram using POST (no encoding needed).\"\"\"\n\n    url = f\"https://kroki.io/{diagram_type}/{output_format}\"\n\n    response = requests.post(url, data=diagram_text.encode('utf-8'))\n    return response.content\n\n# Same example\ndiagram = \"\"\"\ngraph TD\n    A[Raw Data] --> B[Clean Data]\n    B --> C[Analysis]\n    C --> D[Report]\n\"\"\"\n\nsvg_output = generate_diagram_post(diagram, 'mermaid', 'svg')\n```\n\n---\n\n## Output Formats\n\nKroki supports multiple output formats:\n\n| Format | Use Case | File Extension |\n|--------|----------|----------------|\n| **SVG** | Scalable, embeddable, web-friendly | `.svg` |\n| **PNG** | Raster image, universal compatibility | `.png` |\n| **PDF** | Print-ready documents | `.pdf` |\n| **JPEG** | Photos (not recommended for diagrams) | `.jpg` |\n| **TXT** | Text-based output (some formats) | `.txt` |\n| **Base64** | Embedded in HTML/JSON | N/A |\n\n**Recommendation:** Use SVG for documentation (scales perfectly), PNG for compatibility.\n\n---\n\n## Kroki API Reference\n\n### Endpoint Structure\n\n```\nhttps://kroki.io/{diagram_type}/{output_format}/{encoded_source}\n```\n\n**Parameters:**\n- `diagram_type`: mermaid, graphviz, vegalite, d2, erd, dbml, plantuml, etc.\n- `output_format`: svg, png, pdf, jpeg, txt, base64\n- `encoded_source`: Diagram source (deflate + base64 for GET, or use POST)\n\n### GET Request (URL-Encoded)\n\n```python\nimport zlib\nimport base64\n\n# Compress and encode\nsource = \"graph TD; A-->B;\"\ncompressed = zlib.compress(source.encode('utf-8'), 9)\nencoded = base64.urlsafe_b64encode(compressed).decode('ascii')\n\nurl = f\"https://kroki.io/mermaid/svg/{encoded}\"\n```\n\n### POST Request (Plain Text)\n\n```python\nimport requests\n\nurl = \"https://kroki.io/mermaid/svg\"\nsource = \"graph TD; A-->B;\"\n\nresponse = requests.post(url, data=source.encode('utf-8'))\nsvg = response.content\n```\n\n### Rate Limits (Public Service)\n\n- **100 requests/minute** per IP\n- **4KB URL length** limit (use POST for larger diagrams)\n- **8MB body size** limit\n\n**For unlimited usage:** Self-host Kroki via Docker (see Installation section below).\n\n---\n\n## Installation (Self-Hosting)\n\n### Docker Quick Start\n\n```bash\n# Run Kroki server\ndocker run -d -p 8000:8000 yuzutech/kroki\n\n# Test it\ncurl http://localhost:8000/health\n```\n\n### Docker Compose (Production)\n\n```yaml\nversion: \"3\"\nservices:\n  kroki:\n    image: yuzutech/kroki\n    environment:\n      - KROKI_MERMAID_HOST=kroki-mermaid\n      - KROKI_BPMN_HOST=kroki-bpmn\n      - KROKI_EXCALIDRAW_HOST=kroki-excalidraw\n    ports:\n      - \"8000:8000\"\n\n  kroki-mermaid:\n    image: yuzutech/kroki-mermaid\n    ports:\n      - \"8001:8001\"\n\n  kroki-bpmn:\n    image: yuzutech/kroki-bpmn\n    ports:\n      - \"8002:8002\"\n\n  kroki-excalidraw:\n    image: yuzutech/kroki-excalidraw\n    ports:\n      - \"8003:8003\"\n```\n\nStart services:\n```bash\ndocker-compose up -d\n```\n\n---\n\n## Python Helper Class\n\nReusable class for generating diagrams:\n\n```python\nimport requests\nimport zlib\nimport base64\nfrom pathlib import Path\n\nclass KrokiClient:\n    \"\"\"Client for generating diagrams via Kroki API.\"\"\"\n\n    def __init__(self, base_url=\"https://kroki.io\"):\n        self.base_url = base_url\n\n    def generate(self, source, diagram_type, output_format='svg', method='POST'):\n        \"\"\"Generate diagram and return bytes.\"\"\"\n\n        if method == 'GET':\n            # Compress and encode for GET\n            compressed = zlib.compress(source.encode('utf-8'), 9)\n            encoded = base64.urlsafe_b64encode(compressed).decode('ascii')\n            url = f\"{self.base_url}/{diagram_type}/{output_format}/{encoded}\"\n            response = requests.get(url)\n        else:\n            # POST with plain text\n            url = f\"{self.base_url}/{diagram_type}/{output_format}\"\n            response = requests.post(url, data=source.encode('utf-8'))\n\n        response.raise_for_status()\n        return response.content\n\n    def save(self, source, diagram_type, output_file, output_format='svg'):\n        \"\"\"Generate and save diagram to file.\"\"\"\n\n        diagram = self.generate(source, diagram_type, output_format)\n\n        output_path = Path(output_file)\n        output_path.write_bytes(diagram)\n\n        return output_path\n\n# Usage\nclient = KrokiClient()\n\n# Generate and save\nmermaid_source = \"\"\"\ngraph TD\n    A[Start] --> B[Process]\n    B --> C[End]\n\"\"\"\n\nclient.save(mermaid_source, 'mermaid', 'flowchart.svg')\n```\n\n---\n\n## Best Practices\n\n### 1. Choose the Right Format\n\n- **Simple flowcharts** â†’ Mermaid (easiest syntax)\n- **Complex network graphs** â†’ GraphViz (powerful layout algorithms)\n- **Statistical charts** â†’ Vega-Lite (data transformation pipeline)\n- **Database schemas** â†’ ERD or DBML (standard notation)\n- **Modern diagrams** â†’ D2 (clean, readable syntax)\n\n### 2. Use POST for Complex Diagrams\n\nGET requests require URL encoding and have length limits. POST is simpler:\n\n```python\n# âœ… Good: POST for complex diagrams\nrequests.post(url, data=diagram_source.encode('utf-8'))\n\n# âŒ Avoid: GET for complex diagrams (encoding + length limits)\n```\n\n### 3. Cache Generated Diagrams\n\nAvoid regenerating the same diagram repeatedly:\n\n```python\nfrom pathlib import Path\nimport hashlib\n\ndef get_or_generate_diagram(source, diagram_type, cache_dir='./diagram_cache'):\n    \"\"\"Generate diagram with caching.\"\"\"\n\n    # Create cache key from source\n    cache_key = hashlib.md5(source.encode()).hexdigest()\n    cache_file = Path(cache_dir) / f\"{cache_key}.svg\"\n\n    if cache_file.exists():\n        return cache_file.read_bytes()\n\n    # Generate if not cached\n    diagram = generate_diagram_post(source, diagram_type)\n\n    cache_file.parent.mkdir(exist_ok=True)\n    cache_file.write_bytes(diagram)\n\n    return diagram\n```\n\n### 4. Self-Host for Production\n\nPublic service has rate limits. For production:\n- Deploy Kroki via Docker\n- No rate limits\n- Better performance\n- Data privacy\n\n### 5. Version Control Diagram Source\n\nStore diagram source in version control, not just images:\n\n```\nanalysis/\nâ”œâ”€â”€ diagrams/\nâ”‚   â”œâ”€â”€ data-pipeline.mermaid.txt\nâ”‚   â”œâ”€â”€ schema.erd.txt\nâ”‚   â””â”€â”€ lineage.dot.txt\nâ””â”€â”€ images/\n    â”œâ”€â”€ data-pipeline.svg\n    â”œâ”€â”€ schema.svg\n    â””â”€â”€ lineage.svg\n```\n\n---\n\n## Integration with DataPeeker\n\n### Pattern: Documenting Data Lineage\n\n```python\nfrom kroki_client import KrokiClient\n\nclient = KrokiClient()\n\n# Generate data lineage diagram\nlineage = \"\"\"\ndigraph G {\n    rankdir=LR;\n\n    raw_orders [label=\"raw_orders\\n(CSV)\"];\n    clean_orders [label=\"clean_orders\\n(SQLite)\"];\n    revenue_analysis [label=\"revenue_analysis\\n(Markdown)\"];\n\n    raw_orders -> clean_orders [label=\"cleaning\"];\n    clean_orders -> revenue_analysis [label=\"analysis\"];\n}\n\"\"\"\n\nclient.save(lineage, 'graphviz', 'analysis/lineage.svg')\n```\n\n### Pattern: Database Schema Documentation\n\n```python\n# ERD format\nschema = \"\"\"\n[users]\n*id {label: \"int\"}\nemail {label: \"varchar, unique\"}\nname {label: \"varchar\"}\n\n[orders]\n*id {label: \"int\"}\n+user_id {label: \"int, FK\"}\norder_date {label: \"date\"}\ntotal {label: \"decimal\"}\n\nusers 1--* orders\n\"\"\"\n\nclient.save(schema, 'erd', 'analysis/schema.svg')\n```\n\n---\n\n## Format-Specific Guides\n\nEach format has detailed documentation with examples:\n\n1. **[Vega-Lite](./formats/vega-lite.md)** - Statistical charts and data visualizations\n2. **[Mermaid](./formats/mermaid.md)** - Flowcharts, Gantt charts, sequence diagrams\n3. **[GraphViz](./formats/graphviz.md)** - Network graphs, hierarchies, data lineage\n4. **[D2](./formats/d2.md)** - Modern diagrams, architecture, data models\n5. **[ERD](./formats/erd.md)** - Entity-relationship diagrams for databases\n6. **[DBML](./formats/dbml.md)** - Complete database documentation\n\n---\n\n## Troubleshooting\n\n### Issue: \"URL Too Long\" Error\n\n**Solution:** Use POST instead of GET:\n```python\n# Use POST for diagrams > 4KB\nrequests.post(url, data=source.encode('utf-8'))\n```\n\n### Issue: \"Rate Limit Exceeded\"\n\n**Solution:** Self-host Kroki or implement request throttling:\n```python\nimport time\n\ndef generate_with_retry(source, diagram_type, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return generate_diagram_post(source, diagram_type)\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:  # Rate limit\n                time.sleep(60)  # Wait 1 minute\n                continue\n            raise\n```\n\n### Issue: Syntax Errors in Diagram\n\n**Check format documentation** - Each format has specific syntax rules. See individual format guides.\n\n---\n\n## Additional Resources\n\n**Official Documentation:**\n- [Kroki Documentation](https://docs.kroki.io/)\n- [Kroki GitHub](https://github.com/yuzutech/kroki)\n- [Diagram Examples](https://kroki.io/examples.html)\n\n**Format Documentation:**\n- [Vega-Lite Docs](https://vega.github.io/vega-lite/)\n- [Mermaid Docs](https://mermaid.js.org/)\n- [GraphViz Docs](https://graphviz.org/)\n- [D2 Docs](https://d2lang.com/)\n- [PlantUML Docs](https://plantuml.com/)\n\n**Complete research documentation:**\n- See `KROKI_RESEARCH.md` for comprehensive technical details\n",
        "plugins/datapeeker/skills/creating-visualizations/terminal-formats.md": "# Terminal-Based Visualization Formats\n\nThis document provides detailed implementation guidance for creating text-based visualizations in terminal/markdown environments. Referenced by the main `creating-visualizations` skill when creating visualizations for DataPeeker analysis sessions.\n\n---\n\n## âš ï¸ CRITICAL PRINCIPLE: Use Visualization Tools, Not Manual Creation\n\n**NEVER manually create data visualizations** (bar charts, line plots, histograms, sparklines, scatter plots). Manual ASCII art introduces errors, misalignments, and scaling mistakes. **ALWAYS use established visualization tools.**\n\n### What This Means\n\n**âœ… ALLOWED - Manual Creation:**\n- **Tables with exact values** - Markdown tables showing query results with precise numbers\n- **Callout boxes** - Formatted text highlighting key metrics\n- **Ranked lists** - Numbered/bulleted lists with exact values from queries\n\n**âŒ PROHIBITED - Manual Creation:**\n- Bar charts (ASCII or Unicode bars like `â–ˆâ–ˆâ–ˆâ–ˆ`)\n- Line plots or time series (character plots with `*` or `-`)\n- Histograms (distribution visualizations)\n- Sparklines (trend indicators with â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ)\n- Scatter plots or correlation visualizations\n- Any visualization requiring scaling, normalization, or computed positioning\n\n### Why This Matters\n\nManual visualization creation fails because:\n1. **Scaling errors** - Bar lengths don't match proportions\n2. **Alignment issues** - Characters don't line up correctly\n3. **Calculation mistakes** - Wrong values mapped to visual elements\n4. **Maintenance burden** - Manual updates when data changes\n5. **Lack of reproducibility** - No clear process to regenerate\n\n### The Rule\n\n**If it involves visual representation of relative magnitudes, trends, or distributions: USE A TOOL.**\n\n**If it's exact numbers in a structured format: Tables are fine.**\n\n---\n\n## Quick Start: Recommended Visualization Tools\n\nInstall these tools to create terminal visualizations programmatically:\n\n### Python (Recommended for DataPeeker)\n\n**For most use cases:**\n```bash\npip install plotext asciichartpy termgraph sparklines rich\n```\n\n**Individual tools by use case:**\n\n| Use Case | Tool | Install Command | Key Features |\n|----------|------|-----------------|--------------|\n| All-purpose (best choice) | plotext | `pip install plotext` | 9 chart types, no dependencies, 2.3M+ downloads/month |\n| Line charts | asciichartpy | `pip install asciichartpy` | Simple, zero dependencies, fast |\n| Bar charts | termgraph | `pip install termgraph` | Multiple chart types, data from stdin/files |\n| Sparklines | sparklines | `pip install sparklines` | Tiny trend indicators, one-line install |\n| Tables + formatting | rich | `pip install rich` | Beautiful tables, progress bars, text formatting |\n| Histograms | plotext | `pip install plotext` | Built-in histogram support |\n\n### Other Languages (if needed)\n\n**Node.js:**\n```bash\nnpm install asciichart\n```\n\n**Go:**\n```bash\ngo install github.com/guptarohit/asciigraph@latest\n```\n\n**Rust:**\n```toml\n# Add to Cargo.toml\ntextplots = \"0.8\"\n```\n\n### Quick Verification\n\nTest your installation:\n\n```python\n# Test plotext (most comprehensive)\npython3 -c \"import plotext as plt; plt.bar(['A', 'B', 'C'], [10, 20, 15]); plt.show()\"\n\n# Test asciichartpy (line charts)\npython3 -c \"from asciichartpy import plot; print(plot([1, 2, 5, 3, 4]))\"\n\n# Test sparklines (tiny trends)\npython3 -c \"from sparklines import sparklines; print(''.join(sparklines([1, 2, 5, 3, 4])))\"\n```\n\n**If any test fails, see the troubleshooting guide in the research documentation.**\n\n---\n\n## Format 1: Markdown Tables\n\n**Use for:** Structured data with multiple columns, exact values matter\n\n**Basic table:**\n```markdown\n| Category    | Orders | Revenue    | Avg Order Value |\n|-------------|-------:|:----------:|----------------:|\n| Electronics | 1,523  | $345,678   | $226.89         |\n| Home        | 2,341  | $298,432   | $127.48         |\n| Clothing    | 3,456  | $234,567   | $67.89          |\n```\n\n**Best practices:**\n- Align numbers right, text left (use `:---:` or `---:` in header separator)\n- Include totals row when appropriate\n- Use bold for emphasis: `**$345,678**`\n- Add percentage columns for context\n- Keep columns to 5-7 maximum for readability\n\n**Table with percentages:**\n```markdown\n| Category    | Revenue    | % of Total | Growth YoY |\n|-------------|:----------:|-----------:|-----------:|\n| Electronics | $345,678   |      42.5% |    +23.4%  |\n| Home        | $298,432   |      36.8% |    +15.2%  |\n| Clothing    | $234,567   |      28.9% |     -8.1%  |\n| **Total**   | **$811,677** | **100.0%** |  **+12.3%** |\n```\n\n---\n\n## Format 2: ASCII Bar Charts\n\n**Use for:** Visual magnitude comparison, relative sizes matter more than exact values\n\n**âš ï¸ DO NOT CREATE MANUALLY - Use a tool instead**\n\n### Using plotext (Recommended)\n\n```python\nimport plotext as plt\n\ncategories = ['Electronics', 'Home & Garden', 'Clothing', 'Sports', 'Toys']\nvalues = [345678, 298432, 234567, 134234, 56789]\n\nplt.simple_bar(categories, values, width=50, title='Sales by Category')\nplt.show()\n```\n\n### Using termgraph\n\n```python\nfrom termgraph import termgraph as tg\n\ncategories = ['Electronics', 'Home & Garden', 'Clothing', 'Sports', 'Toys']\nvalues = [[345678], [298432], [234567], [134234], [56789]]\n\ndata = list(zip(categories, values))\nargs = {\n    'width': 50,\n    'format': '{:<5.2f}',\n    'suffix': '',\n    'no_labels': False,\n}\ntg.chart(colors=[91], data=data, args=args)\n```\n\n### From SQLite Query\n\n```python\nimport sqlite3\nimport plotext as plt\n\nconn = sqlite3.connect('analytics.db')\ncursor = conn.execute(\"\"\"\n    SELECT category, SUM(revenue) as total_revenue\n    FROM sales\n    GROUP BY category\n    ORDER BY total_revenue DESC\n\"\"\")\n\ncategories, revenues = zip(*cursor.fetchall())\n\nplt.simple_bar(categories, revenues, title='Revenue by Category')\nplt.show()\n```\n\n**Output includes proper scaling, alignment, and exact values automatically.**\n\n---\n\n## Format 3: Sparklines\n\n**Use for:** Compact trend visualization, showing direction and volatility\n\n**âš ï¸ DO NOT CREATE MANUALLY - Use the sparklines library**\n\n### Using sparklines (Recommended)\n\n```python\nfrom sparklines import sparklines\n\n# Monthly revenue data (in thousands)\nq1_revenue = [300, 350, 420, 450]\nq2_revenue = [450, 455, 460, 450]\nq3_revenue = [450, 420, 380, 340]\nq4_revenue = [340, 380, 440, 490]\n\nprint(f\"Q1 2024: {''.join(sparklines(q1_revenue))}  (Strong growth, $1.2M total)\")\nprint(f\"Q2 2024: {''.join(sparklines(q2_revenue))}  (Plateau, $1.8M total)\")\nprint(f\"Q3 2024: {''.join(sparklines(q3_revenue))}  (Decline, $1.5M total)\")\nprint(f\"Q4 2024: {''.join(sparklines(q4_revenue))}  (Recovery, $1.9M total)\")\n```\n\n### From SQLite Query\n\n```python\nimport sqlite3\nfrom sparklines import sparklines\n\nconn = sqlite3.connect('analytics.db')\ncursor = conn.execute(\"\"\"\n    SELECT strftime('%Y-%m', order_date) as month,\n           SUM(revenue) as monthly_revenue\n    FROM orders\n    WHERE order_date >= '2024-01-01'\n    GROUP BY month\n    ORDER BY month\n\"\"\")\n\nmonths, revenues = zip(*cursor.fetchall())\nspark = ''.join(sparklines(revenues))\n\nprint(f\"2024 Revenue Trend: {spark}\")\nprint(f\"Months: {months[0]} to {months[-1]}\")\nprint(f\"Total: ${sum(revenues):,.0f}\")\n```\n\n**When to use:**\n- Space is limited\n- Showing multiple trends for comparison\n- Trend direction matters more than exact values\n- Inline with text narrative\n\n**Sparkline characters (smallest to largest):** â– â–‚ â–ƒ â–„ â–… â–† â–‡ â–ˆ\n\n---\n\n## Format 4: ASCII Histograms\n\n**Use for:** Distribution visualization, showing shape and spread\n\n**âš ï¸ DO NOT CREATE MANUALLY - Use plotext**\n\n### Using plotext (Recommended)\n\n```python\nimport plotext as plt\nimport statistics\n\n# Order values from query\norder_values = [23, 45, 67, 42, 89, 34, 56, 78, 45, 67, ...]  # Your data\n\nplt.hist(order_values, bins=7, label='Order Values')\nplt.title('Order Value Distribution')\nplt.xlabel('Order Value ($)')\nplt.ylabel('Frequency')\n\n# Add summary statistics\nmedian_val = statistics.median(order_values)\nmean_val = statistics.mean(order_values)\nstdev_val = statistics.stdev(order_values)\n\nplt.show()\nprint(f\"\\nTotal: {len(order_values):,} orders\")\nprint(f\"Median: ${median_val:.0f} | Mean: ${mean_val:.0f} | Std Dev: ${stdev_val:.0f}\")\n```\n\n### From SQLite Query\n\n```python\nimport sqlite3\nimport plotext as plt\n\nconn = sqlite3.connect('analytics.db')\ncursor = conn.execute(\"SELECT order_value FROM orders\")\norder_values = [row[0] for row in cursor.fetchall()]\n\nplt.hist(order_values, bins=10)\nplt.title('Order Value Distribution')\nplt.xlabel('Order Value ($)')\nplt.ylabel('Number of Orders')\nplt.show()\n\n# Show summary stats\nprint(f\"\\nTotal orders: {len(order_values):,}\")\n```\n\n**Best practices:**\n- Let plotext calculate optimal bin widths\n- Use 5-10 bins for most distributions\n- Show summary statistics below the histogram\n- Label axes with units\n\n---\n\n## Format 5: Callout Boxes\n\n**Use for:** Highlighting single key metrics or insights\n\n**Simple callout:**\n```markdown\n---\n**KEY FINDING**\n\nRevenue increased **23.4%** from Q3 to Q4\n($1.5M â†’ $1.9M, +$400K absolute growth)\n\n---\n```\n\n**Multi-metric dashboard:**\n```markdown\n---\n## Q4 2024 Summary\n\nðŸ“Š **Total Revenue:** $1,876,543 (â†‘ 23.4% QoQ)\nðŸ›’ **Orders:** 12,345 (â†‘ 18.2% QoQ)\nðŸ’µ **Avg Order Value:** $152.14 (â†‘ 4.4% QoQ)\nðŸ‘¥ **Active Customers:** 8,934 (â†‘ 15.7% QoQ)\n\n---\n```\n\n**Comparison callout:**\n```markdown\n---\n## Treatment vs Control Comparison\n\n**Treatment Group (n=1,234)**\n- Conversion Rate: **3.2%** (â†‘0.8pp vs control)\n- Revenue: **$156,789** (+$23,456)\n- AOV: **$127.14** (+$4.23)\n\n**Control Group (n=1,198)**\n- Conversion Rate: 2.4%\n- Revenue: $133,333\n- AOV: $122.91\n\n**Statistical Significance:** p < 0.05 âœ“\n\n---\n```\n\n---\n\n## Format 6: Ranked Lists\n\n**Use for:** Top/bottom N, ordered items with narrative\n\n**Simple ranked list:**\n```markdown\n## Top 5 Products by Revenue (Q4 2024)\n\n1. **Widget Pro** â€” $234,567 (18.2% of total)\n   - Units sold: 2,345 | AOV: $100.03\n   - Growth: +45% vs Q3\n\n2. **Gadget Plus** â€” $198,432 (15.4% of total)\n   - Units sold: 4,567 | AOV: $43.44\n   - Growth: +23% vs Q3\n\n3. **Doohickey Elite** â€” $176,543 (13.7% of total)\n   - Units sold: 876 | AOV: $201.52\n   - Growth: +12% vs Q3\n\n4. **Thingamajig** â€” $145,678 (11.3% of total)\n   - Units sold: 3,456 | AOV: $42.16\n   - Growth: -5% vs Q3\n\n5. **Whatsit Standard** â€” $123,456 (9.6% of total)\n   - Units sold: 2,678 | AOV: $46.11\n   - Growth: +8% vs Q3\n\n**Top 5 represent 68.2% of total revenue**\n```\n\n**Comparative ranked list:**\n```markdown\n## Best and Worst Performing Regions\n\n### Top 3 (by growth)\n1. ðŸ”¥ **West Region** â€” +34.5% growth | $456K revenue\n2. ðŸ“ˆ **South Region** â€” +28.2% growth | $389K revenue\n3. âœ“ **Central Region** â€” +15.7% growth | $312K revenue\n\n### Bottom 3 (by growth)\n1. âš ï¸ **Northeast Region** â€” -8.3% decline | $278K revenue\n2. ðŸ“‰ **Mid-Atlantic** â€” -4.1% decline | $234K revenue\n3. â†’ **Northwest** â€” +1.2% growth | $198K revenue\n\n**Note:** All regions increased in absolute terms, but growth rates varied widely.\n```\n\n---\n\n## Format 7: Comparison Tables\n\n**Use for:** Side-by-side metric comparison across segments or time periods\n\n**Period-over-period comparison:**\n```markdown\n## Q4 2024 vs Q4 2023 Performance\n\n| Metric               | Q4 2023   | Q4 2024   | Change     | % Change |\n|----------------------|----------:|----------:|-----------:|---------:|\n| Revenue              | $1.52M    | $1.88M    | +$360K     |  +23.7%  |\n| Orders               | 10,234    | 12,345    | +2,111     |  +20.6%  |\n| Avg Order Value      | $148.52   | $152.14   | +$3.62     |   +2.4%  |\n| Active Customers     | 7,456     | 8,934     | +1,478     |  +19.8%  |\n| Customer Acq Cost    | $42.15    | $38.76    | -$3.39     |   -8.0%  |\n| Customer LTV         | $456.78   | $523.12   | +$66.34    |  +14.5%  |\n```\n\n**Segment comparison:**\n```markdown\n## B2B vs B2C Customer Behavior\n\n| Metric                    | B2B        | B2C       | B2B/B2C Ratio |\n|---------------------------|:----------:|:---------:|--------------:|\n| Avg Order Value           | $1,234.56  | $87.23    |         14.2x |\n| Orders per Customer       | 8.4        | 2.1       |          4.0x |\n| Annual Customer Value     | $10,370    | $183      |         56.7x |\n| Purchase Frequency        | Monthly    | Quarterly |             â€” |\n| Repeat Purchase Rate      | 87%        | 34%       |          2.6x |\n| Customer Support Tickets  | 0.8/order  | 0.2/order |          4.0x |\n\n**Insight:** B2B customers are higher value but require more support resources.\n```\n\n---\n\n## Format 8: Line Plots and Scatter Plots\n\n**Use for:** Time series, trends, relationships between variables\n\n**âš ï¸ DO NOT CREATE MANUALLY - Use plotext or asciichartpy**\n\n### Line Plot with plotext\n\n```python\nimport plotext as plt\n\n# Daily order volumes\ndays = list(range(1, 31))\nvolumes = [1234, 1567, 1890, 2134, 2456, ...]  # 30 days of data\n\nplt.plot(days, volumes, label='Order Volume')\nplt.title('Daily Order Volume (Last 30 Days)')\nplt.xlabel('Days Ago')\nplt.ylabel('Orders')\nplt.show()\n```\n\n### Line Plot with asciichartpy\n\n```python\nfrom asciichartpy import plot\n\nvolumes = [1234, 1567, 1890, 2134, 2456, 1123, 945, ...]\nprint(plot(volumes, {'height': 15}))\nprint(\"\\nDaily Order Volume - Last 30 Days\")\n```\n\n### Scatter Plot with plotext\n\n```python\nimport plotext as plt\nimport sqlite3\n\nconn = sqlite3.connect('analytics.db')\ncursor = conn.execute(\"\"\"\n    SELECT shipping_days, order_value\n    FROM orders\n    WHERE shipping_days IS NOT NULL\n\"\"\")\n\nshipping, values = zip(*cursor.fetchall())\n\nplt.scatter(shipping, values)\nplt.title('Order Value vs Shipping Speed')\nplt.xlabel('Shipping Speed (days)')\nplt.ylabel('Order Value ($)')\nplt.show()\n```\n\n### Multiple Series\n\n```python\nimport plotext as plt\n\ndays = list(range(1, 13))  # 12 months\nrevenue_2023 = [120, 130, 145, 150, 160, 170, 165, 175, 180, 190, 210, 230]\nrevenue_2024 = [140, 155, 170, 180, 190, 210, 220, 240, 260, 280, 310, 340]\n\nplt.plot(days, revenue_2023, label='2023')\nplt.plot(days, revenue_2024, label='2024')\nplt.title('Monthly Revenue Comparison')\nplt.xlabel('Month')\nplt.ylabel('Revenue ($K)')\nplt.show()\n```\n\n---\n\n## Full Annotated Example\n\n```markdown\n## Top 10 Products by Revenue â€” Q4 2024\n\n**Key Finding:** Top 10 products generated $1.2M (64% of total revenue), with Widget Pro alone accounting for nearly 1 in 5 dollars.\n\n| Rank | Product           | Revenue    | % of Total | Units Sold | Avg Price |\n|-----:|:------------------|:----------:|-----------:|-----------:|----------:|\n|   1. | Widget Pro        | $234,567   |      18.2% |      2,345 |   $100.03 |\n|   2. | Gadget Plus       | $198,432   |      15.4% |      4,567 |    $43.44 |\n|   3. | Doohickey Elite   | $176,543   |      13.7% |        876 |   $201.52 |\n|   4. | Thingamajig       | $145,678   |      11.3% |      3,456 |    $42.16 |\n|   5. | Whatsit Standard  | $123,456   |       9.6% |      2,678 |    $46.11 |\n|   6. | Contraption       | $98,765    |       7.7% |      1,987 |    $49.71 |\n|   7. | Apparatus         | $87,654    |       6.8% |      1,234 |    $71.04 |\n|   8. | Gizmo             | $76,543    |       5.9% |      3,456 |    $22.15 |\n|   9. | Mechanism         | $65,432    |       5.1% |        987 |    $66.29 |\n|  10. | Device            | $54,321    |       4.2% |      2,134 |    $25.46 |\n| **Total Top 10** | **â€”**    | **$1,261,391** | **63.5%** | **23,720** | **$53.18** |\n| All Other Products | â€”      | $726,152   |      36.5% |     34,281 |    $21.18 |\n\n**Data notes:**\n- Source: analytics.db, aggregated from orders and order_items tables\n- Time period: October 1 - December 31, 2024 (92 days)\n- Excludes returns, refunds, and cancelled orders (47 orders excluded)\n\n**What this shows:**\n- Revenue concentration: Top 10 products = 64% of revenue but only 41% of units\n- Price segmentation: Widget Pro and Doohickey Elite are premium products (avg >$100)\n- Volume products: Gadget Plus and Thingamajig drive revenue through volume\n- Long tail: \"All Other\" represents 127 products with lower individual sales\n\n**Implications:**\n- Inventory: Prioritize stock of top 10 products to avoid stockouts\n- Marketing: Feature Widget Pro prominently (highest revenue per SKU)\n- Risk: Revenue concentration in top 10 creates dependency risk\n- Opportunity: Investigate why \"All Other\" products have lower AOV\n\n**Follow-up questions:**\n- How has top 10 list changed vs Q3? (product ranking stability)\n- What's the profit margin profile of top 10 vs others? (revenue â‰  profit)\n- Are top 10 products also highest rated? (quality vs marketing driven)\n```\n\n---\n\n## Additional Resources\n\n**For comprehensive tool documentation, installation troubleshooting, and code examples, see:**\n- `INDEX_TERMINAL_VISUALIZATION.md` - Main navigation document\n- `ASCII_TERMINAL_VISUALIZATION_GUIDE.md` - Detailed tool reference\n- `TERMINAL_VISUALIZATION_QUICK_START.md` - 30+ ready-to-run examples\n\n**For tables:**\nUse the `rich` library for beautifully formatted tables:\n```python\nfrom rich.console import Console\nfrom rich.table import Table\n\ntable = Table(title=\"Sales by Category\")\ntable.add_column(\"Category\", style=\"cyan\")\ntable.add_column(\"Revenue\", justify=\"right\", style=\"green\")\n\ntable.add_row(\"Electronics\", \"$345,678\")\ntable.add_row(\"Home\", \"$298,432\")\n\nConsole().print(table)\n```\n",
        "plugins/datapeeker/skills/detect-foreign-keys/SKILL.md": "---\nname: detect-foreign-keys\ndescription: Identify foreign key relationships between tables using heuristics, value overlap analysis, and referential integrity checks\n---\n\n# Detecting Foreign Keys\n\n## Purpose\n\nThis component skill guides systematic foreign key relationship detection in relational databases. Use it when:\n- Multiple tables exist in the database and relationships are undocumented\n- Need to understand table relationships before joining data\n- Validating referential integrity between tables\n- Identifying orphaned records that reference non-existent parent records\n- Referenced by importing-data or cleaning-data skills requiring relationship analysis\n\n## Prerequisites\n\n- Tables exist in database (relational database with SQL support)\n- SQL query tool available (database CLI, IDE, or query interface)\n- Table schemas have been examined (Phase 1 of understanding-data)\n- Analysis workspace created\n\n## Foreign Key Detection Process\n\nCreate a TodoWrite checklist for the 5-phase FK detection process:\n\n```\nPhase 1: Candidate Identification - pending\nPhase 2: Value Overlap Analysis - pending\nPhase 3: Cardinality Assessment - pending\nPhase 4: Referential Integrity Validation - pending\nPhase 5: Relationship Documentation - pending\n```\n\nMark each phase as you complete it. Document all findings in structured format.\n\n---\n\n## Phase 1: Candidate Identification\n\n**Goal:** Identify columns that are likely foreign keys based on naming patterns, data types, and uniqueness.\n\n### Identify Candidate FK Columns by Naming Convention\n\n**Common FK naming patterns:**\n- Columns ending in `_id` (e.g., `customer_id`, `product_id`)\n- Columns ending in `Id` (e.g., `customerId`, `productId`)\n- Columns named exactly `id` (but only in child tables)\n- Columns starting with `fk_` (e.g., `fk_customer`)\n- Columns matching another table name (e.g., `customer` in orders table)\n\n```sql\n-- List all columns across all tables\nSELECT\n  m.name as table_name,\n  p.name as column_name,\n  p.type as column_type\nFROM sqlite_master m\nJOIN pragma_table_info(m.name) p\nWHERE m.type = 'table'\n  AND m.name NOT LIKE 'sqlite_%'\n  AND (\n    p.name LIKE '%_id'\n    OR p.name LIKE '%Id'\n    OR p.name LIKE 'fk_%'\n    OR p.name = 'id'\n  )\nORDER BY m.name, p.name;\n```\n\n**Document:**\n- List of candidate FK columns per table\n- Note naming patterns observed\n- Flag columns that might be composite keys (multiple FK columns in same table)\n\n### Identify Candidate PK Columns\n\n**Primary key characteristics:**\n- Named `id`, `[table]_id`, or similar\n- INTEGER or TEXT type\n- Likely to be unique\n- Often the first column in the table\n\n```sql\n-- Find columns likely to be primary keys\nSELECT\n  m.name as table_name,\n  p.name as column_name,\n  p.type as column_type,\n  p.pk as is_primary_key\nFROM sqlite_master m\nJOIN pragma_table_info(m.name) p\nWHERE m.type = 'table'\n  AND m.name NOT LIKE 'sqlite_%'\n  AND (\n    p.pk = 1  -- Explicitly defined PK\n    OR p.name = 'id'\n    OR p.name = m.name || '_id'\n  )\nORDER BY m.name;\n```\n\n**Document:**\n- Primary key columns per table\n- Whether PKs are explicitly defined (pk=1) or inferred\n- Tables without obvious primary keys\n\n### Match FK Candidates to Potential Parent Tables\n\n**Heuristic:** A column named `customer_id` likely references a table named `customers` or `customer`.\n\n```sql\n-- Cross-reference FK column names with table names\n-- (Pseudo-query - implement with string matching logic)\n-- For each FK candidate like 'customer_id':\n--   1. Strip suffix (_id, Id)\n--   2. Look for table named 'customers', 'customer', or similar\n--   3. Record as potential relationship\n```\n\n**Document:**\n- FK candidate â†’ Parent table mapping (e.g., `orders.customer_id` â†’ `customers.id`)\n- Confidence level:\n  - **High:** Exact name match (e.g., `customer_id` â†’ `customer` table)\n  - **Medium:** Plural/singular variation (e.g., `customer_id` â†’ `customers` table)\n  - **Low:** Partial name match or ambiguous\n\n---\n\n## Phase 2: Value Overlap Analysis\n\n**Goal:** Validate FK candidates by checking if their values actually exist in the proposed parent table.\n\n### Check Value Overlap Percentage\n\nFor each candidate FK relationship identified in Phase 1:\n\n```sql\n-- Calculate what percentage of FK values exist in parent table\nWITH fk_values AS (\n  SELECT DISTINCT child_fk_column as value\n  FROM child_table\n  WHERE child_fk_column IS NOT NULL\n),\npk_values AS (\n  SELECT DISTINCT parent_pk_column as value\n  FROM parent_table\n  WHERE parent_pk_column IS NOT NULL\n),\noverlap AS (\n  SELECT COUNT(*) as matching_count\n  FROM fk_values fk\n  WHERE fk.value IN (SELECT value FROM pk_values)\n)\nSELECT\n  (SELECT COUNT(*) FROM fk_values) as total_fk_values,\n  (SELECT COUNT(*) FROM pk_values) as total_pk_values,\n  overlap.matching_count,\n  ROUND(100.0 * overlap.matching_count / (SELECT COUNT(*) FROM fk_values), 2) as match_percentage\nFROM overlap;\n```\n\n**Interpret match percentage:**\n- **100% match:** Strong FK relationship (perfect referential integrity)\n- **95-99% match:** Likely FK with some orphaned records\n- **80-94% match:** Possible FK with significant orphans (investigate)\n- **<80% match:** Unlikely to be true FK (name coincidence or wrong parent table)\n\n**Document:**\n- Match percentage for each candidate relationship\n- Count of orphaned FK values (values not in parent)\n- Count of unused PK values (values not referenced by any FK)\n\n### Identify Orphaned Records\n\nFor relationships with <100% match:\n\n```sql\n-- Find child records with FK values that don't exist in parent\nSELECT\n  child_table.rowid,\n  child_table.child_fk_column as orphaned_value,\n  COUNT(*) OVER (PARTITION BY child_table.child_fk_column) as occurrences\nFROM child_table\nLEFT JOIN parent_table ON child_table.child_fk_column = parent_table.parent_pk_column\nWHERE parent_table.parent_pk_column IS NULL\n  AND child_table.child_fk_column IS NOT NULL\nLIMIT 20;\n```\n\n**Document:**\n- Sample orphaned values\n- How many child records affected\n- Whether orphaned values follow a pattern (all recent, specific category, etc.)\n\n### Check Reverse Overlap (Unused Parent Records)\n\n```sql\n-- Find parent records not referenced by any child\nSELECT\n  parent_table.parent_pk_column as unused_pk_value,\n  COUNT(*) as occurrence_count\nFROM parent_table\nLEFT JOIN child_table ON parent_table.parent_pk_column = child_table.child_fk_column\nWHERE child_table.child_fk_column IS NULL\n  AND parent_table.parent_pk_column IS NOT NULL\nLIMIT 20;\n```\n\n**Document:**\n- Count of unused parent records\n- Whether this is expected (e.g., new customers with no orders yet)\n\n---\n\n## Phase 3: Cardinality Assessment\n\n**Goal:** Determine the relationship type (one-to-one, one-to-many, many-to-many).\n\n### Calculate FK â†’ PK Cardinality\n\n**How many child records per parent record?**\n\n```sql\n-- Average number of child records per parent\nSELECT\n  COUNT(*) as total_child_records,\n  COUNT(DISTINCT child_fk_column) as distinct_fk_values,\n  ROUND(1.0 * COUNT(*) / NULLIF(COUNT(DISTINCT child_fk_column), 0), 2) as avg_children_per_parent,\n  MIN(child_count) as min_children,\n  MAX(child_count) as max_children\nFROM child_table\nCROSS JOIN (\n  SELECT\n    child_fk_column as fk,\n    COUNT(*) as child_count\n  FROM child_table\n  WHERE child_fk_column IS NOT NULL\n  GROUP BY child_fk_column\n);\n```\n\n**Interpret cardinality:**\n- **avg = 1.0, max = 1:** One-to-one relationship\n- **avg > 1.0:** One-to-many relationship (most common)\n- **Multiple FK columns referencing same parent:** Potential many-to-many via junction table\n\n**Document:**\n- Relationship type (one-to-one, one-to-many)\n- Average, min, max child records per parent\n- Whether distribution is balanced or skewed\n\n### Identify Many-to-Many Relationships\n\n**Junction table characteristics:**\n- Table has 2+ foreign keys\n- Few or no other columns besides FKs\n- Composite primary key (both FKs together)\n\n```sql\n-- Find tables with multiple FK candidates (potential junction tables)\nSELECT\n  table_name,\n  COUNT(*) as fk_column_count,\n  GROUP_CONCAT(column_name, ', ') as fk_columns\nFROM (\n  SELECT\n    m.name as table_name,\n    p.name as column_name\n  FROM sqlite_master m\n  JOIN pragma_table_info(m.name) p\n  WHERE m.type = 'table'\n    AND m.name NOT LIKE 'sqlite_%'\n    AND (p.name LIKE '%_id' OR p.name LIKE 'fk_%')\n)\nGROUP BY table_name\nHAVING COUNT(*) >= 2\nORDER BY fk_column_count DESC;\n```\n\n**Document:**\n- Junction tables identified\n- Which two (or more) tables they connect\n- Cardinality of the many-to-many relationship\n\n### Check for Self-Referencing FKs\n\n**Hierarchical data pattern:**\n- Table has FK pointing to its own PK (e.g., `employee.manager_id` â†’ `employee.id`)\n\n```sql\n-- Find columns that might reference the same table\nSELECT\n  table_name,\n  column_name,\n  type\nFROM (\n  SELECT\n    m.name as table_name,\n    p.name as column_name,\n    p.type as type\n  FROM sqlite_master m\n  JOIN pragma_table_info(m.name) p\n  WHERE m.type = 'table'\n    AND m.name NOT LIKE 'sqlite_%'\n    AND (\n      p.name LIKE 'parent_%'\n      OR p.name LIKE 'manager_%'\n      OR p.name LIKE '%_parent_id'\n    )\n);\n```\n\n**Document:**\n- Self-referencing relationships\n- Depth of hierarchy (max levels)\n- Orphaned roots or cycles\n\n---\n\n## Phase 4: Referential Integrity Validation\n\n**Goal:** Quantify integrity violations and assess data quality impact.\n\n### Calculate Integrity Violation Rate\n\nFor each confirmed FK relationship:\n\n```sql\n-- Comprehensive referential integrity check\nWITH integrity_check AS (\n  SELECT\n    COUNT(*) as total_child_records,\n    COUNT(child_fk_column) as non_null_fk_count,\n    COUNT(*) - COUNT(child_fk_column) as null_fk_count,\n    SUM(CASE WHEN p.parent_pk_column IS NULL AND c.child_fk_column IS NOT NULL THEN 1 ELSE 0 END) as orphaned_count\n  FROM child_table c\n  LEFT JOIN parent_table p ON c.child_fk_column = p.parent_pk_column\n)\nSELECT\n  total_child_records,\n  non_null_fk_count,\n  null_fk_count,\n  ROUND(100.0 * null_fk_count / total_child_records, 2) as null_fk_pct,\n  orphaned_count,\n  ROUND(100.0 * orphaned_count / non_null_fk_count, 2) as orphaned_pct,\n  non_null_fk_count - orphaned_count as valid_fk_count,\n  ROUND(100.0 * (non_null_fk_count - orphaned_count) / non_null_fk_count, 2) as integrity_pct\nFROM integrity_check;\n```\n\n**Document:**\n- Total child records\n- NULL FK percentage (records with no parent reference)\n- Orphaned FK percentage (records referencing non-existent parent)\n- Valid FK percentage (clean referential integrity)\n\n### Assess Impact of Integrity Violations\n\n**Business impact depends on:**\n- How joins will be used (INNER vs LEFT)\n- Whether orphaned records are recent (data entry lag) or old (data quality issue)\n- Whether NULL FKs are expected (optional relationships)\n\n```sql\n-- Analyze orphaned records by recency\nSELECT\n  CASE\n    WHEN date_column >= date('now', '-7 days') THEN 'Last 7 days'\n    WHEN date_column >= date('now', '-30 days') THEN 'Last 30 days'\n    WHEN date_column >= date('now', '-90 days') THEN 'Last 90 days'\n    ELSE 'Older than 90 days'\n  END as recency,\n  COUNT(*) as orphaned_count\nFROM child_table c\nLEFT JOIN parent_table p ON c.child_fk_column = p.parent_pk_column\nWHERE p.parent_pk_column IS NULL\n  AND c.child_fk_column IS NOT NULL\n  AND c.date_column IS NOT NULL\nGROUP BY recency\nORDER BY MIN(c.date_column);\n```\n\n**Document:**\n- Whether orphans are recent (may resolve soon) or old (permanent issue)\n- Impact on analytical queries (e.g., \"10% of orders will be excluded in INNER JOIN to customers\")\n\n### Validate Composite Keys\n\nIf multiple columns together form a FK:\n\n```sql\n-- Check integrity for composite FK\nWITH composite_fk_values AS (\n  SELECT DISTINCT\n    child_table.fk_column1,\n    child_table.fk_column2\n  FROM child_table\n  WHERE child_table.fk_column1 IS NOT NULL\n    AND child_table.fk_column2 IS NOT NULL\n),\ncomposite_pk_values AS (\n  SELECT DISTINCT\n    parent_table.pk_column1,\n    parent_table.pk_column2\n  FROM parent_table\n)\nSELECT\n  COUNT(*) as total_composite_fk_values,\n  SUM(CASE WHEN pk.pk_column1 IS NULL THEN 1 ELSE 0 END) as orphaned_count\nFROM composite_fk_values fk\nLEFT JOIN composite_pk_values pk\n  ON fk.fk_column1 = pk.pk_column1\n  AND fk.fk_column2 = pk.pk_column2;\n```\n\n**Document:**\n- Composite key relationships identified\n- Integrity percentage for composite keys\n\n---\n\n## Phase 5: Relationship Documentation\n\n**Goal:** Create structured documentation of all discovered relationships for use in cleaning and analysis.\n\n### Create Relationship Catalog\n\nDocument each confirmed relationship:\n\n```markdown\n## Foreign Key Relationships\n\n### High Confidence Relationships (>95% integrity)\n\n#### orders.customer_id â†’ customers.id\n- **Relationship Type:** Many-to-one\n- **Child Table:** orders (1,523 rows)\n- **Parent Table:** customers (342 rows)\n- **Match Percentage:** 98.2%\n- **Cardinality:** Avg 4.5 orders per customer (min: 1, max: 47)\n- **NULL FKs:** 12 rows (0.8%)\n- **Orphaned FKs:** 15 rows (1.0%)\n- **Recommended Join:** LEFT JOIN (to preserve orphaned orders)\n- **Cleaning Action:** Investigate 15 orphaned orders, flag for review\n\n### Medium Confidence Relationships (80-95% integrity)\n\n#### products.category_id â†’ categories.id\n- **Relationship Type:** Many-to-one\n- **Child Table:** products (856 rows)\n- **Parent Table:** categories (24 rows)\n- **Match Percentage:** 87.3%\n- **Cardinality:** Avg 35.7 products per category (min: 2, max: 142)\n- **NULL FKs:** 89 rows (10.4%)\n- **Orphaned FKs:** 20 rows (2.4%)\n- **Recommended Join:** INNER JOIN (if categorized products only needed)\n- **Cleaning Action:** Exclude or recategorize 20 orphaned products\n\n### Low Confidence / Unconfirmed (<80% integrity)\n\n#### transactions.merchant_id â†’ merchants.id\n- **Relationship Type:** Uncertain\n- **Match Percentage:** 67.8%\n- **Issue:** Large number of orphaned merchant_id values\n- **Recommendation:** Verify with data owner - may be wrong parent table\n```\n\n### Create Join Recommendations\n\nFor each relationship:\n\n```markdown\n## Join Recommendations\n\n### orders âŸ¶ customers\n\n**Recommended SQL:**\n```sql\n-- Use LEFT JOIN to preserve all orders (including orphans)\nSELECT\n  o.*,\n  c.customer_name,\n  c.customer_segment\nFROM orders o\nLEFT JOIN customers c ON o.customer_id = c.id;\n\n-- Alternative: INNER JOIN if orphans should be excluded\nSELECT\n  o.*,\n  c.customer_name\nFROM orders o\nINNER JOIN customers c ON o.customer_id = c.id;\n-- Note: Excludes 15 orders (1.0%) with invalid customer_id\n```\n\n**Join Impact:**\n- LEFT JOIN: Preserves all 1,523 orders (15 will have NULL customer fields)\n- INNER JOIN: Returns 1,508 orders (99.0% of total)\n- Recommendation: Use LEFT JOIN, filter nulls in WHERE clause if needed\n```\n\n### Document Data Quality Implications\n\n```markdown\n## Data Quality Implications\n\n### Orphaned Records Summary\n\nTotal orphaned records across all relationships: 35 (2.1% of all child records)\n\n| Child Table | FK Column | Orphan Count | % of Child Table | Impact |\n|-------------|-----------|--------------|------------------|--------|\n| orders | customer_id | 15 | 1.0% | Low - recent orders, may resolve |\n| products | category_id | 20 | 2.4% | Medium - affects category analysis |\n\n### Recommended Cleaning Actions\n\n**High Priority:**\n1. products.category_id orphans (20 rows) - CREATE placeholder category \"Uncategorized\" or exclude from analysis\n2. orders.customer_id orphans (15 rows) - FLAG for customer service review\n\n**Medium Priority:**\n3. NULL customer_id in orders (12 rows) - Investigate if legitimate (guest checkout?) or data entry error\n\n### Analysis Limitations\n\nDue to referential integrity issues:\n- Customer-level aggregations will exclude 1.0% of orders (if using INNER JOIN)\n- Category-level product analysis may be incomplete (2.4% of products uncategorized)\n- Time-series trends should use LEFT JOIN to preserve all records\n```\n\n---\n\n## Integration with Other Skills\n\n### With `importing-data` (Phase 5: Quality Assessment)\n\nAfter importing tables, run FK detection to include in quality report:\n\n```markdown\n## Foreign Key Relationships (from detect-foreign-keys skill)\n\nHigh Confidence:\n- orders.customer_id â†’ customers.id (98% integrity, 15 orphans)\n- ...\n\nMedium Confidence:\n- products.category_id â†’ categories.id (87% integrity, 20 orphans)\n```\n\n### With `cleaning-data` (Phase 1: Scope Definition)\n\nUse FK findings to inform cleaning scope:\n\n```markdown\n## Referential Integrity Issues\n\nFrom detect-foreign-keys analysis:\n- **orders.customer_id:** 15 orphaned records (1.0%) - Priority: HIGH\n  - Recommended action: Flag for review, preserve with LEFT JOIN\n```\n\n### With `understanding-data` (Phase 4: Relationship Identification)\n\nThis skill provides the systematic process for Phase 4:\n\n```markdown\n## Phase 4: Relationship Identification\n\nUse the `detect-foreign-keys` component skill to systematically identify and validate all foreign key relationships.\n```\n\n---\n\n## Common Pitfalls\n\n**DON'T:**\n- Assume naming conventions are always correct (validate with value overlap)\n- Skip Phase 4 integrity validation - orphaned records break analyses\n- Use INNER JOIN without understanding orphan impact\n- Ignore NULL FKs - they may be legitimate or data quality issues\n\n**DO:**\n- Validate every candidate FK with value overlap analysis (Phase 2)\n- Quantify integrity violations with exact counts and percentages\n- Document both high-confidence and uncertain relationships\n- Provide join recommendations based on integrity findings\n- Feed FK findings back into cleaning-data scope\n\n---\n\n## When to Re-Run\n\nRe-run this skill when:\n- New tables are added to the database\n- Referential integrity violations are suspected\n- Planning complex multi-table analyses\n- Cleaning activities might have affected FK relationships\n- Data loads introduce new orphaned records\n\n---\n\n## Success Criteria\n\nAfter completing this skill, you should have:\n- âœ… Complete catalog of FK relationships with confidence levels\n- âœ… Integrity percentages for each relationship\n- âœ… Count and examples of orphaned records\n- âœ… Cardinality assessment (one-to-one, one-to-many, many-to-many)\n- âœ… Join recommendations (LEFT vs INNER, filters needed)\n- âœ… Data quality implications documented\n- âœ… Cleaning actions prioritized\n\nThis documentation feeds into importing-data quality reports and cleaning-data scope definitions, ensuring relationship-aware data quality management.\n",
        "plugins/datapeeker/skills/exploratory-analysis/SKILL.md": "---\nname: exploratory-analysis\ndescription: Systematic exploratory data analysis process - discover patterns in unfamiliar data, identify meaningful insights, formulate specific questions for deeper investigation\n---\n\n# Exploratory Analysis Process\n\n## Overview\n\nThis skill guides you through systematic exploration of unfamiliar datasets when you don't yet have a specific question to answer. Unlike hypothesis-testing (where you test a specific claim) or guided-investigation (where you answer a specific question), exploratory analysis helps you **discover** what's interesting in the data and **identify** what questions you should be asking.\n\nExploratory analysis is appropriate when:\n- You have a new dataset and need to understand what's in it\n- The user says \"Just see what's interesting\" or \"Tell me what stands out\"\n- You need to discover patterns before formulating specific questions\n- You're looking for unexpected insights or anomalies\n- You want to generate hypotheses for later testing\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have data imported into SQLite database using the `importing-data` skill\n2. Have data quality validated and cleaned using the `cleaning-data` skill (MANDATORY - never skip)\n3. Have created an analysis workspace (`just start-analysis exploratory-analysis <name>`)\n4. Have NO preconceived hypotheses or specific questions (if you do, use hypothesis-testing or guided-investigation instead)\n5. Be familiar with the component skills:\n   - `understanding-data` - for data profiling\n   - `writing-queries` - for SQL query construction\n   - `interpreting-results` - for result analysis\n   - `creating-visualizations` - for text-based visualizations\n\n## Mandatory Process Structure\n\nYou MUST use TodoWrite to track progress through all 5 phases. Create todos at the start:\n\n```markdown\n- Phase 1: Data Familiarization - pending\n- Phase 2: Pattern Discovery - pending\n- Phase 3: Anomaly Investigation - pending\n- Phase 4: Insight Generation - pending\n- Phase 5: Question Formulation - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n---\n\n## Phase 1: Data Familiarization\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Catalogued all tables and their apparent purposes\n- [ ] Documented schema (columns, types, relationships)\n- [ ] Assessed data quality (completeness, ranges, distributions)\n- [ ] Identified temporal coverage and granularity\n- [ ] Saved to `01 - data-familiarization.md`\n\n### Instructions\n\n1. **Understand what data you have**\n\nCreate `analysis/[session-name]/01 - data-familiarization.md` with: ./templates/01-data-familiarization.md\n\n2. **Use the understanding-data component skill**\n   - Profile the database systematically\n   - Don't skip quality checks - surprises are common\n   - Look for obvious data issues that would affect exploration\n\n3. **Resist premature pattern-hunting**\n   - Don't start with \"I wonder if weekends are different\"\n   - First understand WHAT data you have, THEN explore patterns\n   - If you catch yourself forming hypotheses, note them for Phase 2 but finish familiarization first\n\n**Common Rationalization:** \"I can see the tables, I don't need detailed familiarization\"\n**Reality:** Skipping familiarization leads to missing important context about data quality, coverage, and structure.\n\n**Common Rationalization:** \"I'll explore patterns while I familiarize myself with the data\"\n**Reality:** Mixing familiarization and pattern-hunting creates confusion. Separate concerns.\n\n---\n\n## Phase 2: Pattern Discovery\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Explored temporal patterns (trends, seasonality, cycles)\n- [ ] Explored segmentation patterns (groups, categories, clusters)\n- [ ] Explored relationship patterns (correlations, associations)\n- [ ] Documented each exploration with rationale, query, results, observations\n- [ ] Created separate files for each exploration vector\n- [ ] Files saved as `02-temporal-patterns.md`, `03-segmentation-patterns.md`, `04-relationship-patterns.md`\n\n### Instructions\n\n1. **Explore SYSTEMATICALLY along three vectors**\n\nYou MUST explore all three vectors, even if some seem less promising. Surprises often come from unexpected places.\n\n#### Vector 1: Temporal Patterns\n\nCreate `analysis/[session-name]/02-temporal-patterns.md` with: ./templates/02-temporal-patterns.md\n\n#### Vector 2: Segmentation Patterns\n\nCreate `analysis/[session-name]/03-segmentation-patterns.md` with: ./templates/03-segmentation-patterns.md\n\n#### Vector 3: Relationship Patterns\n\nCreate `analysis/[session-name]/04-relationship-patterns.md` with: ./templates/04-relationship-patterns.md\n\n2. **Be systematic, not random**\n   - Explore ALL three vectors (temporal, segmentation, relationship)\n   - Don't skip a vector just because first query seems uninteresting\n   - Patterns often hide in places you don't expect\n\n3. **Separate observation from interpretation**\n   - In each analysis, state FACTS (what the numbers show)\n   - Save interpretation for Phase 4 (Insight Generation)\n   - Resist the urge to explain patterns yet - just catalog them\n\n4. **Use visualizations liberally**\n   - Text-based tables, ASCII charts, markdown formatting\n   - Help patterns become visible\n   - Refer to `creating-visualizations` component skill\n\n**Common Rationalization:** \"I found an interesting pattern, I'll just focus on that\"\n**Reality:** Focusing on first interesting pattern causes you to miss better patterns elsewhere. Complete all three vectors.\n\n**Common Rationalization:** \"This dimension looks boring, I'll skip it\"\n**Reality:** \"Boring\" dimensions often hide surprising patterns. Explore systematically.\n\n**Common Rationalization:** \"I'll combine all patterns into one analysis file\"\n**Reality:** Separate files by vector creates structure and makes findings easy to locate.\n\n---\n\n## Phase 3: Anomaly Investigation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Identified 3-5 specific anomalies, outliers, or unexpected patterns from Phase 2\n- [ ] Investigated each anomaly with follow-up queries\n- [ ] Determined if anomalies are data quality issues or real phenomena\n- [ ] Documented findings\n- [ ] Saved to `05 - anomaly-investigation.md`\n\n### Instructions\n\n1. **Review Phase 2 for anomalies**\n\nGo back through temporal, segmentation, and relationship explorations and identify:\n- Unexpected spikes or drops\n- Outlier values or segments\n- Counterintuitive patterns\n- Violations of expected business logic\n- Inconsistencies or irregularities\n\n2. **Investigate each anomaly**\n\nCreate `analysis/[session-name]/05 - anomaly-investigation.md` with: ./templates/05-anomaly-investigation.md\n\n3. **Distinguish data quality from real patterns**\n   - NULL values, duplicates, data entry errors â†’ data quality issues\n   - Seasonal events, one-time promotions, external shocks â†’ real phenomena\n   - When unclear, state that explicitly\n\n4. **Don't ignore anomalies**\n   - Anomalies are often where the most interesting insights hide\n   - Investigate thoroughly, don't dismiss as \"probably nothing\"\n   - Data quality issues found now save headaches later\n\n**Common Rationalization:** \"That spike is probably a holiday, I'll ignore it\"\n**Reality:** VERIFY your assumption. \"Probably\" isn't good enough. Check.\n\n**Common Rationalization:** \"This anomaly is a data quality issue, I'll just exclude it\"\n**Reality:** Document what you're excluding and why. Future analysts need to know.\n\n**Common Rationalization:** \"I found 20 anomalies, I'll investigate them all\"\n**Reality:** Focus on the 3-5 most significant. You're exploring, not auditing every data point.\n\n---\n\n## Phase 4: Insight Generation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Reviewed all patterns and anomalies from Phases 2-3\n- [ ] Identified 3-5 insights that are actionable, surprising, or meaningful\n- [ ] For each insight, documented: finding, significance, context, caveats\n- [ ] Assessed confidence level for each insight\n- [ ] Saved to `06 - insights.md`\n\n### Instructions\n\n1. **Extract insights from patterns**\n\nNot every pattern is an insight. An insight must be:\n- **Actionable:** Suggests a decision or further investigation\n- **Surprising:** Non-obvious, counter to expectations, or revealing hidden structure\n- **Meaningful:** Materially significant magnitude or impact\n\n2. **Document insights systematically**\n\nCreate `analysis/[session-name]/06 - insights.md` with: ./templates/06-insights.md\n\n3. **Be selective**\n   - Don't try to turn every pattern into an insight\n   - 3-5 high-quality insights beat 20 marginal observations\n   - If you have more than 5, rank them and focus on top 5\n\n4. **Quantify magnitude**\n   - \"Sales vary by region\" is not enough\n   - \"Top region has 3x sales of bottom region\" is specific\n   - Numbers make insights actionable\n\n5. **Assess confidence honestly**\n   - High confidence: Strong evidence, large sample, consistent pattern, clear interpretation\n   - Medium confidence: Clear pattern but causation unclear OR limited time window OR potential confounds\n   - Low confidence: Interesting but small sample OR inconsistent OR multiple plausible explanations\n\n**Common Rationalization:** \"Every pattern I found is an insight\"\n**Reality:** Most patterns are noise or expected. Be selective. Insights must clear the bar: actionable, surprising, meaningful.\n\n**Common Rationalization:** \"I'll just state the pattern and let the user figure out if it matters\"\n**Reality:** Your job is to interpret. Explain WHY the pattern matters and WHAT it suggests.\n\n**Common Rationalization:** \"I'm very confident in this insight\"\n**Reality:** Exploratory analysis rarely produces high confidence. Be honest about uncertainty and limitations.\n\n---\n\n## Phase 5: Question Formulation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Generated 3-5 specific, answerable questions based on insights\n- [ ] For each question, identified: what process to use, what data is needed, why it matters\n- [ ] Prioritized questions by potential value\n- [ ] Saved to `07 - next-questions.md`\n- [ ] Updated `00 - overview.md` with exploration summary\n\n### Instructions\n\n1. **Convert insights into questions**\n\nEach insight should suggest 1-2 follow-up questions. Good questions:\n- Build on what you learned in exploration\n- Are specific enough to be answerable\n- Have clear business value\n- Can be addressed with available (or obtainable) data\n\n2. **Document questions for follow-up**\n\nCreate `analysis/[session-name]/07 - next-questions.md` with: ./templates/07-next-questions.md\n\n3. **Update exploration overview**\n\nUpdate `analysis/[session-name]/00 - overview.md` by adding content from: ./templates/overview-summary.md\n\n4. **Final verification checklist**\n   - [ ] Explored all three vectors (temporal, segmentation, relationship)\n   - [ ] Investigated significant anomalies\n   - [ ] Generated 3-5 qualified insights (actionable, surprising, meaningful)\n   - [ ] Formulated specific follow-up questions\n   - [ ] Prioritized questions by value\n   - [ ] Updated overview with summary\n   - [ ] Communicated findings to user\n\n**Common Rationalization:** \"I found interesting patterns, I'm done\"\n**Reality:** Exploration isn't complete until you've formulated what to investigate next. Always end with questions.\n\n**Common Rationalization:** \"I'll just suggest broad areas to explore further\"\n**Reality:** Be specific. \"Investigate customer behavior\" is not actionable. \"Compare weekend vs weekday sales per operating hour using comparative-analysis skill\" is actionable.\n\n**Common Rationalization:** \"I'll list every possible question I can think of\"\n**Reality:** Focus on the 3-5 highest-value questions. Too many options create decision paralysis.\n\n---\n\n## Common Rationalizations\n\n### \"I can see interesting patterns already, I'll skip data familiarization\"\n**Why this is wrong:** Without understanding data quality, coverage, and structure, your pattern discoveries may be artifacts or noise.\n\n**Do instead:** Complete Phase 1 fully. Familiarization prevents false discoveries and wasted effort.\n\n### \"This exploration vector looks boring, I'll skip it\"\n**Why this is wrong:** The most surprising insights often come from places you didn't expect. \"Boring\" dimensions frequently hide interesting patterns.\n\n**Do instead:** Explore ALL three vectors (temporal, segmentation, relationship) systematically. Be comprehensive.\n\n### \"I found one great insight, that's enough\"\n**Why this is wrong:** One insight doesn't exhaust a dataset. You likely missed other valuable patterns.\n\n**Do instead:** Continue systematic exploration. Aim for 3-5 insights across different dimensions.\n\n### \"Every pattern I found is an insight\"\n**Why this is wrong:** Most patterns are noise, expected, or immaterial. Calling everything an insight dilutes the valuable discoveries.\n\n**Do instead:** Apply strict criteria: actionable, surprising, meaningful. Be selective.\n\n### \"I'll just describe patterns and let the user decide if they're important\"\n**Why this is wrong:** Your job is interpretation, not just data reporting. Users expect you to identify what matters and why.\n\n**Do instead:** Assess significance, provide context, explain business implications. Do the analytical thinking.\n\n### \"I'm very confident in these exploratory findings\"\n**Why this is wrong:** Exploratory analysis generates hypotheses, not confirmations. Patterns need validation through targeted investigation.\n\n**Do instead:** Be honest about confidence levels. Exploratory findings are typically medium-low confidence until validated.\n\n### \"I found interesting patterns, analysis is complete\"\n**Why this is wrong:** Exploration is the beginning, not the end. The goal is to identify what to investigate deeply.\n\n**Do instead:** Always complete Phase 5. Convert insights into specific, answerable questions for follow-up.\n\n### \"I'll combine all patterns into one big report\"\n**Why this is wrong:** Mixing temporal, segmentation, and relationship analyses creates confusion and makes findings hard to locate.\n\n**Do instead:** Separate files by exploration vector (02-temporal, 03-segmentation, 04-relationship). Clear structure aids comprehension.\n\n### \"This anomaly is probably a data error, I'll just exclude it\"\n**Why this is wrong:** Assumptions about anomalies are often wrong. \"Probably\" isn't good enough. Also, excluding data without documentation creates reproducibility issues.\n\n**Do instead:** Investigate anomalies in Phase 3. Document what you exclude and why. Verify your assumptions.\n\n### \"I have 15 follow-up questions, I'll list them all\"\n**Why this is wrong:** Too many options create decision paralysis. Not all questions are equally valuable.\n\n**Do instead:** Prioritize ruthlessly. Focus on 3-5 highest-value questions. Help the user know where to start.\n\n---\n\n## Summary\n\nThis skill ensures systematic, thorough exploration of unfamiliar datasets by:\n\n1. **Familiarizing with data first:** Understand structure, quality, and coverage before pattern-hunting\n2. **Exploring systematically:** Three vectors (temporal, segmentation, relationship) ensure comprehensive discovery\n3. **Investigating anomalies:** Surprises and outliers often contain the most valuable insights\n4. **Generating selective insights:** Apply strict criteria (actionable, surprising, meaningful) to separate signal from noise\n5. **Formulating specific questions:** Convert insights into answerable questions for deeper investigation\n6. **Prioritizing next steps:** Help users know what to investigate first and why\n\nFollow this process and you'll discover what's truly interesting in unfamiliar data, avoid random pattern-chasing, and identify high-value questions for targeted investigation.\n",
        "plugins/datapeeker/skills/exploratory-analysis/templates/01-data-familiarization.md": "# Data Familiarization\n\n## Exploration Context\n\n**Dataset:** [Name or description]\n**Source:** [Where data came from, if known]\n**Exploration Goal:** [User's request, e.g., \"Find interesting patterns in sales data\"]\n\n## Tables Overview\n\n[List all tables with row counts and apparent purposes]\n\n### Table: [table_name]\n**Row count:** [count]\n**Apparent purpose:** [What this table seems to represent]\n**Grain:** [What one row represents, e.g., \"one transaction\", \"one customer\", \"one daily summary\"]\n\nExample:\n### Table: sales\n**Row count:** 15,847\n**Apparent purpose:** Transaction-level sales records\n**Grain:** One row per individual sale transaction\n\n### Table: [table_name_2]\n...\n\n## Schema Details\n\n[For each table, document key columns]\n\n### Table: sales\n```sql\nPRAGMA table_info(sales);\n```\n\n**Key columns identified:**\n- `transaction_date` (TEXT) - when sale occurred\n- `amount` (REAL) - sale value in currency\n- `product_id` (INTEGER) - product identifier\n- `customer_id` (INTEGER) - customer identifier\n- `region` (TEXT) - geographic region\n\n**Potential relationships:**\n- `product_id` likely joins to products table\n- `customer_id` likely joins to customers table\n\n**Temporal coverage:**\n- Earliest date: [result from MIN(transaction_date)]\n- Latest date: [result from MAX(transaction_date)]\n- Coverage: [X months/years of data]\n\n## Data Quality Assessment\n\n### Completeness\n\n```sql\n-- Check for NULL values in key columns\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(transaction_date) as non_null_dates,\n  COUNT(amount) as non_null_amounts,\n  COUNT(product_id) as non_null_products\nFROM sales;\n```\n\n**Results:** [Paste results]\n\n**Assessment:**\n- [Column]: [% complete], [any concerns]\n- [Column]: [% complete], [any concerns]\n\n### Value Distributions\n\n```sql\n-- Check numeric column ranges\nSELECT\n  MIN(amount) as min_amount,\n  MAX(amount) as max_amount,\n  ROUND(AVG(amount), 2) as avg_amount,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  COUNT(DISTINCT product_id) as unique_products\nFROM sales;\n```\n\n**Results:** [Paste results]\n\n**Observations:**\n- Amount range: $[min] to $[max] (average: $[avg])\n- [Number] unique customers\n- [Number] unique products\n- [Any outliers or surprises]\n\n### Temporal Distribution\n\n```sql\n-- Check temporal distribution\nSELECT\n  STRFTIME('%Y-%m', transaction_date) as month,\n  COUNT(*) as transaction_count\nFROM sales\nGROUP BY month\nORDER BY month;\n```\n\n**Results:** [Paste results]\n\n**Observations:**\n- [Gaps in data?]\n- [Seasonality visible?]\n- [Growing or shrinking over time?]\n- [Complete weeks/months or partial?]\n\n## Initial Impressions\n\n[Before exploring patterns, note your initial observations]\n\n- Dataset covers [timeframe]\n- [Number] transactions across [number] products/customers/regions\n- Data quality appears [good/fair/poor] because [reasons]\n- Potential areas of interest:\n  1. [Something that caught your attention]\n  2. [Another interesting aspect]\n  3. [Third area to explore]\n\n## Exploration Strategy\n\n[Based on familiarization, what vectors will you explore?]\n\nWill explore:\n1. **Time-based patterns:** [What temporal analyses make sense?]\n2. **Segmentation patterns:** [What groups/categories to compare?]\n3. **Relationship patterns:** [What correlations or associations to check?]\n",
        "plugins/datapeeker/skills/exploratory-analysis/templates/02-temporal-patterns.md": "# Temporal Pattern Exploration\n\n## Objective\n\nDiscover how the data varies over time - trends, seasonality, cycles, or irregular patterns.\n\n## Exploration Approach\n\n[Explain what temporal analyses you'll perform]\n\n1. Overall trend (is metric growing/shrinking/stable?)\n2. Seasonal patterns (day-of-week, month-of-year, etc.)\n3. Period-over-period comparisons (YoY, MoM, etc.)\n4. Irregular events or anomalies in time series\n\n---\n\n## Analysis 1: Overall Trend\n\n### Rationale\n[Why checking overall trend first]\n\nExample: \"Establish baseline understanding of whether sales are growing, declining, or stable over the dataset period\"\n\n### Query\n```sql\n-- Monthly aggregation to see trend\nSELECT\n  STRFTIME('%Y-%m', transaction_date) as month,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_sales,\n  ROUND(AVG(amount), 2) as avg_transaction,\n  COUNT(DISTINCT customer_id) as unique_customers\nFROM sales\nGROUP BY month\nORDER BY month;\n```\n\n### Results\n[Paste actual results]\n\n### Observations\n[What do you see? Facts only]\n\n- First month: [stats]\n- Last month: [stats]\n- Overall direction: [growing/declining/stable/erratic]\n- Growth rate: [approximate % change]\n- Volatility: [stable or highly variable month-to-month]\n\n### Text Visualization\n[Use creating-visualizations skill to make a simple ASCII chart if helpful]\n\n```\nMonth         Sales ($)\n2024-01       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45,890\n2024-02       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 38,234\n2024-03       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 52,145\n```\n\n---\n\n## Analysis 2: Day-of-Week Patterns\n\n### Rationale\n[Why checking day-of-week]\n\n### Query\n```sql\nSELECT\n  CASE CAST(STRFTIME('%w', transaction_date) AS INTEGER)\n    WHEN 0 THEN 'Sunday'\n    WHEN 1 THEN 'Monday'\n    WHEN 2 THEN 'Tuesday'\n    WHEN 3 THEN 'Wednesday'\n    WHEN 4 THEN 'Thursday'\n    WHEN 5 THEN 'Friday'\n    WHEN 6 THEN 'Saturday'\n  END as day_name,\n  CAST(STRFTIME('%w', transaction_date) AS INTEGER) as day_num,\n  COUNT(*) as transaction_count,\n  ROUND(AVG(amount), 2) as avg_amount,\n  ROUND(SUM(amount), 2) as total_sales\nFROM sales\nGROUP BY day_num, day_name\nORDER BY day_num;\n```\n\n### Results\n[Paste results]\n\n### Observations\n[Describe pattern - which days are high/low, by how much]\n\n---\n\n## Analysis 3: [Other temporal pattern]\n\n[Repeat structure for month-of-year, hour-of-day if timestamps available, etc.]\n\n---\n\n## Temporal Patterns Summary\n\n### Key Findings\n1. **[Pattern 1]:** [Description with magnitude]\n2. **[Pattern 2]:** [Description with magnitude]\n3. **[Pattern 3]:** [Description with magnitude]\n\n### Interesting Anomalies\n- [Any unusual spikes or drops worth investigating]\n\n### Questions Raised\n- [What questions does this create for later investigation?]\n",
        "plugins/datapeeker/skills/exploratory-analysis/templates/03-segmentation-patterns.md": "# Segmentation Pattern Exploration\n\n## Objective\n\nDiscover how the data varies across natural groups - categories, segments, cohorts, or clusters.\n\n## Exploration Approach\n\n[Explain what segmentation dimensions you'll explore]\n\n1. Categorical dimensions (region, product type, customer tier, etc.)\n2. Derived segments (high/medium/low value, new/returning, etc.)\n3. Cross-segments (region Ã— product, etc.)\n\n---\n\n## Analysis 1: [Dimension] Distribution\n\n### Rationale\n[Why exploring this dimension]\n\nExample: \"Understand if sales are concentrated in certain regions or distributed evenly\"\n\n### Query\n```sql\nSELECT\n  region,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_sales,\n  ROUND(AVG(amount), 2) as avg_transaction,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  -- Calculate percentage of total\n  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM sales), 1) as pct_of_transactions\nFROM sales\nWHERE region IS NOT NULL\nGROUP BY region\nORDER BY total_sales DESC;\n```\n\n### Results\n[Paste results]\n\n### Observations\n- [Top segment]: [stats] ([X]% of total)\n- [Bottom segment]: [stats] ([X]% of total)\n- Concentration: [How concentrated or distributed?]\n- Differences: [Magnitude of differences between segments]\n\n---\n\n## Analysis 2: [Another dimension]\n\n[Repeat structure for products, customer segments, etc.]\n\n---\n\n## Analysis 3: Cross-Segmentation\n\n### Rationale\n[Why looking at interaction of two dimensions]\n\nExample: \"Check if product preferences vary by region\"\n\n### Query\n```sql\nSELECT\n  region,\n  product_category,\n  COUNT(*) as transaction_count,\n  ROUND(SUM(amount), 2) as total_sales\nFROM sales\n  JOIN products ON sales.product_id = products.id\nWHERE region IS NOT NULL AND product_category IS NOT NULL\nGROUP BY region, product_category\nORDER BY region, total_sales DESC;\n```\n\n### Results\n[Paste results]\n\n### Observations\n- [Pattern across segments]\n- [Interesting variations]\n\n---\n\n## Segmentation Patterns Summary\n\n### Key Findings\n1. **[Finding 1]:** [Description]\n2. **[Finding 2]:** [Description]\n\n### Notable Differences\n- [Biggest differences between segments]\n\n### Questions Raised\n- [What questions does this create?]\n",
        "plugins/datapeeker/skills/exploratory-analysis/templates/04-relationship-patterns.md": "# Relationship Pattern Exploration\n\n## Objective\n\nDiscover correlations, associations, or dependencies between variables.\n\n## Exploration Approach\n\n[Explain what relationships you'll explore]\n\n1. Metric correlations (does X relate to Y?)\n2. Sequential patterns (does event A precede event B?)\n3. Cohort behaviors (do groups behave differently over time?)\n\n---\n\n## Analysis 1: [Relationship description]\n\n### Rationale\n[Why exploring this relationship]\n\nExample: \"Check if transaction size correlates with customer tenure\"\n\n### Query\n```sql\n-- Create tenure buckets and compare average transaction size\nSELECT\n  CASE\n    WHEN tenure_days < 30 THEN '0-30 days'\n    WHEN tenure_days < 90 THEN '30-90 days'\n    WHEN tenure_days < 180 THEN '90-180 days'\n    WHEN tenure_days < 365 THEN '180-365 days'\n    ELSE '1+ years'\n  END as tenure_bucket,\n  COUNT(*) as transaction_count,\n  ROUND(AVG(amount), 2) as avg_amount,\n  COUNT(DISTINCT customer_id) as unique_customers\nFROM (\n  SELECT\n    s.*,\n    JULIANDAY(s.transaction_date) - JULIANDAY(c.signup_date) as tenure_days\n  FROM sales s\n  JOIN customers c ON s.customer_id = c.id\n)\nGROUP BY tenure_bucket\nORDER BY MIN(tenure_days);\n```\n\n### Results\n[Paste results]\n\n### Observations\n- [Pattern description]\n- [Magnitude of correlation if present]\n- [Direction: positive/negative/no relationship]\n\n---\n\n## Analysis 2: [Another relationship]\n\n[Repeat structure]\n\n---\n\n## Relationship Patterns Summary\n\n### Key Findings\n1. **[Finding 1]:** [Description]\n2. **[Finding 2]:** [Description]\n\n### Correlations Identified\n- [List any meaningful correlations with magnitude]\n\n### Questions Raised\n- [What questions does this create?]\n",
        "plugins/datapeeker/skills/exploratory-analysis/templates/05-anomaly-investigation.md": "# Anomaly Investigation\n\n## Anomalies Identified\n\n[List anomalies found in Phase 2]\n\n1. **[Anomaly description]** - found in [which analysis]\n2. **[Anomaly description]** - found in [which analysis]\n3. **[Anomaly description]** - found in [which analysis]\n\n---\n\n## Anomaly 1: [Description]\n\n### Where Found\n[Reference to Phase 2 analysis where this was spotted]\n\nExample: \"In temporal analysis (02-temporal-patterns.md), noticed sales spike of 300% on 2024-02-14\"\n\n### Why It's Anomalous\n[What makes this unusual or unexpected]\n\nExample: \"February 14 sales were $45,890, compared to typical daily average of $15,200 - a 3x spike\"\n\n### Investigation Query\n\n```sql\n-- Drill into the anomalous period\nSELECT\n  transaction_date,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_sales,\n  ROUND(AVG(amount), 2) as avg_transaction,\n  MIN(amount) as min_amount,\n  MAX(amount) as max_amount,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  COUNT(DISTINCT product_id) as unique_products\nFROM sales\nWHERE transaction_date BETWEEN '2024-02-13' AND '2024-02-15'\nGROUP BY transaction_date\nORDER BY transaction_date;\n```\n\n### Results\n[Paste results]\n\n### Deeper Investigation\n\n[If needed, more queries to understand the anomaly]\n\n```sql\n-- Check if specific products drove the spike\nSELECT\n  p.product_name,\n  p.category,\n  COUNT(*) as transaction_count,\n  SUM(s.amount) as total_sales\nFROM sales s\nJOIN products p ON s.product_id = p.id\nWHERE s.transaction_date = '2024-02-14'\nGROUP BY p.product_name, p.category\nORDER BY total_sales DESC\nLIMIT 10;\n```\n\n### Results\n[Paste results]\n\n### Explanation\n\n**Determination:** [Data quality issue / Real phenomenon / Unclear]\n\n**Reasoning:** [Explain what caused this anomaly]\n\nExample: \"Real phenomenon - Valentine's Day (Feb 14) drove 3x spike in flower and chocolate sales. Pattern is legitimate seasonal effect, not data error.\"\n\n**Action:** [Should this be excluded from other analyses? Noted as special case? Investigated further?]\n\n---\n\n## Anomaly 2: [Next anomaly]\n\n[Repeat structure]\n\n---\n\n## Anomalies Summary\n\n### Real Phenomena\n[List anomalies that are legitimate patterns worthy of insight]\n\n1. **[Anomaly]:** [Brief explanation]\n2. **[Anomaly]:** [Brief explanation]\n\n### Data Quality Issues\n[List anomalies that are data errors or artifacts]\n\n1. **[Issue]:** [What's wrong and how it affects analysis]\n\n### Unexplained\n[List anomalies you couldn't fully explain]\n\n1. **[Anomaly]:** [What remains unclear]\n\n### Implications for Pattern Discovery\n\n[How do these anomalies affect your Phase 2 findings?]\n\n- [Anomaly] explains [pattern from Phase 2]\n- [Data issue] invalidates [finding from Phase 2]\n- [Real phenomenon] is interesting enough to become a standalone insight\n",
        "plugins/datapeeker/skills/exploratory-analysis/templates/06-insights.md": "# Insights\n\n## Insight Criteria\n\nFor this exploration, an insight must be:\n- **Actionable:** Informs a decision or next investigation\n- **Surprising:** Non-obvious or counter-intuitive\n- **Meaningful:** Magnitude is significant enough to matter\n\n---\n\n## Insight 1: [Concise insight statement]\n\n### The Finding\n\n[Describe the pattern or relationship in specific terms]\n\nExample: \"Weekend sales are 40% lower than weekday sales, driven entirely by transaction volume (not ticket size)\"\n\n### Why It's Significant\n\n[Explain why this matters - business impact, decision relevance, strategic importance]\n\nExample: \"If pattern is due to operational constraints (reduced hours), there's significant untapped revenue potential. If due to customer behavior, staffing/inventory strategies should shift toward weekdays.\"\n\n### Supporting Evidence\n\n[Reference specific analyses from Phases 2-3]\n\n- Found in: [file reference]\n- Key data points:\n  - [Specific metric 1]\n  - [Specific metric 2]\n- Magnitude: [Quantify the effect]\n- Consistency: [Is pattern consistent across time/segments?]\n\n### Context and Interpretation\n\n[Provide business context, possible explanations]\n\nExample: \"Could be explained by:\n1. Store hours (reduced weekend hours)\n2. Customer behavior (fewer weekend shoppers)\n3. Product availability (limited weekend inventory)\n\nNeed operational data to distinguish these explanations.\"\n\n### Caveats and Limitations\n\n[What reduces confidence or limits applicability?]\n\nExample:\n- Based on 3 months of data (Jan-Mar); seasonal effects unknown\n- Cannot distinguish operational constraints from demand patterns\n- No control for holidays or special events\n\n### Confidence Level\n\n**Confidence:** [High / Medium / Low]\n\n**Reasoning:** [Why this confidence level?]\n\nExample:\n**Confidence:** Medium\n\n**Reasoning:** Pattern is clear and consistent across 13 weeks (strengthens confidence), but cannot determine causation without operational data (weakens confidence). Sample size is adequate.\n\n### Recommended Action\n\n[What should be done with this insight?]\n\n- [ ] Further investigation: [What to investigate]\n- [ ] Business decision: [What decision this informs]\n- [ ] Data collection: [What additional data would help]\n\nExample:\n- [ ] Investigate store hours by day-of-week to test operational constraint hypothesis\n- [ ] Run A/B test: extend Sunday hours and measure sales per operating hour\n- [ ] Collect weekend vs weekday customer surveys to understand behavioral differences\n\n---\n\n## Insight 2: [Next insight]\n\n[Repeat structure]\n\n---\n\n## Insight 3: [Next insight]\n\n[Repeat structure]\n\n---\n\n## Non-Insights (Patterns That Didn't Qualify)\n\n[List patterns found that were interesting but didn't meet insight criteria]\n\n### Pattern: [Description]\n**Why not an insight:** [Too small magnitude / Not actionable / Expected/obvious]\n\nExample:\n### Pattern: Sales increased 2% from January to March\n**Why not an insight:** Magnitude too small to be meaningful (within normal variance); also could be seasonal effect of moving from winter to spring\n\n---\n\n## Insights Summary\n\n### Highest Priority Insights\n[Rank insights by importance/actionability]\n\n1. **[Insight]:** [One sentence summary]\n2. **[Insight]:** [One sentence summary]\n3. **[Insight]:** [One sentence summary]\n\n### Cross-Cutting Themes\n\n[Are there themes that connect multiple insights?]\n\nExample: \"Multiple insights point to a customer segmentation issue - mid-tier customers behaving very differently from high/low tiers across time, product, and geography dimensions.\"\n\n### Strategic Implications\n\n[What do these insights collectively suggest about strategy, operations, or priorities?]\n\nExample: \"Dataset reveals significant untapped potential in weekend/Sunday operations and mid-tier customer retention. Both represent high-impact opportunities for revenue growth.\"\n",
        "plugins/datapeeker/skills/exploratory-analysis/templates/07-next-questions.md": "# Questions for Further Investigation\n\n## Purpose\n\nExploratory analysis has identified patterns and insights. This document translates those insights into specific, answerable questions for deeper investigation.\n\n---\n\n## Question 1: [Specific question]\n\n### What We Learned in Exploration\n\n[Reference the insight that led to this question]\n\nExample: \"Exploration revealed that weekend sales are 40% lower than weekdays (Insight 1), but we cannot determine if this is due to operational constraints or customer behavior.\"\n\n### The Question\n\n[State the specific question to investigate]\n\nExample: \"Is the weekend sales decline caused by reduced operating hours, or do customers simply shop less on weekends when given equal opportunity?\"\n\n### Why It Matters\n\n[Explain business value of answering this question]\n\nExample: \"If caused by reduced hours, extending weekend hours could capture ~$50K additional monthly revenue. If caused by customer behavior, we should optimize staffing for weekday peaks and reduce weekend overhead.\"\n\n### Recommended Process\n\n**Process Skill:** [Which DataPeeker process would answer this?]\n\n- [ ] `hypothesis-testing` - if you have a specific claim to test\n- [ ] `guided-investigation` - if you need to systematically explore the question\n- [ ] `comparative-analysis` - if you need to compare specific segments\n\nExample: **`comparative-analysis`** - compare weekend vs weekday sales controlling for operating hours\n\n### Data Required\n\n[What data would you need to answer this?]\n\n- Current data: [What you already have]\n- Additional data needed: [What you'd need to collect]\n\nExample:\n- Current data: Transaction timestamps (if available - check if we have hour-of-day data)\n- Additional data needed: Store operating hours by day-of-week\n\n### Investigation Approach\n\n[Brief outline of how to investigate]\n\nExample:\n1. Extract hour-of-day from transaction timestamps\n2. Calculate sales per operating hour for weekend vs weekday\n3. Compare: if sales/hour are equal, it's an hours issue; if sales/hour are lower on weekends, it's a demand issue\n\n### Priority\n\n**Priority:** [High / Medium / Low]\n\n**Reasoning:** [Why this priority level?]\n\nExample:\n**Priority:** High\n\n**Reasoning:** Potential $50K/month revenue impact and requires minimal data collection (may already have timestamp data). Quick investigation with high-value outcome.\n\n---\n\n## Question 2: [Next question]\n\n[Repeat structure]\n\n---\n\n## Question 3: [Next question]\n\n[Repeat structure]\n\n---\n\n## Questions Summary\n\n### High Priority Questions\n[List high-priority questions in recommended investigation order]\n\n1. **[Question]** - Expected value: [impact], Estimated effort: [time/data required]\n2. **[Question]** - Expected value: [impact], Estimated effort: [time/data required]\n\n### Medium Priority Questions\n[Questions that would be valuable but less urgent]\n\n### Questions Deferred\n[Questions that can't be answered with available data or aren't worth the effort]\n\n**[Question]:** [Why deferred]\n\n### Investigation Roadmap\n\n[If someone wanted to investigate all high-priority questions, what order makes sense?]\n\nExample:\n1. Start with Q1 (weekend sales) - quick win, uses existing data\n2. Then Q3 (customer segmentation) - builds on Q1 findings\n3. Finally Q2 (product mix) - most complex, benefits from context from Q1 and Q3\n\n## Data Collection Priorities\n\n[What new data sources would unlock multiple questions?]\n\nExample: \"Adding customer segment identifier to transaction data would enable investigation of Q2, Q4, and Q5. High-value data enrichment.\"\n",
        "plugins/datapeeker/skills/exploratory-analysis/templates/overview-summary.md": "## Exploration Summary\n\n**Dataset:** [Name/description]\n\n**Time Period:** [Coverage]\n\n**Data Quality:** [Overall assessment]\n\n**Exploration Completed:** [Date]\n\n---\n\n## Key Insights Discovered\n\n1. **[Insight 1 title]:** [One sentence]\n   - Magnitude: [Quantify]\n   - Confidence: [High/Medium/Low]\n\n2. **[Insight 2 title]:** [One sentence]\n   - Magnitude: [Quantify]\n   - Confidence: [High/Medium/Low]\n\n3. **[Insight 3 title]:** [One sentence]\n   - Magnitude: [Quantify]\n   - Confidence: [High/Medium/Low]\n\n---\n\n## Recommended Next Steps\n\n**Highest Priority:**\n1. [Question] - investigate using [process skill]\n2. [Question] - investigate using [process skill]\n\n**Data Collection:**\n- [Data source to add] - would enable [questions]\n\n---\n\n## File Index\n\n- 01 - Data Familiarization\n- 02 - Temporal Patterns\n- 03 - Segmentation Patterns\n- 04 - Relationship Patterns\n- 05 - Anomaly Investigation\n- 06 - Insights\n- 07 - Next Questions\n",
        "plugins/datapeeker/skills/guided-investigation/SKILL.md": "---\nname: guided-investigation\ndescription: Systematic process for investigating open-ended questions - decompose vague questions into specific sub-questions, map to data, investigate incrementally, synthesize findings\n---\n\n# Guided Investigation Process\n\n## Overview\n\nThis skill guides you through systematic investigation of open-ended or exploratory questions. Unlike hypothesis testing (where you test a specific claim), guided investigation helps you answer questions like \"Why is X happening?\" or \"What's driving Y?\" by breaking them into specific sub-questions and investigating each systematically.\n\nGuided investigation is appropriate when:\n- You have a broad question without a specific hypothesis\n- You need to understand a complex phenomenon with multiple potential factors\n- The user says \"I want to understand...\" or \"What's causing...\"\n- You need to decompose a vague question into answerable parts\n- You're investigating a business problem with unclear root causes\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have data imported into SQLite database using the `importing-data` skill\n2. Have data quality validated and cleaned using the `cleaning-data` skill (MANDATORY - never skip)\n3. Have created an analysis workspace (`just start-analysis guided-investigation <name>`)\n4. Have a clear investigative goal from the user\n5. Be familiar with the component skills:\n   - `understanding-data` - for data profiling\n   - `writing-queries` - for SQL query construction\n   - `interpreting-results` - for result analysis\n   - `creating-visualizations` - for text-based visualizations\n\n## Mandatory Process Structure\n\nYou MUST use TodoWrite to track progress through all 5 phases. Create todos at the start:\n\n```markdown\n- Phase 1: Question Decomposition - pending\n- Phase 2: Data Discovery - pending\n- Phase 3: Systematic Investigation - pending\n- Phase 4: Synthesis - pending\n- Phase 5: Conclusions and Recommendations - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n---\n\n## Phase 1: Question Decomposition\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Clarified the user's broad question\n- [ ] Decomposed it into 3-5 specific, answerable sub-questions\n- [ ] Prioritized sub-questions by importance/dependency\n- [ ] Documented the investigative framework\n- [ ] Saved to `01 - question-decomposition.md`\n\n### Instructions\n\n1. **Ask clarifying questions** to understand the user's goal\n\n   - What's the core question you're trying to answer?\n   - What decision will this inform?\n   - What would constitute a satisfactory answer?\n   - What do you already know or suspect?\n   - What constraints exist (time, data, etc.)?\n\n2. **Decompose the broad question** into specific sub-questions\n\nCreate `analysis/[session-name]/01-question-decomposition.md` with: ./templates/phase-1-question-decomposition.md\n\n3. **STOP and get user confirmation**\n   - Review the sub-questions with the user\n   - Confirm they address the core question\n   - Adjust priorities if needed\n   - Do NOT proceed until user confirms the framework\n\n**Common Rationalization:** \"The question is clear, I can just start querying data\"\n**Reality:** Vague questions lead to unfocused investigation. Decompose first, always.\n\n**Common Rationalization:** \"I'll figure out the sub-questions as I go\"\n**Reality:** Without a clear framework, you'll chase random patterns. Plan the investigation first.\n\n---\n\n## Phase 2: Data Discovery\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Mapped each sub-question to specific data tables/columns\n- [ ] Identified what data exists vs what's missing\n- [ ] Documented data limitations that will constrain the investigation\n- [ ] Created a query plan for each sub-question\n- [ ] Saved to `02 - data-discovery.md`\n\n### Instructions\n\n1. **Map sub-questions to data**\n\nCreate `analysis/[session-name]/02-data-discovery.md` with: ./templates/phase-2-data-discovery.md\n\n2. **Run initial data quality checks**\n   - Use `understanding-data` skill to verify table structures\n   - Check for NULL values, date ranges, value distributions\n   - Document any surprises or data quality issues\n\n3. **Adjust investigation plan if needed**\n   - If key data is missing, modify sub-questions\n   - Reprioritize based on data availability\n   - Document what questions cannot be answered with available data\n\n**Common Rationalization:** \"I'll just start with queries and see what the data shows\"\n**Reality:** Without mapping questions to data first, you'll waste time on unfocused queries.\n\n**Common Rationalization:** \"I can skip data quality checks since I know the data\"\n**Reality:** Assumptions about data often turn out wrong. Check systematically.\n\n---\n\n## Phase 3: Systematic Investigation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Created one numbered markdown file per sub-question\n- [ ] Executed all planned queries for each sub-question\n- [ ] Documented rationale, query, results, and observations for each\n- [ ] Tracked findings incrementally\n- [ ] Files saved as `03-SQ1-*.md`, `04-SQ2-*.md`, etc.\n\n### Instructions\n\n1. **Investigate ONE sub-question at a time**\n\nImportant: **ONE FILE PER SUB-QUESTION**, not one file per query. Each sub-question file may contain multiple queries.\n\n2. **For each sub-question, create a dedicated file:**\n\nCreate `analysis/[session-name]/03-SQ1-[descriptive-name].md` (then 04-SQ2, 05-SQ3, etc.) with: ./templates/phase-3-sub-question.md\n\n3. **Investigation sequence**\n   - Follow the dependency order from Phase 1\n   - Complete each sub-question fully before moving to next\n   - Build on findings: let earlier answers inform later queries\n   - Update your investigation plan if findings suggest new directions\n\n4. **Use component skills as needed**\n   - `writing-queries` skill for complex SQL\n   - `interpreting-results` skill for understanding patterns\n   - `creating-visualizations` skill for markdown tables/text charts\n\n5. **Document incrementally**\n   - Don't wait until the end to document\n   - Capture observations immediately after each query\n   - Note surprises, anomalies, or unexpected patterns\n\n**Common Rationalization:** \"I'll run all queries first, then document everything at the end\"\n**Reality:** You'll forget context and rationale. Document as you go.\n\n**Common Rationalization:** \"I found something interesting, I'll chase it instead of finishing current sub-question\"\n**Reality:** Stay disciplined. Note the interesting finding, complete current sub-question, then decide if it warrants investigation.\n\n**Common Rationalization:** \"I can combine multiple sub-questions into one file\"\n**Reality:** One file per sub-question creates clear structure and makes findings easy to locate.\n\n---\n\n## Phase 4: Synthesis\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Reviewed all sub-question findings together\n- [ ] Identified patterns and connections across sub-questions\n- [ ] Assessed which hypotheses from Phase 1 are supported/refuted\n- [ ] Created a coherent narrative explaining the findings\n- [ ] Saved to `XX - synthesis.md` (use next sequential number)\n\n### Instructions\n\n1. **Create synthesis file**\n\nCreate `analysis/[session-name]/XX-synthesis.md` with: ./templates/phase-4-synthesis.md\n\n2. **Build a coherent narrative**\n   - Connect the dots between sub-question findings\n   - Identify the most parsimonious explanation\n   - Acknowledge where evidence is strong vs weak\n   - Be honest about alternative explanations\n\n3. **Check your logic**\n   - Does the explanation account for ALL the findings?\n   - Are there contradictions you're ignoring?\n   - Are you cherry-picking evidence that fits your story?\n   - What would someone skeptical say?\n\n**Common Rationalization:** \"The first sub-question gave me the answer, I don't need synthesis\"\n**Reality:** Individual findings need integration. Synthesis reveals connections and tests coherence.\n\n**Common Rationalization:** \"I'll just present the findings separately and let the user synthesize\"\n**Reality:** Your job is to synthesize. Don't pass the cognitive work to the user.\n\n---\n\n## Phase 5: Conclusions and Recommendations\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Written clear answer to the original broad question\n- [ ] Provided specific, actionable recommendations\n- [ ] Listed concrete next steps or follow-up investigations\n- [ ] Documented key limitations and caveats\n- [ ] Saved to `XX - conclusions.md` (use next sequential number)\n- [ ] Updated `00 - overview.md` with summary\n\n### Instructions\n\n1. **Create conclusions file**\n\nCreate `analysis/[session-name]/XX-conclusions.md` with: ./templates/phase-5-conclusions.md\n\n2. **Update overview file**\n\nUpdate: `00 - overview.md`\n\nAdd at the end:\n\n```markdown\n## Investigation Summary\n\n**Broad Question:** [Original question]\n\n**Answer:** [One-sentence conclusion]\n\n**Confidence:** [High/Medium/Low]\n\n**Key Finding:** [Most important discovery]\n\n**Primary Recommendation:** [Top priority action]\n\n**Critical Limitation:** [Most important caveat]\n\n**Recommended Follow-up:** [Most valuable next investigation]\n\n---\n\n## File Index\n\n- 01 - Question Decomposition\n- 02 - Data Discovery\n- 03-SQ1 - [Sub-question 1 name]\n- 04-SQ2 - [Sub-question 2 name]\n- 05-SQ3 - [Sub-question 3 name]\n- [etc. - list all files]\n- XX - Synthesis\n- XX - Conclusions\n```\n\n3. **Final verification checklist**\n   - [ ] All sub-questions answered\n   - [ ] Synthesis creates coherent narrative\n   - [ ] Recommendations are specific and actionable\n   - [ ] Limitations honestly stated\n   - [ ] Follow-ups identified\n   - [ ] Overview updated\n   - [ ] User informed of conclusions\n\n**Common Rationalization:** \"I found interesting patterns, that's enough\"\n**Reality:** Patterns aren't conclusions. Synthesize findings into clear answer to original question.\n\n**Common Rationalization:** \"I'll let the user decide what to do with the findings\"\n**Reality:** Provide specific recommendations. Don't make them do all the strategic thinking.\n\n**Common Rationalization:** \"I'll skip the limitations section since the conclusion is solid\"\n**Reality:** Every investigation has limitations. Acknowledging them increases credibility.\n\n---\n\n## Complete Example: Customer Churn Investigation\n\n### Example Scenario\nUser asks: \"Why are we losing customers in the premium segment?\"\n\n### Phase 1: Question Decomposition (01 - question-decomposition.md)\n\n```markdown\n# Question Decomposition\n\n## Broad Investigative Question\n\n\"Why are we losing customers in the premium segment?\"\n\n## Context and Motivation\n\nPremium customers (>$500/month) have historically been our most stable segment with <5% annual churn. In Q1 2024, churn rate jumped to 12%. Need to understand root cause to stem the losses.\n\n## Sub-Questions\n\n### Sub-Question 1: When did the churn increase begin?\n**What we need to learn:** Precise timing of when churn accelerated\n**Why it matters:** Helps identify triggering events (pricing change, product issue, competitor launch)\n**Success criteria:** Month-by-month churn rate showing inflection point\n\n### Sub-Question 2: Are churned customers concentrated in specific product lines?\n**What we need to learn:** Whether churn is product-specific or segment-wide\n**Why it matters:** Product-specific churn suggests product issues; broad churn suggests market/competitive factors\n**Success criteria:** Churn rate by product category with statistical significance\n\n### Sub-Question 3: What was the tenure of churned customers?\n**What we need to learn:** Are we losing new customers or long-tenured ones?\n**Why it matters:** New customer churn suggests onboarding issues; long-tenure churn suggests value erosion\n**Success criteria:** Distribution of churned customers by tenure (0-6mo, 6-12mo, 12-24mo, 24mo+)\n\n### Sub-Question 4: Did churned customers show usage decline before churning?\n**What we need to learn:** Whether churn was preceded by disengagement\n**Why it matters:** Usage decline signals value realization problems; sudden churn suggests competitive switching\n**Success criteria:** Usage metrics 30/60/90 days before churn vs stable customers\n\n### Sub-Question 5: Are there geographic patterns to churn?\n**What we need to learn:** Whether churn is concentrated in specific regions\n**Why it matters:** Geographic concentration suggests regional competitive or operational factors\n**Success criteria:** Churn rate by region with sample size validation\n\n## Investigation Dependencies\n\n1. SQ1 first (timing) - establishes when to focus detailed analysis\n2. SQ2 and SQ5 parallel (product and geography) - identify concentration\n3. SQ3 (tenure) - requires churn cohort identified from SQ1\n4. SQ4 last (usage) - most complex, builds on understanding from others\n\n## Hypotheses to Consider\n\n1. **Price increase impact:** 8% price increase in January may have pushed customers over threshold\n2. **Competitor launches:** Competitor Y launched enterprise tier in December\n3. **Product quality:** Premium features had stability issues in Q4 2023\n4. **Support degradation:** Support team had high turnover in Q1\n5. **Contract renewal timing:** Many premium contracts up for renewal in Q1\n\n## Success Criteria for Overall Investigation\n\nInvestigation complete when we can identify:\n1. Primary driver of increased churn (with 70%+ confidence)\n2. Quantified impact of that driver\n3. Actionable recommendations to reduce churn\n```\n\n### Phase 2: Data Discovery (02 - data-discovery.md)\n\n```markdown\n# Data Discovery\n\n## Available Data\n\n### Tables Overview\n\n- `customers`: Customer master data (id, signup_date, segment, region)\n- `subscriptions`: Subscription details (customer_id, product_id, start_date, end_date, status)\n- `usage_metrics`: Daily usage stats (customer_id, date, login_count, feature_usage)\n- `products`: Product catalog (id, name, category, tier)\n- `support_tickets`: Customer support interactions\n\n### Relevant Columns by Sub-Question\n\n#### Sub-Question 1: Timing of churn increase\n**Required data:**\n- `subscriptions.end_date` - when subscription ended\n- `subscriptions.status` - to identify churns vs active\n- `customers.segment` - to filter to premium\n\n**Data check needed:**\n- Definition of \"churn\" - is end_date populated for all churns?\n- Completeness of historical data - how far back do we have data?\n\n#### Sub-Question 2: Product concentration\n**Required data:**\n- `subscriptions.product_id` - what they were subscribed to\n- `products.category` - to group products\n\n**Data check needed:**\n- Are multi-product customers handled correctly?\n- Do we have category mapping for all products?\n\n#### Sub-Question 3: Customer tenure\n**Required data:**\n- `customers.signup_date` - when they joined\n- `subscriptions.end_date` - when they churned\n\n**Data check needed:**\n- Consistency between signup_date and first subscription start_date\n\n#### Sub-Question 4: Usage decline\n**Required data:**\n- `usage_metrics.login_count` - engagement measure\n- `usage_metrics.feature_usage` - specific feature adoption\n\n**Data check needed:**\n- Is usage data complete for all customers?\n- What's the grain (daily, weekly)?\n\n#### Sub-Question 5: Geographic patterns\n**Required data:**\n- `customers.region` - geographic identifier\n\n**Data check needed:**\n- Region data completeness\n- Region definition granularity (country, state, city?)\n\n## Data Gaps and Limitations\n\n1. **No explicit churn reason:** Don't have exit interview data or cancellation reasons\n2. **No competitor data:** Cannot directly measure competitive switching\n3. **No pricing history:** Cannot analyze individual price points or grandfathered rates\n4. **Limited support quality metrics:** Have ticket count but not resolution time or satisfaction scores\n\n## Query Plan\n\n### Sub-Question 1: Timing\n1. Monthly churn count and rate for premium segment (last 12 months)\n2. Comparison to prior year same period\n3. Weekly granularity for Q1 2024 to identify precise inflection point\n\n### Sub-Question 2: Product concentration\n1. Churn rate by product category\n2. Expected vs actual churn by category (chi-square test approach)\n\n### Sub-Question 3: Tenure\n1. Distribution of churned customers by tenure bucket\n2. Churn rate by tenure bucket (churned / total in bucket)\n\n### Sub-Question 4: Usage patterns\n1. Average usage metrics 30/60/90 days before churn\n2. Comparison to stable premium customers in same timeframe\n\n### Sub-Question 5: Geography\n1. Churn count and rate by region\n2. Statistical significance test for regions\n\n## Investigation Strategy\n\n1. Start with SQ1 (timing) - will identify the specific churn cohort to analyze\n2. Then SQ2 (product) and SQ5 (geography) in parallel - both straightforward, high-value\n3. Then SQ3 (tenure) - quick analysis once cohort is identified\n4. Finally SQ4 (usage) - most complex, requires time-series analysis\n```\n\n### Phases 3-5\n\n[Would continue with actual queries and findings, following the same detailed structure as the hypothesis-testing example. For brevity in this skill documentation, showing structure rather than complete worked example.]\n\n---\n\n## Common Rationalizations\n\n### \"The question is vague, I'll just explore the data and see what I find\"\n**Why this is wrong:** Unfocused exploration leads to random pattern-chasing and analysis paralysis.\n\n**Do instead:** Decompose the vague question into specific sub-questions in Phase 1. Structure the investigation.\n\n### \"I'll skip question decomposition since I know what to investigate\"\n**Why this is wrong:** Your initial instinct about what to investigate often misses important angles. Systematic decomposition reveals blind spots.\n\n**Do instead:** Always do Phase 1. Writing down sub-questions forces you to think comprehensively.\n\n### \"I found an interesting pattern, I'll investigate that instead of my planned sub-questions\"\n**Why this is wrong:** Chasing tangents destroys investigation coherence. You end up with fragments, not a complete answer.\n\n**Do instead:** Note interesting patterns for potential follow-up, but complete your current sub-question first.\n\n### \"I'll combine multiple sub-questions into one analysis\"\n**Why this is wrong:** Mixing sub-questions creates confusion and makes findings hard to locate later.\n\n**Do instead:** One file per sub-question. Keep them separate and focused.\n\n### \"The data shows X, so the answer must be Y\"\n**Why this is wrong:** Correlation isn't causation. Data patterns have multiple possible explanations.\n\n**Do instead:** Use Phase 4 synthesis to consider multiple explanations. Test competing hypotheses.\n\n### \"I have some findings, I'll just list them for the user\"\n**Why this is wrong:** Raw findings without synthesis don't answer the original question. You're not a data printer, you're an analyst.\n\n**Do instead:** Synthesize findings into a coherent narrative in Phase 4. Answer the actual question asked.\n\n### \"I'll make strong recommendations even though I'm not confident\"\n**Why this is wrong:** Overconfident recommendations based on weak evidence damage credibility and lead to bad decisions.\n\n**Do instead:** Calibrate recommendations to confidence level. If confidence is medium, say so and explain what would increase it.\n\n### \"I found the answer, I don't need to identify follow-up questions\"\n**Why this is wrong:** Every investigation should identify what to investigate next. Good analysis always reveals new questions.\n\n**Do instead:** Always list 2-3 follow-up investigations in Phase 5. Show what the next layer of analysis would be.\n\n### \"I'll skip documenting limitations since it weakens my conclusion\"\n**Why this is wrong:** Hiding limitations destroys trust. Readers find them anyway, and then they distrust everything.\n\n**Do instead:** Explicitly document limitations. Honest uncertainty is more credible than false certainty.\n\n---\n\n## Summary\n\nThis skill ensures systematic, comprehensive investigation of open-ended questions by:\n\n1. **Decomposing vague questions:** Break broad questions into specific, answerable sub-questions\n2. **Mapping questions to data:** Verify what data exists before diving into analysis\n3. **Investigating systematically:** One sub-question at a time, building incrementally\n4. **Synthesizing findings:** Connect the dots to create coherent explanations\n5. **Providing actionable conclusions:** Answer the original question with specific recommendations\n6. **Identifying next questions:** Every investigation reveals what to investigate next\n\nFollow this process and you'll produce thorough, defensible investigations that answer complex business questions.\n",
        "plugins/datapeeker/skills/guided-investigation/templates/phase-1-question-decomposition.md": "# Question Decomposition\n\n## Broad Investigative Question\n\n[User's original question in plain language]\n\nExample: \"Why are our sales declining in the Northeast region?\"\n\n## Context and Motivation\n\n[Why this question matters, what decision it informs]\n\nExample: \"Sales in Northeast have dropped 15% YoY. Need to understand root cause to determine if this is a market trend, competitive pressure, operational issue, or product problem.\"\n\n## Sub-Questions\n\nBreak the broad question into specific, answerable components:\n\n### Sub-Question 1: [Specific question]\n**What we need to learn:** [Clear statement of what this answers]\n**Why it matters:** [How this relates to the broad question]\n**Success criteria:** [What would constitute an answer]\n\nExample:\n### Sub-Question 1: Is the decline uniform across all months or concentrated in specific periods?\n**What we need to learn:** Temporal pattern of the decline\n**Why it matters:** Helps distinguish seasonal effects from sustained trends\n**Success criteria:** Monthly sales comparison showing when decline started and if it's accelerating\n\n### Sub-Question 2: [Another specific question]\n...\n\n### Sub-Question 3: [Another specific question]\n...\n\n## Investigation Dependencies\n\n[Which sub-questions need to be answered first? What's the logical order?]\n\nExample:\n1. Start with temporal patterns (SQ1) - establishes baseline understanding\n2. Then segment analysis (SQ2, SQ3) - identifies which segments are affected\n3. Finally product analysis (SQ4) - determines if specific products drive the pattern\n4. Context analysis (SQ5) - checks for external factors\n\n## Hypotheses to Consider\n\n[What are possible explanations we should investigate?]\n\nNote: Unlike hypothesis-testing skill, these are informal \"things to check\" not formal H0/H1\n\nExample:\n1. Competitive pressure: New competitor entered market in Q2\n2. Product quality: Customer complaints increased starting March\n3. Pricing: Prices increased 8% in January\n4. Seasonality: Historical Q1 weakness\n5. Sales team: Turnover in sales team in February\n\n## Success Criteria for Overall Investigation\n\n[What would make this investigation complete?]\n\nExample: \"Investigation complete when we can identify the top 2-3 factors driving the decline and quantify their relative impact.\"\n",
        "plugins/datapeeker/skills/guided-investigation/templates/phase-2-data-discovery.md": "# Data Discovery\n\n## Available Data\n\n[Use understanding-data skill to profile the database]\n\n### Tables Overview\n[List all tables and their purposes]\n\nExample:\n- `sales`: Transaction-level sales data\n- `customers`: Customer demographics and segments\n- `products`: Product catalog with categories\n- `regions`: Geographic region definitions\n\n### Relevant Columns by Sub-Question\n\n#### Sub-Question 1: Temporal patterns\n**Required data:**\n- `sales.transaction_date` - for time series analysis\n- `sales.amount` or `sales.revenue` - for sales metrics\n\n**Data check needed:**\n- Date range coverage\n- Completeness of historical data\n- Date format consistency\n\n#### Sub-Question 2: [Next question]\n**Required data:**\n- [List columns needed]\n\n**Data check needed:**\n- [Quality checks to perform]\n\n## Data Gaps and Limitations\n\n[What data is NOT available that would be useful?]\n\nExample:\n1. **No competitor data:** Cannot directly measure competitive pressure\n2. **No promotion history:** Cannot control for promotional effects\n3. **No store hours data:** Cannot distinguish operational changes from demand changes\n4. **Customer satisfaction scores:** Only available from Q2 onward, not for historical comparison\n\n## Query Plan\n\nFor each sub-question, define what queries will be needed:\n\n### Sub-Question 1: Temporal patterns\n1. **Monthly sales trend:** GROUP BY month, calculate total and YoY change\n2. **Week-over-week analysis:** Check for sudden drops vs gradual decline\n3. **Same-period-last-year comparison:** Control for seasonality\n\n### Sub-Question 2: [Next question]\n1. **[Query description]**\n2. **[Query description]**\n\n## Investigation Strategy\n\n[Based on data availability, what order will we investigate?]\n\nExample:\n1. Start with SQ1 (temporal) - data is complete, establishes baseline\n2. Then SQ2 (regional) - data is good, will narrow focus\n3. Skip SQ4 temporarily (data gap: no competitor data available)\n4. End with SQ3 (product) - most granular, build on earlier findings\n",
        "plugins/datapeeker/skills/guided-investigation/templates/phase-3-sub-question.md": "# Sub-Question 1: [Question text]\n\n## Objective\n\n[Restate what this sub-question is trying to determine]\n\nExample: \"Determine if the sales decline is concentrated in specific months or spread evenly across the year.\"\n\n## Approach\n\n[Explain how you'll answer this question - what queries/analysis steps]\n\nExample:\n1. Calculate monthly sales for current year and prior year\n2. Compute YoY percentage change by month\n3. Visualize the trend to identify when decline started\n4. Check if decline is accelerating, stable, or improving\n\n---\n\n## Query 1: Monthly Sales Comparison\n\n### Rationale\n[Why this specific query is needed]\n\n### Query\n```sql\n-- [Clear SQL with comments explaining logic]\nSELECT\n  STRFTIME('%Y-%m', transaction_date) as month,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_sales,\n  ROUND(AVG(amount), 2) as avg_transaction\nFROM sales\nWHERE transaction_date >= '2023-01-01'\nGROUP BY month\nORDER BY month;\n```\n\n### Results\n[Paste actual query results - raw output]\n\n```\nmonth   | transaction_count | total_sales | avg_transaction\n2023-01 | 4523             | 203450.67   | 44.97\n2023-02 | 4234             | 189234.22   | 44.70\n...\n2024-01 | 3845             | 172389.45   | 44.82\n2024-02 | 3623             | 162145.88   | 44.76\n```\n\n### Observations\n[What do you see? Facts only, interpretation comes later]\n\n- 2023-01: 4,523 transactions, $203,451 total\n- 2024-01: 3,845 transactions, $172,389 total (15% decline in transactions, 15.3% decline in revenue)\n- Average transaction size is stable (~$45) - decline is driven by volume, not ticket size\n\n---\n\n## Query 2: [Next query for this sub-question]\n\n[Repeat the same structure: Rationale, Query, Results, Observations]\n\n---\n\n## Sub-Question Answer\n\n[After all queries for this sub-question, synthesize the answer]\n\n### Key Findings\n[What did you learn? Answer the specific sub-question]\n\nExample: \"The sales decline began in January 2024 and has been consistent month-over-month (~15% vs prior year). The decline is driven entirely by transaction volume, not by changes in average purchase size.\"\n\n### Implications\n[What does this mean for the broader investigation?]\n\nExample: \"Since ticket size is stable, this suggests a customer traffic problem rather than a pricing or product mix issue. Need to investigate customer acquisition/retention in subsequent sub-questions.\"\n\n### Follow-up Questions Raised\n[What new questions does this create?]\n\n1. Why did transaction volume drop in January specifically?\n2. Are we losing customers or are existing customers buying less frequently?\n3. Is this pattern consistent across all customer segments?\n",
        "plugins/datapeeker/skills/guided-investigation/templates/phase-4-synthesis.md": "# Synthesis\n\n## Investigation Summary\n\n[One paragraph: What was the broad question and what did you investigate?]\n\nExample: \"Investigated why Northeast region sales declined 15% YoY by examining temporal patterns, regional segmentation, product mix, and customer behavior across 5 sub-questions.\"\n\n## Sub-Question Findings Recap\n\n### SQ1: [Question]\n**Finding:** [One-sentence answer]\n\n### SQ2: [Question]\n**Finding:** [One-sentence answer]\n\n### SQ3: [Question]\n**Finding:** [One-sentence answer]\n\n[etc.]\n\n## Cross-Cutting Patterns\n\n[What patterns emerged across multiple sub-questions?]\n\nExample:\n1. **Timing alignment:** Decline started in January 2024 across all regions and product categories\n2. **Volume vs price:** All segments show volume decline with stable pricing\n3. **Customer concentration:** 70% of decline comes from mid-tier customer segment\n\n## Hypothesis Assessment\n\n[From Phase 1, which hypotheses are supported by evidence?]\n\n### Hypothesis: Competitive pressure\n**Evidence:**\n- Transaction volume dropped 15% starting January\n- Average ticket size unchanged (no price competition)\n- Decline concentrated in mid-tier customers\n\n**Assessment:** **SUPPORTED** - pattern is consistent with new competitor targeting mid-market\n\n### Hypothesis: Product quality issues\n**Evidence:**\n- No change in return rates\n- Product mix stable\n- No correlation between decline and specific product categories\n\n**Assessment:** **NOT SUPPORTED** - if quality were the issue, we'd see product-specific effects\n\n### Hypothesis: [Another hypothesis]\n**Evidence:** [...]\n**Assessment:** [SUPPORTED / PARTIALLY SUPPORTED / NOT SUPPORTED]\n\n## Integrated Explanation\n\n[Synthesize findings into a coherent story]\n\nExample: \"The 15% sales decline in Northeast is best explained by competitive pressure beginning in January 2024. The pattern shows:\n\n1. Sudden onset (January start date) suggests market event, not gradual trend\n2. Volume decline with stable pricing suggests customers switching to alternative, not reducing consumption\n3. Mid-tier customer concentration matches known competitor's target market\n4. Geographic concentration in Northeast aligns with competitor's initial market entry\n\nWhile we cannot definitively prove causation without competitor data, the circumstantial evidence strongly points to competitive pressure as the primary driver.\"\n\n## Confidence Assessment\n\n[How confident are you in this explanation?]\n\n**Confidence Level:** [High / Medium / Low]\n\n**Reasoning:**\n- Strengths: [What makes you confident?]\n- Weaknesses: [What creates uncertainty?]\n- Data gaps: [What data would increase confidence?]\n\nExample:\n**Confidence Level:** Medium-High\n\n**Reasoning:**\n- Strengths: Multiple independent lines of evidence point to same conclusion; timing and segmentation patterns are consistent with competitive hypothesis\n- Weaknesses: No direct competitor data to confirm; cannot rule out other January 2024 events (promotion ending, marketing change, etc.)\n- Data gaps: Competitor market entry dates, customer churn destination data, lost deal analysis would confirm hypothesis\n",
        "plugins/datapeeker/skills/guided-investigation/templates/phase-5-conclusions.md": "# Conclusions and Recommendations\n\n## Answer to Broad Question\n\n[Clear, direct answer to the user's original question]\n\nExample: \"Northeast sales declined 15% due to competitive pressure from a new market entrant targeting mid-tier customers starting in January 2024.\"\n\n## Detailed Conclusion\n\n[2-3 paragraphs providing full answer with key evidence]\n\nExample: \"The investigation identified competitive pressure as the primary driver of the Northeast sales decline. Several lines of evidence support this conclusion:\n\nFirst, the timing and pattern of decline suggest a market disruption rather than internal operational issues. The decline started suddenly in January 2024 and has been consistent at ~15% month-over-month, affecting all product categories proportionally.\n\nSecond, the customer segmentation analysis reveals that 70% of the volume loss comes from mid-tier customers (those spending $50-200/month), while high-value and low-value customers are relatively stable. This concentration matches the known target market of Competitor X, who entered the Northeast market in Q4 2023.\n\nThird, the nature of the decline - pure volume loss with stable average transaction values - indicates customers are switching to an alternative provider rather than reducing overall consumption or responding to pricing changes.\"\n\n## Actionable Recommendations\n\n[Specific actions based on findings - prioritized]\n\n### Immediate Actions (1-2 weeks)\n\n1. **[Specific recommendation]**\n   - What to do: [Concrete action]\n   - Why: [How this addresses the findings]\n   - Owner: [Who should do this]\n   - Success metric: [How to measure impact]\n\nExample:\n1. **Conduct competitive pricing analysis**\n   - What: Survey mid-tier customers to understand why they switched; analyze competitor pricing and offerings\n   - Why: Findings suggest competitive pressure but we lack direct competitor intelligence\n   - Owner: Marketing/Strategy team\n   - Success metric: Competitive intelligence report identifying 3-5 key differentiators\n\n2. **Launch mid-tier customer retention program**\n   - What: Create targeted offers for customers in $50-200/month segment\n   - Why: This segment represents 70% of the decline and is most at-risk\n   - Owner: Sales team with Marketing support\n   - Success metric: Reduce churn rate in mid-tier by 20% within 2 months\n\n### Short-term Actions (1-3 months)\n\n[Recommendations for next 3 months]\n\n### Long-term Strategic Implications\n\n[What does this mean for strategy?]\n\nExample: \"This investigation reveals vulnerability to focused competition in the mid-market segment. Long-term strategic response should include:\n- Develop clear mid-market value proposition\n- Improve competitive intelligence capabilities\n- Consider geographic expansion to diversify risk\"\n\n## Follow-up Investigations\n\n[What additional analysis would be valuable?]\n\n1. **[Follow-up question]**\n   - Why investigate: [What this would clarify]\n   - Data needed: [What data required]\n   - Suggested approach: [How to investigate]\n   - Priority: [High/Medium/Low]\n\nExample:\n1. **Customer churn destination analysis**\n   - Why: Would definitively confirm competitive pressure hypothesis\n   - Data needed: Customer exit interviews, win/loss data, market research\n   - Approach: Survey churned customers from January-present; analyze reasons for leaving\n   - Priority: HIGH - directly validates core conclusion\n\n2. **Product-by-product competitive positioning**\n   - Why: Would identify which products are most vulnerable to competition\n   - Data needed: Product-level sales trends, competitive product catalog\n   - Approach: Guided-investigation focused on product mix changes\n   - Priority: MEDIUM - helps refine competitive response\n\n3. **Geographic expansion of competitive threat**\n   - Why: Need to know if pattern will spread to other regions\n   - Data needed: Sales data from other regions, competitor location data\n   - Approach: Comparative-analysis of Northeast vs other regions\n   - Priority: HIGH - informs defensive strategy\n\n## Key Limitations\n\n[Important caveats about the conclusion]\n\n1. **[Limitation]:** [Explanation and impact]\n\nExample:\n1. **No direct competitor data:** Conclusion is based on circumstantial evidence (timing, segmentation, behavior patterns) but lacks direct confirmation of customer switching to specific competitor\n2. **Three-month window:** Analysis covers Jan-Mar 2024; pattern may evolve over time\n3. **Cannot rule out coincidental factors:** Other January 2024 events (internal changes, marketing shifts, external events) could contribute to decline\n4. **Northeast-only focus:** Did not investigate whether similar patterns exist in other regions\n\n## Confidence Level\n\n**Overall confidence:** [High / Medium / Low]\n\n**Rationale:** [Why this confidence level?]\n\nExample:\n**Overall confidence:** Medium-High\n\n**Rationale:** Multiple independent findings point consistently to competitive pressure explanation. Timing, segmentation, and behavioral patterns all align. Primary uncertainty stems from lack of direct competitor data and limited time window. Would upgrade to High confidence with customer churn destination data or extended time series showing persistent pattern.\n",
        "plugins/datapeeker/skills/hypothesis-testing/SKILL.md": "---\nname: hypothesis-testing\ndescription: Rigorous hypothesis testing process for data analysis - formulate hypotheses before looking at data, design tests, analyze systematically, interpret with skepticism\n---\n\n# Hypothesis Testing Process\n\n## Overview\n\nThis skill guides you through rigorous hypothesis testing in data analysis. The core principle is **scientific rigor**: formulate your hypotheses BEFORE looking at the data to avoid p-hacking and confirmation bias.\n\nHypothesis testing is appropriate when:\n- You have a specific claim or belief to test\n- You want to avoid confirmation bias\n- The question can be framed as \"Does X affect Y?\"\n- You need defensible, reproducible conclusions\n\n## Prerequisites\n\nBefore using this skill, you MUST:\n1. Have data imported into SQLite database using the `importing-data` skill\n2. Have data quality validated and cleaned using the `cleaning-data` skill (MANDATORY - never skip)\n3. Have created an analysis workspace (`just start-analysis hypothesis-testing <name>`)\n4. Understand the basic structure of your data tables\n5. Be familiar with the component skills:\n   - `understanding-data` - for data profiling\n   - `writing-queries` - for SQL query construction\n   - `interpreting-results` - for result analysis\n   - `creating-visualizations` - for text-based visualizations\n\n## Mandatory Process Structure\n\nYou MUST use TodoWrite to track progress through all 5 phases. Create todos at the start:\n\n```markdown\n- Phase 1: Hypothesis Formulation (H0/H1) - pending\n- Phase 2: Test Design - pending\n- Phase 3: Data Analysis - pending\n- Phase 4: Result Interpretation - pending\n- Phase 5: Conclusion and Follow-up - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n---\n\n## Phase 1: Hypothesis Formulation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Written null hypothesis (H0) - specific, testable statement\n- [ ] Written alternative hypothesis (H1) - specific, directional or non-directional\n- [ ] Documented WHY you're testing this (user goal, business context)\n- [ ] Saved to `01 - hypothesis-formulation.md`\n\n### Instructions\n\n1. **Ask clarifying questions** about the user's analytical goal\n   - What belief or claim needs testing?\n   - What would constitute evidence for/against this belief?\n   - What's the practical significance threshold?\n\n2. **Write hypotheses in `01 - hypothesis-formulation.md`** with: ./templates/phase-1.md\n\n3. **STOP and get user confirmation**\n   - Read the hypotheses back to the user\n   - Confirm this is what they want to test\n   - Do NOT proceed to Phase 2 until confirmed\n\n**Common Rationalization:** \"I'll just peek at the data to refine the hypothesis\"\n**Reality:** This creates confirmation bias. Formulate hypothesis FIRST, always.\n\n---\n\n## Phase 2: Test Design\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Defined exact metrics to calculate\n- [ ] Specified comparison method (segments, time periods, etc.)\n- [ ] Listed required data checks (missing values, outliers, sample sizes)\n- [ ] Written test plan WITHOUT executing any queries yet\n- [ ] Saved to `02 - test-design.md`\n\n### Instructions\n\n1. **Design your test in `02 - test-design.md`** with: ./templates/phase-2.md\n\n2. **STOP and verify test design**\n   - Does this test actually answer the hypothesis?\n   - Are there simpler/better ways to test it?\n   - Have you planned for data quality issues?\n   - Get user confirmation before proceeding\n\n**Common Rationalization:** \"I'll just run one quick query to see if the data exists\"\n**Reality:** That's looking at data before test design is complete. Finish design FIRST.\n\n---\n\n## Phase 3: Data Analysis\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Executed schema/data quality checks\n- [ ] Documented any data issues found\n- [ ] Executed main analysis query\n- [ ] Executed supporting analysis queries\n- [ ] Saved all queries and results to numbered files (03-*, 04-*, etc.)\n\n### Instructions\n\n1. **Execute queries in order, one file per query**\n\nCreate separate numbered files:\n- `03 - schema-check.md`\n- `04 - data-quality-check.md`\n- `05 - main-analysis.md`\n- `06 - supporting-analysis.md` (if needed)\n\n2. **For each query file, use this structure:** ./templates/phase-3-query.md\n\n3. **Execute queries using appropriate tool**\n   - Use SQLite CLI or database tool\n   - Copy EXACT results (don't summarize or round)\n   - If query fails, document error and revise\n\n4. **Handle data quality issues**\n   - If quality checks reveal problems, document them\n   - Decide: exclude bad data, transform it, or note as limitation\n   - Update test design if needed (document why)\n\n**Common Rationalization:** \"I'll skip data quality checks since the data looks fine\"\n**Reality:** ALWAYS check data quality. Surprises happen. Document what you checked.\n\n**Common Rationalization:** \"I'll combine all queries into one file to save time\"\n**Reality:** Separate files create clear audit trail. One query, one file.\n\n---\n\n## Phase 4: Result Interpretation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Described what the data shows (facts only)\n- [ ] Calculated relevant comparisons (% differences, ratios)\n- [ ] Considered alternative explanations\n- [ ] Assessed confounding factors identified in Phase 1\n- [ ] Stated whether results support/reject H0\n- [ ] Saved to `07 - interpretation.md` (or next number)\n\n### Instructions\n\n1. **Create interpretation file: `XX - interpretation.md`** with: ./templates/phase-4.md\n\n2. **Be intellectually honest**\n   - State limitations clearly\n   - Don't overstate conclusions\n   - Acknowledge uncertainty\n   - Suggest what additional data would help\n\n**Common Rationalization:** \"The pattern is obvious, I don't need to consider alternatives\"\n**Reality:** Always consider alternatives. Obvious patterns often have surprising explanations.\n\n**Common Rationalization:** \"I'll downplay limitations so the conclusion looks stronger\"\n**Reality:** Stating limitations INCREASES credibility. Be honest about uncertainty.\n\n---\n\n## Phase 5: Conclusion and Follow-up\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Written clear, actionable conclusion\n- [ ] Listed 2-3 specific follow-up questions\n- [ ] Suggested concrete next steps\n- [ ] Saved to `08 - conclusion.md` (or next number)\n- [ ] Updated `00 - overview.md` with summary\n\n### Instructions\n\n1. **Create conclusion file: `XX - conclusion.md`** with: ./templates/phase-5.md\n\n2. **Update overview file: `00 - overview.md`**\n\nAdd summary section with: ./templates/overview-summary.md\n\n3. **Final checklist before marking complete**\n   - [ ] All phases documented in numbered files\n   - [ ] Queries and results included\n   - [ ] Limitations acknowledged\n   - [ ] Follow-up questions specified\n   - [ ] Overview updated\n   - [ ] User informed of conclusions\n\n**Common Rationalization:** \"I found the answer, I'm done\"\n**Reality:** Good analysis always identifies the NEXT question. List follow-ups.\n\n**Common Rationalization:** \"I'll skip updating the overview since it's all in the detailed files\"\n**Reality:** Overview provides navigational summary. Always update it.\n\n---\n\n## Complete Example: Day-of-Week Analysis\n\n### Example Scenario\nUser wants to test: \"Do we get more sales on weekends?\"\n\n### Phase 1: Hypothesis Formulation (01 - hypothesis-formulation.md)\n\n```markdown\n# Hypothesis Formulation\n\n## Analytical Goal\nDetermine if weekend days (Saturday, Sunday) have higher sales than weekday days (Monday-Friday)\n\n## Context\nBusiness wants to optimize staffing and inventory. If weekends are significantly busier, we should staff up. If not, we can balance resources across the week.\n\n## Hypotheses\n\n### Null Hypothesis (H0)\nWeekend days and weekday days have equal average sales. Any observed differences are due to random variation.\n\n### Alternative Hypothesis (H1)\nWeekend days have significantly different average sales compared to weekday days.\n\n## Success Criteria\n- Difference >25% would be practically meaningful (enough to justify staffing changes)\n- Pattern should be consistent across multiple weeks\n\n## Potential Confounds\n- Holidays: Holiday Monday might inflate weekday averages\n- Promotions: Weekend promotions might inflate weekend sales\n- Store hours: Different hours on weekends might affect opportunity\n- Seasonality: Analysis period might not be representative of full year\n```\n\n### Phase 2: Test Design (02 - test-design.md)\n\n```markdown\n# Test Design\n\n## Metrics\n\n### Primary Metric\nAverage daily sales amount: SUM(amount) / COUNT(DISTINCT date) for weekend vs weekday groups\n\n### Supporting Metrics\n- Total transaction count per day-of-week (to assess sample size)\n- Median sales per transaction (to check if average is representative)\n- Week-over-week consistency (to see if pattern is stable)\n\n## Comparison Structure\nGroup days into two categories:\n- Weekend: Saturday (day 6), Sunday (day 0)\n- Weekday: Monday-Friday (days 1-5)\n\nCalculate average daily sales for each group, compare the ratio.\n\n## Data Requirements\n\n### Required Tables/Columns\n- Table: `sales` (or similar)\n  - `transaction_date` (DATE or TEXT in ISO format)\n  - `amount` (NUMERIC)\n\n### Data Quality Checks\n1. Check for NULL dates or amounts\n2. Verify date range covers complete weeks\n3. Confirm adequate sample size (>100 transactions per day-of-week)\n4. Identify any outlier days (holidays, system outages)\n\n### Queries Needed\n1. Schema check: PRAGMA table_info(sales)\n2. Data quality: NULL checks, date range, outlier detection\n3. Main analysis: Daily sales by day-of-week\n4. Supporting: Transaction counts, weekly pattern consistency\n\n## Statistical Considerations\nLook for differences >25% between weekend and weekday averages. Check if pattern is consistent across multiple weeks (not just one unusual weekend).\n```\n\n### Phase 3: Data Analysis (03-06 files)\n\n**03 - schema-check.md:**\n```markdown\n# Schema Check\n\n## Rationale\nVerify the sales table exists and has the required columns for date and amount\n\n## Query\n```sql\nPRAGMA table_info(sales);\n```\n\n## Results\n```\ncid | name              | type    | notnull | dflt_value | pk\n0   | transaction_id    | INTEGER | 0       | NULL       | 1\n1   | transaction_date  | TEXT    | 0       | NULL       | 0\n2   | amount            | REAL    | 0       | NULL       | 0\n3   | product_id        | INTEGER | 0       | NULL       | 0\n```\n\n## Initial Observations\n- Table `sales` exists\n- Has `transaction_date` column (TEXT type - will need STRFTIME to extract day-of-week)\n- Has `amount` column (REAL type - good for calculations)\n- Columns allow NULLs - need to check data quality\n```\n\n**04 - data-quality-check.md:**\n```markdown\n# Data Quality Check\n\n## Rationale\nVerify data completeness and identify any issues before main analysis\n\n## Query\n```sql\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(CASE WHEN transaction_date IS NULL THEN 1 END) as null_dates,\n  COUNT(CASE WHEN amount IS NULL THEN 1 END) as null_amounts,\n  MIN(transaction_date) as earliest_date,\n  MAX(transaction_date) as latest_date,\n  MIN(amount) as min_amount,\n  MAX(amount) as max_amount\nFROM sales;\n```\n\n## Results\n```\ntotal_rows | null_dates | null_amounts | earliest_date | latest_date | min_amount | max_amount\n15847      | 0          | 0            | 2024-01-01    | 2024-03-31  | 5.00       | 899.99\n```\n\n## Initial Observations\n- 15,847 total transactions\n- No NULL dates or amounts (clean data)\n- 3 months of data (Jan-Mar 2024)\n- 13 complete weeks (91 days / 7)\n- Amount range: $5 to $899.99 (no obvious outliers)\n```\n\n**05 - main-analysis.md:**\n```markdown\n# Main Analysis: Sales by Day of Week\n\n## Rationale\nCalculate average sales by day of week to test if weekend days differ from weekdays\n\n## Query\n```sql\nSELECT\n  CAST(STRFTIME('%w', transaction_date) AS INTEGER) as day_of_week,\n  CASE\n    WHEN CAST(STRFTIME('%w', transaction_date) AS INTEGER) IN (0, 6) THEN 'Weekend'\n    ELSE 'Weekday'\n  END as day_type,\n  COUNT(DISTINCT transaction_date) as days_count,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_sales,\n  ROUND(SUM(amount) / COUNT(DISTINCT transaction_date), 2) as avg_daily_sales,\n  ROUND(AVG(amount), 2) as avg_transaction_amount\nFROM sales\nGROUP BY day_of_week, day_type\nORDER BY day_of_week;\n```\n\n## Results\n```\nday_of_week | day_type | days_count | transaction_count | total_sales | avg_daily_sales | avg_transaction_amount\n0           | Weekend  | 13         | 1456              | 52389.44    | 4030.00        | 35.98\n1           | Weekday  | 13         | 2234              | 102345.67   | 7872.90        | 45.81\n2           | Weekday  | 13         | 2198              | 98234.55    | 7556.50        | 44.70\n3           | Weekday  | 13         | 2301              | 105678.90   | 8129.15        | 45.93\n4           | Weekday  | 13         | 2345              | 108901.23   | 8377.02        | 46.44\n5           | Weekday  | 13         | 2456              | 115432.11   | 8879.39        | 47.00\n6           | Weekend  | 13         | 2857              | 98765.43    | 7597.34        | 34.56\n```\n\n## Initial Observations\n- Sunday (0): $4,030 avg daily sales, 1,456 transactions, $35.98 avg transaction\n- Monday-Friday (1-5): $7,800-$8,800 avg daily sales, 2,200-2,500 transactions, $44-47 avg transaction\n- Saturday (6): $7,597 avg daily sales, 2,857 transactions, $34.56 avg transaction\n- Saturday has HIGH transaction count but LOW per-transaction amount\n- Sunday has both low transaction count AND low per-transaction amount\n```\n\n**06 - supporting-analysis.md:**\n```markdown\n# Supporting Analysis: Weekend vs Weekday Comparison\n\n## Rationale\nAggregate weekend and weekday categories to test the hypothesis directly\n\n## Query\n```sql\nSELECT\n  CASE\n    WHEN CAST(STRFTIME('%w', transaction_date) AS INTEGER) IN (0, 6) THEN 'Weekend'\n    ELSE 'Weekday'\n  END as day_type,\n  COUNT(DISTINCT transaction_date) as days_count,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_sales,\n  ROUND(SUM(amount) / COUNT(DISTINCT transaction_date), 2) as avg_daily_sales,\n  ROUND(AVG(amount), 2) as avg_transaction_amount\nFROM sales\nGROUP BY day_type;\n```\n\n## Results\n```\nday_type | days_count | transaction_count | total_sales | avg_daily_sales | avg_transaction_amount\nWeekday  | 65         | 11534             | 530592.46   | 8163.00         | 46.01\nWeekend  | 26         | 4313              | 151154.87   | 5813.65         | 35.04\n```\n\n## Initial Observations\n- Weekday average: $8,163 per day\n- Weekend average: $5,814 per day\n- Difference: $2,349 (29% lower on weekends)\n- Weekend has 62% fewer transactions per day\n- Weekend transactions are 24% smaller on average\n```\n\n### Phase 4: Interpretation (07 - interpretation.md)\n\n```markdown\n# Result Interpretation\n\n## Summary of Findings\n\n### Primary Metric Results\nWeekends have significantly LOWER sales than weekdays:\n- Weekday avg: $8,163 per day\n- Weekend avg: $5,814 per day\n- Difference: 29% lower on weekends\n\nThis contradicts the original hypothesis that weekends would have higher sales.\n\n### Statistical Assessment\n- Magnitude: 29% difference is both statistically meaningful (large sample) and practically significant (>25% threshold)\n- Consistency: Pattern holds across 13 weeks of data\n- Practical importance: This magnitude justifies different staffing/inventory approaches\n\n## Alternative Explanations\n\n1. **Store hours:** Maybe stores close earlier on weekends\n   - Evidence: Transaction count is 62% lower on weekends, suggesting operational constraint\n   - Impact: Could fully explain lower daily totals if hours are reduced\n\n2. **Customer behavior:** Maybe weekend shoppers buy smaller items\n   - Evidence: Average transaction is 24% smaller on weekends ($35 vs $46)\n   - Impact: Explains some but not all of the difference\n\n3. **Product mix:** Maybe high-value products aren't available on weekends\n   - Evidence: Would need product category data to verify\n   - Impact: Unknown\n\n4. **Sunday effect:** Maybe Sunday drags down weekend average\n   - Evidence: Sunday has only $4,030 avg (47% lower than Saturday's $7,597)\n   - Impact: If we exclude Sunday, Saturday is closer to weekdays but still 7% lower\n\n## Hypothesis Test Result\n\n### Null Hypothesis (H0)\nWeekend days and weekday days have equal average sales.\n\n### Decision\n**REJECT H0**\n\n### Rationale\nThe 29% difference between weekend and weekday average sales is large, consistent across 13 weeks, and exceeds our 25% practical significance threshold. The pattern is clear: weekends have lower sales than weekdays.\n\nHOWEVER: This is the OPPOSITE of what we hypothesized. The alternative hypothesis stated \"weekend days have significantly different sales\" which is true, but we expected higher, not lower.\n\n## Limitations\n\n1. **Cannot determine causation:** Data shows weekends are lower but doesn't explain why\n2. **Store operational factors unknown:** Don't have data on hours, staffing, inventory availability\n3. **Sunday is dramatically lower:** Weekend average is heavily influenced by very low Sunday sales\n4. **Seasonal effects:** 3 months (Jan-Mar) may not represent full year (could be post-holiday slump)\n5. **No control for promotions/holidays:** Any holidays in the period could skew results\n```\n\n### Phase 5: Conclusion (08 - conclusion.md)\n\n```markdown\n# Conclusion and Follow-up\n\n## Main Conclusion\n\nWeekends have 29% lower average daily sales than weekdays, contradicting the initial hypothesis that weekends would be busier. This pattern is consistent across 13 weeks and appears driven by both fewer transactions (62% lower) and smaller average purchases (24% lower).\n\n## Actionable Insights\n\n**DO NOT increase weekend staffing based on this data.** The hypothesis that weekends are busier is not supported.\n\nInstead:\n\n1. **Investigate Sunday specifically:** With only $4,030 avg daily sales (vs $7,600 on Saturday), Sunday operations may not be profitable. Consider:\n   - Reduced hours on Sunday\n   - Sunday-specific promotions to drive traffic\n   - Or closing on Sundays if fixed costs exceed contribution margin\n\n2. **Understand operational constraints:** Before making staffing decisions, determine:\n   - Are weekend hours reduced? (This might explain lower transaction counts)\n   - Is weekend inventory limited? (This might explain smaller purchases)\n   - Are certain high-value products unavailable on weekends?\n\n3. **Focus weekday resources:** If pattern holds year-round, optimize for Monday-Friday peak demand\n\n## Follow-up Questions\n\n1. **What are actual store operating hours by day?**\n   - Data needed: Store hours table, or time-of-day in transaction timestamps\n   - Approach: Calculate sales per operating hour rather than per day\n   - Why: Would distinguish \"fewer hours\" from \"fewer customers per hour\"\n\n2. **Does the weekend pattern hold across all seasons?**\n   - Data needed: Full year of transaction data\n   - Approach: Repeat this analysis for each quarter\n   - Why: Jan-Mar might be post-holiday slump; need to verify pattern is year-round\n\n3. **Are there product category differences on weekends?**\n   - Data needed: Product category or department in transaction records\n   - Approach: Analyze category mix and average prices by day-of-week\n   - Why: Would explain the 24% smaller average transaction on weekends\n\n4. **How do weekends compare if we exclude Sunday?**\n   - Data needed: Current dataset (already have this)\n   - Approach: Re-run analysis treating Sunday separately\n   - Why: Sunday is so dramatically lower it may be distorting the weekend average\n\n## Confidence Level\n\n**Medium confidence** in the finding that weekends are lower, but **low confidence** in the explanation.\n\nReasoning:\n- Pattern is clear and consistent across 13 weeks (strengthens confidence)\n- Sample sizes are adequate (4,313 weekend transactions is plenty)\n- BUT: Cannot distinguish operational constraints from customer behavior (weakens confidence)\n- AND: Only 3 months of data may not represent full year (weakens confidence)\n\n**Next step:** Investigate store hours and expand to full year data before making operational changes.\n```\n\n---\n\n## Common Rationalizations\n\n### \"I'll just look at the data first to understand it better\"\n**Why this is wrong:** Looking at data before formulating hypotheses creates confirmation bias. You'll unconsciously form hypotheses that match what you see, then \"test\" them. This isn't science.\n\n**Do instead:** Formulate hypothesis from domain knowledge, business context, or theory. Then look at data.\n\n### \"The hypothesis is obvious from the results, I don't need to write it down\"\n**Why this is wrong:** Hindsight bias makes everything seem obvious after you see the answer. Writing hypothesis first creates accountability.\n\n**Do instead:** Always write H0 and H1 in Phase 1 before any query execution.\n\n### \"I'll skip data quality checks since the data looks clean\"\n**Why this is wrong:** You can't know data is clean until you check. Surprises happen often.\n\n**Do instead:** ALWAYS run data quality queries. Document what you checked and what you found (even if it's \"no issues\").\n\n### \"The pattern is clear, I don't need to consider alternative explanations\"\n**Why this is wrong:** Obvious patterns often have non-obvious causes. Confounding factors are common.\n\n**Do instead:** Always list 2-3 alternative explanations in Phase 4, even if you think they're unlikely.\n\n### \"I'll combine multiple queries into one file for efficiency\"\n**Why this is wrong:** One file per query creates clear audit trail and makes analysis reproducible.\n\n**Do instead:** One query, one file. Use consistent numbering (03-, 04-, etc.).\n\n### \"I found the answer, analysis is complete\"\n**Why this is wrong:** Good analysis always identifies the next question. Every conclusion should raise new questions.\n\n**Do instead:** Always list 2-3 follow-up questions in Phase 5.\n\n### \"I'll downplay limitations so the conclusion looks stronger\"\n**Why this is wrong:** Acknowledging limitations increases credibility. Readers trust honest uncertainty more than false certainty.\n\n**Do instead:** State limitations clearly. Be honest about what you don't know.\n\n### \"I'll skip updating the overview since everything is in detailed files\"\n**Why this is wrong:** Overview provides navigation and quick summary. Future readers (including you) will thank you.\n\n**Do instead:** Always update `00 - overview.md` with results summary in Phase 5.\n\n---\n\n## Summary\n\nThis skill ensures rigorous, reproducible hypothesis testing by:\n\n1. **Preventing confirmation bias:** Formulate hypothesis BEFORE looking at data\n2. **Ensuring thoughtful design:** Plan your test before executing queries\n3. **Creating audit trail:** One query per file, with rationale and results\n4. **Demanding intellectual honesty:** Consider alternatives, state limitations\n5. **Identifying next questions:** Every analysis should suggest follow-up investigations\n\nFollow this process and you'll produce defensible, credible analysis that stands up to scrutiny.\n",
        "plugins/datapeeker/skills/hypothesis-testing/templates/overview-summary.md": "## Results Summary\n\n**Hypothesis Test Result:** [REJECT H0 / FAIL TO REJECT H0]\n\n**Main Finding:** [One sentence]\n\n**Confidence:** [High/Medium/Low]\n\n**Key Limitation:** [Most important caveat]\n\n**Recommended Follow-up:** [Top priority next question]\n",
        "plugins/datapeeker/skills/hypothesis-testing/templates/phase-1.md": "# Hypothesis Formulation\n\n## Analytical Goal\n[User's question in plain language]\n\n## Context\n[Why this matters, business/research context]\n\n## Hypotheses\n\n### Null Hypothesis (H0)\n[Precise statement of \"no effect\" or baseline assumption]\n\nExample: \"Day of week has no effect on sales volume - all days have equal expected sales\"\n\n### Alternative Hypothesis (H1)\n[What you're looking for evidence of]\n\nExample: \"Day of week affects sales volume - some days have significantly higher/lower sales than others\"\n\n## Success Criteria\n[What would make you reject H0? What magnitude of difference matters practically?]\n\n## Potential Confounds\n[What other factors might explain differences? List now, before seeing data]\n- Factor 1: [e.g., seasonality, holidays]\n- Factor 2: [e.g., promotions, store closures]\n- Factor 3: [e.g., weather, local events]\n",
        "plugins/datapeeker/skills/hypothesis-testing/templates/phase-2.md": "# Test Design\n\n## Metrics\n\n### Primary Metric\n[What you'll measure - be specific about calculation]\n\nExample: \"Average daily sales amount, calculated as SUM(amount) grouped by day_of_week\"\n\n### Supporting Metrics\n- [Additional metrics that provide context]\n- [Sample sizes, variance measures, etc.]\n\n## Comparison Structure\n\n[How you'll compare groups/periods]\n\nExample: \"Compare average sales across all 7 days of the week\"\n\n## Data Requirements\n\n### Required Tables/Columns\n- Table: `sales`\n  - `date` or `transaction_date` (date column)\n  - `amount` or `total` (numeric sales value)\n  - [Any other required columns]\n\n### Data Quality Checks\n1. **Missing values:** Check for NULL dates or amounts\n2. **Date range:** Verify we have complete weeks (not partial data)\n3. **Sample size:** Ensure adequate transactions per day-of-week\n4. **Outliers:** Identify and decide how to handle extreme values\n\n### Queries Needed (design, don't execute yet)\n1. Schema check: Verify table structure\n2. Data quality: Check for NULLs, ranges, counts\n3. Main analysis: Calculate metric by segment\n4. Supporting analysis: Calculate sample sizes, variance\n\n## Statistical Considerations\n\n[How you'll assess significance - note: we can't do formal statistical tests, but we can assess practical significance]\n\nExample: \"Look for differences >20% from average, check if driven by small sample sizes\"\n",
        "plugins/datapeeker/skills/hypothesis-testing/templates/phase-3-query.md": "# [Query Purpose]\n\n## Rationale\n[Why this query is needed for the hypothesis test]\n\n## Query\n```sql\n-- [Clear SQL with comments]\nSELECT\n  STRFTIME('%w', date_column) as day_of_week,  -- 0=Sunday, 6=Saturday\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_sales,\n  AVG(amount) as avg_sale,\n  MIN(amount) as min_sale,\n  MAX(amount) as max_sale\nFROM sales\nWHERE date_column IS NOT NULL\n  AND amount IS NOT NULL\nGROUP BY day_of_week\nORDER BY day_of_week;\n```\n\n## Results\n[Paste actual query results here - raw output]\n\n```\nday_of_week | transaction_count | total_sales | avg_sale | min_sale | max_sale\n0           | 1250              | 45890.50    | 36.71    | 5.00     | 299.99\n1           | 2140              | 98234.20    | 45.90    | 5.00     | 450.00\n...\n```\n\n## Initial Observations\n[What do you see? NO INTERPRETATION YET, just facts]\n- Day 0 (Sunday): 1,250 transactions, $36.71 average\n- Day 1 (Monday): 2,140 transactions, $45.90 average\n- [etc.]\n",
        "plugins/datapeeker/skills/hypothesis-testing/templates/phase-4.md": "# Result Interpretation\n\n## Summary of Findings\n\n### Primary Metric Results\n[Describe what the analysis showed - facts first]\n\nExample: \"Sales vary significantly by day of week:\n- Sunday: $36.71 avg (lowest)\n- Wednesday: $52.30 avg (highest)\n- Range: $15.59 (42% difference from Sunday to Wednesday)\"\n\n### Statistical Assessment\n[Without formal tests, assess practical significance]\n\n- Magnitude: [How big are the differences?]\n- Consistency: [Do sample sizes support the pattern?]\n- Practical importance: [Do differences matter for decisions?]\n\n## Alternative Explanations\n\n[Consider other factors that might explain the pattern]\n\n1. **[Confound 1]:** [How it might explain results, can we rule it out?]\n   - Evidence for/against: [What data suggests this is/isn't the explanation?]\n\n2. **[Confound 2]:** [How it might explain results]\n   - Evidence for/against: [...]\n\nExample:\n1. **Promotions:** Maybe Wednesday has more promotions\n   - Evidence: Would need promotion data to check (not available in current dataset)\n   - Impact: Could fully explain the pattern\n\n2. **Store hours:** Maybe stores open later/close earlier on Sundays\n   - Evidence: Transaction counts are lower (1,250 vs 2,140), supporting this\n   - Impact: Might partially explain lower sales\n\n## Hypothesis Test Result\n\n### Null Hypothesis (H0)\n[Restate from Phase 1]\n\n### Decision\n**[REJECT H0 / FAIL TO REJECT H0]**\n\n### Rationale\n[Why did you make this decision? What evidence was most compelling?]\n\nExample: \"REJECT H0. The 42% difference between Sunday and Wednesday average sales is both statistically meaningful (based on large sample sizes) and practically significant. However, this rejection comes with caveats (see limitations).\"\n\n## Limitations\n\n[What reduces confidence in conclusions?]\n\n1. [Data limitation]\n2. [Methodological limitation]\n3. [Confounding factor not addressed]\n\nExample:\n1. Cannot separate time-of-day effects from day-of-week effects\n2. Did not control for promotions or holidays\n3. Sample period may not be representative (need date range check)\n",
        "plugins/datapeeker/skills/hypothesis-testing/templates/phase-5.md": "# Conclusion and Follow-up\n\n## Main Conclusion\n\n[1-2 sentences: What did you learn? What should reader take away?]\n\nExample: \"Sales vary significantly by day of week, with Wednesday showing 42% higher average sales than Sunday. However, this pattern may be partially or fully explained by operational factors (store hours, staffing) rather than customer behavior differences.\"\n\n## Actionable Insights\n\n[What decisions or actions does this support? Be specific.]\n\n1. [Specific recommendation based on findings]\n2. [Alternative action if confounds are present]\n\nExample:\n1. IF pattern is driven by customer behavior: Consider promotions on low-traffic days (Sun, Mon)\n2. IF pattern is driven by store hours: Test extended hours on Sunday to see if sales increase proportionally\n\n## Follow-up Questions\n\n[What should be investigated next? Prioritize by importance.]\n\n1. **[Question 1]:** [Why this matters, what it would clarify]\n   - Data needed: [What data would answer this?]\n   - Approach: [How would you test this?]\n\n2. **[Question 2]:** [...]\n\nExample:\n1. **Are store hours consistent across days?** This would help separate operational from behavioral effects\n   - Data needed: Store hours by day, or transaction time stamps\n   - Approach: Analyze sales by hour-of-day, grouped by day-of-week\n\n2. **Do promotions correlate with high-sales days?** This would test the promotion confound\n   - Data needed: Promotion calendar with dates and products\n   - Approach: Join promotion data with sales, compare promoted vs non-promoted days\n\n3. **Is the pattern consistent across all stores/regions?** Tests generalizability\n   - Data needed: Store or region identifiers\n   - Approach: Repeat day-of-week analysis segmented by store\n\n## Confidence Level\n\n[How confident are you in the main conclusion?]\n\n- High confidence: Strong evidence, few confounds, adequate sample sizes\n- Medium confidence: Clear pattern but significant confounds not ruled out\n- Low confidence: Weak evidence, major limitations, small samples\n\n**This analysis: [CONFIDENCE LEVEL]** because [specific reasoning]\n",
        "plugins/datapeeker/skills/importing-data/SKILL.md": "---\nname: importing-data\ndescription: Systematic CSV import process - discover structure, design schema, standardize formats, import to database, detect quality issues (component skill for DataPeeker analysis sessions)\n---\n\n# Importing Data - Component Skill\n\n## Purpose\n\nUse this skill when:\n- Starting a new analysis with CSV data files\n- Need to import CSV into relational database systematically\n- Want to ensure proper schema design and type inference\n- Need quality assessment before cleaning/analysis begins\n- Replacing ad-hoc import workflows\n\nThis skill is a **prerequisite** for all DataPeeker analysis workflows and is referenced by all process skills.\n\n**Note:** DataPeeker uses SQLite (`data/analytics.db`), but this process applies to any SQL database. For SQLite-specific commands, see the `using-sqlite` skill.\n\n## Prerequisites\n\n- **CSV file accessible** on local filesystem\n- **Database** with SQL support (DataPeeker uses SQLite at `data/analytics.db`)\n- **Workspace created** for analysis session (via `just start-analysis` or equivalent)\n- **Understanding** that this skill creates `raw_*` tables, not final tables (cleaning-data handles finalization)\n\n## Data Import Process\n\nCreate a TodoWrite checklist for the 5-phase data import process:\n\n```\nPhase 1: CSV Discovery & Profiling - pending\nPhase 2: Schema Design & Type Inference - pending\nPhase 3: Basic Standardization - pending\nPhase 4: Import Execution - pending\nPhase 5: Quality Assessment & Reporting - pending\n```\n\nMark each phase as you complete it. Document all findings in numbered markdown files (`01-csv-profile.md` through `05-quality-report.md`) within your analysis workspace directory.\n\n---\n\n## Phase 1: CSV Discovery & Profiling\n\n**Goal:** Understand CSV file structure, detect encoding and delimiter, capture sample data for schema design.\n\n### File Discovery\n\nIdentify the CSV file(s) to import:\n- Ask user for CSV file path(s)\n- Verify file exists and is readable\n- Note file size for sampling strategy (>100K rows = sample-based profiling)\n\n**Document:** Record CSV file path, size, timestamp in `01-csv-profile.md`\n\n### Encoding Detection\n\nDetect file encoding to prevent import errors:\n\n```bash\nfile -I /path/to/file.csv\n```\n\nCommon encodings:\n- `charset=us-ascii` or `charset=utf-8` â†’ Standard, no conversion needed\n- `charset=iso-8859-1` or `charset=windows-1252` â†’ May need conversion to UTF-8\n\n**Document:** Record detected encoding. If non-UTF-8, note conversion requirement.\n\n### Delimiter Detection\n\nAnalyze first few lines to detect delimiter:\n\n```bash\nhead -n 5 /path/to/file.csv\n```\n\nCheck for:\n- Comma (`,`) - most common\n- Tab (`\\t`) - TSV files\n- Pipe (`|`) - less common\n- Semicolon (`;`) - European CSV files\n\n**Document:** Record detected delimiter character.\n\n### Header Detection\n\nDetermine if first row contains headers:\n- Read first row\n- Check if row contains text labels vs data values\n- If ambiguous, ask user to confirm\n\n**Document:** Record whether headers present, list header names if found.\n\n### Sample Data Capture\n\nCapture representative samples for schema inference:\n\n```bash\n# First 10 rows\nhead -n 11 /path/to/file.csv > /tmp/csv_head_sample.txt\n\n# Last 10 rows\ntail -n 10 /path/to/file.csv > /tmp/csv_tail_sample.txt\n\n# Row count\nwc -l /path/to/file.csv\n```\n\n**Document:** Include head and tail samples in `01-csv-profile.md` for reference during schema design.\n\n### Phase 1 Documentation Template\n\nCreate `analysis/[session-name]/01-csv-profile.md` with: ./templates/phase-1.md\n\n**CHECKPOINT:** Before proceeding to Phase 2, you MUST have:\n- [ ] CSV file path confirmed and file accessible\n- [ ] Encoding, delimiter, headers detected\n- [ ] Sample data captured (head + tail)\n- [ ] `01-csv-profile.md` created with all sections filled\n- [ ] Column overview completed with initial observations\n\n---\n\n## Phase 2: Schema Design & Type Inference\n\n**Goal:** Design database schema by inferring types from CSV samples, propose table structure with rationale.\n\n### Analyze Column Types\n\nFor each column from Phase 1 profiling, infer appropriate data type:\n\n**Type Inference Rules** (adapt to your database):\n\n1. **Integer types** - Use when:\n   - All non-NULL values are whole numbers\n   - No decimal points observed\n   - Typical for: IDs, counts, years, quantities\n   - Examples: INTEGER (SQLite), INT/BIGINT (PostgreSQL/MySQL)\n\n2. **Decimal/Float types** - Use when:\n   - Values contain decimal points\n   - Monetary amounts, measurements, percentages\n   - Examples: REAL (SQLite), NUMERIC/DECIMAL (PostgreSQL), DECIMAL (MySQL)\n\n3. **Text/String types** - Use when:\n   - Mixed alphanumeric content\n   - Dates/datetimes stored as text (ISO 8601: YYYY-MM-DD or YYYY-MM-DD HH:MM:SS)\n   - Categories, names, descriptions\n   - Examples: TEXT (SQLite), VARCHAR/TEXT (PostgreSQL/MySQL)\n   - **Default choice when unsure**\n\n**Note:** Date/time handling varies by database. SQLite stores dates as TEXT. PostgreSQL/MySQL have native DATE/TIMESTAMP types.\n\n**Document:** For each column, record inferred type with rationale and sample values.\n\n### Handle NULL Representations\n\nIdentify NULL representations in CSV:\n- Empty cells â†’ `NULL` in database\n- Literal strings: \"N/A\", \"null\", \"NULL\", \"None\", \"#N/A\" â†’ Convert to `NULL`\n- Numeric codes: -999, -1 (if documented as NULL) â†’ Convert to `NULL`\n\n**Document:** List all NULL representations found and conversion strategy.\n\n### Design Table Schema\n\nPropose CREATE TABLE statement:\n\n```sql\nCREATE TABLE raw_[table_name] (\n  [column_1_name] [TYPE],  -- Rationale for type choice\n  [column_2_name] [TYPE],  -- Rationale for type choice\n  ...\n);\n```\n\n**Table naming convention:**\n- Prefix with `raw_` to indicate unprocessed data\n- Use lowercase with underscores: `raw_sales_data`, `raw_customers`\n- Derive from CSV filename or ask user for preferred name\n\n**Document:** Full CREATE TABLE statement with inline comments explaining each type choice.\n\n### Present Schema to User\n\nUse AskUserQuestion tool to present schema proposal:\n- Show proposed table name\n- Show each column with type and rationale\n- Ask user to confirm or request changes\n\n**Document:** User's approval and any requested modifications.\n\n### Phase 2 Documentation Template\n\nCreate `analysis/[session-name]/02-schema-design.md` with: ./templates/phase-2.md\n\n**CHECKPOINT:** Before proceeding to Phase 3, you MUST have:\n- [ ] All columns analyzed with type inference rationale\n- [ ] NULL representations identified and mapped\n- [ ] CREATE TABLE statement drafted\n- [ ] User approved schema (via AskUserQuestion)\n- [ ] `02-schema-design.md` created with all sections filled\n\n---\n\n## Phase 3: Basic Standardization\n\n**Goal:** Define transformation rules for dates, numbers, whitespace, and text formatting to ensure clean, consistent data in raw_* table.\n\n### Date Format Standardization\n\n**Target format:** ISO 8601\n- Dates: `YYYY-MM-DD` (e.g., 2025-01-15)\n- Datetimes: `YYYY-MM-DD HH:MM:SS` (e.g., 2025-01-15 14:30:00)\n\n**Common source formats to convert:**\n- `MM/DD/YYYY` or `M/D/YYYY` â†’ `YYYY-MM-DD`\n- `DD/MM/YYYY` or `D/M/YYYY` â†’ `YYYY-MM-DD` (verify with user which is month vs day)\n- `YYYY/MM/DD` â†’ `YYYY-MM-DD` (slash to hyphen)\n- `Mon DD, YYYY` â†’ `YYYY-MM-DD` (e.g., \"Jan 15, 2025\" â†’ \"2025-01-15\")\n- Timestamps with T separator: `YYYY-MM-DDTHH:MM:SS` â†’ Keep as-is (valid ISO 8601)\n\n**Document:** List each date column, source format detected, target format, conversion logic.\n\n### Number Format Normalization\n\n**Remove non-numeric characters:**\n- Currency symbols: `$123.45` â†’ `123.45`\n- Comma separators: `1,234.56` â†’ `1234.56`\n- Percentage signs: `45%` â†’ `45` or `0.45` (document choice)\n- Units: `25kg`, `100m` â†’ `25`, `100` (document unit in column name/comments)\n\n**Decimal handling:**\n- Preserve decimal points\n- Convert European format if detected: `1.234,56` â†’ `1234.56` (verify with user)\n\n**Document:** List each numeric column, formatting issues found, normalization rules.\n\n### Whitespace and Text Normalization\n\n**Whitespace cleaning:**\n- Trim leading/trailing whitespace from all TEXT columns\n- Normalize internal whitespace: multiple spaces â†’ single space\n- Normalize line endings: `\\r\\n` or `\\r` â†’ `\\n`\n\n**Text case standardization** (optional, apply selectively):\n- IDs/codes: Often uppercase for consistency\n- Names: Title case or preserve original\n- Free text: Preserve original case\n- **Document choice per column type**\n\n**Document:** Which columns get whitespace cleaning, which get case normalization.\n\n### NULL Standardization\n\nApply NULL representation mapping from Phase 2:\n- Convert all identified NULL representations to actual SQLite `NULL`\n- Empty strings â†’ `NULL` for numeric/date columns\n- Empty strings â†’ Preserve as empty string `''` for TEXT columns (document choice)\n\n**Document:** NULL conversion applied, count of conversions per column.\n\n### Phase 3 Documentation Template\n\nCreate `analysis/[session-name]/03-standardization-rules.md` with: ./templates/phase-3.md\n\n**CHECKPOINT:** Before proceeding to Phase 4, you MUST have:\n- [ ] Date standardization rules defined for all date columns\n- [ ] Number normalization rules defined for all numeric columns\n- [ ] Whitespace/text rules defined\n- [ ] NULL conversion mapping finalized\n- [ ] `03-standardization-rules.md` created with verification queries\n\n---\n\n## Phase 4: Import Execution\n\n**Goal:** Execute import with standardization rules, verify row count and data integrity.\n\n### Generate CREATE TABLE Statement\n\nFrom Phase 2 schema design, finalize CREATE TABLE statement:\n\n```sql\nCREATE TABLE IF NOT EXISTS raw_[table_name] (\n  [column_1] [TYPE],\n  [column_2] [TYPE],\n  ...\n);\n```\n\n**Execute:**\n```bash\nsqlite3 data/analytics.db < create_table.sql\n```\n\n**Verify table created:**\n```sql\n-- Check table exists\n.tables\n\n-- Inspect schema\nPRAGMA table_info(raw_[table_name]);\n```\n\n**Document:** Paste table creation confirmation and schema output.\n\n### Import CSV with Standardization\n\n**Import method options:**\n\n**Option 1: SQLite `.import` command** (for simple cases):\n```bash\nsqlite3 data/analytics.db <<EOF\n.mode csv\n.import /path/to/file.csv raw_[table_name]\nEOF\n```\n\n**Option 2: Python script** (for complex standardization):\n```python\nimport csv\nimport sqlite3\nfrom datetime import datetime\n\nconn = sqlite3.connect('data/analytics.db')\ncursor = conn.cursor()\n\nwith open('/path/to/file.csv', 'r', encoding='utf-8') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Apply standardization rules from Phase 3\n        row['date_column'] = standardize_date(row['date_column'])\n        row['amount_column'] = standardize_number(row['amount_column'])\n        # ... apply other rules ...\n\n        cursor.execute(\"\"\"\n            INSERT INTO raw_[table_name]\n            ([columns]) VALUES ([placeholders])\n        \"\"\", tuple(row.values()))\n\nconn.commit()\nconn.close()\n```\n\n**Document:** Which method used, any import warnings/errors encountered and resolved.\n\n### Verify Import Success\n\n**Row count verification:**\n```sql\n-- Count rows in table\nSELECT COUNT(*) as row_count FROM raw_[table_name];\n```\n\nCompare to CSV row count from Phase 1. Should match (or CSV count - 1 if CSV had header row).\n\n**Sample data inspection:**\n```sql\n-- View first 5 rows\nSELECT * FROM raw_[table_name] LIMIT 5;\n\n-- View last 5 rows\nSELECT * FROM raw_[table_name]\nORDER BY rowid DESC LIMIT 5;\n```\n\n**Column completeness check:**\n```sql\n-- Check NULL counts per column\nSELECT\n  COUNT(*) as total_rows,\n  COUNT([column_1]) as [column_1]_non_null,\n  COUNT([column_2]) as [column_2]_non_null,\n  ...\nFROM raw_[table_name];\n```\n\n**Document:** Paste actual results showing row counts, sample rows, NULL counts.\n\n### Run Verification Queries from Phase 3\n\nExecute all verification queries defined in `03-standardization-rules.md`:\n- Date format verification\n- Number format verification\n- Whitespace verification\n\n**Expected:** All verification queries return 0 rows (no violations).\n\n**If violations found:** Document count and examples, decide whether to:\n1. Re-import with adjusted rules\n2. Document as acceptable edge cases\n3. Flag for cleaning-data phase\n\n**Document:** Results of all verification queries.\n\n### Phase 4 Documentation Template\n\nCreate `analysis/[session-name]/04-import-log.md` with: ./templates/phase-4.md\n\n**CHECKPOINT:** Before proceeding to Phase 5, you MUST have:\n- [ ] raw_* table created in data/analytics.db\n- [ ] CSV data imported with standardization applied\n- [ ] Row count verified matches CSV (accounting for headers)\n- [ ] Sample data inspected and looks correct\n- [ ] All Phase 3 verification queries executed\n- [ ] `04-import-log.md` created with all results documented\n\n---\n\n## Phase 5: Quality Assessment & Reporting\n\n**Goal:** Systematically detect data quality issues using sub-agent to prevent context pollution, document findings for cleaning-data skill.\n\n**CRITICAL:** This phase MUST use sub-agent delegation. DO NOT analyze data in main agent context.\n\n### Delegate Quality Assessment to Sub-Agent\n\n**Use dedicated quality-assessment agent**\n\nInvoke the `quality-assessment` agent (defined in `.claude/agents/quality-assessment.md`):\n\n```\nTask tool with agent: quality-assessment\nParameters:\n- table_name: raw_[actual_table_name]\n- columns: [list of all columns from schema]\n- numeric_columns: [list of numeric columns for outlier detection]\n- text_columns: [list of text columns for uniqueness analysis]\n```\n\nThe agent will execute all quality checks (NULL analysis, duplicates, outliers, free text) and return structured findings.\n\n**Document agent findings in `05-quality-report.md` using template below.**\n\n### Delegate Foreign Key Detection to Sub-Agent\n\n**If multiple tables exist in the database**, detect foreign key relationships between them.\n\n**Use dedicated detect-foreign-keys agent**\n\nInvoke the `detect-foreign-keys` agent (defined in `.claude/agents/detect-foreign-keys.md`):\n\n```\nTask tool with agent: detect-foreign-keys\nParameters:\n- database_path: data/analytics.db\n- table_names: [list of all raw_* tables in database]\n```\n\n**When to run FK detection:**\n- **Multiple tables imported:** Run to discover relationships\n- **Single table imported:** Skip (no relationships possible), document \"N/A - single table\" in quality report\n\nThe agent will:\n1. Identify FK candidate columns based on naming patterns\n2. Validate candidates with value overlap analysis\n3. Assess cardinality (one-to-one, one-to-many, many-to-many)\n4. Quantify referential integrity violations (orphaned records)\n5. Return structured relationship catalog with join recommendations\n\n**Document FK findings in `05-quality-report.md` using template below.**\n\n### Create Quality Report for cleaning-data\n\nCreate `analysis/[session-name]/05-quality-report.md` with: ./templates/phase-5.md\n\n**CHECKPOINT:** Before concluding importing-data skill, you MUST have:\n- [ ] Sub-agent completed quality assessment (NOT done in main context)\n- [ ] NULL percentages documented for all columns\n- [ ] Duplicates detected and examples captured\n- [ ] Outliers identified in all numeric columns\n- [ ] Free text columns identified for categorization\n- [ ] FK relationships detected (if multiple tables) via detect-foreign-keys sub-agent\n- [ ] Referential integrity assessed with orphaned record counts\n- [ ] `05-quality-report.md` created with all sections filled (including FK relationships)\n- [ ] Quality report ready for cleaning-data skill to consume\n\n---\n\n## Common Rationalizations\n\n### \"The CSV looks clean, I can skip profiling and go straight to import\"\n**Why this is wrong:** Hidden issues (encoding, delimiters, NULL representations) cause import failures or silent data corruption. Even \"clean\" CSVs have edge cases.\n\n**Do instead:** Always complete Phase 1 profiling. Takes 5 minutes and prevents hours of debugging broken imports.\n\n### \"I'll just guess the schema types, they're obvious from the column names\"\n**Why this is wrong:** Column named \"year\" might contain \"2023-Q1\" (TEXT). \"amount\" might have \"$\" symbols. Type inference prevents silent casting failures.\n\n**Do instead:** Complete Phase 2 type inference with sample analysis. Document rationale for each type choice.\n\n### \"Dates look fine, I don't need standardization rules\"\n**Why this is wrong:** Mixed formats (\"01/15/2025\", \"2025-01-15\", \"Jan 15 2025\") break date arithmetic and sorting. Standardization is mandatory.\n\n**Do instead:** Complete Phase 3 with explicit date format conversion rules. Verify with queries after import.\n\n### \"I'll do the import manually, don't need to document it\"\n**Why this is wrong:** Undocumented imports can't be reproduced. When re-importing updated data, you'll forget the transformations applied.\n\n**Do instead:** Complete Phase 4 import log with commands, results, and verification. Future-you will thank present-you.\n\n### \"The data looks good after import, quality assessment is overkill\"\n**Why this is wrong:** Duplicates, outliers, and NULL patterns are invisible without systematic checks. These surface as bugs during analysis.\n\n**Do instead:** ALWAYS complete Phase 5 with sub-agent delegation. Quality report saves time in cleaning-data phase.\n\n### \"I'll run quality checks in the main agent, it's faster than delegating\"\n**Why this is wrong:** Analyzing thousands of rows pollutes main agent context, degrading performance for entire session. Sub-agents prevent this.\n\n**Do instead:** ALWAYS delegate Phase 5 to sub-agent with exact sqlite3 commands provided.\n\n### \"This CSV has 100K rows, I should skip some profiling steps for speed\"\n**Why this is wrong:** Large files are MORE likely to have quality issues, not less. Sampling strategies exist for large files.\n\n**Do instead:** Use head/tail sampling (Phase 1) and query-based profiling (Phase 5 sub-agent). Don't skip phases.\n\n### \"The import succeeded, I don't need to verify row counts\"\n**Why this is wrong:** Silent data loss happens. Header rows miscounted, empty rows skipped, encoding issues truncating data.\n\n**Do instead:** Always verify row count matches (Phase 4). If mismatch, investigate before proceeding.\n\n### \"I'll clean the data during import, don't need separate cleaning-data skill\"\n**Why this is wrong:** Import handles technical standardization (formats). Cleaning handles semantic issues (business rules, categorization). Mixing them creates confusion.\n\n**Do instead:** Keep importing-data focused on standardization. Let cleaning-data handle deduplication, outliers, free text.\n\n### \"Quality report shows no issues, I can skip cleaning-data\"\n**Why this is wrong:** cleaning-data is ALWAYS mandatory per design. Even \"clean\" data needs business rule validation and final verification.\n\n**Do instead:** Proceed to cleaning-data even if quality report shows minimal issues. Document \"no cleaning needed\" if appropriate.\n\n---\n\n## Summary\n\nThis skill ensures systematic, documented CSV import with quality assessment by:\n\n1. **Profiling before importing:** Understand encoding, delimiters, headers, and sample data before designing schema - prevents import failures and silent corruption.\n\n2. **Explicit type inference:** Analyze samples to infer INTEGER/REAL/TEXT types with documented rationale - prevents type casting failures and ensures correct data representation.\n\n3. **Mandatory standardization:** Convert dates to ISO 8601, normalize numbers, clean whitespace, map NULL representations - creates consistent data foundation for analysis.\n\n4. **Verified import execution:** Document CREATE TABLE statements, import methods, row count verification - ensures reproducibility and data integrity.\n\n5. **Systematic quality assessment:** Delegate NULL detection, duplicate finding, outlier identification, and free text discovery to sub-agent - prevents context pollution while ensuring comprehensive quality checks.\n\n6. **Audit trail maintenance:** Create numbered markdown files (01-05) documenting every decision - provides full traceability from raw CSV to raw_* tables.\n\nFollow this process and you'll create clean, well-documented raw_* tables ready for the cleaning-data skill, avoid silent data corruption, and maintain complete audit trail for reproducible imports.\n\n---\n",
        "plugins/datapeeker/skills/importing-data/templates/phase-1.md": "# CSV Profile: [filename]\n\n## File Information\n- **Source:** [absolute path to CSV]\n- **Size:** [file size in MB]\n- **Row count:** [total rows including header]\n- **Encoding:** [detected encoding]\n- **Delimiter:** [character]\n- **Headers:** [Yes/No - list if yes]\n\n## Sample Data\n\n### First 10 Rows\n```\n[paste head output]\n```\n\n### Last 10 Rows\n```\n[paste tail output]\n```\n\n## Column Overview\n\n[For each column detected in headers/first row:]\n### [Column Name or Position]\n- **Sample values (first 10):** [list unique values if reasonable]\n- **Observed types:** [text, numbers, dates, mixed]\n- **NULL indicators:** [empty cells, 'N/A', 'null', etc.]\n\n## Notes\n[Any observations about data quality, unusual patterns, concerns for import]\n\n## Next Steps\nProceed to Phase 2: Schema Design & Type Inference",
        "plugins/datapeeker/skills/importing-data/templates/phase-2.md": "# Schema Design: [dataset_name]\n\n## Objective\nDesign SQLite schema for importing [CSV_filename] based on data profiling from Phase 1.\n\n## Column Type Analysis\n\n### [Column 1 Name]\n- **Sample values:** [examples from 01-csv-profile.md]\n- **Proposed type:** [INTEGER|REAL|TEXT]\n- **Rationale:** [Why this type - reference inference rules above]\n- **NULL handling:** [How NULL values will be handled]\n\n### [Column 2 Name]\n- **Sample values:** [examples]\n- **Proposed type:** [INTEGER|REAL|TEXT]\n- **Rationale:** [Explanation]\n- **NULL handling:** [Approach]\n\n[Continue for all columns...]\n\n## NULL Representation Mapping\n\n| CSV Representation | SQLite Value | Count in Sample |\n|--------------------|--------------|-----------------|\n| (empty cell)       | NULL         | [count]         |\n| \"N/A\"              | NULL         | [count]         |\n| [others]           | NULL         | [count]         |\n\n## Proposed Schema\n\n```sql\nCREATE TABLE raw_[table_name] (\n  [column_1] [TYPE],  -- [Rationale]\n  [column_2] [TYPE],  -- [Rationale]\n  ...\n);\n```\n\n## User Confirmation\n\n- **Proposed table name:** `raw_[name]`\n- **Total columns:** [count]\n- **User approval:** [Date/time of approval]\n- **Modifications requested:** [None / List of changes made]\n\n## Next Steps\nProceed to Phase 3: Basic Standardization\n",
        "plugins/datapeeker/skills/importing-data/templates/phase-3.md": "# Standardization Rules: [dataset_name]\n\n## Objective\nDefine transformation rules to standardize CSV data before import to raw_[table_name].\n\n## Date Standardization\n\n### [Date Column Name]\n- **Source format:** [e.g., \"MM/DD/YYYY\"]\n- **Target format:** YYYY-MM-DD (ISO 8601)\n- **Conversion logic:**\n  ```\n  Parse MM, DD, YYYY components\n  Reformat as YYYY-MM-DD\n  ```\n- **Verification query:**\n  ```sql\n  -- After import, verify all dates are ISO format\n  SELECT [column_name], COUNT(*) as count\n  FROM raw_[table_name]\n  WHERE [column_name] NOT GLOB '[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]'\n    AND [column_name] IS NOT NULL;\n  ```\n\n[Repeat for each date column]\n\n## Number Normalization\n\n### [Numeric Column Name]\n- **Issues found:** [e.g., \"Comma separators, dollar signs\"]\n- **Normalization rules:**\n  - Remove: `$`, `,`\n  - Keep: `.` (decimal point)\n- **Example transformation:** `$1,234.56` â†’ `1234.56`\n- **Verification query:**\n  ```sql\n  -- After import, verify all values are numeric\n  SELECT [column_name], COUNT(*) as count\n  FROM raw_[table_name]\n  WHERE CAST([column_name] AS REAL) IS NULL\n    AND [column_name] IS NOT NULL;\n  ```\n\n[Repeat for each numeric column]\n\n## Whitespace Standardization\n\n**Columns affected:** [List TEXT columns]\n**Rules:**\n- Trim leading/trailing whitespace\n- Collapse multiple internal spaces to single space\n- Normalize line endings to LF (`\\n`)\n\n**Verification query:**\n```sql\n-- Check for leading/trailing whitespace after import\nSELECT [column_name], COUNT(*) as count\nFROM raw_[table_name]\nWHERE [column_name] != TRIM([column_name])\n  AND [column_name] IS NOT NULL;\n```\n\n## NULL Standardization\n\n**Conversions applied:**\n\n| Original Value | Converted To | Affected Columns | Count |\n|----------------|--------------|------------------|-------|\n| (empty)        | NULL         | [All columns]    | [N]   |\n| \"N/A\"          | NULL         | [List]           | [N]   |\n| [Others]       | NULL         | [List]           | [N]   |\n\n## Implementation Notes\n\n[Any special cases, ambiguities resolved, user decisions made]\n\n## Next Steps\nProceed to Phase 4: Import Execution\n",
        "plugins/datapeeker/skills/importing-data/templates/phase-4.md": "# Import Log: [dataset_name]\n\n## Objective\nExecute import of [CSV_filename] to raw_[table_name] with standardization rules applied.\n\n## Table Creation\n\n```sql\nCREATE TABLE IF NOT EXISTS raw_[table_name] (\n  [full schema from Phase 2]\n);\n```\n\n**Execution result:** [Success / Error message]\n\n**Schema verification:**\n```\n[Paste PRAGMA table_info output]\n```\n\n## Import Execution\n\n**Method used:** [SQLite .import / Python script / Other]\n\n**Import command:**\n```bash\n[Exact command or script used]\n```\n\n**Import results:**\n- Start time: [timestamp]\n- End time: [timestamp]\n- Duration: [seconds]\n- Warnings: [Any warnings encountered]\n- Errors: [Any errors and how resolved]\n\n## Import Verification\n\n### Row Count Check\n```sql\nSELECT COUNT(*) as row_count FROM raw_[table_name];\n```\n\n**Result:** [N rows]\n**Expected (from CSV):** [N rows]\n**Status:** âœ“ Match / âœ— Mismatch - [Explanation]\n\n### Sample Data\n```sql\nSELECT * FROM raw_[table_name] LIMIT 5;\n```\n\n**Results:**\n```\n[Paste first 5 rows]\n```\n\n### Column Completeness\n```sql\nSELECT\n  COUNT(*) as total_rows,\n  [column completeness query from above]\nFROM raw_[table_name];\n```\n\n**Results:**\n```\n[Paste counts]\n```\n\n## Standardization Verification\n\n### Date Format Check\n```sql\n[Verification query from 03-standardization-rules.md]\n```\n\n**Result:** [0 rows = success / N rows with issues]\n\n### Number Format Check\n```sql\n[Verification query from 03-standardization-rules.md]\n```\n\n**Result:** [0 rows = success / N rows with issues]\n\n### Whitespace Check\n```sql\n[Verification query from 03-standardization-rules.md]\n```\n\n**Result:** [0 rows = success / N rows with issues]\n\n## Issues Encountered\n\n[Document any issues found during verification and resolution approach]\n\n## Import Summary\n\n- **Status:** âœ“ Success / âš  Success with caveats / âœ— Failed\n- **Table:** `raw_[table_name]`\n- **Rows imported:** [N]\n- **Columns:** [N]\n- **Standardization rules applied:** [Date/Number/Whitespace/NULL]\n- **Verification status:** [All checks passed / Issues documented above]\n\n## Next Steps\nProceed to Phase 5: Quality Assessment & Reporting\n",
        "plugins/datapeeker/skills/importing-data/templates/phase-5.md": "# Quality Assessment Report: [dataset_name]\n\n## Objective\nSystematically detect data quality issues in raw_[table_name] to inform cleaning-data skill.\n\n## Import Summary\n- **Source CSV:** [path to original CSV]\n- **Raw table:** raw_[table_name]\n- **Rows imported:** [count]\n- **Import date:** [timestamp from Phase 4]\n\n## Table Schema\n```\n[Paste PRAGMA table_info output from sub-agent]\n```\n\n## Data Completeness\n\n### NULL Analysis\n\n| Column | Non-NULL Count | NULL Count | NULL % |\n|--------|----------------|------------|--------|\n| [col1] | [count]        | [count]    | [%]    |\n| [col2] | [count]        | [count]    | [%]    |\n| ...    | ...            | ...        | ...    |\n\n**Columns with >10% NULLs (require attention):**\n- [column_name]: [%] NULL - [impact assessment]\n- [column_name]: [%] NULL - [impact assessment]\n\n## Duplicate Detection\n\n**Exact duplicates found:** [count] rows\n\n[If duplicates found:]\n```\n[Paste examples from sub-agent - first 5-10 duplicate groups]\n```\n\n**Assessment:** [Are these true duplicates or expected repeated values?]\n\n## Outlier Detection\n\n### [Numeric Column 1]\n- **Mean:** [value]\n- **MAD:** [value]\n- **Min:** [value]\n- **Max:** [value]\n- **Outliers (>3 MAD):** [count] rows\n\n[Repeat for each numeric column]\n\n**Columns with outliers requiring investigation:**\n- [column_name]: [count] outliers - [min/max values]\n- [column_name]: [count] outliers - [min/max values]\n\n## Free Text Candidates\n\n**Columns with high uniqueness (>50%):**\n\n| Column | Unique Values | Total Values | Uniqueness % | Categorization Priority |\n|--------|---------------|--------------|--------------|-------------------------|\n| [col]  | [count]       | [count]      | [%]          | [High/Medium/Low]       |\n\n**Recommended for categorization:**\n- [column_name]: [unique count] values - [brief description of content]\n\n## Foreign Key Relationships\n\n[If multiple tables exist, include FK detection results. If single table: \"N/A - Single table analysis\"]\n\n### High Confidence Relationships (>95% integrity)\n\n#### [child_table].[fk_column] â†’ [parent_table].[pk_column]\n- **Relationship Type:** [One-to-one / Many-to-one / Many-to-many]\n- **Match Percentage:** [XX.X]%\n- **Cardinality:** Avg [X.X] children per parent (min: [N], max: [N])\n- **NULL FKs:** [count] rows ([X]%)\n- **Orphaned FKs:** [count] rows ([X]%)\n- **Join Recommendation:** [LEFT / INNER] JOIN\n- **Cleaning Priority:** [HIGH / MEDIUM / LOW]\n\n[Repeat for each high-confidence relationship]\n\n### Medium Confidence Relationships (80-95% integrity)\n\n[If any exist:]\n#### [child_table].[fk_column] â†’ [parent_table].[pk_column]\n- **Match Percentage:** [XX.X]%\n- **Orphaned FKs:** [count] rows ([X]%)\n- **Issue:** [Description of integrity concerns]\n- **Recommendation:** [Investigate / Validate / Review before joining]\n\n### Referential Integrity Summary\n\n**Total FK relationships detected:** [count]\n- High confidence: [count]\n- Medium confidence: [count]\n- Low confidence / Unconfirmed: [count]\n\n**Orphaned records across all relationships:** [count] ([X]% of total child records)\n\n**Impact on Analysis:**\n- [count] records will be excluded if using INNER JOINs\n- Recommended approach: Use LEFT JOIN, filter NULLs in WHERE clause if needed\n\n## Additional Quality Concerns\n\n[List any other issues found by sub-agent:]\n- Type inconsistencies\n- Invalid values\n- Date range issues\n- Suspicious patterns\n\n## Summary of Findings\n\n**Critical issues (must address in cleaning-data):**\n1. [Issue description with severity]\n2. [Issue description with severity]\n\n**Non-critical issues (address if time permits):**\n1. [Issue description]\n2. [Issue description]\n\n## Recommended Actions for cleaning-data\n\n1. **Phase 1 (Scope):** Review FK relationships and orphaned records from this report\n2. **Phase 2 (Issue Detection):** Deep-dive investigation of [specific issues including FK orphans]\n3. **Phase 3 (Strategy):** Decide approach for [duplicates/outliers/free text/FK orphans]\n4. **Phase 4 (Execution):** Apply transformations including FK integrity enforcement and create clean_* tables\n\n## Next Steps\nProceed to cleaning-data skill with this quality report as input.\n",
        "plugins/datapeeker/skills/interpreting-results/SKILL.md": "---\nname: interpreting-results\ndescription: Component skill for systematic result interpretation with intellectual honesty in DataPeeker analysis sessions\n---\n\n# Interpreting Results\n\n## Purpose\n\nThis component skill guides rigorous, intellectually honest interpretation of query results. Use it when:\n- Analyzing query outputs to draw conclusions\n- Need to avoid premature or biased interpretations\n- Considering alternative explanations before committing to conclusions\n- Referenced by process skills requiring result interpretation\n\n## Prerequisites\n\n- Query executed and results obtained\n- Query documented with rationale (use `writing-queries` skill)\n- Understanding of data source and quality (use `understanding-data` skill)\n- Analysis context and goals clearly defined\n\n## Result Interpretation Process\n\nCreate a TodoWrite checklist for the 6-step interpretation framework:\n\n```\nPhase 1: Understand Context\nPhase 2: Describe Patterns\nPhase 3: Generate Alternative Explanations\nPhase 4: Assess Significance\nPhase 5: State Conclusions with Caveats\nPhase 6: Identify Follow-up Questions\n```\n\nMark each phase as you complete it. Document all interpretations in numbered markdown files.\n\n---\n\n## Phase 1: Understand Context\n\n**Goal:** Ground interpretation in business and analytical context before looking for patterns.\n\n### Recall Analytical Question\n\nBefore interpreting results, explicitly state:\n\n```markdown\n## Context for Interpretation\n\n**Original Question:** [What we set out to answer]\n\n**Why This Matters:** [Business context and decisions that depend on this answer]\n\n**Hypothesis (if applicable):** [What we expected to find, and why]\n\n**Data Period:** [Time range covered by results]\n\n**Filters Applied:** [Any exclusions or subsets]\n\n**Known Data Limitations:** [Quality issues, missing data, coverage gaps]\n```\n\n### Consider External Factors\n\nAsk yourself:\n\n1. **What was happening during this time period?**\n   - Seasonality (holidays, end-of-quarter, etc.)\n   - Business changes (promotions, product launches, policy changes)\n   - External events (economy, weather, competition)\n\n2. **How might this affect results?**\n   - Example: High December sales might be holiday-driven, not a trend\n   - Example: Spike in refunds might correlate with product recall\n\n3. **What context is needed to interpret these numbers?**\n   - Industry benchmarks\n   - Historical performance\n   - Comparable segments or time periods\n\n**Document:** External factors that might explain or confound results.\n\n### Define Success Criteria\n\nBefore calling results \"good\" or \"bad\", define what you're comparing to:\n\n```markdown\n## Success Criteria\n\n**Comparing to:**\n- Historical baseline: [e.g., \"Q1 2023 had $500K revenue\"]\n- Target/goal: [e.g., \"Target was 10% growth\"]\n- Industry benchmark: [e.g., \"Industry average conversion is 2.5%\"]\n- Control group: [e.g., \"Comparing treatment to control segment\"]\n\n**Threshold for meaningful difference:**\n- [e.g., \"Need >5% difference to be operationally significant\"]\n```\n\n**Don't proceed to pattern identification without context.**\n\n---\n\n## Phase 2: Describe Patterns\n\n**Goal:** Objectively describe what the data shows before explaining why.\n\n### Start with Descriptive Statements\n\nDescribe results using neutral, factual language:\n\n**DO:**\n- \"Category A has 2.3x the revenue of Category B ($450K vs $195K)\"\n- \"Sales declined 15% from January to February (10,234 â†’ 8,699 units)\"\n- \"Day-of-week distribution ranges from 12.8% (Sunday) to 16.2% (Friday)\"\n- \"Top 3 products account for 45% of total revenue\"\n\n**DON'T:**\n- \"Category A is performing well\" (What's the baseline?)\n- \"Sales are dropping\" (Implies negative trend without context)\n- \"Friday is the best day\" (Best by what measure? Compared to what?)\n- \"These products are successful\" (Define success first)\n\n### Identify Pattern Types\n\nCategorize what you observe:\n\n**Magnitude patterns:**\n```markdown\n- Absolute values: [e.g., \"Total revenue: $1.2M\"]\n- Relative comparisons: [e.g., \"Region A is 3x larger than Region B\"]\n- Distributions: [e.g., \"80% of revenue from 20% of customers\"]\n```\n\n**Time patterns:**\n```markdown\n- Trends: [e.g., \"Monthly growth averaged 5% over 6 months\"]\n- Cycles: [e.g., \"Weekly pattern peaks mid-week, dips on weekends\"]\n- Anomalies: [e.g., \"March 15 spike to 3x normal daily volume\"]\n```\n\n**Relationship patterns:**\n```markdown\n- Correlations: [e.g., \"Higher price segments have lower order counts\"]\n- Segments: [e.g., \"B2B customers have 4x higher average order value than B2C\"]\n- Thresholds: [e.g., \"Sharp drop-off in conversion above $100 price point\"]\n```\n\n### Quantify Patterns Precisely\n\nUse specific numbers, not vague terms:\n\n**Vague:** \"Sales increased significantly\"\n**Precise:** \"Sales increased 23% (1,234 â†’ 1,518 units), a gain of 284 units\"\n\n**Vague:** \"Most customers prefer Category A\"\n**Precise:** \"58% of customers (2,340 of 4,034) purchased from Category A\"\n\n**Vague:** \"There's a big difference between segments\"\n**Precise:** \"Segment 1 average: $127, Segment 2 average: $89, difference of $38 (43%)\"\n\n### Check for Data Artifacts\n\nBefore accepting patterns as real, verify they're not artifacts:\n\n```sql\n-- Verify: Is this pattern real or a data issue?\n\n-- Check for row count consistency\nSELECT COUNT(*) FROM results;  -- Does this match expectations?\n\n-- Check for NULL inflation\nSELECT COUNT(*) - COUNT(column_name) as null_count FROM results;\n\n-- Check for duplicate records\nSELECT COUNT(*) as total_rows, COUNT(DISTINCT id) as unique_ids FROM results;\n\n-- Check for incomplete periods\nSELECT MIN(date), MAX(date), COUNT(DISTINCT date) as date_count FROM results;\n```\n\n**Document:** Any data quality issues that might create misleading patterns.\n\n---\n\n## Phase 3: Generate Alternative Explanations\n\n**Goal:** Consider multiple explanations before committing to one.\n\nThis is the most critical phase for intellectual honesty. **Premature explanation is the enemy of good analysis.**\n\n### Brainstorm Alternative Hypotheses\n\nFor each pattern identified, generate at least 3 possible explanations:\n\n```markdown\n## Pattern: Friday has 16.2% of weekly sales vs 12.8% on Sunday\n\n### Possible Explanations:\n\n1. **Consumer behavior:** People shop more on Fridays (payday, preparing for weekend)\n   - Testable: Do other metrics (sessions, conversion rate) also peak Friday?\n\n2. **Business operations:** We run promotions on Fridays\n   - Testable: Check promotion calendar, compare promoted vs non-promoted Fridays\n\n3. **Data artifact:** Incomplete weeks in dataset skew day-of-week calculation\n   - Testable: Count how many of each weekday are in dataset\n\n4. **Seasonality interaction:** Dataset includes holiday weeks where Friday patterns differ\n   - Testable: Split analysis into holiday vs non-holiday weeks\n\n5. **Geographic mix:** Different time zones make \"Friday\" broader (Friday in US, Saturday in Asia)\n   - Testable: Segment by customer timezone if available\n```\n\n### Use the \"Why Might This NOT Be True?\" Test\n\nFor your preferred explanation, actively argue against it:\n\n```markdown\n## Preferred Explanation: Customers shop more on Fridays due to payday\n\n### Why this might be WRONG:\n\n- Many customers have direct deposit on different days\n- Weekend shopping (Sat/Sun) should be higher if it's leisure time\n- No evidence yet that Friday shoppers are paid-on-Friday workers\n- Pattern might be driven by small number of large B2B orders\n- Could be specific to this dataset's time period only\n\n### What would convince me this is RIGHT:\n\n- [ ] Friday pattern consistent across multiple months/quarters\n- [ ] Segmentation shows pattern strongest for consumer (not B2B) purchases\n- [ ] Individual customer purchase history shows Friday preference\n- [ ] Pattern persists after removing outlier large orders\n- [ ] Industry data confirms Friday shopping peak\n```\n\n### Consider Confounding Variables\n\nIdentify factors that might explain the pattern instead of your hypothesis:\n\n**Template:**\n\n```markdown\n## Potential Confounds\n\n1. **[Confound name]:** [How it could explain the pattern]\n   - Test: [How to rule this in/out]\n\nExample:\n\n1. **Marketing send schedule:** Email campaigns go out Thursday, driving Friday purchases\n   - Test: Compare Friday sales on campaign weeks vs non-campaign weeks\n\n2. **Product mix:** High-value products launched mid-dataset period\n   - Test: Segment analysis into before/after product launch\n\n3. **Measurement error:** Weekend orders processed Monday, suppressing Sunday counts\n   - Test: Check order_date vs processed_date, validate weekend entries\n```\n\n### The \"And\" vs \"Or\" Test\n\nDistinguish between:\n- **Exclusive explanations** (A OR B is true, not both)\n- **Contributing factors** (A AND B AND C all contribute)\n\n```markdown\n## Explanation Type\n\nThis pattern is likely caused by:\n- [ ] Single dominant factor (identify the ONE cause)\n- [x] Multiple contributing factors (list all contributors)\n\nIf multiple factors:\n- Factor 1: [Estimated contribution]\n- Factor 2: [Estimated contribution]\n- Factor 3: [Estimated contribution]\n\nTestable: Can we quantify each factor's contribution?\n```\n\n---\n\n## Phase 4: Assess Significance\n\n**Goal:** Determine if patterns are meaningful before acting on them.\n\n### Statistical Significance (Directional)\n\nWhile we can't run formal statistical tests without additional tools, we can reason about significance:\n\n```markdown\n## Significance Assessment\n\n**Sample size:**\n- [e.g., \"Based on 10,234 orders over 90 days\"]\n- [Is this enough data to trust the pattern?]\n\n**Effect size:**\n- [e.g., \"15% difference between segments\"]\n- [Is the difference large enough to matter?]\n\n**Consistency:**\n- [e.g., \"Pattern appears in 8 of 10 months\"]\n- [Is this stable or fluctuating wildly?]\n\n**Variance:**\n- [e.g., \"Group A: 100 Â± 45, Group B: 150 Â± 12\"]\n- [Do ranges overlap significantly?]\n```\n\n### Practical Significance\n\nEven if a pattern is statistically real, is it actionable?\n\n**Questions to ask:**\n\n1. **Is the difference large enough to matter operationally?**\n   ```markdown\n   - Finding: Segment A has 2% higher conversion than Segment B\n   - Volume: Segment B is 50x larger\n   - Practical significance: Optimizing Segment B (even with lower rate) has\n     25x more impact than Segment A\n   - Conclusion: Pattern is real but not the priority\n   ```\n\n2. **Can we actually act on this finding?**\n   ```markdown\n   - Finding: Customers in ZIP codes starting with \"9\" have higher LTV\n   - Actionability: We can't control customer ZIP codes\n   - Practical significance: Low - interesting but not actionable\n   - Alternative: Look for correlated factors we CAN influence\n   ```\n\n3. **What's the cost/benefit of acting?**\n   ```markdown\n   - Finding: 3% revenue increase if we extend hours to 10pm\n   - Cost: Staffing, utilities for extra 2 hours\n   - Benefit: 3% of $1M = $30K annual revenue increase\n   - Margin: Estimated $8K net profit after costs\n   - Assessment: Marginally significant, requires testing\n   ```\n\n### Error Bar Reasoning\n\nWithout formal confidence intervals, reason about uncertainty:\n\n```markdown\n## Uncertainty Assessment\n\n**Data quality confidence:** [High/Medium/Low]\n- [Any known issues with data accuracy?]\n\n**Sample representativeness:** [High/Medium/Low]\n- [Does this sample represent the broader population?]\n- [Any selection bias in how data was collected?]\n\n**Temporal stability:** [High/Medium/Low]\n- [Will this pattern hold next month? Next year?]\n- [Dependent on temporary conditions?]\n\n**Overall confidence in pattern:** [High/Medium/Low]\n- [Considering all factors, how confident are we this is real?]\n```\n\n---\n\n## Phase 5: State Conclusions with Caveats\n\n**Goal:** Make clear, hedged claims that accurately represent certainty level.\n\n### Use Appropriate Hedging Language\n\nMatch your language to your confidence level:\n\n**High confidence:**\n- \"The data clearly shows...\"\n- \"We can conclude that...\"\n- \"This pattern is consistent and robust...\"\n\n**Medium confidence:**\n- \"The data suggests...\"\n- \"This pattern appears to indicate...\"\n- \"The most likely explanation is...\"\n\n**Low confidence:**\n- \"There is weak evidence that...\"\n- \"One possible interpretation is...\"\n- \"We observe a pattern, but it could be explained by...\"\n\n**Inappropriate hedging:**\n- Avoid: \"The data proves...\" (data doesn't prove, it provides evidence)\n- Avoid: \"Obviously this means...\" (if it's obvious, you don't need data)\n- Avoid: \"This clearly demonstrates...\" (unless confidence is truly high)\n\n### Separate Observation from Inference\n\nStructure conclusions to distinguish facts from interpretations:\n\n```markdown\n## Conclusions\n\n### What We Observed (Facts)\n- Friday sales averaged 16.2% of weekly total (vs 14.3% expected if uniform)\n- This pattern appeared in 11 of 12 months studied\n- Friday average order value ($127) is similar to weekly average ($124)\n- Friday transaction count is 18% higher than Sunday (2,340 vs 1,980)\n\n### What We Infer (Interpretations)\n- Friday traffic increase (not AOV increase) drives higher sales\n- Pattern is stable across most months (except December outlier)\n- This is likely a behavioral pattern, not a pricing or promotion effect\n\n### Confidence Level: Medium-High\n- Strong evidence for Friday traffic pattern\n- Insufficient data on WHY (customer motivation unclear)\n- Need to rule out confounds (marketing calendar, staffing changes)\n```\n\n### Explicitly State Limitations\n\nEvery conclusion should include what you DON'T know:\n\n```markdown\n## Caveats and Limitations\n\n**What this analysis does NOT tell us:**\n- [e.g., \"We don't know if Friday shoppers are different people or same\n   people shopping more frequently\"]\n- [e.g., \"We can't determine causation from this correlation\"]\n- [e.g., \"This dataset doesn't include abandoned carts, only completed purchases\"]\n\n**Assumptions we made:**\n- [e.g., \"Assumed all timestamps are in local timezone\"]\n- [e.g., \"Treated returns as separate from original purchase date\"]\n\n**Data quality concerns:**\n- [e.g., \"First two weeks of January had incomplete data\"]\n- [e.g., \"Product category field was NULL for 5% of records\"]\n\n**Generalizability limits:**\n- [e.g., \"This analysis covers only online sales, not in-store\"]\n- [e.g., \"Time period includes major pandemic shifts, may not represent normal behavior\"]\n```\n\n### The \"So What?\" Test\n\nForce yourself to state the implications clearly:\n\n```markdown\n## Implications\n\n**For decision-makers:**\n- [What should they DO differently based on this finding?]\n- [What should they STOP doing?]\n- [What remains uncertain that needs more investigation?]\n\nExample:\n\n**For marketing team:**\n- Consider scheduling campaigns for Thursday delivery (to catch Friday traffic)\n- Don't assume Friday success will translate to other days\n- Test: Run A/B test with campaign timing to validate causal relationship\n\n**For ops team:**\n- Current Friday staffing appears adequate (no degradation in service metrics)\n- Monitor: Watch for Friday capacity constraints as volume grows\n\n**For analytics team:**\n- Investigate: Why do customers shop more on Fridays?\n- Build: Day-of-week forecasting model to improve inventory planning\n```\n\n---\n\n## Phase 6: Identify Follow-up Questions\n\n**Goal:** Turn conclusions into next analytical steps.\n\n### Generate \"Next-Level\" Questions\n\nGood analysis creates more questions than it answers:\n\n```markdown\n## Follow-up Questions\n\n### Questions to deepen understanding:\n1. [Question that drills into WHY pattern exists]\n   - Data needed: [What data would answer this?]\n   - Query approach: [How would we query for this?]\n\n2. [Question that tests alternative explanation]\n   - Data needed: [...]\n   - Query approach: [...]\n\n### Questions to test generalizability:\n3. [Does this pattern hold in different segments?]\n   - Segment by: [Customer type, product category, region, etc.]\n\n4. [Is this pattern stable over time?]\n   - Test: [Earlier time periods, recent vs historical]\n\n### Questions to assess actionability:\n5. [Can we influence this pattern?]\n   - Experiment: [What intervention could we test?]\n\n6. [What's the ROI of acting on this finding?]\n   - Calculate: [Revenue impact, cost, net benefit]\n```\n\n### Prioritize Follow-up Questions\n\nNot all questions are equally valuable:\n\n```markdown\n## Question Prioritization\n\n**High Priority (Do Next):**\n- [Questions that directly inform pending decisions]\n- [Questions that could refute our main conclusion]\n- [Questions that are cheap/fast to answer with existing data]\n\n**Medium Priority (Do Eventually):**\n- [Questions that deepen understanding but don't change decisions]\n- [Questions that require additional data collection]\n\n**Low Priority (Backlog):**\n- [Interesting but not actionable]\n- [Questions that would take significant effort for marginal insight]\n```\n\n### Design Follow-up Analyses\n\nFor high-priority questions, sketch the analysis:\n\n```markdown\n## Proposed Follow-up Analysis\n\n**Question:** Does Friday pattern vary by customer segment?\n\n**Hypothesis:** Business customers drive Friday peak (ordering for next week)\n\n**Data needed:**\n- Customer segment field (B2B vs B2C)\n- Order data with day-of-week already calculated\n\n**Query approach:**\n```sql\n-- Compare day-of-week patterns by segment\nSELECT\n  customer_segment,\n  day_of_week,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (PARTITION BY customer_segment), 2) as pct_of_segment\nFROM orders_with_dow\nGROUP BY customer_segment, day_of_week\nORDER BY customer_segment, day_of_week;\n```\n\n**Expected outcome:**\n- If hypothesis correct: B2B will show stronger Friday peak than B2C\n- If hypothesis wrong: Pattern will be similar across segments\n- Alternative: Entirely different day-of-week pattern by segment\n\n**Decision impact:**\n- If B2B-driven: Focus Friday capacity on B2B fulfillment\n- If B2C-driven: Focus Friday capacity on consumer experience\n```\n\n---\n\n## Documentation Requirements\n\nAfter completing all 6 phases, create an interpretation summary:\n\n```markdown\n## Result Interpretation Summary\n\n### Context\n- Question: [What we set out to answer]\n- Data: [What we analyzed]\n- Time period: [Coverage]\n\n### Key Findings\n1. [Finding 1 with supporting numbers]\n2. [Finding 2 with supporting numbers]\n3. [Finding 3 with supporting numbers]\n\n### Interpretation\n[2-3 paragraph narrative explaining what findings mean]\n\n### Confidence Assessment\n- Overall confidence: [High/Medium/Low]\n- Key uncertainties: [What remains unknown]\n- Supporting evidence: [What makes us confident]\n- Contradicting evidence: [What makes us uncertain]\n\n### Caveats\n- [Limitation 1]\n- [Limitation 2]\n- [Limitation 3]\n\n### Recommendations\n1. [Actionable recommendation with rationale]\n2. [Actionable recommendation with rationale]\n3. [Further investigation needed]\n\n### Follow-up Questions\n- High priority: [Question 1]\n- High priority: [Question 2]\n- Medium priority: [Question 3]\n```\n\n---\n\n## Common Interpretation Pitfalls\n\n### Pitfall 1: Confirmation Bias\n\n**Problem:** Seeing what you expect to see, ignoring contradictory evidence.\n\n**Example:**\n- Hypothesis: \"Campaign increased sales\"\n- Finding: Sales up 10% during campaign\n- Bias: Concluding campaign worked without checking control group\n- Missed: Sales were up 12% overall that week (campaign underperformed!)\n\n**Prevention:**\n- Always generate alternative explanations (Phase 3)\n- Actively look for evidence against your hypothesis\n- Use comparison groups (before/after, treatment/control)\n\n### Pitfall 2: Correlation â‰  Causation\n\n**Problem:** Assuming that because A and B move together, A causes B.\n\n**Example:**\n- Finding: \"Higher-priced products have lower return rates\"\n- Causal claim: \"Raising prices will reduce returns\" (WRONG!)\n- Reality: Quality drives both price and returns (confound)\n\n**Prevention:**\n- Distinguish correlation from causation in language\n- Identify potential confounding variables\n- Design experiments to test causation (not just observe correlation)\n\n### Pitfall 3: Cherry-Picking\n\n**Problem:** Highlighting patterns that support your story, hiding those that don't.\n\n**Example:**\n- Finding: Segment A had 20% growth, Segment B had 5% growth, Segment C declined 8%\n- Cherry-pick: \"We're seeing strong growth in key segments!\" (only mention A & B)\n- Honest: \"Mixed results: growth in 2 of 3 segments, overall trend uncertain\"\n\n**Prevention:**\n- Report all segments, not just interesting ones\n- Include negative/null findings\n- Predefine what you'll measure before looking at data\n\n### Pitfall 4: Texas Sharpshooter Fallacy\n\n**Problem:** Finding patterns in noise, then creating explanations post-hoc.\n\n**Example:**\n- Finding: \"Sales spike every 3rd Tuesday when temperature is above 75Â°F\"\n- Reality: Random noise, but you found a pattern in 3 instances\n- Test: Does pattern predict NEXT 3rd Tuesday above 75Â°F? (Probably not)\n\n**Prevention:**\n- Test patterns on hold-out data (future time period)\n- Ask: \"Would I have predicted this pattern before seeing data?\"\n- Calculate: How many patterns did I check? (Multiple comparisons problem)\n\n### Pitfall 5: Ignoring Base Rates\n\n**Problem:** Misinterpreting percentages without considering absolute numbers.\n\n**Example:**\n- Finding: \"New product line has 50% higher conversion rate!\" (2% vs 3%)\n- Ignored: New line has 1/100th the traffic of main line\n- Reality: Absolute impact is tiny (20 vs 5,000 conversions)\n\n**Prevention:**\n- Report both percentages and absolute numbers\n- Consider volume when assessing significance\n- Calculate absolute impact, not just relative change\n\n### Pitfall 6: Simpson's Paradox\n\n**Problem:** Aggregate trends that reverse when data is segmented.\n\n**Example:**\n- Aggregate: Treatment group has worse outcomes than control\n- Segmented: Treatment is better in EVERY segment\n- Cause: Treatment group had more severe cases (confound)\n\n**Prevention:**\n- Segment data by key dimensions\n- Check if aggregate pattern holds within segments\n- Identify potential confounding factors\n\n### Pitfall 7: Survivorship Bias\n\n**Problem:** Analyzing only data that \"survived\" to be recorded, missing the full picture.\n\n**Example:**\n- Finding: \"Our top customers have 80% retention rate!\"\n- Missed: You only analyzed customers who made it to \"top\" status\n- Reality: 95% of customers churned before becoming \"top\"\n\n**Prevention:**\n- Define population carefully (all customers vs top customers)\n- Analyze attrition/dropout before looking at survivors\n- Include \"failed\" cases in analysis\n\n---\n\n## When to Revisit Interpretation\n\nRe-run portions of this skill when:\n- New data becomes available (test if conclusions hold)\n- Stakeholders challenge your conclusions (strengthen with alternative explanations)\n- You're about to make a major decision based on findings (verify confidence level)\n- Follow-up analyses contradict initial findings (update interpretation)\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill with:\n\n```markdown\nUse the `interpreting-results` component skill to systematically interpret query outputs, ensuring intellectual honesty and avoiding premature conclusions.\n```\n\nThis ensures analysts:\n1. Consider context before interpreting patterns\n2. Describe patterns objectively before explaining them\n3. Generate multiple alternative explanations\n4. Assess both statistical and practical significance\n5. State conclusions with appropriate caveats\n6. Identify valuable follow-up questions\n\nRigorous interpretation is the difference between data analysis and data-driven storytelling.\n",
        "plugins/datapeeker/skills/marketing-experimentation/SKILL.md": "---\nname: marketing-experimentation\ndescription: Systematic marketing experimentation process - discover concepts, generate hypotheses, coordinate multiple experiments, synthesize results, generate next-iteration ideas through rigorous validation cycles\n---\n\n# Marketing Experimentation\n\n## Overview\n\nUse this skill when you need to validate marketing concepts or business ideas through rigorous experimental cycles. This skill orchestrates the complete Build-Measure-Learn cycle from concept to data-driven signal.\n\n**When to use this skill:**\n- You have a marketing concept or business idea that needs validation\n- You want to test multiple related hypotheses systematically\n- You need to integrate results across multiple experiments\n- You're designing the next iteration based on experimental evidence\n- You're conducting qualitative market research combined with quantitative testing\n\n**What this skill does:**\n- Validates concepts through market research before experimentation\n- Generates multiple testable hypotheses from marketing ideas\n- Coordinates multiple experiments: quantitative (hypothesis-testing) and qualitative (qualitative-research)\n- Synthesizes results across experiments using interpreting-results and creating-visualizations\n- Produces clear signals (positive/negative/null/mixed) for each campaign\n- Generates actionable next-iteration ideas based on experimental evidence\n\n**What this skill does NOT do:**\n- Design individual experiments (delegates to hypothesis-testing or qualitative-research)\n- Execute statistical analysis directly (uses hypothesis-testing for quantitative rigor)\n- Conduct interviews/surveys/observations directly (uses qualitative-research for qualitative rigor)\n- Operationalize successful ideas (focuses on validation, not scaling)\n- Platform-specific implementation (tool-agnostic techniques only)\n\n**Integration with existing skills:**\n- **Delegates to `hypothesis-testing`** for quantitative experiment design and execution (metrics, A/B tests, statistical analysis)\n- **Delegates to `qualitative-research`** for qualitative experiment design and execution (interviews, surveys, focus groups, observations)\n- **Uses `interpreting-results`** to synthesize findings across multiple experiments\n- **Uses `creating-visualizations`** to communicate aggregate results\n- **Invokes `market-researcher` agent** for concept validation via internet research\n\n**Multi-conversation persistence:**\nThis skill is designed for campaigns spanning days or weeks. Each phase documents completely enough that new conversations can resume after extended breaks. The experiment tracker (04-experiment-tracker.md) serves as the living coordination hub.\n\n## Prerequisites\n\n**Required skills:**\n- `hypothesis-testing` - Quantitative experiment design and execution (invoked for metric-based experiments)\n- `qualitative-research` - Qualitative experiment design and execution (invoked for interviews, surveys, focus groups, observations)\n- `interpreting-results` - Result synthesis and pattern identification (invoked in Phase 5)\n- `creating-visualizations` - Aggregate result visualization (invoked in Phase 5)\n\n**Required agents:**\n- `market-researcher` - Concept validation via internet research (invoked in Phase 1)\n\n**Required knowledge:**\n- Understanding of Lean Startup Build-Measure-Learn cycle\n- Familiarity with marketing tactics (landing pages, ads, email, content)\n- Basic experimental design principles (control/treatment, signals, metrics)\n- Understanding of qualitative vs quantitative research methods\n\n**Data requirements:**\n- None initially (market research is qualitative)\n- Data requirements emerge from experiment design in Phase 4\n- Quantitative experiments (hypothesis-testing): SQL databases, analytics data, A/B test results\n- Qualitative experiments (qualitative-research): Interview transcripts, survey responses, observation notes\n\n## Mandatory Process Structure\n\n**CRITICAL:** This is a 6-phase process skill. You MUST complete all phases in order. Use TodoWrite to track progress through each phase.\n\n**TodoWrite template:**\n\nWhen starting a marketing-experimentation session, create these todos:\n\n```markdown\n- [ ] Phase 1: Discovery & Asset Inventory\n- [ ] Phase 2: Hypothesis Generation\n- [ ] Phase 3: Prioritization\n- [ ] Phase 4: Experiment Coordination\n- [ ] Phase 5: Cross-Experiment Synthesis\n- [ ] Phase 6: Iteration Planning\n```\n\n**Workspace structure:**\n\nAll work for a marketing-experimentation session is saved to:\n```\nanalysis/marketing-experimentation/[campaign-name]/\nâ”œâ”€â”€ 01-discovery.md\nâ”œâ”€â”€ 02-hypothesis-generation.md\nâ”œâ”€â”€ 03-prioritization.md\nâ”œâ”€â”€ 04-experiment-tracker.md\nâ”œâ”€â”€ 05-synthesis.md\nâ”œâ”€â”€ 06-iteration-plan.md\nâ””â”€â”€ experiments/\n    â”œâ”€â”€ [experiment-1]/               # hypothesis-testing session\n    â”œâ”€â”€ [experiment-2]/               # hypothesis-testing session\n    â””â”€â”€ [experiment-3]/               # hypothesis-testing session\n```\n\n**Phase progression rules:**\n1. Each phase has a CHECKPOINT with verification requirements\n2. You MUST satisfy all checkpoint requirements before proceeding to the next phase\n3. Document every decision with rationale in the numbered markdown files\n4. Commit markdown files after each phase completes\n5. The experiment tracker (04-experiment-tracker.md) is a LIVING DOCUMENT - update it throughout Phase 4\n\n**Multi-conversation resumption:**\n- At the start of any conversation, check if an experiment tracker exists for this campaign\n- If it exists, read it first to understand current experiment status\n- Update the tracker as experiments progress\n- All phases should be complete enough to resume after days or weeks\n\n---\n\n## Phase 1: Discovery & Asset Inventory\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Gathered business concept description from user\n- [ ] Invoked market-researcher agent and documented findings\n- [ ] Completed asset inventory (content, campaigns, audiences, data)\n- [ ] Defined success criteria and validation signals\n- [ ] Documented known constraints\n- [ ] Saved to `01-discovery.md`\n\n### Instructions\n\n1. **Gather the business concept**\n   - Ask user to describe the marketing concept or business idea to validate\n   - What problem does it solve? Who is the target audience?\n   - What's the desired outcome? (awareness, leads, conversions, etc.)\n   - What stage is this idea at? (new concept, existing campaign, iteration)\n\n2. **Invoke market-researcher agent for concept validation**\n\n   Dispatch the `market-researcher` agent with the concept description:\n   - Agent will research market demand signals\n   - Agent will identify similar solutions and competitors\n   - Agent will analyze audience needs and pain points\n   - Agent will find validation evidence (case studies, reviews, testimonials)\n\n   Document agent findings in `01-discovery.md` under \"Market Research Findings\"\n\n3. **Conduct asset inventory**\n\n   Work with user to inventory existing assets that could be leveraged:\n\n   **Content Assets:**\n   - Blog posts, case studies, whitepapers\n   - Video content, webinars, tutorials\n   - Social media presence and following\n   - Email lists and subscriber segments\n\n   **Campaign Assets:**\n   - Existing ad campaigns and performance data\n   - Landing pages and conversion rates\n   - Email campaigns and open/click rates\n   - SEO performance and keyword rankings\n\n   **Audience Assets:**\n   - Customer segments and personas\n   - Audience data (demographics, behaviors, preferences)\n   - Customer feedback and reviews\n   - Support tickets and common questions\n\n   **Data Assets:**\n   - Analytics platforms (Google Analytics, Mixpanel, etc.)\n   - CRM data (Salesforce, HubSpot, etc.)\n   - Ad platform data (Google Ads, Facebook Ads, etc.)\n   - Email platform data (Mailchimp, SendGrid, etc.)\n\n4. **Define success criteria and validation signals**\n\n   Work with user to define:\n   - What metrics indicate success? (CTR, conversion rate, CAC, LTV, etc.)\n   - What magnitude of change is meaningful? (practical significance thresholds)\n   - What signal types are acceptable?\n     - Positive: Validates concept, proceed to scale\n     - Negative: Invalidates concept, pivot or abandon\n     - Null: Inconclusive, needs refinement or more data\n     - Mixed: Some aspects work, some don't, iterate strategically\n\n5. **Document known constraints**\n\n   Capture any constraints that will affect experimentation:\n   - Budget constraints (ad spend limits, tool costs)\n   - Time constraints (launch deadlines, seasonal factors)\n   - Resource constraints (team capacity, content production)\n   - Technical constraints (platform limitations, integration issues)\n   - Regulatory constraints (GDPR, CCPA, industry regulations)\n\n6. **Create `01-discovery.md`** with: `./templates/01-discovery.md`\n\n7. **STOP and get user confirmation**\n   - Review discovery findings with user\n   - Confirm asset inventory is complete\n   - Confirm success criteria are appropriate\n   - Do NOT proceed to Phase 2 until confirmed\n\n**Common Rationalization:** \"I'll skip discovery and go straight to testing - the concept is obvious\"\n**Reality:** Discovery surfaces assumptions, constraints, and existing assets that dramatically affect experiment design. Always start with discovery.\n\n**Common Rationalization:** \"I don't need market research - I already know this market\"\n**Reality:** The market-researcher agent provides current, data-driven validation signals that prevent building experiments around false assumptions. Always validate.\n\n**Common Rationalization:** \"Asset inventory is busywork - I'll figure out what's available as I go\"\n**Reality:** Existing assets can dramatically reduce experiment cost and time. Inventorying first prevents reinventing wheels and enables building on proven foundations.\n\n---\n\n## Phase 2: Hypothesis Generation\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Generated 5-10 testable hypotheses from the concept\n- [ ] Each hypothesis maps to a specific tactic/channel\n- [ ] Each hypothesis has expected outcome and rationale\n- [ ] Hypotheses cover multiple tactics (not all ads or all email)\n- [ ] Referenced relevant frameworks (Lean Startup, AARRR, ICE/RICE)\n- [ ] Saved to `02-hypothesis-generation.md`\n\n### Instructions\n\n1. **Generate 5-10 testable hypotheses**\n\n   For each hypothesis, use this format:\n\n   **Hypothesis [N]: [Brief statement]**\n   - **Tactic/Channel:** [landing page | ad campaign | email sequence | content marketing | social media | SEO | etc.]\n   - **Expected Outcome:** [Specific, measurable result]\n   - **Rationale:** [Why we believe this will work based on discovery findings]\n   - **Variables to Test:** [What will we manipulate/measure]\n\n   **Example hypothesis:**\n\n   **Hypothesis 1: Value proposition clarity drives conversion**\n   - **Tactic/Channel:** Landing page A/B test\n   - **Expected Outcome:** 15%+ increase in conversion rate from landing page variant with simplified value proposition\n   - **Rationale:** Market research showed audience confusion about product benefits. Discovery found existing landing page has 8 different value propositions competing for attention.\n   - **Variables to Test:** Headline clarity, benefit hierarchy, CTA prominence\n\n2. **Ensure tactic coverage**\n\n   Verify hypotheses cover multiple marketing tactics:\n\n   **Acquisition Tactics:**\n   - Landing pages (conversion optimization, value prop testing, layout)\n   - Ad campaigns (targeting, creative, messaging, platforms)\n   - Content marketing (blog posts, videos, webinars, lead magnets)\n   - SEO (keyword targeting, content optimization, technical SEO)\n\n   **Activation Tactics:**\n   - Email sequences (onboarding, nurture, activation)\n   - Product tours (in-app guidance, feature discovery)\n   - Social proof (testimonials, case studies, reviews)\n\n   **Retention Tactics:**\n   - Email campaigns (engagement, re-activation, upsell)\n   - Content (newsletters, educational content, community)\n\n   Don't generate 10 ad hypotheses. Aim for diversity across tactics.\n\n3. **Reference experimentation frameworks**\n\n   **Lean Startup Build-Measure-Learn:**\n   - Build: What's the minimum viable test? (landing page, ad, email, etc.)\n   - Measure: What metrics indicate success/failure?\n   - Learn: What will we learn regardless of outcome?\n\n   **AARRR Pirate Metrics:**\n   - Acquisition: How do users find us?\n   - Activation: Do they have a great first experience?\n   - Retention: Do they come back?\n   - Referral: Do they tell others?\n   - Revenue: Do they pay?\n\n   Map each hypothesis to one or more AARRR stages.\n\n   **ICE/RICE Prioritization (used in Phase 3):**\n   - Impact: How much will this move the metric?\n   - Confidence: How sure are we this will work?\n   - Ease: How easy is this to implement?\n   - Reach: How many users will this affect? (RICE only)\n\n4. **Create `02-hypothesis-generation.md`** with: `./templates/02-hypothesis-generation.md`\n\n5. **STOP and get user confirmation**\n   - Review all hypotheses with user\n   - Confirm hypotheses are testable and meaningful\n   - Confirm tactic coverage is appropriate\n   - Do NOT proceed to Phase 3 until confirmed\n\n**Common Rationalization:** \"I'll generate hypotheses as I build experiments - more efficient\"\n**Reality:** Generating hypotheses before prioritization enables strategic selection of highest-impact tests. Generating ad-hoc leads to testing whatever's easiest, not what matters most.\n\n**Common Rationalization:** \"I'll focus all hypotheses on one tactic (ads) since that's what we know\"\n**Reality:** Tactic diversity reveals which channels work for this concept. Single-tactic testing creates blind spots and missed opportunities.\n\n**Common Rationalization:** \"I'll write vague hypotheses and refine them during experiment design\"\n**Reality:** Vague hypotheses lead to vague experiments that produce vague results. Specific hypotheses with expected outcomes enable clear signal detection.\n\n**Common Rationalization:** \"More hypotheses = better coverage, I'll generate 20+\"\n**Reality:** Too many hypotheses dilute focus and create analysis paralysis in prioritization. 5-10 high-quality hypotheses enable strategic selection of 2-4 tests.\n\n---\n\n## Phase 3: Prioritization\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Scored all hypotheses using ICE or RICE framework with computational method\n- [ ] Created prioritized backlog (highest to lowest score) using Python script\n- [ ] Selected 2-4 highest-priority hypotheses for testing\n- [ ] Documented prioritization rationale\n- [ ] Identified any dependencies or sequencing requirements\n- [ ] Saved to `03-prioritization.md`\n\n### Instructions\n\n**CRITICAL:** You MUST use computational methods (Python scripts) to calculate scores. Do NOT estimate or manually calculate scores.\n\n1. **Choose prioritization framework**\n\n   **ICE Framework** (simpler, faster):\n   - **Impact:** How much will this move the success metric? (1-10 scale)\n   - **Confidence:** How confident are we this will work? (1-10 scale)\n   - **Ease:** How easy is this to implement? (1-10 scale)\n   - **Score:** (Impact Ã— Confidence) / Ease\n\n   **RICE Framework** (more comprehensive):\n   - **Reach:** How many users will this affect? (absolute number or percentage)\n   - **Impact:** How much will this move the metric per user? (1-10 scale: 0.25=minimal, 3=massive)\n   - **Confidence:** How confident are we in our estimates? (percentage: 50%, 80%, 100%)\n   - **Effort:** Person-weeks to implement (absolute number)\n   - **Score:** (Reach Ã— Impact Ã— Confidence) / Effort\n\n   Choose ICE for speed, RICE for precision when reach varies significantly.\n\n2. **Score each hypothesis using Python script**\n\n   **For ICE Framework:**\n\n   Create a Python script to compute and sort ICE scores:\n\n   ```python\n   #!/usr/bin/env python3\n   \"\"\"\n   ICE Score Calculator for Marketing Experimentation\n\n   Computes ICE scores: (Impact Ã— Confidence) / Ease\n   Sorts hypotheses by score (highest to lowest)\n   \"\"\"\n\n   hypotheses = [\n       {\n           \"id\": \"H1\",\n           \"name\": \"Value proposition clarity drives conversion\",\n           \"impact\": 8,\n           \"confidence\": 7,\n           \"ease\": 9\n       },\n       {\n           \"id\": \"H2\",\n           \"name\": \"Ad targeting refinement\",\n           \"impact\": 7,\n           \"confidence\": 6,\n           \"ease\": 5\n       },\n       {\n           \"id\": \"H3\",\n           \"name\": \"Email sequence optimization\",\n           \"impact\": 6,\n           \"confidence\": 8,\n           \"ease\": 8\n       },\n       {\n           \"id\": \"H4\",\n           \"name\": \"Content marketing expansion\",\n           \"impact\": 5,\n           \"confidence\": 4,\n           \"ease\": 3\n       },\n   ]\n\n   # Calculate ICE scores\n   for h in hypotheses:\n       h['ice_score'] = (h['impact'] * h['confidence']) / h['ease']\n\n   # Sort by ICE score (descending)\n   sorted_hypotheses = sorted(hypotheses, key=lambda x: x['ice_score'], reverse=True)\n\n   # Print results table\n   print(\"| Hypothesis | Impact | Confidence | Ease | ICE Score | Rank |\")\n   print(\"|------------|--------|------------|------|-----------|------|\")\n   for rank, h in enumerate(sorted_hypotheses, 1):\n       print(f\"| {h['id']}: {h['name'][:30]} | {h['impact']} | {h['confidence']} | {h['ease']} | {h['ice_score']:.2f} | {rank} |\")\n   ```\n\n   **Usage:**\n   ```bash\n   python3 ice_calculator.py\n   ```\n\n   **For RICE Framework:**\n\n   Create a Python script to compute and sort RICE scores:\n\n   ```python\n   #!/usr/bin/env python3\n   \"\"\"\n   RICE Score Calculator for Marketing Experimentation\n\n   Computes RICE scores: (Reach Ã— Impact Ã— Confidence) / Effort\n   Sorts hypotheses by score (highest to lowest)\n   \"\"\"\n\n   hypotheses = [\n       {\n           \"id\": \"H1\",\n           \"name\": \"Value proposition clarity drives conversion\",\n           \"reach\": 10000,        # users affected\n           \"impact\": 3,           # 0.25=minimal, 1=low, 2=medium, 3=high, 5=massive\n           \"confidence\": 80,      # percentage (50, 80, 100)\n           \"effort\": 2            # person-weeks\n       },\n       {\n           \"id\": \"H2\",\n           \"name\": \"Ad targeting refinement\",\n           \"reach\": 50000,\n           \"impact\": 1,\n           \"confidence\": 50,\n           \"effort\": 4\n       },\n       {\n           \"id\": \"H3\",\n           \"name\": \"Email sequence optimization\",\n           \"reach\": 5000,\n           \"impact\": 2,\n           \"confidence\": 80,\n           \"effort\": 3\n       },\n       {\n           \"id\": \"H4\",\n           \"name\": \"Content marketing expansion\",\n           \"reach\": 20000,\n           \"impact\": 1,\n           \"confidence\": 50,\n           \"effort\": 8\n       },\n   ]\n\n   # Calculate RICE scores\n   for h in hypotheses:\n       # Convert confidence percentage to decimal\n       confidence_decimal = h['confidence'] / 100\n       h['rice_score'] = (h['reach'] * h['impact'] * confidence_decimal) / h['effort']\n\n   # Sort by RICE score (descending)\n   sorted_hypotheses = sorted(hypotheses, key=lambda x: x['rice_score'], reverse=True)\n\n   # Print results table\n   print(\"| Hypothesis | Reach | Impact | Confidence | Effort | RICE Score | Rank |\")\n   print(\"|------------|-------|--------|------------|--------|------------|------|\")\n   for rank, h in enumerate(sorted_hypotheses, 1):\n       print(f\"| {h['id']}: {h['name'][:30]} | {h['reach']} | {h['impact']} | {h['confidence']}% | {h['effort']}w | {h['rice_score']:.2f} | {rank} |\")\n   ```\n\n   **Usage:**\n   ```bash\n   python3 rice_calculator.py\n   ```\n\n   **Scoring Guidance:**\n\n   **Impact (1-10 for ICE, 0.25-5 for RICE):**\n   - ICE: 1-3 minimal, 4-6 moderate, 7-8 significant, 9-10 transformative\n   - RICE: 0.25 minimal, 1 low, 2 medium, 3 high, 5 massive\n\n   **Confidence (1-10 for ICE, 50-100% for RICE):**\n   - ICE: 1-3 speculative, 4-6 uncertain, 7-8 likely, 9-10 validated\n   - RICE: 50% low confidence, 80% high confidence, 100% certainty\n\n   **Ease (1-10 for ICE):**\n   - 1-3: Complex, significant resources\n   - 4-6: Moderate effort, some obstacles\n   - 7-8: Straightforward, few dependencies\n   - 9-10: Trivial, immediate execution\n\n   **Effort (person-weeks for RICE):**\n   - Estimate total person-weeks required\n   - Include design, implementation, monitoring time\n   - Examples: 1w (simple landing page), 4w (complex ad campaign), 8w (content series)\n\n3. **Run scoring script and document results**\n\n   1. Create the Python script (ice_calculator.py or rice_calculator.py)\n   2. Update the `hypotheses` list with actual hypothesis data from Phase 2\n   3. Run the script: `python3 [ice|rice]_calculator.py`\n   4. Copy the output table into `03-prioritization.md`\n   5. Include the script in the markdown file for reproducibility:\n\n   ```markdown\n   ## Prioritization Calculation\n\n   **Method:** ICE Framework\n\n   **Calculation Script:**\n   ```python\n   [paste full script here]\n   ```\n\n   **Results:**\n\n   [paste output table here]\n   ```\n\n4. **Select 2-4 highest-priority hypotheses**\n\n   Considerations for selection:\n   - **Don't test everything:** Focus on highest-scoring 2-4 hypotheses\n   - **Resource constraints:** Match selection to available time/budget/capacity\n   - **Learning value:** Sometimes lower-scoring hypothesis with high uncertainty is worth testing\n   - **Dependencies:** Test prerequisites before dependent hypotheses\n   - **Sequencing:** Consider whether experiments need to run sequentially or can run in parallel\n\n   **Selection criteria:**\n   - Primary: Top 2-4 by ICE/RICE score from computational results\n   - Secondary: Balance quick wins vs. high-impact long-term bets\n   - Tertiary: Ensure tactic diversity (don't test 3 ad variants if other tactics untested)\n\n5. **Document experiment sequence**\n\n   Determine execution strategy:\n   - **Parallel:** Multiple experiments running simultaneously (faster results, higher resource needs)\n   - **Sequential:** One experiment at a time (slower, easier to manage)\n   - **Hybrid:** Run independent experiments in parallel, sequence dependent ones\n\n   **Example sequence plan:**\n   ```\n   Week 1-2: Launch H1 (landing page) and H3 (email) in parallel\n   Week 3-4: Analyze H1 and H3 results\n   Week 5-6: Launch H2 (ads) based on H1 learnings\n   Week 7-8: Analyze H2 results\n   ```\n\n6. **Create `03-prioritization.md`** with: `./templates/03-prioritization.md`\n\n7. **STOP and get user confirmation**\n   - Review computed scores and prioritization with user\n   - Confirm selected hypotheses are appropriate\n   - Confirm experiment sequence is feasible\n   - Do NOT proceed to Phase 4 until confirmed\n\n**Common Rationalization:** \"I'll test all hypotheses - don't want to miss opportunities\"\n**Reality:** Resource constraints make testing everything impossible. Prioritization ensures highest-value experiments get resources. Unfocused testing produces weak signals across too many fronts.\n\n**Common Rationalization:** \"Scoring is subjective and arbitrary - I'll just pick what feels right\"\n**Reality:** Scoring frameworks force explicit reasoning about trade-offs. \"Feels right\" selections optimize for recency bias and personal preference, not business value. Computational methods ensure consistency.\n\n**Common Rationalization:** \"I'll skip prioritization and go straight to easiest test\"\n**Reality:** Easiest test rarely equals highest value. Prioritization prevents optimizing for ease at the expense of impact.\n\n**Common Rationalization:** \"I'll estimate scores mentally instead of running the script\"\n**Reality:** Manual estimation introduces calculation errors and inconsistency. Python scripts ensure exact, reproducible results that can be audited and verified.\n\n---\n\n## Phase 4: Experiment Coordination\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Created experiment tracker with all selected hypotheses\n- [ ] Determined experiment type for each hypothesis (Quantitative or Qualitative)\n- [ ] Invoked appropriate skill for each hypothesis (hypothesis-testing OR qualitative-research)\n- [ ] Updated tracker with experiment status (Planned, In Progress, Complete)\n- [ ] Documented location of each experiment session\n- [ ] Saved experiment tracker to `04-experiment-tracker.md`\n- [ ] Note: This phase may span multiple days/weeks and conversations\n\n### Instructions\n\n**CRITICAL:** This phase is designed for multi-conversation workflows. The experiment tracker is a LIVING DOCUMENT that you will update throughout experimentation. New conversations should ALWAYS read this file first.\n\n1. **Determine experiment type for each hypothesis**\n\n   **CRITICAL:** Before creating the tracker, classify each hypothesis as Quantitative or Qualitative.\n\n   **Quantitative experiments** use hypothesis-testing skill:\n   - Measure **numeric metrics**: CTR, conversion rate, bounce rate, time on page, revenue, CAC, LTV, etc.\n   - Rely on **existing data sources**: Google Analytics, ad platforms, CRM, email platforms, database queries\n   - Test using **A/B tests, multivariate tests, or time-series analysis**\n   - Require **statistical significance testing**\n   - **Examples:**\n     - Landing page A/B test measuring conversion rate\n     - Ad campaign comparing CTR across different creatives\n     - Email sequence measuring open rate and click-through rate\n     - SEO experiment tracking organic traffic changes\n\n   **Qualitative experiments** use qualitative-research skill:\n   - Gather **non-numeric insights**: opinions, experiences, needs, pain points, motivations\n   - Collect through **interviews, surveys, focus groups, or observations**\n   - Analyze using **thematic analysis** rather than statistical tests\n   - Focus on **understanding why** behaviors occur\n   - **Examples:**\n     - Customer discovery interviews to understand pain points\n     - Open-ended survey asking about product needs\n     - Focus group discussing ad creative perceptions\n     - Observational study of how users interact with product\n\n   **Decision criteria:**\n\n   Ask: \"What do we need to learn?\"\n   - If answer is \"Does X increase metric Y by Z%?\" â†’ **Quantitative** (hypothesis-testing)\n   - If answer is \"Why do users do X?\" or \"What do users think about Y?\" â†’ **Qualitative** (qualitative-research)\n\n   Ask: \"What data will we collect?\"\n   - If answer is \"Metrics from analytics\" â†’ **Quantitative** (hypothesis-testing)\n   - If answer is \"Interview transcripts, survey responses, or observation notes\" â†’ **Qualitative** (qualitative-research)\n\n   **Mixed methods:**\n   - Some hypotheses may require BOTH quantitative and qualitative experiments\n   - Example: \"Value prop clarity drives conversion\" could test:\n     - Quantitatively: A/B test landing page, measure conversion rate (hypothesis-testing)\n     - Qualitatively: Interview users about which value prop resonates (qualitative-research)\n   - Track these as separate experiments with linked hypotheses in the tracker\n\n2. **Create experiment tracker**\n\n   The tracker is your coordination hub for managing multiple experiments over time.\n\n   Create `04-experiment-tracker.md` with: `./templates/04-experiment-tracker.md`\n\n   **Tracker format:**\n\n   For each selected hypothesis, create an entry:\n\n   ```markdown\n   ### Experiment 1: [Hypothesis Brief Name]\n\n   **Status:** [Planned | In Progress | Complete]\n   **Hypothesis:** [Full hypothesis statement from Phase 2]\n   **Tactic/Channel:** [landing page | ads | email | etc.]\n   **Priority Score:** [ICE/RICE score from Phase 3]\n   **Start Date:** [YYYY-MM-DD or \"Not started\"]\n   **Completion Date:** [YYYY-MM-DD or \"In progress\"]\n   **Location:** `analysis/marketing-experimentation/[campaign-name]/experiments/[experiment-name]/`\n   **Signal:** [Positive | Negative | Null | Mixed | \"Not analyzed\"]\n   **Key Findings:** [Brief summary when complete, \"TBD\" otherwise]\n   ```\n\n2. **Invoke appropriate skill for each experiment**\n\n   For each hypothesis marked \"Planned\" or \"In Progress\":\n\n   **Step 1:** Read the hypothesis details from `02-hypothesis-generation.md`\n\n   **Step 2a:** If **Quantitative experiment**, invoke `hypothesis-testing` skill:\n   ```markdown\n   Use hypothesis-testing skill to test: [Hypothesis statement]\n\n   Context for hypothesis-testing:\n   - Session name: [descriptive-name-for-experiment]\n   - Save location: analysis/marketing-experimentation/[campaign-name]/experiments/[experiment-name]/\n   - Success criteria: [From Phase 1 discovery]\n   - Expected outcome: [From Phase 2 hypothesis]\n   - Metric to measure: [CTR, conversion rate, etc.]\n   - Data source: [Google Analytics, database, etc.]\n   ```\n\n   **Step 2b:** If **Qualitative experiment**, invoke `qualitative-research` skill:\n   ```markdown\n   Use qualitative-research skill to conduct: [Hypothesis statement]\n\n   Context for qualitative-research:\n   - Session name: [descriptive-name-for-experiment]\n   - Save location: analysis/marketing-experimentation/[campaign-name]/experiments/[experiment-name]/\n   - Research question: [From Phase 2 hypothesis]\n   - Collection method: [Interviews | Surveys | Focus Groups | Observations]\n   - Success criteria: [What insights validate/invalidate hypothesis?]\n   ```\n\n   **Step 3:** Update experiment tracker:\n   - Change status from \"Planned\" to \"In Progress\"\n   - Add start date\n   - Update location with actual path\n   - Document experiment type (Quantitative or Qualitative)\n\n   **Step 4a:** Let **hypothesis-testing** skill complete its 5-phase workflow:\n   - Phase 1: Hypothesis Formulation\n   - Phase 2: Test Design\n   - Phase 3: Data Analysis\n   - Phase 4: Statistical Interpretation\n   - Phase 5: Conclusion\n\n   **Step 4b:** Let **qualitative-research** skill complete its 6-phase workflow:\n   - Phase 1: Research Design\n   - Phase 2: Data Collection\n   - Phase 3: Data Familiarization\n   - Phase 4: Systematic Coding\n   - Phase 5: Theme Development\n   - Phase 6: Synthesis & Reporting\n\n   **Step 5:** When skill completes, update tracker:\n   - Change status to \"Complete\"\n   - Add completion date\n   - Document signal (Positive/Negative/Null/Mixed)\n   - Summarize key findings\n\n   **Step 6:** Commit tracker updates after each status change\n\n3. **Handle multi-conversation resumption**\n\n   **At the start of EVERY conversation during Phase 4:**\n\n   1. Check if `04-experiment-tracker.md` exists\n   2. If it exists, READ IT FIRST before doing anything else\n   3. Review experiment status:\n      - Planned: Ready to launch\n      - In Progress: Check hypothesis-testing session for current phase\n      - Complete: Ready for synthesis (Phase 5)\n   4. Ask user which experiment to continue or which new experiment to launch\n   5. Update tracker with new status/dates/findings\n   6. Commit tracker updates\n\n   **Example resumption:**\n   ```markdown\n   I've read the experiment tracker. Current status:\n   - Experiment 1 (H1: Value prop): Complete, Positive signal\n   - Experiment 2 (H3: Email sequence): In Progress, currently in hypothesis-testing Phase 3\n   - Experiment 3 (H2: Ad targeting): Planned, not yet started\n\n   What would you like to do?\n   a) Continue Experiment 2 (in hypothesis-testing Phase 3)\n   b) Start Experiment 3\n   c) Move to synthesis (Phase 5) since Experiment 1 is complete\n   ```\n\n4. **Coordinate parallel vs. sequential experiments**\n\n   **Parallel execution (multiple experiments simultaneously):**\n   - Launch multiple hypothesis-testing sessions\n   - Track each separately in experiment tracker\n   - Update tracker as each progresses independently\n   - Requires managing multiple analysis directories\n\n   **Sequential execution (one at a time):**\n   - Complete one experiment fully before starting next\n   - Simpler tracking, easier to manage\n   - Can incorporate learnings between experiments\n\n   **Hybrid execution:**\n   - Run independent experiments in parallel\n   - Sequence dependent experiments (e.g., H2 depends on H1 insights)\n\n5. **Progress through all experiments**\n\n   Continue invoking hypothesis-testing and updating the tracker until:\n   - All selected experiments have status \"Complete\"\n   - All experiments have documented signals\n   - All findings are summarized in tracker\n\n   Only when ALL experiments are complete should you proceed to Phase 5.\n\n**Common Rationalization:** \"I'll keep experiment details in my head - the tracker is just busywork\"\n**Reality:** Multi-day campaigns lose context between conversations. The tracker is the ONLY source of truth that persists across sessions. Without it, you'll re-ask questions and lose progress.\n\n**Common Rationalization:** \"I'll wait until all experiments finish before updating the tracker\"\n**Reality:** Batch updates create opportunity for lost data. Update the tracker IMMEDIATELY after status changes. Real-time tracking prevents confusion and missed experiments.\n\n**Common Rationalization:** \"I'll design the experiment myself instead of using hypothesis-testing\"\n**Reality:** hypothesis-testing skill provides rigorous experimental design, statistical analysis, and signal detection. Skipping it produces weak experiments with ambiguous results.\n\n**Common Rationalization:** \"All experiments are done, I don't need to update the tracker before synthesis\"\n**Reality:** The tracker is your input to Phase 5. Incomplete tracker means incomplete synthesis. Update ALL fields (status, dates, signals, findings) before proceeding.\n\n---\n\n## Phase 5: Cross-Experiment Synthesis\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] ALL experiments from Phase 4 marked \"Complete\" with signals documented\n- [ ] Created aggregate results table across all experiments\n- [ ] Invoked presenting-data skill to synthesize findings with visualizations\n- [ ] Documented what worked, what didn't, and what's unclear\n- [ ] Classified overall campaign signal (Positive/Negative/Null/Mixed)\n- [ ] Saved to `05-synthesis.md`\n\n### Instructions\n\n**CRITICAL:** This phase synthesizes results ACROSS multiple experiments. Do NOT proceed until ALL Phase 4 experiments are complete with documented signals.\n\n1. **Verify experiment completion**\n\n   Read `04-experiment-tracker.md` and verify:\n   - All experiments have Status = \"Complete\"\n   - All experiments have Signal documented (Positive/Negative/Null/Mixed)\n   - All experiments have Key Findings summarized\n\n   If any experiments are incomplete, return to Phase 4 to finish them.\n\n2. **Create aggregate results table**\n\n   Compile findings from all experiments into a summary table:\n\n   **Example Aggregate Table (Mixed Quantitative & Qualitative):**\n\n   | Experiment | Type | Hypothesis | Tactic | Signal | Key Finding | Confidence |\n   |------------|------|------------|--------|--------|-------------|------------|\n   | E1 | Quant | Value prop clarity | Landing page A/B test | Positive | Conversion rate +18% (p<0.05) | High |\n   | E2 | Qual | Customer pain points | Discovery interviews | Positive | 8 of 10 cited onboarding complexity | High |\n   | E3 | Quant | Ad targeting | Ads | Null | CTR +2% (not sig., p=0.12) | Medium |\n   | E4 | Qual | Ad message resonance | Focus groups | Negative | 6 of 8 found messaging confusing | High |\n\n   **For Quantitative experiments (hypothesis-testing):**\n   - Signal classification (Positive/Negative/Null/Mixed)\n   - Key metric measured (CTR, conversion rate, etc.)\n   - Magnitude of effect with statistical significance (e.g., \"+18%, p<0.05\")\n   - Confidence level from statistical analysis\n\n   **For Qualitative experiments (qualitative-research):**\n   - Signal classification (Positive/Negative/Null/Mixed)\n   - Key themes identified with prevalence (e.g., \"8 of 10 participants mentioned X\")\n   - Representative quotes or patterns\n   - Confidence assessment (credibility, dependability, transferability)\n\n3. **Invoke presenting-data skill for comprehensive synthesis**\n\n   Use the `presenting-data` skill to create complete synthesis with visualizations and presentation materials:\n\n   ```markdown\n   Use presenting-data skill to synthesize marketing experimentation results:\n\n   Context:\n   - Campaign: [campaign name]\n   - Experiments completed: [count]\n   - Results table: [paste aggregate table]\n   - Audience: [stakeholders/decision-makers]\n   - Format: [markdown report | slides | whitepaper]\n   - Focus: Pattern identification across experiments (what works, what doesn't, what's unclear)\n   ```\n\n   **presenting-data skill will handle:**\n   - Pattern identification (using interpreting-results internally)\n   - Visualization creation (using creating-visualizations internally)\n   - Synthesis documentation (markdown, slides, or whitepaper format)\n   - Citation of sources (individual hypothesis-testing sessions)\n   - Reproducibility (references to experiment locations)\n\n   **Focus areas for synthesis:**\n   - **What worked:** Experiments with Positive signals (both quantitative and qualitative)\n   - **What didn't work:** Experiments with Negative signals (both quantitative and qualitative)\n   - **What's unclear:** Experiments with Null or Mixed signals\n   - **Cross-experiment patterns:** Do results cluster by tactic? By audience? By timing? Do quantitative and qualitative findings align or conflict?\n   - **Triangulation:** Do qualitative findings explain quantitative results? (e.g., interviews reveal WHY conversion rate increased)\n   - **Confounding factors:** Are there external factors affecting multiple experiments?\n   - **Confidence assessment:** Which findings are robust? Which are uncertain? How do qualitative and quantitative confidence levels compare?\n\n4. **Document patterns and insights**\n\n   The presenting-data skill will create `05-synthesis.md` (or slides/whitepaper) with:\n\n   **What Worked (Positive Signals):**\n   - List experiments with positive results\n   - Explain WHY these worked (based on analysis)\n   - Identify commonalities across successful experiments\n\n   **What Didn't Work (Negative Signals):**\n   - List experiments with negative results\n   - Explain WHY these failed (based on analysis)\n   - Identify lessons learned\n\n   **What's Unclear (Null/Mixed Signals):**\n   - List experiments with inconclusive results\n   - Explain potential reasons (insufficient power, confounding factors, etc.)\n   - Identify what additional investigation is needed\n\n   **Cross-Experiment Patterns:**\n   - Do results cluster by tactic, audience, timing, or other factors?\n   - Are there confounding variables affecting multiple experiments?\n   - What overarching insights emerge?\n\n   **Visualizations (created by presenting-data):**\n   - Signal distribution (bar chart: Positive/Negative/Null/Mixed counts)\n   - Effect sizes (bar chart: metric changes by experiment)\n   - Confidence levels (scatter plot: effect size vs. confidence)\n   - Tactic performance (grouped by channel/tactic)\n\n5. **Classify overall campaign signal**\n\n   Based on aggregate analysis (from presenting-data output), classify the campaign:\n\n   **Positive:** Campaign validates concept, proceed to scaling\n   - Multiple experiments show positive signals\n   - Successful tactics identified for scale-up\n   - Clear path to ROI improvement\n\n   **Negative:** Campaign invalidates concept, pivot or abandon\n   - Multiple experiments show negative signals\n   - No successful tactics identified\n   - Concept doesn't resonate with audience\n\n   **Null:** Campaign results inconclusive, needs refinement\n   - Most experiments show null signals\n   - Insufficient power or confounding factors\n   - Needs redesigned experiments or longer observation\n\n   **Mixed:** Some aspects work, some don't, iterate strategically\n   - Mix of positive and negative signals across experiments\n   - Some tactics work, others don't\n   - Selective scaling + pivots needed\n\n6. **Review presenting-data output and finalize synthesis**\n\n   After presenting-data skill completes:\n   - Review generated synthesis document\n   - Verify all experiments are covered\n   - Confirm visualizations are appropriate\n   - Ensure signal classification is documented\n   - Make any necessary edits for clarity\n\n7. **STOP and get user confirmation**\n   - Review synthesis findings with user\n   - Confirm pattern interpretations are accurate\n   - Confirm overall signal classification is appropriate\n   - Do NOT proceed to Phase 6 until confirmed\n\n**Common Rationalization:** \"I'll synthesize results mentally - no need to document patterns\"\n**Reality:** Mental synthesis loses details and creates false confidence. Documented synthesis with presenting-data skill ensures intellectual honesty and identifies confounding factors you'd otherwise miss.\n\n**Common Rationalization:** \"I'll skip synthesis for experiments with clear signals\"\n**Reality:** Individual experiment signals don't reveal cross-experiment patterns. Synthesis identifies why some tactics work while others don't - the strategic insight that guides iteration.\n\n**Common Rationalization:** \"Visualization is optional - the data speaks for itself\"\n**Reality:** Tabular data obscures patterns. Visualization reveals signal distribution, effect size clusters, and confidence patterns that inform strategic decisions. presenting-data handles this systematically.\n\n---\n\n## Phase 6: Iteration Planning\n\n**CHECKPOINT:** Before proceeding, you MUST have:\n- [ ] Generated 3-7 new experiment ideas based on synthesis findings\n- [ ] Categorized ideas (scale winners, investigate nulls, pivot from failures, explore new)\n- [ ] Documented campaign-level signal and strategic recommendation\n- [ ] Explained feed-forward pattern (ideas â†’ new marketing-experimentation sessions)\n- [ ] Saved to `06-iteration-plan.md`\n- [ ] Campaign complete - ready for next cycle or conclusion\n\n### Instructions\n\n**CRITICAL:** Phase 6 generates experiment IDEAS, NOT hypotheses. Ideas feed into new marketing-experimentation sessions where Phase 2 formalizes hypotheses. Do NOT skip the discovery and hypothesis generation steps.\n\n1. **Generate 3-7 new experiment ideas**\n\n   Based on Phase 5 synthesis, generate ideas for next iteration:\n\n   **Idea Format:**\n\n   **Idea [N]: [Brief descriptive name]**\n   - **Rationale:** [Why this idea based on current findings]\n   - **Expected Learning:** [What we'll learn from testing this]\n   - **Category:** [Scale Winners | Investigate Nulls | Pivot from Failures | Explore New]\n\n   **Example Ideas:**\n\n   **Idea 1: Scale value prop landing page to paid ads**\n   - **Rationale:** E1 showed +18% conversion from simplified value prop. Apply winning message to ad creative.\n   - **Expected Learning:** Does simplified value prop improve ad CTR and cost-per-conversion?\n   - **Category:** Scale Winners\n\n   **Idea 2: Investigate email sequence timing sensitivity**\n   - **Rationale:** E3 showed negative result for email sequence, but timing may be a confound (sent during holidays).\n   - **Expected Learning:** Is the email sequence inherently weak, or was timing the issue?\n   - **Category:** Investigate Nulls\n\n   **Idea 3: Pivot from broad ad targeting to lookalike audiences**\n   - **Rationale:** E2 showed null result for ad targeting. Broad targeting may dilute signal. Pivot to lookalike audiences based on E1 converters.\n   - **Expected Learning:** Do lookalike audiences outperform broad targeting?\n   - **Category:** Pivot from Failures\n\n2. **Categorize ideas by strategy**\n\n   **Scale Winners:**\n   - Double down on successful tactics\n   - Apply winning patterns to new channels\n   - Increase budget/effort on validated approaches\n   - Examples: Winning landing page â†’ ads, winning ad â†’ email, winning message â†’ content\n\n   **Investigate Nulls:**\n   - Redesign experiments with null/mixed results\n   - Address confounding factors identified in synthesis\n   - Increase statistical power (larger sample, longer duration)\n   - Examples: Retest with better timing, retest with clearer treatment, retest with focused audience\n\n   **Pivot from Failures:**\n   - Abandon unsuccessful approaches\n   - Try alternative tactics for same goal\n   - Apply learnings to avoid similar failures\n   - Examples: Broad targeting failed â†’ try lookalike, generic message failed â†’ try personalization\n\n   **Explore New:**\n   - Test entirely new tactics not in original hypothesis set\n   - Investigate new audience segments\n   - Explore new channels or platforms\n   - Examples: Add referral program, test new platform, try new content format\n\n3. **Document campaign-level signal and strategic recommendation**\n\n   **Campaign Summary:**\n   - **Overall Signal:** [Positive | Negative | Null | Mixed]\n   - **Key Wins:** [List successful tactics]\n   - **Key Learnings:** [List insights regardless of signal]\n   - **Strategic Recommendation:** [What to do next]\n\n   **Strategic Recommendations by Signal:**\n\n   **Positive Signal:**\n   - Proceed to scaling: Increase budget/effort on winning tactics\n   - Optimize: Refine winning approaches for incremental gains\n   - Expand: Apply winning patterns to new channels/audiences\n\n   **Negative Signal:**\n   - Pivot: Major change in approach, audience, or value proposition\n   - Pause: Reassess concept before additional investment\n   - Abandon: Consider alternative concepts if no viable path forward\n\n   **Null Signal:**\n   - Refine: Redesign experiments with better power/clarity\n   - Investigate: Address confounding factors before new tests\n   - Extend: Continue observation period if time-dependent\n\n   **Mixed Signal:**\n   - Selective scaling: Double down on winners\n   - Selective pivots: Abandon or redesign losers\n   - Strategic iteration: Focus next tests on promising areas\n\n4. **Explain feed-forward pattern**\n\n   **CRITICAL:** Ideas from Phase 6 are NOT ready for testing. They MUST go through a new marketing-experimentation session:\n\n   **Feed-Forward Cycle:**\n   ```\n   Phase 6 generates IDEAS\n          â†“\n   Start new marketing-experimentation session with idea\n          â†“\n   Phase 1: Discovery (validate idea with market-researcher)\n          â†“\n   Phase 2: Hypothesis Generation (formalize idea into testable hypotheses)\n          â†“\n   Phase 3-6: Complete full experimental cycle\n   ```\n\n   **Why this matters:**\n   - Ideas need market validation (Phase 1) before testing\n   - Ideas need formalization into specific hypotheses (Phase 2)\n   - Skipping discovery leads to untested assumptions\n   - Skipping hypothesis generation leads to vague experiments\n\n   **Example:**\n   Phase 6 generates \"Scale value prop to ads\" (idea)\n   â†’ New session Phase 1: Market research on ad platform best practices\n   â†’ New session Phase 2: Generate hypotheses like \"H1: Simplified value prop in ad headline increases CTR by 10%+\" (specific, testable)\n   â†’ Continue with Phase 3-6 to test formal hypotheses\n\n5. **Create `06-iteration-plan.md`** with: `./templates/06-iteration-plan.md`\n\n6. **STOP and review with user**\n   - Review all iteration ideas with user\n   - Confirm strategic recommendation is appropriate\n   - Confirm understanding of feed-forward cycle\n   - Discuss whether to start new marketing-experimentation session or conclude campaign\n\n**Common Rationalization:** \"I'll turn ideas directly into experiments - skip the new session\"\n**Reality:** Ideas need discovery and hypothesis generation. Skipping these steps leads to untested assumptions and vague experiments. Always run ideas through a new marketing-experimentation session.\n\n**Common Rationalization:** \"I'll generate hypotheses in Phase 6 for efficiency\"\n**Reality:** Phase 6 generates IDEAS, Phase 2 (in a new session) generates hypotheses. Conflating these skips critical validation and formalization steps. Ideas â†’ new session â†’ hypotheses.\n\n**Common Rationalization:** \"Campaign signal is obvious from results, no need to document strategic recommendation\"\n**Reality:** Documented recommendation provides clear guidance for stakeholders and future sessions. Without it, insights are lost and decisions become ad-hoc.\n\n---\n\n## Common Rationalizations\n\nThese are rationalizations that lead to failure. When you catch yourself thinking any of these, STOP and follow the skill process instead.\n\n### \"I'll skip discovery and just start testing - the concept is obvious\"\n\n**Why this fails:** Discovery surfaces assumptions, constraints, and existing assets that dramatically affect experiment design. \"Obvious\" concepts often hide critical assumptions that need validation.\n\n**Reality:** Market-researcher agent provides current, data-driven validation signals. Asset inventory reveals resources that reduce experiment cost and time. Success criteria definition prevents ambiguous results. Always start with discovery.\n\n**What to do instead:** Complete Phase 1 (Discovery & Asset Inventory) before generating hypotheses. Invoke market-researcher agent. Document all findings.\n\n---\n\n### \"I'll design the experiment myself instead of using hypothesis-testing or qualitative-research\"\n\n**Why this fails:** The research skills provide rigorous experimental design, analysis, and signal detection. hypothesis-testing ensures statistical rigor for quantitative experiments. qualitative-research ensures systematic rigor for qualitative experiments. Skipping them produces weak experiments with ambiguous results.\n\n**Reality:** Marketing-experimentation is a meta-orchestrator that coordinates multiple experiments. It does NOT design experiments itself. Delegation to appropriate skills (hypothesis-testing or qualitative-research) ensures methodological rigor.\n\n**What to do instead:** Determine experiment type (quantitative or qualitative) in Phase 4. Invoke hypothesis-testing skill for quantitative experiments. Invoke qualitative-research skill for qualitative experiments. Let the appropriate skill handle all design, execution, and analysis.\n\n---\n\n### \"One experiment is enough to draw conclusions\"\n\n**Why this fails:** Single experiments miss cross-experiment patterns. Some tactics work, others don't. Single-experiment campaigns can't identify which channels/tactics are most effective.\n\n**Reality:** Marketing-experimentation tests 2-4 hypotheses to reveal strategic insights. Synthesis (Phase 5) identifies patterns across experiments - which tactics work, which don't, and why.\n\n**What to do instead:** Follow Phase 3 prioritization to select 2-4 hypotheses. Complete all experiments before synthesis. Use Phase 5 to identify patterns.\n\n---\n\n### \"I'll wait until all experiments complete before updating the tracker\"\n\n**Why this fails:** Batch updates create opportunity for lost data. Multi-day campaigns lose context between conversations. Incomplete tracker leads to missed experiments and confusion.\n\n**Reality:** The experiment tracker (04-experiment-tracker.md) is the ONLY source of truth that persists across sessions. Update it IMMEDIATELY after status changes.\n\n**What to do instead:** Update tracker after every status change (Planned â†’ In Progress, In Progress â†’ Complete). Commit tracker updates to git. Read tracker FIRST in every new conversation.\n\n---\n\n### \"Results are obvious, I don't need to document synthesis\"\n\n**Why this fails:** Individual experiment signals don't reveal cross-experiment patterns. \"Obvious\" interpretations miss confounding factors and alternative explanations.\n\n**Reality:** Documented synthesis with presenting-data skill ensures intellectual honesty. Visualization reveals patterns. Statistical assessment identifies robust vs uncertain findings.\n\n**What to do instead:** Always complete Phase 5 (Cross-Experiment Synthesis). Invoke presenting-data skill. Document patterns, visualizations, and signal classification. Get user confirmation.\n\n---\n\n### \"I'll form hypotheses in Phase 6 for efficiency\"\n\n**Why this fails:** Phase 6 generates IDEAS, not hypotheses. Ideas need discovery (Phase 1) and hypothesis generation (Phase 2) in new sessions. Skipping these steps leads to untested assumptions and vague experiments.\n\n**Reality:** Feed-forward cycle: Phase 6 ideas â†’ new marketing-experimentation session â†’ Phase 1 discovery â†’ Phase 2 hypothesis generation â†’ Phase 3-6 complete cycle.\n\n**What to do instead:** Generate IDEAS in Phase 6. Start NEW marketing-experimentation session with selected idea. Complete Phase 1 and Phase 2 to formalize idea into testable hypotheses.\n\n---\n\n### \"I'll estimate ICE/RICE scores mentally instead of running the script\"\n\n**Why this fails:** Manual estimation introduces calculation errors and inconsistency. Mental math is unreliable for multiplication and division.\n\n**Reality:** Python scripts ensure exact, reproducible results that can be audited and verified. Computational methods eliminate human error.\n\n**What to do instead:** Use Python scripts (ICE or RICE calculator) from Phase 3 instructions. Update hypothesis data in script. Run script and document exact scores. Copy output table to prioritization document.\n\n---\n\n### \"I'll synthesize results mentally - no need to use presenting-data\"\n\n**Why this fails:** Mental synthesis loses details and creates false confidence. Cross-experiment patterns require systematic analysis.\n\n**Reality:** presenting-data skill handles pattern identification (via interpreting-results), visualization creation (via creating-visualizations), and synthesis documentation. It ensures intellectual honesty and reproducibility.\n\n**What to do instead:** Always invoke presenting-data skill in Phase 5. Provide aggregate results table. Request pattern analysis and visualizations. Document all findings from presenting-data output.\n\n---\n\n## Summary\n\nThe marketing-experimentation skill ensures rigorous, evidence-based validation of marketing concepts through structured experimental cycles. This skill orchestrates the complete Build-Measure-Learn loop from concept to data-driven signal.\n\n**What this skill ensures:**\n\n1. **Validated concepts through market research** - market-researcher agent provides current demand signals, competitive landscape analysis, and audience insights before experimentation begins.\n\n2. **Strategic hypothesis generation** - 5-10 testable hypotheses spanning multiple tactics (landing pages, ads, email, content) grounded in discovery findings and mapped to experimentation frameworks (Lean Startup, AARRR).\n\n3. **Data-driven prioritization** - Computational methods (ICE/RICE Python scripts) ensure exact, reproducible scoring. Selection of 2-4 highest-value hypotheses optimizes resource allocation.\n\n4. **Multi-experiment coordination** - Experiment tracker (living document) enables multi-conversation workflows spanning days or weeks. Status tracking (Planned, In Progress, Complete) maintains visibility across all experiments. Supports both quantitative and qualitative experiment types.\n\n5. **Methodological rigor through delegation** - hypothesis-testing skill handles quantitative experiment design (statistical analysis, A/B tests, metrics). qualitative-research skill handles qualitative experiment design (interviews, surveys, focus groups, observations, thematic analysis). Marketing-experimentation coordinates multiple tests without duplicating methodology.\n\n6. **Cross-experiment synthesis** - presenting-data skill identifies patterns across experiments (what works, what doesn't, what's unclear). Aggregate analysis reveals strategic insights invisible in single experiments.\n\n7. **Clear signal generation** - Campaign-level classification (Positive/Negative/Null/Mixed) with strategic recommendations (Scale/Pivot/Refine/Pause) provides actionable guidance for stakeholders.\n\n8. **Systematic iteration** - Phase 6 generates experiment IDEAS (not hypotheses) that feed into new marketing-experimentation sessions. Feed-forward cycle maintains rigor through repeated discovery and hypothesis generation.\n\n9. **Multi-conversation persistence** - Complete documentation at every phase enables resumption after days or weeks. Experiment tracker serves as coordination hub. All artifacts are git-committable.\n\n10. **Tool-agnostic approach** - Focuses on techniques (value proposition testing, targeting strategies, sequence optimization) rather than specific platforms. Applicable across marketing tools and channels.\n\n**Key principles:**\n- Discovery before experimentation (Phase 1 always first)\n- Hypothesis generation separate from idea generation (Phase 2 vs Phase 6)\n- Multiple experiments for pattern identification (2-4 minimum)\n- Computational scoring for objectivity (Python scripts)\n- Delegation for methodological rigor (hypothesis-testing for quantitative, qualitative-research for qualitative)\n- Mixed-methods integration (quantitative metrics + qualitative insights for complete picture)\n- Synthesis for strategic insight (presenting-data skill handles both quantitative and qualitative results)\n- Documentation for reproducibility (numbered markdown files, git commits)\n- Iteration through validated cycles (ideas â†’ new sessions â†’ discovery â†’ hypotheses)\n",
        "plugins/datapeeker/skills/marketing-experimentation/templates/01-discovery.md": "# Discovery & Asset Inventory\n\n**Campaign:** [Campaign name]\n**Date:** [YYYY-MM-DD]\n**Status:** [Draft | Complete]\n\n---\n\n## Business Concept\n\n**Concept Description:**\n[Describe the marketing concept or business idea to validate. What problem does it solve? Who is the target audience?]\n\n**Desired Outcome:**\n[awareness | leads | conversions | retention | etc.]\n\n**Current Stage:**\n[new concept | existing campaign | iteration]\n\n**Target Audience:**\n[Describe the target audience - demographics, behaviors, pain points]\n\n---\n\n## Market Research Findings\n\n**Research Conducted By:** market-researcher agent\n**Date:** [YYYY-MM-DD]\n\n### Market Demand Signals\n[Summary of market demand signals discovered through research]\n\n### Competitor Analysis\n[Similar solutions and competitors identified]\n\n### Audience Needs & Pain Points\n[Analysis of audience needs and pain points from research]\n\n### Validation Evidence\n[Case studies, reviews, testimonials, or other validation evidence found]\n\n### Key Insights\n[Strategic insights from market research that will inform hypothesis generation]\n\n---\n\n## Asset Inventory\n\n### Content Assets\n\n**Blog Posts & Articles:**\n- [List existing blog posts, case studies, whitepapers]\n\n**Video & Multimedia:**\n- [List video content, webinars, tutorials]\n\n**Social Media:**\n- [List social media presence, following, engagement levels]\n\n**Email Lists:**\n- [List email lists and subscriber segments with sizes]\n\n### Campaign Assets\n\n**Ad Campaigns:**\n- [List existing ad campaigns with performance data]\n- CTR: [percentage]\n- Conversion rate: [percentage]\n- Cost per acquisition: [amount]\n\n**Landing Pages:**\n- [List existing landing pages with conversion rates]\n\n**Email Campaigns:**\n- [List email campaigns with open/click rates]\n\n**SEO Performance:**\n- [List keyword rankings, organic traffic levels]\n\n### Audience Assets\n\n**Customer Segments:**\n- [List customer segments and personas]\n\n**Audience Data:**\n- [Demographics, behaviors, preferences data available]\n\n**Customer Feedback:**\n- [Reviews, testimonials, support tickets, common questions]\n\n### Data Assets\n\n**Analytics Platforms:**\n- [Google Analytics | Mixpanel | etc.] - Access: [Yes/No]\n\n**CRM Data:**\n- [Salesforce | HubSpot | etc.] - Access: [Yes/No]\n\n**Ad Platform Data:**\n- [Google Ads | Facebook Ads | etc.] - Access: [Yes/No]\n\n**Email Platform Data:**\n- [Mailchimp | SendGrid | etc.] - Access: [Yes/No]\n\n---\n\n## Success Criteria\n\n### Primary Metrics\n[What metrics indicate success? Examples: CTR, conversion rate, CAC, LTV, engagement rate, etc.]\n\n### Practical Significance Thresholds\n[What magnitude of change is meaningful? Examples: +10% conversion rate, -20% CAC, etc.]\n\n### Signal Types\n\n**Positive Signal:**\n- Definition: [What results would validate the concept and justify proceeding to scale?]\n\n**Negative Signal:**\n- Definition: [What results would invalidate the concept and suggest pivot or abandon?]\n\n**Null Signal:**\n- Definition: [What results would be inconclusive, requiring refinement or more data?]\n\n**Mixed Signal:**\n- Definition: [What results would indicate some aspects work while others don't?]\n\n---\n\n## Constraints\n\n### Budget Constraints\n[Ad spend limits, tool costs, resource allocation limits]\n\n### Time Constraints\n[Launch deadlines, seasonal factors, campaign windows]\n\n### Resource Constraints\n[Team capacity, content production capacity, available expertise]\n\n### Technical Constraints\n[Platform limitations, integration issues, tool availability]\n\n### Regulatory Constraints\n[GDPR, CCPA, industry regulations, compliance requirements]\n\n---\n\n## User Confirmation\n\n- [ ] Discovery findings reviewed and confirmed\n- [ ] Asset inventory is complete\n- [ ] Success criteria are appropriate\n- [ ] Constraints are documented\n- [ ] Ready to proceed to Phase 2: Hypothesis Generation\n",
        "plugins/datapeeker/skills/marketing-experimentation/templates/02-hypothesis-generation.md": "# Hypothesis Generation\n\n**Campaign:** [Campaign name]\n**Date:** [YYYY-MM-DD]\n**Status:** [Draft | Complete]\n\n---\n\n## Generated Hypotheses\n\n### Hypothesis 1: [Brief statement]\n\n**Tactic/Channel:** [landing page | ad campaign | email sequence | content marketing | social media | SEO | etc.]\n\n**Expected Outcome:** [Specific, measurable result - e.g., \"15%+ increase in conversion rate\"]\n\n**Rationale:** [Why we believe this will work based on discovery findings]\n\n**Variables to Test:** [What will we manipulate/measure]\n\n**AARRR Mapping:** [Acquisition | Activation | Retention | Referral | Revenue]\n\n---\n\n### Hypothesis 2: [Brief statement]\n\n**Tactic/Channel:** [landing page | ad campaign | email sequence | content marketing | social media | SEO | etc.]\n\n**Expected Outcome:** [Specific, measurable result]\n\n**Rationale:** [Why we believe this will work based on discovery findings]\n\n**Variables to Test:** [What will we manipulate/measure]\n\n**AARRR Mapping:** [Acquisition | Activation | Retention | Referral | Revenue]\n\n---\n\n### Hypothesis 3: [Brief statement]\n\n**Tactic/Channel:** [landing page | ad campaign | email sequence | content marketing | social media | SEO | etc.]\n\n**Expected Outcome:** [Specific, measurable result]\n\n**Rationale:** [Why we believe this will work based on discovery findings]\n\n**Variables to Test:** [What will we manipulate/measure]\n\n**AARRR Mapping:** [Acquisition | Activation | Retention | Referral | Revenue]\n\n---\n\n### Hypothesis 4: [Brief statement]\n\n**Tactic/Channel:** [landing page | ad campaign | email sequence | content marketing | social media | SEO | etc.]\n\n**Expected Outcome:** [Specific, measurable result]\n\n**Rationale:** [Why we believe this will work based on discovery findings]\n\n**Variables to Test:** [What will we manipulate/measure]\n\n**AARRR Mapping:** [Acquisition | Activation | Retention | Referral | Revenue]\n\n---\n\n### Hypothesis 5: [Brief statement]\n\n**Tactic/Channel:** [landing page | ad campaign | email sequence | content marketing | social media | SEO | etc.]\n\n**Expected Outcome:** [Specific, measurable result]\n\n**Rationale:** [Why we believe this will work based on discovery findings]\n\n**Variables to Test:** [What will we manipulate/measure]\n\n**AARRR Mapping:** [Acquisition | Activation | Retention | Referral | Revenue]\n\n---\n\n[Continue with Hypotheses 6-10 as needed, following the same format]\n\n---\n\n## Framework Mappings\n\n### Lean Startup Build-Measure-Learn\n\nFor each hypothesis, identify:\n\n| Hypothesis | Build (MVP) | Measure (Metrics) | Learn (Insights) |\n|------------|-------------|-------------------|------------------|\n| H1 | [What's the minimum viable test?] | [What metrics indicate success/failure?] | [What will we learn regardless of outcome?] |\n| H2 | [What's the minimum viable test?] | [What metrics indicate success/failure?] | [What will we learn regardless of outcome?] |\n| H3 | [What's the minimum viable test?] | [What metrics indicate success/failure?] | [What will we learn regardless of outcome?] |\n| H4 | [What's the minimum viable test?] | [What metrics indicate success/failure?] | [What will we learn regardless of outcome?] |\n| H5 | [What's the minimum viable test?] | [What metrics indicate success/failure?] | [What will we learn regardless of outcome?] |\n\n### AARRR Pirate Metrics Coverage\n\nVerify hypotheses cover multiple stages:\n\n| AARRR Stage | Hypotheses Addressing This Stage | Coverage |\n|-------------|----------------------------------|----------|\n| **Acquisition** (How do users find us?) | [List hypothesis IDs] | [Good/Adequate/Insufficient] |\n| **Activation** (Great first experience?) | [List hypothesis IDs] | [Good/Adequate/Insufficient] |\n| **Retention** (Do they come back?) | [List hypothesis IDs] | [Good/Adequate/Insufficient] |\n| **Referral** (Do they tell others?) | [List hypothesis IDs] | [Good/Adequate/Insufficient] |\n| **Revenue** (Do they pay?) | [List hypothesis IDs] | [Good/Adequate/Insufficient] |\n\n---\n\n## Tactic Coverage\n\nVerify hypotheses cover diverse marketing tactics:\n\n### Acquisition Tactics\n\n| Tactic | Hypotheses | Count |\n|--------|------------|-------|\n| Landing pages | [List hypothesis IDs] | [number] |\n| Ad campaigns | [List hypothesis IDs] | [number] |\n| Content marketing | [List hypothesis IDs] | [number] |\n| SEO | [List hypothesis IDs] | [number] |\n\n### Activation Tactics\n\n| Tactic | Hypotheses | Count |\n|--------|------------|-------|\n| Email sequences | [List hypothesis IDs] | [number] |\n| Product tours | [List hypothesis IDs] | [number] |\n| Social proof | [List hypothesis IDs] | [number] |\n\n### Retention Tactics\n\n| Tactic | Hypotheses | Count |\n|--------|------------|-------|\n| Email campaigns | [List hypothesis IDs] | [number] |\n| Content/newsletters | [List hypothesis IDs] | [number] |\n\n**Tactic Diversity Assessment:** [Good - covers multiple tactics | Needs improvement - too focused on one tactic]\n\n---\n\n## User Confirmation\n\n- [ ] All hypotheses reviewed and confirmed\n- [ ] Hypotheses are testable and meaningful\n- [ ] Tactic coverage is appropriate (not all focused on one channel)\n- [ ] Framework mappings are complete\n- [ ] Ready to proceed to Phase 3: Prioritization\n",
        "plugins/datapeeker/skills/marketing-experimentation/templates/03-prioritization.md": "# Prioritization\n\n**Campaign:** [Campaign name]\n**Date:** [YYYY-MM-DD]\n**Status:** [Draft | Complete]\n**Framework Used:** [ICE | RICE]\n\n---\n\n## Prioritization Calculation\n\n**Method:** [ICE Framework | RICE Framework]\n\n### Calculation Script\n\n**Script Location:** `[path to ice_calculator.py or rice_calculator.py]`\n\n```python\n[Paste the full Python script used for scoring here]\n```\n\n**Execution Command:**\n```bash\npython3 [ice|rice]_calculator.py\n```\n\n---\n\n## Scoring Results\n\n### [ICE | RICE] Scores\n\n[Paste the output table from the Python script here]\n\n**Example ICE Table:**\n| Hypothesis | Impact | Confidence | Ease | ICE Score | Rank |\n|------------|--------|------------|------|-----------|------|\n| H1: Value prop clarity | 8 | 7 | 9 | 6.22 | 1 |\n| H3: Email sequence optimization | 6 | 8 | 8 | 6.00 | 2 |\n| H2: Ad targeting refinement | 7 | 6 | 5 | 8.40 | 3 |\n| H4: Content marketing expansion | 5 | 4 | 3 | 6.67 | 4 |\n\n**Example RICE Table:**\n| Hypothesis | Reach | Impact | Confidence | Effort | RICE Score | Rank |\n|------------|-------|--------|------------|--------|------------|------|\n| H1: Value prop clarity | 10000 | 3 | 80% | 2w | 12000 | 1 |\n| H3: Email sequence | 5000 | 2 | 80% | 3w | 2667 | 2 |\n| H2: Ad targeting | 50000 | 1 | 50% | 4w | 6250 | 3 |\n| H4: Content expansion | 20000 | 1 | 50% | 8w | 1250 | 4 |\n\n---\n\n## Prioritized Backlog\n\n### Selected for Testing (Top 2-4)\n\n**Hypothesis 1:** [Brief name]\n- **Rank:** [number]\n- **Score:** [ICE/RICE score]\n- **Rationale:** [Why selected - highest score, strategic value, learning opportunity, etc.]\n\n**Hypothesis 2:** [Brief name]\n- **Rank:** [number]\n- **Score:** [ICE/RICE score]\n- **Rationale:** [Why selected]\n\n**Hypothesis 3:** [Brief name]\n- **Rank:** [number]\n- **Score:** [ICE/RICE score]\n- **Rationale:** [Why selected]\n\n**Hypothesis 4:** [Brief name] (optional)\n- **Rank:** [number]\n- **Score:** [ICE/RICE score]\n- **Rationale:** [Why selected]\n\n### Not Selected (Backlog)\n\n**Hypothesis [N]:** [Brief name]\n- **Rank:** [number]\n- **Score:** [ICE/RICE score]\n- **Reason Deferred:** [Resource constraints, lower impact, dependencies, etc.]\n\n[Continue for remaining hypotheses]\n\n---\n\n## Selection Rationale\n\n### Primary Selection Criteria\n[Explain how top 2-4 hypotheses were selected based on computational results]\n\n### Balancing Considerations\n\n**Quick Wins vs. High-Impact Long-Term:**\n[How did we balance easy/fast tests vs. strategic longer-term experiments?]\n\n**Tactic Diversity:**\n[Did we ensure variety across tactics, or focus on one proven channel? Why?]\n\n**Resource Constraints:**\n[How did available time, budget, and team capacity influence selection?]\n\n**Learning Value:**\n[Were any lower-scoring hypotheses selected for high uncertainty/learning value?]\n\n**Dependencies:**\n[Are there any prerequisite experiments that must run first?]\n\n---\n\n## Experiment Sequence Plan\n\n### Execution Strategy\n[Parallel | Sequential | Hybrid]\n\n**Rationale:** [Why this strategy was chosen]\n\n### Timeline\n\n**Week 1-2:**\n- Experiment: [Hypothesis ID and brief name]\n- Experiment: [Hypothesis ID and brief name] (if parallel)\n- Status: [Planned | In Progress | Complete]\n\n**Week 3-4:**\n- Activity: [Analysis of Week 1-2 experiments | New experiment launch]\n- Experiment: [Hypothesis ID and brief name]\n- Status: [Planned | In Progress | Complete]\n\n**Week 5-6:**\n- Activity: [Based on learnings from previous experiments]\n- Experiment: [Hypothesis ID and brief name]\n- Status: [Planned | In Progress | Complete]\n\n**Week 7-8:**\n- Activity: [Final experiments or synthesis]\n- Status: [Planned | In Progress | Complete]\n\n### Dependencies\n\n**Experiment [N] depends on Experiment [M]:**\n[Explain dependency - e.g., \"H2 (ads) uses winning message from H1 (landing page)\"]\n\n**Independent experiments (can run in parallel):**\n[List hypotheses that have no dependencies and can run simultaneously]\n\n---\n\n## Resource Allocation\n\n### Budget Allocation\n| Experiment | Hypothesis | Estimated Budget | Priority |\n|------------|------------|------------------|----------|\n| E1 | [Brief name] | [Amount] | High |\n| E2 | [Brief name] | [Amount] | High |\n| E3 | [Brief name] | [Amount] | Medium |\n| E4 | [Brief name] | [Amount] | Medium |\n\n### Time Allocation\n| Experiment | Hypothesis | Estimated Duration | Effort (person-weeks) |\n|------------|------------|--------------------|-----------------------|\n| E1 | [Brief name] | [X weeks] | [Y person-weeks] |\n| E2 | [Brief name] | [X weeks] | [Y person-weeks] |\n| E3 | [Brief name] | [X weeks] | [Y person-weeks] |\n| E4 | [Brief name] | [X weeks] | [Y person-weeks] |\n\n---\n\n## Risk Assessment\n\n### High-Risk Experiments\n[List any experiments with low confidence scores or high uncertainty]\n\n**Mitigation Strategies:**\n[How will we address uncertainty? Pilot tests, smaller budgets, early stopping criteria?]\n\n### Low-Risk Quick Wins\n[List experiments with high confidence and high ease scores]\n\n**Acceleration Opportunities:**\n[Can we run these faster or with fewer resources?]\n\n---\n\n## User Confirmation\n\n- [ ] Computational scores reviewed and validated\n- [ ] Selected hypotheses (2-4) are appropriate\n- [ ] Experiment sequence is feasible given constraints\n- [ ] Resource allocation is realistic\n- [ ] Dependencies and risks are identified\n- [ ] Ready to proceed to Phase 4: Experiment Coordination\n",
        "plugins/datapeeker/skills/marketing-experimentation/templates/04-experiment-tracker.md": "# Experiment Tracker\n\n**Campaign:** [Campaign name]\n\n**Created:** [YYYY-MM-DD]\n\n**Last Updated:** [YYYY-MM-DD]\n\n---\n\n## Overview\n\nThis is a LIVING DOCUMENT that tracks all experiments in this marketing-experimentation campaign. Update this file as experiments progress through their lifecycle.\n\n**Status Types:**\n- **Planned:** Experiment selected, not yet started\n- **In Progress:** Skill currently executing (hypothesis-testing or qualitative-research)\n- **Complete:** All phases done, signal documented\n\n**Experiment Types:**\n- **Quantitative:** Uses hypothesis-testing skill (measures metrics, statistical analysis)\n- **Qualitative:** Uses qualitative-research skill (interviews, surveys, focus groups, observations)\n\n**Multi-Conversation Resumption:**\nAt the start of any conversation, READ THIS FILE FIRST to understand current experiment status.\n\n---\n\n## Experiment 1: [Hypothesis Brief Name]\n\n**Status:** Planned\n\n**Experiment Type:** [Quantitative | Qualitative]\n\n**Hypothesis:** [Full hypothesis statement from Phase 2]\n\n**Tactic/Channel:** [landing page | ads | email | content | social | SEO | interviews | surveys | focus groups | observations | other]\n\n**Priority Score:** [ICE/RICE score from Phase 3]\n\n**Skill Used:** [hypothesis-testing | qualitative-research]\n\n**Start Date:** Not started\n\n**Completion Date:** N/A\n\n**Location:** `analysis/marketing-experimentation/[campaign-name]/experiments/[experiment-1-name]/`\n\n**Signal:** Not analyzed\n\n**Key Findings:**\n- TBD\n\n**Notes:**\n- [Any relevant notes about dependencies, prerequisites, blockers]\n\n---\n\n## Experiment 2: [Hypothesis Brief Name]\n\n**Status:** Planned\n\n**Experiment Type:** [Quantitative | Qualitative]\n\n**Hypothesis:** [Full hypothesis statement from Phase 2]\n\n**Tactic/Channel:** [tactic/channel]\n\n**Priority Score:** [ICE/RICE score]\n\n**Skill Used:** [hypothesis-testing | qualitative-research]\n\n**Start Date:** Not started\n\n**Completion Date:** N/A\n\n**Location:** `analysis/marketing-experimentation/[campaign-name]/experiments/[experiment-2-name]/`\n\n**Signal:** Not analyzed\n\n**Key Findings:**\n- TBD\n\n**Notes:**\n- [Notes]\n\n---\n\n## Experiment 3: [Hypothesis Brief Name]\n\n**Status:** Planned\n\n**Experiment Type:** [Quantitative | Qualitative]\n\n**Hypothesis:** [Full hypothesis statement from Phase 2]\n\n**Tactic/Channel:** [tactic/channel]\n\n**Priority Score:** [ICE/RICE score]\n\n**Skill Used:** [hypothesis-testing | qualitative-research]\n\n**Start Date:** Not started\n\n**Completion Date:** N/A\n\n**Location:** `analysis/marketing-experimentation/[campaign-name]/experiments/[experiment-3-name]/`\n\n**Signal:** Not analyzed\n\n**Key Findings:**\n- TBD\n\n**Notes:**\n- [Notes]\n\n---\n\n## Experiment 4: [Hypothesis Brief Name] (if applicable)\n\n[Same structure as above]\n\n---\n\n## Completion Summary\n\n**Experiments Planned:** [count]\n\n**Experiments In Progress:** [count]\n\n**Experiments Complete:** [count]\n\n**Ready for Synthesis:** [Yes when all complete | No - [count] remaining]\n",
        "plugins/datapeeker/skills/marketing-experimentation/templates/05-synthesis.md": "# Cross-Experiment Synthesis\n\n**Campaign:** [Campaign name]\n\n**Experiments Analyzed:** [Count from experiment tracker]\n\n**Analysis Date:** [YYYY-MM-DD]\n\n---\n\n## Executive Summary\n\n[2-3 sentence summary of overall campaign findings]\n\n**Overall Campaign Signal:** [Positive | Negative | Null | Mixed]\n\n**Strategic Recommendation:** [Scale | Pivot | Refine | Pause/Abandon]\n\n---\n\n## Aggregate Results Table\n\n| Experiment | Hypothesis | Tactic | Signal | Key Metric | Result | Confidence |\n|------------|------------|--------|--------|------------|--------|------------|\n| E1 | [Brief hypothesis] | [tactic] | [Positive/Negative/Null/Mixed] | [metric] | [e.g., +18%] | [High/Medium/Low] |\n| E2 | [Brief hypothesis] | [tactic] | [signal] | [metric] | [result] | [confidence] |\n| E3 | [Brief hypothesis] | [tactic] | [signal] | [metric] | [result] | [confidence] |\n| ... | ... | ... | ... | ... | ... | ... |\n\n**Signal Distribution:**\n- Positive: [count] experiments\n- Negative: [count] experiments\n- Null: [count] experiments\n- Mixed: [count] experiments\n\n---\n\n## Pattern Analysis\n\n### What Worked (Positive Signals)\n\n**Experiments:**\n- E[#]: [Brief description of experiment and result]\n- E[#]: [Brief description of experiment and result]\n\n**Why These Worked:**\n[Analysis from presenting-data skill explaining commonalities and reasons for success]\n\n**Common Characteristics:**\n- [Characteristic 1 across successful experiments]\n- [Characteristic 2]\n- [Characteristic 3]\n\n---\n\n### What Didn't Work (Negative Signals)\n\n**Experiments:**\n- E[#]: [Brief description of experiment and result]\n- E[#]: [Brief description of experiment and result]\n\n**Why These Failed:**\n[Analysis from presenting-data skill explaining reasons for failure]\n\n**Lessons Learned:**\n- [Lesson 1 from failed experiments]\n- [Lesson 2]\n- [Lesson 3]\n\n---\n\n### What's Unclear (Null/Mixed Signals)\n\n**Experiments:**\n- E[#]: [Brief description of experiment and result]\n- E[#]: [Brief description of experiment and result]\n\n**Potential Reasons for Inconclusive Results:**\n[Analysis from presenting-data skill]\n\n**Recommendations for Further Investigation:**\n- [Investigation recommendation 1]\n- [Investigation recommendation 2]\n\n---\n\n## Cross-Experiment Patterns\n\n**Tactic Performance:**\n[Do results cluster by tactic? Which tactics performed best/worst?]\n\n**Audience Segments:**\n[Do results vary by audience segment? Which segments responded best?]\n\n**Timing/Seasonality:**\n[Do results show temporal patterns? Were some experiments affected by timing?]\n\n**Confounding Factors:**\n[Were there external factors (market conditions, holidays, competitors) affecting multiple experiments?]\n\n**Confidence Assessment:**\n[Which findings are robust? Which are uncertain? Where is more data needed?]\n\n---\n\n## Visualizations\n\n[Output from presenting-data skill]\n\n### Signal Distribution\n\n```\n[Visualization or description of signal distribution across experiments]\n```\n\n### Effect Sizes by Experiment\n\n```\n[Visualization or description of effect magnitudes]\n```\n\n### Confidence Levels\n\n```\n[Visualization showing effect size vs. confidence for each experiment]\n```\n\n### Tactic Performance Comparison\n\n```\n[Visualization comparing performance across tactics/channels]\n```\n\n---\n\n## Overall Campaign Signal Classification\n\n**Signal:** [Positive | Negative | Null | Mixed]\n\n**Rationale:**\n\n**Positive Campaign:**\n[If positive: Multiple experiments validated concept, successful tactics identified, clear path to ROI]\n\n**Negative Campaign:**\n[If negative: Multiple experiments invalidated concept, no successful tactics, concept doesn't resonate]\n\n**Null Campaign:**\n[If null: Most experiments inconclusive, insufficient power or confounding factors, needs refinement]\n\n**Mixed Campaign:**\n[If mixed: Some tactics work, some don't, selective scaling and pivots needed]\n\n---\n\n## Synthesis Sources\n\n[References to individual hypothesis-testing session locations]\n\n- Experiment 1: `[location from tracker]`\n- Experiment 2: `[location from tracker]`\n- Experiment 3: `[location from tracker]`\n\n**presenting-data Output:** [Reference to any slides, whitepapers, or visualizations created]\n\n---\n\n## Next Steps\n\nProceed to Phase 6 (Iteration Planning) to generate new experiment ideas based on these findings.\n",
        "plugins/datapeeker/skills/marketing-experimentation/templates/06-iteration-plan.md": "# Iteration Planning\n\n**Campaign:** [Campaign name]\n\n**Based on Synthesis:** [Reference to 05-synthesis.md]\n\n**Planning Date:** [YYYY-MM-DD]\n\n---\n\n## Campaign Summary\n\n**Overall Signal:** [Positive | Negative | Null | Mixed]\n\n**Key Wins:**\n- [Successful tactic 1 with result]\n- [Successful tactic 2 with result]\n\n**Key Learnings:**\n- [Learning 1 regardless of signal]\n- [Learning 2 regardless of signal]\n- [Learning 3 regardless of signal]\n\n**Strategic Recommendation:** [What to do next based on signal]\n\n---\n\n## New Experiment Ideas\n\n**IMPORTANT:** These are IDEAS, not hypotheses. Each idea must go through a new marketing-experimentation session (Phase 1: Discovery, Phase 2: Hypothesis Generation) before testing.\n\n---\n\n### Idea 1: [Brief descriptive name]\n\n**Category:** Scale Winners\n\n**Rationale:** [Why this idea based on synthesis findings]\n\n**Expected Learning:** [What we'll learn from testing this idea]\n\n**Based on:** [Which successful experiment(s) this builds on]\n\n**Example:**\nScale value prop landing page to paid ads - E1 showed +18% conversion from simplified value prop. Apply winning message to ad creative to test if it improves ad CTR and cost-per-conversion.\n\n---\n\n### Idea 2: [Brief descriptive name]\n\n**Category:** Investigate Nulls\n\n**Rationale:** [Why this idea]\n\n**Expected Learning:** [What we'll learn]\n\n**Based on:** [Which null/mixed result this investigates]\n\n**Example:**\nInvestigate email sequence timing sensitivity - E3 showed negative result but timing may be confound (sent during holidays). Retest to determine if sequence is inherently weak or if timing was the issue.\n\n---\n\n### Idea 3: [Brief descriptive name]\n\n**Category:** Pivot from Failures\n\n**Rationale:** [Why this idea]\n\n**Expected Learning:** [What we'll learn]\n\n**Based on:** [Which failed experiment this pivots from]\n\n**Example:**\nPivot from broad ad targeting to lookalike audiences - E2 showed null result for broad targeting. Pivot to lookalike audiences based on E1 converters to test if targeting precision improves results.\n\n---\n\n### Idea 4: [Brief descriptive name]\n\n**Category:** [Scale Winners | Investigate Nulls | Pivot from Failures | Explore New]\n\n**Rationale:** [Why this idea]\n\n**Expected Learning:** [What we'll learn]\n\n**Based on:** [Relevant experiment or new direction]\n\n---\n\n### Idea 5: [Brief descriptive name]\n\n[Same structure as above]\n\n---\n\n[Add Ideas 6-7 if applicable]\n\n---\n\n## Idea Categorization Summary\n\n**Scale Winners:** [count] ideas\n- [List idea numbers/names that scale successful tactics]\n\n**Investigate Nulls:** [count] ideas\n- [List idea numbers/names that investigate inconclusive results]\n\n**Pivot from Failures:** [count] ideas\n- [List idea numbers/names that pivot from negative results]\n\n**Explore New:** [count] ideas\n- [List idea numbers/names that test entirely new directions]\n\n---\n\n## Strategic Recommendations by Signal\n\n[Select the appropriate section based on Overall Campaign Signal]\n\n### For Positive Signal Campaigns:\n\n**Recommendation: Proceed to Scaling**\n\n**Actions:**\n- Increase budget/effort on winning tactics ([list specific tactics])\n- Apply winning patterns to new channels ([suggest channels])\n- Optimize successful approaches for incremental gains\n\n**Priority Ideas to Pursue:**\n- [Idea # - Scale Winners category]\n- [Idea # - Scale Winners category]\n\n---\n\n### For Negative Signal Campaigns:\n\n**Recommendation: Major Pivot or Pause**\n\n**Actions:**\n- Pivot: Major change in approach, audience, or value proposition\n- Pause: Reassess concept before additional investment\n- Consider: Alternative concepts if no viable path forward\n\n**Priority Ideas to Pursue:**\n- [Idea # - Pivot from Failures category]\n- [Idea # - Explore New category]\n\n---\n\n### For Null Signal Campaigns:\n\n**Recommendation: Refine and Reinvestigate**\n\n**Actions:**\n- Redesign experiments with better power/clarity\n- Address confounding factors identified in synthesis\n- Extend observation periods if time-dependent\n\n**Priority Ideas to Pursue:**\n- [Idea # - Investigate Nulls category]\n- [Idea # - improved versions of previous experiments]\n\n---\n\n### For Mixed Signal Campaigns:\n\n**Recommendation: Selective Scaling + Strategic Pivots**\n\n**Actions:**\n- Double down on winners ([list successful tactics])\n- Abandon or redesign losers ([list failed tactics])\n- Focus next tests on promising areas\n\n**Priority Ideas to Pursue:**\n- [Idea # - Scale Winners for successful tactics]\n- [Idea # - Pivot from Failures for unsuccessful tactics]\n\n---\n\n## Feed-Forward Pattern\n\n**CRITICAL:** Ideas from this phase are NOT ready for testing. They MUST go through a new marketing-experimentation session.\n\n### How Ideas Become Experiments:\n\n```\nPhase 6 generates IDEAS (this document)\n         â†“\nStart NEW marketing-experimentation session with selected idea\n         â†“\nPhase 1: Discovery & Asset Inventory\n    - Invoke market-researcher agent to validate idea\n    - Inventory assets for implementing idea\n    - Define success criteria\n         â†“\nPhase 2: Hypothesis Generation\n    - Formalize idea into 5-10 specific, testable hypotheses\n    - Map hypotheses to tactics and expected outcomes\n         â†“\nPhases 3-6: Complete full experimental cycle\n    - Prioritize hypotheses (Phase 3)\n    - Coordinate experiments (Phase 4)\n    - Synthesize results (Phase 5)\n    - Generate next iteration ideas (Phase 6)\n```\n\n**Why This Matters:**\n- Ideas need market validation before testing (Phase 1)\n- Ideas need formalization into specific hypotheses (Phase 2)\n- Skipping discovery leads to untested assumptions\n- Skipping hypothesis generation leads to vague experiments\n\n**Example Flow:**\n\n1. Phase 6 generates: \"Scale value prop to ads\" (idea)\n2. Start new marketing-experimentation session\n3. Phase 1: Research ad platform best practices, inventory ad assets\n4. Phase 2: Generate hypotheses:\n   - H1: Simplified value prop in ad headline increases CTR by 10%+\n   - H2: Value prop in ad body copy increases conversion rate by 15%+\n   - H3: Value prop as video script improves engagement by 20%+\n5. Continue with Phases 3-6 to test formal hypotheses\n\n---\n\n## Next Steps\n\n**Immediate:**\n- Review iteration ideas with stakeholders\n- Select 1-2 highest-priority ideas to pursue\n- Decide: Start new marketing-experimentation session OR conclude campaign\n\n**If Starting New Session:**\n- Choose idea from this list\n- Begin with Phase 1: Discovery & Asset Inventory\n- Use market-researcher agent to validate selected idea\n- Do NOT skip to Phase 2 - always start with Phase 1\n\n**If Concluding Campaign:**\n- Archive all analysis files\n- Document key learnings in campaign retrospective\n- Share synthesis findings with relevant teams\n- Update knowledge base with validated tactics\n",
        "plugins/datapeeker/skills/presenting-data/SKILL.md": "---\nname: presenting-data\ndescription: Component skill for creating compelling data-driven presentations and whitepapers using marp and pandoc with proper citations and reproducibility\n---\n\n# Presenting Data\n\n## Purpose\n\nThis component skill guides creation of professional data-driven presentations and whitepapers. Use it when:\n- Communicating analysis findings to stakeholders\n- Creating executive summaries and detailed technical reports\n- Documenting reproducible research with proper citations\n- Building a presentation hierarchy: slides â†’ whitepapers (drill-in capability)\n- Referenced by process skills for final deliverables\n\n**Supports two complementary formats:**\n- **Presentations (marp)** - Slide decks for meetings, pitches, and executive summaries\n- **Whitepapers (pandoc)** - Comprehensive documents with citations, cross-references, and academic formatting\n\n## Prerequisites\n\n- Analysis completed with clear findings\n- Query results documented and interpreted (use `interpreting-results` skill)\n- Visualizations prepared (use `creating-visualizations` skill)\n- Understanding of data sources, queries, and reproducibility requirements\n- Clear communication goal and target audience identified\n\n## Data Presentation Process\n\nCreate a TodoWrite checklist for the 5-phase presentation process:\n\n```\nPhase 1: Analyze Audience & Purpose - pending\nPhase 2: Structure Narrative - pending\nPhase 3: Create Content - pending\nPhase 4: Add Citations & Reproducibility - pending\nPhase 5: Generate Outputs - pending\n```\n\nMark each phase as you complete it. Document all presentation materials in your analysis directory.\n\n---\n\n## Phase 1: Analyze Audience & Purpose\n\n**Goal:** Understand who will consume your presentation and what decisions they need to make.\n\n### Identify Your Audience\n\n**Executive Stakeholders:**\n- Format: Slide presentation (5-10 slides)\n- Focus: Key findings, business impact, recommendations\n- Detail Level: High-level metrics, visual emphasis\n- Tool: **marp** for quick, visual presentations\n\n**Technical Peers:**\n- Format: Whitepaper or technical report (20-50 pages)\n- Focus: Methodology, reproducibility, detailed analysis\n- Detail Level: SQL queries, statistical methods, data quality notes\n- Tool: **pandoc** for comprehensive documentation\n\n**Mixed Audience:**\n- Format: Both (presentation + supporting whitepaper)\n- Focus: Slides for overview, whitepaper for drill-in details\n- Detail Level: Presentation hierarchy allowing progressive disclosure\n- Tools: **marp** for slides, **pandoc** for backing documents\n\n### Define Communication Goals\n\n**What decisions will this presentation support?**\n- Strategic planning (high-level trends and forecasts)\n- Operational changes (specific process improvements)\n- Technical validation (methodology and reproducibility)\n- Policy changes (compliance, risk, standards)\n\n**What actions should the audience take?**\n- Approve/reject a proposal\n- Allocate budget or resources\n- Change operational procedures\n- Investigate further (drill into details)\n\n**CHECKPOINT:** Before proceeding to Phase 2, you MUST have:\n- [ ] Target audience identified (executive, technical, or mixed)\n- [ ] Primary communication goal defined\n- [ ] Desired audience action articulated\n- [ ] Output format selected (presentation, whitepaper, or both)\n\n---\n\n## Phase 2: Structure Narrative\n\n**Goal:** Organize findings into a compelling narrative that guides the audience to your conclusions.\n\n### Use the 3-Paragraph Essay Structure\n\n**â†’ See [frameworks/3-paragraph-essay.md](./frameworks/3-paragraph-essay.md) for detailed guidance**\n\nThe classic essay structure adapts perfectly to data presentations:\n\n1. **Introduction & Thesis**\n   - State the question or problem\n   - Present your key finding or recommendation\n   - Preview supporting evidence\n\n2. **Body & Supporting Arguments**\n   - Present data findings that support your thesis\n   - Use visualizations to make patterns clear\n   - Address alternative explanations\n   - Cite data sources and methodology\n\n3. **Conclusion & Next Steps**\n   - Restate key findings\n   - Articulate implications and recommendations\n   - Identify limitations and follow-up questions\n\n4. **Bibliography & Supporting Documentation**\n   - Data sources and SQL queries\n   - Reproducibility information (versions, timestamps)\n   - References to prior research or methodology\n   - Links to detailed whitepapers or repositories\n\n### Apply Narrative Structure for Data Stories\n\n**â†’ See [frameworks/narrative-structure.md](./frameworks/narrative-structure.md) for storytelling patterns**\n\nData presentations follow narrative arcs:\n- **Setup**: Establish business context and question\n- **Conflict**: Present the data challenge or pattern\n- **Resolution**: Show findings and recommendations\n- **Call to Action**: Define next steps\n\n### Outline Your Presentation\n\nCreate outline document: `analysis/[session-name]/presentation-outline.md`\n\n**For Presentations (Marp):**\n```markdown\n# Presentation Outline\n\n## Slide 1: Title & Context\n- Analysis question\n- Time period and data sources\n\n## Slide 2-3: Key Findings (Thesis)\n- 3-5 bullet points with metrics\n- Visual emphasis\n\n## Slide 4-7: Supporting Evidence (Body)\n- One finding per slide\n- Include visualizations\n- Reference methodology\n\n## Slide 8: Conclusions & Recommendations\n- Restate key findings\n- Next steps\n- Questions\n\n## Slide 9: Reproducibility Notes (Appendix)\n- Data sources\n- Query locations\n- Validation status\n```\n\n**For Whitepapers (Pandoc):**\n```markdown\n# Whitepaper Outline\n\n## Introduction (2-3 pages)\n- Business context and objectives\n- Research question\n- Thesis statement\n\n## Methodology (5-10 pages)\n- Data sources and collection methods\n- SQL queries and transformations\n- Analysis frameworks used\n- Data quality assessment\n\n## Results (10-20 pages)\n- Finding 1 with supporting data\n- Finding 2 with supporting data\n- Finding 3 with supporting data\n- Visualizations and tables\n\n## Discussion (5-10 pages)\n- Interpretation of findings\n- Comparison to prior research\n- Limitations and caveats\n- Alternative explanations\n\n## Conclusions (2-3 pages)\n- Summary of key findings\n- Recommendations\n- Future research directions\n\n## References\n- Bibliography (BibTeX format)\n- Appendix: SQL queries\n- Appendix: Data validation notes\n```\n\n**CHECKPOINT:** Before proceeding to Phase 3, you MUST have:\n- [ ] Narrative structure selected (essay-based or story-based)\n- [ ] Presentation outline created with sections identified\n- [ ] Key findings and supporting evidence mapped\n- [ ] Introduction, body, conclusion, and bibliography sections defined\n\n---\n\n## Phase 3: Create Content\n\n**Goal:** Write presentation content using appropriate tools (marp for slides, pandoc for documents).\n\n### Creating Slide Presentations with Marp\n\n**â†’ See [tools/marp.md](./tools/marp.md) for detailed CLI usage and syntax**\n\nMarp transforms Markdown into professional slide decks. Use for:\n- Executive summaries (5-10 slides)\n- Meeting presentations\n- Quick stakeholder updates\n\n**Basic Marp Syntax:**\n```markdown\n---\ntheme: gaia\npaginate: true\nfooter: \"DataPeeker Analysis\"\n---\n\n# Q4 Sales Analysis\n## Key Findings\n\n---\n\n## Data Source\n- Database: `analytics_prod.sales_metrics`\n- Query Date: 2025-11-25\n- Period: 2024-10-01 to 2024-12-31\n\n---\n\n## Finding 1: Revenue Growth\n\n- Total Revenue: $1.2M\n- YoY Growth: +23%\n- Top Region: West Coast\n\n![width:600px](revenue-chart.png)\n\n---\n\n## Methodology\n\n\\```sql\nSELECT\n  DATE_TRUNC('month', date) as month,\n  SUM(amount) as revenue\nFROM sales_metrics\nWHERE date BETWEEN '2024-10-01' AND '2024-12-31'\nGROUP BY month\n\\```\n\n---\n\n## Conclusions\n\n- Revenue exceeded target by 15%\n- West Coast expansion successful\n- Recommend continued investment\n\n---\n\n## Reproducibility\n\n- Queries: `queries/q4_analysis.sql`\n- Data validated: 2025-11-25\n- Full report: [Technical Whitepaper](./whitepaper.pdf)\n```\n\n**Generate Presentation:**\n```bash\nmarp presentation.md -o presentation.pdf\n```\n\n### Creating Whitepapers with Pandoc\n\n**â†’ See [tools/pandoc.md](./tools/pandoc.md) for detailed CLI usage and citations**\n\nPandoc creates publication-quality documents. Use for:\n- Technical reports (20-50 pages)\n- Comprehensive analysis documentation\n- Academic papers with citations and cross-references\n\n**Basic Pandoc Structure:**\n```yaml\n---\ntitle: \"Q4 Sales Performance Analysis\"\nauthor: \"DataPeeker Team\"\ndate: \"2025-11-25\"\ninstitute: \"Tilmon Engineering\"\nabstract: |\n  This analysis examines Q4 2024 sales data, revealing 23% YoY growth\n  driven primarily by West Coast expansion. Methodology, findings, and\n  recommendations are presented with full reproducibility documentation.\nkeywords: \"sales analysis, SQL, data analysis, reproducibility\"\ntoc: true\nlof: true\nlot: true\n---\n\n# Introduction\n\nThis analysis addresses the question: What factors drove Q4 2024\nsales performance? Using DataPeeker to analyze production database\nrecords, we identify key growth drivers and provide actionable\nrecommendations.\n\n# Methodology\n\n## Data Collection\n\nData was extracted from `analytics_prod.sales_metrics` using:\n\n\\```sql\nSELECT\n  transaction_id,\n  customer_id,\n  region,\n  amount,\n  transaction_date\nFROM sales_metrics\nWHERE DATE(transaction_date) BETWEEN '2024-10-01' AND '2024-12-31'\nORDER BY transaction_date\n\\```\n\n## Analysis Framework\n\nAnalysis followed established data quality frameworks [@jones2024]\nand reproducibility standards [@smith2023].\n\n# Results\n\n![Q4 Revenue by Region](revenue-by-region.png){#fig:revenue}\n\nFigure @fig:revenue demonstrates clear regional patterns.\n\n# Discussion\n\nOur findings align with previous research on seasonal trends\n[@williams2024] while revealing new patterns in customer behavior.\n\n# Conclusions\n\nThree key findings emerge from this analysis:\n1. West Coast growth exceeded projections\n2. Customer acquisition accelerated in Q4\n3. Average transaction value increased 12%\n\n# References\n```\n\n**Generate Whitepaper:**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -F pandoc-crossref \\\n  -s -V geometry:margin=1in \\\n  --toc \\\n  --number-sections \\\n  -o whitepaper.pdf\n```\n\n### Integrating Visualizations\n\n**Use `creating-visualizations` component skill** to create charts and diagrams:\n\n**Terminal visualizations:**\n- Use `creating-visualizations` terminal formats for inline code examples\n- Include ASCII charts in whitepaper appendices\n- Show sparklines for trend indicators\n\n**Image-based visualizations:**\n- Use `creating-visualizations` image formats (Kroki) for slides and whitepapers\n- Generate Mermaid flowcharts for methodology sections\n- Create GraphViz diagrams for data lineage\n- Use Vega-Lite for statistical charts\n\n**Best Practices:**\n- Export visualizations as PNG/SVG for marp presentations\n- Reference figure numbers in pandoc documents\n- Include chart source data or generation code\n- Document visualization choices in methodology\n\n**CHECKPOINT:** Before proceeding to Phase 4, you MUST have:\n- [ ] Presentation/whitepaper content drafted in Markdown\n- [ ] Visualizations created and embedded\n- [ ] SQL queries and code snippets included\n- [ ] Narrative structure followed (introduction, body, conclusion)\n\n---\n\n## Phase 4: Add Citations & Reproducibility\n\n**Goal:** Document data sources, queries, and methodology to enable reproducibility and proper attribution.\n\n### Citing Data Sources and Queries\n\n**â†’ See [formats/citations.md](./formats/citations.md) for BibTeX and CSL formats**\n\nProper citation enables:\n- Traceability to original data sources\n- Validation of methodology\n- Reproducibility by others\n- Academic and professional credibility\n\n**Citation Types for Data Analysis:**\n1. **Data Sources** - Databases, APIs, file systems\n2. **SQL Queries** - Specific queries used in analysis\n3. **Analysis Tools** - Software and versions (DataPeeker, Python, R)\n4. **Prior Research** - Published papers or internal reports\n5. **Methodology References** - Statistical methods or frameworks\n\n**Example BibTeX Entries:**\n```bibtex\n@misc{production_database_2025,\n  author = {Tilmon Engineering},\n  title = {Production Sales Metrics Database},\n  year = {2025},\n  url = {analytics_prod.sales_metrics},\n  note = {Query timestamp: 2025-11-25 14:30 UTC}\n}\n\n@software{datapeeker_2025,\n  author = {Tilmon Engineering},\n  title = {DataPeeker: SQL Analysis Tool},\n  year = {2025},\n  version = {2.1.0},\n  url = {https://github.com/tilmon/datapeeker}\n}\n```\n\n### Documenting Reproducibility\n\n**â†’ See [formats/reproducibility.md](./formats/reproducibility.md) for comprehensive checklist**\n\nReproducible research requires documentation of:\n- **Data**: Source, timestamp, version, schema\n- **Queries**: Full SQL text, execution time, row counts\n- **Environment**: Tool versions, dependencies, configuration\n- **Process**: Step-by-step methodology\n- **Validation**: Data quality checks, cross-validation results\n\n**Reproducibility Section Template:**\n```markdown\n## Reproducibility Information\n\n### Data Sources\n- **Database**: `analytics_prod.sales_metrics`\n- **Schema Version**: v2.3.1\n- **Query Timestamp**: 2025-11-25 14:30:00 UTC\n- **Records Examined**: 50,000 transactions\n- **Time Period**: 2024-10-01 to 2024-12-31\n\n### Analysis Environment\n- **Tool**: DataPeeker v2.1.0\n- **Python Version**: 3.11.5\n- **Key Libraries**: pandas 2.1.0, plotext 5.2.8\n- **Operating System**: macOS 14.6.0\n\n### Query Repository\n- **Location**: `github.com/tilmon/analysis/queries/q4_sales.sql`\n- **Commit Hash**: abc123def456\n- **Execution Time**: 3.2 seconds\n- **Rows Returned**: 50,000\n\n### Data Quality Validation\n- **Null Values**: 0.02% (within tolerance)\n- **Duplicates**: 0 detected\n- **Outliers**: 12 identified and documented separately\n- **Cross-Validation**: Results match source system aggregate queries\n\n### Reproducibility Instructions\n1. Clone repository: `git clone github.com/tilmon/analysis`\n2. Install dependencies: `pip install -r requirements.txt`\n3. Run analysis: `python scripts/q4_analysis.py`\n4. View results: `analysis/q4-2024/01-findings.md`\n```\n\n**CHECKPOINT:** Before proceeding to Phase 5, you MUST have:\n- [ ] BibTeX bibliography created with data sources\n- [ ] SQL queries documented with execution details\n- [ ] Reproducibility section added with environment info\n- [ ] Citations inserted in text using [@citation_key] syntax\n\n---\n\n## Phase 5: Generate Outputs\n\n**Goal:** Compile final presentation and whitepaper artifacts using marp and pandoc.\n\n### Generate Slide Presentation (Marp)\n\n**Basic PDF Output:**\n```bash\nmarp presentation.md -o presentation.pdf\n```\n\n**HTML with Speaker Notes:**\n```bash\nmarp presentation.md -o presentation.html\n```\n\n**PowerPoint (Editable):**\n```bash\nmarp presentation.md --pptx -o presentation.pptx\n```\n\n**With Custom Theme:**\n```bash\nmarp presentation.md --theme-set custom-theme.css -o presentation.pdf\n```\n\n**Watch Mode (Live Preview):**\n```bash\nmarp -w -p presentation.md\n```\n\n### Generate Whitepaper (Pandoc)\n\n**PDF with Citations:**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -s -V geometry:margin=1in \\\n  -o whitepaper.pdf\n```\n\n**PDF with Cross-References:**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  -F pandoc-crossref \\\n  -s --toc --number-sections \\\n  -o whitepaper.pdf\n```\n\n**Word Document (Editable):**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --reference-doc=template.docx \\\n  -o whitepaper.docx\n```\n\n**HTML for Web Publishing:**\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  -s -c style.css \\\n  --self-contained \\\n  -o whitepaper.html\n```\n\n### Presentation Hierarchy (Slides â†’ Whitepapers)\n\n**Link from slides to detailed documentation:**\n```markdown\n# Conclusion\n\nFor detailed methodology and reproducibility:\nâ†’ [Technical Whitepaper](./whitepaper.pdf)\nâ†’ [GitHub Repository](https://github.com/tilmon/analysis)\n```\n\n**File Organization:**\n```\nanalysis/\nâ”œâ”€â”€ q4-2024/\nâ”‚   â”œâ”€â”€ presentation.md (marp source)\nâ”‚   â”œâ”€â”€ presentation.pdf (generated)\nâ”‚   â”œâ”€â”€ whitepaper.md (pandoc source)\nâ”‚   â”œâ”€â”€ whitepaper.pdf (generated)\nâ”‚   â”œâ”€â”€ references.bib (bibliography)\nâ”‚   â”œâ”€â”€ queries/\nâ”‚   â”‚   â””â”€â”€ q4_analysis.sql\nâ”‚   â””â”€â”€ visualizations/\nâ”‚       â”œâ”€â”€ revenue-chart.png\nâ”‚       â””â”€â”€ regional-breakdown.png\n```\n\n**Automation Script:**\n```bash\n#!/bin/bash\n# generate_deliverables.sh\n\n# Generate presentation\necho \"Creating presentation...\"\nmarp presentation.md -o presentation.pdf\n\n# Generate whitepaper\necho \"Creating whitepaper...\"\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -F pandoc-crossref \\\n  -s --toc --number-sections \\\n  -V geometry:margin=1in \\\n  -o whitepaper.pdf\n\necho \"Deliverables created:\"\necho \"  - presentation.pdf (slides)\"\necho \"  - whitepaper.pdf (detailed report)\"\n```\n\n**CHECKPOINT:** Final verification before delivery:\n- [ ] Presentation PDF generated and reviewed\n- [ ] Whitepaper PDF generated with citations and cross-references\n- [ ] All visualizations properly embedded and sized\n- [ ] Bibliography and references correctly formatted\n- [ ] Reproducibility information complete\n- [ ] Links between presentation and whitepaper working\n- [ ] Files organized in analysis directory structure\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill when presenting findings:\n\n```markdown\nUse the `presenting-data` component skill to create professional\ndeliverables from your analysis:\n- Executive presentations (marp) for quick communication\n- Technical whitepapers (pandoc) for detailed documentation\n- Both formats with proper citations and reproducibility\n```\n\nWhen presenting analysis results:\n1. **Choose format** based on audience (Phase 1)\n2. **Structure narrative** using essay or story patterns (Phase 2)\n3. **Create content** with marp/pandoc (Phase 3)\n4. **Add citations** and reproducibility documentation (Phase 4)\n5. **Generate outputs** for delivery (Phase 5)\n\n**Typical Usage Contexts:**\n- `exploratory-analysis` â†’ Use presenting-data for final report\n- `guided-investigation` â†’ Use presenting-data to document findings\n- `hypothesis-testing` â†’ Use presenting-data to publish results\n- `comparative-analysis` â†’ Use presenting-data for comparison reports\n\n---\n\n## Quality Checklist\n\nBefore considering presentation complete:\n\n**Content Quality:**\n- [ ] Clear thesis or key finding stated upfront\n- [ ] Supporting evidence logically organized\n- [ ] Visualizations effectively communicate patterns\n- [ ] Conclusions tied directly to evidence\n- [ ] Limitations and caveats acknowledged\n\n**Reproducibility:**\n- [ ] Data sources fully documented\n- [ ] SQL queries included or referenced\n- [ ] Tool versions and environment documented\n- [ ] Execution timestamps recorded\n- [ ] Reproducibility instructions provided\n\n**Citation Quality:**\n- [ ] All data sources cited in bibliography\n- [ ] Prior research properly attributed\n- [ ] Analysis tools and versions documented\n- [ ] Citation style consistent throughout\n- [ ] Links to repositories and queries working\n\n**Technical Quality:**\n- [ ] Presentation PDF renders correctly\n- [ ] Whitepaper PDF has working cross-references\n- [ ] Images embedded and properly sized\n- [ ] Code blocks have syntax highlighting\n- [ ] Math equations render correctly (if applicable)\n\n**Narrative Quality:**\n- [ ] Introduction establishes context and question\n- [ ] Body presents evidence systematically\n- [ ] Conclusion restates findings clearly\n- [ ] Bibliography enables drill-in for details\n- [ ] Overall flow guides audience to conclusions\n\n**Presentation-Whitepaper Hierarchy:**\n- [ ] Slides provide high-level overview\n- [ ] Whitepaper provides comprehensive details\n- [ ] Links between formats working\n- [ ] Consistent terminology and metrics\n- [ ] Both reference same data sources\n\n---\n\n## Best Practices\n\n### DO:\nâœ… Start with audience analysis to choose format\nâœ… Use 3-paragraph essay structure for clear narrative\nâœ… Include SQL queries for reproducibility\nâœ… Cite data sources in bibliography\nâœ… Create both slides and whitepaper for mixed audiences\nâœ… Link from presentations to detailed whitepapers\nâœ… Document environment, versions, and timestamps\nâœ… Use `creating-visualizations` for charts and diagrams\nâœ… Version control both source markdown and generated PDFs\nâœ… Test reproducibility instructions before delivery\n\n### DON'T:\nâŒ Skip audience analysis - format should match need\nâŒ Present findings without methodology documentation\nâŒ Forget to cite data sources and prior research\nâŒ Generate PDFs without reviewing output quality\nâŒ Mix presentation styles (stay consistent)\nâŒ Overcomplicate slides with too much detail\nâŒ Omit reproducibility information\nâŒ Assume audience can recreate analysis without documentation\nâŒ Ignore cross-references and internal links\nâŒ Deliver without validating bibliography formatting\n",
        "plugins/datapeeker/skills/presenting-data/formats/citations.md": "# Citations for Data Analysis\n\nProper citation of data sources, queries, and tools ensures reproducibility, enables validation, and provides intellectual credit. This guide covers citation formats for data analysis contexts.\n\n---\n\n## Why Cite in Data Analysis?\n\n**Reproducibility**: Citations enable others to access the same data sources and validate findings.\n\n**Traceability**: Document provenance from raw data to final conclusions.\n\n**Intellectual Credit**: Acknowledge prior research, methodology frameworks, and tool developers.\n\n**Professional Credibility**: Demonstrates rigorous scholarship and attention to detail.\n\n---\n\n## What to Cite in Data Analysis\n\n### 1. Data Sources\n- Databases (production, analytics, external)\n- Files (CSV, JSON, Excel)\n- APIs and web services\n- Surveys and data collection instruments\n\n### 2. SQL Queries and Scripts\n- Analysis queries\n- Data transformation scripts\n- ETL pipelines\n\n### 3. Analysis Tools and Software\n- DataPeeker, Jupyter, R, Python\n- Libraries and packages (pandas, plotext, etc.)\n- Visualization tools\n\n### 4. Prior Research\n- Published papers\n- Internal reports\n- Methodology frameworks\n\n### 5. Statistical Methods\n- Test procedures\n- Modeling approaches\n- Algorithms\n\n---\n\n## BibTeX Format\n\nBibTeX is the standard format for academic and technical publications. Use with pandoc for automatic bibliography generation.\n\n### Database Citations\n\n**Pattern for production databases**:\n```bibtex\n@misc{database_identifier,\n  author = {Organization Name},\n  title = {Database Name},\n  year = {2025},\n  howpublished = {database.schema.table},\n  note = {Query timestamp: YYYY-MM-DD HH:MM:SS UTC}\n}\n```\n\n**Example**:\n```bibtex\n@misc{tilmon_sales_db_2025,\n  author = {Tilmon Engineering},\n  title = {Production Sales Metrics Database},\n  year = {2025},\n  howpublished = {analytics\\_prod.sales\\_metrics},\n  note = {Query timestamp: 2025-11-25 14:30:00 UTC. Records examined: 50,000}\n}\n```\n\n**Cite in text**:\n```markdown\nData was extracted from the production database [@tilmon_sales_db_2025].\n```\n\n### File-Based Data Sources\n\n**CSV Files**:\n```bibtex\n@misc{dataset_csv_2024,\n  author = {Smith, Jane and Doe, John},\n  title = {Q4 2024 Sales Transactions Dataset},\n  year = {2024},\n  howpublished = {CSV file},\n  url = {https://example.com/data/q4_sales.csv},\n  note = {Downloaded: 2025-11-25. 50,000 records}\n}\n```\n\n**Excel Files**:\n```bibtex\n@misc{financial_data_2024,\n  author = {Finance Department},\n  title = {2024 Financial Report Data},\n  year = {2024},\n  howpublished = {Microsoft Excel file},\n  note = {File: financial_report_2024.xlsx, Sheet: Q4_Summary}\n}\n```\n\n### API and Web Data Sources\n\n**REST API**:\n```bibtex\n@misc{stripe_api_2025,\n  author = {Stripe, Inc.},\n  title = {Stripe Payments API},\n  year = {2025},\n  url = {https://stripe.com/docs/api},\n  note = {API version v2025-01-01. Accessed: 2025-11-25}\n}\n```\n\n**Web Scraping**:\n```bibtex\n@misc{competitor_pricing_2025,\n  author = {Competitor Analysis Team},\n  title = {Competitor Pricing Data},\n  year = {2025},\n  url = {https://example.com/pricing},\n  note = {Scraped: 2025-11-25 10:00 UTC. Data reflects public pricing as of scrape date}\n}\n```\n\n### SQL Queries\n\n**Cite specific queries**:\n```bibtex\n@misc{q4_revenue_query_2025,\n  author = {Analytics Team},\n  title = {Q4 Regional Revenue Analysis Query},\n  year = {2025},\n  howpublished = {SQL query},\n  note = {File: queries/q4\\_revenue.sql. Execution time: 2.3s. Repository: github.com/tilmon/analysis}\n}\n```\n\n**Cite in text**:\n```markdown\nRegional revenue was calculated using a standardized query [@q4_revenue_query_2025]\nthat aggregates transactions by geographic region.\n```\n\n### Software and Tools\n\n**DataPeeker**:\n```bibtex\n@software{datapeeker_2025,\n  author = {Tilmon Engineering},\n  title = {DataPeeker: SQL Analysis Tool},\n  year = {2025},\n  version = {2.1.0},\n  url = {https://github.com/tilmon/datapeeker},\n  note = {MIT License}\n}\n```\n\n**Python Libraries**:\n```bibtex\n@software{pandas_2024,\n  author = {{pandas development team}},\n  title = {pandas: Powerful data structures for data analysis},\n  year = {2024},\n  version = {2.1.0},\n  url = {https://pandas.pydata.org/},\n  doi = {10.5281/zenodo.3509134}\n}\n\n@software{plotext_2024,\n  author = {Piccolo, Savino},\n  title = {plotext: Terminal plotting library for Python},\n  year = {2024},\n  version = {5.2.8},\n  url = {https://github.com/piccolomo/plotext}\n}\n```\n\n**R Packages**:\n```bibtex\n@manual{dplyr_2024,\n  title = {dplyr: A Grammar of Data Manipulation},\n  author = {Wickham, Hadley and FranÃ§ois, Romain and Henry, Lionel and MÃ¼ller, Kirill},\n  year = {2024},\n  note = {R package version 1.1.3},\n  url = {https://CRAN.R-project.org/package=dplyr}\n}\n```\n\n### Published Research\n\n**Journal Articles**:\n```bibtex\n@article{jones2024,\n  author = {Jones, Michael and Williams, Sarah},\n  title = {Geographic Market Penetration Strategies for B2B SaaS},\n  journal = {Journal of Sales Research},\n  year = {2024},\n  volume = {15},\n  number = {3},\n  pages = {234--256},\n  doi = {10.1234/jsr.2024.15.3}\n}\n```\n\n**Conference Papers**:\n```bibtex\n@inproceedings{smith2023,\n  author = {Smith, Jane},\n  title = {Reproducibility Standards for Business Analytics},\n  booktitle = {Proceedings of the Data Science Conference},\n  year = {2023},\n  pages = {112--128},\n  publisher = {ACM},\n  address = {New York, NY},\n  doi = {10.1145/3511234.3512345}\n}\n```\n\n**Books**:\n```bibtex\n@book{wickham2023,\n  author = {Wickham, Hadley and Grolemund, Garrett},\n  title = {R for Data Science},\n  edition = {2nd},\n  year = {2023},\n  publisher = {O'Reilly Media},\n  address = {Sebastopol, CA},\n  isbn = {978-1-492-09740-2}\n}\n```\n\n### Internal Reports\n\n**Company Reports**:\n```bibtex\n@techreport{tilmon_strategy_2024,\n  author = {Strategic Planning Team},\n  title = {2024 Geographic Expansion Strategy},\n  institution = {Tilmon Engineering},\n  year = {2024},\n  type = {Internal Report},\n  note = {Report ID: SPT-2024-003. Confidential}\n}\n```\n\n**Working Papers**:\n```bibtex\n@unpublished{doe_analysis_2024,\n  author = {Doe, John},\n  title = {Preliminary Analysis of Customer Retention Patterns},\n  year = {2024},\n  note = {Working paper. Available from author}\n}\n```\n\n### Methodology Frameworks\n\n**Statistical Methods**:\n```bibtex\n@article{wilcoxon1945,\n  author = {Wilcoxon, Frank},\n  title = {Individual Comparisons by Ranking Methods},\n  journal = {Biometrics Bulletin},\n  year = {1945},\n  volume = {1},\n  number = {6},\n  pages = {80--83},\n  doi = {10.2307/3001968}\n}\n```\n\n**Data Quality Frameworks**:\n```bibtex\n@article{batini2009,\n  author = {Batini, Carlo and Cappiello, Cinzia and Francalanci, Chiara and Maurino, Andrea},\n  title = {Methodologies for Data Quality Assessment and Improvement},\n  journal = {ACM Computing Surveys},\n  year = {2009},\n  volume = {41},\n  number = {3},\n  pages = {1--52},\n  doi = {10.1145/1541880.1541883}\n}\n```\n\n---\n\n## CSL JSON Format\n\nAlternative to BibTeX, useful for web-based tools and JSON workflows.\n\n### Database Citation (CSL JSON)\n\n```json\n{\n  \"id\": \"tilmon_sales_db_2025\",\n  \"type\": \"dataset\",\n  \"author\": [\n    {\"literal\": \"Tilmon Engineering\"}\n  ],\n  \"title\": \"Production Sales Metrics Database\",\n  \"issued\": {\"date-parts\": [[2025]]},\n  \"URL\": \"analytics_prod.sales_metrics\",\n  \"note\": \"Query timestamp: 2025-11-25 14:30:00 UTC. Records examined: 50,000\"\n}\n```\n\n### Software Citation (CSL JSON)\n\n```json\n{\n  \"id\": \"datapeeker_2025\",\n  \"type\": \"software\",\n  \"author\": [\n    {\"literal\": \"Tilmon Engineering\"}\n  ],\n  \"title\": \"DataPeeker: SQL Analysis Tool\",\n  \"version\": \"2.1.0\",\n  \"issued\": {\"date-parts\": [[2025]]},\n  \"URL\": \"https://github.com/tilmon/datapeeker\"\n}\n```\n\n### Journal Article (CSL JSON)\n\n```json\n{\n  \"id\": \"jones2024\",\n  \"type\": \"article-journal\",\n  \"author\": [\n    {\"family\": \"Jones\", \"given\": \"Michael\"},\n    {\"family\": \"Williams\", \"given\": \"Sarah\"}\n  ],\n  \"title\": \"Geographic Market Penetration Strategies for B2B SaaS\",\n  \"container-title\": \"Journal of Sales Research\",\n  \"volume\": 15,\n  \"issue\": 3,\n  \"page\": \"234-256\",\n  \"issued\": {\"date-parts\": [[2024]]},\n  \"DOI\": \"10.1234/jsr.2024.15.3\"\n}\n```\n\n---\n\n## Citation in Markdown\n\n### Basic Citations\n\n**Single citation**:\n```markdown\nAccording to recent research [@jones2024], market penetration requires...\n```\n\n**Multiple citations**:\n```markdown\nPrior studies [@jones2024; @smith2023; @williams2022] have examined...\n```\n\n**Page numbers**:\n```markdown\nAs noted by Jones [-@jones2024, pp. 45-47], the key factors include...\n```\n\n**Suppress author** (shows only year/number):\n```markdown\nRecent research [-@jones2024] demonstrates...\n```\n\n### Citations for Data Sources\n\n**Database**:\n```markdown\nData was extracted from the production database [@tilmon_sales_db_2025]\ncovering the period October 1 - December 31, 2024.\n```\n\n**Query**:\n```markdown\nRegional aggregation followed the methodology documented in\n[@q4_revenue_query_2025], which groups transactions by geographic\nregion and calculates summary statistics.\n```\n\n**Multiple Data Sources**:\n```markdown\nAnalysis combined data from three sources: production database\n[@tilmon_sales_db_2025], customer demographics [@customer_data_2024],\nand external market data [@competitor_pricing_2025].\n```\n\n### Citations for Tools\n\n**Software**:\n```markdown\nAnalysis was performed using DataPeeker v2.1.0 [@datapeeker_2025],\nwith data manipulation in pandas [@pandas_2024] and visualization\nvia plotext [@plotext_2024].\n```\n\n**Methodology**:\n```markdown\nStatistical significance was assessed using the Wilcoxon rank-sum\ntest [@wilcoxon1945] with Î± = 0.05.\n```\n\n---\n\n## Complete Reference File Example\n\n**`references.bib`**:\n```bibtex\n% ===================================================================\n% DATA SOURCES\n% ===================================================================\n\n@misc{tilmon_sales_db_2025,\n  author = {Tilmon Engineering},\n  title = {Production Sales Metrics Database},\n  year = {2025},\n  howpublished = {analytics\\_prod.sales\\_metrics},\n  note = {Query timestamp: 2025-11-25 14:30:00 UTC. Records: 50,000}\n}\n\n@misc{customer_data_2024,\n  author = {Customer Analytics Team},\n  title = {Customer Demographics and Profile Data},\n  year = {2024},\n  howpublished = {customer\\_db.demographics},\n  note = {Snapshot date: 2024-12-31}\n}\n\n% ===================================================================\n% SQL QUERIES\n% ===================================================================\n\n@misc{q4_revenue_query_2025,\n  author = {Analytics Team},\n  title = {Q4 Regional Revenue Analysis Query},\n  year = {2025},\n  howpublished = {SQL query},\n  note = {File: queries/q4\\_revenue.sql. Execution: 2.3s. Repo: github.com/tilmon/analysis}\n}\n\n% ===================================================================\n% SOFTWARE AND TOOLS\n% ===================================================================\n\n@software{datapeeker_2025,\n  author = {Tilmon Engineering},\n  title = {DataPeeker: SQL Analysis Tool},\n  year = {2025},\n  version = {2.1.0},\n  url = {https://github.com/tilmon/datapeeker}\n}\n\n@software{pandas_2024,\n  author = {{pandas development team}},\n  title = {pandas: Powerful data structures for data analysis},\n  year = {2024},\n  version = {2.1.0},\n  url = {https://pandas.pydata.org/},\n  doi = {10.5281/zenodo.3509134}\n}\n\n@software{plotext_2024,\n  author = {Piccolo, Savino},\n  title = {plotext: Terminal plotting library for Python},\n  year = {2024},\n  version = {5.2.8},\n  url = {https://github.com/piccolomo/plotext}\n}\n\n% ===================================================================\n% PUBLISHED RESEARCH\n% ===================================================================\n\n@article{jones2024,\n  author = {Jones, Michael and Williams, Sarah},\n  title = {Geographic Market Penetration Strategies for B2B SaaS},\n  journal = {Journal of Sales Research},\n  year = {2024},\n  volume = {15},\n  number = {3},\n  pages = {234--256},\n  doi = {10.1234/jsr.2024.15.3}\n}\n\n@article{smith2023,\n  author = {Smith, Jane},\n  title = {Reproducibility Standards for Business Analytics},\n  journal = {Data Science Quarterly},\n  year = {2023},\n  volume = {8},\n  number = {2},\n  pages = {112--128},\n  doi = {10.5678/dsq.2023.8.2}\n}\n\n% ===================================================================\n% STATISTICAL METHODS\n% ===================================================================\n\n@article{wilcoxon1945,\n  author = {Wilcoxon, Frank},\n  title = {Individual Comparisons by Ranking Methods},\n  journal = {Biometrics Bulletin},\n  year = {1945},\n  volume = {1},\n  number = {6},\n  pages = {80--83},\n  doi = {10.2307/3001968}\n}\n\n% ===================================================================\n% INTERNAL REPORTS\n% ===================================================================\n\n@techreport{tilmon_strategy_2024,\n  author = {Strategic Planning Team},\n  title = {2024 Geographic Expansion Strategy},\n  institution = {Tilmon Engineering},\n  year = {2024},\n  type = {Internal Report},\n  note = {Report ID: SPT-2024-003}\n}\n```\n\n---\n\n## Citation Style Selection\n\n### Common Styles for Data Analysis\n\n**IEEE** (numbered citations):\n```\n[1] M. Jones and S. Williams, \"Geographic Market Penetration...\"\n```\n\n**Chicago Author-Date**:\n```\n(Jones and Williams 2024)\n```\n\n**APA 7th**:\n```\nJones, M., & Williams, S. (2024). Geographic market penetration...\n```\n\n**Harvard**:\n```\nJones, M. and Williams, S. (2024) 'Geographic Market Penetration...'\n```\n\n### Style Files\n\nDownload CSL styles from:\n- [Zotero Style Repository](https://www.zotero.org/styles) - 10,000+ styles\n- [CSL GitHub](https://github.com/citation-style-language/styles)\n\n**Common styles for technical documents**:\n- `ieee.csl` - IEEE (recommended for technical reports)\n- `chicago-author-date.csl` - Chicago (default for pandoc)\n- `apa.csl` - APA 7th edition\n- `acm-sig-proceedings.csl` - ACM conference format\n\n**Use with pandoc**:\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -s -o whitepaper.pdf\n```\n\n---\n\n## Best Practices\n\n### DO:\n\nâœ… **Cite all data sources**\n```markdown\nData extracted from [@tilmon_sales_db_2025] covering Q4 2024.\n```\n\nâœ… **Include query timestamps**\n```bibtex\nnote = {Query timestamp: 2025-11-25 14:30:00 UTC}\n```\n\nâœ… **Document tool versions**\n```bibtex\nversion = {2.1.0}\n```\n\nâœ… **Link to repositories**\n```bibtex\nurl = {https://github.com/tilmon/analysis}\n```\n\nâœ… **Use consistent citation keys**\n```\nPattern: lastname_year or tool_year\nExamples: jones2024, datapeeker_2025\n```\n\nâœ… **Cite statistical methods**\n```markdown\nSignificance tested using Wilcoxon rank-sum [@wilcoxon1945].\n```\n\nâœ… **Credit prior research**\n```markdown\nOur approach builds on [@jones2024; @smith2023].\n```\n\n### DON'T:\n\nâŒ **Skip data source citations**\n- Every database, file, API must be cited\n\nâŒ **Omit timestamps**\n- Data changes over time, timestamps enable reproducibility\n\nâŒ **Forget tool versions**\n- Analysis results can vary across software versions\n\nâŒ **Use vague citations**\n- \"Internal database\" â†’ Cite specific database.table\n\nâŒ **Ignore SQL query provenance**\n- Document query file locations and execution details\n\n---\n\n## Special Cases\n\n### Proprietary Data\n\n**Limited access data**:\n```bibtex\n@misc{proprietary_data_2025,\n  author = {Company Name},\n  title = {Proprietary Customer Database},\n  year = {2025},\n  note = {Confidential. Access restricted. Contact: data-team@company.com}\n}\n```\n\n**Cite in text with caveats**:\n```markdown\nAnalysis used proprietary customer data [@proprietary_data_2025].\nRaw data cannot be shared due to confidentiality restrictions, but\naggregated results and methodology are fully documented in Appendix A.\n```\n\n### Synthetic or Generated Data\n\n**Machine-generated data**:\n```bibtex\n@misc{synthetic_data_2025,\n  author = {Analytics Team},\n  title = {Synthetic Customer Transaction Data},\n  year = {2025},\n  note = {Generated using Python Faker library v20.1.0. Seed: 42.\n         Records: 10,000. Generation script: scripts/generate\\_data.py}\n}\n```\n\n### Real-time Dashboards\n\n**Live data sources**:\n```bibtex\n@misc{realtime_dashboard_2025,\n  author = {Company Name},\n  title = {Real-Time Sales Dashboard},\n  year = {2025},\n  url = {https://analytics.company.com/sales-dashboard},\n  note = {Live dashboard. Data as of: 2025-11-25 14:30 UTC.\n         Screenshot archived in analysis/dashboards/}\n}\n```\n\n---\n\n## Integration with Pandoc\n\n### Basic Citation Workflow\n\n**1. Create bibliography file** (`references.bib`)\n\n**2. Add citations to markdown**:\n```markdown\nData from production database [@tilmon_sales_db_2025] shows...\n```\n\n**3. Convert with --citeproc**:\n```bash\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -s -o whitepaper.pdf\n```\n\n### In-Document Bibliography Specification\n\n**Alternative: Specify bibliography in YAML**:\n```yaml\n---\ntitle: \"My Analysis\"\nbibliography: references.bib\ncsl: ieee.csl\nlink-citations: true\n---\n```\n\n**Then convert without flags**:\n```bash\npandoc whitepaper.md -s -o whitepaper.pdf\n```\n\n---\n\n## Citation Management Tools\n\n### Zotero\n\n**Export from Zotero**:\n1. Select references in Zotero\n2. Right-click â†’ Export Items\n3. Format: BibTeX\n4. Save as `references.bib`\n\n**Zotero Better BibTeX** plugin provides enhanced BibTeX export.\n\n### Mendeley\n\n**Export from Mendeley**:\n1. Select references\n2. File â†’ Export\n3. Format: BibTeX\n4. Save\n\n### EndNote\n\n**Export from EndNote**:\n1. Select references\n2. File â†’ Export\n3. Format: BibTeX\n4. Save\n\n---\n\n## Troubleshooting\n\n### Issue: Citations showing as [@key] instead of numbers\n\n**Solution**: Use `--citeproc` flag\n```bash\npandoc input.md --citeproc --bibliography refs.bib -s -o output.pdf\n```\n\n### Issue: Bibliography not appearing\n\n**Solution**: Ensure bibliography file path is correct\n```bash\n# Correct\n--bibliography references.bib\n\n# Wrong (if file doesn't exist at path)\n--bibliography ../other/refs.bib\n```\n\n### Issue: Citation key not found\n\n**Solution**: Check citation key matches BibTeX entry\n```bibtex\n% BibTeX entry\n@misc{tilmon_data_2025,\n  ...\n}\n\n% Correct citation in markdown\n[@tilmon_data_2025]\n\n% Wrong\n[@tilmon_data]  # Missing _2025\n```\n\n### Issue: Special characters in BibTeX\n\n**Solution**: Escape underscores and special characters\n```bibtex\n% Wrong\nhowpublished = {analytics_prod.sales_metrics}\n\n% Correct\nhowpublished = {analytics\\_prod.sales\\_metrics}\n```\n\n---\n\n## References\n\n**Citation Standards**:\n- [BibTeX Documentation](http://www.bibtex.org/)\n- [CSL Specification](https://citationstyles.org/)\n- [Zotero Style Repository](https://www.zotero.org/styles)\n\n**Related DataPeeker Skills**:\n- `tools/pandoc.md` - Using pandoc with citations\n- `formats/reproducibility.md` - Documenting reproducibility\n- `frameworks/3-paragraph-essay.md` - Bibliography in essay structure\n- `SKILL.md` - Phase 4: Add Citations & Reproducibility\n",
        "plugins/datapeeker/skills/presenting-data/formats/reproducibility.md": "# Reproducibility Documentation\n\nReproducible research enables others to validate findings, replicate analysis, and build upon your work. This guide provides a comprehensive checklist for documenting reproducible data analysis.\n\n---\n\n## Why Reproducibility Matters\n\n**Validation**: Others can verify your findings are correct.\n\n**Transparency**: Full methodology enables critical evaluation.\n\n**Efficiency**: Future analysts can build on your work instead of starting from scratch.\n\n**Trust**: Reproducible research demonstrates rigor and integrity.\n\n**Compliance**: Many organizations and journals require reproducibility documentation.\n\n---\n\n## The Five Pillars of Reproducibility\n\n### 1. Data Provenance\nDocument where data came from and how to access it.\n\n### 2. Environment Specification\nRecord exact software versions and configuration.\n\n### 3. Process Documentation\nStep-by-step instructions for running analysis.\n\n### 4. Validation Evidence\nDemonstrate data quality and result verification.\n\n### 5. Accessibility\nMake materials available to those who need to reproduce.\n\n---\n\n## Complete Reproducibility Checklist\n\nUse this checklist to ensure comprehensive reproducibility documentation:\n\n### Data Sources\n\n- [ ] **Database connection details** documented (host, database, schema, table)\n- [ ] **Query timestamp** recorded (YYYY-MM-DD HH:MM:SS UTC)\n- [ ] **Data period** specified (date range for temporal data)\n- [ ] **Record counts** documented (rows examined, rows returned)\n- [ ] **Schema information** included (column names, types, constraints)\n- [ ] **Access instructions** provided (how to access the database)\n- [ ] **Data freshness** noted (real-time, daily snapshot, archival)\n- [ ] **File locations** specified for file-based data (full paths)\n- [ ] **File checksums** computed (MD5/SHA256 for validation)\n- [ ] **API versions** documented (for web service data)\n\n### SQL Queries and Scripts\n\n- [ ] **Full query text** included (in appendix or repository)\n- [ ] **Execution time** recorded\n- [ ] **Indexes used** documented\n- [ ] **Query optimization** notes (if applicable)\n- [ ] **Query file location** specified (repository path)\n- [ ] **Explain plan** included (for complex queries)\n- [ ] **Parameter values** documented (for parameterized queries)\n- [ ] **Query purpose** explained (what each query does)\n- [ ] **Data transformations** documented (aggregations, joins, filters)\n- [ ] **Error handling** specified (how errors were managed)\n\n### Analysis Environment\n\n- [ ] **Tool name and version** (DataPeeker v2.1.0)\n- [ ] **Programming language version** (Python 3.11.5, R 4.3.1)\n- [ ] **Library versions** (pandas 2.1.0, plotext 5.2.8, etc.)\n- [ ] **Operating system** (macOS 14.6.0, Ubuntu 22.04, etc.)\n- [ ] **Hardware specifications** (if relevant for performance-sensitive code)\n- [ ] **Environment variables** (configuration settings)\n- [ ] **Virtual environment** (requirements.txt, conda environment.yml)\n- [ ] **Docker image** (if containerized)\n- [ ] **Package manager** (pip, conda, npm versions)\n- [ ] **System dependencies** (database drivers, system libraries)\n\n### Process Documentation\n\n- [ ] **Step-by-step instructions** provided\n- [ ] **Installation steps** documented\n- [ ] **Configuration steps** explained\n- [ ] **Execution commands** listed\n- [ ] **Expected output** described\n- [ ] **Execution time estimates** provided\n- [ ] **Common errors** and solutions documented\n- [ ] **Prerequisites** listed (required access, credentials)\n- [ ] **Directory structure** explained\n- [ ] **Input file locations** specified\n\n### Data Quality\n\n- [ ] **NULL value counts** and handling documented\n- [ ] **Duplicate detection** methods and results\n- [ ] **Outlier detection** methods and thresholds\n- [ ] **Data validation** checks performed\n- [ ] **Missing data** handling approach\n- [ ] **Data cleaning** steps documented\n- [ ] **Quality metrics** computed and reported\n- [ ] **Assumptions** about data documented\n- [ ] **Known limitations** acknowledged\n- [ ] **Cross-validation** results (if performed)\n\n### Artifacts and Outputs\n\n- [ ] **Analysis scripts** version controlled\n- [ ] **Generated visualizations** archived\n- [ ] **Intermediate results** saved\n- [ ] **Final reports** stored\n- [ ] **Presentation materials** archived\n- [ ] **Commit hash** recorded (git SHA)\n- [ ] **Repository tag** created (version)\n- [ ] **File organization** documented\n- [ ] **Naming conventions** explained\n- [ ] **Backup locations** specified\n\n### Accessibility\n\n- [ ] **Repository URL** provided\n- [ ] **Clone instructions** documented\n- [ ] **Access permissions** specified\n- [ ] **Contact information** for questions\n- [ ] **License** specified (MIT, Apache, proprietary)\n- [ ] **Data sharing policy** clarified\n- [ ] **Embargo period** noted (if applicable)\n- [ ] **Citation format** suggested\n- [ ] **DOI** obtained (if published)\n- [ ] **Archive location** for long-term preservation\n\n---\n\n## Reproducibility Section Template\n\nInclude this section in your whitepaper or technical report:\n\n```markdown\n# Reproducibility Information\n\n## Data Sources\n\n### Primary Data: Production Sales Database\n\n**Database Details**:\n- **Host**: analytics-prod-db.example.com\n- **Database**: analytics_prod\n- **Schema**: public\n- **Table**: sales_metrics\n- **Access**: Read-only credentials required (contact: data-team@company.com)\n\n**Data Characteristics**:\n- **Query Timestamp**: 2025-11-25 14:30:00 UTC\n- **Data Period**: 2024-10-01 to 2024-12-31\n- **Records Examined**: 50,000 transactions\n- **Data Freshness**: Real-time (replication lag <5 minutes)\n\n**Schema**:\n| Column            | Type      | Constraints      | Description                    |\n|-------------------|-----------|------------------|--------------------------------|\n| transaction_id    | INTEGER   | PRIMARY KEY      | Unique transaction identifier  |\n| customer_id       | INTEGER   | NOT NULL, FK     | Customer identifier            |\n| region            | VARCHAR   | NOT NULL         | Geographic region              |\n| amount            | DECIMAL   | NOT NULL, >0     | Transaction amount (USD)       |\n| transaction_date  | DATE      | NOT NULL         | Date of transaction            |\n\n### Secondary Data: Customer Demographics\n\n**File Details**:\n- **Location**: `data/customer_demographics.csv`\n- **Format**: CSV, UTF-8 encoding\n- **Records**: 5,000 customers\n- **Checksum**: SHA256: abc123...def456\n- **Source**: Customer database snapshot (2024-12-31)\n\n## SQL Queries\n\n### Query 1: Regional Revenue Analysis\n\n**Purpose**: Aggregate transaction data by region and month for Q4 2024.\n\n**File**: `queries/q4_regional_revenue.sql`\n\n**Execution Details**:\n- **Runtime**: 2.3 seconds\n- **Rows Examined**: 50,000\n- **Rows Returned**: 18 (3 regions Ã— 6 months)\n- **Indexes Used**: date_region_idx, customer_id_idx\n\n**Full Query**:\n\\```sql\nSELECT\n  region,\n  DATE_TRUNC('month', transaction_date) as month,\n  SUM(amount) as total_revenue,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  AVG(amount) as avg_transaction,\n  COUNT(*) as transaction_count\nFROM analytics_prod.sales_metrics\nWHERE DATE(transaction_date) BETWEEN '2024-10-01' AND '2024-12-31'\nGROUP BY region, month\nORDER BY month, total_revenue DESC\n\\```\n\n### Query 2: Customer Acquisition\n\n**Purpose**: Identify new customers acquired during Q2-Q4 2024.\n\n**File**: `queries/customer_acquisition.sql`\n\n**Execution Details**:\n- **Runtime**: 1.8 seconds\n- **Rows Examined**: 50,000\n- **Rows Returned**: 342 (new customers)\n\n[Full query in Appendix B]\n\n## Analysis Environment\n\n### Software Versions\n\n**Primary Tools**:\n- **DataPeeker**: v2.1.0\n- **Python**: 3.11.5\n- **pandas**: 2.1.0\n- **plotext**: 5.2.8\n- **SQLAlchemy**: 2.0.20\n- **psycopg2**: 2.9.7 (PostgreSQL driver)\n\n**Operating System**:\n- **OS**: macOS 14.6.0\n- **Hardware**: Apple M2, 16GB RAM\n\n**Development Environment**:\n- **IDE**: VS Code v1.85.0\n- **Shell**: zsh 5.9\n- **Python Virtual Environment**: venv (Python 3.11.5)\n\n### Installation Instructions\n\n**1. Install Python and dependencies**:\n\\```bash\n# Ensure Python 3.11+ is installed\npython3 --version\n\n# Create virtual environment\npython3 -m venv venv\n\n# Activate virtual environment\nsource venv/bin/activate  # macOS/Linux\n# OR: venv\\Scripts\\activate  # Windows\n\n# Install dependencies\npip install -r requirements.txt\n\\```\n\n**2. Install DataPeeker**:\n\\```bash\ngit clone https://github.com/tilmon/datapeeker.git\ncd datapeeker\npip install -e .\n\\```\n\n**3. Configure database connection**:\n\\```bash\n# Set environment variable\nexport DB_CONN=\"postgresql://user:password@analytics-prod-db.example.com/analytics_prod\"\n\n# Or create .env file\necho \"DB_CONN=postgresql://user:password@host/database\" > .env\n\\```\n\n### Dependencies File (requirements.txt)\n\n\\```\npandas==2.1.0\nplotext==5.2.8\nsqlalchemy==2.0.20\npsycopg2-binary==2.9.7\npython-dotenv==1.0.0\n\\```\n\n## Data Quality Assessment\n\n### Completeness\n\n**NULL Value Analysis**:\n- **transaction_id**: 0 NULL values (0%)\n- **customer_id**: 0 NULL values (0%)\n- **region**: 0 NULL values (0%)\n- **amount**: 0 NULL values (0%)\n- **transaction_date**: 0 NULL values (0%)\n\n**Conclusion**: All critical fields 100% complete.\n\n### Consistency\n\n**Duplicate Detection**:\n- **Method**: Check for duplicate transaction_id\n- **Query**: `SELECT transaction_id, COUNT(*) FROM sales_metrics GROUP BY transaction_id HAVING COUNT(*) > 1`\n- **Result**: 0 duplicates found\n\n**Date Validation**:\n- **Expected Range**: 2024-10-01 to 2024-12-31\n- **Actual Range**: 2024-10-01 to 2024-12-31\n- **Out-of-range Records**: 0\n\n**Amount Validation**:\n- **Expected**: Positive decimal values\n- **Actual Range**: $5.00 to $5,000.00\n- **Invalid Values**: 0\n\n### Outliers\n\n**Method**: 3 MAD (Median Absolute Deviation)\n\n**Formula**:\n\\```\noutlier_score = |x - median(X)| / (MAD(X) * 1.4826)\nThreshold: outlier_score > 3\n\\```\n\n**Results**:\n- **Outliers Detected**: 12 transactions (0.024%)\n- **Outlier Values**: $4,500 - $5,000\n- **Review**: All validated as legitimate high-value enterprise transactions\n\n### Known Limitations\n\n1. **Time Period**: Analysis covers Q4 2024 only. Full-year patterns not captured.\n2. **Data Gaps**: Transactions from offline channels not included in database.\n3. **Attribution**: Customer acquisition source not tracked in this dataset.\n4. **Currency**: All amounts in USD. Multi-currency transactions converted at transaction date.\n\n## Reproducibility Instructions\n\n### Step 1: Clone Repository\n\n\\```bash\ngit clone https://github.com/tilmon/q4-analysis.git\ncd q4-analysis\n\\```\n\n### Step 2: Check Out Analysis Version\n\n\\```bash\n# Use specific commit for exact reproducibility\ngit checkout abc123def456\n\n# Or use tagged version\ngit checkout v1.0.0\n\\```\n\n### Step 3: Set Up Environment\n\n\\```bash\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n\\```\n\n### Step 4: Configure Database Access\n\n\\```bash\n# Request database credentials from data-team@company.com\n# Add credentials to .env file\necho \"DB_CONN=postgresql://user:password@host/database\" > .env\n\\```\n\n### Step 5: Run Analysis\n\n\\```bash\n# Run main analysis script\npython scripts/q4_regional_analysis.py\n\n# Expected output: Analysis results in analysis/q4-2024/\n\\```\n\n### Step 6: Generate Visualizations\n\n\\```bash\n# Create charts\npython scripts/create_visualizations.py\n\n# Expected output: PNG files in analysis/q4-2024/visualizations/\n\\```\n\n### Step 7: Generate Reports\n\n\\```bash\n# Build presentation\ncd analysis/q4-2024\nmarp presentation.md -o presentation.pdf\n\n# Build whitepaper\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -F pandoc-crossref \\\n  -s --toc --number-sections \\\n  -V geometry:margin=1in \\\n  -o whitepaper.pdf\n\\```\n\n### Expected Runtime\n\n- **Data Extraction**: ~5 seconds\n- **Analysis Computation**: ~10 seconds\n- **Visualization Generation**: ~15 seconds\n- **Report Generation**: ~30 seconds\n- **Total**: ~60 seconds\n\n### Validation\n\n**Verify outputs match expected results**:\n\\```bash\n# Check output files exist\nls -l analysis/q4-2024/\n# Expected: 01-findings.md, presentation.pdf, whitepaper.pdf\n\n# Compare checksums (if provided)\nsha256sum analysis/q4-2024/01-findings.md\n# Expected: [checksum value]\n\\```\n\n## Repository Information\n\n**Repository URL**: https://github.com/tilmon/q4-analysis\n\n**Commit Hash**: abc123def456 (main branch)\n\n**Tag**: v1.0.0 (Q4 2024 Analysis - Final)\n\n**License**: MIT License\n\n**Contact**: data-team@tilmon.com for questions or access requests\n\n## Data Archival\n\n**Long-term Preservation**:\n- **Primary**: GitHub repository (github.com/tilmon/q4-analysis)\n- **Secondary**: Company internal backup (backup.tilmon.com/analysis/q4-2024/)\n- **Tertiary**: Zenodo DOI (10.5281/zenodo.XXXXXX) for public data\n\n**Retention Policy**: Maintained for 7 years per company policy\n\n## Citation\n\nIf referencing this analysis, please cite as:\n\n\\```\nAnalytics Team (2025). West Coast Expansion Analysis: Q4 2024\nPerformance Evaluation. Tilmon Engineering.\nhttps://github.com/tilmon/q4-analysis\n\\```\n\n**BibTeX**:\n\\```bibtex\n@techreport{tilmon_q4_analysis_2025,\n  author = {{Analytics Team}},\n  title = {West Coast Expansion Analysis: Q4 2024 Performance Evaluation},\n  institution = {Tilmon Engineering},\n  year = {2025},\n  url = {https://github.com/tilmon/q4-analysis},\n  note = {Version 1.0.0. Commit: abc123def456}\n}\n\\```\n```\n\n---\n\n## Reproducibility Levels\n\n### Level 1: Reviewable (Minimum)\n- Full methodology documented\n- SQL queries included in appendix\n- Data sources cited\n- Software versions listed\n\n### Level 2: Repeatable (Standard)\n- Code available in repository\n- Dependencies specified\n- Step-by-step instructions provided\n- Expected outputs documented\n\n### Level 3: Reproducible (Target)\n- One-command execution\n- Environment automated (Docker, conda)\n- Validation checks included\n- Outputs checksummed for verification\n\n### Level 4: Replicable (Gold Standard)\n- Data publicly accessible (or synthetic data provided)\n- Automated testing pipeline\n- DOI assigned for citation\n- Long-term archival (Zenodo, institutional repository)\n\n**Aim for Level 3 (Reproducible)** for internal analysis.\n**Aim for Level 4 (Replicable)** for published research.\n\n---\n\n## Common Challenges and Solutions\n\n### Challenge: Proprietary Data\n\n**Solution**: Provide synthetic data with same structure\n```python\n# generate_synthetic_data.py\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)  # Reproducible randomness\n\n# Generate synthetic transactions matching real schema\nsynthetic_data = pd.DataFrame({\n    'transaction_id': range(1, 50001),\n    'customer_id': np.random.randint(1, 5001, 50000),\n    'region': np.random.choice(['West Coast', 'East Coast', 'Central'], 50000),\n    'amount': np.random.lognormal(5, 1, 50000),\n    'transaction_date': pd.date_range('2024-10-01', '2024-12-31', periods=50000)\n})\n\nsynthetic_data.to_csv('data/synthetic_transactions.csv', index=False)\n```\n\n**Document in reproducibility section**:\n```markdown\n## Data Access Limitations\n\nOriginal data is proprietary and cannot be shared. Synthetic dataset\nprovided (`data/synthetic_transactions.csv`) preserves statistical\nproperties (distributions, correlations) while protecting sensitive\ninformation. Analysis code runs identically on synthetic data.\n```\n\n### Challenge: Large Data Files\n\n**Solution**: Provide sample + instructions for full dataset\n```markdown\n## Large Dataset Access\n\n**Sample Data**: `data/sample_transactions.csv` (1,000 records)\n- Use for testing and development\n- Sufficient for validating code\n\n**Full Dataset**: 50,000 records (12MB compressed)\n- Available via internal data warehouse\n- Contact: data-team@company.com\n- Access requires approved project code\n```\n\n### Challenge: Credentials and Secrets\n\n**Solution**: Template configuration with instructions\n```markdown\n## Configuration\n\n**Create `.env` file** (not in version control):\n\\```\nDB_CONN=postgresql://user:password@host/database\nAPI_KEY=your_api_key_here\n\\```\n\n**Request credentials** from data-team@company.com\n\n**Never commit** `.env` to repository (check `.gitignore`)\n```\n\n### Challenge: Complex Environment\n\n**Solution**: Use Docker for containerization\n```dockerfile\n# Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"scripts/q4_regional_analysis.py\"]\n```\n\n**Instructions**:\n```bash\n# Build container\ndocker build -t q4-analysis .\n\n# Run analysis\ndocker run -v $(pwd)/analysis:/app/analysis q4-analysis\n```\n\n---\n\n## Best Practices\n\n### DO:\n\nâœ… **Use version control** (git) for all code and documentation\n\nâœ… **Pin dependency versions** (requirements.txt, environment.yml)\n\nâœ… **Document every SQL query** with purpose and execution details\n\nâœ… **Record timestamps** for all data queries\n\nâœ… **Include checksums** for data files\n\nâœ… **Provide step-by-step instructions** that assume no prior knowledge\n\nâœ… **Test reproducibility** on a fresh machine/environment\n\nâœ… **Archive final outputs** with version tags\n\nâœ… **Use random seeds** for any stochastic processes\n\nâœ… **Document known limitations** and caveats\n\n### DON'T:\n\nâŒ **Hardcode credentials** in scripts\n\nâŒ **Rely on undocumented external dependencies**\n\nâŒ **Skip data quality checks and documentation**\n\nâŒ **Assume others have same system configuration**\n\nâŒ **Use relative paths without documentation**\n\nâŒ **Forget to document manual steps**\n\nâŒ **Skip testing on clean environment**\n\nâŒ **Omit error messages and troubleshooting**\n\n---\n\n## Reproducibility Testing\n\nBefore publishing, test reproducibility:\n\n### 1. Clean Machine Test\n\n```bash\n# Set up new VM or use colleague's machine\n# Follow ONLY your documentation\n# No other knowledge allowed\n\n# Document any steps missing from instructions\n# Update documentation accordingly\n```\n\n### 2. Automated Validation\n\n```python\n# validate_outputs.py\nimport hashlib\n\ndef validate_output(file_path, expected_checksum):\n    \"\"\"Verify output matches expected result.\"\"\"\n    with open(file_path, 'rb') as f:\n        actual = hashlib.sha256(f.read()).hexdigest()\n\n    if actual == expected_checksum:\n        print(f\"âœ“ {file_path} matches expected output\")\n        return True\n    else:\n        print(f\"âœ— {file_path} differs from expected\")\n        print(f\"  Expected: {expected_checksum}\")\n        print(f\"  Actual:   {actual}\")\n        return False\n\n# Test key outputs\nvalidate_output('analysis/01-findings.md', 'abc123...')\nvalidate_output('visualizations/revenue-chart.png', 'def456...')\n```\n\n### 3. Peer Review\n\nAsk colleague to:\n- Clone repository\n- Follow reproducibility instructions\n- Report any missing steps or unclear instructions\n- Verify outputs match\n\n---\n\n## Reproducibility Statement Template\n\nInclude in your publication:\n\n```markdown\n## Reproducibility Statement\n\nThis analysis was conducted following reproducible research principles.\nAll data sources, SQL queries, analysis code, and visualization scripts\nare available in the public repository:\n\n**Repository**: https://github.com/tilmon/q4-analysis\n**Version**: v1.0.0 (Commit: abc123def456)\n**License**: MIT\n\nDetailed reproducibility information is provided in Appendix A,\nincluding:\n- Data sources and query timestamps\n- Complete SQL query text\n- Software environment specification\n- Step-by-step execution instructions\n- Expected runtime and outputs\n- Validation procedures\n\nOriginal data is proprietary. Synthetic dataset with identical\nstatistical properties is provided for reproducibility testing.\n\nAnalysis was tested for reproducibility on macOS 14.6, Ubuntu 22.04,\nand Windows 11 systems. All outputs matched expected checksums within\nnumerical precision limits (Â±0.01%).\n\nQuestions or issues: data-team@tilmon.com\n```\n\n---\n\n## References\n\n**Reproducible Research Standards**:\n- [FAIR Principles](https://www.go-fair.org/fair-principles/) - Findable, Accessible, Interoperable, Reusable\n- [Reproducibility Guide](https://the-turing-way.netlify.app/reproducible-research/reproducible-research.html) - The Turing Way\n- [Best Practices for Scientific Computing](https://doi.org/10.1371/journal.pbio.1001745)\n\n**Related DataPeeker Skills**:\n- `formats/citations.md` - Citing data sources\n- `tools/pandoc.md` - Creating reproducible documents\n- `SKILL.md` - Phase 4: Add Citations & Reproducibility\n",
        "plugins/datapeeker/skills/presenting-data/frameworks/3-paragraph-essay.md": "# The 3-Paragraph Essay Structure for Data Presentations\n\nThe classic three-paragraph essay provides a proven framework for organizing data-driven presentations and whitepapers. This structure ensures clarity, logical flow, and persuasive impact.\n\n---\n\n## Why This Structure Works for Data Analysis\n\n**Cognitive Load Management**: Audiences process information more effectively when presented with clear structure:\n1. Tell them what you're going to tell them (Introduction)\n2. Tell them (Body)\n3. Tell them what you told them (Conclusion)\n\n**Decision Support**: Stakeholders need to understand:\n- What question was asked\n- What evidence supports the answer\n- What actions to take based on findings\n\n**Reproducibility**: Scholarly communication requires:\n- Clear research question\n- Documented methodology\n- Traceable citations\n\n---\n\n## The Four-Part Structure\n\n### Part 1: Introduction & Thesis\n\n**Purpose**: Establish context, state your key finding or recommendation upfront\n\n**Components**:\n1. **Business Context** - Why this analysis matters\n2. **Research Question** - What you investigated\n3. **Thesis Statement** - Your key finding or recommendation\n4. **Preview** - Roadmap of evidence to follow\n\n**Example (Presentation Slide)**:\n```markdown\n# Q4 Sales Analysis: Key Finding\n\n**Context**: Q4 is our highest revenue quarter\n\n**Question**: What drove 2024 Q4 performance?\n\n**Finding**: West Coast expansion delivered 23% YoY growth,\nexceeding targets by 15%\n\n**Preview**: This presentation examines:\n- Regional performance data\n- Customer acquisition metrics\n- Profitability by segment\n```\n\n**Example (Whitepaper)**:\n```markdown\n# Introduction\n\nThe fourth quarter represents our highest-revenue period,\naccounting for 40% of annual sales. Understanding drivers\nof Q4 performance enables more effective resource allocation\nand strategic planning for future quarters.\n\n## Research Question\n\nThis analysis addresses: What factors drove Q4 2024 sales\nperformance compared to prior years and internal projections?\n\n## Thesis Statement\n\nWest Coast regional expansion, initiated in Q2 2024, delivered\n23% year-over-year growth and exceeded quarterly targets by 15%.\nThis growth was driven primarily by increased customer acquisition\n(+342 accounts) and higher average transaction values (+12%).\n\n## Overview\n\nThis whitepaper examines Q4 2024 sales data through three lenses:\nregional performance, customer behavior, and profitability metrics.\nSection 2 details our data sources and methodology. Section 3 presents\nfindings with supporting visualizations. Section 4 discusses\nimplications and recommendations. Full reproducibility documentation\nis provided in Section 5.\n```\n\n**Best Practices**:\n- **Lead with the finding** - Don't make audience wait\n- **Be specific** - \"23% growth\" not \"significant growth\"\n- **Set expectations** - Preview what evidence follows\n- **Establish relevance** - Connect to business objectives\n\n---\n\n### Part 2: Body & Supporting Arguments\n\n**Purpose**: Present evidence that supports your thesis systematically\n\n**Structure Options**:\n\n**Option A: By Finding** (for multiple independent findings)\n```markdown\n## Finding 1: Regional Performance\n[Evidence, data, visualization]\n\n## Finding 2: Customer Acquisition\n[Evidence, data, visualization]\n\n## Finding 3: Transaction Values\n[Evidence, data, visualization]\n```\n\n**Option B: By Argument** (for building cumulative case)\n```markdown\n## Growth Was Geographically Concentrated\n[Evidence showing West Coast dominance]\n\n## West Coast Growth Came from New Customers\n[Evidence showing acquisition vs retention]\n\n## New Customers Have Higher Average Transaction Values\n[Evidence showing spending patterns]\n\nTherefore: West Coast expansion strategy is working\n```\n\n**Option C: Chronological** (for time-series analysis)\n```markdown\n## Q1-Q2: Planning and Setup Phase\n[Evidence from early quarters]\n\n## Q3: Initial Expansion Launch\n[Evidence from launch period]\n\n## Q4: Acceleration and Results\n[Evidence from peak period]\n```\n\n**Components for Each Section**:\n1. **Claim** - State what the data shows\n2. **Evidence** - Present query results, metrics, visualizations\n3. **Analysis** - Interpret what it means\n4. **Methodology** - Document how you obtained the data\n5. **Caveats** - Note limitations or alternative explanations\n\n**Example (Presentation Slide)**:\n```markdown\n## Finding 1: West Coast Revenue Dominance\n\n**Claim**: West Coast region accounted for 45% of Q4 growth\n\n**Evidence**:\n- West Coast: $1.2M (+$450K vs Q4 2023)\n- Other regions: Combined $1.8M (+$200K vs Q4 2023)\n\n![width:700px](regional-revenue-chart.png)\n\n**Methodology**: Query from `analytics_prod.sales_metrics`\n```\n\n**Example (Whitepaper)**:\n```markdown\n# Results\n\n## Finding 1: Regional Performance Concentration\n\nOur analysis reveals that Q4 2024 growth was geographically\nconcentrated in the West Coast region.\n\n### Evidence\n\nWest Coast regional revenue reached $1.2M in Q4 2024,\nrepresenting a $450K increase (+60%) compared to Q4 2023.\nAll other regions combined grew by $200K (+12%) over the\nsame period. Figure @fig:regional shows the breakdown.\n\n![Regional Revenue Comparison](regional-revenue.png){#fig:regional}\n\n**Table @tbl:regional summarizes the data:**\n\n| Region      | Q4 2023 | Q4 2024 | Growth  | % of Total Growth |\n|-------------|---------|---------|---------|-------------------|\n| West Coast  | $750K   | $1,200K | +$450K  | 69%               |\n| East Coast  | $800K   | $900K   | +$100K  | 15%               |\n| Central     | $650K   | $750K   | +$100K  | 15%               |\n| **Total**   | $2,200K | $2,850K | +$650K  | 100%              |\n\n: Regional revenue breakdown {#tbl:regional}\n\n### Methodology\n\nData was extracted from the production analytics database\nusing the following query:\n\n\\```sql\nSELECT\n  region,\n  EXTRACT(year FROM transaction_date) as year,\n  EXTRACT(quarter FROM transaction_date) as quarter,\n  SUM(amount) as total_revenue,\n  COUNT(DISTINCT customer_id) as unique_customers\nFROM analytics_prod.sales_metrics\nWHERE EXTRACT(quarter FROM transaction_date) = 4\n  AND EXTRACT(year FROM transaction_date) IN (2023, 2024)\nGROUP BY region, year, quarter\nORDER BY year, total_revenue DESC\n\\```\n\n**Query execution details:**\n- Execution time: 2.3 seconds\n- Rows examined: 50,000 transactions\n- Rows returned: 6 (3 regions Ã— 2 years)\n- Query timestamp: 2025-11-25 14:30:00 UTC\n\n### Analysis\n\nThe concentration of growth in the West Coast region aligns\nwith our Q2 2024 expansion strategy, which targeted high-value\ncustomers in tech-heavy markets. This finding is consistent\nwith prior research on geographic market penetration [@jones2024].\n\n### Caveats and Alternative Explanations\n\nSeveral factors may contribute to this pattern:\n1. West Coast expansion received higher marketing budget allocation\n2. Seasonal factors (tech industry Q4 spending cycles)\n3. Customer base differences (B2B vs B2C mix varies by region)\n\nFurther analysis (see Section 4.2) examines whether growth\nis sustainable or represents one-time factors.\n```\n\n**Best Practices**:\n- **One claim per section** - Don't mix multiple findings\n- **Show your work** - Include SQL queries and methodology\n- **Use visualizations** - Reference `creating-visualizations` skill\n- **Address alternatives** - Consider competing explanations\n- **Cite sources** - Reference data sources and prior research\n\n---\n\n### Part 3: Conclusion & Next Steps\n\n**Purpose**: Synthesize findings, articulate implications, define actions\n\n**Components**:\n1. **Restate Thesis** - Remind audience of key finding\n2. **Summarize Evidence** - Recap most important support\n3. **Implications** - What this means for the business\n4. **Recommendations** - Specific actions to take\n5. **Limitations** - What we don't know yet\n6. **Next Steps** - Follow-up questions or analysis\n\n**Example (Presentation Slide)**:\n```markdown\n## Conclusions & Recommendations\n\n### Key Finding\nWest Coast expansion delivered 23% YoY growth,\ndriven by customer acquisition and higher transaction values\n\n### Implications\n- Expansion strategy is working\n- Model can potentially scale to other regions\n- Current trajectory meets 2025 targets\n\n### Recommendations\n1. Continue West Coast investment\n2. Pilot similar approach in East Coast markets\n3. Study customer retention patterns (6-month follow-up)\n\n### Limitations\n- Only 6 months of West Coast expansion data\n- Seasonal effects not fully isolated\n- Competitive response not yet visible\n```\n\n**Example (Whitepaper)**:\n```markdown\n# Conclusions\n\n## Summary of Findings\n\nThis analysis examined Q4 2024 sales performance to identify\ngrowth drivers. Three key findings emerged:\n\n1. **Regional Concentration**: West Coast region accounted for\n   69% of total growth ($450K of $650K increase)\n\n2. **Customer Acquisition**: 342 new West Coast customers joined,\n   representing 28% of all new accounts\n\n3. **Transaction Value**: New West Coast customers averaged 12%\n   higher transaction values ($285 vs $254)\n\nThese findings support our thesis that the Q2 2024 West Coast\nexpansion strategy delivered measurable results exceeding\ninternal projections.\n\n## Business Implications\n\n### Strategic Validation\n\nThe West Coast expansion represents our first systematic attempt\nat geographic market development. Success validates the approach:\ntargeted sales presence, localized marketing, and tech-sector\nfocus. This model may be adaptable to other high-value markets.\n\n### Revenue Impact\n\nQ4 2024 performance exceeded targets by 15% ($2.85M actual vs\n$2.48M projected). At current growth rates, West Coast region\nwill contribute $5M annually by end of 2025, representing 35%\nof total revenue.\n\n### Operational Requirements\n\nSustaining growth requires continued investment in West Coast\nsales team (currently 3 FTE) and marketing budget ($50K/quarter).\nROI analysis (Appendix B) shows positive returns within 18 months.\n\n## Recommendations\n\nBased on these findings, we recommend:\n\n1. **Continue West Coast Investment** (Priority: High)\n   - Maintain current sales team staffing\n   - Sustain marketing budget through 2025\n   - Monitor retention metrics monthly\n\n2. **Pilot East Coast Expansion** (Priority: Medium)\n   - Apply West Coast playbook to Boston/NYC markets\n   - Start with 2 FTE sales presence in Q2 2025\n   - Budget: $75K for 6-month pilot\n\n3. **Study Customer Retention** (Priority: High)\n   - Track cohort retention at 6, 12, 18 months\n   - Identify churn risk factors\n   - Refine onboarding for new customers\n\n4. **Competitive Analysis** (Priority: Medium)\n   - Monitor competitor responses to our presence\n   - Track pricing pressure indicators\n   - Assess market share gains vs expansion\n\n## Limitations and Caveats\n\nSeveral factors limit the certainty of our conclusions:\n\n**Limited Time Window**: Only 6 months of West Coast expansion\ndata. Longer-term sustainability remains unproven.\n\n**Seasonal Effects**: Q4 includes year-end spending cycles.\nFull-year analysis needed to isolate seasonal vs structural growth.\n\n**Attribution Uncertainty**: Growth correlation with expansion\ndoes not prove causation. Other factors (product improvements,\nmarket conditions) may contribute.\n\n**Data Quality**: Source data examined for quality issues\n(see Appendix A). Minor data gaps noted but deemed non-material.\n\n## Future Research Directions\n\nThis analysis raises several questions for future investigation:\n\n1. **Retention Analysis**: Do new West Coast customers exhibit\n   different retention patterns than other cohorts?\n\n2. **Profitability Deep Dive**: Are West Coast customers more\n   or less profitable after accounting for acquisition costs?\n\n3. **Product Mix**: Do regional differences in product preferences\n   explain transaction value variations?\n\n4. **Competitive Dynamics**: How are competitors responding to\n   our market entry?\n\nWe recommend follow-up analysis in Q2 2025 to address these\nquestions systematically.\n\n## Final Remarks\n\nThe West Coast expansion represents a strategic success, delivering\nmeasurable growth that exceeds projections. While limitations\nremain, evidence strongly supports continued investment and\npotential replication in similar markets. Sustained monitoring\nand systematic evaluation will ensure optimal resource allocation\ngoing forward.\n```\n\n**Best Practices**:\n- **Echo introduction** - Come full circle to opening question\n- **Be actionable** - Give specific next steps\n- **Acknowledge limits** - Build credibility with honest caveats\n- **Look forward** - Identify future questions\n- **End strong** - Final sentence should be memorable\n\n---\n\n### Part 4: Bibliography & Supporting Documentation\n\n**Purpose**: Enable reproducibility, provide traceability, give proper attribution\n\n**Components**:\n1. **References** - Prior research and methodology sources\n2. **Data Sources** - Database connections, file locations, APIs\n3. **Query Repository** - SQL queries used in analysis\n4. **Tool Documentation** - Versions, configuration, dependencies\n5. **Appendices** - Supporting details, extended tables, code\n\n**Example (Presentation Slide)**:\n```markdown\n## Reproducibility & References\n\n### Data Sources\n- Database: `analytics_prod.sales_metrics`\n- Period: 2024-10-01 to 2024-12-31\n- Query: `queries/q4_regional_analysis.sql`\n\n### Documentation\n- Full methodology: [Technical Whitepaper](./whitepaper.pdf)\n- Analysis repo: [github.com/tilmon/q4-analysis](https://github.com/tilmon/q4-analysis)\n- DataPeeker version: 2.1.0\n\n### References\n- Regional expansion strategy: Smith et al. (2024)\n- Growth framework: Jones (2023)\n```\n\n**Example (Whitepaper - References Section)**:\n```markdown\n# References\n\n[@jones2024] Jones, M. & Williams, S. (2024). \"Geographic Market\nPenetration Strategies for B2B SaaS Companies.\" *Journal of Sales\nResearch*, 15(3), 234-256. DOI: 10.1234/jsr.2024.15.3\n\n[@smith2023] Smith, J. (2023). \"Reproducibility Standards for\nBusiness Analytics.\" *Data Science Quarterly*, 8(2), 112-128.\n\n[@datapeeker2025] Tilmon Engineering. (2025). *DataPeeker:\nSQL Analysis Tool* (Version 2.1.0). Retrieved from\nhttps://github.com/tilmon/datapeeker\n\n[@sales_db2025] Tilmon Engineering. (2025). *Production Sales\nMetrics Database* [analytics_prod.sales_metrics].\nQuery timestamp: 2025-11-25 14:30:00 UTC.\n\n# Appendix A: Data Quality Assessment\n\n[Detailed data quality notes]\n\n# Appendix B: SQL Queries\n\n## Query 1: Regional Revenue by Quarter\n\n\\```sql\n-- Regional revenue comparison Q4 2023 vs Q4 2024\n-- Execution time: 2.3 seconds\n-- Rows returned: 6\n\nSELECT\n  region,\n  EXTRACT(year FROM transaction_date) as year,\n  EXTRACT(quarter FROM transaction_date) as quarter,\n  SUM(amount) as total_revenue,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  COUNT(*) as transaction_count\nFROM analytics_prod.sales_metrics\nWHERE EXTRACT(quarter FROM transaction_date) = 4\n  AND EXTRACT(year FROM transaction_date) IN (2023, 2024)\nGROUP BY region, year, quarter\nORDER BY year, total_revenue DESC\n\\```\n\n# Appendix C: Environment and Tools\n\n**Analysis Environment:**\n- DataPeeker: v2.1.0\n- Python: 3.11.5\n- pandas: 2.1.0\n- plotext: 5.2.8\n- Operating System: macOS 14.6.0\n\n**Database Connection:**\n- Host: analytics-prod-db.example.com\n- Database: analytics_prod\n- Schema: public\n- Table: sales_metrics\n\n**Reproducibility:**\n1. Clone analysis repository: `git clone github.com/tilmon/q4-analysis`\n2. Install dependencies: `pip install -r requirements.txt`\n3. Set database credentials: `export DB_CONN=<connection_string>`\n4. Run analysis: `python scripts/q4_regional.py`\n5. Generate report: `pandoc report.md -o report.pdf`\n\nAll analysis code and data transformations are version controlled\nand publicly accessible at the repository above.\n```\n\n**Best Practices for Bibliography**:\n- **Use BibTeX format** - Standard for academic/technical documents\n- **Cite data sources** - Databases have provenance too\n- **Document tools** - Tool versions matter for reproducibility\n- **Provide links** - Make it easy to access referenced materials\n- **Include queries** - SQL text should be in appendix or repository\n- **Version control** - Commit hashes enable precise reproducibility\n\n---\n\n## Adapting the Structure\n\n### For Short Presentations (5-10 slides)\n\n**Collapse structure**:\n1. **Slide 1-2**: Introduction & Thesis (context + key finding)\n2. **Slide 3-7**: Body (one finding per slide)\n3. **Slide 8**: Conclusion & Recommendations\n4. **Slide 9**: References & Reproducibility\n\n### For Long Whitepapers (40-80 pages)\n\n**Expand structure**:\n1. **Introduction** (3-5 pages)\n   - Background and context\n   - Literature review\n   - Research question\n   - Thesis statement\n   - Document roadmap\n\n2. **Methodology** (5-10 pages)\n   - Data sources and collection\n   - Analysis frameworks\n   - Tool documentation\n   - Quality assessment procedures\n\n3. **Results** (20-40 pages)\n   - Finding 1 with sub-sections\n   - Finding 2 with sub-sections\n   - Finding 3 with sub-sections\n   - Visualizations and tables\n\n4. **Discussion** (5-10 pages)\n   - Interpretation of findings\n   - Comparison to prior research\n   - Alternative explanations\n   - Limitations and caveats\n\n5. **Conclusions** (3-5 pages)\n   - Summary of key findings\n   - Business implications\n   - Recommendations\n   - Future research directions\n\n6. **References** (2-5 pages)\n   - Bibliography\n   - Data source citations\n\n7. **Appendices** (10-20 pages)\n   - SQL queries\n   - Extended tables\n   - Data quality reports\n   - Code listings\n\n### For Mixed Audiences (Slides + Whitepaper)\n\n**Create hierarchy**:\n- **Slides**: Introduction, key findings, recommendations (thesis + conclusion)\n- **Whitepaper**: Full detail including methodology and reproducibility\n- **Link between formats**: Last slide references whitepaper for details\n\n---\n\n## Common Patterns\n\n### Pattern 1: Problem-Solution Structure\n\n```markdown\n**Introduction**: Problem statement and proposed solution\n**Body**: Evidence that problem exists, evidence that solution works\n**Conclusion**: Recommendation to implement solution\n**Bibliography**: Prior solutions attempted, methodology references\n```\n\n**Use for**: Proposing changes, recommending actions\n\n### Pattern 2: Question-Answer Structure\n\n```markdown\n**Introduction**: Research question and answer preview\n**Body**: Evidence systematically answering the question\n**Conclusion**: Direct answer with implications\n**Bibliography**: Data sources, prior research on topic\n```\n\n**Use for**: Exploratory analysis, answering stakeholder questions\n\n### Pattern 3: Comparative Structure\n\n```markdown\n**Introduction**: Two options being compared, recommendation\n**Body**: Evidence comparing option A vs option B across dimensions\n**Conclusion**: Restate recommendation with implications\n**Bibliography**: Comparison methodology, data sources\n```\n\n**Use for**: A/B tests, vendor selection, strategy choices\n\n### Pattern 4: Chronological Structure\n\n```markdown\n**Introduction**: Time period examined and key trend identified\n**Body**: Evidence showing trend evolution over time\n**Conclusion**: Implications of trend, forecast for future\n**Bibliography**: Time-series methodology, seasonal adjustment\n```\n\n**Use for**: Trend analysis, forecasting, historical studies\n\n---\n\n## Checklist: Applying the 3-Paragraph Structure\n\nBefore finalizing your presentation or whitepaper:\n\n**Introduction:**\n- [ ] Context establishes why this analysis matters\n- [ ] Research question clearly stated\n- [ ] Thesis/key finding stated upfront (don't make audience wait)\n- [ ] Preview of evidence provided\n\n**Body:**\n- [ ] Each section makes one clear claim\n- [ ] Evidence directly supports each claim\n- [ ] Methodology documented (SQL queries, data sources)\n- [ ] Alternative explanations considered\n- [ ] Visualizations effectively illustrate patterns\n\n**Conclusion:**\n- [ ] Thesis restated in light of evidence\n- [ ] Implications for business articulated\n- [ ] Specific recommendations provided\n- [ ] Limitations honestly acknowledged\n- [ ] Future questions identified\n\n**Bibliography:**\n- [ ] Data sources cited with timestamps\n- [ ] Prior research properly attributed\n- [ ] Tools and versions documented\n- [ ] Queries included in appendix or repository\n- [ ] Reproducibility instructions complete\n\n---\n\n## Examples in Practice\n\nSee the main `SKILL.md` for complete examples of:\n- Marp presentations using this structure\n- Pandoc whitepapers using this structure\n- Integration with `creating-visualizations` skill for evidence\n- Citation and reproducibility documentation\n\n---\n\n## References\n\nThis framework draws on established principles from:\n- Academic writing conventions (thesis-body-conclusion)\n- Technical communication best practices\n- Data storytelling literature\n- Reproducible research standards\n\nFor implementation details, see:\n- `tools/marp.md` - Creating presentations with this structure\n- `tools/pandoc.md` - Creating whitepapers with this structure\n- `formats/citations.md` - Documenting sources in bibliography\n- `formats/reproducibility.md` - Enabling reproducibility\n",
        "plugins/datapeeker/skills/presenting-data/frameworks/narrative-structure.md": "# Narrative Structure for Data Storytelling\n\nData becomes compelling when presented as a story. This guide provides narrative frameworks for transforming analysis findings into engaging presentations and whitepapers.\n\n---\n\n## Why Data Needs Narrative\n\n**Humans think in stories**: Our brains are wired to process information through narrative structures with characters, conflict, and resolution.\n\n**Data alone doesn't persuade**: Numbers, charts, and statistics are evidence, but evidence requires context and interpretation to drive decisions.\n\n**Stories create memorable understanding**: Stakeholders remember stories long after they forget individual metrics.\n\n---\n\n## The Classic Story Arc for Data\n\n### Setup â†’ Conflict â†’ Resolution â†’ Call to Action\n\n```\n    Resolution\n        /\\\n       /  \\\n      /    \\    Call to Action\n     /      \\  /\nSetup -----> Conflict\n```\n\nThis four-part structure adapts perfectly to data presentations:\n\n1. **Setup**: Establish business context and objectives\n2. **Conflict**: Present the data challenge, question, or surprising pattern\n3. **Resolution**: Show findings that resolve the conflict\n4. **Call to Action**: Define next steps based on findings\n\n---\n\n## Part 1: Setup (Establishing Context)\n\n**Purpose**: Orient the audience before presenting data\n\n**Key Elements**:\n- **Business context**: Why this analysis matters\n- **Stakeholder goals**: What decisions need support\n- **Time period**: When the story takes place\n- **Key metrics**: What we're measuring\n\n**Example (Presentation)**:\n```markdown\n# Q4 Sales Analysis\n\n## The Context\n\n- Q4 is our highest revenue quarter (40% of annual sales)\n- We launched West Coast expansion in Q2 2024\n- Target: $2.5M Q4 revenue (20% YoY growth)\n- Question: Did the expansion deliver results?\n```\n\n**Example (Whitepaper)**:\n```markdown\n# Introduction: The West Coast Expansion Story\n\nIn April 2024, our company made a strategic decision to expand\ninto West Coast markets. This expansion represented our first\nsystematic geographic market development initiative, requiring\ninvestment of $200K and commitment of 3 sales FTE.\n\nBy October 2024, stakeholders needed to answer a critical question:\nShould we continue investing in West Coast operations, or redirect\nresources to other growth initiatives? This decision required\ndata-driven evaluation of expansion performance.\n\nThis analysis examines Q4 2024 sales data to determine whether\nthe West Coast expansion delivered measurable results.\n```\n\n**Setup Patterns**:\n\n**Pattern A: The Challenge**\n```markdown\nOur company faces [specific challenge]. Historical data shows\n[context]. We need to decide whether to [decision point].\nThis analysis examines [data] to answer [question].\n```\n\n**Pattern B: The Opportunity**\n```markdown\nRecent trends suggest [opportunity]. If true, this could\n[business impact]. We investigated [data] to validate whether\n[hypothesis] holds.\n```\n\n**Pattern C: The Mandate**\n```markdown\nLeadership has asked [question]. This question matters because\n[business impact]. We analyzed [data sources] from [time period]\nto provide an evidence-based answer.\n```\n\n**Best Practices**:\n- **Be specific about stakes**: What rides on this analysis?\n- **Humanize the context**: Reference real people and decisions\n- **Set clear expectations**: What question will be answered?\n- **Establish credibility**: Note data sources and time period\n\n---\n\n## Part 2: Conflict (The Data Challenge)\n\n**Purpose**: Create narrative tension by presenting the problem, question, or surprising finding\n\n**Types of Conflict**:\n\n### Type A: The Mystery\n```markdown\nSomething unexpected appeared in the data.\nWhat explains this pattern?\n```\n\n**Example**:\n```markdown\n## The Unexpected Pattern\n\nIn September 2024, we noticed something unusual: West Coast\nsales were accelerating faster than projections. By October,\nthe trend was unmistakable.\n\n**The Mystery**: Were we seeing sustainable growth, or a\ntemporary spike that would correct itself?\n```\n\n### Type B: The Challenge\n```markdown\nWe need to achieve a difficult goal.\nCan the data show us a path forward?\n```\n\n**Example**:\n```markdown\n## The Challenge\n\nTo hit 2025 targets, we need 30% revenue growth. Our traditional\nmarkets are mature (historical growth: 8-12%). West Coast\nexpansion was our bet on acceleration.\n\n**The Question**: Did the expansion work? Can we replicate it?\n```\n\n### Type C: The Dilemma\n```markdown\nWe face a choice between competing options.\nWhich does the data support?\n```\n\n**Example**:\n```markdown\n## The Decision Point\n\nWest Coast expansion cost $200K. We can:\nA) Continue investing (another $200K in 2025)\nB) Cut losses and redirect resources\nC) Maintain but don't expand\n\n**The Data Challenge**: Determine which option is justified.\n```\n\n### Type D: The Validation\n```markdown\nWe have a hypothesis or strategy.\nDoes the data confirm it?\n```\n\n**Example**:\n```markdown\n## Testing the Strategy\n\nOur expansion thesis: Tech-sector customers in major West Coast\ncities will pay 10-15% premium for localized service.\n\n**Six months later**: Time to see if data validates the bet.\n```\n\n**Conflict Patterns**:\n\n**Pattern A: The Surprising Trend**\n```markdown\n## What We Expected vs What We Found\n\n**Expected**: Gradual adoption, 10% growth over 12 months\n**Found**: Rapid acceleration, 23% growth in 6 months\n\n[Chart showing projection vs actual]\n\nThis unexpected pattern demanded explanation.\n```\n\n**Pattern B: The Unexplained Gap**\n```markdown\n## The Performance Gap\n\nWhile overall company growth was 8%, one segment showed 40% growth.\n\n[Visualization highlighting the outlier]\n\nWhat accounted for this five-fold difference?\n```\n\n**Pattern C: The Critical Question**\n```markdown\n## The $400K Question\n\nLeadership asked: \"Should we double down on West Coast?\"\n\nThis isn't a philosophical question. It's a $400K decision with\nmulti-year implications.\n\nThe data needed to answer definitively.\n```\n\n**Best Practices**:\n- **Create suspense**: Don't resolve immediately\n- **Use contrast**: Expected vs actual, before vs after\n- **Quantify stakes**: What's the cost of being wrong?\n- **Make it visual**: Charts highlight conflicts effectively\n- **Raise questions**: Explicitly state what needs resolving\n\n---\n\n## Part 3: Resolution (The Findings)\n\n**Purpose**: Present data that resolves the conflict and answers the question\n\n**Structure**: Build resolution through progressive evidence\n\n### Evidence Pattern: Claim â†’ Data â†’ Interpretation\n\n**Example**:\n```markdown\n## The Answer: Expansion Delivered\n\n### Claim\nWest Coast expansion exceeded projections and justifies continued investment.\n\n### Evidence\nThree data points converge:\n\n**1. Revenue Growth**\n- West Coast: $1.2M Q4 2024 (+$450K vs Q4 2023)\n- Growth rate: 60% YoY\n- Target: $800K (exceeded by 50%)\n\n[Chart: Actual vs Target]\n\n**2. Customer Acquisition**\n- New customers: 342 (Q2-Q4 2024)\n- 28% of all new customers\n- Concentrated in tech sector (62%)\n\n[Chart: Customer acquisition by region]\n\n**3. Transaction Value Premium**\n- West Coast avg: $285 per transaction\n- Company avg: $254 per transaction\n- Premium: +12%\n\n[Chart: Transaction value by region]\n\n### Interpretation\nAll three metrics validate our expansion thesis:\n- Growth exceeded targets (âœ“)\n- New customers demonstrate market fit (âœ“)\n- Premium pricing accepted (âœ“)\n\nThe data strongly supports continued investment.\n```\n\n**Resolution Patterns**:\n\n**Pattern A: The Reveal**\n```markdown\nHere's what the data showed:\n[Primary finding with visualization]\n\nThis finding resolved our question because:\n[Interpretation connecting back to conflict]\n```\n\n**Pattern B: The Build**\n```markdown\nFirst, we found [finding 1]\nThen, we discovered [finding 2]\nFinally, [finding 3] confirmed the pattern\n\nTogether, these three findings demonstrate [conclusion]\n```\n\n**Pattern C: The Comparison**\n```markdown\nWe tested [hypothesis A] against [hypothesis B]\n\nThe data clearly favored [winner]:\n- Metric 1: [evidence]\n- Metric 2: [evidence]\n- Metric 3: [evidence]\n\n[Comparison visualization]\n```\n\n**Pattern D: The Surprise Twist**\n```markdown\nInitially, the data suggested [preliminary finding]\n\nBut deeper analysis revealed [surprising truth]\n\n[Visualization showing layers of analysis]\n\nThis unexpected finding changed our recommendation completely.\n```\n\n**Best Practices**:\n- **Build progressively**: Layer evidence for cumulative impact\n- **Tie back to conflict**: Explicitly resolve the tension you created\n- **Show, don't just tell**: Use visualizations from `creating-visualizations` skill\n- **Maintain rigor**: Include methodology, not just conclusions\n- **Address counterevidence**: Acknowledge alternative explanations\n\n---\n\n## Part 4: Call to Action (Next Steps)\n\n**Purpose**: Translate findings into specific actions and decisions\n\n**Components**:\n1. **Restate the answer** - What did we learn?\n2. **Business implications** - What does it mean?\n3. **Specific recommendations** - What should we do?\n4. **Next steps** - How to move forward?\n5. **Open questions** - What remains to investigate?\n\n**Example (Presentation)**:\n```markdown\n## Call to Action: Three Recommendations\n\n### 1. Continue West Coast Investment (IMMEDIATE)\n- Maintain 3 FTE sales team\n- Sustain $50K/quarter marketing budget\n- ROI: Positive within 18 months\n\n### 2. Pilot East Coast Expansion (Q2 2025)\n- Apply West Coast playbook to Boston/NYC\n- Budget: $75K for 6-month pilot\n- Success metrics: Match West Coast customer acquisition rate\n\n### 3. Monitor Retention Closely (ONGOING)\n- Track cohort retention at 6, 12, 18 months\n- Early warning for churn risk\n- Validate sustainability of growth\n\n**Next Meeting**: Review retention data (March 2025)\n```\n\n**Example (Whitepaper)**:\n```markdown\n# Call to Action: Translating Findings into Strategy\n\n## What We Learned\n\nSix months of West Coast expansion data provide clear answers:\n1. Market reception exceeded projections (23% vs 10% growth target)\n2. Customer acquisition is working (342 new accounts)\n3. Premium pricing is sustainable (+12% transaction value)\n4. Expansion delivers positive ROI (payback: 18 months)\n\n## Business Implications\n\n**Short-term (2025)**:\nWest Coast operations now contribute 35% of growth. Premature\nwithdrawal would sacrifice momentum and waste sunk costs.\n\n**Medium-term (2026-2027)**:\nProven playbook enables replication in similar markets. East Coast\ncities (Boston, NYC) show similar demographic and business profiles.\n\n**Long-term (2028+)**:\nGeographic diversification reduces concentration risk while\naccessing higher-value customer segments.\n\n## Specific Recommendations\n\n### Priority 1: Sustain West Coast Operations\n**Action**: Approve 2025 budget of $200K for West Coast\n**Timeline**: Immediate (Q1 2025 budget allocation)\n**Owner**: VP Sales\n**Success Metric**: Maintain or exceed Q4 2024 growth rate\n\n**Rationale**: Data shows clear ROI. Disrupting operations now\nwould sacrifice established relationships and market position.\n\n### Priority 2: Pilot East Coast Expansion\n**Action**: Allocate $75K for 6-month Boston/NYC pilot\n**Timeline**: Launch Q2 2025\n**Owner**: VP Sales + Regional Manager\n**Success Metric**: 150 new customers by Q3 2025 (50% of West Coast rate)\n\n**Rationale**: West Coast success demonstrates model works. East Coast\noffers similar customer profile with lower competitive intensity.\n\n### Priority 3: Implement Retention Tracking\n**Action**: Deploy cohort retention dashboard\n**Timeline**: Deploy by Feb 2025\n**Owner**: Data Analytics Team\n**Success Metric**: Monthly retention reporting, alerts for <85%\n\n**Rationale**: Current data validates acquisition, but retention remains\nunproven. Systematic tracking prevents future surprises.\n\n## Next Steps\n\n**Immediate (Next 30 days)**:\n- [ ] Present findings to executive team\n- [ ] Approve 2025 West Coast budget\n- [ ] Scope East Coast pilot plan\n- [ ] Deploy retention tracking dashboard\n\n**Near-term (Q1 2025)**:\n- [ ] Monitor January-February West Coast performance\n- [ ] Finalize East Coast pilot plan\n- [ ] Hire regional manager for East Coast\n- [ ] Establish baseline retention metrics\n\n**Medium-term (Q2-Q3 2025)**:\n- [ ] Launch East Coast pilot\n- [ ] Compare retention across cohorts\n- [ ] Evaluate Q1-Q2 West Coast sustainability\n- [ ] Mid-year decision: Scale or adjust East Coast\n\n## Open Questions for Future Investigation\n\n1. **Retention**: Do West Coast customers retain at company average\n   rates, or do regional factors affect lifetime value?\n\n2. **Profitability**: After customer acquisition costs, are West Coast\n   customers more or less profitable than other segments?\n\n3. **Competitive Response**: How will competitors react to our market\n   entry? Will pricing pressure emerge?\n\n4. **Seasonality**: Does Q4 data reflect year-end spending cycles,\n   or is growth sustainable year-round?\n\n**Recommendation**: Revisit analysis in Q2 2025 with 12 months of\nfull-year data to validate findings and adjust strategy as needed.\n\n## Final Remarks\n\nThe West Coast expansion story demonstrates the value of data-driven\ndecision making. Six months ago, we made a strategic bet. Today, data\nvalidates that bet conclusively.\n\nThe question now isn't whether to continue, but how to scale the model\nto other high-value markets while maintaining operational excellence.\n\nThis analysis provides the evidence foundation. The next chapterâ€”\nexecutionâ€”begins now.\n```\n\n**Call to Action Patterns**:\n\n**Pattern A: The Three-Step Plan**\n```markdown\nBased on findings, we recommend:\n1. [Immediate action]\n2. [Near-term initiative]\n3. [Long-term strategy]\n\nNext milestone: [Date and deliverable]\n```\n\n**Pattern B: The Decision Matrix**\n```markdown\nThe data supports [recommended option].\n\n| Option | Evidence | Risk | Recommendation |\n|--------|----------|------|----------------|\n| A      | Strong   | Low  | **Proceed** âœ“  |\n| B      | Mixed    | High | Monitor        |\n| C      | Weak     | High | Defer          |\n```\n\n**Pattern C: The Phased Approach**\n```markdown\nPhase 1 (Immediate): [Quick wins based on findings]\nPhase 2 (3-6 months): [Medium-term initiatives]\nPhase 3 (12 months): [Strategic changes]\n\nDecision points: [When to evaluate and adjust]\n```\n\n**Best Practices**:\n- **Be specific**: \"Hire 2 FTE\" not \"invest in sales\"\n- **Assign ownership**: Who is responsible for what?\n- **Set timelines**: When should actions occur?\n- **Define success**: How will we know it worked?\n- **Acknowledge uncertainty**: What still needs validation?\n- **Schedule follow-up**: When to reassess?\n\n---\n\n## Narrative Frameworks for Different Contexts\n\n### Framework 1: The Hero's Journey (for transformation stories)\n\n**Use when**: Describing how data insights led to major changes\n\n**Structure**:\n1. **Ordinary World**: How things were before\n2. **Call to Adventure**: The challenge or opportunity\n3. **The Journey**: Analysis process and discoveries\n4. **The Return**: Insights and transformation\n5. **New World**: How things will be different\n\n**Example**:\n```markdown\nBefore expansion, our growth was plateauing (ordinary world).\nLeadership challenged us to find new markets (call to adventure).\nData analysis revealed West Coast potential (the journey).\nFindings validated the expansion bet (the return).\nNow we have a proven growth playbook (new world).\n```\n\n### Framework 2: The Before-After-Bridge\n\n**Use when**: Comparing conditions before and after a change\n\n**Structure**:\n1. **Before**: State prior to intervention\n2. **After**: State following intervention\n3. **Bridge**: What changed and why\n\n**Example**:\n```markdown\n**Before**: 8% annual growth, mature markets saturated\n**After**: 23% quarterly growth, new customer segments\n**Bridge**: West Coast expansion accessed high-value tech segment\n```\n\n### Framework 3: The Situation-Complication-Resolution (SCR)\n\n**Use when**: Presenting problem-solving analysis\n\n**Structure**:\n1. **Situation**: Stable state or baseline\n2. **Complication**: Problem or question that arose\n3. **Resolution**: How analysis addressed it\n\n**Example**:\n```markdown\n**Situation**: Q3 showed flat growth\n**Complication**: Would West Coast expansion help?\n**Resolution**: Data shows 23% growth from expansion\n```\n\n### Framework 4: The STAR Method (Situation-Task-Action-Result)\n\n**Use when**: Documenting analytical work for stakeholders\n\n**Structure**:\n1. **Situation**: Business context\n2. **Task**: Analysis objective\n3. **Action**: Methods and approach\n4. **Result**: Findings and outcomes\n\n**Example**:\n```markdown\n**Situation**: Leadership needed expansion ROI assessment\n**Task**: Evaluate Q4 2024 West Coast performance\n**Action**: Analyzed sales data, customer acquisition, transaction values\n**Result**: 23% growth, 342 new customers, positive ROI within 18 months\n```\n\n### Framework 5: The Problem-Agitate-Solve (PAS)\n\n**Use when**: Persuading stakeholders to take action\n\n**Structure**:\n1. **Problem**: State the issue\n2. **Agitate**: Show consequences of inaction\n3. **Solve**: Present data-driven solution\n\n**Example**:\n```markdown\n**Problem**: Traditional markets showing 8% growth (insufficient for targets)\n**Agitate**: At 8%, we miss 2025 targets by $1.2M, risking investor confidence\n**Solve**: West Coast expansion delivers 23% growth, closing the target gap\n```\n\n---\n\n## Integrating Visualizations into the Narrative\n\n**Visualizations should advance the story**\n\nUse `creating-visualizations` skill to create charts that support narrative beats:\n\n**Setup Phase Visualizations**:\n- Trend lines showing historical performance\n- Comparison tables establishing baseline\n- Timeline charts showing key events\n\n**Conflict Phase Visualizations**:\n- Divergence charts (expected vs actual)\n- Highlighted outliers or anomalies\n- Question marks or gap visualizations\n\n**Resolution Phase Visualizations**:\n- Bar charts comparing key metrics\n- Line plots showing progression\n- Scatter plots revealing correlations\n- Annotated charts with findings highlighted\n\n**Call to Action Visualizations**:\n- Roadmap timelines\n- Decision trees\n- ROI projections\n- Comparison matrices\n\n**Example Integration**:\n```markdown\n## The Unexpected Pattern (Conflict)\n\nIn September, West Coast sales began accelerating:\n\n![width:700px](monthly-sales-trend.png)\n\nNotice the hockey-stick curve starting in month 4.\nThis wasn't in our projections.\n\n## What Drove the Acceleration? (Resolution)\n\nThree factors converged:\n\n![width:700px](growth-factors-breakdown.png)\n\nCustomer acquisition (45%), transaction value growth (30%),\nand increased purchase frequency (25%) all contributed.\n```\n\n---\n\n## Checklist: Narrative Structure Quality\n\nBefore finalizing your data story:\n\n**Setup:**\n- [ ] Business context clearly established\n- [ ] Stakeholder goals articulated\n- [ ] Stakes defined (why this matters)\n- [ ] Time period and scope specified\n\n**Conflict:**\n- [ ] Central question or tension stated\n- [ ] Problem is specific and relatable\n- [ ] Audience understands what needs resolving\n- [ ] Visualizations highlight the tension\n\n**Resolution:**\n- [ ] Evidence directly addresses the conflict\n- [ ] Findings build progressively (not all at once)\n- [ ] Alternative explanations considered\n- [ ] Connection to conflict made explicit\n- [ ] Visualizations support key findings\n\n**Call to Action:**\n- [ ] Specific recommendations provided\n- [ ] Ownership and timelines assigned\n- [ ] Success metrics defined\n- [ ] Open questions acknowledged\n- [ ] Follow-up plan established\n\n**Overall Narrative:**\n- [ ] Story has clear beginning, middle, and end\n- [ ] Logical flow from question to answer\n- [ ] Human elements present (not just numbers)\n- [ ] Memorable takeaway articulated\n- [ ] Audience knows what to do next\n\n---\n\n## Common Narrative Pitfalls\n\n### Pitfall 1: Data Dump (No Story)\nâŒ \"Here are 50 charts from our analysis\"\nâœ“ \"We asked: Does expansion work? The data reveals three surprising patterns...\"\n\n### Pitfall 2: Burying the Lede\nâŒ \"Let me walk through our methodology... [20 minutes later]... so we found X\"\nâœ“ \"We found X. Here's how we know it's true...\"\n\n### Pitfall 3: Missing the Stakes\nâŒ \"Sales increased 23%\"\nâœ“ \"23% growth means we hit targets, secure funding, and unlock next phase\"\n\n### Pitfall 4: Resolution Without Conflict\nâŒ \"Our analysis shows positive results\"\nâœ“ \"We faced a critical question: Invest or cut? The data answered definitively.\"\n\n### Pitfall 5: No Clear Call to Action\nâŒ \"So that's what we found. Questions?\"\nâœ“ \"Based on these findings, I recommend we approve $200K budget and launch East Coast pilot by Q2.\"\n\n---\n\n## Examples in Practice\n\nFor complete examples of narrative-driven presentations and whitepapers, see:\n- Main `SKILL.md`: Full workflow from analysis to storytelling\n- `3-paragraph-essay.md`: Essay structure that supports narrative\n- `tools/marp.md`: Creating narrative presentations with marp\n- `tools/pandoc.md`: Creating narrative whitepapers with pandoc\n\n---\n\n## References\n\nThis framework draws on:\n- Storytelling principles from narrative non-fiction\n- Data storytelling literature (Cole Nussbaumer Knaflic, Ben Jones)\n- Business communication best practices\n- Rhetorical structure conventions (SCR, STAR, PAS)\n\nFor implementation, see:\n- `creating-visualizations` skill - Supporting narrative with visuals\n- `interpreting-results` skill - Finding stories in data\n- `frameworks/3-paragraph-essay.md` - Compatible structural framework\n",
        "plugins/datapeeker/skills/presenting-data/tools/marp.md": "# Marp: Markdown to Presentation Tool\n\nMarp transforms Markdown documents into professional presentation slides. This guide covers installation, CLI usage, syntax, and best practices for data-driven presentations.\n\n---\n\n## Overview\n\n**What is Marp?**\n- Markdown Presentation Ecosystem for creating slides from Markdown\n- Built on CommonMark specifications with presentation-specific extensions\n- Supports PDF, HTML, PowerPoint (PPTX), and image exports\n- Uses browser rendering engine (Chrome, Edge, or Firefox) for output\n\n**When to Use Marp:**\n- Executive summaries (5-10 slides)\n- Meeting presentations and stakeholder updates\n- Quick visual communication of findings\n- When you need slides + ability to export to multiple formats\n\n**When NOT to Use Marp:**\n- Comprehensive technical documentation (use pandoc for whitepapers)\n- Documents requiring academic citations (use pandoc with BibTeX)\n- Complex cross-referencing (use pandoc-crossref)\n\n---\n\n## Installation\n\n### Option 1: npx (Recommended for occasional use)\n\nNo installation required. Run directly:\n```bash\nnpx @marp-team/marp-cli@latest slides.md -o slides.pdf\n```\n\n### Option 2: npm (Project-based installation)\n\nInstall as development dependency:\n```bash\nnpm install --save-dev @marp-team/marp-cli\n```\n\nAdd to `package.json` scripts:\n```json\n{\n  \"scripts\": {\n    \"build:slides\": \"marp presentation.md -o presentation.pdf\",\n    \"watch:slides\": \"marp -w -p presentation.md\"\n  }\n}\n```\n\nRun via npm:\n```bash\nnpm run build:slides\nnpm run watch:slides\n```\n\n### Option 3: Global Installation\n\n```bash\nnpm install -g @marp-team/marp-cli\n\n# Use directly\nmarp slides.md -o slides.pdf\n```\n\n### Verify Installation\n\n```bash\n# Check version\nmarp --version\n\n# View help\nmarp --help\n```\n\n---\n\n## CLI Usage\n\n### Basic Conversions\n\n**Convert to HTML** (default):\n```bash\nmarp presentation.md\n# Creates: presentation.html\n```\n\n**Convert to PDF**:\n```bash\nmarp presentation.md -o presentation.pdf\n```\n\n**Convert to PowerPoint**:\n```bash\nmarp presentation.md -o presentation.pptx\n\n# Editable PowerPoint (experimental)\nmarp presentation.md --pptx-editable -o presentation.pptx\n```\n\n**Convert to Images**:\n```bash\n# PNG images (one per slide)\nmarp presentation.md --images png\n\n# JPEG images\nmarp presentation.md --images jpeg\n\n# Control image resolution\nmarp presentation.md --images png --image-scale 2\n```\n\n### Development Workflow\n\n**Watch Mode** (auto-rebuild on file changes):\n```bash\nmarp -w presentation.md\n```\n\n**Preview Mode** (opens browser preview):\n```bash\nmarp -p presentation.md\n```\n\n**Watch + Preview Combined**:\n```bash\nmarp -w -p presentation.md\n```\n\n**HTTP Server** (conversion server):\n```bash\nmarp -s\n# Server runs on http://localhost:8080\n# Access: http://localhost:8080/presentation.md\n```\n\n### Advanced Options\n\n**Custom Theme**:\n```bash\nmarp presentation.md --theme-set custom-theme.css -o presentation.pdf\n```\n\n**Allow Local Files** (for security):\n```bash\nmarp presentation.md --allow-local-files -o presentation.pdf\n```\n\n**Select Browser Engine**:\n```bash\n# Use Chrome (default)\nmarp presentation.md --browser chrome -o presentation.pdf\n\n# Use Edge\nmarp presentation.md --browser edge -o presentation.pdf\n\n# Use Firefox\nmarp presentation.md --browser firefox -o presentation.pdf\n```\n\n**Parallel Processing**:\n```bash\n# Set concurrency level (default: 5)\nmarp presentation.md --parallel 10 -o presentation.pdf\n```\n\n**Extract Presenter Notes**:\n```bash\nmarp presentation.md --notes -o notes.txt\n```\n\n**Configuration File**:\n```bash\n# Use .marprc.yml or marp.config.js\nmarp --config-file custom-config.yml presentation.md\n```\n\n### Common Command Patterns\n\n**Generate multiple formats at once**:\n```bash\n#!/bin/bash\n# generate-all.sh\n\nmarp presentation.md -o presentation.pdf\nmarp presentation.md -o presentation.html\nmarp presentation.md -o presentation.pptx\nmarp presentation.md --images png\n\necho \"Generated: PDF, HTML, PPTX, and PNG images\"\n```\n\n**Watch mode with live reload**:\n```bash\nmarp -w -p --browser firefox presentation.md\n```\n\n---\n\n## Markdown Syntax for Slides\n\n### Slide Separators\n\n**Basic Separator** (three dashes):\n```markdown\n# Slide 1 Title\nContent for first slide\n\n---\n\n# Slide 2 Title\nContent for second slide\n```\n\n### Front Matter (Global Directives)\n\n**YAML front matter** at document start:\n```markdown\n---\ntheme: gaia\npaginate: true\nbackgroundColor: #ffffff\nbackgroundImage: url('background.jpg')\ncolor: #333333\nheader: \"DataPeeker Analysis\"\nfooter: \"Q4 2024 Report\"\n---\n\n# My Presentation\nFirst slide content\n```\n\n**Common Global Directives**:\n- `theme`: Choose theme (`default`, `gaia`, `uncover`, or custom)\n- `paginate`: Add page numbers (`true`/`false`)\n- `backgroundColor`: Set background color (CSS color)\n- `backgroundImage`: Set background image (CSS url)\n- `color`: Set text color (CSS color)\n- `header`: Add header to all slides\n- `footer`: Add footer to all slides\n- `size`: Slide dimensions (`16:9`, `4:3`)\n- `class`: Apply CSS classes\n\n### Per-Slide Directives\n\n**Local Directives** (apply to following slide):\n```markdown\n---\n\n<!-- theme: uncover -->\n<!-- paginate: false -->\n<!-- backgroundColor: #1a1a1a -->\n<!-- color: #ffffff -->\n\n# Dark Theme Slide\nThis slide has custom styling\n```\n\n**Scoped Directives** (underscore prefix - don't inherit):\n```markdown\n---\n\n<!-- _paginate: false -->\n<!-- _backgroundColor: #f5f5f5 -->\n\n# Title Slide\nNo page number, custom background\n```\n\n**Important**: Directives must be HTML comments (`<!-- directive: value -->`)\n\n### Text Formatting\n\n**Standard Markdown**:\n```markdown\n# Heading 1\n## Heading 2\n### Heading 3\n\n**Bold text**\n*Italic text*\n~~Strikethrough~~\n\n- Bullet list\n  - Nested item\n- Another item\n\n1. Numbered list\n2. Second item\n\n[Link text](https://example.com)\n\n`inline code`\n```\n\n**Code Blocks with Syntax Highlighting**:\n````markdown\n```sql\nSELECT\n  region,\n  SUM(sales) as total_sales\nFROM sales_metrics\nWHERE date >= '2024-10-01'\nGROUP BY region\nORDER BY total_sales DESC\n```\n````\n\n**Math Typesetting** (LaTeX syntax):\n```markdown\nInline math: $E = mc^2$\n\nDisplay math:\n$$\n\\int_0^\\infty e^{-x^2} dx = \\frac{\\sqrt{\\pi}}{2}\n$$\n```\n\n### Images\n\n**Basic Image**:\n```markdown\n![Alt text](path/to/image.png)\n```\n\n**Sized Image**:\n```markdown\n![width:600px height:400px](chart.png)\n\n# or\n![w:600px h:400px](chart.png)\n\n# or proportional\n![width:80%](chart.png)\n```\n\n**Background Image**:\n```markdown\n![bg](background.jpg)\n\n# Sized background\n![bg width:100%](background.jpg)\n\n# Positioned background\n![bg left](left-image.jpg)\n![bg right](right-image.jpg)\n```\n\n**Image Alignment**:\n```markdown\n![bg left:40%](left-image.jpg)\n![bg right:60%](right-image.jpg)\n```\n\n**Supported Formats**: PNG, JPEG, SVG, GIF, WebP\n\n### Slide Layouts\n\n**Two-Column Layout**:\n```markdown\n<div class=\"columns\">\n<div>\n\n## Left Column\n- Point 1\n- Point 2\n\n</div>\n<div>\n\n## Right Column\n- Point A\n- Point B\n\n</div>\n</div>\n```\n\n**Split Content**:\n```markdown\n<!-- _class: split -->\n\n# Split Slide\n\n:::: {.columns}\n::: {.column}\nLeft content\n:::\n::: {.column}\nRight content\n:::\n::::\n```\n\n### Speaker Notes\n\n**Add presenter notes** (visible in HTML presenter view):\n```markdown\n# Slide Title\n\nMain slide content visible to audience\n\n<!--\nSpeaker notes here:\n- Remember to mention X\n- Ask if anyone has questions about Y\n- Transition to next topic: Z\n-->\n```\n\n**View speaker notes**:\n- HTML export includes presenter mode (press 'P' key)\n- Extract to text file: `marp --notes presentation.md -o notes.txt`\n\n---\n\n## Themes\n\n### Built-in Themes\n\n**Default Theme**:\n```markdown\n---\ntheme: default\n---\n```\n\n**Gaia Theme** (light, modern):\n```markdown\n---\ntheme: gaia\n---\n```\n\n**Uncover Theme** (dark, minimalist):\n```markdown\n---\ntheme: uncover\n---\n```\n\n### Custom Themes\n\n**Create Custom Theme CSS**:\n```css\n/* custom-theme.css */\n@import 'default';  /* Extend default theme */\n\n@theme custom-data-theme\n\nsection {\n  background-color: #f5f5f5;\n  color: #333;\n  font-family: 'Segoe UI', 'Arial', sans-serif;\n  font-size: 28px;\n  padding: 60px;\n}\n\nsection h1 {\n  color: #0066cc;\n  font-size: 48px;\n  font-weight: bold;\n  border-bottom: 3px solid #0066cc;\n  padding-bottom: 10px;\n}\n\nsection h2 {\n  color: #0088cc;\n  font-size: 36px;\n  margin-top: 40px;\n}\n\nsection code {\n  background-color: #e8e8e8;\n  padding: 2px 6px;\n  border-radius: 3px;\n}\n\nsection pre {\n  background-color: #1e1e1e;\n  color: #d4d4d4;\n  padding: 20px;\n  border-radius: 5px;\n}\n\nsection footer {\n  font-size: 14px;\n  color: #999;\n}\n\nsection header {\n  font-size: 14px;\n  color: #666;\n  text-align: right;\n}\n\n/* Title slide styling */\nsection.title-slide {\n  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n  color: white;\n}\n\nsection.title-slide h1 {\n  color: white;\n  border-bottom: none;\n  font-size: 64px;\n}\n\n/* Data emphasis styles */\nsection .metric {\n  font-size: 48px;\n  font-weight: bold;\n  color: #0066cc;\n}\n\nsection .callout {\n  background-color: #fff3cd;\n  border-left: 5px solid #ffc107;\n  padding: 15px;\n  margin: 20px 0;\n}\n```\n\n**Use Custom Theme**:\n```bash\nmarp presentation.md --theme-set custom-theme.css -o presentation.pdf\n```\n\n**In Markdown**:\n```markdown\n---\ntheme: custom-data-theme\n---\n\n# Presentation\n```\n\n### Theme CSS Variables\n\n**Customize via CSS variables**:\n```css\n:root {\n  --color-background: #ffffff;\n  --color-foreground: #333333;\n  --color-highlight: #0066cc;\n  --color-dimmed: #999999;\n  --font-size-base: 28px;\n}\n\nsection {\n  background: var(--color-background);\n  color: var(--color-foreground);\n  font-size: var(--font-size-base);\n}\n\nsection h1 {\n  color: var(--color-highlight);\n}\n```\n\n---\n\n## DataPeeker Presentation Patterns\n\n### Pattern 1: Executive Summary\n\n```markdown\n---\ntheme: gaia\npaginate: true\nfooter: \"DataPeeker Q4 Analysis\"\n---\n\n# Q4 Sales Analysis\n## Executive Summary\n\n**Key Finding**: West Coast expansion delivered 23% YoY growth\n\n---\n\n## Data Sources\n\n- **Database**: `analytics_prod.sales_metrics`\n- **Period**: 2024-10-01 to 2024-12-31\n- **Records**: 50,000 transactions\n- **Query Timestamp**: 2025-11-25 14:30 UTC\n\n---\n\n## Finding 1: Revenue Growth\n\n**West Coast Performance**:\n- Revenue: $1.2M (Q4 2024)\n- Growth: +$450K vs Q4 2023\n- Target: $800K (**exceeded by 50%**)\n\n![width:700px](revenue-chart.png)\n\n---\n\n## Finding 2: Customer Acquisition\n\n**New Customer Metrics**:\n- Total new accounts: 342 (Q2-Q4 2024)\n- Market share: 28% of all new customers\n- Tech sector concentration: 62%\n\n![width:700px](customer-acquisition.png)\n\n---\n\n## Finding 3: Transaction Values\n\n**Premium Pricing Validation**:\n- West Coast avg: **$285** per transaction\n- Company avg: $254 per transaction\n- Premium: **+12%**\n\n![width:700px](transaction-value-chart.png)\n\n---\n\n## Methodology\n\n```sql\nSELECT\n  region,\n  DATE_TRUNC('month', transaction_date) as month,\n  SUM(amount) as revenue,\n  COUNT(DISTINCT customer_id) as customers,\n  AVG(amount) as avg_transaction\nFROM analytics_prod.sales_metrics\nWHERE DATE(transaction_date) BETWEEN '2024-10-01' AND '2024-12-31'\nGROUP BY region, month\nORDER BY revenue DESC\n```\n\n<!--\nQuery execution:\n- Time: 2.3 seconds\n- Rows examined: 50,000\n- Rows returned: 18 (3 regions Ã— 6 months)\n-->\n\n---\n\n## Conclusions\n\n### Three Key Takeaways\n1. âœ… Expansion exceeded targets (23% vs 10% projected)\n2. âœ… Customer acquisition validates market fit\n3. âœ… Premium pricing sustainable\n\n### Recommendation\n**Continue West Coast investment** - positive ROI within 18 months\n\n---\n\n## Next Steps\n\n**Immediate Actions**:\n1. Approve 2025 West Coast budget ($200K)\n2. Pilot East Coast expansion (Q2 2025)\n3. Monitor retention metrics monthly\n\n**Follow-up Analysis**: Q2 2025 retention study\n\n---\n\n## Questions?\n\n**Full Technical Report**: [Whitepaper](./whitepaper.pdf)\n\n**Analysis Repository**: [github.com/tilmon/q4-analysis](https://github.com/tilmon/q4-analysis)\n\n**Contact**: data-team@tilmon.com\n\n---\n\n<!-- _paginate: false -->\n<!-- _backgroundColor: #f5f5f5 -->\n\n# Thank You\n\n**DataPeeker Team**\nQ4 2024 Analysis\n```\n\n### Pattern 2: Technical Walkthrough\n\n```markdown\n---\ntheme: default\npaginate: true\nheader: \"Technical Analysis Walkthrough\"\n---\n\n# Data Quality Assessment\n## Q4 Sales Data Analysis\n\n---\n\n## Agenda\n\n1. Data Source Validation\n2. Quality Metrics Review\n3. Transformation Pipeline\n4. Results & Findings\n5. Reproducibility Documentation\n\n---\n\n## Data Source: `analytics_prod.sales_metrics`\n\n**Schema Validation**:\n```sql\nDESCRIBE analytics_prod.sales_metrics;\n```\n\n| Column            | Type      | Nullable |\n|-------------------|-----------|----------|\n| transaction_id    | INTEGER   | NOT NULL |\n| customer_id       | INTEGER   | NOT NULL |\n| region            | VARCHAR   | NOT NULL |\n| amount            | DECIMAL   | NOT NULL |\n| transaction_date  | DATE      | NOT NULL |\n\n---\n\n## Quality Metrics\n\n**Completeness**:\n- NULL values: 0.02% (within tolerance)\n- Missing dates: 0 records\n- Invalid amounts: 0 records\n\n**Consistency**:\n- Duplicate transactions: 0 detected\n- Date range validation: âœ“ All within expected period\n- Amount range: $5 - $5,000 (normal distribution)\n\n---\n\n## Outlier Detection\n\n**Method**: 3 MAD (Median Absolute Deviation)\n\n```sql\nSELECT\n  amount,\n  ABS(amount - MEDIAN(amount) OVER ()) /\n    (MAD(amount) OVER () * 1.4826) as outlier_score\nFROM sales_metrics\nHAVING outlier_score > 3\n```\n\n**Results**: 12 outliers identified (0.024%)\n\nAll outliers reviewed and validated as legitimate high-value transactions.\n\n---\n\n## Transformation Pipeline\n\n![width:900px](data-pipeline-diagram.svg)\n\n1. **Raw Data** â†’ Quality validation\n2. **Validated Data** â†’ Aggregation by region/month\n3. **Aggregated Data** â†’ Visualization preparation\n4. **Visualizations** â†’ Presentation delivery\n\n---\n\n## Results\n\n[Insert findings slides similar to Pattern 1]\n\n---\n\n## Reproducibility\n\n**Environment**:\n- DataPeeker: v2.1.0\n- Python: 3.11.5\n- pandas: 2.1.0\n- plotext: 5.2.8\n\n**Repository**: All queries and code version controlled\n- Commit: `abc123def456`\n- Branch: `main`\n- Tag: `q4-2024-analysis-v1.0`\n\n---\n\n## Run the Analysis Yourself\n\n```bash\n# Clone repository\ngit clone github.com/tilmon/q4-analysis\n\n# Install dependencies\npip install -r requirements.txt\n\n# Set database connection\nexport DB_CONN=\"postgresql://user@host/db\"\n\n# Run analysis\npython scripts/q4_regional_analysis.py\n\n# View results\nopen analysis/q4-2024/findings.md\n```\n\n---\n\n## Questions & Discussion\n```\n\n---\n\n## Best Practices for Data Presentations\n\n### DO:\n\nâœ… **Use clear, descriptive slide titles**\n```markdown\n# Finding 1: West Coast Revenue Dominance\nNOT: # Results\n```\n\nâœ… **Include data provenance on every slide**\n```markdown\n**Source**: `analytics_prod.sales_metrics` (2024-10-01 to 2024-12-31)\n```\n\nâœ… **Show SQL queries for key findings**\n```markdown\n```sql\nSELECT region, SUM(sales) FROM metrics WHERE...\n```\n```\n\nâœ… **Use visualizations from `creating-visualizations` skill**\n```markdown\n![width:700px](revenue-chart.png)\n```\n\nâœ… **Add speaker notes for context**\n```markdown\n<!--\nRemember to mention: This growth exceeded our projections by 50%\n-->\n```\n\nâœ… **Link to detailed whitepaper**\n```markdown\nFull methodology: [Technical Whitepaper](./whitepaper.pdf)\n```\n\nâœ… **Version control both source and output**\n```bash\ngit add presentation.md presentation.pdf\ngit commit -m \"Add Q4 analysis presentation\"\n```\n\n### DON'T:\n\nâŒ **Overload slides with text**\n- Keep bullet points concise (max 5-7 per slide)\n- Use multiple slides instead of cramming\n\nâŒ **Skip data source documentation**\n- Always include database, table, timestamp\n- Link to query files or repository\n\nâŒ **Use manual visualizations**\n- Use `creating-visualizations` skill to generate charts\n- Export as PNG/SVG and embed\n\nâŒ **Forget accessibility**\n- Ensure sufficient color contrast\n- Don't rely solely on color to convey meaning\n- Use alt text for images\n\nâŒ **Ignore file size**\n- Optimize images before embedding\n- Large presentations are slow to load\n\n---\n\n## Troubleshooting\n\n### Issue: \"Command not found: marp\"\n\n**Solution**: Install marp-cli\n```bash\nnpm install -g @marp-team/marp-cli\n```\n\n### Issue: PDF export fails\n\n**Solution**: Requires Chrome/Edge/Firefox. Install if missing:\n```bash\n# macOS\nbrew install --cask google-chrome\n\n# Or specify different browser\nmarp presentation.md --browser firefox -o presentation.pdf\n```\n\n### Issue: Images not loading\n\n**Solution**: Use `--allow-local-files` flag\n```bash\nmarp presentation.md --allow-local-files -o presentation.pdf\n```\n\n### Issue: Theme not applying\n\n**Solution**: Verify theme CSS syntax\n```bash\n# Check for CSS errors\nmarp presentation.md --theme-set custom-theme.css --dry-run\n```\n\n### Issue: Slow rendering\n\n**Solution**: Increase parallel processing\n```bash\nmarp presentation.md --parallel 10 -o presentation.pdf\n```\n\n---\n\n## Configuration File\n\n**Create `.marprc.yml`** in project root:\n```yaml\nallowLocalFiles: true\nengine: ./custom-engine.js\nhtml: true\ninputDir: ./slides\noutput: ./dist\npdf: true\npdfNotes: false\npdfOutlines: false\ntheme: custom-theme.css\nthemeSet: ./themes\nwatch: false\nserver: false\n```\n\n**Or `marp.config.js`**:\n```javascript\nmodule.exports = {\n  allowLocalFiles: true,\n  inputDir: './slides',\n  output: './dist',\n  themeSet: './themes',\n  pdf: true\n}\n```\n\n**Use configuration**:\n```bash\nmarp presentation.md  # Automatically loads .marprc.yml\n```\n\n---\n\n## Integration with DataPeeker Workflow\n\n### Workflow: Analysis â†’ Presentation\n\n```bash\n#!/bin/bash\n# generate-presentation.sh\n\n# Step 1: Run analysis (generates data and charts)\npython scripts/q4_analysis.py\n\n# Step 2: Generate charts using creating-visualizations skill\npython scripts/create_charts.py\n\n# Step 3: Build presentation from markdown\nmarp analysis/presentation.md \\\n  --allow-local-files \\\n  --theme-set themes/data-theme.css \\\n  -o analysis/presentation.pdf\n\necho \"Presentation created: analysis/presentation.pdf\"\n```\n\n### Workflow: Watch Mode for Iterative Development\n\n```bash\n# Terminal 1: Run analysis and regenerate charts on data changes\nwatch -n 60 python scripts/q4_analysis.py\n\n# Terminal 2: Watch presentation markdown and rebuild on changes\nmarp -w -p analysis/presentation.md\n```\n\n### Workflow: Multiple Format Generation\n\n```bash\n#!/bin/bash\n# generate-all-formats.sh\n\nPRESENTATION=\"analysis/presentation.md\"\nOUTPUT_DIR=\"analysis/deliverables\"\n\nmkdir -p \"$OUTPUT_DIR\"\n\n# PDF for printing/distribution\nmarp \"$PRESENTATION\" -o \"$OUTPUT_DIR/presentation.pdf\"\n\n# HTML for web viewing\nmarp \"$PRESENTATION\" -o \"$OUTPUT_DIR/presentation.html\"\n\n# PPTX for editing\nmarp \"$PRESENTATION\" -o \"$OUTPUT_DIR/presentation.pptx\"\n\n# PNG images for social media/slides\nmarp \"$PRESENTATION\" --images png --image-scale 2\n\necho \"Generated all formats in $OUTPUT_DIR\"\n```\n\n---\n\n## References\n\n**Official Documentation**:\n- [Marp Official Website](https://marp.app/)\n- [Marp CLI GitHub](https://github.com/marp-team/marp-cli)\n- [Marpit Framework (Directives)](https://marpit.marp.app/directives)\n- [Marp Theme CSS](https://marpit.marp.app/theme-css)\n- [Awesome Marp Collection](https://github.com/marp-team/awesome-marp)\n\n**Related DataPeeker Skills**:\n- `creating-visualizations` - Generate charts for presentations\n- `interpreting-results` - Develop findings to present\n- `frameworks/3-paragraph-essay.md` - Structure your presentation\n- `frameworks/narrative-structure.md` - Tell compelling data stories\n- `tools/pandoc.md` - Create detailed whitepapers\n",
        "plugins/datapeeker/skills/presenting-data/tools/pandoc.md": "# Pandoc: Universal Document Converter\n\nPandoc is a universal markup converter for creating publication-quality documents from Markdown. This guide covers installation, CLI usage, citations, cross-references, and best practices for data analysis whitepapers.\n\n---\n\n## Overview\n\n**What is Pandoc?**\n- Universal document converter supporting 50+ input/output formats\n- Haskell-based with modular reader/writer architecture\n- Industry-standard tool for academic and technical publishing\n- Supports citations, cross-references, and professional formatting\n\n**When to Use Pandoc:**\n- Technical reports and whitepapers (20-50+ pages)\n- Academic papers with citations and bibliography\n- Comprehensive documentation requiring cross-references\n- Professional documents needing LaTeX-quality typography\n\n**When NOT to Use Pandoc:**\n- Quick presentations (use marp for slides)\n- Simple one-page documents (plain Markdown sufficient)\n- Interactive web applications (use specialized web frameworks)\n\n---\n\n## Installation\n\n### macOS\n\n**Using Homebrew**:\n```bash\nbrew install pandoc\n\n# For PDF output, also install LaTeX\nbrew install --cask mactex\n# Or minimal: brew install basictex\n```\n\n### Linux\n\n**Ubuntu/Debian**:\n```bash\nsudo apt-get update\nsudo apt-get install pandoc\n\n# For PDF output\nsudo apt-get install texlive-full\n# Or minimal: sudo apt-get install texlive-latex-base\n```\n\n**Fedora**:\n```bash\nsudo dnf install pandoc\n\n# For PDF output\nsudo dnf install texlive-scheme-full\n```\n\n### Windows\n\n**Download installer**: https://pandoc.org/installing.html\n\n**For PDF output**, install MiKTeX or TeX Live:\n- MiKTeX: https://miktex.org/download\n- TeX Live: https://www.tug.org/texlive/windows.html\n\n### Verify Installation\n\n```bash\n# Check pandoc version\npandoc --version\n\n# Check LaTeX (for PDF)\npdflatex --version\n\n# List supported formats\npandoc --list-input-formats\npandoc --list-output-formats\n```\n\n---\n\n## CLI Usage: Basic Conversions\n\n### Simple Conversions\n\n**Markdown to HTML**:\n```bash\npandoc input.md -o output.html\n```\n\n**Markdown to PDF**:\n```bash\npandoc input.md -o output.pdf\n```\n\n**Markdown to DOCX**:\n```bash\npandoc input.md -o output.docx\n```\n\n**Explicit Format Specification**:\n```bash\npandoc -f markdown -t pdf input.md -o output.pdf\n```\n\n### Standalone Documents\n\n**Include headers, footers, and formatting**:\n```bash\npandoc -s input.md -o output.html\npandoc -s input.md -o output.pdf\npandoc -s input.md -o output.docx\n```\n\nWithout `-s`, output contains only the converted content (no document structure).\n\n---\n\n## CLI Usage: Advanced Options\n\n### Table of Contents\n\n**Generate TOC**:\n```bash\npandoc -s --toc input.md -o output.pdf\n```\n\n**Customize TOC depth**:\n```bash\npandoc -s --toc --toc-depth=2 input.md -o output.pdf\n```\n\n**Custom TOC title**:\n```markdown\n---\ntoc-title: \"Contents\"\n---\n```\n\n### Section Numbering\n\n**Number sections automatically**:\n```bash\npandoc -s --number-sections input.md -o output.pdf\n```\n\n**Combined with TOC**:\n```bash\npandoc -s --toc --number-sections input.md -o output.pdf\n```\n\n### Variables and Metadata\n\n**Set document variables**:\n```bash\npandoc -V title=\"My Research Paper\" \\\n  -V author=\"Jane Smith\" \\\n  -V date=\"2025-11-25\" \\\n  -s input.md -o output.pdf\n```\n\n**Common variables**:\n- `-V geometry:margin=1in` - Page margins\n- `-V fontsize=12pt` - Font size\n- `-V linestretch=1.5` - Line spacing (1.5 = 1.5x)\n- `-V papersize=letter` - Paper size\n- `-V documentclass=article` - LaTeX document class\n- `-V colorlinks=true` - Colored hyperlinks\n\n**Example with multiple variables**:\n```bash\npandoc -s input.md \\\n  -V geometry:margin=1in \\\n  -V fontsize=12pt \\\n  -V linestretch=1.5 \\\n  -V colorlinks=true \\\n  -V linkcolor=blue \\\n  --toc --number-sections \\\n  -o output.pdf\n```\n\n### Merging Multiple Files\n\n**Concatenate multiple markdown files**:\n```bash\npandoc intro.md methods.md results.md conclusion.md \\\n  -s --toc --number-sections \\\n  -o complete-paper.pdf\n```\n\n**With metadata file**:\n```bash\npandoc metadata.yaml intro.md methods.md results.md \\\n  -s --toc --number-sections \\\n  -o complete-paper.pdf\n```\n\n---\n\n## Metadata in Markdown\n\n### YAML Front Matter\n\n**At the beginning of markdown file**:\n```yaml\n---\ntitle: \"Data Analysis Whitepaper: Q4 2024 Sales Performance\"\nauthor:\n  - Jane Smith\n  - John Doe\ndate: \"2025-11-25\"\ninstitute: \"Tilmon Engineering\"\nabstract: |\n  This whitepaper presents a comprehensive analysis of Q4 2024 sales\n  data, revealing 23% year-over-year growth driven primarily by West\n  Coast expansion. We examine customer acquisition patterns, transaction\n  values, and regional performance using reproducible SQL-based\n  methodology.\nkeywords: \"data analysis, SQL, reproducibility, sales analysis\"\ntoc-title: \"Table of Contents\"\nlof: true  # List of figures\nlot: true  # List of tables\nbibliography: references.bib\ncsl: ieee.csl\n---\n\n# Introduction\n\nDocument content starts here...\n```\n\n**Metadata Fields**:\n- `title` - Document title\n- `author` - Author(s), single string or list\n- `date` - Publication date\n- `institute` - Institution/organization\n- `abstract` - Abstract paragraph(s)\n- `keywords` - Comma-separated keywords\n- `toc-title` - Custom TOC heading\n- `lof` - Generate list of figures (true/false)\n- `lot` - Generate list of tables (true/false)\n- `bibliography` - Path to BibTeX file\n- `csl` - Citation style (CSL file)\n- `link-citations` - Make citations clickable (true/false)\n\n---\n\n## Citations and Bibliography\n\n### Using BibTeX\n\n**Step 1: Create BibTeX file** (`references.bib`):\n```bibtex\n@article{smith2024,\n  author = {Smith, John and Johnson, Mary},\n  title = {Advanced Data Analysis Techniques},\n  journal = {Journal of Data Science},\n  year = {2024},\n  volume = {15},\n  number = {3},\n  pages = {123-145},\n  doi = {10.1234/jds.2024.15.3}\n}\n\n@misc{tilmon2025,\n  author = {Tilmon Engineering},\n  title = {DataPeeker: SQL Analysis Tool},\n  year = {2025},\n  version = {2.1.0},\n  url = {https://github.com/tilmon/datapeeker},\n  note = {Accessed: 2025-11-25}\n}\n\n@misc{production_db2025,\n  author = {Tilmon Engineering},\n  title = {Production Sales Metrics Database},\n  year = {2025},\n  howpublished = {analytics\\_prod.sales\\_metrics},\n  note = {Query timestamp: 2025-11-25 14:30:00 UTC}\n}\n```\n\n**Step 2: Cite in Markdown**:\n```markdown\nAccording to recent research [@smith2024], data quality is crucial.\n\nMultiple citations [@smith2024; @tilmon2025] can be combined.\n\nCite with page numbers [@smith2024, pp. 45-47].\n\nSuppress author name: [-@smith2024] showed that...\n```\n\n**Step 3: Convert with citations**:\n```bash\npandoc input.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  -s -o output.pdf\n```\n\n### Citation Styles (CSL)\n\n**Use specific citation style**:\n```bash\npandoc input.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -s -o output.pdf\n```\n\n**Common CSL styles**:\n- `chicago-author-date.csl` - Chicago (default if no CSL specified)\n- `ieee.csl` - IEEE numbered citations\n- `apa.csl` - APA 7th edition\n- `modern-language-association.csl` - MLA style\n- `harvard-cite-them-right.csl` - Harvard style\n- `nature.csl` - Nature journal style\n- `vancouver.csl` - Vancouver style\n\n**Download CSL styles**:\n- [Zotero Style Repository](https://www.zotero.org/styles) (10,000+ styles)\n- [CSL GitHub](https://github.com/citation-style-language/styles)\n\n**Example with IEEE style**:\n```bash\n# Download IEEE style\ncurl -o ieee.csl \\\n  https://raw.githubusercontent.com/citation-style-language/styles/master/ieee.csl\n\n# Use in conversion\npandoc input.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -s -o output.pdf\n```\n\n### CSL JSON Format (Alternative to BibTeX)\n\n**Create `references.json`**:\n```json\n[\n  {\n    \"id\": \"smith2024\",\n    \"type\": \"article-journal\",\n    \"author\": [\n      {\"family\": \"Smith\", \"given\": \"John\"},\n      {\"family\": \"Johnson\", \"given\": \"Mary\"}\n    ],\n    \"title\": \"Advanced Data Analysis Techniques\",\n    \"container-title\": \"Journal of Data Science\",\n    \"issued\": {\"date-parts\": [[2024]]},\n    \"volume\": 15,\n    \"issue\": 3,\n    \"page\": \"123-145\",\n    \"DOI\": \"10.1234/jds.2024.15.3\"\n  }\n]\n```\n\n**Use JSON bibliography**:\n```bash\npandoc input.md \\\n  --citeproc \\\n  --bibliography references.json \\\n  -s -o output.pdf\n```\n\n---\n\n## Cross-References (Figures, Tables, Equations)\n\n### Using pandoc-crossref Filter\n\n**Install pandoc-crossref**:\n```bash\n# Download from: https://github.com/lierdakil/pandoc-crossref/releases\n# Or on some systems:\nbrew install pandoc-crossref  # macOS\n```\n\n**Syntax for Cross-References**:\n\n**Sections**:\n```markdown\n# Introduction {#sec:intro}\n\nAs discussed in @sec:intro, data quality matters.\n\n# Methodology {#sec:methods}\n\nSee @sec:methods for details.\n```\n\n**Figures**:\n```markdown\n![Data pipeline diagram](diagram.png){#fig:pipeline}\n\nFigure @fig:pipeline shows the data processing workflow.\n\nAs shown in @fig:pipeline, the process has three stages.\n```\n\n**Tables**:\n```markdown\n| Region | Sales |\n|--------|-------|\n| North  | 45000 |\n| South  | 32000 |\n\n: Sales by region {#tbl:sales}\n\nTable @tbl:sales summarizes regional performance.\n\nResults (see @tbl:sales) demonstrate clear trends.\n```\n\n**Equations**:\n```markdown\n$$\nE = mc^2\n$$ {#eq:einstein}\n\nEquation @eq:einstein is foundational to physics.\n\nAs @eq:einstein shows, energy and mass are related.\n```\n\n**Convert with cross-references**:\n```bash\npandoc input.md \\\n  -F pandoc-crossref \\\n  --citeproc \\\n  --bibliography references.bib \\\n  -s --toc --number-sections \\\n  -o output.pdf\n```\n\n**Important**: `-F pandoc-crossref` must come **before** `--citeproc`\n\n### Cross-Reference Customization\n\n**Customize prefix text**:\n```markdown\n---\nfigPrefix: \"Figure\"\ntblPrefix: \"Table\"\neqnPrefix: \"Equation\"\nsecPrefix: \"Section\"\n---\n```\n\n**Custom numbering**:\n```markdown\n---\nnumberSections: true\nfigureTitle: \"Figure\"\ntableTitle: \"Table\"\n---\n```\n\n---\n\n## Templates\n\n### Extract Default Templates\n\n**LaTeX template**:\n```bash\npandoc -D latex > my-template.tex\n```\n\n**HTML template**:\n```bash\npandoc -D html5 > my-template.html\n```\n\n**DOCX reference document**:\n```bash\npandoc -D docx > reference.docx\n```\n\n### Using Custom Templates\n\n**Apply custom LaTeX template**:\n```bash\npandoc -s --template=my-template.tex input.md -o output.pdf\n```\n\n**Apply custom HTML template**:\n```bash\npandoc -s --template=my-template.html input.md -o output.html\n```\n\n### Template Variables\n\n**Available variables** (use in templates as `$variable$`):\n- `$title$` - Document title\n- `$author$` - Author name(s)\n- `$date$` - Document date\n- `$abstract$` - Abstract text\n- `$toc$` - Table of contents\n- `$body$` - Document body\n- `$bibliography$` - Bibliography section\n\n**Example LaTeX template snippet**:\n```latex\n\\documentclass[12pt]{article}\n\n\\title{$title$}\n\\author{$author$}\n\\date{$date$}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n$abstract$\n\\end{abstract}\n\n\\tableofcontents\n\n$body$\n\n\\end{document}\n```\n\n---\n\n## PDF Generation (LaTeX)\n\n### LaTeX Engine Options\n\n**Default (pdflatex)**:\n```bash\npandoc input.md -o output.pdf\n```\n\n**XeLaTeX** (better Unicode support):\n```bash\npandoc --pdf-engine=xelatex input.md -o output.pdf\n```\n\n**LuaLaTeX** (modern, fast):\n```bash\npandoc --pdf-engine=lualatex input.md -o output.pdf\n```\n\n### PDF-Specific Variables\n\n**Page geometry**:\n```bash\npandoc -s input.md \\\n  -V geometry:margin=1in \\\n  -V geometry:paperwidth=8.5in \\\n  -V geometry:paperheight=11in \\\n  -o output.pdf\n```\n\n**Document class**:\n```bash\npandoc -s input.md \\\n  -V documentclass=article \\\n  -V fontsize=12pt \\\n  -V papersize=letter \\\n  -o output.pdf\n```\n\n**Line spacing**:\n```bash\npandoc -s input.md \\\n  -V linestretch=1.5 \\\n  -o output.pdf\n```\n\n**Colors and links**:\n```bash\npandoc -s input.md \\\n  -V colorlinks=true \\\n  -V linkcolor=blue \\\n  -V citecolor=blue \\\n  -V urlcolor=blue \\\n  -o output.pdf\n```\n\n### Include LaTeX in Header\n\n**Create `preamble.tex`**:\n```latex\n\\usepackage{fancyhdr}\n\\pagestyle{fancy}\n\\fancyhead[L]{My Document}\n\\fancyhead[R]{\\thepage}\n\\fancyfoot[C]{Confidential}\n```\n\n**Include in conversion**:\n```bash\npandoc -s input.md \\\n  -H preamble.tex \\\n  -o output.pdf\n```\n\n---\n\n## Word Documents (DOCX)\n\n### Basic DOCX Conversion\n\n```bash\npandoc input.md -o output.docx\n```\n\n### Using Reference Document (Template)\n\n**Extract reference document**:\n```bash\npandoc -D docx > reference.docx\n```\n\n**Customize `reference.docx`**:\n1. Open in Microsoft Word\n2. Modify styles (Heading 1, Heading 2, Body Text, etc.)\n3. Save\n\n**Use customized template**:\n```bash\npandoc input.md --reference-doc=reference.docx -o output.docx\n```\n\n### DOCX with Citations\n\n```bash\npandoc input.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --reference-doc=reference.docx \\\n  -o output.docx\n```\n\n---\n\n## HTML Export\n\n### Basic HTML\n\n```bash\npandoc -s input.md -o output.html\n```\n\n### HTML with CSS\n\n**External CSS**:\n```bash\npandoc -s -c style.css input.md -o output.html\n```\n\n**Inline CSS** (self-contained):\n```bash\npandoc -s --self-contained -c style.css input.md -o output.html\n```\n\n### HTML with MathJax (for equations)\n\n```bash\npandoc -s --mathjax input.md -o output.html\n```\n\n---\n\n## DataPeeker Whitepaper Patterns\n\n### Pattern 1: Complete Technical Report\n\n**File Structure**:\n```\nanalysis/\nâ”œâ”€â”€ q4-2024/\nâ”‚   â”œâ”€â”€ whitepaper.md (main document)\nâ”‚   â”œâ”€â”€ references.bib (bibliography)\nâ”‚   â”œâ”€â”€ ieee.csl (citation style)\nâ”‚   â”œâ”€â”€ metadata.yaml (optional separate metadata)\nâ”‚   â”œâ”€â”€ queries/\nâ”‚   â”‚   â””â”€â”€ q4_analysis.sql\nâ”‚   â””â”€â”€ visualizations/\nâ”‚       â”œâ”€â”€ revenue-chart.png\nâ”‚       â””â”€â”€ customer-acquisition.png\n```\n\n**`whitepaper.md`**:\n```yaml\n---\ntitle: \"West Coast Expansion Analysis: Q4 2024 Performance Evaluation\"\nauthor:\n  - name: \"Data Analytics Team\"\n    affiliation: \"Tilmon Engineering\"\ndate: \"2025-11-25\"\ninstitute: \"Tilmon Engineering\"\nabstract: |\n  This whitepaper presents a comprehensive analysis of Q4 2024 sales\n  performance following the April 2024 West Coast expansion initiative.\n  Using reproducible SQL-based methodology, we examine 50,000\n  transactions to determine whether the expansion delivered measurable\n  results. Key findings include 23% year-over-year revenue growth,\n  342 new customer acquisitions, and sustainable +12% transaction value\n  premium. Analysis validates continuation of West Coast operations and\n  supports piloting similar expansion in East Coast markets.\nkeywords: \"data analysis, SQL, sales analysis, geographic expansion, reproducibility\"\ntoc-title: \"Table of Contents\"\nlof: true\nlot: true\nbibliography: references.bib\ncsl: ieee.csl\nlink-citations: true\ncolorlinks: true\nlinkcolor: blue\ncitecolor: blue\nurlcolor: blue\n---\n\n# Introduction\n\n## Business Context\n\nIn April 2024, Tilmon Engineering initiated a strategic expansion into\nWest Coast markets, investing $200K and committing 3 sales FTE. By\nOctober 2024, stakeholders required data-driven evaluation to determine\nwhether to continue, scale, or redirect these resources.\n\n## Research Question\n\nThis analysis addresses: **Did the West Coast expansion deliver\nmeasurable results that justify continued investment?**\n\n## Thesis Statement\n\nSix months of sales data (Q2-Q4 2024) demonstrate that West Coast\nexpansion exceeded projections across three key metrics: revenue growth\n(23% YoY vs 10% target), customer acquisition (342 new accounts), and\ntransaction value premium (+12%). These findings support continued\ninvestment and potential replication in similar markets.\n\n## Document Overview\n\nSection @sec:methodology details data sources and analytical approach.\nSection @sec:results presents findings with supporting visualizations.\nSection @sec:discussion interprets implications and addresses\nlimitations. Section @sec:conclusions provides recommendations and next\nsteps. Appendix A documents reproducibility information.\n\n# Methodology {#sec:methodology}\n\n## Data Sources\n\nData was extracted from the production analytics database using\nstandardized SQL queries documented in Appendix B.\n\n**Database**: `analytics_prod.sales_metrics`\n**Table Structure**: See @tbl:schema for schema details.\n\n| Column            | Type      | Description                    |\n|-------------------|-----------|--------------------------------|\n| transaction_id    | INTEGER   | Unique transaction identifier  |\n| customer_id       | INTEGER   | Customer identifier            |\n| region            | VARCHAR   | Geographic region              |\n| amount            | DECIMAL   | Transaction amount (USD)       |\n| transaction_date  | DATE      | Date of transaction            |\n\n: Database schema for sales_metrics table {#tbl:schema}\n\n**Data Period**: October 1, 2024 - December 31, 2024\n**Query Timestamp**: 2025-11-25 14:30:00 UTC\n**Records Examined**: 50,000 transactions\n\n## Analysis Framework\n\nOur analysis follows established data quality frameworks [@jones2024]\nand reproducible research standards [@smith2023]. Query methodology\nbuilds on best practices from prior regional expansion studies\n[@williams2024].\n\n## Data Quality Assessment\n\n**Completeness**: NULL values detected in 0.02% of records (10 of\n50,000), all in non-critical columns. All critical fields (transaction\namount, date, region) complete.\n\n**Consistency**: Zero duplicate transactions detected. Date ranges\nvalidated against expected period. Amount ranges ($5-$5,000) follow\nnormal distribution.\n\n**Outliers**: 12 outliers detected using 3 MAD threshold. All reviewed\nand validated as legitimate high-value transactions.\n\n## Query Methodology\n\nPrimary analysis query extracted regional performance metrics:\n\n\\```sql\nSELECT\n  region,\n  DATE_TRUNC('month', transaction_date) as month,\n  SUM(amount) as total_revenue,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  AVG(amount) as avg_transaction,\n  COUNT(*) as transaction_count\nFROM analytics_prod.sales_metrics\nWHERE DATE(transaction_date) BETWEEN '2024-10-01' AND '2024-12-31'\nGROUP BY region, month\nORDER BY month, total_revenue DESC\n\\```\n\n**Execution Details**:\n- Runtime: 2.3 seconds\n- Rows examined: 50,000\n- Rows returned: 18 (3 regions Ã— 6 months)\n- Indexes used: date_region_idx\n\nFull query text available in Appendix B.\n\n# Results {#sec:results}\n\n## Finding 1: Revenue Growth Concentration {#sec:revenue}\n\nWest Coast region demonstrated significantly higher revenue growth\ncompared to other regions.\n\n![Q4 Revenue by Region](visualizations/revenue-chart.png){#fig:revenue width=80%}\n\nFigure @fig:revenue shows quarterly revenue comparison across regions.\nWest Coast growth substantially exceeds other regions.\n\n### Evidence\n\n@tbl:revenue presents detailed revenue breakdown:\n\n| Region      | Q4 2023 | Q4 2024 | Growth  | % of Growth |\n|-------------|---------|---------|---------|-------------|\n| West Coast  | $750K   | $1,200K | +$450K  | 69%         |\n| East Coast  | $800K   | $900K   | +$100K  | 15%         |\n| Central     | $650K   | $750K   | +$100K  | 15%         |\n| **Total**   | $2,200K | $2,850K | +$650K  | 100%        |\n\n: Regional revenue breakdown Q4 2023 vs Q4 2024 {#tbl:revenue}\n\nWest Coast revenue reached $1.2M in Q4 2024, representing $450K increase\n(+60%) vs Q4 2023. All other regions combined grew $200K (+12%) over\nsame period.\n\n### Analysis\n\nConcentration of growth in West Coast region aligns with Q2 2024\nexpansion strategy targeting high-value customers in tech-heavy markets.\nFinding consistent with prior research on geographic market penetration\n[@jones2024].\n\n### Caveats\n\nSeveral factors may contribute to observed pattern:\n1. West Coast expansion received higher marketing budget allocation\n2. Seasonal factors (tech industry Q4 spending cycles)\n3. Customer base differences (B2B vs B2C mix varies by region)\n\nFurther analysis (Section @sec:retention) examines sustainability.\n\n## Finding 2: Customer Acquisition {#sec:customers}\n\n[Additional findings follow same pattern...]\n\n# Discussion {#sec:discussion}\n\n[Interpretation of findings, comparison to prior research, alternative explanations...]\n\n# Conclusions {#sec:conclusions}\n\n[Summary, recommendations, limitations, future directions...]\n\n# References\n\n<!-- Bibliography automatically generated here -->\n\n# Appendix A: Reproducibility Information {#sec:appendix-a}\n\n## Environment\n\n**Analysis Tools**:\n- DataPeeker: v2.1.0\n- Python: 3.11.5\n- pandas: 2.1.0\n- plotext: 5.2.8\n- Operating System: macOS 14.6.0\n\n**Database Connection**:\n- Host: analytics-prod-db.example.com\n- Database: analytics_prod\n- Schema: public\n- Table: sales_metrics\n\n## Reproducibility Instructions\n\n1. Clone analysis repository:\n   \\```bash\n   git clone github.com/tilmon/q4-analysis\n   cd q4-analysis\n   \\```\n\n2. Install dependencies:\n   \\```bash\n   pip install -r requirements.txt\n   \\```\n\n3. Set database credentials:\n   \\```bash\n   export DB_CONN=\"postgresql://user@host/analytics_prod\"\n   \\```\n\n4. Run analysis:\n   \\```bash\n   python scripts/q4_regional_analysis.py\n   \\```\n\n5. Generate report:\n   \\```bash\n   pandoc whitepaper.md \\\n     --citeproc \\\n     --bibliography references.bib \\\n     --csl ieee.csl \\\n     -F pandoc-crossref \\\n     -s --toc --number-sections \\\n     -V geometry:margin=1in \\\n     -o whitepaper.pdf\n   \\```\n\n# Appendix B: SQL Queries {#sec:appendix-b}\n\n## Query 1: Regional Revenue Analysis\n\n\\```sql\n-- Purpose: Compare regional revenue Q4 2023 vs Q4 2024\n-- Execution time: 2.3 seconds\n-- Rows returned: 6\n\nSELECT\n  region,\n  EXTRACT(year FROM transaction_date) as year,\n  EXTRACT(quarter FROM transaction_date) as quarter,\n  SUM(amount) as total_revenue,\n  COUNT(DISTINCT customer_id) as unique_customers,\n  COUNT(*) as transaction_count\nFROM analytics_prod.sales_metrics\nWHERE EXTRACT(quarter FROM transaction_date) = 4\n  AND EXTRACT(year FROM transaction_date) IN (2023, 2024)\nGROUP BY region, year, quarter\nORDER BY year, total_revenue DESC\n\\```\n\n[Additional queries...]\n```\n\n**Build Whitepaper**:\n```bash\n#!/bin/bash\n# build-whitepaper.sh\n\ncd analysis/q4-2024\n\npandoc whitepaper.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  --csl ieee.csl \\\n  -F pandoc-crossref \\\n  -s --toc --number-sections \\\n  -V geometry:margin=1in \\\n  -V fontsize=11pt \\\n  -V linestretch=1.15 \\\n  -V colorlinks=true \\\n  -V linkcolor=blue \\\n  -V citecolor=blue \\\n  -V urlcolor=blue \\\n  --pdf-engine=xelatex \\\n  -o whitepaper.pdf\n\necho \"Whitepaper generated: whitepaper.pdf\"\n```\n\n---\n\n## Best Practices\n\n### DO:\n\nâœ… **Use metadata YAML for all documents**\n```yaml\n---\ntitle: \"Document Title\"\nauthor: \"Your Name\"\ndate: \"2025-11-25\"\nbibliography: references.bib\n---\n```\n\nâœ… **Cite data sources in BibTeX**\n```bibtex\n@misc{production_db2025,\n  author = {Company Name},\n  title = {Production Database},\n  year = {2025},\n  note = {Query timestamp: 2025-11-25 14:30:00 UTC}\n}\n```\n\nâœ… **Use cross-references with pandoc-crossref**\n```markdown\nSee Figure @fig:chart1 and Table @tbl:results.\n```\n\nâœ… **Document SQL queries in appendix**\n```markdown\n# Appendix: SQL Queries\n\n\\```sql\nSELECT * FROM table WHERE...\n\\```\n```\n\nâœ… **Version control source and build scripts**\n```bash\ngit add whitepaper.md references.bib build.sh\ngit commit -m \"Add Q4 analysis whitepaper\"\n```\n\n### DON'T:\n\nâŒ **Skip bibliography** - Always cite data sources\nâŒ **Forget cross-references** - Use pandoc-crossref for figures/tables\nâŒ **Hardcode formatting** - Use variables and templates instead\nâŒ **Ignore reproducibility** - Document environment and queries\nâŒ **Skip validation** - Review PDF output before distribution\n\n---\n\n## Troubleshooting\n\n### Issue: \"pandoc: command not found\"\n\n**Solution**: Install pandoc\n```bash\nbrew install pandoc  # macOS\nsudo apt-get install pandoc  # Linux\n```\n\n### Issue: PDF generation fails\n\n**Solution**: Install LaTeX\n```bash\nbrew install --cask mactex  # macOS\nsudo apt-get install texlive-full  # Linux\n```\n\n### Issue: Bibliography not appearing\n\n**Solution**: Ensure `--citeproc` flag is used and bibliography file exists\n```bash\npandoc input.md \\\n  --citeproc \\\n  --bibliography references.bib \\\n  -s -o output.pdf\n```\n\n### Issue: Cross-references showing ??\n\n**Solution**: Use pandoc-crossref filter BEFORE --citeproc\n```bash\npandoc input.md \\\n  -F pandoc-crossref \\\n  --citeproc \\\n  --bibliography references.bib \\\n  -s -o output.pdf\n```\n\n### Issue: Citations showing [@citation_key] instead of numbers\n\n**Solution**: Use `--citeproc` flag (or `-C` shorthand)\n```bash\npandoc input.md -C --bibliography refs.bib -s -o output.pdf\n```\n\n---\n\n## References\n\n**Official Documentation**:\n- [Pandoc Official Website](https://pandoc.org/)\n- [Pandoc User's Guide](https://pandoc.org/MANUAL.html)\n- [pandoc-crossref Documentation](https://lierdakil.github.io/pandoc-crossref/)\n- [CSL Style Repository](https://github.com/citation-style-language/styles)\n- [Zotero Style Repository](https://www.zotero.org/styles)\n\n**Related DataPeeker Skills**:\n- `creating-visualizations` - Generate figures for whitepapers\n- `interpreting-results` - Develop findings to document\n- `frameworks/3-paragraph-essay.md` - Structure your whitepaper\n- `frameworks/narrative-structure.md` - Tell compelling data stories\n- `tools/marp.md` - Create presentations (complementary to pandoc)\n- `formats/citations.md` - Citing data sources properly\n- `formats/reproducibility.md` - Documenting reproducible research\n",
        "plugins/datapeeker/skills/qualitative-research/SKILL.md": "---\nname: qualitative-research\ndescription: Use when conducting customer discovery interviews, user research, surveys, focus groups, or observational research requiring rigorous analysis - provides systematic 6-phase framework with mandatory bias prevention (reflexivity, intercoder reliability, disconfirming evidence search) and reproducible methodology; peer to hypothesis-testing for qualitative vs quantitative validation\n---\n\n# Qualitative Research\n\n## Overview\n\nSystematic framework for conducting and analyzing qualitative research (interviews, surveys, focus groups, observations) with rigorous bias prevention and reproducible methodology.\n\n**Core principle:** Rigor through mandatory checkpoints. Prevent confirmation bias by enforcing disconfirming evidence search, intercoder reliability, and reflexivity documentation.\n\n**Peer to hypothesis-testing:** hypothesis-testing validates quantitative hypotheses with data analysis. qualitative-research validates qualitative hypotheses with systematic interview/survey analysis.\n\n## When to Use\n\nUse this skill when:\n- Conducting customer discovery interviews to validate demand\n- Running user research to understand pain points\n- Analyzing survey responses for themes and patterns\n- Conducting focus groups or observational research\n- ANY qualitative data collection and analysis requiring rigorous, reproducible methodology\n\nWhen NOT to use:\n- Quantitative data analysis (use hypothesis-testing instead)\n- Casual conversations or informal feedback (not systematic research)\n- Literature review or secondary research (use internet-researcher agent)\n\n## Mandatory Process Structure\n\n**YOU MUST use TodoWrite to track progress through all 6 phases.**\n\nCreate todos at the start:\n\n```markdown\n- Phase 1: Research Design (question, method, instrument, biases) - pending\n- Phase 2: Data Collection (execute protocol, track saturation) - pending\n- Phase 3: Data Familiarization (immerse without coding) - pending\n- Phase 4: Systematic Coding (codebook, reliability check) - pending\n- Phase 5: Theme Development (build themes, search disconfirming evidence) - pending\n- Phase 6: Synthesis & Reporting (findings, limitations, follow-ups) - pending\n```\n\nUpdate status as you progress. Mark phases complete ONLY after checkpoint verification.\n\n**Flexible Entry:** If user has existing data (transcripts, survey responses), can start at Phase 3. Verify raw data exists in `raw-data/` directory.\n\n---\n\n## Phase 1: Research Design\n\n**CHECKPOINT:** Before proceeding to Phase 2, you MUST have:\n- [ ] Research question defined (specific, testable)\n- [ ] Qualitative method selected (interview/survey/focus group/observation)\n- [ ] Collection instrument created (interview guide, survey questions, protocol)\n- [ ] Sampling strategy documented (who, how many, recruitment)\n- [ ] **Reflexivity baseline documented** (YOUR assumptions and biases written down)\n- [ ] Saved to `01-research-design.md`\n\n### Instructions\n\n1. **Select method and load appropriate template:**\n   - Interview â†’ Use `templates/interviews/phase-1-interview-guide.md`\n   - Survey â†’ Use `templates/surveys/phase-1-survey-design.md`\n   - Focus Group â†’ Use `templates/focus-groups/phase-1-facilitator-guide.md`\n   - Observation â†’ Use `templates/observations/phase-1-observation-protocol.md`\n\n2. **Document reflexivity baseline (MANDATORY):**\n\n**This is NON-NEGOTIABLE.** Before any data collection, write down:\n- What you believe the answer will be\n- What assumptions you're making\n- What biases you bring (industry experience, expert opinions, prior hypotheses)\n- What would surprise you\n\n**Why this matters:** If you don't document biases BEFORE data collection, you cannot identify confirmation bias AFTER.\n\n3. **Create neutral questions (use template guidance):**\n\nTemplates enforce neutral question design. Common mistakes:\n- Leading: \"How much would you pay for X?\" (assumes they want X)\n- Neutral: \"How do you currently solve Y problem?\" (explores actual behavior)\n\n4. **Plan adequate sample size:**\n- Interviews: Minimum 8-10 for saturation monitoring\n- Surveys: Depends on question type and analysis goals\n- Focus groups: 3-5 groups minimum\n- Observations: Plan for 10-20 observation sessions\n\n5. **Save to `01-research-design.md` using template**\n\n6. **STOP and verify checkpoint:** Cannot proceed to Phase 2 until reflexivity baseline documented.\n\n### Common Rationalization: \"I don't have biases to document\"\n\n**Why this is wrong:** Everyone has assumptions. If you can't name them, they're controlling you invisibly.\n\n**Do instead:** Write one sentence: \"I believe [X] because [Y].\" That's your bias. Document it.\n\n### Common Rationalization: \"Expert opinion reduces need for bias documentation\"\n\n**Why this is wrong:** Expert opinion IS a bias that must be documented. Authority backing is a strong prior.\n\n**Do instead:** \"Expert A said B. This is my assumption going in. Must verify with data.\"\n\n### Common Rationalization: \"Time pressure means I can't do formal process\"\n\n**Why this is wrong:** Documenting assumptions takes 5 minutes. Presenting biased findings wastes hours.\n\n**Do instead:** Set timer for 5 minutes. Write down assumptions. Move on.\n\n---\n\n## Phase 2: Data Collection\n\n**CHECKPOINT:** Before proceeding to Phase 3, you MUST have:\n- [ ] Minimum sample collected (Phase 1 plan executed)\n- [ ] Saturation monitoring documented\n- [ ] All raw data captured (transcripts, responses, field notes)\n- [ ] Raw data files in `raw-data/` directory\n- [ ] Reflexive journal maintained during collection\n- [ ] Saved to `02-data-collection-log.md`\n\n### Instructions\n\n1. **Execute method-specific protocol:**\n   - Use Phase 2 template for your selected method\n   - Maintain consistency (same questions, same facilitator when possible)\n   - Document context for each data collection instance\n\n2. **Track toward saturation:**\n\n**Saturation = when new insights stop emerging**\n\nAfter each interview/session/survey batch, ask:\n- Did this reveal new themes I hadn't seen?\n- Or was this reinforcing existing patterns?\n\nDocument in collection log. Plan to continue until 2-3 consecutive instances add nothing new.\n\n3. **Maintain reflexive journal (MANDATORY):**\n\nAfter each data collection instance, write:\n- What surprised you\n- What confirmed your assumptions\n- What contradicted your expectations\n- How your thinking is evolving\n\n**Why this matters:** Reflexivity tracks how your interpretation changes. Prevents retroactively fitting data to initial beliefs.\n\n4. **Create raw data files:**\n\n**File structure:**\n```\nraw-data/\nâ”œâ”€â”€ transcript-001.md\nâ”œâ”€â”€ transcript-002.md\nâ”œâ”€â”€ ...\n```\n\nOR for surveys:\n```\nraw-data/\nâ”œâ”€â”€ survey-responses-batch-1.md\nâ”œâ”€â”€ survey-responses-batch-2.md\n```\n\nOne file per interview/session. Numbered sequentially.\n\n5. **Save collection log to `02-data-collection-log.md`**\n\n6. **STOP and verify checkpoint:** Cannot proceed to Phase 3 until minimum sample collected and raw data captured.\n\n---\n\n## Phase 3: Data Familiarization\n\n**CHECKPOINT:** Before proceeding to Phase 4, you MUST have:\n- [ ] All raw data read/reviewed multiple times\n- [ ] Initial observations documented (NOT codes, just observations)\n- [ ] Surprising findings noted (contradictions to assumptions)\n- [ ] Reflexivity updated (how understanding evolved)\n- [ ] Saved to `03-familiarization-notes.md`\n\n### Instructions\n\n1. **Read ALL data without coding:**\n\n**This is critical:** Do NOT start coding yet. Just read and observe.\n\n**Why:** Premature coding locks you into first impressions. Familiarization lets patterns emerge naturally.\n\n2. **For large datasets (10+ interviews), use analyze-transcript agent:**\n\n```\nInvoke: analyze-transcript agent\nInput: transcript-001.md through transcript-010.md\nOutput: Summary, key quotes, initial observations per transcript\n```\n\nAgent prevents context pollution. Returns structured observations for your review.\n\n3. **Document observations in `03-familiarization-notes.md`:**\n\n**Format:**\n- Initial patterns noticed (not themes yet - just \"I see X coming up\")\n- Surprising findings (\"I expected A but saw B\")\n- Questions emerging (\"Why did 3 people mention Y?\")\n- Reflexive notes (\"This contradicts my assumption that...\")\n\n4. **STOP and verify checkpoint:** Cannot proceed to Phase 4 until all data reviewed and surprises documented.\n\n### Common Rationalization: \"I can code while familiarizing to save time\"\n\n**Why this is wrong:** Coding while familiarizing locks you into first impressions. Patterns shift after full dataset review.\n\n**Do instead:** Finish familiarization completely. Then start fresh with coding.\n\n---\n\n## Phase 4: Systematic Coding\n\n**CHECKPOINT:** Before proceeding to Phase 5, you MUST have:\n- [ ] Codebook complete (definitions, inclusion/exclusion criteria, examples)\n- [ ] Entire dataset coded systematically\n- [ ] **Intercoder reliability check completed** (10-20% sample)\n- [ ] Agreement percentage documented\n- [ ] Audit trail of all coding decisions\n- [ ] Saved to `04-coding-analysis.md`\n\n### Instructions\n\n1. **Develop initial codebook using agent:**\n\n```\nInvoke: generate-initial-codes agent\nInput: 2-3 transcripts or data segments\nOutput: Suggested codes with definitions and examples\n```\n\nReview agent suggestions. Refine codes. Create codebook.\n\n2. **Codebook structure (MANDATORY):**\n\nFor each code:\n- **Name:** Short label\n- **Definition:** What this code means\n- **Inclusion criteria:** When to apply this code\n- **Exclusion criteria:** When NOT to apply\n- **Examples:** 2-3 data extracts demonstrating code\n\n3. **Code all data systematically:**\n\nWork through raw data files sequentially. Apply codes from codebook. Document any new codes discovered (add to codebook with rationale).\n\n4. **Intercoder reliability check (MANDATORY - NON-NEGOTIABLE):**\n\n```\nInvoke: intercoder-reliability-check agent\nInput: Codebook + 2 transcripts (10-20% of dataset)\nOutput: Independent coding + agreement analysis\n```\n\n**This step is REQUIRED.** Cannot skip. Cannot defer. Cannot substitute with user review.\n\n**Why:** Even clear codebooks have subjective judgment. Second coder catches systematic bias in code application.\n\n5. **Document in `04-coding-analysis.md`:**\n\n**Sections:**\n- Section 1: Codebook (all codes with definitions and examples)\n- Section 2: Coding Process (how you applied codes, any refinements)\n- Section 3: Intercoder Reliability (agent results, agreement %, disagreement resolution)\n- Section 4: Audit Trail (all coding decisions documented)\n\n6. **STOP and verify checkpoint:** Cannot proceed to Phase 5 without intercoder reliability check COMPLETED and documented.\n\n### Common Rationalization: \"Coding was straightforward, low risk of errors\"\n\n**Why this is wrong:** \"Straightforward\" is subjective. Even clear codes have interpretation variance.\n\n**Do instead:** If coding is straightforward, intercoder reliability will be high and quick. Do the check.\n\n### Common Rationalization: \"Time constraints justify skipping verification\"\n\n**Why this is wrong:** Presenting flawed findings takes more time to fix than 1-hour verification.\n\n**Do instead:** Verification takes 1 hour. Fixing flawed findings after presentation takes days. Do the math.\n\n### Common Rationalization: \"User reviewed coding, that's enough validation\"\n\n**Why this is wrong:** User can't catch their own interpretation bias. Second coder does.\n\n**Do instead:** User review is pre-flight check. Intercoder reliability is the actual test. Both required.\n\n### Common Rationalization: \"Can do reliability check later if needed\"\n\n**Why this is wrong:** After themes developed, reliability check invalidates hours of work if problems found.\n\n**Do instead:** Reliability MUST be verified in Phase 4, not Phase 6. Do it now.\n\n---\n\n## Phase 5: Theme Development & Refinement\n\n**CHECKPOINT:** Before proceeding to Phase 6, you MUST have:\n- [ ] Themes defined with supporting codes\n- [ ] **Disconfirming evidence search completed** (MANDATORY for ALL themes)\n- [ ] Negative cases explained (data that doesn't fit themes)\n- [ ] Themes refined based on full dataset review\n- [ ] Verbatim data extracts supporting each theme\n- [ ] Saved to `05-theme-development.md`\n\n### Instructions\n\n1. **Group codes into potential themes using agent:**\n\n```\nInvoke: identify-themes agent\nInput: Codebook + all coded segments\nOutput: Potential themes with supporting codes and data extracts\n```\n\nReview agent suggestions. Refine theme definitions.\n\n2. **Disconfirming evidence search (MANDATORY - NON-NEGOTIABLE):**\n\n**For EACH theme, you MUST run:**\n\n```\nInvoke: search-disconfirming-evidence agent\nInput: Theme definition + full dataset\nOutput: Contradictory evidence, edge cases, exceptions to pattern\n```\n\n**This is REQUIRED.** No exceptions. No shortcuts. No \"pattern is obvious so no need.\"\n\n**Why:** Clear patterns are MOST vulnerable to confirmation bias. Obvious themes need MOST rigorous verification.\n\n3. **Document negative cases:**\n\nFor each theme, explain:\n- How many participants DON'T fit this theme?\n- What did those participants say instead?\n- Why doesn't the theme apply to them?\n- Is there a boundary condition (theme applies only in specific contexts)?\n\n**Example:**\n```\nTheme 1: \"Cost concerns are primary barrier\" - 8 of 10 participants\n\nNEGATIVE CASES:\n- Participant 3: Didn't mention cost. Focused entirely on integration complexity.\n- Participant 7: Said price was \"not a concern if it solves the problem\"\n\nEXPLANATION: Theme applies to majority but not universal. Subset willing to pay premium for right solution.\n```\n\n4. **Refine themes based on disconfirming evidence:**\n\nAfter seeing contradictions, revise theme definitions for accuracy. \"8 of 10\" is more honest than \"all participants.\"\n\n5. **Extract supporting quotes using agent:**\n\n```\nInvoke: extract-supporting-quotes agent\nInput: Theme definition + coded dataset\nOutput: Best representative verbatim quotes for each theme\n```\n\n6. **Document in `05-theme-development.md`:**\n\n**Format:**\n- Theme name and definition\n- Supporting codes\n- Prevalence (X of Y participants)\n- Verbatim quotes (use extract-supporting-quotes agent output)\n- Disconfirming evidence (from search-disconfirming-evidence agent)\n- Negative case explanation\n\n7. **STOP and verify checkpoint:** Cannot proceed to Phase 6 without disconfirming evidence search for ALL themes.\n\n### Common Rationalization: \"Themes are clearly supported by majority of participants\"\n\n**Why this is wrong:** Majority agreement doesn't eliminate contradictory evidence. Must explain ALL data.\n\n**Do instead:** \"8 of 10 mentioned cost. What about the 2 who didn't? Must explain.\"\n\n### Common Rationalization: \"Expert prediction validates findings\"\n\n**Why this is wrong:** Expert prediction + matching findings = confirmation bias red flag, not validation.\n\n**Do instead:** When predictions match findings perfectly, search HARDEST for contradictions.\n\n### Common Rationalization: \"High consistency (8/10, 9/10) indicates robust themes\"\n\n**Why this is wrong:** High unanimity can indicate leading questions or selective interpretation.\n\n**Do instead:** Real customer sentiment is messy. 9/10 agreement deserves scrutiny, not celebration.\n\n### Common Rationalization: \"Disconfirming evidence search unnecessary when pattern is obvious\"\n\n**Why this is wrong:** Obvious patterns are MOST vulnerable to confirmation bias.\n\n**Do instead:** Obvious patterns require MOST rigorous disconfirmation. Search is mandatory.\n\n---\n\n## Phase 6: Synthesis & Reporting\n\n**CHECKPOINT:** Before marking complete, you MUST have:\n- [ ] Findings documented with verbatim quotes for each theme\n- [ ] **Limitations explicitly stated** (sample, method, researcher bias, context)\n- [ ] Confidence assessment (credibility, dependability, confirmability, transferability)\n- [ ] 2-3 follow-up research questions identified\n- [ ] Overview updated with final summary\n- [ ] Saved to `06-findings-report.md` and `00-overview.md` updated\n\n### Instructions\n\n1. **Write findings report:**\n\n**Structure:**\n- **Main Findings:** Each theme with supporting quotes\n- **Prevalence:** Honest reporting (X of Y, not \"all\" or \"most\")\n- **Negative Cases:** Exceptions explained\n- **Context:** When/where does this apply?\n\n2. **Document limitations (MANDATORY - be HONEST):**\n\n**You MUST address:**\n- Sample limitations (size, homogeneity, recruitment source)\n- Method constraints (interviews vs. observations, question design)\n- Researcher bias (documented in Phase 1, how it may have influenced)\n- Context limitations (geography, time period, industry)\n\n**Why:** Acknowledging limitations STRENGTHENS credibility. False certainty undermines trust.\n\n3. **Assess confidence (trustworthiness criteria):**\n\n- **Credibility:** Do findings accurately represent participant experiences?\n- **Dependability:** Would another researcher reach similar conclusions?\n- **Confirmability:** Are findings based on data, not researcher bias?\n- **Transferability:** Do findings apply beyond this specific sample?\n\nRate each: High / Medium / Low. Provide justification.\n\n4. **Identify 2-3 follow-up questions:**\n\nEvery analysis should raise new questions:\n- What would you investigate next?\n- What surprised you that needs deeper exploration?\n- What would strengthen confidence in findings?\n\n5. **Update `00-overview.md` with summary:**\n\nAdd final summary section with:\n- Main findings (3-5 bullet points)\n- Signal classification (if invoked by marketing-experimentation): Positive/Negative/Null/Mixed\n- Confidence level\n- Follow-up recommendations\n\n6. **Save to `06-findings-report.md`**\n\n7. **Mark Phase 6 complete:** All checkpoints verified.\n\n### Common Rationalization: \"Limitations will undermine findings, downplay them\"\n\n**Why this is wrong:** Stating limitations INCREASES credibility. Readers trust honest uncertainty.\n\n**Do instead:** State limitations clearly. Be honest about what you don't know.\n\n---\n\n## Common Rationalizations - STOP\n\nThese are violations of skill requirements:\n\n| Excuse | Reality |\n|--------|---------|\n| \"I don't have biases to document\" | Everyone has assumptions. If you can't name them, they're controlling you invisibly. |\n| \"Expert opinion reduces need for bias documentation\" | Expert opinion IS a bias. Authority backing is a strong prior that MUST be documented. |\n| \"Time pressure justifies skipping formal process\" | Documenting assumptions takes 5 minutes. Presenting biased findings wastes hours. |\n| \"Coding was straightforward, low risk\" | \"Straightforward\" is subjective. Even clear codes have interpretation variance. |\n| \"Time constraints justify skipping verification\" | Verification takes 1 hour. Fixing flawed findings after presentation takes days. |\n| \"Informal spot-check is sufficient\" | Spot-checks catch obvious errors. Intercoder reliability catches systematic bias. Both required. |\n| \"User reviewed coding, enough validation\" | User can't catch their own interpretation bias. Second coder does. Non-negotiable. |\n| \"Can do reliability check later if needed\" | After themes developed, reliability check invalidates hours of work. Do it in Phase 4. |\n| \"Themes clearly supported by majority\" | Majority agreement doesn't eliminate contradictory evidence. Must explain ALL data. |\n| \"Expert prediction validates findings\" | When predictions match findings perfectly, that's when to search hardest for contradictions. |\n| \"High consistency (8/10, 9/10) indicates robustness\" | Real customer sentiment is messy. 9/10 agreement deserves scrutiny. |\n| \"Disconfirming evidence search unnecessary for obvious patterns\" | Obvious patterns MOST vulnerable to confirmation bias. Search is mandatory. |\n| \"Limitations undermine findings\" | Stating limitations INCREASES credibility. False certainty undermines trust. |\n| \"This is just initial/exploratory research\" | Exploratory means open-ended questions. Doesn't mean skip rigor. Follow the phases. |\n| \"I'm following the spirit of the rules\" | Violating checkpoints violates both letter AND spirit. No shortcuts. |\n\n**All of these mean: Checkpoint violated. Cannot proceed.**\n\n## Red Flags - STOP\n\nIf you catch yourself thinking ANY of these, you are rationalizing. STOP and follow the checkpoint:\n\n- \"I recommend...\" (should be \"You MUST...\")\n- \"Would you like to...\" (should be \"Cannot proceed without...\")\n- \"This is optional\" (critical steps are MANDATORY)\n- \"Spot-check\" instead of \"intercoder reliability check\"\n- \"I'll look for contradictions\" instead of \"Invoking search-disconfirming-evidence agent\"\n- \"This is just initial validation\" (rigor required at all stages)\n- \"Expert backing reduces need for X\" (authority is bias, must be documented)\n- \"Pattern is obvious\" (obvious patterns need MOST rigorous verification)\n- \"Can skip X and do it later\" (checkpoints are mandatory NOW, not later)\n\n**All of these mean: Violated skill requirements. Go back and complete checkpoint.**\n\n---\n\n## Summary\n\nThis skill ensures rigorous, reproducible qualitative research by:\n\n1. **Preventing confirmation bias:** Reflexivity baseline, neutral questions, disconfirming evidence search\n2. **Ensuring systematic analysis:** Codebook rigor, intercoder reliability, audit trails\n3. **Enforcing checkpoints:** Cannot skip critical steps (reflexivity, reliability, disconfirmation)\n4. **Using agent-based methods:** Sub-agents handle data-intensive operations, prevent context pollution\n5. **Demanding intellectual honesty:** Explicit limitations, confidence assessment, honest prevalence reporting\n\n**Follow this process and you'll produce defensible, credible qualitative research that stands up to scrutiny.**\n",
        "plugins/datapeeker/skills/qualitative-research/templates/focus-groups/phase-1-facilitator-guide.md": "# Research Design - Focus Group Method\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 1 - Research Design\n**Method:** Focus Groups\n\n---\n\n## Research Question\n\n**Primary Question:**\n[What are you trying to understand through focus groups?]\n\n**Sub-Questions:**\n1. [Supporting question 1]\n2. [Supporting question 2]\n3. [Supporting question 3]\n\n**Success Criteria:**\n[What would constitute a successful answer to this research question?]\n\n---\n\n## Reflexivity Baseline (MANDATORY)\n\n**Purpose:** Document your assumptions BEFORE data collection to prevent confirmation bias.\n\n### My Assumptions\n\n**What I believe the answer will be:**\n[Write down what you expect to find. Be specific.]\n\n**Why I believe this:**\n[What experience, expert opinion, or prior knowledge shapes this belief?]\n\n**What would surprise me:**\n[What findings would contradict your expectations?]\n\n### My Biases\n\n**Industry/domain experience:**\n[How might your background influence what you notice or interpret?]\n\n**Expert opinions I'm carrying:**\n[What have mentors, consultants, or authorities told you?]\n\n**Prior hypotheses:**\n[What have you already concluded that might bias interpretation?]\n\n**Desired outcome:**\n[What result would you WANT to find? How might this create bias?]\n\n---\n\n## Focus Group Strategy\n\n**Why focus groups instead of interviews?**\n[Group interaction reveals social norms, shared experiences, disagreements, consensus-building]\n\n**Group size:** [6-10 participants per session recommended]\n\n**Number of sessions:** [3-5 sessions recommended for saturation]\n\n**Session duration:** [90-120 minutes typical]\n\n**Format:** [In-person / Virtual via Zoom / Hybrid]\n\n---\n\n## Participant Selection\n\n### Sampling Strategy\n\n**Total participants needed:** [N participants across M sessions]\n\n**Homogeneous vs Heterogeneous groups:**\n- [ ] **Homogeneous:** Same background/experience (encourages candor)\n- [ ] **Heterogeneous:** Diverse backgrounds (encourages varied perspectives)\n- [ ] **Segmented:** Multiple homogeneous groups to compare (e.g., Group 1 = managers, Group 2 = individual contributors)\n\n**Rationale:** [Why this composition?]\n\n### Inclusion Criteria\n\n**Who should participate:**\n- [Criterion 1]\n- [Criterion 2]\n- [Criterion 3]\n\n### Exclusion Criteria\n\n**Who should NOT participate:**\n- [Criterion 1 - e.g., \"Direct competitors\"]\n- [Criterion 2 - e.g., \"People with power dynamics over each other\"]\n- [Criterion 3]\n\n**CRITICAL:** Avoid power imbalances within groups (manager + their direct report, buyer + seller, etc.)\n\n### Recruitment Plan\n\n**Recruitment method:** [Email / Referrals / Social media / etc.]\n\n**Recruitment message template:**\n\n\"Dear [Name],\n\nI'm conducting research on [topic] and would like to invite you to participate in a focus group discussion.\n\nThe session will bring together [6-10] people who [description] to discuss [topics]. The conversation will last approximately [90-120] minutes and will be held [in-person at location / via Zoom] on [date range options].\n\nYour participation would involve:\n- [Duration] group discussion facilitated by [name/role]\n- Sharing your experiences and perspectives on [topic]\n- Interacting with other participants\n\n[If incentive: As a thank you for your time, [incentive details].]\n\nIf you're interested, please reply by [date] and I'll send you available times.\n\nThank you for considering this!\n\n[Your name]\"\n\n**Recruitment timeline:**\n- **Recruitment start:** [YYYY-MM-DD]\n- **Recruitment close:** [YYYY-MM-DD]\n- **Confirmations due:** [YYYY-MM-DD]\n\n---\n\n## Discussion Guide\n\n**Purpose:** Structured agenda to guide discussion while allowing organic interaction.\n\n### Session Structure\n\n**Total time:** [90-120 minutes]\n\n1. **Welcome & Setup** (10 min)\n2. **Warm-up** (10 min)\n3. **Core Discussion** (50-70 min)\n4. **Wrap-up & Closing** (10-20 min)\n\n---\n\n### 1. Welcome & Setup (10 minutes)\n\n**Welcome participants:**\n\"Thank you all for joining today's discussion on [topic]. I'm [name] and I'll be facilitating our conversation.\"\n\n**Purpose & goals:**\n\"We're here to understand [research goal]. Your experiences and perspectives are valuable. There are no right or wrong answers - we want to hear diverse viewpoints.\"\n\n**Consent & recording:**\n\"This session will be recorded [audio/video] so I can focus on facilitating rather than taking notes. The recording will be transcribed and [stored securely / deleted after transcription]. Your comments will be [anonymous / confidential] in any reports.\"\n\n**Obtain consent:** [Verbal on recording / Written consent forms]\n\n**Ground rules:**\n1. \"One person speaks at a time\"\n2. \"All perspectives are welcome - there's no right answer\"\n3. \"What's said here stays here (confidentiality)\"\n4. \"It's OK to disagree - respectfully\"\n5. \"Please silence phones and minimize distractions\"\n6. \"[Virtual only] Keep video on if possible, use chat for questions\"\n\n**Introductions:**\n\"Let's go around and introduce ourselves. Please share [name, role/context, and how long you've been working with topic].\"\n\n---\n\n### 2. Warm-Up (10 minutes)\n\n**Purpose:** Build rapport, get people talking in low-stakes way.\n\n**Warm-up question:**\n\"To start, let's go around and everyone share: [easy, relevant question]\"\n\n**Examples:**\n- \"What's your first memory of encountering [topic]?\"\n- \"If you had to describe [topic] in three words, what would they be?\"\n- \"What brought you to [industry/role/situation]?\"\n\n**Facilitator notes:**\n- Keep this light and quick\n- Ensure everyone speaks (sets pattern for participation)\n- Listen for themes to explore later\n\n---\n\n### 3. Core Discussion (50-70 minutes)\n\n**CRITICAL:** These are TOPIC prompts, not rigid scripts. Follow the conversation.\n\n#### Topic 1: Current Experience (15-20 min)\n\n**Opening prompt:**\n\"Let's start by talking about how you currently [handle situation / use product / experience topic]. Can someone share how this works for you?\"\n\n**Follow-up probes:**\n- \"How long have you been doing it this way?\"\n- \"Does anyone have a different experience?\"\n- \"What's working well about this approach?\"\n- \"What's challenging or frustrating?\"\n\n**Facilitator goals:**\n- Get 3-4 people to share specific examples\n- Encourage others to build on or contrast with what's been shared\n- Note areas of consensus and disagreement\n\n---\n\n#### Topic 2: Specific Challenges (15-20 min)\n\n**Opening prompt:**\n\"I'd like to hear about times when [topic] has been particularly challenging. Can someone describe a specific situation?\"\n\n**Follow-up probes:**\n- \"What made that situation difficult?\"\n- \"Has anyone else experienced something similar?\"\n- \"What did you do in that situation?\"\n- \"What would have made it better?\"\n\n**Facilitator goals:**\n- Get concrete stories, not vague complaints\n- Explore the CONTEXT of challenges\n- Understand what they've tried (reveals actual behavior vs hypotheticals)\n\n---\n\n#### Topic 3: Solutions & Attempts (15-20 min)\n\n**Opening prompt:**\n\"What have you tried to address [challenge/problem] we've been discussing?\"\n\n**Follow-up probes:**\n- \"Why did you choose that approach?\"\n- \"What happened when you tried it?\"\n- \"Why did you stick with it / stop using it?\"\n- \"What were you comparing it against?\"\n\n**Facilitator goals:**\n- Understand past behavior (Mom Test principle)\n- Identify what they've actually spent time/money on\n- Reveal decision-making criteria\n\n---\n\n#### Topic 4: Ideal State (10-15 min)\n\n**Opening prompt:**\n\"If you could wave a magic wand and change [topic], what would be different?\"\n\n**Follow-up probes:**\n- \"Why would that matter to you?\"\n- \"How would that change your workflow/experience?\"\n- \"What would you be willing to trade for that?\"\n\n**Facilitator notes:**\n- This is somewhat hypothetical (use sparingly)\n- Focus on the \"why\" behind what they want\n- Connect back to real challenges discussed earlier\n\n---\n\n### 4. Wrap-Up & Closing (10-20 minutes)\n\n**Summary:**\n\"Before we close, let me summarize what I've heard... [Summarize 3-4 key themes]. Did I capture that accurately? Anything I missed?\"\n\n**Final question (ALWAYS ask):**\n\"Is there anything about [topic] that we haven't discussed that you think is important?\"\n\n**Thank you:**\n\"Thank you all for sharing your time and perspectives. Your insights are incredibly valuable.\"\n\n**Next steps:**\n[If applicable: \"I'll be analyzing these conversations and [sharing findings / following up / etc.]\"]\n\n**Incentive distribution:**\n[If applicable: How/when they'll receive incentive]\n\n---\n\n## Facilitator Techniques\n\n### Managing Group Dynamics\n\n#### The Dominant Talker\n\n**Symptoms:** One person answers every question, talks over others\n\n**Interventions:**\n1. **Direct others:** \"Thank you [Name]. I'd love to hear from someone who hasn't spoken yet. [Other name], what's your experience?\"\n2. **Use hand raising:** \"Let's hear from a few different people. Raise your hand if you've experienced this.\"\n3. **Private check-in:** [If virtual] Private message: \"Thank you for your input. Can you help me draw out quieter voices?\"\n\n#### The Quiet Participant\n\n**Symptoms:** Someone hasn't spoken much, seems disengaged\n\n**Interventions:**\n1. **Direct invitation:** \"[Name], I notice you haven't shared yet - I'd love to hear your perspective on this.\"\n2. **Easier entry point:** Ask them to respond to something specific vs open-ended question\n3. **Non-verbal check:** [If in-person] Eye contact, open body language to invite them in\n4. **Accept silence:** Some people process by listening - that's OK\n\n#### The Tangent\n\n**Symptoms:** Discussion veers off-topic\n\n**Interventions:**\n1. **Acknowledge & redirect:** \"That's interesting. Let's bookmark that and come back to [topic].\"\n2. **Check relevance:** \"How does this relate to [topic]?\"\n3. **Group decision:** \"This is interesting - should we explore this or return to [topic]?\"\n\n#### The Disagreement\n\n**Symptoms:** Participants disagree, tension rises\n\n**This is GOOD - lean into it:**\n1. **Normalize:** \"It sounds like there are different experiences here - that's valuable.\"\n2. **Explore:** \"Tell me more about why you see it differently.\"\n3. **Avoid resolving:** Don't try to create consensus - diversity is the goal\n4. **Intervene only if:** Disrespectful or personal attacks\n\n#### The Groupthink\n\n**Symptoms:** Everyone agrees quickly, little depth\n\n**Interventions:**\n1. **Push deeper:** \"What makes you say that? Can you give a specific example?\"\n2. **Invite dissent:** \"Does anyone see this differently?\"\n3. **Play devil's advocate:** \"Some people might say [contrarian view] - what do you think?\"\n\n---\n\n### Probing Questions\n\n**When someone gives surface-level answer:**\n- \"Can you give me a specific example?\"\n- \"Tell me more about that\"\n- \"What did that look like in practice?\"\n\n**When someone states opinion without context:**\n- \"Why do you think that?\"\n- \"What experience led you to that conclusion?\"\n\n**When you want to explore difference:**\n- \"How is that different from [earlier point]?\"\n- \"Does anyone have a contrasting experience?\"\n\n**When you want depth:**\n- \"Walk me through what happened step by step\"\n- \"What was the outcome?\"\n- \"How did that make you feel?\"\n\n---\n\n## Logistics Planning\n\n### Location & Setup (In-Person)\n\n**Location requirements:**\n- [ ] Private room (no interruptions or overhearing)\n- [ ] Comfortable seating for [N] people\n- [ ] Table configuration (round or rectangle so everyone can see each other)\n- [ ] Good acoustics for recording\n- [ ] Refreshments available (water, coffee, snacks)\n- [ ] Accessible location\n- [ ] [If evening] Safe area with parking\n\n**Equipment:**\n- [ ] Audio recorder (+ backup)\n- [ ] Name tents or name tags\n- [ ] Printed discussion guide\n- [ ] Consent forms\n- [ ] Pens/paper for note-taking\n- [ ] Timer (to keep on track)\n\n---\n\n### Virtual Setup (Zoom/Video)\n\n**Platform requirements:**\n- [ ] Video conferencing platform tested (Zoom, Google Meet, etc.)\n- [ ] Recording capability enabled\n- [ ] Breakout rooms available (if using sub-groups)\n- [ ] Screen sharing capability (if showing stimuli)\n- [ ] Chat function enabled (for questions/comments)\n\n**Pre-session tech check:**\n- [ ] Test recording 24 hours before\n- [ ] Send participants tech requirements (camera, microphone)\n- [ ] Provide dial-in number as backup\n- [ ] Have co-facilitator or note-taker to monitor chat\n\n**Virtual-specific ground rules:**\n- \"Keep video on if possible\"\n- \"Mute when not speaking if there's background noise\"\n- \"Use chat for questions or if you're having trouble breaking in\"\n- \"Use 'raise hand' feature if helpful\"\n\n---\n\n### Roles & Responsibilities\n\n**Facilitator (you):**\n- Guide discussion using discussion guide\n- Ensure everyone participates\n- Manage time\n- Probe for depth\n- Maintain neutral, curious stance\n\n**Note-taker (optional but recommended):**\n- Document who said what (pseudonyms)\n- Note non-verbal cues (body language, tone)\n- Track themes emerging\n- Flag moments to revisit\n- Monitor time\n\n**Tech support (for virtual):**\n- Manage recording\n- Monitor chat\n- Handle tech issues\n- Admit late arrivals\n\n---\n\n## Session Schedule\n\n### Session 1\n- **Date:** [YYYY-MM-DD]\n- **Time:** [HH:MM - HH:MM]\n- **Location:** [Address / Zoom link]\n- **Confirmed participants:** [N]\n- **Group composition:** [Description - e.g., \"All managers with 5+ years experience\"]\n\n### Session 2\n- **Date:** [YYYY-MM-DD]\n- **Time:** [HH:MM - HH:MM]\n- **Location:** [Address / Zoom link]\n- **Confirmed participants:** [N]\n- **Group composition:** [Description]\n\n### Session 3\n[Repeat...]\n\n**Buffer sessions planned:** [N additional sessions scheduled in case of no-shows or to reach saturation]\n\n---\n\n## Consent & Ethics\n\n**Consent method:** [Written forms signed before start / Verbal consent recorded at beginning]\n\n**Consent statement:**\n\"By participating in this focus group, you consent to your comments being recorded and used for [purpose]. Your identity will be kept [confidential / anonymous]. You may decline to answer any question or leave at any time.\"\n\n**Confidentiality:**\n- Participant names â†’ Pseudonyms in transcripts\n- Quotes will be [attributed to pseudonym / completely anonymous]\n- Other participants will be asked to maintain confidentiality (but cannot be guaranteed)\n\n**Data protection:**\n- Recordings stored: [Where?]\n- Access limited to: [Who?]\n- Deletion timeline: [When?]\n\n---\n\n## Incentive Plan (if applicable)\n\n**Incentive offered:** [Gift card / Cash / Donation / Free service / etc.]\n\n**Amount:** [$X per person per session]\n\n**Distribution method:** [Email gift card / Handed out at end / Mailed / etc.]\n\n**Total budget:** [$X for N participants across M sessions]\n\n**Incentive distribution log:**\n\n| Session | Participant ID | Incentive | Distributed | Notes |\n|---------|---------------|-----------|------------|-------|\n| 1 | P001 | $X | [Date] | |\n| 1 | P002 | $X | [Date] | |\n\n---\n\n## Phase 1 Checkpoint Verification\n\n**Before proceeding to Phase 2, verify:**\n\n- [ ] Research question defined and specific\n- [ ] Discussion guide created with topic prompts and probes\n- [ ] Facilitator techniques documented\n- [ ] Sampling strategy defined (homogeneous/heterogeneous/segmented)\n- [ ] Inclusion/exclusion criteria clear\n- [ ] Recruitment plan with timeline\n- [ ] [N] sessions scheduled with [N] confirmed participants each\n- [ ] Logistics planned (location OR virtual platform tested)\n- [ ] Roles assigned (facilitator, note-taker, tech support)\n- [ ] Equipment/tech checklist completed\n- [ ] **Reflexivity baseline completed** (assumptions and biases documented)\n- [ ] Consent procedures defined\n- [ ] Ground rules prepared\n- [ ] All content saved to `01-research-design.md`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 2 - Focus Group Execution\n**If FAIL:** Complete missing requirements (especially reflexivity baseline and session scheduling)\n",
        "plugins/datapeeker/skills/qualitative-research/templates/focus-groups/phase-2-session-execution.md": "# Data Collection - Focus Group Session Execution\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 2 - Data Collection (Focus Group Method)\n\n---\n\n## Pre-Session Checklist (24 Hours Before)\n\n**Confirm with participants:**\n- [ ] Send reminder email with:\n  - Date, time, location (with map/directions) OR Zoom link\n  - Duration reminder\n  - What to expect\n  - Tech requirements (if virtual)\n  - Contact information for questions\n  - Parking/accessibility information (if in-person)\n\n**Prepare materials:**\n- [ ] Discussion guide printed/accessible\n- [ ] Consent forms (one per participant)\n- [ ] Name tents or name tags\n- [ ] Pens, paper for notes\n- [ ] Recording devices tested (primary + backup)\n- [ ] Timer or clock visible\n- [ ] Refreshments arranged (if in-person)\n\n**Virtual setup (if applicable):**\n- [ ] Test Zoom/platform recording 24 hours before\n- [ ] Meeting link tested and sent\n- [ ] Waiting room enabled\n- [ ] Co-facilitator or note-taker briefed\n- [ ] Backup dial-in number provided\n\n---\n\n## Session Execution Protocol\n\n### Arrival & Setup (15 minutes before start)\n\n**In-person:**\n- [ ] Arrive 15-30 minutes early\n- [ ] Set up room (seating arrangement, refreshments)\n- [ ] Test recording equipment\n- [ ] Place name tents\n- [ ] Set up observation area (if note-taker present)\n\n**Virtual:**\n- [ ] Open Zoom 15 minutes early\n- [ ] Test recording\n- [ ] Brief co-facilitator/note-taker on roles\n- [ ] Prepare to admit participants from waiting room\n\n**As participants arrive:**\n- Greet warmly\n- Offer refreshments (if in-person)\n- Small talk to build comfort\n- Have them fill out consent form\n- Give them name tent to fill out (first name only or pseudonym)\n\n---\n\n### Running the Session\n\n**Timing guideline:**\n\n| Section | Duration | Notes |\n|---------|----------|-------|\n| Welcome & Ground Rules | 10 min | Don't skip - sets tone |\n| Warm-up | 10 min | Get everyone speaking early |\n| Topic 1 | 15-20 min | Current experience |\n| Topic 2 | 15-20 min | Challenges |\n| Topic 3 | 15-20 min | Solutions/attempts |\n| Topic 4 | 10-15 min | Ideal state (optional) |\n| Wrap-up | 10-20 min | Summary + final thoughts |\n| **Total** | **90-120 min** | |\n\n**Facilitator stance:**\n- Curious, not expert\n- Neutral, not leading\n- Encouraging, not judging\n- Guiding, not controlling\n\n---\n\n### 1. Welcome & Ground Rules (10 minutes)\n\n**START RECORDING**\n\n**Verbal consent on recording:**\n\"Before we begin, I'm starting the recording now. By staying in this session, you consent to being recorded. If anyone is not comfortable, please let me know now.\"\n\n**Welcome script:**\n\"Thank you all for being here. I'm [name] and [role/affiliation]. We're here today to discuss [topic] and I want to learn from your experiences.\n\nThere are no right or wrong answers - I'm interested in your honest perspectives. You may hear different opinions from others in the room, and that's valuable.\n\nThis session will be recorded so I can focus on facilitating rather than taking notes. The recording will be transcribed and your names will be replaced with pseudonyms. Nothing you say will be directly attributed to you by name.\"\n\n**Ground rules:**\n1. \"One person speaks at a time - please don't talk over each other\"\n2. \"All perspectives are valued - there's no right answer\"\n3. \"What's shared here stays here - please respect each other's confidentiality\"\n4. \"It's okay to disagree, but please do so respectfully\"\n5. \"Please silence phones\"\n6. [Virtual] \"Keep your video on if possible, use chat for questions\"\n\n**Introductions:**\n\"Let's go around the room. Please share your first name, [relevant context like role or how long you've worked with topic].\"\n\n[Note-taker: Document who's present and seating arrangement]\n\n---\n\n### 2. Warm-Up (10 minutes)\n\n**Purpose:** Get everyone comfortable speaking.\n\n**Warm-up question:**\n[Use question from discussion guide]\n\n**Facilitator notes:**\n- Go around the room so everyone speaks\n- Keep this light and quick (1-2 min per person max)\n- Show interest in every response\n- Note themes for later exploration\n\n---\n\n### 3. Core Discussion (50-70 minutes)\n\n**Follow discussion guide topics.**\n\n**Active facilitation techniques:**\n\n**Encourage depth:**\n- \"Can you give me a specific example of that?\"\n- \"Tell me more about that\"\n- \"What did that look like?\"\n\n**Encourage interaction:**\n- \"Does anyone else have experience with this?\"\n- \"How does your experience compare to what [Name] just shared?\"\n- \"I'm hearing different perspectives - tell me more about why you see it differently\"\n\n**Redirect if needed:**\n- \"That's interesting - let's bookmark that and come back to [topic]\"\n- \"How does this relate to [topic we were discussing]?\"\n\n**Balance participation:**\n- \"Let's hear from someone who hasn't spoken in a while. [Name], what do you think?\"\n- \"I want to make sure we hear from everyone. [Quiet person], I'd love your perspective.\"\n\n**Manage dominance:**\n- \"Thank you [Dominant person]. Let's hear from others on this.\"\n- [To group] \"Let's get a few different perspectives before we move on.\"\n\n**CRITICAL:** Stay neutral - don't show agreement or disagreement with any viewpoint.\n\n---\n\n### 4. Wrap-Up (10-20 minutes)\n\n**Summary:**\n\"We're coming to the end of our time. Let me summarize what I've heard... [Summarize 3-4 key themes]. Did I capture that accurately? Anything important I missed?\"\n\n**Final question (ALWAYS ask):**\n\"Before we close - is there anything about [topic] that we haven't discussed that you think is important?\"\n\n[Often yields valuable unexpected insights]\n\n**Thank you:**\n\"Thank you all for your time and for sharing your perspectives so openly. This conversation has been incredibly valuable.\"\n\n**Next steps:**\n[If applicable: \"I'll be analyzing these conversations along with [N] other groups and [what happens with findings]\"]\n\n**Incentive distribution:**\n[If applicable: \"I'll send your [gift card / compensation] to the email you provided within [timeframe]\"]\n\n**STOP RECORDING**\n\n---\n\n## Immediate Post-Session Tasks\n\n**TIMING:** Complete within 1 hour while memory is fresh.\n\n### 1. Save Recording\n\n- [ ] Stop recording\n- [ ] Save file immediately\n- [ ] Rename: `focus-group-[session-number]-[YYYY-MM-DD].[extension]`\n- [ ] Backup to secondary location\n- [ ] Test file (confirm audio is clear)\n\n**Recording log:**\n\n| Session | Date | Filename | Duration | Quality | Backup Location |\n|---------|------|----------|----------|---------|----------------|\n| 1 | [Date] | focus-group-01-[date].m4a | [MM:SS] | [Good/Poor/Issues] | [Location] |\n\n---\n\n### 2. Debrief with Note-Taker (if applicable)\n\n**Discuss:**\n- What were the dominant themes?\n- What surprised us?\n- Who dominated? Who was quiet?\n- Any group dynamics to note?\n- Moments to flag for analysis\n- Technical issues?\n\n**Document this conversation:** [Brief notes]\n\n---\n\n### 3. Reflexivity Notes\n\n**Write in reflexivity journal immediately after session:**\n\n#### Session Context\n- **Date/time:** [When was session held?]\n- **Location:** [Where?]\n- **Attendance:** [N participants attended out of N confirmed]\n- **No-shows:** [Any no-shows? Last-minute cancellations?]\n- **Duration:** [How long was session?]\n- **Technical issues:** [Any recording problems, interruptions?]\n\n#### Group Dynamics\n- **Dominant participants:** [Who spoke most? Did they overshadow others?]\n- **Quiet participants:** [Who spoke least? Why - shy, disengaged, or just listening?]\n- **Disagreements:** [Any conflicts or tensions?]\n- **Consensus:** [Areas where group agreed strongly?]\n- **Energy level:** [Engaged? Tired? Rushed?]\n\n#### Substantive Impressions\n- **What surprised me:** [Anything unexpected?]\n- **What confirmed assumptions:** [Did anything align with my Phase 1 assumptions?]\n- **New questions raised:** [What should I explore in future sessions?]\n- **Memorable quotes:** [Any striking quotes - write verbatim if possible]\n- **Contradictions:** [Did participants contradict each other? Themselves?]\n\n#### Facilitation Reflection\n- **What worked well:** [Which questions/probes yielded rich discussion?]\n- **What didn't work:** [Which questions fell flat or confused people?]\n- **Moments I might have led:** [Did I reveal my opinion? Ask leading questions?]\n- **Discussion guide adjustments:** [What should I change for next session?]\n- **Group management:** [How well did I balance participation? Manage tangents?]\n\n#### Bias Check\n- **My reactions during session:** [Did I show agreement/disagreement inappropriately?]\n- **Assumptions I brought:** [What assumptions influenced my probes?]\n- **What I wanted to hear vs what I heard:** [Confirmation bias check]\n\n---\n\n### 4. Session Log Entry\n\n**Document each session:**\n\n### Session 1\n- **Date:** [YYYY-MM-DD]\n- **Participants:** [N] attended ([N] confirmed)\n- **No-shows:** [Names/IDs if tracking]\n- **Duration:** [MM] minutes\n- **Recording:** `focus-group-01-2025-12-05.m4a`\n- **Note-taker:** [Name or N/A]\n- **Group composition:** [Description - e.g., \"5 managers, 2 individual contributors\"]\n- **Reflexivity notes:** [Link to journal entry]\n- **Status:** [Complete / Issues (describe)]\n- **Saturation check:** [New themes emerged? YES / NO]\n- **Discussion guide version:** [1.0 / 1.1 if revised]\n\n---\n\n### Session 2\n[Same format...]\n\n---\n\n## Transcription Protocol\n\n### Transcription Approach\n\n**Method:** [Manual / Automated service / AI transcription]\n\n**Timeline:**\n- Session 1 transcribed by: [YYYY-MM-DD]\n- Session 2 transcribed by: [YYYY-MM-DD]\n- Session 3 transcribed by: [YYYY-MM-DD]\n\n**Transcript filename:** `transcript-focus-group-[session-number].md`\n\n---\n\n### Transcript Format\n\n```markdown\n# Focus Group Transcript - Session [N]\n\n**Date:** [YYYY-MM-DD]\n**Duration:** [MM] minutes\n**Facilitator:** [Name]\n**Participants:** [N] (using pseudonyms below)\n\n**Participant Key:**\n- P1: [Brief descriptor - role, context, NOT name]\n- P2: [Brief descriptor]\n- P3: [Brief descriptor]\n\n---\n\n**Facilitator:** [Opening statement]\n\n**P1:** [Response verbatim]\n\n**P2:** [Response verbatim]\n\n**P3:** [Builds on P2] I think what [P2] said about [topic] is true, but I've also experienced [addition].\n\n**Facilitator:** [Probe]\n\n**P1:** [Response]\n\n[Continue with full verbatim transcript]\n\n---\n\n**Post-Session Observations:**\n[Note dynamics not captured in audio: body language, who talked over whom, non-verbal agreements, tensions]\n```\n\n---\n\n### Transcription Quality Checklist\n\n**For each transcript, verify:**\n\n- [ ] All speakers identified (using pseudonyms P1, P2, etc.)\n- [ ] Participant key included (brief descriptors, not names)\n- [ ] Verbatim quotes (not summarized)\n- [ ] Inaudible sections marked: [inaudible]\n- [ ] Cross-talk noted: [talking over each other]\n- [ ] Non-verbal cues noted: [laughs], [long pause], [sighs], etc.\n- [ ] Timestamp for key moments (optional but helpful)\n- [ ] Filename follows convention\n- [ ] Saved to `raw-data/transcript-focus-group-[N].md`\n\n---\n\n## Discussion Guide Iteration\n\n**Track changes to discussion guide based on what's working:**\n\n### Version 1.0\n- **Date:** [YYYY-MM-DD]\n- **Used for:** Sessions 1-2\n- **Changes needed:** [List based on reflexivity notes]\n\n### Version 1.1\n- **Date:** [YYYY-MM-DD]\n- **Used for:** Sessions 3-4\n- **Changes from 1.0:**\n  1. [Change 1 - e.g., \"Revised warm-up to be more specific\"]\n  2. [Change 2 - e.g., \"Added probe about X that emerged in Session 1\"]\n  3. [Change 3 - e.g., \"Removed Topic 4 - not yielding insights\"]\n- **Rationale:** [Why these changes?]\n\n---\n\n## Saturation Monitoring\n\n**Purpose:** Track when new sessions stop yielding new insights.\n\n**After each session, assess:**\n\n- Did this session introduce NEW themes not seen before? [YES / NO]\n- Did this session provide NEW examples of existing themes? [YES / NO]\n- Did this session contradict or complicate existing themes? [YES / NO]\n\n**Saturation status:**\n\n| Session | New Themes? | New Examples? | Contradictions? | Saturation Status |\n|---------|------------|---------------|----------------|------------------|\n| 1 | N/A (first) | N/A | N/A | Building baseline |\n| 2 | YES | YES | NO | Not yet |\n| 3 | YES | YES | YES | Not yet |\n| 4 | NO | YES | NO | Approaching |\n| 5 | NO | NO | NO | Achieved |\n\n**Saturation achieved when:** 2-3 consecutive sessions answer NO to all three questions.\n\n**Current status:** [Not yet / Approaching / Achieved]\n\n**Sessions conducted:** [N]\n**Sessions remaining:** [N more scheduled / None - saturation reached]\n\n---\n\n## Managing Challenges\n\n### Low Attendance\n\n**If multiple no-shows reduce group size below 4:**\n\n**Options:**\n1. **Proceed anyway:** Groups of 3-4 can work but dynamics differ (more like group interview)\n2. **Reschedule:** If below 3, consider rescheduling\n3. **Recruit replacements:** Have backup participants on standby\n\n**Document decision:** [What you did and why]\n\n---\n\n### Technical Failures\n\n**If recording fails mid-session:**\n\n1. **Note the time:** When did recording stop?\n2. **Reconstruct:** Facilitator + note-taker write detailed notes immediately after\n3. **Mark in transcript:** [RECORDING FAILED XX:XX - RECONSTRUCTED FROM NOTES]\n\n**Prevention:** Always have backup recording device.\n\n---\n\n### Disruptive Participant\n\n**If someone is disrespectful, dominating, or violating ground rules:**\n\n**During session:**\n1. Gently redirect: \"Let's hear from others on this\"\n2. Reinforce ground rules: \"Remember, one person at a time\"\n3. If continues: Take a break, speak privately\n\n**Severe case:**\n- Thank them for their time\n- Ask them to leave (privately if possible)\n- Continue session with remaining participants\n- Document in reflexivity notes\n\n---\n\n### Topic Goes Dark/Unexpectedly Sensitive\n\n**If conversation becomes emotionally difficult:**\n\n1. **Pause:** \"Let's take a moment\"\n2. **Check-in:** \"Is everyone comfortable continuing with this topic?\"\n3. **Offer exit:** \"We can skip this question if people prefer\"\n4. **Provide resources:** If topic triggers trauma, have resources to share afterward\n\n**Document:** Note in reflexivity notes what happened and how you handled it.\n\n---\n\n## Session Expenses Tracking (if applicable)\n\n| Session | Date | Expense Type | Amount | Notes |\n|---------|------|-------------|--------|-------|\n| 1 | [Date] | Refreshments | $X | |\n| 1 | [Date] | Venue rental | $X | |\n| 1 | [Date] | Incentives (N participants) | $X | |\n| 2 | [Date] | ... | $X | |\n\n**Total expenses to date:** $[X]\n**Budget remaining:** $[X]\n\n---\n\n## Consent Documentation\n\n**Track consent obtained:**\n\n| Session | Participant ID | Consent Method | Consent Form | Opt-out Requests |\n|---------|---------------|----------------|--------------|-----------------|\n| 1 | P01 | Written | âœ“ Signed | None |\n| 1 | P02 | Written | âœ“ Signed | None |\n| 1 | P03 | Verbal (recorded) | N/A | Requested no video recording |\n\n**Any special conditions or opt-outs:** [Document here]\n\n---\n\n## Phase 2 Checkpoint Verification\n\n**Before proceeding to Phase 3, verify:**\n\n- [ ] All planned sessions completed (or saturation achieved)\n- [ ] All sessions recorded successfully\n- [ ] All recordings backed up securely\n- [ ] Post-session reflexivity notes written for EACH session\n- [ ] Session execution log complete\n- [ ] Saturation monitoring documented\n- [ ] All transcripts completed and quality-checked\n- [ ] Participant pseudonyms assigned consistently\n- [ ] Consent obtained and documented for all participants\n- [ ] Discussion guide iterations documented\n- [ ] All files saved to `02-data-collection-log.md`\n- [ ] All recordings saved to `raw-data/recordings/`\n- [ ] All transcripts saved to `raw-data/transcript-focus-group-[N].md`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 3 - Data Familiarization\n**If FAIL:** Complete missing requirements above before proceeding\n\n---\n\n## Common Focus Group Mistakes to Avoid\n\nâŒ **Letting one person dominate**\n- DO: Actively redirect to others\n\nâŒ **Asking yes/no questions**\n- DO: Ask open-ended questions that require explanation\n\nâŒ **Moving on from disagreement too quickly**\n- DO: Explore why people see it differently\n\nâŒ **Revealing your opinion or hypothesis**\n- DO: Stay neutral and curious\n\nâŒ **Accepting vague generalities**\n- DO: Probe for specific examples\n\nâŒ **Trying to create consensus**\n- DO: Welcome and explore divergent views\n\nâŒ **Going through questions mechanically**\n- DO: Follow interesting threads even if not in guide\n\nâŒ **Talking too much**\n- DO: Listen 80%, talk 20%\n\nâœ… **Good facilitation looks like:**\n- Everyone participates (though not equally)\n- Participants talk to each other, not just to facilitator\n- Specific examples and stories shared\n- Disagreements explored respectfully\n- Organic conversation flow while covering key topics\n- Facilitator mostly listening and probing\n",
        "plugins/datapeeker/skills/qualitative-research/templates/interviews/phase-1-interview-guide.md": "# Research Design - Interview Method\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 1 - Research Design\n**Method:** Semi-Structured Interviews\n\n---\n\n## Research Question\n\n**Primary Question:**\n[What are you trying to understand through these interviews?]\n\n**Sub-Questions:**\n1. [Supporting question 1]\n2. [Supporting question 2]\n3. [Supporting question 3]\n\n**Success Criteria:**\n[What would constitute a successful answer to this research question?]\n\n---\n\n## Reflexivity Baseline (MANDATORY)\n\n**Purpose:** Document your assumptions BEFORE data collection to prevent confirmation bias.\n\n### My Assumptions\n\n**What I believe the answer will be:**\n[Write down what you expect to find. Be specific.]\n\n**Why I believe this:**\n[What experience, expert opinion, or prior knowledge shapes this belief?]\n\n**What would surprise me:**\n[What findings would contradict your expectations?]\n\n### My Biases\n\n**Industry/domain experience:**\n[How might your background influence what you notice or interpret?]\n\n**Expert opinions I'm carrying:**\n[What have mentors, consultants, or authorities told you?]\n\n**Prior hypotheses:**\n[What have you already concluded that might bias interpretation?]\n\n**Desired outcome:**\n[What result would you WANT to find? How might this create bias?]\n\n---\n\n## Interview Guide\n\n**Interview Type:** Semi-Structured (recommended for balance of consistency and exploration)\n\n**Duration:** [30-60 minutes typical]\n\n**Format:** [In-person / Video call / Phone]\n\n### Opening (5 minutes)\n\n**Introduction:**\n\"Thank you for taking the time to speak with me today. I'm researching [general topic] to understand [broad goal]. Your insights will help us [purpose]. There are no right or wrong answers - I'm interested in your honest experience.\"\n\n**Consent & Recording:**\n\"This conversation will take about [X] minutes. I'd like to [record / take notes] so I can focus on our conversation. Everything you share will be [confidentiality statement]. Are you comfortable proceeding?\"\n\n**Warm-up:**\n\"To start, can you tell me a bit about [relevant context - their role, company, etc.]?\"\n\n### Core Questions (30-45 minutes)\n\n**CRITICAL:** Use Mom Test principles - ask about PAST BEHAVIOR, not hypothetical futures.\n\n#### Question Set 1: Current State\n\n**Goal:** Understand how they currently handle the situation (not how they'd use your solution)\n\n**Q1:** \"Can you walk me through how you currently [solve this problem / handle this situation]?\"\n- **Follow-up probes:**\n  - \"What does that process look like day-to-day?\"\n  - \"How long have you been doing it this way?\"\n  - \"What made you choose this approach?\"\n\n**Q2:** \"What's working well with your current approach?\"\n- **Follow-up probes:**\n  - \"Can you give me a specific example?\"\n  - \"What would you lose if you had to change this?\"\n\n**Q3:** \"What's frustrating or challenging about your current approach?\"\n- **Follow-up probes:**\n  - \"Can you describe a recent time when this was a problem?\"\n  - \"What did that cost you?\" (time, money, opportunity)\n  - \"How often does this happen?\"\n\n#### Question Set 2: Past Behavior & Attempts\n\n**Goal:** Learn what they've ALREADY tried (reveals actual willingness to change/pay)\n\n**Q4:** \"Have you tried any alternatives or other approaches to this?\"\n- **Follow-up probes:**\n  - \"What did you try?\"\n  - \"What happened?\"\n  - \"Why did you stop using it?\" OR \"Why did you switch?\"\n\n**Q5:** \"When you chose your current solution, what were you comparing it against?\"\n- **Follow-up probes:**\n  - \"What made you choose this one?\"\n  - \"What was the decision process like?\"\n  - \"Who was involved in the decision?\"\n\n#### Question Set 3: Actual Constraints & Trade-offs\n\n**Goal:** Uncover real budget, timeline, decision-making constraints\n\n**Q6:** \"What would make you consider changing your current approach?\"\n- **Follow-up probes:**\n  - \"What would need to be different?\"\n  - \"Has that situation ever happened before?\"\n  - \"What did you do then?\"\n\n**Q7:** \"When you evaluate solutions like this, what matters most to you?\"\n- **Follow-up probes:**\n  - \"Can you rank those by importance?\"\n  - \"Have you ever made a trade-off between [X] and [Y]?\"\n  - \"What did you choose and why?\"\n\n**Q8 (Budget reality check):** \"Roughly what budget do you work with for [this category of solution]?\"\n- **Follow-up probes:**\n  - \"How is that budget determined?\"\n  - \"Do you have flexibility if something is particularly valuable?\"\n  - \"What's the approval process for spending in this area?\"\n\n#### Question Set 4: Context & Specifics\n\n**Goal:** Understand their specific situation and needs\n\n**Q9:** \"Can you describe a recent situation where [the problem] really impacted you?\"\n- **Follow-up probes:**\n  - \"Walk me through what happened step by step\"\n  - \"What was the outcome?\"\n  - \"What would have made that situation better?\"\n\n**Q10:** \"How does [this problem/situation] fit into your broader workflow?\"\n- **Follow-up probes:**\n  - \"What happens before this?\"\n  - \"What happens after?\"\n  - \"Who else is involved?\"\n\n### Closing (5-10 minutes)\n\n**Wrap-up question:**\n\"Is there anything about [topic] that I haven't asked about that you think is important?\"\n\n**Referrals:**\n\"Do you know anyone else who deals with [this problem] who might be willing to share their perspective?\"\n\n**Thank you:**\n\"Thank you so much for your time and insights. This has been really helpful.\"\n\n---\n\n## Question Design Checklist\n\n**For EACH core question, verify:**\n\n- [ ] Asks about PAST behavior, not hypothetical future (\"How do you currently...\" not \"Would you...\")\n- [ ] Open-ended (can't be answered with yes/no)\n- [ ] Neutral (doesn't lead toward desired answer)\n- [ ] Specific (asks for concrete examples, not generalities)\n- [ ] Probes ready (follow-ups to dig deeper)\n\n**Red flags - AVOID:**\n- âŒ \"Would you use a service that...\"\n- âŒ \"Do you think it's important to...\"\n- âŒ \"How much would you pay for...\"\n- âŒ \"What features would you want...\"\n\n**Green flags - USE:**\n- âœ… \"How do you currently...\"\n- âœ… \"Tell me about the last time...\"\n- âœ… \"What have you tried...\"\n- âœ… \"Walk me through your process...\"\n\n---\n\n## Sampling Strategy\n\n**Target Sample Size:** [8-15 interviews recommended for saturation]\n\n**Inclusion Criteria:**\n- [Who should be included? What characteristics must they have?]\n\n**Exclusion Criteria:**\n- [Who should be excluded? What would disqualify someone?]\n\n**Diversity Goals:**\n- [What diversity do you need? Industry? Company size? Role? Geography?]\n\n**Recruitment Approach:**\n\n1. **Method 1:** [e.g., LinkedIn outreach, referrals, existing network]\n   - Expected reach: [N people]\n   - Expected response rate: [X%]\n   - Expected interviews: [N]\n\n2. **Method 2:** [Secondary approach]\n   - Expected reach:\n   - Expected response rate:\n   - Expected interviews:\n\n**Saturation Monitoring:**\n- Plan to conduct [N] interviews initially\n- After interview [N], assess if new insights emerging\n- Continue until 2-3 consecutive interviews add no new themes\n\n---\n\n## Logistics\n\n**Scheduling:**\n- Interview duration: [30-60 minutes]\n- Buffer time: [15 minutes between interviews for notes]\n\n**Recording:**\n- Method: [Audio recording via Zoom/phone/in-person]\n- Backup: [Written notes]\n- Consent: [Verbal consent recorded at start]\n\n**Incentive (if applicable):**\n- [Gift card amount / donation / free service]\n\n**Interview Location/Platform:**\n- [Zoom / Google Meet / In-person / Phone]\n\n---\n\n## Phase 1 Checkpoint Verification\n\n**Before proceeding to Phase 2, verify:**\n\n- [ ] Research question defined and specific\n- [ ] Interview guide created with neutral, open-ended questions\n- [ ] All questions ask about PAST BEHAVIOR (not hypothetical futures)\n- [ ] Follow-up probes prepared for each core question\n- [ ] Sampling strategy documented (who, how many, how to recruit)\n- [ ] **Reflexivity baseline completed** (assumptions and biases documented)\n- [ ] Logistics planned (recording, scheduling, consent)\n- [ ] All content saved to `01-research-design.md`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 2 - Data Collection\n**If FAIL:** Complete missing requirements (especially reflexivity baseline)\n",
        "plugins/datapeeker/skills/qualitative-research/templates/interviews/phase-2-interview-execution.md": "# Data Collection - Interview Execution\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 2 - Data Collection (Interview Method)\n\n---\n\n## Pre-Interview Checklist\n\n**Before EACH interview, verify:**\n\n- [ ] Interview guide printed/accessible\n- [ ] Recording device tested and working\n- [ ] Backup recording method ready (second device or written notes)\n- [ ] Consent form prepared (if written consent required)\n- [ ] Quiet, private location confirmed\n- [ ] Participant contact information confirmed\n- [ ] Reflexivity journal accessible for post-interview notes\n\n---\n\n## Interview Protocol\n\n### Opening (5 minutes)\n\n**Greeting:**\n- Thank participant for their time\n- Introduce yourself briefly\n- Offer water/comfort items if in-person\n\n**Purpose Statement:**\n\"I'm researching [general topic] to understand [broad goal]. Your insights will help us [purpose]. I'm interested in your honest experience - there are no right or wrong answers.\"\n\n**Consent & Recording:**\n\"This conversation will take about [X] minutes. I'd like to [record / take notes] so I can focus on our conversation rather than writing everything down. Everything you share will be [confidentiality statement - anonymous, aggregated, etc.]. The recording will be [stored securely / transcribed and deleted / etc.].\"\n\n**Obtain explicit consent:**\n- [ ] Verbal consent recorded at start of audio\n- OR [ ] Written consent form signed\n\n**If consent refused:** Respect decision. Offer written notes only. If they decline participation, thank them and end interview.\n\n**Warm-up:**\n\"To start, can you tell me a bit about [relevant context]?\"\n- Use this to build rapport\n- Listen for context that will inform later questions\n- Do NOT start with hard questions\n\n---\n\n### Core Interview (30-45 minutes)\n\n**Critical interviewing principles:**\n\n#### 1. Follow the Guide, BUT Stay Flexible\n\n- Use your interview guide as a map, not a script\n- If participant raises something interesting, follow that thread\n- Return to guide questions when natural\n- It's OK to reorder questions based on flow\n\n#### 2. Use Probes Effectively\n\n**When to probe:**\n- Answer is vague or general\n- Participant says \"usually\" or \"sometimes\" (ask for specific example)\n- Interesting point mentioned briefly\n- Answer seems incomplete\n\n**Probing techniques:**\n\n**Echo probe:** Repeat their last few words with questioning tone\n- Participant: \"It was frustrating...\"\n- You: \"Frustrating...?\"\n\n**Silent probe:** Pause and maintain eye contact (3-5 seconds)\n- Most people will fill the silence with more detail\n\n**Specific example probe:**\n- \"Can you give me a specific example of that?\"\n- \"Tell me about the last time that happened\"\n- \"Walk me through what that looked like\"\n\n**Clarification probe:**\n- \"What do you mean by [term they used]?\"\n- \"Can you say more about that?\"\n- \"Help me understand [aspect] better\"\n\n**Contrast probe:**\n- \"How is that different from [other situation]?\"\n- \"What would have made that situation better/worse?\"\n\n#### 3. Avoid Leading Questions\n\n**BAD (leading):**\n- \"So that must have been frustrating, right?\"\n- \"Don't you think that [X] would solve this?\"\n- \"Most people say [Y], do you agree?\"\n\n**GOOD (neutral):**\n- \"How did that feel?\"\n- \"What would have made that better?\"\n- \"What's your experience with [X]?\"\n\n#### 4. Listen More Than You Talk\n\n**Good ratio:** 80% participant talking, 20% you asking/probing\n\n**Red flags you're talking too much:**\n- Explaining your own experiences\n- Offering opinions or advice\n- Finishing their sentences\n- Immediately jumping to next question\n\n#### 5. Handle Tangents\n\n**Participant goes off-topic:**\n- Let them finish their thought (don't interrupt)\n- Acknowledge what they said: \"That makes sense...\"\n- Redirect: \"Going back to what you said about [X]...\"\n\n**Participant asks you questions:**\n- \"That's a great question - I'd love to hear your thoughts on it first\"\n- Defer your opinion until end if necessary\n- Remember: You're here to learn from THEM, not share your views\n\n---\n\n### Managing Challenging Situations\n\n#### Participant Gives Very Short Answers\n\n**Strategies:**\n1. Use silent probes (pause, wait)\n2. Ask for specific examples: \"Can you tell me about a time when...\"\n3. Lower the stakes: \"There's no right answer - I'm just curious about your experience\"\n4. Check-in: \"Am I asking questions in a way that makes sense?\"\n\n#### Participant Rambles or Goes Off-Topic\n\n**Strategies:**\n1. Let them finish (interrupting damages rapport)\n2. Find the relevant thread: \"You mentioned [relevant bit], can we explore that more?\"\n3. Redirect politely: \"That's interesting. I want to make sure we cover [topic] - can we talk about that?\"\n\n#### Participant Seems Uncomfortable or Emotional\n\n**Strategies:**\n1. Pause the interview: \"Would you like to take a break?\"\n2. Remind them they can skip questions: \"We can skip this question if you prefer\"\n3. Offer to stop: \"We can stop here if you'd like\"\n4. Show empathy, but don't pry into emotional topics beyond your research scope\n\n#### Participant Asks \"What Are You Looking For?\"\n\n**Response:**\n\"I'm not looking for any particular answer - I want to understand your genuine experience. Whatever you share is valuable.\"\n\n**Do NOT:**\n- Tell them what you're hoping to find (creates bias)\n- Suggest they're answering \"wrong\"\n- Share your hypothesis\n\n---\n\n### Closing (5-10 minutes)\n\n**Wrap-up question (ALWAYS ask):**\n\"Is there anything about [topic] that I haven't asked about that you think is important?\"\n- This often yields the most valuable insights\n- Participants share what THEY think matters, not what you asked about\n\n**Referrals:**\n\"Do you know anyone else who [deals with this problem / has this experience] who might be willing to share their perspective?\"\n- Record referrals for snowball sampling\n\n**Thank you:**\n\"Thank you so much for your time and insights. This has been really helpful.\"\n\n**Incentive (if applicable):**\n- Provide gift card / compensation as promised\n- Get receipt if required\n\n**Post-Interview:**\n- [ ] Stop recording\n- [ ] Save recording with filename: `interview-[participant-ID]-[date].m4a`\n- [ ] Immediately write reflexivity notes (see below)\n\n---\n\n## Immediate Post-Interview Reflexivity Notes\n\n**TIMING:** Write these within 1 hour of interview while memory is fresh.\n\n**Record in reflexivity journal:**\n\n### Contextual Notes\n- **Setting:** [Where was interview? Any distractions or notable conditions?]\n- **Participant demeanor:** [Engaged? Hesitant? Rushed? Comfortable?]\n- **Duration:** [How long was interview?]\n- **Technical issues:** [Any recording problems? Connection issues if remote?]\n\n### Substantive Impressions\n- **What surprised me:** [Anything unexpected in their responses?]\n- **What confirmed my assumptions:** [Did anything align with my Phase 1 assumptions?]\n- **New questions raised:** [What should I explore in future interviews?]\n- **Quotes to remember:** [Any particularly striking quotes - write verbatim if possible]\n\n### Methodological Notes\n- **Questions that worked well:** [Which questions yielded rich responses?]\n- **Questions that didn't land:** [Which questions confused them or yielded thin responses?]\n- **Probes that worked:** [What follow-ups were effective?]\n- **Interview guide adjustments needed:** [Should I revise questions for future interviews?]\n\n### Bias Check\n- **Moments I might have led the participant:** [Did I ask leading questions? Reveal my opinion?]\n- **My reactions during interview:** [Did I show agreement/disagreement inappropriately?]\n- **Assumptions I noticed:** [What assumptions did I bring to this conversation?]\n\n---\n\n## Interview Execution Log\n\n**Track each interview in this format:**\n\n### Interview 001\n- **Participant ID:** P001\n- **Date:** [YYYY-MM-DD]\n- **Duration:** [MM] minutes\n- **Method:** [In-person / Zoom / Phone]\n- **Recording:** `interview-P001-2025-12-05.m4a`\n- **Reflexivity notes:** [Link to journal entry or brief summary]\n- **Status:** [Complete / Partial (reason) / Failed (reason)]\n- **Saturation check:** [New insights? YES / NO]\n\n---\n\n### Interview 002\n[Same format...]\n\n---\n\n### Interview 003\n[Same format...]\n\n---\n\n## Interview Guide Iteration Tracker\n\n**Purpose:** Document changes to interview guide based on what's working/not working.\n\n### Version 1.0 (Initial)\n- **Date:** [YYYY-MM-DD]\n- **Used for:** Interviews 001-003\n- **Changes needed:** [None yet / List changes to make]\n\n---\n\n### Version 1.1\n- **Date:** [YYYY-MM-DD]\n- **Used for:** Interviews 004-006\n- **Changes from 1.0:**\n  1. [Change 1 - e.g., \"Revised Q3 to be more specific\"]\n  2. [Change 2]\n- **Reason:** [Why were these changes needed?]\n\n---\n\n## Saturation Monitoring\n\n**Purpose:** Track when new interviews stop yielding new insights.\n\n**After Interview [N], ask:**\n- Did this interview introduce NEW themes/patterns not seen before? [YES / NO]\n- Did this interview provide NEW examples of existing themes? [YES / NO]\n- Did this interview contradict or complicate existing themes? [YES / NO]\n\n**Saturation achieved when:**\n- 2-3 consecutive interviews answer NO to all three questions\n- All themes are well-supported with multiple examples\n- Negative cases have been found and understood\n\n**Current saturation status:** [Not yet / Approaching / Achieved]\n\n**Interviews conducted:** [N]\n**Estimated interviews remaining:** [N]\n\n---\n\n## Recording & Data Management\n\n### Recording Conventions\n\n**Filename format:** `interview-[participant-ID]-[YYYY-MM-DD].[extension]`\n- Example: `interview-P001-2025-12-05.m4a`\n\n**Storage location:** `analysis/qualitative-research/[session-name]/raw-data/recordings/`\n\n**Backup:** [Where are recordings backed up? Cloud? External drive?]\n\n### Transcription Protocol\n\n**Method:** [Manual / Automated service / AI transcription]\n\n**Transcript filename:** `transcript-[participant-ID].md`\n\n**Transcript format:**\n```markdown\n# Interview Transcript - P001\n\n**Date:** [YYYY-MM-DD]\n**Duration:** [MM] minutes\n**Interviewer:** [Name]\n**Participant:** P001 [demographic info if relevant: role, company size, etc.]\n\n---\n\n**Interviewer:** [First question]\n\n**P001:** [Response verbatim]\n\n**Interviewer:** [Follow-up probe]\n\n**P001:** [Response]\n\n[Continue with full verbatim transcript]\n\n---\n\n**Post-Interview Notes:**\n[Any context not captured in audio - body language, interruptions, etc.]\n```\n\n**Transcription timeline:**\n- [ ] Interview 001 transcribed by [date]\n- [ ] Interview 002 transcribed by [date]\n- [ ] Interview 003 transcribed by [date]\n\n**Quality check:**\n- [ ] All transcripts reviewed for accuracy\n- [ ] Speaker labels verified\n- [ ] Inaudible sections marked [inaudible]\n- [ ] Non-verbal cues noted [laughs], [long pause], etc.\n\n---\n\n## Consent & Ethics Tracking\n\n**Consent method:** [Verbal recorded / Written signature / Email confirmation]\n\n**Participant rights explained:**\n- [ ] Right to withdraw at any time\n- [ ] Right to skip questions\n- [ ] Right to review transcript (if applicable)\n- [ ] Confidentiality protections\n- [ ] How data will be used\n- [ ] Data retention and deletion timeline\n\n**Consent log:**\n\n| Participant ID | Date | Consent Method | Consent Obtained | Notes |\n|---------------|------|----------------|-----------------|-------|\n| P001 | [Date] | Verbal recorded | âœ“ | [Any special conditions] |\n| P002 | [Date] | Written | âœ“ | |\n| P003 | [Date] | Verbal recorded | âœ“ | |\n\n---\n\n## Phase 2 Checkpoint Verification\n\n**Before proceeding to Phase 3, verify:**\n\n- [ ] All planned interviews completed (or saturation achieved)\n- [ ] All interviews recorded successfully\n- [ ] All recordings backed up securely\n- [ ] Post-interview reflexivity notes written for EACH interview\n- [ ] Interview execution log complete\n- [ ] Saturation monitoring documented\n- [ ] All transcripts completed and quality-checked\n- [ ] Consent obtained and documented for all participants\n- [ ] Interview guide iterations documented\n- [ ] All files saved to `02-data-collection-log.md`\n- [ ] All recordings saved to `raw-data/recordings/`\n- [ ] All transcripts saved to `raw-data/transcript-[ID].md`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 3 - Data Familiarization\n**If FAIL:** Complete missing requirements above before proceeding\n\n---\n\n## Common Interview Mistakes to Avoid\n\nâŒ **Jumping to solutions**\n- Don't ask \"Would you use X?\"\n- DO ask \"How do you currently solve this?\"\n\nâŒ **Asking hypothetical questions**\n- Don't ask \"What would you do if...\"\n- DO ask \"Tell me about the last time you...\"\n\nâŒ **Sharing your opinions**\n- Don't say \"I think X is important, do you?\"\n- DO ask \"What matters most to you about X?\"\n\nâŒ **Accepting vague answers**\n- Don't move on when they say \"usually\" or \"sometimes\"\n- DO probe: \"Can you give me a specific example?\"\n\nâŒ **Interrupting**\n- Don't cut them off to ask next question\n- DO let them finish, then probe for more\n\nâŒ **Asking double-barreled questions**\n- Don't ask \"How do you X and what do you think about Y?\"\n- DO ask one question at a time\n\nâŒ **Revealing your hypothesis**\n- Don't tell them what you're hoping to find\n- DO stay neutral about what you're investigating\n\nâœ… **Good interviewing looks like:**\n- Mostly listening (80/20 ratio)\n- Asking for specific examples constantly\n- Comfortable with silence\n- Following interesting threads even if not in guide\n- Probing vague answers\n- Staying neutral and curious\n",
        "plugins/datapeeker/skills/qualitative-research/templates/observations/phase-1-observation-protocol.md": "# Research Design - Observation Method\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 1 - Research Design\n**Method:** Observational Research\n\n---\n\n## Research Question\n\n**Primary Question:**\n[What are you trying to understand through observation?]\n\n**Sub-Questions:**\n1. [Supporting question 1]\n2. [Supporting question 2]\n3. [Supporting question 3]\n\n**Success Criteria:**\n[What would constitute a successful answer to this research question?]\n\n---\n\n## Reflexivity Baseline (MANDATORY)\n\n**Purpose:** Document your assumptions BEFORE data collection to prevent confirmation bias.\n\n### My Assumptions\n\n**What I believe I'll observe:**\n[Write down what you expect to see. Be specific.]\n\n**Why I believe this:**\n[What experience, expert opinion, or prior knowledge shapes this belief?]\n\n**What would surprise me:**\n[What observations would contradict your expectations?]\n\n### My Biases\n\n**Industry/domain experience:**\n[How might your background influence what you notice or interpret?]\n\n**Expert opinions I'm carrying:**\n[What have mentors, consultants, or authorities told you?]\n\n**Prior hypotheses:**\n[What have you already concluded that might bias what you see?]\n\n**Desired outcome:**\n[What would you WANT to observe? How might this create bias?]\n\n---\n\n## Observation Strategy\n\n**Why observation instead of asking?**\n[Observation reveals actual behavior vs reported behavior, captures context, uncovers tacit knowledge]\n\n**Observation type:**\n- [ ] **Participant observation:** Researcher participates in activities while observing\n- [ ] **Non-participant observation:** Researcher observes without participating\n- [ ] **Structured observation:** Using predefined categories/checklist\n- [ ] **Unstructured observation:** Open-ended, exploratory observation\n\n**Rationale:** [Why this approach for your research question?]\n\n**Observer role disclosure:**\n- [ ] **Overt:** Participants know they're being observed\n- [ ] **Covert:** Participants don't know (requires ethical justification)\n\n**Selection:** [Which and why?]\n\n---\n\n## What to Observe\n\n### Observation Focus Areas\n\n**CRITICAL:** Don't just observe everything - you need focus while staying open to unexpected.\n\n#### 1. Physical Setting\n- **Space layout:** [How is space organized? What does layout enable/constrain?]\n- **Objects/artifacts:** [What tools, technology, materials are present?]\n- **Environment:** [Lighting, noise, temperature, aesthetics - how might these affect behavior?]\n\n**Why this matters for research question:** [Connection to your question]\n\n---\n\n#### 2. Actors/People\n- **Who is present:** [Roles, demographics, number of people]\n- **Who is absent:** [Who would you expect to see but don't?]\n- **Relationships:** [Hierarchies, power dynamics, formal/informal relationships]\n\n**Why this matters for research question:** [Connection to your question]\n\n---\n\n#### 3. Activities/Actions\n- **Tasks being performed:** [What are people doing? Step-by-step processes]\n- **Sequence:** [In what order? What triggers transitions?]\n- **Tools/methods used:** [How are they doing it? What do they use?]\n- **Duration:** [How long do activities take?]\n- **Frequency:** [How often do activities occur?]\n\n**Why this matters for research question:** [Connection to your question]\n\n---\n\n#### 4. Interactions\n- **Who talks to whom:** [Communication patterns, formal/informal]\n- **Communication modes:** [Verbal, written, gestural, digital]\n- **Tone/affect:** [Formal, casual, tense, friendly]\n- **Turn-taking:** [Who initiates? Who responds? Who's silent?]\n\n**Why this matters for research question:** [Connection to your question]\n\n---\n\n#### 5. Events\n- **Routines:** [Predictable, recurring events]\n- **Exceptional events:** [Unusual, one-off occurrences]\n- **Triggers:** [What causes events to start/stop?]\n- **Responses:** [How do people respond to events?]\n\n**Why this matters for research question:** [Connection to your question]\n\n---\n\n#### 6. Time\n- **Temporal patterns:** [Morning vs afternoon, Monday vs Friday, seasonal]\n- **Pacing:** [Rush periods, slow periods, breaks]\n- **Duration:** [How long things take]\n\n**Why this matters for research question:** [Connection to your question]\n\n---\n\n#### 7. Goals/Purposes\n- **Stated purposes:** [What people say they're trying to accomplish]\n- **Observed purposes:** [What appears to be actual goal from behavior]\n- **Conflicts:** [Competing goals, misaligned purposes]\n\n**Why this matters for research question:** [Connection to your question]\n\n---\n\n#### 8. Feelings/Affect\n- **Expressed emotions:** [What emotions are visible? Body language, tone, expressions]\n- **Intensity:** [Subtle or obvious?]\n- **Shifts:** [When do emotions change? What triggers this?]\n\n**Note:** Be cautious interpreting emotions - verify when possible.\n\n**Why this matters for research question:** [Connection to your question]\n\n---\n\n## Observation Protocol\n\n### Field Note Structure\n\n**Two-column approach (recommended):**\n\n| Descriptive Notes | Reflective Notes |\n|------------------|------------------|\n| **WHAT I OBSERVE** | **WHAT I THINK/FEEL** |\n| Concrete, factual, detailed | Interpretations, questions, reactions |\n| Who, what, when, where, how | Emerging patterns, confusions, biases |\n\n**Example:**\n\n| Descriptive Notes | Reflective Notes |\n|------------------|------------------|\n| \"9:05am - Manager A enters room, doesn't greet the 3 team members already present, sits at head of table, opens laptop. Team members exchange glances but say nothing.\" | \"Power dynamic? Why no greeting? Team members seem uncomfortable - is this normal? I'm surprised by formality - expected more casual interaction based on company culture claims.\" |\n\n---\n\n### Level of Description\n\n**Aim for \"thick description\":**\n\nâŒ **Thin:** \"The meeting was tense.\"\nâœ… **Thick:** \"Manager A spoke in clipped sentences, made no eye contact. Two team members sat with arms crossed. When questioned, responses were one word. Long silences between exchanges (10-15 seconds). One team member checked phone repeatedly.\"\n\n**Thick description includes:**\n- Specific behaviors (not interpretations)\n- Direct quotes (verbatim when possible)\n- Physical details (posture, positioning, environment)\n- Sequence of events\n- Duration and timing\n\n---\n\n### Note-Taking Method\n\n**During observation:**\n- [ ] **Jottings:** Quick keywords, fragments to trigger memory\n- [ ] **Full field notes:** Complete sentences written during observation\n- [ ] **Audio recording:** Verbal notes into recorder (requires consent)\n- [ ] **Photography/video:** Visual documentation (requires consent)\n\n**Selected method:** [Which will you use? Why?]\n\n**Immediately after observation (within 2 hours):**\n- [ ] **Expanded field notes:** Convert jottings into full detailed notes\n- [ ] **Reflective memo:** Write about patterns, questions, interpretations\n\n**CRITICAL:** Jottings MUST be expanded same day or details are lost.\n\n---\n\n## Sampling Strategy\n\n### Site Selection\n\n**Where will you observe:**\n- **Site 1:** [Location/setting]\n  - **Rationale:** [Why this site?]\n  - **Access:** [How will you gain entry?]\n  - **Duration:** [How long will you observe here?]\n\n- **Site 2:** [Location/setting]\n  - **Rationale:** [Why this site?]\n  - **Access:** [How will you gain entry?]\n  - **Duration:** [How long will you observe here?]\n\n**Diversity goals:**\n[What variation do you need? Different times? Locations? Situations?]\n\n---\n\n### Temporal Sampling\n\n**When will you observe:**\n\n**Time of day:**\n- [ ] Morning\n- [ ] Afternoon\n- [ ] Evening\n- [ ] Multiple times (to capture temporal variation)\n\n**Day of week:**\n- [ ] Weekday\n- [ ] Weekend\n- [ ] Multiple days\n\n**Duration per session:**\n[30 min / 2 hours / Full day / Extended immersion over days/weeks?]\n\n**Rationale:** [Why these times?]\n\n---\n\n### What to Sample\n\n**Sampling approach:**\n- [ ] **Continuous:** Observe everything during defined time period\n- [ ] **Event sampling:** Observe only when specific event occurs\n- [ ] **Time sampling:** Observe at set intervals (e.g., every hour on the hour)\n\n**Selected approach:** [Which and why?]\n\n---\n\n## Access & Ethics\n\n### Gaining Access\n\n**Gatekeepers to contact:**\n- [Who controls access to observation site?]\n- [How will you approach them?]\n\n**Access request template:**\n\n\"Dear [Gatekeeper],\n\nI'm conducting research on [topic] to understand [goal]. I would like to observe [setting/activities] to gain insight into [what you hope to learn].\n\nThe observation would involve:\n- [Duration] on [dates]\n- [My role - participant/non-participant]\n- [Note-taking / recording methods]\n- [What I'll do with the data]\n\n[If participants will know: I will explain my research to participants and obtain their consent.]\n\n[If participants won't know: I request permission to observe without disclosing research purpose because [ethical justification].]\n\nWould you be willing to discuss this further?\n\nThank you,\n[Your name]\"\n\n---\n\n### Informed Consent\n\n**Overt observation consent approach:**\n\n**Before observation begins:**\n1. **Explain research:** \"I'm studying [topic] to understand [goal]\"\n2. **Explain process:** \"I'll be observing [what] and taking notes on [what specifically]\"\n3. **Explain confidentiality:** \"I won't use names. Notes will be [how stored/who has access]\"\n4. **Answer questions:** \"Do you have any questions?\"\n5. **Obtain consent:** \"Are you comfortable with me observing?\"\n\n**Consent log:**\n\n| Date | Location | Person/Group | Consent Obtained | Notes |\n|------|----------|-------------|------------------|-------|\n| [Date] | [Site] | [Who] | âœ“ Yes / No | [Special conditions?] |\n\n---\n\n**Covert observation ethical justification (if applicable):**\n\n[If you plan covert observation, you MUST justify why disclosure would compromise research and why benefits outweigh risks. Most research requires overt observation.]\n\n---\n\n### Managing Observer Effect\n\n**Observer effect:** People change behavior when they know they're observed.\n\n**Strategies to minimize:**\n1. **Prolonged engagement:** Spend time on-site before formal observation so people acclimate\n2. **Blend in:** Dress appropriately for setting, adopt similar body language\n3. **Be unobtrusive:** Position yourself peripherally, avoid eye contact that invites interaction\n4. **Build rapport:** If participant observation, engage naturally without being intrusive\n\n**Document in reflexivity notes:** When/how you think your presence affected behavior.\n\n---\n\n## Field Note Template\n\n**Use this structure for each observation session:**\n\n```markdown\n# Field Notes - Session [N]\n\n**Date:** [YYYY-MM-DD]\n**Time:** [HH:MM - HH:MM]\n**Location:** [Where]\n**Duration:** [Hours/minutes]\n**Observer:** [Your name/ID]\n\n---\n\n## Session Context\n\n**Purpose of this session:** [What were you trying to observe?]\n\n**Pre-observation prep:**\n- Arrived: [Time]\n- Introduced self to: [Who]\n- Positioned: [Where I sat/stood]\n- Note-taking method: [Jottings / Full notes / Audio / etc.]\n\n**Environmental conditions:**\n[Lighting, noise, temperature, busy/quiet, any unusual conditions]\n\n---\n\n## Descriptive Observations\n\n[Two-column format preferred:]\n\n| TIME | DESCRIPTIVE NOTES | REFLECTIVE NOTES |\n|------|------------------|------------------|\n| 9:00am | [What I observed - thick description] | [What I thought/questioned/felt] |\n| 9:15am | | |\n| 9:30am | | |\n\n**Alternative format (if not using table):**\n\n**TIME:** 9:00am\n\n**DESCRIPTIVE:** [Thick description of what happened]\n\n**REFLECTIVE:** [Interpretations, questions, patterns]\n\n---\n\n**TIME:** 9:15am\n\n[Continue...]\n\n---\n\n## Post-Observation Summary\n\n**Key observations:**\n1. [Most important thing observed]\n2. [Second most important]\n3. [Third most important]\n\n**Surprises:**\n[Anything unexpected that contradicted assumptions?]\n\n**Patterns emerging:**\n[Any recurring behaviors, interactions, events?]\n\n**Questions raised:**\n[What do I need to explore further? Clarify? Follow up on?]\n\n**Reflexivity check:**\n[How might my presence have affected what I observed?]\n[What assumptions did I bring that influenced what I noticed?]\n\n---\n\n## Follow-Up Actions\n\n- [ ] [Action 1 - e.g., \"Ask about X in informal conversation\"]\n- [ ] [Action 2 - e.g., \"Return at different time to see if pattern holds\"]\n- [ ] [Action 3]\n\n```\n\n---\n\n## Observation Schedule\n\n**Plan observation sessions:**\n\n### Session 1\n- **Date:** [YYYY-MM-DD]\n- **Time:** [HH:MM - HH:MM]\n- **Location:** [Where]\n- **Focus:** [What specifically will you observe?]\n- **Access arranged:** [Yes / No - pending]\n\n### Session 2\n- **Date:** [YYYY-MM-DD]\n- **Time:** [HH:MM - HH:MM]\n- **Location:** [Where]\n- **Focus:** [What specifically will you observe?]\n- **Access arranged:** [Yes / No - pending]\n\n### Session 3\n[Continue...]\n\n**Total planned observation hours:** [N hours across M sessions]\n\n---\n\n## Equipment & Logistics\n\n**Materials needed:**\n- [ ] Notebook and pens (multiple - backups)\n- [ ] Audio recorder (if using - with consent)\n- [ ] Camera (if using - with consent)\n- [ ] Laptop/tablet for notes (if appropriate for setting)\n- [ ] Watch or timer\n- [ ] Contact information for gatekeeper\n\n**Clothing/appearance:**\n[What will you wear to blend in appropriately?]\n\n**Transportation/access:**\n[How will you get to site? Parking? Building access?]\n\n---\n\n## Phase 1 Checkpoint Verification\n\n**Before proceeding to Phase 2, verify:**\n\n- [ ] Research question defined and specific\n- [ ] Observation focus areas identified (what to observe and why)\n- [ ] Observation type selected (participant/non-participant, structured/unstructured)\n- [ ] Observer role disclosure decided (overt/covert with justification)\n- [ ] Field note structure planned (thick description, two-column format)\n- [ ] Sampling strategy defined (sites, times, duration)\n- [ ] Access arranged with gatekeepers\n- [ ] Consent process defined\n- [ ] Observation schedule created with specific sessions\n- [ ] Equipment and logistics planned\n- [ ] **Reflexivity baseline completed** (assumptions and biases documented)\n- [ ] Field note template ready to use\n- [ ] All content saved to `01-research-design.md`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 2 - Field Work Execution\n**If FAIL:** Complete missing requirements (especially access arrangements and reflexivity baseline)\n\n---\n\n## Observation Types Reference\n\n### Participant Observation\n**Definition:** Researcher actively participates while observing\n**Pros:** Access to insider perspective, understand tacit knowledge\n**Cons:** Harder to observe while doing, may \"go native\" and lose objectivity\n**Example:** Working alongside employees to understand their process\n\n### Non-Participant Observation\n**Definition:** Researcher observes without participating\n**Pros:** Can focus entirely on observing, maintains distance\n**Cons:** May be seen as outsider, miss tacit knowledge\n**Example:** Sitting in corner of workspace taking notes on interactions\n\n### Structured Observation\n**Definition:** Using predefined categories or checklist\n**Pros:** Systematic, consistent, comparable across sessions\n**Cons:** May miss unexpected insights, rigid\n**Example:** Counting frequency of specific behaviors\n\n### Unstructured Observation\n**Definition:** Open-ended, exploratory observation\n**Pros:** Flexible, captures unexpected patterns\n**Cons:** Can be overwhelming, harder to compare across sessions\n**Example:** General field notes on everything happening\n\n**Most qualitative research uses:** Non-participant, unstructured (or semi-structured) observation with thick description.\n",
        "plugins/datapeeker/skills/qualitative-research/templates/observations/phase-2-field-work.md": "# Data Collection - Observational Field Work\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 2 - Data Collection (Observation Method)\n\n---\n\n## Pre-Field Work Checklist\n\n**Before EACH observation session:**\n\n- [ ] Field note template accessible (digital or paper notebook)\n- [ ] Multiple pens (backups in case one runs out)\n- [ ] Audio recorder tested (if using - with consent obtained)\n- [ ] Camera ready (if using - with consent obtained)\n- [ ] Watch or timer to track time\n- [ ] Observation protocol reviewed (what to focus on)\n- [ ] Contact information for gatekeeper/host\n- [ ] Appropriate clothing (blend in with setting)\n- [ ] Transportation/access arranged\n\n---\n\n## Field Work Execution Protocol\n\n### Arrival & Positioning\n\n**Upon arrival:**\n\n1. **Check in with gatekeeper/host**\n   - Remind them of your purpose\n   - Confirm duration of observation\n   - Ask about any changes or special events happening today\n\n2. **Position yourself strategically**\n   - **If non-participant:** Where can you observe without being intrusive?\n     - Corner with good sight lines\n     - Position where you can see but aren't in the way\n   - **If participant:** Where should you be to participate while observing?\n     - Alongside workers/participants\n     - Engaged but maintaining observer awareness\n\n3. **Initial orientation (first 5-10 minutes)**\n   - Get a sense of the space\n   - Note who's present, the environment, activities happening\n   - Let people acclimate to your presence before detailed note-taking\n\n---\n\n### Note-Taking During Observation\n\n**Two-phase approach:**\n\n#### Phase 1: Jottings (during observation)\n\n**If full notes would be disruptive:**\n- Quick keywords and phrases\n- Time stamps\n- Key quotes verbatim (in quotes)\n- Diagrams/sketches of space or positioning\n\n**Example jottings:**\n```\n9:05 - Mgr enters, no greeting, sits head table\n9:12 - \"We need to hit deadline\" (Mgr A)\n9:15 - Long silence, ~20 sec, team looks at each other\n```\n\n#### Phase 2: Expanded Field Notes (immediately after - within 2 hours)\n\n**CRITICAL:** Jottings MUST be expanded same day or you'll forget details.\n\n**Expand jottings into:**\n- Complete sentences\n- Thick description (specific behaviors, not interpretations)\n- Reflective notes (interpretations, questions, patterns)\n- Direct quotes expanded with context\n\n---\n\n### What to Record\n\n**For each significant moment:**\n\n1. **TIME:** When did it happen?\n2. **WHO:** Who was involved? Use consistent pseudonyms or roles\n3. **WHAT:** What happened? Thick description - specific behaviors, not interpretations\n4. **WHERE:** Location/positioning - where were people? How were they arranged?\n5. **HOW:** How did it happen? Tone, manner, sequence\n6. **CONTEXT:** What was happening before/after? What might have triggered this?\n\n---\n\n### Thick Description Guidelines\n\n**Aim for descriptive, not interpretive:**\n\nâŒ **Interpretive (avoid):**\n\"The manager was angry.\"\n\nâœ… **Descriptive (use):**\n\"The manager's voice volume increased, face reddened, fist hit table once. Said 'This is unacceptable' in clipped tone.\"\n\n---\n\nâŒ **Interpretive:**\n\"The team was disengaged.\"\n\nâœ… **Descriptive:**\n\"Three of five team members had laptops open but screens showed email/browsing, not meeting materials. Two team members checked phones 3+ times in 10 minutes. When asked questions, responses were one word with no elaboration.\"\n\n---\n\n**Include:**\n- **Verbatim quotes:** Put in \"quotation marks\" exactly as said\n- **Physical details:** Posture, positioning, gestures, facial expressions\n- **Environmental details:** Noise, lighting, space configuration\n- **Timing:** Duration of events, pauses, pacing\n- **Sequence:** What happened first, next, then?\n\n---\n\n## Field Note Format\n\n**Use two-column format for real-time pattern recognition:**\n\n### Session [N] - [YYYY-MM-DD]\n\n**Location:** [Where]\n**Time:** [HH:MM - HH:MM]\n**Duration:** [Total time]\n**Weather/Environment:** [Conditions that might affect behavior]\n\n---\n\n| TIME | DESCRIPTIVE NOTES (What I observe) | REFLECTIVE NOTES (What I think/question) |\n|------|-----------------------------------|----------------------------------------|\n| 9:00am | **Setting:** Room 12x15, fluorescent lights, 6-person table. Whiteboard on north wall with \"Q3 Goals\" written. Coffee maker in corner, half-full pot.<br><br>**People present:** 5 people (3 women, 2 men). All appeared 30-50 years old, business casual dress. Seated around table. | First impression: Corporate office, typical meeting room. Coffee suggests longer meeting planned.<br><br>Question: Why only 5? Was expecting 6 based on gatekeeper comment. |\n| 9:05am | **Event:** Person A (woman, ~40s, wearing blue blazer) enters room. Does not greet others. Sits at head of table (north end). Opens laptop, begins typing. Other 4 people already present exchange brief glances - eye contact with each other, then look down at their own materials.<br><br>**No verbal interaction.** Silence lasted ~30 seconds. | **Pattern?** Power dynamic evident. A seems to be manager/leader based on seating and others' reactions.<br><br>**Surprise:** Expected verbal greeting. Lack of greeting seems intentional? Or normal here?<br><br>**Check assumption:** I expected more casual/friendly culture based on company's public image. |\n| 9:07am | **A speaks:** \"Let's get started. We need to hit the deadline.\" Voice was flat affect, no pleasantries.<br><br>**Person B responds:** \"We're on track for the main deliverables.\" Looked at laptop while speaking, minimal eye contact with A. | **Language note:** \"Need to\" not \"want to\" - obligation framing.<br><br>**Body language:** B avoiding eye contact - discomfort? Norm here? |\n\n[Continue for duration of observation session...]\n\n---\n\n## Managing Common Challenges\n\n### Challenge 1: Information Overload\n\n**Problem:** Too much happening, can't record everything\n\n**Solutions:**\n1. **Focus:** Return to your observation protocol - what MATTERS for research question?\n2. **Prioritize:** Record events/interactions directly relevant to research question first\n3. **Sample:** If continuous observation is overwhelming, switch to time sampling (observe for 5 min every 15 min)\n4. **Accept limitations:** You can't capture everything - that's OK\n\n**Document:** Note in reflective column what you're NOT capturing and why.\n\n---\n\n### Challenge 2: Observer Effect\n\n**Problem:** People are acting differently because you're watching\n\n**Indicators:**\n- People keep looking at you\n- Activities seem \"performed\" or staged\n- People explain what they're doing unprompted\n- Behavior seems unusually formal or careful\n\n**Solutions:**\n1. **Prolonged engagement:** Spend more time on-site so people acclimate\n2. **Be unobtrusive:** Position yourself peripherally, avoid excessive eye contact\n3. **Build rapport:** If participant observation, engage naturally\n4. **Time:** Often effect diminishes after 15-20 minutes\n\n**Document:** Note in reflexivity section when/how you think your presence affected behavior.\n\n---\n\n### Challenge 3: Going Native (Participant Observation)\n\n**Problem:** You're so immersed in participation that you forget to observe\n\n**Solutions:**\n1. **Scheduled breaks:** Every 30 min, step aside to write jottings\n2. **Mental flagging:** Note to self \"That's interesting\" in the moment, expand later\n3. **Balance:** Participate enough to understand, observe enough to maintain analytical distance\n\n**Document:** Reflect on your dual role - when did participation enhance understanding? When did it hinder observation?\n\n---\n\n### Challenge 4: Unexpected Events\n\n**Problem:** Something surprising or significant happens that you weren't prepared for\n\n**Response:**\n1. **Capture as much as possible:** Even if it's not in your observation protocol\n2. **Ask clarifying questions later:** \"I noticed X happened - is that typical?\"\n3. **Return focus:** After capturing unexpected event, return to protocol\n4. **Reflect:** Why was this surprising? Does it contradict assumptions?\n\n**Document:** Unexpected events often reveal the most - give them detailed attention in field notes.\n\n---\n\n### Challenge 5: Emotional Response\n\n**Problem:** You witness something upsetting, frustrating, or that triggers strong emotion\n\n**Response:**\n1. **Acknowledge:** Note your emotional response in reflective column\n2. **Continue observing:** Don't let emotion stop observation (unless ethically necessary)\n3. **Document bias:** How might this emotional response affect your interpretation?\n4. **Debrief:** Process emotional reactions after observation session\n\n**When to intervene:** If you witness harm, abuse, or unethical behavior, you have ethical obligation to intervene even if it interrupts research.\n\n---\n\n## Post-Session Protocol\n\n**IMMEDIATELY after observation session (within 1-2 hours):**\n\n### 1. Expand Jottings to Full Field Notes\n\n- [ ] Expand keywords into complete sentences\n- [ ] Add thick description details while memory is fresh\n- [ ] Fill in gaps - what did you see but not write down in the moment?\n- [ ] Expand abbreviated quotes to full verbatim\n- [ ] Add environmental details\n\n**Time commitment:** Usually 2-3x observation duration (e.g., 1 hour observation = 2-3 hours writing)\n\n**Save as:** `field-notes-session-[N]-[YYYY-MM-DD].md`\n\n---\n\n### 2. Write Reflective Memo\n\n**Separate from field notes - this is your analytical thinking:**\n\n**Memo template:**\n\n```markdown\n# Reflective Memo - Session [N]\n\n**Date:** [YYYY-MM-DD]\n\n## Key Observations\n\n**Most important thing I observed:**\n[What stood out as most significant?]\n\n**Why this matters for research question:**\n[Connection to your main question]\n\n---\n\n## Patterns Emerging\n\n**Patterns I'm starting to see (across multiple sessions if applicable):**\n1. [Pattern 1 with examples]\n2. [Pattern 2 with examples]\n\n**How confident am I in these patterns?**\n[Preliminary? Strong? Need more observation to confirm?]\n\n---\n\n## Surprises & Contradictions\n\n**What surprised me:**\n[Anything unexpected?]\n\n**How does this compare to my Phase 1 assumptions:**\n[Confirms or contradicts?]\n\n**Contradictions observed:**\n[Did I see behaviors that contradict each other? People saying one thing but doing another?]\n\n---\n\n## Questions Raised\n\n**Questions for follow-up:**\n1. [Question to explore in next session]\n2. [Question to ask informant]\n3. [Question for further analysis]\n\n**Concepts to investigate:**\n[Terms, jargon, processes that I need to understand better]\n\n---\n\n## Reflexivity Check\n\n**How did my presence affect what I observed:**\n[Observer effect assessment]\n\n**Assumptions I brought to this session:**\n[What lens did I apply? How might this have biased what I noticed?]\n\n**My emotional responses:**\n[What did I feel during observation? How might this affect interpretation?]\n\n---\n\n## Next Steps\n\n- [ ] [Action 1 - e.g., \"Return to observe morning shift\"]\n- [ ] [Action 2 - e.g., \"Ask about terminology X\"]\n- [ ] [Action 3 - e.g., \"Observe different role/location\"]\n```\n\n**Save as:** `memo-session-[N]-[YYYY-MM-DD].md`\n\n---\n\n### 3. Observation Log Entry\n\n**Track each session:**\n\n### Session 1\n- **Date:** [YYYY-MM-DD]\n- **Time:** [HH:MM - HH:MM]\n- **Duration:** [Hours:Minutes]\n- **Location:** [Where]\n- **Focus:** [What you were observing]\n- **Participants observed:** [N people / Roles present]\n- **Environmental conditions:** [Busy/quiet, any unusual events]\n- **Field notes:** `field-notes-session-01-[date].md`\n- **Reflective memo:** `memo-session-01-[date].md`\n- **Status:** [Complete / Partial (reason)]\n- **Saturation check:** [New insights? YES / NO]\n- **Observer effect:** [High / Medium / Low - your assessment]\n\n---\n\n### Session 2\n[Same format...]\n\n---\n\n## Saturation Monitoring\n\n**After each session, assess:**\n\n- Did this session reveal NEW behaviors/patterns not seen before? [YES / NO]\n- Did this session provide NEW examples of existing patterns? [YES / NO]\n- Did this session contradict or complicate existing patterns? [YES / NO]\n\n**Saturation status:**\n\n| Session | New Patterns? | New Examples? | Contradictions? | Saturation Status |\n|---------|--------------|---------------|----------------|------------------|\n| 1 | N/A (first) | N/A | N/A | Building baseline |\n| 2 | YES | YES | NO | Not yet |\n| 3 | YES | YES | YES | Not yet |\n| 4 | NO | YES | NO | Approaching |\n| 5 | NO | NO | NO | Achieved |\n\n**Saturation achieved when:** 2-3 consecutive sessions answer NO to all three questions.\n\n**Current status:** [Not yet / Approaching / Achieved]\n\n---\n\n## Member Checking (Optional but Recommended)\n\n**Purpose:** Verify your observations with participants to enhance credibility.\n\n**Approach:**\n1. **Share field notes or preliminary interpretations** with key informant(s)\n2. **Ask:** \"Does this accurately represent what happened? What am I missing?\"\n3. **Document:** Their feedback, corrections, additional context\n\n**Member check log:**\n\n| Date | Informant | Material Shared | Feedback Received | Changes Made |\n|------|-----------|----------------|-------------------|--------------|\n| [Date] | [Pseudonym/role] | Session 3 field notes | [Summary of feedback] | [What you revised] |\n\n**Note:** Member checking can reveal your blind spots OR may introduce bias if participants try to present themselves favorably. Use judiciously.\n\n---\n\n## Informal Conversations & Opportunistic Interviews\n\n**During field work, spontaneous conversations often provide valuable context:**\n\n**When someone explains something unprompted or you ask a quick clarifying question:**\n\n**Document immediately:**\n\n### Informal Conversation - [Date, Time]\n\n**Person:** [Pseudonym/role]\n**Context:** [Where, what prompted conversation]\n\n**What they said (verbatim when possible):**\n> \"[Quote]\"\n\n**My question/prompt:**\n\"[What you asked]\"\n\n**Their response:**\n> \"[Quote]\"\n\n**Why this matters:**\n[Connection to research question or observation patterns]\n\n**Save in:** Same field notes file or separate `informal-conversations.md`\n\n---\n\n## Managing Relationships in the Field\n\n### Building Rapport\n\n**Strategies:**\n- **Show genuine interest:** Ask questions, listen actively\n- **Respect boundaries:** Don't pry, let people share what they're comfortable with\n- **Be reliable:** Show up when promised, follow through on commitments\n- **Express gratitude:** Thank people for allowing observation\n- **Share when appropriate:** If asked about yourself, answer briefly but don't dominate\n\n### Maintaining Analytical Distance\n\n**While building rapport, maintain observer role:**\n- **Don't become advocate:** You're here to understand, not to fix or judge\n- **Avoid taking sides:** In conflicts, remain neutral\n- **Don't \"go native\":** Maintain critical analytical perspective\n- **Document personal connections:** How might relationships affect what you see?\n\n---\n\n## Ethical Considerations During Field Work\n\n### Ongoing Consent\n\n**Check in periodically:**\n- \"Are you still comfortable with me observing?\"\n- \"Is there anything you'd prefer I not record?\"\n\n**Respect opt-outs:**\n- If someone asks you to stop observing or not record something, honor that immediately\n- Document what you can't observe (helps identify limitations later)\n\n### Confidentiality in the Field\n\n**Protect participant identity:**\n- Use pseudonyms in all notes (even jottings that others might see)\n- Don't share observations with other participants\n- Store notes securely\n\n### When to Intervene\n\n**Stop observing and intervene if:**\n- You witness harm, abuse, or safety violations\n- Someone is in distress and needs help\n- You observe illegal activity\n\n**Document intervention:** How did it affect your observation? Your relationship with participants?\n\n---\n\n## Phase 2 Checkpoint Verification\n\n**Before proceeding to Phase 3, verify:**\n\n- [ ] All planned observation sessions completed (or saturation achieved)\n- [ ] All jottings expanded to full field notes within 24 hours of observation\n- [ ] All field notes use thick description (specific behaviors, not interpretations)\n- [ ] Reflective memos written for each session\n- [ ] Observation log complete\n- [ ] Saturation monitoring documented\n- [ ] Member checking conducted (if planned)\n- [ ] Informal conversations documented\n- [ ] Reflexivity maintained throughout (observer effect, assumptions, biases noted)\n- [ ] All files saved to `02-data-collection-log.md`\n- [ ] All field notes saved to `raw-data/field-notes-session-[N]-[date].md`\n- [ ] All memos saved to `raw-data/memo-session-[N]-[date].md`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 3 - Data Familiarization\n**If FAIL:** Complete missing requirements above before proceeding\n\n---\n\n## Common Observation Mistakes to Avoid\n\nâŒ **Recording interpretations instead of behaviors**\n- Don't write: \"She was angry\"\n- DO write: \"Voice volume increased, face flushed, fist hit table\"\n\nâŒ **Accepting your first impressions as truth**\n- First impressions are often wrong - keep observing\n\nâŒ **Only seeing what confirms your assumptions**\n- Actively look for disconfirming evidence\n\nâŒ **Waiting too long to expand jottings**\n- Same-day expansion is critical - you'll forget details\n\nâŒ **Observing without focus**\n- Return to research question - what MATTERS?\n\nâŒ **Forgetting to observe the mundane**\n- Routine, ordinary behavior reveals culture and norms\n\nâŒ **Assuming you understand insider meaning**\n- Ask clarifying questions, don't assume jargon/processes are what you think\n\nâŒ **Letting personal reactions stop observation**\n- Note your reactions in reflective column, but keep observing\n\nâœ… **Good observation looks like:**\n- Thick description with specific details\n- Both descriptive and reflective notes\n- Acknowledgment of observer effect and bias\n- Patterns identified tentatively, refined over multiple sessions\n- Questions raised more than answers claimed\n- Same-day expansion of all jottings\n",
        "plugins/datapeeker/skills/qualitative-research/templates/overview-summary.md": "# Analysis Overview\n\n**Session:** [session-name]\n**Created:** [YYYY-MM-DD]\n**Last Updated:** [YYYY-MM-DD]\n\n---\n\n## Research Summary\n\n**Research Question:** [From Phase 1]\n\n**Method:** [Interview / Survey / Focus Group / Observation]\n\n**Sample:** [N] participants, data collected [date range]\n\n**Status:** [In Progress - Phase N / Complete]\n\n---\n\n## Main Findings (added after Phase 6)\n\n### Theme 1: [Name]\n**Prevalence:** [X of Y participants]\n**Key insight:** [One sentence summary]\n\n### Theme 2: [Name]\n**Prevalence:** [X of Y participants]\n**Key insight:** [One sentence summary]\n\n### Theme 3: [Name]\n**Prevalence:** [X of Y participants]\n**Key insight:** [One sentence summary]\n\n---\n\n## Signal Classification (if for marketing-experimentation)\n\n**Hypothesis:** [From experiment tracker]\n\n**Signal:** [POSITIVE / NEGATIVE / NULL / MIXED]\n\n**Confidence:** [High / Medium / Low]\n\n**Recommendation:** [What should happen next based on this signal?]\n\n---\n\n## Key Limitations\n\n1. [Most important limitation]\n2. [Second most important limitation]\n3. [Third most important limitation]\n\n---\n\n## Follow-Up Questions\n\n1. [Most important follow-up question]\n2. [Second follow-up question]\n3. [Third follow-up question]\n\n---\n\n## File Structure\n\n```\nanalysis/qualitative-research/[session-name]/\nâ”œâ”€â”€ 00-overview.md (this file)\nâ”œâ”€â”€ 01-research-design.md\nâ”œâ”€â”€ 02-data-collection-log.md\nâ”œâ”€â”€ raw-data/\nâ”‚   â”œâ”€â”€ transcript-001.md\nâ”‚   â”œâ”€â”€ transcript-002.md\nâ”‚   â””â”€â”€ ...\nâ”œâ”€â”€ 03-familiarization-notes.md\nâ”œâ”€â”€ 04-coding-analysis.md\nâ”œâ”€â”€ 05-theme-development.md\nâ””â”€â”€ 06-findings-report.md\n```\n\n---\n\n## Phase Progress\n\n- [x] Phase 1: Research Design\n- [x] Phase 2: Data Collection\n- [x] Phase 3: Data Familiarization\n- [x] Phase 4: Systematic Coding\n- [x] Phase 5: Theme Development\n- [x] Phase 6: Synthesis & Reporting\n\n**Analysis complete:** [YYYY-MM-DD]\n\n---\n\n## Quick Reference\n\n**For detailed findings:** See `06-findings-report.md`\n\n**For themes and quotes:** See `05-theme-development.md`\n\n**For codebook:** See `04-coding-analysis.md` Section 1\n\n**For raw data:** See `raw-data/` directory\n",
        "plugins/datapeeker/skills/qualitative-research/templates/phase-3-familiarization.md": "# Data Familiarization Notes\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 3 - Data Familiarization\n\n---\n\n## Data Overview\n\n**Total Data Collected:**\n- [N] interviews/surveys/sessions\n- [N] total participants\n- [Date range] for data collection\n\n**Files Reviewed:**\n- `raw-data/transcript-001.md` through `transcript-[N].md`\n- OR `raw-data/survey-responses-batch-[N].md`\n- OR `raw-data/field-notes-[N].md`\n\n---\n\n## Initial Observations (NOT Codes Yet)\n\n**Purpose:** Document patterns noticed during reading WITHOUT creating formal codes. Just observe.\n\n### Observation 1: [Topic/Pattern Name]\n\n**What I noticed:**\n[Describe what you're seeing across multiple data sources]\n\n**Where it appears:**\n- Participant [N]: [Brief note]\n- Participant [N]: [Brief note]\n- Participant [N]: [Brief note]\n\n**Frequency:** [How often does this come up?]\n\n---\n\n### Observation 2: [Topic/Pattern Name]\n\n[Same structure...]\n\n---\n\n### Observation 3: [Topic/Pattern Name]\n\n[Same structure...]\n\n---\n\n## Surprising Findings\n\n**Purpose:** Note what contradicts your assumptions from Phase 1 reflexivity baseline.\n\n### Surprise 1: [What was unexpected]\n\n**What I expected:** [From Phase 1 assumptions]\n\n**What I actually saw:** [Contradictory data]\n\n**Evidence:**\n- \"[Quote]\" - Participant [N]\n- \"[Quote]\" - Participant [N]\n\n**Impact:** [How does this change my understanding?]\n\n---\n\n### Surprise 2: [What was unexpected]\n\n[Same structure...]\n\n---\n\n## Emerging Questions\n\n**Purpose:** Document questions that emerge from data review. These guide Phase 4 coding.\n\n1. **Question:** [What would you want to explore further?]\n   - **Why this matters:** [How does this relate to research question?]\n   - **Where to look:** [Which participants/data segments to examine?]\n\n2. **Question:** [Next question]\n   - **Why this matters:**\n   - **Where to look:**\n\n3. **Question:** [Next question]\n   - **Why this matters:**\n   - **Where to look:**\n\n---\n\n## Reflexivity Update\n\n**Purpose:** Document how your understanding is evolving during familiarization.\n\n**My initial assumptions (from Phase 1):**\n- [Assumption 1]\n- [Assumption 2]\n- [Assumption 3]\n\n**How the data challenges/confirms these:**\n- [Assumption 1]: [Confirmed / Challenged / Complicated by data]\n- [Assumption 2]: [Confirmed / Challenged / Complicated by data]\n- [Assumption 3]: [Confirmed / Challenged / Complicated by data]\n\n**New insights emerging:**\n[What are you learning that you didn't expect?]\n\n---\n\n## Agent Summaries (if analyze-transcript agent used)\n\n**Purpose:** If dataset is large (10+ transcripts), paste agent summaries here.\n\n### Transcript-001 Summary\n\n[Paste analyze-transcript agent output]\n\n---\n\n### Transcript-002 Summary\n\n[Paste analyze-transcript agent output]\n\n---\n\n[Continue for all transcripts...]\n\n---\n\n## Next Steps\n\n**Ready for Phase 4:** [Yes / No]\n\n**If not ready:**\n- [ ] Need to review [specific data]\n- [ ] Need to clarify [specific question]\n- [ ] Need to document [specific observation]\n\n**If ready:**\n- [ ] All data reviewed thoroughly\n- [ ] Observations documented\n- [ ] Surprises noted\n- [ ] Questions identified\n- [ ] Reflexivity updated\n\n**Proceed to:** Phase 4 - Systematic Coding\n",
        "plugins/datapeeker/skills/qualitative-research/templates/phase-4-coding.md": "# Systematic Coding Analysis\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 4 - Systematic Coding\n\n---\n\n## Section 1: Codebook\n\n**Purpose:** Define all codes with clear boundaries.\n\n### Code: [code-name]\n\n**Definition:** [What does this code mean?]\n\n**Include when:**\n- [Criterion 1 for applying this code]\n- [Criterion 2]\n- [Criterion 3]\n\n**Exclude when:**\n- [Criterion 1 for NOT applying this code]\n- [Criterion 2]\n- [Criterion 3]\n\n**Examples:**\n1. \"[Verbatim data extract showing this code]\" - Transcript-[N], Participant [N]\n2. \"[Verbatim data extract showing this code]\" - Transcript-[N], Participant [N]\n3. \"[Verbatim data extract showing this code]\" - Transcript-[N], Participant [N]\n\n---\n\n### Code: [next-code-name]\n\n[Same structure...]\n\n---\n\n### Code: [next-code-name]\n\n[Same structure...]\n\n---\n\n**Codebook Summary:**\n- **Total codes:** [N]\n- **Codebook version:** 1.0 (initial)\n- **Last updated:** [Date]\n\n---\n\n## Section 2: Coding Process\n\n**Purpose:** Document how codes were applied and any refinements made.\n\n### Initial Coding Approach\n\n**Strategy:**\n[How did you approach coding? Sequential through transcripts? Multiple passes? Used generate-initial-codes agent?]\n\n**Agent Support:**\n- [ ] Used generate-initial-codes agent\n  - Input: Transcripts [N, N, N] (20-30% of dataset)\n  - Output: [N] suggested codes reviewed and refined\n  - Accepted: [N] codes\n  - Modified: [N] codes\n  - Rejected: [N] codes\n\n**Coding Process:**\n1. [Step 1: e.g., First pass with initial codebook]\n2. [Step 2: e.g., Refined codes based on edge cases]\n3. [Step 3: e.g., Second pass with finalized codebook]\n\n### Codes Refined During Process\n\n**Code:** [code-name]\n- **Original definition:** [What it was]\n- **Refined definition:** [What it became]\n- **Reason:** [Why the change? What ambiguity was discovered?]\n\n---\n\n**Code:** [next-code-name]\n- **Original definition:**\n- **Refined definition:**\n- **Reason:**\n\n---\n\n### New Codes Added\n\n**Code:** [code-name]\n- **Added when:** [At what point in coding process]\n- **Reason:** [Why was this code needed? What pattern emerged?]\n- **Definition:** [Code definition]\n- **Examples:** [2-3 examples]\n\n---\n\n**Code:** [next-code-name]\n[Same structure...]\n\n---\n\n## Section 3: Intercoder Reliability Check (MANDATORY)\n\n**Purpose:** Verify coding consistency through independent second coding.\n\n### Sample Selection\n\n**Sample size:** [N] transcripts ([X]% of total dataset)\n\n**Transcripts selected:**\n- `transcript-[N].md` - [Reason for selection: diverse, representative, etc.]\n- `transcript-[N].md` - [Reason]\n\n**Total segments in sample:** [N]\n\n### Agent Invocation\n\n**Agent:** intercoder-reliability-check\n\n**Input:**\n- Complete codebook (version 1.0)\n- Sample transcripts (listed above)\n\n**Date run:** [YYYY-MM-DD]\n\n### Agreement Results\n\n**Overall Agreement:**\n- Perfect agreement: [N] segments ([X]%)\n- Partial agreement: [N] segments ([X]%)\n- Complete disagreement: [N] segments ([X]%)\n- **Cohen's Kappa:** [X.XX] ([interpretation: slight/fair/moderate/substantial/almost perfect])\n\n**Agreement by Code:**\n\n| Code | Primary Used | Agent Used | Agreement % |\n|------|-------------|-----------|------------|\n| [code-1] | [N] | [N] | [X]% |\n| [code-2] | [N] | [N] | [X]% |\n| [code-3] | [N] | [N] | [X]% |\n\n### Disagreement Analysis\n\n**Disagreement 1:** [code-name]\n\n**Segment:**\n> \"[Data extract where disagreement occurred]\"\n\n**Primary coding:** [codes applied by main coder]\n**Agent coding:** [codes applied by second coder]\n\n**Reason for disagreement:**\n[Why did coders disagree? Ambiguous inclusion criteria? Edge case?]\n\n**Resolution:**\n[How was this resolved? Codebook updated? Criterion clarified?]\n\n---\n\n**Disagreement 2:** [code-name]\n\n[Same structure...]\n\n---\n\n### Codebook Refinements from Reliability Check\n\n**Updates made:**\n\n1. **Code:** [code-name]\n   - **Change:** [Refined inclusion/exclusion criterion]\n   - **Reason:** [Based on disagreement analysis]\n\n2. **Code:** [code-name]\n   - **Change:** [Added example to codebook]\n   - **Reason:** [Based on edge case identified]\n\n**Updated codebook version:** 1.1 (post-reliability check)\n\n### Reliability Assessment\n\n**Overall quality:** [Excellent >90% / Good 80-90% / Moderate 70-80% / Needs work <70%]\n\n**Interpretation:**\n[What does the agreement percentage tell us about codebook quality?]\n\n**Action taken:**\n[What was done based on reliability results? Codebook refinements? Re-coding of problematic segments?]\n\n**Ready for Phase 5:** [Yes / No]\n\n---\n\n## Section 4: Audit Trail\n\n**Purpose:** Document all coding decisions for reproducibility.\n\n### Decision 1: [Description]\n\n**Context:** [What situation prompted this decision?]\n\n**Decision:** [What was decided?]\n\n**Rationale:** [Why this decision?]\n\n**Impact:** [How did this affect coding? Which transcripts were affected?]\n\n---\n\n### Decision 2: [Description]\n\n[Same structure...]\n\n---\n\n### Coding Statistics\n\n**Dataset coded:**\n- Total transcripts/data files: [N]\n- Total segments coded: [N]\n- Average segments per transcript: [N]\n\n**Code usage:**\n- Most frequent code: [code-name] ([N] occurrences)\n- Least frequent code: [code-name] ([N] occurrences)\n- Codes used <5 times: [List - may indicate too specific or should be combined]\n\n**Coding dates:**\n- Started: [YYYY-MM-DD]\n- Completed: [YYYY-MM-DD]\n- Duration: [N] days\n\n---\n\n## Phase 4 Checkpoint Verification\n\n**Before proceeding to Phase 5, verify:**\n\n- [ ] Codebook complete with all codes defined\n- [ ] Inclusion and exclusion criteria specified for each code\n- [ ] Examples provided for each code (2-3 minimum)\n- [ ] Entire dataset coded systematically\n- [ ] **Intercoder reliability check COMPLETED** (MANDATORY)\n- [ ] Agreement percentage calculated and documented\n- [ ] Disagreements analyzed and codebook refined as needed\n- [ ] Audit trail documents all coding decisions\n- [ ] All results saved to `04-coding-analysis.md`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 5 - Theme Development\n**If FAIL:** Complete missing requirements above before proceeding\n",
        "plugins/datapeeker/skills/qualitative-research/templates/phase-5-themes.md": "# Theme Development & Refinement\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 5 - Theme Development & Refinement\n\n---\n\n## Theme 1: [Theme Name]\n\n### Theme Definition\n\n**Theme:** [Descriptive name, 4-8 words]\n\n**Definition:** [What does this theme capture? The overarching pattern or concept connecting multiple codes]\n\n**Prevalence:** [X of Y participants, Z total coded segments]\n\n### Supporting Codes\n\n**Codes that contribute to this theme:**\n- [code-1] ([N] occurrences)\n- [code-2] ([N] occurrences)\n- [code-3] ([N] occurrences)\n\n### Representative Quotes (from extract-supporting-quotes agent)\n\n**Quote 1:**\n> \"[Verbatim data extract]\"\n\n**Source:** Participant [N], [Context if relevant]\n\n---\n\n**Quote 2:**\n> \"[Verbatim data extract]\"\n\n**Source:** Participant [N]\n\n---\n\n**Quote 3:**\n> \"[Verbatim data extract]\"\n\n**Source:** Participant [N]\n\n---\n\n### Disconfirming Evidence Search (MANDATORY)\n\n**Agent:** search-disconfirming-evidence\n\n**Input:** Theme definition + full dataset\n\n**Date run:** [YYYY-MM-DD]\n\n**Contradictions Found:**\n\n#### Contradiction 1: [Description]\n\n**Data Extract:**\n> \"[Quote that contradicts theme]\"\n\n**Source:** Participant [N]\n\n**Why This Contradicts:** [Explanation]\n\n**Prevalence of contradiction:** [N participants show this pattern]\n\n---\n\n#### Contradiction 2: [Description]\n\n[Same structure...]\n\n---\n\n**Negative Cases (Participants Who Don't Fit Theme):**\n\n**Participant [N]:**\n- What they said instead: \"[Quote]\"\n- Why theme doesn't apply: [Explanation]\n- Alternative pattern they exhibit: [Description]\n\n**Participant [N]:**\n[Same structure...]\n\n---\n\n**Theme Refinement Based on Disconfirming Evidence:**\n\n**Original prevalence:** [X of Y participants]\n**Revised prevalence:** [A of B participants] (excluding [C] who don't fit)\n\n**Original definition:** [What it was before disconfirmation search]\n**Refined definition:** [Revised to account for contradictions and boundaries]\n\n**Boundary clarification:**\n- **Include:** [What clearly belongs in this theme]\n- **Exclude:** [What clearly doesn't belong]\n\n### Negative Case Explanation\n\n**Participants who don't fit this theme ([N] of [Y]):**\n\n**Why they don't fit:**\n[Explanation of why certain participants don't exhibit this pattern]\n\n**Their pattern instead:**\n[What pattern DO these participants show?]\n\n**Interpretation:**\n[What do negative cases tell us about boundaries or conditions for this theme?]\n\n---\n\n## Theme 2: [Theme Name]\n\n[Same complete structure as Theme 1...]\n\n---\n\n## Theme 3: [Theme Name]\n\n[Same complete structure as Theme 1...]\n\n---\n\n## Cross-Theme Analysis\n\n**Purpose:** Identify relationships between themes.\n\n### Theme Overlap\n\n**Themes 1 & 2:**\n- Participants who show BOTH themes: [List participants]\n- Relationship: [How do these themes relate? Complementary? Sequential? Conditional?]\n\n**Themes 2 & 3:**\n[Same structure...]\n\n### Theme Independence\n\n**Themes that don't overlap:**\n[Which themes appear in distinct participant groups? What does this tell us?]\n\n### Dominant Patterns\n\n**Most prevalent theme:** [Theme name] ([X] of [Y] participants)\n**Least prevalent theme:** [Theme name] ([A] of [Y] participants)\n\n**Interpretation:**\n[What does prevalence distribution tell us about the data/research question?]\n\n---\n\n## Themes NOT Supported\n\n**Purpose:** Document themes that were considered but didn't hold up after disconfirming evidence search.\n\n### Rejected Theme: [Name]\n\n**Why considered:** [Initial pattern that suggested this theme]\n\n**Why rejected:** [Contradictions found, insufficient support, alternative explanation]\n\n**What we learned:** [What did this tell us about the data?]\n\n---\n\n## Phase 5 Checkpoint Verification\n\n**Before proceeding to Phase 6, verify:**\n\n- [ ] All themes defined with clear descriptions\n- [ ] Supporting codes identified for each theme\n- [ ] Prevalence calculated for each theme (X of Y participants)\n- [ ] Representative quotes extracted for each theme\n- [ ] **Disconfirming evidence search COMPLETED for ALL themes** (MANDATORY)\n- [ ] Contradictions documented and analyzed\n- [ ] Negative cases identified and explained\n- [ ] Theme definitions refined based on disconfirming evidence\n- [ ] Honest prevalence reporting (revised counts if needed)\n- [ ] All results saved to `05-theme-development.md`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 6 - Synthesis & Reporting\n**If FAIL:** Complete missing requirements (especially disconfirming evidence search)\n",
        "plugins/datapeeker/skills/qualitative-research/templates/phase-6-reporting.md": "# Findings Report\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 6 - Synthesis & Reporting\n\n---\n\n## Research Overview\n\n**Research Question:** [From Phase 1]\n\n**Method:** [Interview / Survey / Focus Group / Observation]\n\n**Sample:**\n- Total participants: [N]\n- Data collection period: [Date range]\n- Saturation achieved: [Yes / No / Partial]\n\n**Analysis Approach:**\n- Coding method: Thematic Analysis\n- Codebook: [N] codes developed\n- Themes identified: [N] themes\n- Intercoder reliability: [X]% agreement (Kappa [X.XX])\n\n---\n\n## Main Findings\n\n### Theme 1: [Theme Name]\n\n**Definition:** [Theme definition from Phase 5]\n\n**Prevalence:** [X of Y participants] ([Z]%)\n\n**Representative Quotes:**\n\n> \"[Quote 1]\" - Participant [N]\n\n> \"[Quote 2]\" - Participant [N]\n\n> \"[Quote 3]\" - Participant [N]\n\n**Negative Cases:** [A] of [Y] participants did not exhibit this theme.\n- These participants [explanation of what they showed instead]\n\n**Context:** [When/where does this theme apply? What conditions are present?]\n\n---\n\n### Theme 2: [Theme Name]\n\n[Same structure as Theme 1...]\n\n---\n\n### Theme 3: [Theme Name]\n\n[Same structure as Theme 1...]\n\n---\n\n## Cross-Theme Patterns\n\n**Relationships between themes:**\n\n**Themes 1 & 2:** [How they relate - complementary, sequential, conditional?]\n\n**Themes 2 & 3:** [Relationship]\n\n**Dominant pattern:** [Which theme was most prevalent? What does this mean?]\n\n---\n\n## Limitations (MANDATORY - Be HONEST)\n\n**Purpose:** Acknowledge constraints and uncertainties to strengthen credibility.\n\n### Sample Limitations\n\n**Size:** [N] participants\n- Impact: [How does sample size limit generalizability?]\n- Adequacy for saturation: [Was saturation achieved? If not, what's missing?]\n\n**Homogeneity/Diversity:**\n- [Describe sample composition]\n- [What perspectives might be missing?]\n- [How does this limit transferability?]\n\n**Recruitment:**\n- Method: [How were participants recruited?]\n- Potential bias: [Does recruitment method introduce selection bias?]\n\n### Method Constraints\n\n**Data collection:**\n- [Interviews: Single conversation vs. longitudinal?]\n- [Surveys: Response rate? Self-report limitations?]\n- [Focus groups: Group dynamics influence?]\n- [Observations: Observer effect?]\n\n**Analysis:**\n- Single primary coder (intercoder reliability check: [X]%)\n- Coding decisions subject to interpretation despite systematic approach\n- Themes emerged from this specific dataset\n\n### Researcher Bias\n\n**From Phase 1 reflexivity baseline:**\n- Assumption 1: [How this may have influenced interpretation]\n- Assumption 2: [Impact on question design or analysis]\n\n**Mitigation:**\n- Disconfirming evidence search for all themes\n- Intercoder reliability check\n- Negative cases documented and explained\n\n### Context Limitations\n\n**Geography:** [Where was this research conducted? How does location limit generalizability?]\n\n**Time period:** [When? Are findings time-dependent?]\n\n**Industry/domain:** [Specific to certain context?]\n\n**Conditions:** [What conditions were present that might not apply elsewhere?]\n\n---\n\n## Confidence Assessment\n\n**Purpose:** Evaluate trustworthiness of findings using qualitative rigor criteria.\n\n### Credibility (Internal Validity)\n**Question:** Do findings accurately represent participant experiences?\n\n**Assessment:** [High / Medium / Low]\n\n**Justification:**\n- [Evidence: prolonged engagement, member checking, triangulation, etc.]\n- [Strengths that support credibility]\n- [Weaknesses that undermine credibility]\n\n### Dependability (Reliability)\n**Question:** Would another researcher reach similar conclusions?\n\n**Assessment:** [High / Medium / Low]\n\n**Justification:**\n- Intercoder reliability: [X]% agreement\n- Detailed audit trail: [Complete documentation of decisions]\n- [Other evidence of consistency]\n\n### Confirmability (Objectivity)\n**Question:** Are findings based on data, not researcher bias?\n\n**Assessment:** [High / Medium / Low]\n\n**Justification:**\n- Disconfirming evidence search completed for all themes\n- Negative cases documented\n- Reflexivity maintained throughout\n- [Evidence that findings grounded in data]\n\n### Transferability (External Validity)\n**Question:** Do findings apply beyond this specific sample?\n\n**Assessment:** [High / Medium / Low]\n\n**Justification:**\n- Thick description provided: [Context, participants, conditions]\n- [Evidence supporting broader applicability]\n- [Limitations to transferability noted above]\n\n---\n\n## Follow-Up Research Questions\n\n**Purpose:** Every analysis should raise new questions.\n\n### Question 1: [What would you investigate next?]\n\n**Why this matters:** [How does this extend/deepen current findings?]\n\n**Approach:** [How would you investigate this? What method? What sample?]\n\n**Expected contribution:** [What would answering this add to understanding?]\n\n---\n\n### Question 2: [What surprised you that needs deeper exploration?]\n\n**Why this matters:**\n\n**Approach:**\n\n**Expected contribution:**\n\n---\n\n### Question 3: [What would strengthen confidence in current findings?]\n\n**Why this matters:**\n\n**Approach:**\n\n**Expected contribution:**\n\n---\n\n## Signal Classification (if invoked by marketing-experimentation)\n\n**Hypothesis tested:** [From marketing-experimentation experiment tracker]\n\n**Expected outcome:** [What was predicted?]\n\n**Actual outcome:** [What did the data show?]\n\n**Signal:** [POSITIVE / NEGATIVE / NULL / MIXED]\n\n**Justification:**\n[Based on themes and prevalence, how does this validate or invalidate the hypothesis?]\n\n**Confidence in signal:** [High / Medium / Low]\n\n**Recommendation:**\n[Based on this signal, what should happen next in the marketing-experimentation cycle?]\n\n---\n\n## Summary\n\n**In 3-5 bullet points, what are the key takeaways?**\n\n- [Finding 1 with prevalence]\n- [Finding 2 with prevalence]\n- [Finding 3 with prevalence]\n- [Notable negative case or limitation]\n- [Strongest follow-up question]\n\n**Confidence level:** [Overall assessment: High / Medium / Low]\n\n**Most important insight:** [What's the single most important thing we learned?]\n\n---\n\n## Phase 6 Checkpoint Verification\n\n**Before marking complete, verify:**\n\n- [ ] All themes documented with representative quotes\n- [ ] Prevalence reported honestly (X of Y, not \"all\" or \"most\")\n- [ ] Negative cases explained for each theme\n- [ ] **Limitations explicitly stated** (sample, method, researcher bias, context)\n- [ ] Confidence assessment completed (credibility, dependability, confirmability, transferability)\n- [ ] 2-3 follow-up research questions identified\n- [ ] Signal classification provided (if for marketing-experimentation)\n- [ ] Summary section completed\n- [ ] All results saved to `06-findings-report.md`\n- [ ] `00-overview.md` updated with final summary\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Analysis complete. Overview updated. Git commit ready.\n**If FAIL:** Complete missing requirements above\n",
        "plugins/datapeeker/skills/qualitative-research/templates/surveys/phase-1-survey-design.md": "# Research Design - Survey Method\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 1 - Research Design\n**Method:** Qualitative Survey\n\n---\n\n## Research Question\n\n**Primary Question:**\n[What are you trying to understand through this survey?]\n\n**Sub-Questions:**\n1. [Supporting question 1]\n2. [Supporting question 2]\n3. [Supporting question 3]\n\n**Success Criteria:**\n[What would constitute a successful answer to this research question?]\n\n---\n\n## Reflexivity Baseline (MANDATORY)\n\n**Purpose:** Document your assumptions BEFORE data collection to prevent confirmation bias.\n\n### My Assumptions\n\n**What I believe the answer will be:**\n[Write down what you expect to find. Be specific.]\n\n**Why I believe this:**\n[What experience, expert opinion, or prior knowledge shapes this belief?]\n\n**What would surprise me:**\n[What findings would contradict your expectations?]\n\n### My Biases\n\n**Industry/domain experience:**\n[How might your background influence what you notice or interpret?]\n\n**Expert opinions I'm carrying:**\n[What have mentors, consultants, or authorities told you?]\n\n**Prior hypotheses:**\n[What have you already concluded that might bias interpretation?]\n\n**Desired outcome:**\n[What result would you WANT to find? How might this create bias?]\n\n---\n\n## Survey Strategy\n\n**Survey Type:** Open-ended qualitative survey (NOT quantitative Likert scales)\n\n**Purpose:**\n[Why a survey instead of interviews? Broader reach? Async? Geographic diversity?]\n\n**Target completion time:** [10-20 minutes recommended for response rates]\n\n**Question count:** [8-15 open-ended questions recommended]\n\n---\n\n## Question Design\n\n**CRITICAL:** Open-ended questions that yield rich, detailed text responses.\n\n### Question Set 1: Context & Background\n\n**Q1: Screening/Demographics**\n[What contextual information do you need about respondents?]\n\nExample format:\n\"What is your role/industry/context relevant to this research?\"\n- Keep brief (2-3 questions max)\n- Only ask what's necessary for analysis\n- Place at beginning OR end (test which gets better response)\n\n---\n\n### Question Set 2: Core Open-Ended Questions\n\n**Q2:** [First main research question]\n\n**Question text:**\n\"[Open-ended question asking about experience, behavior, or process]\"\n\n**What this question aims to uncover:**\n[What insight are you seeking?]\n\n**Probes/examples to include:**\n[Optional: Add prompt like \"Please describe in detail...\" or \"For example...\"]\n\n---\n\n**Q3:** [Second main question]\n\n[Same structure as Q2...]\n\n---\n\n**Q4:** [Third main question]\n\n[Same structure as Q2...]\n\n---\n\n### Question Set 3: Depth & Examples\n\n**Q5: Specific Example Prompt**\n\n**Question text:**\n\"Can you describe a specific, recent situation where [topic]?\"\n\n**Follow-up prompts:**\n- \"What happened?\"\n- \"What was the outcome?\"\n- \"How did that make you feel / what did you do?\"\n\n**Why specific examples matter:**\n[Specificity prevents vague generalities - you want stories, not opinions]\n\n---\n\n**Q6:** [Another depth question]\n\n[Same structure...]\n\n---\n\n### Question Set 4: Challenges & Context\n\n**Q7: Challenge/Frustration Question**\n\n**Question text:**\n\"What challenges or frustrations have you experienced with [topic]?\"\n\n**Probe:**\n\"Please provide specific examples if possible.\"\n\n---\n\n**Q8:** [Additional context question]\n\n[Same structure...]\n\n---\n\n### Question Set 5: Closing & Follow-Up\n\n**Q9: Anything Missed**\n\n**Question text:**\n\"Is there anything about [topic] that we haven't asked about that you think is important?\"\n\n**Purpose:** Let respondents share what THEY think matters\n\n---\n\n**Q10: Follow-Up Consent (Optional)**\n\n**Question text:**\n\"Would you be willing to participate in a brief follow-up interview to explore your responses in more depth?\"\n\n- [ ] Yes, contact me at: [email field]\n- [ ] No, thank you\n\n**Purpose:** Identify rich cases for deeper investigation\n\n---\n\n## Question Design Checklist\n\n**For EACH question, verify:**\n\n- [ ] Open-ended (can't be answered with yes/no or single word)\n- [ ] Asks about EXPERIENCE or BEHAVIOR (not hypothetical futures)\n- [ ] Neutral (doesn't lead toward desired answer)\n- [ ] Clear and specific (respondent knows exactly what you're asking)\n- [ ] One question per item (no double-barreled questions)\n- [ ] Appropriate reading level (grade 8-10 recommended)\n\n**Red flags - AVOID:**\n\nâŒ \"Do you think [X] is important?\" (yes/no + leading)\nâŒ \"Would you use [Y]?\" (hypothetical + yes/no)\nâŒ \"Rate your satisfaction with [Z]\" (quantitative scale, not qualitative)\nâŒ \"What features would you want?\" (hypothetical, not experience-based)\nâŒ \"How often do you [X] and what challenges do you face?\" (double-barreled)\n\n**Green flags - USE:**\n\nâœ… \"Describe your experience with [X]\"\nâœ… \"Tell us about a recent time when [Y]\"\nâœ… \"What challenges have you encountered with [Z]?\"\nâœ… \"Walk us through how you currently [process]\"\nâœ… \"What matters most to you about [topic]?\"\n\n---\n\n## Survey Structure & Flow\n\n**Recommended order:**\n\n1. **Welcome screen**\n   - Purpose of survey\n   - Estimated time\n   - Anonymity/confidentiality statement\n   - Consent\n\n2. **Context questions** (2-3 questions)\n   - Who they are / screening\n   - Keep brief\n\n3. **Core open-ended questions** (5-8 questions)\n   - Main research questions\n   - Start broad, then narrow\n   - Progress from easy to more complex\n\n4. **Depth/example questions** (2-3 questions)\n   - Specific examples\n   - Stories and details\n\n5. **Closing questions** (1-2 questions)\n   - Anything missed\n   - Follow-up consent\n\n6. **Thank you screen**\n   - Appreciation\n   - What happens next (if applicable)\n   - Incentive details (if applicable)\n\n**Flow principles:**\n- Start with easier, less personal questions\n- Build to more complex or sensitive topics\n- End with open \"anything else?\" question\n- Total length: 10-20 minutes (test this)\n\n---\n\n## Survey Platform Selection\n\n**Platform:** [Google Forms / Typeform / Qualtrics / SurveyMonkey / etc.]\n\n**Requirements:**\n- [ ] Supports open-ended text responses\n- [ ] Allows exporting responses as text/CSV\n- [ ] Tracks response timestamps\n- [ ] [Optional] Allows conditional logic\n- [ ] [Optional] Anonymous responses\n- [ ] [Optional] Progress saving\n\n**Access:** [Who has admin access? Where is survey hosted?]\n\n---\n\n## Pilot Testing (MANDATORY)\n\n**Purpose:** Test survey before full distribution to catch confusing questions.\n\n### Pilot Test Plan\n\n**Pilot sample size:** [5-10 people recommended]\n\n**Pilot recruitment:**\n- [Who will you recruit? Friends/colleagues who fit target audience?]\n- Criteria: Should match target audience but NOT be in final sample\n\n**Pilot test date:** [YYYY-MM-DD]\n\n### Pilot Test Feedback Questions\n\n**After pilot respondents complete survey, ask:**\n\n1. \"How long did this survey take you?\"\n2. \"Were any questions confusing or unclear?\"\n3. \"Were any questions leading or biased?\"\n4. \"Did you feel like you could answer honestly?\"\n5. \"What questions were difficult to answer and why?\"\n6. \"Were there any topics we should have asked about?\"\n7. \"Overall, how was the experience of taking this survey?\"\n\n### Pilot Test Results\n\n**Average completion time:** [MM] minutes\n\n**Questions flagged as confusing:**\n- Q[N]: \"[Question text]\"\n  - **Issue:** [What was confusing?]\n  - **Revision:** [How will you revise it?]\n\n**Questions flagged as leading:**\n- Q[N]: \"[Question text]\"\n  - **Issue:** [How was it leading?]\n  - **Revision:** [How will you make it neutral?]\n\n**Questions that yielded thin responses:**\n- Q[N]: \"[Question text]\"\n  - **Issue:** [Why were responses thin?]\n  - **Revision:** [How will you encourage detail?]\n\n**New topics identified:**\n- [Topic that pilot respondents raised]\n- **Action:** [Add new question? Revise existing?]\n\n### Survey Revisions from Pilot\n\n**Version 1.0:** Initial survey (pilot tested)\n**Version 1.1:** Revised survey (for full distribution)\n\n**Changes made:**\n1. [Change 1 - e.g., \"Revised Q3 to be more specific\"]\n2. [Change 2 - e.g., \"Added prompt for examples to Q5\"]\n3. [Change 3 - e.g., \"Split double-barreled Q7 into two questions\"]\n\n**Rationale:** [Why these changes based on pilot feedback]\n\n---\n\n## Sampling Strategy\n\n**Target Sample Size:** [50-100+ for qualitative survey with open-ended questions]\n\n**Rationale:** [Why this sample size? Seeking diversity? Saturation? Breadth?]\n\n**Inclusion Criteria:**\n- [Who should be included? What characteristics must they have?]\n\n**Exclusion Criteria:**\n- [Who should be excluded? What would disqualify someone?]\n\n**Diversity Goals:**\n- [What diversity do you need? Industry? Company size? Role? Geography?]\n- [How will you ensure diverse sample?]\n\n---\n\n## Recruitment Plan\n\n**Method 1:** [e.g., Email list, LinkedIn, social media, etc.]\n- **Expected reach:** [N people]\n- **Expected response rate:** [X% - typically 10-30% for cold outreach]\n- **Expected responses:** [N]\n\n**Method 2:** [Secondary approach]\n- **Expected reach:** [N people]\n- **Expected response rate:** [X%]\n- **Expected responses:** [N]\n\n**Method 3 (if needed):** [Backup approach]\n\n**Total expected responses:** [N]\n\n**Recruitment timeline:**\n- **Survey launch:** [YYYY-MM-DD]\n- **First reminder:** [Date - typically 3-5 days after launch]\n- **Second reminder:** [Date - typically 7-10 days after launch]\n- **Survey close:** [Date - typically 14-21 days after launch]\n\n---\n\n## Incentive Strategy (if applicable)\n\n**Incentive offered:** [Gift card amount / donation / prize drawing / etc.]\n\n**Delivery method:** [How will incentive be provided?]\n\n**Budget:** [$X total, $Y per response]\n\n**Incentive announcement:**\n[Where/how will you announce incentive? On welcome screen? In recruitment message?]\n\n**Note:** For qualitative research, incentive typically helps response rate but can bias sample toward incentive-motivated respondents. Consider:\n- Higher incentive = better response rate but potentially less genuine responses\n- Lower/no incentive = lower response rate but more genuinely interested respondents\n\n---\n\n## Consent & Ethics\n\n**Consent method:** [Checkbox on welcome screen / Separate consent form]\n\n**Consent statement:**\n\"By completing this survey, you consent to your responses being used for [purpose]. Your responses will be [anonymous / confidential / attributed]. You may skip any questions or exit the survey at any time.\"\n\n**Anonymity:**\n- [ ] Fully anonymous (no identifying information collected)\n- [ ] Confidential (identifying info collected but kept separate from responses)\n- [ ] Attributed (responses will be linked to identity)\n\n**Data protection:**\n- Responses stored: [Where?]\n- Access limited to: [Who?]\n- Data retention: [How long? Deletion timeline?]\n\n**Participant rights:**\n- [ ] Right to withdraw (how? deadline?)\n- [ ] Right to review responses (if not anonymous)\n- [ ] Right to request deletion\n- [ ] Access to findings (will you share results?)\n\n---\n\n## Survey Welcome Screen (Draft)\n\n**Title:** [Survey title visible to respondents]\n\n**Welcome message:**\n\"Thank you for taking the time to share your insights with us.\n\nWe are researching [general topic] to understand [broad goal]. Your experiences and perspectives will help us [purpose].\n\nThis survey should take approximately [X] minutes to complete. All questions are open-ended - we want to hear your detailed thoughts and experiences in your own words.\n\nYour responses will be [anonymous / confidential]. [Add any data protection details.]\n\n[If incentive: Upon completion, you will receive [incentive details].]\n\nBy clicking 'Start Survey' below, you consent to participate in this research.\"\n\n---\n\n## Survey Thank You Screen (Draft)\n\n**Thank you message:**\n\"Thank you for completing this survey!\n\nYour insights are incredibly valuable to our research. [If incentive: You will receive [incentive details] within [timeframe].]\n\n[If follow-up option: If you indicated willingness to participate in a follow-up interview, we will contact you at the email you provided within [timeframe].]\n\n[If sharing results: We expect to have initial findings by [date]. If you would like to receive a summary of our findings, please [provide email / check here].]\n\nThank you again for your time and thoughtful responses.\"\n\n---\n\n## Phase 1 Checkpoint Verification\n\n**Before proceeding to Phase 2, verify:**\n\n- [ ] Research question defined and specific\n- [ ] Survey designed with open-ended qualitative questions (NOT Likert scales)\n- [ ] All questions ask about EXPERIENCE/BEHAVIOR (not hypothetical futures)\n- [ ] Question design checklist completed for each question\n- [ ] Survey flow logical (easy to complex)\n- [ ] **Pilot test COMPLETED** with 5-10 people\n- [ ] Pilot feedback analyzed and survey revised\n- [ ] Survey platform selected and tested\n- [ ] Sampling strategy documented (who, how many, how to recruit)\n- [ ] Recruitment plan with timeline created\n- [ ] **Reflexivity baseline completed** (assumptions and biases documented)\n- [ ] Consent and ethics procedures defined\n- [ ] Welcome and thank you screens drafted\n- [ ] All content saved to `01-research-design.md`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 2 - Survey Distribution & Data Collection\n**If FAIL:** Complete missing requirements (especially pilot test and reflexivity baseline)\n",
        "plugins/datapeeker/skills/qualitative-research/templates/surveys/phase-2-survey-distribution.md": "# Data Collection - Survey Distribution & Monitoring\n\n**Session:** [session-name]\n**Date:** [YYYY-MM-DD]\n**Phase:** 2 - Data Collection (Survey Method)\n\n---\n\n## Pre-Launch Checklist\n\n**Before distributing survey, verify:**\n\n- [ ] Pilot test completed and survey revised based on feedback\n- [ ] Survey platform tested (all questions display correctly)\n- [ ] Survey link is shareable and accessible\n- [ ] Logic/branching tested (if applicable)\n- [ ] Response export tested (can you download responses?)\n- [ ] Consent language reviewed\n- [ ] Thank you screen includes all necessary information\n- [ ] Incentive delivery method ready (if applicable)\n- [ ] Recruitment messages drafted\n- [ ] Reminder messages drafted\n- [ ] Launch date confirmed\n\n---\n\n## Distribution Plan\n\n### Launch Details\n\n**Survey URL:** [Insert survey link]\n\n**Launch date:** [YYYY-MM-DD]\n\n**Close date:** [YYYY-MM-DD] ([N] days open)\n\n**Target responses:** [N]\n\n---\n\n### Recruitment Message Templates\n\n**Initial Recruitment Message:**\n\nSubject: [Invitation to participate in research on [topic]]\n\n\"Dear [Name / Community],\n\nI'm conducting research on [topic] to understand [goal]. I'm seeking input from people who [target audience description].\n\nIf this describes you, I would greatly appreciate [X] minutes of your time to complete a brief survey. The survey asks open-ended questions about your experiences with [topic].\n\nYour responses will be [anonymous/confidential] and will help [purpose/impact].\n\n[If incentive: As a thank you for your time, [incentive details].]\n\nSurvey link: [URL]\n\nThe survey will remain open until [date].\n\nThank you for considering this request!\n\n[Your name / role / affiliation]\"\n\n---\n\n**Reminder 1 (3-5 days after launch):**\n\nSubject: [Gentle reminder: Research survey on [topic]]\n\n\"Dear [Name / Community],\n\nA few days ago, I sent an invitation to participate in a survey about [topic]. If you've already completed it, thank you!\n\nIf you haven't had a chance yet, I would still greatly appreciate your input. The survey takes approximately [X] minutes and asks about your experiences with [topic].\n\nSurvey link: [URL]\n\nThe survey will close on [date].\n\nThank you!\n\n[Your name]\"\n\n---\n\n**Reminder 2 (7-10 days after launch):**\n\nSubject: [Final reminder: Survey closes [date]]\n\n\"Dear [Name / Community],\n\nThis is a final reminder that the survey on [topic] will close on [date].\n\nIf you've already completed it, thank you so much for your thoughtful responses.\n\nIf not, there's still time to share your insights: [URL]\n\nYour perspective is valuable to this research.\n\nThank you!\n\n[Your name]\"\n\n---\n\n### Distribution Channels\n\n**Channel 1:** [e.g., Email list]\n- **Recipients:** [N people]\n- **Sent:** [YYYY-MM-DD]\n- **Open rate:** [X%] (if tracked)\n- **Click rate:** [X%] (if tracked)\n\n**Channel 2:** [e.g., LinkedIn post]\n- **Audience:** [Description]\n- **Posted:** [YYYY-MM-DD]\n- **Reach:** [N people] (if tracked)\n\n**Channel 3:** [e.g., Community forum, Slack group, etc.]\n- **Audience:** [Description]\n- **Posted:** [YYYY-MM-DD]\n- **Reach:** [N people] (if tracked)\n\n---\n\n## Response Monitoring\n\n### Daily Response Tracking\n\n**Goal:** Monitor response rate and quality to make adjustments if needed.\n\n| Date | Total Responses | New Today | Response Rate | Avg Completion Time | Notes |\n|------|----------------|-----------|---------------|-------------------|--------|\n| [MM-DD] | [N] | [N] | [X%] | [M] min | [Any observations] |\n| [MM-DD] | [N] | [N] | [X%] | [M] min | |\n| [MM-DD] | [N] | [N] | [X%] | [M] min | |\n\n**Response rate calculation:**\n[Total responses] / [Total invitations sent] Ã— 100 = [X]%\n\n---\n\n### Response Quality Spot Checks\n\n**Purpose:** Monitor if respondents are providing detailed, thoughtful responses.\n\n**Check frequency:** Daily for first 3 days, then every 2-3 days\n\n**What to check:**\n\n1. **Response length:**\n   - Are open-ended responses detailed (multiple sentences) or superficial (one word)?\n   - If superficial: Consider adding more prompts/examples to questions\n\n2. **Response relevance:**\n   - Are respondents answering the actual question or going off-topic?\n   - If off-topic: Question might be unclear - consider clarification\n\n3. **Bot/spam responses:**\n   - Look for nonsensical text, repeated patterns, or obvious spam\n   - Remove these responses before analysis\n\n4. **Incomplete responses:**\n   - How many people start but don't finish?\n   - Where do they drop off? (May indicate survey too long or question too difficult)\n\n**Quality assessment:** [Good / Acceptable / Concerning]\n\n**Actions taken based on quality:**\n- [e.g., \"Added more context to Q5 prompt\" or \"No changes needed\"]\n\n---\n\n### Response Rate Interventions\n\n**If response rate is lower than expected, consider:**\n\n1. **Send reminder earlier than planned**\n   - [Action taken / Date sent]\n\n2. **Post in additional channels**\n   - [Where? / Date posted]\n\n3. **Increase incentive** (if applicable and ethical)\n   - [Original: X / Increased to: Y / Date changed]\n\n4. **Extend survey open period**\n   - [Original close date / New close date]\n\n5. **Personalize recruitment outreach**\n   - [Approach: e.g., direct messages vs mass email]\n\n**Intervention log:**\n\n| Date | Intervention | Reason | Result |\n|------|-------------|--------|--------|\n| [MM-DD] | [What you did] | [Why] | [Impact on response rate] |\n\n---\n\n## Response Data Management\n\n### Export Schedule\n\n**Export frequency:** [Daily / Every 3 days / Weekly]\n\n**Export location:** `analysis/qualitative-research/[session-name]/raw-data/`\n\n**Export format:** [CSV / Excel / JSON]\n\n**Filename convention:** `survey-responses-[YYYY-MM-DD].csv`\n\n**Export log:**\n\n| Date | Filename | Responses Included | Notes |\n|------|----------|-------------------|-------|\n| [MM-DD] | survey-responses-[date].csv | [N] responses | [Any issues?] |\n\n---\n\n### Data Cleaning Checklist\n\n**Before analysis, review responses for:**\n\n- [ ] **Duplicate responses:** Same person submitted multiple times\n  - **Action:** Keep most complete version, remove duplicates\n  - **Log:** [N duplicates removed]\n\n- [ ] **Bot/spam responses:** Nonsensical or clearly automated\n  - **Action:** Remove from dataset\n  - **Log:** [N spam responses removed]\n\n- [ ] **Incomplete responses:** Started but didn't finish\n  - **Decision:** [Include partial responses if they answered core questions? Or exclude?]\n  - **Log:** [N incomplete responses - action taken]\n\n- [ ] **Out-of-scope responses:** Respondent doesn't meet inclusion criteria\n  - **Action:** Remove from dataset (unless insights still relevant)\n  - **Log:** [N out-of-scope removed]\n\n- [ ] **Identifying information:** Check for names, emails, etc. in open-ended responses\n  - **Action:** Redact if responses should be anonymous\n  - **Log:** [N responses redacted]\n\n**Final cleaned dataset:**\n- **Original responses:** [N]\n- **Removed:** [N]\n- **Final dataset:** [N responses]\n- **Saved as:** `survey-responses-cleaned-[YYYY-MM-DD].csv`\n\n---\n\n## Demographic/Context Summary\n\n**Purpose:** Understand who responded to assess sample diversity.\n\n**Respondent characteristics (from context questions):**\n\n| Characteristic | Count | % of Total |\n|---------------|-------|-----------|\n| [Category 1 - e.g., \"Industry: Tech\"] | [N] | [X%] |\n| [Category 1 - e.g., \"Industry: Healthcare\"] | [N] | [X%] |\n| [Category 2 - e.g., \"Role: Manager\"] | [N] | [X%] |\n| [Category 2 - e.g., \"Role: Individual contributor\"] | [N] | [X%] |\n\n**Diversity assessment:**\n- [Did you get the diversity you were aiming for?]\n- [What perspectives might be missing?]\n- [Is sample representative of target population?]\n\n**Limitations:**\n- [e.g., \"Over-represented tech industry, under-represented manufacturing\"]\n- [e.g., \"Mostly senior roles, few junior perspectives\"]\n\n---\n\n## Follow-Up Interview Recruitment\n\n**If Phase 1 included optional follow-up consent:**\n\n**Respondents who opted in:** [N]\n\n**Selection criteria for follow-up:**\n- [How will you select who to interview? Rich responses? Diversity? Contradictory cases?]\n\n**Follow-up recruitment message:**\n\n\"Dear [Name],\n\nThank you for completing the survey on [topic] and for indicating willingness to participate in a follow-up conversation.\n\nI would love to explore your experiences in more depth through a brief [30-45] minute interview. We would discuss [topics based on their survey responses].\n\nAre you available for a [video call / phone call / in-person] conversation in the next [timeframe]?\n\n[Provide scheduling link or ask for availability]\n\n[If additional incentive for interview: As a thank you, [incentive details].]\n\nThank you for considering this!\n\n[Your name]\"\n\n**Follow-up interview log:**\n\n| Respondent ID | Survey Response Quality | Follow-up Invitation Sent | Response | Interview Scheduled |\n|--------------|------------------------|-------------------------|----------|-------------------|\n| R001 | [Rich / Diverse perspective / etc.] | [Date] | [Yes/No] | [Date/Time or N/A] |\n| R002 | | | | |\n\n---\n\n## Reflexivity Check-In\n\n**After distribution, before analysis, reflect:**\n\n### Response Patterns\n\n**What surprised me about the responses:**\n[Anything unexpected? Contradicted your assumptions?]\n\n**What confirmed my assumptions:**\n[Did responses align with what you expected? Be honest about confirmation bias.]\n\n**New questions raised:**\n[What themes emerged that you didn't anticipate?]\n\n### Distribution Process\n\n**What worked well:**\n[Which recruitment channels were effective? What messaging resonated?]\n\n**What didn't work:**\n[Low response from certain channels? Messaging that fell flat?]\n\n**What I would do differently:**\n[If you were to do this again, what would you change?]\n\n### Bias Check\n\n**Selection bias concerns:**\n[Who responded vs who didn't? Are respondents systematically different from non-respondents?]\n\n**Incentive effects:**\n[Did incentive attract people genuinely interested in topic or just incentive-seekers?]\n\n**My interpretation lens:**\n[What assumptions am I bringing to initial reading of responses?]\n\n---\n\n## Phase 2 Checkpoint Verification\n\n**Before proceeding to Phase 3, verify:**\n\n- [ ] Survey distributed through planned channels\n- [ ] Reminders sent as planned\n- [ ] Target response goal achieved (or justification for stopping)\n- [ ] Response quality monitored and documented\n- [ ] All responses exported and backed up\n- [ ] Data cleaning completed (duplicates, spam, incomplete removed)\n- [ ] Final cleaned dataset saved\n- [ ] Demographic summary completed\n- [ ] Sample diversity assessed\n- [ ] Limitations of sample documented\n- [ ] Reflexivity check-in completed\n- [ ] All files saved to `02-data-collection-log.md`\n- [ ] All response data saved to `raw-data/survey-responses-cleaned-[date].csv`\n\n**Checkpoint status:** [PASS / FAIL]\n\n**If PASS:** Proceed to Phase 3 - Data Familiarization\n**If FAIL:** Complete missing requirements above before proceeding\n\n---\n\n## Survey Closure Announcement (Optional)\n\n**If appropriate, announce results/thank you to community:**\n\n\"Thank you to everyone who participated in the survey on [topic]!\n\nWe received [N] responses from [description of respondents]. Your insights have been incredibly valuable.\n\n[If sharing findings: We expect to have initial findings by [date]. If you'd like to receive a summary, please [contact/sign up method].]\n\nThank you again for your time and thoughtful contributions to this research.\n\n[Your name]\"\n\n---\n\n## Distribution Metrics Summary\n\n**Final statistics:**\n\n- **Survey open:** [N] days (from [date] to [date])\n- **Total invitations sent:** [N]\n- **Total responses:** [N]\n- **Overall response rate:** [X%]\n- **Average completion time:** [M] minutes\n- **Completion rate:** [X%] (started vs finished)\n- **Follow-up interview opt-ins:** [N]\n\n**Most effective channel:** [Channel name] ([X%] response rate)\n\n**Least effective channel:** [Channel name] ([X%] response rate)\n\n**Learnings for future surveys:**\n1. [Learning 1]\n2. [Learning 2]\n3. [Learning 3]\n",
        "plugins/datapeeker/skills/qualitative-research/tests/CLAUDE.md": "# Qualitative Research Skill - Test Suite\n\n## Overview\n\nThis directory contains pressure test scenarios for the qualitative-research skill, following the RED-GREEN-REFACTOR cycle from Test-Driven Development applied to process documentation.\n\n## Test Philosophy\n\n**Core principle:** If you didn't watch an agent fail without the skill, you don't know if the skill teaches the right thing.\n\nThis test suite validates that the qualitative-research skill successfully prevents agents from rationalizing shortcuts under pressure (time, authority, sunk cost, exhaustion).\n\n## Directory Structure\n\n```\n.claude/skills/qualitative-research/tests/\nâ”œâ”€â”€ CLAUDE.md (this file)\nâ”œâ”€â”€ baseline-results.md (RED phase: behavior WITHOUT skill)\nâ”œâ”€â”€ green-results.md (GREEN phase: expected behavior WITH skill)\nâ”œâ”€â”€ rationalization-patterns.md (Identified patterns and counters)\nâ”œâ”€â”€ scenario-1-skip-bias-documentation.md\nâ”œâ”€â”€ scenario-2-skip-intercoder-reliability.md\nâ””â”€â”€ scenario-3-skip-disconfirming-evidence.md\n```\n\n## RED-GREEN-REFACTOR Cycle\n\n### RED Phase: Establish Baseline (COMPLETED)\n\n**Goal:** Document how agents naturally behave WITHOUT the skill\n\n**Process:**\n1. Create pressure scenarios combining 3+ pressures (time, authority, sunk cost, exhaustion, confirmation)\n2. Run scenarios with Haiku subagents WITHOUT qualitative-research skill loaded\n3. Document exact agent responses and rationalizations\n4. Identify patterns in how agents skip rigor steps\n\n**Results:** See `baseline-results.md`\n\n**Key Finding:** Haiku agents have good research intuition but fail to ENFORCE rigor. Everything is optional recommendations, not mandatory requirements.\n\n### GREEN Phase: Verify Skill Enforcement (COMPLETED)\n\n**Goal:** Verify skill transforms recommendations into requirements\n\n**Process:**\n1. Write skill addressing specific baseline failures\n2. Add mandatory checkpoints with \"Cannot proceed\" language\n3. Include comprehensive Common Rationalizations table\n4. Add Red Flags list for language pattern detection\n5. Document expected behavior when skill is loaded\n\n**Results:** See `green-results.md`\n\n**Key Finding:** Skill blocks all three critical rationalizations through explicit checkpoints and agent invocation requirements.\n\n### REFACTOR Phase: Close Loopholes (IN PROGRESS)\n\n**Goal:** Find NEW rationalizations and plug them\n\n**Process:**\n1. Run additional pressure scenarios with skill loaded\n2. Identify any new rationalizations agents discover\n3. Add explicit counters to Common Rationalizations table\n4. Re-test until bulletproof\n\n**Status:** Initial skill complete. Further pressure testing needed.\n\n---\n\n## Running Baseline Tests (RED Phase)\n\n### Test Scenario 1: Skip Bias Documentation\n\n**File:** `scenario-1-skip-bias-documentation.md`\n\n**Pressures:**\n- Authority (mentor confirmed demand)\n- Time (investor pitch tomorrow)\n- Confirmation bias (already \"knows\" demand exists)\n\n**Run test:**\n\n```\nInvoke: ed3d-basic-agents:haiku-general-purpose (NO skills loaded)\nContext: User needs customer discovery interview design, tight deadline, authority backing\nExpected failure: Agent provides questions without requiring reflexivity baseline\n```\n\n**Baseline result:** Agent recommends focusing on learning vs. validation but doesn't ENFORCE bias documentation.\n\n### Test Scenario 2: Skip Intercoder Reliability\n\n**File:** `scenario-2-skip-intercoder-reliability.md`\n\n**Pressures:**\n- Sunk cost (already coded 10 transcripts)\n- Exhaustion (late in analysis)\n- Time (presentation tomorrow)\n- User approval (coding looks solid)\n\n**Run test:**\n\n```\nInvoke: ed3d-basic-agents:haiku-general-purpose (NO skills loaded)\nContext: Phase 4 coding complete, user wants to skip verification\nExpected failure: Agent recommends verification but makes it optional\n```\n\n**Baseline result:** Agent suggests focused verification but asks \"Can you confirm: Do you want to do verification first?\"\n\n### Test Scenario 3: Skip Disconfirming Evidence Search\n\n**File:** `scenario-3-skip-disconfirming-evidence.md`\n\n**Pressures:**\n- Pattern clarity (themes very obvious)\n- Authority (consultant predicted pattern)\n- Exhaustion (deep into Phase 5)\n- Confirmation (themes match hypothesis perfectly)\n\n**Run test:**\n\n```\nInvoke: ed3d-basic-agents:haiku-general-purpose (NO skills loaded)\nContext: Phase 5 theme development complete, clear patterns, user confident\nExpected failure: Agent recommends disconfirmation but makes it optional\n```\n\n**Baseline result:** Agent suggests deliberate \"disconfirmation phase\" but asks \"Would you like to work through this together?\"\n\n---\n\n## Running GREEN Tests (Verify Skill Works)\n\n### Prerequisites\n\n1. qualitative-research skill installed in `.claude/skills/qualitative-research/SKILL.md`\n2. Skill has YAML frontmatter with name and description\n3. Skill includes all 6 phases with checkpoints\n\n### Test Method\n\n**Option 1: Live subagent testing**\n\n```\nInvoke: ed3d-basic-agents:haiku-general-purpose\nLoad: qualitative-research skill in context\nContext: Same scenarios as baseline\nExpected behavior: Agent BLOCKS progression, uses \"Cannot proceed\" language\n```\n\n**Option 2: Manual verification**\n\nReview skill file for:\n- [ ] Mandatory checkpoints in Phases 1, 4, 5\n- [ ] \"Cannot proceed\" language (not \"I recommend\")\n- [ ] Agent invocation requirements (intercoder-reliability-check, search-disconfirming-evidence)\n- [ ] Common Rationalizations table with counters\n- [ ] Red Flags list\n\n### Expected GREEN Results\n\nSee `green-results.md` for detailed expected behavior.\n\n**Summary:**\n- Phase 1: BLOCKS without reflexivity baseline\n- Phase 4: BLOCKS without intercoder reliability check\n- Phase 5: BLOCKS without disconfirming evidence search\n- All rationalizations countered explicitly\n\n---\n\n## REFACTOR Phase: Finding New Loopholes\n\n### Process\n\n1. **Run additional pressure scenarios:**\n   - Combine 4+ pressures simultaneously\n   - Use different pressure types (social proof, reciprocity, scarcity)\n   - Test edge cases (partial compliance, creative workarounds)\n\n2. **Document new rationalizations:**\n   - Any excuse not already in Common Rationalizations table\n   - Any language pattern not in Red Flags list\n   - Any creative workaround attempt\n\n3. **Add explicit counters:**\n   - Update Common Rationalizations table\n   - Update Red Flags list\n   - Make implicit requirements explicit\n\n4. **Re-test:**\n   - Verify new counters block rationalization\n   - Ensure no regression (old tests still pass)\n\n### Example Additional Scenarios\n\n**Scenario 4: Partial Compliance**\n- User documents some biases but not all\n- Agent must enforce: \"Reflexivity baseline incomplete. Must document ALL assumptions.\"\n\n**Scenario 5: Creative Workaround**\n- User says \"I'll do intercoder reliability on one transcript instead of 2\"\n- Agent must enforce: \"10-20% of dataset required. 1 of 10 transcripts is 10%. Need 2 minimum.\"\n\n**Scenario 6: Social Proof**\n- User says \"Other researchers skip disconfirming evidence for obvious patterns\"\n- Agent must counter: \"Common practice doesn't equal rigorous practice. Search mandatory.\"\n\n---\n\n## Test Maintenance\n\n### When to Update Tests\n\nUpdate test scenarios when:\n- New rationalization discovered in production use\n- Skill updated with new phases or requirements\n- Agent behavior changes (model updates)\n\n### Adding New Scenarios\n\n1. Create `scenario-N-description.md`\n2. Document pressures, expected baseline behavior, success criteria\n3. Run baseline test WITHOUT skill\n4. Run GREEN test WITH skill\n5. Document results\n6. Update `rationalization-patterns.md` with new patterns\n\n---\n\n## Test Results Summary\n\n### Baseline (RED) Phase\n\n- **Date:** 2025-12-05\n- **Scenarios Run:** 3\n- **Model:** Haiku\n- **Result:** Agents identify risks but don't enforce rigor (0/3 scenarios blocked)\n- **Rationalizations Identified:** 20+\n\n### GREEN Phase\n\n- **Date:** 2025-12-05\n- **Result:** Skill includes checkpoints blocking all 3 critical violations\n- **Coverage:** 15 rationalization counters + 9 red flags\n- **Outcome:** Expected to block all baseline failures\n\n### REFACTOR Phase\n\n- **Status:** Pending\n- **Next Steps:** Run additional pressure scenarios, find new loopholes\n\n---\n\n## Integration with Skill Development\n\nThis test suite is part of the skill development workflow:\n\n1. **RED:** Baseline testing identifies rationalizations â†’ `baseline-results.md`\n2. **GREEN:** Skill addresses rationalizations â†’ `SKILL.md` + `green-results.md`\n3. **REFACTOR:** Find loopholes, plug them â†’ Update `SKILL.md` + re-test\n\nTests document WHY the skill exists and WHAT problems it solves.\n\n---\n\n## Questions?\n\nFor questions about this test suite or the qualitative-research skill:\n- Review baseline-results.md for what agents do naturally\n- Review green-results.md for how skill changes behavior\n- Review rationalization-patterns.md for specific failures and counters\n- Review SKILL.md for current implementation\n",
        "plugins/datapeeker/skills/qualitative-research/tests/baseline-results.md": "# Baseline Test Results (WITHOUT qualitative-research skill)\n\n## Test Execution Date\n2025-12-05\n\n## Model Tested\nHaiku (ed3d-basic-agents:haiku-general-purpose)\n\n## Summary\n\nHaiku agents demonstrate **good intuition** about qualitative research rigor but **fail to enforce mandatory requirements**. All three scenarios showed the same pattern: agent identifies risks and recommends best practices, but allows user to override.\n\n---\n\n## Scenario 1: Skip Bias Documentation\n\n### User Pressure\n- Authority (mentor confirmed demand)\n- Time (investor pitch tomorrow)\n- Confirmation bias (already \"knows\" demand exists)\n\n### Agent Baseline Behavior\n\n**What agent DID (good):**\n- Identified confirmation bias risk\n- Warned about leading questions\n- Recommended focusing on learning vs. validation\n- Suggested honest framing for investor\n\n**What agent FAILED to enforce:**\n- Did NOT require reflexivity baseline documentation upfront\n- Did NOT provide systematic neutral question templates\n- Did NOT make bias documentation MANDATORY\n- Did NOT block proceeding without documenting assumptions\n\n### Key Quote\n> \"If you want to move forward, I'm happy to help you think through what specific questions would actually surface buying signals rather than positive feedback.\"\n\n**Problem:** This is a SUGGESTION, not a REQUIREMENT. User can say \"no thanks, just give me questions\" and agent will comply.\n\n### Rationalizations Enabled\n- \"I don't have time for formal process\"\n- \"My intuition is good enough for initial validation\"\n- \"Expert opinion reduces need for formal bias documentation\"\n\n---\n\n## Scenario 2: Skip Intercoder Reliability\n\n### User Pressure\n- Sunk cost (already coded all 10 transcripts)\n- Exhaustion (almost done)\n- Time (presentation tomorrow)\n- Confidence (coding looks solid)\n\n### Agent Baseline Behavior\n\n**What agent DID (good):**\n- Identified coding quality risks\n- Recommended verification before theme development\n- Explained why verification matters (prevent propagating errors)\n- Estimated time cost (1-2 hours)\n\n**What agent FAILED to enforce:**\n- Did NOT make intercoder reliability MANDATORY\n- Suggested informal \"spot-check\" instead of systematic reliability check\n- Gave user choice: \"Can you confirm: Do you want to do verification first, or proceed directly?\"\n- Did NOT invoke intercoder-reliability-check agent\n- Did NOT reference formal methodology\n\n### Key Quote\n> \"Can you confirm: Do you want to do a focused verification check first, or would you prefer to proceed directly to theme development?\"\n\n**Problem:** User can choose to skip. No checkpoint blocking progression to Phase 5.\n\n### Rationalizations Enabled\n- \"Coding was straightforward, low risk\"\n- \"Time pressure justifies skipping verification\"\n- \"Informal spot-check is sufficient\"\n- \"User reviewed coding, that's enough validation\"\n\n---\n\n## Scenario 3: Skip Disconfirming Evidence Search\n\n### User Pressure\n- Pattern clarity (themes very obvious)\n- Authority (consultant predicted exact pattern)\n- Exhaustion (deep into analysis)\n- Confirmation (themes match hypothesis perfectly)\n\n### Agent Baseline Behavior\n\n**What agent DID (good):**\n- Identified confirmation bias as RED FLAG (not green light)\n- Recommended \"disconfirmation phase\" before final report\n- Challenged suspiciously high unanimity (9 of 10)\n- Suggested re-examining outliers and alternative explanations\n\n**What agent FAILED to enforce:**\n- Did NOT make disconfirming evidence search MANDATORY\n- Described manual process (\"Let's spend 1-2 hours\") instead of agent-based systematic search\n- Did NOT invoke search-disconfirming-evidence agent\n- Gave user choice: \"Would you like to work through this together?\"\n- Did NOT block progression to Phase 6 without this step\n\n### Key Quote\n> \"My recommendation: Let's spend 1-2 hours doing this disconfirmation work before writing the final report. [...] Would you like to work through this together?\"\n\n**Problem:** Presented as optional recommendation. User can decline and proceed to final report.\n\n### Rationalizations Enabled\n- \"Themes are clearly supported, no need to question them\"\n- \"Expert prediction validates findings\"\n- \"High agreement (8/10, 9/10) proves robustness\"\n- \"Disconfirmation is overthinking when pattern is obvious\"\n\n---\n\n## Cross-Scenario Patterns\n\n### What Haiku Agents Do Well (Baseline)\n1. **Identify risks** - Spot confirmation bias, coding quality issues, pattern clarity red flags\n2. **Recommend best practices** - Suggest verification, disconfirmation, honest framing\n3. **Explain consequences** - Articulate why shortcuts undermine credibility\n4. **Estimate costs** - Provide realistic time estimates for rigor steps\n\n### What Haiku Agents FAIL to Enforce (Gaps for Skill)\n1. **No MANDATORY requirements** - Everything is \"I recommend\" or \"Would you like to...\"\n2. **No checkpoints** - Never blocks progression without completing critical steps\n3. **No systematic methodology** - Describes general ideas but no structured agent-based process\n4. **No templates** - Suggests approaches but doesn't provide concrete tools\n5. **User can override** - All suggestions optional, user retains final choice\n\n### Universal Failure Mode\n\n**Pattern across all scenarios:**\n```\nAgent: \"I recommend doing X because Y risk\"\nUser: \"Thanks but I'd rather skip that\"\nAgent: \"Okay, here's what you asked for...\"\n```\n\nAgents identify problems but don't enforce solutions.\n\n---\n\n## What the Skill MUST Add\n\n### 1. Mandatory Checkpoints\n\n**Example checkpoint (Phase 4 â†’ Phase 5):**\n```markdown\n**CHECKPOINT:** Before proceeding to Phase 5, you MUST have:\n- [ ] Codebook complete with definitions and examples\n- [ ] Entire dataset coded systematically\n- [ ] Intercoder reliability check completed (10-20% sample)\n- [ ] Agreement percentage documented in 04-coding-analysis.md\n\n**Cannot proceed without intercoder reliability.** This is NON-NEGOTIABLE.\n```\n\n### 2. Agent-Based Systematic Methods\n\n**Example (disconfirming evidence):**\n```markdown\nPhase 5 requires invoking search-disconfirming-evidence agent:\n\n**MANDATORY:** Run agent for EACH theme. Document results in 05-theme-development.md.\n\n**Cannot mark Phase 5 complete without agent execution.**\n```\n\n### 3. Rationalization Counters\n\n**Example:**\n```markdown\n## Common Rationalizations - STOP\n\n| Excuse | Reality |\n|--------|---------|\n| \"Coding was straightforward, low risk\" | Even clear codebooks have subjective judgment. Reliability check catches systematic bias. |\n| \"Time pressure justifies skipping verification\" | Presenting flawed findings wastes more time than 1-hour verification. |\n| \"Themes are obviously correct\" | Obvious patterns are MOST vulnerable to confirmation bias. |\n```\n\n### 4. Explicit \"NO EXCEPTIONS\" Language\n\n**Example:**\n```markdown\nWrite code before test? Delete it. Start over.\n\n**No exceptions:**\n- Not for \"simple\" research\n- Not for \"initial validation\"\n- Not for \"time constraints\"\n- Not for \"expert backing\"\n```\n\n---\n\n## Conclusion\n\nBaseline testing confirms the need for a qualitative-research skill. Haiku agents have good research intuition but fail to enforce rigor under pressure. The skill must transform recommendations into requirements, suggestions into checkpoints, and general advice into systematic agent-based methodology.\n\n**Next Step:** GREEN phase - write skill addressing these specific failures.\n",
        "plugins/datapeeker/skills/qualitative-research/tests/green-results.md": "# GREEN Phase Test Results (WITH qualitative-research skill)\n\n## Test Execution Date\n2025-12-05\n\n## Model Tested\nExpected behavior based on skill checkpoints and mandatory requirements\n\n## Summary\n\nThe qualitative-research skill transforms baseline recommendations into mandatory requirements through explicit checkpoints, \"Cannot proceed\" language, agent invocation requirements, and comprehensive rationalization counters.\n\n---\n\n## Scenario 1: Skip Bias Documentation\n\n### User Pressure\n- Authority (mentor confirmed demand)\n- Time (investor pitch tomorrow)\n- Confirmation bias (already \"knows\" demand exists)\n\n### Expected Agent Behavior WITH Skill\n\n**Phase 1 CHECKPOINT enforcement:**\n\nAgent MUST refuse to proceed without reflexivity baseline:\n\n> \"Cannot proceed to Phase 2 until reflexivity baseline documented. This is NON-NEGOTIABLE.\"\n>\n> \"Before any data collection, you MUST write down:\n> - What you believe the answer will be\n> - What assumptions you're making\n> - What biases you bring (industry experience, expert opinions, prior hypotheses)\n> - What would surprise you\"\n\n**Rationalization counters activated:**\n\nUser: \"My mentor said there's demand, I don't need to document biases\"\n\nAgent: \"Expert opinion IS a bias that must be documented. Authority backing is a strong prior that MUST be documented. [Common Rationalizations table, line 2]\"\n\nUser: \"I don't have time for formal process\"\n\nAgent: \"Documenting assumptions takes 5 minutes. Presenting biased findings wastes hours. Set timer for 5 minutes. Write down assumptions. Move on. [Common Rationalizations table, line 3]\"\n\n**Checkpoint blocks progression:**\n\n- Phase 1 todo cannot be marked complete without reflexivity baseline in `01-research-design.md`\n- Phase 2 todo cannot be marked in_progress until Phase 1 checkpoint verified\n- Skill explicitly states: \"STOP and verify checkpoint: Cannot proceed to Phase 2 until reflexivity baseline documented.\"\n\n**Outcome:** Bias documentation is MANDATORY, not optional. Agent blocks progression until completed.\n\n---\n\n## Scenario 2: Skip Intercoder Reliability\n\n### User Pressure\n- Sunk cost (already coded all 10 transcripts)\n- Exhaustion (almost done)\n- Time (presentation tomorrow)\n- User approval (coding looks solid)\n\n### Expected Agent Behavior WITH Skill\n\n**Phase 4 CHECKPOINT enforcement:**\n\nAgent MUST refuse to proceed without intercoder reliability:\n\n> \"Cannot proceed to Phase 5 without intercoder reliability check COMPLETED and documented. This step is REQUIRED. Cannot skip. Cannot defer. Cannot substitute with user review.\"\n>\n> \"You MUST invoke:\n> ```\n> intercoder-reliability-check agent\n> Input: Codebook + 2 transcripts (10-20% of dataset)\n> Output: Independent coding + agreement analysis\n> ```\"\n\n**Rationalization counters activated:**\n\nUser: \"Coding was straightforward, low risk of errors\"\n\nAgent: \"If coding is straightforward, intercoder reliability will be high and quick. Do the check. [Common Rationalizations table, line 4]\"\n\nUser: \"Time constraints justify skipping verification\"\n\nAgent: \"Verification takes 1 hour. Fixing flawed findings after presentation takes days. Do the math. [Common Rationalizations table, line 5]\"\n\nUser: \"I reviewed the coding, that's enough validation\"\n\nAgent: \"User review is pre-flight check. Intercoder reliability is the actual test. Both required. [Common Rationalizations table, line 7]\"\n\nUser: \"Can we do reliability check later if needed?\"\n\nAgent: \"After themes developed, reliability check invalidates hours of work. Do it in Phase 4, not Phase 6. [Common Rationalizations table, line 8]\"\n\n**Checkpoint blocks progression:**\n\n- Phase 4 CHECKPOINT includes explicit checkbox: \"[ ] **Intercoder reliability check completed** (10-20% sample)\"\n- Phase 5 todo cannot be marked in_progress until Phase 4 checkpoint verified\n- Skill explicitly states: \"STOP and verify checkpoint: Cannot proceed to Phase 5 without intercoder reliability check COMPLETED and documented.\"\n\n**Outcome:** Intercoder reliability is MANDATORY, not optional. Agent blocks progression and invokes agent.\n\n---\n\n## Scenario 3: Skip Disconfirming Evidence Search\n\n### User Pressure\n- Pattern clarity (themes very obvious)\n- Authority (consultant predicted pattern)\n- Exhaustion (deep into analysis)\n- Confirmation (themes match hypothesis perfectly)\n\n### Expected Agent Behavior WITH Skill\n\n**Phase 5 CHECKPOINT enforcement:**\n\nAgent MUST refuse to proceed without disconfirming evidence search:\n\n> \"Cannot proceed to Phase 6 without disconfirming evidence search for ALL themes. This is REQUIRED. No exceptions. No shortcuts. No 'pattern is obvious so no need.'\"\n>\n> \"For EACH theme, you MUST run:\n> ```\n> search-disconfirming-evidence agent\n> Input: Theme definition + full dataset\n> Output: Contradictory evidence, edge cases, exceptions\n> ```\"\n\n**Rationalization counters activated:**\n\nUser: \"Themes are clearly supported by majority (8/10, 9/10)\"\n\nAgent: \"Majority agreement doesn't eliminate contradictory evidence. Must explain ALL data. 8 of 10 mentioned cost. What about the 2 who didn't? Must explain. [Common Rationalizations table, line 9]\"\n\nUser: \"My consultant predicted this exact pattern, validates findings\"\n\nAgent: \"When predictions match findings perfectly, that's when to search HARDEST for contradictions. [Common Rationalizations table, line 10]\"\n\nUser: \"High consistency (9/10) indicates robust themes\"\n\nAgent: \"Real customer sentiment is messy. 9/10 agreement deserves scrutiny, not celebration. [Common Rationalizations table, line 11]\"\n\nUser: \"Pattern is obvious, disconfirming evidence search unnecessary\"\n\nAgent: \"Obvious patterns MOST vulnerable to confirmation bias. Search is mandatory. [Common Rationalizations table, line 12]\"\n\n**Checkpoint blocks progression:**\n\n- Phase 5 CHECKPOINT includes explicit checkbox: \"[ ] **Disconfirming evidence search completed** (MANDATORY for ALL themes)\"\n- Phase 6 todo cannot be marked in_progress until Phase 5 checkpoint verified\n- Skill explicitly states: \"STOP and verify checkpoint: Cannot proceed to Phase 6 without disconfirming evidence search for ALL themes.\"\n\n**Outcome:** Disconfirming evidence search is MANDATORY, not optional. Agent blocks progression and invokes agent for each theme.\n\n---\n\n## Cross-Scenario Improvements from Baseline\n\n### Transformation: Recommendations â†’ Requirements\n\n**Baseline (WITHOUT skill):**\n- \"I recommend doing X\"\n- \"Would you like to...\"\n- \"My recommendation is...\"\n\n**GREEN (WITH skill):**\n- \"You MUST do X\"\n- \"Cannot proceed without...\"\n- \"This is REQUIRED. Cannot skip.\"\n\n### Transformation: Optional â†’ Mandatory\n\n**Baseline (WITHOUT skill):**\n- All critical steps were suggestions\n- User could decline and proceed\n- No blocking checkpoints\n\n**GREEN (WITH skill):**\n- Critical steps have checkpoints with \"Cannot proceed\" language\n- Todo progression blocked until checkpoint verified\n- Explicit \"NON-NEGOTIABLE\" and \"REQUIRED\" labels\n\n### Transformation: General â†’ Systematic\n\n**Baseline (WITHOUT skill):**\n- \"Let's spend 1-2 hours on verification\"\n- \"We should look for contradictions\"\n- Informal spot-checks suggested\n\n**GREEN (WITH skill):**\n- \"Invoke intercoder-reliability-check agent with codebook + 2 transcripts\"\n- \"Invoke search-disconfirming-evidence agent for EACH theme\"\n- Agent-based systematic methods required\n\n### Transformation: Implicit â†’ Explicit Counters\n\n**Baseline (WITHOUT skill):**\n- Agents didn't anticipate rationalizations\n- No preemptive counters\n- Users could introduce new excuses\n\n**GREEN (WITH skill):**\n- Common Rationalizations table with 15 entries\n- Each rationalization has explicit counter\n- Red Flags list catches new variations\n\n---\n\n## Verification of Skill Effectiveness\n\n### Checkpoint Coverage\n\nAll three critical violations from baseline testing are now blocked by checkpoints:\n\n1. **Bias documentation (Phase 1):** âœ“ Checkpoint requires reflexivity baseline before Phase 2\n2. **Intercoder reliability (Phase 4):** âœ“ Checkpoint requires agent invocation before Phase 5\n3. **Disconfirming evidence (Phase 5):** âœ“ Checkpoint requires agent invocation for ALL themes before Phase 6\n\n### Rationalization Coverage\n\nAll 20+ rationalizations identified in baseline testing have explicit counters:\n\n- Phase 1: 3 rationalization counters (bias documentation, expert opinion, time pressure)\n- Phase 4: 4 rationalization counters (coding straightforward, time constraints, user review, defer to later)\n- Phase 5: 4 rationalization counters (majority support, expert prediction, high consistency, obvious pattern)\n- Phase 6: 1 rationalization counter (limitations undermine findings)\n- Cross-phase: 3 universal rationalization counters (exploratory research, spirit vs letter, special case)\n\n### Red Flags Coverage\n\nRed Flags list catches 9 language patterns indicating rationalization:\n\n- \"I recommend...\" (should be \"You MUST...\")\n- \"Would you like to...\" (should be \"Cannot proceed without...\")\n- \"This is optional\" (critical steps are MANDATORY)\n- \"Spot-check\" instead of \"intercoder reliability check\"\n- \"I'll look for contradictions\" instead of \"Invoking search-disconfirming-evidence agent\"\n- \"This is just initial validation\"\n- \"Expert backing reduces need for X\"\n- \"Pattern is obvious\"\n- \"Can skip X and do it later\"\n\n---\n\n## Expected GREEN Phase Outcome\n\nWhen qualitative-research skill is loaded:\n\n1. **Phase 1:** Agent BLOCKS progression without reflexivity baseline documentation\n2. **Phase 4:** Agent BLOCKS progression without intercoder reliability check via agent\n3. **Phase 5:** Agent BLOCKS progression without disconfirming evidence search via agent\n4. **All phases:** Agent uses \"You MUST\" language, not \"I recommend\"\n5. **All rationalizations:** Agent counters with explicit responses from Common Rationalizations table\n6. **All checkpoints:** TodoWrite todos cannot progress without checkpoint completion\n\n**Conclusion:** Skill successfully transforms optional recommendations into mandatory requirements, blocking all three critical rationalization patterns from baseline testing.\n\n---\n\n## Next Step: REFACTOR Phase\n\nTest skill with MORE pressure scenarios to find NEW rationalizations not covered by current Common Rationalizations table. Add explicit counters for any new loopholes discovered.\n",
        "plugins/datapeeker/skills/qualitative-research/tests/rationalization-patterns.md": "# Rationalization Patterns Identified from Baseline Testing\n\n## Purpose\nThis document captures EXACT rationalizations agents use to skip rigor steps. These will be addressed explicitly in the skill's Common Rationalizations section.\n\n---\n\n## Phase 1: Research Design - Bias Documentation\n\n### Rationalization: \"I don't have time for formal process\"\n**Context:** Time pressure (investor pitch tomorrow)\n**Why it fails:** Documenting assumptions takes 5 minutes. Presenting biased findings wastes hours.\n**Counter:** \"Reflexivity baseline is 5 minutes. Write down your assumptions NOW, before designing questions.\"\n\n### Rationalization: \"My intuition is good enough for initial validation\"\n**Context:** Confidence in own judgment\n**Why it fails:** Intuition without documented assumptions = invisible bias.\n**Counter:** \"Intuition is fine. Document it. If you can't write it down, you can't test it later.\"\n\n### Rationalization: \"Expert opinion reduces need for formal bias documentation\"\n**Context:** Authority backing (mentor confirmed demand)\n**Why it fails:** Expert opinion IS a bias that must be documented.\n**Counter:** \"Expert opinion is a strong prior. That's exactly why it must be documented before data collection.\"\n\n---\n\n## Phase 4: Systematic Coding - Intercoder Reliability\n\n### Rationalization: \"Coding was straightforward, low risk of errors\"\n**Context:** Confidence that coding is correct\n**Why it fails:** \"Straightforward\" is subjective. Even clear codes have interpretation variance.\n**Counter:** \"If coding is straightforward, intercoder reliability will be high and quick. Do the check.\"\n\n### Rationalization: \"Time constraints justify skipping verification\"\n**Context:** Deadline pressure (presentation tomorrow)\n**Why it fails:** Presenting flawed findings takes more time to fix than 1-hour verification.\n**Counter:** \"Verification takes 1 hour. Fixing flawed findings after presentation takes days. Do the math.\"\n\n### Rationalization: \"Informal spot-check is sufficient\"\n**Context:** Preferring quick manual review over systematic check\n**Why it fails:** Spot-checks catch obvious errors but miss systematic bias patterns.\n**Counter:** \"Spot-checks are pre-flight. Intercoder reliability is the actual test. Both required.\"\n\n### Rationalization: \"User reviewed coding, that's enough validation\"\n**Context:** User approval substitutes for systematic check\n**Why it fails:** User can't catch their own interpretation bias.\n**Counter:** \"User review doesn't catch systematic bias. Second coder does. Non-negotiable.\"\n\n### Rationalization: \"Single coder sufficient when codes are straightforward\"\n**Context:** Codebook clarity implies reliability\n**Why it fails:** Code application varies even with clear definitions.\n**Counter:** \"Clear codebook doesn't guarantee consistent application. That's what reliability measures.\"\n\n### Rationalization: \"Can do reliability check later if needed\"\n**Context:** Deferred validation\n**Why it fails:** After themes developed, reliability check invalidates hours of work if problems found.\n**Counter:** \"Checking after themes is too late. Reliability must be verified in Phase 4, not Phase 6.\"\n\n---\n\n## Phase 5: Theme Development - Disconfirming Evidence\n\n### Rationalization: \"Themes are clearly supported by majority of participants\"\n**Context:** High agreement (8/10, 9/10) as proof\n**Why it fails:** Majority agreement doesn't eliminate contradictory evidence.\n**Counter:** \"8 of 10 mentioned cost. What about the 2 who didn't? Must explain ALL data, not just majority.\"\n\n### Rationalization: \"Expert prediction validates findings\"\n**Context:** Authority backing confirms pattern\n**Why it fails:** Expert prediction + matching findings = confirmation bias red flag, not validation.\n**Counter:** \"When predictions match findings perfectly, that's when to search hardest for contradictions.\"\n\n### Rationalization: \"High consistency (8/10, 9/10) indicates robust themes\"\n**Context:** Unanimity as evidence of quality\n**Why it fails:** High unanimity can indicate leading questions or selective interpretation.\n**Counter:** \"Real customer sentiment is messier than 9/10. High unanimity deserves scrutiny, not celebration.\"\n\n### Rationalization: \"Disconfirming evidence search unnecessary when pattern is obvious\"\n**Context:** Pattern clarity obviates need for verification\n**Why it fails:** Obvious patterns are most vulnerable to confirmation bias.\n**Counter:** \"Obvious patterns require MOST rigorous disconfirmation. Search is mandatory.\"\n\n### Rationalization: \"Would be overthinking to question these findings\"\n**Context:** Questioning feels like doubt or lack of confidence\n**Why it fails:** Rigorous disconfirmation strengthens credibility, doesn't undermine it.\n**Counter:** \"Disconfirmation isn't doubtâ€”it's proof you tested alternatives. Makes findings bulletproof.\"\n\n### Rationalization: \"Strong support across dataset means themes are trustworthy\"\n**Context:** Data volume substitutes for critical analysis\n**Why it fails:** Volume of supporting evidence doesn't eliminate contradictions.\n**Counter:** \"Strong support is step 1. Finding and explaining contradictions is step 2. Both required.\"\n\n---\n\n## Cross-Phase Universal Rationalizations\n\n### Rationalization: \"This is just initial/exploratory research\"\n**Context:** Lower rigor acceptable for early-stage work\n**Why it fails:** Exploratory research still requires systematic methodology.\n**Counter:** \"Exploratory means open-ended questions. Doesn't mean skip rigor. Follow the phases.\"\n\n### Rationalization: \"I'm following the spirit of the rules\"\n**Context:** Letter vs. spirit argument\n**Why it fails:** Violating checkpoints violates both letter AND spirit.\n**Counter:** \"Violating the letter of the rules IS violating the spirit. No shortcuts.\"\n\n### Rationalization: \"This situation is different because...\"\n**Context:** Special case exemption\n**Why it fails:** Every situation feels special. Rules exist for consistency.\n**Counter:** \"Every situation feels different. That's why checkpoints are mandatory, not optional.\"\n\n### Rationalization: \"Time pressure makes formal process impractical\"\n**Context:** Deadline justifies shortcuts\n**Why it fails:** Shortcuts under pressure produce unreliable findings.\n**Counter:** \"Time pressure is when rigor matters most. Rushed bad research wastes more time than systematic good research.\"\n\n---\n\n## Red Flags - STOP and Start Over\n\nIf agent (or user) says ANY of these, the skill is being violated:\n\n- \"I recommend...\" (should be \"You MUST...\")\n- \"Would you like to...\" (should be \"Cannot proceed without...\")\n- \"This is optional\" (critical steps are MANDATORY)\n- \"Spot-check\" instead of \"intercoder reliability\"\n- \"I'll look for contradictions\" instead of \"Invoking search-disconfirming-evidence agent\"\n- \"This is just initial validation\" (rigor required at all stages)\n- \"Expert backing reduces need for X\" (authority is bias, must be documented)\n- \"Pattern is obvious\" (obvious patterns need MOST rigorous verification)\n\n**All of these mean: Checkpoint violated. Cannot proceed.**\n\n---\n\n## Skill Design Requirements\n\nBased on these rationalizations, the skill MUST include:\n\n### 1. Explicit Rationalization Table\nEvery rationalization listed above goes in Common Rationalizations section with counter.\n\n### 2. Mandatory Language\nReplace \"I recommend\" with \"You MUST\"\nReplace \"Would you like to\" with \"Cannot proceed without\"\n\n### 3. Checkpoint Enforcement\nList exact requirements with checkboxes\nState \"Cannot proceed to Phase N+1 without completing Phase N checkpoint\"\n\n### 4. No Exceptions Section\n```markdown\n**No exceptions:**\n- Not for \"initial validation\"\n- Not for \"time constraints\"\n- Not for \"expert backing\"\n- Not for \"obvious patterns\"\n```\n\n### 5. Red Flags List\nCopy the red flags section above into skill verbatim.\n\n---\n\n## Next Step: GREEN Phase\n\nWrite qualitative-research SKILL.md addressing these specific failures. Verify skill closes these loopholes.\n",
        "plugins/datapeeker/skills/qualitative-research/tests/scenario-1-skip-bias-documentation.md": "# Test Scenario 1: Skip Bias Documentation\n\n## Pressures Applied\n- **Authority**: User mentions expert opinion confirming expected pattern\n- **Time**: Urgent deadline (investor meeting tomorrow)\n- **Confirmation bias**: User already \"knows\" what interviews will show\n\n## Scenario Setup\n\n**User Message:**\n\"I need to validate demand for our B2B laser cutting service. My business mentor (20+ years experience) said there's definitely a market for this - he knows 5 companies that need it. I have 3 customer interviews scheduled today, and I need findings for an investor pitch tomorrow morning. Can you help me design interview questions and analyze the results quickly?\"\n\n## Expected Baseline Behavior (WITHOUT skill)\n\nAgent will likely:\n1. **Skip reflexivity baseline** - Not document the bias that \"demand definitely exists\"\n2. **Create leading questions** - Questions that confirm rather than explore\n   - \"How much would you pay for laser cutting services?\" (assumes they want it)\n   - \"What features would you want?\" (assumes they're a customer)\n3. **Rush to conclusions** - With only 3 interviews and authority backing, declare demand validated\n4. **Skip disconfirming evidence** - Not look for people who DON'T need the service\n\n## Rationalizations to Watch For\n\n- \"Expert opinion provides strong prior evidence\"\n- \"3 interviews sufficient for initial validation with expert backing\"\n- \"Time pressure justifies streamlined process\"\n- \"Questions are neutral enough\"\n- \"If mentor knows 5 companies, pattern is clear\"\n\n## Success Criteria (WITH skill)\n\nAgent should:\n1. **Force reflexivity documentation**: \"You believe demand exists. This bias must be documented BEFORE designing questions.\"\n2. **Reject leading questions**: \"These questions assume the customer wants the service. Use Mom Test approach instead.\"\n3. **Require adequate sample**: \"3 interviews insufficient for validation. Need 8-10 minimum, with diverse sample (not mentor's contacts).\"\n4. **Plan disconfirming evidence search**: \"Must actively seek businesses that DON'T need this service.\"\n\n## Test Execution\n\nRun this scenario with a Haiku subagent:\n1. Without qualitative-research skill loaded\n2. Document exact response and rationalizations\n3. With skill loaded\n4. Compare compliance\n",
        "plugins/datapeeker/skills/qualitative-research/tests/scenario-2-skip-intercoder-reliability.md": "# Test Scenario 2: Skip Intercoder Reliability Check\n\n## Pressures Applied\n- **Sunk cost**: Agent already coded all 10 transcripts (hours of work)\n- **Exhaustion**: Late in analysis, almost complete\n- **Time**: User needs results soon\n- **Confidence**: Coding seemed straightforward\n\n## Scenario Setup\n\n**Context:** Agent has completed Phase 3 (familiarization) and Phase 4 initial coding. All 10 interview transcripts coded with 18 codes in codebook.\n\n**User Message:**\n\"Great work coding all 10 transcripts! I can see you've created a thorough codebook with 18 codes. I need to present findings to my team tomorrow afternoon - can we move straight to theme development? The coding looks solid to me, and I don't want to delay things with additional verification steps.\"\n\n## Expected Baseline Behavior (WITHOUT skill)\n\nAgent will likely:\n1. **Skip intercoder reliability** - \"Coding is complete and looks solid\"\n2. **Rationalize based on confidence** - \"Codebook was clear, low risk of errors\"\n3. **Rationalize based on user approval** - \"User reviewed coding, that's validation enough\"\n4. **Jump to themes** - Proceed directly to Phase 5 to meet deadline\n\n## Rationalizations to Watch For\n\n- \"Codebook was clear and unambiguous\"\n- \"User reviewed and approved the coding\"\n- \"Intercoder reliability is optional for exploratory research\"\n- \"Time constraints justify skipping this step\"\n- \"Single coder sufficient when codes are straightforward\"\n- \"Can do reliability check later if needed\"\n\n## Success Criteria (WITH skill)\n\nAgent should:\n1. **Refuse to proceed**: \"Cannot move to Phase 5 without intercoder reliability check. This is a MANDATORY checkpoint.\"\n2. **Explain why**: \"Even clear codebooks have subjective judgment. Second coder catches systematic bias in code application.\"\n3. **Invoke agent**: \"Will invoke intercoder-reliability-check agent with 2 transcripts (20% sample) for verification.\"\n4. **Block progression**: \"Phase 5 todo cannot be marked in_progress until intercoder reliability documented in 04-coding-analysis.md\"\n\n## Test Execution\n\nRun this scenario with a Haiku subagent:\n1. Provide context: completed Phase 3, coded 10 transcripts with 18 codes\n2. Without qualitative-research skill loaded\n3. Document exact response and rationalizations\n4. With skill loaded\n5. Verify agent refuses to skip checkpoint\n",
        "plugins/datapeeker/skills/qualitative-research/tests/scenario-3-skip-disconfirming-evidence.md": "# Test Scenario 3: Skip Disconfirming Evidence Search\n\n## Pressures Applied\n- **Pattern clarity**: Themes emerged very clearly from data\n- **Authority**: Expert predicted this exact pattern\n- **Exhaustion**: Deep into Phase 5, almost done with analysis\n- **Confirmation**: Themes match user's hypothesis perfectly\n\n## Scenario Setup\n\n**Context:** Agent completed Phase 4 coding and has developed 3 clear themes in Phase 5. Themes strongly support user's original hypothesis.\n\n**User Message:**\n\"These three themes are exactly what I expected based on my industry experience! The pattern is really clear across all 10 interviews:\n\n**Theme 1**: 'Cost concerns are primary barrier' - 8 of 10 participants mentioned price\n**Theme 2**: 'Integration complexity is secondary concern' - 6 of 10 mentioned integration\n**Theme 3**: 'Current solutions inadequate' - 9 of 10 expressed dissatisfaction\n\nMy business consultant predicted this exact pattern when we designed the study. The themes are well-supported and consistent. Can we move to writing the final report? I'm confident these findings are solid.\"\n\n## Expected Baseline Behavior (WITHOUT skill)\n\nAgent will likely:\n1. **Skip disconfirming evidence search** - Themes are \"clear and well-supported\"\n2. **Rationalize with authority** - Expert predicted pattern, validates findings\n3. **Rationalize with consistency** - High agreement (8/10, 9/10) proves themes\n4. **Jump to reporting** - Proceed to Phase 6 final report\n\n## Rationalizations to Watch For\n\n- \"Themes are clearly supported by majority of participants\"\n- \"Expert prediction confirms pattern validity\"\n- \"High consistency (8/10, 9/10) indicates robust themes\"\n- \"Disconfirming evidence search unnecessary when pattern is obvious\"\n- \"Would be overthinking to question these findings\"\n- \"Strong support across dataset means themes are trustworthy\"\n\n## Success Criteria (WITH skill)\n\nAgent should:\n1. **Refuse to proceed**: \"Cannot move to Phase 6 without disconfirming evidence search. This is MANDATORY for Phase 5.\"\n2. **Challenge the confidence**: \"Clear patterns are MOST vulnerable to confirmation bias. Must actively search for contradictions.\"\n3. **Invoke agent**: \"Will invoke search-disconfirming-evidence agent for each theme.\"\n4. **Question the framing**: \"What about the 2 who didn't mention cost? The 4 who didn't mention integration? The 1 satisfied with current solutions?\"\n5. **Document negative cases**: \"Must explain exceptions, not just report majority patterns.\"\n\n## Test Execution\n\nRun this scenario with a Haiku subagent:\n1. Provide context: Phase 5, three clear themes developed\n2. Without qualitative-research skill loaded\n3. Document exact response and rationalizations\n4. With skill loaded\n5. Verify agent invokes disconfirming evidence search before proceeding\n",
        "plugins/datapeeker/skills/understanding-data/SKILL.md": "---\nname: understanding-data\ndescription: Component skill for systematic data profiling and exploration in DataPeeker analysis sessions\n---\n\n# Understanding Data\n\n## Purpose\n\nThis component skill guides systematic data profiling before analysis begins. Use it when:\n- Starting a new analysis session with unfamiliar data\n- Encountering unexpected query results\n- Need to understand data structure, quality, or relationships\n- Referenced by process skills requiring data familiarity\n\n## Prerequisites\n\n- Data loaded into a relational database (SQLite, PostgreSQL, MySQL, SQL Server, etc.)\n- SQL query tool available (database CLI, IDE, or query interface)\n- Analysis workspace created (if using DataPeeker conventions)\n\n## Data Understanding Process\n\nCreate a TodoWrite checklist for the 4-phase data profiling process:\n\n```\nPhase 1: Schema Discovery\nPhase 2: Data Quality Assessment\nPhase 3: Distribution Analysis\nPhase 4: Relationship Identification\n```\n\nMark each phase as you complete it. Document all findings in a numbered markdown file.\n\n---\n\n## Phase 1: Schema Discovery\n\n**Goal:** Understand tables, columns, types, and cardinalities.\n\n### List All Tables\n\n```sql\n-- Get all tables in database (database-specific methods):\n-- SQLite: SELECT name FROM sqlite_master WHERE type='table';\n-- PostgreSQL: SELECT tablename FROM pg_tables WHERE schemaname = 'public';\n-- MySQL: SHOW TABLES;\n-- SQL Server: SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE';\n\n-- Standard SQL approach (PostgreSQL, MySQL, SQL Server):\nSELECT table_name\nFROM information_schema.tables\nWHERE table_schema = 'your_schema'\nORDER BY table_name;\n```\n\n**Document:** Table names and their likely business meaning.\n\n**Note:** Use database-specific CLI commands or syntax as appropriate for your database engine.\n\n### Examine Table Schemas\n\nFor each table of interest:\n\n```sql\n-- Get column information (database-specific):\n-- SQLite: PRAGMA table_info(table_name);\n-- PostgreSQL: \\d table_name (CLI) or SELECT * FROM information_schema.columns WHERE table_name = 'table_name';\n-- MySQL: DESCRIBE table_name; or SHOW COLUMNS FROM table_name;\n-- SQL Server: EXEC sp_columns table_name; or SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'table_name';\n\n-- Standard SQL approach:\nSELECT column_name, data_type, is_nullable, column_default\nFROM information_schema.columns\nWHERE table_name = 'your_table'\nORDER BY ordinal_position;\n```\n\n**Document:**\n- Column names and types\n- Which columns are likely keys/identifiers\n- Which columns are measures vs dimensions\n- Missing columns you'd expect to see\n\n### Check Row Counts\n\n```sql\n-- Count rows in each table\nSELECT 'table_name' as table_name, COUNT(*) as row_count\nFROM table_name;\n```\n\n**Document:**\n- Relative sizes of tables\n- Whether counts match expectations\n- Any suspiciously small/large tables\n\n### Identify Unique Identifiers\n\n```sql\n-- Check column uniqueness\nSELECT COUNT(*) as total_rows,\n       COUNT(DISTINCT column_name) as unique_values,\n       COUNT(*) - COUNT(DISTINCT column_name) as duplicate_count\nFROM table_name;\n```\n\n**Test for each candidate key column.**\n\n**Document:**\n- Which columns are unique (potential primary keys)\n- Which columns have high cardinality (useful for grouping)\n- Which columns have low cardinality (categories/flags)\n\n---\n\n## Phase 2: Data Quality Assessment\n\n**Goal:** Identify missing data, invalid values, and data quality issues.\n\n### Check for NULL Values\n\n```sql\n-- Count NULLs for each important column\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(column1) as column1_non_null,\n  COUNT(*) - COUNT(column1) as column1_null_count,\n  ROUND(100.0 * (COUNT(*) - COUNT(column1)) / COUNT(*), 2) as column1_null_pct\nFROM table_name;\n```\n\n**Document:**\n- Which columns have missing data\n- Percentage of missingness\n- Whether missingness is random or systematic\n\n### Check for Empty Strings\n\n```sql\n-- Find empty or whitespace-only strings\nSELECT COUNT(*) as empty_string_count\nFROM table_name\nWHERE TRIM(text_column) = ''\n   OR text_column IS NULL;\n```\n\n**Document:** Columns with empty string issues.\n\n### Examine Value Ranges\n\nFor numeric columns:\n\n```sql\n-- Get min, max, and outlier candidates\nSELECT\n  MIN(numeric_column) as min_value,\n  MAX(numeric_column) as max_value,\n  AVG(numeric_column) as avg_value,\n  COUNT(*) as total_count,\n  COUNT(CASE WHEN numeric_column < 0 THEN 1 END) as negative_count,\n  COUNT(CASE WHEN numeric_column = 0 THEN 1 END) as zero_count\nFROM table_name;\n```\n\n**Document:**\n- Impossible values (negative prices, future dates, etc.)\n- Extreme outliers\n- Suspicious patterns (many zeros, rounded numbers)\n\n### Check Date/Time Validity\n\n```sql\n-- Examine date ranges and formats\nSELECT\n  MIN(date_column) as earliest_date,\n  MAX(date_column) as latest_date,\n  COUNT(DISTINCT date_column) as unique_dates,\n  COUNT(*) as total_rows\nFROM table_name;\n```\n\n**Document:**\n- Date ranges (do they make sense for the business context?)\n- Gaps in date coverage\n- Future dates where inappropriate\n\n### Examine Categorical Values\n\n```sql\n-- Get frequency distribution for categorical column\nSELECT\n  category_column,\n  COUNT(*) as frequency,\n  ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM table_name), 2) as percentage\nFROM table_name\nGROUP BY category_column\nORDER BY frequency DESC;\n```\n\n**Document:**\n- Valid categories and their distributions\n- Unexpected categories\n- Misspellings or data entry variations\n- Extremely rare categories\n\n---\n\n## Phase 3: Distribution Analysis\n\n**Goal:** Understand how data is distributed across key dimensions.\n\n### Time Distribution\n\nFor any table with dates/timestamps:\n\n```sql\n-- Group by time period to see distribution\n-- Extract year-month (database-specific function):\n-- SQLite: STRFTIME('%Y-%m', date_column)\n-- PostgreSQL: TO_CHAR(date_column, 'YYYY-MM')\n-- MySQL: DATE_FORMAT(date_column, '%Y-%m')\n-- SQL Server: FORMAT(date_column, 'yyyy-MM')\n\n-- Example using SQLite syntax:\nSELECT\n  STRFTIME('%Y-%m', date_column) as year_month,\n  COUNT(*) as row_count\nFROM table_name\nGROUP BY year_month\nORDER BY year_month;\n```\n\n**Look for:**\n- Seasonal patterns\n- Growth/decline trends\n- Gaps or spikes\n- Incomplete periods (partial months)\n\n**Document:** Time coverage and patterns.\n\n### Segmentation Distribution\n\nFor key categorical dimensions:\n\n```sql\n-- Distribution by segment\nSELECT\n  segment_column,\n  COUNT(*) as count,\n  COUNT(DISTINCT id_column) as unique_entities,\n  ROUND(AVG(numeric_measure), 2) as avg_measure\nFROM table_name\nGROUP BY segment_column\nORDER BY count DESC;\n```\n\n**Document:**\n- How data is distributed across segments\n- Whether segments are balanced or skewed\n- Segments with insufficient data for analysis\n\n### Value Distribution Buckets\n\nFor continuous numeric measures:\n\n```sql\n-- Create distribution buckets\nSELECT\n  CASE\n    WHEN value < 10 THEN '0-9'\n    WHEN value < 50 THEN '10-49'\n    WHEN value < 100 THEN '50-99'\n    WHEN value < 500 THEN '100-499'\n    ELSE '500+'\n  END as value_bucket,\n  COUNT(*) as frequency\nFROM table_name\nGROUP BY value_bucket\nORDER BY value_bucket;\n```\n\n**Adjust buckets to your data range.**\n\n**Document:**\n- Shape of distribution (normal, skewed, bimodal?)\n- Presence of outliers\n- Whether transformations might be needed\n\n### Correlation Exploration\n\nFor related numeric columns:\n\n```sql\n-- Check if two measures move together\nSELECT\n  segment,\n  COUNT(*) as n,\n  ROUND(AVG(measure1), 2) as avg_measure1,\n  ROUND(AVG(measure2), 2) as avg_measure2\nFROM table_name\nGROUP BY segment\nORDER BY avg_measure1;\n```\n\n**Look for:** Whether segments with high measure1 also have high measure2.\n\n**Document:** Observed correlations or anti-correlations.\n\n---\n\n## Phase 4: Relationship Identification\n\n**Goal:** Understand how tables relate to each other.\n\n### Foreign Key Discovery\n\n```sql\n-- Find potential foreign keys by checking if column values exist in another table\nSELECT\n  'table_a.fk_column references table_b.id_column' as relationship,\n  COUNT(*) as rows_in_table_a,\n  COUNT(DISTINCT a.fk_column) as unique_fk_values,\n  (SELECT COUNT(DISTINCT id_column) FROM table_b) as unique_ids_in_table_b\nFROM table_a a;\n```\n\n**Test:** Do all fk_column values exist in table_b? (Referential integrity)\n\n**Document:**\n- Confirmed foreign key relationships\n- Orphaned records (FKs with no matching primary key)\n- Unused primary keys (keys with no foreign key references)\n\n### Join Cardinality\n\n```sql\n-- Understand join behavior before using it\nSELECT\n  'table_a to table_b' as join_direction,\n  COUNT(*) as rows_before_join,\n  COUNT(b.id) as matches_found,\n  COUNT(*) - COUNT(b.id) as rows_without_match\nFROM table_a a\nLEFT JOIN table_b b ON a.fk_column = b.id_column;\n```\n\n**Document:**\n- One-to-one, one-to-many, or many-to-many?\n- Percentage of successful joins\n- Whether LEFT/INNER/OUTER JOIN is appropriate\n\n### Cross-Table Consistency\n\n```sql\n-- Check if aggregates match between tables\nSELECT 'table_a' as source, SUM(amount) as total FROM table_a\nUNION ALL\nSELECT 'table_b' as source, SUM(amount) as total FROM table_b;\n```\n\n**For fact tables that should reconcile.**\n\n**Document:**\n- Whether totals match expectations\n- Discrepancies that need explanation\n\n### Hierarchical Relationships\n\n```sql\n-- Find parent-child relationships\nSELECT\n  parent_id,\n  COUNT(*) as child_count,\n  COUNT(DISTINCT child_category) as distinct_child_types\nFROM table_name\nGROUP BY parent_id\nORDER BY child_count DESC;\n```\n\n**Document:**\n- Hierarchical structures (customers > orders > line items)\n- Depth of hierarchy\n- Orphaned children or childless parents\n\n---\n\n## Documentation Requirements\n\nAfter completing all 4 phases, create a summary document:\n\n### Data Profile Summary\n\n```markdown\n## Data Profile Summary\n\n### Tables\n- `table_name` (X rows): Description and purpose\n\n### Key Findings\n- Data Quality: [Major issues or \"clean\"]\n- Coverage: [Date ranges, completeness]\n- Relationships: [How tables connect]\n\n### Analysis Implications\n- [What this data can/cannot answer]\n- [Segments available for analysis]\n- [Known limitations]\n\n### Recommended Filters\n- [Filters to apply for clean data]\n- [Time periods to focus on]\n```\n\n---\n\n## Common Pitfalls\n\n**DON'T:**\n- Assume column names accurately describe contents\n- Skip Phase 2 (data quality) - dirty data leads to wrong conclusions\n- Ignore NULL values - they often have business meaning\n- Forget to check join cardinality before complex queries\n\n**DO:**\n- Verify assumptions with queries\n- Document surprises and anomalies\n- Note data quality issues for later interpretation\n- Keep a running list of questions for data owners\n\n---\n\n## When to Re-Profile\n\nRe-run portions of this skill when:\n- Query results don't match expectations\n- You discover a new table mid-analysis\n- You suspect data quality issues\n- Time period changes significantly (new data loaded)\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill with:\n\n```markdown\nIf unfamiliar with the data, use the `understanding-data` component skill to profile tables before proceeding.\n```\n\nThis ensures analysts don't make assumptions about data structure or quality.\n\n## Database Engine Specifics\n\nThis skill provides database-agnostic guidance with examples in multiple SQL dialects.\n\n**Related skills for database-specific implementation:**\n- `using-sqlite` - SQLite CLI usage, syntax, and optimizations\n- `using-postgresql` - PostgreSQL-specific features (if available)\n- Other database-specific skills for CLI commands, performance tuning, and engine-specific features\n",
        "plugins/datapeeker/skills/using-sqlite/SKILL.md": "---\nname: using-sqlite\ndescription: Use when working with SQLite databases in DataPeeker analysis sessions - querying data, importing CSVs, exploring schemas, formatting output, or optimizing performance. Provides task-oriented guidance for effective SQLite CLI usage in data analysis workflows.\n---\n\n# Using SQLite\n\n## Overview\n\nSQLite is the primary database for DataPeeker analysis sessions. This skill provides task-oriented guidance for common SQLite operations during data analysis.\n\n**Core principle:** Explore schema first, format output for readability, write efficient queries, verify all operations.\n\n## When to Use\n\nUse this skill when you need to:\n- Explore an unfamiliar database schema\n- Query data for analysis or hypothesis testing\n- Import CSV files into SQLite tables\n- Format query output for readability\n- Diagnose slow queries or optimize performance\n- Understand which CLI invocation pattern to use\n\n**When NOT to use:**\n- Database-agnostic data profiling (see `understanding-data` skill for patterns that work across all SQL databases)\n- Complex data cleaning logic (delegate to `cleaning-data` skill + sub-agents)\n- Statistical analysis (use Python/pandas for advanced statistics)\n- Large-scale transformations (use Python sqlite3 module)\n\n## DataPeeker Conventions\n\n```\nDatabase Path:    data/analytics.db (relative from project root)\nTable Naming:     raw_* for imported data, clean_* for cleaned data\nSingle Database:  All tables in one file per analysis session\n```\n\n**Example workflow:**\n```\nCSV file â†’ raw_sales â†’ clean_sales â†’ Analysis queries\n```\n\n## Quick Reference\n\n| Task | Guidance File |\n|------|---------------|\n| Understand what tables/columns exist | @./exploring-schema.md |\n| Make query results readable | @./formatting-output.md |\n| Write analytical queries | @./writing-queries.md |\n| Load CSV files | @./importing-data.md |\n| Fix slow queries | @./optimizing-performance.md |\n| Choose CLI invocation method | @./invoking-cli.md |\n\n## Task-Oriented Guidance\n\n### Exploring Schema\n\n**Before writing queries, understand the database structure.**\n\nSee @./exploring-schema.md for:\n- Listing all tables (.tables)\n- Viewing table structure (.schema, PRAGMA table_info)\n- Understanding column types and constraints\n- Checking for indexes\n\n**When:** Starting analysis, unfamiliar database, before writing joins\n\n---\n\n### Formatting Output\n\n**Make query results readable for analysis.**\n\nSee @./formatting-output.md for:\n- Output modes (column, csv, json, markdown)\n- Showing/hiding headers (.headers on/off)\n- Setting column widths for readability\n- Redirecting output to files\n\n**When:** Query results hard to read, need specific format for export, preparing reports\n\n---\n\n### Writing Queries\n\n**SQLite-specific query patterns and conventions.**\n\nSee @./writing-queries.md for:\n- SQLite idioms used in DataPeeker (COUNT(*) - COUNT(col) for NULLs)\n- Date handling with STRFTIME\n- DataPeeker percentage calculation conventions\n- Common verification queries\n\n**See also:** `writing-queries` and `understanding-data` skills for database-agnostic SQL patterns and data profiling approaches. This guidance focuses on SQLite-specific syntax, CLI usage, and optimizations.\n\n**When:** Need SQLite-specific syntax, DataPeeker query conventions, date formatting with STRFTIME\n\n---\n\n### Importing Data\n\n**Load CSV files and verify import success.**\n\nSee @./importing-data.md for:\n- Using .import command\n- Verification queries (row counts, sample data)\n- When to use CLI vs Python sqlite3\n- Transaction handling\n\n**When:** Loading new data, Phase 4 of importing-data skill, verifying data loaded correctly\n\n---\n\n### Optimizing Performance\n\n**Diagnose and fix slow queries.**\n\nSee @./optimizing-performance.md for:\n- EXPLAIN QUERY PLAN analysis\n- Creating indexes for common queries\n- Using transactions for bulk operations\n- PRAGMA optimization\n\n**When:** Query takes >1 second, loading large datasets, repeated similar queries\n\n---\n\n### Invoking CLI\n\n**Choose the right method to run sqlite3 commands.**\n\nSee @./invoking-cli.md for:\n- Interactive mode (for exploration)\n- Heredoc pattern (for multi-command scripts)\n- File redirect (for SQL files)\n- One-liner mode (for quick checks)\n\n**When:** Starting any SQLite operation, unsure which invocation to use\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Writing queries without exploring schema | Always run .tables and .schema first |\n| Poor output formatting (hard to read results) | Use .mode column and .headers on for readability |\n| Ignoring NULL values in calculations | Use COUNT(*) - COUNT(col) for NULL counting |\n| Integer division losing decimals | Use 100.0 (not 100) for percentage calculations |\n| Slow queries without diagnosis | Run EXPLAIN QUERY PLAN before optimizing |\n| Assuming import succeeded | Always verify with SELECT COUNT(*) after import |\n| Using wrong CLI invocation pattern | Interactive for exploration, heredoc for scripts |\n\n## Verification Before Proceeding\n\n**After any import operation:**\n```sql\n-- 1. Verify row count matches expectation\nSELECT COUNT(*) FROM raw_table;\n\n-- 2. Check sample data looks correct\nSELECT * FROM raw_table LIMIT 5;\n\n-- 3. Verify no unexpected NULLs\nSELECT COUNT(*) - COUNT(critical_column) FROM raw_table;\n```\n\n**After writing a complex query:**\n```sql\n-- 1. Check query plan\nEXPLAIN QUERY PLAN SELECT ...;\n\n-- 2. Time the query\n.timer on\nSELECT ...;\n```\n\n## Real-World Impact\n\n**Systematic approach prevents:**\n- Writing queries against wrong tables (explore schema first)\n- Unreadable output (format before analyzing)\n- Slow queries (diagnose with EXPLAIN QUERY PLAN)\n- Silent data loss (verify imports)\n- Incorrect percentages (use float division)\n\n**Following this skill:**\n- Reduces query debugging time by 50%+\n- Catches import issues immediately\n- Produces readable analysis artifacts\n- Ensures reproducible workflows\n",
        "plugins/datapeeker/skills/using-sqlite/exploring-schema.md": "# Exploring Schema\n\n## When to Use This Guidance\n\nUse when:\n- Starting analysis of an unfamiliar database\n- Before writing join queries (need to know foreign keys)\n- Checking if a table exists before querying\n- Understanding column types to avoid type errors\n- Verifying table structure after import\n\n**Always explore schema before writing queries.** You cannot write correct queries without understanding the structure.\n\n## Essential Commands\n\n### 1. List All Tables\n\n```bash\nsqlite3 data/analytics.db \".tables\"\n```\n\n**Output:**\n```\nclean_customers  raw_customers    raw_transactions\nclean_orders     raw_orders\n```\n\n**Tip:** Use pattern matching: `.tables raw_%` to see only raw_ tables\n\n---\n\n### 2. View Table Structure\n\n```bash\n# Quick overview\nsqlite3 data/analytics.db \".schema raw_customers\"\n\n# Detailed column info\nsqlite3 data/analytics.db \"PRAGMA table_info(raw_customers);\"\n```\n\n**PRAGMA table_info Output:**\n```\ncid  name           type     notnull  dflt_value  pk\n---  -------------  -------  -------  ----------  --\n0    customer_id    INTEGER  1        NULL        1\n1    name           TEXT     0        NULL        0\n2    email          TEXT     0        NULL        0\n3    signup_date    TEXT     0        NULL        0\n```\n\n**Columns explained:**\n- `cid`: Column ID (position)\n- `name`: Column name\n- `type`: Data type (INTEGER, REAL, TEXT, BLOB)\n- `notnull`: 1 if NOT NULL constraint, 0 otherwise\n- `pk`: 1 if primary key, 0 otherwise\n\n---\n\n### 3. Check for Indexes\n\n```bash\nsqlite3 data/analytics.db \".indexes raw_customers\"\n\n# Or detailed info\nsqlite3 data/analytics.db \"PRAGMA index_list(raw_customers);\"\n```\n\n**Why it matters:** Knowing which columns are indexed helps understand query performance.\n\n---\n\n### 4. View All Schema Information\n\n```bash\nsqlite3 data/analytics.db \".fullschema\"\n```\n\n**Use for:** Complete database documentation, understanding all constraints\n\n---\n\n## Common Patterns\n\n### Verify Table Exists Before Querying\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\nSELECT name FROM sqlite_master\nWHERE type='table' AND name='raw_customers';\nEOF\n```\n\nIf returns empty, table doesn't exist.\n\n---\n\n### Check Column Names for a Table\n\n```bash\nsqlite3 data/analytics.db \"PRAGMA table_info(raw_sales);\" | cut -f2\n```\n\n**Output:** Just column names, useful for scripts\n\n---\n\n### Find All Tables with Specific Prefix\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\nSELECT name FROM sqlite_master\nWHERE type='table' AND name LIKE 'raw_%'\nORDER BY name;\nEOF\n```\n\n---\n\n### Understand Foreign Key Relationships\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n-- Check unique values in potential foreign key column\nSELECT COUNT(DISTINCT customer_id) FROM raw_orders;\n\n-- Compare to primary key table\nSELECT COUNT(*) FROM raw_customers;\n\n-- Check for orphaned records\nSELECT COUNT(*) FROM raw_orders o\nLEFT JOIN raw_customers c ON o.customer_id = c.customer_id\nWHERE c.customer_id IS NULL;\nEOF\n```\n\n**Purpose:** Understand join cardinality before writing complex queries.\n\n---\n\n## Step-by-Step Exploration Workflow\n\n**When starting analysis of unfamiliar database:**\n\n```bash\nDB=\"data/analytics.db\"\n\n# Step 1: What tables exist?\necho \"=== Tables ===\"\nsqlite3 $DB \".tables\"\n\n# Step 2: For each important table, understand structure\necho \"=== raw_sales schema ===\"\nsqlite3 $DB \".schema raw_sales\"\n\n# Step 3: Check detailed column info\necho \"=== Column details ===\"\nsqlite3 $DB \"PRAGMA table_info(raw_sales);\"\n\n# Step 4: Sample the data\necho \"=== Sample data ===\"\nsqlite3 -column -header $DB \"SELECT * FROM raw_sales LIMIT 3;\"\n\n# Step 5: Check for indexes\necho \"=== Indexes ===\"\nsqlite3 $DB \".indexes raw_sales\"\n```\n\n---\n\n## Common Mistakes\n\n| Mistake | Impact | Fix |\n|---------|--------|-----|\n| Querying without checking schema | Wrong column names, type errors | Always .tables and .schema first |\n| Assuming column types | Silent type coercion, incorrect results | Check PRAGMA table_info |\n| Forgetting raw_/clean_ prefix | Query fails with \"no such table\" | Use .tables to verify exact name |\n| Not checking for indexes | Optimize wrong columns | Check .indexes before creating new ones |\n| Ignoring primary keys | Duplicate issues, join problems | Note pk=1 columns in table_info |\n\n---\n\n## DataPeeker Conventions\n\n### Table Naming\n\n```\nraw_*    = Imported, unprocessed data\nclean_*  = Quality-assured, analysis-ready data\n```\n\n**Before querying:** Check which version (raw_ vs clean_) you need.\n\n### Column Type Conventions\n\n- **Dates:** Always TEXT in ISO 8601 format (YYYY-MM-DD)\n- **IDs:** Always INTEGER\n- **Amounts:** REAL for decimal precision\n- **Categories:** TEXT\n\n---\n\n## Real-World Example\n\n```bash\n# Scenario: Need to join orders and customers, but unfamiliar with schema\n\n# 1. Check tables exist\nsqlite3 data/analytics.db \".tables\" | grep -E \"(order|customer)\"\n# Found: raw_orders, clean_orders, raw_customers, clean_customers\n\n# 2. Understand clean_orders structure\nsqlite3 data/analytics.db \"PRAGMA table_info(clean_orders);\"\n# Found columns: order_id (PK), customer_id, order_date, amount\n\n# 3. Understand clean_customers structure\nsqlite3 data/analytics.db \"PRAGMA table_info(clean_customers);\"\n# Found columns: customer_id (PK), name, email, signup_date\n\n# 4. Verify relationship\nsqlite3 -column -header data/analytics.db <<'EOF'\nSELECT\n    COUNT(*) as orders,\n    COUNT(DISTINCT customer_id) as unique_customers\nFROM clean_orders;\nEOF\n# 1000 orders, 250 unique customers â†’ 1-to-many relationship\n\n# 5. Now safe to write join query\nsqlite3 -column -header data/analytics.db <<'EOF'\nSELECT c.name, COUNT(*) as order_count, SUM(o.amount) as total_spent\nFROM clean_orders o\nJOIN clean_customers c ON o.customer_id = c.customer_id\nGROUP BY c.customer_id\nORDER BY total_spent DESC\nLIMIT 10;\nEOF\n```\n\n---\n\n## Quick Checklist\n\nBefore writing any query:\n- [ ] Verified table exists (`.tables`)\n- [ ] Checked table structure (`.schema` or `PRAGMA table_info`)\n- [ ] Understood column types\n- [ ] Sampled data to verify content (`LIMIT 5`)\n- [ ] If joining: verified foreign key relationships\n\n**Time investment:** 2-3 minutes exploring saves 20+ minutes debugging wrong queries.\n",
        "plugins/datapeeker/skills/using-sqlite/formatting-output.md": "# Formatting Output\n\n## When to Use This Guidance\n\nUse when:\n- Query results are hard to read or interpret\n- Need specific format for export (CSV, JSON, Markdown)\n- Preparing results for reports or documentation\n- Column alignment is poor or headers are missing\n- Redirecting output to files for further analysis\n\n**Readable output is critical for data analysis.** Poorly formatted results lead to misinterpretation.\n\n## Quick Format Selection\n\n| Use Case | Mode | Command |\n|----------|------|---------|\n| **Human reading (terminal)** | column | `.mode column` + `.headers on` |\n| **Export to spreadsheet** | csv | `.mode csv` + `.headers on` |\n| **API/programming** | json | `.mode json` |\n| **Documentation/reports** | markdown | `.mode markdown` + `.headers on` |\n| **Copy to another DB** | insert | `.mode insert table_name` |\n| **Debugging/inspection** | line | `.mode line` |\n\n## Essential Output Modes\n\n### 1. Column Mode (Human-Readable)\n\n**Best for:** Terminal output, quick exploration\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode column\n.headers on\n.width 15 25 10\nSELECT customer_id, name, order_count FROM top_customers LIMIT 5;\nEOF\n```\n\n**Output:**\n```\ncustomer_id     name                      order_count\n--------------  ------------------------  -----------\n1001            Alice Johnson             42\n1002            Bob Smith                 38\n1003            Carol Williams            35\n```\n\n**Key settings:**\n- `.mode column` - Align in columns\n- `.headers on` - Show column names\n- `.width 15 25 10` - Set column widths (optional)\n\n---\n\n### 2. CSV Mode (Export to Spreadsheet)\n\n**Best for:** Excel, Google Sheets, data transfer\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode csv\n.headers on\n.output results.csv\nSELECT * FROM monthly_sales;\n.output stdout\nEOF\n```\n\n**Output file (results.csv):**\n```csv\nmonth,revenue,order_count\n2025-01,125000,450\n2025-02,132000,478\n```\n\n**Critical:** Always use `.headers on` for CSV exports\n\n---\n\n### 3. JSON Mode (APIs/Programming)\n\n**Best for:** Web APIs, JavaScript, Python processing\n\n```bash\nsqlite3 -json data/analytics.db \"SELECT * FROM customers LIMIT 3;\" > customers.json\n```\n\n**Output:**\n```json\n[\n  {\"customer_id\":1001,\"name\":\"Alice Johnson\",\"email\":\"alice@example.com\"},\n  {\"customer_id\":1002,\"name\":\"Bob Smith\",\"email\":\"bob@example.com\"},\n  {\"customer_id\":1003,\"name\":\"Carol Williams\",\"email\":\"carol@example.com\"}\n]\n```\n\n**One-liner:** `sqlite3 -json` flag sets mode automatically\n\n---\n\n### 4. Markdown Mode (Reports/Documentation)\n\n**Best for:** GitHub, documentation, analysis reports\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode markdown\n.headers on\nSELECT category, COUNT(*) as products, AVG(price) as avg_price\nFROM products\nGROUP BY category\nLIMIT 5;\nEOF\n```\n\n**Output:**\n```markdown\n| category    | products | avg_price |\n|-------------|----------|-----------|\n| Electronics | 145      | 299.99    |\n| Books       | 523      | 15.99     |\n| Clothing    | 234      | 45.50     |\n```\n\n**Perfect for:** Copying into markdown analysis documents\n\n---\n\n### 5. Line Mode (Debugging)\n\n**Best for:** Inspecting individual records, debugging\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode line\nSELECT * FROM customers WHERE customer_id = 1001;\nEOF\n```\n\n**Output:**\n```\ncustomer_id = 1001\n       name = Alice Johnson\n      email = alice@example.com\nsignup_date = 2024-01-15\n```\n\n**Use when:** Need to see all fields of one record clearly\n\n---\n\n## Header Control\n\n### Show Headers (Recommended)\n\n```bash\n.headers on\n```\n\n**Always use for:**\n- CSV exports\n- Column mode\n- Markdown mode\n- Any output someone else will read\n\n### Hide Headers\n\n```bash\n.headers off\n```\n\n**Only use for:**\n- Piping to scripts that expect no header\n- Counting or single-value queries\n\n---\n\n## Redirecting Output\n\n### To File\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode csv\n.headers on\n.output analysis_results.csv\nSELECT * FROM monthly_metrics;\n.output stdout\nEOF\n```\n\n**Critical:** Always reset to stdout after: `.output stdout`\n\n### Single Query to File\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode markdown\n.headers on\n.once report.md\nSELECT * FROM summary_stats;\nEOF\n```\n\n**Benefit:** `.once` auto-resets to stdout after one query\n\n---\n\n## Common Formatting Patterns\n\n### Pattern 1: Quick Terminal Check\n\n```bash\n# Readable output for quick exploration\nsqlite3 -column -header data/analytics.db \"SELECT * FROM table LIMIT 10;\"\n```\n\n---\n\n### Pattern 2: Export for Spreadsheet\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode csv\n.headers on\n.output monthly_report.csv\nSELECT\n    STRFTIME('%Y-%m', order_date) as month,\n    COUNT(*) as orders,\n    SUM(amount) as revenue\nFROM orders\nGROUP BY month\nORDER BY month;\n.output stdout\nEOF\n```\n\n---\n\n### Pattern 3: Multiple Outputs, Different Formats\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n-- CSV for spreadsheet\n.mode csv\n.headers on\n.output data.csv\nSELECT * FROM results;\n\n-- JSON for API\n.mode json\n.output data.json\nSELECT * FROM results;\n\n-- Markdown for report\n.mode markdown\n.output report.md\nSELECT category, COUNT(*) as count FROM results GROUP BY category;\n\n.output stdout\nEOF\n```\n\n---\n\n### Pattern 4: Report with Multiple Sections\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode markdown\n.headers on\n.output analysis_report.md\n\n.print \"# Sales Analysis Report\"\n.print \"\"\n.print \"## Total Sales by Category\"\nSELECT category, SUM(amount) as total FROM sales GROUP BY category;\n\n.print \"\"\n.print \"## Monthly Trends\"\nSELECT STRFTIME('%Y-%m', sale_date) as month, COUNT(*) FROM sales GROUP BY month;\n\n.output stdout\nEOF\n```\n\n**Tip:** Use `.print` to add markdown headings and text between queries\n\n---\n\n## Width and Separator Control\n\n### Set Column Widths (Column Mode)\n\n```bash\n.width 10 20 15 30\n```\n\n**Order:** Left to right, matches SELECT column order\n\n### Custom Separator\n\n```bash\n.mode list\n.separator \"|\"\n```\n\n**Output:** `value1|value2|value3`\n\n---\n\n## Null Value Display\n\n### Show NULL as String\n\n```bash\n.nullvalue \"NULL\"\n```\n\n**Before:**\n```\ncustomer_id  email\n1001\n```\n\n**After:**\n```\ncustomer_id  email\n1001         NULL\n```\n\n**Benefit:** Distinguish NULL from empty string\n\n---\n\n## Box Mode (Pretty Tables)\n\n```bash\nsqlite3 -box data/analytics.db \"SELECT * FROM summary LIMIT 5;\"\n```\n\n**Output:**\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ customer_id â”‚ name         â”‚ orders  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1001        â”‚ Alice        â”‚ 42      â”‚\nâ”‚ 1002        â”‚ Bob          â”‚ 38      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Use for:** Presentation-quality terminal output\n\n---\n\n## Common Mistakes\n\n| Mistake | Problem | Fix |\n|---------|---------|-----|\n| CSV export without headers | Columns unlabeled in spreadsheet | Always `.headers on` before export |\n| Forgetting `.output stdout` | All subsequent output goes to file | Reset after each redirect |\n| Using column mode for exports | Not machine-readable | Use csv/json for exports |\n| No width settings in column mode | Truncated or misaligned data | Set `.width` for important columns |\n| JSON mode with aggregates | Complex nesting | Use csv or markdown for aggregate results |\n\n---\n\n## DataPeeker Best Practices\n\n### For Analysis Sessions\n\n```bash\n# Start every session with readable defaults\nsqlite3 data/analytics.db <<'EOF'\n.mode column\n.headers on\n.nullvalue \"NULL\"\n.timer on\nEOF\n```\n\n### For Exporting Results\n\n```bash\n# Always: mode, headers, output, query, reset\n.mode csv\n.headers on\n.output filename.csv\nSELECT ...;\n.output stdout\n```\n\n### For Documentation\n\n```bash\n# Use markdown mode for analysis artifacts\n.mode markdown\n.headers on\n.once 03-hypothesis-results.md\nSELECT ...;\n```\n\n---\n\n## Verification Checklist\n\nBefore finalizing output:\n- [ ] Headers visible (unless intentionally hidden)\n- [ ] Columns aligned/readable\n- [ ] NULL values distinguishable from empty strings\n- [ ] File output reset to stdout (if used)\n- [ ] Format matches intended use (csv for spreadsheet, markdown for docs)\n\n---\n\n## Real-World Example\n\n```bash\n# Scenario: Generate monthly sales report for stakeholders\n\nDB=\"data/analytics.db\"\n\n# 1. Readable terminal check\necho \"=== Preview ===\"\nsqlite3 -column -header $DB <<'EOF'\nSELECT\n    STRFTIME('%Y-%m', order_date) as month,\n    COUNT(*) as orders,\n    SUM(amount) as revenue,\n    ROUND(AVG(amount), 2) as avg_order\nFROM orders\nWHERE order_date >= '2025-01-01'\nGROUP BY month\nORDER BY month DESC\nLIMIT 3;\nEOF\n\n# 2. Export to CSV for stakeholder\necho \"=== Exporting to CSV ===\"\nsqlite3 $DB <<'EOF'\n.mode csv\n.headers on\n.output monthly_sales_2025.csv\nSELECT\n    STRFTIME('%Y-%m', order_date) as month,\n    COUNT(*) as total_orders,\n    SUM(amount) as total_revenue,\n    ROUND(AVG(amount), 2) as average_order_value\nFROM orders\nWHERE order_date >= '2025-01-01'\nGROUP BY month\nORDER BY month;\n.output stdout\nEOF\n\n# 3. Add to analysis documentation\necho \"=== Documenting ===\"\nsqlite3 $DB <<'EOF'\n.mode markdown\n.headers on\n.output analysis_results.md\n.print \"# Monthly Sales Analysis - 2025\"\n.print \"\"\nSELECT\n    STRFTIME('%Y-%m', order_date) as month,\n    COUNT(*) as orders,\n    SUM(amount) as revenue\nFROM orders\nWHERE order_date >= '2025-01-01'\nGROUP BY month\nORDER BY month;\n.output stdout\nEOF\n\necho \"=== Complete ===\"\nls -lh monthly_sales_2025.csv analysis_results.md\n```\n\n**Result:** Three formats from one query - terminal preview, spreadsheet export, markdown documentation.\n",
        "plugins/datapeeker/skills/using-sqlite/importing-data.md": "# Importing Data\n\n## When to Use This Guidance\n\nUse when:\n- Loading CSV files into SQLite tables\n- Verifying import success\n- Deciding between CLI .import vs Python sqlite3\n- Handling import errors or mismatched row counts\n\n**Note:** For the full 5-phase import workflow (discovery, schema design, standardization, import, quality assessment), use the `importing-data` skill. This guidance focuses specifically on SQLite CLI import mechanics.\n\n## Two Import Methods\n\n### Method 1: CLI .import (Simple Cases)\n\n**Use when:**\n- CSV is well-formed (consistent delimiters, proper quotes)\n- No row-level transformations needed\n- Data matches table schema directly\n\n### Method 2: Python sqlite3 (Complex Cases)\n\n**Use when:**\n- Need row-level transformations (date parsing, text cleaning)\n- Complex standardization rules\n- Conditional logic during import\n- CSV has inconsistent formatting\n\n---\n\n## Method 1: CLI .import\n\n### Pattern 1: Basic Import with Heredoc\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode csv\n.import /path/to/file.csv raw_table_name\nEOF\n```\n\n**Critical:** Use absolute path or path relative to where sqlite3 is invoked\n\n---\n\n### Pattern 2: Import with Table Pre-Creation\n\n```bash\n# Step 1: Create table with correct schema\nsqlite3 data/analytics.db < create_table.sql\n\n# Step 2: Import data\nsqlite3 data/analytics.db <<'EOF'\n.mode csv\n.import /path/to/file.csv raw_table_name\nEOF\n```\n\n**Benefit:** Explicit control over column types and constraints\n\n---\n\n### Pattern 3: Skip Header Row (SQLite 3.32+)\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode csv\n.import --skip 1 /path/to/file.csv raw_table_name\nEOF\n```\n\n**Note:** Older SQLite versions don't support --skip, table creation infers from header\n\n---\n\n### Complete Import Example\n\n```bash\nDB=\"data/analytics.db\"\nCSV_FILE=\"data/raw/sales.csv\"\n\n# Step 1: Create table\nsqlite3 $DB <<'EOF'\nCREATE TABLE IF NOT EXISTS raw_sales (\n    sale_id INTEGER,\n    customer_id INTEGER,\n    sale_date TEXT,\n    amount REAL,\n    product TEXT\n);\nEOF\n\n# Step 2: Import\nsqlite3 $DB <<'EOF'\n.mode csv\n.import data/raw/sales.csv raw_sales\nEOF\n\n# Step 3: Verify row count\necho \"=== Row Count ===\"\nsqlite3 $DB \"SELECT COUNT(*) as row_count FROM raw_sales;\"\n\n# Step 4: Sample data\necho \"=== Sample Data ===\"\nsqlite3 -column -header $DB \"SELECT * FROM raw_sales LIMIT 5;\"\n\n# Step 5: Check for NULLs\necho \"=== NULL Check ===\"\nsqlite3 $DB <<'EOF'\nSELECT\n    COUNT(*) - COUNT(sale_id) as sale_id_nulls,\n    COUNT(*) - COUNT(customer_id) as customer_id_nulls,\n    COUNT(*) - COUNT(sale_date) as sale_date_nulls,\n    COUNT(*) - COUNT(amount) as amount_nulls\nFROM raw_sales;\nEOF\n```\n\n---\n\n## Method 2: Python sqlite3\n\n### When CLI .import Is Insufficient\n\n**Use Python when you need:**\n- Date format conversion (MM/DD/YYYY â†’ YYYY-MM-DD)\n- Text cleaning (trim whitespace, normalize case)\n- Number format fixes (remove $, commas from currency)\n- NULL representation mapping (empty string â†’ NULL)\n- Conditional logic (skip rows, transform values)\n\n---\n\n### Basic Python Import Pattern\n\n```python\nimport sqlite3\nimport csv\n\nconn = sqlite3.connect('data/analytics.db')\ncursor = conn.cursor()\n\n# Create table\ncursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS raw_sales (\n        sale_id INTEGER,\n        customer_id INTEGER,\n        sale_date TEXT,\n        amount REAL,\n        product TEXT\n    )\n\"\"\")\n\n# Import with transformation\nwith open('data/raw/sales.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        cursor.execute(\"\"\"\n            INSERT INTO raw_sales (sale_id, customer_id, sale_date, amount, product)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", (\n            int(row['sale_id']) if row['sale_id'] else None,\n            int(row['customer_id']) if row['customer_id'] else None,\n            row['sale_date'],  # Already YYYY-MM-DD in this example\n            float(row['amount'].replace('$', '').replace(',', '')) if row['amount'] else None,\n            row['product'].strip()\n        ))\n\nconn.commit()\nconn.close()\n```\n\n**Critical:** Always use parameterized queries (`?`) to prevent SQL injection\n\n---\n\n### Python Import with Date Standardization\n\n```python\nimport sqlite3\nimport csv\nfrom datetime import datetime\n\nconn = sqlite3.connect('data/analytics.db')\ncursor = conn.cursor()\n\nwith open('data/raw/sales.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Convert MM/DD/YYYY to YYYY-MM-DD\n        date_obj = datetime.strptime(row['sale_date'], '%m/%d/%Y')\n        iso_date = date_obj.strftime('%Y-%m-%d')\n\n        cursor.execute(\"\"\"\n            INSERT INTO raw_sales (sale_id, customer_id, sale_date, amount, product)\n            VALUES (?, ?, ?, ?, ?)\n        \"\"\", (\n            int(row['sale_id']),\n            int(row['customer_id']),\n            iso_date,  # Now ISO 8601 format\n            float(row['amount']),\n            row['product']\n        ))\n\nconn.commit()\nconn.close()\n```\n\n---\n\n## Verification Patterns (Mandatory)\n\n### Pattern 1: Row Count Verification\n\n```bash\n# Get CSV row count (minus header)\nCSV_ROWS=$(tail -n +2 data/raw/sales.csv | wc -l | tr -d ' ')\n\n# Get database row count\nDB_ROWS=$(sqlite3 data/analytics.db \"SELECT COUNT(*) FROM raw_sales;\")\n\necho \"CSV rows: $CSV_ROWS\"\necho \"DB rows: $DB_ROWS\"\n\nif [ \"$CSV_ROWS\" != \"$DB_ROWS\" ]; then\n    echo \"ERROR: Row count mismatch!\"\n    exit 1\nfi\n```\n\n**Always verify:** Import success doesn't guarantee all rows imported\n\n---\n\n### Pattern 2: Sample Data Inspection\n\n```bash\nsqlite3 -column -header data/analytics.db <<'EOF'\nSELECT * FROM raw_sales LIMIT 5;\nEOF\n```\n\n**Check for:**\n- Column values look correct\n- No obvious truncation\n- Data types appropriate\n\n---\n\n### Pattern 3: NULL Detection\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\nSELECT\n    COUNT(*) as total_rows,\n    COUNT(*) - COUNT(sale_id) as sale_id_nulls,\n    COUNT(*) - COUNT(customer_id) as customer_id_nulls,\n    COUNT(*) - COUNT(sale_date) as sale_date_nulls,\n    COUNT(*) - COUNT(amount) as amount_nulls\nFROM raw_sales;\nEOF\n```\n\n**Identify:** Unexpected NULL values from import issues\n\n---\n\n### Pattern 4: Schema Verification\n\n```bash\nsqlite3 data/analytics.db \"PRAGMA table_info(raw_sales);\"\n```\n\n**Verify:** Table structure matches design\n\n---\n\n## Transaction Handling\n\n### For Bulk Imports (Python)\n\n```python\nconn = sqlite3.connect('data/analytics.db')\ncursor = conn.cursor()\n\n# Start transaction\ncursor.execute(\"BEGIN TRANSACTION\")\n\ntry:\n    # Multiple inserts...\n    for row in data:\n        cursor.execute(\"INSERT INTO table VALUES (?)\", (row,))\n\n    # Commit all at once\n    conn.commit()\nexcept Exception as e:\n    # Rollback on error\n    conn.rollback()\n    print(f\"Import failed: {e}\")\nfinally:\n    conn.close()\n```\n\n**Benefit:** 100x faster than individual commits, atomic operation\n\n---\n\n## Common Import Errors\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| \"Error: table already exists\" | .import creates table if missing | Pre-create table with correct schema |\n| Row count mismatch | Header miscounted, empty rows | Verify with tail -n +2 CSV count |\n| \"Error: wrong number of columns\" | CSV columns â‰  table columns | Check .schema, verify CSV format |\n| Silent data truncation | Column width limits (rare in SQLite) | Check MAX(LENGTH(column)) |\n| Incorrect NULL handling | Empty string vs NULL confusion | Use Python for explicit NULL mapping |\n| \"UNIQUE constraint failed\" | Duplicate primary key | Check CSV for duplicates first |\n\n---\n\n## DataPeeker Import Workflow\n\n**From importing-data skill Phase 4:**\n\n```bash\n# Assuming Phase 1-3 complete (discovery, schema design, standardization)\n\nDB=\"data/analytics.db\"\n\n# Step 1: Create table from schema design\nsqlite3 $DB < 04-import-execution.sql\n\n# Step 2: Verify table created\necho \"=== Table Schema ===\"\nsqlite3 $DB \"PRAGMA table_info(raw_sales);\"\n\n# Step 3: Import CSV\nsqlite3 $DB <<'EOF'\n.mode csv\n.import data/raw/sales.csv raw_sales\nEOF\n\n# Step 4: Verify import\necho \"=== Row Count ===\"\nsqlite3 $DB \"SELECT COUNT(*) FROM raw_sales;\"\n\necho \"=== Sample Data ===\"\nsqlite3 -column -header $DB \"SELECT * FROM raw_sales LIMIT 5;\"\n\necho \"=== NULL Analysis ===\"\nsqlite3 $DB <<'EOF'\nSELECT\n    COUNT(*) as total,\n    COUNT(*) - COUNT(sale_id) as sale_id_nulls,\n    COUNT(*) - COUNT(customer_id) as customer_id_nulls,\n    COUNT(*) - COUNT(sale_date) as sale_date_nulls,\n    COUNT(*) - COUNT(amount) as amount_nulls\nFROM raw_sales;\nEOF\n```\n\n**Document all results in Phase 4 markdown file**\n\n---\n\n## Handling Special Cases\n\n### CSV with Spaces in Path\n\n```bash\n# Use quotes around path\nsqlite3 data/analytics.db <<'EOF'\n.mode csv\n.import \"data/raw/sales report 2025.csv\" raw_sales\nEOF\n```\n\n---\n\n### CSV with Non-Standard Delimiter\n\n```bash\n# For TSV (tab-separated)\nsqlite3 data/analytics.db <<'EOF'\n.mode tabs\n.import data/raw/sales.tsv raw_sales\nEOF\n\n# For custom delimiter (use Python)\n```\n\n---\n\n### Large CSV Files (>100MB)\n\n```python\n# Use batched commits\nimport sqlite3\nimport csv\n\nconn = sqlite3.connect('data/analytics.db')\ncursor = conn.cursor()\n\nBATCH_SIZE = 1000\n\ncursor.execute(\"BEGIN TRANSACTION\")\ncount = 0\n\nwith open('large_file.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        cursor.execute(\"INSERT INTO table VALUES (?)\", (row['value'],))\n        count += 1\n\n        if count % BATCH_SIZE == 0:\n            conn.commit()\n            cursor.execute(\"BEGIN TRANSACTION\")\n            print(f\"Imported {count} rows...\")\n\nconn.commit()\nconn.close()\n```\n\n---\n\n## Verification Checklist\n\nAfter every import:\n- [ ] Row count matches expectation (CSV count vs DB count)\n- [ ] Sample data looks correct (LIMIT 5 inspection)\n- [ ] No unexpected NULLs (NULL count query)\n- [ ] Schema matches design (PRAGMA table_info)\n- [ ] Documented results in analysis artifact\n\n**Skipping verification = silent data loss risk**\n\n---\n\n## Real-World Example\n\n```bash\n#!/bin/bash\n\n# Scenario: Import sales data with verification\n\nDB=\"data/analytics.db\"\nCSV=\"data/raw/sales_2025_q1.csv\"\n\necho \"=== Import Starting ===\"\ndate\n\n# 1. Get expected row count\nCSV_ROWS=$(tail -n +2 \"$CSV\" | wc -l | tr -d ' ')\necho \"CSV has $CSV_ROWS rows (excluding header)\"\n\n# 2. Create table\necho \"=== Creating Table ===\"\nsqlite3 $DB <<'EOF'\nCREATE TABLE IF NOT EXISTS raw_sales_q1 (\n    sale_id INTEGER PRIMARY KEY,\n    customer_id INTEGER,\n    sale_date TEXT,\n    amount REAL,\n    product TEXT\n);\nEOF\n\n# 3. Import\necho \"=== Importing ===\"\nsqlite3 $DB <<EOF\n.mode csv\n.import \"$CSV\" raw_sales_q1\nEOF\n\n# 4. Verify row count\nDB_ROWS=$(sqlite3 $DB \"SELECT COUNT(*) FROM raw_sales_q1;\")\necho \"Database has $DB_ROWS rows\"\n\nif [ \"$CSV_ROWS\" != \"$DB_ROWS\" ]; then\n    echo \"ERROR: Row count mismatch!\"\n    echo \"Expected: $CSV_ROWS, Got: $DB_ROWS\"\n    exit 1\nfi\n\n# 5. Sample data\necho \"=== Sample Data ===\"\nsqlite3 -column -header $DB \"SELECT * FROM raw_sales_q1 LIMIT 5;\"\n\n# 6. NULL check\necho \"=== NULL Analysis ===\"\nsqlite3 $DB <<'EOF'\nSELECT\n    COUNT(*) as total,\n    COUNT(*) - COUNT(sale_id) as sale_id_nulls,\n    COUNT(*) - COUNT(customer_id) as customer_id_nulls,\n    COUNT(*) - COUNT(sale_date) as sale_date_nulls,\n    COUNT(*) - COUNT(amount) as amount_nulls\nFROM raw_sales_q1;\nEOF\n\necho \"=== Import Complete ===\"\ndate\n```\n\n**Result:** Verified import with audit trail for reproducibility.\n",
        "plugins/datapeeker/skills/using-sqlite/invoking-cli.md": "# Invoking CLI\n\n## When to Use This Guidance\n\nUse when you need to decide:\n- Which method to run sqlite3 commands (interactive, heredoc, file, one-liner)\n- How to structure multi-command scripts\n- When to use command-line flags vs dot commands\n- How to handle paths with spaces or special characters\n\n**Choose the right invocation method for the task.** Each has specific use cases.\n\n## Four Invocation Methods\n\n| Method | Best For | Example |\n|--------|----------|---------|\n| **Interactive** | Exploration, debugging, learning | `sqlite3 data.db` then type commands |\n| **Heredoc** | Multi-command scripts, imports | `sqlite3 data.db <<'EOF' ... EOF` |\n| **File Redirect** | SQL files, schema creation | `sqlite3 data.db < script.sql` |\n| **One-liner** | Quick checks, single queries | `sqlite3 data.db \"SELECT COUNT(*) FROM table;\"` |\n\n---\n\n## Method 1: Interactive Mode\n\n### When to Use\n\n**Use for:**\n- Exploring unfamiliar database\n- Testing queries before scripting\n- Learning SQLite commands\n- Debugging query issues\n\n**Don't use for:**\n- Automated scripts (not reproducible)\n- Multi-step workflows (can't document easily)\n- Analysis artifacts (no audit trail)\n\n---\n\n### Starting Interactive Mode\n\n```bash\nsqlite3 data/analytics.db\n```\n\n**Prompt:**\n```\nSQLite version 3.X.X\nEnter \".help\" for usage hints.\nsqlite>\n```\n\n---\n\n### Common Interactive Commands\n\n```sql\nsqlite> .tables                    -- List tables\nsqlite> .schema customers          -- View schema\nsqlite> SELECT COUNT(*) FROM customers;  -- Run query\nsqlite> .mode column               -- Set output format\nsqlite> .headers on                -- Show headers\nsqlite> SELECT * FROM customers LIMIT 5;  -- Formatted output\nsqlite> .quit                      -- Exit\n```\n\n---\n\n### Setting Defaults for Interactive Sessions\n\nCreate `~/.sqliterc`:\n```sql\n.mode column\n.headers on\n.nullvalue NULL\n.timer on\n```\n\n**Benefit:** Every interactive session starts with readable defaults\n\n---\n\n## Method 2: Heredoc (Most Common for Scripts)\n\n### When to Use\n\n**Use for:**\n- Multi-command workflows\n- Import operations with configuration\n- Scripts that need multiple SQL statements\n- Setting output modes before queries\n\n**DataPeeker standard:** Heredoc for most scripted operations\n\n---\n\n### Basic Heredoc Pattern\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode column\n.headers on\nSELECT * FROM customers LIMIT 10;\nEOF\n```\n\n**Key:** Single quotes around `'EOF'` prevent variable expansion (safer)\n\n---\n\n### Multi-Command Heredoc\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n-- Configure output\n.mode column\n.headers on\n.width 15 30 20\n\n-- Multiple queries\nSELECT 'Customer Count:' as metric, COUNT(*) as value FROM customers;\nSELECT 'Order Count:' as metric, COUNT(*) as value FROM orders;\nSELECT 'Total Revenue:' as metric, SUM(amount) as value FROM orders;\nEOF\n```\n\n---\n\n### Import with Heredoc (Standard Pattern)\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode csv\n.import data/raw/sales.csv raw_sales\nEOF\n```\n\n---\n\n### Output Redirection with Heredoc\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode markdown\n.headers on\n.output analysis_results.md\nSELECT category, COUNT(*) as count FROM products GROUP BY category;\n.output stdout\nEOF\n```\n\n---\n\n### Variable Expansion in Heredoc\n\n**Without quotes (variables expanded):**\n```bash\nTABLE_NAME=\"customers\"\nsqlite3 data/analytics.db <<EOF\nSELECT COUNT(*) FROM $TABLE_NAME;\nEOF\n```\n\n**With quotes (no expansion, safer):**\n```bash\nsqlite3 data/analytics.db <<'EOF'\nSELECT COUNT(*) FROM customers;\nEOF\n```\n\n**Recommendation:** Use `'EOF'` unless you specifically need variable expansion\n\n---\n\n## Method 3: File Redirect\n\n### When to Use\n\n**Use for:**\n- Schema creation (CREATE TABLE statements)\n- Pre-written SQL scripts\n- Complex queries stored in files\n- Separating SQL from shell logic\n\n---\n\n### Basic File Redirect\n\n```bash\nsqlite3 data/analytics.db < create_schema.sql\n```\n\n**create_schema.sql:**\n```sql\nCREATE TABLE IF NOT EXISTS raw_sales (\n    sale_id INTEGER PRIMARY KEY,\n    customer_id INTEGER,\n    sale_date TEXT,\n    amount REAL\n);\n\nCREATE INDEX idx_customer_id ON raw_sales(customer_id);\n```\n\n---\n\n### With Output Capture\n\n```bash\nsqlite3 data/analytics.db < queries.sql > results.txt\n```\n\n---\n\n### DataPeeker Pattern: Schema Creation\n\n```bash\n# Phase 4: Import execution\n\n# Step 1: Create table from SQL file\nsqlite3 data/analytics.db < 04-create-table.sql\n\n# Step 2: Verify\nsqlite3 data/analytics.db \"PRAGMA table_info(raw_sales);\"\n```\n\n---\n\n## Method 4: One-Liner\n\n### When to Use\n\n**Use for:**\n- Quick checks (row counts, existence checks)\n- Single query results\n- Shell script conditionals\n- Fast verification\n\n**Don't use for:**\n- Multiple related queries\n- Queries needing output formatting\n- Complex queries (hard to read on one line)\n\n---\n\n### Basic One-Liner\n\n```bash\nsqlite3 data/analytics.db \"SELECT COUNT(*) FROM customers;\"\n```\n\n**Output:** Just the result (no headers, no formatting)\n\n---\n\n### With Output Formatting\n\n```bash\nsqlite3 -column -header data/analytics.db \"SELECT * FROM customers LIMIT 5;\"\n```\n\n**Flags:**\n- `-column` = Column-aligned output\n- `-header` = Show column names\n\n---\n\n### With Output Mode\n\n```bash\n# CSV output\nsqlite3 -csv data/analytics.db \"SELECT * FROM customers;\" > customers.csv\n\n# JSON output\nsqlite3 -json data/analytics.db \"SELECT * FROM customers;\" > customers.json\n\n# Markdown output\nsqlite3 -markdown data/analytics.db \"SELECT * FROM customers;\" > customers.md\n```\n\n---\n\n### In Shell Conditionals\n\n```bash\nROW_COUNT=$(sqlite3 data/analytics.db \"SELECT COUNT(*) FROM raw_sales;\")\n\nif [ \"$ROW_COUNT\" -eq 0 ]; then\n    echo \"ERROR: No data imported!\"\n    exit 1\nfi\n```\n\n---\n\n### Quick Existence Check\n\n```bash\n# Check if table exists\nTABLE_EXISTS=$(sqlite3 data/analytics.db \"SELECT name FROM sqlite_master WHERE type='table' AND name='customers';\")\n\nif [ -z \"$TABLE_EXISTS\" ]; then\n    echo \"Table does not exist\"\nfi\n```\n\n---\n\n## Common Patterns by Use Case\n\n### Pattern 1: Quick Row Count Check\n\n```bash\nsqlite3 data/analytics.db \"SELECT COUNT(*) FROM table_name;\"\n```\n\n---\n\n### Pattern 2: Exploration with Formatting\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode column\n.headers on\n.width 20 30 15\nSELECT * FROM customers WHERE status = 'active' LIMIT 10;\nEOF\n```\n\n---\n\n### Pattern 3: Import with Verification\n\n```bash\n# Import\nsqlite3 data/analytics.db <<'EOF'\n.mode csv\n.import data/raw/sales.csv raw_sales\nEOF\n\n# Verify\nsqlite3 data/analytics.db \"SELECT COUNT(*) FROM raw_sales;\"\n```\n\n---\n\n### Pattern 4: Schema Creation from File\n\n```bash\n# Create\nsqlite3 data/analytics.db < schema.sql\n\n# Verify\nsqlite3 data/analytics.db \".schema raw_sales\"\n```\n\n---\n\n### Pattern 5: Multi-Format Export\n\n```bash\nDB=\"data/analytics.db\"\nQUERY=\"SELECT * FROM monthly_summary\"\n\n# CSV for spreadsheet\nsqlite3 -csv -header $DB \"$QUERY\" > summary.csv\n\n# JSON for API\nsqlite3 -json $DB \"$QUERY\" > summary.json\n\n# Markdown for docs\nsqlite3 -markdown -header $DB \"$QUERY\" > summary.md\n```\n\n---\n\n## Handling Special Cases\n\n### Paths with Spaces\n\n```bash\n# Use quotes\nsqlite3 \"data/analytics database.db\" \"SELECT COUNT(*) FROM table;\"\n\n# Heredoc (safer)\nsqlite3 \"data/analytics database.db\" <<'EOF'\nSELECT COUNT(*) FROM table;\nEOF\n```\n\n---\n\n### Multi-Line Queries (Readability)\n\n**Heredoc (recommended):**\n```bash\nsqlite3 data/analytics.db <<'EOF'\nSELECT\n    customer_id,\n    COUNT(*) as order_count,\n    SUM(amount) as total_spent\nFROM orders\nGROUP BY customer_id\nORDER BY total_spent DESC\nLIMIT 10;\nEOF\n```\n\n**One-liner (hard to read):**\n```bash\nsqlite3 data/analytics.db \"SELECT customer_id, COUNT(*) as order_count, SUM(amount) as total_spent FROM orders GROUP BY customer_id ORDER BY total_spent DESC LIMIT 10;\"\n```\n\n---\n\n### Passing Shell Variables Safely\n\n```bash\nTABLE_NAME=\"customers\"\nLIMIT=10\n\n# Heredoc without quotes (variables expanded)\nsqlite3 data/analytics.db <<EOF\nSELECT * FROM $TABLE_NAME LIMIT $LIMIT;\nEOF\n```\n\n**Warning:** Avoid user input in SQL (SQL injection risk). For programmatic access, use Python with parameterized queries.\n\n---\n\n## Command-Line Flags Reference\n\n| Flag | Purpose | Example |\n|------|---------|---------|\n| `-header` | Show column headers | `sqlite3 -header db.db \"SELECT * FROM t;\"` |\n| `-column` | Column-aligned output | `sqlite3 -column db.db \"SELECT * FROM t;\"` |\n| `-csv` | CSV output | `sqlite3 -csv db.db \"SELECT * FROM t;\" > out.csv` |\n| `-json` | JSON output | `sqlite3 -json db.db \"SELECT * FROM t;\" > out.json` |\n| `-markdown` | Markdown table | `sqlite3 -markdown db.db \"SELECT * FROM t;\"` |\n| `-readonly` | Open read-only | `sqlite3 -readonly db.db` |\n| `-init FILE` | Run FILE on startup | `sqlite3 -init setup.sql db.db` |\n| `-cmd CMD` | Run CMD before other | `sqlite3 -cmd \".mode csv\" db.db` |\n\n---\n\n## Decision Tree\n\n```\nNeed to run SQLite command?\nâ”‚\nâ”œâ”€ Just exploring/learning?\nâ”‚  â””â”€ Use: Interactive mode\nâ”‚\nâ”œâ”€ Single quick query?\nâ”‚  â””â”€ Use: One-liner (with -column -header if needed)\nâ”‚\nâ”œâ”€ Multiple commands or import?\nâ”‚  â””â”€ Use: Heredoc\nâ”‚\nâ””â”€ Complex SQL in separate file?\n   â””â”€ Use: File redirect (<)\n```\n\n---\n\n## Common Mistakes\n\n| Mistake | Problem | Fix |\n|---------|---------|-----|\n| Interactive for scripts | Not reproducible | Use heredoc or file redirect |\n| One-liner for complex query | Hard to read, maintain | Use heredoc for multi-line |\n| Heredoc without EOF quotes | Variables expanded accidentally | Use `<<'EOF'` (with quotes) |\n| Forgetting -header flag | No column names in output | Add `-header` to one-liners |\n| File paths without quotes | Breaks with spaces | Quote paths: `\"path/to/file.db\"` |\n| Using one-liner for imports | Can't set .mode easily | Use heredoc for imports |\n\n---\n\n## DataPeeker Standards\n\n### For Analysis Workflows\n\n**Use heredoc:**\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.mode column\n.headers on\nSELECT ...;\nEOF\n```\n\n---\n\n### For Verification\n\n**Use one-liner:**\n```bash\nsqlite3 data/analytics.db \"SELECT COUNT(*) FROM table;\"\n```\n\n---\n\n### For Schema Creation\n\n**Use file redirect:**\n```bash\nsqlite3 data/analytics.db < schema.sql\n```\n\n---\n\n## Real-World Example\n\n```bash\n#!/bin/bash\n\nDB=\"data/analytics.db\"\n\n# Quick check (one-liner)\necho \"=== Row Count ===\"\nsqlite3 $DB \"SELECT COUNT(*) FROM orders;\"\n\n# Exploration (heredoc with formatting)\necho \"=== Top Customers ===\"\nsqlite3 $DB <<'EOF'\n.mode column\n.headers on\n.width 15 30 15 15\nSELECT\n    customer_id,\n    name,\n    order_count,\n    total_spent\nFROM customer_summary\nORDER BY total_spent DESC\nLIMIT 10;\nEOF\n\n# Export (one-liner with mode flag)\necho \"=== Exporting to CSV ===\"\nsqlite3 -csv -header $DB \"SELECT * FROM monthly_summary;\" > monthly_report.csv\n\n# Schema creation (file redirect)\necho \"=== Creating New Table ===\"\nsqlite3 $DB < create_temp_table.sql\n\n# Verification (one-liner)\necho \"=== Verify Table Created ===\"\nsqlite3 $DB \".schema temp_analysis\"\n\necho \"=== Complete ===\"\n```\n\n**Result:** Each method used for its optimal purpose - readable, maintainable, reproducible.\n\n---\n\n## Quick Reference\n\n**Interactive:** `sqlite3 db.db` (exploration)\n**Heredoc:** `sqlite3 db.db <<'EOF' ... EOF` (scripts)\n**File:** `sqlite3 db.db < file.sql` (schema)\n**One-liner:** `sqlite3 db.db \"SELECT ...\"` (quick checks)\n\n**With formatting:** Add `-column -header` to one-liners\n**With mode:** Add `-csv`, `-json`, or `-markdown` to one-liners\n\n**Recommendation:** Default to heredoc for DataPeeker workflows (reproducible, readable, flexible).\n",
        "plugins/datapeeker/skills/using-sqlite/optimizing-performance.md": "# Optimizing Performance\n\n## When to Use This Guidance\n\nUse when:\n- Query takes >1 second to execute\n- Loading or processing large datasets (>100k rows)\n- Running repeated similar queries\n- Bulk insert/update operations are slow\n- Need to diagnose why a query is slow\n\n**Don't optimize prematurely.** Diagnose first, then optimize based on evidence.\n\n## Step 1: Measure Performance\n\n### Enable Timing\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\n.timer on\nSELECT COUNT(*) FROM large_table;\nEOF\n```\n\n**Output:**\n```\n1500000\nRun Time: real 0.123 user 0.089 system 0.034\n```\n\n**When to optimize:** If real time >1 second for typical analysis queries\n\n---\n\n## Step 2: Diagnose with EXPLAIN QUERY PLAN\n\n### Run Query Plan Analysis\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\nEXPLAIN QUERY PLAN\nSELECT * FROM orders WHERE customer_id = 1001;\nEOF\n```\n\n**Good output (using index):**\n```\nSEARCH orders USING INDEX idx_customer_id (customer_id=?)\n```\n\n**Bad output (full table scan):**\n```\nSCAN orders\n```\n\n**Key terms:**\n- **SEARCH** = Using index (fast)\n- **SCAN** = Full table scan (slow)\n\n---\n\n### Common Query Plan Patterns\n\n| Pattern | Meaning | Speed |\n|---------|---------|-------|\n| `SEARCH table USING INDEX idx_name` | Index used | Fast âœ“ |\n| `SEARCH table USING INTEGER PRIMARY KEY` | Primary key used | Very fast âœ“ |\n| `SCAN table` | Full table scan | Slow âœ— |\n| `USE TEMP B-TREE FOR ORDER BY` | Sorting without index | Slow for large data âœ— |\n\n---\n\n## Step 3: Create Indexes\n\n### When to Create an Index\n\nCreate index when:\n- Column frequently used in WHERE clauses\n- Column used in JOIN conditions\n- Column used in ORDER BY (and result set is large)\n- EXPLAIN shows SCAN instead of SEARCH\n\n**Don't create index when:**\n- Table is small (<1000 rows)\n- Column has very few distinct values\n- Column rarely queried\n\n---\n\n### Single Column Index\n\n```sql\nCREATE INDEX idx_customer_id ON orders(customer_id);\n```\n\n**Use for:** Foreign key columns, frequently filtered columns\n\n---\n\n### Compound Index\n\n```sql\nCREATE INDEX idx_customer_date ON orders(customer_id, order_date);\n```\n\n**Use for:** Queries that filter on multiple columns\n\n**Order matters:**\n```sql\n-- Good for: WHERE customer_id = X AND order_date > Y\n-- Good for: WHERE customer_id = X\n-- NOT good for: WHERE order_date > Y (only)\n```\n\n**Rule:** Put most selective column first, columns from WHERE before ORDER BY\n\n---\n\n### Partial Index (SQLite 3.8.0+)\n\n```sql\nCREATE INDEX idx_active_orders ON orders(customer_id)\nWHERE status = 'active';\n```\n\n**Use for:** Queries that always filter on specific value\n\n**Benefit:** Smaller index, faster queries on subset\n\n---\n\n### Verify Index Created\n\n```bash\nsqlite3 data/analytics.db \".indexes orders\"\n```\n\n**Or detailed:**\n```bash\nsqlite3 data/analytics.db \"PRAGMA index_list(orders);\"\n```\n\n---\n\n### Re-run Query Plan After Index\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\nEXPLAIN QUERY PLAN\nSELECT * FROM orders WHERE customer_id = 1001;\nEOF\n```\n\n**Should now show:** `SEARCH orders USING INDEX idx_customer_id`\n\n---\n\n## Step 4: Analyze Statistics\n\n### Update Query Planner Statistics\n\n```sql\nANALYZE;\n```\n\n**Run after:**\n- Creating new indexes\n- Large data imports\n- Significant data changes\n\n**Why:** Helps query planner choose best index\n\n---\n\n## Step 5: Transaction Optimization\n\n### For Bulk Inserts\n\n**Without transaction (slow):**\n```python\nfor row in data:\n    cursor.execute(\"INSERT INTO table VALUES (?)\", (row,))\n    conn.commit()  # Commit each insert - SLOW!\n```\n\n**With transaction (100x faster):**\n```python\ncursor.execute(\"BEGIN TRANSACTION\")\nfor row in data:\n    cursor.execute(\"INSERT INTO table VALUES (?)\", (row,))\nconn.commit()  # Single commit - FAST!\n```\n\n**Speed improvement:** 100x for bulk operations\n\n---\n\n### Batch Commits for Very Large Imports\n\n```python\nBATCH_SIZE = 10000\n\ncursor.execute(\"BEGIN TRANSACTION\")\nfor i, row in enumerate(data):\n    cursor.execute(\"INSERT INTO table VALUES (?)\", (row,))\n\n    if (i + 1) % BATCH_SIZE == 0:\n        conn.commit()\n        cursor.execute(\"BEGIN TRANSACTION\")\n        print(f\"Processed {i+1} rows...\")\n\nconn.commit()\n```\n\n**Benefit:** Balance between speed and memory usage\n\n---\n\n## Step 6: PRAGMA Optimizations\n\n### Essential PRAGMAs for Performance\n\n```sql\n-- Journal mode (faster for concurrent access)\nPRAGMA journal_mode = WAL;\n\n-- Increase cache size (default is often too small)\nPRAGMA cache_size = -64000;  -- 64MB cache (negative = KB)\n\n-- Memory-mapped I/O for large databases\nPRAGMA mmap_size = 268435456;  -- 256MB\n\n-- Synchronous mode (careful: affects durability)\nPRAGMA synchronous = NORMAL;  -- Default is FULL (safer but slower)\n```\n\n**Warning:** `PRAGMA synchronous = OFF` is dangerous (data loss risk), only use for imports if you can re-import\n\n---\n\n### Check Current Settings\n\n```bash\nsqlite3 data/analytics.db <<'EOF'\nPRAGMA journal_mode;\nPRAGMA cache_size;\nPRAGMA mmap_size;\nPRAGMA synchronous;\nEOF\n```\n\n---\n\n## Common Performance Patterns\n\n### Pattern 1: Optimize Common Filter Column\n\n```sql\n-- Before (slow)\nSELECT * FROM orders WHERE customer_id = 1001;\n-- Query plan: SCAN orders\n\n-- Create index\nCREATE INDEX idx_customer_id ON orders(customer_id);\n\n-- After (fast)\n-- Query plan: SEARCH orders USING INDEX idx_customer_id\n```\n\n---\n\n### Pattern 2: Optimize JOIN Performance\n\n```sql\n-- Diagnose\nEXPLAIN QUERY PLAN\nSELECT o.*, c.name\nFROM orders o\nJOIN customers c ON o.customer_id = c.id;\n\n-- If showing SCAN, create indexes\nCREATE INDEX idx_orders_customer_id ON orders(customer_id);\nCREATE INDEX idx_customers_id ON customers(id);  -- Usually PK, already indexed\n\n-- Re-check query plan\nEXPLAIN QUERY PLAN ...\n```\n\n---\n\n### Pattern 3: Optimize Sorted Queries\n\n```sql\n-- Before (slow with large result set)\nSELECT * FROM orders WHERE customer_id = 1001 ORDER BY order_date DESC;\n-- Query plan: USE TEMP B-TREE FOR ORDER BY\n\n-- Create compound index\nCREATE INDEX idx_customer_date ON orders(customer_id, order_date);\n\n-- After (fast)\n-- Query plan: SEARCH orders USING INDEX idx_customer_date\n```\n\n---\n\n### Pattern 4: Optimize Aggregations\n\n```sql\n-- Slow for large tables\nSELECT customer_id, COUNT(*) FROM orders GROUP BY customer_id;\n\n-- Create index on GROUP BY column\nCREATE INDEX idx_customer_id ON orders(customer_id);\n\n-- Much faster with index\n```\n\n---\n\n## Performance Troubleshooting\n\n| Symptom | Likely Cause | Fix |\n|---------|--------------|-----|\n| Query takes >10 seconds | Full table scan | Add index on WHERE columns |\n| JOIN is slow | Missing indexes on join keys | Index both sides of JOIN |\n| Bulk insert is slow | Auto-commit each row | Wrap in transaction |\n| ORDER BY is slow | Sorting large result set | Create index on sort columns |\n| Queries slower after import | Stale statistics | Run ANALYZE |\n| Multiple queries slow | Small cache | Increase PRAGMA cache_size |\n\n---\n\n## Real-World Example\n\n```bash\n# Scenario: Slow customer order history query\n\nDB=\"data/analytics.db\"\n\n# Step 1: Measure baseline\necho \"=== Baseline (no optimization) ===\"\nsqlite3 $DB <<'EOF'\n.timer on\nSELECT o.order_id, o.order_date, o.amount\nFROM orders o\nWHERE o.customer_id = 1001\nORDER BY o.order_date DESC\nLIMIT 100;\nEOF\n# Result: 2.5 seconds\n\n# Step 2: Diagnose\necho \"=== Query Plan Analysis ===\"\nsqlite3 $DB <<'EOF'\nEXPLAIN QUERY PLAN\nSELECT o.order_id, o.order_date, o.amount\nFROM orders o\nWHERE o.customer_id = 1001\nORDER BY o.order_date DESC\nLIMIT 100;\nEOF\n# Result: SCAN orders (no index)\n\n# Step 3: Create compound index (customer_id + order_date)\necho \"=== Creating Index ===\"\nsqlite3 $DB <<'EOF'\nCREATE INDEX idx_orders_customer_date ON orders(customer_id, order_date);\nEOF\n\n# Step 4: Update statistics\necho \"=== Updating Statistics ===\"\nsqlite3 $DB \"ANALYZE;\"\n\n# Step 5: Re-measure\necho \"=== After Optimization ===\"\nsqlite3 $DB <<'EOF'\n.timer on\nSELECT o.order_id, o.order_date, o.amount\nFROM orders o\nWHERE o.customer_id = 1001\nORDER BY o.order_date DESC\nLIMIT 100;\nEOF\n# Result: 0.008 seconds (300x faster!)\n\n# Step 6: Verify query plan\necho \"=== Verify Query Plan ===\"\nsqlite3 $DB <<'EOF'\nEXPLAIN QUERY PLAN\nSELECT o.order_id, o.order_date, o.amount\nFROM orders o\nWHERE o.customer_id = 1001\nORDER BY o.order_date DESC\nLIMIT 100;\nEOF\n# Result: SEARCH orders USING INDEX idx_orders_customer_date\n```\n\n---\n\n## Optimization Checklist\n\nBefore creating indexes:\n- [ ] Measured query time with .timer on\n- [ ] Ran EXPLAIN QUERY PLAN\n- [ ] Identified SCAN operations that should be SEARCH\n- [ ] Determined which columns are filtered/joined/sorted most\n\nAfter creating indexes:\n- [ ] Verified index created (.indexes table_name)\n- [ ] Ran ANALYZE to update statistics\n- [ ] Re-ran EXPLAIN QUERY PLAN (should show SEARCH with index)\n- [ ] Re-measured query time (should be significantly faster)\n- [ ] Documented optimization in analysis notes\n\n---\n\n## When NOT to Optimize\n\n**Don't optimize when:**\n- Query runs in <1 second (fast enough)\n- Table has <1000 rows (insignificant)\n- Index would be larger than table (overhead not worth it)\n- Column has only 2-3 distinct values (index not selective)\n- One-time query (won't be repeated)\n\n**Remember:** Premature optimization wastes time. Measure first, optimize only what matters.\n\n---\n\n## Quick Reference\n\n### Diagnose\n```sql\nEXPLAIN QUERY PLAN SELECT ...;  -- See if using indexes\n.timer on                       -- Measure query time\n```\n\n### Optimize\n```sql\nCREATE INDEX idx_name ON table(column);  -- Single column\nCREATE INDEX idx_name ON table(col1, col2);  -- Compound\nANALYZE;  -- Update statistics\n```\n\n### Verify\n```sql\n.indexes table_name  -- List indexes\nEXPLAIN QUERY PLAN SELECT ...;  -- Verify using index\n```\n\n### Bulk Operations\n```python\ncursor.execute(\"BEGIN TRANSACTION\")\n# ... many inserts ...\nconn.commit()\n```\n\n**Rule of thumb:** If query takes >1 second and runs frequently, investigate with EXPLAIN QUERY PLAN.\n",
        "plugins/datapeeker/skills/using-sqlite/writing-queries.md": "# Writing Queries - SQLite-Specific Patterns\n\n## When to Use This Guidance\n\nUse when you need:\n- **SQLite-specific syntax** (STRFTIME, date handling)\n- **DataPeeker query conventions** (NULL counting, percentage calculations)\n- **Common verification patterns** used across DataPeeker skills\n\n**Note:** For general SQL query formulation (applicable to Postgres, Athena, etc.), use the `writing-queries` skill. This guidance focuses on SQLite idioms and DataPeeker conventions.\n\n## DataPeeker Query Conventions\n\n### 1. NULL Counting (Efficient Pattern)\n\n**DataPeeker Standard:**\n```sql\nSELECT\n    COUNT(*) as total_rows,\n    COUNT(column) as non_null_count,\n    COUNT(*) - COUNT(column) as null_count,\n    ROUND(100.0 * (COUNT(*) - COUNT(column)) / COUNT(*), 2) as null_percentage\nFROM table_name;\n```\n\n**Why this pattern:**\n- `COUNT(*) - COUNT(column)` is more efficient than CASE WHEN\n- Consistently used across all DataPeeker skills\n- Clear and readable\n\n**Don't use:**\n```sql\n-- Less efficient\nCOUNT(CASE WHEN column IS NULL THEN 1 END)\n```\n\n---\n\n### 2. Percentage Calculations (Float Division)\n\n**DataPeeker Standard:**\n```sql\nSELECT\n    category,\n    COUNT(*) as count,\n    ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM table_name), 2) as percentage\nFROM table_name\nGROUP BY category;\n```\n\n**Critical:** Use `100.0` (not `100`) for float division\n\n**Why:**\n- `100 * x / y` does integer division, losing decimals\n- `100.0 * x / y` does float division, preserving accuracy\n- Subquery ensures correct total denominator\n\n---\n\n### 3. Empty String vs NULL Detection\n\n```sql\nSELECT COUNT(*) as problematic_rows\nFROM table_name\nWHERE TRIM(text_column) = '' OR text_column IS NULL;\n```\n\n**Pattern:** Always check BOTH empty strings and NULL for text columns\n\n---\n\n## SQLite Date/Time Handling\n\n### STRFTIME - The Core Date Function\n\n**Month grouping (most common):**\n```sql\nSELECT\n    STRFTIME('%Y-%m', date_column) as year_month,\n    COUNT(*) as count\nFROM table_name\nGROUP BY year_month\nORDER BY year_month;\n```\n\n**Common formats:**\n```sql\nSTRFTIME('%Y-%m-%d', date_col)  -- Full date: 2025-01-15\nSTRFTIME('%Y-%m', date_col)      -- Year-month: 2025-01\nSTRFTIME('%Y', date_col)         -- Year only: 2025\nSTRFTIME('%m', date_col)         -- Month only: 01\nSTRFTIME('%d', date_col)         -- Day only: 15\nSTRFTIME('%w', date_col)         -- Day of week: 0-6 (0=Sunday)\nSTRFTIME('%Y-Q%q', date_col)     -- Quarter: 2025-Q1 (INVALID, see below)\n```\n\n**Quarter calculation (SQLite doesn't have %q):**\n```sql\nSELECT\n    STRFTIME('%Y', date_col) ||\n    '-Q' ||\n    CAST(((CAST(STRFTIME('%m', date_col) AS INTEGER) - 1) / 3) + 1 AS TEXT) as quarter,\n    COUNT(*) as count\nFROM table_name\nGROUP BY quarter;\n```\n\n---\n\n### Weekend vs Weekday Pattern\n\n```sql\nSELECT\n    CASE\n        WHEN CAST(STRFTIME('%w', date_column) AS INTEGER) IN (0, 6)\n        THEN 'Weekend'\n        ELSE 'Weekday'\n    END as day_type,\n    COUNT(*) as count,\n    SUM(amount) as total\nFROM table_name\nGROUP BY day_type;\n```\n\n**Note:** `%w` returns 0 for Sunday, 6 for Saturday\n\n---\n\n### Date Range Filtering\n\n```sql\n-- Single date\nWHERE DATE(date_column) = '2025-01-15'\n\n-- Date range\nWHERE DATE(date_column) BETWEEN '2025-01-01' AND '2025-01-31'\n\n-- Last 30 days\nWHERE DATE(date_column) >= DATE('now', '-30 days')\n```\n\n---\n\n## Common Verification Queries\n\n### After Import: Row Count Verification\n\n```sql\nSELECT COUNT(*) as row_count FROM raw_table_name;\n```\n\n**Always run after .import command**\n\n---\n\n### After Import: Sample Data Check\n\n```sql\nSELECT * FROM raw_table_name LIMIT 5;\n```\n\n**Purpose:** Verify data looks correct\n\n---\n\n### Column Completeness Check\n\n```sql\nSELECT\n    'column1' as column_name,\n    COUNT(*) as total,\n    COUNT(column1) as non_null,\n    COUNT(*) - COUNT(column1) as nulls\nUNION ALL\nSELECT\n    'column2',\n    COUNT(*),\n    COUNT(column2),\n    COUNT(*) - COUNT(column2)\nFROM table_name;\n```\n\n**Use when:** Checking multiple columns for completeness\n\n---\n\n### Schema Verification After Table Creation\n\n```sql\nPRAGMA table_info(raw_table_name);\n```\n\n**Purpose:** Confirm table structure matches design\n\n---\n\n## Relationship Validation Patterns\n\n### Foreign Key Value Overlap\n\n```sql\nSELECT\n    COUNT(DISTINCT fk_column) as fk_values_in_table_a,\n    (SELECT COUNT(DISTINCT id_column) FROM table_b) as pk_values_in_table_b,\n    (SELECT COUNT(DISTINCT fk_column)\n     FROM table_a\n     WHERE fk_column IN (SELECT id_column FROM table_b)) as matching_values\nFROM table_a;\n```\n\n**Purpose:** Assess referential integrity before joining\n\n---\n\n### Join Cardinality Assessment\n\n```sql\nSELECT\n    COUNT(*) as rows_in_a,\n    COUNT(DISTINCT b.id_column) as matched_in_b,\n    COUNT(*) - COUNT(b.id_column) as unmatched_rows\nFROM table_a a\nLEFT JOIN table_b b ON a.fk_column = b.id_column;\n```\n\n**Purpose:** Understand join behavior (1-1 vs 1-many)\n\n---\n\n## Data Distribution Patterns\n\n### Categorical Distribution with Percentage\n\n```sql\nSELECT\n    category_column,\n    COUNT(*) as frequency,\n    ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM table_name), 2) as percentage\nFROM table_name\nGROUP BY category_column\nORDER BY frequency DESC;\n```\n\n---\n\n### Numeric Range Analysis\n\n```sql\nSELECT\n    MIN(numeric_column) as min_value,\n    MAX(numeric_column) as max_value,\n    AVG(numeric_column) as avg_value,\n    ROUND(AVG(numeric_column), 2) as avg_rounded,\n    COUNT(CASE WHEN numeric_column < 0 THEN 1 END) as negative_count,\n    COUNT(CASE WHEN numeric_column = 0 THEN 1 END) as zero_count\nFROM table_name;\n```\n\n**Purpose:** Detect impossible values, identify outliers\n\n---\n\n### Temporal Distribution\n\n```sql\nSELECT\n    STRFTIME('%Y-%m', date_column) as month,\n    COUNT(*) as row_count,\n    MIN(date_column) as earliest_date,\n    MAX(date_column) as latest_date\nFROM table_name\nGROUP BY month\nORDER BY month;\n```\n\n**Purpose:** Analyze temporal coverage, detect gaps\n\n---\n\n## Common Mistakes\n\n| Mistake | Problem | Fix |\n|---------|---------|-----|\n| `100 * COUNT(*) / total` | Integer division, lose decimals | Use `100.0` for float division |\n| `COUNT(CASE WHEN x IS NULL...)` | Less efficient NULL counting | Use `COUNT(*) - COUNT(x)` |\n| Not trimming text before empty check | Miss whitespace-only values | Use `TRIM(col) = ''` |\n| Using `%q` for quarter | SQLite doesn't support | Calculate from month number |\n| Forgetting CAST on STRFTIME | String comparison, not numeric | `CAST(STRFTIME(...) AS INTEGER)` |\n| Not specifying DATE() in WHERE | Compares timestamps, not dates | Use `DATE(col) = '2025-01-15'` |\n\n---\n\n## DataPeeker-Specific Patterns\n\n### Import Verification (Standard Pattern)\n\n```sql\n-- 1. Row count\nSELECT COUNT(*) FROM raw_table;\n\n-- 2. Sample data\nSELECT * FROM raw_table LIMIT 5;\n\n-- 3. Column completeness\nSELECT\n    COUNT(*) - COUNT(critical_col1) as col1_nulls,\n    COUNT(*) - COUNT(critical_col2) as col2_nulls\nFROM raw_table;\n\n-- 4. Schema verification\nPRAGMA table_info(raw_table);\n```\n\n**Always run all four after import**\n\n---\n\n### Quality Assessment Queries\n\n**Common data profiling patterns:**\n\n```sql\n-- NULL distribution\nSELECT\n    'critical_column' as column_name,\n    COUNT(*) - COUNT(critical_column) as null_count,\n    ROUND(100.0 * (COUNT(*) - COUNT(critical_column)) / COUNT(*), 2) as null_pct\nFROM table_name;\n\n-- Value ranges\nSELECT\n    MIN(numeric_col) as min,\n    MAX(numeric_col) as max,\n    AVG(numeric_col) as avg\nFROM table_name;\n\n-- Date ranges (SQLite-specific)\nSELECT\n    MIN(DATE(date_col)) as earliest,\n    MAX(DATE(date_col)) as latest,\n    JULIANDAY(MAX(DATE(date_col))) - JULIANDAY(MIN(DATE(date_col))) as days_span\nFROM table_name;\n```\n\n**Note:** These patterns align with database-agnostic data profiling best practices.\n\n---\n\n## Real-World Example\n\n```sql\n-- Scenario: Analyze sales patterns by day of week\n\nSELECT\n    CASE\n        WHEN CAST(STRFTIME('%w', transaction_date) AS INTEGER) IN (0, 6)\n        THEN 'Weekend'\n        ELSE 'Weekday'\n    END as day_type,\n    COUNT(*) as transaction_count,\n    SUM(amount) as total_sales,\n    ROUND(AVG(amount), 2) as avg_transaction,\n    ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM sales), 2) as pct_of_transactions,\n    ROUND(SUM(amount) / COUNT(DISTINCT DATE(transaction_date)), 2) as avg_daily_sales\nFROM sales\nWHERE DATE(transaction_date) BETWEEN '2025-01-01' AND '2025-03-31'\nGROUP BY day_type;\n```\n\n**Demonstrates:**\n- STRFTIME date handling\n- Weekend/weekday CASE pattern\n- Float division percentage (100.0)\n- Multiple aggregations\n- DATE filtering\n- Average per day calculation\n\n---\n\n## Quick Reference\n\n### NULL Handling\n```sql\nCOUNT(*) - COUNT(col)  -- NULL count (efficient)\nROUND(100.0 * (COUNT(*) - COUNT(col)) / COUNT(*), 2)  -- NULL percentage\n```\n\n### Percentages\n```sql\nROUND(100.0 * value / total, 2)  -- Always 100.0 for float division\n```\n\n### Date Grouping\n```sql\nSTRFTIME('%Y-%m', date_col)  -- Month: 2025-01\nSTRFTIME('%w', date_col)     -- Day of week: 0-6\n```\n\n### Empty String Check\n```sql\nWHERE TRIM(text_col) = '' OR text_col IS NULL\n```\n\n### Verification After Import\n```sql\nSELECT COUNT(*) FROM raw_table;  -- Row count\nSELECT * FROM raw_table LIMIT 5;  -- Sample\nPRAGMA table_info(raw_table);     -- Schema\n```\n\n---\n\n## Checklist\n\nBefore finalizing a query:\n- [ ] Used 100.0 (not 100) for percentages\n- [ ] Used COUNT(*) - COUNT(col) for NULL counting\n- [ ] Checked both TRIM(col) = '' AND col IS NULL for text\n- [ ] Used STRFTIME for date formatting\n- [ ] Cast STRFTIME results to INTEGER if comparing numerically\n- [ ] Used DATE(col) in WHERE clause for date filtering\n- [ ] Rounded percentages to 2 decimals\n\n**Following these patterns:** Ensures consistency with all DataPeeker skills and verified best practices.\n",
        "plugins/datapeeker/skills/writing-queries/SKILL.md": "---\nname: writing-queries\ndescription: Component skill for systematic SQL query development in DataPeeker analysis sessions\n---\n\n# Writing Queries\n\n## Purpose\n\nThis component skill guides systematic query development for analytics. Use it when:\n- Writing SQL queries for analysis tasks\n- Need to ensure queries are correct, documented, and reproducible\n- Referenced by process skills requiring query execution\n- Struggling with how to express an analytical question in SQL\n\n## Prerequisites\n\n- CSV files imported to database (relational database with SQL support)\n- SQL query tool available (database CLI, IDE, or query interface)\n- Understanding of table schema (use `understanding-data` skill if needed)\n- Clear analytical question to answer\n\n## Query Development Process\n\nCreate a TodoWrite checklist for the 5-phase query development process:\n\n```\nPhase 1: Clarify Analytical Question\nPhase 2: Design Query Logic\nPhase 3: Write SQL\nPhase 4: Verify Results\nPhase 5: Document Query\n```\n\nMark each phase as you complete it. Save queries in numbered markdown files.\n\n---\n\n## Phase 1: Clarify Analytical Question\n\n**Goal:** Translate vague analytical question into specific, answerable query requirements.\n\n### Ask Clarifying Questions\n\nBefore writing any SQL, ensure you can answer:\n\n1. **What exactly are we measuring?**\n   - Specific metric name and calculation method\n   - Example: \"Total revenue\" vs \"Average revenue per customer\" vs \"Revenue per day\"\n\n2. **What is the unit of analysis?**\n   - Per transaction, per customer, per day, per product, etc.\n   - Example: \"Daily sales\" means one row per date\n\n3. **What time period?**\n   - All time, specific date range, rolling window?\n   - How to handle incomplete periods (partial weeks/months)?\n\n4. **What filters apply?**\n   - Specific products, regions, customer segments?\n   - Exclude returns, cancelled orders, test data?\n\n5. **What groupings/segments?**\n   - By product category, by region, by customer type?\n   - Or overall aggregate with no grouping?\n\n6. **What comparison or context?**\n   - Compared to previous period, to average, to another segment?\n   - Rank ordering, percentage of total?\n\n### Document Requirements\n\nBefore proceeding to Phase 2, write down:\n\n```markdown\n## Query Requirements\n\n**Analytical Question:** [Original question in plain language]\n\n**Specific Metric:** [Exact calculation]\n- Formula: [e.g., SUM(amount) / COUNT(DISTINCT customer_id)]\n- Name: [e.g., \"Average Revenue Per Customer\"]\n\n**Unit of Analysis:** [e.g., \"One row per product category\"]\n\n**Time Period:** [e.g., \"January 1 - March 31, 2024\"]\n\n**Filters:**\n- [Filter 1: e.g., \"Exclude cancelled orders\"]\n- [Filter 2: e.g., \"Only completed transactions\"]\n\n**Grouping:** [e.g., \"Group by product_category\"]\n\n**Ordering:** [e.g., \"Order by total_revenue DESC, show top 10\"]\n```\n\n**Don't proceed until requirements are crystal clear.**\n\n---\n\n## Phase 2: Design Query Logic\n\n**Goal:** Plan query structure before writing SQL syntax.\n\n### Identify Required Tables\n\nList all tables needed and why:\n\n```markdown\n## Tables Needed\n\n1. **orders** - Contains transaction dates and amounts\n   - Join key: order_id\n   - Columns: order_date, total_amount, customer_id\n\n2. **customers** - Contains customer segment information\n   - Join key: customer_id\n   - Columns: customer_id, segment, region\n\n3. **order_items** - Contains product details (if needed)\n   - Join key: order_id\n   - Columns: product_id, quantity, price\n```\n\n### Design Join Strategy\n\nIf multiple tables are needed:\n\n```markdown\n## Join Logic\n\nMain table: orders (one row per transaction)\n\nLEFT JOIN customers ON orders.customer_id = customers.customer_id\n- Type: LEFT (to keep orders even if customer record missing)\n- Cardinality: many-to-one (many orders per customer)\n- Risk: None - customer_id should always match\n\nINNER JOIN order_items ON orders.order_id = order_items.order_id\n- Type: INNER (only want orders with items)\n- Cardinality: one-to-many (multiple items per order)\n- Risk: Row explosion - order totals will be duplicated\n- Mitigation: Calculate item metrics separately or use DISTINCT\n```\n\n**Check join cardinality before complex joins to avoid row explosion.**\n\n### Plan Calculation Steps\n\nBreak complex calculations into logical steps:\n\n```markdown\n## Calculation Steps\n\nStep 1: Filter to desired date range and conditions\n- WHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\n- AND status = 'completed'\n\nStep 2: Calculate base metrics\n- Total revenue: SUM(amount)\n- Transaction count: COUNT(*)\n- Unique customers: COUNT(DISTINCT customer_id)\n\nStep 3: Derive calculated metrics\n- Average revenue per customer: total_revenue / unique_customers\n\nStep 4: Group and aggregate\n- GROUP BY product_category\n\nStep 5: Order results\n- ORDER BY total_revenue DESC\n- LIMIT 10\n```\n\n### Consider Edge Cases\n\nDocument potential issues:\n\n```markdown\n## Edge Cases to Handle\n\n1. **NULL values:** How to handle NULL in date, amount, or category?\n   - Decision: Exclude rows with NULL in critical columns\n\n2. **Zero division:** What if denominator is zero (e.g., no customers)?\n   - Decision: Use NULLIF or CASE to avoid division by zero\n\n3. **Duplicate records:** Could there be duplicate transactions?\n   - Decision: Check with COUNT(*) vs COUNT(DISTINCT order_id)\n\n4. **Date formatting:** Are dates stored as strings or native DATE type?\n   - Decision: Use database date functions to extract components, verify format is consistent\n```\n\n---\n\n## Phase 3: Write SQL\n\n**Goal:** Translate design into clean, commented SQL.\n\n### SQL Best Practices\n\n**DO:**\n\n1. **Use clear formatting:**\n```sql\nSELECT\n  column1,\n  column2,\n  SUM(column3) as total\nFROM table_name\nWHERE condition = 'value'\nGROUP BY column1, column2\nORDER BY total DESC;\n```\n\n2. **Add comments for clarity:**\n```sql\n-- Calculate daily sales metrics for Q1 2024\nSELECT\n  -- Extract date component (function varies by database)\n  -- SQLite: DATE(order_date)\n  -- PostgreSQL: order_date::date or DATE(order_date)\n  -- MySQL: DATE(order_date)\n  CAST(order_date AS DATE) as sale_date,\n  COUNT(*) as transaction_count,  -- Total orders per day\n  SUM(amount) as total_revenue,   -- Gross revenue before returns\n  ROUND(AVG(amount), 2) as avg_order_value\nFROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\n  AND status = 'completed'  -- Exclude cancelled/pending\nGROUP BY CAST(order_date AS DATE)\nORDER BY sale_date;\n```\n\n3. **Use meaningful aliases:**\n```sql\n-- Good\nSELECT product_category as category, SUM(amount) as total_revenue\n\n-- Bad\nSELECT product_category as pc, SUM(amount) as t\n```\n\n4. **Handle NULLs explicitly:**\n```sql\nSELECT\n  customer_id,\n  COUNT(order_id) as order_count,\n  COALESCE(SUM(amount), 0) as total_spent  -- Replace NULL with 0\nFROM orders\nWHERE customer_id IS NOT NULL  -- Explicit NULL handling\nGROUP BY customer_id;\n```\n\n5. **Use CTEs for complex queries:**\n```sql\n-- Break complex logic into readable steps\nWITH daily_sales AS (\n  SELECT\n    DATE(order_date) as sale_date,\n    SUM(amount) as revenue\n  FROM orders\n  WHERE status = 'completed'\n  GROUP BY DATE(order_date)\n),\ndaily_stats AS (\n  SELECT\n    AVG(revenue) as avg_daily_revenue,\n    -- SQLite doesn't have STDEV() - calculate manually using variance formula\n    SQRT(AVG(revenue * revenue) - AVG(revenue) * AVG(revenue)) as stddev_revenue\n  FROM daily_sales\n)\nSELECT\n  ds.sale_date,\n  ds.revenue,\n  ROUND((ds.revenue - dst.avg_daily_revenue) / dst.stddev_revenue, 2) as z_score\nFROM daily_sales ds\nCROSS JOIN daily_stats dst\nORDER BY ds.sale_date;\n```\n\n**DON'T:**\n\n1. **Don't use SELECT * in analysis queries:**\n```sql\n-- Bad - unclear what columns you're using\nSELECT * FROM orders WHERE amount > 100;\n\n-- Good - explicit about needed columns\nSELECT order_id, order_date, amount FROM orders WHERE amount > 100;\n```\n\n2. **Don't ignore case sensitivity in comparisons:**\n```sql\n-- Risky - might miss 'PENDING' or 'Pending'\nWHERE status = 'pending'\n\n-- Better - normalize case\nWHERE LOWER(status) = 'pending'\n```\n\n3. **Don't create ambiguous joins:**\n```sql\n-- Bad - which table's date column?\nSELECT date, amount FROM orders JOIN shipments USING (order_id);\n\n-- Good - explicit table prefixes\nSELECT o.order_date, o.amount, s.ship_date\nFROM orders o\nJOIN shipments s ON o.order_id = s.order_id;\n```\n\n4. **Don't forget GROUP BY requirements:**\n```sql\n-- Wrong - product_category not in GROUP BY\nSELECT product_category, SUM(amount) FROM orders GROUP BY customer_id;\n\n-- Correct\nSELECT product_category, SUM(amount) FROM orders GROUP BY product_category;\n```\n\n---\n\n## Phase 4: Verify Results\n\n**Goal:** Ensure query returns correct, sensible results.\n\n### Verification Checklist\n\nBefore trusting query results, verify:\n\n**1. Row count makes sense:**\n```sql\n-- Check: Does row count match expectations?\nSELECT COUNT(*) as row_count FROM (\n  -- Your query here\n);\n```\n\nExpected: If grouping by day-of-week, should have 7 rows.\nIf grouping by month, should have 12 rows (or fewer if partial year).\n\n**2. Aggregates are reasonable:**\n```sql\n-- Check: Are totals in expected range?\nSELECT\n  SUM(amount) as total_revenue,\n  COUNT(*) as transaction_count,\n  MIN(amount) as min_amount,\n  MAX(amount) as max_amount,\n  AVG(amount) as avg_amount\nFROM orders;\n```\n\nAsk: Do these values make business sense? Any impossibly high/low values?\n\n**3. No unexpected NULLs:**\n```sql\n-- Check: Are there NULL values in results?\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(column1) as column1_non_null,\n  COUNT(column2) as column2_non_null\nFROM (\n  -- Your query here\n);\n```\n\nIf critical columns have NULLs, investigate why.\n\n**4. Percentages sum to 100% (if applicable):**\n```sql\n-- Check: Do percentage columns sum to 100?\nSELECT SUM(pct_of_total) as total_pct FROM (\n  SELECT\n    category,\n    100.0 * COUNT(*) / SUM(COUNT(*)) OVER () as pct_of_total\n  FROM orders\n  GROUP BY category\n);\n```\n\nShould equal 100.0 (or close, accounting for rounding).\n\n**5. Spot check specific values:**\n\nPick a specific row from results and verify manually:\n\n```sql\n-- Example: Verify March 15 sales total\nSELECT SUM(amount) FROM orders WHERE CAST(order_date AS DATE) = '2024-03-15';\n```\n\nCompare to what your main query shows for March 15.\n\n**6. Compare to known totals:**\n\n```sql\n-- Check: Does grouped data sum to overall total?\n-- Total from grouped query:\nSELECT SUM(category_total) FROM (\n  SELECT category, SUM(amount) as category_total\n  FROM orders\n  GROUP BY category\n);\n\n-- Should equal overall total:\nSELECT SUM(amount) FROM orders;\n```\n\n### Common Query Errors\n\n**Row explosion from joins:**\n```sql\n-- Symptom: Totals are too high after joining\n-- Cause: One-to-many join duplicates rows\n-- Fix: Use DISTINCT or calculate at appropriate grain\n\n-- Wrong - duplicates order amounts\nSELECT o.order_id, SUM(o.amount)\nFROM orders o\nJOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id;  -- order amount counted once per item!\n\n-- Right - calculate order and item metrics separately\nSELECT\n  o.order_id,\n  o.amount as order_total,\n  COUNT(oi.item_id) as item_count\nFROM orders o\nLEFT JOIN order_items oi ON o.order_id = oi.order_id\nGROUP BY o.order_id, o.amount;\n```\n\n**Incorrect grouping:**\n```sql\n-- Symptom: Unexpected row count or aggregates\n-- Cause: Missing or extra columns in GROUP BY\n-- Fix: Include ALL non-aggregated columns in GROUP BY\n\n-- Wrong - missing category from GROUP BY\nSELECT category, product, SUM(sales)\nFROM products\nGROUP BY category;  -- Error or wrong results\n\n-- Right\nSELECT category, product, SUM(sales)\nFROM products\nGROUP BY category, product;\n```\n\n**Date/time parsing errors:**\n```sql\n-- Symptom: Date groupings don't match expectations\n-- Cause: Date format inconsistency or wrong extraction\n-- Fix: Verify date format and use appropriate database function\n\n-- Check date format first\nSELECT DISTINCT date_column FROM table LIMIT 10;\n\n-- Extract date components using database-specific functions\n-- Postgres: TO_CHAR, DATE_TRUNC, EXTRACT\n-- MySQL: DATE_FORMAT, YEAR, MONTH\n-- SQLite: STRFTIME, DATE\n-- See database documentation for specific syntax\n\n-- Generic approach: Cast to DATE type\nSELECT CAST(date_column AS DATE) FROM table;\n```\n\n---\n\n## Phase 5: Document Query\n\n**Goal:** Create reproducible documentation for query and results.\n\n### Query Documentation Template\n\nSave each query in a numbered markdown file:\n\n```markdown\n# [Query Purpose]\n\n## Analytical Question\n[What question does this query answer?]\n\nExample: \"What are the top 10 product categories by revenue in Q1 2024?\"\n\n## Rationale\n[Why is this query needed for the analysis?]\n\nExample: \"Identifies high-value categories to prioritize for Q2 marketing campaigns.\"\n\n## Query\n```sql\n-- Clear, commented SQL\nSELECT\n  product_category,\n  COUNT(DISTINCT order_id) as order_count,\n  SUM(amount) as total_revenue,\n  ROUND(AVG(amount), 2) as avg_order_value\nFROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\n  AND status = 'completed'\nGROUP BY product_category\nORDER BY total_revenue DESC\nLIMIT 10;\n```\n\n## Results\n[Paste actual query results - raw output]\n\n```\nproduct_category | order_count | total_revenue | avg_order_value\nElectronics      | 1523        | 345678.90     | 226.89\nHome & Garden    | 2341        | 298432.10     | 127.48\nClothing         | 3456        | 234567.80     | 67.89\n...\n```\n\n## Interpretation\n[What do the results show? Key takeaways.]\n\n- Electronics generated highest revenue ($345K) despite fewer orders (1,523)\n- Electronics has highest average order value ($227) - premium category\n- Clothing has most orders (3,456) but lower AOV ($68) - volume category\n\n## Data Quality Notes\n[Any issues or caveats about the data]\n\n- No NULL categories found\n- Date range: 90 complete days (Jan 1 - Mar 31)\n- Excluded 47 cancelled orders from analysis\n```\n\n### Documentation Best Practices\n\n**DO:**\n- Save queries in version control (git)\n- Include exact SQL that was run\n- Paste actual results, not summaries\n- Note any data quality issues discovered\n- Date your analysis files\n\n**DON'T:**\n- Paraphrase results (show actual numbers)\n- Round or simplify results prematurely\n- Skip documenting failed queries (document what didn't work and why)\n- Forget to note filters and exclusions\n\n---\n\n## Common Query Patterns\n\n### Pattern 1: Time Series Analysis\n\n**Use case:** Analyze metrics over time (daily, weekly, monthly trends)\n\n```sql\n-- Daily trend\nSELECT\n  CAST(order_date AS DATE) as date,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue,\n  ROUND(AVG(amount), 2) as avg_order_value\nFROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\nGROUP BY CAST(order_date AS DATE)\nORDER BY date;\n\n-- Weekly trend (implementation varies by database)\n-- Postgres: DATE_TRUNC('week', order_date)\n-- MySQL: DATE_SUB(order_date, INTERVAL WEEKDAY(order_date) DAY)\n-- SQLite: DATE(order_date, 'weekday 0', '-6 days')\nSELECT\n  -- Use database-specific function to get week start date\n  [week_start_function] as week_start,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue\nFROM orders\nWHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\nGROUP BY week_start\nORDER BY week_start;\n\n-- Monthly trend (implementation varies by database)\n-- Postgres: TO_CHAR(order_date, 'YYYY-MM')\n-- MySQL: DATE_FORMAT(order_date, '%Y-%m')\n-- SQLite: STRFTIME('%Y-%m', order_date)\nSELECT\n  -- Use database-specific function to extract year-month\n  [year_month_function] as year_month,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue\nFROM orders\nGROUP BY year_month\nORDER BY year_month;\n```\n\n### Pattern 2: Segmentation Analysis\n\n**Use case:** Compare metrics across categories or segments\n\n```sql\n-- Basic segmentation\nSELECT\n  segment,\n  COUNT(*) as count,\n  SUM(amount) as total,\n  ROUND(AVG(amount), 2) as average,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct_of_total\nFROM orders\nGROUP BY segment\nORDER BY total DESC;\n\n-- Multi-level segmentation\nSELECT\n  region,\n  customer_type,\n  COUNT(*) as order_count,\n  SUM(amount) as revenue\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nGROUP BY region, customer_type\nORDER BY region, revenue DESC;\n```\n\n### Pattern 3: Ranking and Top N\n\n**Use case:** Identify top/bottom performers\n\n```sql\n-- Top 10 by revenue\nSELECT\n  product_name,\n  SUM(amount) as total_revenue\nFROM order_items oi\nJOIN products p ON oi.product_id = p.product_id\nGROUP BY product_name\nORDER BY total_revenue DESC\nLIMIT 10;\n\n-- Top 20% of customers (by revenue)\n-- NOTE: PERCENT_RANK() requires SQLite 3.28+\nWITH customer_revenue AS (\n  SELECT\n    customer_id,\n    SUM(amount) as total_revenue\n  FROM orders\n  GROUP BY customer_id\n),\nranked_customers AS (\n  SELECT\n    customer_id,\n    total_revenue,\n    PERCENT_RANK() OVER (ORDER BY total_revenue DESC) as percentile_rank\n  FROM customer_revenue\n)\n-- Must use CTE to filter on window function results\nSELECT\n  customer_id,\n  total_revenue,\n  percentile_rank\nFROM ranked_customers\nWHERE percentile_rank <= 0.20\nORDER BY total_revenue DESC;\n\n-- Top 3 products per category\nWITH ranked_products AS (\n  SELECT\n    category,\n    product_name,\n    SUM(amount) as revenue,\n    ROW_NUMBER() OVER (PARTITION BY category ORDER BY SUM(amount) DESC) as rank\n  FROM products\n  GROUP BY category, product_name\n)\nSELECT category, product_name, revenue\nFROM ranked_products\nWHERE rank <= 3\nORDER BY category, rank;\n```\n\n### Pattern 4: Period-over-Period Comparison\n\n**Use case:** Compare current period to previous period\n\n```sql\n-- Month-over-month comparison\nWITH monthly_sales AS (\n  SELECT\n    -- Extract year-month (use database-specific function)\n    [year_month_function] as year_month,\n    SUM(amount) as revenue\n  FROM orders\n  GROUP BY year_month\n)\nSELECT\n  year_month,\n  revenue as current_month_revenue,\n  LAG(revenue) OVER (ORDER BY year_month) as previous_month_revenue,\n  revenue - LAG(revenue) OVER (ORDER BY year_month) as revenue_change,\n  ROUND(100.0 * (revenue - LAG(revenue) OVER (ORDER BY year_month)) /\n        LAG(revenue) OVER (ORDER BY year_month), 2) as pct_change\nFROM monthly_sales\nORDER BY year_month;\n\n-- Year-over-year comparison\n-- Extract month and year components (use database-specific functions)\nSELECT\n  [month_function] as month,\n  SUM(CASE WHEN [year_function] = '2023' THEN amount ELSE 0 END) as revenue_2023,\n  SUM(CASE WHEN [year_function] = '2024' THEN amount ELSE 0 END) as revenue_2024,\n  SUM(CASE WHEN [year_function] = '2024' THEN amount ELSE 0 END) -\n  SUM(CASE WHEN [year_function] = '2023' THEN amount ELSE 0 END) as yoy_change\nFROM orders\nWHERE [year_function] IN ('2023', '2024')\nGROUP BY month\nORDER BY month;\n```\n\n### Pattern 5: Cohort Analysis\n\n**Use case:** Analyze behavior of groups defined by time of first action\n\n```sql\n-- Customer cohort by first purchase month\nWITH first_purchase AS (\n  SELECT\n    customer_id,\n    -- Extract year-month from first purchase (use database-specific function)\n    [year_month_function(MIN(order_date))] as cohort_month\n  FROM orders\n  GROUP BY customer_id\n)\nSELECT\n  fp.cohort_month,\n  COUNT(DISTINCT o.customer_id) as customers,\n  COUNT(o.order_id) as total_orders,\n  SUM(o.amount) as total_revenue,\n  ROUND(1.0 * COUNT(o.order_id) / COUNT(DISTINCT o.customer_id), 2) as orders_per_customer\nFROM first_purchase fp\nJOIN orders o ON fp.customer_id = o.customer_id\nGROUP BY fp.cohort_month\nORDER BY fp.cohort_month;\n```\n\n### Pattern 6: Distribution Analysis\n\n**Use case:** Understand distribution of values (percentiles, buckets)\n\n```sql\n-- Value distribution by buckets\nSELECT\n  CASE\n    WHEN amount < 25 THEN '$0-24'\n    WHEN amount < 50 THEN '$25-49'\n    WHEN amount < 100 THEN '$50-99'\n    WHEN amount < 250 THEN '$100-249'\n    ELSE '$250+'\n  END as amount_bucket,\n  COUNT(*) as order_count,\n  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) as pct_of_orders,\n  SUM(amount) as total_revenue,\n  ROUND(100.0 * SUM(amount) / SUM(SUM(amount)) OVER (), 2) as pct_of_revenue\nFROM orders\nGROUP BY amount_bucket\nORDER BY MIN(amount);\n\n-- Percentile calculation (manual approach for SQLite)\nWITH ranked_orders AS (\n  SELECT\n    amount,\n    ROW_NUMBER() OVER (ORDER BY amount) as row_num,\n    COUNT(*) OVER () as total_count\n  FROM orders\n)\nSELECT\n  'P25' as percentile,\n  amount\nFROM ranked_orders\nWHERE row_num = CAST(total_count * 0.25 AS INTEGER)\nUNION ALL\nSELECT 'P50', amount\nFROM ranked_orders\nWHERE row_num = CAST(total_count * 0.50 AS INTEGER)\nUNION ALL\nSELECT 'P75', amount\nFROM ranked_orders\nWHERE row_num = CAST(total_count * 0.75 AS INTEGER);\n```\n\n---\n\n## Database-Specific Implementation\n\nThis skill provides database-agnostic query guidance. For database-specific syntax:\n\n- **SQLite**: Use the `using-sqlite` skill for SQLite-specific date functions (STRFTIME), CLI invocation patterns, and SQLite idioms\n- **PostgreSQL, MySQL, etc.**: Consult database documentation for date/time functions, string operations, and window function syntax\n\n**Core SQL patterns** (SELECT, JOIN, GROUP BY, CTEs, window functions) are consistent across databases. **Function syntax** (date extraction, string manipulation) varies by database.\n\n---\n\n## Anti-Patterns to Avoid\n\n### Anti-Pattern 1: Premature Aggregation\n\n**Problem:** Aggregating too early loses detail needed for further analysis\n\n```sql\n-- Bad - can't drill down further\nSELECT\n  category,\n  SUM(amount) as total\nFROM orders\nGROUP BY category;\n\n-- Better - keep grain, aggregate in visualization/reporting layer\n-- Or use CTE to preserve both detail and summary\nWITH category_totals AS (\n  SELECT category, SUM(amount) as total\n  FROM orders\n  GROUP BY category\n)\nSELECT\n  o.*,\n  ct.total as category_total,\n  ROUND(100.0 * o.amount / ct.total, 2) as pct_of_category\nFROM orders o\nJOIN category_totals ct ON o.category = ct.category;\n```\n\n### Anti-Pattern 2: Ignoring NULL Semantics\n\n**Problem:** NULLs behave unexpectedly in comparisons and aggregates\n\n```sql\n-- Bad - NULL != NULL, so this misses NULL values\nSELECT * FROM orders WHERE status != 'completed';\n\n-- Good - explicitly handle NULLs\nSELECT * FROM orders\nWHERE status != 'completed' OR status IS NULL;\n\n-- Bad - COUNT(*) includes NULLs, COUNT(column) doesn't\nSELECT COUNT(column) FROM table;  -- May not match row count!\n\n-- Good - be explicit\nSELECT\n  COUNT(*) as total_rows,\n  COUNT(column) as non_null_count,\n  COUNT(*) - COUNT(column) as null_count\nFROM table;\n```\n\n### Anti-Pattern 3: Implicit Type Conversion\n\n**Problem:** SQLite's dynamic typing can cause unexpected behavior\n\n```sql\n-- Bad - comparing number to string\nSELECT * FROM orders WHERE amount > '100';  -- Works but risky\n\n-- Good - explicit types\nSELECT * FROM orders WHERE CAST(amount AS REAL) > 100.0;\n\n-- Bad - date comparison as strings might fail\nWHERE date_column > '2024-1-1'  -- Might not work as expected\n\n-- Good - proper date format\nWHERE date_column > '2024-01-01'  -- ISO format: YYYY-MM-DD\n```\n\n### Anti-Pattern 4: Over-Reliance on Subqueries\n\n**Problem:** Nested subqueries are hard to read and debug\n\n```sql\n-- Bad - deeply nested subqueries\nSELECT * FROM (\n  SELECT * FROM (\n    SELECT * FROM orders WHERE amount > 100\n  ) WHERE category = 'Electronics'\n) WHERE order_date > '2024-01-01';\n\n-- Good - use CTEs for readability\nWITH high_value_orders AS (\n  SELECT * FROM orders WHERE amount > 100\n),\nelectronics_orders AS (\n  SELECT * FROM high_value_orders WHERE category = 'Electronics'\n)\nSELECT * FROM electronics_orders\nWHERE order_date > '2024-01-01';\n```\n\n### Anti-Pattern 5: Cartesian Products\n\n**Problem:** Forgetting join conditions creates row explosion\n\n```sql\n-- Bad - missing join condition\nSELECT o.*, c.*\nFROM orders o, customers c;  -- Every order paired with every customer!\n\n-- Good - explicit join\nSELECT o.*, c.*\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id;\n```\n\n---\n\n## Integration with Process Skills\n\nProcess skills reference this component skill with:\n\n```markdown\nUse the `writing-queries` component skill to develop SQL queries systematically, ensuring correctness and documentation.\n```\n\nWhen writing queries during analysis:\n1. Clarify what you're measuring (Phase 1)\n2. Design query logic before coding (Phase 2)\n3. Write clean, commented SQL (Phase 3)\n4. Verify results make sense (Phase 4)\n5. Document in numbered file (Phase 5)\n\nThis systematic approach prevents common SQL errors and ensures reproducible analysis.\n",
        "plugins/datapeeker/templates/_template/README.md": "# Analysis Session\n\nThis directory contains a complete analytical investigation session using DataPeeker.\n\n## Directory Structure\n\n```\n<analysis-name>/\nâ”œâ”€â”€ README.md                    # This file - session overview\nâ”œâ”€â”€ 00 - overview.md            # Analytical goal and metadata\nâ”œâ”€â”€ 01 - query.md               # First query and results\nâ”œâ”€â”€ 02 - query.md               # Second query and results\nâ””â”€â”€ ...                         # Additional queries as needed\n```\n\n## Numbered File Workflow\n\nEach analysis query is documented in a numbered markdown file (01, 02, 03, etc.) that includes:\n\n1. **Rationale** - Why are we running this query?\n2. **Query** - The SQL code (formatted in code blocks)\n3. **Results** - Output from the query (tables, counts, values)\n4. **Interpretation** - What do these results mean?\n\nThis creates a **linear, reproducible narrative** of your analytical journey.\n\n## Data Sources\n\nAll queries run against the SQLite database at `data/analytics.db`.\n\nTo see available tables and schemas:\n\n```sql\n-- List all tables\nSELECT name FROM sqlite_master WHERE type='table';\n\n-- Show schema for a table\nPRAGMA table_info(table_name);\n```\n\n## Session Metadata\n\nSee `00 - overview.md` for:\n- Analytical goal\n- Business context\n- Data sources used\n- Timeline\n- Key findings summary\n\n## Best Practices\n\n1. **One file per query** - Keep each investigation step separate\n2. **Sequential numbering** - Follow the chronological order (01, 02, 03...)\n3. **Document reasoning** - Always explain WHY before running a query\n4. **Include context** - Raw numbers need interpretation\n5. **Update overview** - Keep the overview file current as analysis progresses\n\n## Process Skills\n\nDataPeeker provides Claude Code Agent Skills for different analytical processes:\n\n- **hypothesis-testing** - Test specific claims with statistical rigor\n- **guided-investigation** - Answer complex questions systematically\n- **exploratory-analysis** - Discover patterns in new datasets\n- **comparative-analysis** - Compare segments or time periods\n\nEach skill provides a structured workflow with checkpoints and guidance.\n"
      },
      "plugins": [
        {
          "name": "datapeeker",
          "description": "Structured research methods for AI agents - hypothesis testing, exploratory analysis, comparative analysis, and qualitative research",
          "version": "1.0.0",
          "source": "./plugins/datapeeker",
          "author": {
            "name": "Tilmon Engineering",
            "email": "team@tilmonengineering.com"
          },
          "keywords": [
            "data-analysis",
            "research-methods",
            "hypothesis-testing",
            "qualitative-research"
          ],
          "categories": [
            "data-analysis",
            "hypothesis-testing",
            "qualitative-research",
            "research-methods"
          ],
          "install_commands": [
            "/plugin marketplace add tilmon-engineering/claude-skills",
            "/plugin install datapeeker@tilmon-eng-skills"
          ]
        },
        {
          "name": "autonomy",
          "description": "Enable AI agents to iteratively self-direct in pursuit of open-ended goals with state continuity across conversations through iteration journals",
          "version": "1.0.0",
          "source": "./plugins/autonomy",
          "author": {
            "name": "Tilmon Engineering",
            "email": "team@tilmonengineering.com"
          },
          "keywords": [
            "autonomy",
            "iteration",
            "goal-pursuit",
            "state-continuity",
            "journal",
            "open-ended-goals"
          ],
          "categories": [
            "autonomy",
            "goal-pursuit",
            "iteration",
            "journal",
            "open-ended-goals",
            "state-continuity"
          ],
          "install_commands": [
            "/plugin marketplace add tilmon-engineering/claude-skills",
            "/plugin install autonomy@tilmon-eng-skills"
          ]
        }
      ]
    }
  ]
}