{
  "author": {
    "id": "raintree-technology",
    "display_name": "Raintree Technology",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/242706288?v=4",
    "url": "https://github.com/raintree-technology",
    "bio": "Apps & OSS Tooling",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 14,
      "total_commands": 7,
      "total_skills": 0,
      "total_stars": 39,
      "total_forks": 5
    }
  },
  "marketplaces": [
    {
      "name": "claude-starter-complete",
      "version": "2.0.0",
      "description": "Complete collection of 40+ production-ready skills including Stripe, Supabase, Expo, Plaid, Aptos, and more. Plus meta-commands, orchestration, and workflows.",
      "owner_info": {
        "name": "Raintree Technology",
        "url": "https://github.com/raintree-technology"
      },
      "keywords": [
        "stripe",
        "supabase",
        "plaid",
        "expo",
        "aptos",
        "blockchain",
        "payments",
        "backend",
        "mobile",
        "workflows",
        "orchestration",
        "meta-commands",
        "toon"
      ],
      "repo_full_name": "raintree-technology/claude-starter",
      "repo_url": "https://github.com/raintree-technology/claude-starter",
      "repo_description": "Platform-agnostic, production-ready Claude Code configurations with hooks, commands, skills, examples, and more.",
      "homepage": "",
      "signals": {
        "stars": 39,
        "forks": 5,
        "pushed_at": "2026-01-21T17:44:55Z",
        "created_at": "2025-11-08T01:08:10Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/README.md",
          "type": "blob",
          "size": 2713
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 3636
        },
        {
          "path": "templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/commands/debug",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/commands/debug/command-validate.md",
          "type": "blob",
          "size": 11399
        },
        {
          "path": "templates/.claude/commands/debug/explain-ranking.md",
          "type": "blob",
          "size": 9558
        },
        {
          "path": "templates/.claude/commands/debug/skill-graph.md",
          "type": "blob",
          "size": 5062
        },
        {
          "path": "templates/.claude/commands/debug/workflow-debug.md",
          "type": "blob",
          "size": 14008
        },
        {
          "path": "templates/.claude/commands/meta",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/commands/meta/create-command.md",
          "type": "blob",
          "size": 12957
        },
        {
          "path": "templates/.claude/commands/meta/edit-command.md",
          "type": "blob",
          "size": 9951
        },
        {
          "path": "templates/.claude/commands/meta/workflow-compose.md",
          "type": "blob",
          "size": 13023
        },
        {
          "path": "templates/.claude/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/anthropic",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-code",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-code/skill.md",
          "type": "blob",
          "size": 4937
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-command-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-command-builder/skill.md",
          "type": "blob",
          "size": 13087
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-hook-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-hook-builder/skill.md",
          "type": "blob",
          "size": 17830
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-mcp-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-mcp-expert/skill.md",
          "type": "blob",
          "size": 16092
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-settings-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-settings-expert/skill.md",
          "type": "blob",
          "size": 15039
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-skill-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/anthropic/claude-skill-builder/skill.md",
          "type": "blob",
          "size": 9958
        },
        {
          "path": "templates/.claude/skills/anthropic/skill.md",
          "type": "blob",
          "size": 4406
        },
        {
          "path": "templates/.claude/skills/aptos",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/dapp-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/dapp-integration/skill.md",
          "type": "blob",
          "size": 16175
        },
        {
          "path": "templates/.claude/skills/aptos/decibel",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/decibel/skill.md",
          "type": "blob",
          "size": 8234
        },
        {
          "path": "templates/.claude/skills/aptos/framework",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/framework/skill.md",
          "type": "blob",
          "size": 19778
        },
        {
          "path": "templates/.claude/skills/aptos/gas-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/gas-optimization/skill.md",
          "type": "blob",
          "size": 18715
        },
        {
          "path": "templates/.claude/skills/aptos/move-language",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/move-language/skill.md",
          "type": "blob",
          "size": 18828
        },
        {
          "path": "templates/.claude/skills/aptos/move-prover",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/move-prover/skill.md",
          "type": "blob",
          "size": 26391
        },
        {
          "path": "templates/.claude/skills/aptos/move-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/move-testing/skill.md",
          "type": "blob",
          "size": 11617
        },
        {
          "path": "templates/.claude/skills/aptos/object-model",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/object-model/skill.md",
          "type": "blob",
          "size": 20598
        },
        {
          "path": "templates/.claude/skills/aptos/shelby",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/cli-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/cli-assistant/skill.md",
          "type": "blob",
          "size": 19272
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/dapp-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/dapp-builder/skill.md",
          "type": "blob",
          "size": 25864
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/network-rpc",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/network-rpc/skill.md",
          "type": "blob",
          "size": 21329
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/protocol-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/protocol-expert/skill.md",
          "type": "blob",
          "size": 15742
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/sdk-developer",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/sdk-developer/skill.md",
          "type": "blob",
          "size": 12217
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/skill.md",
          "type": "blob",
          "size": 4555
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/smart-contracts",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/smart-contracts/skill.md",
          "type": "blob",
          "size": 18640
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/storage-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/shelby/storage-integration/skill.md",
          "type": "blob",
          "size": 24096
        },
        {
          "path": "templates/.claude/skills/aptos/skill.md",
          "type": "blob",
          "size": 7160
        },
        {
          "path": "templates/.claude/skills/aptos/token-standards",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/aptos/token-standards/skill.md",
          "type": "blob",
          "size": 17947
        },
        {
          "path": "templates/.claude/skills/expo",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/expo/eas-build",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/expo/eas-build/skill.md",
          "type": "blob",
          "size": 2767
        },
        {
          "path": "templates/.claude/skills/expo/eas-update",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/expo/eas-update/skill.md",
          "type": "blob",
          "size": 2834
        },
        {
          "path": "templates/.claude/skills/expo/expo-router",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/expo/expo-router/skill.md",
          "type": "blob",
          "size": 3279
        },
        {
          "path": "templates/.claude/skills/expo/skill.md",
          "type": "blob",
          "size": 6401
        },
        {
          "path": "templates/.claude/skills/ios",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/ios/skill.md",
          "type": "blob",
          "size": 6306
        },
        {
          "path": "templates/.claude/skills/plaid",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/plaid/accounts",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/plaid/accounts/skill.md",
          "type": "blob",
          "size": 5078
        },
        {
          "path": "templates/.claude/skills/plaid/auth",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/plaid/auth/skill.md",
          "type": "blob",
          "size": 3441
        },
        {
          "path": "templates/.claude/skills/plaid/identity",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/plaid/identity/skill.md",
          "type": "blob",
          "size": 4372
        },
        {
          "path": "templates/.claude/skills/plaid/skill.md",
          "type": "blob",
          "size": 12767
        },
        {
          "path": "templates/.claude/skills/plaid/transactions",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/plaid/transactions/skill.md",
          "type": "blob",
          "size": 4004
        },
        {
          "path": "templates/.claude/skills/shopify",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/shopify/skill.md",
          "type": "blob",
          "size": 13557
        },
        {
          "path": "templates/.claude/skills/stripe",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/stripe/skill.md",
          "type": "blob",
          "size": 53965
        },
        {
          "path": "templates/.claude/skills/supabase",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/supabase/skill.md",
          "type": "blob",
          "size": 59203
        },
        {
          "path": "templates/.claude/skills/toon-formatter",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/toon-formatter/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/toon-formatter/docs/INSTALL.md",
          "type": "blob",
          "size": 5086
        },
        {
          "path": "templates/.claude/skills/toon-formatter/docs/toon-guide.md",
          "type": "blob",
          "size": 14136
        },
        {
          "path": "templates/.claude/skills/toon-formatter/skill.md",
          "type": "blob",
          "size": 2240
        },
        {
          "path": "templates/.claude/skills/whop",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/.claude/skills/whop/skill.md",
          "type": "blob",
          "size": 21175
        }
      ],
      "files": {
        ".claude-plugin/README.md": "# Claude Starter Plugin Marketplace\n\nThis directory contains the plugin marketplace configuration for claude-starter, making it easy for users to install the entire collection of 40+ skills, commands, and workflows with a single command.\n\n## For Users: Installation\n\n### Install Everything\n```bash\n/plugin marketplace add raintree-technology/claude-starter\n```\n\n### Install Specific Categories\n```bash\n# Just payment skills\n/plugin marketplace add raintree-technology/claude-starter/stripe-payments\n\n# Just mobile development\n/plugin marketplace add raintree-technology/claude-starter/expo-mobile\n\n# Just blockchain/Aptos\n/plugin marketplace add raintree-technology/claude-starter/aptos-blockchain\n\n# Just automation tools\n/plugin marketplace add raintree-technology/claude-starter/meta-commands\n/plugin marketplace add raintree-technology/claude-starter/workflows\n```\n\n## What's Included\n\n### Skills (40 total across 10 categories)\n- **AI & Claude Code** (7 skills): claude-code, skill-builder, command-builder, hook-builder, settings-expert, mcp-expert\n- **Payments**: Stripe\n- **Backend**: Supabase\n- **Banking**: Plaid (5 skills)\n- **Mobile**: Expo (4 skills), iOS\n- **Blockchain**: Aptos (17 skills including Shelby Protocol)\n- **E-commerce**: Shopify, Whop\n- **Optimization**: TOON Formatter\n\n### Commands (14 total)\n- 5 TOON format commands\n- 2 marketplace commands\n- 3 meta-commands\n- 4 debugging commands\n\n### Workflows (4 total)\n- production-release\n- ci-pipeline\n- daily-maintenance\n- hotfix\n\n### Systems\n- Orchestration engine\n- Validation system\n- Command registry\n\n## For Developers: Publishing\n\nThis plugin marketplace configuration makes claude-starter discoverable in multiple ways:\n\n### 1. Via Claude Code Plugin System\nUsers can install with `/plugin marketplace add`\n\n### 2. Via SkillsMP\n- Automatically indexed when GitHub repo has 2+ stars\n- Each skill appears individually in search\n- Users discover via `/discover-skills`\n\n### 3. Via NPX\n```bash\nnpx create-claude-starter@latest\n```\n\n## Structure\n\n```\n.claude-plugin/\n├── marketplace.json    # Plugin marketplace configuration\n└── README.md          # This file\n```\n\n## Marketplace Metadata\n\nThe `marketplace.json` file defines:\n- 14 installable plugin units\n- Complete metadata (name, description, category)\n- Source paths to skill directories\n- Keywords for discovery\n\n## Updates\n\nWhen skills are updated in the main repository, the plugin marketplace automatically reflects the changes. No separate publication step needed.\n\n## Learn More\n\n- [Claude Code Plugin Marketplaces](https://code.claude.com/docs/en/plugin-marketplaces.md)\n- [Agent Skills Specification](https://agentskills.io/)\n- [SkillsMP](https://skillsmp.com/)\n",
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"claude-starter-complete\",\n  \"version\": \"2.0.0\",\n  \"description\": \"Complete collection of 40+ production-ready skills including Stripe, Supabase, Expo, Plaid, Aptos, and more. Plus meta-commands, orchestration, and workflows.\",\n  \"owner\": {\n    \"name\": \"Raintree Technology\",\n    \"url\": \"https://github.com/raintree-technology\"\n  },\n  \"repository\": \"https://github.com/raintree-technology/claude-starter\",\n  \"plugins\": [\n    {\n      \"name\": \"ai-skills\",\n      \"source\": \"./templates/.claude/skills/anthropic\",\n      \"description\": \"Claude Code expertise: skills, commands, hooks, MCP, settings (7 skills)\",\n      \"category\": \"ai\"\n    },\n    {\n      \"name\": \"stripe-payments\",\n      \"source\": \"./templates/.claude/skills/stripe\",\n      \"description\": \"Complete Stripe API integration (payments, subscriptions, webhooks)\",\n      \"category\": \"api\"\n    },\n    {\n      \"name\": \"supabase-backend\",\n      \"source\": \"./templates/.claude/skills/supabase\",\n      \"description\": \"Supabase backend (PostgreSQL, Auth, Storage, Edge Functions)\",\n      \"category\": \"backend\"\n    },\n    {\n      \"name\": \"plaid-banking\",\n      \"source\": \"./templates/.claude/skills/plaid\",\n      \"description\": \"Plaid banking API (Auth, Transactions, Identity, Accounts - 5 skills)\",\n      \"category\": \"api\"\n    },\n    {\n      \"name\": \"expo-mobile\",\n      \"source\": \"./templates/.claude/skills/expo\",\n      \"description\": \"Expo/React Native mobile development (EAS Build, Update, Router - 4 skills)\",\n      \"category\": \"mobile\"\n    },\n    {\n      \"name\": \"aptos-blockchain\",\n      \"source\": \"./templates/.claude/skills/aptos\",\n      \"description\": \"Aptos blockchain development (Move language, Shelby Protocol, Decibel - 17 skills)\",\n      \"category\": \"blockchain\"\n    },\n    {\n      \"name\": \"shopify-ecommerce\",\n      \"source\": \"./templates/.claude/skills/shopify\",\n      \"description\": \"Shopify e-commerce platform integration\",\n      \"category\": \"ecommerce\"\n    },\n    {\n      \"name\": \"whop-platform\",\n      \"source\": \"./templates/.claude/skills/whop\",\n      \"description\": \"Whop digital products and memberships platform\",\n      \"category\": \"ecommerce\"\n    },\n    {\n      \"name\": \"ios-development\",\n      \"source\": \"./templates/.claude/skills/ios\",\n      \"description\": \"iOS/Swift development\",\n      \"category\": \"mobile\"\n    },\n    {\n      \"name\": \"toon-formatter\",\n      \"source\": \"./templates/.claude/skills/toon-formatter\",\n      \"description\": \"TOON format for 30-60% token savings on tabular data\",\n      \"category\": \"optimization\"\n    },\n    {\n      \"name\": \"meta-commands\",\n      \"source\": \"./templates/.claude/commands/meta\",\n      \"description\": \"Meta-commands for creating custom commands from templates\",\n      \"category\": \"automation\"\n    },\n    {\n      \"name\": \"debug-tools\",\n      \"source\": \"./templates/.claude/commands/debug\",\n      \"description\": \"Debugging tools (skill-graph, explain-ranking, workflow-debug, command-validate)\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"workflows\",\n      \"source\": \"./templates/.claude/workflows\",\n      \"description\": \"YAML workflow automation (production-release, ci-pipeline, daily-maintenance, hotfix)\",\n      \"category\": \"automation\"\n    },\n    {\n      \"name\": \"orchestration\",\n      \"source\": \"./templates/.claude/orchestration\",\n      \"description\": \"Skill orchestration engine for multi-skill collaboration\",\n      \"category\": \"automation\"\n    }\n  ],\n  \"keywords\": [\n    \"stripe\",\n    \"supabase\",\n    \"plaid\",\n    \"expo\",\n    \"aptos\",\n    \"blockchain\",\n    \"payments\",\n    \"backend\",\n    \"mobile\",\n    \"workflows\",\n    \"orchestration\",\n    \"meta-commands\",\n    \"toon\"\n  ]\n}\n",
        "templates/.claude/commands/debug/command-validate.md": "# Command Validator\n\nValidate generated commands against schema and best practices.\n\n## Purpose\n\nEnsure command quality by:\n- Validating structure against JSON schema\n- Checking for required sections and metadata\n- Verifying frontmatter format\n- Testing argument parsing logic\n- Detecting security issues\n- Enforcing Claude Code best practices\n\n## Allowed Tools\n\n- Read(templates/.claude/commands/**/*.md)\n- Read(templates/.claude/validators/command-schema.json)\n- Bash(node templates/.claude/validators/validate-command.js)\n\n## Model Preference\n\nhaiku\n\n## Instructions\n\nWhen this command is invoked (e.g., `/command-validate deploy-staging.md`):\n\n### 1. Load Command File\n\nParse the command markdown file:\n- Extract frontmatter (if present)\n- Parse markdown sections\n- Identify command structure (instructions, examples, options)\n\n### 2. Schema Validation\n\nValidate against `command-schema.json`:\n\n```json\nRequired fields:\n✓ name (matches filename, lowercase-with-dashes)\n✓ description (50-200 chars, clear purpose)\n✓ category (valid enum value)\n\nOptional but recommended:\n⚠ allowedTools (specific tool patterns, not *)\n⚠ modelPreference (sonnet/haiku/opus based on complexity)\n⚠ arguments (if command takes parameters)\n\nValidation rules:\n✓ name: ^[a-z][a-z0-9-]*$ (2-50 chars)\n✓ category: one of [automation, code-quality, development, ...]\n✓ allowedTools: pattern ^(Read|Write|Edit|Bash|...)\\\\(.*\\\\)$\n✓ arguments: array of {name, type, required, description}\n```\n\n### 3. Structure Validation\n\nCheck markdown sections:\n\n```markdown\nRequired sections:\n✓ # Command Title (H1, matches name)\n✓ ## Purpose (clear 1-2 sentence description)\n✓ ## Instructions (step-by-step execution logic)\n\nRecommended sections:\n⚠ ## Allowed Tools (explicit tool permissions)\n⚠ ## Model Preference (haiku for simple, sonnet for complex)\n⚠ ## Examples (usage examples with outputs)\n⚠ ## Error Handling (how to handle failures)\n\nBest practices:\n✓ Clear, actionable instructions\n✓ No ambiguous language\n✓ Specific tool patterns (not wildcards)\n✓ Error cases covered\n✓ Examples with expected outputs\n```\n\n### 4. Security Validation\n\nCheck for security issues:\n\n```\nHigh-severity issues:\n❌ Bash(*) - Unrestricted bash access\n❌ eval() or exec() - Code injection risk\n❌ Unvalidated user input - Injection vulnerabilities\n❌ Hardcoded secrets - API keys, tokens, passwords\n\nMedium-severity issues:\n⚠ Write(*) - Unrestricted file writes\n⚠ No input validation - Potential for malformed data\n⚠ Missing error handling - Could expose sensitive info\n⚠ Overly broad tool permissions - Violates least privilege\n\nBest practices:\n✓ Use specific tool patterns: Bash(git:*) not Bash(*)\n✓ Validate all arguments before use\n✓ Use environment variables for secrets\n✓ Sanitize user input in bash commands\n✓ Limit file access to specific directories\n```\n\n### 5. Quality Checks\n\nAssess command quality:\n\n#### A. Clarity Score\n```\nInstructions clarity: 8/10\n✓ Step-by-step format\n✓ Clear action verbs (create, validate, execute)\n⚠ Some ambiguous references (\"the file\" - which file?)\n\nExamples quality: 7/10\n✓ Multiple examples provided\n✓ Shows expected outputs\n⚠ Missing edge cases (empty input, errors)\n\nDocumentation: 9/10\n✓ Purpose is clear\n✓ All options documented\n✓ Error handling explained\n```\n\n#### B. Completeness Score\n```\nMetadata: 80%\n✓ name, description, category\n✓ allowedTools\n⚠ Missing modelPreference\n❌ No arguments schema (but command accepts arguments)\n\nSections: 75%\n✓ Purpose, Instructions, Examples\n⚠ No \"Error Handling\" section\n⚠ No \"Output Format\" section\n❌ Missing \"Notes\" or \"Caveats\"\n\nError handling: 60%\n⚠ Basic error cases covered\n⚠ Missing validation for edge cases\n❌ No rollback/cleanup logic for failures\n```\n\n#### C. Best Practices Score\n```\nClaude Code conventions: 90%\n✓ Follows markdown command format\n✓ Clear instructions for Claude\n✓ Uses allowed tools appropriately\n⚠ Could benefit from more specific tool patterns\n\nUsability: 85%\n✓ Easy to understand\n✓ Good examples\n✓ Clear purpose\n⚠ Could add more usage tips\n⚠ Missing common pitfalls section\n```\n\n### 6. Argument Validation\n\nIf command accepts arguments, validate parsing logic:\n\n```markdown\n## Defined Arguments\n{\n  \"name\": \"environment\",\n  \"type\": \"string\",\n  \"required\": true,\n  \"allowed\": [\"staging\", \"production\"],\n  \"description\": \"Deployment target\"\n}\n\n## Instructions Validation\n✓ Argument is mentioned in instructions\n✓ Validation logic present (check allowed values)\n✓ Error message for invalid values\n⚠ No default value (but not required - OK)\n⚠ No example showing argument usage\n\n## Recommendations\n1. Add default value: \"default\": \"staging\"\n2. Show argument in examples: /command --environment production\n3. Add validation early in instructions\n```\n\n### 7. Generate Validation Report\n\nOutput comprehensive report:\n\n```markdown\n# Command Validation Report\n\n**Command:** deploy-staging.md\n**Category:** deployment\n**Validated:** 2025-12-28 10:15:32\n\n---\n\n## Summary\n\nOverall Score: **78/100** (Good - needs improvements)\n\n| Category        | Score | Status |\n|-----------------|-------|--------|\n| Schema          | 85%   | ✅ PASS |\n| Structure       | 75%   | ⚠ WARN  |\n| Security        | 90%   | ✅ PASS |\n| Clarity         | 80%   | ✅ PASS |\n| Completeness    | 70%   | ⚠ WARN  |\n| Best Practices  | 88%   | ✅ PASS |\n\n---\n\n## Issues Found\n\n### High Priority (2)\n\n❌ **Missing argument schema**\n- Command accepts `--environment` but no argument metadata in frontmatter\n- Fix: Add arguments array to frontmatter\n- Impact: CLI validation won't work, no auto-completion\n\n❌ **No error handling section**\n- Instructions don't cover failure scenarios\n- Fix: Add \"## Error Handling\" section with rollback logic\n- Impact: Users won't know how to recover from failures\n\n### Medium Priority (3)\n\n⚠ **Overly broad tool permissions**\n- Location: allowedTools: [\"Bash(*)\"]\n- Issue: Unrestricted bash access\n- Fix: Use specific patterns: [\"Bash(git:*)\", \"Bash(npm:*)\"]\n- Impact: Security risk, violates least privilege\n\n⚠ **Missing model preference**\n- Issue: No modelPreference specified\n- Fix: Add \"haiku\" for simple commands, \"sonnet\" for complex\n- Impact: May use slower/more expensive model than needed\n\n⚠ **Incomplete examples**\n- Issue: Examples don't show error cases\n- Fix: Add example with invalid input and expected error\n- Impact: Users won't know how command handles errors\n\n### Low Priority (5)\n\nℹ️ No \"Output Format\" section (recommended for commands with output)\nℹ️ No \"Notes\" section (good for caveats and tips)\nℹ️ Argument examples missing from \"Examples\" section\nℹ️ No cleanup/rollback logic mentioned\nℹ️ Could benefit from \"Common Pitfalls\" section\n\n---\n\n## Recommendations\n\n### Immediate Fixes (Required for quality)\n\n1. **Add argument schema to frontmatter:**\n```yaml\n---\narguments:\n  - name: environment\n    type: string\n    required: true\n    allowed: [staging, production]\n    description: Deployment target environment\n---\n```\n\n2. **Add error handling section:**\n```markdown\n## Error Handling\n\nIf deployment fails:\n1. Check error logs: `.claude/logs/deploy-<timestamp>.log`\n2. Rollback: `git reset --hard HEAD`\n3. Clean build artifacts: `rm -rf dist/`\n4. Retry with --verbose flag for details\n```\n\n3. **Restrict tool permissions:**\n```markdown\n## Allowed Tools\n\n- Bash(git:*)\n- Bash(npm:*)\n- Bash(ssh:*)\n- Read(package.json)\n- Read(.env.*)\n```\n\n### Suggested Improvements (Quality enhancements)\n\n4. **Add model preference:**\n```markdown\n## Model Preference\n\nsonnet (complex deployment logic requires capable model)\n```\n\n5. **Expand examples with error cases:**\n```markdown\n## Examples\n\nSuccess case:\n```bash\n/deploy-staging --environment staging\n✓ Deployed to staging\n```\n\nError case:\n```bash\n/deploy-staging --environment invalid\n❌ Error: Invalid environment. Must be 'staging' or 'production'\n```\n```\n\n6. **Add output format section:**\n```markdown\n## Output Format\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━\nDeployment to staging\n━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nBuilding... ✓\nTesting... ✓\nDeploying... ✓\n\nURL: https://staging.example.com\nVersion: v1.2.3-staging\nTime: 2m 34s\n```\n```\n\n---\n\n## Security Analysis\n\n✅ No hardcoded secrets detected\n✅ No dangerous eval/exec patterns\n⚠ Bash(*) - Recommendation: Use specific patterns\n✅ Input validation present\n✅ No XSS/injection vulnerabilities detected\n\n**Security Score: 8/10** (Good, but could be more restrictive)\n\n---\n\n## Compliance Check\n\nClaude Code Best Practices:\n✅ Uses markdown format\n✅ Clear instructions for Claude\n✅ Follows frontmatter schema\n⚠ Tool permissions could be more specific\n✅ No anti-patterns detected\n✅ Appropriate for automation\n\n**Compliance Score: 9/10** (Excellent)\n\n---\n\n## Next Steps\n\n1. ✅ Fix high-priority issues (argument schema, error handling)\n2. ⚠ Address medium-priority warnings (tool permissions, model preference)\n3. ℹ️ Consider low-priority improvements (examples, documentation)\n\nAfter fixes, re-run validation:\n```bash\n/command-validate deploy-staging.md\n```\n\nOr auto-fix common issues:\n```bash\n/command-validate deploy-staging.md --auto-fix\n```\n\n---\n\n**Validation completed in 234ms**\n**Report saved to:** `.claude/debug/validation-deploy-staging-20251228-101532.md`\n```\n\n### 8. Auto-Fix Mode\n\nSupport automatic fixes for common issues:\n\n```bash\n/command-validate deploy-staging.md --auto-fix\n```\n\nAuto-fixable issues:\n- Missing model preference (infer from complexity)\n- Missing frontmatter (generate from content)\n- Overly broad tool permissions (suggest restrictions)\n- Missing sections (add templates)\n- Formatting issues (fix markdown)\n\nShow diff before applying:\n```diff\n--- deploy-staging.md (original)\n+++ deploy-staging.md (auto-fixed)\n@@ -1,3 +1,10 @@\n+---\n+category: deployment\n+modelPreference: sonnet\n+allowedTools:\n+  - Bash(git:*)\n+  - Bash(npm:*)\n+---\n # Deploy to Staging\n\n Deploy application to staging environment.\n```\n\n### 9. Command-Line Options\n\n- `<command>` - Command file to validate (required)\n- `--strict` - Fail on warnings (not just errors)\n- `--auto-fix` - Automatically fix common issues\n- `--output <file>` - Save report to file\n- `--json` - Output as JSON (for tooling)\n- `--category <cat>` - Validate only specific category\n- `--batch` - Validate all commands in directory\n\n### 10. Examples\n\n```bash\n# Validate single command\n/command-validate deploy-staging.md\n\n# Strict mode (fail on warnings)\n/command-validate deploy-staging.md --strict\n\n# Auto-fix issues\n/command-validate deploy-staging.md --auto-fix\n\n# Batch validate all commands\n/command-validate --batch templates/.claude/commands/\n\n# JSON output for CI/CD\n/command-validate deploy-staging.md --json > validation.json\n```\n\n## Error Handling\n\n- If command file doesn't exist, list available commands\n- If schema file missing, use built-in defaults\n- If auto-fix conflicts with user intent, show options\n- If batch mode, continue on errors and summarize at end\n\n## Notes\n\n- Focus on actionable feedback (how to fix, not just what's wrong)\n- Provide examples of fixes in the report\n- Balance strictness with practicality\n- Save reports for tracking improvement over time\n- Integrate with /create-command to validate at creation time\n",
        "templates/.claude/commands/debug/explain-ranking.md": "# Explain Skill Ranking\n\nExplain why specific skills were selected and ranked for a given query.\n\n## Purpose\n\nThis debugging tool provides transparency into the orchestration engine's decision-making:\n- Why certain skills were selected (or not selected)\n- How the ranking algorithm scored each skill\n- What factors contributed most to the final ranking\n- How to improve skill selection for your use case\n\n## Allowed Tools\n\n- Read(templates/.claude/skills/*/skill.json)\n- Read(templates/.claude/orchestration/*.ts)\n- Bash(node)\n\n## Model Preference\n\nhaiku\n\n## Instructions\n\nWhen this command is invoked with a query (e.g., `/explain-ranking \"build payment system\"`):\n\n### 1. Parse the Query\n\nExtract:\n- **Primary intent**: What the user wants (build, analyze, fix, etc.)\n- **Domain keywords**: Technical terms (payment, database, mobile, etc.)\n- **Context clues**: Implicit requirements (e.g., \"mobile\" implies UI needed)\n\nExample:\n```\nQuery: \"build payment system with recurring subscriptions\"\n\nParsed:\n- Intent: build (weight: high for implementation skills)\n- Keywords: payment, recurring, subscription\n- Domains: fintech, saas\n- Implicit: database needed (store subscriptions), webhooks (process events)\n```\n\n### 2. Simulate Orchestration\n\nRun the ranking algorithm step-by-step for transparency:\n\n#### A. Semantic Matching (35% weight)\n```\nstripe:\n  Query embedding: [0.23, -0.45, 0.67, ...]\n  Skill embedding: [0.21, -0.42, 0.71, ...]\n  Cosine similarity: 0.94\n  Score: 0.94 * 0.35 = 0.329\n\nsupabase:\n  Cosine similarity: 0.78\n  Score: 0.78 * 0.35 = 0.273\n```\n\n#### B. Keyword Matching (25% weight)\n```\nstripe:\n  Matched tags: payment_processing (exact), subscription_billing (exact)\n  Match rate: 2/2 primary tags = 1.0\n  Score: 1.0 * 0.25 = 0.250\n\nsupabase:\n  Matched tags: database (implied)\n  Match rate: 0/2 primary, 1/3 secondary = 0.17\n  Score: 0.17 * 0.25 = 0.043\n```\n\n#### C. Context Relevance (20% weight)\n```\nstripe:\n  Query mentions: subscriptions, payment\n  Skill capabilities: create_subscription, process_payment\n  Relevance: 0.95\n  Score: 0.95 * 0.20 = 0.190\n\nsupabase:\n  Query implications: need to store subscription data\n  Skill capabilities: database_operations, realtime\n  Relevance: 0.70\n  Score: 0.70 * 0.20 = 0.140\n```\n\n#### D. User History (10% weight)\n```\nstripe:\n  Past queries: 3 payment-related queries in last 7 days\n  Success rate: 100%\n  Score: 1.0 * 0.10 = 0.100\n\nsupabase:\n  Past queries: 5 database queries in last 7 days\n  Success rate: 100%\n  Score: 1.0 * 0.10 = 0.100\n```\n\n#### E. Skill Priority (10% weight)\n```\nstripe:\n  Priority: 8/10\n  Normalized: 0.80\n  Score: 0.80 * 0.10 = 0.080\n\nsupabase:\n  Priority: 9/10\n  Normalized: 0.90\n  Score: 0.90 * 0.10 = 0.090\n```\n\n### 3. Calculate Final Scores\n\nSum all components:\n```\nstripe:\n  Semantic:    0.329 (35%)\n  Keyword:     0.250 (25%)\n  Context:     0.190 (20%)\n  History:     0.100 (10%)\n  Priority:    0.080 (10%)\n  ─────────────────────────\n  TOTAL:       0.949 ⭐ RANK #1\n\nsupabase:\n  Semantic:    0.273 (35%)\n  Keyword:     0.043 (25%)\n  Context:     0.140 (20%)\n  History:     0.100 (10%)\n  Priority:    0.090 (10%)\n  ─────────────────────────\n  TOTAL:       0.646 ⭐ RANK #2\n\nexpo:\n  TOTAL:       0.234 ⭐ RANK #7 (not selected - threshold: 0.5)\n```\n\n### 4. Explain Selection Decisions\n\n#### Selected Skills (above threshold)\n```\n✅ SELECTED: stripe (0.949)\n   Why: Perfect semantic match for \"payment\" and \"subscription\"\n   Contributing factors:\n   - Exact keyword matches: payment_processing, subscription_billing\n   - High context relevance: handles recurring payments\n   - Proven success: 100% success rate in past queries\n\n✅ SELECTED: supabase (0.646)\n   Why: Needed to store subscription and customer data\n   Contributing factors:\n   - Implied requirement: subscriptions need persistent storage\n   - Collaboration: stripe provides data to supabase\n   - High priority: 9/10 general-purpose skill\n```\n\n#### Rejected Skills (below threshold)\n```\n❌ REJECTED: expo (0.234 < 0.5 threshold)\n   Why: Mobile framework not relevant for backend payment system\n   Low scores:\n   - Semantic: 0.067 (no match for \"payment\" or \"subscription\")\n   - Keyword: 0.000 (mobile, react-native don't match)\n   - Context: 0.050 (UI not mentioned in query)\n\n   Would activate if query mentioned: \"mobile app\", \"React Native\", \"iOS/Android\"\n```\n\n### 5. Provide Recommendations\n\n#### For This Query\n```\nRecommendations:\n1. Add \"webhooks\" to query → Would activate webhook handling capabilities\n2. Mention \"database\" explicitly → Would boost supabase score\n3. Consider plaid skill → Bank account verification for subscriptions\n```\n\n#### For Skill Improvement\n```\nSuggested Enhancements:\n1. stripe skill: Add more secondary tags (e.g., \"revenue\", \"billing\")\n2. Create \"subscription\" skill → Dedicated subscription management\n3. Enhance collaboration metadata → Link stripe + supabase + webhooks\n```\n\n### 6. Command-Line Options\n\nSupport options:\n- `--query \"text\"` - Query to analyze (required)\n- `--threshold <0.0-1.0>` - Selection threshold (default: 0.5)\n- `--top <n>` - Show top N skills (default: 10)\n- `--verbose` - Show all calculation details\n- `--skills <id,id,...>` - Explain specific skills only\n- `--suggest` - Show recommendations for improving ranking\n\n### 7. Output Format\n\n```markdown\n# Skill Ranking Explanation\n\n## Query Analysis\n**Input:** \"build payment system with recurring subscriptions\"\n\n**Parsed Intent:**\n- Action: build\n- Domain: fintech, saas\n- Keywords: payment, recurring, subscription\n- Implied needs: database, webhooks\n\n---\n\n## Ranking Results\n\n### Selected Skills (2)\n\n#### 1. stripe (0.949) ⭐⭐⭐⭐⭐\n**Why selected:** Perfect match for payment processing and subscriptions\n\n**Score Breakdown:**\n| Factor          | Score | Weight | Contribution | Notes                           |\n|-----------------|-------|--------|--------------|--------------------------------|\n| Semantic Match  | 0.94  | 35%    | 0.329        | Near-perfect embedding match   |\n| Keyword Match   | 1.00  | 25%    | 0.250        | Exact: payment, subscription   |\n| Context         | 0.95  | 20%    | 0.190        | Handles recurring billing      |\n| User History    | 1.00  | 10%    | 0.100        | 100% success, 3 recent uses    |\n| Skill Priority  | 0.80  | 10%    | 0.080        | High-priority skill (8/10)     |\n\n**Key Matches:**\n- Primary tags: payment_processing ✓, subscription_billing ✓\n- Capabilities: create_subscription ✓, process_payment ✓\n- Actions: charge_customer, manage_subscription\n\n---\n\n#### 2. supabase (0.646) ⭐⭐⭐\n**Why selected:** Database needed for subscription storage\n\n**Score Breakdown:**\n| Factor          | Score | Weight | Contribution | Notes                           |\n|-----------------|-------|--------|--------------|--------------------------------|\n| Semantic Match  | 0.78  | 35%    | 0.273        | Moderate match (backend focus) |\n| Keyword Match   | 0.17  | 25%    | 0.043        | Implied: database needed       |\n| Context         | 0.70  | 20%    | 0.140        | Store customer/subscription    |\n| User History    | 1.00  | 10%    | 0.100        | 100% success, 5 recent uses    |\n| Skill Priority  | 0.90  | 10%    | 0.090        | Top-priority skill (9/10)      |\n\n**Collaboration:**\n- Receives data from: stripe (payment events, customer data)\n- Shared context: user_id, subscription_id\n\n---\n\n### Not Selected (8)\n\n#### expo (0.234) ❌\n**Why rejected:** Mobile framework not needed for backend system\n\n**Would activate if query mentioned:**\n- \"mobile app\" or \"iOS/Android\"\n- \"React Native\"\n- \"client app\" or \"user interface\"\n\n---\n\n## Recommendations\n\n### Improve This Query\n1. Add \"webhooks\" → Activate webhook handling\n2. Mention \"customer portal\" → Activate frontend skills\n3. Add \"bank accounts\" → Consider plaid skill\n\n### Improve Skills\n1. **stripe:** Add secondary tags: revenue, billing, invoice\n2. **Create new skill:** Dedicated subscription-management skill\n3. **supabase:** Add \"billing\" to semantic tags\n\n### Adjust Threshold\nCurrent: 0.5 (balanced)\n- Lower to 0.3 → Include expo (0.234) - good for exploratory queries\n- Raise to 0.7 → Only stripe (0.949) - good for focused queries\n\n---\n\n## Debug Info\n\n**Orchestration Config:**\n- Semantic matching: enabled\n- Embedding provider: openai\n- Cache hit rate: 95%\n- Selection threshold: 0.5\n- Max active skills: 5\n\n**Performance:**\n- Total skills evaluated: 40\n- Ranking time: 234ms\n- Embeddings: 2 API calls (38 cached)\n```\n\n### 8. Examples\n\n```bash\n# Explain ranking for query\n/explain-ranking --query \"build payment system\"\n\n# Show only top 5 skills\n/explain-ranking --query \"mobile app with auth\" --top 5\n\n# Verbose mode (show all calculations)\n/explain-ranking --query \"e-commerce store\" --verbose\n\n# Explain specific skills\n/explain-ranking --query \"api integration\" --skills stripe,plaid,supabase\n\n# Get recommendations\n/explain-ranking --query \"real-time chat\" --suggest\n```\n\n## Error Handling\n\n- If orchestration not enabled in settings.json, show error with instructions\n- If no skills have orchestration metadata, explain limitation\n- If query is empty, show usage examples\n- If semantic matching fails (API error), fall back to keyword-only explanation\n\n## Notes\n\n- Make calculations transparent and easy to understand\n- Focus on actionable insights (how to improve selection)\n- Show both successes (why skills ranked high) and failures (why skills rejected)\n- Provide specific, concrete recommendations\n",
        "templates/.claude/commands/debug/skill-graph.md": "# Skill Graph Visualizer\n\nVisualize skill dependencies, collaboration patterns, and orchestration relationships.\n\n## Purpose\n\nThis debugging tool helps you understand:\n- How skills are connected and can collaborate\n- Which skills depend on or complement each other\n- Data flow between skills (who provides/consumes what)\n- Orchestration priorities and patterns\n- Skills recommended together for common tasks\n\n## Allowed Tools\n\n- Read(templates/.claude/skills/*/skill.json)\n- Glob(templates/.claude/skills/**/skill.json)\n- Bash(dot)\n\n## Model Preference\n\nhaiku\n\n## Instructions\n\nWhen this command is invoked:\n\n### 1. Load All Skills\n\nRead all skill.json files to extract:\n- Skill ID and name\n- Orchestration metadata (priority, cooperation level)\n- Collaboration data (canProvideDataTo, canConsumeDataFrom, sharedContext)\n- Dependencies (recommended, complements)\n- Semantic tags and capabilities\n\n### 2. Build Dependency Graph\n\nCreate a directed graph showing:\n- **Nodes**: Skills (sized by priority, colored by domain)\n- **Edges**: Relationships between skills\n  - Solid arrows: \"provides data to\" relationships\n  - Dashed arrows: \"recommended with\" relationships\n  - Dotted arrows: \"complements\" relationships\n\n### 3. Analyze Patterns\n\nIdentify:\n- **Clusters**: Groups of skills that work together (e.g., full-stack clusters)\n- **Hubs**: Central skills that many others depend on\n- **Isolated Skills**: Skills with no collaboration metadata\n- **Cycles**: Skills that have circular dependencies (potential issues)\n- **Missing Links**: Skills that should collaborate but don't have metadata\n\n### 4. Generate Visualization\n\nCreate output in three formats:\n\n#### A. ASCII Tree (Quick View)\n```\nSkills by Domain:\n└── fintech\n    ├── stripe (priority: 8) [hub: 3 connections]\n    │   ├── → supabase (provides: payment data)\n    │   ├── → plaid (provides: customer data)\n    │   └── ⟷ whop (complements)\n    └── plaid (priority: 7)\n        └── → supabase (provides: bank data)\n```\n\n#### B. DOT Format (Graphviz)\n```dot\ndigraph skills {\n  rankdir=LR;\n  node [shape=box];\n\n  // Nodes\n  stripe [label=\"Stripe\\n(priority: 8)\" fillcolor=lightblue];\n  supabase [label=\"Supabase\\n(priority: 9)\" fillcolor=lightgreen];\n\n  // Edges\n  stripe -> supabase [label=\"payment data\"];\n}\n```\n\nSave to `.claude/debug/skill-graph.dot` and offer to render:\n```bash\ndot -Tpng .claude/debug/skill-graph.dot -o .claude/debug/skill-graph.png\n```\n\n#### C. Interactive Report\n```markdown\n# Skill Collaboration Report\n\n## Summary\n- Total Skills: 40\n- Skills with Orchestration: 5 (12.5%)\n- Total Connections: 12\n- Isolated Skills: 35\n\n## Key Clusters\n\n### Full-Stack SaaS Cluster\nSkills that work together for SaaS applications:\n- **stripe** (payment processing) → supabase (store transactions)\n- **supabase** (database/auth) ← expo (mobile client)\n- **expo** (mobile) ⟷ anthropic (AI features)\n\nShared Context: user_id, auth_token, api_keys\n\n### Banking/Fintech Cluster\n- **plaid** (banking data) → supabase (store transactions)\n- **stripe** (payments) ⟷ plaid (account verification)\n\n## Recommendations\n\n### Skills to Enhance Next\nPriority order for adding orchestration metadata:\n1. **shopify** - Should complement stripe, whop (e-commerce)\n2. **ios** - Should collaborate with expo (mobile)\n3. **aptos/** skills - Form blockchain cluster\n\n### Missing Collaborations\nSuggested relationships to add:\n- expo → stripe (in-app purchases)\n- shopify → stripe (payment gateway)\n- anthropic → all skills (AI enhancement layer)\n```\n\n### 5. Command-Line Options\n\nSupport options:\n- `--format <ascii|dot|report>` - Output format (default: ascii)\n- `--cluster <domain>` - Show only skills in domain (fintech, mobile, etc.)\n- `--skill <id>` - Show connections for specific skill\n- `--isolated` - Show only skills without orchestration metadata\n- `--render` - Auto-render DOT to PNG (requires graphviz)\n\n### 6. Examples\n\n```bash\n# Quick ASCII view\n/skill-graph\n\n# Full report with recommendations\n/skill-graph --format report\n\n# Visualize specific skill\n/skill-graph --skill stripe\n\n# Generate and render graph\n/skill-graph --format dot --render\n\n# Find skills needing orchestration metadata\n/skill-graph --isolated\n```\n\n## Output Format\n\nPresent the visualization clearly with:\n1. **Quick Stats** (total skills, connections, clusters)\n2. **Main Visualization** (format based on option)\n3. **Actionable Insights** (which skills to enhance, missing links)\n4. **Next Steps** (suggest improvements to orchestration metadata)\n\n## Error Handling\n\n- If no skills have orchestration metadata, show warning and suggest enhancing pilot skills\n- If graphviz not installed and --render used, show installation instructions\n- If invalid skill ID provided, list available skills\n\n## Notes\n\n- Focus on actionable insights, not just visualization\n- Highlight skills that would benefit most from orchestration metadata\n- Suggest realistic collaboration patterns based on domain knowledge\n- Keep ASCII output readable in terminal (max 120 chars wide)\n",
        "templates/.claude/commands/debug/workflow-debug.md": "# Workflow Debugger\n\nStep-by-step workflow execution debugger with breakpoints and state inspection.\n\n## Purpose\n\nDebug YAML workflows by:\n- Executing workflows step-by-step with pauses\n- Inspecting state and variables at each step\n- Validating workflow structure before execution\n- Identifying errors in workflow logic\n- Profiling execution time and resource usage\n\n## Allowed Tools\n\n- Read(templates/.claude/workflows/*.yml)\n- Read(templates/.claude/utils/workflows/*.js)\n- Bash(node)\n\n## Model Preference\n\nhaiku\n\n## Instructions\n\nWhen this command is invoked (e.g., `/workflow-debug production-release.yml`):\n\n### 1. Load and Validate Workflow\n\n```bash\n# Parse YAML\nconst workflow = yaml.load(fs.readFileSync(workflowPath));\n\n# Validate structure\n- ✓ Required fields: name, steps\n- ✓ Valid step types: bash, command, parallel, manual\n- ✓ Variable references: ${{ inputs.*, env.*, steps.*.* }}\n- ✓ Circular dependencies in parallel steps\n- ✓ Timeout values within limits\n- ⚠ Warnings: Missing error handlers, no timeouts, etc.\n```\n\n**Output:**\n```markdown\n## Workflow Validation: production-release.yml\n\n### Structure\n✓ Name: \"Production Release\"\n✓ Version: 1.0\n✓ Inputs: version_type (string, default: patch)\n✓ Environment: NODE_ENV=production\n✓ Steps: 6 total (4 sequential, 2 parallel)\n\n### Variable Analysis\nVariables defined:\n- inputs.version_type (string)\n- inputs.skip_tests (boolean)\n- env.NODE_ENV (string)\n\nVariable usage:\n- ✓ ${inputs.version_type} → Step 4 (valid)\n- ✓ ${{ steps.build.exit_code }} → Step 5 (valid)\n- ⚠ ${{ steps.deploy.output }} → Step 6 (undefined - deploy hasn't run yet)\n\n### Potential Issues\n⚠ Warning: Step \"Deploy\" has no timeout (default: 5min)\n⚠ Warning: No on_failure handler for critical steps\n✓ No circular dependencies detected\n✓ All step references are valid\n\n### Recommendations\n1. Add timeout to \"Deploy\" step\n2. Add on_failure handler for \"Version Bump\" step\n3. Consider adding --dry-run option for testing\n```\n\n### 2. Interactive Debugging Mode\n\nProvide step-by-step execution with controls:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nWORKFLOW DEBUGGER: production-release.yml\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWorkflow: Production Release\nInputs: version_type=patch, skip_tests=false\nEnvironment: NODE_ENV=production\n\nSteps to execute: 6\nExecution mode: step-by-step\n\nControls:\n  [n] next     - Execute next step\n  [c] continue - Run to completion\n  [s] skip     - Skip current step\n  [i] inspect  - View current state\n  [b] break    - Set breakpoint\n  [q] quit     - Abort workflow\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n[STEP 1/6] Pre-flight Checks (parallel)\n  ├─ Check git status\n  └─ Check npm outdated\n\nState before execution:\n{\n  \"inputs\": { \"version_type\": \"patch\", \"skip_tests\": false },\n  \"env\": { \"NODE_ENV\": \"production\" },\n  \"steps\": []\n}\n\nPress [n] to execute, [s] to skip, [i] to inspect: _\n```\n\n### 3. State Inspection\n\nWhen user presses `[i]`, show current state:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nSTATE INSPECTOR (Step 3/6)\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n## Inputs\n{\n  \"version_type\": \"patch\",\n  \"skip_tests\": false\n}\n\n## Environment\n{\n  \"NODE_ENV\": \"production\",\n  \"CI\": \"false\"\n}\n\n## Step Results\n{\n  \"0\": {\n    \"name\": \"Pre-flight Checks\",\n    \"status\": \"success\",\n    \"exit_code\": 0,\n    \"duration\": 234,\n    \"output\": \"On branch main\\nnothing to commit...\"\n  },\n  \"1\": {\n    \"name\": \"Quality Checks\",\n    \"status\": \"success\",\n    \"exit_code\": 0,\n    \"duration\": 1523,\n    \"output\": \"✓ All checks passed\"\n  },\n  \"2\": {\n    \"name\": \"Build and Test\",\n    \"status\": \"running\",\n    \"start_time\": 1640000000000\n  }\n}\n\n## Available Variables\nExpand any expression:\n- ${{ inputs.version_type }} → \"patch\"\n- ${{ env.NODE_ENV }} → \"production\"\n- ${{ steps.0.exit_code }} → 0\n- ${{ steps.1.duration }} → 1523\n- ${{ steps.2.status }} → \"running\"\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### 4. Step Execution Details\n\nAfter each step, show detailed results:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n[STEP 3/6] Build and Test\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nType: bash\nCommand: npm run build && npm test\nTimeout: 1800000ms (30 minutes)\nFail on error: true\n\nVariables expanded:\n  (none)\n\nStarting execution...\n\n┌─────────────────────────────────────────────────────────────┐\n│ COMMAND OUTPUT                                              │\n└─────────────────────────────────────────────────────────────┘\n\n> build\n> tsc && vite build\n\n✓ 52 modules transformed.\ndist/index.js  125.4 kB\n\n> test\n> vitest run\n\n✓ src/utils.test.ts (5 tests) 234ms\n✓ src/api.test.ts (12 tests) 456ms\n\nTest Files  2 passed (2)\n     Tests  17 passed (17)\n  Start at  10:15:32\n  Duration  1.23s\n\n┌─────────────────────────────────────────────────────────────┐\n│ EXECUTION SUMMARY                                           │\n└─────────────────────────────────────────────────────────────┘\n\nStatus: ✅ SUCCESS\nExit Code: 0\nDuration: 1,523ms\nOutput Lines: 24\nErrors: 0\n\nStep result stored in steps[2]:\n{\n  \"name\": \"Build and Test\",\n  \"status\": \"success\",\n  \"exit_code\": 0,\n  \"duration\": 1523,\n  \"output\": \"...\",\n  \"stdout\": \"...\",\n  \"stderr\": \"\"\n}\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nPress [n] for next step, [i] to inspect state: _\n```\n\n### 5. Error Debugging\n\nWhen a step fails:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n[STEP 4/6] Version Bump\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nStatus: ❌ FAILED\nExit Code: 1\nDuration: 145ms\n\n┌─────────────────────────────────────────────────────────────┐\n│ ERROR OUTPUT                                                │\n└─────────────────────────────────────────────────────────────┘\n\nnpm ERR! code ENOENT\nnpm ERR! syscall open\nnpm ERR! path /path/to/package.json\nnpm ERR! errno -2\nnpm ERR! enoent ENOENT: no such file or directory\n\n┌─────────────────────────────────────────────────────────────┐\n│ ERROR ANALYSIS                                              │\n└─────────────────────────────────────────────────────────────┘\n\nError Type: File Not Found\nRoot Cause: package.json missing in current directory\nStep: Version Bump (npm version patch)\n\nPossible Solutions:\n1. Check current working directory (should be project root)\n2. Ensure package.json exists before running workflow\n3. Add pre-flight check: test -f package.json\n\nWorkflow Context:\n- This step has fail_on_error: true\n- No on_failure handler defined\n- Workflow will abort here unless continued manually\n\n┌─────────────────────────────────────────────────────────────┐\n│ DEBUGGING OPTIONS                                           │\n└─────────────────────────────────────────────────────────────┘\n\n[r] retry    - Re-execute this step\n[s] skip     - Skip and continue (mark as succeeded)\n[f] fix      - Edit workflow and retry\n[i] inspect  - View full state\n[q] quit     - Abort workflow\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nChoose action: _\n```\n\n### 6. Performance Profiling\n\nAfter workflow completes, show performance report:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nWORKFLOW EXECUTION COMPLETE\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nWorkflow: Production Release\nStatus: ✅ SUCCESS\nTotal Duration: 45.3s\n\n## Performance Profile\n\n| Step                  | Duration  | % Total | Status  |\n|-----------------------|-----------|---------|---------|\n| Pre-flight Checks     |   0.2s    |  0.4%   | ✅      |\n| Quality Checks        |   1.5s    |  3.3%   | ✅      |\n| Build and Test        |  28.4s    | 62.7%   | ✅      | ⚠ SLOWEST\n| Version Bump          |   0.1s    |  0.2%   | ✅      |\n| Git Tag and Push      |   2.3s    |  5.1%   | ✅      |\n| Deploy                |  12.8s    | 28.3%   | ✅      |\n|-----------------------|-----------|---------|---------|\n| TOTAL                 |  45.3s    | 100%    |         |\n\n## Resource Usage\n\n- Peak Memory: 512 MB\n- CPU Time: 23.4s\n- Network: 45 MB downloaded, 12 MB uploaded\n- Disk I/O: 234 MB written\n\n## Bottlenecks\n\n⚠ Step \"Build and Test\" took 62.7% of total time\n  Recommendations:\n  - Enable build cache (vite cache, tsc incremental)\n  - Run tests in parallel (--maxWorkers)\n  - Consider splitting build and test into separate steps\n\n⚠ Step \"Deploy\" took 28.3% of total time\n  Recommendations:\n  - Use incremental deployment\n  - Parallelize upload if possible\n  - Check network bandwidth\n\n## Optimization Suggestions\n\n1. Enable caching:\n   Add to workflow env:\n   ```yaml\n   env:\n     VITE_CACHE: true\n     TSC_INCREMENTAL: true\n   ```\n\n2. Parallelize independent steps:\n   ```yaml\n   - name: \"Build and Test\"\n     type: parallel\n     steps:\n       - bash: npm run build\n       - bash: npm test\n   ```\n\n3. Add timeout guards:\n   All steps completed well under limits, but consider adding\n   timeouts to prevent hanging on failures.\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### 7. Command-Line Options\n\nSupport options:\n- `<workflow>` - Workflow file to debug (required)\n- `--input key=value` - Set input variables\n- `--auto` - Auto-continue (no pauses, just show output)\n- `--break <step>` - Set breakpoint at step number or name\n- `--validate-only` - Only validate, don't execute\n- `--profile` - Show performance profile after completion\n- `--trace` - Show detailed trace of all operations\n\n### 8. Examples\n\n```bash\n# Interactive debugging\n/workflow-debug production-release.yml\n\n# With inputs\n/workflow-debug production-release.yml --input version_type=minor\n\n# Auto-run with profiling\n/workflow-debug ci-pipeline.yml --auto --profile\n\n# Validate workflow structure\n/workflow-debug hotfix.yml --validate-only\n\n# Break at specific step\n/workflow-debug production-release.yml --break \"Deploy\"\n\n# Full trace output\n/workflow-debug daily-maintenance.yml --trace\n```\n\n## Output Format\n\nClear, colorized terminal output with:\n- 📊 Visual progress indicators\n- 🎯 Step-by-step execution status\n- 🔍 State inspection on demand\n- 🐛 Error analysis and suggestions\n- ⚡ Performance profiling\n- 💡 Optimization recommendations\n\n## Error Handling\n\n- If workflow file doesn't exist, list available workflows\n- If workflow has syntax errors, show YAML parse errors with line numbers\n- If step fails, provide error analysis and recovery options\n- If timeout occurs, show partial results and allow continuation\n- If Ctrl+C pressed, ask whether to abort or pause\n\n## Notes\n\n- Make debugging interactive and helpful\n- Provide context for every error\n- Show state changes clearly\n- Offer actionable recommendations\n- Keep output readable (not overwhelming)\n- Save full execution log to `.claude/debug/workflow-<timestamp>.log`\n",
        "templates/.claude/commands/meta/create-command.md": "# Create Command\n\nGenerate a new Claude Code command from templates with full validation and automatic registry management.\n\n**Usage:** `/create-command <name> [options]`\n\n**Arguments:**\n- `name` (string) - Command name (kebab-case, e.g., deploy-staging)\n\n**Options:**\n- `--template <type>` - Template to use (basic-prompt, with-arguments, with-bash, git-workflow, code-generator, analyzer)\n- `--description \"<text>\"` - Command description\n- `--category <cat>` - Category (automation, code-quality, development, deployment, documentation, git, meta, optimization, testing, workflow, utility)\n- `--interactive` - Interactive mode with prompts (default)\n- `--dry-run` - Preview without creating files\n- `--force` - Overwrite existing command\n\n## Purpose\n\nEnables infinite extensibility by allowing users to create custom commands programmatically. Commands are generated from proven templates, automatically validated, and registered for conflict-free operation.\n\n## Workflow\n\n### 1. Validate Command Name\n\nCheck command name follows conventions and has no conflicts:\n\n```bash\n# Run validation\nnode .claude/validators/validate-command.js --name-only \"$COMMAND_NAME\"\n\n# Check for conflicts\nnode .claude/validators/check-conflicts.js \"$COMMAND_NAME\"\n```\n\n**Validation rules:**\n- Must be kebab-case (lowercase, hyphens only)\n- 2-50 characters\n- Cannot be a reserved name (help, version, init, config, settings)\n- Cannot conflict with existing commands\n- No consecutive hyphens\n- Cannot start/end with hyphen\n\n### 2. Select Template\n\nChoose the appropriate template based on command purpose:\n\n**Available Templates:**\n\n1. **basic-prompt** - Simple text-based command\n   - Best for: Q&A, analysis, recommendations\n   - No bash execution\n   - No file modifications\n   - Example: /explain-concept, /suggest-architecture\n\n2. **with-arguments** - Command with parameters and validation\n   - Best for: Parameterized operations\n   - Accepts user input\n   - Validates arguments\n   - Example: /generate-component <name>, /refactor-function <path>\n\n3. **with-bash** - Execute bash commands\n   - Best for: System operations, tool execution\n   - Runs bash commands\n   - Pre-flight checks\n   - Safety features\n   - Example: /install-deps, /check-disk-space\n\n4. **git-workflow** - Git operations and workflows\n   - Best for: Git automation\n   - Branch management\n   - Commit/push operations\n   - Follows git safety protocol\n   - Example: /create-pr, /sync-branch\n\n5. **code-generator** - File/code generation\n   - Best for: Creating new files\n   - Follows project patterns\n   - Validates generated code\n   - Integrates with codebase\n   - Example: /generate-test, /scaffold-api\n\n6. **analyzer** - Code analysis and reporting\n   - Best for: Auditing, linting, metrics\n   - Multi-scope support\n   - Detailed reporting\n   - Severity levels\n   - Example: /check-security, /analyze-performance\n\n**Interactive template selection:**\n\n```\n? What type of command are you creating?\n  1. Basic prompt (text-based, no execution)\n  2. With arguments (accepts parameters)\n  3. With bash (executes commands)\n  4. Git workflow (git operations)\n  5. Code generator (creates files)\n  6. Analyzer (audits and reports)\n>\n\n? What will this command do? (helps determine best template)\n> Deploy code to staging environment\n\nRecommended: git-workflow\nReason: Involves git operations, deployment commands, safety checks\n\n? Use recommended template? (Y/n)\n```\n\n### 3. Gather Requirements\n\nCollect information needed to populate template:\n\n**Required information:**\n- **Name:** Command name (validated)\n- **Description:** What the command does (10-200 characters)\n- **Category:** Command category for organization\n\n**Template-specific information:**\n\n**For with-arguments:**\n- Arguments list (name, type, description, required/optional, defaults)\n- Validation rules (regex, min/max, allowed values)\n\n**For with-bash:**\n- Required tools (git, npm, docker, etc.)\n- Bash commands to execute\n- Pre-flight checks\n- Safety warnings\n\n**For git-workflow:**\n- Git operations (checkout, commit, push, merge)\n- Branch operations\n- Remote sync requirements\n- Rollback procedures\n\n**For code-generator:**\n- File patterns to generate\n- Code templates\n- Naming conventions\n- Integration steps\n\n**For analyzer:**\n- Analysis scopes\n- Tools to run (eslint, knip, etc.)\n- Severity levels\n- Report format\n\n**Interactive prompts example:**\n\n```\nCreating command: deploy-staging\nTemplate: git-workflow\n\n? Short description (10-200 chars):\n> Deploy code to staging environment with automated tests\n\n? Category:\n  automation\n  deployment (selected)\n  git\n\n? Git operations needed (select all):\n  [x] Checkout branch\n  [x] Pull latest\n  [ ] Create branch\n  [x] Push changes\n  [ ] Create PR\n\n? Deployment steps:\n> 1. Run tests\n> 2. Build production bundle\n> 3. Deploy to staging server\n> 4. Verify deployment\n\n? Safety level:\n  (x) Require confirmation before deploy\n  ( ) No confirmation needed\n\n? Add rollback procedure? (Y/n) y\n```\n\n### 4. Generate Command File\n\nPopulate template with collected information:\n\n```bash\n# Load template\nTEMPLATE_PATH=\".claude/templates/commands/${TEMPLATE}.md\"\n\n# Replace placeholders\nsed \"s/{{COMMAND_NAME}}/$COMMAND_NAME/g\" \"$TEMPLATE_PATH\" > command.tmp\nsed -i \"s/{{COMMAND_DESCRIPTION}}/$DESCRIPTION/g\" command.tmp\nsed -i \"s/{{CATEGORY}}/$CATEGORY/g\" command.tmp\n\n# Add template-specific content\n# ... (based on template type and gathered requirements)\n\n# Save to generated commands directory\nOUTPUT_PATH=\".claude/commands/generated/${COMMAND_NAME}.md\"\nmv command.tmp \"$OUTPUT_PATH\"\n```\n\n**Generated file structure:**\n\n```markdown\n---\nname: deploy-staging\nversion: 1.0.0\ncategory: deployment\nmodel: sonnet\nallowedTools: [\"Bash(git:*)\", \"Bash(npm:*)\", \"Bash(ssh:*)\"]\nrequiredTools: [\"git\", \"npm\", \"ssh\"]\ntemplate:\n  source: git-workflow\n  generated: 2025-12-28T10:15:00Z\n  generator: create-command\n---\n\n# Deploy Staging\n\nDeploy code to staging environment with automated tests.\n\n**Usage:** `/deploy-staging [branch]`\n\n**Arguments:**\n- `branch` (string) - Branch to deploy (default: main)\n\n... [rest of generated content]\n```\n\n### 5. Validate Generated Command\n\nEnsure generated command meets all standards:\n\n```bash\n# Validate against schema\nnode .claude/validators/validate-command.js \"$OUTPUT_PATH\"\n\n# Check result\nif [[ $? -ne 0 ]]; then\n  echo \"Validation failed! Review errors above.\"\n  echo \"Command file: $OUTPUT_PATH\"\n  exit 1\nfi\n```\n\n**Validation checks:**\n- Schema compliance\n- Required sections present\n- Valid frontmatter\n- Proper markdown formatting\n- Code block syntax\n- Example usage provided\n\n### 6. Update Registry\n\nRegister the new command:\n\n```bash\n# Add to registry\nnode .claude/validators/update-registry.js add \"$OUTPUT_PATH\"\n```\n\n**Registry entry:**\n\n```json\n{\n  \"commands\": {\n    \"deploy-staging\": {\n      \"path\": \"generated/deploy-staging.md\",\n      \"type\": \"generated\",\n      \"created\": \"2025-12-28T10:15:00Z\",\n      \"generator\": \"create-command\",\n      \"metadata\": {\n        \"description\": \"Deploy code to staging environment with automated tests\",\n        \"arguments\": [\"branch\"],\n        \"tools\": [\"Bash(git:*)\", \"Bash(npm:*)\", \"Bash(ssh:*)\"],\n        \"category\": \"deployment\",\n        \"template\": \"git-workflow\"\n      }\n    }\n  }\n}\n```\n\n### 7. Preview and Confirm\n\nShow preview before finalizing (unless --force):\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nCommand Created Successfully\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nName: deploy-staging\nTemplate: git-workflow\nCategory: deployment\nFile: .claude/commands/generated/deploy-staging.md\n\nUsage: /deploy-staging [branch]\n\nDescription:\nDeploy code to staging environment with automated tests.\n\nValidation: ✅ PASSED\nRegistry: ✅ UPDATED\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nNext steps:\n1. Review the generated file\n2. Test the command: /deploy-staging\n3. Edit if needed: /edit-command deploy-staging\n4. Share with team via git commit\n\nCommand is immediately available for use!\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n## Examples\n\n```bash\n# Interactive mode (default)\n/create-command my-deployment\n\n# Specify template\n/create-command check-performance --template analyzer\n\n# Full specification\n/create-command deploy-prod \\\n  --template git-workflow \\\n  --description \"Deploy to production with safety checks\" \\\n  --category deployment\n\n# Preview without creating\n/create-command my-command --dry-run\n\n# Overwrite existing\n/create-command existing-command --force\n```\n\n## Template Selection Guide\n\n**Use basic-prompt when:**\n- No external tools needed\n- No file modifications\n- Pure analysis or recommendations\n- Example: \"Explain this error message\"\n\n**Use with-arguments when:**\n- Need user input\n- Parameterized operations\n- Input validation required\n- Example: \"Generate component with specific props\"\n\n**Use with-bash when:**\n- Running system commands\n- Installing dependencies\n- Checking system state\n- Example: \"Check if Docker is running\"\n\n**Use git-workflow when:**\n- Git operations\n- Branch management\n- Deployment via git\n- Example: \"Create release branch\"\n\n**Use code-generator when:**\n- Creating new files\n- Scaffolding code\n- Following project patterns\n- Example: \"Generate API endpoint\"\n\n**Use analyzer when:**\n- Code auditing\n- Running linters\n- Gathering metrics\n- Example: \"Find unused imports\"\n\n## Safety Features\n\n- ✅ Automatic name validation\n- ✅ Conflict detection (prevents duplicates)\n- ✅ Schema validation (ensures quality)\n- ✅ Template validation (only known templates)\n- ✅ Dry-run mode (preview before creating)\n- ✅ Force mode (explicit overwrite only)\n- ✅ Registry tracking (all commands tracked)\n- ✅ Rollback support (delete via /edit-command)\n\n## Best Practices\n\n### Command Naming\n\n**Good names:**\n- `deploy-staging`\n- `check-coverage`\n- `generate-api-test`\n- `audit-dependencies`\n\n**Bad names:**\n- `DeployStaging` (not kebab-case)\n- `deploy_staging` (use hyphens not underscores)\n- `d` (too short, unclear)\n- `deploy-to-staging-environment-with-tests` (too long)\n\n### Template Selection\n\n1. **Start simple:** Use basic-prompt for first iteration\n2. **Add complexity:** Upgrade to with-arguments if needed\n3. **Add execution:** Move to with-bash only if truly needed\n4. **Be specific:** Use specialized templates (git-workflow, code-generator) when appropriate\n\n### Command Design\n\n1. **Single responsibility:** One command, one task\n2. **Clear description:** Describe what, not how\n3. **Good examples:** Show common use cases\n4. **Safety first:** Add confirmations for destructive operations\n5. **Fail gracefully:** Include error handling\n\n## Troubleshooting\n\n**Error: \"Command name is reserved\"**\n```\nSolution: Choose a different name. Reserved names:\nhelp, version, init, config, settings, claude, agent, system, internal\n```\n\n**Error: \"Command already exists\"**\n```\nSolution 1: Use different name\nSolution 2: Remove existing: /edit-command <name> --delete\nSolution 3: Force overwrite: /create-command <name> --force\n```\n\n**Error: \"Validation failed\"**\n```\nSolution: Review validation errors and fix:\n- Check command name format\n- Ensure description is 10-200 characters\n- Verify category is valid\n- Check template exists\n```\n\n**Error: \"Template not found\"**\n```\nSolution: Use valid template name:\nbasic-prompt, with-arguments, with-bash, git-workflow, code-generator, analyzer\n```\n\n## Advanced Usage\n\n### Creating Command from Workflow\n\nGenerate command from existing workflow YAML:\n\n```bash\n/workflow-compose my-workflow\n/create-command my-workflow --from-workflow .claude/workflows/my-workflow.yml\n```\n\n### Batch Command Creation\n\nCreate multiple related commands:\n\n```bash\n/create-command deploy-dev --template git-workflow --category deployment\n/create-command deploy-staging --template git-workflow --category deployment\n/create-command deploy-prod --template git-workflow --category deployment\n```\n\n### Custom Template\n\nCreate custom template for reuse:\n\n```bash\n# 1. Create command manually\n/create-command my-custom-type\n\n# 2. Save as template\ncp .claude/commands/generated/my-custom-type.md \\\n   .claude/templates/commands/my-template.md\n\n# 3. Generalize with placeholders\n# Edit my-template.md and replace specific values with {{PLACEHOLDERS}}\n\n# 4. Use custom template\n/create-command new-command --template my-template\n```\n\n## Related Commands\n\n- `/edit-command` - Modify existing commands\n- `/workflow-compose` - Create multi-step workflows\n- `/command-validate` - Validate command files\n- `/audit-code` - Check command quality\n\n---\n\n**Pro Tip:** Start with `/create-command --interactive` to learn the system, then use direct arguments once comfortable for faster command creation.\n",
        "templates/.claude/commands/meta/edit-command.md": "# Edit Command\n\nModify existing commands with validation, diff preview, and automatic registry updates.\n\n**Usage:** `/edit-command <name> [options]`\n\n**Arguments:**\n- `name` (string) - Command name to edit\n\n**Options:**\n- `--description \"<text>\"` - Update description\n- `--category <cat>` - Change category\n- `--add-tool <tool>` - Add allowed tool\n- `--remove-tool <tool>` - Remove allowed tool\n- `--add-arg <name:type:desc>` - Add argument\n- `--remove-arg <name>` - Remove argument\n- `--model <model>` - Change model (sonnet, opus, haiku)\n- `--interactive` - Interactive edit mode\n- `--delete` - Delete command\n- `--dry-run` - Preview changes without applying\n\n## Purpose\n\nEnables safe modification of existing commands with automatic validation and conflict checking. Maintains registry consistency and shows diffs before applying changes.\n\n## Workflow\n\n### 1. Locate Command\n\nFind command file in registry:\n\n```bash\n# Load registry\nREGISTRY=\".claude/command-registry.json\"\n\n# Look up command\nCOMMAND_PATH=$(node -e \"\n  const registry = require('$REGISTRY');\n  const cmd = registry.commands['$COMMAND_NAME'];\n  if (!cmd) {\n    console.error('Command not found: $COMMAND_NAME');\n    process.exit(1);\n  }\n  console.log('.claude/commands/' + cmd.path);\n\")\n\n# Verify file exists\nif [[ ! -f \"$COMMAND_PATH\" ]]; then\n  echo \"Error: Command file not found: $COMMAND_PATH\"\n  exit 1\nfi\n```\n\n### 2. Parse Current State\n\nExtract current command metadata:\n\n```bash\n# Read frontmatter\nFRONTMATTER=$(awk '/^---$/,/^---$/{print}' \"$COMMAND_PATH\" | sed '1d;$d')\n\n# Read content\nCONTENT=$(awk '/^---$/,/^---$/{next} {print}' \"$COMMAND_PATH\")\n\n# Parse metadata\nCURRENT_DESC=$(grep \"description:\" <<< \"$FRONTMATTER\" | cut -d'\"' -f2)\nCURRENT_CATEGORY=$(grep \"category:\" <<< \"$FRONTMATTER\" | awk '{print $2}')\nCURRENT_MODEL=$(grep \"model:\" <<< \"$FRONTMATTER\" | awk '{print $2}')\nCURRENT_TOOLS=$(grep \"allowedTools:\" <<< \"$FRONTMATTER\")\n```\n\n### 3. Apply Modifications\n\nMake requested changes to metadata:\n\n**Update description:**\n\n```bash\nif [[ -n \"$NEW_DESCRIPTION\" ]]; then\n  # Validate length\n  DESC_LEN=${#NEW_DESCRIPTION}\n  if [[ $DESC_LEN -lt 10 || $DESC_LEN -gt 200 ]]; then\n    echo \"Error: Description must be 10-200 characters\"\n    exit 1\n  fi\n\n  # Update in frontmatter\n  sed -i \"s/description:.*/description: \\\"$NEW_DESCRIPTION\\\"/\" \"$TEMP_FILE\"\nfi\n```\n\n**Change category:**\n\n```bash\nVALID_CATEGORIES=(\"automation\" \"code-quality\" \"development\" \"deployment\" \"documentation\" \"git\" \"meta\" \"optimization\" \"testing\" \"workflow\" \"utility\")\n\nif [[ -n \"$NEW_CATEGORY\" ]]; then\n  # Validate category\n  if [[ ! \" ${VALID_CATEGORIES[@]} \" =~ \" ${NEW_CATEGORY} \" ]]; then\n    echo \"Error: Invalid category: $NEW_CATEGORY\"\n    echo \"Valid categories: ${VALID_CATEGORIES[*]}\"\n    exit 1\n  fi\n\n  # Update in frontmatter\n  sed -i \"s/category:.*/category: $NEW_CATEGORY/\" \"$TEMP_FILE\"\nfi\n```\n\n**Add/remove tools:**\n\n```bash\nif [[ -n \"$ADD_TOOL\" ]]; then\n  # Validate tool syntax\n  if [[ ! \"$ADD_TOOL\" =~ ^(Read|Write|Edit|Bash|Glob|Grep|Task|WebFetch|WebSearch|LSP|AskUserQuestion|TodoWrite|Skill)\\(.*\\)$ ]]; then\n    echo \"Error: Invalid tool syntax: $ADD_TOOL\"\n    echo \"Example: Bash(git:*), Read(*), Edit(*)\"\n    exit 1\n  fi\n\n  # Add to allowedTools array\n  # ... (implementation)\nfi\n\nif [[ -n \"$REMOVE_TOOL\" ]]; then\n  # Remove from allowedTools array\n  # ... (implementation)\nfi\n```\n\n**Add/remove arguments:**\n\n```bash\nif [[ -n \"$ADD_ARG\" ]]; then\n  # Parse argument: name:type:description\n  IFS=':' read -r ARG_NAME ARG_TYPE ARG_DESC <<< \"$ADD_ARG\"\n\n  # Validate argument\n  if [[ ! \"$ARG_NAME\" =~ ^[a-z][a-z0-9_]*$ ]]; then\n    echo \"Error: Invalid argument name: $ARG_NAME\"\n    exit 1\n  fi\n\n  # Add to arguments section\n  # ... (implementation)\nfi\n```\n\n**Change model:**\n\n```bash\nVALID_MODELS=(\"sonnet\" \"opus\" \"haiku\")\n\nif [[ -n \"$NEW_MODEL\" ]]; then\n  # Validate model\n  if [[ ! \" ${VALID_MODELS[@]} \" =~ \" ${NEW_MODEL} \" ]]; then\n    echo \"Error: Invalid model: $NEW_MODEL\"\n    echo \"Valid models: sonnet, opus, haiku\"\n    exit 1\n  fi\n\n  # Update in frontmatter\n  sed -i \"s/model:.*/model: $NEW_MODEL/\" \"$TEMP_FILE\"\nfi\n```\n\n### 4. Validate Changes\n\nEnsure modifications produce valid command:\n\n```bash\n# Create temporary file with changes\ncp \"$COMMAND_PATH\" \"$TEMP_FILE\"\n\n# Apply all modifications\n# ... (apply changes to $TEMP_FILE)\n\n# Validate modified command\nnode .claude/validators/validate-command.js \"$TEMP_FILE\"\n\nif [[ $? -ne 0 ]]; then\n  echo \"Error: Validation failed\"\n  echo \"Temporary file: $TEMP_FILE\"\n  echo \"Review errors above and try again\"\n  exit 1\nfi\n```\n\n### 5. Show Diff\n\nDisplay changes before applying:\n\n```bash\n# Show colored diff\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\necho \"Changes to: $COMMAND_NAME\"\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\necho\n\n# Git-style diff\ndiff -u \"$COMMAND_PATH\" \"$TEMP_FILE\" || true\n\necho\necho \"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\"\n\n# Summary of changes\necho\necho \"Summary:\"\n[[ \"$CURRENT_DESC\" != \"$NEW_DESC\" ]] && echo \"  Description: $CURRENT_DESC → $NEW_DESC\"\n[[ \"$CURRENT_CATEGORY\" != \"$NEW_CATEGORY\" ]] && echo \"  Category: $CURRENT_CATEGORY → $NEW_CATEGORY\"\n[[ \"$CURRENT_MODEL\" != \"$NEW_MODEL\" ]] && echo \"  Model: $CURRENT_MODEL → $NEW_MODEL\"\necho\n```\n\n### 6. Confirm and Apply\n\nAsk for confirmation (unless --dry-run or --force):\n\n```bash\nif [[ \"$DRY_RUN\" == \"true\" ]]; then\n  echo \"Dry run - no changes applied\"\n  exit 0\nfi\n\nif [[ \"$FORCE\" != \"true\" ]]; then\n  read -p \"Apply these changes? (y/N) \" -n 1 -r\n  echo\n  if [[ ! $REPLY =~ ^[Yy]$ ]]; then\n    echo \"Aborted\"\n    exit 0\n  fi\nfi\n\n# Apply changes\nmv \"$TEMP_FILE\" \"$COMMAND_PATH\"\n\necho \"✅ Changes applied successfully\"\n```\n\n### 7. Update Registry\n\nUpdate command metadata in registry:\n\n```bash\n# Update registry with new metadata\nnode .claude/validators/update-registry.js update \"$COMMAND_NAME\" \\\n  \"description=$NEW_DESC\" \\\n  \"category=$NEW_CATEGORY\" \\\n  \"model=$NEW_MODEL\"\n\necho \"✅ Registry updated\"\n```\n\n## Examples\n\n```bash\n# Interactive mode\n/edit-command my-command --interactive\n\n# Update description\n/edit-command deploy-staging --description \"Deploy to staging with smoke tests\"\n\n# Change category\n/edit-command my-tool --category optimization\n\n# Add tool permission\n/edit-command deploy-prod --add-tool \"Bash(ssh:*)\"\n\n# Change model\n/edit-command expensive-command --model haiku\n\n# Add argument\n/edit-command generate-component --add-arg \"style:string:Component style (css/scss/styled)\"\n\n# Preview changes\n/edit-command my-command --description \"New desc\" --dry-run\n\n# Delete command\n/edit-command old-command --delete\n```\n\n## Interactive Mode\n\nWhen using `--interactive`, presents menu-driven interface:\n\n```\nEditing command: deploy-staging\nCurrent state:\n  Description: Deploy code to staging environment\n  Category: deployment\n  Model: sonnet\n  Tools: Bash(git:*), Bash(npm:*)\n  Arguments: branch (string)\n\nWhat would you like to edit?\n  1. Description\n  2. Category\n  3. Model\n  4. Tools (add/remove)\n  5. Arguments (add/remove)\n  6. Delete command\n  7. View full file\n  8. Done\n>\n\n? Select option: 1\n\nCurrent description:\n  Deploy code to staging environment\n\n? New description:\n> Deploy code to staging with automated smoke tests\n\n✅ Description updated\n\nWhat would you like to edit?\n  ...\n```\n\n## Delete Command\n\nSafe command deletion with confirmation:\n\n```bash\n/edit-command old-command --delete\n\n# Prompt:\nYou are about to delete: old-command\nLocation: .claude/commands/generated/old-command.md\n\nThis will:\n  - Remove the command file\n  - Update the registry\n  - Remove from conflict tracking\n\nThis action cannot be undone.\n\n? Are you sure? (y/N)\n```\n\n## Safety Features\n\n- ✅ Validation before applying changes\n- ✅ Diff preview shows exact changes\n- ✅ Confirmation required (unless --force)\n- ✅ Dry-run mode for testing\n- ✅ Automatic backup before editing\n- ✅ Registry consistency maintained\n- ✅ Rollback on validation failure\n\n## Advanced Usage\n\n### Bulk Editing\n\nEdit multiple commands with same change:\n\n```bash\n# Change all deployment commands to use haiku\nfor cmd in deploy-dev deploy-staging deploy-prod; do\n  /edit-command $cmd --model haiku --force\ndone\n```\n\n### Template Upgrade\n\nMigrate command to different template:\n\n```bash\n# Manually:\n# 1. Note current command details\n/edit-command old-command --interactive\n\n# 2. Create new command with updated template\n/create-command old-command --template new-template --force\n\n# 3. Port over custom logic\n```\n\n### Version Control Integration\n\nTrack command changes in git:\n\n```bash\n# Before editing\ngit add .claude/commands/generated/my-command.md\n\n# Edit\n/edit-command my-command --description \"Updated functionality\"\n\n# Review\ngit diff .claude/commands/generated/my-command.md\n\n# Commit\ngit commit -m \"Update my-command description\"\n```\n\n## Troubleshooting\n\n**Error: \"Command not found\"**\n```\nSolution: Check command name and registry\n- List commands: cat .claude/command-registry.json | jq '.commands | keys'\n- Check spelling\n- Verify command exists\n```\n\n**Error: \"Validation failed after edit\"**\n```\nSolution: Review validation errors\n- Check syntax errors\n- Verify all required fields\n- Ensure tools are properly formatted\n- Validate argument types\n```\n\n**Error: \"Registry update failed\"**\n```\nSolution: Manually fix registry\n- Check .claude/command-registry.json syntax\n- Verify file permissions\n- Re-run: node .claude/validators/update-registry.js\n```\n\n## Related Commands\n\n- `/create-command` - Create new commands\n- `/command-validate` - Validate command files\n- `/workflow-compose` - Create workflows from commands\n\n---\n\n**Pro Tip:** Use `/edit-command --dry-run` to preview changes before applying them, especially when editing critical production commands.\n",
        "templates/.claude/commands/meta/workflow-compose.md": "# Workflow Compose\n\nCreate YAML-based workflow automation with command generation.\n\n**Usage:** `/workflow-compose <name> [options]`\n\n**Arguments:**\n- `name` (string) - Workflow name (kebab-case)\n\n**Options:**\n- `--interactive` - Interactive workflow builder (default)\n- `--from-commands \"<cmd1> <cmd2>...\"` - Build from existing commands\n- `--template <type>` - Workflow template (ci-pipeline, release, deployment, maintenance)\n- `--generate-command` - Auto-generate command wrapper\n- `--dry-run` - Preview without creating files\n\n## Purpose\n\nEnables complex automation by composing multiple commands into reusable YAML workflows. Workflows support sequential and parallel execution, conditional logic, variable substitution, and error handling.\n\n## Workflow\n\n### 1. Define Workflow Structure\n\nSpecify workflow metadata and configuration:\n\n**Interactive prompts:**\n\n```\nCreating workflow: production-release\n\n? Workflow description:\n> Complete production release with quality checks and deployment\n\n? Workflow pattern:\n  1. Sequential - Steps run one after another\n  2. Parallel - Steps run simultaneously\n  3. Hierarchical - Parent task with sub-tasks\n  4. Mixed - Combination of above\n> 4\n\n? Require inputs from user?\n> yes\n\n? Input parameters (comma-separated):\n> version_type, skip_tests\n\n? Environment variables needed?\n> NODE_ENV=production, CI=true\n```\n\n### 2. Add Workflow Steps\n\nBuild step-by-step workflow:\n\n**Step types:**\n- `command` - Execute Claude Code command\n- `bash` - Run bash command\n- `parallel` - Run multiple steps concurrently\n- `sequential` - Explicit sequential steps\n- `manual` - Wait for user input\n\n**Interactive step builder:**\n\n```\nStep 1: What type of step?\n  1. Command (/audit-code, /optimize, etc.)\n  2. Bash (npm test, git push, etc.)\n  3. Parallel steps\n  4. Manual checkpoint\n> 1\n\n? Which command?\n> /audit-code --strict\n\n? Step name:\n> Quality Checks\n\n? Continue on error?\n> no\n\n✅ Step 1 added: Quality Checks (/audit-code --strict)\n\nAdd another step? (Y/n) y\n\nStep 2: What type of step?\n> 2\n\n? Bash command:\n> npm run build && npm test\n\n? Step name:\n> Build and Test\n\n? Timeout (milliseconds):\n> 1800000\n\n? Continue on error?\n> no\n\n✅ Step 2 added: Build and Test\n\nAdd another step? (Y/n) y\n\nStep 3: What type of step?\n> 1\n\n? Which command?\n> /release ${inputs.version_type}\n\n? Step name:\n> Release\n\n? Require confirmation?\n> yes\n\n✅ Step 3 added: Release\n\nAdd another step? (Y/n) y\n\nStep 4: What type of step?\n> 2\n\n? Bash command:\n> npm run deploy:production\n\n? Step name:\n> Deploy\n\n? When to run this step?\n  1. Always\n  2. Only if previous step succeeded\n  3. Only if previous step failed\n  4. Custom condition\n> 2\n\n? Condition:\n> steps.release.exit_code == 0\n\n✅ Step 4 added: Deploy (conditional)\n\nAdd another step? (Y/n) n\n```\n\n### 3. Add Error Handling\n\nDefine failure and success handlers:\n\n```\n? What should happen on failure?\n  1. Rollback changes\n  2. Send notification\n  3. Run cleanup command\n  4. Custom steps\n  5. Nothing (default)\n> 1\n\n? Rollback steps:\n> /clean build\n> git reset --hard HEAD\n\n✅ Failure handler added\n\n? What should happen on success?\n  1. Send notification\n  2. Run command\n  3. Custom message\n  4. Nothing\n> 3\n\n? Success message:\n> Production release complete! Version: ${inputs.version_type}\n\n✅ Success handler added\n```\n\n### 4. Generate Workflow YAML\n\nCreate YAML file from specifications:\n\n```yaml\n# File: .claude/workflows/production-release.yml\nname: \"Production Release\"\nversion: \"1.0\"\ndescription: \"Complete production release with quality checks and deployment\"\n\ninputs:\n  version_type:\n    type: string\n    default: patch\n    allowed: [patch, minor, major]\n    description: \"Version bump type\"\n\n  skip_tests:\n    type: boolean\n    default: false\n    description: \"Skip test suite\"\n\nenv:\n  NODE_ENV: production\n  CI: true\n\nsteps:\n  - name: \"Quality Checks\"\n    command: /audit-code --strict\n    fail_on_error: true\n\n  - name: \"Build and Test\"\n    bash: npm run build && npm test\n    timeout: 1800000\n    fail_on_error: true\n    when: ${{ inputs.skip_tests == false }}\n\n  - name: \"Release\"\n    command: /release ${inputs.version_type}\n    confirm: true\n    fail_on_error: true\n\n  - name: \"Deploy\"\n    when: ${{ steps.release.exit_code == 0 }}\n    bash: npm run deploy:production\n    fail_on_error: true\n\non_failure:\n  - command: /clean build\n  - bash: git reset --hard HEAD\n  - message: \"Release failed - changes rolled back\"\n\non_success:\n  - message: \"Production release complete! Version: ${inputs.version_type}\"\n\nmetadata:\n  created: \"2025-12-28T10:15:00Z\"\n  generator: \"workflow-compose\"\n  category: \"deployment\"\n```\n\n### 5. Validate Workflow\n\nEnsure workflow is valid:\n\n```bash\n# Validate YAML syntax\nnode -e \"\n  const yaml = require('js-yaml');\n  const fs = require('fs');\n  try {\n    const doc = yaml.load(fs.readFileSync('.claude/workflows/production-release.yml', 'utf8'));\n    console.log('✅ YAML syntax valid');\n  } catch (e) {\n    console.error('❌ YAML syntax error:', e.message);\n    process.exit(1);\n  }\n\"\n\n# Validate workflow schema\nnode .claude/utils/workflows/validator.js .claude/workflows/production-release.yml\n\n# Check referenced commands exist\nfor cmd in audit-code release clean; do\n  if ! grep -q \"\\\"$cmd\\\"\" .claude/command-registry.json; then\n    echo \"Warning: Command /$cmd not found in registry\"\n  fi\ndone\n```\n\n### 6. Generate Command Wrapper (Optional)\n\nAuto-generate command to run workflow:\n\n```bash\nif [[ \"$GENERATE_COMMAND\" == \"true\" ]]; then\n  # Create command that executes workflow\n  /create-command \"production-release\" --template with-arguments --force\n\n  # Populate with workflow execution\n  cat > .claude/commands/generated/production-release.md <<'EOF'\n# Production Release\n\nRun production release workflow.\n\n**Usage:** `/production-release [version_type] [options]`\n\n**Arguments:**\n- `version_type` (string) - Version bump type: patch, minor, major (default: patch)\n\n**Options:**\n- `--skip-tests` - Skip test suite\n- `--dry-run` - Preview workflow without execution\n\n## Purpose\n\nExecutes production release workflow defined in .claude/workflows/production-release.yml\n\n## Workflow\n\n### 1. Execute Workflow\n\nRun workflow with parameters:\n\n\\`\\`\\`bash\nnode .claude/utils/workflows/engine.js \\\n  .claude/workflows/production-release.yml \\\n  --input version_type=${VERSION_TYPE} \\\n  --input skip_tests=${SKIP_TESTS}\n\\`\\`\\`\n\n## Examples\n\n\\`\\`\\`bash\n# Patch release\n/production-release\n\n# Minor release\n/production-release minor\n\n# Major release, skip tests\n/production-release major --skip-tests\n\n# Preview\n/production-release --dry-run\n\\`\\`\\`\nEOF\n\n  echo \"✅ Command wrapper created: /production-release\"\nfi\n```\n\n### 7. Preview and Save\n\nShow summary and save workflow:\n\n```\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nWorkflow Created Successfully\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nName: production-release\nDescription: Complete production release with quality checks and deployment\n\nFile: .claude/workflows/production-release.yml\nCommand: /production-release (auto-generated)\n\nInputs:\n  - version_type (string): Version bump type [patch|minor|major]\n  - skip_tests (boolean): Skip test suite\n\nSteps: 4\n  1. Quality Checks (/audit-code --strict)\n  2. Build and Test (npm run build && npm test)\n  3. Release (/release ${inputs.version_type})\n  4. Deploy (npm run deploy:production) [conditional]\n\nError Handling:\n  On Failure: Rollback + cleanup\n  On Success: Success message\n\nValidation: ✅ PASSED\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nNext steps:\n1. Review workflow: cat .claude/workflows/production-release.yml\n2. Test workflow: /workflow production-release --dry-run\n3. Run workflow: /production-release patch\n\nWorkflow is ready to use!\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n## Examples\n\n### Simple Sequential Workflow\n\n```bash\n/workflow-compose daily-checks\n\n# Interactive prompts:\nSteps:\n1. /audit-code\n2. /optimize session\n3. npm outdated\n\nResult: .claude/workflows/daily-checks.yml\n```\n\n### Parallel Execution Workflow\n\n```bash\n/workflow-compose ci-pipeline --template ci-pipeline\n\n# Generated YAML:\nsteps:\n  - name: \"Lint and Test\"\n    type: parallel\n    steps:\n      - bash: npm run lint\n      - bash: npm run type-check\n      - bash: npm test\n```\n\n### From Existing Commands\n\n```bash\n/workflow-compose my-process --from-commands \"/audit-code /optimize /clean\"\n\n# Automatically creates workflow with those commands in sequence\n```\n\n## Workflow Templates\n\n### CI Pipeline Template\n\n```yaml\nname: \"CI Pipeline\"\nsteps:\n  - bash: npm ci\n  - type: parallel\n    steps:\n      - bash: npm run lint\n      - bash: npm run type-check\n      - command: /audit-code\n  - bash: npm test -- --coverage\n  - bash: npm run build\n```\n\n### Release Template\n\n```yaml\nname: \"Release\"\ninputs:\n  version_type:\n    type: string\n    allowed: [patch, minor, major]\nsteps:\n  - command: /audit-code --strict\n  - bash: npm test\n  - command: /release ${inputs.version_type}\n  - bash: git push origin main --tags\n```\n\n### Deployment Template\n\n```yaml\nname: \"Deploy\"\ninputs:\n  environment:\n    type: string\n    allowed: [dev, staging, production]\nsteps:\n  - bash: npm run build\n  - bash: npm run deploy:${inputs.environment}\n  - bash: curl -f https://${inputs.environment}.example.com/health\n```\n\n### Maintenance Template\n\n```yaml\nname: \"Daily Maintenance\"\nschedule:\n  cron: \"0 9 * * *\"\nsteps:\n  - bash: npm update\n  - command: /clean cache temp\n  - command: /audit-code --report\n```\n\n## Workflow DSL Reference\n\n### Input Types\n\n```yaml\ninputs:\n  string_input:\n    type: string\n    default: \"value\"\n    allowed: [option1, option2]  # Optional\n\n  number_input:\n    type: number\n    default: 10\n    min: 0\n    max: 100\n\n  boolean_input:\n    type: boolean\n    default: false\n\n  choice_input:\n    type: choice\n    choices: [a, b, c]\n```\n\n### Conditional Logic\n\n```yaml\nsteps:\n  - name: \"Conditional Step\"\n    when: ${{ inputs.env == 'production' }}\n    bash: npm run deploy:prod\n\n  - name: \"Based on Previous Step\"\n    when: ${{ steps.build.exit_code == 0 }}\n    bash: echo \"Build succeeded\"\n```\n\n### Variable Substitution\n\n```yaml\nenv:\n  APP_NAME: myapp\n  VERSION: 1.0.0\n\nsteps:\n  - bash: echo \"Deploying ${env.APP_NAME} v${env.VERSION}\"\n  - bash: deploy --version ${inputs.version}\n  - bash: echo \"Result: ${steps.deploy.output}\"\n```\n\n### Error Handling\n\n```yaml\non_failure:\n  - command: /clean build\n  - bash: git reset --hard HEAD\n  - message: \"Workflow failed - rolled back\"\n\non_success:\n  - message: \"Workflow complete!\"\n  - bash: echo \"Success\" >> workflow.log\n```\n\n## Advanced Features\n\n### Nested Workflows\n\n```yaml\nsteps:\n  - name: \"Run Another Workflow\"\n    workflow: .claude/workflows/sub-workflow.yml\n    inputs:\n      param: value\n```\n\n### Retry Logic\n\n```yaml\nsteps:\n  - name: \"Flaky Test\"\n    bash: npm test\n    retry:\n      attempts: 3\n      delay: 1000\n      on_exit_codes: [1, 2]\n```\n\n### Timeout\n\n```yaml\nsteps:\n  - name: \"Long Running Task\"\n    bash: npm run heavy-task\n    timeout: 3600000  # 1 hour in ms\n```\n\n### Manual Checkpoints\n\n```yaml\nsteps:\n  - name: \"Build\"\n    bash: npm run build\n\n  - name: \"Review Build\"\n    manual: \"Review build output before deploying. Press Enter to continue.\"\n\n  - name: \"Deploy\"\n    bash: npm run deploy\n```\n\n## Safety Features\n\n- ✅ YAML validation\n- ✅ Step dependency checking\n- ✅ Command existence validation\n- ✅ Dry-run mode\n- ✅ Automatic rollback on failure\n- ✅ Timeout protection\n- ✅ Confirmation for destructive steps\n\n## Best Practices\n\n1. **Name steps clearly:** Use descriptive names\n2. **Add error handling:** Always define on_failure\n3. **Use timeouts:** Prevent infinite hangs\n4. **Test with --dry-run:** Preview before running\n5. **Version control:** Commit workflows to git\n6. **Keep workflows focused:** One workflow, one purpose\n7. **Document inputs:** Clear descriptions for all inputs\n\n## Troubleshooting\n\n**Error: \"Invalid YAML syntax\"**\n```\nSolution: Check YAML formatting\n- Ensure proper indentation (2 spaces)\n- Quote strings with special characters\n- Validate at yamllint.com\n```\n\n**Error: \"Command not found\"**\n```\nSolution: Ensure referenced commands exist\n- Check command registry\n- Create missing commands first\n- Use full command names (include /prefix in workflow)\n```\n\n**Error: \"Workflow validation failed\"**\n```\nSolution: Review validation errors\n- Check all required fields present\n- Verify step types are valid\n- Ensure conditional syntax is correct\n```\n\n## Related Commands\n\n- `/workflow` - Execute workflows\n- `/create-command` - Create individual commands\n- `/edit-command` - Modify commands\n\n---\n\n**Pro Tip:** Start with simple sequential workflows, then add parallel execution and conditional logic as needed. Use templates for common patterns.\n",
        "templates/.claude/skills/anthropic/claude-code/skill.md": "---\nname: claude-code-expert\ndescription: Expert on Claude Code CLI, skills, commands, hooks, plugins, MCP, settings, and workflows. Triggers on claude code, cli, skill, command, hook, plugin, mcp, slash command, settings\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Claude Code Expert\n\n## Purpose\n\nProvide expert guidance on Claude Code CLI features, including skills, commands, hooks, plugins, MCP integration, and configuration based on official Claude Code documentation.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Claude Code** - CLI tool, features, usage\n- **Skills** - creating, using, configuring skills\n- **Commands** - slash commands, custom commands\n- **Hooks** - pre/post tool use hooks, validation\n- **Plugins** - MCP plugins, plugin system\n- **Configuration** - settings.json, CLAUDE.md, customization\n- **Features** - agents, memory, sandboxing, headless mode\n\n## Knowledge Base\n\nDocumentation is stored in Markdown format (multiple languages):\n- **Location:** `docs/`\n- **Index:** `docs/INDEX.md`\n- **Format:** `.md` files\n- **Note:** English docs have `_en` suffix, e.g., `docs_en_skills.md`\n\n## Process\n\nWhen a user asks about Claude Code:\n\n### 1. Identify Topic\n```\nCommon topics:\n- Getting started / installation\n- Creating skills\n- Writing slash commands\n- Implementing hooks\n- Using MCP plugins\n- Configuration (settings.json, CLAUDE.md)\n- Agents and sub-agents\n- Memory and context management\n- Sandboxing and security\n- Headless/CI mode\n- IDE integration (VS Code, JetBrains)\n```\n\n### 2. Search Documentation\n\nUse Grep to find relevant English docs:\n```bash\n# Search for specific topics (focus on English docs)\nGrep \"skill\" docs/ --output-mode files_with_matches --glob \"*_en_*.md\"\nGrep \"hook|validation\" docs/ --output-mode content -C 3 --glob \"*_en_*.md\"\n```\n\nCheck the INDEX.md for navigation:\n```bash\nRead docs/INDEX.md\n```\n\n### 3. Read Relevant Files\n\nRead the most relevant English documentation files:\n```bash\n# Prefer English (_en) versions\nRead docs/code_claude_com/docs_en_skills.md\nRead docs/code_claude_com/docs_en_slash-commands.md\n```\n\n### 4. Provide Answer\n\nStructure your response:\n- **Direct answer** - solve the user's problem first\n- **File examples** - show skill.md, command.md structure\n- **Configuration** - show settings.json snippets\n- **Best practices** - mention Claude Code-specific patterns\n- **References** - cite specific docs (prefer English versions)\n- **File paths** - use proper `.claude/` directory structure\n\n## Example Workflows\n\n### Example 1: Creating a Skill\n```\nUser: \"How do I create a skill in Claude Code?\"\n\n1. Search: Grep \"skill\" docs/ --glob \"*_en_*.md\"\n2. Read: docs_en_skills.md\n3. Answer:\n   - Explain skill.md frontmatter format\n   - Show directory structure\n   - Provide skill template\n   - Explain trigger keywords\n   - Mention allowed-tools\n```\n\n### Example 2: Writing Hooks\n```\nUser: \"How do I create a post-edit hook?\"\n\n1. Search: Grep \"hook|PostToolUse\" docs/ --glob \"*_en_*.md\"\n2. Read: docs_en_hooks.md, docs_en_hooks-guide.md\n3. Answer:\n   - Explain hook types (PostToolUse, etc.)\n   - Show hook file structure\n   - Demonstrate settings.json configuration\n   - Provide validation example\n```\n\n### Example 3: MCP Integration\n```\nUser: \"How do I use MCP plugins with Claude Code?\"\n\n1. Search: Grep \"mcp|plugin\" docs/ --glob \"*_en_*.md\"\n2. Read: docs_en_mcp.md, docs_en_plugins.md\n3. Answer:\n   - Explain MCP (Model Context Protocol)\n   - Show plugin installation\n   - Demonstrate configuration\n   - List available plugins\n```\n\n## Key Concepts to Reference\n\n**Core Components:**\n- Skills (auto-invoked knowledge domains)\n- Commands (slash commands, manual workflows)\n- Hooks (validation, automation)\n- Plugins (MCP extensions)\n- CLAUDE.md (project instructions)\n- settings.json (configuration)\n\n**Features:**\n- Agents and sub-agents\n- Memory system\n- Sandboxing (Docker, Podman)\n- Headless mode (CI/CD)\n- IDE integration (VS Code, JetBrains)\n- Third-party integrations\n\n**Directory Structure:**\n```\n.claude/\n├── skills/           # Auto-invoked skills\n├── commands/         # Slash commands\n├── hooks/            # Validation hooks\n├── docs/             # Documentation\n└── settings.json     # Configuration\n```\n\n**Configuration Files:**\n- `.claude/settings.json` - Claude Code settings\n- `CLAUDE.md` - Project-specific instructions\n- `skill.md` - Skill definition (with frontmatter)\n- `command-name.md` - Command workflow\n\n## Response Style\n\n- **Practical** - developers want working examples\n- **File-structure focused** - show exact file locations\n- **Configuration-clear** - precise JSON/YAML examples\n- **English-first** - reference `_en` docs when available\n- **Cite sources** - reference specific doc files\n\n## Follow-up Suggestions\n\nAfter answering, suggest:\n- Related Claude Code features\n- Configuration best practices\n- Testing and debugging approaches\n- Community resources\n- Advanced workflows\n",
        "templates/.claude/skills/anthropic/claude-command-builder/skill.md": "---\nname: claude-command-builder\ndescription: Interactive slash command creator for Claude Code. Triggers when user mentions creating commands, slash commands, command templates, command arguments, or wants to build a new command workflow.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Claude Code Command Builder\n\n## Purpose\n\nGuide users through creating effective Claude Code slash commands with proper structure, argument handling, and workflow design. Auto-invokes when users want to create or modify custom commands.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Creating commands** - \"create command\", \"make command\", \"new slash command\"\n- **Command structure** - \"command template\", \"command format\", \"command frontmatter\"\n- **Arguments** - \"$ARGUMENTS\", \"$1\", \"$2\", \"command parameters\", \"positional args\"\n- **Workflows** - \"command workflow\", \"command steps\", \"command process\"\n- **Bash execution** - \"!`command`\", \"execute bash in command\", \"command with bash\"\n\n## Knowledge Base\n\n- Official docs: `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_slash-commands.md`\n- Project guide: `.claude/docs/creating-components.md`\n- Examples in repository: `.claude/commands/`\n\n## Process\n\n### 1. Gather Requirements\n\nAsk the user:\n\n```\nLet me help you create a Claude Code slash command! I need a few details:\n\n1. **Command name** (lowercase-with-hyphens):\n   Example: deploy, review-pr, commit, analyze-tokens\n   This will be invoked as: /your-command-name\n\n2. **What does this command do?**\n   Describe the workflow in 1-2 sentences.\n\n3. **Does it need arguments?**\n   - None (simple prompt)\n   - All arguments: $ARGUMENTS\n   - Positional: $1, $2, $3, etc.\n\n4. **Does it need bash execution?**\n   Commands that run before the slash command (e.g., !`git status`)\n\n5. **Scope:**\n   - Personal (`~/.claude/commands/`) - just for you\n   - Project (`.claude/commands/`) - shared with team\n\n6. **Namespace/subdirectory?**\n   Example: git/, deploy/, testing/\n   Helps organize related commands\n```\n\n### 2. Validate Input\n\nCheck the command name:\n- Must be valid filename (no spaces, special chars except hyphen)\n- Descriptive and memorable\n- Won't conflict with built-in commands\n- Use hyphens (not underscores)\n\nValidate arguments:\n- Define expected arguments\n- Provide defaults if needed\n- Document argument order\n\n### 3. Determine Command Type\n\n**Simple Prompt (no frontmatter):**\n```markdown\nAnalyze this code for performance issues and suggest optimizations.\n```\n\n**With Arguments:**\n```markdown\n---\nargument-hint: [file-path]\ndescription: Analyze file for performance issues\n---\n\nAnalyze the file at $1 for performance issues and suggest optimizations.\n```\n\n**With Bash Execution:**\n```markdown\n---\nallowed-tools: Bash(git status:*), Bash(git diff:*)\ndescription: Create a git commit\n---\n\n## Current State\n\n- Git status: !`git status`\n- Staged changes: !`git diff --staged`\n- Recent commits: !`git log --oneline -5`\n\n## Your Task\n\nBased on the above changes, create a git commit with a clear, conventional commit message.\n```\n\n**Full-Featured:**\n```markdown\n---\nallowed-tools: Bash(npm run:*), Bash(git add:*), Bash(git commit:*)\nargument-hint: [component-name]\ndescription: Create a new React component with tests\nmodel: sonnet\n---\n\n# Create React Component\n\nComponent name: $1\n\nExecute the following workflow:\n\n1. **Validate Input**\n   !`test -n \"$1\" && echo \"Creating component: $1\" || echo \"Error: Component name required\"`\n\n2. **Check Existing Files**\n   !`ls src/components/$1.tsx 2>/dev/null || echo \"Component does not exist\"`\n\n3. **Create Files**\n   Create the following files:\n   - `src/components/$1.tsx`\n   - `src/components/$1.test.tsx`\n   - `src/components/$1.module.css`\n\n4. **Run Tests**\n   After creation, run: !`npm run test -- $1`\n```\n\n### 4. Generate Command File\n\nCreate command structure based on complexity:\n\n**Template for Simple Command:**\n```markdown\nBrief description of what the command does.\n\n[Prompt instructions for Claude]\n```\n\n**Template for Command with Frontmatter:**\n```markdown\n---\nargument-hint: [arg1] [arg2]\ndescription: Brief description shown in /help\nallowed-tools: Bash(command:*), Read, Write\nmodel: sonnet\ndisable-model-invocation: false\n---\n\n# Command Name\n\nUsage: /command-name [args]\n\n[Detailed instructions]\n```\n\n### 5. Build Command Workflow\n\nStructure the workflow with clear steps:\n\n```markdown\nExecute the following workflow:\n\n1. **Step Name**\n   ```bash\n   # Bash command (if needed)\n   command arg1 arg2\n   ```\n   - What this step does\n   - Validation checks\n   - Error handling\n\n2. **Next Step**\n   [Instructions for Claude]\n   - What to check\n   - How to proceed\n   - What to output\n\n3. **Final Step**\n   - Summary of results\n   - Next actions for user\n   - Success criteria\n```\n\n### 6. Add Argument Handling\n\n**All Arguments ($ARGUMENTS):**\n```markdown\nFix issue #$ARGUMENTS following our coding standards.\n```\nUser runs: `/fix-issue 123 high-priority`\nBecomes: \"Fix issue #123 high-priority following our coding standards.\"\n\n**Positional Arguments ($1, $2, $3):**\n```markdown\nReview PR #$1 with priority $2 and assign to $3.\nFocus on: $4\n```\nUser runs: `/review-pr 456 high alice security`\nBecomes individual parameters you can reference separately.\n\n**With Defaults:**\n```markdown\n---\nargument-hint: [environment] [branch]\n---\n\nDeploy to environment: ${1:-staging}\nFrom branch: ${2:-main}\n```\n\n### 7. Add Bash Execution (if needed)\n\nUse `!` prefix to execute commands before processing:\n\n```markdown\n---\nallowed-tools: Bash(git:*)\n---\n\n## Context\n\n- Current branch: !`git branch --show-current`\n- Status: !`git status --short`\n- Recent commits: !`git log --oneline -5`\n\n## Your Task\n\n[Instructions based on the above context]\n```\n\n**Important:**\n- Must specify `allowed-tools` with specific Bash permissions\n- Output is included in command context\n- Commands run before Claude processes the prompt\n\n### 8. Add File References\n\nUse `@` prefix to reference files:\n\n```markdown\nReview the implementation in @src/utils/helpers.js\n\nCompare @src/old-version.js with @src/new-version.js\n\nAnalyze all files in @src/components/\n```\n\n### 9. Configure Thinking Mode (if needed)\n\nFor complex problems, trigger extended thinking:\n\n```markdown\nCarefully analyze the following code and think through...\n\nLet's approach this step by step...\n\nConsider all edge cases before implementing...\n```\n\nThese keywords can trigger extended thinking mode.\n\n### 10. Create the File\n\nSave to correct location:\n\n**Personal command:**\n```bash\n~/.claude/commands/command-name.md\n~/.claude/commands/category/command-name.md  # With namespace\n```\n\n**Project command:**\n```bash\n.claude/commands/command-name.md\n.claude/commands/category/command-name.md  # With namespace\n```\n\n### 11. Test the Command\n\nProvide testing instructions:\n\n```\nTo test your command:\n1. Restart Claude Code or start a new session\n2. Type: /help\n3. Find your command in the list\n4. Try: /your-command-name [args]\n5. Verify it behaves as expected\n```\n\n**Test cases:**\n```bash\n# No arguments\n/your-command\n\n# With arguments\n/your-command arg1\n/your-command arg1 arg2\n\n# Edge cases\n/your-command \"\"\n/your-command \"with spaces\"\n```\n\n## Frontmatter Reference\n\nField| Purpose| Example\n---|---|---\n`argument-hint`| Show expected arguments in autocomplete| `[pr-number] [priority]`\n`description`| Brief description for `/help` menu| `Review pull request`\n`allowed-tools`| Tools command can use| `Bash(git:*), Read, Write`\n`model`| Specific model to use| `claude-sonnet-4-5-20250929`\n`disable-model-invocation`| Prevent SlashCommand tool from calling this| `true`\n\n## Bash Tool Permissions\n\nWhen using `!` prefix or needing bash execution:\n\n```markdown\n---\nallowed-tools: Bash(git add:*), Bash(git commit:*), Bash(git push:*)\n---\n```\n\n**Permission patterns:**\n- `Bash(git:*)` - All git commands\n- `Bash(npm run:*)` - All npm run scripts\n- `Bash(git add:*), Bash(git commit:*)` - Specific git commands\n\n## Argument Patterns\n\n### Pattern 1: All Arguments\n```markdown\nRun tests for: $ARGUMENTS\n```\nUsage: `/test users api database`\nBecomes: \"Run tests for: users api database\"\n\n### Pattern 2: Positional\n```markdown\nDeploy $1 to $2 environment with tag $3\n```\nUsage: `/deploy my-app staging v1.2.3`\nBecomes: \"Deploy my-app to staging environment with tag v1.2.3\"\n\n### Pattern 3: Mixed\n```markdown\n---\nargument-hint: <file> [rest of args]\n---\n\nAnalyze file $1 for: $ARGUMENTS\n```\nUsage: `/analyze src/app.js performance security`\nBecomes: \"Analyze file src/app.js for: src/app.js performance security\"\nNote: $ARGUMENTS includes all args, so $1 is duplicated\n\n**Better approach:**\n```markdown\nAnalyze file $1 for: ${2:+${@:2}}\n```\nThis uses $1 separately and remaining args starting from $2\n\n### Pattern 4: With Defaults\n```markdown\nEnvironment: ${1:-production}\nVerbose: ${2:-false}\n```\n\n## Command Size Guidelines\n\n- ✅ **Good:** < 100 lines\n- ⚠️ **Warning:** 100-150 lines\n- ❌ **Too large:** > 250 lines (must refactor)\n\n**If too large:**\n- Extract to external script\n- Split into multiple commands\n- Use sub-commands pattern\n\n## Common Command Types\n\n### 1. Git Workflow\n```markdown\n---\nallowed-tools: Bash(git:*)\ndescription: Create conventional commit\n---\n\n## Context\n- Status: !`git status --short`\n- Diff: !`git diff HEAD`\n\nCreate a conventional commit message.\n```\n\n### 2. Code Generator\n```markdown\n---\nargument-hint: [component-name]\ndescription: Generate React component\n---\n\nCreate a new React component named $1:\n- Component file\n- Test file\n- Storybook story\n```\n\n### 3. Analysis Tool\n```markdown\n---\nargument-hint: [file-path]\ndescription: Analyze code complexity\n---\n\nAnalyze @$1 for:\n- Cyclomatic complexity\n- Code smells\n- Improvement suggestions\n```\n\n### 4. Deployment Helper\n```markdown\n---\nallowed-tools: Bash(npm:*), Bash(git:*)\nargument-hint: [environment]\ndescription: Deploy to environment\n---\n\nDeploy to ${1:-staging}:\n1. Run tests: !`npm test`\n2. Build: !`npm run build`\n3. Deploy: !`npm run deploy:$1`\n```\n\n### 5. Documentation Generator\n```markdown\n---\nargument-hint: [file-pattern]\ndescription: Generate API documentation\n---\n\nGenerate documentation for: $1\nInclude:\n- Function signatures\n- Parameters\n- Return types\n- Examples\n```\n\n## Examples from TOON Formatter\n\n**Simple version:**\n```markdown\n# Convert to TOON\n\nConvert the specified JSON file to TOON v2.0 format with automatic optimization and show token savings.\n\nUsage: /convert-to-toon <file>\n```\n\n**Advanced version with bash:**\n```markdown\n---\nallowed-tools: Bash(jq:*), Bash(.claude/skills/toon-formatter/bin/toon:*)\nargument-hint: <file> [--delimiter comma|tab|pipe]\ndescription: Convert JSON to TOON format\n---\n\n# Convert to TOON\n\nFile: $1\nDelimiter: ${2:-comma}\n\n1. **Validate**: !`test -f \"$1\" && jq empty \"$1\" 2>&1`\n2. **Analyze**: !`jq 'if type == \"array\" then length else 0 end' \"$1\"`\n3. **Convert**: !`.claude/skills/toon-formatter/bin/toon encode \"$1\"`\n4. Show savings comparison\n```\n\n## Troubleshooting\n\n### Command Not Found\n\n**Check:**\n```bash\n# List all commands\nls ~/.claude/commands/*.md\nls .claude/commands/*.md\n\n# Verify filename\nls .claude/commands/your-command.md\n```\n\n**Remember:**\n- Filename (without `.md`) becomes command name\n- Hyphens in filename become hyphens in command\n- Case-sensitive on Linux/Mac\n\n### Arguments Not Working\n\n**Debug:**\n```markdown\nDebug: $ARGUMENTS\nDebug $1: \"$1\"\nDebug $2: \"$2\"\n```\n\nRun command and check output to see what's being passed.\n\n### Bash Commands Not Executing\n\n**Check:**\n1. `allowed-tools` includes correct Bash permissions\n2. Using `!` prefix: `!`command``\n3. Backticks are correct: \\`command\\` not 'command'\n4. Command is allowed by permissions\n\n### Command Not in /help\n\n**Possible reasons:**\n- File not in correct location\n- File doesn't have `.md` extension\n- Syntax error in frontmatter\n- Need to restart Claude Code\n\n## Best Practices\n\n### DO:\n✅ Provide clear argument hints\n✅ Include usage examples\n✅ Handle errors gracefully\n✅ Show progress for long operations\n✅ Document expected behavior\n✅ Test with various inputs\n✅ Use descriptive command names\n\n### DON'T:\n❌ Make commands too complex (>250 lines)\n❌ Forget to specify allowed-tools for Bash\n❌ Use unclear argument names\n❌ Skip error handling\n❌ Hardcode values (use arguments)\n❌ Forget to test edge cases\n\n## Comparison: Commands vs Skills\n\n**Use Commands when:**\n- You want explicit control (manual invocation)\n- Simple, repetitive prompts\n- Specific workflow steps\n- Frequently-used templates\n\n**Use Skills when:**\n- Claude should auto-detect need\n- Complex, multi-file workflows\n- Comprehensive domain knowledge\n- Team needs standardized expertise\n\n**Can use both:**\n- Command invokes skill explicitly\n- Skill activates automatically\n- Command provides quick access\n- Skill provides deep capability\n\n## Resources\n\n- **Official Command Docs:** `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_slash-commands.md`\n- **Project Component Guide:** `.claude/docs/creating-components.md`\n- **Command Examples:** `.claude/commands/` directory\n- **Skills vs Commands:** Section in slash-commands.md\n",
        "templates/.claude/skills/anthropic/claude-hook-builder/skill.md": "---\nname: claude-hook-builder\ndescription: Interactive hook creator for Claude Code. Triggers when user mentions creating hooks, PreToolUse, PostToolUse, hook validation, hook configuration, settings.json hooks, or wants to automate tool execution workflows.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Claude Code Hook Builder\n\n## Purpose\n\nGuide users through creating effective Claude Code hooks for tool validation, automation, and workflow enhancement. Auto-invokes when users want to create or configure hooks.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Creating hooks** - \"create hook\", \"make hook\", \"new hook\", \"add hook\"\n- **Hook events** - \"PreToolUse\", \"PostToolUse\", \"UserPromptSubmit\", \"Stop\", \"SessionStart\"\n- **Validation** - \"validate\", \"check\", \"prevent\", \"block\", \"approve\"\n- **Automation** - \"auto-format\", \"auto-lint\", \"automatic\", \"trigger\"\n- **Hook configuration** - \"settings.json hooks\", \"hook matcher\", \"hook command\"\n\n## Knowledge Base\n\n- Official docs: `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_hooks.md`\n- Hook guide: `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_hooks-guide.md`\n- Project guide: `.claude/docs/creating-components.md`\n\n## Process\n\n### 1. Gather Requirements\n\nAsk the user:\n\n```\nLet me help you create a Claude Code hook! I need some details:\n\n1. **What should this hook do?**\n   Examples:\n   - Auto-format code after editing files\n   - Validate bash commands before execution\n   - Add context when user submits prompts\n   - Prevent access to sensitive files\n   - Run tests after file changes\n\n2. **When should it trigger?**\n   - PreToolUse (before tool execution)\n   - PostToolUse (after tool execution)\n   - UserPromptSubmit (when user sends message)\n   - Stop (when Claude finishes responding)\n   - SubagentStop (when subagent finishes)\n   - SessionStart (when session begins)\n   - SessionEnd (when session ends)\n   - Notification (when notification sent)\n   - PermissionRequest (when permission requested)\n\n3. **Which tools should it match?**\n   - Specific tool (Write, Edit, Bash, Read, etc.)\n   - Multiple tools (Write|Edit)\n   - All tools (*)\n   - MCP tools (mcp__server__tool)\n\n4. **What should it return?**\n   - Simple exit code (0 = success, 2 = block)\n   - JSON with decision control\n   - Additional context for Claude\n   - Modified tool inputs\n\n5. **Scope:**\n   - User-level (`~/.claude/settings.json`)\n   - Project-level (`.claude/settings.json`)\n   - Local project (`.claude/settings.local.json`)\n```\n\n### 2. Determine Hook Type\n\n**Bash Command Hook:**\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"/path/to/script.sh\"\n}\n```\n- Runs a shell command\n- Fast, deterministic\n- Good for validation, formatting\n\n**Prompt-based Hook:**\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Evaluate if Claude should stop: $ARGUMENTS\"\n}\n```\n- Uses LLM for decision\n- Context-aware, intelligent\n- Good for complex decisions (Stop, SubagentStop)\n\n### 3. Choose Hook Event\n\n#### PreToolUse\nRuns before tool executes.\n\n**Use for:**\n- Validate inputs\n- Auto-approve safe operations\n- Block dangerous commands\n- Modify tool parameters\n\n**JSON Output:**\n```json\n{\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"PreToolUse\",\n    \"permissionDecision\": \"allow\" | \"deny\" | \"ask\",\n    \"permissionDecisionReason\": \"Why this decision\",\n    \"updatedInput\": {\n      \"field\": \"new value\"\n    }\n  }\n}\n```\n\n#### PostToolUse\nRuns after tool completes.\n\n**Use for:**\n- Auto-format code\n- Run linters\n- Validate outputs\n- Log operations\n\n**JSON Output:**\n```json\n{\n  \"decision\": \"block\" | undefined,\n  \"reason\": \"Why blocking\",\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"PostToolUse\",\n    \"additionalContext\": \"Extra info for Claude\"\n  }\n}\n```\n\n#### UserPromptSubmit\nRuns when user submits prompt.\n\n**Use for:**\n- Add context automatically\n- Validate prompts\n- Block sensitive prompts\n- Inject current time/date\n\n**JSON Output:**\n```json\n{\n  \"decision\": \"block\" | undefined,\n  \"reason\": \"Why blocking\",\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"UserPromptSubmit\",\n    \"additionalContext\": \"Extra context\"\n  }\n}\n```\n\n#### Stop / SubagentStop\nRuns when Claude/subagent finishes.\n\n**Use for:**\n- Verify tasks completed\n- Continue if work remains\n- Intelligent stoppage control\n\n**JSON Output:**\n```json\n{\n  \"decision\": \"block\" | undefined,\n  \"reason\": \"Why must continue\"\n}\n```\n\n#### SessionStart\nRuns when session starts.\n\n**Use for:**\n- Load environment variables\n- Set up development context\n- Install dependencies\n- Inject initial context\n\n**JSON Output:**\n```json\n{\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"SessionStart\",\n    \"additionalContext\": \"Initial context\"\n  }\n}\n```\n\n**Special:** Can persist environment variables:\n```bash\n#!/bin/bash\nif [ -n \"$CLAUDE_ENV_FILE\" ]; then\n  echo 'export NODE_ENV=production' >> \"$CLAUDE_ENV_FILE\"\nfi\n```\n\n#### SessionEnd\nRuns when session ends.\n\n**Use for:**\n- Cleanup tasks\n- Save session stats\n- Log session data\n\n### 4. Create Hook Script\n\nFor bash command hooks, create a script:\n\n**Template:**\n```bash\n#!/usr/bin/env bash\n\n# Read JSON input from stdin\nINPUT=$(cat)\n\n# Parse JSON (requires jq)\nTOOL_NAME=$(echo \"$INPUT\" | jq -r '.tool_name // empty')\nFILE_PATH=$(echo \"$INPUT\" | jq -r '.tool_input.file_path // empty')\n\n# Your validation logic here\nif [[ condition ]]; then\n  echo \"Error message\" >&2\n  exit 2  # Block operation\nfi\n\n# Success\nexit 0\n```\n\n**Python Template:**\n```python\n#!/usr/bin/env python3\nimport json\nimport sys\n\n# Read JSON input\ntry:\n    input_data = json.load(sys.stdin)\nexcept json.JSONDecodeError as e:\n    print(f\"Error: Invalid JSON: {e}\", file=sys.stderr)\n    sys.exit(1)\n\ntool_name = input_data.get(\"tool_name\", \"\")\ntool_input = input_data.get(\"tool_input\", {})\n\n# Your logic here\nif condition:\n    # Block with error\n    print(\"Error message\", file=sys.stderr)\n    sys.exit(2)\n\n# Or return JSON for control\noutput = {\n    \"decision\": \"approve\",\n    \"reason\": \"Auto-approved\"\n}\nprint(json.dumps(output))\nsys.exit(0)\n```\n\n### 5. Configure in settings.json\n\nAdd hook configuration:\n\n**Basic Hook:**\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/format.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**Multiple Hooks:**\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/format.sh\",\n            \"timeout\": 30\n          },\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/lint.sh\",\n            \"timeout\": 60\n          }\n        ]\n      }\n    ],\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/validate-bash.py\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**No Matcher (events without tools):**\n```json\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/add-context.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### 6. Hook Input Reference\n\nEach event receives JSON on stdin:\n\n**Common fields:**\n```json\n{\n  \"session_id\": \"abc123\",\n  \"transcript_path\": \"/path/to/transcript.jsonl\",\n  \"cwd\": \"/current/working/dir\",\n  \"permission_mode\": \"default\",\n  \"hook_event_name\": \"PostToolUse\"\n}\n```\n\n**PreToolUse/PostToolUse:**\n```json\n{\n  \"tool_name\": \"Write\",\n  \"tool_input\": {\n    \"file_path\": \"/path/to/file.txt\",\n    \"content\": \"file content\"\n  },\n  \"tool_response\": { /* PostToolUse only */\n    \"success\": true\n  }\n}\n```\n\n**UserPromptSubmit:**\n```json\n{\n  \"prompt\": \"User's submitted message\"\n}\n```\n\n**Stop/SubagentStop:**\n```json\n{\n  \"stop_hook_active\": false\n}\n```\n\n### 7. Exit Codes\n\n- **0**: Success\n  - stdout shown in verbose mode (Ctrl+O)\n  - For UserPromptSubmit/SessionStart: stdout added to context\n  - JSON parsed if present\n\n- **2**: Blocking error\n  - stderr shown to Claude\n  - Operation blocked (behavior varies by event)\n  - JSON in stdout ignored\n\n- **Other**: Non-blocking warning\n  - stderr shown in verbose mode\n  - Execution continues\n\n### 8. Test the Hook\n\n**Test script directly:**\n```bash\n# Create test input\necho '{\n  \"tool_name\": \"Write\",\n  \"tool_input\": {\n    \"file_path\": \"test.txt\",\n    \"content\": \"hello\"\n  }\n}' | .claude/hooks/your-hook.sh\n\n# Check exit code\necho $?\n```\n\n**Test in Claude Code:**\n```\n1. Add hook to settings.json\n2. Restart Claude Code\n3. Run /hooks to verify it's loaded\n4. Trigger the hook (e.g., write a file)\n5. Check verbose mode (Ctrl+O) for output\n```\n\n**Debug mode:**\n```bash\nclaude --debug\n# Shows hook execution details\n```\n\n### 9. Provide Configuration\n\nShow the complete configuration:\n\n```json\n{\n  \"hooks\": {\n    \"EventName\": [\n      {\n        \"matcher\": \"ToolPattern\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/script.sh\",\n            \"timeout\": 60\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Hook Examples\n\n### Example 1: Auto-Format Python Files\n\n**Hook script** (`.claude/hooks/format-python.sh`):\n```bash\n#!/usr/bin/env bash\nINPUT=$(cat)\n\n# Get file path\nFILE_PATH=$(echo \"$INPUT\" | jq -r '.tool_input.file_path // empty')\n\n# Only process .py files\nif [[ \"$FILE_PATH\" == *.py ]]; then\n  # Run black formatter\n  python -m black \"$FILE_PATH\" 2>&1\n\n  if [[ $? -eq 0 ]]; then\n    echo \"Formatted: $FILE_PATH\" >&2\n  fi\nfi\n\nexit 0\n```\n\n**Configuration:**\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/format-python.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Example 2: Validate Bash Commands\n\n**Hook script** (`.claude/hooks/validate-bash.py`):\n```python\n#!/usr/bin/env python3\nimport json\nimport sys\nimport re\n\n# Dangerous patterns\nDANGEROUS = [\n    (r'\\brm\\s+-rf\\s+/', 'Dangerous: rm -rf on root'),\n    (r'>\\s*/dev/sd[a-z]', 'Dangerous: writing to block device'),\n    (r'\\bcurl\\s+.*\\|\\s*bash', 'Dangerous: piping curl to bash'),\n]\n\ntry:\n    data = json.load(sys.stdin)\nexcept:\n    sys.exit(1)\n\nif data.get('tool_name') != 'Bash':\n    sys.exit(0)\n\ncommand = data.get('tool_input', {}).get('command', '')\n\n# Check for dangerous patterns\nfor pattern, message in DANGEROUS:\n    if re.search(pattern, command):\n        print(f\"⚠️  {message}\", file=sys.stderr)\n        print(f\"Command: {command}\", file=sys.stderr)\n        sys.exit(2)  # Block\n\nsys.exit(0)  # Allow\n```\n\n**Configuration:**\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/validate-bash.py\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Example 3: Add Timestamp to Prompts\n\n**Hook script** (`.claude/hooks/add-timestamp.sh`):\n```bash\n#!/usr/bin/env bash\n\n# Output current timestamp\necho \"Current time: $(date '+%Y-%m-%d %H:%M:%S %Z')\"\n\nexit 0\n```\n\n**Configuration:**\n```json\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/add-timestamp.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Example 4: Auto-Approve Documentation Reads\n\n**Hook script** (`.claude/hooks/auto-approve-docs.py`):\n```python\n#!/usr/bin/env python3\nimport json\nimport sys\n\ndata = json.load(sys.stdin)\n\nif data.get('tool_name') != 'Read':\n    sys.exit(0)\n\nfile_path = data.get('tool_input', {}).get('file_path', '')\n\n# Auto-approve docs\nif any(file_path.endswith(ext) for ext in ['.md', '.txt', '.json', '.yaml']):\n    output = {\n        \"hookSpecificOutput\": {\n            \"hookEventName\": \"PreToolUse\",\n            \"permissionDecision\": \"allow\",\n            \"permissionDecisionReason\": \"Documentation file auto-approved\"\n        },\n        \"suppressOutput\": True\n    }\n    print(json.dumps(output))\n    sys.exit(0)\n\nsys.exit(0)\n```\n\n**Configuration:**\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Read\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/auto-approve-docs.py\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Example 5: Prevent Sensitive File Access\n\n**Hook script** (`.claude/hooks/block-secrets.sh`):\n```bash\n#!/usr/bin/env bash\nINPUT=$(cat)\n\nFILE_PATH=$(echo \"$INPUT\" | jq -r '.tool_input.file_path // empty')\n\n# Block sensitive files\nif [[ \"$FILE_PATH\" =~ \\.env ||\n      \"$FILE_PATH\" =~ secrets/ ||\n      \"$FILE_PATH\" =~ \\.aws/ ]]; then\n  echo \"⛔ Access to sensitive file blocked: $FILE_PATH\" >&2\n  exit 2\nfi\n\nexit 0\n```\n\n**Configuration:**\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Read|Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/block-secrets.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Example 6: Intelligent Stop Hook (Prompt-based)\n\n**Configuration:**\n```json\n{\n  \"hooks\": {\n    \"Stop\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"Evaluate whether Claude should stop. Context: $ARGUMENTS\\n\\nCheck if:\\n1. All tasks are complete\\n2. Tests are passing\\n3. No errors need addressing\\n\\nRespond with JSON: {\\\"decision\\\": \\\"approve\\\" or \\\"block\\\", \\\"reason\\\": \\\"explanation\\\"}\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Example 7: Session Setup Hook\n\n**Hook script** (`.claude/hooks/session-setup.sh`):\n```bash\n#!/usr/bin/env bash\n\n# Set up environment for session\nif [ -n \"$CLAUDE_ENV_FILE\" ]; then\n  # Load nvm\n  source ~/.nvm/nvm.sh\n  nvm use 20\n\n  # Capture environment changes\n  export -p >> \"$CLAUDE_ENV_FILE\"\n\n  # Add custom variables\n  echo 'export NODE_ENV=development' >> \"$CLAUDE_ENV_FILE\"\nfi\n\n# Add context\necho \"Development environment initialized\"\necho \"Node version: $(node --version)\"\n\nexit 0\n```\n\n**Configuration:**\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"matcher\": \"startup\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/session-setup.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Matcher Patterns\n\n**Exact match:**\n```json\n\"matcher\": \"Write\"\n```\n\n**Multiple tools (regex):**\n```json\n\"matcher\": \"Write|Edit|NotebookEdit\"\n```\n\n**All tools:**\n```json\n\"matcher\": \"*\"\n```\nOr:\n```json\n\"matcher\": \"\"\n```\n\n**MCP tools:**\n```json\n\"matcher\": \"mcp__github__.*\"\n\"matcher\": \"mcp__.*__write.*\"\n```\n\n**Event-specific matchers:**\n\nNotification:\n```json\n\"matcher\": \"permission_prompt\"\n\"matcher\": \"idle_prompt\"\n```\n\nPreCompact:\n```json\n\"matcher\": \"manual\"\n\"matcher\": \"auto\"\n```\n\nSessionStart:\n```json\n\"matcher\": \"startup\"\n\"matcher\": \"resume\"\n\"matcher\": \"clear\"\n```\n\n## Environment Variables\n\nAvailable in hook scripts:\n\n- `$CLAUDE_PROJECT_DIR` - Absolute path to project root\n- `$CLAUDE_CODE_REMOTE` - \"true\" if remote/web, empty if local\n- `$CLAUDE_ENV_FILE` - (SessionStart only) File to persist env vars\n- Standard environment variables\n\n## Best Practices\n\n### DO:\n✅ Keep hooks fast (<100ms recommended)\n✅ Provide clear error messages\n✅ Use appropriate exit codes\n✅ Quote variables in bash: `\"$VAR\"`\n✅ Validate inputs before processing\n✅ Test thoroughly before deploying\n✅ Use `$CLAUDE_PROJECT_DIR` for portability\n✅ Document what your hook does\n\n### DON'T:\n❌ Run slow operations (full test suites)\n❌ Block legitimate operations unnecessarily\n❌ Use hooks for everything (be selective)\n❌ Forget to handle errors\n❌ Skip input validation\n❌ Hardcode absolute paths\n❌ Leave debug output in production\n\n## Security Considerations\n\n**⚠️ USE AT YOUR OWN RISK**\n\nHooks execute arbitrary commands:\n- Can modify/delete any files\n- Can access sensitive data\n- Can cause data loss\n- Anthropic provides no warranty\n\n**Best practices:**\n- Validate and sanitize inputs\n- Quote all variables\n- Block path traversal (`..`)\n- Use absolute paths\n- Skip sensitive files\n- Test in safe environment first\n\n## Troubleshooting\n\n### Hook Not Running\n\n**Check:**\n1. Hook is in `settings.json` correctly\n2. Matcher pattern is correct (case-sensitive)\n3. Script has execute permissions: `chmod +x script.sh`\n4. Script shebang is correct: `#!/usr/bin/env bash`\n5. Restart Claude Code after config changes\n\n**Debug:**\n```bash\n# Run with debug mode\nclaude --debug\n\n# Check hook execution in output\n# Shows: \"Executing hooks for PostToolUse:Write\"\n```\n\n### Hook Errors\n\n**Check:**\n1. Script runs standalone: `echo '{}' | ./script.sh`\n2. Exit code is correct: `echo $?`\n3. JSON output is valid: `./script.sh | jq .`\n4. Timeout is sufficient (default: 60s)\n\n**View errors:**\n- Verbose mode: Ctrl+O\n- Debug mode: `claude --debug`\n- Check stderr output\n\n### Permissions Issues\n\n**Check:**\n```bash\n# Make script executable\nchmod +x .claude/hooks/script.sh\n\n# Verify permissions\nls -la .claude/hooks/\n```\n\n### JSON Parse Errors\n\n**Validate JSON:**\n```bash\n# Test JSON output\necho '{}' | ./script.sh | jq .\n\n# Common issues:\n# - Missing quotes\n# - Trailing commas\n# - Single quotes instead of double\n```\n\n## Resources\n\n- **Official Hook Docs:** `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_hooks.md`\n- **Hook Guide:** `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_hooks-guide.md`\n- **Settings Reference:** `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_settings.md`\n- **Project Guide:** `.claude/docs/creating-components.md`\n",
        "templates/.claude/skills/anthropic/claude-mcp-expert/skill.md": "---\nname: claude-mcp-expert\ndescription: Expert on Model Context Protocol (MCP) integration, MCP servers, installation, configuration, and authentication. Triggers when user mentions MCP, MCP servers, installing MCP, connecting tools, MCP resources, MCP prompts, or remote/local MCP servers.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Claude Code MCP Expert\n\n## Purpose\n\nProvide expert guidance on integrating MCP (Model Context Protocol) servers with Claude Code, including installation, configuration, authentication, and troubleshooting. Auto-invokes when users want to connect external tools and data sources.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **MCP** - \"MCP\", \"Model Context Protocol\", \"MCP server\"\n- **Installation** - \"install MCP\", \"add MCP server\", \"connect MCP\"\n- **Server types** - \"HTTP server\", \"SSE server\", \"stdio server\", \"remote MCP\"\n- **Configuration** - \".mcp.json\", \"MCP configuration\", \"MCP settings\"\n- **Authentication** - \"MCP auth\", \"OAuth\", \"API key for MCP\"\n- **Popular services** - \"Sentry MCP\", \"GitHub MCP\", \"Notion MCP\", \"Jira MCP\"\n- **MCP resources** - \"MCP resources\", \"MCP prompts\", \"MCP tools\"\n\n## Knowledge Base\n\n- Official docs: `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_mcp.md`\n- MCP website: https://modelcontextprotocol.io/\n- Server list: MCP documentation has comprehensive server list\n\n## What is MCP?\n\n**Model Context Protocol (MCP)** is an open-source standard that allows Claude Code to connect to:\n- External tools and services\n- Databases (PostgreSQL, MongoDB, etc.)\n- APIs (GitHub, Jira, Slack, etc.)\n- File systems and data sources\n- Custom integrations\n\n**Benefits:**\n- Access hundreds of integrations\n- Standardized protocol\n- Community-built servers\n- Easy to install and configure\n\n## Process\n\n### 1. Identify Need\n\nAsk the user:\n\n```\nWhat would you like to connect with MCP?\n\n**Popular integrations:**\n- Development: Sentry, GitHub, Socket\n- Project Management: Jira, Linear, Notion, Asana\n- Databases: PostgreSQL, MongoDB, MySQL\n- Communication: Slack, Gmail\n- Cloud: AWS, Google Cloud\n- File Systems: Local files, Google Drive\n\nOr describe what you want to do, and I'll recommend an MCP server.\n```\n\n### 2. Choose Server Type\n\nExplain the three types:\n\n**HTTP Server (recommended for remote services):**\n```bash\nclaude mcp add --transport http server-name https://mcp.service.com/mcp\n```\n- Most common for cloud services\n- Example: Sentry, GitHub, Notion\n\n**SSE Server (Server-Sent Events):**\n```bash\nclaude mcp add --transport sse server-name https://mcp.service.com/sse\n```\n- For streaming services\n- Example: Asana, Atlassian\n\n**stdio Server (local/npm packages):**\n```bash\nclaude mcp add --transport stdio server-name -- npx -y package-name\n```\n- For local tools or npm packages\n- Example: Filesystem, custom scripts\n\n### 3. Installation\n\n#### Quick Install (Interactive)\n\n```bash\n# Recommended: Use /mcp command in Claude Code\n/mcp\n\n# Then select:\n# - Add new server\n# - Follow prompts\n# - Authenticate if needed\n```\n\n#### Command Line Install\n\n**HTTP Server:**\n```bash\nclaude mcp add --transport http sentry https://mcp.sentry.dev/mcp\n```\n\n**SSE Server:**\n```bash\nclaude mcp add --transport sse asana https://mcp.asana.com/sse\n```\n\n**stdio Server with environment variables:**\n```bash\nclaude mcp add --transport stdio github \\\n  --env GITHUB_TOKEN=your_token \\\n  -- npx -y @modelcontextprotocol/server-github\n```\n\n### 4. Configuration Scopes\n\nExplain where MCP servers are stored:\n\n#### User Scope (Global)\n**Location:** `~/.claude/.mcp.json`\n\n**Use for:**\n- Personal integrations\n- Services you use across all projects\n- Personal API keys\n\n**Install:**\n```bash\nclaude mcp add --transport http sentry https://mcp.sentry.dev/mcp --user\n```\n\n#### Project Scope (Team)\n**Location:** `.claude/.mcp.json`\n\n**Use for:**\n- Team-shared integrations\n- Project-specific services\n- Committed to git\n\n**Install:**\n```bash\nclaude mcp add --transport http linear https://mcp.linear.app/mcp --project\n```\n\n#### Local Scope (Personal + Project)\n**Location:** `.claude/.mcp.local.json`\n\n**Use for:**\n- Personal overrides in project\n- Testing configurations\n- Not committed to git\n\n**Install:**\n```bash\nclaude mcp add --transport http notion https://mcp.notion.com/mcp --local\n```\n\n### 5. Authentication\n\nMost MCP servers require authentication:\n\n#### OAuth (Recommended)\n```bash\n# Install server\nclaude mcp add --transport http github https://mcp.github.com/mcp\n\n# Authenticate via /mcp command\n/mcp\n# Select: Authenticate with server\n# Browser opens, authorize app\n```\n\n#### API Key (via environment)\n```bash\nclaude mcp add --transport stdio server-name \\\n  --env API_KEY=your_api_key \\\n  --env TEAM_ID=your_team_id \\\n  -- npx -y package-name\n```\n\n#### From settings.json\n```json\n{\n  \"env\": {\n    \"GITHUB_TOKEN\": \"ghp_xxxxx\",\n    \"SENTRY_AUTH_TOKEN\": \"sntrys_xxxxx\"\n  }\n}\n```\n\n### 6. Verify Installation\n\nAfter installation:\n\n```bash\n# Check MCP servers\n/mcp\n\n# Lists:\n# - Active servers\n# - Available tools\n# - Authentication status\n# - Resources and prompts\n```\n\n## Popular MCP Servers\n\n### Development Tools\n\n**Sentry** (Error monitoring):\n```bash\nclaude mcp add --transport http sentry https://mcp.sentry.dev/mcp\n```\nUse for: \"Check Sentry for errors in the last 24 hours\"\n\n**GitHub** (Code hosting):\n```bash\nclaude mcp add --transport http github https://api.github.com/mcp\n```\nUse for: \"Create a PR for this feature\", \"Review open issues\"\n\n**Socket** (Security analysis):\n```bash\nclaude mcp add --transport http socket https://mcp.socket.dev/\n```\nUse for: \"Analyze dependencies for security vulnerabilities\"\n\n### Project Management\n\n**Jira** (via Atlassian):\n```bash\nclaude mcp add --transport sse atlassian https://mcp.atlassian.com/v1/sse\n```\nUse for: \"Implement feature from JIRA-123\", \"Update ticket status\"\n\n**Linear**:\n```bash\nclaude mcp add --transport http linear https://mcp.linear.app/mcp\n```\nUse for: \"Create issue\", \"List my assigned tasks\"\n\n**Notion**:\n```bash\nclaude mcp add --transport http notion https://mcp.notion.com/mcp\n```\nUse for: \"Read project docs\", \"Update status page\"\n\n**Asana**:\n```bash\nclaude mcp add --transport sse asana https://mcp.asana.com/sse\n```\nUse for: \"Create task\", \"Check project timeline\"\n\n### Databases\n\n**PostgreSQL** (local):\n```bash\nclaude mcp add --transport stdio postgres \\\n  --env POSTGRES_CONNECTION_STRING=\"postgresql://user:pass@localhost:5432/db\" \\\n  -- npx -y @modelcontextprotocol/server-postgres\n```\n\n**MongoDB** (local):\n```bash\nclaude mcp add --transport stdio mongodb \\\n  --env MONGODB_URI=\"mongodb://localhost:27017/mydb\" \\\n  -- npx -y @modelcontextprotocol/server-mongodb\n```\n\n### File Systems\n\n**Filesystem** (local files):\n```bash\nclaude mcp add --transport stdio filesystem \\\n  --env ALLOWED_DIRECTORIES=/path/to/dir1:/path/to/dir2 \\\n  -- npx -y @modelcontextprotocol/server-filesystem\n```\n\n**Google Drive**:\n```bash\nclaude mcp add --transport stdio google-drive \\\n  -- npx -y @modelcontextprotocol/server-gdrive\n```\n\n### Communication\n\n**Slack**:\n```bash\nclaude mcp add --transport stdio slack \\\n  --env SLACK_BOT_TOKEN=xoxb-your-token \\\n  --env SLACK_TEAM_ID=T1234567 \\\n  -- npx -y @modelcontextprotocol/server-slack\n```\n\n**Gmail**:\n```bash\nclaude mcp add --transport stdio gmail \\\n  -- npx -y @modelcontextprotocol/server-gmail\n```\n\n## MCP Configuration Files\n\n### .mcp.json Structure\n\n```json\n{\n  \"mcpServers\": {\n    \"server-name\": {\n      \"transport\": \"http\" | \"sse\" | \"stdio\",\n      \"url\": \"https://mcp.service.com/mcp\",  // For http/sse\n      \"command\": \"npx\",  // For stdio\n      \"args\": [\"-y\", \"package-name\"],  // For stdio\n      \"env\": {\n        \"API_KEY\": \"your_key\",\n        \"TEAM_ID\": \"your_id\"\n      }\n    }\n  }\n}\n```\n\n### Example: Multiple Servers\n\n```json\n{\n  \"mcpServers\": {\n    \"sentry\": {\n      \"transport\": \"http\",\n      \"url\": \"https://mcp.sentry.dev/mcp\"\n    },\n    \"github\": {\n      \"transport\": \"http\",\n      \"url\": \"https://api.github.com/mcp\"\n    },\n    \"postgres\": {\n      \"transport\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": {\n        \"POSTGRES_CONNECTION_STRING\": \"postgresql://localhost:5432/mydb\"\n      }\n    }\n  }\n}\n```\n\n### Environment Variable Expansion\n\nUse environment variables in config:\n\n```json\n{\n  \"mcpServers\": {\n    \"github\": {\n      \"transport\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_TOKEN\": \"${GITHUB_TOKEN}\"\n      }\n    }\n  }\n}\n```\n\nThen set in shell or settings.json:\n```json\n{\n  \"env\": {\n    \"GITHUB_TOKEN\": \"ghp_xxxxx\"\n  }\n}\n```\n\n## Using MCP Features\n\n### MCP Tools\n\nOnce server is connected, use tools:\n\n```\n# GitHub MCP\n\"Create a PR for this feature\"\n\"List open issues assigned to me\"\n\"Add a comment to PR #123\"\n\n# Sentry MCP\n\"Check errors in the last 24 hours\"\n\"Show details for error SENTRY-123\"\n\n# PostgreSQL MCP\n\"Find 10 random users in the database\"\n\"Query users table for active accounts\"\n\n# Notion MCP\n\"Read the project roadmap page\"\n\"Update the status page with this info\"\n```\n\n### MCP Resources\n\nMCP resources are files/data the server provides:\n\n**Reference resources:**\n```\n@mcp://server-name/resource-name\n```\n\n**Example:**\n```\nReview @mcp://github/README.md and suggest improvements\n```\n\n### MCP Prompts as Slash Commands\n\nMCP prompts become slash commands:\n\n**Format:**\n```\n/mcp__<server-name>__<prompt-name> [args]\n```\n\n**Example:**\n```\n/mcp__github__create_issue \"Bug in login\" high\n/mcp__jira__update_status PROJ-123 \"In Progress\"\n```\n\n## Managing MCP Servers\n\n### Via /mcp Command\n\n```bash\n# Interactive management\n/mcp\n\n# Options:\n# - View all servers\n# - Add new server\n# - Remove server\n# - Authenticate\n# - Clear auth tokens\n# - View tools/prompts/resources\n```\n\n### Via Command Line\n\n```bash\n# List servers\nclaude mcp list\n\n# Add server\nclaude mcp add --transport http name url\n\n# Remove server\nclaude mcp remove name\n\n# Test connection\nclaude mcp test name\n```\n\n### Via Configuration\n\nEdit `.mcp.json` directly:\n\n```json\n{\n  \"mcpServers\": {\n    \"new-server\": {\n      \"transport\": \"http\",\n      \"url\": \"https://mcp.example.com/mcp\"\n    }\n  }\n}\n```\n\n## Enterprise MCP Configuration\n\n### Managed MCP Servers\n\n**Location (enterprise-controlled):**\n- macOS: `/Library/Application Support/ClaudeCode/managed-mcp.json`\n- Linux/WSL: `/etc/claude-code/managed-mcp.json`\n- Windows: `C:\\ProgramData\\ClaudeCode\\managed-mcp.json`\n\n**Features:**\n- Deployed by IT/DevOps\n- Always available to users\n- Users cannot remove\n- Take precedence over user servers\n\n### Allowlist/Denylist\n\n**In managed-settings.json:**\n\n```json\n{\n  \"allowedMcpServers\": [\n    {\"serverName\": \"github\"},\n    {\"serverName\": \"sentry\"}\n  ],\n  \"deniedMcpServers\": [\n    {\"serverName\": \"filesystem\"}\n  ]\n}\n```\n\n**Behavior:**\n- `allowedMcpServers`: Only these servers can be installed\n- `deniedMcpServers`: These servers are blocked (takes precedence)\n- Undefined `allowedMcpServers`: No restrictions\n- Empty array `allowedMcpServers`: Complete lockdown\n\n## Plugin-Provided MCP Servers\n\nPlugins can bundle MCP servers:\n\n**Automatic installation:**\n1. Install plugin\n2. MCP servers from plugin are available\n3. No manual configuration needed\n\n**Example:**\n```bash\n# Install plugin that includes MCP servers\n/plugin install company-tools@internal\n\n# MCP servers from plugin are now active\n```\n\n## Import from Claude Desktop\n\nIf you have MCP servers configured in Claude Desktop:\n\n```bash\n# Import configuration\nclaude mcp import-from-desktop\n\n# Or manually copy from:\n# ~/Library/Application Support/Claude/claude_desktop_config.json\n```\n\n## MCP Output Limits\n\n**Warning thresholds:**\n- 10,000 tokens: Warning shown\n- 25,000 tokens: Default maximum (configurable)\n\n**Configure limit:**\n```bash\nexport MAX_MCP_OUTPUT_TOKENS=50000\n```\n\nOr in settings.json:\n```json\n{\n  \"env\": {\n    \"MAX_MCP_OUTPUT_TOKENS\": \"50000\"\n  }\n}\n```\n\n## Security Considerations\n\n⚠️ **Use MCP servers at your own risk:**\n- Anthropic has not verified all servers\n- Trust servers you install\n- Be careful with servers fetching untrusted content\n- Risk of prompt injection\n- Servers have access to data you authorize\n\n**Best practices:**\n- Only install trusted servers\n- Review server permissions\n- Use OAuth when available\n- Store API keys securely\n- Audit server access regularly\n- Use project scope for team servers\n- Use user scope for personal servers\n\n## Troubleshooting\n\n### Server Not Connecting\n\n**Check:**\n1. Server URL is correct\n2. Authentication is complete\n3. Network connectivity\n4. Server is running (for stdio)\n\n**Debug:**\n```bash\n# Run with debug mode\nclaude --debug\n\n# Look for MCP connection errors\n# Check: \"MCP server 'name' connected\"\n```\n\n### Authentication Failing\n\n**Solutions:**\n1. Re-authenticate: `/mcp` → Authenticate\n2. Check API key is valid\n3. Verify OAuth tokens not expired\n4. Clear and re-auth: `/mcp` → Clear auth\n\n### Tools Not Available\n\n**Check:**\n1. Server is connected: `/mcp`\n2. Authentication complete\n3. Restart Claude Code\n4. Server provides expected tools\n\n**Verify tools:**\n```bash\n/mcp\n# Select server\n# View \"Available tools\"\n```\n\n### stdio Server Errors\n\n**Check:**\n1. Command is installed: `which npx`\n2. Package exists: `npx -y package-name --version`\n3. Environment variables set\n4. Permissions to execute\n\n**Test manually:**\n```bash\n# Run stdio command directly\nnpx -y @modelcontextprotocol/server-github\n# Should start server, show JSON-RPC messages\n```\n\n### Configuration Not Loading\n\n**Check:**\n1. JSON syntax valid: `jq . < .mcp.json`\n2. File location correct\n3. Restart Claude Code\n4. Check scope (user/project/local)\n\n**Validate:**\n```bash\n# Validate JSON\ncat .claude/.mcp.json | jq .\n\n# Check active config\n/mcp\n```\n\n### Permission Denied Errors\n\n**Check:**\n1. File permissions: `chmod 644 .mcp.json`\n2. Allowed in settings (enterprise)\n3. Not in denylist\n4. OAuth scope sufficient\n\n## Best Practices\n\n### DO:\n✅ Use project scope for team integrations\n✅ Use user scope for personal tools\n✅ Authenticate with OAuth when available\n✅ Store sensitive keys in settings.json (not .mcp.json)\n✅ Test servers before sharing with team\n✅ Document required authentication for team\n✅ Use environment variables for secrets\n✅ Review server permissions regularly\n\n### DON'T:\n❌ Commit API keys to git\n❌ Install untrusted servers\n❌ Share OAuth tokens\n❌ Use excessive output limits\n❌ Bypass enterprise restrictions\n❌ Forget to authenticate after install\n❌ Mix personal/team servers in wrong scope\n\n## Example Workflows\n\n### Workflow 1: Issue to PR\n\n**Setup:**\n```bash\nclaude mcp add --transport http github https://api.github.com/mcp --project\nclaude mcp add --transport sse atlassian https://mcp.atlassian.com/v1/sse --project\n```\n\n**Usage:**\n```\n\"Read JIRA ticket ENG-123, implement the feature, and create a PR on GitHub\"\n```\n\n### Workflow 2: Error Investigation\n\n**Setup:**\n```bash\nclaude mcp add --transport http sentry https://mcp.sentry.dev/mcp --user\n```\n\n**Usage:**\n```\n\"Check Sentry for errors in the last hour and fix them\"\n```\n\n### Workflow 3: Database Query to Email\n\n**Setup:**\n```bash\nclaude mcp add --transport stdio postgres --env POSTGRES_CONNECTION_STRING=... -- npx -y @modelcontextprotocol/server-postgres\nclaude mcp add --transport stdio gmail -- npx -y @modelcontextprotocol/server-gmail\n```\n\n**Usage:**\n```\n\"Find 10 beta users from database and draft Gmail invites to feedback session\"\n```\n\n## Use Claude Code as MCP Server\n\nClaude Code can act as an MCP server for other applications:\n\n```bash\n# Expose Claude Code via MCP\nclaude mcp serve --port 8080\n\n# Other apps can connect to:\n# http://localhost:8080/mcp\n```\n\n**Capabilities exposed:**\n- Claude Code tools (Read, Write, Edit, etc.)\n- Project skills\n- Slash commands (as MCP prompts)\n\n## Resources\n\n- **Official MCP Docs:** `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_mcp.md`\n- **MCP Website:** https://modelcontextprotocol.io/\n- **MCP Servers List:** https://github.com/modelcontextprotocol/servers\n- **Anthropic MCP Guide:** https://docs.anthropic.com/en/docs/agents-and-tools/mcp\n",
        "templates/.claude/skills/anthropic/claude-settings-expert/skill.md": "---\nname: claude-settings-expert\ndescription: Expert on Claude Code settings.json configuration, permissions, sandbox, environment variables, and settings hierarchy. Triggers when user mentions settings.json, permissions, allow rules, deny rules, sandbox, hooks configuration, or settings precedence.\nallowed-tools: Read, Write, Edit, Grep, Glob\nmodel: sonnet\n---\n\n# Claude Code Settings Expert\n\n## Purpose\n\nProvide expert guidance on configuring Claude Code through settings.json, including permissions, hooks, sandbox configuration, environment variables, and settings hierarchy. Auto-invokes when users need help with settings.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Settings files** - \"settings.json\", \"configure\", \"configuration\"\n- **Permissions** - \"allow\", \"deny\", \"ask\", \"permissions\", \"permission rules\"\n- **Hooks** - \"hooks configuration\", \"PreToolUse hooks\", \"PostToolUse hooks\"\n- **Sandbox** - \"sandbox settings\", \"sandboxing\", \"filesystem isolation\"\n- **Environment** - \"environment variables\", \"env\", \"$CLAUDE_PROJECT_DIR\"\n- **Hierarchy** - \"settings precedence\", \"user settings\", \"project settings\"\n\n## Knowledge Base\n\n- Official docs: `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_settings.md`\n- IAM docs: Look for IAM/permissions documentation\n- Hooks docs: `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_hooks.md`\n\n## Settings File Locations\n\n### Hierarchy (highest to lowest precedence)\n\n1. **Enterprise managed policies**\n   - macOS: `/Library/Application Support/ClaudeCode/managed-settings.json`\n   - Linux/WSL: `/etc/claude-code/managed-settings.json`\n   - Windows: `C:\\ProgramData\\ClaudeCode\\managed-settings.json`\n   - Cannot be overridden by users\n\n2. **Command line arguments**\n   - Temporary overrides for specific session\n   - Example: `claude --dangerously-skip-permissions`\n\n3. **Local project settings**\n   - `.claude/settings.local.json`\n   - Personal project settings (not committed)\n   - Git-ignored automatically\n\n4. **Shared project settings**\n   - `.claude/settings.json`\n   - Team-shared settings in source control\n   - Committed to repository\n\n5. **User settings**\n   - `~/.claude/settings.json`\n   - Personal global settings\n   - Apply to all projects\n\n### When to Use Each\n\n**User settings** (`~/.claude/settings.json`):\n- Personal preferences across all projects\n- Personal API keys\n- Personal slash commands\n- Personal output style\n\n**Project settings** (`.claude/settings.json`):\n- Team permissions\n- Project-specific hooks\n- Required plugins/marketplaces\n- Team workflow configuration\n\n**Local project** (`.claude/settings.local.json`):\n- Personal project overrides\n- Experimental settings\n- Local-only preferences\n- Not shared with team\n\n## Process\n\n### 1. Identify Need\n\nAsk clarifying questions:\n\n```\nWhat would you like to configure?\n\n1. **Permissions** - Allow/deny specific tools or commands\n2. **Hooks** - Automate tool execution workflows\n3. **Sandbox** - Enable filesystem/network isolation\n4. **Environment** - Set environment variables\n5. **Plugins** - Configure plugins and marketplaces\n6. **Model** - Override default model\n7. **Other** - Company announcements, cleanup, etc.\n```\n\n### 2. Determine Scope\n\nAsk about scope:\n\n```\nWhere should this configuration apply?\n\n- **User-level** (`~/.claude/settings.json`) - All your projects\n- **Project-level** (`.claude/settings.json`) - This project, shared with team\n- **Local** (`.claude/settings.local.json`) - This project, just you\n```\n\n### 3. Build Configuration\n\nBased on needs, construct the appropriate JSON:\n\n## Permission Configuration\n\n### Structure\n\n```json\n{\n  \"permissions\": {\n    \"allow\": [\"permission-rule\"],\n    \"ask\": [\"permission-rule\"],\n    \"deny\": [\"permission-rule\"],\n    \"additionalDirectories\": [\"../path\"],\n    \"defaultMode\": \"default\" | \"plan\" | \"acceptEdits\" | \"bypassPermissions\",\n    \"disableBypassPermissionsMode\": \"disable\"\n  }\n}\n```\n\n### Permission Rules\n\n**Tool-specific:**\n```json\n{\n  \"permissions\": {\n    \"allow\": [\n      \"Read(~/.zshrc)\",\n      \"Bash(git diff:*)\",\n      \"Bash(npm run lint:*)\",\n      \"Bash(npm run test:*)\"\n    ],\n    \"deny\": [\n      \"Read(./.env)\",\n      \"Read(./.env.*)\",\n      \"Read(./secrets/**)\",\n      \"Bash(curl:*)\",\n      \"WebFetch\",\n      \"WebSearch\"\n    ]\n  }\n}\n```\n\n**Bash permissions:**\n- Use prefix matching (not regex)\n- `Bash(git:*)` - All git commands\n- `Bash(git diff:*)` - Only git diff commands\n- Can be bypassed (not for security, use sandbox for that)\n\n**Read/Write permissions:**\n- `Read(./secrets/**)` - Recursive pattern\n- `Read(./.env)` - Specific file\n- `Write(./dist/**)` - Allow writes to dist\n\n**Permission modes:**\n- `default` - Ask for permission\n- `plan` - Plan mode (no execution without approval)\n- `acceptEdits` - Auto-approve edits only\n- `bypassPermissions` - Approve all (dangerous)\n\n### Example: Secure Development\n\n```json\n{\n  \"permissions\": {\n    \"allow\": [\n      \"Bash(git status:*)\",\n      \"Bash(git diff:*)\",\n      \"Bash(git log:*)\",\n      \"Bash(npm run lint:*)\",\n      \"Bash(npm run test:*)\",\n      \"Read(~/.gitconfig)\"\n    ],\n    \"deny\": [\n      \"Read(./.env)\",\n      \"Read(./.env.*)\",\n      \"Read(./secrets/**)\",\n      \"Read(./config/credentials.json)\",\n      \"Bash(curl:*)\",\n      \"Bash(wget:*)\",\n      \"Bash(rm -rf:*)\",\n      \"WebFetch\",\n      \"WebSearch\"\n    ],\n    \"defaultMode\": \"default\"\n  }\n}\n```\n\n## Hooks Configuration\n\n### Structure\n\n```json\n{\n  \"hooks\": {\n    \"EventName\": [\n      {\n        \"matcher\": \"ToolPattern\",\n        \"hooks\": [\n          {\n            \"type\": \"command\" | \"prompt\",\n            \"command\": \"bash command\",\n            \"prompt\": \"LLM prompt with $ARGUMENTS\",\n            \"timeout\": 60\n          }\n        ]\n      }\n    ]\n  },\n  \"disableAllHooks\": false\n}\n```\n\n### Example: Auto-Format and Validate\n\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/format.sh\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/validate-bash.py\"\n          }\n        ]\n      }\n    ],\n    \"UserPromptSubmit\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/add-context.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n## Sandbox Configuration\n\n### Structure\n\n```json\n{\n  \"sandbox\": {\n    \"enabled\": true | false,\n    \"autoAllowBashIfSandboxed\": true | false,\n    \"excludedCommands\": [\"git\", \"docker\"],\n    \"allowUnsandboxedCommands\": true | false,\n    \"network\": {\n      \"allowUnixSockets\": [\"~/.ssh/agent-socket\"],\n      \"allowLocalBinding\": true | false,\n      \"httpProxyPort\": 8080,\n      \"socksProxyPort\": 8081\n    },\n    \"enableWeakerNestedSandbox\": false\n  }\n}\n```\n\n### Example: Strict Sandbox\n\n```json\n{\n  \"sandbox\": {\n    \"enabled\": true,\n    \"autoAllowBashIfSandboxed\": true,\n    \"excludedCommands\": [\"docker\"],\n    \"allowUnsandboxedCommands\": false,\n    \"network\": {\n      \"allowUnixSockets\": [\"/var/run/docker.sock\"],\n      \"allowLocalBinding\": true\n    }\n  },\n  \"permissions\": {\n    \"deny\": [\n      \"Read(.envrc)\",\n      \"Read(~/.aws/**)\",\n      \"Read(./secrets/**)\"\n    ]\n  }\n}\n```\n\n**Notes:**\n- Filesystem access via Read/Write/Edit deny rules\n- Network access via WebFetch allow/deny rules\n- macOS/Linux only (not Windows)\n\n## Environment Variables\n\n### Structure\n\n```json\n{\n  \"env\": {\n    \"KEY\": \"value\",\n    \"NODE_ENV\": \"production\",\n    \"API_URL\": \"https://api.example.com\"\n  }\n}\n```\n\n### Common Variables\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_MODEL\": \"claude-sonnet-4-5-20250929\",\n    \"DISABLE_TELEMETRY\": \"1\",\n    \"DISABLE_AUTOUPDATER\": \"1\",\n    \"DISABLE_PROMPT_CACHING\": \"0\",\n    \"MAX_THINKING_TOKENS\": \"10000\",\n    \"BASH_DEFAULT_TIMEOUT_MS\": \"120000\"\n  }\n}\n```\n\n## Plugin Configuration\n\n### Structure\n\n```json\n{\n  \"enabledPlugins\": {\n    \"plugin-name@marketplace-name\": true | false\n  },\n  \"extraKnownMarketplaces\": {\n    \"marketplace-name\": {\n      \"source\": {\n        \"source\": \"github\" | \"git\" | \"directory\",\n        \"repo\": \"org/repo\",\n        \"url\": \"https://git.example.com/repo.git\",\n        \"path\": \"/local/path\"\n      }\n    }\n  }\n}\n```\n\n### Example: Team Plugins\n\n```json\n{\n  \"enabledPlugins\": {\n    \"code-formatter@company-tools\": true,\n    \"deployment@company-tools\": true,\n    \"security-scanner@company-tools\": false\n  },\n  \"extraKnownMarketplaces\": {\n    \"company-tools\": {\n      \"source\": {\n        \"source\": \"github\",\n        \"repo\": \"company/claude-plugins\"\n      }\n    }\n  }\n}\n```\n\n## Other Settings\n\n### Model Override\n\n```json\n{\n  \"model\": \"claude-sonnet-4-5-20250929\"\n}\n```\n\n### Cleanup Period\n\n```json\n{\n  \"cleanupPeriodDays\": 20\n}\n```\n\n### Company Announcements\n\n```json\n{\n  \"companyAnnouncements\": [\n    \"Welcome to Acme Corp! Review code guidelines at docs.acme.com\",\n    \"Reminder: Code reviews required for all PRs\"\n  ]\n}\n```\n\n### Force Login Method\n\n```json\n{\n  \"forceLoginMethod\": \"console\",\n  \"forceLoginOrgUUID\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n}\n```\n\n### API Key Helper\n\n```json\n{\n  \"apiKeyHelper\": \"/bin/generate_temp_api_key.sh\"\n}\n```\n\n### Disable Co-Authored-By\n\n```json\n{\n  \"includeCoAuthoredBy\": false\n}\n```\n\n### Status Line\n\n```json\n{\n  \"statusLine\": {\n    \"type\": \"command\",\n    \"command\": \"~/.claude/statusline.sh\"\n  }\n}\n```\n\n### Output Style\n\n```json\n{\n  \"outputStyle\": \"Explanatory\"\n}\n```\n\n## Complete Example: Enterprise Project\n\n```json\n{\n  \"permissions\": {\n    \"allow\": [\n      \"Bash(npm run:*)\",\n      \"Bash(git status:*)\",\n      \"Bash(git diff:*)\",\n      \"Bash(git log:*)\",\n      \"Bash(docker ps:*)\",\n      \"Bash(kubectl get:*)\"\n    ],\n    \"deny\": [\n      \"Read(./.env)\",\n      \"Read(./.env.*)\",\n      \"Read(./secrets/**)\",\n      \"Read(./config/production.json)\",\n      \"Bash(curl:*)\",\n      \"Bash(wget:*)\",\n      \"Bash(git push:*)\",\n      \"WebFetch\",\n      \"WebSearch\"\n    ],\n    \"ask\": [\n      \"Bash(git add:*)\",\n      \"Bash(git commit:*)\",\n      \"Write(./src/**)\"\n    ],\n    \"defaultMode\": \"default\",\n    \"disableBypassPermissionsMode\": \"disable\"\n  },\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/lint-and-format.sh\",\n            \"timeout\": 60\n          }\n        ]\n      }\n    ],\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"$CLAUDE_PROJECT_DIR/.claude/hooks/validate-bash.py\"\n          }\n        ]\n      }\n    ]\n  },\n  \"sandbox\": {\n    \"enabled\": true,\n    \"autoAllowBashIfSandboxed\": true,\n    \"excludedCommands\": [\"docker\", \"kubectl\"],\n    \"network\": {\n      \"allowUnixSockets\": [\"/var/run/docker.sock\"],\n      \"allowLocalBinding\": true\n    }\n  },\n  \"env\": {\n    \"NODE_ENV\": \"development\",\n    \"DISABLE_TELEMETRY\": \"1\"\n  },\n  \"enabledPlugins\": {\n    \"security-scanner@company-tools\": true,\n    \"code-quality@company-tools\": true\n  },\n  \"extraKnownMarketplaces\": {\n    \"company-tools\": {\n      \"source\": {\n        \"source\": \"github\",\n        \"repo\": \"company/claude-plugins\"\n      }\n    }\n  },\n  \"companyAnnouncements\": [\n    \"Welcome to Company Dev! See docs.company.com for coding standards\",\n    \"Reminder: All PRs require security scan approval\"\n  ],\n  \"model\": \"claude-sonnet-4-5-20250929\",\n  \"cleanupPeriodDays\": 20,\n  \"includeCoAuthoredBy\": true\n}\n```\n\n## Validation\n\n### Check JSON Syntax\n\n```bash\n# Validate JSON\ncat .claude/settings.json | jq .\n\n# Pretty-print\ncat .claude/settings.json | jq . > temp.json && mv temp.json .claude/settings.json\n```\n\n### Test Configuration\n\n```bash\n# Start Claude Code with debug\nclaude --debug\n\n# Check settings loaded\n# Look for: \"Loading settings from...\"\n```\n\n### Verify Permissions\n\n```bash\n# Run /permissions in Claude Code\n/permissions\n\n# Check what's allowed/denied\n```\n\n## Troubleshooting\n\n### Settings Not Applied\n\n**Check:**\n1. JSON syntax is valid (`jq` validation)\n2. File location is correct\n3. Restart Claude Code after changes\n4. Check precedence (higher-level settings override)\n\n### Permission Rules Not Working\n\n**Check:**\n1. Rule syntax: `Tool(pattern)`\n2. Case-sensitive tool names\n3. Bash uses prefix matching (not regex)\n4. Precedence: deny > ask > allow\n\n### Hooks Not Running\n\n**Check:**\n1. `disableAllHooks` is not `true`\n2. Hook configuration is valid\n3. Script has execute permissions\n4. Restart Claude Code\n5. Use `/hooks` to verify loaded\n\n### Sandbox Issues\n\n**Check:**\n1. macOS/Linux only (not Windows)\n2. Docker/Podman installed\n3. Permissions for Docker socket\n4. Check `excludedCommands` for needed tools\n\n## Best Practices\n\n### DO:\n✅ Use project settings for team configuration\n✅ Use local settings for personal overrides\n✅ Validate JSON syntax before committing\n✅ Document why rules exist (comments via tools)\n✅ Test settings before sharing with team\n✅ Use specific permission rules\n✅ Set appropriate timeouts for hooks\n✅ Use `$CLAUDE_PROJECT_DIR` in hooks\n\n### DON'T:\n❌ Commit sensitive data to project settings\n❌ Use `bypassPermissions` mode in shared config\n❌ Make permission rules too broad\n❌ Forget to restart after changes\n❌ Use hardcoded paths in hooks\n❌ Disable all hooks globally without reason\n❌ Override enterprise managed settings (you can't)\n\n## Sensitive Files\n\n### Exclude from Claude Code\n\n```json\n{\n  \"permissions\": {\n    \"deny\": [\n      \"Read(./.env)\",\n      \"Read(./.env.*)\",\n      \"Read(./.envrc)\",\n      \"Read(./secrets/**)\",\n      \"Read(./.aws/**)\",\n      \"Read(./config/credentials.json)\",\n      \"Read(./config/production.json)\",\n      \"Read(./.ssh/**)\",\n      \"Read(./build/**)\",\n      \"Read(./dist/**)\",\n      \"Read(./node_modules/**)\"\n    ]\n  }\n}\n```\n\n## Settings Merging\n\nSettings from different levels are merged:\n\n**Example:**\n\nUser settings:\n```json\n{\n  \"permissions\": {\n    \"allow\": [\"Read(~/.gitconfig)\"]\n  }\n}\n```\n\nProject settings:\n```json\n{\n  \"permissions\": {\n    \"deny\": [\"Read(./.env)\"]\n  }\n}\n```\n\n**Merged result:**\n```json\n{\n  \"permissions\": {\n    \"allow\": [\"Read(~/.gitconfig)\"],\n    \"deny\": [\"Read(./.env)\"]\n  }\n}\n```\n\nArrays are concatenated, objects are merged.\n\n## Interactive Configuration\n\nOffer to create/update settings:\n\n```\nI can help you configure this. Would you like me to:\n\n1. Create a new settings.json file\n2. Update existing settings.json\n3. Show you what to add manually\n\nWhere should I make these changes?\n- User settings (~/.claude/settings.json)\n- Project settings (.claude/settings.json)\n- Local settings (.claude/settings.local.json)\n```\n\n## Resources\n\n- **Official Settings Docs:** `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_settings.md`\n- **Hooks Reference:** `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_hooks.md`\n- **Sandbox Guide:** Look for sandboxing documentation\n- **IAM Guide:** Look for permissions documentation\n",
        "templates/.claude/skills/anthropic/claude-skill-builder/skill.md": "---\nname: claude-skill-builder\ndescription: Interactive skill creator for Claude Code. Triggers when user mentions creating skills, building skills, skill templates, skill frontmatter, allowed-tools, or wants to scaffold a new skill.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Claude Code Skill Builder\n\n## Purpose\n\nGuide users through creating well-structured Claude Code skills with proper frontmatter, clear descriptions, and effective trigger keywords. Auto-invokes when users want to create or modify skills.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Creating skills** - \"create a skill\", \"make a skill\", \"new skill\"\n- **Skill structure** - \"skill template\", \"skill format\", \"skill frontmatter\"\n- **Trigger keywords** - \"skill description\", \"when to invoke\", \"trigger words\"\n- **Skill configuration** - \"allowed-tools\", \"skill permissions\", \"skill model\"\n- **Skill organization** - \"skill directory\", \"skill files\", \"multi-file skill\"\n\n## Knowledge Base\n\n- Official docs: `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_skills.md`\n- Project guide: `.claude/docs/creating-components.md`\n- Examples in repository: `.claude/skills/`\n\n## Process\n\n### 1. Gather Requirements\n\nAsk the user:\n\n```\nLet me help you create a Claude Code skill! I need a few details:\n\n1. **Skill name** (lowercase-with-hyphens):\n   Example: nextjs-expert, pdf-processor, git-commit-helper\n\n2. **What does this skill do?**\n   Describe the capability in 1-2 sentences.\n\n3. **When should this skill activate?**\n   List keywords users will mention: Next.js, React, API, authentication, etc.\n\n4. **What tools will it need?**\n   - Read (read files)\n   - Write (create new files)\n   - Edit (modify existing files)\n   - Grep (search file contents)\n   - Glob (find files by pattern)\n   - Bash (run shell commands)\n\n5. **Scope:**\n   - Personal (`~/.claude/skills/`) - just for you\n   - Project (`.claude/skills/`) - shared with team\n\n6. **Category/subdirectory?**\n   Example: data, api, frontend, backend, testing\n```\n\n### 2. Validate Input\n\nCheck the skill name:\n- Must be lowercase\n- Use hyphens (not underscores or spaces)\n- Maximum 64 characters\n- Descriptive and clear\n\nValidate description:\n- Maximum 1024 characters\n- Must include trigger keywords\n- Should explain both WHAT and WHEN\n\n### 3. Create Skill Structure\n\nDetermine file structure:\n\n**Single file skill (simple):**\n```\nskill-name/\n└── SKILL.md\n```\n\n**Multi-file skill (complex):**\n```\nskill-name/\n├── SKILL.md (overview and main instructions)\n├── REFERENCE.md (detailed API reference)\n├── EXAMPLES.md (code examples)\n├── scripts/\n│   └── helper.py (utility scripts)\n└── templates/\n    └── template.txt (file templates)\n```\n\n### 4. Generate SKILL.md\n\nCreate frontmatter with:\n- `name`: Skill identifier (from user input)\n- `description`: Clear description with ALL trigger keywords\n- `allowed-tools`: Only tools actually needed\n- `model`: sonnet (default), opus (complex), or haiku (fast)\n\nTemplate structure:\n```markdown\n---\nname: skill-identifier\ndescription: What this does and when to use it. Include keywords: keyword1, keyword2, keyword3\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Skill Name\n\n## Purpose\n[Clear explanation of what this skill provides]\n\n## When to Use\n- Trigger condition 1 (specific keywords)\n- Trigger condition 2 (user scenarios)\n- Trigger condition 3 (related topics)\n\n## Process\n\n### 1. Analyze the Request\n[How to understand what the user wants]\n\n### 2. Gather Context\nUse tools to collect information:\n- Read relevant files\n- Search for patterns\n- Find related code\n\n### 3. Provide Solution\n[Step-by-step approach to solving the problem]\n\n### 4. Verify Result\n[How to confirm the solution works]\n\n## Examples\n\n### Example 1: Basic Usage\n**User request:** [Example request]\n**Process:** [How skill handles it]\n**Output:** [What user sees]\n\n### Example 2: Advanced Usage\n**User request:** [Complex scenario]\n**Process:** [Multi-step handling]\n**Output:** [Comprehensive result]\n\n## Best Practices\n- Practice 1: [Specific guidance]\n- Practice 2: [Common pattern]\n- Practice 3: [Expert tip]\n\n## Common Pitfalls\n- ❌ **Pitfall 1**: [What to avoid and why]\n  ✅ **Instead**: [Better approach]\n\n- ❌ **Pitfall 2**: [Common mistake]\n  ✅ **Instead**: [Correct way]\n\n## Resources\n- [Official Documentation](url)\n- [Related Guide](path/to/guide.md)\n```\n\n### 5. Create Supporting Files (if needed)\n\nFor complex skills, offer to create:\n\n**REFERENCE.md:**\n```markdown\n# Detailed Reference\n\n## API Documentation\n[Comprehensive API details]\n\n## Configuration Options\n[All available settings]\n\n## Advanced Features\n[Expert-level capabilities]\n```\n\n**EXAMPLES.md:**\n```markdown\n# Examples\n\n## Example 1: [Scenario]\n[Detailed code example with explanation]\n\n## Example 2: [Scenario]\n[Another complete example]\n```\n\n### 6. Test and Validate\n\nAfter creating the skill:\n\n```bash\n# Verify file exists\nls -la path/to/skill/SKILL.md\n\n# Check file structure\ncat path/to/skill/SKILL.md | head -20\n\n# Validate YAML frontmatter\ncat path/to/skill/SKILL.md | sed -n '1,/^---$/p'\n```\n\nProvide testing instructions:\n```\nTo test your skill:\n1. Restart Claude Code or start a new session\n2. Mention one of your trigger keywords\n3. Watch for skill activation\n4. Ask: \"What skills are available?\" to verify it's loaded\n```\n\n### 7. Provide Next Steps\n\nGive the user:\n- Path to the created skill file\n- How to test it\n- How to modify it\n- How to share it (if project skill)\n- How to add more supporting files\n\n## Skill Size Guidelines\n\nWarn if skill is getting large:\n- ✅ **Good:** < 600 lines\n- ⚠️ **Warning:** 600-900 lines (consider splitting)\n- ❌ **Too large:** > 900 lines (must split or refactor)\n\n**If too large, suggest:**\n- Split into multiple focused skills\n- Move detailed docs to separate reference files\n- Extract examples to EXAMPLES.md\n- Link to external documentation\n\n## Frontmatter Best Practices\n\n### Description Field\n\n✅ **Good descriptions:**\n```yaml\ndescription: Expert in Next.js App Router, server components, server actions, and React Server Components. Use when user mentions Next.js, RSC, App Router, or server-side React patterns.\n```\n\n❌ **Bad descriptions:**\n```yaml\ndescription: Helps with Next.js\n```\n\n### Allowed Tools\n\nOnly request tools you actually use:\n- Don't request all tools \"just in case\"\n- Be specific about needs\n- Consider security implications\n\n**Examples:**\n```yaml\n# Read-only skill\nallowed-tools: Read, Grep, Glob\n\n# Code generator\nallowed-tools: Read, Write, Grep, Glob\n\n# Full development workflow\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\n```\n\n### Model Selection\n\nChoose based on complexity:\n- `haiku`: Fast, simple tasks, quick responses\n- `sonnet`: Balanced, most skills (recommended)\n- `opus`: Complex reasoning, advanced tasks\n\n## Example Skills to Reference\n\nPoint users to examples in the repository:\n- `.claude/skills/data/toon-formatter/skill.md` - Data processing\n- `.claude/skills/ai/anthropic/skill.md` - API expertise\n- `.claude/skills/ai/claude-code/skill.md` - Tool knowledge\n\n## Common Skill Types\n\n### API/Framework Expert\n**Purpose:** Deep knowledge of a specific framework or API\n**Triggers:** Framework name, features, patterns\n**Tools:** Read, Grep, Glob\n**Example:** Next.js expert, FastAPI expert, React expert\n\n### Code Generator\n**Purpose:** Create boilerplate or scaffolding\n**Triggers:** \"generate\", \"create\", \"scaffold\", \"template\"\n**Tools:** Write, Read, Grep, Glob\n**Example:** Component generator, test file creator\n\n### Code Analyzer\n**Purpose:** Review and analyze existing code\n**Triggers:** \"review\", \"analyze\", \"check\", \"audit\"\n**Tools:** Read, Grep, Glob\n**Example:** Security auditor, performance analyzer\n\n### Development Workflow\n**Purpose:** Automate common dev tasks\n**Triggers:** Workflow steps, automation keywords\n**Tools:** Read, Write, Edit, Bash\n**Example:** Deployment helper, commit message generator\n\n### Data Processor\n**Purpose:** Transform or analyze data\n**Triggers:** Data formats, transformation keywords\n**Tools:** Read, Write, Edit, Grep\n**Example:** TOON formatter, JSON converter\n\n## Troubleshooting\n\n### Skill Not Activating\n\n**Check:**\n1. Description has specific trigger keywords\n2. File is named `SKILL.md` (case-sensitive)\n3. File is in correct location:\n   - Personal: `~/.claude/skills/category/skill-name/SKILL.md`\n   - Project: `.claude/skills/category/skill-name/SKILL.md`\n4. YAML frontmatter is valid (no tabs, proper indentation)\n5. Restart Claude Code to load new skills\n\n**Debug with:**\n```bash\n# Verify location\nls ~/.claude/skills/*/SKILL.md\nls .claude/skills/*/SKILL.md\n\n# Check YAML syntax\nhead -20 path/to/SKILL.md\n\n# Run in debug mode\nclaude --debug\n```\n\n### Skill Conflicts\n\nIf multiple skills have similar triggers:\n- Make descriptions more specific\n- Use distinct keywords\n- Reference specific use cases\n- Consider combining into one skill\n\n### File Not Found Errors\n\nCheck that referenced files exist:\n```bash\n# From SKILL.md, check references like [reference.md](reference.md)\nls path/to/skill/reference.md\n```\n\n## Best Practices Summary\n\n### DO:\n✅ Include specific trigger keywords in description\n✅ Keep skills focused on one capability\n✅ Provide clear examples in the skill\n✅ Use progressive disclosure (reference external files)\n✅ Test the skill before sharing\n✅ Document version changes\n\n### DON'T:\n❌ Make descriptions too vague\n❌ Try to handle everything in one skill\n❌ Request unnecessary tools\n❌ Skip examples\n❌ Forget to test activation triggers\n❌ Go over 900 lines without splitting\n\n## Resources\n\n- **Official Skill Docs:** `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_skills.md`\n- **Anthropic Skill Guide:** https://docs.claude.com/en/docs/agents-and-tools/agent-skills/\n- **Project Component Guide:** `.claude/docs/creating-components.md`\n- **Skill Examples:** `.claude/skills/` directory\n",
        "templates/.claude/skills/anthropic/skill.md": "---\nname: anthropic-expert\ndescription: Expert on Anthropic Claude API, models, prompt engineering, function calling, vision, and best practices. Triggers on anthropic, claude, api, prompt, function calling, vision, messages api, embeddings\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Anthropic API Expert\n\n## Purpose\n\nProvide expert guidance on Anthropic's Claude API, including prompt engineering, function calling, vision capabilities, and best practices based on official Anthropic documentation.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Anthropic** - company, API, platform\n- **Claude** - models (Opus, Sonnet, Haiku), capabilities\n- **API** - Messages API, streaming, embeddings\n- **Features** - function calling, vision, extended context, prompt caching\n- **Integration** - SDKs (Python, TypeScript), REST API\n\n## Knowledge Base\n\n**Full access to official Anthropic documentation (when available):**\n- **Location:** `docs/`\n- **Files:** 199 markdown files\n- **Format:** `.md` files\n\n**Note:** Documentation must be pulled separately:\n```bash\npipx install docpull\ndocpull https://docs.anthropic.com -o .claude/skills/anthropic/docs\n```\n\n## Process\n\nWhen a user asks about Anthropic/Claude:\n\n### 1. Identify Topic\n```\nCommon topics:\n- Getting started / API keys\n- Model selection (Opus, Sonnet, Haiku)\n- Messages API / streaming\n- Prompt engineering techniques\n- Function/tool calling\n- Vision and image analysis\n- Extended context (200K tokens)\n- Prompt caching\n- Rate limits and pricing\n- Error handling\n```\n\n### 2. Search Documentation\n\nUse Grep to find relevant docs:\n```bash\n# Search for specific topics\nGrep \"function calling|tool\" docs/ --output-mode files_with_matches -i\nGrep \"vision|image\" docs/ --output-mode content -C 3\n```\n\nCheck the INDEX.md for navigation:\n```bash\nRead docs/INDEX.md\n```\n\n### 3. Read Relevant Files\n\nRead the most relevant documentation files:\n```bash\nRead docs/path/to/relevant-doc.md\n```\n\n### 4. Provide Answer\n\nStructure your response:\n- **Direct answer** - solve the user's problem first\n- **Code examples** - show API calls with proper formatting\n- **Best practices** - mention Claude-specific patterns\n- **Model selection** - recommend appropriate model (Opus/Sonnet/Haiku)\n- **References** - cite specific docs for deeper reading\n- **Cost optimization** - mention prompt caching, model choice\n\n## Example Workflows\n\n### Example 1: Function Calling\n```\nUser: \"How do I implement function calling with Claude?\"\n\n1. Search: Grep \"function calling|tool\" docs/\n2. Read: Function calling documentation\n3. Answer:\n   - Explain tool use format\n   - Show request/response example\n   - Discuss tool choice vs any\n   - Best practices for tool definitions\n```\n\n### Example 2: Vision Capabilities\n```\nUser: \"Can Claude analyze images?\"\n\n1. Search: Grep \"vision|image\" docs/ -i\n2. Read: Vision API documentation\n3. Answer:\n   - Supported image formats\n   - Image encoding (base64, URLs)\n   - Show example API call\n   - Limitations and best practices\n```\n\n### Example 3: Prompt Engineering\n```\nUser: \"How do I write better prompts for Claude?\"\n\n1. Search: Grep \"prompt|engineering\" docs/\n2. Read: Prompt engineering guide\n3. Answer:\n   - Clear instructions principle\n   - Examples and context\n   - XML tags for structure\n   - Chain of thought prompting\n```\n\n## Key Concepts to Reference\n\n**Models:**\n- Claude 3.5 Opus - most capable\n- Claude 3.5 Sonnet - balanced (recommended for most use cases)\n- Claude 3.5 Haiku - fast and economical\n\n**API Features:**\n- Messages API (primary interface)\n- Streaming responses\n- Function/tool calling\n- Vision (image analysis)\n- Extended context (200K tokens)\n- Prompt caching (reduce costs)\n\n**Best Practices:**\n- System prompts vs user messages\n- XML tags for structure\n- Few-shot examples\n- Clear, specific instructions\n- Appropriate model selection\n\n**SDKs:**\n- Python SDK (`anthropic`)\n- TypeScript SDK (`@anthropic-ai/sdk`)\n- REST API (curl/HTTP)\n\n## Response Style\n\n- **Clear** - API developers want precise answers\n- **Code-first** - show working examples\n- **Model-aware** - recommend appropriate Claude model\n- **Cost-conscious** - mention caching, model choice\n- **Cite sources** - reference specific doc sections\n\n## Follow-up Suggestions\n\nAfter answering, suggest:\n- Related API features\n- Cost optimization strategies\n- Error handling patterns\n- Testing approaches\n- Safety and moderation considerations\n",
        "templates/.claude/skills/aptos/dapp-integration/skill.md": "---\nname: aptos-dapp-integration\ndescription: Expert on building decentralized applications on Aptos with frontend integration, wallet connectivity (Petra, Martian, Pontem), TypeScript SDK, transaction submission, and wallet adapter patterns. Triggers on keywords wallet connect, petra, martian, pontem, typescript sdk, aptos sdk, dapp, frontend integration, wallet adapter, transaction, sign\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Aptos DApp Integration Expert\n\n## Purpose\n\nProvide expert guidance on integrating Aptos blockchain with frontend applications, including wallet connectivity, transaction submission, account management, and using the TypeScript SDK for building decentralized applications.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Wallets** - Petra, Martian, Pontem, wallet connection, wallet adapter\n- **Frontend Integration** - React, Next.js, Vue, dApp UI\n- **TypeScript SDK** - @aptos-labs/ts-sdk, aptos client, API calls\n- **Transactions** - sign transaction, submit transaction, transaction builder\n- **Accounts** - account creation, authentication, user session\n- **Smart Contract Calls** - view functions, entry functions, payload\n\n## Core Integration Concepts\n\n### Wallet Connection Flow\n\n```\n1. Detect wallet extension (Petra, Martian, etc.)\n2. Request connection approval from user\n3. Get connected account address\n4. Listen for account/network changes\n5. Sign and submit transactions\n6. Handle wallet disconnection\n```\n\n### TypeScript SDK Architecture\n\n```typescript\nimport { Aptos, AptosConfig, Network } from \"@aptos-labs/ts-sdk\";\n\n// Initialize client\nconst config = new AptosConfig({ network: Network.TESTNET });\nconst aptos = new Aptos(config);\n\n// Query blockchain\nconst account = await aptos.getAccountInfo({ accountAddress: \"0x...\" });\n\n// Submit transactions\nconst transaction = await aptos.transaction.build.simple({...});\n```\n\n## Process\n\nWhen a user asks about DApp integration:\n\n### 1. Identify Integration Need\n\n```\nCommon scenarios:\n- Connecting wallet to React/Next.js app\n- Reading on-chain data (account info, resources, events)\n- Submitting transactions from frontend\n- Handling wallet switching/disconnection\n- Managing user authentication\n- Displaying NFTs or token balances\n- Building transaction UI\n```\n\n### 2. Choose Integration Approach\n\n**Option A: Wallet Adapter (Recommended)**\n- Use @aptos-labs/wallet-adapter-react\n- Supports multiple wallets\n- Standard hooks and components\n- Best for production apps\n\n**Option B: Direct Wallet Integration**\n- Integrate with specific wallet (Petra, Martian)\n- More control, less abstraction\n- Good for custom requirements\n\n**Option C: SDK Only (Read-Only)**\n- Use TypeScript SDK\n- No wallet needed\n- Query blockchain data\n- Good for analytics, explorers\n\n### 3. Provide Integration Solution\n\nStructure your response:\n- **Setup** - installation and configuration\n- **Code example** - working integration code\n- **Wallet handling** - connection/disconnection logic\n- **Transaction flow** - how to build and submit\n- **Error handling** - common issues and solutions\n- **Best practices** - security and UX considerations\n\n## Wallet Adapter Integration\n\n### Installation\n\n```bash\nnpm install @aptos-labs/wallet-adapter-react \\\n            @aptos-labs/wallet-adapter-ant-design \\\n            petra-plugin-wallet-adapter \\\n            @martianwallet/aptos-wallet-adapter\n```\n\n### React Setup\n\n```typescript\nimport { AptosWalletAdapterProvider } from \"@aptos-labs/wallet-adapter-react\";\nimport { PetraWallet } from \"petra-plugin-wallet-adapter\";\nimport { MartianWallet } from \"@martianwallet/aptos-wallet-adapter\";\n\nfunction App() {\n  const wallets = [new PetraWallet(), new MartianWallet()];\n\n  return (\n    <AptosWalletAdapterProvider\n      plugins={wallets}\n      autoConnect={true}\n      onError={(error) => console.error(error)}\n    >\n      <YourApp />\n    </AptosWalletAdapterProvider>\n  );\n}\n```\n\n### Using Wallet Hooks\n\n```typescript\nimport { useWallet } from \"@aptos-labs/wallet-adapter-react\";\n\nfunction WalletButton() {\n  const {\n    connect,\n    disconnect,\n    account,\n    connected,\n    wallet,\n    signAndSubmitTransaction\n  } = useWallet();\n\n  const handleConnect = async () => {\n    try {\n      await connect(\"Petra\"); // or wallet.name\n    } catch (error) {\n      console.error(\"Failed to connect:\", error);\n    }\n  };\n\n  if (connected) {\n    return (\n      <div>\n        <p>Connected: {account?.address}</p>\n        <button onClick={disconnect}>Disconnect</button>\n      </div>\n    );\n  }\n\n  return <button onClick={handleConnect}>Connect Wallet</button>;\n}\n```\n\n## TypeScript SDK Patterns\n\n### Initialize Client\n\n```typescript\nimport { Aptos, AptosConfig, Network } from \"@aptos-labs/ts-sdk\";\n\n// Mainnet\nconst mainnetConfig = new AptosConfig({ network: Network.MAINNET });\nconst mainnet = new Aptos(mainnetConfig);\n\n// Testnet\nconst testnetConfig = new AptosConfig({ network: Network.TESTNET });\nconst testnet = new Aptos(testnetConfig);\n\n// Custom node\nconst customConfig = new AptosConfig({\n  fullnode: \"https://custom-node.example.com\",\n  indexer: \"https://custom-indexer.example.com\",\n});\nconst custom = new Aptos(customConfig);\n```\n\n### Read Blockchain Data\n\n```typescript\n// Get account info\nconst accountInfo = await aptos.getAccountInfo({\n  accountAddress: \"0x1234...\"\n});\n\n// Get account resources\nconst resources = await aptos.getAccountResources({\n  accountAddress: \"0x1234...\"\n});\n\n// Get specific resource\nconst coinResource = await aptos.getAccountResource({\n  accountAddress: \"0x1234...\",\n  resourceType: \"0x1::coin::CoinStore<0x1::aptos_coin::AptosCoin>\"\n});\n\n// Get account modules\nconst modules = await aptos.getAccountModules({\n  accountAddress: \"0x1234...\"\n});\n\n// Get events\nconst events = await aptos.getAccountEventsByEventType({\n  accountAddress: \"0x1234...\",\n  eventType: \"0x1::coin::WithdrawEvent\"\n});\n```\n\n### View Functions (Read-Only)\n\n```typescript\n// Call view function to read state\nconst result = await aptos.view({\n  payload: {\n    function: \"0x1234::my_module::get_balance\",\n    typeArguments: [],\n    functionArguments: [\"0xabcd...\"],\n  },\n});\n\nconsole.log(\"Balance:\", result[0]);\n```\n\n## Transaction Patterns\n\n### Simple Transaction (Entry Function)\n\n```typescript\nimport { useWallet } from \"@aptos-labs/wallet-adapter-react\";\n\nfunction TransferButton() {\n  const { signAndSubmitTransaction } = useWallet();\n\n  const handleTransfer = async () => {\n    try {\n      const response = await signAndSubmitTransaction({\n        data: {\n          function: \"0x1::coin::transfer\",\n          typeArguments: [\"0x1::aptos_coin::AptosCoin\"],\n          functionArguments: [\n            \"0xrecipient...\", // recipient address\n            \"100000000\",     // amount (in octas)\n          ],\n        },\n      });\n\n      // Wait for transaction\n      await aptos.waitForTransaction({\n        transactionHash: response.hash,\n      });\n\n      console.log(\"Transfer successful:\", response.hash);\n    } catch (error) {\n      console.error(\"Transfer failed:\", error);\n    }\n  };\n\n  return <button onClick={handleTransfer}>Transfer APT</button>;\n}\n```\n\n### Transaction with SDK (More Control)\n\n```typescript\nimport { Account, Aptos } from \"@aptos-labs/ts-sdk\";\n\nasync function submitTransaction(aptos: Aptos, sender: Account) {\n  // Build transaction\n  const transaction = await aptos.transaction.build.simple({\n    sender: sender.accountAddress,\n    data: {\n      function: \"0x1234::my_module::my_function\",\n      typeArguments: [],\n      functionArguments: [arg1, arg2],\n    },\n  });\n\n  // Sign transaction\n  const senderAuthenticator = aptos.transaction.sign({\n    signer: sender,\n    transaction,\n  });\n\n  // Submit transaction\n  const committedTxn = await aptos.transaction.submit.simple({\n    transaction,\n    senderAuthenticator,\n  });\n\n  // Wait for confirmation\n  const executedTransaction = await aptos.waitForTransaction({\n    transactionHash: committedTxn.hash,\n  });\n\n  return executedTransaction;\n}\n```\n\n### Multi-Agent Transaction\n\n```typescript\nconst transaction = await aptos.transaction.build.multiAgent({\n  sender: sender.accountAddress,\n  secondarySignerAddresses: [secondSigner.accountAddress],\n  data: {\n    function: \"0x1234::escrow::create_escrow\",\n    functionArguments: [/* ... */],\n  },\n});\n\nconst senderAuth = aptos.transaction.sign({ signer: sender, transaction });\nconst secondAuth = aptos.transaction.sign({ signer: secondSigner, transaction });\n\nconst response = await aptos.transaction.submit.multiAgent({\n  transaction,\n  senderAuthenticator: senderAuth,\n  additionalSignersAuthenticators: [secondAuth],\n});\n```\n\n## Common DApp Patterns\n\n### Pattern 1: Check Token Balance\n\n```typescript\nasync function getAptBalance(address: string): Promise<number> {\n  try {\n    const resource = await aptos.getAccountResource({\n      accountAddress: address,\n      resourceType: \"0x1::coin::CoinStore<0x1::aptos_coin::AptosCoin>\",\n    });\n\n    return Number(resource.coin.value);\n  } catch (error) {\n    console.error(\"Error fetching balance:\", error);\n    return 0;\n  }\n}\n```\n\n### Pattern 2: Display NFTs\n\n```typescript\nasync function getUserNFTs(address: string) {\n  const nfts = await aptos.getAccountOwnedTokens({\n    accountAddress: address,\n  });\n\n  return nfts.map(nft => ({\n    name: nft.current_token_data?.token_name,\n    description: nft.current_token_data?.description,\n    uri: nft.current_token_data?.token_uri,\n    collection: nft.current_token_data?.collection_name,\n  }));\n}\n```\n\n### Pattern 3: Transaction Status Tracking\n\n```typescript\nimport { useState } from \"react\";\n\nfunction useTransactionStatus() {\n  const [status, setStatus] = useState<\"idle\" | \"pending\" | \"success\" | \"error\">(\"idle\");\n  const [hash, setHash] = useState<string | null>(null);\n\n  const submitTransaction = async (txn: any) => {\n    try {\n      setStatus(\"pending\");\n      const response = await signAndSubmitTransaction(txn);\n      setHash(response.hash);\n\n      await aptos.waitForTransaction({ transactionHash: response.hash });\n      setStatus(\"success\");\n\n      return response;\n    } catch (error) {\n      setStatus(\"error\");\n      throw error;\n    }\n  };\n\n  return { status, hash, submitTransaction };\n}\n```\n\n### Pattern 4: Network Switching\n\n```typescript\nimport { Network } from \"@aptos-labs/ts-sdk\";\nimport { useWallet } from \"@aptos-labs/wallet-adapter-react\";\n\nfunction NetworkSwitch() {\n  const { network, changeNetwork } = useWallet();\n\n  const switchToMainnet = async () => {\n    try {\n      await changeNetwork(Network.MAINNET);\n    } catch (error) {\n      console.error(\"Failed to switch network:\", error);\n    }\n  };\n\n  return (\n    <div>\n      <p>Current: {network?.name}</p>\n      <button onClick={switchToMainnet}>Switch to Mainnet</button>\n    </div>\n  );\n}\n```\n\n## Direct Wallet Integration (Petra)\n\n### Detect and Connect\n\n```typescript\ndeclare global {\n  interface Window {\n    aptos?: any;\n  }\n}\n\nasync function connectPetra() {\n  if (!window.aptos) {\n    throw new Error(\"Petra wallet not installed\");\n  }\n\n  try {\n    const response = await window.aptos.connect();\n    const account = await window.aptos.account();\n\n    return {\n      address: account.address,\n      publicKey: account.publicKey,\n    };\n  } catch (error) {\n    throw new Error(\"User rejected connection\");\n  }\n}\n```\n\n### Sign and Submit with Petra\n\n```typescript\nasync function signAndSubmit(payload: any) {\n  const pendingTransaction = await window.aptos.signAndSubmitTransaction(payload);\n\n  // Wait for confirmation\n  const client = new Aptos(new AptosConfig({ network: Network.MAINNET }));\n  const txn = await client.waitForTransaction({\n    transactionHash: pendingTransaction.hash,\n  });\n\n  return txn;\n}\n```\n\n## Error Handling\n\n### Common Errors\n\n```typescript\ntry {\n  await signAndSubmitTransaction(txn);\n} catch (error: any) {\n  if (error.code === 4001) {\n    console.error(\"User rejected transaction\");\n  } else if (error.message.includes(\"INSUFFICIENT_BALANCE\")) {\n    console.error(\"Insufficient balance for transaction\");\n  } else if (error.message.includes(\"SEQUENCE_NUMBER_TOO_OLD\")) {\n    console.error(\"Transaction nonce issue - retry\");\n  } else {\n    console.error(\"Transaction failed:\", error);\n  }\n}\n```\n\n### Wallet Detection\n\n```typescript\nfunction detectWallets() {\n  const wallets = {\n    petra: !!window.aptos,\n    martian: !!window.martian,\n    pontem: !!window.pontem,\n  };\n\n  if (!Object.values(wallets).some(Boolean)) {\n    return { installed: false, wallets };\n  }\n\n  return { installed: true, wallets };\n}\n```\n\n## UI/UX Best Practices\n\n### Loading States\n\n```typescript\nfunction TransactionButton() {\n  const [loading, setLoading] = useState(false);\n\n  const handleClick = async () => {\n    setLoading(true);\n    try {\n      await submitTransaction();\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <button disabled={loading} onClick={handleClick}>\n      {loading ? \"Processing...\" : \"Submit Transaction\"}\n    </button>\n  );\n}\n```\n\n### Transaction Confirmation UI\n\n```typescript\nfunction TransactionStatus({ hash }: { hash: string }) {\n  const explorerUrl = `https://explorer.aptoslabs.com/txn/${hash}?network=mainnet`;\n\n  return (\n    <div className=\"success-message\">\n      <p>✅ Transaction Successful!</p>\n      <a href={explorerUrl} target=\"_blank\" rel=\"noopener noreferrer\">\n        View on Explorer\n      </a>\n    </div>\n  );\n}\n```\n\n### Account Display\n\n```typescript\nfunction AccountDisplay({ address }: { address: string }) {\n  const shortened = `${address.slice(0, 6)}...${address.slice(-4)}`;\n\n  return (\n    <div className=\"account\">\n      <span title={address}>{shortened}</span>\n      <button onClick={() => navigator.clipboard.writeText(address)}>\n        📋 Copy\n      </button>\n    </div>\n  );\n}\n```\n\n## Security Considerations\n\n### ✅ Best Practices\n\n- Always validate user input before building transactions\n- Show transaction details to user before signing\n- Use HTTPS for all API calls\n- Don't store private keys in frontend\n- Verify transaction success before updating UI\n- Handle wallet disconnection gracefully\n- Check network before submitting transactions\n\n### ❌ Avoid\n\n- Never ask users for private keys\n- Don't auto-submit transactions without user action\n- Avoid hardcoding sensitive data\n- Don't ignore error handling\n- Never bypass wallet confirmation dialogs\n\n## Testing DApp Integration\n\n### Mock Wallet for Tests\n\n```typescript\nconst mockWallet = {\n  connect: jest.fn().mockResolvedValue({ address: \"0x123...\" }),\n  disconnect: jest.fn(),\n  signAndSubmitTransaction: jest.fn().mockResolvedValue({ hash: \"0xabc...\" }),\n};\n```\n\n### Testing with Testnet\n\n```typescript\n// Use testnet for development\nconst config = new AptosConfig({ network: Network.TESTNET });\n\n// Get testnet APT from faucet\nawait aptos.fundAccount({\n  accountAddress: account.accountAddress,\n  amount: 100000000, // 1 APT\n});\n```\n\n## Framework-Specific Examples\n\n### Next.js App Router\n\n```typescript\n// app/providers.tsx\n\"use client\";\n\nimport { AptosWalletAdapterProvider } from \"@aptos-labs/wallet-adapter-react\";\n\nexport function Providers({ children }: { children: React.ReactNode }) {\n  return (\n    <AptosWalletAdapterProvider plugins={wallets} autoConnect>\n      {children}\n    </AptosWalletAdapterProvider>\n  );\n}\n```\n\n### React Query Integration\n\n```typescript\nimport { useQuery } from \"@tanstack/react-query\";\n\nfunction useAccountBalance(address: string | undefined) {\n  return useQuery({\n    queryKey: [\"balance\", address],\n    queryFn: () => getAptBalance(address!),\n    enabled: !!address,\n    refetchInterval: 10000, // Refetch every 10s\n  });\n}\n```\n\n## Response Style\n\n- **Code-first** - Show working integration examples\n- **Framework-aware** - Adapt to React/Next.js/Vue\n- **Security-conscious** - Highlight security considerations\n- **UX-focused** - Suggest good user experience patterns\n- **Error handling** - Always include error handling\n\n## Follow-up Suggestions\n\nAfter helping with integration, suggest:\n- Error handling improvements\n- Loading and success states\n- Transaction simulation before submission\n- Event listening for real-time updates\n- Mobile wallet integration (wallet connect)\n- Testing strategies for DApp\n",
        "templates/.claude/skills/aptos/decibel/skill.md": "---\nname: decibel-expert\ndescription: Expert on Decibel on-chain perpetual futures trading platform on Aptos. Covers trading engine, orderbook, TypeScript SDK, REST APIs, WebSocket streams, market data, position management, TWAP orders, and vault operations. Triggers on keywords decibel, perpetual futures, aptos trading, on-chain trading, decibel sdk, perps, orderbook, twap, market data, trading api.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Decibel Trading Platform Expert\n\n## Purpose\n\nProvide expert guidance on Decibel, a fully on-chain perpetual futures trading platform built on Aptos blockchain. Help developers and traders integrate with Decibel's APIs, understand the architecture, and build trading applications.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Decibel** - trading platform, exchange, perpetual futures\n- **Trading** - on-chain trading, derivatives, perps, futures\n- **Aptos Trading** - Aptos-based exchange, Move contracts\n- **APIs** - REST API, WebSocket, market data, trading endpoints\n- **SDK** - TypeScript SDK, decibel-sdk, trading library\n- **Features** - TWAP orders, orderbook, positions, vaults, subaccounts\n- **Market Data** - prices, trades, orderbook depth, OHLC, candlesticks\n\n## Knowledge Base\n\nDocumentation is stored in Markdown format:\n- **Location:** `docs/`\n- **Files:** 44 documentation pages (180 KB)\n- **Format:** `.md` files organized by category\n\n## Documentation Coverage\n\n### Quick Start (5 files)\n- Overview and getting started\n- Market data (unauthenticated requests)\n- Authenticated requests\n- API reference\n- Placing your first order\n\n### Architecture (4 files)\n- Perp Engine contract overview\n- Global risk controls\n- Position management\n- Orderbook implementation\n\n### TypeScript SDK (6 files)\n- Overview and installation\n- Configuration\n- Read SDK (market data, positions)\n- Write SDK (orders, transactions)\n- Advanced usage\n\n### REST APIs (17 files)\n**User Endpoints:**\n- Account overview\n- Active TWAP orders\n- Delegations\n- Funding rate history\n- Open orders\n- Order history\n- Order details\n- Positions\n- Subaccounts\n- Trade history\n- TWAP history\n\n**Market Data:**\n- Asset contexts\n- Available markets\n- Candlestick/OHLC data\n- Orderbook depth\n- Market prices\n- Recent trades\n\n**Analytics & Vaults:**\n- Leaderboard\n- Public vaults\n\n### WebSocket APIs (1 file)\n- Bulk order fills stream\n- Account updates\n- Market trades\n- Order updates\n- Position updates\n\n### Transactions (10 files)\n- Overview and optimized building\n- Formatting prices and sizes\n- Account management (create subaccount, deposit, withdraw)\n- Order management (place, cancel)\n- Position management (TP/SL orders)\n\n## Process\n\nWhen a user asks about Decibel:\n\n### 1. Identify Topic\n\n```\nCommon topics:\n- Getting started / API setup\n- TypeScript SDK integration\n- REST API endpoints\n- WebSocket real-time data\n- Placing orders (market, limit, TWAP)\n- Position management\n- Account/subaccount management\n- Market data queries\n- Orderbook depth\n- Vault operations\n- Smart contract architecture\n- Aptos integration\n```\n\n### 2. Search Documentation\n\nUse Grep to find relevant docs:\n```bash\n# Search for specific topics\nGrep -i \"pattern\" path:docs/ output_mode:files_with_matches\n\n# Examples:\nGrep -i \"place order\" path:docs/ output_mode:content\nGrep -i \"websocket\" path:docs/ output_mode:content\nGrep -i \"typescript sdk\" path:docs/ output_mode:content\n```\n\n### 3. Read Documentation\n\nRead the most relevant file:\n```bash\nRead docs/quickstart-placing-your-first-order.md\nRead docs/typescript-sdk-write-sdk.md\nRead docs/rest-api-user-positions.md\n```\n\n### 4. Provide Guidance\n\nAnswer based on official documentation:\n- Cite specific API endpoints with examples\n- Show TypeScript SDK code samples\n- Explain smart contract functions\n- Provide transaction formats\n- Include error handling\n- Show WebSocket subscription examples\n\n## Key Platform Details\n\n**Platform:** Decibel - On-chain perpetual futures trading on Aptos\n\n**Base URLs:**\n- REST API: `https://api.netna.aptoslabs.com/decibel`\n- WebSocket: `wss://api.netna.aptoslabs.com/decibel`\n- Package Address: `0xb8a5788314451ce4d2fbbad32e1bad88d4184b73943b7fe5166eab93cf1a5a95`\n\n**Core Features:**\n- Perpetual futures trading\n- TWAP (Time-Weighted Average Price) orders\n- Fully on-chain orderbook\n- Real-time WebSocket streams\n- Subaccount support\n- Vault strategies\n- Move smart contracts on Aptos\n\n**Trading Features:**\n- Market and limit orders\n- Take-profit and stop-loss orders\n- Position management\n- Leverage trading\n- Funding rate settlements\n- Risk controls\n\n**Developer Tools:**\n- TypeScript SDK (`@decibel/sdk`)\n- REST API (comprehensive)\n- WebSocket API (real-time)\n- Aptos Move contracts\n- Smart contract ABIs\n\n## Common Use Cases\n\n### 1. Market Data Queries\n```\n- Get available markets\n- Fetch current prices\n- Query orderbook depth\n- Retrieve OHLC/candlestick data\n- Stream real-time trades\n```\n\n### 2. Account Management\n```\n- Create subaccounts\n- Deposit/withdraw funds\n- Check account balance\n- View positions\n- Manage delegations\n```\n\n### 3. Order Placement\n```\n- Place market orders\n- Place limit orders\n- Create TWAP orders\n- Set TP/SL orders\n- Cancel orders\n```\n\n### 4. Position Management\n```\n- Open positions\n- Close positions\n- Query position details\n- Get funding rate history\n- Set risk parameters\n```\n\n### 5. Real-Time Monitoring\n```\n- Subscribe to order updates\n- Monitor position changes\n- Track market trades\n- Watch account changes\n- Receive fills notifications\n```\n\n## Example Queries to Handle\n\n**\"How do I place an order on Decibel?\"**\n→ Search: `quickstart-placing-your-first-order.md`, `transactions-order-management-place-order.md`\n→ Provide: Step-by-step guide with TypeScript SDK example and REST API endpoint\n\n**\"What WebSocket streams are available?\"**\n→ Search: `websocket-bulk-order-fills.md`\n→ Provide: List of WebSocket channels with subscription examples\n\n**\"How does the orderbook work?\"**\n→ Search: `architecture-orderbook.md`\n→ Provide: Architecture explanation and smart contract details\n\n**\"How do I get market data?\"**\n→ Search: `quickstart-market-data.md`, `rest-api-market-data-*.md`\n→ Provide: Unauthenticated API endpoints with examples\n\n**\"What is a TWAP order?\"**\n→ Search: `rest-api-user-active-twap.md`, `rest-api-user-twap-history.md`\n→ Provide: TWAP explanation with placement and monitoring examples\n\n## Integration Patterns\n\n### TypeScript SDK\n```typescript\nimport { DecibelClient } from '@decibel/sdk';\n\nconst client = new DecibelClient({\n  apiKey: 'your-api-key',\n  network: 'mainnet'\n});\n\n// Query market data\nconst markets = await client.getMarkets();\nconst prices = await client.getPrices();\n\n// Place order\nconst order = await client.placeOrder({\n  market: 'BTC-PERP',\n  side: 'buy',\n  type: 'limit',\n  price: 50000,\n  size: 1\n});\n```\n\n### REST API\n```bash\n# Get market prices (unauthenticated)\nGET https://api.netna.aptoslabs.com/decibel/market-data/prices\n\n# Get account positions (authenticated)\nGET https://api.netna.aptoslabs.com/decibel/user/positions\nHeaders: Authorization: Bearer {token}\n```\n\n### WebSocket\n```javascript\nconst ws = new WebSocket('wss://api.netna.aptoslabs.com/decibel');\n\nws.send(JSON.stringify({\n  type: 'subscribe',\n  channel: 'trades',\n  market: 'BTC-PERP'\n}));\n```\n\n## Best Practices\n\n1. **Always read official docs** - Use Grep and Read tools\n2. **Provide complete examples** - Include error handling\n3. **Cite API endpoints** - Show exact URLs and parameters\n4. **Explain Aptos integration** - Reference Move contracts\n5. **Show SDK usage** - Prefer TypeScript SDK when applicable\n6. **Include WebSocket examples** - For real-time use cases\n7. **Mention risk controls** - Explain position limits and safety features\n8. **Reference transaction formatting** - Show proper price/size encoding\n\n## Related Skills\n\n- **Aptos Expert** - For blockchain-level questions\n- **TypeScript** - For SDK integration help\n- **WebSocket** - For real-time streaming guidance\n\n## Notes\n\n- Decibel is fully on-chain on Aptos blockchain\n- All trades settled via smart contracts\n- TWAP orders for reduced slippage\n- Comprehensive risk controls built-in\n- Vault strategies for advanced trading\n- Subaccounts for organization and delegation\n",
        "templates/.claude/skills/aptos/framework/skill.md": "---\nname: aptos-framework\ndescription: Expert on Aptos Framework (0x1 standard library) - account, coin, fungible_asset, object, timestamp, table, event, vector, string, option, error, and other core modules. Triggers on keywords aptos framework, 0x1, account module, table, smarttable, event, timestamp, randomness, aggregator, resource account\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Aptos Framework Expert\n\n## Purpose\n\nProvide comprehensive guidance on the Aptos Framework (0x1 address) - the standard library of core modules that power Aptos blockchain. These modules provide fundamental functionality for accounts, storage, events, randomness, and more.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Framework Modules** - 0x1::*, aptos_framework::*, standard library\n- **Account Management** - account creation, auth keys, rotation\n- **Storage** - Table, SimpleMap, SmartTable, efficient data structures\n- **Events** - event emission, event handles, indexing\n- **Randomness** - VRF, secure random numbers\n- **Time** - timestamp, block time access\n- **Resources** - resource accounts, deterministic addresses\n- **Aggregator** - parallel execution primitives\n\n## Framework Architecture\n\n### Core Framework Modules (0x1::)\n\n```\naptos_framework/\n├── account.move           - Account management\n├── aptos_account.move     - High-level account operations\n├── aptos_coin.move        - Native APT token\n├── aptos_governance.move  - On-chain governance\n├── coin.move              - Fungible token standard (v1)\n├── fungible_asset.move    - Fungible asset standard (v2)\n├── object.move            - Object model primitives\n├── timestamp.move         - Block timestamp access\n├── table.move             - Key-value storage\n├── smart_table.move       - Auto-split table\n├── event.move             - Event emission\n├── randomness.move        - Secure randomness\n├── aggregator.move        - Parallel execution\n├── aggregator_v2.move     - Improved aggregator\n├── resource_account.move  - Deterministic deployment\n├── transaction_fee.move   - Fee collection\n└── staking_contract.move  - Validator staking\n```\n\n### Standard Library (std::)\n\n```\nmove-stdlib/\n├── vector.move      - Dynamic arrays\n├── option.move      - Optional values\n├── string.move      - UTF8 strings\n├── signer.move      - Signer operations\n├── error.move       - Error codes\n├── bcs.move         - Binary serialization\n├── hash.move        - Cryptographic hashing\n└── fixed_point64.move - Fixed-point math\n```\n\n## account.move - Account Management\n\n### Core Functions\n\n```move\nuse aptos_framework::account;\n\n// Create new account at address\npublic fun create_account(new_address: address) {\n    account::create_account(new_address);\n}\n\n// Get account's sequence number\npublic fun get_sequence_number(addr: address): u64 {\n    account::get_sequence_number(addr)\n}\n\n// Get authentication key\npublic fun get_authentication_key(addr: address): vector<u8> {\n    account::get_authentication_key(addr)\n}\n\n// Check if account exists\npublic fun exists_at(addr: address): bool {\n    account::exists_at(addr)\n}\n```\n\n### Account Rotation\n\n```move\n// Rotate authentication key\npublic entry fun rotate_authentication_key(\n    account: &signer,\n    new_auth_key: vector<u8>\n) {\n    account::rotate_authentication_key(account, new_auth_key);\n}\n\n// Offer rotation capability to another address\npublic entry fun offer_rotation_capability(\n    account: &signer,\n    rotation_capability_offerer: address\n) {\n    account::offer_rotation_capability(\n        account,\n        rotation_capability_offerer,\n        vector::empty()\n    );\n}\n```\n\n### SignerCapability Pattern\n\n```move\nuse aptos_framework::account::{Self, SignerCapability};\n\nstruct ModuleData has key {\n    signer_cap: SignerCapability\n}\n\npublic fun initialize(deployer: &signer) {\n    let (resource_signer, signer_cap) = account::create_resource_account(\n        deployer,\n        b\"SEED\"\n    );\n\n    move_to(&resource_signer, ModuleData { signer_cap });\n}\n\npublic fun use_resource_account() acquires ModuleData {\n    let module_data = borrow_global<ModuleData>(@my_module);\n    let resource_signer = account::create_signer_with_capability(&module_data.signer_cap);\n\n    // Use resource_signer for operations\n}\n```\n\n## table.move - Scalable Key-Value Storage\n\n### Basic Table Operations\n\n```move\nuse aptos_framework::table::{Self, Table};\n\nstruct Registry has key {\n    data: Table<address, UserData>\n}\n\npublic fun initialize(account: &signer) {\n    move_to(account, Registry {\n        data: table::new()\n    });\n}\n\npublic fun add_user(\n    registry_addr: address,\n    user_addr: address,\n    user_data: UserData\n) acquires Registry {\n    let registry = borrow_global_mut<Registry>(registry_addr);\n    table::add(&mut registry.data, user_addr, user_data);\n}\n\npublic fun get_user(\n    registry_addr: address,\n    user_addr: address\n): &UserData acquires Registry {\n    let registry = borrow_global<Registry>(registry_addr);\n    table::borrow(&registry.data, user_addr)\n}\n\npublic fun update_user(\n    registry_addr: address,\n    user_addr: address\n): &mut UserData acquires Registry {\n    let registry = borrow_global_mut<Registry>(registry_addr);\n    table::borrow_mut(&mut registry.data, user_addr)\n}\n\npublic fun remove_user(\n    registry_addr: address,\n    user_addr: address\n): UserData acquires Registry {\n    let registry = borrow_global_mut<Registry>(registry_addr);\n    table::remove(&mut registry.data, user_addr)\n}\n\npublic fun has_user(\n    registry_addr: address,\n    user_addr: address\n): bool acquires Registry {\n    let registry = borrow_global<Registry>(registry_addr);\n    table::contains(&registry.data, user_addr)\n}\n```\n\n### Table vs SimpleMap vs SmartTable\n\n| Feature | Vector | SimpleMap | Table | SmartTable |\n|---------|--------|-----------|-------|------------|\n| Max size | ~1000 | ~1000 | Unlimited | Unlimited |\n| Gas cost (read) | O(n) | O(n) | O(1) | O(1) |\n| Gas cost (write) | O(n) | O(n) | O(1) | O(1) |\n| Storage | On-chain | On-chain | Global storage | Global + auto-split |\n| Iteration | ✅ Easy | ✅ Easy | ❌ Not supported | ⚠️ Complex |\n| Best for | Small lists | Small maps | Large maps | Very large maps |\n\n### SmartTable (Auto-Splitting)\n\n```move\nuse aptos_framework::smart_table::{Self, SmartTable};\n\nstruct LargeRegistry has key {\n    data: SmartTable<address, UserData>\n}\n\npublic fun initialize(account: &signer) {\n    move_to(account, LargeRegistry {\n        data: smart_table::new()\n    });\n}\n\n// Same API as Table\npublic fun add_user(addr: address, data: UserData) acquires LargeRegistry {\n    let registry = borrow_global_mut<LargeRegistry>(@my_module);\n    smart_table::add(&mut registry.data, addr, data);\n}\n\n// SmartTable automatically splits when buckets get large\n// Better for very large datasets (100k+ entries)\n```\n\n## event.move - Event Emission\n\n### Event Handles (V1)\n\n```move\nuse aptos_framework::event::{Self, EventHandle};\n\nstruct TransferEvent has drop, store {\n    from: address,\n    to: address,\n    amount: u64,\n}\n\nstruct Events has key {\n    transfer_events: EventHandle<TransferEvent>\n}\n\npublic fun initialize(account: &signer) {\n    move_to(account, Events {\n        transfer_events: account::new_event_handle<TransferEvent>(account)\n    });\n}\n\npublic fun emit_transfer(\n    from: address,\n    to: address,\n    amount: u64\n) acquires Events {\n    let events = borrow_global_mut<Events>(@my_module);\n    event::emit_event(&mut events.transfer_events, TransferEvent {\n        from,\n        to,\n        amount,\n    });\n}\n```\n\n### Event API (V2 - Recommended)\n\n```move\nuse aptos_framework::event;\n\n#[event]\nstruct TransferEvent has drop, store {\n    from: address,\n    to: address,\n    amount: u64,\n}\n\npublic fun transfer(from: address, to: address, amount: u64) {\n    // Direct emission (no EventHandle needed!)\n    event::emit(TransferEvent { from, to, amount });\n}\n```\n\n**Event V2 Advantages:**\n- No EventHandle management\n- Cleaner code\n- Better indexing support\n- Automatic event routing\n\n## timestamp.move - Block Time\n\n```move\nuse aptos_framework::timestamp;\n\npublic fun get_current_time(): u64 {\n    timestamp::now_seconds()\n}\n\npublic fun get_current_time_microseconds(): u64 {\n    timestamp::now_microseconds()\n}\n\n// Time-based logic\npublic fun is_expired(deadline: u64): bool {\n    timestamp::now_seconds() >= deadline\n}\n\npublic fun create_with_deadline(duration: u64): u64 {\n    timestamp::now_seconds() + duration\n}\n```\n\n**Important:** Block timestamp is set by validators, can have small drift.\n\n## randomness.move - Secure Randomness\n\n### VRF-based Random Numbers\n\n```move\nuse aptos_framework::randomness;\n\n#[randomness]\npublic entry fun random_mint(user: &signer) {\n    let random_value = randomness::u64_integer();\n\n    let rarity = if (random_value % 100 < 1) {\n        // 1% chance - legendary\n        3\n    } else if (random_value % 100 < 10) {\n        // 9% chance - rare\n        2\n    } else {\n        // 90% chance - common\n        1\n    };\n\n    mint_nft(user, rarity);\n}\n\n// Random in range\n#[randomness]\npublic entry fun random_reward(user: &signer) {\n    let amount = randomness::u64_range(100, 1000); // 100 to 999\n    transfer_reward(user, amount);\n}\n```\n\n**Requirements:**\n- Must use `#[randomness]` attribute\n- Must be entry function\n- Only works on-chain (not in view functions)\n\n### Random Bytes\n\n```move\n#[randomness]\npublic entry fun random_selection() {\n    let random_bytes = randomness::bytes(32); // 32 random bytes\n    // Use for cryptographic purposes\n}\n```\n\n## resource_account.move - Deterministic Deployment\n\n### Creating Resource Accounts\n\n```move\nuse aptos_framework::resource_account;\nuse aptos_framework::account;\n\npublic fun create_resource_acct(deployer: &signer) {\n    let seed = b\"MY_RESOURCE\";\n\n    // Create resource account\n    let (resource_signer, signer_cap) = account::create_resource_account(\n        deployer,\n        seed\n    );\n\n    // Resource account address is deterministic:\n    // hash(deployer_address, seed)\n    let resource_addr = signer::address_of(&resource_signer);\n\n    // Store signer capability to use later\n    move_to(&resource_signer, ResourceData {\n        signer_cap\n    });\n}\n```\n\n### Use Cases for Resource Accounts\n\n1. **Module Storage** - Store module data at predictable address\n2. **Liquidity Pools** - Each pool at deterministic address\n3. **Protocol Treasuries** - Controlled programmatically\n4. **Registry Systems** - Well-known addresses\n\n```move\n// Example: Liquidity Pool at deterministic address\npublic fun create_pool<X, Y>(deployer: &signer) {\n    let seed = b\"POOL_\";\n    vector::append(&mut seed, type_name<X>());\n    vector::append(&mut seed, b\"_\");\n    vector::append(&mut seed, type_name<Y>());\n\n    let (pool_signer, signer_cap) = account::create_resource_account(\n        deployer,\n        seed\n    );\n\n    move_to(&pool_signer, Pool<X, Y> {\n        reserve_x: 0,\n        reserve_y: 0,\n        signer_cap,\n    });\n}\n```\n\n## aggregator_v2.move - Parallel Execution\n\n### Aggregators for Concurrent Modification\n\n```move\nuse aptos_framework::aggregator_v2::{Self, Aggregator};\n\nstruct Stats has key {\n    total_users: Aggregator<u64>,\n    total_volume: Aggregator<u64>,\n}\n\npublic fun initialize(account: &signer) {\n    move_to(account, Stats {\n        total_users: aggregator_v2::create_aggregator(0),\n        total_volume: aggregator_v2::create_aggregator(0),\n    });\n}\n\npublic fun increment_users() acquires Stats {\n    let stats = borrow_global_mut<Stats>(@my_module);\n    aggregator_v2::add(&mut stats.total_users, 1);\n}\n\npublic fun add_volume(amount: u64) acquires Stats {\n    let stats = borrow_global_mut<Stats>(@my_module);\n    aggregator_v2::add(&mut stats.total_volume, amount);\n}\n\npublic fun get_total_users(): u64 acquires Stats {\n    let stats = borrow_global<Stats>(@my_module);\n    aggregator_v2::read(&stats.total_users)\n}\n```\n\n**Why Use Aggregators:**\n- Enable parallel transaction execution\n- Multiple transactions can increment same aggregator concurrently\n- No conflicts/retries like regular u64 fields\n- **Critical for high-throughput protocols**\n\n## option.move - Optional Values\n\n```move\nuse std::option::{Self, Option};\n\nstruct Profile has key {\n    name: String,\n    bio: Option<String>,  // Optional field\n}\n\npublic fun create_profile(account: &signer, name: String) {\n    move_to(account, Profile {\n        name,\n        bio: option::none()  // No bio initially\n    });\n}\n\npublic fun set_bio(account: &signer, bio: String) acquires Profile {\n    let addr = signer::address_of(account);\n    let profile = borrow_global_mut<Profile>(addr);\n\n    if (option::is_some(&profile.bio)) {\n        // Update existing bio\n        *option::borrow_mut(&mut profile.bio) = bio;\n    } else {\n        // Set bio for first time\n        option::fill(&mut profile.bio, bio);\n    }\n}\n\npublic fun get_bio(addr: address): Option<String> acquires Profile {\n    let profile = borrow_global<Profile>(addr);\n    option::clone(&profile.bio)\n}\n\n// Using option value\npublic fun print_bio(addr: address) acquires Profile {\n    let bio_opt = get_bio(addr);\n    if (option::is_some(&bio_opt)) {\n        let bio = option::extract(&mut bio_opt);\n        // Use bio\n    } else {\n        // No bio set\n    }\n}\n```\n\n## string.move - UTF8 Strings\n\n```move\nuse std::string::{Self, String};\n\npublic fun create_message(): String {\n    string::utf8(b\"Hello, Aptos!\")\n}\n\npublic fun concatenate(s1: String, s2: String): String {\n    let mut result = s1;\n    string::append(&mut result, s2);\n    result\n}\n\npublic fun substring(s: &String, start: u64, end: u64): String {\n    string::sub_string(s, start, end)\n}\n\npublic fun string_length(s: &String): u64 {\n    string::length(s)\n}\n\n// String to bytes\npublic fun to_bytes(s: &String): vector<u8> {\n    *string::bytes(s)\n}\n```\n\n## vector.move - Dynamic Arrays\n\n```move\nuse std::vector;\n\npublic fun vector_operations() {\n    let mut v = vector::empty<u64>();\n\n    // Add elements\n    vector::push_back(&mut v, 10);\n    vector::push_back(&mut v, 20);\n    vector::push_back(&mut v, 30);\n\n    // Get length\n    let len = vector::length(&v);  // 3\n\n    // Access elements\n    let first = *vector::borrow(&v, 0);  // 10\n\n    // Modify elements\n    let second = vector::borrow_mut(&mut v, 1);\n    *second = 25;\n\n    // Remove element\n    let last = vector::pop_back(&mut v);  // 30\n\n    // Check if contains\n    let has_ten = vector::contains(&v, &10);  // true\n\n    // Find index\n    let (found, index) = vector::index_of(&v, &25);\n\n    // Reverse\n    vector::reverse(&mut v);\n\n    // Append another vector\n    vector::append(&mut v, vector[40, 50]);\n\n    // Remove and return element\n    let removed = vector::remove(&mut v, 0);\n\n    // Swap elements\n    vector::swap(&mut v, 0, 1);\n}\n```\n\n## Common Patterns\n\n### Pattern 1: Registry with Table\n\n```move\nuse aptos_framework::table::{Self, Table};\n\nstruct Registry<K: copy + drop, V: store> has key {\n    data: Table<K, V>,\n    count: u64,\n}\n\npublic fun initialize<K: copy + drop, V: store>(account: &signer) {\n    move_to(account, Registry<K, V> {\n        data: table::new(),\n        count: 0,\n    });\n}\n\npublic fun register<K: copy + drop, V: store>(\n    registry_addr: address,\n    key: K,\n    value: V\n) acquires Registry {\n    let registry = borrow_global_mut<Registry<K, V>>(registry_addr);\n    assert!(!table::contains(&registry.data, key), ERROR_ALREADY_EXISTS);\n\n    table::add(&mut registry.data, key, value);\n    registry.count = registry.count + 1;\n}\n```\n\n### Pattern 2: Event-Driven State Changes\n\n```move\n#[event]\nstruct StateChanged has drop, store {\n    old_state: u8,\n    new_state: u8,\n    timestamp: u64,\n}\n\npublic fun change_state(new_state: u8) acquires State {\n    let state = borrow_global_mut<State>(@my_module);\n    let old = state.value;\n\n    state.value = new_state;\n\n    event::emit(StateChanged {\n        old_state: old,\n        new_state,\n        timestamp: timestamp::now_seconds(),\n    });\n}\n```\n\n### Pattern 3: Time-Locked Operations\n\n```move\nstruct TimeLock has key {\n    unlock_time: u64,\n    amount: u64,\n}\n\npublic fun create_timelock(\n    account: &signer,\n    amount: u64,\n    lock_duration: u64\n) {\n    let unlock_time = timestamp::now_seconds() + lock_duration;\n\n    move_to(account, TimeLock {\n        unlock_time,\n        amount,\n    });\n}\n\npublic fun withdraw(account: &signer) acquires TimeLock {\n    let addr = signer::address_of(account);\n    let timelock = move_from<TimeLock>(addr);\n\n    assert!(\n        timestamp::now_seconds() >= timelock.unlock_time,\n        ERROR_STILL_LOCKED\n    );\n\n    let TimeLock { unlock_time: _, amount } = timelock;\n    // Transfer amount to user\n}\n```\n\n### Pattern 4: Resource Account Pool\n\n```move\nstruct Pool<phantom X, phantom Y> has key {\n    reserve_x: u64,\n    reserve_y: u64,\n    signer_cap: SignerCapability,\n}\n\npublic fun create_pool<X, Y>(creator: &signer) {\n    let seed = b\"POOL\";\n    let (pool_signer, signer_cap) = account::create_resource_account(\n        creator,\n        seed\n    );\n\n    coin::register<X>(&pool_signer);\n    coin::register<Y>(&pool_signer);\n\n    move_to(&pool_signer, Pool<X, Y> {\n        reserve_x: 0,\n        reserve_y: 0,\n        signer_cap,\n    });\n}\n\npublic fun swap<X, Y>(amount_in: u64): u64 acquires Pool {\n    let pool_addr = account::create_resource_address(&@my_module, b\"POOL\");\n    let pool = borrow_global_mut<Pool<X, Y>>(pool_addr);\n\n    // Swap logic using signer_cap for transfers\n    let pool_signer = account::create_signer_with_capability(&pool.signer_cap);\n    // ...\n}\n```\n\n## Framework Module Reference\n\n### Quick Reference Table\n\n| Module | Key Functions | Use Case |\n|--------|--------------|----------|\n| account | create_account, rotate_authentication_key | Account management |\n| coin | transfer, balance, register | Fungible tokens |\n| fungible_asset | mint, burn, transfer | Advanced tokens |\n| object | create_object, transfer | Object model |\n| table | add, borrow, remove | Large key-value stores |\n| smart_table | add, borrow, remove | Very large stores |\n| event | emit, emit_event | Event emission |\n| timestamp | now_seconds | Time access |\n| randomness | u64_integer, bytes | Secure randomness |\n| aggregator_v2 | create, add, read | Parallel execution |\n| resource_account | create_resource_account | Deterministic addresses |\n\n## Best Practices\n\n### ✅ Do\n\n- **Use SmartTable for large datasets** - Better than Table for 100k+ entries\n- **Use Event V2 API** - Simpler than EventHandle\n- **Use Aggregator for counters** - Enables parallel execution\n- **Use resource accounts for protocols** - Deterministic addresses\n- **Check timestamp carefully** - Validator-set, can have drift\n- **Use randomness for fair selection** - VRF-based security\n\n### ❌ Avoid\n\n- **Don't iterate over Tables** - Not supported, use vector/map if needed\n- **Don't trust timestamp for exact timing** - Block-level granularity\n- **Don't use randomness in view functions** - Not supported\n- **Don't forget to handle Option::none** - Check before unwrapping\n- **Don't create too many event handles** - Use Event V2 instead\n\n## Response Style\n\n- **Module-focused** - Reference specific framework modules\n- **Pattern-driven** - Show common framework usage patterns\n- **Performance-aware** - Mention gas implications\n- **Practical** - Real-world examples with framework modules\n- **Reference docs** - Link to specific module documentation\n\n## Follow-up Suggestions\n\nAfter helping with framework modules, suggest:\n- Gas optimization for storage structures\n- Event indexing strategies\n- Parallel execution with aggregators\n- Resource account architectures\n- Time-based protocol designs\n- Random number generation patterns\n",
        "templates/.claude/skills/aptos/gas-optimization/skill.md": "---\nname: aptos-gas-optimization\ndescription: Expert on Aptos gas optimization, performance tuning, storage costs, execution efficiency, inline functions, aggregator usage, parallel execution, table vs vector tradeoffs, and gas profiling tools. Triggers on keywords gas optimization, performance, gas cost, storage fee, inline, aggregator, parallel execution, gas profiling, optimization\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Aptos Gas & Performance Optimization Expert\n\n## Purpose\n\nProvide expert guidance on optimizing gas costs and performance for Aptos smart contracts. Cover storage optimization, execution efficiency, parallel execution enablers, profiling tools, and cost-effective design patterns.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Gas Costs** - gas fees, transaction costs, gas optimization\n- **Performance** - slow transactions, execution time, throughput\n- **Storage** - storage fees, data structures, table vs vector\n- **Optimization** - code optimization, inline functions, efficiency\n- **Parallel Execution** - aggregators, concurrent transactions\n- **Profiling** - gas profiling, benchmarking, analysis\n\n## Gas Model Overview\n\n### Aptos Gas Components\n\n```\nTotal Gas Cost = Execution Gas + Storage Gas + IO Gas\n```\n\n**1. Execution Gas:**\n- Function calls, loops, computations\n- Instruction-level costs\n- Type operations, memory access\n\n**2. Storage Gas:**\n- Per-byte storage cost\n- Write amplification\n- State rent (upcoming)\n\n**3. IO Gas:**\n- Reading from storage\n- Writing to storage\n- Event emission\n\n### Gas Units to APT Conversion\n\n```\nGas Fee (APT) = Gas Units × Gas Unit Price\nGas Unit Price = market-determined (typically 100-1000 octas per gas unit)\n1 APT = 100,000,000 octas\n```\n\n**Example:**\n```\nTransaction uses 1,000 gas units\nGas price = 100 octas/unit\nCost = 1,000 × 100 = 100,000 octas = 0.001 APT\n```\n\n## Storage Optimization\n\n### Data Structure Costs\n\n| Structure | Read Cost | Write Cost | Storage Cost | Best For |\n|-----------|-----------|------------|--------------|----------|\n| u64 | ~10 gas | ~15 gas | 8 bytes | Simple counters |\n| vector<u64>(100) | ~100 gas | ~150 gas | 800 bytes | Small lists |\n| SimpleMap(100) | ~100 gas | ~150 gas | ~1KB | Small maps |\n| Table(100) | ~15 gas | ~20 gas | ~2KB | Medium maps |\n| SmartTable(100) | ~15 gas | ~20 gas | ~2KB | Large maps |\n| Aggregator | ~10 gas | ~12 gas | 32 bytes | **Parallel counters** |\n\n### Choosing the Right Data Structure\n\n```move\n// ❌ Bad: Vector for large datasets\nstruct Registry has key {\n    users: vector<User>  // O(n) search, expensive for 1000+ items\n}\n\n// ✅ Better: Table for large datasets\nstruct Registry has key {\n    users: Table<address, User>  // O(1) lookup, scalable\n}\n\n// ✅ Best: SmartTable for very large datasets\nstruct Registry has key {\n    users: SmartTable<address, User>  // Auto-splitting, 100k+ items\n}\n```\n\n### Storage Pattern: Minimize State\n\n```move\n// ❌ Bad: Storing redundant data\nstruct User has store {\n    address: address,        // Redundant (key already has this)\n    total_deposits: u64,     // Can be calculated\n    deposit_history: vector<u64>,\n    last_update: u64,        // Often unnecessary\n}\n\n// ✅ Good: Minimal state\nstruct User has store {\n    deposit_history: vector<u64>,  // Essential data only\n}\n\n// Calculate total on-demand (no storage cost)\npublic fun get_total_deposits(user: &User): u64 {\n    let mut total = 0;\n    let i = 0;\n    while (i < vector::length(&user.deposit_history)) {\n        total = total + *vector::borrow(&user.deposit_history, i);\n        i = i + 1;\n    };\n    total\n}\n```\n\n### Struct Packing\n\n```move\n// ❌ Bad: Inefficient packing (64 bytes)\nstruct Data has store {\n    flag1: bool,        // 1 byte + 7 padding\n    value1: u64,        // 8 bytes\n    flag2: bool,        // 1 byte + 7 padding\n    value2: u64,        // 8 bytes\n    flag3: bool,        // 1 byte + 7 padding\n    value3: u64,        // 8 bytes\n}\n\n// ✅ Good: Optimized packing (32 bytes)\nstruct Data has store {\n    // Group small fields together\n    flag1: bool,        // 1 byte\n    flag2: bool,        // 1 byte\n    flag3: bool,        // 1 byte\n    // Padding: 5 bytes\n    // Then larger fields\n    value1: u64,        // 8 bytes\n    value2: u64,        // 8 bytes\n    value3: u64,        // 8 bytes\n}\n\n// ✅ Best: Pack flags into single byte\nstruct Data has store {\n    flags: u8,          // All 3 flags in 1 byte (bitwise)\n    value1: u64,\n    value2: u64,\n    value3: u64,\n}\n\npublic fun get_flag1(data: &Data): bool {\n    (data.flags & 0b001) != 0\n}\n\npublic fun set_flag1(data: &mut Data, value: bool) {\n    if (value) {\n        data.flags = data.flags | 0b001;\n    } else {\n        data.flags = data.flags & 0b110;\n    }\n}\n```\n\n## Execution Optimization\n\n### Inline Functions\n\n```move\n// Small, frequently called functions should be inline\ninline fun add(a: u64, b: u64): u64 {\n    a + b\n}\n\ninline fun min(a: u64, b: u64): u64 {\n    if (a < b) a else b\n}\n\ninline fun max(a: u64, b: u64): u64 {\n    if (a > b) a else b\n}\n\npublic fun calculate(): u64 {\n    // Inlined: no function call overhead\n    let sum = add(10, 20);\n    let minimum = min(sum, 50);\n    minimum\n}\n```\n\n**When to inline:**\n- Functions < 5 lines\n- Called frequently\n- Simple computations\n- Math helpers\n- Validation checks\n\n**When NOT to inline:**\n- Large functions (increases code size)\n- Rarely called functions\n- Recursive functions (not supported)\n\n### Loop Optimization\n\n```move\n// ❌ Bad: Inefficient loop\npublic fun sum_vector(v: &vector<u64>): u64 {\n    let mut sum = 0;\n    let mut i = 0;\n    while (i < vector::length(v)) {  // Calls length() every iteration!\n        sum = sum + *vector::borrow(v, i);\n        i = i + 1;\n    };\n    sum\n}\n\n// ✅ Good: Cache length\npublic fun sum_vector(v: &vector<u64>): u64 {\n    let mut sum = 0;\n    let len = vector::length(v);  // Call once\n    let mut i = 0;\n    while (i < len) {\n        sum = sum + *vector::borrow(v, i);\n        i = i + 1;\n    };\n    sum\n}\n\n// ✅ Best: Use built-in functions when available\npublic fun sum_vector(v: &vector<u64>): u64 {\n    vector::fold(v, 0, |acc, x| acc + *x)\n}\n```\n\n### Early Returns\n\n```move\n// ❌ Bad: Unnecessary work\npublic fun validate_and_process(amount: u64, user: address) {\n    let valid = amount > 0 && amount < MAX_AMOUNT && is_whitelisted(user);\n\n    if (valid) {\n        // Expensive operations\n        complex_calculation();\n        update_state();\n        emit_events();\n    }\n}\n\n// ✅ Good: Early return\npublic fun validate_and_process(amount: u64, user: address) {\n    // Check cheapest conditions first\n    if (amount == 0) return;\n    if (amount >= MAX_AMOUNT) return;\n    if (!is_whitelisted(user)) return;\n\n    // Only do expensive work if all checks pass\n    complex_calculation();\n    update_state();\n    emit_events();\n}\n```\n\n### Minimize Global Storage Access\n\n```move\n// ❌ Bad: Multiple borrows\npublic fun update_balance(addr: address, amount: u64) acquires Balance {\n    let balance = borrow_global_mut<Balance>(addr);\n    balance.value = balance.value + amount;\n\n    let balance2 = borrow_global_mut<Balance>(addr);  // Second borrow!\n    balance2.last_update = timestamp::now_seconds();\n}\n\n// ✅ Good: Single borrow\npublic fun update_balance(addr: address, amount: u64) acquires Balance {\n    let balance = borrow_global_mut<Balance>(addr);\n    balance.value = balance.value + amount;\n    balance.last_update = timestamp::now_seconds();\n}\n```\n\n### Batch Operations\n\n```move\n// ❌ Bad: Individual operations\npublic entry fun transfer_to_many(\n    sender: &signer,\n    recipients: vector<address>,\n    amounts: vector<u64>\n) {\n    let i = 0;\n    while (i < vector::length(&recipients)) {\n        transfer(sender, *vector::borrow(&recipients, i), *vector::borrow(&amounts, i));\n        i = i + 1;\n    }\n}\n\n// ✅ Good: Batched operation (single transaction)\npublic entry fun batch_transfer(\n    sender: &signer,\n    recipients: vector<address>,\n    amounts: vector<u64>\n) acquires Balance {\n    let sender_addr = signer::address_of(sender);\n    let sender_balance = borrow_global_mut<Balance>(sender_addr);\n\n    let mut i = 0;\n    let len = vector::length(&recipients);\n\n    // Calculate total first\n    let mut total = 0;\n    while (i < len) {\n        total = total + *vector::borrow(&amounts, i);\n        i = i + 1;\n    };\n\n    assert!(sender_balance.value >= total, ERROR_INSUFFICIENT_BALANCE);\n\n    // Single deduction from sender\n    sender_balance.value = sender_balance.value - total;\n\n    // Batch credit recipients\n    i = 0;\n    while (i < len) {\n        let recipient = *vector::borrow(&recipients, i);\n        let amount = *vector::borrow(&amounts, i);\n\n        let recipient_balance = borrow_global_mut<Balance>(recipient);\n        recipient_balance.value = recipient_balance.value + amount;\n\n        i = i + 1;\n    }\n}\n```\n\n## Parallel Execution with Aggregators\n\n### Understanding Aggregators\n\n**Problem:** Traditional counter creates conflicts\n```move\n// ❌ Bad: Conflicts on concurrent access\nstruct Stats has key {\n    total_users: u64  // Multiple txns modifying = conflict!\n}\n\npublic fun register_user() acquires Stats {\n    let stats = borrow_global_mut<Stats>(@protocol);\n    stats.total_users = stats.total_users + 1;\n    // If 2 txns do this simultaneously, one must retry\n}\n```\n\n**Solution:** Aggregators enable parallel updates\n```move\n// ✅ Good: No conflicts!\nuse aptos_framework::aggregator_v2::{Self, Aggregator};\n\nstruct Stats has key {\n    total_users: Aggregator<u64>  // Concurrent-safe!\n}\n\npublic fun register_user() acquires Stats {\n    let stats = borrow_global_mut<Stats>(@protocol);\n    aggregator_v2::add(&mut stats.total_users, 1);\n    // Multiple txns can do this in parallel!\n}\n```\n\n### When to Use Aggregators\n\n**✅ Use aggregators for:**\n- Global counters (total users, total volume)\n- Protocol-level statistics\n- Supply tracking\n- Frequently updated metrics\n\n**❌ Don't use aggregators for:**\n- Per-user balances (no conflicts)\n- Rarely updated values\n- Values that need exact reads mid-transaction\n\n### Aggregator Patterns\n\n```move\nuse aptos_framework::aggregator_v2::{Self, Aggregator};\n\nstruct Protocol has key {\n    // High-throughput stats\n    total_swaps: Aggregator<u64>,\n    total_volume: Aggregator<u128>,\n    active_pools: Aggregator<u64>,\n\n    // Low-throughput data (use regular fields)\n    admin: address,\n    fee_rate: u64,\n}\n\npublic fun initialize(deployer: &signer) {\n    move_to(deployer, Protocol {\n        total_swaps: aggregator_v2::create_aggregator(0),\n        total_volume: aggregator_v2::create_aggregator(0),\n        active_pools: aggregator_v2::create_aggregator(0),\n        admin: signer::address_of(deployer),\n        fee_rate: 30,  // 0.3%\n    });\n}\n\npublic fun record_swap(volume: u128) acquires Protocol {\n    let protocol = borrow_global_mut<Protocol>(@my_protocol);\n\n    // Parallel-safe increments\n    aggregator_v2::add(&mut protocol.total_swaps, 1);\n    aggregator_v2::add(&mut protocol.total_volume, volume);\n}\n\n// Reading aggregator value\npublic fun get_total_volume(): u128 acquires Protocol {\n    let protocol = borrow_global<Protocol>(@my_protocol);\n    aggregator_v2::read(&protocol.total_volume)\n}\n```\n\n## Event Optimization\n\n### Event V1 vs V2 Costs\n\n```move\n// ❌ Expensive: Event V1 (requires EventHandle)\nuse aptos_framework::event::{Self, EventHandle};\n\nstruct Events has key {\n    transfer_events: EventHandle<TransferEvent>  // Storage overhead\n}\n\npublic fun emit_v1() acquires Events {\n    let events = borrow_global_mut<Events>(@module);\n    event::emit_event(&mut events.transfer_events, TransferEvent {});\n    // Higher gas: borrow_global_mut + emit_event\n}\n\n// ✅ Cheap: Event V2 (direct emission)\n#[event]\nstruct TransferEvent has drop, store {\n    from: address,\n    to: address,\n    amount: u64,\n}\n\npublic fun emit_v2() {\n    event::emit(TransferEvent { from: @0x1, to: @0x2, amount: 100 });\n    // Lower gas: direct emission\n}\n```\n\n### Event Size Optimization\n\n```move\n// ❌ Bad: Large event payload\n#[event]\nstruct DetailedEvent has drop, store {\n    user_address: address,\n    user_name: String,           // Expensive!\n    full_history: vector<u64>,   // Very expensive!\n    timestamp: u64,\n    metadata: vector<u8>,        // Expensive!\n}\n\n// ✅ Good: Minimal event payload\n#[event]\nstruct OptimizedEvent has drop, store {\n    user: address,        // Just the address\n    amount: u64,          // Essential data only\n    event_type: u8,       // Use codes instead of strings\n}\n\n// Off-chain can look up details using user address\n```\n\n## Gas Profiling Tools\n\n### Using CLI Gas Profiler\n\n```bash\n# Run tests with gas profiling\naptos move test --gas\n\n# Output shows gas usage per function\nRunning Move unit tests\n[ PASS    ] 0x1::my_module::test_transfer\nGas used: 1,234 gas units\n\n# Detailed gas profile\naptos move test --gas --verbose\n```\n\n### Simulation and Benchmarking\n\n```bash\n# Simulate transaction locally\naptos move run \\\n  --function-id 0x1::my_module::my_function \\\n  --args address:0x123 u64:1000 \\\n  --profile gas\n\n# Output:\n# Gas used: 2,500 units\n# Storage: +120 bytes\n# Estimated cost: 0.0025 APT\n```\n\n### In-Code Gas Assertions\n\n```move\n#[test]\nfun test_gas_usage() {\n    let gas_before = aptos_framework::transaction_context::get_gas_used();\n\n    // Operation to test\n    expensive_function();\n\n    let gas_after = aptos_framework::transaction_context::get_gas_used();\n    let gas_used = gas_after - gas_before;\n\n    assert!(gas_used < 1000, 0);  // Assert max gas\n}\n```\n\n## Advanced Optimization Patterns\n\n### Pattern 1: Lazy Initialization\n\n```move\n// ❌ Bad: Eager initialization (storage cost upfront)\npublic fun register_user(account: &signer) {\n    move_to(account, UserData {\n        balance: 0,\n        rewards: 0,\n        history: vector::empty(),\n        config: default_config(),  // Created but might never be used\n    });\n}\n\n// ✅ Good: Lazy initialization\npublic fun register_user(account: &signer) {\n    move_to(account, UserData {\n        balance: 0,\n        rewards: 0,\n        history: vector::empty(),\n        config: option::none(),  // Only create when needed\n    });\n}\n\npublic fun get_or_create_config(user: &mut UserData): &mut Config {\n    if (option::is_none(&user.config)) {\n        option::fill(&mut user.config, default_config());\n    };\n    option::borrow_mut(&mut user.config)\n}\n```\n\n### Pattern 2: Bitmap Flags\n\n```move\n// ❌ Bad: Multiple boolean fields (8 bytes)\nstruct Permissions has store {\n    can_mint: bool,      // 1 byte + padding\n    can_burn: bool,      // 1 byte + padding\n    can_freeze: bool,    // 1 byte + padding\n    can_transfer: bool,  // 1 byte + padding\n    can_update: bool,    // 1 byte + padding\n}\n\n// ✅ Good: Bitmap (1 byte)\nstruct Permissions has store {\n    flags: u8  // All 5 flags in 1 byte\n}\n\nconst FLAG_MINT: u8 = 0b00001;\nconst FLAG_BURN: u8 = 0b00010;\nconst FLAG_FREEZE: u8 = 0b00100;\nconst FLAG_TRANSFER: u8 = 0b01000;\nconst FLAG_UPDATE: u8 = 0b10000;\n\npublic fun has_permission(perms: &Permissions, flag: u8): bool {\n    (perms.flags & flag) != 0\n}\n\npublic fun set_permission(perms: &mut Permissions, flag: u8, value: bool) {\n    if (value) {\n        perms.flags = perms.flags | flag;\n    } else {\n        perms.flags = perms.flags & !flag;\n    }\n}\n```\n\n### Pattern 3: Merkle Proof Verification (Off-Chain Heavy)\n\n```move\n// Instead of storing whitelist on-chain\n// ❌ Bad: Store entire whitelist (expensive!)\nstruct Whitelist has key {\n    addresses: vector<address>  // 10,000 addresses = huge storage!\n}\n\n// ✅ Good: Store only Merkle root (32 bytes)\nstruct Whitelist has key {\n    merkle_root: vector<u8>  // 32 bytes\n}\n\npublic fun verify_whitelisted(\n    user: address,\n    proof: vector<vector<u8>>\n): bool acquires Whitelist {\n    let whitelist = borrow_global<Whitelist>(@module);\n    verify_merkle_proof(user, proof, &whitelist.merkle_root)\n}\n\n// User provides proof off-chain, we verify on-chain\n```\n\n## Cost Comparison Table\n\n| Operation | Gas Cost (approx) | Notes |\n|-----------|------------------|-------|\n| u64 addition | 1 | Primitive op |\n| u64 multiplication | 2 | Primitive op |\n| Vector push_back | 5-10 | Depends on size |\n| Table lookup | 10-15 | O(1) access |\n| SmartTable lookup | 10-15 | O(1) access |\n| borrow_global | 20-50 | Depends on resource size |\n| move_to | 50-200 | Depends on resource size |\n| Event emission (V2) | 50-100 | Per event |\n| Event emission (V1) | 100-200 | Higher overhead |\n| String operation | 10-50 | Depends on length |\n| Cryptographic hash | 100-500 | SHA256, etc |\n\n## Best Practices Summary\n\n### ✅ Do\n\n- **Use aggregators for global counters** - Enable parallel execution\n- **Use inline for small functions** - Reduce call overhead\n- **Cache vector lengths in loops** - Avoid repeated calls\n- **Use Event V2** - Cheaper than V1\n- **Batch operations** - Single transaction vs multiple\n- **Minimize storage** - Store only essential data\n- **Use Table/SmartTable for large datasets** - Not vectors\n- **Pack booleans into bitmaps** - Save storage space\n- **Profile with --gas flag** - Measure before optimizing\n\n### ❌ Avoid\n\n- **Don't iterate over Tables** - Not supported\n- **Don't store redundant data** - Calculate on-demand\n- **Don't use large event payloads** - Keep events minimal\n- **Don't access global storage repeatedly** - Borrow once\n- **Don't use vector for large datasets** - Use Table/SmartTable\n- **Don't inline large functions** - Increases code size\n- **Don't optimize prematurely** - Profile first\n\n## Gas Optimization Checklist\n\nBefore deploying to mainnet:\n\n- [ ] Run `aptos move test --gas` and review gas usage\n- [ ] Use aggregators for all global counters\n- [ ] Inline small helper functions\n- [ ] Use Event V2 for all events\n- [ ] Use Table/SmartTable for large datasets\n- [ ] Minimize struct sizes (pack fields efficiently)\n- [ ] Cache loop lengths and frequently accessed values\n- [ ] Batch operations where possible\n- [ ] Use bitmap flags instead of multiple booleans\n- [ ] Store Merkle roots instead of full lists\n- [ ] Early returns for validation logic\n- [ ] Minimize global storage access\n\n## Response Style\n\n- **Measure-first** - Always profile before optimizing\n- **Pattern-driven** - Show before/after optimization patterns\n- **Cost-aware** - Mention gas implications explicitly\n- **Practical** - Real-world optimization examples\n- **Tool-focused** - Reference profiling commands\n\n## Follow-up Suggestions\n\nAfter helping with gas optimization, suggest:\n- Profile specific functions with --gas flag\n- Implement aggregators for high-throughput paths\n- Review storage structure efficiency\n- Consider parallel execution opportunities\n- Benchmark against similar protocols\n- Set up gas regression tests\n",
        "templates/.claude/skills/aptos/move-language/skill.md": "---\nname: aptos-move-language\ndescription: Expert on Move programming language - abilities (copy/drop/store/key), generics, phantom types, references, global storage operations, signer pattern, visibility modifiers, friend functions, inline optimization, and advanced type system. Triggers on keywords move language, abilities, generics, phantom type, borrow global, signer, friend, inline, type parameter\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Move Language Expert\n\n## Purpose\n\nProvide deep expertise on the Move programming language, focusing on its unique type system, abilities, generics, resource safety, and Aptos-specific features. Move is designed for safe digital asset programming with linear types and formal verification support.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Abilities** - copy, drop, store, key, ability constraints\n- **Generics** - type parameters, phantom types, constraints\n- **References** - borrowing, &T, &mut T, borrow_global\n- **Global Storage** - move_to, move_from, exists, borrow_global_mut\n- **Signer** - authentication, signer pattern, access control\n- **Visibility** - public, public(friend), entry, private\n- **Advanced** - inline, friend functions, spec blocks\n\n## Move Language Fundamentals\n\n### Type System Overview\n\nMove is a **statically typed, compiled language** with:\n- **Linear types** (resources can't be copied or dropped arbitrarily)\n- **Generics** with ability constraints\n- **No null/undefined** - explicit Option<T>\n- **No dynamic dispatch** - all calls resolved at compile time\n- **Memory safety** without garbage collection\n\n### Primitive Types\n\n```move\n// Integers\nlet x: u8 = 255;         // 0 to 255\nlet y: u16 = 65535;      // 0 to 65,535\nlet z: u32 = 4294967295; // 0 to 4,294,967,295\nlet w: u64 = 18446744073709551615; // 0 to 2^64-1\nlet v: u128 = 340282366920938463463374607431768211455; // 0 to 2^128-1\nlet t: u256 = 115792089237316195423570985008687907853269984665640564039457584007913129639935; // 0 to 2^256-1\n\n// Boolean\nlet flag: bool = true;\n\n// Address\nlet addr: address = @0x1;\nlet named: address = @aptos_framework;\n\n// Vectors\nlet nums: vector<u64> = vector[1, 2, 3];\nlet empty: vector<address> = vector::empty();\n```\n\n### No Implicit Conversions\n\n```move\nlet x: u8 = 10;\nlet y: u64 = 20;\n\n// ❌ Error: type mismatch\n// let z = x + y;\n\n// ✅ Correct: explicit casting\nlet z = (x as u64) + y;\n```\n\n## Abilities - Move's Type Superpowers\n\n### The Four Abilities\n\n```move\nstruct Resource has key, store {\n    value: u64\n}\n// key:   Can be stored in global storage as a top-level resource\n// store: Can be stored inside other structs\n// copy:  Can be copied (duplicated)\n// drop:  Can be dropped/discarded\n```\n\n### Ability Semantics\n\n| Ability | Meaning | Example Use Case |\n|---------|---------|------------------|\n| `copy` | Type can be copied by value | Primitives, small configs |\n| `drop` | Type can be discarded | References, temporary data |\n| `store` | Can be stored in structs/global storage | Most data types |\n| `key` | Can be top-level resource in global storage | Account resources, NFTs |\n\n### Ability Constraints\n\n```move\n// No abilities - can't copy, drop, store, or use as resource\nstruct Capability {}\n\n// Only store - can be inside structs, but not global storage\nstruct InnerData has store {\n    value: u64\n}\n\n// store + key - can be global resource\nstruct Account has store, key {\n    balance: u64,\n    inner: InnerData,  // ✅ InnerData has store\n}\n\n// copy + drop + store - behaves like primitive\nstruct Point has copy, drop, store {\n    x: u64,\n    y: u64,\n}\n```\n\n### Critical Rules\n\n**Rule 1:** Fields must have compatible abilities\n```move\n// ❌ ERROR: InnerData doesn't have 'key'\nstruct Account has key {\n    data: InnerData  // InnerData needs 'store' at minimum\n}\n\n// ✅ CORRECT\nstruct Account has key {\n    data: InnerData  // InnerData has 'store'\n}\n```\n\n**Rule 2:** Structs without `drop` must be explicitly handled\n```move\nstruct NoDrop has store, key {\n    value: u64\n}\n\nfun use_no_drop(nd: NoDrop) {\n    // ❌ ERROR: Can't drop NoDrop\n    // Function ends and nd is dropped implicitly\n}\n\nfun use_no_drop_correct(nd: NoDrop) {\n    // ✅ Must explicitly destructure or store\n    let NoDrop { value: _ } = nd;  // Unpack and drop fields\n}\n```\n\n**Rule 3:** `copy` requires all fields to have `copy`\n```move\nstruct NotCopyable has store {\n    x: u64\n}\n\n// ❌ ERROR: Can't have copy because NotCopyable doesn't\nstruct Container has copy {\n    inner: NotCopyable\n}\n\n// ✅ CORRECT\nstruct Container has store {\n    inner: NotCopyable\n}\n```\n\n## Generics and Type Parameters\n\n### Basic Generics\n\n```move\nstruct Box<T> has store {\n    value: T\n}\n\nstruct Pair<T1, T2> has store {\n    first: T1,\n    second: T2,\n}\n\npublic fun create_box<T: store>(value: T): Box<T> {\n    Box { value }\n}\n```\n\n### Ability Constraints on Type Parameters\n\n```move\n// T must have 'store' ability\npublic fun store_in_box<T: store>(value: T): Box<T> {\n    Box { value }\n}\n\n// T must have 'copy + drop'\npublic fun duplicate<T: copy + drop>(value: T): (T, T) {\n    (value, copy value)\n}\n\n// T must have all abilities\npublic fun full_featured<T: copy + drop + store + key>(value: T) {\n    // Can do anything with T\n}\n\n// No constraints (very limited - can only pass around)\npublic fun unconstrained<T>(value: T): T {\n    value  // Can't copy, can't drop, can't store\n}\n```\n\n### Phantom Type Parameters\n\nPhantom types don't appear in struct fields but affect type safety:\n\n```move\nstruct Coin<phantom CoinType> has store {\n    value: u64  // CoinType doesn't appear here!\n}\n\nstruct BTC {}\nstruct ETH {}\n\n// These are different types!\nlet btc: Coin<BTC> = Coin { value: 100 };\nlet eth: Coin<ETH> = Coin { value: 50 };\n\n// ❌ ERROR: Type mismatch\n// let mixed = btc + eth;\n```\n\n**Why Phantom Types?**\n- Zero runtime overhead (erased at compile time)\n- Type-level guarantees (can't mix BTC and ETH)\n- Ability inheritance doesn't require CoinType to have abilities\n\n```move\n// Even if SomeCoin doesn't have 'store', Coin<SomeCoin> can have it\nstruct SomeCoin {}  // No abilities!\n\nstruct Coin<phantom CoinType> has store {\n    value: u64\n}\n\n// ✅ Works! Phantom type doesn't affect abilities\nlet coin: Coin<SomeCoin> = Coin { value: 100 };\n```\n\n### Multiple Type Parameters\n\n```move\nstruct Pool<phantom X, phantom Y> has key {\n    reserve_x: u64,\n    reserve_y: u64,\n    lp_supply: u64,\n}\n\npublic fun create_pool<X, Y>(account: &signer) {\n    move_to(account, Pool<X, Y> {\n        reserve_x: 0,\n        reserve_y: 0,\n        lp_supply: 0,\n    });\n}\n\n// Pool<BTC, ETH> ≠ Pool<ETH, BTC>\n```\n\n## References and Borrowing\n\n### Immutable References (&T)\n\n```move\nfun read_value(x: &u64): u64 {\n    *x  // Dereference to read\n}\n\nlet val = 42;\nlet ref = &val;\nlet copy = *ref;  // copy is 42, val is still 42\n```\n\n### Mutable References (&mut T)\n\n```move\nfun increment(x: &mut u64) {\n    *x = *x + 1;\n}\n\nlet mut val = 42;\nincrement(&mut val);\n// val is now 43\n```\n\n### Reference Rules\n\n1. **Can't have mutable + immutable refs simultaneously**\n```move\nlet mut x = 10;\nlet r1 = &x;\nlet r2 = &mut x;  // ❌ ERROR: Can't have both\n```\n\n2. **Only one mutable reference at a time**\n```move\nlet mut x = 10;\nlet r1 = &mut x;\nlet r2 = &mut x;  // ❌ ERROR: x already borrowed mutably\n```\n\n3. **References can't outlive their values**\n```move\nfun get_ref(): &u64 {\n    let x = 42;\n    &x  // ❌ ERROR: Can't return ref to local variable\n}\n```\n\n### Reference Copying for `copy` Types\n\n```move\nfun copy_from_ref(x: &u64): u64 {\n    *x  // ✅ u64 has copy, so this works\n}\n\nstruct NoCopy has store {}\n\nfun copy_from_ref_no_copy(x: &NoCopy): NoCopy {\n    *x  // ❌ ERROR: NoCopy doesn't have copy ability\n}\n```\n\n## Global Storage Operations\n\n### The Five Global Storage Functions\n\n```move\n// 1. move_to<T> - Store resource at signer's address\npublic fun initialize(account: &signer) {\n    move_to(account, MyResource { value: 0 });\n}\n\n// 2. move_from<T> - Remove and return resource\npublic fun destroy(account: &signer): MyResource {\n    move_from<MyResource>(signer::address_of(account))\n}\n\n// 3. borrow_global<T> - Immutable borrow\npublic fun read_value(addr: address): u64 acquires MyResource {\n    let resource = borrow_global<MyResource>(addr);\n    resource.value\n}\n\n// 4. borrow_global_mut<T> - Mutable borrow\npublic fun update_value(addr: address, new_val: u64) acquires MyResource {\n    let resource = borrow_global_mut<MyResource>(addr);\n    resource.value = new_val;\n}\n\n// 5. exists<T> - Check if resource exists\npublic fun has_resource(addr: address): bool {\n    exists<MyResource>(addr)\n}\n```\n\n### The `acquires` Annotation\n\n**Critical:** Functions that use `borrow_global` or `borrow_global_mut` must declare `acquires`:\n\n```move\nstruct Balance has key {\n    coins: u64\n}\n\n// ✅ Correct\npublic fun get_balance(addr: address): u64 acquires Balance {\n    borrow_global<Balance>(addr).coins\n}\n\n// ❌ ERROR: Missing 'acquires Balance'\npublic fun get_balance_wrong(addr: address): u64 {\n    borrow_global<Balance>(addr).coins\n}\n\n// Multiple acquires\npublic fun transfer(from: address, to: address) acquires Balance {\n    let from_balance = borrow_global_mut<Balance>(from);\n    let to_balance = borrow_global_mut<Balance>(to);\n    // ...\n}\n```\n\n### Resource Existence Patterns\n\n```move\n// Pattern 1: Ensure resource exists\npublic fun ensure_initialized(account: &signer) {\n    let addr = signer::address_of(account);\n    if (!exists<MyResource>(addr)) {\n        move_to(account, MyResource { value: 0 });\n    }\n}\n\n// Pattern 2: Get or create\npublic fun get_or_create(account: &signer): &mut MyResource acquires MyResource {\n    let addr = signer::address_of(account);\n    if (!exists<MyResource>(addr)) {\n        move_to(account, MyResource { value: 0 });\n    };\n    borrow_global_mut<MyResource>(addr)\n}\n\n// Pattern 3: Assert exists\npublic fun must_exist(addr: address) acquires MyResource {\n    assert!(exists<MyResource>(addr), ERROR_NOT_INITIALIZED);\n    let resource = borrow_global<MyResource>(addr);\n    // ...\n}\n```\n\n## Signer - Authentication Primitive\n\n### What is Signer?\n\n`signer` is Move's authentication primitive - represents authority to act on behalf of an account.\n\n```move\n// ✅ Only the signer can authorize operations on their account\npublic entry fun initialize(account: &signer) {\n    // 'account' proves the caller owns this address\n    move_to(account, MyResource { value: 0 });\n}\n\n// ❌ Can't fake a signer - runtime provides it\npublic fun fake_signer(addr: address) {\n    // No way to create signer from address!\n}\n```\n\n### Signer Operations\n\n```move\nuse std::signer;\n\npublic fun get_address(account: &signer): address {\n    signer::address_of(account)\n}\n\n// Common pattern: get address for storage lookup\npublic fun update_resource(account: &signer, val: u64) acquires MyResource {\n    let addr = signer::address_of(account);\n    let resource = borrow_global_mut<MyResource>(addr);\n    resource.value = val;\n}\n```\n\n### Access Control with Signer\n\n```move\nconst ERROR_UNAUTHORIZED: u64 = 1;\n\npublic entry fun admin_only(admin: &signer) {\n    assert!(signer::address_of(admin) == @admin_address, ERROR_UNAUTHORIZED);\n    // Only @admin_address can call this\n}\n\npublic entry fun owner_only(owner: &signer, resource_addr: address) acquires Owner {\n    let owner_addr = signer::address_of(owner);\n    let owner_resource = borrow_global<Owner>(resource_addr);\n    assert!(owner_resource.owner == owner_addr, ERROR_UNAUTHORIZED);\n    // Only the owner can call this\n}\n```\n\n## Visibility Modifiers\n\n### Function Visibility\n\n```move\nmodule my_module {\n    // Private (default) - only callable within module\n    fun private_function() { }\n\n    // Public - callable from anywhere, but not as entry point\n    public fun public_function() { }\n\n    // Public(friend) - only callable from this module + friend modules\n    public(friend) fun friend_function() { }\n\n    // Entry - callable as transaction entry point (public entry)\n    public entry fun entry_function(account: &signer) { }\n\n    // Entry (module-local) - entry point but not callable from other modules\n    entry fun local_entry(account: &signer) { }\n}\n```\n\n### Friend Functions\n\n```move\nmodule admin {\n    friend user_module;  // Declare friend\n\n    public(friend) fun admin_function() {\n        // Only callable from admin module or user_module\n    }\n}\n\nmodule user_module {\n    use admin::admin_function;\n\n    public fun user_function() {\n        admin_function();  // ✅ Allowed (we're a friend)\n    }\n}\n\nmodule other_module {\n    use admin::admin_function;\n\n    public fun other_function() {\n        admin_function();  // ❌ ERROR: Not a friend\n    }\n}\n```\n\n### Entry Functions\n\nEntry functions can be called directly as transaction entry points:\n\n```move\n// ✅ Valid entry function signatures\npublic entry fun simple() { }\npublic entry fun with_signer(account: &signer) { }\npublic entry fun with_args(account: &signer, amount: u64, recipient: address) { }\n\n// ❌ Invalid - entry functions can't return values\npublic entry fun returns_value(): u64 { 0 }\n\n// ❌ Invalid - entry functions can't have reference parameters (except &signer)\npublic entry fun ref_param(x: &u64) { }\n```\n\n## Advanced Features\n\n### Inline Functions\n\nMark functions for inlining to save gas:\n\n```move\ninline fun add(a: u64, b: u64): u64 {\n    a + b\n}\n\npublic fun calculate(): u64 {\n    add(5, 10)  // Inlined: becomes 5 + 10 directly\n}\n```\n\n**When to use `inline`:**\n- Small functions called frequently\n- Wrappers around simple operations\n- Gas-critical paths\n\n### Constant Values\n\n```move\nconst MAX_SUPPLY: u64 = 1_000_000;\nconst ERROR_INSUFFICIENT_BALANCE: u64 = 1;\nconst MODULE_NAME: vector<u8> = b\"MyModule\";\n\npublic fun check_supply(amount: u64) {\n    assert!(amount <= MAX_SUPPLY, ERROR_INSUFFICIENT_BALANCE);\n}\n```\n\n### Module Initialization\n\n```move\nfun init_module(deployer: &signer) {\n    // Called exactly once when module is published\n    move_to(deployer, ModuleConfig {\n        admin: signer::address_of(deployer),\n        version: 1,\n    });\n}\n```\n\n### Struct Unpacking\n\n```move\nstruct Point has copy, drop {\n    x: u64,\n    y: u64,\n}\n\n// Unpack all fields\nlet Point { x, y } = point;\n\n// Unpack some fields, ignore others\nlet Point { x, y: _ } = point;\n\n// Unpack and rename\nlet Point { x: x_coord, y: y_coord } = point;\n```\n\n### Vector Operations\n\n```move\nuse std::vector;\n\nlet mut v = vector::empty<u64>();\nvector::push_back(&mut v, 10);\nvector::push_back(&mut v, 20);\n\nlet len = vector::length(&v);\nlet first = vector::borrow(&v, 0);\nlet mut last = vector::borrow_mut(&mut v, 1);\n*last = 30;\n\nlet popped = vector::pop_back(&mut v);\n\nvector::append(&mut v, vector[40, 50]);\n\n// Iteration\nlet i = 0;\nwhile (i < vector::length(&v)) {\n    let elem = vector::borrow(&v, i);\n    // Use elem\n    i = i + 1;\n}\n```\n\n## Common Patterns\n\n### Pattern 1: Capability Pattern\n\n```move\nstruct AdminCap has key, store {}\n\npublic fun initialize_admin(account: &signer) {\n    move_to(account, AdminCap {});\n}\n\npublic fun admin_only_function(admin: &signer) acquires AdminCap {\n    let admin_addr = signer::address_of(admin);\n    assert!(exists<AdminCap>(admin_addr), ERROR_NO_ADMIN_CAP);\n    // Admin has proven they have AdminCap\n}\n\n// Transfer admin capability\npublic fun transfer_admin(admin: &signer, new_admin: address) acquires AdminCap {\n    let cap = move_from<AdminCap>(signer::address_of(admin));\n    // In practice, you'd need new_admin's signer\n    // This is simplified for demonstration\n}\n```\n\n### Pattern 2: Witness Pattern\n\n```move\nstruct MyModule has drop {}  // Witness type\n\nstruct Config<phantom T> has key {\n    value: u64\n}\n\n// Only callable once - witness can only be created in init_module\nfun init_module(account: &signer) {\n    initialize(account, MyModule {});\n}\n\npublic fun initialize<T: drop>(account: &signer, _witness: T) {\n    move_to(account, Config<T> { value: 0 });\n}\n```\n\n### Pattern 3: Hot Potato Pattern\n\n```move\n// No abilities - can't copy, drop, or store\nstruct Receipt {\n    amount: u64\n}\n\npublic fun buy(): Receipt {\n    Receipt { amount: 100 }\n}\n\npublic fun redeem(receipt: Receipt) {\n    let Receipt { amount } = receipt;  // Must unpack\n    // Process redemption\n}\n\n// Caller MUST call both buy() and redeem()\n// Can't drop Receipt, so must be consumed\n```\n\n## Type System Best Practices\n\n### ✅ Do\n\n- **Use abilities explicitly** - Think about copy/drop/store/key\n- **Leverage phantom types** - Zero-cost type safety\n- **Constrain generics** - Add ability constraints as needed\n- **Use references** - Avoid unnecessary copies\n- **Check exists before borrow** - Prevent runtime errors\n- **Mark admin functions** - Use signer for access control\n\n### ❌ Avoid\n\n- **Over-constraining generics** - Don't require abilities you don't use\n- **Ignoring ability requirements** - Compiler errors indicate design issues\n- **Returning references to locals** - Lifetime violation\n- **Missing acquires** - Always annotate global storage access\n- **Copying large structs** - Use references when possible\n\n## Common Errors\n\n### Error: \"Field requires ability 'store'\"\n\n```move\n// ❌ Problem\nstruct Container has key {\n    inner: Inner  // Inner needs 'store'\n}\n\nstruct Inner {  // Missing 'store'\n    value: u64\n}\n\n// ✅ Solution\nstruct Inner has store {\n    value: u64\n}\n```\n\n### Error: \"Missing acquires annotation\"\n\n```move\n// ❌ Problem\npublic fun get_value(addr: address): u64 {\n    borrow_global<Resource>(addr).value\n}\n\n// ✅ Solution\npublic fun get_value(addr: address): u64 acquires Resource {\n    borrow_global<Resource>(addr).value\n}\n```\n\n### Error: \"Type parameter T requires constraint\"\n\n```move\n// ❌ Problem\npublic fun store<T>(value: T) {\n    // Can't do anything with T\n}\n\n// ✅ Solution\npublic fun store<T: store>(value: T) {\n    // Now T can be stored\n}\n```\n\n## Testing Move Code\n\n```move\n#[test]\nfun test_abilities() {\n    let point = Point { x: 10, y: 20 };\n    let copy = point;  // ✅ Has copy\n    let Point { x, y } = point;  // ✅ Has drop\n}\n\n#[test(account = @0x123)]\nfun test_signer(account: &signer) {\n    let addr = signer::address_of(account);\n    assert!(addr == @0x123, 0);\n}\n\n#[test]\n#[expected_failure(abort_code = ERROR_INVALID)]\nfun test_failure() {\n    assert!(false, ERROR_INVALID);\n}\n```\n\n## Response Style\n\n- **Type-first** - Explain type implications before code\n- **Ability-aware** - Always discuss ability constraints\n- **Safety-focused** - Highlight Move's safety guarantees\n- **Pattern-driven** - Show idiomatic Move patterns\n- **Error-preventing** - Explain common mistakes upfront\n\n## Follow-up Suggestions\n\nAfter helping with Move language, suggest:\n- Ability design for custom types\n- Generic function optimization\n- Move Prover specifications\n- Gas optimization techniques\n- Testing strategies for complex types\n- Migration from other smart contract languages\n",
        "templates/.claude/skills/aptos/move-prover/skill.md": "---\nname: aptos-move-prover\ndescription: Expert on Move Prover formal verification - specification language (MSL), preconditions, postconditions, invariants, aborts_if, ensures, requires, modifies, emits, global invariants, schema patterns, quantifiers, helper functions, pragma directives, verification strategies, and debugging proofs. Triggers on keywords move prover, formal verification, spec, invariant, ensures, requires, aborts_if, precondition, postcondition, quantifier, schema, pragma\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Move Prover Expert\n\n## Purpose\n\nProvide deep expertise on the Move Prover - a formal verification tool that mathematically proves correctness properties of Move smart contracts. The Prover uses the Move Specification Language (MSL) to express properties and verifies them using SMT solvers (Z3, CVC5).\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Move Prover** - formal verification, prove, verification\n- **Specifications** - spec blocks, spec module, spec fun\n- **Properties** - invariants, preconditions, postconditions\n- **MSL Keywords** - ensures, requires, aborts_if, modifies, emits\n- **Advanced** - quantifiers, schema, global invariants, pragma\n- **Verification** - soundness, completeness, verification errors\n- **Debugging** - proof failures, counterexamples, timeout\n\n## Move Prover Overview\n\n### What is the Move Prover?\n\nThe Move Prover is a **formal verification tool** that:\n- Mathematically proves contract properties hold for ALL possible inputs\n- Uses SMT (Satisfiability Modulo Theories) solvers\n- Finds bugs that testing might miss\n- Provides mathematical guarantees of correctness\n- Generates counterexamples for failed proofs\n\n### Why Use the Move Prover?\n\n**Testing vs Verification:**\n```move\n// Testing: Checks specific inputs\n#[test]\nfun test_transfer() {\n    transfer(alice, bob, 100);  // Only tests this one case\n}\n\n// Verification: Proves for ALL inputs\nspec transfer {\n    ensures sender_balance == old(sender_balance) - amount;\n    ensures recipient_balance == old(recipient_balance) + amount;\n    // Proven for ALL possible values of amount, sender, recipient!\n}\n```\n\n**Benefits:**\n- ✅ Catches edge cases testing misses\n- ✅ Proves absence of integer overflow/underflow\n- ✅ Guarantees invariants always hold\n- ✅ Verifies access control is sound\n- ✅ Documents intended behavior formally\n\n## Specification Language (MSL) Basics\n\n### Spec Blocks\n\n```move\nmodule 0x1::counter {\n    struct Counter has key {\n        value: u64\n    }\n\n    public fun increment(addr: address) acquires Counter {\n        let counter = borrow_global_mut<Counter>(addr);\n        counter.value = counter.value + 1;\n    }\n\n    spec increment {\n        // Specification for increment function\n        requires exists<Counter>(addr);\n        ensures global<Counter>(addr).value == old(global<Counter>(addr).value) + 1;\n    }\n}\n```\n\n### Spec Module\n\n```move\nspec module {\n    // Module-level specifications\n    pragma verify = true;  // Enable verification for this module\n    pragma aborts_if_is_strict;  // Require complete abort specifications\n}\n```\n\n## Core Specification Constructs\n\n### 1. Preconditions - `requires`\n\nConditions that must be true when function is called:\n\n```move\npublic fun withdraw(account: &signer, amount: u64) acquires Balance {\n    let addr = signer::address_of(account);\n    let balance = borrow_global_mut<Balance>(addr);\n    balance.coins = balance.coins - amount;\n}\n\nspec withdraw {\n    requires exists<Balance>(signer::address_of(account));\n    requires global<Balance>(signer::address_of(account)).coins >= amount;\n    // Function only called when these conditions are true\n}\n```\n\n**Multiple requires:**\n```move\nspec withdraw {\n    requires exists<Balance>(addr);\n    requires global<Balance>(addr).coins >= amount;\n    requires amount > 0;\n    requires amount <= MAX_WITHDRAW;\n}\n```\n\n### 2. Postconditions - `ensures`\n\nConditions that must be true after function executes:\n\n```move\npublic fun transfer(from: address, to: address, amount: u64) acquires Balance {\n    let from_balance = borrow_global_mut<Balance>(from);\n    from_balance.coins = from_balance.coins - amount;\n\n    let to_balance = borrow_global_mut<Balance>(to);\n    to_balance.coins = to_balance.coins + amount;\n}\n\nspec transfer {\n    // Postconditions\n    ensures global<Balance>(from).coins == old(global<Balance>(from).coins) - amount;\n    ensures global<Balance>(to).coins == old(global<Balance>(to).coins) + amount;\n\n    // Conservation of funds\n    ensures global<Balance>(from).coins + global<Balance>(to).coins ==\n            old(global<Balance>(from).coins + global<Balance>(to).coins);\n}\n```\n\n### 3. Abort Conditions - `aborts_if`\n\nSpecify when function should abort:\n\n```move\nconst ERROR_INSUFFICIENT_BALANCE: u64 = 1;\nconst ERROR_NOT_INITIALIZED: u64 = 2;\n\npublic fun withdraw(addr: address, amount: u64) acquires Balance {\n    assert!(exists<Balance>(addr), ERROR_NOT_INITIALIZED);\n    let balance = borrow_global_mut<Balance>(addr);\n    assert!(balance.coins >= amount, ERROR_INSUFFICIENT_BALANCE);\n    balance.coins = balance.coins - amount;\n}\n\nspec withdraw {\n    aborts_if !exists<Balance>(addr) with ERROR_NOT_INITIALIZED;\n    aborts_if global<Balance>(addr).coins < amount with ERROR_INSUFFICIENT_BALANCE;\n\n    // These are the ONLY ways this function can abort\n}\n```\n\n**Conditional aborts:**\n```move\nspec withdraw {\n    // Only aborts if these conditions are met\n    aborts_if !exists<Balance>(addr);\n    aborts_if exists<Balance>(addr) && global<Balance>(addr).coins < amount;\n}\n```\n\n### 4. Abort Specification Completeness\n\n```move\nspec module {\n    pragma aborts_if_is_strict;\n    // Now MUST specify all abort conditions\n}\n\npublic fun transfer(from: address, to: address, amount: u64) acquires Balance {\n    assert!(exists<Balance>(from), 1);\n    assert!(exists<Balance>(to), 2);\n    assert!(global<Balance>(from).coins >= amount, 3);\n\n    // ... transfer logic\n}\n\nspec transfer {\n    // Must list ALL abort conditions\n    aborts_if !exists<Balance>(from);\n    aborts_if !exists<Balance>(to);\n    aborts_if global<Balance>(from).coins < amount;\n    // Missing any condition = verification error\n}\n```\n\n### 5. Modified Resources - `modifies`\n\nSpecify which global resources are modified:\n\n```move\nspec withdraw {\n    modifies global<Balance>(addr);\n    // Only Balance at addr is modified, nothing else\n}\n\nspec transfer {\n    modifies global<Balance>(from);\n    modifies global<Balance>(to);\n    // Only these two resources are modified\n}\n```\n\n### 6. Event Emission - `emits`\n\nSpecify events that should be emitted:\n\n```move\n#[event]\nstruct TransferEvent has drop, store {\n    from: address,\n    to: address,\n    amount: u64,\n}\n\npublic fun transfer(from: address, to: address, amount: u64) {\n    // ... transfer logic\n    event::emit(TransferEvent { from, to, amount });\n}\n\nspec transfer {\n    emits TransferEvent {\n        from: from,\n        to: to,\n        amount: amount\n    } to @my_module;\n}\n```\n\n## Advanced Features\n\n### The `old()` Operator\n\nAccess values from before function execution:\n\n```move\nspec increment {\n    ensures global<Counter>(addr).value == old(global<Counter>(addr).value) + 1;\n    //                                      ^^^ value before function ran\n}\n\nspec swap {\n    ensures a == old(b);\n    ensures b == old(a);\n}\n```\n\n### Global State Access\n\n```move\nspec withdraw {\n    // Access global state\n    let balance = global<Balance>(addr);\n    let initial_coins = old(global<Balance>(addr).coins);\n\n    ensures balance.coins == initial_coins - amount;\n}\n```\n\n### Let Bindings in Specs\n\n```move\nspec transfer {\n    let from_balance = global<Balance>(from);\n    let to_balance = global<Balance>(to);\n    let from_initial = old(from_balance.coins);\n    let to_initial = old(to_balance.coins);\n\n    ensures from_balance.coins == from_initial - amount;\n    ensures to_balance.coins == to_initial + amount;\n}\n```\n\n### Conditional Ensures\n\n```move\nspec withdraw {\n    ensures result == true ==> global<Balance>(addr).coins == old(global<Balance>(addr).coins) - amount;\n    ensures result == false ==> global<Balance>(addr).coins == old(global<Balance>(addr).coins);\n}\n```\n\n## Quantifiers\n\n### Universal Quantification - `forall`\n\nProve property holds for ALL values:\n\n```move\nspec module {\n    // For all addresses, if Balance exists, coins >= 0\n    invariant forall addr: address:\n        exists<Balance>(addr) ==> global<Balance>(addr).coins >= 0;\n}\n\nspec transfer {\n    // Conservation: total supply unchanged\n    ensures forall addr: address where addr != from && addr != to:\n        global<Balance>(addr).coins == old(global<Balance>(addr).coins);\n}\n```\n\n### Existential Quantification - `exists`\n\nProve property holds for AT LEAST ONE value:\n\n```move\nspec module {\n    // There exists at least one admin\n    invariant exists addr: address:\n        exists<AdminCap>(addr);\n}\n```\n\n### Range Quantifiers\n\n```move\nspec module {\n    // For all indices in vector, value is positive\n    invariant forall i in 0..len(balances):\n        balances[i] > 0;\n}\n\nspec sum_vector {\n    ensures result == sum(0..len(v), |i| v[i]);\n}\n```\n\n## Global Invariants\n\n### Module Invariants\n\nProperties that hold for entire module:\n\n```move\nspec module {\n    // Global supply never exceeds max\n    invariant [global]\n        forall addr: address:\n            exists<Supply>(addr) ==>\n            global<Supply>(addr).total <= MAX_SUPPLY;\n\n    // Conservation of total supply\n    invariant [global]\n        sum_of_balances() == global<TotalSupply>(@admin).value;\n}\n```\n\n### Struct Invariants\n\nProperties that always hold for a struct:\n\n```move\nstruct Balance has key {\n    coins: u64,\n    locked: u64,\n}\n\nspec Balance {\n    // Available coins >= locked coins\n    invariant coins >= locked;\n\n    // Locked amount is multiple of lock unit\n    invariant locked % LOCK_UNIT == 0;\n}\n```\n\n### Update Invariants\n\nProperties about state changes:\n\n```move\nspec module {\n    // Balance can only increase or decrease, never become negative\n    invariant update [global]\n        forall addr: address:\n            old(exists<Balance>(addr)) ==>\n            exists<Balance>(addr) &&\n            global<Balance>(addr).coins <= old(global<Balance>(addr).coins) + MAX_DEPOSIT;\n}\n```\n\n## Schema - Reusable Specifications\n\n### Defining Schema\n\n```move\nspec schema BalanceExists {\n    addr: address;\n    requires exists<Balance>(addr);\n}\n\nspec schema SufficientBalance {\n    addr: address;\n    amount: u64;\n    requires global<Balance>(addr).coins >= amount;\n}\n\nspec schema BalanceNotChanged {\n    addr: address;\n    ensures global<Balance>(addr).coins == old(global<Balance>(addr).coins);\n}\n```\n\n### Using Schema\n\n```move\nspec withdraw {\n    include BalanceExists;\n    include SufficientBalance;\n}\n\nspec deposit {\n    include BalanceExists { addr: recipient };\n}\n\n// Apply to multiple functions\nspec withdraw, transfer, burn {\n    include BalanceExists;\n}\n```\n\n### Schema with Variables\n\n```move\nspec schema TransferEnsures {\n    from: address;\n    to: address;\n    amount: u64;\n\n    ensures global<Balance>(from).coins == old(global<Balance>(from).coins) - amount;\n    ensures global<Balance>(to).coins == old(global<Balance>(to).coins) + amount;\n}\n\nspec transfer {\n    include TransferEnsures;\n}\n```\n\n### Schema Composition\n\n```move\nspec schema ValidTransfer {\n    from: address;\n    to: address;\n    amount: u64;\n\n    include BalanceExists { addr: from };\n    include BalanceExists { addr: to };\n    include SufficientBalance { addr: from };\n}\n\nspec transfer {\n    include ValidTransfer;\n    include TransferEnsures;\n}\n```\n\n## Helper Functions\n\n### Spec Functions\n\nDefine helper functions for specifications:\n\n```move\nspec module {\n    // Sum of all balances\n    fun sum_of_balances(): u64 {\n        sum(all_addresses(), |addr| {\n            if (exists<Balance>(addr)) {\n                global<Balance>(addr).coins\n            } else {\n                0\n            }\n        })\n    }\n\n    // Check if address is admin\n    fun is_admin(addr: address): bool {\n        exists<AdminCap>(addr)\n    }\n\n    // Get total supply\n    fun total_supply(): u64 {\n        global<Supply>(@admin).total\n    }\n}\n\nspec transfer {\n    // Conservation using helper function\n    ensures sum_of_balances() == old(sum_of_balances());\n}\n```\n\n### Spec-Only Functions\n\nFunctions only used in specifications:\n\n```move\nspec module {\n    fun balance_of(addr: address): u64 {\n        if (exists<Balance>(addr)) {\n            global<Balance>(addr).coins\n        } else {\n            0\n        }\n    }\n}\n\nspec transfer {\n    ensures balance_of(from) == old(balance_of(from)) - amount;\n    ensures balance_of(to) == old(balance_of(to)) + amount;\n}\n```\n\n## Pragma Directives\n\n### Verification Control\n\n```move\nspec module {\n    // Enable/disable verification for module\n    pragma verify = true;\n\n    // Timeout for each function (seconds)\n    pragma timeout = 60;\n\n    // Random seed for solver\n    pragma random_seed = 123;\n\n    // Require strict abort specs\n    pragma aborts_if_is_strict;\n\n    // Require partial abort specs (default)\n    pragma aborts_if_is_partial;\n}\n```\n\n### Function-Specific Pragmas\n\n```move\nspec complex_function {\n    pragma verify = false;  // Skip verification of this function\n}\n\nspec timeout_function {\n    pragma timeout = 120;  // Increase timeout to 2 minutes\n}\n\nspec opaque_function {\n    pragma opaque;  // Don't inline this function in proofs\n}\n```\n\n### Solver Configuration\n\n```move\nspec module {\n    // Use specific SMT solver\n    pragma solver = \"z3\";  // or \"cvc5\"\n\n    // Increase memory limit\n    pragma memory_limit = 16384;  // MB\n\n    // Verification condition generation\n    pragma verify_duration_estimate = 300;\n}\n```\n\n## Advanced Patterns\n\n### Pattern 1: Access Control Verification\n\n```move\nstruct AdminCap has key {}\n\npublic fun admin_only_function(admin: &signer) acquires AdminCap {\n    let admin_addr = signer::address_of(admin);\n    assert!(exists<AdminCap>(admin_addr), ERROR_NOT_ADMIN);\n    // ... admin operations\n}\n\nspec admin_only_function {\n    // Precondition: caller must have AdminCap\n    requires exists<AdminCap>(signer::address_of(admin));\n\n    // Abort condition\n    aborts_if !exists<AdminCap>(signer::address_of(admin));\n}\n\nspec module {\n    // Global invariant: only one admin exists\n    invariant [global]\n        forall addr1: address, addr2: address:\n            exists<AdminCap>(addr1) && exists<AdminCap>(addr2) ==> addr1 == addr2;\n}\n```\n\n### Pattern 2: Supply Conservation\n\n```move\nspec module {\n    // Helper: sum of all balances\n    fun total_balance(): u64 {\n        sum(all_addresses(), |addr| {\n            if (exists<Balance>(addr)) {\n                global<Balance>(addr).coins\n            } else {\n                0\n            }\n        })\n    }\n\n    // Invariant: total balance equals total supply\n    invariant [global]\n        total_balance() == global<TotalSupply>(@deployer).value;\n}\n\nspec mint {\n    // Minting increases total supply\n    ensures global<TotalSupply>(@deployer).value ==\n            old(global<TotalSupply>(@deployer).value) + amount;\n}\n\nspec burn {\n    // Burning decreases total supply\n    ensures global<TotalSupply>(@deployer).value ==\n            old(global<TotalSupply>(@deployer).value) - amount;\n}\n\nspec transfer {\n    // Transfer doesn't change total supply\n    ensures global<TotalSupply>(@deployer).value ==\n            old(global<TotalSupply>(@deployer).value);\n}\n```\n\n### Pattern 3: Integer Overflow Prevention\n\n```move\nspec add_balance {\n    // Precondition: addition won't overflow\n    requires global<Balance>(addr).coins + amount <= MAX_U64;\n\n    // Postcondition: new balance is sum\n    ensures global<Balance>(addr).coins == old(global<Balance>(addr).coins) + amount;\n\n    // Won't abort due to overflow (arithmetic is checked in Move)\n    aborts_if false;  // No custom aborts\n}\n```\n\n### Pattern 4: Reentrancy Safety (N/A in Move)\n\n```move\n// Move has no reentrancy issues, but can verify atomicity\nspec withdraw_and_call {\n    // State changes are atomic\n    ensures global<Balance>(addr).coins == old(global<Balance>(addr).coins) - amount;\n\n    // No intermediate states observable\n    modifies global<Balance>(addr);\n}\n```\n\n### Pattern 5: Temporal Properties\n\n```move\nspec module {\n    // Once frozen, always frozen\n    invariant update [global]\n        forall addr: address:\n            old(exists<Frozen>(addr)) ==> exists<Frozen>(addr);\n\n    // Can only freeze, never unfreeze\n    invariant update [global]\n        forall addr: address:\n            !old(exists<Frozen>(addr)) || exists<Frozen>(addr);\n}\n```\n\n## Complex Specification Examples\n\n### Example 1: Escrow Contract\n\n```move\nstruct Escrow has key {\n    amount: u64,\n    sender: address,\n    recipient: address,\n    deadline: u64,\n    released: bool,\n}\n\nspec module {\n    // Escrow invariants\n    invariant [global]\n        forall addr: address:\n            exists<Escrow>(addr) ==>\n            (global<Escrow>(addr).amount > 0 &&\n             global<Escrow>(addr).deadline > 0);\n}\n\npublic fun create_escrow(\n    sender: &signer,\n    recipient: address,\n    amount: u64,\n    deadline: u64\n) {\n    // ...\n}\n\nspec create_escrow {\n    requires amount > 0;\n    requires deadline > timestamp::now_seconds();\n    requires !exists<Escrow>(signer::address_of(sender));\n\n    ensures exists<Escrow>(signer::address_of(sender));\n    ensures global<Escrow>(signer::address_of(sender)).amount == amount;\n    ensures global<Escrow>(signer::address_of(sender)).recipient == recipient;\n    ensures global<Escrow>(signer::address_of(sender)).deadline == deadline;\n    ensures global<Escrow>(signer::address_of(sender)).released == false;\n}\n\npublic fun release(escrow_addr: address) acquires Escrow {\n    let escrow = borrow_global_mut<Escrow>(escrow_addr);\n    assert!(timestamp::now_seconds() >= escrow.deadline, ERROR_NOT_EXPIRED);\n    assert!(!escrow.released, ERROR_ALREADY_RELEASED);\n\n    escrow.released = true;\n    // Transfer amount to recipient\n}\n\nspec release {\n    let escrow = global<Escrow>(escrow_addr);\n\n    requires exists<Escrow>(escrow_addr);\n    requires timestamp::now_seconds() >= escrow.deadline;\n    requires !escrow.released;\n\n    ensures global<Escrow>(escrow_addr).released == true;\n\n    // All other fields unchanged\n    ensures global<Escrow>(escrow_addr).amount == old(escrow.amount);\n    ensures global<Escrow>(escrow_addr).sender == old(escrow.sender);\n    ensures global<Escrow>(escrow_addr).recipient == old(escrow.recipient);\n    ensures global<Escrow>(escrow_addr).deadline == old(escrow.deadline);\n}\n```\n\n### Example 2: Multi-Sig Wallet\n\n```move\nstruct MultiSig has key {\n    owners: vector<address>,\n    threshold: u64,\n    proposal_count: u64,\n}\n\nstruct Proposal has key {\n    approvals: u64,\n    executed: bool,\n}\n\nspec module {\n    // Multi-sig invariants\n    invariant [global]\n        forall addr: address:\n            exists<MultiSig>(addr) ==>\n            (global<MultiSig>(addr).threshold > 0 &&\n             global<MultiSig>(addr).threshold <= vector::length(&global<MultiSig>(addr).owners));\n}\n\npublic fun approve_proposal(\n    owner: &signer,\n    wallet_addr: address,\n    proposal_id: u64\n) acquires MultiSig, Proposal {\n    // ...\n}\n\nspec approve_proposal {\n    let multisig = global<MultiSig>(wallet_addr);\n    let owner_addr = signer::address_of(owner);\n\n    // Preconditions\n    requires exists<MultiSig>(wallet_addr);\n    requires exists<Proposal>(proposal_id);\n    requires vector::contains(&multisig.owners, &owner_addr);\n    requires !global<Proposal>(proposal_id).executed;\n\n    // Postcondition: approvals increased\n    ensures global<Proposal>(proposal_id).approvals ==\n            old(global<Proposal>(proposal_id).approvals) + 1;\n\n    // Proposal executed when threshold reached\n    ensures global<Proposal>(proposal_id).approvals >= multisig.threshold ==>\n            global<Proposal>(proposal_id).executed;\n}\n```\n\n### Example 3: Staking with Rewards\n\n```move\nstruct Stake has key {\n    amount: u64,\n    start_time: u64,\n    reward_rate: u64,\n}\n\nspec module {\n    // Helper: calculate expected rewards\n    fun calculate_rewards(stake: Stake, current_time: u64): u64 {\n        let duration = current_time - stake.start_time;\n        (stake.amount * stake.reward_rate * duration) / PRECISION\n    }\n}\n\npublic fun claim_rewards(staker: &signer) acquires Stake {\n    let addr = signer::address_of(staker);\n    let stake = borrow_global<Stake>(addr);\n\n    let current_time = timestamp::now_seconds();\n    let rewards = calculate_rewards(stake, current_time);\n\n    // Transfer rewards\n}\n\nspec claim_rewards {\n    let addr = signer::address_of(staker);\n    let stake = global<Stake>(addr);\n    let current_time = timestamp::now_seconds();\n\n    requires exists<Stake>(addr);\n    requires current_time >= stake.start_time;\n\n    // Rewards calculated correctly\n    ensures result == calculate_rewards(stake, current_time);\n\n    // Stake amount unchanged\n    ensures global<Stake>(addr).amount == old(stake.amount);\n}\n```\n\n## Debugging Verification Failures\n\n### Reading Error Messages\n\n```\nerror: post-condition does not hold\n   ┌── example.move:15:9 ───\n   │\n15 │         ensures global<Balance>(addr).coins == old(global<Balance>(addr).coins) + amount;\n   │         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   │\n   = Model:\n     addr = 0x1\n     amount = MAX_U64\n     old(global<Balance>(addr).coins) = 1\n     Result: overflow in addition\n```\n\n**Solution:** Add precondition to prevent overflow\n```move\nspec deposit {\n    requires global<Balance>(addr).coins + amount <= MAX_U64;\n    ensures global<Balance>(addr).coins == old(global<Balance>(addr).coins) + amount;\n}\n```\n\n### Counterexamples\n\n```move\nspec transfer {\n    ensures global<Balance>(from).coins >= 0;\n}\n\n// Prover output:\n// Counterexample:\n//   from = 0x1\n//   to = 0x2\n//   amount = 100\n//   old(global<Balance>(from).coins) = 50\n//   Result: assertion failure (underflow)\n```\n\n**Solution:** Add precondition\n```move\nspec transfer {\n    requires global<Balance>(from).coins >= amount;\n    ensures global<Balance>(from).coins >= 0;\n}\n```\n\n### Common Issues\n\n**1. Missing preconditions**\n```move\n// ❌ Fails: no precondition about existence\nspec withdraw {\n    ensures global<Balance>(addr).coins == old(global<Balance>(addr).coins) - amount;\n}\n\n// ✅ Fixed\nspec withdraw {\n    requires exists<Balance>(addr);\n    ensures global<Balance>(addr).coins == old(global<Balance>(addr).coins) - amount;\n}\n```\n\n**2. Incomplete abort specifications**\n```move\nspec module {\n    pragma aborts_if_is_strict;\n}\n\n// ❌ Fails: missing abort condition\nspec transfer {\n    aborts_if !exists<Balance>(from);\n    // Missing: aborts_if !exists<Balance>(to);\n    // Missing: aborts_if global<Balance>(from).coins < amount;\n}\n\n// ✅ Fixed\nspec transfer {\n    aborts_if !exists<Balance>(from);\n    aborts_if !exists<Balance>(to);\n    aborts_if global<Balance>(from).coins < amount;\n}\n```\n\n**3. Timeouts**\n```move\n// Complex verification may timeout\nspec complex_function {\n    pragma timeout = 300;  // Increase to 5 minutes\n}\n\n// Or simplify specification\nspec complex_function {\n    pragma verify = false;  // Skip for now\n}\n```\n\n## Running the Move Prover\n\n### Basic Commands\n\n```bash\n# Verify all modules in package\naptos move prove\n\n# Verify specific module\naptos move prove --filter MyModule\n\n# Verify with verbose output\naptos move prove --verbose\n\n# Skip timeout warnings\naptos move prove --skip-timeout\n\n# Generate coverage report\naptos move prove --coverage\n```\n\n### Configuration File (Move.toml)\n\n```toml\n[prover]\n# Enable/disable prover\nenabled = true\n\n# Timeout per function (seconds)\ntimeout = 60\n\n# SMT solver\nsolver = \"z3\"\n\n# Memory limit (MB)\nmemory_limit = 8192\n\n# Random seed\nrandom_seed = 42\n\n# Verbosity level (0-4)\nverbosity_level = 1\n```\n\n### CI/CD Integration\n\n```yaml\n# .github/workflows/verify.yml\nname: Move Prover\n\non: [push, pull_request]\n\njobs:\n  verify:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Install Aptos CLI\n        run: |\n          curl -fsSL \"https://aptos.dev/scripts/install_cli.py\" | python3\n\n      - name: Install Move Prover\n        run: |\n          aptos move setup-prover\n\n      - name: Run Move Prover\n        run: |\n          aptos move prove --verbose\n```\n\n## Best Practices\n\n### ✅ Do\n\n- **Start simple** - Add basic specs first, then refine\n- **Use schema** - Reuse common specifications\n- **Write helper functions** - Keep specs readable\n- **Specify aborts completely** - Use `pragma aborts_if_is_strict`\n- **Verify invariants** - Especially for critical properties\n- **Document with specs** - Specs are executable documentation\n- **Run in CI** - Catch regressions early\n- **Profile verification** - Identify slow functions\n\n### ❌ Avoid\n\n- **Over-specifying** - Don't prove trivial properties\n- **Ignoring timeouts** - Investigate and fix, don't skip\n- **Skipping aborts** - Incomplete specs miss bugs\n- **Complex quantifiers** - May cause timeouts\n- **Verifying everything** - Focus on critical functions\n- **Assuming soundness** - Prover can have false positives\n\n## Specification Checklist\n\nFor each critical function:\n\n- [ ] Preconditions (`requires`) - What must be true before?\n- [ ] Postconditions (`ensures`) - What must be true after?\n- [ ] Abort conditions (`aborts_if`) - When should it fail?\n- [ ] Modified resources (`modifies`) - What state changes?\n- [ ] Events (`emits`) - What events are emitted?\n- [ ] Invariants - What always holds?\n- [ ] Conservation laws - Are resources conserved?\n- [ ] Access control - Is authorization correct?\n- [ ] Integer overflow - Are bounds checked?\n- [ ] Temporal properties - Do state transitions make sense?\n\n## Response Style\n\n- **Formal but practical** - Explain verification benefits clearly\n- **Example-driven** - Show specs with corresponding code\n- **Error-aware** - Help debug verification failures\n- **Pattern-focused** - Teach reusable specification patterns\n- **Tool-savvy** - Reference prover commands and pragmas\n\n## Follow-up Suggestions\n\nAfter helping with Move Prover, suggest:\n- Start with simple function specs\n- Add module-level invariants\n- Create schema for common patterns\n- Set up CI/CD verification\n- Profile and optimize slow verifications\n- Document critical properties formally\n- Review Aptos framework specs as examples\n",
        "templates/.claude/skills/aptos/move-testing/skill.md": "---\nname: aptos-move-testing\ndescription: Expert on testing Move smart contracts on Aptos, including unit tests, integration tests, Move Prover formal verification, debugging strategies, and test coverage. Triggers on keywords move test, unit test, integration test, move prover, formal verification, debug, coverage, assert, expect\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Aptos Move Testing Expert\n\n## Purpose\n\nProvide expert guidance on testing Move smart contracts on Aptos blockchain, including unit tests, integration tests, formal verification with Move Prover, debugging strategies, and achieving comprehensive test coverage.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Testing** - unit tests, integration tests, test coverage, test cases\n- **Move Prover** - formal verification, specifications, invariants\n- **Debugging** - errors, failures, stack traces, logging\n- **Test Commands** - `aptos move test`, `aptos move prove`\n- **Assertions** - `assert!`, test expectations, error codes\n- **Mock Data** - test accounts, signers, resources\n\n## Core Testing Concepts\n\n### Unit Testing in Move\n\n```move\n#[test]\nfun test_basic_functionality() {\n    // Test code here\n    assert!(condition, ERROR_CODE);\n}\n\n#[test(account = @0x1)]\nfun test_with_signer(account: &signer) {\n    // Test with signer parameter\n}\n\n#[test]\n#[expected_failure(abort_code = ERROR_CODE)]\nfun test_expected_failure() {\n    // Code that should fail\n}\n```\n\n### Test Attributes\n\n- `#[test]` - Basic test function\n- `#[test(param = @address)]` - Test with named parameters (signers/addresses)\n- `#[test_only]` - Functions/modules only compiled in test mode\n- `#[expected_failure]` - Test should abort\n- `#[expected_failure(abort_code = N)]` - Test should abort with specific code\n\n## Process\n\nWhen a user asks about Move testing:\n\n### 1. Identify Testing Need\n\n```\nCommon scenarios:\n- Writing unit tests for new functions\n- Testing resource operations (move_to, move_from, borrow_global)\n- Verifying error conditions and abort codes\n- Integration testing with multiple modules\n- Formal verification with Move Prover\n- Debugging test failures\n- Improving test coverage\n```\n\n### 2. Determine Test Type\n\n**Unit Tests:**\n- Test individual functions in isolation\n- Mock dependencies\n- Fast execution\n- Use `#[test]` attribute\n\n**Integration Tests:**\n- Test module interactions\n- Test complete workflows\n- Use multiple signers\n- Test state changes\n\n**Formal Verification:**\n- Use Move Prover\n- Write specifications\n- Verify invariants\n- Prove correctness mathematically\n\n### 3. Provide Testing Solution\n\nStructure your response:\n- **Test strategy** - what to test and why\n- **Code example** - working test code\n- **Setup** - any required test fixtures or helper functions\n- **Assertions** - what to verify\n- **Edge cases** - corner cases to consider\n- **Commands** - how to run the tests\n\n## Testing Patterns\n\n### Pattern 1: Resource Testing\n\n```move\n#[test_only]\nuse std::signer;\n\n#[test(account = @0x123)]\nfun test_resource_creation(account: &signer) {\n    let addr = signer::address_of(account);\n\n    // Create resource\n    create_resource(account);\n\n    // Verify resource exists\n    assert!(exists<MyResource>(addr), ERROR_RESOURCE_NOT_FOUND);\n\n    // Verify resource state\n    let resource = borrow_global<MyResource>(addr);\n    assert!(resource.value == expected_value, ERROR_INVALID_STATE);\n}\n```\n\n### Pattern 2: Error Condition Testing\n\n```move\n#[test]\n#[expected_failure(abort_code = ERROR_INSUFFICIENT_BALANCE)]\nfun test_insufficient_balance() {\n    // Setup account with low balance\n    // Attempt operation that should fail\n    transfer(from, to, amount_too_large);\n}\n```\n\n### Pattern 3: Multi-Signer Testing\n\n```move\n#[test(alice = @0x123, bob = @0x456)]\nfun test_transfer(alice: &signer, bob: &signer) {\n    let alice_addr = signer::address_of(alice);\n    let bob_addr = signer::address_of(bob);\n\n    // Setup initial state\n    initialize(alice);\n    initialize(bob);\n\n    // Perform transfer\n    transfer(alice, bob_addr, 100);\n\n    // Verify balances\n    assert!(get_balance(alice_addr) == 900, ERROR_INVALID_BALANCE);\n    assert!(get_balance(bob_addr) == 100, ERROR_INVALID_BALANCE);\n}\n```\n\n### Pattern 4: Test-Only Helpers\n\n```move\n#[test_only]\nmodule test_helpers {\n    use std::signer;\n\n    public fun setup_account(account: &signer): address {\n        let addr = signer::address_of(account);\n        // Common setup logic\n        addr\n    }\n\n    public fun create_test_token(account: &signer, amount: u64) {\n        // Create tokens for testing\n    }\n}\n```\n\n## Move Prover Integration\n\n### Specification Syntax\n\n```move\nspec module {\n    // Module-level invariants\n    invariant forall addr: address:\n        exists<Balance>(addr) ==> global<Balance>(addr).value >= 0;\n}\n\nspec transfer {\n    // Pre-conditions\n    requires sender_balance >= amount;\n    requires sender != recipient;\n\n    // Post-conditions\n    ensures global<Balance>(sender).value == old(global<Balance>(sender).value) - amount;\n    ensures global<Balance>(recipient).value == old(global<Balance>(recipient).value) + amount;\n\n    // Abort conditions\n    aborts_if sender_balance < amount;\n}\n```\n\n### Running Move Prover\n\n```bash\n# Verify all specs in package\naptos move prove\n\n# Verify specific module\naptos move prove --filter MyModule\n\n# Verbose output\naptos move prove --verbose\n```\n\n## Test Commands\n\n### Basic Testing\n\n```bash\n# Run all tests in package\naptos move test\n\n# Run specific test\naptos move test --filter test_name\n\n# Run tests with coverage\naptos move test --coverage\n\n# Run with gas profiling\naptos move test --gas\n\n# Verbose output\naptos move test --verbose\n```\n\n### Test Output Interpretation\n\n```\nRunning Move unit tests\n[ PASS    ] 0x1::my_module::test_success\n[ FAIL    ] 0x1::my_module::test_failure\n[ TIMEOUT ] 0x1::my_module::test_slow\n\nError: Assertion failed\n   ┌── my_module.move:42:9 ───\n   │\n42 │         assert!(balance == 100, ERROR_INVALID_BALANCE);\n   │         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n```\n\n## Debugging Strategies\n\n### 1. Add Debug Prints (Test Only)\n\n```move\n#[test_only]\nuse std::debug;\n\n#[test]\nfun test_with_debug() {\n    debug::print(&b\"Starting test\");\n    debug::print(&value);\n    // Test logic\n}\n```\n\n### 2. Break Down Complex Tests\n\n```move\n// Instead of one large test\n#[test]\nfun test_complex_workflow() {\n    setup();\n    step1();\n    step2();\n    step3();\n    verify();\n}\n\n// Break into multiple tests\n#[test]\nfun test_step1() { /* ... */ }\n\n#[test]\nfun test_step2() { /* ... */ }\n\n#[test]\nfun test_step3() { /* ... */ }\n```\n\n### 3. Use Specific Abort Codes\n\n```move\nconst ERROR_INVALID_AMOUNT: u64 = 1;\nconst ERROR_INSUFFICIENT_BALANCE: u64 = 2;\nconst ERROR_UNAUTHORIZED: u64 = 3;\n\n// In code\nassert!(amount > 0, ERROR_INVALID_AMOUNT);\n\n// In test\n#[expected_failure(abort_code = ERROR_INVALID_AMOUNT)]\nfun test_zero_amount() { /* ... */ }\n```\n\n## Test Coverage Best Practices\n\n### What to Test\n\n✅ **Critical Paths:**\n- Main business logic\n- State transitions\n- Access control\n- Resource management\n\n✅ **Error Conditions:**\n- Invalid inputs\n- Insufficient permissions\n- Resource not found\n- Arithmetic overflow\n\n✅ **Edge Cases:**\n- Zero values\n- Maximum values\n- Empty collections\n- Boundary conditions\n\n✅ **Invariants:**\n- Total supply conservation\n- Balance constraints\n- Ownership rules\n\n### Coverage Goals\n\n```\nAim for:\n- 100% of public functions\n- 100% of abort conditions\n- All state transitions\n- All access control checks\n```\n\n## Common Testing Patterns\n\n### Setup/Teardown Pattern\n\n```move\n#[test_only]\nfun setup_test_env(account: &signer): TestContext {\n    initialize_module(account);\n    create_test_resources(account);\n    TestContext { /* ... */ }\n}\n\n#[test(account = @0x1)]\nfun test_feature(account: &signer) {\n    let ctx = setup_test_env(account);\n    // Use ctx\n    // No teardown needed (Move tests are isolated)\n}\n```\n\n### Parameterized Testing\n\n```move\n#[test_only]\nfun verify_transfer_amount(amount: u64, expected_result: bool) {\n    if (expected_result) {\n        transfer(sender, receiver, amount);\n    } else {\n        // Should fail\n    }\n}\n\n#[test]\nfun test_valid_amounts() {\n    verify_transfer_amount(1, true);\n    verify_transfer_amount(100, true);\n    verify_transfer_amount(1000, true);\n}\n\n#[test]\n#[expected_failure]\nfun test_invalid_amounts() {\n    verify_transfer_amount(0, false);\n}\n```\n\n### State Verification\n\n```move\n#[test]\nfun test_state_consistency(account: &signer) {\n    let addr = signer::address_of(account);\n\n    // Initial state\n    let before = get_state(addr);\n\n    // Perform operation\n    perform_operation(account);\n\n    // Verify state changes\n    let after = get_state(addr);\n    assert!(after.field1 == before.field1 + delta, ERROR_STATE);\n}\n```\n\n## Integration with CI/CD\n\n### GitHub Actions Example\n\n```yaml\nname: Move Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Install Aptos CLI\n        run: |\n          curl -fsSL \"https://aptos.dev/scripts/install_cli.py\" | python3\n\n      - name: Run Move Tests\n        run: |\n          aptos move test --coverage\n\n      - name: Run Move Prover\n        run: |\n          aptos move prove\n```\n\n## Troubleshooting Common Issues\n\n### Issue: Test Timeout\n\n```move\n// Problem: Test takes too long\n#[test]\nfun test_slow() {\n    for (i in 0..1000000) { /* ... */ }\n}\n\n// Solution: Reduce iterations or split tests\n#[test]\nfun test_optimized() {\n    for (i in 0..100) { /* ... */ }\n}\n```\n\n### Issue: Resource Already Exists\n\n```move\n// Problem: Resource published twice\n#[test(account = @0x1)]\nfun test_duplicate() {\n    move_to(account, Resource {});\n    move_to(account, Resource {}); // ERROR!\n}\n\n// Solution: Check existence or use different accounts\n#[test(account = @0x1)]\nfun test_correct() {\n    if (!exists<Resource>(addr)) {\n        move_to(account, Resource {});\n    }\n}\n```\n\n### Issue: Signer Not Available\n\n```move\n// Problem: Need signer but don't have one\n#[test]\nfun test_needs_signer() {\n    let signer = ???; // Can't create signer!\n}\n\n// Solution: Use test parameter\n#[test(account = @0x1)]\nfun test_with_signer(account: &signer) {\n    // Use account\n}\n```\n\n## Advanced Testing Techniques\n\n### Property-Based Testing\n\n```move\n#[test_only]\nfun property_sum_commutative(a: u64, b: u64) {\n    assert!(add(a, b) == add(b, a), ERROR_PROPERTY);\n}\n\n#[test]\nfun test_properties() {\n    property_sum_commutative(1, 2);\n    property_sum_commutative(100, 50);\n    property_sum_commutative(0, 999);\n}\n```\n\n### Fuzz Testing Concepts\n\n```move\n#[test]\nfun test_with_various_inputs() {\n    // Test boundary values\n    test_function(0);\n    test_function(1);\n    test_function(u64::MAX);\n\n    // Test common values\n    test_function(100);\n    test_function(1000);\n\n    // Test edge cases\n    test_function(u64::MAX - 1);\n}\n```\n\n## Documentation References\n\nWhen answering questions, reference:\n- Aptos Move testing documentation\n- Move Prover specifications\n- Example test suites in Aptos framework\n- Testing best practices guides\n\n## Response Style\n\n- **Example-first** - Show working test code immediately\n- **Explain pattern** - Clarify what the test verifies\n- **Cover edge cases** - Mention corner cases to test\n- **Debugging tips** - Help troubleshoot failures\n- **Best practices** - Mention testing standards\n\n## Follow-up Suggestions\n\nAfter helping with tests, suggest:\n- Additional test cases to consider\n- Move Prover specifications\n- Code coverage analysis\n- Integration testing strategies\n- CI/CD integration for automated testing\n",
        "templates/.claude/skills/aptos/object-model/skill.md": "---\nname: aptos-object-model\ndescription: Expert on Aptos Object Model - ObjectCore, Object<T> wrapper, constructor references, ExtendRef/DeleteRef/TransferRef capabilities, object ownership, named vs generated objects, composability, and migration from resource-only patterns. Triggers on keywords object model, objectcore, constructorref, extendref, deleteref, transferref, named object, object ownership, composable object\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Aptos Object Model Expert\n\n## Purpose\n\nProvide expert guidance on the Aptos Object Model - a powerful abstraction that enables composable, transferable, and flexible on-chain assets. The Object Model is the foundation for modern Aptos development including Token V2 (Digital Assets), Fungible Assets, and custom composable resources.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Object Model** - objects, ObjectCore, Object<T>, object-based design\n- **Object Creation** - ConstructorRef, named objects, object generation\n- **Object Capabilities** - ExtendRef, DeleteRef, TransferRef, LinearTransferRef\n- **Object Patterns** - composability, nesting, soul-bound, ownership\n- **Migration** - moving from resource-only to object-based architecture\n- **Standards** - Digital Assets (NFTs), Fungible Assets built on objects\n\n## Core Concepts\n\n### What is the Aptos Object Model?\n\nThe Object Model is Aptos's object-oriented programming abstraction that:\n\n1. **Wraps resources** - Any resource can become an \"object\"\n2. **Enables transfer** - Objects can be transferred between accounts\n3. **Provides composability** - Objects can own other objects\n4. **Manages lifecycle** - Creation, extension, deletion via refs\n5. **Separates ownership** - Owner address ≠ object address\n\n### Key Difference from Traditional Resources\n\n```move\n// Traditional Resource (can't be transferred directly)\nstruct MyResource has key {\n    value: u64\n}\n// Lives at account address, can't move to another account\n\n// Object-wrapped Resource (transferable)\nstruct MyResource has key {\n    value: u64\n}\n// Lives at object address, object itself can transfer between accounts\n```\n\n## Object Architecture\n\n### ObjectCore - The Foundation\n\nEvery object contains an `ObjectCore` at its address:\n\n```move\nstruct ObjectCore has key {\n    guid_creation_num: u64,      // For generating unique IDs\n    owner: address,              // Who owns this object\n    allow_ungated_transfer: bool, // Can anyone transfer it?\n    transfer_events: EventHandle<TransferEvent>,\n}\n```\n\n**Key Points:**\n- `ObjectCore` is automatically created during object creation\n- Object's address ≠ owner's address (objects live at their own address)\n- Owner can be an account OR another object (composition!)\n- Transfer permissions controlled via `allow_ungated_transfer`\n\n### Object<T> - The Type Wrapper\n\n```move\nstruct Object<phantom T> has copy, drop, store {\n    inner: address  // The object's address\n}\n```\n\n**Important:**\n- `Object<T>` is a **typed reference** to an object\n- It's NOT the object itself, just a pointer\n- Has `copy + drop + store` (can be copied, stored anywhere)\n- Type parameter `T` indicates what resource is at that address\n- Phantom type parameter (zero runtime cost)\n\n### Type Hierarchy Example\n\n```move\n// An NFT is an Object wrapping a Token resource\nObject<Token>\n\n// A Fungible Asset metadata object\nObject<Metadata>\n\n// Generic object\nObject<ObjectCore>  // Can reference any object\n```\n\n## Object Creation\n\n### Method 1: Named Objects (Deterministic Address)\n\n```move\nuse aptos_framework::object;\n\npublic fun create_named_object(creator: &signer) {\n    let constructor_ref = object::create_named_object(\n        creator,\n        b\"MY_UNIQUE_SEED\"  // Seed for deterministic address\n    );\n\n    // Object address is deterministic: hash(creator_address, seed)\n    let object_signer = object::generate_signer(&constructor_ref);\n    let object_addr = signer::address_of(&object_signer);\n\n    // Move resources to object address\n    move_to(&object_signer, MyResource { value: 100 });\n}\n```\n\n**Named Object Features:**\n- Deterministic address: `hash(creator_address, seed)`\n- Same creator + seed = same address (idempotent)\n- Useful for singletons, registries, well-known objects\n- Can't create duplicate with same creator + seed\n\n### Method 2: Generated Objects (Random Address)\n\n```move\npublic fun create_generated_object(creator: &signer) {\n    let constructor_ref = object::create_object(creator);\n\n    // Object address is generated (non-deterministic)\n    let object_signer = object::generate_signer(&constructor_ref);\n\n    move_to(&object_signer, MyResource { value: 100 });\n}\n```\n\n**Generated Object Features:**\n- Non-deterministic address (GUID-based)\n- Each call creates new unique object\n- Useful for collections (NFTs, tokens, etc.)\n\n### Method 3: Object from Account\n\n```move\npublic entry fun create_object_from_account(caller: &signer) {\n    // Convert account into an object\n    let constructor_ref = object::create_object_from_account(caller);\n    // Now the account itself becomes an object\n}\n```\n\n### Method 4: Sticky Object (Cannot Delete)\n\n```move\npublic fun create_sticky_object(creator: &signer) {\n    let constructor_ref = object::create_sticky_object(creator);\n    // No DeleteRef can be generated - object is permanent\n}\n```\n\n## Object References and Capabilities\n\n### ConstructorRef - The Master Key\n\n```move\nstruct ConstructorRef {\n    self: address,\n    can_delete: bool\n}\n```\n\nOnly available during object creation. Used to generate all other refs:\n\n```move\npublic fun create_with_refs(creator: &signer) {\n    let constructor_ref = object::create_object(creator);\n\n    // Generate various capabilities\n    let extend_ref = object::generate_extend_ref(&constructor_ref);\n    let transfer_ref = object::generate_transfer_ref(&constructor_ref);\n    let delete_ref = object::generate_delete_ref(&constructor_ref);\n    let mutator_ref = property_map::generate_mutator_ref(&constructor_ref);\n\n    // Store refs for later use\n    let object_signer = object::generate_signer(&constructor_ref);\n    move_to(&object_signer, Refs {\n        extend_ref,\n        transfer_ref,\n        delete_ref,\n        mutator_ref,\n    });\n}\n\nstruct Refs has key {\n    extend_ref: ExtendRef,\n    transfer_ref: TransferRef,\n    delete_ref: DeleteRef,\n    mutator_ref: property_map::MutatorRef,\n}\n```\n\n### ExtendRef - Access Object After Creation\n\n```move\nstruct ExtendRef has drop, store {\n    self: address\n}\n```\n\n**Purpose:** Get signer of object in future transactions\n\n```move\npublic fun modify_object_later(\n    object_addr: address\n) acquires Refs {\n    let refs = borrow_global<Refs>(object_addr);\n\n    // Generate signer from ExtendRef\n    let object_signer = object::generate_signer_for_extending(&refs.extend_ref);\n\n    // Now can modify resources at object address\n    let resource = borrow_global_mut<MyResource>(object_addr);\n    resource.value = resource.value + 10;\n}\n```\n\n### TransferRef - Control Object Transfers\n\n```move\nstruct TransferRef has drop, store {\n    self: address\n}\n```\n\n**Purpose:** Enable/disable transfers, force transfers\n\n```move\n// Disable ungated transfers (make soul-bound)\npublic fun make_soul_bound(object_addr: address) acquires Refs {\n    let refs = borrow_global<Refs>(object_addr);\n    object::disable_ungated_transfer(&refs.transfer_ref);\n}\n\n// Re-enable ungated transfers\npublic fun make_transferable(object_addr: address) acquires Refs {\n    let refs = borrow_global<Refs>(object_addr);\n    object::enable_ungated_transfer(&refs.transfer_ref);\n}\n\n// Force transfer (bypass ownership check)\npublic fun admin_transfer(\n    object_addr: address,\n    new_owner: address\n) acquires Refs {\n    let refs = borrow_global<Refs>(object_addr);\n    object::transfer_with_ref(&refs.transfer_ref, new_owner);\n}\n```\n\n### LinearTransferRef - One-Time Transfer\n\n```move\nstruct LinearTransferRef {\n    self: address,\n    owner: address\n}\n```\n\n**Purpose:** Transfer object exactly once (consumed on use)\n\n```move\npublic fun create_and_transfer_to(\n    creator: &signer,\n    recipient: address\n) {\n    let constructor_ref = object::create_object(creator);\n\n    // Generate linear transfer ref\n    let transfer_ref = object::generate_transfer_ref(&constructor_ref);\n    let linear_transfer_ref = object::generate_linear_transfer_ref(&transfer_ref);\n\n    // Transfer to recipient (consumes linear_transfer_ref)\n    object::transfer_with_ref(linear_transfer_ref, recipient);\n}\n```\n\n### DeleteRef - Destroy Objects\n\n```move\nstruct DeleteRef has drop, store {\n    self: address\n}\n```\n\n**Purpose:** Delete object and all resources at its address\n\n```move\npublic fun delete_object(object_addr: address) acquires Refs, MyResource {\n    // Remove all resources first\n    let MyResource { value: _ } = move_from<MyResource>(object_addr);\n    let Refs { delete_ref, extend_ref, transfer_ref, mutator_ref } =\n        move_from<Refs>(object_addr);\n\n    // Delete the object\n    object::delete(delete_ref);\n}\n```\n\n## Object Ownership and Transfer\n\n### Reading Object Information\n\n```move\nuse aptos_framework::object;\n\npublic fun get_object_info<T: key>(obj: Object<T>): (address, address, bool) {\n    let object_addr = object::object_address(&obj);\n    let owner_addr = object::owner(obj);\n    let is_owner_account = object::is_owner(obj, owner_addr);\n\n    (object_addr, owner_addr, is_owner_account)\n}\n\n// Check if object exists\npublic fun object_exists<T: key>(addr: address): bool {\n    object::object_exists<T>(addr)\n}\n```\n\n### User-Initiated Transfer\n\n```move\nuse aptos_framework::object;\n\npublic entry fun transfer_object<T: key>(\n    owner: &signer,\n    object: Object<T>,\n    new_owner: address\n) {\n    // Only works if ungated transfer is enabled\n    object::transfer(owner, object, new_owner);\n}\n```\n\n### Checking Transfer Permissions\n\n```move\npublic fun check_transferable<T: key>(obj: Object<T>): bool {\n    object::ungated_transfer_allowed(obj)\n}\n```\n\n## Object Composition (Nesting)\n\n### Child Object Pattern\n\n```move\npublic fun create_parent_and_child(creator: &signer) {\n    // Create parent object\n    let parent_ref = object::create_named_object(creator, b\"PARENT\");\n    let parent_signer = object::generate_signer(&parent_ref);\n    let parent_addr = signer::address_of(&parent_signer);\n\n    // Create child object OWNED BY parent\n    let child_ref = object::create_object_from_object(&parent_signer);\n    let child_signer = object::generate_signer(&child_ref);\n    let child_addr = signer::address_of(&child_signer);\n\n    // Transfer child to parent\n    let transfer_ref = object::generate_transfer_ref(&child_ref);\n    let linear_transfer_ref = object::generate_linear_transfer_ref(&transfer_ref);\n    object::transfer_with_ref(linear_transfer_ref, parent_addr);\n\n    // Now: parent owns child, creator owns parent\n}\n```\n\n### Querying Nested Ownership\n\n```move\npublic fun get_root_owner<T: key>(obj: Object<T>): address {\n    let mut current_addr = object::object_address(&obj);\n\n    loop {\n        if (!object::is_object(current_addr)) {\n            // Reached an account (not an object) - this is root owner\n            return current_addr;\n        };\n\n        let owner = object::owner(object::address_to_object<ObjectCore>(current_addr));\n        if (owner == current_addr) {\n            return current_addr; // Self-owned\n        };\n        current_addr = owner;\n    }\n}\n```\n\n## Common Patterns\n\n### Pattern 1: Registry / Singleton\n\n```move\npublic fun get_or_create_registry(creator: &signer): address {\n    let seed = b\"MY_REGISTRY\";\n    let creator_addr = signer::address_of(creator);\n    let object_addr = object::create_object_address(&creator_addr, seed);\n\n    if (!object::object_exists<Registry>(object_addr)) {\n        let constructor_ref = object::create_named_object(creator, seed);\n        let object_signer = object::generate_signer(&constructor_ref);\n\n        move_to(&object_signer, Registry {\n            items: simple_map::create()\n        });\n    };\n\n    object_addr\n}\n```\n\n### Pattern 2: NFT with Composable Items\n\n```move\nstruct NFT has key {\n    name: String,\n    uri: String,\n}\n\nstruct EquippedItem has key {\n    item_object: Object<Item>,\n}\n\nstruct Item has key {\n    name: String,\n    power: u64,\n}\n\npublic fun equip_item_to_nft(\n    nft_obj: Object<NFT>,\n    item_obj: Object<Item>\n) acquires Refs {\n    let nft_addr = object::object_address(&nft_obj);\n\n    // Get ExtendRef to modify NFT\n    let refs = borrow_global<Refs>(nft_addr);\n    let nft_signer = object::generate_signer_for_extending(&refs.extend_ref);\n\n    // Store item reference in NFT\n    if (!exists<EquippedItem>(nft_addr)) {\n        move_to(&nft_signer, EquippedItem { item_object: item_obj });\n    };\n\n    // Transfer item ownership to NFT object\n    let item_addr = object::object_address(&item_obj);\n    let item_refs = borrow_global<Refs>(item_addr);\n    object::transfer_with_ref(&item_refs.transfer_ref, nft_addr);\n}\n```\n\n### Pattern 3: Soul-Bound Token (SBT)\n\n```move\npublic fun create_soul_bound_token(\n    creator: &signer,\n    recipient: address,\n    name: String\n) {\n    let constructor_ref = token::create_named_token(\n        creator,\n        string::utf8(b\"Soul Bound Collection\"),\n        string::utf8(b\"Description\"),\n        name,\n        option::none(),\n        string::utf8(b\"https://uri.com\"),\n    );\n\n    // Disable transfers (soul-bound)\n    let transfer_ref = object::generate_transfer_ref(&constructor_ref);\n    object::disable_ungated_transfer(&transfer_ref);\n\n    // Transfer to recipient (one-time transfer during creation)\n    let linear_transfer_ref = object::generate_linear_transfer_ref(&transfer_ref);\n    object::transfer_with_ref(linear_transfer_ref, recipient);\n\n    // Don't store transfer_ref - now permanently non-transferable\n}\n```\n\n### Pattern 4: Upgradeable Module Data\n\n```move\nstruct ModuleData has key {\n    version: u64,\n    config: Config,\n}\n\nstruct DataRefs has key {\n    extend_ref: ExtendRef,\n}\n\npublic fun initialize_module_data(admin: &signer) {\n    let constructor_ref = object::create_named_object(admin, b\"MODULE_DATA\");\n    let object_signer = object::generate_signer(&constructor_ref);\n\n    move_to(&object_signer, ModuleData {\n        version: 1,\n        config: create_config(),\n    });\n\n    let extend_ref = object::generate_extend_ref(&constructor_ref);\n    move_to(&object_signer, DataRefs { extend_ref });\n}\n\npublic fun upgrade_module_data(new_config: Config) acquires DataRefs, ModuleData {\n    let data_addr = object::create_object_address(&@my_module, b\"MODULE_DATA\");\n    let refs = borrow_global<DataRefs>(data_addr);\n\n    let object_signer = object::generate_signer_for_extending(&refs.extend_ref);\n    let data = borrow_global_mut<ModuleData>(data_addr);\n\n    data.version = data.version + 1;\n    data.config = new_config;\n}\n```\n\n## Migration from Resource-Only to Object Model\n\n### Before (Resource-Only)\n\n```move\nstruct OldNFT has key {\n    name: String,\n    uri: String,\n}\n\n// Can't transfer between accounts\n// Lives at owner's address\n// No composition\n```\n\n### After (Object Model)\n\n```move\nstruct NewNFT has key {\n    name: String,\n    uri: String,\n}\n\npublic fun create_nft(creator: &signer, recipient: address) {\n    let constructor_ref = object::create_object(creator);\n    let object_signer = object::generate_signer(&constructor_ref);\n\n    move_to(&object_signer, NewNFT {\n        name: string::utf8(b\"My NFT\"),\n        uri: string::utf8(b\"https://...\"),\n    });\n\n    // Transfer to recipient\n    let transfer_ref = object::generate_transfer_ref(&constructor_ref);\n    let linear_ref = object::generate_linear_transfer_ref(&transfer_ref);\n    object::transfer_with_ref(linear_ref, recipient);\n}\n\n// ✅ Can transfer\n// ✅ Lives at own address\n// ✅ Composable\n```\n\n## TypeScript SDK Integration\n\n### Creating Objects\n\n```typescript\nimport { Aptos, AptosConfig, Network } from \"@aptos-labs/ts-sdk\";\n\nconst aptos = new Aptos(new AptosConfig({ network: Network.TESTNET }));\n\n// Call object creation function\nconst txn = await aptos.transaction.build.simple({\n  sender: account.accountAddress,\n  data: {\n    function: \"0x123::my_module::create_named_object\",\n    functionArguments: [],\n  },\n});\n\nconst response = await aptos.signAndSubmitTransaction({\n  signer: account,\n  transaction: txn,\n});\n```\n\n### Querying Objects\n\n```typescript\n// Get object data\nconst objectData = await aptos.getAccountResource({\n  accountAddress: \"0xobject_address\",\n  resourceType: \"0x1::object::ObjectCore\"\n});\n\nconsole.log(\"Owner:\", objectData.owner);\nconsole.log(\"Transferable:\", objectData.allow_ungated_transfer);\n\n// Get custom resource at object address\nconst nftData = await aptos.getAccountResource({\n  accountAddress: \"0xobject_address\",\n  resourceType: \"0x123::my_module::NFT\"\n});\n```\n\n### Transferring Objects\n\n```typescript\nconst txn = await aptos.transaction.build.simple({\n  sender: owner.accountAddress,\n  data: {\n    function: \"0x1::object::transfer\",\n    typeArguments: [\"0x123::my_module::NFT\"],\n    functionArguments: [\n      objectAddress,\n      recipientAddress,\n    ],\n  },\n});\n```\n\n## Best Practices\n\n### ✅ Do\n\n- **Store refs at object address** - Keep ExtendRef, DeleteRef, etc. as resources at the object's own address\n- **Use named objects for singletons** - Registries, module configs, well-known objects\n- **Use generated objects for collections** - NFTs, tokens, user-specific objects\n- **Disable ungated transfer for SBTs** - Use soul-bound pattern for credentials\n- **Document object relationships** - Make ownership hierarchy clear\n- **Clean up on deletion** - Remove all resources before calling object::delete()\n\n### ❌ Avoid\n\n- **Don't lose ConstructorRef** - Generate all needed refs during creation\n- **Don't store refs elsewhere** - Store at object address, not creator address\n- **Don't forget to transfer initial ownership** - Objects start owned by creator\n- **Don't mix object and non-object patterns** - Pick one architecture\n- **Don't delete without cleanup** - Remove resources first\n\n## Common Errors and Solutions\n\n### Error: \"Object does not exist\"\n\n```move\n// Problem: Object wasn't created or wrong address\nlet obj = object::address_to_object<T>(wrong_address);\n\n// Solution: Verify object exists first\nassert!(object::object_exists<T>(address), ERROR_OBJECT_NOT_FOUND);\nlet obj = object::address_to_object<T>(address);\n```\n\n### Error: \"Ungated transfer not allowed\"\n\n```move\n// Problem: Trying to transfer soul-bound object\nobject::transfer(owner, obj, recipient); // FAILS\n\n// Solution: Use TransferRef for forced transfer\nlet refs = borrow_global<Refs>(object_addr);\nobject::transfer_with_ref(&refs.transfer_ref, recipient);\n```\n\n### Error: \"Object already exists\"\n\n```move\n// Problem: Creating named object with duplicate seed\nlet constructor_ref = object::create_named_object(creator, b\"SEED\");\n\n// Solution: Check existence first or use different seed\nlet addr = object::create_object_address(&creator_addr, b\"SEED\");\nif (!object::object_exists<ObjectCore>(addr)) {\n    let constructor_ref = object::create_named_object(creator, b\"SEED\");\n    // ...\n}\n```\n\n## Security Considerations\n\n### Access Control\n\n```move\n// Verify ownership before operations\npublic fun modify_only_by_owner(\n    owner: &signer,\n    obj: Object<MyResource>\n) {\n    assert!(\n        object::is_owner(obj, signer::address_of(owner)),\n        ERROR_NOT_OWNER\n    );\n\n    // Safe to modify\n}\n```\n\n### Capability Protection\n\n```move\n// Store refs at object address (not accessible from outside)\nstruct Refs has key {\n    extend_ref: ExtendRef,\n    transfer_ref: TransferRef,\n    delete_ref: DeleteRef,\n}\n\n// ✅ Good: Stored at object address\nmove_to(&object_signer, Refs { /* ... */ });\n\n// ❌ Bad: Stored at creator address (could be accessed elsewhere)\n// move_to(creator, Refs { /* ... */ });\n```\n\n## Documentation References\n\nReference official Aptos docs:\n- aptos.dev/guides/objects\n- aptos.dev/standards/digital-asset\n- aptos.dev/standards/fungible-asset\n- Aptos Framework source: 0x1::object\n\n## Response Style\n\n- **Concept-first** - Explain object model concepts before code\n- **Pattern-driven** - Show common patterns and use cases\n- **Comparison** - Compare to resource-only approach when helpful\n- **Visual** - Use diagrams/structure to show object relationships\n- **Security-aware** - Highlight ownership and access control\n\n## Follow-up Suggestions\n\nAfter helping with objects, suggest:\n- Object composition strategies\n- Ref management patterns\n- Migration path from resources\n- Gas optimization for object operations\n- Testing object-based contracts\n- Integration with Token/FA standards\n",
        "templates/.claude/skills/aptos/shelby/cli-assistant/skill.md": "---\nname: shelby-cli-assistant\ndescription: Expert on Shelby CLI tool for command-line blob storage operations. Helps with shelby commands, uploads, downloads, account management, context configuration, faucet operations, and CLI troubleshooting. Triggers on keywords shelby cli, shelby upload, shelby download, shelby init, shelby account, shelby context, @shelby-protocol/cli, CLI installation, shelby command.\nallowed-tools: Read, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Shelby CLI Assistant\n\n## Purpose\n\nProvide expert guidance on using the Shelby Protocol CLI tool for managing accounts, uploading/downloading blobs, configuring contexts (networks), and troubleshooting command-line operations.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **CLI Tool** - Shelby CLI, @shelby-protocol/cli, command-line, terminal\n- **Commands** - shelby upload, shelby download, shelby init, shelby account\n- **Operations** - CLI upload, CLI download, account management, context setup\n- **Issues** - CLI errors, troubleshooting, insufficient tokens, configuration\n- **Setup** - Installation, initialization, funding, API keys\n\n## Knowledge Base\n\nShelby CLI documentation location:\n```\n.claude/skills/blockchain/aptos/docs/\n```\n\nKey files:\n- `tools_cli.md` - Getting started, installation, quick start\n- `tools_cli_commands_account-management.md` - Account operations\n- `tools_cli_commands_context-management.md` - Network configuration\n- `tools_cli_commands_uploads-and-downloads.md` - Upload/download commands\n- `tools_cli_commands_faucet.md` - Token acquisition\n- `tools_cli_management.md` - CLI management and updates\n\n## Installation & Setup\n\n### Installation\n\n```bash\n# npm\nnpm install -g @shelby-protocol/cli\n\n# pnpm\npnpm add -g @shelby-protocol/cli\n\n# yarn\nyarn global add @shelby-protocol/cli\n\n# bun\nbun add -g @shelby-protocol/cli\n```\n\n**Prerequisite:** Node.js and npm must be installed.\n\n### Initialization\n\n```bash\nshelby init\n```\n\n**What it does:**\n- Creates config file at `~/.shelby/config.yaml`\n- Prompts for API key (optional but recommended to avoid rate limits)\n- Sets up default contexts (shelbynet and local)\n- Creates default account\n- Sets default context and account\n\n**Example config file:**\n```yaml\ncontexts:\n  shelbynet:\n    aptos_network:\n      name: shelbynet\n      fullnode: https://api.shelbynet.shelby.xyz/v1\n      faucet: https://faucet.shelbynet.shelby.xyz\n      indexer: https://api.shelbynet.shelby.xyz/v1/graphql\n    shelby_network:\n      rpc_endpoint: https://api.shelbynet.shelby.xyz/shelby\n  local:\n    aptos_network:\n      name: local\n      fullnode: http://127.0.0.1:8080/v1\n      faucet: http://127.0.0.1:8081\n    shelby_network:\n      rpc_endpoint: http://localhost:9090/\n\naccounts:\n  alice:\n    private_key: ed25519-priv-0x8...\n    address: \"0xfcba...a51c\"\n\ndefault_context: shelbynet\ndefault_account: alice\n```\n\n## Account Management\n\n### Create Account\n\n**Interactive mode:**\n```bash\nshelby account create\n```\n\n**Non-interactive mode:**\n```bash\nshelby account create \\\n  --name alice \\\n  --scheme ed25519 \\\n  --private-key ed25519-priv-0x... \\\n  --address 0x...\n```\n\n**Options:**\n- `--name <account-name>` - Label for credentials\n- `--scheme <signature-scheme>` - Currently supports `ed25519`\n- `--private-key <key>` - Raw private key (ed25519-priv-0x… format)\n- `--address <aptos-address>` - Aptos account address (0x…)\n\n**Note:** All four flags required for non-interactive mode.\n\n### List Accounts\n\n```bash\nshelby account list\n```\n\n**Output:**\n```\n┌──────────────┬────────────────────────────────────────────────┬──────────────────┐\n│ Name         │ Address                                        │ Private Key      │\n├──────────────┼────────────────────────────────────────────────┼──────────────────┤\n│ alice        │ 0xfcb......................................0fb │ ed25519-priv-0x8 │\n│ (default)    │ c276e3e598938e00a51c                           │ adf5...          │\n└──────────────┴────────────────────────────────────────────────┴──────────────────┘\n```\n\n### Switch Account\n\n```bash\nshelby account use <ACCOUNT_NAME>\n```\n\n**Example:**\n```bash\nshelby account use alice\n# ✅ Now using account 'alice'\n```\n\n### Delete Account\n\n```bash\nshelby account delete <ACCOUNT_NAME>\n```\n\n**Example:**\n```bash\nshelby account delete bob\n# ✅ Account 'bob' deleted successfully\n```\n\n### Check Balance\n\n```bash\nshelby account balance\n```\n\n**Output:**\n```\n💰  Balance:\n\n┌─────────┬───────────────────────────────────┬─────────────────────┬───────────────────┐\n│ Token   │ Asset                             │ Balance             │ Raw Units         │\n├─────────┼───────────────────────────────────┼─────────────────────┼───────────────────┤\n│ APT     │ 0x1::aptos_coin::AptosCoin        │ 9.998885 APT        │ 999,888,500       │\n├─────────┼───────────────────────────────────┼─────────────────────┼───────────────────┤\n│ ShelbyU │ 0x1b18363a9f1fe5e6ebf247daba5cc1c │ 9.99993056          │ 999,993,056       │\n│ SD      │ 18052bb232efdc4c50f556053922d98e1 │ ShelbyUSD           │                   │\n└─────────┴───────────────────────────────────┴─────────────────────┴───────────────────┘\n```\n\n**Options:**\n- `-a, --account <name>` - Query specific account\n- `-c, --context <name>` - Use different context\n- `--address <hex>` - Query raw Aptos address\n\n### List Blobs\n\n```bash\nshelby account blobs\n```\n\n**Output:**\n```\n📦  Stored Blobs\n┌─────────────────────────────────────────────┬───────────────┬─────────────────────────┐\n│ Name                                        │ Size          │ Expires                 │\n├─────────────────────────────────────────────┼───────────────┼─────────────────────────┤\n│ .gitignore-v1                               │ 494 B         │ Oct 11, 2025, 4:03 PM   │\n└─────────────────────────────────────────────┴───────────────┴─────────────────────────┘\n```\n\n**Options:**\n- `-a, --account <name>` - List blobs for specific account\n\n**Note:** Requires Shelby indexer endpoint in context configuration.\n\n## Context Management\n\n### List Contexts\n\n```bash\nshelby context list\n```\n\n**Output shows:**\n- Aptos configurations (fullnode, indexer, faucet)\n- Shelby configurations (RPC endpoint)\n- Default context marked with `(default)`\n\n### Create/Update Context\n\n```bash\nshelby context create <CONTEXT_NAME>\nshelby context update <CONTEXT_NAME>\n```\n\n### Switch Context\n\n```bash\nshelby context use <CONTEXT_NAME>\n```\n\n## Funding Accounts\n\n### Required Tokens\n\n1. **APT** - Aptos tokens for gas fees\n2. **ShelbyUSD** - Tokens for storage and bandwidth\n\n### Get APT Tokens\n\n**Option 1: Faucet URL**\n```bash\nshelby faucet --no-open  # Prints faucet URL\n# Or remove --no-open to automatically open in browser\n```\n\n**Option 2: Aptos CLI**\n```bash\n# First configure aptos CLI\naptos init --profile shelby-alice \\\n  --assume-yes \\\n  --private-key ed25519-priv-0xa... \\\n  --network custom \\\n  --rest-url https://api.shelbynet.aptoslabs.com \\\n  --faucet-url https://faucet.shelbynet.shelby.xyz/\n\n# Fund account\naptos account fund-with-faucet \\\n  --profile shelby-alice \\\n  --amount 1000000000000000000\n```\n\n### Get ShelbyUSD Tokens\n\n```bash\nshelby faucet --no-open\n# Visit the provided faucet URL to get ShelbyUSD tokens\n```\n\n**Faucet URLs:**\n- APT: Via `shelby faucet` or Aptos CLI\n- ShelbyUSD: https://faucet.shelbynet.shelby.xyz\n\n## Upload Operations\n\n### Basic Upload\n\n```bash\nshelby upload <src> <dst> -e <expiration>\n```\n\n**Required:**\n- `<src>` - Local file path\n- `<dst>` - Blob name in Shelby (max 1024 chars, no trailing `/`)\n- `-e, --expiration` - Expiration timestamp\n\n**Example:**\n```bash\nshelby upload local-video.mp4 videos/my-video.mp4 -e tomorrow\nshelby upload ./file.txt data/file.txt --expiration \"2025-12-31\"\nshelby upload document.pdf docs/doc.pdf -e 1735689600\n```\n\n### Expiration Formats\n\n**Human-readable:**\n```bash\n-e \"tomorrow\"\n-e \"in 2 days\"\n-e \"next Friday\"\n-e \"next month\"\n-e \"2025-12-31\"\n```\n\n**Unix timestamp:**\n```bash\n-e 1735689600  # Seconds since epoch\n```\n\n**ISO date:**\n```bash\n-e \"2025-01-01T00:00:00Z\"\n-e \"2025-01-01T00:00:00-0500\"\n```\n\n**Using date command (Unix):**\n```bash\nshelby upload file.txt blob.txt -e $(date -d \"+1 hour\" +%s)\nshelby upload file.txt blob.txt -e $(date -d \"+30 days\" +%s)\n```\n\n### Upload Options\n\n**Flag** | **Alias** | **Type** | **Description**\n---|---|---|---\n`--expiration <datetime>` | `-e` | string | Expiration timestamp (required)\n`--recursive` | `-r` | flag | Upload entire directory\n`--assume-yes` | | flag | Skip cost confirmation (for scripts)\n`--output-commitments <file>` | | string | Save commitments to file\n\n### Directory Upload\n\n```bash\nshelby upload -r ./my-directory/ remote-dir/ -e \"2025-12-31\"\nshelby upload --recursive ./website/ site/ --expiration tomorrow\n```\n\n**Requirements:**\n- `<dst>` MUST end with `/` for recursive uploads\n- Follows canonical directory layout\n\n**Example:**\n\n**Local structure:**\n```\n.\n├── bar\n└── foo\n    ├── baz\n    └── buzz\n```\n\n**Uploaded as:**\n```\n<account>/remote-dir/bar\n<account>/remote-dir/foo/baz\n<account>/remote-dir/foo/buzz\n```\n\n**Note:** No directory blobs created (no `<account>/remote-dir/foo`).\n\n### Auto-confirm for Scripts\n\n```bash\n# Skip interactive cost confirmation\nshelby upload file.txt blob.txt -e tomorrow --assume-yes\n```\n\n**Use case:** Automation, CI/CD pipelines, scripts\n\n## Download Operations\n\n### Basic Download\n\n```bash\nshelby download <src> <dst>\n```\n\n**Example:**\n```bash\nshelby download videos/my-video.mp4 ./local-video.mp4\nshelby download data/file.txt ./downloaded-file.txt\n```\n\n### Download Options\n\n**Flag** | **Alias** | **Description**\n---|---|---\n`--recursive` | `-r` | Download directory (src and dst must end with `/`)\n`--force` | `-f` | Overwrite existing files\n\n### Force Overwrite\n\n```bash\nshelby download blob.txt ./existing-file.txt --force\n```\n\n### Directory Download\n\n```bash\nshelby download -r remote-dir/ ./local-dir/\nshelby download --recursive site/ ./website/\n```\n\n**Requirements:**\n- Both `<src>` and `<dst>` MUST end with `/`\n- Recreates directory structure locally\n\n**Example:**\n\n**Shelby blobs:**\n```\nmy-files/document.pdf\nmy-files/images/photo1.jpg\nmy-files/images/photo2.jpg\n```\n\n**Download:**\n```bash\nshelby download -r my-files/ ./local-files/\n```\n\n**Result:**\n```\n./local-files/\n├── document.pdf\n└── images/\n    ├── photo1.jpg\n    └── photo2.jpg\n```\n\n### Validation Rules\n\n**Without --force:**\n- Parent directory of `<dst>` must exist\n- `<dst>` file must not exist\n- For recursive: `<dst>` directory must be empty\n\n**With --force:**\n- Any existing `<dst>` completely removed before download\n\n### Download from Other Accounts\n\n**Current limitation:** CLI only downloads from active account.\n\n**Workaround - Use REST API:**\n```bash\ncurl https://api.shelbynet.shelby.xyz/shelby/v1/blobs/<account>/<blob-name>\n```\n\n**Example:**\n```bash\n# Download blob 'foo' from account 0x89ca7d...76357\ncurl https://api.shelbynet.shelby.xyz/shelby/v1/blobs/0x89ca7dfadf5788830b0d5826a56b370ced0d7938c4628f4b57f346ab54f76357/foo\n```\n\n## Common Workflows\n\n### First-Time Setup\n\n```bash\n# 1. Install CLI\nnpm install -g @shelby-protocol/cli\n\n# 2. Initialize\nshelby init\n\n# 3. List accounts\nshelby account list\n\n# 4. Fund account (get address from list command)\nshelby faucet\n\n# 5. Check balance\nshelby account balance\n\n# 6. Upload test file\nshelby upload test.txt test-blob.txt -e tomorrow --assume-yes\n\n# 7. Verify upload\nshelby account blobs\n\n# 8. Download file\nshelby download test-blob.txt downloaded-test.txt\n```\n\n### Upload Website\n\n```bash\n# Upload entire website directory\nshelby upload -r ./dist/ my-site/ -e \"2026-01-01\" --assume-yes\n\n# Verify uploaded files\nshelby account blobs\n\n# Download for verification\nshelby download -r my-site/ ./verify-download/\n```\n\n### Batch Upload with Script\n\n```bash\n#!/bin/bash\n\n# Upload multiple files without prompts\nfor file in *.mp4; do\n  shelby upload \"$file\" \"videos/$file\" \\\n    -e $(date -d \"+90 days\" +%s) \\\n    --assume-yes\ndone\n```\n\n### Context Switching\n\n```bash\n# Work with local development network\nshelby context use local\nshelby upload test.txt dev-test.txt -e tomorrow\n\n# Switch to production\nshelby context use shelbynet\nshelby upload prod.txt production-data.txt -e \"2025-12-31\"\n```\n\n## Troubleshooting\n\n### Issue: \"Insufficient ShelbyUSD tokens\"\n\n**Error:**\n```\nInsufficient Shelby tokens. Please fund your account with Shelby tokens to continue.\n```\n\n**Solution:**\n```bash\n# Check current balance\nshelby account balance\n\n# Get ShelbyUSD from faucet\nshelby faucet\n# Visit the faucet URL and fund your account\n\n# Verify balance updated\nshelby account balance\n```\n\n### Issue: \"Insufficient APT for gas\"\n\n**Solution:**\n```bash\n# Option 1: Use shelby faucet\nshelby faucet\n\n# Option 2: Use aptos CLI\naptos account fund-with-faucet --profile shelby-alice --amount 1000000000\n```\n\n### Issue: \"Blob already exists\"\n\n**Error:**\n```\nBlob name already exists\n```\n\n**Solution:**\n- Choose different blob name, OR\n- Delete old blob first, OR\n- Wait for old blob to expire\n\n### Issue: \"File changed size during upload\"\n\n**Cause:**\nFile modified between listing and upload.\n\n**Solution:**\n- Ensure files are stable during upload\n- Don't modify files during upload process\n- Use `--assume-yes` and upload atomically\n\n### Issue: \"Parent directory does not exist\"\n\n**Error on download:**\n```\nParent directory of destination must exist\n```\n\n**Solution:**\n```bash\n# Create parent directory first\nmkdir -p ./path/to/parent/\n\n# Then download\nshelby download blob.txt ./path/to/parent/file.txt\n```\n\n### Issue: \"Context not configured properly\"\n\n**Solution:**\n```bash\n# List contexts to see configuration\nshelby context list\n\n# Update context if needed\nshelby context update shelbynet\n\n# Or create new context\nshelby context create mycontext\n```\n\n### Issue: \"Command not found: shelby\"\n\n**Solution:**\n```bash\n# Reinstall globally\nnpm install -g @shelby-protocol/cli\n\n# Or ensure npm global bin is in PATH\nnpm config get prefix\nexport PATH=\"$(npm config get prefix)/bin:$PATH\"\n```\n\n## Best Practices\n\n### 1. Use API Keys\n\n```bash\n# During init, provide API key to avoid rate limits\nshelby init\n# Enter API key when prompted\n```\n\n### 2. Auto-confirm for Scripts\n\n```bash\n# Always use --assume-yes in scripts/automation\nshelby upload file.txt blob.txt -e tomorrow --assume-yes\n```\n\n### 3. Blob Naming Convention\n\n```bash\n# ✅ Use hierarchical paths\nshelby upload file.txt users/alice/documents/file.txt\n\n# ✅ Organize by type\nshelby upload video.mp4 videos/2024/november/video.mp4\n\n# ❌ Avoid trailing slashes (unless recursive)\nshelby upload file.txt path/to/file/  # Invalid\n```\n\n### 4. Check Balance Before Large Uploads\n\n```bash\n# Check balance first\nshelby account balance\n\n# Then upload\nshelby upload -r ./large-dir/ remote/ -e \"2025-12-31\"\n```\n\n### 5. Use Meaningful Expiration Times\n\n```bash\n# Temporary data\n-e \"in 7 days\"\n\n# Long-term storage\n-e \"2026-12-31\"\n\n# Time-based content\n-e $(date -d \"+90 days\" +%s)\n```\n\n### 6. Verify Uploads\n\n```bash\n# After upload, always verify\nshelby upload file.txt blob.txt -e tomorrow --assume-yes\nshelby account blobs  # Confirm blob appears\n\n# Optional: Download and compare\nshelby download blob.txt verify.txt\ndiff file.txt verify.txt\n```\n\n## Process for Helping Users\n\n### 1. Identify Issue Category\n\n**Setup Issues:**\n- Installation problems\n- Initialization failures\n- Configuration errors\n\n**Funding Issues:**\n- Insufficient tokens\n- Faucet not working\n- Balance not updating\n\n**Upload/Download Issues:**\n- Command syntax errors\n- File path problems\n- Permission errors\n- Blob naming issues\n\n**Configuration Issues:**\n- Account management\n- Context switching\n- Network selection\n\n### 2. Search Documentation\n\n```bash\n# For upload/download issues\nRead docs/tools_cli_commands_uploads-and-downloads.md\n\n# For account issues\nRead docs/tools_cli_commands_account-management.md\n\n# For setup issues\nRead docs/tools_cli.md\n```\n\n### 3. Provide Solution\n\n**Structure:**\n1. **Diagnose** - Identify root cause\n2. **Command** - Show exact command to run\n3. **Verify** - How to confirm it worked\n4. **Explain** - Why the issue occurred\n5. **Prevent** - Best practices to avoid in future\n\n### 4. Follow-up\n\n- Suggest related operations\n- Mention best practices\n- Recommend monitoring (balance checks, blob lists)\n\n## Response Style\n\n- **Command-first** - Show exact CLI commands immediately\n- **Copy-paste ready** - Format for easy terminal use\n- **Complete** - Include all required flags and options\n- **Tested** - Only suggest commands from official docs\n- **Contextual** - Consider user's platform (bash vs zsh, etc.)\n\n## Example Interaction\n\n```\nUser: \"How do I upload a directory to Shelby?\"\n\nResponse:\n1. Show recursive upload command with all options\n2. Explain directory naming requirements (trailing /)\n3. Show canonical directory layout example\n4. Mention --assume-yes for scripts\n5. Suggest verification with `shelby account blobs`\n6. Reference: docs/tools_cli_commands_uploads-and-downloads.md\n```\n\n## References\n\nWhen helping users, cite specific documentation:\n- CLI guide: `.claude/skills/blockchain/aptos/docs/tools_cli.md`\n- Commands: `.claude/skills/blockchain/aptos/docs/tools_cli_commands_*.md`\n- Config file location: `~/.shelby/config.yaml`\n\n## Limitations\n\n- Only reference official Shelby CLI documentation\n- Acknowledge current limitations (e.g., cross-account downloads)\n- Suggest REST API workarounds when CLI has limitations\n- Don't invent commands or flags not in documentation\n",
        "templates/.claude/skills/aptos/shelby/dapp-builder/skill.md": "---\nname: shelby-dapp-builder\ndescription: Expert on building decentralized applications with Shelby Protocol storage on Aptos. Helps with dApp architecture, wallet integration (Petra), browser SDK usage, React/Vue integration, file uploads, content delivery, and building Shelby-powered applications. Triggers on keywords Shelby dApp, build on Shelby, Shelby application, Petra wallet, browser storage, web3 app, decentralized app Shelby, React Shelby, Vue Shelby.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Shelby DApp Builder\n\n## Purpose\n\nGuide developers in building decentralized applications (dApps) that leverage Shelby Protocol for storage. Covers wallet integration, browser SDK usage, frontend frameworks (React, Vue), user authentication, and complete dApp architecture patterns.\n\n## When to Use\n\nAuto-invoke when users ask about:\n- **DApp Development** - Build Shelby app, create dApp, web3 application\n- **Browser Integration** - Browser SDK, client-side uploads, web app\n- **Wallet Integration** - Petra wallet, Aptos wallet, authentication\n- **Frontend Frameworks** - React Shelby, Vue Shelby, Next.js integration\n- **User Experience** - File uploads, download flows, progress tracking\n- **Architecture** - DApp architecture, frontend/backend split, design patterns\n\n## Knowledge Base\n\nDApp development documentation:\n```\n.claude/skills/blockchain/aptos/docs/\n```\n\nKey files:\n- `sdks_typescript_browser.md` - Browser SDK overview\n- `sdks_typescript_browser_guides_upload.md` - Upload workflows\n- `tools_wallets_petra-setup.md` - Wallet integration\n- `sdks_typescript_acquire-api-keys.md` - API key setup\n- `protocol.md` - Protocol fundamentals\n\n## Core DApp Architecture\n\n### Three-Layer Pattern\n\n```\n┌─────────────────────────────────────────────┐\n│         Frontend (Browser)                  │\n│  - React/Vue/vanilla JS                     │\n│  - Shelby Browser SDK                       │\n│  - Wallet integration (Petra)               │\n│  - User interface                           │\n└──────────────────┬──────────────────────────┘\n                   │ HTTPS\n                   ↓\n┌─────────────────────────────────────────────┐\n│         Shelby RPC Servers                  │\n│  - Blob storage/retrieval                   │\n│  - Erasure coding                           │\n│  - Payment processing                       │\n└──────────────────┬──────────────────────────┘\n                   │\n                   ↓\n┌─────────────────────────────────────────────┐\n│         Aptos Blockchain                    │\n│  - Smart contracts                          │\n│  - Wallet accounts                          │\n│  - State coordination                       │\n└─────────────────────────────────────────────┘\n```\n\n### Key Components\n\n**1. Wallet Integration**\n- User authentication via Petra wallet\n- Transaction signing\n- Account management\n- Balance checking\n\n**2. Shelby Browser SDK**\n- File uploads from browser\n- Blob downloads\n- Session management\n- Progress tracking\n\n**3. Frontend Framework**\n- React, Vue, or vanilla JavaScript\n- File upload UI\n- Content gallery\n- User feedback\n\n## Wallet Integration (Petra)\n\n### Setup\n\n**Install Petra Wallet:**\n```\nChrome Extension: https://petra.app/\n```\n\n**Detect Wallet:**\n```typescript\n// Check if Petra is installed\nconst isPetraInstalled = 'aptos' in window;\n\nif (!isPetraInstalled) {\n  alert('Please install Petra Wallet');\n  window.open('https://petra.app/', '_blank');\n}\n```\n\n### Connect Wallet\n\n```typescript\nimport { Network } from '@aptos-labs/ts-sdk';\n\nclass WalletManager {\n  async connect(): Promise<string> {\n    try {\n      // Request wallet connection\n      const response = await window.aptos.connect();\n\n      // Get account address\n      const account = await window.aptos.account();\n\n      console.log('Connected:', account.address);\n      return account.address;\n    } catch (error) {\n      console.error('Wallet connection failed:', error);\n      throw error;\n    }\n  }\n\n  async disconnect(): Promise<void> {\n    await window.aptos.disconnect();\n  }\n\n  async getAccount(): Promise<any> {\n    const account = await window.aptos.account();\n    return account;\n  }\n\n  async getNetwork(): Promise<string> {\n    const network = await window.aptos.network();\n    return network;\n  }\n\n  async signAndSubmitTransaction(payload: any): Promise<any> {\n    const response = await window.aptos.signAndSubmitTransaction(payload);\n    return response;\n  }\n}\n\nexport const walletManager = new WalletManager();\n```\n\n### Check Balance\n\n```typescript\nasync function checkBalance(address: string): Promise<any> {\n  const response = await fetch(\n    `https://api.shelbynet.shelby.xyz/v1/accounts/${address}/resources`\n  );\n\n  const resources = await response.json();\n\n  // Find APT and ShelbyUSD balances\n  const aptCoin = resources.find(\n    r => r.type === '0x1::coin::CoinStore<0x1::aptos_coin::AptosCoin>'\n  );\n\n  const shelbyUSD = resources.find(\n    r => r.type.includes('ShelbyUSD')\n  );\n\n  return {\n    apt: aptCoin?.data?.coin?.value || '0',\n    shelbyUSD: shelbyUSD?.data?.coin?.value || '0'\n  };\n}\n```\n\n## Browser SDK Integration\n\n### Installation\n\n```bash\nnpm install @shelby-protocol/sdk @aptos-labs/ts-sdk\n```\n\n### Initialize SDK\n\n```typescript\nimport { ShelbyClient } from '@shelby-protocol/sdk/browser';\nimport { Network, Account, Ed25519PrivateKey } from '@aptos-labs/ts-sdk';\n\nclass ShelbyService {\n  private client: ShelbyClient;\n  private account: Account;\n\n  async initialize(walletAddress: string): Promise<void> {\n    // For wallet-based signing, use wallet provider\n    // For API key usage (server-side), create account differently\n\n    this.client = new ShelbyClient({\n      network: Network.SHELBYNET,\n      apiKey: process.env.NEXT_PUBLIC_SHELBY_API_KEY // Optional\n    });\n  }\n\n  getClient(): ShelbyClient {\n    return this.client;\n  }\n}\n\nexport const shelbyService = new ShelbyService();\n```\n\n### File Upload from Browser\n\n```typescript\nclass FileUploader {\n  async uploadFile(\n    file: File,\n    onProgress?: (progress: number) => void\n  ): Promise<string> {\n    // Read file as ArrayBuffer\n    const arrayBuffer = await file.arrayBuffer();\n    const data = new Uint8Array(arrayBuffer);\n\n    // Generate blob name\n    const blobName = `uploads/${Date.now()}-${file.name}`;\n\n    // Set expiration (30 days from now)\n    const expirationTimestamp = Date.now() + 30 * 24 * 60 * 60 * 1000;\n\n    // Upload to Shelby\n    await shelbyService.getClient().uploadBlob({\n      blobName,\n      data,\n      expirationTimestamp,\n      onProgress: (progressEvent) => {\n        const percentage = (progressEvent.loaded / progressEvent.total) * 100;\n        onProgress?.(percentage);\n      }\n    });\n\n    return blobName;\n  }\n\n  async uploadMultiple(\n    files: FileList,\n    onProgress?: (file: string, progress: number) => void\n  ): Promise<string[]> {\n    const uploads = Array.from(files).map(async (file) => {\n      const blobName = await this.uploadFile(file, (progress) => {\n        onProgress?.(file.name, progress);\n      });\n      return blobName;\n    });\n\n    return await Promise.all(uploads);\n  }\n}\n\nexport const fileUploader = new FileUploader();\n```\n\n### Download Blob\n\n```typescript\nasync function downloadBlob(blobName: string, account: string): Promise<Blob> {\n  const data = await shelbyService.getClient().getBlob(blobName);\n\n  // Convert to Blob for browser download\n  return new Blob([data]);\n}\n\nasync function downloadAndSave(blobName: string, account: string, filename: string) {\n  const blob = await downloadBlob(blobName, account);\n\n  // Create download link\n  const url = URL.createObjectURL(blob);\n  const a = document.createElement('a');\n  a.href = url;\n  a.download = filename;\n  document.body.appendChild(a);\n  a.click();\n  document.body.removeChild(a);\n  URL.revokeObjectURL(url);\n}\n```\n\n## React Integration\n\n### Upload Component\n\n```typescript\nimport React, { useState } from 'react';\nimport { fileUploader } from './shelby-service';\n\nexport function FileUploadComponent() {\n  const [selectedFile, setSelectedFile] = useState<File | null>(null);\n  const [uploading, setUploading] = useState(false);\n  const [progress, setProgress] = useState(0);\n  const [blobName, setBlobName] = useState<string | null>(null);\n\n  const handleFileSelect = (event: React.ChangeEvent<HTMLInputElement>) => {\n    const file = event.target.files?.[0];\n    if (file) {\n      setSelectedFile(file);\n    }\n  };\n\n  const handleUpload = async () => {\n    if (!selectedFile) return;\n\n    setUploading(true);\n    setProgress(0);\n\n    try {\n      const result = await fileUploader.uploadFile(\n        selectedFile,\n        (percentage) => {\n          setProgress(percentage);\n        }\n      );\n\n      setBlobName(result);\n      alert('Upload successful!');\n    } catch (error) {\n      console.error('Upload failed:', error);\n      alert('Upload failed: ' + error.message);\n    } finally {\n      setUploading(false);\n    }\n  };\n\n  return (\n    <div className=\"upload-component\">\n      <h2>Upload to Shelby</h2>\n\n      <input\n        type=\"file\"\n        onChange={handleFileSelect}\n        disabled={uploading}\n      />\n\n      {selectedFile && (\n        <div className=\"file-info\">\n          <p>Selected: {selectedFile.name}</p>\n          <p>Size: {(selectedFile.size / 1024 / 1024).toFixed(2)} MB</p>\n        </div>\n      )}\n\n      <button\n        onClick={handleUpload}\n        disabled={!selectedFile || uploading}\n      >\n        {uploading ? 'Uploading...' : 'Upload'}\n      </button>\n\n      {uploading && (\n        <div className=\"progress-bar\">\n          <div\n            className=\"progress-fill\"\n            style={{ width: `${progress}%` }}\n          />\n          <span>{progress.toFixed(1)}%</span>\n        </div>\n      )}\n\n      {blobName && (\n        <div className=\"success\">\n          <p>Blob Name: {blobName}</p>\n          <p>Stored successfully on Shelby!</p>\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n### Gallery Component\n\n```typescript\nimport React, { useState, useEffect } from 'react';\n\ninterface BlobMetadata {\n  name: string;\n  size: number;\n  expirationTimestamp: number;\n  url: string;\n}\n\nexport function BlobGalleryComponent({ account }: { account: string }) {\n  const [blobs, setBlobs] = useState<BlobMetadata[]>([]);\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    loadBlobs();\n  }, [account]);\n\n  const loadBlobs = async () => {\n    try {\n      // Query indexer or smart contract for user's blobs\n      const response = await fetch(\n        `https://api.shelbynet.shelby.xyz/indexer/v1/blobs/${account}`\n      );\n\n      const data = await response.json();\n\n      const blobList = data.map((blob: any) => ({\n        name: blob.name,\n        size: blob.size,\n        expirationTimestamp: blob.expiration,\n        url: `https://api.shelbynet.shelby.xyz/shelby/v1/blobs/${account}/${blob.name}`\n      }));\n\n      setBlobs(blobList);\n    } catch (error) {\n      console.error('Failed to load blobs:', error);\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  const handleDownload = async (blob: BlobMetadata) => {\n    const filename = blob.name.split('/').pop() || 'download';\n    await downloadAndSave(blob.name, account, filename);\n  };\n\n  if (loading) {\n    return <div>Loading your blobs...</div>;\n  }\n\n  return (\n    <div className=\"blob-gallery\">\n      <h2>Your Blobs</h2>\n\n      {blobs.length === 0 ? (\n        <p>No blobs found. Upload your first file!</p>\n      ) : (\n        <div className=\"blob-grid\">\n          {blobs.map((blob) => (\n            <div key={blob.name} className=\"blob-card\">\n              <h3>{blob.name}</h3>\n              <p>Size: {(blob.size / 1024 / 1024).toFixed(2)} MB</p>\n              <p>\n                Expires: {new Date(blob.expirationTimestamp).toLocaleDateString()}\n              </p>\n\n              {blob.name.match(/\\.(jpg|jpeg|png|gif)$/i) && (\n                <img src={blob.url} alt={blob.name} className=\"blob-preview\" />\n              )}\n\n              <button onClick={() => handleDownload(blob)}>\n                Download\n              </button>\n            </div>\n          ))}\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n### Complete React App\n\n```typescript\nimport React, { useState, useEffect } from 'react';\nimport { walletManager } from './wallet-manager';\nimport { shelbyService } from './shelby-service';\nimport { FileUploadComponent } from './FileUploadComponent';\nimport { BlobGalleryComponent } from './BlobGalleryComponent';\n\nexport function App() {\n  const [connected, setConnected] = useState(false);\n  const [account, setAccount] = useState<string | null>(null);\n  const [balance, setBalance] = useState({ apt: '0', shelbyUSD: '0' });\n\n  useEffect(() => {\n    checkWalletConnection();\n  }, []);\n\n  const checkWalletConnection = async () => {\n    try {\n      const isConnected = await window.aptos?.isConnected();\n      if (isConnected) {\n        const account = await window.aptos.account();\n        setAccount(account.address);\n        setConnected(true);\n        await loadBalance(account.address);\n        await shelbyService.initialize(account.address);\n      }\n    } catch (error) {\n      console.error('Failed to check wallet:', error);\n    }\n  };\n\n  const handleConnect = async () => {\n    try {\n      const address = await walletManager.connect();\n      setAccount(address);\n      setConnected(true);\n      await loadBalance(address);\n      await shelbyService.initialize(address);\n    } catch (error) {\n      console.error('Connection failed:', error);\n    }\n  };\n\n  const handleDisconnect = async () => {\n    await walletManager.disconnect();\n    setConnected(false);\n    setAccount(null);\n  };\n\n  const loadBalance = async (address: string) => {\n    const balances = await checkBalance(address);\n    setBalance(balances);\n  };\n\n  return (\n    <div className=\"app\">\n      <header>\n        <h1>Shelby Storage DApp</h1>\n\n        {!connected ? (\n          <button onClick={handleConnect}>Connect Petra Wallet</button>\n        ) : (\n          <div className=\"wallet-info\">\n            <p>Account: {account?.slice(0, 6)}...{account?.slice(-4)}</p>\n            <p>APT: {(parseInt(balance.apt) / 1e8).toFixed(4)}</p>\n            <p>ShelbyUSD: {(parseInt(balance.shelbyUSD) / 1e8).toFixed(4)}</p>\n            <button onClick={handleDisconnect}>Disconnect</button>\n          </div>\n        )}\n      </header>\n\n      {connected && account ? (\n        <main>\n          <FileUploadComponent />\n          <BlobGalleryComponent account={account} />\n        </main>\n      ) : (\n        <div className=\"connect-prompt\">\n          <p>Please connect your Petra wallet to use this app</p>\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n## Vue Integration\n\n### Upload Component (Vue 3)\n\n```vue\n<template>\n  <div class=\"upload-component\">\n    <h2>Upload to Shelby</h2>\n\n    <input\n      type=\"file\"\n      @change=\"handleFileSelect\"\n      :disabled=\"uploading\"\n    />\n\n    <div v-if=\"selectedFile\" class=\"file-info\">\n      <p>Selected: {{ selectedFile.name }}</p>\n      <p>Size: {{ fileSizeMB }} MB</p>\n    </div>\n\n    <button\n      @click=\"handleUpload\"\n      :disabled=\"!selectedFile || uploading\"\n    >\n      {{ uploading ? 'Uploading...' : 'Upload' }}\n    </button>\n\n    <div v-if=\"uploading\" class=\"progress-bar\">\n      <div\n        class=\"progress-fill\"\n        :style=\"{ width: `${progress}%` }\"\n      />\n      <span>{{ progress.toFixed(1) }}%</span>\n    </div>\n\n    <div v-if=\"blobName\" class=\"success\">\n      <p>Blob Name: {{ blobName }}</p>\n      <p>Stored successfully on Shelby!</p>\n    </div>\n  </div>\n</template>\n\n<script setup lang=\"ts\">\nimport { ref, computed } from 'vue';\nimport { fileUploader } from './shelby-service';\n\nconst selectedFile = ref<File | null>(null);\nconst uploading = ref(false);\nconst progress = ref(0);\nconst blobName = ref<string | null>(null);\n\nconst fileSizeMB = computed(() => {\n  if (!selectedFile.value) return 0;\n  return (selectedFile.value.size / 1024 / 1024).toFixed(2);\n});\n\nconst handleFileSelect = (event: Event) => {\n  const target = event.target as HTMLInputElement;\n  const file = target.files?.[0];\n  if (file) {\n    selectedFile.value = file;\n  }\n};\n\nconst handleUpload = async () => {\n  if (!selectedFile.value) return;\n\n  uploading.value = true;\n  progress.value = 0;\n\n  try {\n    const result = await fileUploader.uploadFile(\n      selectedFile.value,\n      (percentage) => {\n        progress.value = percentage;\n      }\n    );\n\n    blobName.value = result;\n    alert('Upload successful!');\n  } catch (error) {\n    console.error('Upload failed:', error);\n    alert('Upload failed: ' + error.message);\n  } finally {\n    uploading.value = false;\n  }\n};\n</script>\n```\n\n## Complete DApp Examples\n\n### Example 1: Decentralized Image Gallery\n\n```typescript\n// Image Gallery DApp\nclass ImageGalleryDApp {\n  async uploadImage(imageFile: File, metadata: any) {\n    // Resize/compress image if needed\n    const optimized = await this.optimizeImage(imageFile);\n\n    // Upload to Shelby\n    const blobName = `gallery/${metadata.albumId}/${Date.now()}.jpg`;\n\n    await shelbyService.getClient().uploadBlob({\n      blobName,\n      data: optimized,\n      expirationTimestamp: Date.now() + 365 * 24 * 60 * 60 * 1000\n    });\n\n    // Store metadata on-chain or in separate index\n    await this.storeMetadata(blobName, metadata);\n\n    return blobName;\n  }\n\n  async loadGallery(albumId: string): Promise<Image[]> {\n    // Query blobs by prefix\n    const images = await this.queryBlobs(`gallery/${albumId}/`);\n\n    return images.map(blob => ({\n      url: this.getBlobURL(blob.name),\n      metadata: blob.metadata,\n      thumbnail: this.getBlobURL(blob.name, { size: 'thumbnail' })\n    }));\n  }\n\n  getBlobURL(blobName: string, options?: any): string {\n    const baseUrl = `https://api.shelbynet.shelby.xyz/shelby/v1/blobs`;\n    return `${baseUrl}/${this.account}/${blobName}`;\n  }\n}\n```\n\n### Example 2: Decentralized Video Platform\n\n```typescript\n// Video Streaming DApp\nclass VideoStreamingDApp {\n  async uploadVideo(videoFile: File, metadata: any) {\n    // Transcode to HLS format\n    const hlsSegments = await this.transcodeToHLS(videoFile);\n\n    // Upload each segment\n    const videoId = Date.now().toString();\n    const uploadPromises = hlsSegments.map(segment =>\n      shelbyService.getClient().uploadBlob({\n        blobName: `videos/${videoId}/${segment.name}`,\n        data: segment.data,\n        expirationTimestamp: Date.now() + 90 * 24 * 60 * 60 * 1000\n      })\n    );\n\n    await Promise.all(uploadPromises);\n\n    // Upload playlist manifest\n    await this.uploadPlaylist(videoId, hlsSegments);\n\n    // Store video metadata\n    await this.storeVideoMetadata(videoId, metadata);\n\n    return videoId;\n  }\n\n  getStreamURL(videoId: string): string {\n    return `https://api.shelbynet.shelby.xyz/shelby/v1/blobs/${this.account}/videos/${videoId}/playlist.m3u8`;\n  }\n}\n```\n\n### Example 3: Decentralized File Sharing\n\n```typescript\n// File Sharing DApp\nclass FileSharingDApp {\n  async shareFile(file: File, recipients: string[], expireDays: number) {\n    // Upload file to Shelby\n    const shareId = this.generateShareId();\n    const blobName = `shares/${shareId}/${file.name}`;\n\n    await shelbyService.getClient().uploadBlob({\n      blobName,\n      data: await file.arrayBuffer(),\n      expirationTimestamp: Date.now() + expireDays * 24 * 60 * 60 * 1000\n    });\n\n    // Create shareable link\n    const shareLink = `https://myapp.com/share/${shareId}`;\n\n    // Store access control (on-chain or off-chain)\n    await this.storeAccessControl(shareId, recipients);\n\n    return shareLink;\n  }\n\n  async accessSharedFile(shareId: string, userAccount: string) {\n    // Check access control\n    const hasAccess = await this.checkAccess(shareId, userAccount);\n    if (!hasAccess) {\n      throw new Error('Access denied');\n    }\n\n    // Download file\n    const metadata = await this.getShareMetadata(shareId);\n    return await downloadBlob(metadata.blobName, metadata.owner);\n  }\n}\n```\n\n## Best Practices\n\n### 1. User Experience\n\n**Progress Feedback:**\n```typescript\n// Always show upload/download progress\nconst [progress, setProgress] = useState(0);\n\nawait uploadFile(file, (percent) => {\n  setProgress(percent);\n  // Update UI\n});\n```\n\n**Error Handling:**\n```typescript\ntry {\n  await uploadFile(file);\n} catch (error) {\n  if (error.code === 'INSUFFICIENT_FUNDS') {\n    alert('Please add ShelbyUSD to your wallet');\n  } else if (error.code === 'FILE_TOO_LARGE') {\n    alert('File size exceeds limit');\n  } else {\n    alert('Upload failed. Please try again.');\n  }\n}\n```\n\n**Loading States:**\n```typescript\n// Show loading indicators\nconst [loading, setLoading] = useState(false);\n\n// Disable actions during operations\n<button disabled={loading || uploading}>Upload</button>\n```\n\n### 2. Performance Optimization\n\n**Image Optimization:**\n```typescript\nasync function optimizeImage(file: File): Promise<Uint8Array> {\n  return new Promise((resolve) => {\n    const img = new Image();\n    img.src = URL.createObjectURL(file);\n\n    img.onload = () => {\n      const canvas = document.createElement('canvas');\n      const ctx = canvas.getContext('2d')!;\n\n      // Resize if needed\n      const maxWidth = 1920;\n      const maxHeight = 1080;\n      let { width, height } = img;\n\n      if (width > maxWidth) {\n        height = (height * maxWidth) / width;\n        width = maxWidth;\n      }\n\n      canvas.width = width;\n      canvas.height = height;\n      ctx.drawImage(img, 0, 0, width, height);\n\n      // Convert to blob with compression\n      canvas.toBlob(\n        (blob) => {\n          blob!.arrayBuffer().then(buffer => {\n            resolve(new Uint8Array(buffer));\n          });\n        },\n        'image/jpeg',\n        0.85 // Quality\n      );\n    };\n  });\n}\n```\n\n**Lazy Loading:**\n```typescript\n// Load blobs on-demand\nconst [visibleBlobs, setVisibleBlobs] = useState<string[]>([]);\n\nuseEffect(() => {\n  const observer = new IntersectionObserver((entries) => {\n    entries.forEach(entry => {\n      if (entry.isIntersecting) {\n        // Load blob when visible\n        loadBlob(entry.target.dataset.blobName);\n      }\n    });\n  });\n\n  // Observe blob containers\n}, []);\n```\n\n### 3. Security\n\n**Wallet Integration:**\n```typescript\n// Always verify wallet connection\nconst isConnected = await window.aptos?.isConnected();\nif (!isConnected) {\n  throw new Error('Please connect wallet');\n}\n\n// Verify network\nconst network = await window.aptos.network();\nif (network !== 'shelbynet') {\n  alert('Please switch to Shelby Network');\n}\n```\n\n**Input Validation:**\n```typescript\n// Validate file types\nconst allowedTypes = ['image/jpeg', 'image/png', 'image/gif'];\nif (!allowedTypes.includes(file.type)) {\n  throw new Error('File type not supported');\n}\n\n// Validate file size\nconst maxSize = 100 * 1024 * 1024; // 100MB\nif (file.size > maxSize) {\n  throw new Error('File too large');\n}\n```\n\n### 4. Cost Management\n\n**Estimate Costs:**\n```typescript\nasync function estimateUploadCost(fileSize: number, durationDays: number): Promise<number> {\n  // Rough estimation based on size and duration\n  const costPerMBPerDay = 0.001; // Example rate in ShelbyUSD\n  const sizeMB = fileSize / (1024 * 1024);\n  return sizeMB * durationDays * costPerMBPerDay;\n}\n\n// Show cost before upload\nconst cost = await estimateUploadCost(file.size, 30);\nconst confirmed = confirm(`This upload will cost ~${cost.toFixed(4)} ShelbyUSD. Continue?`);\n```\n\n## Process for Helping Users\n\n### 1. Understand App Requirements\n\n**Questions:**\n- What type of dApp are you building?\n- What files/data need storage?\n- Who are your users?\n- What's the user flow?\n- Mobile or desktop (or both)?\n\n### 2. Design Architecture\n\n**Choose Pattern:**\n- Pure client-side (browser SDK only)\n- Hybrid (frontend + backend API)\n- Server-side rendering with Shelby\n\n**Plan Components:**\n- Authentication/wallet\n- Upload interface\n- Download/display logic\n- State management\n\n### 3. Provide Implementation\n\n**Show:**\n- Complete code examples\n- Component structure\n- Integration patterns\n- Best practices\n\n### 4. Testing & Deployment\n\n**Guide:**\n- Local testing workflow\n- Testnet deployment\n- Production considerations\n- Monitoring\n\n## Response Style\n\n- **Framework-aware** - Provide React, Vue, or vanilla JS examples\n- **Complete** - Show full component implementations\n- **User-focused** - Emphasize UX and user feedback\n- **Practical** - Working code ready to adapt\n- **Modern** - Use current best practices (hooks, composition API)\n\n## Example Interaction\n\n```\nUser: \"How do I build a React app for uploading images to Shelby?\"\n\nResponse:\n1. Show wallet integration (Petra)\n2. Provide complete React upload component\n3. Show gallery/display component\n4. Include image optimization\n5. Add progress tracking and error handling\n6. Suggest deployment strategy\n7. Reference: sdks_typescript_browser_guides_upload.md\n```\n\n## Limitations\n\n- Browser SDK has different constraints than Node.js\n- Wallet-based signing differs from API key approach\n- CORS considerations for direct API calls\n- Be clear about testnet vs mainnet differences\n\n## Follow-up Suggestions\n\n- State management (Redux, Zustand)\n- Mobile app development (React Native)\n- PWA features (offline support)\n- Testing strategies (Jest, Playwright)\n- Performance monitoring\n- User analytics\n",
        "templates/.claude/skills/aptos/shelby/network-rpc/skill.md": "---\nname: shelby-network-rpc\ndescription: Expert on Shelby Protocol network infrastructure, RPC servers, storage providers, Cavalier implementation, tile architecture, performance optimization, connection management, and DoubleZero private network. Triggers on keywords Shelby RPC, storage provider, Cavalier, tile architecture, private network, DoubleZero, network performance, RPC endpoint, request hedging, connection pooling.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Shelby Network & RPC Expert\n\n## Purpose\n\nExpert guidance on Shelby Protocol's network infrastructure, RPC server architecture, storage provider implementation (Cavalier), performance optimization, and the DoubleZero private fiber network.\n\n## When to Use\n\nAuto-invoke when users ask about:\n- **RPC Servers** - RPC architecture, endpoints, HTTP APIs, performance\n- **Storage Providers** - Cavalier implementation, tile architecture, disk I/O\n- **Network** - DoubleZero fiber network, private network, connectivity\n- **Performance** - Request hedging, connection pooling, streaming, optimization\n- **Operations** - Deployment, monitoring, scaling, troubleshooting\n- **Infrastructure** - Network topology, system components, coordination\n\n## Knowledge Base\n\nNetwork and infrastructure documentation:\n```\n.claude/skills/blockchain/aptos/docs/\n```\n\nKey files:\n- `protocol_architecture_rpcs.md` - RPC server architecture\n- `protocol_architecture_storage-providers.md` - Storage provider implementation\n- `protocol_architecture_overview.md` - System topology\n- `protocol_architecture_networks.md` - Network configuration\n- `apis_rpc_*.md` - RPC API specifications\n\n## Network Topology\n\n### Three-Layer Architecture\n\n```\n┌─────────────────────────────────────────────────┐\n│           Users (Public Internet)               │\n└──────────────────┬──────────────────────────────┘\n                   │ HTTPS\n                   ↓\n┌─────────────────────────────────────────────────┐\n│              Shelby RPC Servers                 │\n│  - Public internet connectivity                 │\n│  - Private network connectivity                 │\n│  - HTTP REST APIs                               │\n│  - Payment & session management                 │\n└──────────┬──────────────────────────────────────┘\n           │ DoubleZero Private Fiber Network\n           ↓\n┌─────────────────────────────────────────────────┐\n│         Storage Provider Servers (16/PG)        │\n│  - Cavalier implementation (C, high-perf)       │\n│  - Tile architecture                            │\n│  - Direct disk I/O                              │\n│  - Private network only                         │\n└──────────┬──────────────────────────────────────┘\n           │\n           ↓\n┌─────────────────────────────────────────────────┐\n│           Aptos L1 Blockchain                   │\n│  - Smart contracts                              │\n│  - State coordination                           │\n│  - All participants have access                 │\n└─────────────────────────────────────────────────┘\n```\n\n### DoubleZero Private Network\n\n**Purpose:**\n- Dedicated fiber network for internal communication\n- Connects RPC servers to storage providers\n- Avoids public internet limitations\n\n**Benefits:**\n- **Predictable Performance** - No public internet congestion\n- **Low Latency** - Direct fiber connections\n- **High Bandwidth** - Dedicated capacity\n- **Security** - Isolated from public networks\n- **Reliability** - Controlled infrastructure\n\n**Use Cases:**\n- Chunk retrieval during reads\n- Chunk distribution during writes\n- Storage provider health checks\n- Internal coordination\n\n## RPC Server Architecture\n\n### Core Responsibilities\n\n**User-Facing Layer:**\n1. **HTTP REST APIs** - Blob read/write operations\n2. **Session Management** - User sessions and authentication\n3. **Payment Handling** - Micropayment channels, cost calculation\n4. **Error Handling** - User-friendly error responses\n\n**Backend Coordination:**\n1. **Erasure Coding** - Encode/decode chunks during write/read\n2. **Commitment Calculations** - Verify chunk integrity\n3. **Blockchain Integration** - Query/update Aptos L1 state\n4. **Storage Provider Management** - Connection pooling, health monitoring\n\n### HTTP Endpoints\n\n**Read Operations:**\n```\nGET /v1/blobs/{account}/{blob_name}\n  - Fetch entire blob\n  - Supports HTTP range requests\n  - Returns blob data with appropriate content-type\n\nRange Request:\n  GET /v1/blobs/{account}/{blob_name}\n  Headers: Range: bytes=0-1023\n\nResponse:\n  206 Partial Content\n  Content-Range: bytes 0-1023/100000\n  [blob data]\n```\n\n**Write Operations:**\n```\nPUT /v1/blobs/{account}/{blob_name}\n  - Upload entire blob\n  - Requires session/payment authorization\n\nMultipart Upload:\n  POST /v1/multipart-uploads/start\n  POST /v1/multipart-uploads/{id}/part\n  POST /v1/multipart-uploads/{id}/complete\n```\n\n**Session Management:**\n```\nPOST /v1/sessions/create\n  - Create payment session\n  - Returns session token\n\nPOST /v1/sessions/{id}/micropayment-channel\n  - Create micropayment channel for efficient reads\n```\n\n### Read Path Workflow\n\n**User Request → RPC Response:**\n\n1. **Receive Request**\n```\nClient → GET /v1/blobs/0x123.../video.mp4\n      → RPC validates session/payment\n```\n\n2. **Cache Check** (Optional Fast Path)\n```\nRPC → Check local cache\n    → If hit: Return cached data (fast!)\n    → If miss: Continue to step 3\n```\n\n3. **Query Placement Group**\n```\nRPC → Read smart contract or indexer\n    → Identify blob's placement group\n    → Get list of 16 storage providers\n```\n\n4. **Request Chunks** (Private Network)\n```\nRPC → Request chunks from storage providers\n    → Via DoubleZero private network\n    → Need 10 of 16 chunks for reconstruction\n    → Parallel requests for low latency\n```\n\n5. **Validate & Reconstruct**\n```\nRPC → Validate chunks against commitments\n    → Decode erasure coding\n    → Reassemble original blob data\n    → Handle byte range if requested\n```\n\n6. **Return to Client**\n```\nRPC → Stream data to client\n    → Charge micropayment\n    → Update session balance\n```\n\n**Graceful Degradation:**\n- If some storage providers unavailable (up to 6)\n- Automatically use parity chunks\n- Reconstruct data from available chunks\n- Transparent to end user\n\n### Write Path Workflow\n\n**SDK Upload → RPC Coordination:**\n\n1. **Receive Upload**\n```\nClient SDK → Sends non-erasure-coded data to RPC\n           → Conserves client bandwidth\n           → Includes blob metadata and commitments\n```\n\n2. **Verify Commitments**\n```\nRPC → Independently performs erasure coding\n    → Recomputes chunk commitments\n    → Validates against on-chain metadata\n    → Ensures consistency\n```\n\n3. **Distribute Chunks** (Private Network)\n```\nRPC → Get placement group from blockchain\n    → Send each of 16 chunks to assigned storage providers\n    → Via DoubleZero private network\n    → Parallel distribution\n```\n\n4. **Collect Acknowledgments**\n```\nRPC → Storage providers validate chunks\n    → Return signed acknowledgments\n    → RPC aggregates acknowledgments\n```\n\n5. **Finalize on Blockchain**\n```\nRPC → Submit aggregated acknowledgments to smart contract\n    → Smart contract transitions blob to \"written\"\n    → Blob now available for reads\n```\n\n### Performance Optimizations\n\n#### 1. Streaming Data Pipeline\n\n**Zero-Copy Architecture:**\n```\nClient Upload → RPC receives stream\n             → Erasure code on-the-fly\n             → Stream to storage providers\n             → No large buffers needed\n```\n\n**Benefits:**\n- Reduced time-to-first-byte\n- Constant memory usage per connection\n- Supports high concurrency\n- No latency bubbles\n\n**Implementation Pattern:**\n```\nData arrives → Process immediately (don't wait for complete upload)\n            → Transform in small chunks\n            → Forward to next stage\n            → Minimize buffering\n```\n\n#### 2. Connection Pooling\n\n**Storage Provider Connections:**\n```\nRPC maintains connection pool:\n  - Pre-established TCP connections\n  - Reused across many requests\n  - Request tracking mechanism\n  - Keep flow control state fresh\n```\n\n**Benefits:**\n- No connection establishment latency\n- No TCP slow-start penalty\n- Higher throughput\n- Lower overhead\n\n#### 3. Request Hedging (Future)\n\n**Tail Latency Optimization:**\n```\nNeed 10 of 16 chunks:\n  - Request 14 chunks (over-request)\n  - Use first 10 valid responses\n  - Cancel remaining requests\n  - Reduces p99 latency\n```\n\n**Requirements:**\n- Careful network congestion management\n- Traffic prioritization\n- Cancellation support\n\n#### 4. Resource Management\n\n**Bounded Queues:**\n```\nConnection pools: Fixed capacity → Prevent memory exhaustion\nProcessing queues: Size limits → Protect during traffic spikes\n```\n\n**Backpressure:**\n```\nStorage provider congested → Apply backpressure up chain\nNetwork congested         → Slow down data ingestion\n                          → Don't buffer unlimited data\n```\n\n**Automatic Cleanup:**\n```\nSessions: Expire after timeout\nPending uploads: Clean up stale operations\nCached metadata: TTL-based eviction\n```\n\n### Scalability\n\n**Horizontal Scaling:**\n\nRPC servers are mostly stateless:\n- Session management requires some database state\n- All other operations are stateless\n- Easy to add more RPC instances\n- Load balance across instances\n\n**Session State Management:**\n```\nLocal persistent database (current implementation):\n  - SQLite or similar\n  - Stores active sessions\n  - Persists across restarts\n\nCan migrate to distributed database if needed:\n  - Redis cluster\n  - PostgreSQL\n  - Maintains scalability\n```\n\n**Deployment Pattern:**\n```\nLoad Balancer\n    ↓\n┌────────────────┬────────────────┬────────────────┐\n│  RPC Server 1  │  RPC Server 2  │  RPC Server N  │\n└────────────────┴────────────────┴────────────────┘\n```\n\n## Storage Provider Architecture (Cavalier)\n\n### Implementation Overview\n\n**Cavalier:** Jump Crypto's reference implementation\n- High-performance C codebase\n- Leverages [Firedancer](https://github.com/firedancer-io/firedancer) utilities\n- Tile-based architecture\n- Designed for maximum performance\n\n### Tile Architecture\n\n**Philosophy:**\nModern CPUs have many cores, but multi-threaded apps struggle with:\n- Cache coherency overhead\n- NUMA penalties\n- Unpredictable performance\n- Synchronization costs\n\n**Solution: Tiles**\n- Isolated processes on dedicated CPU cores\n- Communicate via shared memory\n- No locks, no cache contention\n- Explicit, predictable performance\n\n**Tile Model Principles:**\n```\nExplicit Communication:\n  - Moving data between cores is explicit\n  - No performance surprises\n\nResource Locality:\n  - Tile controls its core's caches\n  - Dedicated core scheduling\n  - No interference from other processes\n\nIsolation:\n  - Tile state is isolated\n  - Security-sensitive tiles can be sandboxed\n  - Protected state\n```\n\n**Similar To:**\n- Erlang actors\n- Go goroutines + channels\n- Seastar (shared-nothing with message passing)\n\n### Workspaces\n\n**Shared Memory Management:**\n```\nWorkspace:\n  - Section of shared memory\n  - Backed by huge pages (TLB efficiency)\n  - CPU topology aware\n  - Holds application state, queues, buffers\n\nBenefits:\n  - Efficient inter-tile communication\n  - Persistent state across restarts\n  - Debugging-friendly layout\n  - Dynamic memory allocators within workspace\n```\n\n### Cavalier Tile Types\n\n**1. System Tile**\n```\nRole: General orchestration\n  - Metrics reporting\n  - Status checking\n  - Coordination\n  - Health monitoring\n```\n\n**2. Server Tile(s)**\n```\nRole: Manage RPC communication\n  - Single-threaded epoll event loop\n  - Multiple concurrent TCP connections\n  - Protocol: Lightweight protobuf-based\n  - Buffers: Dedicated per connection\n  - Backpressure management\n\nScalability:\n  - Add more server tiles as RPC connections grow\n  - Each tile handles subset of connections\n```\n\n**3. Engine Tile**\n```\nRole: Physical storage operations\n  - Uses io_uring (async I/O)\n  - Separate read/write queues per drive\n  - Direct I/O (bypasses page cache)\n  - Predictable performance\n\nDrive Management:\n  - Minimal partition tables\n  - Fixed-depth queues per drive\n  - Fine-tuned concurrency control\n  - Characteristics-based tuning\n\nRequest Flow:\n  Server tile → Engine tile (shared memory queue)\n              → io_uring to drives\n              → Completion → Response to server tile\n```\n\n**4. Aptos Client Tile**\n```\nRole: Blockchain state access\n  - HTTP requests to Aptos Indexer (libcurl)\n  - Maintains local database of blobs/chunks\n  - Responds to metadata queries from other tiles\n  - Manages chunk expiration and deletion\n\nDatabase:\n  - Blobs this storage provider is responsible for\n  - Chunk assignments\n  - Expiration times\n  - Local cache of L1 state\n\nCleanup:\n  - Notifies engine tile of expired chunks\n  - Frees disk space\n```\n\n### Tile Communication\n\n```\n┌──────────────┐   Shared Memory    ┌──────────────┐\n│ Server Tile  │ ←─────Queue───────→ │ Engine Tile  │\n└──────────────┘                     └──────────────┘\n       ↓                                     ↓\n Shared Memory                          Direct I/O\n    Queue                              to Physical\n       ↓                                  Drives\n┌──────────────┐\n│ Aptos Client │\n│    Tile      │\n└──────────────┘\n```\n\n### Performance Characteristics\n\n**Why Tiles Are Fast:**\n- No locks, no cache bouncing\n- Explicit CPU affinity\n- Shared memory (no syscalls for communication)\n- Direct I/O (no kernel page cache overhead)\n- Dedicated cores (no context switching)\n\n**Scalability:**\n```\nMore RPC connections → Add server tiles\nLarger metadata → Shard across Aptos client tiles\nMore drives → Engine tile handles multiple drives efficiently\n```\n\n## Monitoring & Observability\n\n### Distributed Tracing\n\n**Correlation IDs:**\n```\nRequest arrives → Generate unique correlation ID\n                → Flows through all components:\n                  - RPC server\n                  - Storage providers\n                  - Blockchain interactions\n                → Enables end-to-end tracing\n```\n\n**Use Cases:**\n- Debug performance issues\n- Understand system behavior under load\n- Track failed requests\n- Analyze tail latency\n\n### Key Metrics\n\n**RPC Server Metrics:**\n```\nRequest Metrics:\n  - Request latency (p50, p95, p99)\n  - Throughput (requests/sec)\n  - Error rates by type\n  - Success rate\n\nStorage Provider Metrics:\n  - Connection health\n  - Chunk retrieval latency\n  - Availability percentage\n  - Failover events\n\nSystem Metrics:\n  - CPU usage\n  - Memory usage\n  - Network throughput\n  - Cache hit rate\n\nCost Metrics:\n  - ShelbyUSD payments\n  - Gas costs\n  - Bandwidth usage\n```\n\n**Storage Provider Metrics:**\n```\nDisk I/O:\n  - IOPS (read/write)\n  - Queue depths\n  - Latency per drive\n  - Throughput\n\nTile Metrics:\n  - Queue backlogs\n  - Processing rates\n  - Inter-tile latency\n\nNetwork:\n  - Connection count\n  - Data transferred\n  - Error rates\n```\n\n### Monitoring Integration\n\n**Standard Interfaces:**\n```\nPrometheus:\n  - Metrics endpoint\n  - Time-series data\n  - Alerting integration\n\nDistributed Tracing:\n  - OpenTelemetry compatible\n  - Jaeger/Zipkin integration\n  - Span context propagation\n\nLogging:\n  - Structured JSON logs\n  - Log levels (debug, info, warn, error)\n  - Correlation ID in every log\n```\n\n## Operational Considerations\n\n### RPC Server Operations\n\n**Deployment:**\n```\n1. Configure network endpoints (public + private)\n2. Set up session database\n3. Configure storage provider connections\n4. Set up monitoring and alerting\n5. Configure load balancer\n6. Deploy multiple instances for HA\n```\n\n**Monitoring:**\n```\n- Watch request latency (alert on p99 spikes)\n- Monitor storage provider health\n- Track error rates\n- Alert on session database issues\n- Monitor cache effectiveness\n```\n\n**Scaling:**\n```\nHorizontal: Add more RPC instances\nVertical: Increase resources per instance\nCache: Expand cache for popular content\nDatabase: Shard session database if needed\n```\n\n### Storage Provider Operations\n\n**Deployment:**\n```\n1. Provision hardware (NVMe drives, high CPU)\n2. Configure huge pages for workspaces\n3. Set CPU affinity for tiles\n4. Configure private network connectivity\n5. Register with smart contract\n6. Join placement groups\n```\n\n**Monitoring:**\n```\n- Disk health and SMART metrics\n- Tile queue depths\n- Chunk validation success rate\n- Audit performance\n- Network connectivity to RPCs\n```\n\n**Maintenance:**\n```\n- Regular disk health checks\n- Periodic integrity audits\n- Software updates\n- Data migration for failed drives\n- Graceful exit from placement groups\n```\n\n## Performance Troubleshooting\n\n### High Latency\n\n**Diagnose:**\n1. Check correlation IDs in traces\n2. Identify bottleneck component\n3. Analyze metrics for that component\n\n**Common Causes:**\n```\nStorage Provider Issues:\n  - Disk saturation (high queue depth)\n  - Network congestion\n  - Provider offline\n\nRPC Server Issues:\n  - Cache miss rate high\n  - Connection pool exhausted\n  - CPU saturation\n\nNetwork Issues:\n  - DoubleZero congestion\n  - Packet loss\n  - Routing issues\n```\n\n### Low Throughput\n\n**Diagnose:**\n```\nCheck:\n  - RPC server CPU usage\n  - Storage provider queue depths\n  - Network utilization\n  - Cache hit rate\n```\n\n**Solutions:**\n```\nScale horizontally: Add more RPC instances\nOptimize caching: Increase cache size, tune eviction\nConnection tuning: Adjust pool sizes\nBatch requests: Group operations where possible\n```\n\n## Process for Helping Users\n\n### 1. Identify Topic\n\n**Architecture Questions:**\n- \"How do RPC servers work?\"\n- \"What is the tile architecture?\"\n- \"How does the private network help?\"\n\n**Performance Questions:**\n- \"Why is my read slow?\"\n- \"How can I optimize throughput?\"\n- \"What is request hedging?\"\n\n**Operational Questions:**\n- \"How do I deploy an RPC server?\"\n- \"How to monitor storage providers?\"\n- \"How to scale the system?\"\n\n### 2. Search Documentation\n\n```bash\n# RPC architecture\nRead docs/protocol_architecture_rpcs.md\n\n# Storage providers\nRead docs/protocol_architecture_storage-providers.md\n\n# Network topology\nRead docs/protocol_architecture_overview.md\n```\n\n### 3. Provide Answer\n\n**Structure:**\n1. **Explain component** - Architecture and purpose\n2. **Show data flow** - How requests are processed\n3. **Performance aspects** - Optimizations and trade-offs\n4. **Operational guidance** - Deployment and monitoring\n5. **Troubleshooting** - Common issues and solutions\n\n### 4. Use Diagrams\n\nShow network topology, data flows, and component interactions.\n\n## Key Concepts to Reference\n\n**Three-Tier Architecture:**\n```\nUsers ← Public Internet → RPC Servers ← Private Network → Storage Providers\n                              ↓\n                        Aptos Blockchain\n```\n\n**Tile Model:**\n```\nIsolated processes + Dedicated cores + Shared memory = Predictable performance\n```\n\n**Performance Stack:**\n```\nStreaming pipeline + Connection pooling + Zero-copy + Direct I/O = High throughput\n```\n\n## Response Style\n\n- **Technical** - Detailed architecture explanations\n- **Performance-focused** - Emphasize optimizations\n- **Operational** - Practical deployment guidance\n- **Diagrammatic** - Use ASCII diagrams for clarity\n- **Referenced** - Cite specific components and papers\n\n## Example Interaction\n\n```\nUser: \"How does Shelby achieve low latency reads?\"\n\nResponse:\n1. Explain RPC caching layer (fast path)\n2. Describe private fiber network (no internet congestion)\n3. Detail parallel chunk retrieval (10 of 16, concurrent)\n4. Discuss streaming pipeline (no buffering delays)\n5. Mention connection pooling (no setup latency)\n6. Reference: protocol_architecture_rpcs.md, Cavalier tile architecture\n```\n\n## Limitations\n\n- Don't share proprietary Cavalier source code\n- Reference documented architecture and patterns\n- Acknowledge when details are implementation-specific\n- Focus on conceptual understanding and documented behavior\n\n## Follow-up Suggestions\n\n- Performance tuning strategies\n- Monitoring best practices\n- Scaling considerations\n- Firedancer project (for Cavalier deep dive)\n- Distributed systems patterns\n",
        "templates/.claude/skills/aptos/shelby/protocol-expert/skill.md": "---\nname: shelby-protocol-expert\ndescription: Expert on Shelby Protocol architecture, erasure coding, placement groups, read/write procedures, Clay Codes, chunking, storage providers, RPC servers, and decentralized storage system design on Aptos blockchain. Triggers on keywords Shelby Protocol, erasure coding, Clay Codes, placement groups, Shelby architecture, storage provider, blob storage, chunking, Shelby whitepaper.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Shelby Protocol Expert\n\n## Purpose\n\nProvide expert guidance on Shelby Protocol's architecture, design principles, erasure coding mechanisms, data durability, read/write procedures, and system components for developers building on or integrating with the decentralized storage network.\n\n## When to Use\n\nAuto-invoke when users ask about:\n- **Architecture** - Shelby system, protocol design, components, infrastructure\n- **Data Engineering** - Erasure coding, Clay Codes, chunking, placement groups\n- **Operations** - Read procedure, write procedure, blob operations, data flow\n- **System Design** - Storage providers, RPC servers, smart contracts, auditing\n- **Performance** - Bandwidth optimization, recovery algorithms, private network\n- **Concepts** - Decentralized storage, blob naming, data durability, data integrity\n\n## Knowledge Base\n\n**Full access to Shelby Protocol documentation (when available):**\n- **Location:** `docs/` (relative to this skill)\n- Pull with: `docpull https://docs.shelby.cloud -o .claude/skills/aptos/shelby/docs` (when available)\n\n**Note:** Documentation must be pulled separately. Skill works with built-in knowledge but benefits from comprehensive protocol docs.\n\nKey documentation files (when pulled):\n- `protocol.md` - Protocol introduction and key components\n- `protocol_architecture_overview.md` - Comprehensive architecture\n- `protocol_architecture_rpcs.md` - RPC server operations\n- `protocol_architecture_storage-providers.md` - Storage provider mechanics\n- `protocol_architecture_smart-contracts.md` - Smart contract layer\n- `protocol_architecture_token-economics.md` - Economic model\n- `protocol_architecture_networks.md` - Network topology\n- `protocol_architecture_white-paper.md` - Academic foundation\n\n## System Architecture\n\n### Key Components\n\n1. **Aptos Smart Contract**\n   - Manages system state\n   - Enforces Byzantine Fault Tolerance (BFT)\n   - Handles correctness-critical operations\n   - Performs data correctness audits\n   - Manages economic logic and settlements\n\n2. **Storage Provider (SP) Servers**\n   - Store erasure-coded chunks of user data\n   - Serve chunk data to RPC servers\n   - Participate in auditing system\n   - Receive micropayments for storage and bandwidth\n\n3. **Shelby RPC Servers**\n   - User-facing API endpoints\n   - Handle blob upload/download requests\n   - Perform erasure coding/decoding\n   - Manage payment channels\n   - Cache frequently accessed data\n   - Coordinate with Storage Providers\n\n4. **Private Network (DoubleZero Fiber)**\n   - Dedicated fiber network for internal communication\n   - Connects RPC servers to Storage Providers\n   - Ensures consistent high performance\n   - Avoids public internet limitations\n\n### Network Topology\n\n```\nUser (Public Internet)\n  ↓\nShelby RPC Server (Public + Private Network)\n  ↓ (Private Fiber Network)\nStorage Provider Servers (16 per placement group)\n  ↓\nAptos L1 Blockchain (accessible by all)\n```\n\n## Data Model\n\n### Accounts and Blob Naming\n\n**Namespace:**\n- User blobs stored in account-specific namespace\n- Account = hex representation of Aptos address\n- Format: `0x123.../user/defined/path/file.ext`\n\n**Blob Names:**\n- User-defined, unique within namespace\n- Max length: 1024 characters\n- Must NOT end with `/`\n- No directory structure (flat namespace)\n\n**Canonical Directory Layout:**\n```\nInput:\n  .\n  ├── bar\n  └── foo\n      ├── baz\n      └── buzz\n\nUploaded as:\n  <account>/<prefix>/bar\n  <account>/<prefix>/foo/baz\n  <account>/<prefix>/foo/buzz\n```\n\n**Important:** You can create both `<account>/foo` as a blob AND `<account>/foo/bar`, but this violates canonical structure.\n\n### Chunking & Erasure Coding\n\n**Chunkset Basics:**\n- Blob data split into 10MB fixed-size chunksets\n- Last chunkset zero-padded if needed (padding not returned on reads)\n- Each chunkset erasure coded into 16 chunks\n\n**Erasure Coding Scheme: Clay Codes**\n- **Data chunks:** 10 chunks (original user data)\n- **Parity chunks:** 6 chunks (error correction)\n- **Total:** 16 chunks per chunkset\n- **Recovery requirement:** Any 10 of 16 chunks can reconstruct data\n- **Chunk size:** 1MB each\n\n**Why Clay Codes?**\n- Optimal storage footprint (same as Reed-Solomon)\n- Bandwidth-optimized repair algorithm\n- 4x less network traffic during recovery vs Reed-Solomon\n- Efficient recovery without fetching 10 full chunks\n\n**Recovery Methods:**\n1. **Standard Recovery:** Fetch any 10 full chunks (10MB total)\n2. **Optimized Recovery:** Read smaller portions from more servers (2.5MB total)\n\n**Reference:** [Clay Codes Paper](https://www.usenix.org/system/files/conference/fast18/fast18-vajha.pdf)\n\n### Placement Groups\n\n**Purpose:**\n- Efficiently manage chunk locations without massive metadata\n- Control data locality and failure domains\n- Reduce on-chain storage requirements\n\n**How It Works:**\n1. Blob randomly assigned to a placement group (load balancing)\n2. All chunks of blob stored on same 16 storage providers\n3. Each placement group = exactly 16 storage provider slots\n4. Smart contract tracks placement group assignments, not individual chunks\n\n**Benefits:**\n- Minimal on-chain metadata\n- Predictable chunk locations\n- Simplified read/write coordination\n- Flexible failure domain management\n\n**Reference:** [Ceph RADOS Paper](https://ceph.com/assets/pdfs/weil-rados-pdsw07.pdf)\n\n## Read Procedure\n\n**Step-by-Step Process:**\n\n1. **RPC Selection**\n   - Client selects available RPC server from network\n\n2. **Session Establishment**\n   - Client creates payment mechanism with RPC server\n   - Session established for subsequent requests\n\n3. **Read Request**\n   - Client sends HTTP request specifying blob or byte range\n   - Includes payment authorization\n\n4. **Cache Check (Optional)**\n   - RPC server checks local cache\n   - Returns cached data if present (fast path)\n\n5. **Chunk Location Query**\n   - RPC server queries smart contract\n   - Identifies which storage providers hold chunks\n   - Determines placement group for blob\n\n6. **Chunk Retrieval**\n   - RPC server fetches necessary chunks from storage providers\n   - Uses private DoubleZero fiber network\n   - Pays storage providers via micropayment channel\n   - Only needs 10 of 16 chunks for reconstruction\n\n7. **Data Validation & Assembly**\n   - RPC server validates chunks against blob metadata\n   - Reassembles requested data from chunks\n   - Returns data to client\n\n8. **Incremental Payment**\n   - Client session payments deducted as data transferred\n   - Client can perform additional reads using same session\n\n## Write Procedure\n\n**Step-by-Step Process:**\n\n1. **RPC Selection**\n   - Client selects available RPC server\n\n2. **Local Erasure Coding**\n   - SDK computes erasure coded chunks locally\n   - Processes chunk-by-chunk to minimize memory\n   - Calculates cryptographic commitments for each chunk\n\n3. **Metadata Transaction**\n   - SDK submits transaction to Aptos blockchain\n   - Includes blob metadata and merkle root of chunk commitments\n   - **Storage payment processed on-chain at this point**\n   - Placement group assigned\n\n4. **Data Transmission**\n   - SDK sends original (non-erasure-coded) data to RPC\n   - Conserves bandwidth (no need to send parity chunks)\n\n5. **RPC Verification**\n   - RPC server independently performs erasure coding\n   - Recomputes chunk commitments\n   - Validates computed values match on-chain metadata\n\n6. **Chunk Distribution**\n   - RPC server distributes 16 chunks to assigned storage providers\n   - Based on blob's placement group\n   - Uses private fiber network\n\n7. **Storage Provider Acknowledgment**\n   - Each storage provider validates received chunk\n   - Returns signed acknowledgment to RPC\n\n8. **Finalization Transaction**\n   - RPC aggregates all storage provider acknowledgments\n   - Submits final transaction to smart contract\n   - Smart contract transitions blob to \"written\" state\n\n9. **Blob Available**\n   - Blob now durably stored and available for reads\n\n## Token Economics\n\n### Two-Token Model\n\n**APT (Aptos Tokens):**\n- Pay for blockchain gas fees\n- Transaction costs on Aptos L1\n- Required for smart contract interactions\n\n**ShelbyUSD:**\n- Pay for storage and bandwidth\n- Storage provider compensation\n- RPC server payments\n- Micropayment channel funding\n\n### Paid Reads Model\n\n**Why paid reads?**\n- Incentivizes storage providers to deliver good service\n- Ensures high read performance and availability\n- Aligns economic incentives with user needs\n- Supports read-heavy workloads (video streaming, AI inference, analytics)\n\n### Payment Mechanisms\n\n**Micropayment Channels:**\n- Efficient off-chain payment aggregation\n- Reduces transaction costs\n- Enables fast, frequent payments\n- Settled periodically on-chain\n\n**Storage Commitments:**\n- Pre-paid during write operation\n- Ensures data durability guarantees\n- Based on blob size and expiration time\n\n## Auditing System\n\n**Purpose:**\n- Ensure data integrity across network\n- Verify storage providers maintain chunks correctly\n- Reward honest participation\n- Penalize malicious or negligent behavior\n\n**How It Works:**\n- Smart contract periodically audits storage providers\n- Providers prove possession of chunks via cryptographic challenges\n- Successful audits earn rewards\n- Failed audits result in penalties\n\n**Benefits:**\n- Data correctness without trusted parties\n- Economic incentives for honest behavior\n- Decentralized verification\n\n## Performance Characteristics\n\n### High-Performance Design\n\n**Dedicated Bandwidth:**\n- DoubleZero private fiber network\n- Avoids public internet congestion\n- Consistent, predictable performance\n- Low latency for internal operations\n\n**Efficient Recovery:**\n- Clay Code bandwidth optimization\n- 4x less network traffic during repairs\n- Faster recovery from node/disk failures\n- Lower operational costs\n\n**Optimized for Read-Heavy Workloads:**\n- Video streaming\n- AI training and inference\n- Large-scale data analytics\n- Content delivery\n\n### Scalability\n\n**Placement Groups:**\n- Distributes load across storage providers\n- Random assignment for load balancing\n- Flexible capacity expansion\n\n**Erasure Coding:**\n- Minimizes storage overhead (1.6x vs 3x for triple replication)\n- Efficient bandwidth usage\n- Optimal storage footprint\n\n## Why Aptos?\n\n**High Transaction Throughput:**\n- Supports frequent micropayments\n- Fast finality times\n- Scalable settlement layer\n\n**Resource-Efficient Execution:**\n- Low cost for storage commitments\n- Efficient smart contract execution\n- Move language safety guarantees\n\n**Team Expertise:**\n- Aptos team from Meta's large-scale platforms\n- Experience with global distributed systems\n- Perfect match for Shelby's requirements\n\n## Why Jump Crypto?\n\n**Engineering Foundation:**\n- Built on Jump Trading Group's infrastructure experience\n- High-performance storage and compute systems\n- Expertise in:\n  - High-performance I/O\n  - Efficient concurrency\n  - Low-level code optimizations\n  - Distributed systems\n\n## Use Cases\n\n**Ideal Workloads:**\n1. **Video Streaming** - High bandwidth reads, global distribution\n2. **AI Training/Inference** - Large datasets, frequent access\n3. **Data Analytics** - Big data processing, read-heavy patterns\n4. **Content Delivery** - Static assets, global availability\n5. **Archival Storage** - Long-term data retention, periodic access\n\n**Key Requirements Met:**\n- Robust storage with data durability guarantees\n- Significant capacity (petabyte scale)\n- High read bandwidth\n- Reasonable pricing\n- User control over data\n- Censorship resistance\n\n## Design Trade-offs\n\n**Optimized For:**\n- Read-heavy workloads\n- Large blob storage (multi-MB to GB files)\n- High bandwidth requirements\n- Data durability\n\n**Not Optimized For:**\n- Frequent updates/modifications (blobs are immutable)\n- Small file storage (overhead from 10MB chunksets)\n- Low-latency random access to small portions\n- Free/subsidized storage (user-pays model)\n\n## Process for Helping Users\n\n### 1. Identify Question Category\n\n**Architecture Questions:**\n- \"How does Shelby work?\"\n- \"What are the system components?\"\n- \"How is data stored?\"\n\n**Technical Deep-Dive:**\n- \"Explain erasure coding in Shelby\"\n- \"How do placement groups work?\"\n- \"What happens during a read/write?\"\n\n**Design Decisions:**\n- \"Why Clay Codes?\"\n- \"Why Aptos blockchain?\"\n- \"Why paid reads?\"\n\n**Use Case Evaluation:**\n- \"Is Shelby good for X?\"\n- \"How does Shelby compare to Y?\"\n- \"What are the trade-offs?\"\n\n### 2. Search Documentation\n\n```bash\n# Architecture questions\nRead docs/protocol_architecture_overview.md\n\n# Erasure coding details\nGrep \"erasure|clay|chunking\" docs/ --output-mode content\n\n# Economic model\nRead docs/protocol_architecture_token-economics.md\n\n# Specific components\nRead docs/protocol_architecture_rpcs.md\nRead docs/protocol_architecture_storage-providers.md\n```\n\n### 3. Provide Comprehensive Answer\n\n**Structure:**\n1. **High-level explanation** - Core concept in simple terms\n2. **Technical details** - Precise mechanics and algorithms\n3. **Why it matters** - Benefits and trade-offs\n4. **Practical implications** - How it affects users/developers\n5. **References** - Link to specific documentation sections\n\n### 4. Use Diagrams Where Helpful\n\n**Data Flow:**\n```\nClient → RPC Server → Storage Providers (16)\n         ↓\n    Aptos Smart Contract\n```\n\n**Chunking:**\n```\n10MB Blob → Erasure Code → 16 Chunks (10 data + 6 parity)\n                          → Distributed to Placement Group\n```\n\n## Key Concepts to Reference\n\n**Data Durability:**\n- Erasure coding provides fault tolerance\n- Can lose up to 6 chunks per chunkset\n- Storage provider diversity\n- Auditing system ensures integrity\n\n**Performance:**\n- Private fiber network (DoubleZero)\n- Clay Code bandwidth optimization\n- RPC caching layer\n- Parallel chunk retrieval\n\n**Decentralization:**\n- No single point of failure\n- Aptos blockchain coordination\n- Distributed storage providers\n- Trustless auditing\n\n**Economic Alignment:**\n- Paid reads incentivize performance\n- Storage commitments ensure durability\n- Micropayment channels reduce friction\n- Auditing rewards honest behavior\n\n## Response Style\n\n- **Educational** - Explain concepts clearly with context\n- **Technical** - Provide accurate implementation details\n- **Balanced** - Discuss both benefits and trade-offs\n- **Referenced** - Cite papers and documentation\n- **Practical** - Relate concepts to real-world usage\n\n## Example Interaction\n\n```\nUser: \"How does Shelby ensure my data won't be lost?\"\n\nResponse:\n1. Explain erasure coding (10 data + 6 parity chunks)\n2. Describe placement group distribution (16 storage providers)\n3. Detail auditing system (cryptographic proofs)\n4. Discuss economic incentives (rewards for honest storage)\n5. Quantify durability (can lose 6 of 16 chunks)\n6. Reference: Clay Codes paper, protocol_architecture_overview.md\n```\n\n## Limitations\n\n- Only reference official Shelby Protocol documentation\n- Acknowledge when documentation lacks detail\n- Don't speculate on future features\n- Cite academic papers when discussing algorithms\n- Distinguish between protocol design and implementation details\n\n## Follow-up Suggestions\n\nAfter answering, suggest:\n- Related architectural components to explore\n- Practical SDK integration for concepts discussed\n- Performance implications for use case\n- Cost considerations\n- Alternative approaches and trade-offs\n",
        "templates/.claude/skills/aptos/shelby/sdk-developer/skill.md": "---\nname: shelby-sdk-developer\ndescription: Expert in Shelby Protocol TypeScript SDK for decentralized storage on Aptos. Helps with blob uploads/downloads, session management, micropayment channels, multipart uploads, and SDK integration for Node.js and browser environments. Triggers on keywords ShelbyNodeClient, ShelbyClient, @shelby-protocol/sdk, Shelby SDK, decentralized blob storage, Shelby upload, Shelby download, Shelby session.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Shelby SDK Developer\n\n## Purpose\n\nExpert guidance for developers integrating Shelby Protocol's decentralized storage system using the TypeScript SDK. Shelby is a high-performance blob storage network built on Aptos blockchain with erasure coding, micropayment channels, and dedicated private bandwidth.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **SDK Integration** - ShelbyNodeClient, ShelbyClient, @shelby-protocol/sdk\n- **Operations** - upload blob, download blob, Shelby storage, file storage\n- **Features** - session management, micropayment channel, multipart upload\n- **Environments** - Node.js Shelby, browser Shelby, TypeScript SDK\n- **Workflows** - blob operations, storage integration, decentralized storage\n\n## Knowledge Base\n\nAll Shelby Protocol documentation is located in:\n```\n.claude/skills/blockchain/aptos/docs/\n```\n\nKey documentation files:\n- `sdks_typescript.md` - SDK overview and installation\n- `sdks_typescript_core_specifications.md` - Core SDK types and functions\n- `sdks_typescript_node_specifications.md` - Node.js specific APIs\n- `sdks_typescript_browser.md` - Browser environment APIs\n- `sdks_typescript_node_guides_uploading-file.md` - Upload workflows\n- `sdks_typescript_acquire-api-keys.md` - API key setup\n- `protocol_architecture_overview.md` - System architecture\n\n## Core Concepts\n\n### 1. Shelby Architecture\n- **Aptos Smart Contract** - Manages system state and data correctness audits\n- **Storage Providers (SP)** - Store erasure-coded chunks of user data\n- **RPC Servers** - User-facing API for blob operations\n- **Private Network** - Fiber network for internal communication\n\n### 2. Data Model\n- **Blobs** - User data stored in account-specific namespaces\n- **Chunking** - Data split into 10MB chunksets, erasure coded to 16 chunks\n- **Erasure Coding** - Clay Codes provide 10 data + 6 parity chunks\n- **Placement Groups** - Manage chunk distribution across 16 storage providers\n\n### 3. SDK Components\n\n**Node.js:**\n```typescript\nimport { ShelbyNodeClient } from \"@shelby-protocol/sdk/node\";\nimport { Network } from \"@aptos-labs/ts-sdk\";\n\nconst config = {\n  network: Network.SHELBYNET,\n  apiKey: \"aptoslabs_***\",\n};\n\nconst shelbyClient = new ShelbyNodeClient(config);\n```\n\n**Browser:**\n```typescript\nimport { ShelbyClient } from '@shelby-protocol/sdk/browser';\nimport { Network } from '@aptos-labs/ts-sdk';\n\nconst config = {\n  network: Network.SHELBYNET,\n  apiKey: \"aptoslabs_***\",\n};\n\nconst shelbyClient = new ShelbyClient(config);\n```\n\n## Common Tasks\n\n### Installation\n\n```bash\nnpm install @shelby-protocol/sdk @aptos-labs/ts-sdk\n```\n\n### Upload Workflow\n\n1. **Create Client**\n```typescript\nconst client = new ShelbyNodeClient({\n  network: Network.SHELBYNET,\n  apiKey: process.env.SHELBY_API_KEY\n});\n```\n\n2. **Upload Blob**\n```typescript\n// SDK handles erasure coding and chunk distribution\nconst result = await client.uploadBlob({\n  blobName: \"user/data/file.txt\",\n  data: fileBuffer,\n  expirationTimestamp: Date.now() + (30 * 24 * 60 * 60 * 1000) // 30 days\n});\n```\n\n3. **Multipart Upload (Large Files)**\n```typescript\n// For files > chunkset size (10MB)\nconst upload = await client.startMultipartUpload({\n  blobName: \"large-dataset.bin\",\n  expirationTimestamp: futureTimestamp\n});\n\nfor (const part of fileParts) {\n  await client.uploadPart({\n    uploadId: upload.id,\n    partNumber: partNum,\n    data: part\n  });\n}\n\nawait client.completeMultipartUpload({\n  uploadId: upload.id\n});\n```\n\n### Download Workflow\n\n```typescript\n// Download entire blob\nconst blob = await client.getBlob(\"user/data/file.txt\");\n\n// Download byte range\nconst partialBlob = await client.getBlob(\"user/data/file.txt\", {\n  range: { start: 0, end: 1024 }\n});\n```\n\n### Session Management\n\n```typescript\n// Create session for multiple operations\nconst session = await client.createSession({\n  rpcUrl: \"https://api.shelbynet.shelby.xyz/shelby\",\n  paymentAmount: 1000000 // ShelbyUSD micro-units\n});\n\n// Use session for reads\nconst data = await client.getBlob(\"blob/name\", { session });\n\n// Close session when done\nawait session.close();\n```\n\n### Micropayment Channels\n\n```typescript\n// Create micropayment channel for efficient payments\nconst channel = await client.createMicropaymentChannel({\n  amount: 10000000, // ShelbyUSD micro-units\n  recipient: rpcServerAddress\n});\n\n// Channel automatically manages payments during operations\n```\n\n## SDK Specifications\n\n### Core Types\n\n**BlobMetadata:**\n```typescript\ninterface BlobMetadata {\n  blobName: string;\n  size: number;\n  chunkCount: number;\n  merkleRoot: string;\n  expirationTimestamp: number;\n  placementGroup: number;\n}\n```\n\n**UploadOptions:**\n```typescript\ninterface UploadOptions {\n  blobName: string;\n  data: Buffer | ReadableStream;\n  expirationTimestamp: number;\n  overwrite?: boolean;\n}\n```\n\n**SessionConfig:**\n```typescript\ninterface SessionConfig {\n  rpcUrl: string;\n  paymentAmount: number;\n  autoRenew?: boolean;\n}\n```\n\n### Node.js Specific\n\n**File Upload Helper:**\n```typescript\nimport { uploadFile } from \"@shelby-protocol/sdk/node\";\n\nawait uploadFile({\n  client,\n  filePath: \"/path/to/file.txt\",\n  blobName: \"stored/file.txt\",\n  expirationTimestamp: Date.now() + 30 * 24 * 60 * 60 * 1000\n});\n```\n\n**Stream Support:**\n```typescript\nimport fs from 'fs';\n\nconst readStream = fs.createReadStream('large-file.bin');\nawait client.uploadBlob({\n  blobName: \"stream-upload.bin\",\n  data: readStream,\n  expirationTimestamp: futureDate\n});\n```\n\n### Browser Specific\n\n**File Input Handling:**\n```typescript\nasync function handleFileUpload(file: File) {\n  const arrayBuffer = await file.arrayBuffer();\n\n  await shelbyClient.uploadBlob({\n    blobName: `uploads/${file.name}`,\n    data: new Uint8Array(arrayBuffer),\n    expirationTimestamp: Date.now() + 30 * 24 * 60 * 60 * 1000\n  });\n}\n```\n\n**Progress Tracking:**\n```typescript\nconst result = await shelbyClient.uploadBlob({\n  blobName: \"file.txt\",\n  data: fileData,\n  expirationTimestamp: futureDate,\n  onProgress: (progress) => {\n    console.log(`Upload: ${progress.percentage}%`);\n  }\n});\n```\n\n## Token Economics\n\n### Required Tokens\n\n1. **APT (Aptos Tokens)** - Gas fees for blockchain transactions\n2. **ShelbyUSD** - Storage and bandwidth payments\n\n### Funding Accounts\n\n**Get tokens from faucet:**\n```bash\n# APT tokens\naptos account fund-with-faucet --profile my-profile --amount 1000000000\n\n# ShelbyUSD tokens\n# Visit: https://faucet.shelbynet.shelby.xyz\n```\n\n**Check balance:**\n```typescript\nconst balance = await client.getAccountBalance();\nconsole.log(`APT: ${balance.apt}`);\nconsole.log(`ShelbyUSD: ${balance.shelbyUSD}`);\n```\n\n## Best Practices\n\n### 1. Client Initialization\n```typescript\n// ✅ Singleton pattern for long-lived applications\nclass ShelbyService {\n  private static client: ShelbyNodeClient;\n\n  static getClient() {\n    if (!this.client) {\n      this.client = new ShelbyNodeClient({\n        network: Network.SHELBYNET,\n        apiKey: process.env.SHELBY_API_KEY\n      });\n    }\n    return this.client;\n  }\n}\n```\n\n### 2. Error Handling\n```typescript\ntry {\n  await client.uploadBlob(options);\n} catch (error) {\n  if (error.code === 'INSUFFICIENT_FUNDS') {\n    // Handle funding issue\n  } else if (error.code === 'BLOB_ALREADY_EXISTS') {\n    // Handle duplicate blob\n  } else {\n    // Generic error handling\n  }\n}\n```\n\n### 3. Blob Naming\n```typescript\n// ✅ Use hierarchical paths\n\"users/0x123.../documents/report.pdf\"\n\"projects/my-app/assets/logo.png\"\n\n// ❌ Avoid ending with /\n\"users/0x123.../\" // Invalid\n\n// ✅ Canonical directory structure\n\"prefix/bar\"\n\"prefix/foo/baz\"\n\"prefix/foo/buzz\"\n```\n\n### 4. Large File Handling\n```typescript\n// Files > 10MB: Use multipart upload\nconst FILE_SIZE_THRESHOLD = 10 * 1024 * 1024; // 10MB\n\nasync function smartUpload(file: Buffer, blobName: string) {\n  if (file.length > FILE_SIZE_THRESHOLD) {\n    return await multipartUpload(file, blobName);\n  }\n  return await client.uploadBlob({ blobName, data: file, ... });\n}\n```\n\n### 5. Session Reuse\n```typescript\n// ✅ Reuse sessions for multiple operations\nconst session = await client.createSession({...});\n\nfor (const blob of blobsToDownload) {\n  await client.getBlob(blob, { session });\n}\n\nawait session.close();\n```\n\n## Common Issues & Solutions\n\n### Issue: \"Insufficient ShelbyUSD tokens\"\n**Solution:**\n```typescript\n// Check balance first\nconst balance = await client.getAccountBalance();\nif (balance.shelbyUSD < estimatedCost) {\n  throw new Error('Please fund account with ShelbyUSD');\n}\n```\n\n### Issue: \"Blob name already exists\"\n**Solution:**\n```typescript\n// Use overwrite flag or check existence\nconst exists = await client.blobExists(blobName);\nif (exists) {\n  await client.uploadBlob({ ..., overwrite: true });\n}\n```\n\n### Issue: \"Session expired\"\n**Solution:**\n```typescript\n// Enable auto-renew\nconst session = await client.createSession({\n  rpcUrl: \"...\",\n  paymentAmount: 1000000,\n  autoRenew: true // Automatically renew when funds low\n});\n```\n\n### Issue: \"RPC server unavailable\"\n**Solution:**\n```typescript\n// Implement retry logic\nasync function uploadWithRetry(options, maxRetries = 3) {\n  for (let i = 0; i < maxRetries; i++) {\n    try {\n      return await client.uploadBlob(options);\n    } catch (error) {\n      if (i === maxRetries - 1) throw error;\n      await sleep(1000 * Math.pow(2, i)); // Exponential backoff\n    }\n  }\n}\n```\n\n## Performance Optimization\n\n### 1. Concurrent Uploads\n```typescript\n// Upload multiple blobs in parallel\nconst uploads = files.map(file =>\n  client.uploadBlob({\n    blobName: file.name,\n    data: file.data,\n    expirationTimestamp: futureDate\n  })\n);\n\nawait Promise.all(uploads);\n```\n\n### 2. Byte Range Downloads\n```typescript\n// Only download needed portion\nconst header = await client.getBlob(\"large-file.bin\", {\n  range: { start: 0, end: 1023 } // First 1KB\n});\n```\n\n### 3. Local Caching\n```typescript\n// Cache frequently accessed blobs\nconst cache = new Map<string, Buffer>();\n\nasync function getCachedBlob(blobName: string) {\n  if (cache.has(blobName)) {\n    return cache.get(blobName);\n  }\n\n  const data = await client.getBlob(blobName);\n  cache.set(blobName, data);\n  return data;\n}\n```\n\n## Process for Helping Users\n\n### 1. Identify Task\n- Setup/installation\n- Upload implementation\n- Download implementation\n- Session management\n- Error troubleshooting\n- Performance optimization\n\n### 2. Search Documentation\n```bash\n# Find relevant docs\nGrep \"upload|download\" docs/ --output-mode files_with_matches\nRead docs/sdks_typescript_node_guides_uploading-file.md\n```\n\n### 3. Provide Solution\n- Show complete code example\n- Explain key concepts\n- Handle error cases\n- Reference token requirements\n- Suggest optimizations\n\n### 4. Follow-up\n- Testing recommendations\n- Monitoring suggestions\n- Cost optimization tips\n- Security best practices\n\n## References\n\nWhen helping users, cite specific documentation:\n- SDK guides: `.claude/skills/blockchain/aptos/docs/sdks_typescript_*.md`\n- Protocol architecture: `.claude/skills/blockchain/aptos/docs/protocol_architecture_*.md`\n- API endpoints: `.claude/skills/blockchain/aptos/docs/apis_rpc_*.md`\n\n## Response Style\n\n- **Code-first** - Show working examples immediately\n- **Practical** - Focus on real-world usage\n- **Complete** - Include imports, config, error handling\n- **Modern** - Use async/await, TypeScript best practices\n- **Tested** - Only suggest patterns documented in official guides\n\n## Example Interaction\n\n```\nUser: \"How do I upload a file to Shelby from Node.js?\"\n\nResponse:\n1. Install dependencies\n2. Show complete upload example with error handling\n3. Explain token requirements (APT + ShelbyUSD)\n4. Mention blob naming best practices\n5. Suggest multipart upload for large files\n6. Reference: docs/sdks_typescript_node_guides_uploading-file.md\n```\n",
        "templates/.claude/skills/aptos/shelby/skill.md": "---\nname: shelby-expert\ndescription: Expert on Shelby Protocol decentralized blob storage on Aptos blockchain. Coordinates 7 specialized sub-skills covering protocol architecture, SDK usage, smart contracts, CLI tools, RPC infrastructure, dApp building, and storage integration. Triggers on keywords Shelby Protocol, Shelby storage, decentralized storage, Aptos storage, blob storage, Shelby.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Shelby Protocol Expert\n\n## Purpose\n\nProvide expert guidance on Shelby Protocol decentralized blob storage system on Aptos blockchain. Coordinates 7 specialized sub-skills to cover all aspects of the protocol.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Shelby** - media player, platform, integration\n- **Media** - video, audio, streaming, playback\n- **SDK** - integration, API, TypeScript, JavaScript\n- **Features** - playlists, chapters, subtitles, live streaming\n- **CLI** - command-line tools, scripts\n\n## Knowledge Base\n\nDocumentation is stored in TOON format (40-60% token savings):\n- **Location:** `docs/`\n- **Index:** `docs/INDEX.md`\n- **Format:** `.toon` or `.md` files\n\n## Process\n\nWhen a user asks about Shelby:\n\n### 1. Identify Topic\n```\nCommon topics:\n- Getting started / setup\n- SDK integration (React, Vue, vanilla JS)\n- Media player configuration\n- Streaming protocols (HLS, DASH)\n- Playlist management\n- Custom UI components\n- CLI usage\n- API reference\n```\n\n### 2. Search Documentation\n\nUse Grep to find relevant docs:\n```bash\n# Search for specific topics\nGrep \"sdk|integration\" docs/ --output-mode files_with_matches\nGrep \"streaming|playback\" docs/ --output-mode content -C 3\n```\n\nCheck the INDEX.md for navigation:\n```bash\nRead docs/INDEX.md\n```\n\n### 3. Read Relevant Files\n\nRead the most relevant documentation files:\n```bash\nRead docs/path/to/relevant-doc.md\n# or .toon format if available\n```\n\n### 4. Provide Answer\n\nStructure your response:\n- **Direct answer** - solve the user's problem first\n- **Code examples** - show integration code when applicable\n- **Configuration** - provide setup instructions\n- **References** - cite specific docs (file paths) for deeper reading\n- **Best practices** - mention Shelby-specific patterns\n\n## Example Workflows\n\n### Example 1: Basic Integration\n```\nUser: \"How do I integrate Shelby into my React app?\"\n\n1. Search: Grep \"react|integration\" docs/\n2. Read: Integration guide\n3. Answer:\n   - Show npm install command\n   - Provide basic React component\n   - Explain configuration options\n   - Link to full API docs\n```\n\n### Example 2: Custom Playlists\n```\nUser: \"How do I create custom playlists with Shelby?\"\n\n1. Search: Grep \"playlist\" docs/ -i\n2. Read: Playlist documentation\n3. Answer:\n   - Explain playlist API\n   - Show creation example\n   - Discuss management methods\n   - Reference playlist options\n```\n\n### Example 3: Streaming Configuration\n```\nUser: \"What streaming formats does Shelby support?\"\n\n1. Search: Grep \"streaming|hls|dash\" docs/\n2. Read: Streaming guide\n3. Answer:\n   - List supported formats\n   - Provide configuration examples\n   - Explain adaptive bitrate\n   - Show troubleshooting tips\n```\n\n## Key Concepts to Reference\n\n**Media Player:**\n- Player initialization\n- Configuration options\n- Event handling\n- Custom controls\n- Responsive design\n\n**Streaming:**\n- HLS (HTTP Live Streaming)\n- DASH (Dynamic Adaptive Streaming)\n- Progressive download\n- Live streaming\n- DRM support (if available)\n\n**SDK Features:**\n- TypeScript/JavaScript API\n- React/Vue components\n- Plugin system\n- Theming and styling\n- Analytics integration\n\n**CLI Tools:**\n- Media processing\n- Transcoding\n- Playlist generation\n- Deployment helpers\n\n## TOON Format Notes\n\nIf documentation is in `.toon` format:\n- Most content is directly readable (tabular data)\n- Use TOON decoder for complex structures if needed:\n  ```bash\n  /Users/zach/Documents/claude-starter/.claude/skills/toon-formatter/bin/toon decode file.toon\n  ```\n\n## Limitations\n\n- Only reference official Shelby documentation\n- If docs are incomplete, acknowledge gaps\n- For latest updates, suggest checking shelby.xyz or docs.shelby.xyz\n- Don't invent APIs or features not in docs\n\n## Response Style\n\n- **Practical** - developers want working code\n- **Code-first** - show examples immediately\n- **Modern** - use current JavaScript/TypeScript patterns\n- **Cite sources** - reference specific doc paths\n\n## Follow-up Suggestions\n\nAfter answering, suggest:\n- Performance optimization\n- Error handling patterns\n- Testing strategies\n- Browser compatibility\n- Community resources or examples\n",
        "templates/.claude/skills/aptos/shelby/smart-contracts/skill.md": "---\nname: shelby-smart-contracts\ndescription: Expert on Shelby Protocol smart contracts on Aptos blockchain. Helps with blob metadata, micropayment channels, auditing system, storage commitments, placement group assignments, Move modules, and on-chain state management. Triggers on keywords Shelby smart contract, Shelby Move, blob metadata, micropayment channel, Shelby auditing, placement group assignment, storage commitment, Aptos contract.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Shelby Smart Contract Developer\n\n## Purpose\n\nExpert guidance on Shelby Protocol's smart contract layer built on Aptos blockchain using Move language. Covers blob metadata management, micropayment channels, auditing mechanisms, system participation, and on-chain coordination.\n\n## When to Use\n\nAuto-invoke when users ask about:\n- **Smart Contracts** - Shelby contracts, Move modules, on-chain logic\n- **Blob Metadata** - Blob registration, commitments, state transitions\n- **Payments** - Micropayment channels, storage payments, settlements\n- **Auditing** - Data audits, proof verification, rewards/penalties\n- **System State** - Placement groups, storage provider registration, coordination\n- **Move Development** - Aptos Move, smart contract integration, transactions\n\n## Knowledge Base\n\nSmart contract documentation:\n```\n.claude/skills/blockchain/aptos/docs/\n```\n\nKey files:\n- `protocol_architecture_smart-contracts.md` - Smart contract architecture\n- `protocol_architecture_overview.md` - System interactions\n- `protocol_architecture_token-economics.md` - Economic model\n- `protocol_architecture_white-paper.md` - Formal specifications\n\n## Smart Contract Overview\n\n### Role in System\n\n**Single Source of Truth:**\n- All critical state stored on-chain\n- Coordinates Storage Providers, RPCs, and SDKs\n- Enforces Byzantine Fault Tolerance\n- Manages economic logic and settlements\n\n**Key Functions:**\n1. **Blob Metadata Management** - Register, track, update blob state\n2. **Micropayment Channels** - Enable efficient off-chain payments\n3. **System Participation** - Manage storage provider enrollment\n4. **Auditing** - Verify data integrity, distribute rewards/penalties\n\n## Blob Metadata\n\n### Data Structure\n\n**Core Fields:**\n- **Blob Name** - User-defined identifier (max 1024 chars)\n- **Owner Account** - Aptos address of blob owner\n- **Cryptographic Commitment** - Merkle root of chunk commitments\n- **Size** - Total blob size in bytes\n- **Placement Group** - Assigned group of 16 storage providers\n- **Expiration** - Timestamp when blob expires\n- **State** - Current status (pending, written, expired)\n- **Payment Info** - Storage fees paid, amounts\n\n### Write Path (Metadata Creation)\n\n**Step 1: SDK Prepares Transaction**\n```\nUser Data → SDK computes erasure coded chunks locally\n         → SDK calculates cryptographic commitments for each chunk\n         → SDK creates Merkle tree of chunk commitments\n         → SDK prepares transaction with merkle root\n```\n\n**Step 2: Blob Registration Transaction**\n\nTransaction submitted to smart contract includes:\n- Blob name and owner account\n- Cryptographic blob commitment (merkle root)\n- Payment for storage (based on size and expiration)\n- Encoding information (chunk count, chunkset details)\n\n**Step 3: Smart Contract Execution**\n\nSmart contract:\n1. **Validates transaction** - Checks signature, account balance\n2. **Takes payment** - Deducts storage fees in ShelbyUSD\n3. **Assigns placement group** - Randomly selects from available groups\n4. **Creates metadata entry** - Initializes blob record\n5. **Sets state to \"pending\"** - Awaiting storage provider acknowledgments\n\n**Placement Group Assignment:**\n- Random selection for load balancing\n- Determines 16 storage providers for blob\n- All chunks of blob go to same placement group\n\n**Step 4: Storage Provider Acknowledgments**\n\nAfter RPC distributes chunks:\n1. Each storage provider receives chunk\n2. Provider validates chunk against commitment\n3. Provider creates **signed acknowledgment**\n4. Acknowledgments sent to RPC server\n5. RPC aggregates acknowledgments\n6. RPC submits aggregated acknowledgments on-chain\n\n**Alternative:** Storage providers can submit acknowledgments directly if RPC is unresponsive.\n\n**Step 5: State Transition to \"Written\"**\n\nWhen sufficient acknowledgments received:\n- Smart contract transitions blob to **\"written\" state**\n- Blob now considered durably stored\n- Blob available for reads\n- Write payment held for distribution via audits\n\n### Read Path (Metadata Access)\n\n**No On-Chain Updates Required:**\n- SDK and RPC read smart contract state directly\n- Or use indexer for derived information\n- Enables low latency and high throughput\n- Read operations don't modify blockchain state\n\n**Information Retrieved:**\n- Blob existence and state\n- Placement group assignment\n- Storage provider locations\n- Expiration time\n- Chunk count and commitments (for validation)\n\n## Micropayment Channels\n\n### Purpose\n\nEnable efficient payments from RPC servers to storage providers during read operations without on-chain overhead for each payment.\n\n### How It Works\n\n**Channel Lifecycle:**\n\n1. **Creation (On-Chain)**\n```\nRPC Server → Creates micropayment channel transaction\n          → Deposits initial amount (e.g., 10000000 ShelbyUSD micro-units)\n          → Smart contract locks funds\n          → Channel opened for recipient (storage provider)\n```\n\n2. **Intermediate Payments (Off-Chain)**\n```\nRead Request → RPC pays storage provider for chunk retrieval\n            → Payment signed by RPC\n            → Storage provider validates signature\n            → No blockchain transaction needed\n            → Provider accumulates signed payments\n```\n\n3. **Settlement (On-Chain)**\n```\nStorage Provider → Submits accumulated payments to smart contract\n                → Smart contract validates signatures\n                → Transfers funds from locked amount\n                → Channel balance updated\n```\n\n4. **Closure (On-Chain)**\n```\nEither Party → Submits closure transaction\n            → Smart contract settles final balance\n            → Returns unused funds to sender\n            → Channel closed\n```\n\n### Benefits\n\n**Performance:**\n- Fast read operations (no blockchain latency)\n- High throughput (unlimited off-chain payments)\n- Low cost (minimal on-chain transactions)\n\n**Security:**\n- Cryptographic signatures guarantee payment\n- Smart contract enforces settlements\n- Receiver can always claim valid payments\n- Sender's funds locked for guarantee\n\n**Flexibility:**\n- Bulk settlements reduce gas costs\n- Asynchronous payment processing\n- Channel reuse for multiple operations\n\n### Implementation Pattern\n\n```typescript\n// Pseudocode - SDK/RPC integration\nclass MicropaymentChannel {\n  async create(recipient: Address, amount: number) {\n    // Submit on-chain transaction\n    const tx = await contract.createChannel({\n      recipient,\n      amount,\n      sender: this.account.address()\n    });\n    return new Channel(tx.channelId);\n  }\n\n  async signPayment(channelId: string, amount: number) {\n    // Create signed payment (off-chain)\n    const payment = {\n      channelId,\n      amount,\n      nonce: this.getNonce()\n    };\n    return this.account.sign(payment);\n  }\n\n  async settle(signedPayments: SignedPayment[]) {\n    // Submit batch settlement (on-chain)\n    await contract.settlePayments({\n      payments: signedPayments\n    });\n  }\n}\n```\n\n## System Participation\n\n### Storage Provider Management\n\n**Joining the System:**\n\nTransaction to smart contract includes:\n- Provider identity (Aptos address)\n- Network endpoint information\n- Capacity commitment\n- Stake/bond (if required)\n\n**Smart Contract Actions:**\n1. Validates provider credentials\n2. Assigns provider to placement group slot(s)\n3. Updates placement group mappings\n4. Broadcasts provider availability\n\n**Leaving the System:**\n\n1. Provider submits exit transaction\n2. Smart contract initiates exit procedure\n3. Data migration may be required\n4. Provider removed from placement groups\n5. Stake/bond returned (if no penalties)\n\n### Placement Group Management\n\n**On-Chain Structure:**\n```\nPlacement Group {\n  id: u64,\n  storage_providers: [Address; 16],  // Exactly 16 slots\n  active: bool,\n  created_at: timestamp\n}\n\nMapping: PlacementGroupId → [StorageProviderAddress; 16]\n```\n\n**Dynamic Updates:**\n- New providers assigned to available slots\n- Exiting providers removed from slots\n- System maintains minimum provider count\n- Load balancing via random blob assignment\n\n## Auditing System\n\n### Purpose\n\nEnsure data integrity and honest storage provider behavior through cryptographic verification.\n\n### Audit Mechanism\n\n**Periodic Audits:**\n\n1. **Challenge Generation**\n```\nSmart Contract → Selects random blob and chunk\n              → Creates cryptographic challenge\n              → Broadcasts to assigned storage provider\n```\n\n2. **Proof Generation**\n```\nStorage Provider → Receives challenge\n                → Generates succinct proof of possession\n                → Proof uses chunk data and commitment\n                → Submits proof to smart contract\n```\n\n3. **Verification**\n```\nSmart Contract → Validates proof against blob commitment\n              → Checks proof correctness\n              → Updates provider reputation\n```\n\n4. **Rewards/Penalties**\n```\nSuccess → Provider earns portion of storage payment\n        → Reputation score increased\n\nFailure → Provider penalized (stake reduction)\n        → Reputation score decreased\n        → May be removed from system if repeated failures\n```\n\n### Payment Distribution\n\n**Write Payment Flow:**\n\n1. **User pays during blob registration**\n   - Payment deposited into smart contract\n   - Held in escrow\n\n2. **Storage providers acknowledge writes**\n   - Only providers with acknowledgments eligible for payment\n\n3. **Audit intervals distribute payments**\n   - Providers passing audits receive pro-rata share\n   - Payments released from escrow periodically\n\n4. **Failed audits forfeit payments**\n   - Non-compliant providers don't receive payment\n   - Payments redistributed or returned\n\n### Audit Formalization\n\n**Reference:** See [whitepaper](/protocol/architecture/white-paper) for:\n- Cryptographic commitment schemes\n- Proof generation algorithms\n- Challenge-response protocols\n- Security proofs and analysis\n\n## On-Chain State Management\n\n### State Categories\n\n**1. Blob Registry**\n- All blob metadata\n- Indexed by account + blob name\n- Stores commitments, state, expiration\n\n**2. Placement Groups**\n- Group assignments\n- Storage provider mappings\n- Availability status\n\n**3. Provider Registry**\n- Active storage providers\n- Network endpoints\n- Reputation scores\n- Stake balances\n\n**4. Payment Channels**\n- Channel metadata\n- Locked balances\n- Settlement history\n\n**5. System Parameters**\n- Pricing (storage cost per byte, per time)\n- Audit frequency\n- Minimum stake requirements\n- Penalty amounts\n\n### Read vs Write Operations\n\n**Read Operations (Free, Fast):**\n- Query blob metadata\n- Check blob existence\n- Get placement group info\n- List account blobs (via indexer)\n\n**Write Operations (Paid, Slower):**\n- Register new blob\n- Submit acknowledgments\n- Create/settle payment channels\n- Join/leave as storage provider\n- Submit audit proofs\n\n## Move Smart Contract Patterns\n\n### Resource-Oriented Architecture\n\n**Move's Resource Model:**\n```move\n// Conceptual structure (not actual Shelby code)\nmodule shelby::storage {\n    struct BlobMetadata has key {\n        name: String,\n        owner: address,\n        commitment: vector<u8>,\n        size: u64,\n        placement_group: u64,\n        expiration: u64,\n        state: u8,  // 0=pending, 1=written, 2=expired\n    }\n\n    struct PlacementGroup has key {\n        id: u64,\n        providers: vector<address>,\n        active: bool\n    }\n\n    public entry fun register_blob(\n        account: &signer,\n        name: String,\n        commitment: vector<u8>,\n        size: u64,\n        expiration: u64\n    ) {\n        // Validate inputs\n        // Take payment\n        // Assign placement group\n        // Create metadata resource\n        // Emit event\n    }\n}\n```\n\n### Transaction Patterns\n\n**1. Blob Registration**\n```typescript\n// SDK submits transaction\nconst txn = await aptosClient.generateTransaction(account.address(), {\n  function: \"shelby::storage::register_blob\",\n  type_arguments: [],\n  arguments: [\n    blobName,           // string\n    commitment,         // vector<u8>\n    blobSize,          // u64\n    expirationTime     // u64\n  ]\n});\n\nconst signedTxn = await aptosClient.signTransaction(account, txn);\nconst result = await aptosClient.submitTransaction(signedTxn);\n```\n\n**2. Acknowledgment Submission**\n```typescript\n// RPC or storage provider submits\nconst txn = await aptosClient.generateTransaction(provider.address(), {\n  function: \"shelby::storage::acknowledge_write\",\n  type_arguments: [],\n  arguments: [\n    blobOwner,         // address\n    blobName,          // string\n    chunkIndex,        // u64\n    signature          // vector<u8>\n  ]\n});\n```\n\n**3. Micropayment Channel Creation**\n```typescript\nconst txn = await aptosClient.generateTransaction(rpc.address(), {\n  function: \"shelby::payments::create_channel\",\n  type_arguments: [],\n  arguments: [\n    recipient,         // address (storage provider)\n    depositAmount      // u64 (ShelbyUSD micro-units)\n  ]\n});\n```\n\n## Integration with SDK\n\n### SDK Responsibilities\n\n**Before On-Chain Transaction:**\n1. Compute erasure coded chunks locally\n2. Calculate cryptographic commitments\n3. Build merkle tree\n4. Estimate costs\n5. Prepare transaction payload\n\n**After On-Chain Transaction:**\n1. Monitor transaction confirmation\n2. Retrieve placement group assignment\n3. Distribute chunks to storage providers\n4. Manage payment channels for reads\n\n### Transaction Sequencing\n\n**Upload Flow:**\n```\n1. SDK: Compute commitments (off-chain)\n2. SDK: Submit registration transaction (on-chain)\n3. SDK: Wait for confirmation\n4. SDK: Query placement group (on-chain read)\n5. SDK: Send data to RPC (off-chain)\n6. RPC: Distribute chunks (off-chain via private network)\n7. Providers: Submit acknowledgments (on-chain)\n8. Contract: Transition to \"written\" state (on-chain)\n```\n\n## Best Practices\n\n### 1. Gas Optimization\n\n**Batch Operations:**\n- Aggregate multiple acknowledgments in single transaction\n- Settle payment channels in bulk\n- Minimize on-chain updates\n\n**Efficient Data Structures:**\n- Use compact blob commitments\n- Store minimal metadata on-chain\n- Leverage indexers for queries\n\n### 2. Error Handling\n\n```typescript\ntry {\n  const result = await submitBlobRegistration(blob);\n  // Wait for confirmation\n  await waitForTransaction(result.hash);\n} catch (error) {\n  if (error.code === 'INSUFFICIENT_FUNDS') {\n    // Handle funding issue\n  } else if (error.code === 'BLOB_EXISTS') {\n    // Handle duplicate blob\n  } else {\n    // Generic error handling\n  }\n}\n```\n\n### 3. State Monitoring\n\n**Track Blob State:**\n```typescript\nasync function waitForWritten(account: string, blobName: string) {\n  while (true) {\n    const metadata = await contract.getBlobMetadata(account, blobName);\n\n    if (metadata.state === BlobState.WRITTEN) {\n      return metadata;\n    } else if (metadata.state === BlobState.FAILED) {\n      throw new Error('Blob write failed');\n    }\n\n    await sleep(5000); // Poll every 5 seconds\n  }\n}\n```\n\n### 4. Payment Management\n\n**Monitor Channel Balance:**\n```typescript\nasync function ensureChannelFunded(channelId: string, requiredAmount: number) {\n  const channel = await contract.getChannel(channelId);\n\n  if (channel.balance < requiredAmount) {\n    // Top up channel\n    await contract.fundChannel(channelId, additionalAmount);\n  }\n}\n```\n\n## Process for Helping Users\n\n### 1. Identify Question Type\n\n**Architecture Questions:**\n- \"How do smart contracts manage blob state?\"\n- \"What happens on-chain during upload?\"\n- \"How do micropayment channels work?\"\n\n**Integration Questions:**\n- \"How do I submit a blob registration transaction?\"\n- \"How do I query blob metadata?\"\n- \"How do I create a payment channel?\"\n\n**Debugging Questions:**\n- \"Why is my blob stuck in pending state?\"\n- \"Transaction failed with error X\"\n- \"How do I verify acknowledgments were submitted?\"\n\n### 2. Search Documentation\n\n```bash\n# Smart contract architecture\nRead docs/protocol_architecture_smart-contracts.md\n\n# Overall system flow\nRead docs/protocol_architecture_overview.md\n\n# Economic model\nRead docs/protocol_architecture_token-economics.md\n```\n\n### 3. Provide Answer\n\n**Structure:**\n1. **Concept** - Explain on-chain mechanism\n2. **Flow** - Show transaction sequence\n3. **Code** - Provide integration example\n4. **Verification** - How to confirm success\n5. **Edge Cases** - Common issues and solutions\n\n### 4. Reference Official Sources\n\n- Smart contract documentation\n- Aptos Move documentation\n- Whitepaper for formal specifications\n- SDK integration guides\n\n## Key Concepts to Reference\n\n**State Transitions:**\n```\nBlob States:\n  pending → written → expired\n           ↓\n         failed\n```\n\n**Payment Flow:**\n```\nUser → Storage Payment → Smart Contract (escrow)\n                       → Audit-based distribution\n                       → Storage Providers (rewards)\n```\n\n**Audit Cycle:**\n```\nChallenge → Proof → Verification → Reward/Penalty\n  ↓                                      ↓\nSmart Contract                   Reputation Update\n```\n\n## Response Style\n\n- **Architectural** - Explain how components interact\n- **Secure** - Emphasize cryptographic guarantees\n- **Practical** - Show real transaction examples\n- **Referenced** - Cite Move patterns and Aptos docs\n- **Complete** - Cover both on-chain and off-chain aspects\n\n## Example Interaction\n\n```\nUser: \"How does Shelby verify storage providers actually stored my data?\"\n\nResponse:\n1. Explain blob commitment during registration\n2. Describe acknowledgment mechanism\n3. Detail periodic audit system with cryptographic proofs\n4. Show proof verification in smart contract\n5. Explain reward/penalty distribution\n6. Reference: protocol_architecture_smart-contracts.md, whitepaper\n```\n\n## Limitations\n\n- Don't show actual Shelby Move contract code (not public)\n- Reference conceptual patterns and documented behavior\n- For implementation details, defer to official documentation\n- Acknowledge when internals are proprietary\n- Focus on documented interfaces and behaviors\n\n## Follow-up Suggestions\n\nAfter answering, suggest:\n- Related on-chain operations\n- SDK integration patterns\n- Transaction monitoring strategies\n- Gas optimization techniques\n- Error handling best practices\n- Aptos Move learning resources\n",
        "templates/.claude/skills/aptos/shelby/storage-integration/skill.md": "---\nname: shelby-storage-integration\ndescription: Expert on integrating Shelby Protocol decentralized storage into applications. Helps with use case evaluation, architecture design, migration strategies, cost optimization, performance tuning for video streaming, AI training, data analytics, content delivery, and read-heavy workloads. Triggers on keywords integrate Shelby, decentralized storage integration, video streaming storage, AI training data, data analytics storage, migration to Shelby, storage architecture, content delivery, Shelby use case.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Shelby Storage Integration Expert\n\n## Purpose\n\nGuide developers and architects in integrating Shelby Protocol's decentralized storage into their applications. Covers use case evaluation, architecture design patterns, migration strategies, cost optimization, and performance tuning for read-heavy workloads.\n\n## When to Use\n\nAuto-invoke when users ask about:\n- **Integration** - Integrate Shelby, add decentralized storage, use Shelby in app\n- **Use Cases** - Video streaming, AI training, data analytics, content delivery\n- **Architecture** - Storage architecture, system design, data flow\n- **Migration** - Migrate to Shelby, move from S3/GCS, centralized to decentralized\n- **Optimization** - Cost optimization, performance tuning, bandwidth efficiency\n- **Evaluation** - Is Shelby right for X, Shelby vs alternatives, trade-offs\n\n## Knowledge Base\n\nIntegration documentation:\n```\n.claude/skills/blockchain/aptos/docs/\n```\n\nKey files:\n- `protocol.md` - Protocol introduction and key benefits\n- `protocol_architecture_overview.md` - System architecture\n- `sdks_typescript.md` - SDK integration guides\n- `protocol_architecture_token-economics.md` - Cost model\n- `tools_ai-llms.md` - AI/LLM integration patterns\n\n## Ideal Use Cases\n\n### 1. Video Streaming\n\n**Why Shelby Excels:**\n- High read bandwidth for concurrent viewers\n- Global distribution via decentralized storage providers\n- Pay-per-read model aligns with usage patterns\n- Private fiber network ensures consistent performance\n- Supports HLS/DASH chunked streaming\n\n**Architecture Pattern:**\n```\nVideo Upload Flow:\n  Producer → Transcode to HLS/DASH\n          → Upload segments to Shelby\n          → Store playlist manifest\n          → Set expiration based on content lifecycle\n\nVideo Playback Flow:\n  Player → Request manifest\n         → Shelby RPC serves playlist\n         → Player requests segments\n         → RPC retrieves chunks from storage providers\n         → Cached segments served with low latency\n```\n\n**Example Integration:**\n```typescript\nimport { ShelbyNodeClient } from '@shelby-protocol/sdk/node';\nimport { Network } from '@aptos-labs/ts-sdk';\n\nclass VideoStreamingService {\n  private shelby: ShelbyNodeClient;\n\n  constructor() {\n    this.shelby = new ShelbyNodeClient({\n      network: Network.SHELBYNET,\n      apiKey: process.env.SHELBY_API_KEY\n    });\n  }\n\n  async uploadVideo(videoPath: string, videoId: string) {\n    // Transcode to HLS\n    const segments = await this.transcodeToHLS(videoPath);\n\n    // Upload each segment\n    const expirationTime = Date.now() + 90 * 24 * 60 * 60 * 1000; // 90 days\n\n    for (const segment of segments) {\n      await this.shelby.uploadBlob({\n        blobName: `videos/${videoId}/${segment.name}`,\n        data: segment.data,\n        expirationTimestamp: expirationTime\n      });\n    }\n\n    // Upload playlist manifest\n    await this.shelby.uploadBlob({\n      blobName: `videos/${videoId}/playlist.m3u8`,\n      data: this.generatePlaylist(segments),\n      expirationTimestamp: expirationTime\n    });\n\n    return this.getStreamingURL(videoId);\n  }\n\n  getStreamingURL(videoId: string): string {\n    return `https://api.shelbynet.shelby.xyz/shelby/v1/blobs/${this.account}/videos/${videoId}/playlist.m3u8`;\n  }\n}\n```\n\n**Cost Optimization:**\n- Set appropriate expiration times (remove old content)\n- Use adaptive bitrate (multiple quality tiers)\n- Leverage RPC caching for popular content\n- Consider tiered storage (hot vs cold)\n\n### 2. AI Training & Inference\n\n**Why Shelby Excels:**\n- Store large training datasets (multi-TB)\n- High read bandwidth for distributed training\n- Durable storage with erasure coding\n- Cost-effective for long-term dataset storage\n- Fast random access to dataset samples\n\n**Architecture Pattern:**\n```\nTraining Pipeline:\n  Data Collection → Clean & Label\n                 → Upload to Shelby\n                 → Create dataset index\n                 → Distributed training nodes fetch samples\n                 → Model checkpoints stored in Shelby\n\nInference Pipeline:\n  Model artifacts in Shelby\n  → Inference service downloads model\n  → Cache model locally\n  → Serve predictions\n```\n\n**Example Integration:**\n```typescript\nclass AIDatasetManager {\n  async uploadDataset(datasetPath: string, datasetName: string) {\n    const files = await this.listFiles(datasetPath);\n\n    // Upload all dataset files\n    await Promise.all(\n      files.map(file =>\n        this.shelby.uploadBlob({\n          blobName: `datasets/${datasetName}/${file.relativePath}`,\n          data: fs.readFileSync(file.fullPath),\n          expirationTimestamp: Date.now() + 365 * 24 * 60 * 60 * 1000 // 1 year\n        })\n      )\n    );\n\n    // Create dataset index\n    const index = {\n      name: datasetName,\n      files: files.map(f => f.relativePath),\n      totalSize: files.reduce((sum, f) => sum + f.size, 0),\n      createdAt: Date.now()\n    };\n\n    await this.shelby.uploadBlob({\n      blobName: `datasets/${datasetName}/index.json`,\n      data: Buffer.from(JSON.stringify(index)),\n      expirationTimestamp: Date.now() + 365 * 24 * 60 * 60 * 1000\n    });\n  }\n\n  async downloadSample(datasetName: string, samplePath: string): Promise<Buffer> {\n    return await this.shelby.getBlob(`datasets/${datasetName}/${samplePath}`);\n  }\n\n  async streamDataset(datasetName: string, callback: (sample: Buffer) => void) {\n    const index = await this.getDatasetIndex(datasetName);\n\n    for (const file of index.files) {\n      const data = await this.downloadSample(datasetName, file);\n      callback(data);\n    }\n  }\n}\n```\n\n**Best Practices:**\n- Chunk large files for parallel download\n- Implement local caching layer\n- Use batch downloads for training epochs\n- Version datasets with naming conventions\n- Compress data before upload\n\n### 3. Data Analytics & Big Data\n\n**Why Shelby Excels:**\n- Store raw data, processed results, and archives\n- High-throughput batch reads\n- Durable long-term storage\n- Cost-effective for data lakes\n- Supports columnar formats (Parquet, ORC)\n\n**Architecture Pattern:**\n```\nAnalytics Pipeline:\n  Data Sources → Ingest to Shelby (raw data)\n              → Spark/Dask jobs read from Shelby\n              → Process and analyze\n              → Write results back to Shelby\n              → Dashboards query results\n\nData Lake Structure:\n  raw/YYYY/MM/DD/source/data.parquet\n  processed/YYYY/MM/DD/dataset/results.parquet\n  aggregates/YYYY/MM/metrics.json\n```\n\n**Example Integration:**\n```typescript\nclass DataLake {\n  async ingestRawData(source: string, data: Buffer) {\n    const date = new Date();\n    const path = `raw/${date.getFullYear()}/${date.getMonth() + 1}/${date.getDate()}/${source}/${Date.now()}.parquet`;\n\n    await this.shelby.uploadBlob({\n      blobName: path,\n      data: data,\n      expirationTimestamp: Date.now() + 730 * 24 * 60 * 60 * 1000 // 2 years\n    });\n\n    return path;\n  }\n\n  async runAnalysis(inputPaths: string[], outputPath: string) {\n    // Download raw data\n    const datasets = await Promise.all(\n      inputPaths.map(path => this.shelby.getBlob(path))\n    );\n\n    // Process with analytics engine\n    const results = await this.processData(datasets);\n\n    // Upload results\n    await this.shelby.uploadBlob({\n      blobName: outputPath,\n      data: results,\n      expirationTimestamp: Date.now() + 365 * 24 * 60 * 60 * 1000\n    });\n  }\n\n  async queryMetrics(date: Date): Promise<any> {\n    const path = `aggregates/${date.getFullYear()}/${date.getMonth() + 1}/metrics.json`;\n    const data = await this.shelby.getBlob(path);\n    return JSON.parse(data.toString());\n  }\n}\n```\n\n**Optimization Strategies:**\n- Partition data by time/category\n- Use efficient formats (Parquet, ORC)\n- Implement metadata indexing\n- Cache frequently accessed aggregates\n- Lifecycle management for archival data\n\n### 4. Content Delivery Network (CDN)\n\n**Why Shelby Excels:**\n- Global distribution of static assets\n- Censorship-resistant content delivery\n- Pay-per-use model (no upfront capacity planning)\n- Automatic redundancy and availability\n- Decentralized infrastructure\n\n**Architecture Pattern:**\n```\nCDN Integration:\n  Build Process → Generate static assets\n               → Upload to Shelby\n               → Update DNS/routing\n               → Serve via Shelby RPC endpoints\n\nAsset Types:\n  - JavaScript bundles\n  - CSS stylesheets\n  - Images (optimized)\n  - Fonts\n  - HTML pages\n```\n\n**Example Integration:**\n```typescript\nclass ShelbyBasedCDN {\n  async deployWebsite(buildDir: string, siteId: string) {\n    const files = await this.getAllFiles(buildDir);\n\n    // Upload all static assets\n    for (const file of files) {\n      const contentType = this.getContentType(file.name);\n\n      await this.shelby.uploadBlob({\n        blobName: `sites/${siteId}/${file.relativePath}`,\n        data: fs.readFileSync(file.fullPath),\n        expirationTimestamp: Date.now() + 365 * 24 * 60 * 60 * 1000\n      });\n    }\n\n    // Generate asset manifest\n    const manifest = {\n      siteId,\n      files: files.map(f => ({\n        path: f.relativePath,\n        hash: this.hashFile(f.fullPath),\n        size: f.size\n      })),\n      deployedAt: Date.now()\n    };\n\n    await this.shelby.uploadBlob({\n      blobName: `sites/${siteId}/manifest.json`,\n      data: Buffer.from(JSON.stringify(manifest)),\n      expirationTimestamp: Date.now() + 365 * 24 * 60 * 60 * 1000\n    });\n\n    return this.getSiteURL(siteId);\n  }\n\n  getSiteURL(siteId: string): string {\n    return `https://api.shelbynet.shelby.xyz/shelby/v1/blobs/${this.account}/sites/${siteId}`;\n  }\n\n  async getAsset(siteId: string, assetPath: string): Promise<Buffer> {\n    return await this.shelby.getBlob(`sites/${siteId}/${assetPath}`);\n  }\n}\n```\n\n**Performance Tips:**\n- Use content hashing for cache busting\n- Implement edge caching layer\n- Compress assets (gzip, brotli)\n- Optimize images before upload\n- Use immutable URLs for versioning\n\n### 5. Archival & Backup Storage\n\n**Why Shelby Excels:**\n- Durable long-term storage (erasure coding)\n- Cost-effective for infrequently accessed data\n- Cryptographic integrity verification\n- Decentralized redundancy\n- No vendor lock-in\n\n**Architecture Pattern:**\n```\nBackup Strategy:\n  Production Data → Periodic snapshots\n                 → Upload to Shelby\n                 → Verify upload success\n                 → Track backup metadata\n                 → Periodic restore tests\n\nRetention Policy:\n  Daily: 7 days\n  Weekly: 4 weeks\n  Monthly: 12 months\n  Yearly: indefinite\n```\n\n**Example Integration:**\n```typescript\nclass BackupManager {\n  async createBackup(database: string, backupName: string) {\n    // Export database\n    const backup = await this.exportDatabase(database);\n\n    // Compress backup\n    const compressed = await this.compress(backup);\n\n    // Upload to Shelby\n    const result = await this.shelby.uploadBlob({\n      blobName: `backups/${database}/${backupName}.tar.gz`,\n      data: compressed,\n      expirationTimestamp: this.getRetentionExpiration(backupName)\n    });\n\n    // Store backup metadata\n    await this.recordBackup({\n      database,\n      name: backupName,\n      size: compressed.length,\n      blobName: `backups/${database}/${backupName}.tar.gz`,\n      createdAt: Date.now()\n    });\n\n    return result;\n  }\n\n  async restoreBackup(database: string, backupName: string) {\n    // Download from Shelby\n    const compressed = await this.shelby.getBlob(`backups/${database}/${backupName}.tar.gz`);\n\n    // Decompress\n    const backup = await this.decompress(compressed);\n\n    // Restore database\n    await this.importDatabase(database, backup);\n  }\n\n  getRetentionExpiration(backupName: string): number {\n    if (backupName.includes('daily')) {\n      return Date.now() + 7 * 24 * 60 * 60 * 1000; // 7 days\n    } else if (backupName.includes('weekly')) {\n      return Date.now() + 28 * 24 * 60 * 60 * 1000; // 4 weeks\n    } else if (backupName.includes('monthly')) {\n      return Date.now() + 365 * 24 * 60 * 60 * 1000; // 1 year\n    } else {\n      return Date.now() + 3650 * 24 * 60 * 60 * 1000; // 10 years\n    }\n  }\n}\n```\n\n## Integration Architecture Patterns\n\n### Pattern 1: Direct Integration\n\n```\nApplication → Shelby SDK → Shelby RPC → Storage Providers\n```\n\n**When to use:**\n- Simple applications\n- Low request volume\n- Minimal caching needs\n\n**Implementation:**\n```typescript\n// Direct SDK usage in application\napp.get('/video/:id', async (req, res) => {\n  const video = await shelbyClient.getBlob(`videos/${req.params.id}/stream.mp4`);\n  res.send(video);\n});\n```\n\n### Pattern 2: Caching Layer\n\n```\nApplication → Local Cache → Shelby SDK → Shelby RPC\n```\n\n**When to use:**\n- High read frequency\n- Popular content\n- Latency-sensitive applications\n\n**Implementation:**\n```typescript\nclass CachedShelbyClient {\n  private cache: Map<string, Buffer> = new Map();\n\n  async getBlob(blobName: string): Promise<Buffer> {\n    // Check cache first\n    if (this.cache.has(blobName)) {\n      return this.cache.get(blobName)!;\n    }\n\n    // Fetch from Shelby\n    const data = await this.shelby.getBlob(blobName);\n\n    // Cache for future requests\n    this.cache.set(blobName, data);\n\n    return data;\n  }\n}\n```\n\n### Pattern 3: Asynchronous Upload\n\n```\nApplication → Queue → Worker → Shelby SDK\n```\n\n**When to use:**\n- High upload volume\n- Background processing\n- Decoupled architecture\n\n**Implementation:**\n```typescript\n// Producer\nasync function handleFileUpload(file: File) {\n  await queue.enqueue({\n    type: 'UPLOAD_TO_SHELBY',\n    payload: {\n      filePath: file.path,\n      blobName: `uploads/${Date.now()}-${file.name}`,\n      expirationTime: Date.now() + 30 * 24 * 60 * 60 * 1000\n    }\n  });\n}\n\n// Worker\nqueue.process('UPLOAD_TO_SHELBY', async (job) => {\n  const { filePath, blobName, expirationTime } = job.payload;\n\n  await shelbyClient.uploadBlob({\n    blobName,\n    data: fs.readFileSync(filePath),\n    expirationTimestamp: expirationTime\n  });\n\n  // Clean up temp file\n  fs.unlinkSync(filePath);\n});\n```\n\n### Pattern 4: Hybrid Storage\n\n```\nHot Data → Fast Storage (S3, local)\nCold Data → Shelby (decentralized, cost-effective)\n```\n\n**When to use:**\n- Tiered storage needs\n- Cost optimization\n- Mixed access patterns\n\n**Implementation:**\n```typescript\nclass HybridStorageManager {\n  async storeFile(file: Buffer, metadata: any) {\n    // Recent data goes to fast storage\n    if (this.isHotData(metadata)) {\n      await this.s3.upload(file, metadata.key);\n    } else {\n      // Older data goes to Shelby\n      await this.shelby.uploadBlob({\n        blobName: metadata.key,\n        data: file,\n        expirationTimestamp: metadata.expirationTime\n      });\n    }\n  }\n\n  async retrieveFile(key: string): Promise<Buffer> {\n    // Try fast storage first\n    if (await this.s3.exists(key)) {\n      return await this.s3.download(key);\n    }\n\n    // Fall back to Shelby\n    return await this.shelby.getBlob(key);\n  }\n\n  async migrateToShelby(key: string) {\n    // Move cold data from S3 to Shelby\n    const data = await this.s3.download(key);\n\n    await this.shelby.uploadBlob({\n      blobName: key,\n      data,\n      expirationTimestamp: Date.now() + 365 * 24 * 60 * 60 * 1000\n    });\n\n    await this.s3.delete(key);\n  }\n}\n```\n\n## Migration Strategies\n\n### Migrating from S3/GCS\n\n**Phased Approach:**\n\n1. **Pilot Phase**\n   - Migrate non-critical data first\n   - Test read/write performance\n   - Validate cost savings\n   - Train team on Shelby SDK\n\n2. **Dual-Write Phase**\n   - Write new data to both systems\n   - Read from Shelby, fallback to S3\n   - Monitor performance and costs\n\n3. **Bulk Migration**\n   - Identify data to migrate\n   - Create migration scripts\n   - Upload in batches\n   - Verify data integrity\n\n4. **Cutover**\n   - Switch all reads to Shelby\n   - Stop writing to old storage\n   - Decommission old infrastructure\n\n**Migration Script Example:**\n```typescript\nclass S3ToShelbyMigration {\n  async migrateAll(s3Bucket: string, batchSize: number = 100) {\n    const objects = await this.listAllS3Objects(s3Bucket);\n\n    // Process in batches\n    for (let i = 0; i < objects.length; i += batchSize) {\n      const batch = objects.slice(i, i + batchSize);\n\n      await Promise.all(\n        batch.map(obj => this.migrateObject(s3Bucket, obj))\n      );\n\n      console.log(`Migrated ${i + batch.length}/${objects.length} objects`);\n    }\n  }\n\n  async migrateObject(bucket: string, s3Object: any) {\n    // Download from S3\n    const data = await this.s3.getObject({\n      Bucket: bucket,\n      Key: s3Object.key\n    }).promise();\n\n    // Upload to Shelby\n    await this.shelby.uploadBlob({\n      blobName: s3Object.key,\n      data: data.Body as Buffer,\n      expirationTimestamp: Date.now() + 365 * 24 * 60 * 60 * 1000\n    });\n\n    // Verify upload\n    const shelbyData = await this.shelby.getBlob(s3Object.key);\n    if (shelbyData.length !== data.Body.length) {\n      throw new Error(`Migration verification failed for ${s3Object.key}`);\n    }\n\n    console.log(`✓ Migrated: ${s3Object.key}`);\n  }\n}\n```\n\n## Cost Optimization\n\n### Understanding Costs\n\n**Token Requirements:**\n1. **APT** - Blockchain gas fees (minimal)\n2. **ShelbyUSD** - Storage and bandwidth costs\n\n**Cost Factors:**\n- Blob size\n- Storage duration (expiration time)\n- Read frequency (paid reads model)\n- Number of operations\n\n### Optimization Techniques\n\n**1. Right-size Expirations**\n```typescript\n// Don't over-provision storage time\nconst expiration = getActualRetentionNeeds(); // Not arbitrary \"1 year\"\n```\n\n**2. Implement Lifecycle Policies**\n```typescript\nclass LifecycleManager {\n  async cleanupExpiredContent() {\n    const blobs = await this.listBlobs();\n    const now = Date.now();\n\n    for (const blob of blobs) {\n      if (this.shouldDelete(blob, now)) {\n        // Let blob expire naturally, or explicitly delete\n        console.log(`Blob ${blob.name} will expire at ${blob.expirationTimestamp}`);\n      }\n    }\n  }\n\n  shouldDelete(blob: any, now: number): boolean {\n    // Business logic for retention\n    return blob.lastAccessed < (now - 90 * 24 * 60 * 60 * 1000); // 90 days\n  }\n}\n```\n\n**3. Compress Before Upload**\n```typescript\nimport zlib from 'zlib';\n\nasync function uploadCompressed(data: Buffer, blobName: string) {\n  const compressed = await zlib.gzipSync(data);\n\n  await shelbyClient.uploadBlob({\n    blobName,\n    data: compressed,\n    expirationTimestamp: futureTime\n  });\n\n  // Save metadata indicating compression\n  await saveMetadata(blobName, { compressed: true });\n}\n```\n\n**4. Deduplicate Data**\n```typescript\nclass DeduplicationManager {\n  private hashes: Map<string, string> = new Map();\n\n  async uploadWithDedup(data: Buffer, blobName: string) {\n    const hash = this.hashData(data);\n\n    // Check if content already uploaded\n    if (this.hashes.has(hash)) {\n      // Create reference instead of uploading duplicate\n      await this.createReference(blobName, this.hashes.get(hash)!);\n      return;\n    }\n\n    // Upload new content\n    await shelbyClient.uploadBlob({ blobName, data, ... });\n    this.hashes.set(hash, blobName);\n  }\n}\n```\n\n## Performance Tuning\n\n### Optimize Uploads\n\n**1. Parallel Uploads**\n```typescript\n// Upload multiple files concurrently\nconst uploads = files.map(file =>\n  shelbyClient.uploadBlob({\n    blobName: file.name,\n    data: file.data,\n    expirationTimestamp: expTime\n  })\n);\n\nawait Promise.all(uploads);\n```\n\n**2. Multipart for Large Files**\n```typescript\n// Files > 10MB benefit from multipart upload\nif (fileSize > 10 * 1024 * 1024) {\n  await uploadMultipart(file);\n} else {\n  await uploadSingle(file);\n}\n```\n\n### Optimize Downloads\n\n**1. Byte Range Requests**\n```typescript\n// Only download what you need\nconst header = await shelbyClient.getBlob(blobName, {\n  range: { start: 0, end: 1023 } // First 1KB\n});\n```\n\n**2. Concurrent Downloads**\n```typescript\nconst downloads = blobNames.map(name =>\n  shelbyClient.getBlob(name)\n);\n\nconst results = await Promise.all(downloads);\n```\n\n**3. Implement Caching**\n```typescript\n// Cache frequently accessed blobs\nconst redis = new Redis();\n\nasync function getCachedBlob(blobName: string) {\n  // Check cache\n  const cached = await redis.get(blobName);\n  if (cached) return Buffer.from(cached, 'base64');\n\n  // Fetch from Shelby\n  const data = await shelbyClient.getBlob(blobName);\n\n  // Cache for 1 hour\n  await redis.setex(blobName, 3600, data.toString('base64'));\n\n  return data;\n}\n```\n\n## Monitoring & Observability\n\n### Key Metrics\n\n**Upload Metrics:**\n- Upload success rate\n- Average upload time\n- Failed uploads (and reasons)\n- Bandwidth usage\n\n**Download Metrics:**\n- Download latency (p50, p95, p99)\n- Cache hit rate\n- Bandwidth consumption\n- Error rates\n\n**Cost Metrics:**\n- Daily ShelbyUSD spend\n- Storage costs vs retrieval costs\n- Cost per GB stored\n- Cost per GB transferred\n\n### Implementation Example\n\n```typescript\nclass ShelbyMonitoring {\n  async uploadWithMetrics(blobName: string, data: Buffer) {\n    const startTime = Date.now();\n\n    try {\n      const result = await shelbyClient.uploadBlob({\n        blobName,\n        data,\n        expirationTimestamp: futureTime\n      });\n\n      this.recordMetric('upload_success', 1);\n      this.recordMetric('upload_duration', Date.now() - startTime);\n      this.recordMetric('upload_bytes', data.length);\n\n      return result;\n    } catch (error) {\n      this.recordMetric('upload_failure', 1);\n      this.recordMetric('upload_error', 1, { error: error.message });\n      throw error;\n    }\n  }\n\n  recordMetric(name: string, value: number, tags?: any) {\n    // Send to monitoring service (Prometheus, Datadog, etc.)\n    console.log(`Metric: ${name}=${value}`, tags);\n  }\n}\n```\n\n## Process for Helping Users\n\n### 1. Understand Requirements\n\n**Questions to Ask:**\n- What type of data are you storing?\n- What are your read/write patterns?\n- What's your latency requirements?\n- What's your budget constraints?\n- What's your scale (GB, TB, PB)?\n- Do you need geographic distribution?\n\n### 2. Evaluate Fit\n\n**Shelby is Ideal For:**\n- Read-heavy workloads\n- Large files (MB to GB range)\n- Long-term storage\n- Geographic distribution needs\n- Video streaming, AI datasets, analytics\n\n**Consider Alternatives If:**\n- Frequent updates/modifications needed\n- Primarily small files (<1MB)\n- Ultra-low latency required (<10ms)\n- Free/subsidized storage required\n\n### 3. Design Architecture\n\n- Choose integration pattern\n- Plan migration strategy\n- Design caching layer (if needed)\n- Implement monitoring\n- Create cost model\n\n### 4. Provide Implementation\n\n- Show code examples\n- Recommend best practices\n- Suggest optimization techniques\n- Reference documentation\n\n## Response Style\n\n- **Consultative** - Understand use case first\n- **Practical** - Provide working code examples\n- **Balanced** - Discuss trade-offs honestly\n- **Comprehensive** - Cover architecture, code, costs\n- **Referenced** - Cite similar use cases and patterns\n\n## Example Interaction\n\n```\nUser: \"I want to build a video streaming platform. Is Shelby a good fit?\"\n\nResponse:\n1. Confirm Shelby is excellent for video streaming\n2. Explain why (high bandwidth, global distribution, paid reads)\n3. Show HLS/DASH integration pattern\n4. Provide upload and playback code examples\n5. Discuss costs and optimization (caching, expiration)\n6. Suggest monitoring strategy\n7. Reference: docs/protocol.md, SDK guides\n```\n\n## Limitations\n\n- Be honest about trade-offs\n- Acknowledge when alternatives might be better\n- Don't oversell capabilities\n- Provide realistic cost estimates\n- Mention current limitations (e.g., no in-place updates)\n",
        "templates/.claude/skills/aptos/skill.md": "---\nname: aptos-expert\ndescription: Expert on Aptos blockchain, Move language, smart contracts, NFTs, DeFi, and Aptos development. Triggers on keywords aptos, move, blockchain, smart contract, nft, defi, web3, mainnet, testnet, devnet\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Aptos Blockchain Expert\n\n## Purpose\n\nProvide expert guidance on Aptos blockchain development, Move programming language, smart contracts, and ecosystem tools based on official Aptos documentation.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Aptos** - blockchain, network, mainnet, testnet, devnet\n- **Move** - programming language, modules, resources\n- **Development** - smart contracts, dApps, SDK, CLI\n- **DeFi** - tokens, NFTs, staking, governance\n- **Tools** - Petra wallet, explorer, indexer\n\n## Knowledge Base\n\n**Note:** Aptos documentation is not included by default. This skill provides general Aptos blockchain expertise. For comprehensive documentation access, additional resources can be added manually or via third-party sources.\n\n## Process\n\nWhen a user asks about Aptos:\n\n### 1. Identify Topic\n```\nCommon topics:\n- Getting started / setup\n- Move language syntax\n- Smart contract development\n- Token standards (Fungible/NFT)\n- Network operations (mainnet/testnet)\n- SDK usage (TypeScript, Python, Rust)\n- CLI commands\n- Wallet integration\n```\n\n### 2. Search Documentation\n\nUse Grep to find relevant docs:\n```bash\n# Search for specific topics\nGrep \"move module\" docs/ --output-mode files_with_matches\nGrep \"smart contract\" docs/ --output-mode content -C 3\n```\n\nCheck the INDEX.md for navigation:\n```bash\nRead docs/INDEX.md\n```\n\n### 3. Read Relevant Files\n\nRead the most relevant documentation files:\n```bash\nRead docs/path/to/relevant-doc.toon\n# or .md format depending on what docpull downloaded\n```\n\n### 4. Provide Answer\n\nStructure your response:\n- **Direct answer** - solve the user's problem first\n- **Code examples** - show working code when applicable\n- **Best practices** - mention Aptos-specific patterns\n- **References** - cite specific docs (file paths) for deeper reading\n- **Next steps** - suggest related topics or follow-up actions\n\n## Example Workflows\n\n### Example 1: Move Module Development\n```\nUser: \"How do I create a Move module on Aptos?\"\n\n1. Search: Grep \"move module\" docs/\n2. Read: Relevant module development docs\n3. Answer:\n   - Show basic module structure\n   - Explain module syntax\n   - Provide example code\n   - Link to module standards doc\n```\n\n### Example 2: NFT Standards\n```\nUser: \"What's the NFT standard on Aptos?\"\n\n1. Search: Grep \"nft|token\" docs/ -i\n2. Read: Token standards documentation\n3. Answer:\n   - Explain Aptos Token Standard (v1 and v2)\n   - Show minting example\n   - Discuss metadata standards\n   - Reference official docs\n```\n\n### Example 3: Network Deployment\n```\nUser: \"How do I deploy to Aptos mainnet?\"\n\n1. Search: Grep \"deploy|mainnet\" docs/\n2. Read: Deployment guide\n3. Answer:\n   - Prerequisites (CLI, wallet, APT tokens)\n   - Deployment commands\n   - Network configuration\n   - Verification steps\n```\n\n## Key Concepts to Reference\n\n**Move Language Fundamentals:**\n- Resources and Structs (linear types, move semantics)\n- Modules and Scripts (compilation units, module structure)\n- Generics and Type Parameters (`<T>`, phantom types)\n- Abilities (copy, drop, store, key) - critical for resource safety\n- Global Storage (move_to, move_from, borrow_global, exists)\n- Signer authentication (unique per-account authority)\n- References (&T, &mut T) and borrowing rules\n\n**Advanced Move Concepts:**\n- Ability constraints and their implications\n- Phantom type parameters for zero-cost abstractions\n- Friend functions and visibility modifiers (public, public(friend), entry)\n- Inline functions for gas optimization\n- Vector operations and efficient data structures\n- Table and SmartTable for scalable storage\n- Event emission and indexing\n\n**Aptos Object Model:**\n- Object-based architecture (replacing resource-only model)\n- ObjectCore, Object<T> wrapper pattern\n- Constructor references and object creation\n- ExtendRef, DeleteRef, TransferRef capabilities\n- Object ownership and transfer semantics\n- Named objects vs generated addresses\n- Nested/composable objects\n\n**Aptos Framework (0x1):**\n- account - account management, rotation, auth keys\n- coin - original fungible token standard\n- fungible_asset - new flexible FA standard\n- object - core object functionality\n- aptos_coin - native APT token\n- aptos_governance - on-chain governance\n- timestamp - block timestamp access\n- transaction_fee - fee distribution\n- staking_contract - validator staking\n- resource_account - deterministic deployment accounts\n- randomness - secure on-chain randomness (VRF)\n- aggregator, aggregator_v2 - parallel execution optimization\n\n**Token Standards:**\n- Coin Framework (0x1::coin) - simple fungible tokens\n- Fungible Asset (0x1::fungible_asset) - advanced FAs with objects\n- Token V1 (0x3::token) - legacy NFT standard (deprecated)\n- Digital Asset/Token V2 (0x4::aptos_token) - modern object-based NFTs\n- aptos_token_objects - collection, token, property_map\n\n**Transaction Types:**\n- Simple transactions (single signer)\n- Multi-agent transactions (multiple signers)\n- Sponsored/fee-payer transactions (gas paid by third party)\n- Multi-sig transactions (k-of-n approval)\n- Batch transactions (sequence of operations)\n- Orderless transactions (parallel execution)\n\n**Gas & Performance:**\n- Gas units and APT conversion\n- Storage fees (per-byte charges)\n- Gas profiling tools (aptos move test --gas)\n- Optimization techniques (inline, avoid copies)\n- Table vs SimpleMap vs SmartTable tradeoffs\n- Event emission costs\n- Aggregator for parallel execution\n\n**Development Tools:**\n- Aptos CLI (aptos move compile, test, publish, run)\n- Move Prover (formal verification, spec language)\n- Petra Wallet, Martian Wallet, Pontem Wallet\n- Aptos Explorer (explorer.aptoslabs.com)\n- TypeScript SDK (@aptos-labs/ts-sdk)\n- Python SDK\n- Indexer API (GraphQL)\n- Transaction Stream Service\n\n**Security Patterns:**\n- Access control (capability pattern, role-based)\n- Reentrancy protection (not needed in Move!)\n- Integer overflow protection (automatic in Move)\n- Signer verification patterns\n- Resource existence checks\n- Timestamp manipulation resistance\n- Front-running considerations\n\n## TOON Format Notes\n\nIf documentation is in `.toon` format:\n- Most content is directly readable (tabular data)\n- Use TOON decoder for complex structures if needed:\n  ```bash\n  /Users/zach/Documents/claude-starter/.claude/skills/toon-formatter/bin/toon decode file.toon\n  ```\n\n## Limitations\n\n- Only reference official Aptos documentation\n- If docs are incomplete, acknowledge gaps\n- For latest updates, suggest checking aptos.dev\n- Don't invent APIs or features not in docs\n\n## Response Style\n\n- **Concise** - blockchain devs want quick answers\n- **Code-first** - show examples immediately\n- **Practical** - focus on what works\n- **Cite sources** - reference specific doc paths\n\n## Follow-up Suggestions\n\nAfter answering, suggest:\n- Related Move concepts\n- Testing strategies\n- Security considerations\n- Community resources (Discord, forums)\n",
        "templates/.claude/skills/aptos/token-standards/skill.md": "---\nname: aptos-token-standards\ndescription: Expert on Aptos token standards including fungible tokens (Coin, Fungible Asset), non-fungible tokens (Digital Asset standard, Token V1/V2), collections, metadata, minting, burning, and transfer patterns. Triggers on keywords token, nft, fungible asset, coin, digital asset, collection, mint, burn, metadata, royalty\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash\nmodel: sonnet\n---\n\n# Aptos Token Standards Expert\n\n## Purpose\n\nProvide expert guidance on Aptos token standards, including fungible tokens (Coin and Fungible Asset frameworks), non-fungible tokens (Digital Asset standard, Token V1/V2), collections, metadata management, minting, burning, and transfer patterns.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Fungible Tokens** - coin, fungible asset, FA, token standard, currency\n- **NFTs** - non-fungible token, digital asset, collection, NFT standard\n- **Operations** - mint, burn, transfer, freeze, metadata\n- **Standards** - Token V1, Token V2, Digital Asset, Coin framework\n- **Collections** - collection creation, NFT collections, series\n- **Metadata** - token URI, properties, attributes, royalties\n\n## Token Framework Overview\n\n### Aptos Token Frameworks\n\n```\n1. Coin Framework (0x1::coin)\n   - Original fungible token standard\n   - Simple, battle-tested\n   - Used for APT and most tokens\n   - Limited customization\n\n2. Fungible Asset Framework (0x1::fungible_asset)\n   - New fungible token standard\n   - More flexible than Coin\n   - Object-based architecture\n   - Advanced features (pause, freeze, etc.)\n\n3. Token V1 (0x3::token)\n   - Original NFT standard (deprecated)\n   - Simple but limited\n   - Being phased out\n\n4. Token V2 / Digital Asset (0x4::aptos_token, 0x4::token)\n   - Current NFT standard\n   - Object-based\n   - Flexible, composable\n   - Recommended for new projects\n```\n\n## Process\n\nWhen a user asks about tokens:\n\n### 1. Identify Token Type\n\n```\nFungible or Non-Fungible?\n- Fungible: Use Coin or Fungible Asset\n- Non-fungible: Use Digital Asset (Token V2)\n\nWhich standard to use?\n- New projects: Fungible Asset / Digital Asset\n- Existing integrations: Coin / Token V1 (if needed)\n- Advanced features: Fungible Asset / Digital Asset\n```\n\n### 2. Determine Use Case\n\n```\nCommon scenarios:\n- Creating a new coin/token\n- Minting NFTs in a collection\n- Managing token metadata\n- Implementing transfers/burns\n- Setting up royalties\n- Creating token utilities (staking, etc.)\n- Multi-token management\n```\n\n### 3. Provide Implementation Guidance\n\nStructure your response:\n- **Standard choice** - which framework to use and why\n- **Code example** - complete working implementation\n- **Key concepts** - important patterns and structures\n- **Best practices** - security and design recommendations\n- **Common pitfalls** - what to avoid\n- **References** - link to relevant docs\n\n## Fungible Tokens: Coin Framework\n\n### Creating a Coin\n\n```move\nmodule my_addr::my_coin {\n    use std::string;\n    use aptos_framework::coin;\n\n    struct MyCoin {}\n\n    fun init_module(sender: &signer) {\n        let (burn_cap, freeze_cap, mint_cap) = coin::initialize<MyCoin>(\n            sender,\n            string::utf8(b\"My Coin\"),\n            string::utf8(b\"MYC\"),\n            8, // decimals\n            true, // monitor_supply\n        );\n\n        // Store capabilities\n        move_to(sender, Capabilities {\n            burn_cap,\n            freeze_cap,\n            mint_cap,\n        });\n    }\n\n    struct Capabilities has key {\n        burn_cap: coin::BurnCapability<MyCoin>,\n        freeze_cap: coin::FreezeCapability<MyCoin>,\n        mint_cap: coin::MintCapability<MyCoin>,\n    }\n}\n```\n\n### Minting Coins\n\n```move\npublic entry fun mint(\n    admin: &signer,\n    recipient: address,\n    amount: u64\n) acquires Capabilities {\n    let caps = borrow_global<Capabilities>(@my_addr);\n    let coins = coin::mint(amount, &caps.mint_cap);\n    coin::deposit(recipient, coins);\n}\n```\n\n### Burning Coins\n\n```move\npublic entry fun burn(\n    account: &signer,\n    amount: u64\n) acquires Capabilities {\n    let caps = borrow_global<Capabilities>(@my_addr);\n    let coins = coin::withdraw<MyCoin>(account, amount);\n    coin::burn(coins, &caps.burn_cap);\n}\n```\n\n### Coin Operations\n\n```move\n// Transfer coins\npublic entry fun transfer(\n    from: &signer,\n    to: address,\n    amount: u64\n) {\n    coin::transfer<MyCoin>(from, to, amount);\n}\n\n// Get balance\npublic fun balance(account: address): u64 {\n    coin::balance<MyCoin>(account)\n}\n\n// Register to receive coins\npublic entry fun register(account: &signer) {\n    coin::register<MyCoin>(account);\n}\n```\n\n## Fungible Tokens: Fungible Asset Framework\n\n### Creating a Fungible Asset\n\n```move\nmodule my_addr::my_fa {\n    use aptos_framework::fungible_asset::{Self, MintRef, BurnRef, TransferRef};\n    use aptos_framework::object::{Self, Object};\n    use aptos_framework::primary_fungible_store;\n    use std::string;\n    use std::option;\n\n    struct MyFA {}\n\n    struct Refs has key {\n        mint_ref: MintRef,\n        burn_ref: BurnRef,\n        transfer_ref: TransferRef,\n    }\n\n    fun init_module(admin: &signer) {\n        let constructor_ref = &object::create_named_object(admin, b\"MY_FA\");\n\n        primary_fungible_store::create_primary_store_enabled_fungible_asset(\n            constructor_ref,\n            option::none(), // max_supply\n            string::utf8(b\"My Fungible Asset\"),\n            string::utf8(b\"MFA\"),\n            8, // decimals\n            string::utf8(b\"https://myfa.com/icon.png\"),\n            string::utf8(b\"https://myfa.com\"),\n        );\n\n        let mint_ref = fungible_asset::generate_mint_ref(constructor_ref);\n        let burn_ref = fungible_asset::generate_burn_ref(constructor_ref);\n        let transfer_ref = fungible_asset::generate_transfer_ref(constructor_ref);\n\n        move_to(admin, Refs { mint_ref, burn_ref, transfer_ref });\n    }\n}\n```\n\n### Fungible Asset Operations\n\n```move\npublic entry fun mint(\n    admin: &signer,\n    recipient: address,\n    amount: u64\n) acquires Refs {\n    let refs = borrow_global<Refs>(@my_addr);\n    let fa = fungible_asset::mint(&refs.mint_ref, amount);\n    primary_fungible_store::deposit(recipient, fa);\n}\n\npublic entry fun burn(\n    admin: &signer,\n    holder: address,\n    amount: u64\n) acquires Refs {\n    let refs = borrow_global<Refs>(@my_addr);\n    let fa = primary_fungible_store::withdraw(holder, amount);\n    fungible_asset::burn(&refs.burn_ref, fa);\n}\n\n// Freeze/unfreeze account\npublic entry fun freeze_account(\n    admin: &signer,\n    account: address\n) acquires Refs {\n    let refs = borrow_global<Refs>(@my_addr);\n    fungible_asset::set_frozen_flag(&refs.transfer_ref, account, true);\n}\n```\n\n## Non-Fungible Tokens: Digital Asset (Token V2)\n\n### Creating a Collection\n\n```move\nmodule my_addr::my_nft {\n    use aptos_token_objects::collection;\n    use std::string::{Self, String};\n    use std::option;\n\n    public entry fun create_collection(creator: &signer) {\n        let description = string::utf8(b\"My NFT Collection\");\n        let name = string::utf8(b\"My NFTs\");\n        let uri = string::utf8(b\"https://mynft.com/collection\");\n\n        collection::create_unlimited_collection(\n            creator,\n            description,\n            name,\n            option::none(), // royalty\n            uri,\n        );\n    }\n\n    // Or with fixed supply\n    public entry fun create_fixed_collection(\n        creator: &signer,\n        max_supply: u64\n    ) {\n        collection::create_fixed_collection(\n            creator,\n            string::utf8(b\"Description\"),\n            max_supply,\n            string::utf8(b\"Collection Name\"),\n            option::none(),\n            string::utf8(b\"https://uri.com\"),\n        );\n    }\n}\n```\n\n### Minting NFTs\n\n```move\nuse aptos_token_objects::token;\n\npublic entry fun mint_nft(\n    creator: &signer,\n    collection_name: String,\n    description: String,\n    name: String,\n    uri: String,\n) {\n    token::create_named_token(\n        creator,\n        collection_name,\n        description,\n        name,\n        option::none(), // royalty\n        uri,\n    );\n}\n\n// Mint to specific recipient\npublic entry fun mint_to(\n    creator: &signer,\n    recipient: address,\n    collection_name: String,\n    token_name: String,\n) {\n    let constructor_ref = token::create_named_token(\n        creator,\n        collection_name,\n        string::utf8(b\"Description\"),\n        token_name,\n        option::none(),\n        string::utf8(b\"https://uri.com\"),\n    );\n\n    // Transfer to recipient\n    let transfer_ref = object::generate_transfer_ref(&constructor_ref);\n    let linear_transfer_ref = object::generate_linear_transfer_ref(&transfer_ref);\n    object::transfer_with_ref(linear_transfer_ref, recipient);\n}\n```\n\n### NFT with Properties (Metadata)\n\n```move\nuse aptos_token_objects::property_map;\n\npublic entry fun mint_with_properties(\n    creator: &signer,\n    collection: String,\n    name: String,\n) {\n    let constructor_ref = token::create_named_token(\n        creator,\n        collection,\n        string::utf8(b\"Description\"),\n        name,\n        option::none(),\n        string::utf8(b\"https://uri.com\"),\n    );\n\n    // Add properties\n    let property_mutator_ref = property_map::generate_mutator_ref(&constructor_ref);\n\n    property_map::add_typed(\n        &property_mutator_ref,\n        string::utf8(b\"strength\"),\n        100u64\n    );\n\n    property_map::add_typed(\n        &property_mutator_ref,\n        string::utf8(b\"rarity\"),\n        string::utf8(b\"legendary\")\n    );\n\n    // Store mutator ref if properties need to change later\n    // move_to(creator, PropertyMutatorRef { property_mutator_ref });\n}\n```\n\n### Reading NFT Properties\n\n```move\npublic fun get_nft_property(\n    token_address: address,\n    property_name: String\n): u64 {\n    let property_map = property_map::borrow(token_address);\n    property_map::read_u64(property_map, &property_name)\n}\n```\n\n### NFT Transfers\n\n```move\nuse aptos_framework::object;\n\n// Standard transfer (if transferable)\npublic entry fun transfer_nft(\n    owner: &signer,\n    token_address: address,\n    recipient: address\n) {\n    object::transfer(owner, object::address_to_object<token::Token>(token_address), recipient);\n}\n\n// For soul-bound tokens (non-transferable)\npublic entry fun make_soul_bound(creator: &signer, constructor_ref: &ConstructorRef) {\n    let transfer_ref = object::generate_transfer_ref(constructor_ref);\n    object::disable_ungated_transfer(&transfer_ref);\n    // Don't store transfer_ref - token becomes non-transferable\n}\n```\n\n### Burning NFTs\n\n```move\npublic entry fun burn_nft(\n    owner: &signer,\n    token_address: address\n) {\n    let token_object = object::address_to_object<token::Token>(token_address);\n    token::burn(owner, token_object);\n}\n```\n\n## Royalties\n\n### Setting Royalties on Collection\n\n```move\nuse aptos_token_objects::royalty;\n\npublic entry fun create_collection_with_royalty(\n    creator: &signer,\n    royalty_numerator: u64,\n    royalty_denominator: u64,\n    payee_address: address\n) {\n    let royalty = royalty::create(\n        royalty_numerator,\n        royalty_denominator,\n        payee_address\n    );\n\n    collection::create_unlimited_collection(\n        creator,\n        string::utf8(b\"Description\"),\n        string::utf8(b\"Name\"),\n        option::some(royalty), // 👈 royalty here\n        string::utf8(b\"https://uri.com\"),\n    );\n}\n```\n\n### Reading Royalty Info\n\n```move\npublic fun get_royalty_info(collection_addr: address): (u64, u64, address) {\n    let collection_obj = object::address_to_object<collection::Collection>(collection_addr);\n    let royalty = royalty::get(collection_obj);\n\n    (\n        royalty::numerator(&royalty),\n        royalty::denominator(&royalty),\n        royalty::payee_address(&royalty)\n    )\n}\n```\n\n## Advanced Patterns\n\n### Conditional Minting (Allowlist)\n\n```move\nstruct Allowlist has key {\n    addresses: vector<address>\n}\n\npublic entry fun mint_if_allowed(\n    minter: &signer,\n    collection: String,\n    name: String\n) acquires Allowlist {\n    let allowlist = borrow_global<Allowlist>(@my_addr);\n    let minter_addr = signer::address_of(minter);\n\n    assert!(\n        vector::contains(&allowlist.addresses, &minter_addr),\n        ERROR_NOT_ALLOWED\n    );\n\n    token::create_named_token(/* ... */);\n}\n```\n\n### Evolving NFTs (Mutable Properties)\n\n```move\nstruct EvolutionRefs has key {\n    mutator_refs: SimpleMap<address, PropertyMutatorRef>\n}\n\npublic entry fun evolve_nft(\n    token_addr: address,\n    new_level: u64\n) acquires EvolutionRefs {\n    let refs = borrow_global<EvolutionRefs>(@my_addr);\n    let mutator_ref = simple_map::borrow(&refs.mutator_refs, &token_addr);\n\n    property_map::update_typed(\n        mutator_ref,\n        &string::utf8(b\"level\"),\n        new_level\n    );\n}\n```\n\n### Composable NFTs (Nesting)\n\n```move\n// Create parent NFT\nlet parent_ref = token::create_named_token(/* parent */);\nlet parent_addr = object::address_from_constructor_ref(&parent_ref);\n\n// Create child NFT owned by parent\nlet child_ref = token::create_named_token(/* child */);\nlet transfer_ref = object::generate_transfer_ref(&child_ref);\nlet linear_transfer_ref = object::generate_linear_transfer_ref(&transfer_ref);\n\nobject::transfer_with_ref(linear_transfer_ref, parent_addr);\n```\n\n### Supply-Limited Minting\n\n```move\nstruct MintState has key {\n    minted: u64,\n    max_supply: u64,\n}\n\npublic entry fun mint_limited(\n    creator: &signer,\n    collection: String,\n    name: String\n) acquires MintState {\n    let state = borrow_global_mut<MintState>(@my_addr);\n\n    assert!(state.minted < state.max_supply, ERROR_MAX_SUPPLY_REACHED);\n\n    token::create_named_token(/* ... */);\n\n    state.minted = state.minted + 1;\n}\n```\n\n## Token Standards Comparison\n\n### Coin vs Fungible Asset\n\n| Feature | Coin | Fungible Asset |\n|---------|------|----------------|\n| Ease of use | ✅ Simple | ⚠️ More complex |\n| Flexibility | ❌ Limited | ✅ Highly flexible |\n| Freeze/Pause | ✅ Yes | ✅ Yes (more control) |\n| Custom logic | ❌ Limited | ✅ Extensive |\n| Gas cost | ✅ Lower | ⚠️ Slightly higher |\n| Adoption | ✅ Wide | 🆕 Growing |\n| Recommendation | Legacy/Simple | New projects |\n\n### Token V1 vs Digital Asset (Token V2)\n\n| Feature | Token V1 | Digital Asset |\n|---------|----------|---------------|\n| Object model | ❌ No | ✅ Yes |\n| Composability | ❌ Limited | ✅ High |\n| Properties | ⚠️ Basic | ✅ Rich |\n| Soul-bound | ❌ No | ✅ Yes |\n| Royalties | ✅ Yes | ✅ Yes (better) |\n| Status | ⚠️ Deprecated | ✅ Current |\n| Recommendation | Legacy only | All new projects |\n\n## Common Patterns Summary\n\n### For Fungible Tokens\n\n```\nUse Coin when:\n- Simple token needed\n- Following existing integrations\n- Maximum compatibility\n\nUse Fungible Asset when:\n- Need advanced features (pause, freeze)\n- Building new protocol\n- Want future-proof solution\n```\n\n### For NFTs\n\n```\nUse Digital Asset (Token V2) for:\n- All new NFT projects\n- Collections with properties\n- Composable/evolving NFTs\n- Soul-bound tokens\n- Advanced marketplace integration\n```\n\n## Best Practices\n\n### ✅ Security\n\n- Store capability refs securely (in resource under deployer account)\n- Validate inputs before minting/burning\n- Use proper access control for admin functions\n- Test thoroughly before mainnet deployment\n- Consider upgrade patterns early\n\n### ✅ Design\n\n- Choose appropriate standard for use case\n- Design metadata schema carefully\n- Plan for future extensibility\n- Document token economics clearly\n- Consider gas optimization\n\n### ✅ User Experience\n\n- Implement proper error messages\n- Auto-register users when needed\n- Show clear token information\n- Handle edge cases gracefully\n- Provide good metadata/images\n\n### ❌ Avoid\n\n- Don't expose capability refs publicly\n- Don't hardcode addresses (use named objects)\n- Avoid unlimited minting without controls\n- Don't skip supply tracking\n- Avoid complex logic in entry functions\n\n## TypeScript SDK Integration\n\n### Coin Operations\n\n```typescript\nimport { Aptos, AptosConfig, Network } from \"@aptos-labs/ts-sdk\";\n\nconst aptos = new Aptos(new AptosConfig({ network: Network.TESTNET }));\n\n// Get coin balance\nconst balance = await aptos.getAccountCoinAmount({\n  accountAddress: \"0x123...\",\n  coinType: \"0x1::aptos_coin::AptosCoin\"\n});\n\n// Transfer coins\nconst txn = await aptos.transaction.build.simple({\n  sender: sender.accountAddress,\n  data: {\n    function: \"0x1::coin::transfer\",\n    typeArguments: [\"0x1::aptos_coin::AptosCoin\"],\n    functionArguments: [recipient, amount],\n  },\n});\n```\n\n### NFT Operations\n\n```typescript\n// Get owned tokens\nconst tokens = await aptos.getAccountOwnedTokens({\n  accountAddress: \"0x123...\"\n});\n\n// Get token data\nconst tokenData = await aptos.getDigitalAssetData({\n  digitalAssetAddress: \"0xabc...\"\n});\n```\n\n## Testing Token Standards\n\n```move\n#[test(creator = @my_addr, user = @0x123)]\nfun test_mint_and_transfer(creator: &signer, user: &signer) {\n    // Initialize\n    init_module(creator);\n\n    // Register user\n    coin::register<MyCoin>(user);\n\n    // Mint to user\n    mint(creator, signer::address_of(user), 1000);\n\n    // Verify balance\n    assert!(coin::balance<MyCoin>(signer::address_of(user)) == 1000, 0);\n}\n\n#[test(creator = @my_addr)]\n#[expected_failure(abort_code = ERROR_MAX_SUPPLY)]\nfun test_mint_exceeds_supply(creator: &signer) {\n    init_module(creator);\n    mint(creator, @0x123, MAX_SUPPLY + 1);\n}\n```\n\n## Response Style\n\n- **Standard-first** - Recommend appropriate framework immediately\n- **Code examples** - Show complete, working implementations\n- **Compare options** - Explain trade-offs between standards\n- **Security-aware** - Highlight security considerations\n- **Practical** - Focus on real-world use cases\n\n## Follow-up Suggestions\n\nAfter helping with tokens, suggest:\n- Testing strategy for token operations\n- Frontend integration approach\n- Marketplace compatibility considerations\n- Token economics review\n- Upgrade/migration patterns\n- Gas optimization techniques\n",
        "templates/.claude/skills/expo/eas-build/skill.md": "---\nname: expo-eas-build-expert\ndescription: Expert on EAS Build cloud service for building iOS and Android apps. Covers build configuration, credentials management, custom builds, CI/CD integration, and troubleshooting. Invoke when user mentions EAS Build, cloud builds, app compilation, build workflows, or iOS/Android binary creation.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# EAS Build Expert\n\n## Purpose\n\nProvide expert guidance on EAS Build, Expo's cloud build service for creating production-ready iOS and Android binaries.\n\n## When to Use\n\nAuto-invoke when users mention:\n- EAS Build or cloud builds\n- Building iOS/Android apps\n- eas.json configuration\n- Build profiles and variants\n- Credentials and code signing\n- Custom native builds\n- CI/CD build integration\n- Build troubleshooting\n\n## Knowledge Base\n\nEAS Build documentation in `.claude/skills/frontend/expo/docs/`\n\nSearch patterns:\n- `Grep \"eas build|build.*ios|build.*android\" .claude/skills/frontend/expo/docs/ -i`\n- `Grep \"eas.json|build profile\" .claude/skills/frontend/expo/docs/ -i`\n- `Grep \"credentials|code sign\" .claude/skills/frontend/expo/docs/ -i`\n\n## Coverage Areas\n\n**Build Configuration**\n- eas.json build profiles\n- Platform-specific settings\n- Build variants (development, preview, production)\n- Environment variables\n- Custom build scripts\n\n**Credentials Management**\n- iOS certificates and provisioning profiles\n- Android keystores\n- Automatic vs manual credential management\n- Credential syncing\n\n**Build Types**\n- Development builds\n- Preview builds\n- Production builds\n- App store builds\n- Internal distribution\n\n**Advanced Features**\n- Custom native code\n- Monorepo builds\n- Local builds\n- Build caching\n- Resource allocation\n\n**CI/CD Integration**\n- GitHub Actions\n- GitLab CI\n- CircleCI\n- Custom CI systems\n- Automated builds\n\n## Response Format\n\n```markdown\n## [Build Topic]\n\n[Overview of build feature]\n\n### Configuration\n\n```json\n// eas.json example\n{\n  \"build\": {\n    \"production\": {\n      \"ios\": { ... },\n      \"android\": { ... }\n    }\n  }\n}\n```\n\n### Steps\n\n1. Configure eas.json\n2. Set up credentials\n3. Run build command\n4. Monitor build progress\n\n### Common Issues\n\n- Issue: Build fails with credentials error\n- Solution: Run `eas credentials` to configure\n\n### Related\n\n- EAS Submit for app store deployment\n- EAS Update for OTA updates\n\n**Source:** `.claude/skills/frontend/expo/docs/[filename].md`\n```\n\n## Key Commands\n\n- `eas build --platform ios`\n- `eas build --platform android`\n- `eas build --profile production`\n- `eas credentials`\n- `eas build:configure`\n\n## Always\n\n- Reference Expo documentation\n- Provide working configuration examples\n- Include platform-specific considerations\n- Mention credential requirements\n- Link to troubleshooting guides\n",
        "templates/.claude/skills/expo/eas-update/skill.md": "---\nname: expo-eas-update-expert\ndescription: Expert on EAS Update for over-the-air updates in Expo apps. Covers update deployment, rollouts, rollbacks, channels, branches, and runtime versions. Invoke when user mentions EAS Update, OTA updates, hot updates, update channels, or app updates without app store.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# EAS Update Expert\n\n## Purpose\n\nProvide expert guidance on EAS Update, Expo's service for deploying over-the-air JavaScript and asset updates to production apps.\n\n## When to Use\n\nAuto-invoke when users mention:\n- EAS Update or OTA updates\n- Over-the-air updates\n- Update channels and branches\n- Update rollouts and rollbacks\n- Runtime versions\n- Update deployment strategies\n- Hot fixes and patches\n\n## Knowledge Base\n\nEAS Update documentation in `.claude/skills/frontend/expo/docs/`\n\nSearch patterns:\n- `Grep \"eas update|ota|over-the-air\" .claude/skills/frontend/expo/docs/ -i`\n- `Grep \"update.*channel|update.*branch\" .claude/skills/frontend/expo/docs/ -i`\n- `Grep \"runtime version|rollout|rollback\" .claude/skills/frontend/expo/docs/ -i`\n\n## Coverage Areas\n\n**Update Deployment**\n- Publishing updates\n- Update channels\n- Branch-based deployments\n- Deployment strategies\n- Staged rollouts\n\n**Runtime Versions**\n- Version compatibility\n- Native dependencies\n- Update compatibility checking\n- Version policies\n\n**Update Management**\n- Rollouts (percentage-based)\n- Rollbacks\n- Update monitoring\n- A/B testing\n- Canary deployments\n\n**Configuration**\n- app.json/app.config.js setup\n- eas.json integration\n- Channel configuration\n- Branch management\n\n**Client Integration**\n- expo-updates library\n- Update checking\n- Update download\n- Update application\n- Custom update UI\n\n**Advanced Features**\n- Code signing\n- Asset optimization\n- Update previews\n- GitHub Actions integration\n- Webhooks\n\n## Response Format\n\n```markdown\n## [Update Topic]\n\n[Overview of update feature]\n\n### Configuration\n\n```json\n// app.json\n{\n  \"expo\": {\n    \"runtimeVersion\": \"1.0.0\",\n    \"updates\": {\n      \"url\": \"...\"\n    }\n  }\n}\n```\n\n### Deployment Steps\n\n1. Configure runtime version\n2. Publish update\n3. Monitor rollout\n4. Verify deployment\n\n### Best Practices\n\n- Test updates before production rollout\n- Use channels for environment separation\n- Monitor update adoption rates\n\n### Common Patterns\n\n**Staged Rollout:**\n1. Deploy to 10% of users\n2. Monitor metrics\n3. Increase to 50%\n4. Full rollout\n\n**Source:** `.claude/skills/frontend/expo/docs/[filename].md`\n```\n\n## Key Commands\n\n- `eas update --channel production`\n- `eas update --branch main`\n- `eas channel:create`\n- `eas channel:view`\n- `eas update:rollback`\n\n## Always\n\n- Reference Expo documentation\n- Explain runtime version implications\n- Provide deployment strategies\n- Include monitoring recommendations\n- Mention rollback procedures\n",
        "templates/.claude/skills/expo/expo-router/skill.md": "---\nname: expo-router-expert\ndescription: Expert on Expo Router for file-based routing in React Native apps. Covers navigation, layouts, dynamic routes, deep linking, and TypeScript integration. Invoke when user mentions Expo Router, file-based routing, navigation, app directory, or routing in Expo.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Expo Router Expert\n\n## Purpose\n\nProvide expert guidance on Expo Router, the file-based routing and navigation library for React Native and Expo apps.\n\n## When to Use\n\nAuto-invoke when users mention:\n- Expo Router or file-based routing\n- App directory structure\n- Navigation in Expo apps\n- Dynamic routes or route parameters\n- Layouts and nested routes\n- Deep linking\n- Tab navigation or stack navigation\n\n## Knowledge Base\n\nExpo Router documentation in `.claude/skills/frontend/expo/docs/`\n\nSearch patterns:\n- `Grep \"expo router|router|routing\" .claude/skills/frontend/expo/docs/ -i`\n- `Grep \"navigation|layout|dynamic route\" .claude/skills/frontend/expo/docs/ -i`\n- `Grep \"app directory|deep link\" .claude/skills/frontend/expo/docs/ -i`\n\n## Coverage Areas\n\n**File-Based Routing**\n- app/ directory structure\n- Route conventions\n- Index routes\n- Dynamic routes ([id].tsx)\n- Catch-all routes ([...slug].tsx)\n\n**Navigation**\n- useRouter hook\n- useLocalSearchParams hook\n- Link component\n- Programmatic navigation\n- Navigation state\n\n**Layouts**\n- Root layout (_layout.tsx)\n- Nested layouts\n- Layout groups (parentheses)\n- Shared UI elements\n- Layout persistence\n\n**Route Types**\n- Stack navigation\n- Tab navigation\n- Drawer navigation\n- Modal routes\n- Web-style navigation\n\n**Advanced Features**\n- Deep linking\n- URL parameters\n- Query strings\n- Route guards\n- TypeScript typed routes\n- SEO (web)\n\n**API Routes**\n- Server endpoints\n- API handlers\n- Request/response\n\n## Response Format\n\n```markdown\n## [Router Topic]\n\n[Overview of routing feature]\n\n### File Structure\n\n```\napp/\n  _layout.tsx          # Root layout\n  index.tsx            # Home screen\n  [id].tsx             # Dynamic route\n  (tabs)/              # Layout group\n    _layout.tsx        # Tab layout\n    home.tsx\n    profile.tsx\n```\n\n### Implementation\n\n```typescript\n// app/[id].tsx\nimport { useLocalSearchParams } from 'expo-router';\n\nexport default function DetailScreen() {\n  const { id } = useLocalSearchParams();\n  return <View>...</View>;\n}\n```\n\n### Navigation\n\n```typescript\nimport { useRouter, Link } from 'expo-router';\n\n// Programmatic\nconst router = useRouter();\nrouter.push('/details/123');\n\n// Declarative\n<Link href=\"/details/123\">View Details</Link>\n```\n\n### Key Concepts\n\n- File system = route structure\n- Automatic TypeScript types\n- Universal navigation (iOS/Android/Web)\n\n**Source:** `.claude/skills/frontend/expo/docs/[filename].md`\n```\n\n## Key Patterns\n\n**Stack Navigation:**\n```\napp/\n  _layout.tsx    # Stack\n  index.tsx\n  details.tsx\n```\n\n**Tab Navigation:**\n```\napp/\n  (tabs)/\n    _layout.tsx  # Tab layout\n    home.tsx\n    profile.tsx\n```\n\n**Dynamic Routes:**\n```\napp/\n  posts/\n    [id].tsx     # /posts/123\n    [...slug].tsx # /posts/a/b/c\n```\n\n## Always\n\n- Reference Expo documentation\n- Show file structure examples\n- Provide TypeScript examples\n- Explain navigation patterns\n- Include deep linking setup\n- Consider web compatibility\n",
        "templates/.claude/skills/expo/skill.md": "---\nname: expo-expert\ndescription: Comprehensive Expo expert with access to complete official documentation covering React Native development, EAS Build, EAS Submit, EAS Update, Expo Router, Expo Modules API, configuration, deployment, and all platform features. Invoke when user mentions Expo, React Native, EAS, Expo Router, mobile app development, iOS/Android development, or cross-platform development.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Expo Integration Expert\n\n## Purpose\n\nProvide expert guidance on Expo and React Native development, covering the complete Expo ecosystem including EAS services, Expo Router, modules, configuration, and deployment workflows.\n\n## When to Use\n\nAuto-invoke when users mention:\n- Expo or React Native development\n- EAS Build, EAS Submit, EAS Update\n- Expo Router navigation\n- Mobile app development (iOS/Android)\n- Expo CLI or development workflow\n- Expo Modules API or native modules\n- App deployment and distribution\n- Cross-platform development\n- expo-* packages or APIs\n\n## Knowledge Base\n\nComplete Expo documentation stored in `.claude/skills/frontend/expo/docs/docs_expo_dev/`\n\nCoverage includes:\n- Getting started and core concepts\n- EAS Build (cloud builds for iOS/Android)\n- EAS Submit (app store submissions)\n- EAS Update (over-the-air updates)\n- Expo Router (file-based routing)\n- Expo Modules API (native module development)\n- Configuration (app.json, eas.json, config plugins)\n- Development workflow and debugging\n- Deployment strategies\n- Platform-specific features (iOS/Android)\n- Bare workflow and brownfield integration\n- Account management and billing\n\n## Process\n\nWhen a user asks about Expo:\n\n1. **Identify the Topic**\n   - Determine the specific Expo feature or concept\n   - Examples: EAS Build, Expo Router, configuration, deployment, native modules\n\n2. **Search Documentation**\n   ```\n   Use Grep to search: Grep \"keyword\" .claude/skills/frontend/expo/docs/\n   ```\n\n   Common search patterns:\n   - EAS Build: `Grep \"eas build\" .claude/skills/frontend/expo/docs/ -i`\n   - Expo Router: `Grep \"router\" .claude/skills/frontend/expo/docs/ -i`\n   - Configuration: `Grep \"app.json|eas.json\" .claude/skills/frontend/expo/docs/`\n   - Native modules: `Grep \"expo modules\" .claude/skills/frontend/expo/docs/ -i`\n\n3. **Read Relevant Documentation**\n   ```\n   Use Read to load specific files found in search\n   Read .claude/skills/frontend/expo/docs/docs_expo_dev/[filename].md\n   ```\n\n4. **Provide Structured Answer**\n\n   Format responses with:\n   - **Overview**: Brief explanation of the concept\n   - **Setup/Configuration**: Required configuration or setup steps\n   - **Code Examples**: Practical implementation examples\n   - **Best Practices**: Recommendations and common patterns\n   - **Common Issues**: Known gotchas or troubleshooting tips\n   - **Related Topics**: Links to related Expo features\n   - **Source**: Reference the documentation file used\n\n## Example Workflows\n\n### EAS Build Questions\n```\nUser: \"How do I set up EAS Build for my Expo app?\"\n\n1. Search: Grep \"eas build\" .claude/skills/frontend/expo/docs/ -i\n2. Read: build_introduction.md, build_setup.md\n3. Answer with setup steps, configuration, and examples\n```\n\n### Expo Router Questions\n```\nUser: \"How does file-based routing work in Expo Router?\"\n\n1. Search: Grep \"router|routing\" .claude/skills/frontend/expo/docs/ -i\n2. Read: Router documentation files\n3. Explain routing patterns, file structure, navigation\n```\n\n### Configuration Questions\n```\nUser: \"What are config plugins in Expo?\"\n\n1. Search: Grep \"config plugin\" .claude/skills/frontend/expo/docs/ -i\n2. Read: config-plugins_introduction.md, related files\n3. Explain plugins, usage, development, examples\n```\n\n### Deployment Questions\n```\nUser: \"How do I submit my Expo app to the App Store?\"\n\n1. Search: Grep \"submit|app store\" .claude/skills/frontend/expo/docs/ -i\n2. Read: deploy documentation, EAS Submit guides\n3. Provide submission workflow, requirements, automation\n```\n\n## Response Format\n\nAlways structure responses as:\n\n```markdown\n## [Topic Name]\n\n[Brief overview paragraph]\n\n### Setup\n\n[Configuration steps, installation, prerequisites]\n\n### Implementation\n\n```typescript\n// Code examples with comments\n```\n\n### Key Points\n\n- Important concept 1\n- Important concept 2\n- Important concept 3\n\n### Common Issues\n\n- Issue and solution\n- Gotcha and workaround\n\n### Related\n\n- Related feature or concept\n- Link to additional documentation\n\n**Source:** `.claude/skills/frontend/expo/docs/docs_expo_dev/[filename].md`\n```\n\n## Important Notes\n\n- Always search documentation first before answering\n- Reference specific documentation files in responses\n- Provide practical, working code examples\n- Explain platform-specific differences (iOS vs Android)\n- Mention EAS services when relevant (Build, Submit, Update)\n- Include configuration examples when applicable\n- Highlight breaking changes or version-specific features\n- Use TypeScript examples by default\n- Consider bare workflow users when relevant\n\n## Coverage Areas\n\n**Development Workflow**\n- Expo CLI and development tools\n- Hot reloading and fast refresh\n- Debugging tools and strategies\n- Development builds\n\n**EAS Services**\n- EAS Build (cloud builds)\n- EAS Submit (app store automation)\n- EAS Update (OTA updates)\n- EAS Metadata (store listings)\n\n**Navigation & Routing**\n- Expo Router file-based routing\n- Navigation patterns\n- Deep linking\n- Authentication flows\n\n**Native Integration**\n- Expo Modules API\n- Config plugins\n- Native code integration\n- Bare workflow\n\n**Configuration**\n- app.json / app.config.js\n- eas.json\n- Config plugins\n- Environment variables\n\n**Deployment**\n- App store submission\n- Internal distribution\n- TestFlight and Google Play\n- CI/CD integration\n\n**Platform Features**\n- Push notifications\n- File system\n- Camera and media\n- Location services\n- Authentication\n- Analytics\n\n## Do Not\n\n- Provide outdated information (check doc version/date)\n- Make assumptions about user's Expo SDK version\n- Recommend deprecated approaches\n- Provide React Native CLI solutions when Expo has a better way\n- Ignore platform-specific requirements\n\n## Always\n\n- Search documentation before answering\n- Provide working code examples\n- Reference source documentation\n- Mention version requirements if relevant\n- Consider both managed and bare workflows\n- Link related Expo features\n- Highlight EAS service integration opportunities\n",
        "templates/.claude/skills/ios/skill.md": "---\nname: ios-expert\ndescription: Expert on iOS development with Swift, UIKit, SwiftUI, Xcode, app architecture, platform features, and Apple ecosystem integration. Invoke when user mentions iOS, iPhone, iPad, Swift, SwiftUI, UIKit, Xcode, Apple development, or iOS-specific features.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# iOS Development Expert\n\n## Purpose\n\nProvide expert guidance on iOS development covering Swift programming, UIKit, SwiftUI, Xcode, app architecture, platform features, and Apple ecosystem integration.\n\n## When to Use\n\nAuto-invoke when users mention:\n- iOS development or iPhone/iPad apps\n- Swift programming language\n- SwiftUI or UIKit frameworks\n- Xcode IDE and development tools\n- Apple platform features\n- iOS-specific APIs and services\n- App Store development\n- Apple ecosystem integration\n- iOS app architecture patterns\n\n## Knowledge Base\n\niOS development documentation stored in `.claude/skills/frontend/ios/docs/`\n\nCoverage includes:\n- Swift language fundamentals\n- SwiftUI declarative UI framework\n- UIKit imperative UI framework\n- iOS SDK and platform APIs\n- Xcode development environment\n- App lifecycle and architecture\n- iOS design patterns (MVC, MVVM, etc.)\n- Platform-specific features\n- App Store submission and guidelines\n\n## Process\n\nWhen a user asks about iOS development:\n\n1. **Identify the Topic**\n   - Determine the specific iOS concept or feature\n   - Examples: SwiftUI views, UIKit controllers, Swift syntax, Xcode configuration\n\n2. **Search Documentation**\n   ```\n   Use Grep to search: Grep \"keyword\" .claude/skills/frontend/ios/docs/\n   ```\n\n   Common search patterns:\n   - SwiftUI: `Grep \"swiftui\" .claude/skills/frontend/ios/docs/ -i`\n   - UIKit: `Grep \"uikit\" .claude/skills/frontend/ios/docs/ -i`\n   - Swift language: `Grep \"swift\" .claude/skills/frontend/ios/docs/ -i`\n   - Xcode: `Grep \"xcode\" .claude/skills/frontend/ios/docs/ -i`\n\n3. **Read Relevant Documentation**\n   ```\n   Use Read to load specific files found in search\n   Read .claude/skills/frontend/ios/docs/[filename].md\n   ```\n\n4. **Provide Structured Answer**\n\n   Format responses with:\n   - **Overview**: Brief explanation of the concept\n   - **Setup/Configuration**: Required setup or imports\n   - **Code Examples**: Practical Swift/SwiftUI/UIKit examples\n   - **Best Practices**: Apple's recommendations and patterns\n   - **Common Issues**: Known gotchas or troubleshooting\n   - **Related Topics**: Links to related iOS features\n   - **Source**: Reference the documentation file used\n\n## Example Workflows\n\n### SwiftUI Questions\n```\nUser: \"How do I create a list view in SwiftUI?\"\n\n1. Search: Grep \"list|swiftui\" .claude/skills/frontend/ios/docs/ -i\n2. Read: SwiftUI documentation files\n3. Answer with SwiftUI List examples, modifiers, data binding\n```\n\n### UIKit Questions\n```\nUser: \"How do I set up a UITableView?\"\n\n1. Search: Grep \"uitableview\" .claude/skills/frontend/ios/docs/ -i\n2. Read: UIKit documentation\n3. Explain delegate/datasource pattern, cell configuration\n```\n\n### Swift Language Questions\n```\nUser: \"What are Swift optionals?\"\n\n1. Search: Grep \"optional\" .claude/skills/frontend/ios/docs/ -i\n2. Read: Swift language documentation\n3. Explain optional syntax, unwrapping, optional chaining\n```\n\n### Xcode Questions\n```\nUser: \"How do I configure build settings in Xcode?\"\n\n1. Search: Grep \"build setting|xcode\" .claude/skills/frontend/ios/docs/ -i\n2. Read: Xcode configuration documentation\n3. Provide build settings, schemes, configuration guidance\n```\n\n## Response Format\n\nAlways structure responses as:\n\n```markdown\n## [Topic Name]\n\n[Brief overview paragraph]\n\n### Setup\n\n[Required imports, configuration, prerequisites]\n\n### Implementation\n\n```swift\n// Code examples with comments\nimport SwiftUI\n\nstruct ContentView: View {\n    var body: some View {\n        Text(\"Hello, iOS!\")\n    }\n}\n```\n\n### Key Points\n\n- Important concept 1\n- Important concept 2\n- Important concept 3\n\n### Common Issues\n\n- Issue and solution\n- Gotcha and workaround\n\n### Related\n\n- Related feature or concept\n- Link to additional documentation\n\n**Source:** `.claude/skills/frontend/ios/docs/[filename].md`\n```\n\n## Important Notes\n\n- Always search documentation first before answering\n- Reference specific documentation files in responses\n- Provide working Swift code examples\n- Use Swift naming conventions (camelCase, PascalCase)\n- Consider both SwiftUI and UIKit when relevant\n- Mention iOS version requirements when applicable\n- Include proper imports (import SwiftUI, import UIKit, etc.)\n- Use modern Swift syntax and patterns\n- Consider device differences (iPhone vs iPad)\n\n## Coverage Areas\n\n**Swift Programming**\n- Language fundamentals\n- Optionals and error handling\n- Protocols and generics\n- Closures and functions\n- Value types vs reference types\n- Concurrency (async/await)\n\n**SwiftUI**\n- Declarative views\n- State management (@State, @Binding, @ObservedObject)\n- View modifiers\n- Navigation and routing\n- Data flow\n- Animations\n\n**UIKit**\n- View controllers\n- Auto Layout\n- UITableView / UICollectionView\n- Navigation controllers\n- Delegates and protocols\n- Storyboards and XIBs\n\n**iOS Platform**\n- App lifecycle\n- Background tasks\n- Notifications\n- Core Data / SwiftData\n- Networking (URLSession)\n- File system\n- Location services\n- Camera and photos\n\n**Xcode**\n- Project configuration\n- Build settings\n- Debugging tools\n- Interface Builder\n- Testing (XCTest)\n- Instruments\n\n**Architecture**\n- MVC (Model-View-Controller)\n- MVVM (Model-View-ViewModel)\n- Coordinator pattern\n- Dependency injection\n- Clean architecture\n\n**App Store**\n- App submission process\n- App Store guidelines\n- TestFlight\n- Provisioning profiles\n- Code signing\n\n## Do Not\n\n- Provide Objective-C solutions (prefer Swift)\n- Use deprecated APIs without noting alternatives\n- Ignore memory management considerations\n- Provide solutions incompatible with current iOS versions\n- Mix SwiftUI and UIKit patterns without clear explanation\n\n## Always\n\n- Search documentation before answering\n- Provide working Swift code examples\n- Reference source documentation\n- Mention iOS version requirements\n- Consider both iPhone and iPad layouts\n- Use proper Swift naming conventions\n- Include error handling where appropriate\n- Mention App Store guidelines when relevant\n- Consider accessibility best practices\n",
        "templates/.claude/skills/plaid/accounts/skill.md": "---\nname: plaid-accounts-expert\ndescription: Expert on Plaid accounts and account management. Covers account data retrieval, balance checking, account types, multi-account handling, and account webhooks. Invoke when user mentions Plaid accounts, account balance, account types, or account management.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Plaid Accounts Expert\n\n## Purpose\n\nProvide expert guidance on Plaid account data structures, balance retrieval, account types, and account management.\n\n## When to Use\n\nAuto-invoke when users mention:\n- Plaid accounts or account data\n- Account balances\n- Account types (checking, savings, credit)\n- Multiple accounts\n- Account metadata\n- Balance webhooks\n- Account updates\n\n## Knowledge Base\n\nPlaid accounts documentation in `.claude/skills/api/plaid/docs/`\n\nSearch patterns:\n- `Grep \"account|/accounts/get|/accounts/balance\" .claude/skills/api/plaid/docs/ -i`\n- `Grep \"balance|account.*type|account.*subtype\" .claude/skills/api/plaid/docs/ -i`\n- `Grep \"available.*balance|current.*balance\" .claude/skills/api/plaid/docs/ -i`\n\n## Coverage Areas\n\n**Account Data**\n- Account IDs\n- Account names\n- Account masks (last 4 digits)\n- Account types\n- Account subtypes\n- Official names\n\n**Account Types**\n- Depository (checking, savings, money market)\n- Credit (credit card, line of credit)\n- Loan (mortgage, student, auto)\n- Investment (401k, IRA, brokerage)\n- Other (prepaid, cash management)\n\n**Balance Information**\n- Available balance\n- Current balance\n- Limit (credit accounts)\n- ISO currency codes\n- Unofficial currency codes\n- Real-time balance updates\n\n**Account Management**\n- Multiple account handling\n- Account selection UI\n- Account persistence\n- Account refresh\n- Item rotation\n\n**Webhooks**\n- DEFAULT_UPDATE\n- NEW_ACCOUNTS_AVAILABLE\n- BALANCE updates\n- Error notifications\n\n## Response Format\n\n```markdown\n## [Accounts Topic]\n\n[Overview of accounts feature]\n\n### API Request\n\n```javascript\nconst response = await client.accountsBalanceGet({\n  access_token: accessToken,\n});\n\nconst { accounts } = response.data;\n```\n\n### Response Structure\n\n```json\n{\n  \"accounts\": [{\n    \"account_id\": \"vzeNDwK7KQIm4yEog683uElbp9GRLEFXGK98D\",\n    \"balances\": {\n      \"available\": 100.50,\n      \"current\": 110.25,\n      \"limit\": null,\n      \"iso_currency_code\": \"USD\"\n    },\n    \"mask\": \"0000\",\n    \"name\": \"Plaid Checking\",\n    \"official_name\": \"Plaid Gold Standard 0% Interest Checking\",\n    \"type\": \"depository\",\n    \"subtype\": \"checking\"\n  }]\n}\n```\n\n### Account Types & Subtypes\n\n**Depository:**\n- checking, savings, hsa, cd, money market, paypal, prepaid\n\n**Credit:**\n- credit card, paypal\n\n**Loan:**\n- auto, business, commercial, construction, consumer, home equity, loan, mortgage, overdraft, line of credit, student\n\n**Investment:**\n- 401k, 403b, 457b, 529, brokerage, cash isa, education savings account, gic, health reimbursement arrangement, ira, isa, keogh, lif, life insurance, lira, lrif, lrsp, non-taxable brokerage account, other, other annuity, other insurance, prif, rdsp, resp, retirement, rlif, rrif, rrsp, sarsep, sep ira, simple ira, sipp, stock plan, tfsa, trust, ugma, utma, variable annuity\n\n### Integration Patterns\n\n**Balance Checking:**\n```javascript\nasync function checkSufficientFunds(\n  accessToken,\n  accountId,\n  amount\n) {\n  const response = await client.accountsBalanceGet({\n    access_token: accessToken,\n    options: { account_ids: [accountId] }\n  });\n\n  const account = response.data.accounts[0];\n  return account.balances.available >= amount;\n}\n```\n\n**Multi-Account Selection:**\n```javascript\n// Let user select from multiple accounts\nconst accounts = response.data.accounts\n  .filter(a => a.type === 'depository')\n  .map(a => ({\n    id: a.account_id,\n    name: a.name,\n    mask: a.mask,\n    balance: a.balances.available\n  }));\n```\n\n### Best Practices\n\n- Cache account_id, not full account data\n- Use /accounts/balance/get for real-time balances\n- Handle multiple accounts per Item\n- Display mask for user recognition\n- Filter by type for specific use cases\n- Implement webhook handlers\n- Respect balance update frequency\n\n### Common Issues\n\n- Issue: Stale balance data\n- Solution: Call /accounts/balance/get for real-time\n\n- Issue: Missing available balance\n- Solution: Use current balance as fallback\n\n**Source:** `.claude/skills/api/plaid/docs/[filename].md`\n```\n\n## Key Endpoints\n\n- `/accounts/get` - Get account metadata\n- `/accounts/balance/get` - Get real-time balances\n- `/item/get` - Get Item (institution connection) info\n\n## Balance Types\n\n**Available Balance:**\n- Amount available for withdrawal/spending\n- Accounts for pending transactions\n- Use for payment authorization\n\n**Current Balance:**\n- Total account balance\n- May include pending holds\n- Use for display purposes\n\n**Limit:**\n- Credit limit (credit accounts only)\n- null for non-credit accounts\n\n## Always\n\n- Reference Plaid documentation\n- Explain balance types clearly\n- Handle multiple accounts\n- Show type/subtype usage\n- Include webhook integration\n- Consider real-time requirements\n- Provide account selection patterns\n",
        "templates/.claude/skills/plaid/auth/skill.md": "---\nname: plaid-auth-expert\ndescription: Expert on Plaid Auth product for bank account authentication and verification. Covers account and routing number retrieval, account ownership verification, balance checks, and integration patterns. Invoke when user mentions Plaid Auth, ACH verification, bank account verification, or routing numbers.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Plaid Auth Expert\n\n## Purpose\n\nProvide expert guidance on Plaid Auth, the product for retrieving bank account and routing numbers for ACH transfers and account verification.\n\n## When to Use\n\nAuto-invoke when users mention:\n- Plaid Auth product\n- Bank account verification\n- Account and routing numbers\n- ACH payment setup\n- Account ownership verification\n- Balance verification\n- Instant account verification\n\n## Knowledge Base\n\nPlaid Auth documentation in `.claude/skills/api/plaid/docs/`\n\nSearch patterns:\n- `Grep \"auth|account.*routing|ach\" .claude/skills/api/plaid/docs/ -i`\n- `Grep \"account.*verification|ownership\" .claude/skills/api/plaid/docs/ -i`\n- `Grep \"balance.*check|instant.*verification\" .claude/skills/api/plaid/docs/ -i`\n\n## Coverage Areas\n\n**Auth Product Features**\n- Account and routing number retrieval\n- Account ownership verification\n- Real-time balance checks\n- Account type identification\n- Multiple account support\n\n**Integration Patterns**\n- Link initialization for Auth\n- Token exchange\n- Auth endpoint usage\n- Error handling\n- Webhook notifications\n\n**Verification Methods**\n- Instant verification (preferred)\n- Micro-deposit verification (fallback)\n- Same-day micro-deposits\n- Manual verification\n\n**Use Cases**\n- ACH payment setup\n- Payment method verification\n- Direct deposit enrollment\n- Account linking\n- Payout verification\n\n**Security & Compliance**\n- PCI compliance considerations\n- Data encryption\n- Token management\n- NACHA guidelines\n- Account validation\n\n## Response Format\n\n```markdown\n## [Auth Topic]\n\n[Overview of Auth feature]\n\n### API Request\n\n```javascript\nconst response = await client.authGet({\n  access_token: accessToken,\n});\n\nconst { accounts, numbers } = response.data;\n// accounts: Array of account objects\n// numbers.ach: ACH routing numbers\n```\n\n### Response Structure\n\n```json\n{\n  \"accounts\": [{\n    \"account_id\": \"...\",\n    \"name\": \"Checking\",\n    \"type\": \"depository\",\n    \"subtype\": \"checking\"\n  }],\n  \"numbers\": {\n    \"ach\": [{\n      \"account\": \"0000123456789\",\n      \"routing\": \"011401533\",\n      \"account_id\": \"...\"\n    }]\n  }\n}\n```\n\n### Integration Steps\n\n1. Initialize Link with Auth product\n2. Receive public_token from Link success\n3. Exchange for access_token\n4. Call /auth/get endpoint\n5. Store account/routing securely\n\n### Best Practices\n\n- Never log or store account/routing in plaintext\n- Use access_token, not account numbers in your DB\n- Implement webhook handlers for updates\n- Handle institution errors gracefully\n\n### Common Issues\n\n- Issue: Empty numbers object\n- Solution: Check institution supports Auth\n\n**Source:** `.claude/skills/api/plaid/docs/[filename].md`\n```\n\n## Key Endpoints\n\n- `/link/token/create` - Initialize Link\n- `/item/public_token/exchange` - Get access token\n- `/auth/get` - Retrieve account numbers\n- `/accounts/balance/get` - Check balances\n\n## Always\n\n- Reference Plaid documentation\n- Emphasize security best practices\n- Include error handling\n- Mention webhook integration\n- Explain verification methods\n- Consider institution compatibility\n",
        "templates/.claude/skills/plaid/identity/skill.md": "---\nname: plaid-identity-expert\ndescription: Expert on Plaid Identity product for retrieving account holder information. Covers identity verification, KYC compliance, name/address retrieval, and fraud prevention. Invoke when user mentions Plaid Identity, account holder info, KYC, identity verification, or user information.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Plaid Identity Expert\n\n## Purpose\n\nProvide expert guidance on Plaid Identity, the product for retrieving account holder information for KYC and identity verification.\n\n## When to Use\n\nAuto-invoke when users mention:\n- Plaid Identity product\n- Account holder information\n- KYC (Know Your Customer)\n- Identity verification\n- Name and address retrieval\n- User information validation\n- Fraud prevention\n\n## Knowledge Base\n\nPlaid Identity documentation in `.claude/skills/api/plaid/docs/`\n\nSearch patterns:\n- `Grep \"identity|/identity/get|account.*holder\" .claude/skills/api/plaid/docs/ -i`\n- `Grep \"kyc|identity.*verification\" .claude/skills/api/plaid/docs/ -i`\n- `Grep \"name.*address|owner.*information\" .claude/skills/api/plaid/docs/ -i`\n\n## Coverage Areas\n\n**Identity Data**\n- Account holder names\n- Email addresses\n- Phone numbers\n- Physical addresses\n- Multiple owners support\n\n**Verification Use Cases**\n- KYC compliance\n- Account ownership verification\n- User onboarding\n- Fraud prevention\n- Address validation\n- Identity matching\n\n**Data Quality**\n- Data availability by institution\n- Field completeness\n- Data accuracy\n- Multiple account holders\n- Business vs personal accounts\n\n**Compliance**\n- FCRA compliance considerations\n- Data retention policies\n- Privacy regulations\n- Consent requirements\n- Permissible purposes\n\n## Response Format\n\n```markdown\n## [Identity Topic]\n\n[Overview of Identity feature]\n\n### API Request\n\n```javascript\nconst response = await client.identityGet({\n  access_token: accessToken,\n});\n\nconst { accounts, item } = response.data;\n```\n\n### Response Structure\n\n```json\n{\n  \"accounts\": [{\n    \"account_id\": \"...\",\n    \"owners\": [{\n      \"names\": [\"John Doe\"],\n      \"emails\": [{\n        \"data\": \"john@example.com\",\n        \"primary\": true,\n        \"type\": \"primary\"\n      }],\n      \"phone_numbers\": [{\n        \"data\": \"5555551234\",\n        \"primary\": true,\n        \"type\": \"mobile\"\n      }],\n      \"addresses\": [{\n        \"data\": {\n          \"street\": \"123 Main St\",\n          \"city\": \"San Francisco\",\n          \"region\": \"CA\",\n          \"postal_code\": \"94105\",\n          \"country\": \"US\"\n        },\n        \"primary\": true\n      }]\n    }]\n  }]\n}\n```\n\n### Integration Steps\n\n1. Initialize Link with Identity product\n2. Exchange public_token for access_token\n3. Call /identity/get endpoint\n4. Extract account holder information\n5. Validate against user-provided data\n6. Store for KYC compliance\n\n### Best Practices\n\n- Request minimum necessary data\n- Document permissible purpose\n- Implement data retention policy\n- Handle missing fields gracefully\n- Verify data freshness\n- Support multiple owners\n\n### Common Use Cases\n\n**User Onboarding:**\n```javascript\nconst { owners } = accounts[0];\nconst primaryOwner = owners[0];\n\n// Validate name matches\nconst providedName = user.legal_name;\nconst bankName = primaryOwner.names[0];\nconst nameMatch = validateName(providedName, bankName);\n```\n\n**Address Verification:**\n```javascript\nconst primaryAddress = owners[0].addresses\n  .find(addr => addr.primary);\n\nif (primaryAddress) {\n  // Use for address validation\n  const verified = matchAddress(\n    userAddress,\n    primaryAddress.data\n  );\n}\n```\n\n**Source:** `.claude/skills/api/plaid/docs/[filename].md`\n```\n\n## Key Endpoints\n\n- `/identity/get` - Retrieve identity data\n- `/identity/match` - Match user-provided data\n- `/link/token/create` - Initialize with Identity\n\n## Data Availability\n\nNot all institutions provide all fields:\n- Names: ~100% available\n- Addresses: ~80% available\n- Emails: ~60% available\n- Phone numbers: ~50% available\n\n## Compliance Considerations\n\n- Document KYC purpose\n- Obtain user consent\n- Implement data retention limits\n- Follow FCRA guidelines (if applicable)\n- Respect privacy regulations (GDPR, CCPA)\n\n## Always\n\n- Reference Plaid documentation\n- Handle missing fields\n- Emphasize compliance requirements\n- Include data validation examples\n- Consider institution limitations\n- Explain permissible purposes\n- Show multiple owner handling\n",
        "templates/.claude/skills/plaid/skill.md": "---\nname: plaid-expert\ndescription: Expert guidance for Plaid banking API integration including Link, Auth, Transactions, Identity, and webhook handling. Invoke when user mentions Plaid, bank connections, financial data, ACH, or banking APIs.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash, WebFetch\nmodel: sonnet\n---\n\n# Plaid Integration Expert\n\n## Purpose\n\nProvide comprehensive guidance for integrating Plaid's financial data APIs to connect bank accounts, retrieve transactions, verify identities, and process ACH transfers.\n\n## When to Use\n\nInvoke when user mentions:\n- \"Plaid\" or \"banking API\"\n- \"Link\" (Plaid Link flow)\n- \"bank account connection\" or \"financial data\"\n- \"ACH transfers\" or \"bank-to-bank payments\"\n- \"transaction history\" or \"account balance\"\n- \"income verification\" or \"identity verification\"\n\n## Core Products\n\n### 1. Auth\n**Purpose:** Retrieve bank account and routing numbers for ACH, wire transfers, and bank-to-bank payments.\n\n**Use cases:**\n- ACH payment processing\n- Wire transfers\n- Bank account verification\n\n### 2. Transactions\n**Purpose:** Access transaction history for budgeting, expense tracking, and financial insights.\n\n**Features:**\n- Up to 24 months of history\n- Categorized transactions\n- Merchant information\n- Pending and posted transactions\n\n### 3. Identity\n**Purpose:** Verify user identity through bank account ownership.\n\n**Data retrieved:**\n- Account holder names\n- Email addresses\n- Phone numbers\n- Physical addresses\n\n### 4. Balance\n**Purpose:** Real-time account balance checking to prevent payment failures.\n\n**Use cases:**\n- Insufficient funds detection\n- Payment preflight checks\n- Account monitoring\n\n### 5. Investments\n**Purpose:** Access holdings and transactions from investment accounts.\n\n**Coverage:**\n- Stocks, bonds, ETFs\n- 401(k), IRA, brokerage accounts\n- Real-time valuations\n\n### 6. Liabilities\n**Purpose:** Loan and credit data access.\n\n**Coverage:**\n- Student loans\n- Mortgages\n- Credit cards\n- Auto loans\n\n## Plaid Link Integration\n\n### What is Link?\n\nPlaid Link is the client-side component that users interact with to securely connect their bank accounts. It's a modal/iframe that handles the entire authentication flow.\n\n### Link Flow\n\n1. User clicks \"Connect Bank Account\"\n2. Link modal opens\n3. User searches for their bank\n4. User enters credentials (or OAuth)\n5. User selects accounts to link\n6. Link returns a `public_token`\n7. Exchange `public_token` for `access_token` server-side\n\n### Frontend Integration (React)\n\n**Install:**\n```bash\nnpm install react-plaid-link\n```\n\n**Implementation:**\n```tsx\nimport { usePlaidLink } from 'react-plaid-link';\n\nfunction PlaidLinkButton() {\n  const [linkToken, setLinkToken] = useState(null);\n\n  // 1. Create link token (call your backend)\n  useEffect(() => {\n    async function createLinkToken() {\n      const response = await fetch('/api/plaid/create-link-token', {\n        method: 'POST',\n      });\n      const data = await response.json();\n      setLinkToken(data.link_token);\n    }\n    createLinkToken();\n  }, []);\n\n  // 2. Configure Link\n  const { open, ready } = usePlaidLink({\n    token: linkToken,\n    onSuccess: async (public_token, metadata) => {\n      // 3. Exchange public token for access token\n      await fetch('/api/plaid/exchange-token', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ public_token }),\n      });\n    },\n    onExit: (err, metadata) => {\n      if (err) console.error(err);\n    },\n  });\n\n  return (\n    <button onClick={() => open()} disabled={!ready}>\n      Connect Bank Account\n    </button>\n  );\n}\n```\n\n### Backend: Create Link Token\n\n**Node.js example:**\n```javascript\nconst plaid = require('plaid');\n\nconst client = new plaid.PlaidApi(\n  new plaid.Configuration({\n    basePath: plaid.PlaidEnvironments.sandbox,\n    baseOptions: {\n      headers: {\n        'PLAID-CLIENT-ID': process.env.PLAID_CLIENT_ID,\n        'PLAID-SECRET': process.env.PLAID_SECRET,\n      },\n    },\n  })\n);\n\n// Create link token\napp.post('/api/plaid/create-link-token', async (req, res) => {\n  const request = {\n    user: {\n      client_user_id: req.user.id, // Your app's user ID\n    },\n    client_name: 'Your App Name',\n    products: ['auth', 'transactions'],\n    country_codes: ['US'],\n    language: 'en',\n  };\n\n  try {\n    const response = await client.linkTokenCreate(request);\n    res.json({ link_token: response.data.link_token });\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n```\n\n### Backend: Exchange Public Token\n\n**Store access token securely in your database:**\n```javascript\napp.post('/api/plaid/exchange-token', async (req, res) => {\n  const { public_token } = req.body;\n\n  try {\n    const response = await client.itemPublicTokenExchange({\n      public_token: public_token,\n    });\n\n    const access_token = response.data.access_token;\n    const item_id = response.data.item_id;\n\n    // Store access_token securely in your database\n    await db.users.update(req.user.id, {\n      plaid_access_token: access_token,\n      plaid_item_id: item_id,\n    });\n\n    res.json({ success: true });\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n```\n\n## Retrieving Data\n\n### Get Auth (Account/Routing Numbers)\n\n```javascript\nasync function getAuthData(access_token) {\n  const response = await client.authGet({\n    access_token: access_token,\n  });\n\n  const accounts = response.data.accounts;\n  const numbers = response.data.numbers;\n\n  // ACH numbers\n  const ach = numbers.ach[0];\n  console.log('Account:', ach.account);\n  console.log('Routing:', ach.routing);\n\n  return { accounts, numbers };\n}\n```\n\n### Get Transactions\n\n```javascript\nasync function getTransactions(access_token) {\n  const request = {\n    access_token: access_token,\n    start_date: '2024-01-01',\n    end_date: '2024-12-31',\n  };\n\n  const response = await client.transactionsGet(request);\n  let transactions = response.data.transactions;\n\n  // Handle pagination\n  while (transactions.length < response.data.total_transactions) {\n    const paginatedRequest = {\n      ...request,\n      offset: transactions.length,\n    };\n    const paginatedResponse = await client.transactionsGet(paginatedRequest);\n    transactions = transactions.concat(paginatedResponse.data.transactions);\n  }\n\n  return transactions;\n}\n```\n\n**Transaction object structure:**\n```javascript\n{\n  transaction_id: 'abc123',\n  account_id: 'xyz789',\n  amount: 12.34, // Positive = money out, Negative = money in\n  date: '2024-11-16',\n  name: 'Starbucks',\n  merchant_name: 'Starbucks',\n  category: ['Food and Drink', 'Restaurants', 'Coffee Shop'],\n  pending: false,\n  payment_channel: 'in store'\n}\n```\n\n### Get Balance\n\n```javascript\nasync function getBalance(access_token) {\n  const response = await client.accountsBalanceGet({\n    access_token: access_token,\n  });\n\n  const accounts = response.data.accounts;\n  accounts.forEach(account => {\n    console.log(`${account.name}: ${account.balances.current}`);\n  });\n\n  return accounts;\n}\n```\n\n### Get Identity\n\n```javascript\nasync function getIdentity(access_token) {\n  const response = await client.identityGet({\n    access_token: access_token,\n  });\n\n  const identity = response.data.accounts[0].owners[0];\n  console.log('Name:', identity.names[0]);\n  console.log('Email:', identity.emails[0].data);\n  console.log('Phone:', identity.phone_numbers[0].data);\n\n  return response.data;\n}\n```\n\n## Webhooks\n\n### Setup Webhook URL\n\nConfigure in Plaid Dashboard or via API when creating link token:\n```javascript\n{\n  webhook: 'https://your-domain.com/api/plaid/webhook',\n}\n```\n\n### Webhook Verification\n\n**Verify webhook authenticity:**\n```javascript\nconst crypto = require('crypto');\n\napp.post('/api/plaid/webhook', express.json(), async (req, res) => {\n  const { webhook_type, webhook_code } = req.body;\n\n  // Verify webhook signature\n  const plaidSignature = req.headers['plaid-verification'];\n  const timestamp = req.headers['plaid-timestamp'];\n\n  const payload = JSON.stringify(req.body);\n  const hash = crypto\n    .createHmac('sha256', process.env.PLAID_WEBHOOK_SECRET)\n    .update(`${timestamp}.${payload}`)\n    .digest('hex');\n\n  if (hash !== plaidSignature) {\n    return res.status(401).send('Invalid signature');\n  }\n\n  // Handle webhook events\n  if (webhook_type === 'TRANSACTIONS') {\n    switch (webhook_code) {\n      case 'INITIAL_UPDATE':\n        console.log('Initial transactions available');\n        break;\n      case 'DEFAULT_UPDATE':\n        console.log('New transactions available');\n        // Fetch new transactions\n        break;\n      case 'TRANSACTIONS_REMOVED':\n        console.log('Transactions removed');\n        break;\n    }\n  }\n\n  res.json({ received: true });\n});\n```\n\n**Common webhook events:**\n- `TRANSACTIONS: INITIAL_UPDATE` - First batch ready\n- `TRANSACTIONS: DEFAULT_UPDATE` - New transactions available\n- `ITEM: ERROR` - Connection issue\n- `ITEM: PENDING_EXPIRATION` - Credentials expiring soon\n- `AUTH: AUTOMATICALLY_VERIFIED` - Micro-deposit verification complete\n\n## Environments\n\n**Sandbox** (testing):\n```javascript\nbasePath: plaid.PlaidEnvironments.sandbox\n```\n- Test credentials: `user_good` / `pass_good`\n- No real bank connections\n- Simulate transactions\n\n**Development** (limited live connections):\n```javascript\nbasePath: plaid.PlaidEnvironments.development\n```\n- Up to 100 live bank connections\n- Free for testing\n\n**Production** (live):\n```javascript\nbasePath: plaid.PlaidEnvironments.production\n```\n- Real bank connections\n- Requires approval and billing\n\n## Error Handling\n\n### Update Mode (Re-authentication)\n\nWhen credentials expire or change:\n```javascript\nconst { open } = usePlaidLink({\n  token: linkToken,\n  onSuccess: async (public_token, metadata) => {\n    // Item re-linked successfully\n  },\n});\n\n// Create link token with update mode\nconst request = {\n  access_token: existing_access_token,\n  // ... other config\n};\n```\n\n### Common Errors\n\n**ITEM_LOGIN_REQUIRED:**\n- User needs to re-authenticate\n- Solution: Trigger Link in update mode\n\n**RATE_LIMIT_EXCEEDED:**\n- Too many requests\n- Solution: Implement exponential backoff\n\n**PRODUCT_NOT_READY:**\n- Data still syncing\n- Solution: Wait for webhook or retry\n\n## Security Best Practices\n\n1. **Never expose secret keys client-side:**\n   - Use `PLAID_CLIENT_ID` and `PLAID_SECRET` server-side only\n   - Link tokens are safe for client use\n\n2. **Encrypt access tokens:**\n   - Store access tokens encrypted in database\n   - Never log access tokens\n\n3. **Verify webhook signatures:**\n   - Prevents forged webhooks\n   - Use crypto verification\n\n4. **Use HTTPS:**\n   - Required for webhook endpoints\n   - Required for production\n\n5. **Implement proper user consent:**\n   - Clear disclosure about data access\n   - Follow Plaid's branding guidelines\n\n## Next.js API Route Example\n\n**Create link token:**\n```typescript\n// app/api/plaid/create-link-token/route.ts\nimport { NextResponse } from 'next/server';\nimport { Configuration, PlaidApi, PlaidEnvironments } from 'plaid';\n\nconst client = new PlaidApi(\n  new Configuration({\n    basePath: PlaidEnvironments.sandbox,\n    baseOptions: {\n      headers: {\n        'PLAID-CLIENT-ID': process.env.PLAID_CLIENT_ID!,\n        'PLAID-SECRET': process.env.PLAID_SECRET!,\n      },\n    },\n  })\n);\n\nexport async function POST(req: Request) {\n  const session = await getSession();\n\n  const response = await client.linkTokenCreate({\n    user: { client_user_id: session.user.id },\n    client_name: 'Your App',\n    products: ['auth', 'transactions'],\n    country_codes: ['US'],\n    language: 'en',\n  });\n\n  return NextResponse.json({ link_token: response.data.link_token });\n}\n```\n\n## Testing in Sandbox\n\n**Test credentials:**\n- Username: `user_good`\n- Password: `pass_good`\n- MFA: `1234`\n\n**Test institutions:**\n- \"Platypus Bank\" - Full feature support\n- \"Tartan Bank\" - OAuth flow\n- \"Houndstooth Bank\" - Error testing\n\n## Useful Resources\n\n- **Official Docs:** https://plaid.com/docs/\n- **API Reference:** https://plaid.com/docs/api/\n- **Plaid Link:** https://plaid.com/docs/link/\n- **Webhooks:** https://plaid.com/docs/api/webhooks/\n- **Quickstart:** https://github.com/plaid/quickstart\n\n## Implementation Checklist\n\n- [ ] Sign up for Plaid account\n- [ ] Get client ID and secret keys\n- [ ] Install Plaid SDK: `npm install plaid react-plaid-link`\n- [ ] Set environment variables\n- [ ] Implement link token creation endpoint\n- [ ] Implement token exchange endpoint\n- [ ] Integrate Plaid Link on frontend\n- [ ] Store access tokens securely (encrypted)\n- [ ] Set up webhook endpoint with verification\n- [ ] Handle ITEM_LOGIN_REQUIRED errors\n- [ ] Test with sandbox credentials\n- [ ] Request production access\n- [ ] Implement error handling and retry logic\n- [ ] Add proper user consent/disclosure\n",
        "templates/.claude/skills/plaid/transactions/skill.md": "---\nname: plaid-transactions-expert\ndescription: Expert on Plaid Transactions product for retrieving banking transactions. Covers transaction sync, categorization, webhooks, recurring transactions, and historical data retrieval. Invoke when user mentions Plaid Transactions, transaction history, bank transactions, or transaction categorization.\nallowed-tools: Read, Grep, Glob\nmodel: sonnet\n---\n\n# Plaid Transactions Expert\n\n## Purpose\n\nProvide expert guidance on Plaid Transactions, the product for retrieving and monitoring banking transaction history.\n\n## When to Use\n\nAuto-invoke when users mention:\n- Plaid Transactions product\n- Banking transaction history\n- Transaction sync or updates\n- Transaction categorization\n- Recurring transactions\n- Transaction webhooks\n- Historical transaction data\n\n## Knowledge Base\n\nPlaid Transactions documentation in `.claude/skills/api/plaid/docs/`\n\nSearch patterns:\n- `Grep \"transaction|/transactions/get|/transactions/sync\" .claude/skills/api/plaid/docs/ -i`\n- `Grep \"transaction.*category|recurring\" .claude/skills/api/plaid/docs/ -i`\n- `Grep \"transaction.*webhook|historical.*transaction\" .claude/skills/api/plaid/docs/ -i`\n\n## Coverage Areas\n\n**Transaction Retrieval**\n- /transactions/sync (recommended)\n- /transactions/get (legacy)\n- Historical data (up to 24 months)\n- Real-time updates\n- Pagination\n\n**Transaction Data**\n- Transaction details\n- Merchant information\n- Category classification\n- Location data\n- Payment channel\n\n**Categorization**\n- Automatic categorization\n- Category taxonomy\n- Personal finance categories\n- Detailed categories\n- Category confidence scores\n\n**Updates & Webhooks**\n- SYNC_UPDATES_AVAILABLE\n- DEFAULT_UPDATE\n- TRANSACTIONS_REMOVED\n- Real-time notifications\n- Update polling strategies\n\n**Advanced Features**\n- Recurring transaction detection\n- Income insights\n- Transaction enrichment\n- Personal finance management\n- Spending analysis\n\n## Response Format\n\n```markdown\n## [Transactions Topic]\n\n[Overview of feature]\n\n### API Request\n\n```javascript\n// Recommended: Transactions Sync\nconst response = await client.transactionsSync({\n  access_token: accessToken,\n  cursor: lastCursor,\n});\n\nconst { added, modified, removed, next_cursor } = response.data;\n```\n\n### Response Structure\n\n```json\n{\n  \"added\": [{\n    \"transaction_id\": \"...\",\n    \"amount\": 12.50,\n    \"date\": \"2024-01-15\",\n    \"name\": \"Starbucks\",\n    \"merchant_name\": \"Starbucks\",\n    \"category\": [\"Food and Drink\", \"Restaurants\"],\n    \"category_id\": \"13005000\",\n    \"pending\": false\n  }],\n  \"modified\": [],\n  \"removed\": []\n}\n```\n\n### Integration Pattern\n\n**Initial Sync:**\n1. Call /transactions/sync without cursor\n2. Process added transactions\n3. Store next_cursor\n4. Repeat until has_more = false\n\n**Ongoing Sync:**\n1. Listen for SYNC_UPDATES_AVAILABLE webhook\n2. Call /transactions/sync with stored cursor\n3. Process added/modified/removed\n4. Update stored cursor\n\n### Best Practices\n\n- Use /transactions/sync (not /transactions/get)\n- Store cursor for incremental updates\n- Implement webhook handlers\n- Handle removed transactions\n- Respect rate limits\n- Process pending status changes\n\n### Common Patterns\n\n**Spending Analysis:**\n```javascript\nconst spending = transactions\n  .filter(t => t.amount > 0) // Positive = debit\n  .reduce((sum, t) => sum + t.amount, 0);\n```\n\n**Source:** `.claude/skills/api/plaid/docs/[filename].md`\n```\n\n## Key Endpoints\n\n- `/transactions/sync` - Sync transactions (recommended)\n- `/transactions/get` - Get transactions (legacy)\n- `/transactions/recurring/get` - Recurring transactions\n- `/transactions/refresh` - Force refresh\n\n## Webhooks\n\n- `SYNC_UPDATES_AVAILABLE` - New transaction data\n- `DEFAULT_UPDATE` - Periodic updates (legacy)\n- `TRANSACTIONS_REMOVED` - Deleted transactions\n\n## Always\n\n- Reference Plaid documentation\n- Recommend /transactions/sync over /transactions/get\n- Explain cursor-based pagination\n- Include webhook integration\n- Handle pending transactions\n- Show categorization usage\n- Consider rate limits\n",
        "templates/.claude/skills/shopify/skill.md": "---\nname: shopify-expert\ndescription: Comprehensive Shopify development expert with access to 24 official documentation files covering APIs (GraphQL Admin, Storefront, REST), app development, themes, Liquid, Hydrogen, checkout, extensions, webhooks, Functions, CLI, subscriptions, payments, and all platform features. Invoke when user mentions Shopify, e-commerce, online store, product management, orders, checkout, themes, or headless commerce.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash, WebFetch\nmodel: sonnet\n---\n\n# Shopify Development Expert\n\n## Purpose\n\nProvide comprehensive, accurate guidance for building on Shopify's platform based on 24+ official documentation files. Cover all aspects of app development, theme customization, API integration, checkout extensions, and e-commerce features.\n\n## Documentation Coverage\n\n**Full access to official Shopify documentation (when available):**\n- **Location:** `docs/shopify/`\n- **Files:** 25 markdown files\n- **Coverage:** Complete API reference, guides, best practices, and implementation patterns\n\n**Note:** Documentation must be pulled separately:\n```bash\npipx install docpull\ndocpull https://shopify.dev/docs -o .claude/skills/shopify/docs\n```\n\n**Major Areas:**\n- GraphQL Admin API (products, orders, customers, inventory)\n- Storefront API (cart, checkout, customer accounts)\n- REST Admin API (legacy support)\n- App development (authentication, webhooks, extensions)\n- Theme development (Liquid, sections, blocks)\n- Headless commerce (Hydrogen, Oxygen)\n- Checkout customization (UI extensions, validation)\n- Shopify Functions (discounts, delivery, payments)\n- POS extensions (in-person sales)\n- Subscriptions and selling plans\n- Metafields and custom data\n- Shopify Flow automation\n- CLI and development tools\n- Privacy and compliance\n- Performance optimization\n\n## When to Use\n\nInvoke when user mentions:\n- **Platform:** Shopify, e-commerce, online store, merchant\n- **APIs:** GraphQL, REST, Storefront API, Admin API\n- **Products:** product management, collections, variants, inventory\n- **Orders:** order processing, fulfillment, shipping\n- **Customers:** customer data, accounts, authentication\n- **Checkout:** checkout customization, payment methods, delivery options\n- **Themes:** Liquid templates, theme development, sections, blocks\n- **Apps:** app development, extensions, webhooks, OAuth\n- **Headless:** Hydrogen, React, headless commerce, Oxygen\n- **Functions:** Shopify Functions, custom logic, discounts\n- **Subscriptions:** recurring billing, selling plans, subscriptions\n- **Tools:** Shopify CLI, development workflow\n- **POS:** point of sale, retail, in-person payments\n\n## How to Use Documentation\n\nWhen answering questions:\n\n1. **Search for specific topics:**\n   ```bash\n   # Use Grep to find relevant docs\n   grep -r \"checkout\" .claude/skills/shopify/docs/ --include=\"*.md\"\n   ```\n\n2. **Read specific documentation:**\n   ```bash\n   # API docs\n   cat .claude/skills/shopify/docs/shopify/api-admin-graphql.md\n   cat .claude/skills/shopify/docs/shopify/api-storefront.md\n   ```\n\n3. **Find implementation guides:**\n   ```bash\n   # List all guides\n   ls .claude/skills/shopify/docs/shopify/\n   ```\n\n## Core Authentication\n\n### OAuth 2.0 Flow\n\n```javascript\n// Redirect to Shopify OAuth\nconst authUrl = `https://${shop}/admin/oauth/authorize?` +\n  `client_id=${process.env.SHOPIFY_API_KEY}&` +\n  `scope=read_products,write_products&` +\n  `redirect_uri=${redirectUri}&` +\n  `state=${nonce}`;\n\n// Exchange code for access token\nconst response = await fetch(\n  `https://${shop}/admin/oauth/access_token`,\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      client_id: process.env.SHOPIFY_API_KEY,\n      client_secret: process.env.SHOPIFY_API_SECRET,\n      code\n    })\n  }\n);\n\nconst { access_token } = await response.json();\n```\n\n### Session Tokens (Modern Embedded Apps)\n\n```javascript\nimport { shopifyApi } from '@shopify/shopify-api';\n\nconst shopify = shopifyApi({\n  apiKey: process.env.SHOPIFY_API_KEY,\n  apiSecretKey: process.env.SHOPIFY_API_SECRET,\n  scopes: ['read_products', 'write_products'],\n  hostName: process.env.HOST,\n  isEmbeddedApp: true,\n});\n```\n\n## GraphQL Admin API\n\n### Query Products\n\n```graphql\nquery {\n  products(first: 10) {\n    edges {\n      node {\n        id\n        title\n        handle\n        priceRange {\n          minVariantPrice {\n            amount\n            currencyCode\n          }\n        }\n        variants(first: 5) {\n          edges {\n            node {\n              id\n              sku\n              inventoryQuantity\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n### Create Product\n\n```graphql\nmutation {\n  productCreate(input: {\n    title: \"New Product\"\n    vendor: \"My Store\"\n    productType: \"Apparel\"\n    variants: [{\n      price: \"29.99\"\n      sku: \"PROD-001\"\n    }]\n  }) {\n    product {\n      id\n      title\n    }\n    userErrors {\n      field\n      message\n    }\n  }\n}\n```\n\n### Fetch Orders\n\n```graphql\nquery {\n  orders(first: 25, query: \"fulfillment_status:unfulfilled\") {\n    edges {\n      node {\n        id\n        name\n        createdAt\n        totalPriceSet {\n          shopMoney {\n            amount\n            currencyCode\n          }\n        }\n        customer {\n          email\n        }\n        lineItems(first: 10) {\n          edges {\n            node {\n              title\n              quantity\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n## Storefront API\n\n### Create Cart\n\n```graphql\nmutation {\n  cartCreate(input: {\n    lines: [{\n      merchandiseId: \"gid://shopify/ProductVariant/123\"\n      quantity: 1\n    }]\n  }) {\n    cart {\n      id\n      checkoutUrl\n      cost {\n        totalAmount {\n          amount\n          currencyCode\n        }\n      }\n    }\n  }\n}\n```\n\n### Update Cart\n\n```graphql\nmutation {\n  cartLinesUpdate(\n    cartId: \"gid://shopify/Cart/xyz\"\n    lines: [{\n      id: \"gid://shopify/CartLine/abc\"\n      quantity: 2\n    }]\n  ) {\n    cart {\n      id\n      lines(first: 10) {\n        edges {\n          node {\n            quantity\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n## Webhooks\n\n### Setup Webhook\n\n```javascript\n// Register webhook via API\nconst webhook = await shopify.webhooks.register({\n  topic: 'ORDERS_CREATE',\n  address: 'https://your-app.com/webhooks/orders-create',\n  format: 'json'\n});\n```\n\n### Verify Webhook\n\n```javascript\nimport crypto from 'crypto';\n\nfunction verifyWebhook(body, hmacHeader, secret) {\n  const hash = crypto\n    .createHmac('sha256', secret)\n    .update(body, 'utf8')\n    .digest('base64');\n\n  return hash === hmacHeader;\n}\n\n// In webhook handler\napp.post('/webhooks/orders-create', async (req, res) => {\n  const hmac = req.headers['x-shopify-hmac-sha256'];\n  const body = await req.text();\n\n  if (!verifyWebhook(body, hmac, process.env.SHOPIFY_API_SECRET)) {\n    return res.status(401).send('Invalid HMAC');\n  }\n\n  const order = JSON.parse(body);\n  // Process order...\n\n  res.status(200).send('OK');\n});\n```\n\n## Liquid Templates\n\n### Basic Liquid\n\n```liquid\n<!-- Output product title -->\n{{ product.title }}\n\n<!-- Conditional logic -->\n{% if product.available %}\n  <button>Add to Cart</button>\n{% else %}\n  <span>Sold Out</span>\n{% endif %}\n\n<!-- Loop through variants -->\n{% for variant in product.variants %}\n  <option value=\"{{ variant.id }}\">\n    {{ variant.title }} - {{ variant.price | money }}\n  </option>\n{% endfor %}\n```\n\n### Custom Section\n\n```liquid\n{% schema %}\n{\n  \"name\": \"Featured Product\",\n  \"settings\": [\n    {\n      \"type\": \"product\",\n      \"id\": \"product\",\n      \"label\": \"Product\"\n    }\n  ]\n}\n{% endschema %}\n\n{% if section.settings.product %}\n  {% assign product = section.settings.product %}\n  <div class=\"featured-product\">\n    <img src=\"{{ product.featured_image | img_url: '500x' }}\" alt=\"{{ product.title }}\">\n    <h2>{{ product.title }}</h2>\n    <p>{{ product.price | money }}</p>\n  </div>\n{% endif %}\n```\n\n## Shopify Functions\n\n### Discount Function\n\n```javascript\n// Function to apply volume discount\nexport default (input) => {\n  const quantity = input.cart.lines.reduce((sum, line) => sum + line.quantity, 0);\n\n  let discountPercentage = 0;\n  if (quantity >= 10) discountPercentage = 20;\n  else if (quantity >= 5) discountPercentage = 10;\n\n  if (discountPercentage > 0) {\n    return {\n      discounts: [{\n        message: `${discountPercentage}% volume discount`,\n        targets: [{\n          orderSubtotal: {\n            excludedVariantIds: []\n          }\n        }],\n        value: {\n          percentage: {\n            value: discountPercentage.toString()\n          }\n        }\n      }]\n    };\n  }\n\n  return { discounts: [] };\n};\n```\n\n### Delivery Customization\n\n```javascript\n// Hide specific delivery options\nexport default (input) => {\n  const operations = [];\n\n  // Hide express shipping for orders under $100\n  const cartTotal = parseFloat(input.cart.cost.subtotalAmount.amount);\n\n  if (cartTotal < 100) {\n    const expressOptions = input.cart.deliveryGroups[0].deliveryOptions\n      .filter(option => option.title.toLowerCase().includes('express'));\n\n    expressOptions.forEach(option => {\n      operations.push({\n        hide: {\n          deliveryOptionHandle: option.handle\n        }\n      });\n    });\n  }\n\n  return { operations };\n};\n```\n\n## Hydrogen (Headless Commerce)\n\n### Product Page\n\n```typescript\n// app/routes/products.$handle.tsx\nimport {json, LoaderFunctionArgs} from '@shopify/remix-oxygen';\nimport {useLoaderData} from '@remix-run/react';\n\nexport async function loader({params, context}: LoaderFunctionArgs) {\n  const {product} = await context.storefront.query(PRODUCT_QUERY, {\n    variables: {handle: params.handle},\n  });\n\n  return json({product});\n}\n\nexport default function Product() {\n  const {product} = useLoaderData<typeof loader>();\n\n  return (\n    <div>\n      <h1>{product.title}</h1>\n      <img src={product.featuredImage.url} alt={product.title} />\n      <p>{product.description}</p>\n      <AddToCartButton productId={product.id} />\n    </div>\n  );\n}\n\nconst PRODUCT_QUERY = `#graphql\n  query Product($handle: String!) {\n    product(handle: $handle) {\n      id\n      title\n      description\n      featuredImage {\n        url\n        altText\n      }\n      variants(first: 10) {\n        nodes {\n          id\n          price {\n            amount\n            currencyCode\n          }\n        }\n      }\n    }\n  }\n`;\n```\n\n## Shopify CLI\n\n### Common Commands\n\n```bash\n# Create new app\nshopify app init\n\n# Start development server\nshopify app dev\n\n# Deploy app\nshopify app deploy\n\n# Create extension\nshopify app generate extension\n\n# Create theme\nshopify theme init\n\n# Serve theme locally\nshopify theme dev --store=your-store.myshopify.com\n\n# Push theme\nshopify theme push\n\n# Pull theme\nshopify theme pull\n```\n\n## Testing\n\n### Test Stores\n\n1. Create Partner account: https://partners.shopify.com\n2. Create development store\n3. Install your app\n4. Test features\n\n### Test Data\n\n```javascript\n// Create test product\nconst product = await shopify.rest.Product.save({\n  session,\n  title: \"Test Product\",\n  body_html: \"<strong>Test description</strong>\",\n  vendor: \"Test Vendor\",\n  product_type: \"Test Type\",\n  variants: [{\n    price: \"19.99\",\n    sku: \"TEST-001\"\n  }]\n});\n\n// Create test order\nconst order = await shopify.rest.Order.save({\n  session,\n  line_items: [{\n    variant_id: 123456789,\n    quantity: 1\n  }],\n  customer: {\n    email: \"test@example.com\"\n  }\n});\n```\n\n## Security Best Practices\n\n1. **API Keys:**\n   - Store in environment variables\n   - Never commit to version control\n   - Use separate keys per environment\n   - Rotate if compromised\n\n2. **Webhooks:**\n   - ALWAYS verify HMAC signatures\n   - Use HTTPS endpoints only\n   - Return 200 immediately\n   - Process async\n\n3. **Access Scopes:**\n   - Request minimal scopes\n   - Document why each scope is needed\n   - Review periodically\n\n4. **Rate Limits:**\n   - Respect API rate limits\n   - Implement exponential backoff\n   - Monitor API usage\n\n## Common Errors\n\n### API Authentication\n\n- `Invalid access token` - Check token is valid and has correct scopes\n- `Shop not found` - Verify shop domain format\n- `Missing access token` - Include X-Shopify-Access-Token header\n\n### GraphQL Errors\n\n- `User errors` - Check `userErrors` field in response\n- `Throttled` - Reduce request rate\n- `Field not found` - Verify API version supports field\n\n### Webhook Issues\n\n- `Invalid HMAC` - Check webhook secret and verification logic\n- `Delivery failed` - Ensure endpoint returns 200 within timeout\n- `Not receiving webhooks` - Check webhook registration and endpoint URL\n\n## Resources\n\n- **Dashboard:** https://partners.shopify.com\n- **Documentation:** https://shopify.dev\n- **GraphiQL Admin:** https://shopify.dev/docs/apps/tools/graphiql-admin-api\n- **Community:** https://community.shopify.com\n- **Status:** https://www.shopifystatus.com\n\n## Documentation Quick Reference\n\n**Need to find something specific?**\n\n```bash\n# Search all docs\ngrep -r \"search term\" .claude/skills/shopify/docs/\n\n# Find specific topics\nls .claude/skills/shopify/docs/shopify/\n\n# Read specific guide\ncat .claude/skills/shopify/docs/shopify/webhooks.md\n```\n\n**Common doc files:**\n- `api-admin-graphql.md` - GraphQL Admin API\n- `api-storefront.md` - Storefront API\n- `authentication.md` - OAuth and auth flows\n- `webhooks.md` - Webhook handling\n- `apps.md` - App development\n- `themes.md` - Theme development\n- `liquid.md` - Liquid reference\n- `hydrogen.md` - Headless commerce\n- `checkout.md` - Checkout customization\n- `functions.md` - Shopify Functions\n- `cli.md` - CLI commands\n",
        "templates/.claude/skills/stripe/skill.md": "---\nname: stripe-expert\ndescription: Comprehensive Stripe API expert with access to 3,253 official documentation files covering all payment processing, billing, subscriptions, webhooks, Connect, Terminal, Radar, Identity, Tax, Climate, and integrations. Invoke when user mentions Stripe, payments, subscriptions, billing, payment processing, checkout, invoices, or any payment-related features.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash, WebFetch\nmodel: sonnet\n---\n\n# Stripe Integration Expert\n\n## Purpose\n\nProvide comprehensive, accurate guidance for integrating Stripe payment infrastructure based on 3,253+ official Stripe documentation files. Cover all aspects of payment processing, billing, subscriptions, webhooks, fraud prevention, and advanced features.\n\n## Documentation Coverage\n\n**Full access to official Stripe documentation (when available):**\n- **Location:** `docs/stripe/`\n- **Files:** 3,253 markdown files\n- **Coverage:** Complete API reference, guides, integrations, and best practices\n\n**Note:** Documentation must be pulled separately:\n```bash\npipx install docpull\ndocpull https://docs.stripe.com -o .claude/skills/stripe/docs\n```\n\n**Major Areas:**\n- Payment processing (Payment Intents, Checkout, Elements)\n- Subscription billing (Subscriptions, Invoices, Metering)\n- Customer management\n- Payment methods (cards, wallets, bank transfers, buy now pay later)\n- Webhooks and events\n- Connect (marketplace and platform payments)\n- Terminal (in-person payments)\n- Radar (fraud detection)\n- Identity verification\n- Tax calculation\n- Climate carbon removal\n- Issuing (card creation)\n- Treasury (banking-as-a-service)\n- Financial Connections\n- Agentic Commerce (AI agent payments)\n\n## When to Use\n\nInvoke when user mentions:\n- **Payment Processing:** Stripe, payments, checkout, payment intent, one-time payment\n- **Subscriptions:** billing, recurring payments, subscription, metered billing, usage-based\n- **Customer Management:** customers, payment methods, invoices\n- **Webhooks:** webhooks, events, payment confirmation, notifications\n- **Advanced Features:** Connect, marketplace, platform, split payments, ACH, SEPA\n- **Fraud & Security:** Radar, 3D Secure, SCA, PCI compliance, fraud detection\n- **Tax:** tax calculation, VAT, GST, sales tax\n- **In-Person:** Terminal, card readers, point of sale\n- **Identity:** verification, KYC, identity checks\n- **Banking:** Treasury, financial accounts, payouts, bank accounts\n\n## How to Use Documentation\n\nWhen answering questions:\n\n1. **Search for specific topics:**\n   ```bash\n   # Use Grep to find relevant docs\n   grep -r \"payment intent\" docs/stripe/ --include=\"*.md\"\n   ```\n\n2. **Read specific API references:**\n   ```bash\n   # API docs are in docs/stripe/api/\n   cat docs/stripe/api/payment_intents/api-payment_intents-create.md\n   ```\n\n3. **Find integration guides:**\n   ```bash\n   # Guides and tutorials\n   ls docs/stripe/*.md\n   ```\n\n## Core Authentication\n\n### API Keys\n\n```javascript\n// Server-side (Node.js)\nconst stripe = require('stripe')(process.env.STRIPE_SECRET_KEY);\n\n// Client-side (safe for browsers)\nconst stripe = Stripe(process.env.NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY);\n```\n\n**Key Types:**\n- **Test:** `pk_test_...` (publishable), `sk_test_...` (secret)\n- **Live:** `pk_live_...` (publishable), `sk_live_...` (secret)\n- **Restricted:** Custom permissions for limited access\n\n**Security:**\n- NEVER commit secret keys\n- Use environment variables\n- Rotate immediately if exposed\n- Use restricted keys when possible\n\n## Payment Integration Patterns\n\n### Pattern 1: Stripe Checkout (Fastest)\n\n**Best for:** Quick setup, standard checkout flow, subscriptions\n\n```typescript\n// Server-side API route\nimport Stripe from 'stripe';\nconst stripe = new Stripe(process.env.STRIPE_SECRET_KEY!);\n\nexport async function POST(req: Request) {\n  const session = await stripe.checkout.sessions.create({\n    payment_method_types: ['card'],\n    line_items: [{\n      price: 'price_xxx', // Pre-created price ID\n      quantity: 1,\n    }],\n    mode: 'payment', // or 'subscription' or 'setup'\n    success_url: `${process.env.NEXT_PUBLIC_URL}/success?session_id={CHECKOUT_SESSION_ID}`,\n    cancel_url: `${process.env.NEXT_PUBLIC_URL}/cancel`,\n  });\n\n  return Response.json({ url: session.url });\n}\n\n// Client-side redirect\nwindow.location.href = checkoutUrl;\n```\n\n### Pattern 2: Payment Element (Custom UI)\n\n**Best for:** Custom checkout experience, embedded in your site\n\n```typescript\n// Server: Create Payment Intent\nconst paymentIntent = await stripe.paymentIntents.create({\n  amount: 2000, // $20.00 in cents\n  currency: 'usd',\n  automatic_payment_methods: { enabled: true },\n});\n\n// Client: Mount Payment Element\nimport { Elements, PaymentElement } from '@stripe/react-stripe-js';\nimport { loadStripe } from '@stripe/stripe-js';\n\nconst stripePromise = loadStripe(process.env.NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY!);\n\nfunction CheckoutForm() {\n  return (\n    <Elements stripe={stripePromise} options={{ clientSecret }}>\n      <PaymentElement />\n      <button onClick={handleSubmit}>Pay</button>\n    </Elements>\n  );\n}\n```\n\n### Pattern 3: Subscriptions\n\n**Create subscription:**\n```typescript\nconst subscription = await stripe.subscriptions.create({\n  customer: 'cus_xxx',\n  items: [{ price: 'price_xxx' }],\n  payment_behavior: 'default_incomplete',\n  payment_settings: { save_default_payment_method: 'on_subscription' },\n  expand: ['latest_invoice.payment_intent'],\n});\n\n// Return client secret for 3DS\nconst clientSecret = subscription.latest_invoice.payment_intent.client_secret;\n```\n\n**Subscription lifecycle:**\n- `incomplete` → `active` → `past_due` → `canceled` / `unpaid`\n\n**Metered billing:**\n```typescript\n// Report usage\nawait stripe.subscriptionItems.createUsageRecord('si_xxx', {\n  quantity: 100,\n  timestamp: Math.floor(Date.now() / 1000),\n});\n```\n\n## Webhook Implementation\n\n### Setup Webhook Endpoint\n\n```typescript\nimport { headers } from 'next/headers';\nimport Stripe from 'stripe';\n\nconst stripe = new Stripe(process.env.STRIPE_SECRET_KEY!);\nconst webhookSecret = process.env.STRIPE_WEBHOOK_SECRET!;\n\nexport async function POST(req: Request) {\n  const body = await req.text();\n  const signature = headers().get('stripe-signature')!;\n\n  let event: Stripe.Event;\n\n  try {\n    // Verify signature (CRITICAL for security)\n    event = stripe.webhooks.constructEvent(body, signature, webhookSecret);\n  } catch (err) {\n    return new Response(`Webhook Error: ${err.message}`, { status: 400 });\n  }\n\n  // Handle event\n  switch (event.type) {\n    case 'payment_intent.succeeded':\n      const paymentIntent = event.data.object as Stripe.PaymentIntent;\n      // Fulfill order, send confirmation email\n      await fulfillOrder(paymentIntent);\n      break;\n\n    case 'payment_intent.payment_failed':\n      // Notify customer of failure\n      await notifyPaymentFailure(event.data.object);\n      break;\n\n    case 'customer.subscription.created':\n    case 'customer.subscription.updated':\n      // Update subscription status in database\n      await updateSubscription(event.data.object);\n      break;\n\n    case 'customer.subscription.deleted':\n      // Cancel user access\n      await cancelAccess(event.data.object);\n      break;\n\n    case 'invoice.payment_succeeded':\n      // Subscription payment succeeded\n      await confirmSubscriptionPayment(event.data.object);\n      break;\n\n    case 'invoice.payment_failed':\n      // Notify customer to update payment method\n      await notifyPaymentMethodUpdate(event.data.object);\n      break;\n\n    case 'charge.dispute.created':\n      // Handle dispute\n      await handleDispute(event.data.object);\n      break;\n\n    default:\n      console.log(`Unhandled event type: ${event.type}`);\n  }\n\n  return new Response(JSON.stringify({ received: true }));\n}\n```\n\n**Critical webhook events:**\n- `payment_intent.succeeded` - Payment completed\n- `payment_intent.payment_failed` - Payment failed\n- `customer.subscription.*` - Subscription lifecycle\n- `invoice.payment_succeeded` - Recurring payment succeeded\n- `invoice.payment_failed` - Recurring payment failed\n- `charge.dispute.created` - Customer disputed charge\n- `checkout.session.completed` - Checkout session completed\n\n**Testing webhooks locally:**\n```bash\n# Install Stripe CLI\nbrew install stripe/stripe-cli/stripe\n\n# Login\nstripe login\n\n# Forward webhooks to local server\nstripe listen --forward-to localhost:3000/api/webhooks\n\n# Trigger test events\nstripe trigger payment_intent.succeeded\nstripe trigger customer.subscription.created\n```\n\n## Customer Management\n\n```typescript\n// Create customer\nconst customer = await stripe.customers.create({\n  email: 'customer@example.com',\n  name: 'John Doe',\n  metadata: { user_id: '12345' },\n  payment_method: 'pm_xxx',\n  invoice_settings: {\n    default_payment_method: 'pm_xxx',\n  },\n});\n\n// Attach payment method\nawait stripe.paymentMethods.attach('pm_xxx', {\n  customer: 'cus_xxx',\n});\n\n// List customer's payment methods\nconst paymentMethods = await stripe.paymentMethods.list({\n  customer: 'cus_xxx',\n  type: 'card',\n});\n\n// Update customer\nawait stripe.customers.update('cus_xxx', {\n  metadata: { plan: 'premium' },\n});\n```\n\n## Advanced Features\n\n### Connect (Marketplaces & Platforms)\n\n**Create connected account:**\n```typescript\nconst account = await stripe.accounts.create({\n  type: 'express',\n  country: 'US',\n  email: 'vendor@example.com',\n  capabilities: {\n    card_payments: { requested: true },\n    transfers: { requested: true },\n  },\n});\n\n// Create account link for onboarding\nconst accountLink = await stripe.accountLinks.create({\n  account: account.id,\n  refresh_url: 'https://example.com/reauth',\n  return_url: 'https://example.com/return',\n  type: 'account_onboarding',\n});\n```\n\n**Split payments:**\n```typescript\n// Application fee\nawait stripe.paymentIntents.create({\n  amount: 10000,\n  currency: 'usd',\n  application_fee_amount: 1000, // 10% platform fee\n  transfer_data: {\n    destination: 'acct_xxx', // Connected account\n  },\n});\n\n// Or use separate transfers\nawait stripe.transfers.create({\n  amount: 9000,\n  currency: 'usd',\n  destination: 'acct_xxx',\n});\n```\n\n### Radar (Fraud Prevention)\n\n```typescript\n// Rules are configured in Dashboard\n// But you can review risk scores:\nconst paymentIntent = await stripe.paymentIntents.retrieve('pi_xxx', {\n  expand: ['latest_charge.payment_method_details.card'],\n});\n\nconst riskScore = paymentIntent.latest_charge?.outcome?.risk_score;\nconst riskLevel = paymentIntent.latest_charge?.outcome?.risk_level; // 'normal', 'elevated', 'highest'\n\n// Block high-risk payments\nif (riskLevel === 'highest') {\n  await stripe.paymentIntents.cancel('pi_xxx');\n}\n```\n\n### Tax Calculation\n\n```typescript\nconst session = await stripe.checkout.sessions.create({\n  mode: 'payment',\n  line_items: [{\n    price: 'price_xxx',\n    quantity: 1,\n  }],\n  automatic_tax: { enabled: true }, // Stripe calculates tax automatically\n  customer_update: {\n    address: 'auto', // Collect address for tax\n  },\n  success_url: 'https://example.com/success',\n  cancel_url: 'https://example.com/cancel',\n});\n```\n\n## Testing\n\n### Test Card Numbers\n\n**Success:**\n- `4242 4242 4242 4242` - Visa\n- `5555 5555 5555 4444` - Mastercard\n- `3782 822463 10005` - American Express\n\n**Requires 3D Secure:**\n- `4000 0025 0000 3155` - Visa (3DS required)\n- `4000 0027 6000 3184` - Visa (3DS required, decline)\n\n**Declined:**\n- `4000 0000 0000 9995` - Generic decline\n- `4000 0000 0000 9987` - Insufficient funds\n- `4000 0000 0000 9979` - Stolen card\n\n**Other:**\n- Any future expiry date (e.g., `12/34`)\n- Any 3-digit CVC (4 digits for Amex)\n- Any 5-digit ZIP\n\n## Security Best Practices\n\n1. **API Keys:**\n   - Never expose secret keys client-side\n   - Use restricted keys when possible\n   - Rotate keys periodically\n   - Monitor key usage in Dashboard\n\n2. **Webhooks:**\n   - ALWAYS verify webhook signatures\n   - Use HTTPS endpoints only\n   - Return 200 immediately, process async\n   - Handle retries (Stripe retries failed webhooks)\n\n3. **Amounts:**\n   - Validate amounts server-side\n   - Never trust client-sent amounts\n   - Use Price objects when possible\n\n4. **PCI Compliance:**\n   - Never store card numbers\n   - Use Stripe Elements/Checkout\n   - Let Stripe handle sensitive data\n\n5. **Idempotency:**\n   - Use idempotency keys for critical operations\n   - Prevents duplicate charges on retry\n\n## Common Errors\n\n**Authentication:**\n- `Invalid API Key` - Check key format and environment\n- `No such customer` - Customer deleted or wrong ID\n\n**Payment failures:**\n- `card_declined` - Generic decline (ask customer to try different card)\n- `insufficient_funds` - Not enough money\n- `incorrect_cvc` - Wrong security code\n- `expired_card` - Card expired\n\n**Subscription errors:**\n- `resource_missing` - Price or product doesn't exist\n- `payment_intent_authentication_failure` - 3DS failed\n\n## Framework-Specific Guides\n\n### Next.js App Router\n\n```typescript\n// app/api/create-payment-intent/route.ts\nimport { NextRequest } from 'next/server';\nimport Stripe from 'stripe';\n\nconst stripe = new Stripe(process.env.STRIPE_SECRET_KEY!);\n\nexport async function POST(req: NextRequest) {\n  const { amount } = await req.json();\n\n  const paymentIntent = await stripe.paymentIntents.create({\n    amount,\n    currency: 'usd',\n    automatic_payment_methods: { enabled: true },\n  });\n\n  return Response.json({ clientSecret: paymentIntent.client_secret });\n}\n```\n\n### React Hook\n\n```typescript\nimport { useState } from 'react';\nimport { loadStripe } from '@stripe/stripe-js';\n\nexport function useStripeCheckout() {\n  const [loading, setLoading] = useState(false);\n\n  const checkout = async (priceId: string) => {\n    setLoading(true);\n\n    const res = await fetch('/api/checkout', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ priceId }),\n    });\n\n    const { sessionId } = await res.json();\n\n    const stripe = await loadStripe(process.env.NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY!);\n    await stripe?.redirectToCheckout({ sessionId });\n\n    setLoading(false);\n  };\n\n  return { checkout, loading };\n}\n```\n\n## TypeScript Types & Configuration\n\n### Database Schema Types\n\n```typescript\n// types/stripe.ts\nimport Stripe from 'stripe';\n\nexport interface StripeCustomerData {\n  stripeCustomerId: string;\n  stripeSubscriptionId?: string;\n  stripePriceId?: string;\n  stripeCurrentPeriodEnd?: Date;\n  userId: string;\n}\n\nexport interface CreateCheckoutSessionParams {\n  userId: string;\n  priceId: string;\n  successUrl: string;\n  cancelUrl: string;\n  metadata?: Record<string, string>;\n}\n\nexport interface WebhookEventData {\n  event: Stripe.Event;\n  customerId?: string;\n  subscriptionId?: string;\n  invoiceId?: string;\n}\n\nexport type StripeSubscriptionStatus =\n  | 'incomplete'\n  | 'incomplete_expired'\n  | 'trialing'\n  | 'active'\n  | 'past_due'\n  | 'canceled'\n  | 'unpaid';\n\nexport interface SubscriptionUpdate {\n  subscriptionId: string;\n  status: StripeSubscriptionStatus;\n  currentPeriodEnd: Date;\n  cancelAtPeriodEnd: boolean;\n  priceId: string;\n}\n```\n\n### Stripe Client Configuration\n\n```typescript\n// lib/stripe/config.ts\nimport Stripe from 'stripe';\n\nif (!process.env.STRIPE_SECRET_KEY) {\n  throw new Error('STRIPE_SECRET_KEY is not set');\n}\n\nexport const stripe = new Stripe(process.env.STRIPE_SECRET_KEY, {\n  apiVersion: '2024-11-20.acacia',\n  typescript: true,\n  appInfo: {\n    name: 'Your App Name',\n    version: '1.0.0',\n  },\n  // Retry logic for network failures\n  maxNetworkRetries: 2,\n  timeout: 30000, // 30 seconds\n});\n\n// Client-side configuration\nexport const getStripe = () => {\n  if (!process.env.NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY) {\n    throw new Error('NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY is not set');\n  }\n  return loadStripe(process.env.NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY);\n};\n```\n\n### Environment Variables\n\n```bash\n# .env.local\nSTRIPE_SECRET_KEY=sk_test_...\nNEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY=pk_test_...\nSTRIPE_WEBHOOK_SECRET=whsec_...\n\n# Production\nSTRIPE_SECRET_KEY=sk_live_...\nNEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY=pk_live_...\nSTRIPE_WEBHOOK_SECRET=whsec_...\n```\n\n## Next.js App Router Integration\n\n### Server Actions for Payments\n\n```typescript\n// app/actions/stripe.ts\n'use server';\n\nimport { stripe } from '@/lib/stripe/config';\nimport { auth } from '@/lib/auth';\nimport { revalidatePath } from 'next/cache';\n\nexport async function createCheckoutSession(priceId: string) {\n  const session = await auth();\n  if (!session?.user?.id) {\n    throw new Error('Unauthorized');\n  }\n\n  const checkoutSession = await stripe.checkout.sessions.create({\n    customer_email: session.user.email,\n    mode: 'subscription',\n    payment_method_types: ['card'],\n    line_items: [{\n      price: priceId,\n      quantity: 1,\n    }],\n    success_url: `${process.env.NEXT_PUBLIC_URL}/dashboard?success=true`,\n    cancel_url: `${process.env.NEXT_PUBLIC_URL}/pricing?canceled=true`,\n    metadata: {\n      userId: session.user.id,\n    },\n  });\n\n  return { url: checkoutSession.url };\n}\n\nexport async function createPortalSession() {\n  const session = await auth();\n  if (!session?.user?.stripeCustomerId) {\n    throw new Error('No Stripe customer found');\n  }\n\n  const portalSession = await stripe.billingPortal.sessions.create({\n    customer: session.user.stripeCustomerId,\n    return_url: `${process.env.NEXT_PUBLIC_URL}/dashboard`,\n  });\n\n  return { url: portalSession.url };\n}\n\nexport async function cancelSubscription(subscriptionId: string) {\n  const session = await auth();\n  if (!session?.user?.id) {\n    throw new Error('Unauthorized');\n  }\n\n  await stripe.subscriptions.update(subscriptionId, {\n    cancel_at_period_end: true,\n  });\n\n  revalidatePath('/dashboard');\n  return { success: true };\n}\n```\n\n### Client Component with Server Actions\n\n```typescript\n// app/components/PricingCard.tsx\n'use client';\n\nimport { useState } from 'react';\nimport { createCheckoutSession } from '@/app/actions/stripe';\n\nexport function PricingCard({ priceId, name, price, features }) {\n  const [loading, setLoading] = useState(false);\n\n  const handleCheckout = async () => {\n    try {\n      setLoading(true);\n      const { url } = await createCheckoutSession(priceId);\n      if (url) window.location.href = url;\n    } catch (error) {\n      console.error('Checkout error:', error);\n      alert('Failed to start checkout');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"pricing-card\">\n      <h3>{name}</h3>\n      <p>${price}/mo</p>\n      <ul>\n        {features.map(f => <li key={f}>{f}</li>)}\n      </ul>\n      <button onClick={handleCheckout} disabled={loading}>\n        {loading ? 'Loading...' : 'Subscribe'}\n      </button>\n    </div>\n  );\n}\n```\n\n### Route Handlers\n\n```typescript\n// app/api/create-payment-intent/route.ts\nimport { NextRequest } from 'next/server';\nimport { stripe } from '@/lib/stripe/config';\nimport { auth } from '@/lib/auth';\n\nexport async function POST(req: NextRequest) {\n  try {\n    const session = await auth();\n    if (!session?.user) {\n      return Response.json({ error: 'Unauthorized' }, { status: 401 });\n    }\n\n    const { amount, currency = 'usd' } = await req.json();\n\n    // Validate amount server-side (CRITICAL)\n    if (!amount || amount < 50) { // $0.50 minimum\n      return Response.json({ error: 'Invalid amount' }, { status: 400 });\n    }\n\n    const paymentIntent = await stripe.paymentIntents.create({\n      amount,\n      currency,\n      automatic_payment_methods: { enabled: true },\n      metadata: {\n        userId: session.user.id,\n      },\n    });\n\n    return Response.json({\n      clientSecret: paymentIntent.client_secret\n    });\n  } catch (error) {\n    console.error('Payment intent error:', error);\n    return Response.json(\n      { error: 'Failed to create payment intent' },\n      { status: 500 }\n    );\n  }\n}\n```\n\n## Production Patterns\n\n### Error Handling\n\n```typescript\n// lib/stripe/errors.ts\nimport Stripe from 'stripe';\n\nexport function handleStripeError(error: unknown): {\n  message: string;\n  userMessage: string;\n  code?: string;\n} {\n  if (error instanceof Stripe.errors.StripeError) {\n    // Card errors\n    if (error.type === 'StripeCardError') {\n      return {\n        message: error.message,\n        userMessage: 'Your card was declined. Please try a different payment method.',\n        code: error.code,\n      };\n    }\n\n    // Rate limit errors\n    if (error.type === 'StripeRateLimitError') {\n      return {\n        message: 'Too many requests',\n        userMessage: 'Too many requests. Please try again in a moment.',\n      };\n    }\n\n    // Invalid request errors\n    if (error.type === 'StripeInvalidRequestError') {\n      return {\n        message: error.message,\n        userMessage: 'Invalid request. Please contact support.',\n        code: error.code,\n      };\n    }\n\n    // API errors\n    if (error.type === 'StripeAPIError') {\n      return {\n        message: 'Stripe API error',\n        userMessage: 'Payment service temporarily unavailable. Please try again.',\n      };\n    }\n\n    // Connection errors\n    if (error.type === 'StripeConnectionError') {\n      return {\n        message: 'Network error',\n        userMessage: 'Network error. Please check your connection and try again.',\n      };\n    }\n\n    // Authentication errors\n    if (error.type === 'StripeAuthenticationError') {\n      return {\n        message: 'Authentication failed',\n        userMessage: 'Authentication error. Please contact support.',\n      };\n    }\n  }\n\n  // Generic error\n  return {\n    message: error instanceof Error ? error.message : 'Unknown error',\n    userMessage: 'An unexpected error occurred. Please try again.',\n  };\n}\n\n// Usage in API routes\nexport async function POST(req: NextRequest) {\n  try {\n    const paymentIntent = await stripe.paymentIntents.create({...});\n    return Response.json({ clientSecret: paymentIntent.client_secret });\n  } catch (error) {\n    const { userMessage, message } = handleStripeError(error);\n    console.error('Stripe error:', message);\n    return Response.json({ error: userMessage }, { status: 400 });\n  }\n}\n```\n\n### Idempotency Keys\n\n```typescript\n// lib/stripe/idempotency.ts\nimport { v4 as uuidv4 } from 'uuid';\n\nexport async function createPaymentWithIdempotency(\n  amount: number,\n  userId: string\n) {\n  const idempotencyKey = `payment_${userId}_${Date.now()}_${uuidv4()}`;\n\n  try {\n    const paymentIntent = await stripe.paymentIntents.create(\n      {\n        amount,\n        currency: 'usd',\n        metadata: { userId },\n      },\n      {\n        idempotencyKey, // Prevents duplicate charges on retry\n      }\n    );\n\n    return paymentIntent;\n  } catch (error) {\n    // If request fails, retry with same idempotency key\n    // Stripe will return the original result instead of creating duplicate\n    throw error;\n  }\n}\n\n// For subscriptions\nexport async function createSubscriptionIdempotent(\n  customerId: string,\n  priceId: string\n) {\n  const idempotencyKey = `sub_${customerId}_${priceId}`;\n\n  return stripe.subscriptions.create(\n    {\n      customer: customerId,\n      items: [{ price: priceId }],\n    },\n    { idempotencyKey }\n  );\n}\n```\n\n### Retry Logic with Exponential Backoff\n\n```typescript\n// lib/stripe/retry.ts\nexport async function retryStripeOperation<T>(\n  operation: () => Promise<T>,\n  maxRetries = 3\n): Promise<T> {\n  let lastError: Error;\n\n  for (let i = 0; i < maxRetries; i++) {\n    try {\n      return await operation();\n    } catch (error) {\n      lastError = error as Error;\n\n      // Don't retry certain errors\n      if (error instanceof Stripe.errors.StripeCardError) {\n        throw error; // Card declined - don't retry\n      }\n      if (error instanceof Stripe.errors.StripeInvalidRequestError) {\n        throw error; // Invalid params - don't retry\n      }\n\n      // Retry with exponential backoff\n      const delay = Math.min(1000 * Math.pow(2, i), 10000);\n      await new Promise(resolve => setTimeout(resolve, delay));\n    }\n  }\n\n  throw lastError!;\n}\n\n// Usage\nconst paymentIntent = await retryStripeOperation(() =>\n  stripe.paymentIntents.create({\n    amount: 1000,\n    currency: 'usd',\n  })\n);\n```\n\n### Rate Limiting\n\n```typescript\n// lib/stripe/rate-limit.ts\nimport { Ratelimit } from '@upstash/ratelimit';\nimport { Redis } from '@upstash/redis';\n\nconst ratelimit = new Ratelimit({\n  redis: Redis.fromEnv(),\n  limiter: Ratelimit.slidingWindow(10, '10 s'), // 10 requests per 10 seconds\n});\n\nexport async function POST(req: NextRequest) {\n  const ip = req.ip ?? '127.0.0.1';\n  const { success, limit, reset, remaining } = await ratelimit.limit(ip);\n\n  if (!success) {\n    return Response.json(\n      { error: 'Too many requests' },\n      {\n        status: 429,\n        headers: {\n          'X-RateLimit-Limit': limit.toString(),\n          'X-RateLimit-Remaining': remaining.toString(),\n          'X-RateLimit-Reset': reset.toString(),\n        },\n      }\n    );\n  }\n\n  // Process Stripe request\n  const paymentIntent = await stripe.paymentIntents.create({...});\n  return Response.json({ clientSecret: paymentIntent.client_secret });\n}\n```\n\n## Stripe Connect Deep Dive\n\n### Account Creation & Onboarding\n\n```typescript\n// lib/stripe/connect.ts\nexport async function createConnectedAccount(email: string, country: string) {\n  const account = await stripe.accounts.create({\n    type: 'express', // or 'standard' or 'custom'\n    country,\n    email,\n    capabilities: {\n      card_payments: { requested: true },\n      transfers: { requested: true },\n    },\n    business_type: 'individual', // or 'company'\n  });\n\n  return account;\n}\n\nexport async function createAccountOnboardingLink(accountId: string) {\n  const accountLink = await stripe.accountLinks.create({\n    account: accountId,\n    refresh_url: `${process.env.NEXT_PUBLIC_URL}/connect/reauth`,\n    return_url: `${process.env.NEXT_PUBLIC_URL}/connect/return`,\n    type: 'account_onboarding',\n  });\n\n  return accountLink.url;\n}\n\n// Check if account is fully onboarded\nexport async function isAccountOnboarded(accountId: string): Promise<boolean> {\n  const account = await stripe.accounts.retrieve(accountId);\n\n  return (\n    account.details_submitted &&\n    account.charges_enabled &&\n    account.payouts_enabled\n  );\n}\n```\n\n### Marketplace Payment Flows\n\n```typescript\n// Pattern 1: Direct Charge (platform receives, then transfers)\nexport async function createDirectCharge(\n  amount: number,\n  connectedAccountId: string\n) {\n  const paymentIntent = await stripe.paymentIntents.create({\n    amount,\n    currency: 'usd',\n    application_fee_amount: Math.floor(amount * 0.1), // 10% platform fee\n    transfer_data: {\n      destination: connectedAccountId,\n    },\n  });\n\n  return paymentIntent;\n}\n\n// Pattern 2: Destination Charge (connected account receives, platform takes fee)\nexport async function createDestinationCharge(\n  amount: number,\n  connectedAccountId: string\n) {\n  const paymentIntent = await stripe.paymentIntents.create(\n    {\n      amount,\n      currency: 'usd',\n      application_fee_amount: Math.floor(amount * 0.1),\n    },\n    {\n      stripeAccount: connectedAccountId, // Charge appears on connected account\n    }\n  );\n\n  return paymentIntent;\n}\n\n// Pattern 3: Separate Transfers\nexport async function createSeparateTransfer(\n  amount: number,\n  connectedAccountId: string,\n  chargeId: string\n) {\n  const platformFee = Math.floor(amount * 0.1);\n  const transferAmount = amount - platformFee;\n\n  const transfer = await stripe.transfers.create({\n    amount: transferAmount,\n    currency: 'usd',\n    destination: connectedAccountId,\n    source_transaction: chargeId,\n  });\n\n  return transfer;\n}\n```\n\n### Multi-Party Payments (Split Payments)\n\n```typescript\n// Split payment between multiple sellers\nexport async function createMultiPartyPayment(\n  totalAmount: number,\n  sellers: Array<{ accountId: string; amount: number }>\n) {\n  // Create charge on platform account\n  const paymentIntent = await stripe.paymentIntents.create({\n    amount: totalAmount,\n    currency: 'usd',\n  });\n\n  // After charge succeeds, create transfers to each seller\n  const transfers = await Promise.all(\n    sellers.map(seller =>\n      stripe.transfers.create({\n        amount: seller.amount,\n        currency: 'usd',\n        destination: seller.accountId,\n      })\n    )\n  );\n\n  return { paymentIntent, transfers };\n}\n```\n\n### Connect Webhooks\n\n```typescript\n// Listen for Connect account events\nexport async function POST(req: Request) {\n  const event = await verifyWebhook(req);\n\n  switch (event.type) {\n    case 'account.updated':\n      const account = event.data.object as Stripe.Account;\n      // Check if account completed onboarding\n      if (account.details_submitted && account.charges_enabled) {\n        await updateDatabase({ accountId: account.id, status: 'active' });\n      }\n      break;\n\n    case 'account.application.deauthorized':\n      // User disconnected their account\n      const deauthAccount = event.data.object as Stripe.Account;\n      await handleAccountDisconnection(deauthAccount.id);\n      break;\n\n    case 'capability.updated':\n      // Track capability status (card_payments, transfers, etc.)\n      const capability = event.data.object;\n      await trackCapabilityStatus(capability);\n      break;\n\n    case 'payout.paid':\n      // Payout to connected account succeeded\n      const payout = event.data.object as Stripe.Payout;\n      await notifyVendorPayoutSuccess(payout);\n      break;\n\n    case 'payout.failed':\n      // Payout failed - notify vendor\n      const failedPayout = event.data.object as Stripe.Payout;\n      await notifyVendorPayoutFailure(failedPayout);\n      break;\n  }\n\n  return Response.json({ received: true });\n}\n```\n\n## Stripe Terminal (In-Person Payments)\n\n### Reader Setup\n\n```typescript\n// lib/stripe/terminal.ts\nexport async function registerReader(\n  registrationCode: string,\n  label: string,\n  locationId: string\n) {\n  const reader = await stripe.terminal.readers.create({\n    registration_code: registrationCode,\n    label,\n    location: locationId,\n  });\n\n  return reader;\n}\n\nexport async function createTerminalLocation(address: {\n  line1: string;\n  city: string;\n  state: string;\n  postal_code: string;\n  country: string;\n}) {\n  const location = await stripe.terminal.locations.create({\n    display_name: 'Retail Store',\n    address,\n  });\n\n  return location;\n}\n```\n\n### Payment Collection\n\n```typescript\n// Create payment intent for Terminal\nexport async function createTerminalPaymentIntent(amount: number) {\n  const paymentIntent = await stripe.paymentIntents.create({\n    amount,\n    currency: 'usd',\n    payment_method_types: ['card_present'],\n    capture_method: 'manual', // Authorize first, capture later\n  });\n\n  return paymentIntent;\n}\n\n// Server-side: Process Terminal connection token\nexport async function POST(req: NextRequest) {\n  const connectionToken = await stripe.terminal.connectionTokens.create();\n  return Response.json({ secret: connectionToken.secret });\n}\n```\n\n### Client-Side Terminal Integration\n\n```typescript\n// Client component for Terminal SDK\n'use client';\n\nimport { loadStripeTerminal } from '@stripe/terminal-js';\n\nexport function TerminalReader() {\n  useEffect(() => {\n    const initTerminal = async () => {\n      const terminal = await loadStripeTerminal();\n\n      const term = terminal.create({\n        onFetchConnectionToken: async () => {\n          const res = await fetch('/api/terminal/connection-token');\n          const { secret } = await res.json();\n          return secret;\n        },\n        onUnexpectedReaderDisconnect: () => {\n          console.log('Reader disconnected');\n        },\n      });\n\n      // Discover readers\n      const discoverResult = await term.discoverReaders();\n      if (discoverResult.discoveredReaders.length > 0) {\n        // Connect to first reader\n        await term.connectReader(discoverResult.discoveredReaders[0]);\n      }\n    };\n\n    initTerminal();\n  }, []);\n\n  return <div>Terminal Reader Component</div>;\n}\n```\n\n## Stripe Radar (Fraud Prevention)\n\n### Custom Fraud Rules\n\n```typescript\n// Review high-risk charges before processing\nexport async function reviewHighRiskCharge(paymentIntentId: string) {\n  const paymentIntent = await stripe.paymentIntents.retrieve(paymentIntentId, {\n    expand: ['latest_charge'],\n  });\n\n  const charge = paymentIntent.latest_charge as Stripe.Charge;\n  const outcome = charge?.outcome;\n\n  if (outcome?.risk_level === 'highest') {\n    // Block the payment\n    await stripe.paymentIntents.cancel(paymentIntentId);\n    return { action: 'blocked', reason: outcome.reason };\n  }\n\n  if (outcome?.risk_level === 'elevated' && outcome?.risk_score > 65) {\n    // Require manual review\n    return { action: 'review', riskScore: outcome.risk_score };\n  }\n\n  return { action: 'approve' };\n}\n\n// Add custom fraud metadata\nexport async function createPaymentWithFraudCheck(\n  amount: number,\n  email: string,\n  ipAddress: string\n) {\n  const paymentIntent = await stripe.paymentIntents.create({\n    amount,\n    currency: 'usd',\n    metadata: {\n      customer_email: email,\n      customer_ip: ipAddress,\n      order_id: generateOrderId(),\n    },\n    // Radar uses this data for fraud detection\n  });\n\n  return paymentIntent;\n}\n```\n\n### 3D Secure Authentication\n\n```typescript\n// Force 3D Secure for high-value transactions\nexport async function createSecurePayment(amount: number) {\n  const paymentIntent = await stripe.paymentIntents.create({\n    amount,\n    currency: 'usd',\n    payment_method_options: {\n      card: {\n        request_three_d_secure: amount > 10000 ? 'any' : 'automatic',\n      },\n    },\n  });\n\n  return paymentIntent;\n}\n\n// Handle 3DS authentication in client\nasync function handleCardAction(clientSecret: string) {\n  const stripe = await getStripe();\n  const { error, paymentIntent } = await stripe.handleCardAction(clientSecret);\n\n  if (error) {\n    console.error('3DS failed:', error);\n    return { success: false };\n  }\n\n  if (paymentIntent.status === 'requires_confirmation') {\n    // Confirm on server\n    await fetch('/api/confirm-payment', {\n      method: 'POST',\n      body: JSON.stringify({ paymentIntentId: paymentIntent.id }),\n    });\n  }\n\n  return { success: true };\n}\n```\n\n### Reviewing Disputes\n\n```typescript\n// Handle dispute webhooks\ncase 'charge.dispute.created':\n  const dispute = event.data.object as Stripe.Dispute;\n\n  // Automatically submit evidence for low-amount disputes\n  if (dispute.amount < 5000) { // $50\n    await stripe.disputes.update(dispute.id, {\n      evidence: {\n        customer_email_address: dispute.metadata.customer_email,\n        customer_name: dispute.metadata.customer_name,\n        shipping_tracking_number: dispute.metadata.tracking_number,\n      },\n    });\n  } else {\n    // Flag for manual review\n    await notifyDisputeTeam(dispute);\n  }\n  break;\n\ncase 'charge.dispute.closed':\n  const closedDispute = event.data.object as Stripe.Dispute;\n  if (closedDispute.status === 'won') {\n    await notifyDisputeWon(closedDispute);\n  } else {\n    await processDisputeLoss(closedDispute);\n  }\n  break;\n```\n\n## Stripe Identity\n\n### Identity Verification\n\n```typescript\n// Create verification session\nexport async function createIdentityVerification(email: string) {\n  const verificationSession = await stripe.identity.verificationSessions.create({\n    type: 'document',\n    metadata: {\n      user_email: email,\n    },\n    options: {\n      document: {\n        require_live_capture: true,\n        require_matching_selfie: true,\n        allowed_types: ['driving_license', 'passport', 'id_card'],\n      },\n    },\n  });\n\n  return {\n    clientSecret: verificationSession.client_secret,\n    url: verificationSession.url,\n  };\n}\n\n// Check verification status\nexport async function checkVerificationStatus(sessionId: string) {\n  const session = await stripe.identity.verificationSessions.retrieve(sessionId);\n\n  return {\n    status: session.status, // 'requires_input', 'verified', 'canceled'\n    verified: session.status === 'verified',\n    lastError: session.last_error,\n  };\n}\n\n// Webhook handler\ncase 'identity.verification_session.verified':\n  const verifiedSession = event.data.object;\n  await updateUserVerificationStatus(\n    verifiedSession.metadata.user_email,\n    'verified'\n  );\n  break;\n\ncase 'identity.verification_session.requires_input':\n  const requiresInputSession = event.data.object;\n  await notifyUserVerificationIssue(requiresInputSession);\n  break;\n```\n\n## Stripe Issuing (Card Creation)\n\n### Create Virtual/Physical Cards\n\n```typescript\n// Create cardholder\nexport async function createCardholder(\n  name: string,\n  email: string,\n  phone: string\n) {\n  const cardholder = await stripe.issuing.cardholders.create({\n    name,\n    email,\n    phone_number: phone,\n    billing: {\n      address: {\n        line1: '123 Main St',\n        city: 'San Francisco',\n        state: 'CA',\n        postal_code: '94111',\n        country: 'US',\n      },\n    },\n    type: 'individual',\n  });\n\n  return cardholder;\n}\n\n// Create virtual card\nexport async function createVirtualCard(cardholderId: string) {\n  const card = await stripe.issuing.cards.create({\n    cardholder: cardholderId,\n    currency: 'usd',\n    type: 'virtual',\n    status: 'active',\n    spending_controls: {\n      spending_limits: [{\n        amount: 100000, // $1,000 limit\n        interval: 'monthly',\n      }],\n      allowed_categories: ['food_restaurants', 'gas_stations'],\n    },\n  });\n\n  return card;\n}\n\n// Retrieve card details (PAN, CVV, etc.) - SENSITIVE\nexport async function getCardDetails(cardId: string) {\n  const card = await stripe.issuing.cards.retrieve(cardId, {\n    expand: ['number', 'cvc'],\n  });\n\n  return {\n    number: card.number,\n    cvc: card.cvc,\n    expMonth: card.exp_month,\n    expYear: card.exp_year,\n  };\n}\n```\n\n### Authorization Controls\n\n```typescript\n// Webhook: Real-time authorization approval\ncase 'issuing_authorization.request':\n  const authorization = event.data.object as Stripe.Issuing.Authorization;\n\n  // Check spending limits\n  const userLimits = await getUserSpendingLimits(authorization.cardholder);\n\n  if (authorization.amount > userLimits.perTransaction) {\n    // Decline authorization\n    await stripe.issuing.authorizations.decline(authorization.id, {\n      metadata: { reason: 'exceeds_limit' },\n    });\n  } else {\n    // Approve authorization\n    await stripe.issuing.authorizations.approve(authorization.id);\n  }\n  break;\n\ncase 'issuing_authorization.created':\n  // Log transaction for user\n  await logCardTransaction(event.data.object);\n  break;\n```\n\n## Stripe Tax\n\n### Automatic Tax Calculation\n\n```typescript\n// Enable tax on Checkout\nexport async function createCheckoutWithTax(priceId: string) {\n  const session = await stripe.checkout.sessions.create({\n    mode: 'payment',\n    line_items: [{ price: priceId, quantity: 1 }],\n    automatic_tax: { enabled: true },\n    customer_update: {\n      address: 'auto', // Collect address for tax calculation\n      shipping: 'auto',\n    },\n    success_url: `${process.env.NEXT_PUBLIC_URL}/success`,\n    cancel_url: `${process.env.NEXT_PUBLIC_URL}/cancel`,\n  });\n\n  return session;\n}\n\n// Calculate tax for custom amount\nexport async function calculateTax(\n  amount: number,\n  customerAddress: {\n    line1: string;\n    city: string;\n    state: string;\n    postal_code: string;\n    country: string;\n  }\n) {\n  const calculation = await stripe.tax.calculations.create({\n    currency: 'usd',\n    line_items: [{\n      amount,\n      reference: 'product_001',\n    }],\n    customer_details: {\n      address: customerAddress,\n      address_source: 'shipping',\n    },\n  });\n\n  return {\n    amountTotal: calculation.amount_total,\n    taxAmount: calculation.tax_amount_exclusive,\n    taxBreakdown: calculation.tax_breakdown,\n  };\n}\n\n// Tax-inclusive pricing\nexport async function createTaxInclusivePrice() {\n  const price = await stripe.prices.create({\n    unit_amount: 1000,\n    currency: 'usd',\n    product: 'prod_xxx',\n    tax_behavior: 'inclusive', // Tax is included in price\n  });\n\n  return price;\n}\n```\n\n## Stripe Treasury (Banking-as-a-Service)\n\n### Financial Accounts\n\n```typescript\n// Create financial account for user\nexport async function createFinancialAccount(userId: string) {\n  const financialAccount = await stripe.treasury.financialAccounts.create({\n    supported_currencies: ['usd'],\n    features: {\n      card_issuing: { requested: true },\n      deposit_insurance: { requested: true },\n      financial_addresses: { aba: { requested: true } },\n      inbound_transfers: { ach: { requested: true } },\n      outbound_payments: {\n        ach: { requested: true },\n        us_domestic_wire: { requested: true },\n      },\n    },\n    metadata: { user_id: userId },\n  });\n\n  return financialAccount;\n}\n\n// Get account balance\nexport async function getAccountBalance(accountId: string) {\n  const account = await stripe.treasury.financialAccounts.retrieve(accountId);\n\n  return {\n    available: account.balance.cash.usd,\n    pending: account.balance.inbound_pending.usd,\n  };\n}\n\n// Create outbound payment\nexport async function sendPayment(\n  financialAccountId: string,\n  amount: number,\n  destinationRoutingNumber: string,\n  destinationAccountNumber: string\n) {\n  const outboundPayment = await stripe.treasury.outboundPayments.create({\n    financial_account: financialAccountId,\n    amount,\n    currency: 'usd',\n    destination_payment_method_data: {\n      type: 'us_bank_account',\n      us_bank_account: {\n        routing_number: destinationRoutingNumber,\n        account_number: destinationAccountNumber,\n        account_holder_type: 'individual',\n      },\n    },\n  });\n\n  return outboundPayment;\n}\n```\n\n## Stripe Climate\n\n### Carbon Removal Contributions\n\n```typescript\n// Add carbon removal to checkout\nexport async function createCheckoutWithClimate(priceId: string) {\n  const session = await stripe.checkout.sessions.create({\n    mode: 'payment',\n    line_items: [{ price: priceId, quantity: 1 }],\n    // Let customer choose carbon removal contribution\n    custom_fields: [{\n      key: 'climate_contribution',\n      label: { type: 'custom', custom: 'Contribute to carbon removal?' },\n      type: 'dropdown',\n      dropdown: {\n        options: [\n          { label: 'No contribution', value: '0' },\n          { label: '$1 - Remove 1kg CO2', value: '100' },\n          { label: '$5 - Remove 5kg CO2', value: '500' },\n          { label: '$10 - Remove 10kg CO2', value: '1000' },\n        ],\n      },\n    }],\n    success_url: `${process.env.NEXT_PUBLIC_URL}/success`,\n    cancel_url: `${process.env.NEXT_PUBLIC_URL}/cancel`,\n  });\n\n  return session;\n}\n\n// Process climate contribution\nexport async function createClimateOrder(amount: number, metadata: any) {\n  const order = await stripe.climate.orders.create({\n    amount, // Amount in cents to spend on carbon removal\n    metric_tons: amount / 1000, // ~$1 per kg\n    beneficiary: 'your_business',\n    metadata,\n  });\n\n  return order;\n}\n\n// List carbon removal suppliers\nexport async function listClimateSuppliers() {\n  const suppliers = await stripe.climate.suppliers.list({ limit: 10 });\n  return suppliers.data;\n}\n```\n\n## Metered Billing & Usage-Based Pricing\n\n### Setup Usage-Based Subscription\n\n```typescript\n// Create metered price\nexport async function createMeteredPrice(productId: string) {\n  const price = await stripe.prices.create({\n    product: productId,\n    currency: 'usd',\n    recurring: {\n      interval: 'month',\n      usage_type: 'metered', // Charge based on usage\n    },\n    billing_scheme: 'tiered', // or 'per_unit'\n    tiers_mode: 'graduated',\n    tiers: [\n      { up_to: 1000, unit_amount: 10 }, // $0.10 per unit for first 1000\n      { up_to: 5000, unit_amount: 8 },  // $0.08 per unit for 1001-5000\n      { up_to: 'inf', unit_amount: 5 }, // $0.05 per unit for 5001+\n    ],\n  });\n\n  return price;\n}\n\n// Report usage\nexport async function reportUsage(\n  subscriptionItemId: string,\n  quantity: number,\n  action: 'increment' | 'set' = 'increment'\n) {\n  const usageRecord = await stripe.subscriptionItems.createUsageRecord(\n    subscriptionItemId,\n    {\n      quantity,\n      timestamp: Math.floor(Date.now() / 1000),\n      action, // 'increment' adds to total, 'set' overwrites\n    },\n    { idempotencyKey: `usage_${subscriptionItemId}_${Date.now()}` }\n  );\n\n  return usageRecord;\n}\n\n// Get usage summary\nexport async function getUsageSummary(subscriptionItemId: string) {\n  const summary = await stripe.subscriptionItems.listUsageRecordSummaries(\n    subscriptionItemId,\n    { limit: 1 }\n  );\n\n  return {\n    totalUsage: summary.data[0]?.total_usage || 0,\n    period: {\n      start: summary.data[0]?.period?.start,\n      end: summary.data[0]?.period?.end,\n    },\n  };\n}\n```\n\n### Real-World Usage Tracking\n\n```typescript\n// Track API calls\nexport async function trackApiCall(userId: string, endpoint: string) {\n  const user = await getUser(userId);\n\n  if (!user.subscriptionItemId) {\n    throw new Error('No active subscription');\n  }\n\n  // Report usage to Stripe\n  await reportUsage(user.subscriptionItemId, 1, 'increment');\n\n  // Log locally for analytics\n  await logApiCall({ userId, endpoint, timestamp: new Date() });\n}\n\n// Middleware to track usage\nexport async function middleware(req: NextRequest) {\n  const session = await auth();\n\n  if (session?.user?.id) {\n    // Track this request as usage\n    await trackApiCall(session.user.id, req.nextUrl.pathname);\n  }\n\n  return NextResponse.next();\n}\n```\n\n## Billing Portal Customization\n\n### Create Custom Billing Portal\n\n```typescript\n// Configure portal\nexport async function configurePortal() {\n  const configuration = await stripe.billingPortal.configurations.create({\n    business_profile: {\n      headline: 'Manage your subscription',\n    },\n    features: {\n      customer_update: {\n        enabled: true,\n        allowed_updates: ['email', 'address', 'shipping', 'phone', 'tax_id'],\n      },\n      invoice_history: { enabled: true },\n      payment_method_update: { enabled: true },\n      subscription_cancel: {\n        enabled: true,\n        mode: 'at_period_end',\n        cancellation_reason: {\n          enabled: true,\n          options: [\n            'too_expensive',\n            'missing_features',\n            'switched_service',\n            'unused',\n            'customer_service',\n            'too_complex',\n            'low_quality',\n            'other',\n          ],\n        },\n      },\n      subscription_pause: {\n        enabled: true,\n      },\n      subscription_update: {\n        enabled: true,\n        default_allowed_updates: ['price', 'quantity', 'promotion_code'],\n        products: [\n          {\n            product: 'prod_basic',\n            prices: ['price_basic_monthly', 'price_basic_yearly'],\n          },\n          {\n            product: 'prod_pro',\n            prices: ['price_pro_monthly', 'price_pro_yearly'],\n          },\n        ],\n      },\n    },\n  });\n\n  return configuration.id;\n}\n\n// Create portal session with config\nexport async function createCustomPortalSession(\n  customerId: string,\n  configId: string\n) {\n  const session = await stripe.billingPortal.sessions.create({\n    customer: customerId,\n    return_url: `${process.env.NEXT_PUBLIC_URL}/dashboard`,\n    configuration: configId,\n  });\n\n  return session.url;\n}\n```\n\n## Testing & Development\n\n### Test Mode Helpers\n\n```typescript\n// lib/stripe/test-helpers.ts\nexport async function createTestCustomer(email: string) {\n  if (!process.env.STRIPE_SECRET_KEY?.startsWith('sk_test_')) {\n    throw new Error('Test helpers only work in test mode');\n  }\n\n  const customer = await stripe.customers.create({\n    email,\n    metadata: { test: 'true' },\n  });\n\n  return customer;\n}\n\nexport async function createTestSubscription(\n  customerId: string,\n  priceId: string\n) {\n  // Use test card that won't require payment\n  const paymentMethod = await stripe.paymentMethods.create({\n    type: 'card',\n    card: { token: 'tok_visa' }, // Test token\n  });\n\n  await stripe.paymentMethods.attach(paymentMethod.id, { customer: customerId });\n\n  const subscription = await stripe.subscriptions.create({\n    customer: customerId,\n    items: [{ price: priceId }],\n    default_payment_method: paymentMethod.id,\n  });\n\n  return subscription;\n}\n\n// Trigger test webhook locally\nexport async function triggerTestWebhook(eventType: string) {\n  // Uses Stripe CLI: stripe trigger <event>\n  const { exec } = require('child_process');\n  exec(`stripe trigger ${eventType}`);\n}\n```\n\n### Test Scenarios\n\n```typescript\n// Test card numbers for different scenarios\nexport const TEST_CARDS = {\n  // Successful payments\n  visa: '4242424242424242',\n  mastercard: '5555555555554444',\n  amex: '378282246310005',\n  discover: '6011111111111117',\n\n  // 3D Secure required\n  visa3DS: '4000002500003155',\n  visa3DSDecline: '4000008400001629',\n\n  // Declined\n  genericDecline: '4000000000009995',\n  insufficientFunds: '4000000000009987',\n  lostCard: '4000000000009987',\n  stolenCard: '4000000000009979',\n  expiredCard: '4000000000000069',\n  incorrectCVC: '4000000000000127',\n  processingError: '4000000000000119',\n\n  // Other scenarios\n  disputeWarning: '4000000000002685', // Triggers early fraud warning\n  alwaysDispute: '4000000000000259', // Will be disputed immediately\n};\n\n// Test with specific card\nexport async function testPaymentFlow(scenario: keyof typeof TEST_CARDS) {\n  const paymentMethod = await stripe.paymentMethods.create({\n    type: 'card',\n    card: { number: TEST_CARDS[scenario], exp_month: 12, exp_year: 2034, cvc: '123' },\n  });\n\n  const paymentIntent = await stripe.paymentIntents.create({\n    amount: 1000,\n    currency: 'usd',\n    payment_method: paymentMethod.id,\n    confirm: true,\n  });\n\n  return paymentIntent;\n}\n```\n\n## Performance Optimization\n\n### Expand Related Objects\n\n```typescript\n// Bad: Multiple API calls\nconst invoice = await stripe.invoices.retrieve('in_xxx');\nconst subscription = await stripe.subscriptions.retrieve(invoice.subscription);\nconst customer = await stripe.customers.retrieve(invoice.customer);\n\n// Good: Single API call with expand\nconst invoice = await stripe.invoices.retrieve('in_xxx', {\n  expand: ['subscription', 'customer', 'payment_intent'],\n});\n\n// Now access directly\nconsole.log(invoice.subscription.status);\nconsole.log(invoice.customer.email);\nconsole.log(invoice.payment_intent.client_secret);\n```\n\n### Batch Operations\n\n```typescript\n// Retrieve multiple customers efficiently\nexport async function batchRetrieveCustomers(customerIds: string[]) {\n  const customers = await Promise.all(\n    customerIds.map(id =>\n      stripe.customers.retrieve(id).catch(() => null) // Handle missing customers\n    )\n  );\n\n  return customers.filter(Boolean);\n}\n\n// Use list endpoints with pagination\nexport async function getAllActiveSubscriptions() {\n  const subscriptions: Stripe.Subscription[] = [];\n  let hasMore = true;\n  let startingAfter: string | undefined;\n\n  while (hasMore) {\n    const page = await stripe.subscriptions.list({\n      status: 'active',\n      limit: 100,\n      starting_after: startingAfter,\n    });\n\n    subscriptions.push(...page.data);\n    hasMore = page.has_more;\n    startingAfter = page.data[page.data.length - 1]?.id;\n  }\n\n  return subscriptions;\n}\n```\n\n### Caching Strategies\n\n```typescript\n// Cache prices (they rarely change)\nimport { unstable_cache } from 'next/cache';\n\nexport const getPrices = unstable_cache(\n  async () => {\n    const prices = await stripe.prices.list({\n      active: true,\n      expand: ['data.product'],\n    });\n    return prices.data;\n  },\n  ['stripe-prices'],\n  {\n    revalidate: 3600, // 1 hour\n    tags: ['prices'],\n  }\n);\n\n// Invalidate cache when price changes\nexport async function POST(req: Request) {\n  const event = await verifyWebhook(req);\n\n  if (event.type === 'price.updated' || event.type === 'price.created') {\n    revalidateTag('prices');\n  }\n\n  return Response.json({ received: true });\n}\n```\n\n## Documentation Quick Reference\n\n**Need to find something specific?**\n\nUse Grep to search the 3,253 documentation files:\n\n```bash\n# Search all docs\ngrep -r \"search term\" .claude/skills/api/stripe/docs/\n\n# Search API references only\ngrep -r \"search term\" .claude/skills/api/stripe/docs/api/\n\n# Find specific endpoint\nls .claude/skills/api/stripe/docs/api/payment_intents/\n```\n\n**Common doc locations:**\n- API Reference: `docs/api/`\n- Payment Intents: `docs/api/payment_intents/`\n- Subscriptions: `docs/api/subscriptions/`\n- Webhooks: `docs/api/webhook_endpoints/`\n- Connect: `docs/api/accounts/`\n\n## Resources\n\n- **Dashboard:** https://dashboard.stripe.com\n- **API Docs:** https://docs.stripe.com/api\n- **Testing:** https://docs.stripe.com/testing\n- **Stripe CLI:** https://docs.stripe.com/stripe-cli\n- **Status:** https://status.stripe.com\n- **Changelog:** https://docs.stripe.com/changelog\n\n## Implementation Checklist\n\n**Setup:**\n- [ ] Install SDK: `npm install stripe @stripe/stripe-js @stripe/react-stripe-js`\n- [ ] Get API keys from Dashboard (test + live)\n- [ ] Set environment variables\n- [ ] Configure TypeScript types\n\n**Payment Flow:**\n- [ ] Choose integration (Checkout vs Payment Element vs Payment Links)\n- [ ] Implement payment creation (server-side)\n- [ ] Add client-side payment UI\n- [ ] Handle payment confirmation\n- [ ] Add error handling with proper user messages\n- [ ] Add loading states\n- [ ] Test 3D Secure flow\n\n**Webhooks:**\n- [ ] Create webhook endpoint (`/api/webhooks`)\n- [ ] Verify webhook signatures (CRITICAL)\n- [ ] Handle key events (payment_intent.succeeded, customer.subscription.*)\n- [ ] Test with Stripe CLI\n- [ ] Deploy and configure webhook in Dashboard\n- [ ] Monitor webhook delivery\n\n**Production:**\n- [ ] Implement retry logic with exponential backoff\n- [ ] Add idempotency keys for critical operations\n- [ ] Set up rate limiting\n- [ ] Security audit (no exposed keys, webhook verification)\n- [ ] Test in production mode before launch\n- [ ] Set up monitoring and alerts\n- [ ] Document runbook for common issues\n\n**Advanced (if needed):**\n- [ ] Configure Connect for marketplaces\n- [ ] Set up Radar rules for fraud prevention\n- [ ] Enable automatic tax calculation\n- [ ] Configure billing portal\n- [ ] Set up usage-based billing\n- [ ] Implement dispute handling\n",
        "templates/.claude/skills/supabase/skill.md": "---\nname: supabase-expert\ndescription: Comprehensive Supabase expert with access to 2,616 official documentation files covering PostgreSQL database, authentication, real-time subscriptions, storage, edge functions, vector embeddings, and all platform features. Invoke when user mentions Supabase, PostgreSQL, database, auth, real-time, storage, edge functions, backend-as-a-service, or pgvector.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash, WebFetch\nmodel: sonnet\n---\n\n# Supabase Integration Expert\n\n## Purpose\n\nProvide comprehensive, accurate guidance for building applications with Supabase based on 2,616+ official documentation files. Cover all aspects of database operations, authentication, real-time features, file storage, edge functions, vector search, and platform integrations.\n\n## Documentation Coverage\n\n**Full access to official Supabase documentation (when available):**\n- **Location:** `docs/supabase_com/`\n- **Files:** 2,616 markdown files\n- **Coverage:** Complete guides, API references, client libraries, and platform docs\n\n**Note:** Documentation must be pulled separately:\n```bash\npipx install docpull\ndocpull https://supabase.com/docs -o .claude/skills/supabase/docs\n```\n\n**Major Areas:**\n- **Database:** PostgreSQL, Row Level Security (RLS), migrations, functions, triggers\n- **Authentication:** Email/password, OAuth, magic links, SSO, MFA, phone auth\n- **Real-time:** Database changes, broadcast, presence, channels\n- **Storage:** File uploads, image transformations, CDN, buckets\n- **Edge Functions:** Deno runtime, serverless, global deployment\n- **Vector/AI:** pgvector, embeddings, semantic search, RAG\n- **Client Libraries:** JavaScript, Python, Dart (Flutter), Swift, Kotlin\n- **Platform:** CLI, local development, branching, observability\n- **Integrations:** Next.js, React, Vue, Svelte, React Native, Expo\n\n## When to Use\n\nInvoke when user mentions:\n- **Database:** PostgreSQL, Postgres, SQL, database, tables, queries, migrations\n- **Auth:** authentication, login, signup, OAuth, SSO, multi-factor, magic links\n- **Real-time:** real-time, subscriptions, websocket, live data, presence, broadcast\n- **Storage:** file upload, file storage, images, S3, CDN, buckets\n- **Functions:** edge functions, serverless, API, Deno, cloud functions\n- **Security:** Row Level Security, RLS, policies, permissions, access control\n- **AI/ML:** vector search, embeddings, pgvector, semantic search, AI, RAG\n- **Framework Integration:** Next.js, React, Supabase client, hooks\n\n## How to Use Documentation\n\nWhen answering questions:\n\n1. **Search for specific topics:**\n   ```bash\n   # Use Grep to find relevant docs\n   grep -r \"row level security\" docs/supabase_com/ --include=\"*.md\"\n   ```\n\n2. **Find guides:**\n   ```bash\n   # Guides are organized by feature\n   ls docs/supabase_com/guides_*\n   ```\n\n3. **Check reference docs:**\n   ```bash\n   # Reference docs for client libraries\n   ls docs/supabase_com/reference_*\n   ```\n\n## Quick Start\n\n### Installation\n\n```bash\nnpm install @supabase/supabase-js\n```\n\n### Initialize Client\n\n```typescript\nimport { createClient } from '@supabase/supabase-js';\n\nconst supabase = createClient(\n  process.env.NEXT_PUBLIC_SUPABASE_URL!,\n  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!\n);\n```\n\n**Environment Variables:**\n- `NEXT_PUBLIC_SUPABASE_URL` - Your project URL (safe for client)\n- `NEXT_PUBLIC_SUPABASE_ANON_KEY` - Anonymous/public key (safe for client)\n- `SUPABASE_SERVICE_ROLE_KEY` - Admin key (server-side only, bypasses RLS)\n\n## Database Operations\n\n### CRUD Operations\n\n```typescript\n// Insert\nconst { data, error } = await supabase\n  .from('posts')\n  .insert({\n    title: 'Hello World',\n    content: 'My first post',\n    user_id: user.id,\n  })\n  .select()\n  .single();\n\n// Read (with filters)\nconst { data: posts } = await supabase\n  .from('posts')\n  .select('*')\n  .eq('published', true)\n  .order('created_at', { ascending: false })\n  .limit(10);\n\n// Update\nconst { data, error } = await supabase\n  .from('posts')\n  .update({ published: true })\n  .eq('id', postId)\n  .select()\n  .single();\n\n// Delete\nconst { error } = await supabase\n  .from('posts')\n  .delete()\n  .eq('id', postId);\n\n// Upsert (insert or update)\nconst { data, error } = await supabase\n  .from('profiles')\n  .upsert({\n    id: user.id,\n    name: 'John Doe',\n    updated_at: new Date().toISOString(),\n  })\n  .select();\n```\n\n### Advanced Queries\n\n```typescript\n// Joins\nconst { data } = await supabase\n  .from('posts')\n  .select(`\n    *,\n    author:profiles(name, avatar),\n    comments(count)\n  `)\n  .eq('published', true);\n\n// Full-text search\nconst { data } = await supabase\n  .from('posts')\n  .select('*')\n  .textSearch('title', `'nextjs' & 'supabase'`);\n\n// Range queries\nconst { data } = await supabase\n  .from('posts')\n  .select('*')\n  .gte('created_at', '2024-01-01')\n  .lt('created_at', '2024-12-31');\n\n// JSON queries\nconst { data } = await supabase\n  .from('posts')\n  .select('*')\n  .contains('metadata', { tags: ['tutorial'] });\n```\n\n### Database Functions\n\n```typescript\n// Call stored procedure\nconst { data, error } = await supabase\n  .rpc('get_user_stats', {\n    user_id: userId,\n  });\n\n// Call with filters\nconst { data } = await supabase\n  .rpc('search_posts', { search_term: 'supabase' })\n  .limit(10);\n```\n\n## Authentication\n\n### Sign Up / Sign In\n\n```typescript\n// Email/password signup\nconst { data, error } = await supabase.auth.signUp({\n  email: 'user@example.com',\n  password: 'secure-password',\n  options: {\n    data: {\n      first_name: 'John',\n      last_name: 'Doe',\n    },\n  },\n});\n\n// Email/password sign in\nconst { data, error } = await supabase.auth.signInWithPassword({\n  email: 'user@example.com',\n  password: 'secure-password',\n});\n\n// Magic link (passwordless)\nconst { data, error } = await supabase.auth.signInWithOtp({\n  email: 'user@example.com',\n  options: {\n    emailRedirectTo: 'https://example.com/auth/callback',\n  },\n});\n\n// Phone/SMS\nconst { data, error } = await supabase.auth.signInWithOtp({\n  phone: '+1234567890',\n});\n```\n\n### OAuth Providers\n\n```typescript\n// Google sign in\nconst { data, error } = await supabase.auth.signInWithOAuth({\n  provider: 'google',\n  options: {\n    redirectTo: 'http://localhost:3000/auth/callback',\n    scopes: 'profile email',\n  },\n});\n```\n\n**Supported providers:**\n- Google, GitHub, GitLab, Bitbucket\n- Azure, Apple, Discord, Facebook\n- Slack, Spotify, Twitch, Twitter/X\n- Linear, Notion, Figma, and more\n\n### User Session Management\n\n```typescript\n// Get current user\nconst { data: { user } } = await supabase.auth.getUser();\n\n// Get session\nconst { data: { session } } = await supabase.auth.getSession();\n\n// Sign out\nconst { error } = await supabase.auth.signOut();\n\n// Listen to auth changes\nsupabase.auth.onAuthStateChange((event, session) => {\n  if (event === 'SIGNED_IN') {\n    console.log('User signed in:', session.user);\n  }\n  if (event === 'SIGNED_OUT') {\n    console.log('User signed out');\n  }\n  if (event === 'TOKEN_REFRESHED') {\n    console.log('Token refreshed');\n  }\n});\n```\n\n### Multi-Factor Authentication (MFA)\n\n```typescript\n// Enroll MFA\nconst { data, error } = await supabase.auth.mfa.enroll({\n  factorType: 'totp',\n  friendlyName: 'My Authenticator App',\n});\n\n// Verify MFA\nconst { data, error } = await supabase.auth.mfa.challengeAndVerify({\n  factorId: data.id,\n  code: '123456',\n});\n\n// List factors\nconst { data: factors } = await supabase.auth.mfa.listFactors();\n```\n\n## Row Level Security (RLS)\n\n### Enable RLS\n\n```sql\n-- Enable RLS on table\nALTER TABLE posts ENABLE ROW LEVEL SECURITY;\n```\n\n### Create Policies\n\n```sql\n-- Public read access\nCREATE POLICY \"Posts are viewable by everyone\"\n  ON posts FOR SELECT\n  USING (true);\n\n-- Users can insert their own posts\nCREATE POLICY \"Users can create posts\"\n  ON posts FOR INSERT\n  WITH CHECK (auth.uid() = user_id);\n\n-- Users can update only their posts\nCREATE POLICY \"Users can update own posts\"\n  ON posts FOR UPDATE\n  USING (auth.uid() = user_id);\n\n-- Users can delete only their posts\nCREATE POLICY \"Users can delete own posts\"\n  ON posts FOR DELETE\n  USING (auth.uid() = user_id);\n\n-- Conditional access (e.g., premium users)\nCREATE POLICY \"Premium content for premium users\"\n  ON posts FOR SELECT\n  USING (\n    NOT premium OR\n    (auth.uid() IN (\n      SELECT user_id FROM subscriptions\n      WHERE status = 'active'\n    ))\n  );\n```\n\n### Helper Functions\n\n```sql\n-- Get current user ID\nauth.uid()\n\n-- Get current JWT\nauth.jwt()\n\n-- Access JWT claims\n(auth.jwt()->>'role')::text\n(auth.jwt()->>'email')::text\n```\n\n## Real-time Subscriptions\n\n### Listen to Database Changes\n\n```typescript\nconst channel = supabase\n  .channel('posts-changes')\n  .on(\n    'postgres_changes',\n    {\n      event: '*', // or 'INSERT', 'UPDATE', 'DELETE'\n      schema: 'public',\n      table: 'posts',\n    },\n    (payload) => {\n      console.log('Change received:', payload);\n    }\n  )\n  .subscribe();\n\n// Cleanup\nchannel.unsubscribe();\n```\n\n### Filter Real-time Events\n\n```typescript\n// Only listen to specific user's posts\nconst channel = supabase\n  .channel('my-posts')\n  .on(\n    'postgres_changes',\n    {\n      event: 'INSERT',\n      schema: 'public',\n      table: 'posts',\n      filter: `user_id=eq.${userId}`,\n    },\n    (payload) => {\n      console.log('New post:', payload.new);\n    }\n  )\n  .subscribe();\n```\n\n### Broadcast (Ephemeral Messages)\n\n```typescript\nconst channel = supabase.channel('chat-room');\n\n// Send message\nawait channel.send({\n  type: 'broadcast',\n  event: 'message',\n  payload: { text: 'Hello!', user: 'John' },\n});\n\n// Receive messages\nchannel.on('broadcast', { event: 'message' }, (payload) => {\n  console.log('Message:', payload.payload);\n});\n\nawait channel.subscribe();\n```\n\n### Presence Tracking\n\n```typescript\nconst channel = supabase.channel('room-1');\n\n// Track presence\nchannel\n  .on('presence', { event: 'sync' }, () => {\n    const state = channel.presenceState();\n    console.log('Online users:', Object.keys(state).length);\n  })\n  .on('presence', { event: 'join' }, ({ key, newPresences }) => {\n    console.log('User joined:', newPresences);\n  })\n  .on('presence', { event: 'leave' }, ({ key, leftPresences }) => {\n    console.log('User left:', leftPresences);\n  })\n  .subscribe(async (status) => {\n    if (status === 'SUBSCRIBED') {\n      await channel.track({\n        user_id: userId,\n        online_at: new Date().toISOString(),\n      });\n    }\n  });\n```\n\n## Storage\n\n### Upload Files\n\n```typescript\nconst file = event.target.files[0];\n\nconst { data, error } = await supabase.storage\n  .from('avatars')\n  .upload(`public/${userId}/avatar.png`, file, {\n    cacheControl: '3600',\n    upsert: true,\n  });\n\n// Upload from base64\nconst { data, error } = await supabase.storage\n  .from('avatars')\n  .upload('file.png', decode(base64String), {\n    contentType: 'image/png',\n  });\n```\n\n### Download Files\n\n```typescript\n// Download as blob\nconst { data, error } = await supabase.storage\n  .from('avatars')\n  .download('public/avatar.png');\n\nconst url = URL.createObjectURL(data);\n```\n\n### Public URLs\n\n```typescript\n// Get public URL (for public buckets)\nconst { data } = supabase.storage\n  .from('avatars')\n  .getPublicUrl('public/avatar.png');\n\nconsole.log(data.publicUrl);\n```\n\n### Signed URLs (Private Files)\n\n```typescript\n// Create temporary access URL\nconst { data, error } = await supabase.storage\n  .from('private-files')\n  .createSignedUrl('document.pdf', 3600); // 1 hour\n\nconsole.log(data.signedUrl);\n```\n\n### Image Transformations\n\n```typescript\nconst { data } = supabase.storage\n  .from('avatars')\n  .getPublicUrl('avatar.png', {\n    transform: {\n      width: 400,\n      height: 400,\n      resize: 'cover', // 'contain', 'cover', 'fill'\n      quality: 80,\n    },\n  });\n```\n\n### List Files\n\n```typescript\nconst { data, error } = await supabase.storage\n  .from('avatars')\n  .list('public', {\n    limit: 100,\n    offset: 0,\n    sortBy: { column: 'created_at', order: 'desc' },\n  });\n```\n\n## Edge Functions\n\n### Create Function\n\n```bash\n# Install Supabase CLI\nnpm install -g supabase\n\n# Initialize project\nsupabase init\n\n# Create function\nsupabase functions new my-function\n```\n\n### Function Example\n\n```typescript\n// supabase/functions/my-function/index.ts\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts';\nimport { createClient } from 'https://esm.sh/@supabase/supabase-js@2';\n\nserve(async (req) => {\n  try {\n    // Initialize Supabase client\n    const supabase = createClient(\n      Deno.env.get('SUPABASE_URL')!,\n      Deno.env.get('SUPABASE_ANON_KEY')!,\n      {\n        global: {\n          headers: { Authorization: req.headers.get('Authorization')! },\n        },\n      }\n    );\n\n    // Get authenticated user\n    const { data: { user }, error: authError } = await supabase.auth.getUser();\n\n    if (authError || !user) {\n      return new Response(JSON.stringify({ error: 'Unauthorized' }), {\n        status: 401,\n        headers: { 'Content-Type': 'application/json' },\n      });\n    }\n\n    // Query database\n    const { data: posts, error } = await supabase\n      .from('posts')\n      .select('*')\n      .eq('user_id', user.id);\n\n    if (error) throw error;\n\n    return new Response(JSON.stringify({ posts }), {\n      headers: { 'Content-Type': 'application/json' },\n    });\n  } catch (error) {\n    return new Response(JSON.stringify({ error: error.message }), {\n      status: 500,\n      headers: { 'Content-Type': 'application/json' },\n    });\n  }\n});\n```\n\n### Deploy Function\n\n```bash\n# Deploy single function\nsupabase functions deploy my-function\n\n# Deploy all functions\nsupabase functions deploy\n```\n\n### Invoke Function\n\n```typescript\nconst { data, error } = await supabase.functions.invoke('my-function', {\n  body: { name: 'World' },\n});\n\nconsole.log(data);\n```\n\n## Vector Search (AI/ML)\n\n### Enable pgvector\n\n```sql\n-- Enable extension\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Create table with vector column\nCREATE TABLE documents (\n  id BIGSERIAL PRIMARY KEY,\n  content TEXT,\n  embedding VECTOR(1536) -- OpenAI ada-002 dimensions\n);\n\n-- Create HNSW index for fast similarity search\nCREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops);\n```\n\n### Store Embeddings\n\n```typescript\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\n// Generate embedding\nconst response = await openai.embeddings.create({\n  model: 'text-embedding-ada-002',\n  input: 'Supabase is awesome',\n});\n\nconst embedding = response.data[0].embedding;\n\n// Store in database\nconst { data, error } = await supabase\n  .from('documents')\n  .insert({\n    content: 'Supabase is awesome',\n    embedding,\n  });\n```\n\n### Similarity Search\n\n```typescript\n// Find similar documents\nconst { data, error } = await supabase.rpc('match_documents', {\n  query_embedding: embedding,\n  match_threshold: 0.78,\n  match_count: 10,\n});\n```\n\n**Similarity search function:**\n```sql\nCREATE FUNCTION match_documents (\n  query_embedding VECTOR(1536),\n  match_threshold FLOAT,\n  match_count INT\n)\nRETURNS TABLE (\n  id BIGINT,\n  content TEXT,\n  similarity FLOAT\n)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  RETURN QUERY\n  SELECT\n    documents.id,\n    documents.content,\n    1 - (documents.embedding <=> query_embedding) AS similarity\n  FROM documents\n  WHERE 1 - (documents.embedding <=> query_embedding) > match_threshold\n  ORDER BY documents.embedding <=> query_embedding\n  LIMIT match_count;\nEND;\n$$;\n```\n\n## Next.js Integration\n\n### Server Components\n\n```typescript\n// app/posts/page.tsx\nimport { createServerComponentClient } from '@supabase/auth-helpers-nextjs';\nimport { cookies } from 'next/headers';\n\nexport default async function PostsPage() {\n  const supabase = createServerComponentClient({ cookies });\n\n  const { data: posts } = await supabase\n    .from('posts')\n    .select('*')\n    .order('created_at', { ascending: false });\n\n  return (\n    <div>\n      {posts?.map((post) => (\n        <div key={post.id}>{post.title}</div>\n      ))}\n    </div>\n  );\n}\n```\n\n### Client Components\n\n```typescript\n// app/new-post/page.tsx\n'use client';\n\nimport { createClientComponentClient } from '@supabase/auth-helpers-nextjs';\nimport { useState } from 'react';\n\nexport default function NewPostPage() {\n  const supabase = createClientComponentClient();\n  const [title, setTitle] = useState('');\n\n  const handleSubmit = async (e: FormEvent) => {\n    e.preventDefault();\n\n    const { error } = await supabase\n      .from('posts')\n      .insert({ title });\n\n    if (error) console.error(error);\n  };\n\n  return <form onSubmit={handleSubmit}>...</form>;\n}\n```\n\n### Middleware (Auth Protection)\n\n```typescript\n// middleware.ts\nimport { createMiddlewareClient } from '@supabase/auth-helpers-nextjs';\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\nexport async function middleware(req: NextRequest) {\n  const res = NextResponse.next();\n  const supabase = createMiddlewareClient({ req, res });\n\n  const { data: { session } } = await supabase.auth.getSession();\n\n  // Redirect to login if not authenticated\n  if (!session && req.nextUrl.pathname.startsWith('/dashboard')) {\n    return NextResponse.redirect(new URL('/login', req.url));\n  }\n\n  return res;\n}\n\nexport const config = {\n  matcher: ['/dashboard/:path*'],\n};\n```\n\n### Route Handlers\n\n```typescript\n// app/api/posts/route.ts\nimport { createRouteHandlerClient } from '@supabase/auth-helpers-nextjs';\nimport { cookies } from 'next/headers';\nimport { NextResponse } from 'next/server';\n\nexport async function GET() {\n  const supabase = createRouteHandlerClient({ cookies });\n\n  const { data: posts } = await supabase\n    .from('posts')\n    .select('*');\n\n  return NextResponse.json({ posts });\n}\n\nexport async function POST(request: Request) {\n  const supabase = createRouteHandlerClient({ cookies });\n  const body = await request.json();\n\n  const { data, error } = await supabase\n    .from('posts')\n    .insert(body)\n    .select()\n    .single();\n\n  if (error) {\n    return NextResponse.json({ error: error.message }, { status: 400 });\n  }\n\n  return NextResponse.json({ post: data });\n}\n```\n\n## Database Migrations\n\n### Create Migration\n\n```bash\nsupabase migration new create_posts_table\n```\n\n### Migration File Example\n\n```sql\n-- supabase/migrations/20241116000000_create_posts_table.sql\n\n-- Create table\nCREATE TABLE posts (\n  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,\n  user_id UUID REFERENCES auth.users NOT NULL,\n  title TEXT NOT NULL,\n  content TEXT,\n  published BOOLEAN DEFAULT false,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL,\n  updated_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL\n);\n\n-- Enable RLS\nALTER TABLE posts ENABLE ROW LEVEL SECURITY;\n\n-- Create policies\nCREATE POLICY \"Public posts are viewable by everyone\"\n  ON posts FOR SELECT\n  USING (published = true);\n\nCREATE POLICY \"Users can view their own posts\"\n  ON posts FOR SELECT\n  USING (auth.uid() = user_id);\n\nCREATE POLICY \"Users can create posts\"\n  ON posts FOR INSERT\n  WITH CHECK (auth.uid() = user_id);\n\nCREATE POLICY \"Users can update own posts\"\n  ON posts FOR UPDATE\n  USING (auth.uid() = user_id);\n\n-- Create indexes\nCREATE INDEX posts_user_id_idx ON posts(user_id);\nCREATE INDEX posts_created_at_idx ON posts(created_at DESC);\nCREATE INDEX posts_published_idx ON posts(published) WHERE published = true;\n\n-- Create updated_at trigger\nCREATE OR REPLACE FUNCTION handle_updated_at()\nRETURNS TRIGGER AS $$\nBEGIN\n  NEW.updated_at = NOW();\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER set_updated_at\n  BEFORE UPDATE ON posts\n  FOR EACH ROW\n  EXECUTE FUNCTION handle_updated_at();\n```\n\n### Run Migrations\n\n```bash\n# Apply migrations locally\nsupabase db reset\n\n# Push to remote (production)\nsupabase db push\n```\n\n## TypeScript Integration\n\n### Database Type Generation\n\n```bash\n# Generate types from your database schema\nsupabase gen types typescript --project-id YOUR_PROJECT_ID > types/supabase.ts\n\n# Or from local development\nsupabase gen types typescript --local > types/supabase.ts\n```\n\n### Type-Safe Client\n\n```typescript\n// lib/supabase/types.ts\nexport type Json =\n  | string\n  | number\n  | boolean\n  | null\n  | { [key: string]: Json | undefined }\n  | Json[]\n\nexport interface Database {\n  public: {\n    Tables: {\n      posts: {\n        Row: {\n          id: string\n          created_at: string\n          title: string\n          content: string | null\n          user_id: string\n          published: boolean\n        }\n        Insert: {\n          id?: string\n          created_at?: string\n          title: string\n          content?: string | null\n          user_id: string\n          published?: boolean\n        }\n        Update: {\n          id?: string\n          created_at?: string\n          title?: string\n          content?: string | null\n          user_id?: string\n          published?: boolean\n        }\n      }\n      profiles: {\n        Row: {\n          id: string\n          name: string | null\n          avatar_url: string | null\n          created_at: string\n        }\n        Insert: {\n          id: string\n          name?: string | null\n          avatar_url?: string | null\n          created_at?: string\n        }\n        Update: {\n          id?: string\n          name?: string | null\n          avatar_url?: string | null\n          created_at?: string\n        }\n      }\n    }\n    Views: {\n      [_ in never]: never\n    }\n    Functions: {\n      [_ in never]: never\n    }\n    Enums: {\n      [_ in never]: never\n    }\n  }\n}\n\n// lib/supabase/client.ts\nimport { createClient } from '@supabase/supabase-js'\nimport { Database } from './types'\n\nexport const supabase = createClient<Database>(\n  process.env.NEXT_PUBLIC_SUPABASE_URL!,\n  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!\n)\n\n// Now you get full type safety!\nconst { data } = await supabase\n  .from('posts')  // ✅ TypeScript knows this table exists\n  .select('title, content, profiles(name)')  // ✅ TypeScript validates columns\n  .eq('published', true)  // ✅ TypeScript validates types\n\n// data is typed as:\n// Array<{ title: string; content: string | null; profiles: { name: string | null } }>\n```\n\n### Server vs Client Supabase\n\n```typescript\n// lib/supabase/client.ts - Client-side (respects RLS)\nimport { createBrowserClient } from '@supabase/ssr'\nimport { Database } from './types'\n\nexport function createClient() {\n  return createBrowserClient<Database>(\n    process.env.NEXT_PUBLIC_SUPABASE_URL!,\n    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!\n  )\n}\n\n// lib/supabase/server.ts - Server-side (Next.js App Router)\nimport { createServerClient, type CookieOptions } from '@supabase/ssr'\nimport { cookies } from 'next/headers'\nimport { Database } from './types'\n\nexport function createClient() {\n  const cookieStore = cookies()\n\n  return createServerClient<Database>(\n    process.env.NEXT_PUBLIC_SUPABASE_URL!,\n    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,\n    {\n      cookies: {\n        get(name: string) {\n          return cookieStore.get(name)?.value\n        },\n        set(name: string, value: string, options: CookieOptions) {\n          try {\n            cookieStore.set({ name, value, ...options })\n          } catch (error) {\n            // Called from Server Component - ignore\n          }\n        },\n        remove(name: string, options: CookieOptions) {\n          try {\n            cookieStore.set({ name, value: '', ...options })\n          } catch (error) {\n            // Called from Server Component - ignore\n          }\n        },\n      },\n    }\n  )\n}\n\n// lib/supabase/admin.ts - Admin client (bypasses RLS)\nimport { createClient } from '@supabase/supabase-js'\nimport { Database } from './types'\n\nexport const supabaseAdmin = createClient<Database>(\n  process.env.NEXT_PUBLIC_SUPABASE_URL!,\n  process.env.SUPABASE_SERVICE_ROLE_KEY!,  // ⚠️ Server-side only!\n  {\n    auth: {\n      autoRefreshToken: false,\n      persistSession: false\n    }\n  }\n)\n```\n\n## Next.js App Router Patterns\n\n### Server Components (Recommended)\n\n```typescript\n// app/posts/page.tsx\nimport { createClient } from '@/lib/supabase/server'\n\nexport default async function PostsPage() {\n  const supabase = createClient()\n\n  // Fetch data on server (no loading state needed!)\n  const { data: posts } = await supabase\n    .from('posts')\n    .select('*, profiles(*)')\n    .eq('published', true)\n    .order('created_at', { ascending: false })\n\n  return (\n    <div>\n      {posts?.map(post => (\n        <article key={post.id}>\n          <h2>{post.title}</h2>\n          <p>By {post.profiles?.name}</p>\n          <div>{post.content}</div>\n        </article>\n      ))}\n    </div>\n  )\n}\n```\n\n### Server Actions for Mutations\n\n```typescript\n// app/actions/posts.ts\n'use server'\n\nimport { createClient } from '@/lib/supabase/server'\nimport { revalidatePath } from 'next/cache'\nimport { redirect } from 'next/navigation'\n\nexport async function createPost(formData: FormData) {\n  const supabase = createClient()\n\n  const { data: { user } } = await supabase.auth.getUser()\n  if (!user) {\n    redirect('/login')\n  }\n\n  const title = formData.get('title') as string\n  const content = formData.get('content') as string\n\n  const { error } = await supabase\n    .from('posts')\n    .insert({\n      title,\n      content,\n      user_id: user.id,\n    })\n\n  if (error) {\n    throw new Error(error.message)\n  }\n\n  revalidatePath('/posts')\n  redirect('/posts')\n}\n\nexport async function updatePost(id: string, formData: FormData) {\n  const supabase = createClient()\n\n  const { data: { user } } = await supabase.auth.getUser()\n  if (!user) throw new Error('Unauthorized')\n\n  const { error } = await supabase\n    .from('posts')\n    .update({\n      title: formData.get('title') as string,\n      content: formData.get('content') as string,\n    })\n    .eq('id', id)\n    .eq('user_id', user.id)  // Ensure user owns the post\n\n  if (error) throw new Error(error.message)\n\n  revalidatePath('/posts')\n}\n\nexport async function deletePost(id: string) {\n  const supabase = createClient()\n\n  const { data: { user } } = await supabase.auth.getUser()\n  if (!user) throw new Error('Unauthorized')\n\n  const { error } = await supabase\n    .from('posts')\n    .delete()\n    .eq('id', id)\n    .eq('user_id', user.id)\n\n  if (error) throw new Error(error.message)\n\n  revalidatePath('/posts')\n}\n```\n\n### Client Component with Real-time\n\n```typescript\n// app/components/PostsList.tsx\n'use client'\n\nimport { useEffect, useState } from 'react'\nimport { createClient } from '@/lib/supabase/client'\nimport { Database } from '@/lib/supabase/types'\n\ntype Post = Database['public']['Tables']['posts']['Row']\n\nexport function PostsList({ initialPosts }: { initialPosts: Post[] }) {\n  const [posts, setPosts] = useState(initialPosts)\n  const supabase = createClient()\n\n  useEffect(() => {\n    const channel = supabase\n      .channel('posts-changes')\n      .on(\n        'postgres_changes',\n        {\n          event: '*',\n          schema: 'public',\n          table: 'posts',\n          filter: 'published=eq.true',\n        },\n        (payload) => {\n          if (payload.eventType === 'INSERT') {\n            setPosts(prev => [payload.new as Post, ...prev])\n          } else if (payload.eventType === 'UPDATE') {\n            setPosts(prev =>\n              prev.map(post =>\n                post.id === payload.new.id ? (payload.new as Post) : post\n              )\n            )\n          } else if (payload.eventType === 'DELETE') {\n            setPosts(prev => prev.filter(post => post.id !== payload.old.id))\n          }\n        }\n      )\n      .subscribe()\n\n    return () => {\n      supabase.removeChannel(channel)\n    }\n  }, [supabase])\n\n  return (\n    <div>\n      {posts.map(post => (\n        <article key={post.id}>\n          <h2>{post.title}</h2>\n          <p>{post.content}</p>\n        </article>\n      ))}\n    </div>\n  )\n}\n```\n\n### Route Handlers\n\n```typescript\n// app/api/posts/route.ts\nimport { createClient } from '@/lib/supabase/server'\nimport { NextRequest, NextResponse } from 'next/server'\n\nexport async function GET(request: NextRequest) {\n  const supabase = createClient()\n\n  const { searchParams } = new URL(request.url)\n  const limit = parseInt(searchParams.get('limit') || '10')\n\n  const { data, error } = await supabase\n    .from('posts')\n    .select('*')\n    .eq('published', true)\n    .order('created_at', { ascending: false })\n    .limit(limit)\n\n  if (error) {\n    return NextResponse.json({ error: error.message }, { status: 500 })\n  }\n\n  return NextResponse.json(data)\n}\n\nexport async function POST(request: NextRequest) {\n  const supabase = createClient()\n\n  const { data: { user } } = await supabase.auth.getUser()\n  if (!user) {\n    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })\n  }\n\n  const body = await request.json()\n\n  const { data, error } = await supabase\n    .from('posts')\n    .insert({\n      ...body,\n      user_id: user.id,\n    })\n    .select()\n    .single()\n\n  if (error) {\n    return NextResponse.json({ error: error.message }, { status: 500 })\n  }\n\n  return NextResponse.json(data)\n}\n```\n\n## Advanced Authentication\n\n### Email/Password with Email Confirmation\n\n```typescript\n// app/actions/auth.ts\n'use server'\n\nimport { createClient } from '@/lib/supabase/server'\nimport { redirect } from 'next/navigation'\n\nexport async function signUp(formData: FormData) {\n  const supabase = createClient()\n\n  const email = formData.get('email') as string\n  const password = formData.get('password') as string\n  const name = formData.get('name') as string\n\n  const { error } = await supabase.auth.signUp({\n    email,\n    password,\n    options: {\n      data: {\n        name,  // Stored in auth.users.raw_user_meta_data\n      },\n      emailRedirectTo: `${process.env.NEXT_PUBLIC_URL}/auth/callback`,\n    },\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  return { success: true, message: 'Check your email to confirm your account' }\n}\n\nexport async function signIn(formData: FormData) {\n  const supabase = createClient()\n\n  const email = formData.get('email') as string\n  const password = formData.get('password') as string\n\n  const { error } = await supabase.auth.signInWithPassword({\n    email,\n    password,\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  redirect('/dashboard')\n}\n\nexport async function signOut() {\n  const supabase = createClient()\n  await supabase.auth.signOut()\n  redirect('/')\n}\n```\n\n### OAuth (Google, GitHub, etc.)\n\n```typescript\n// app/actions/auth.ts\nexport async function signInWithGoogle() {\n  const supabase = createClient()\n\n  const { data, error } = await supabase.auth.signInWithOAuth({\n    provider: 'google',\n    options: {\n      redirectTo: `${process.env.NEXT_PUBLIC_URL}/auth/callback`,\n      queryParams: {\n        access_type: 'offline',\n        prompt: 'consent',\n      },\n    },\n  })\n\n  if (data?.url) {\n    redirect(data.url)\n  }\n}\n\nexport async function signInWithGitHub() {\n  const supabase = createClient()\n\n  const { data } = await supabase.auth.signInWithOAuth({\n    provider: 'github',\n    options: {\n      redirectTo: `${process.env.NEXT_PUBLIC_URL}/auth/callback`,\n      scopes: 'read:user user:email',\n    },\n  })\n\n  if (data?.url) {\n    redirect(data.url)\n  }\n}\n```\n\n### Magic Links\n\n```typescript\nexport async function sendMagicLink(email: string) {\n  const supabase = createClient()\n\n  const { error } = await supabase.auth.signInWithOtp({\n    email,\n    options: {\n      emailRedirectTo: `${process.env.NEXT_PUBLIC_URL}/auth/callback`,\n    },\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  return { success: true, message: 'Check your email for the login link' }\n}\n```\n\n### Phone Auth (SMS)\n\n```typescript\nexport async function sendPhoneOTP(phone: string) {\n  const supabase = createClient()\n\n  const { error } = await supabase.auth.signInWithOtp({\n    phone,\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  return { success: true }\n}\n\nexport async function verifyPhoneOTP(phone: string, token: string) {\n  const supabase = createClient()\n\n  const { error } = await supabase.auth.verifyOtp({\n    phone,\n    token,\n    type: 'sms',\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  redirect('/dashboard')\n}\n```\n\n### Multi-Factor Authentication (MFA)\n\n```typescript\n// Enable MFA for user\nexport async function enableMFA() {\n  const supabase = createClient()\n\n  const { data, error } = await supabase.auth.mfa.enroll({\n    factorType: 'totp',\n    friendlyName: 'Authenticator App',\n  })\n\n  if (error) throw error\n\n  // data.totp.qr_code - QR code to scan\n  // data.totp.secret - Secret to enter manually\n  return data\n}\n\n// Verify MFA\nexport async function verifyMFA(factorId: string, code: string) {\n  const supabase = createClient()\n\n  const { data, error } = await supabase.auth.mfa.challengeAndVerify({\n    factorId,\n    code,\n  })\n\n  if (error) throw error\n  return data\n}\n```\n\n### Auth Callback Handler\n\n```typescript\n// app/auth/callback/route.ts\nimport { createClient } from '@/lib/supabase/server'\nimport { NextRequest, NextResponse } from 'next/server'\n\nexport async function GET(request: NextRequest) {\n  const requestUrl = new URL(request.url)\n  const code = requestUrl.searchParams.get('code')\n\n  if (code) {\n    const supabase = createClient()\n    await supabase.auth.exchangeCodeForSession(code)\n  }\n\n  // Redirect to dashboard or wherever\n  return NextResponse.redirect(new URL('/dashboard', request.url))\n}\n```\n\n### Protected Routes\n\n```typescript\n// middleware.ts\nimport { createServerClient, type CookieOptions } from '@supabase/ssr'\nimport { NextResponse, type NextRequest } from 'next/server'\n\nexport async function middleware(request: NextRequest) {\n  let response = NextResponse.next({\n    request: {\n      headers: request.headers,\n    },\n  })\n\n  const supabase = createServerClient(\n    process.env.NEXT_PUBLIC_SUPABASE_URL!,\n    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,\n    {\n      cookies: {\n        get(name: string) {\n          return request.cookies.get(name)?.value\n        },\n        set(name: string, value: string, options: CookieOptions) {\n          request.cookies.set({\n            name,\n            value,\n            ...options,\n          })\n          response = NextResponse.next({\n            request: {\n              headers: request.headers,\n            },\n          })\n          response.cookies.set({\n            name,\n            value,\n            ...options,\n          })\n        },\n        remove(name: string, options: CookieOptions) {\n          request.cookies.set({\n            name,\n            value: '',\n            ...options,\n          })\n          response = NextResponse.next({\n            request: {\n              headers: request.headers,\n            },\n          })\n          response.cookies.set({\n            name,\n            value: '',\n            ...options,\n          })\n        },\n      },\n    }\n  )\n\n  const { data: { user } } = await supabase.auth.getUser()\n\n  // Protect dashboard routes\n  if (request.nextUrl.pathname.startsWith('/dashboard') && !user) {\n    return NextResponse.redirect(new URL('/login', request.url))\n  }\n\n  // Redirect authenticated users away from auth pages\n  if (request.nextUrl.pathname.startsWith('/login') && user) {\n    return NextResponse.redirect(new URL('/dashboard', request.url))\n  }\n\n  return response\n}\n\nexport const config = {\n  matcher: ['/dashboard/:path*', '/login', '/signup'],\n}\n```\n\n## Advanced Row Level Security\n\n### Complex RLS Policies\n\n```sql\n-- Users can only see published posts or their own drafts\nCREATE POLICY \"Users can read appropriate posts\"\n  ON posts FOR SELECT\n  USING (\n    published = true\n    OR\n    auth.uid() = user_id\n  );\n\n-- Users can update only their own posts\nCREATE POLICY \"Users can update own posts\"\n  ON posts FOR UPDATE\n  USING (auth.uid() = user_id)\n  WITH CHECK (auth.uid() = user_id);\n\n-- Team-based access\nCREATE TABLE teams (\n  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n  name TEXT NOT NULL\n);\n\nCREATE TABLE team_members (\n  team_id UUID REFERENCES teams,\n  user_id UUID REFERENCES auth.users,\n  role TEXT CHECK (role IN ('owner', 'admin', 'member')),\n  PRIMARY KEY (team_id, user_id)\n);\n\nCREATE TABLE team_documents (\n  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n  team_id UUID REFERENCES teams,\n  title TEXT,\n  content TEXT\n);\n\n-- Only team members can see team documents\nCREATE POLICY \"Team members can view documents\"\n  ON team_documents FOR SELECT\n  USING (\n    team_id IN (\n      SELECT team_id\n      FROM team_members\n      WHERE user_id = auth.uid()\n    )\n  );\n\n-- Only team owners/admins can delete\nCREATE POLICY \"Team admins can delete documents\"\n  ON team_documents FOR DELETE\n  USING (\n    team_id IN (\n      SELECT team_id\n      FROM team_members\n      WHERE user_id = auth.uid()\n        AND role IN ('owner', 'admin')\n    )\n  );\n```\n\n### Function-Based RLS\n\n```sql\n-- Create helper function\nCREATE OR REPLACE FUNCTION is_team_admin(team_id UUID)\nRETURNS BOOLEAN AS $$\nBEGIN\n  RETURN EXISTS (\n    SELECT 1\n    FROM team_members\n    WHERE team_members.team_id = is_team_admin.team_id\n      AND team_members.user_id = auth.uid()\n      AND team_members.role IN ('owner', 'admin')\n  );\nEND;\n$$ LANGUAGE plpgsql SECURITY DEFINER;\n\n-- Use in policy\nCREATE POLICY \"Admins can update team settings\"\n  ON teams FOR UPDATE\n  USING (is_team_admin(id))\n  WITH CHECK (is_team_admin(id));\n```\n\n### RLS with JWT Claims\n\n```sql\n-- Access custom JWT claims\nCREATE POLICY \"Premium users can view premium content\"\n  ON premium_content FOR SELECT\n  USING (\n    (auth.jwt() -> 'user_metadata' ->> 'subscription_tier') = 'premium'\n  );\n\n-- Role-based access\nCREATE POLICY \"Admins have full access\"\n  ON sensitive_data FOR ALL\n  USING (\n    (auth.jwt() -> 'user_metadata' ->> 'role') = 'admin'\n  );\n```\n\n## Advanced Real-time Features\n\n### Presence (Who's Online)\n\n```typescript\n'use client'\n\nimport { useEffect, useState } from 'react'\nimport { createClient } from '@/lib/supabase/client'\n\nexport function OnlineUsers() {\n  const [onlineUsers, setOnlineUsers] = useState<any[]>([])\n  const supabase = createClient()\n\n  useEffect(() => {\n    const channel = supabase.channel('online-users')\n\n    channel\n      .on('presence', { event: 'sync' }, () => {\n        const state = channel.presenceState()\n        const users = Object.values(state).flat()\n        setOnlineUsers(users)\n      })\n      .on('presence', { event: 'join' }, ({ newPresences }) => {\n        console.log('Users joined:', newPresences)\n      })\n      .on('presence', { event: 'leave' }, ({ leftPresences }) => {\n        console.log('Users left:', leftPresences)\n      })\n      .subscribe(async (status) => {\n        if (status === 'SUBSCRIBED') {\n          // Track this user\n          const { data: { user } } = await supabase.auth.getUser()\n          if (user) {\n            await channel.track({\n              user_id: user.id,\n              email: user.email,\n              online_at: new Date().toISOString(),\n            })\n          }\n        }\n      })\n\n    return () => {\n      supabase.removeChannel(channel)\n    }\n  }, [])\n\n  return (\n    <div>\n      <h3>{onlineUsers.length} users online</h3>\n      <ul>\n        {onlineUsers.map((user, i) => (\n          <li key={i}>{user.email}</li>\n        ))}\n      </ul>\n    </div>\n  )\n}\n```\n\n### Broadcast (Send Messages)\n\n```typescript\n// Cursor tracking\nexport function CollaborativeCanvas() {\n  const supabase = createClient()\n\n  useEffect(() => {\n    const channel = supabase.channel('canvas')\n\n    channel\n      .on('broadcast', { event: 'cursor' }, (payload) => {\n        // Update cursor position\n        updateCursor(payload.payload)\n      })\n      .subscribe()\n\n    // Send cursor position\n    const handleMouseMove = (e: MouseEvent) => {\n      channel.send({\n        type: 'broadcast',\n        event: 'cursor',\n        payload: { x: e.clientX, y: e.clientY },\n      })\n    }\n\n    window.addEventListener('mousemove', handleMouseMove)\n\n    return () => {\n      window.removeEventListener('mousemove', handleMouseMove)\n      supabase.removeChannel(channel)\n    }\n  }, [])\n\n  return <canvas />\n}\n```\n\n### Postgres Changes (Database Events)\n\n```typescript\n// Listen to specific columns\nconst channel = supabase\n  .channel('post-changes')\n  .on(\n    'postgres_changes',\n    {\n      event: 'UPDATE',\n      schema: 'public',\n      table: 'posts',\n      filter: 'id=eq.123',  // Specific row\n    },\n    (payload) => {\n      console.log('Post updated:', payload)\n    }\n  )\n  .subscribe()\n\n// Listen to multiple tables\nconst channel = supabase\n  .channel('changes')\n  .on(\n    'postgres_changes',\n    { event: '*', schema: 'public', table: 'posts' },\n    handlePostChange\n  )\n  .on(\n    'postgres_changes',\n    { event: '*', schema: 'public', table: 'comments' },\n    handleCommentChange\n  )\n  .subscribe()\n```\n\n## Advanced Storage\n\n### Image Transformations\n\n```typescript\n// Upload with transformation\nexport async function uploadAvatar(file: File, userId: string) {\n  const supabase = createClient()\n\n  const fileExt = file.name.split('.').pop()\n  const fileName = `${userId}-${Date.now()}.${fileExt}`\n  const filePath = `avatars/${fileName}`\n\n  const { error: uploadError } = await supabase.storage\n    .from('avatars')\n    .upload(filePath, file, {\n      cacheControl: '3600',\n      upsert: false,\n    })\n\n  if (uploadError) throw uploadError\n\n  // Get transformed image URL\n  const { data } = supabase.storage\n    .from('avatars')\n    .getPublicUrl(filePath, {\n      transform: {\n        width: 200,\n        height: 200,\n        resize: 'cover',\n        quality: 80,\n      },\n    })\n\n  return data.publicUrl\n}\n```\n\n### Signed URLs (Private Files)\n\n```typescript\n// Generate signed URL (expires after 1 hour)\nexport async function getPrivateFileUrl(path: string) {\n  const supabase = createClient()\n\n  const { data, error } = await supabase.storage\n    .from('private-files')\n    .createSignedUrl(path, 3600)  // 1 hour\n\n  if (error) throw error\n\n  return data.signedUrl\n}\n\n// Upload to private bucket\nexport async function uploadPrivateFile(file: File, userId: string) {\n  const supabase = createClient()\n\n  const filePath = `${userId}/${file.name}`\n\n  const { error } = await supabase.storage\n    .from('private-files')\n    .upload(filePath, file)\n\n  if (error) throw error\n\n  return filePath\n}\n```\n\n### Storage RLS\n\n```sql\n-- Enable RLS on storage.objects\nCREATE POLICY \"Users can upload to their own folder\"\n  ON storage.objects FOR INSERT\n  WITH CHECK (\n    bucket_id = 'avatars' AND\n    (storage.foldername(name))[1] = auth.uid()::text\n  );\n\nCREATE POLICY \"Users can view their own files\"\n  ON storage.objects FOR SELECT\n  USING (\n    bucket_id = 'avatars' AND\n    (storage.foldername(name))[1] = auth.uid()::text\n  );\n\nCREATE POLICY \"Users can update their own files\"\n  ON storage.objects FOR UPDATE\n  USING (\n    bucket_id = 'avatars' AND\n    (storage.foldername(name))[1] = auth.uid()::text\n  );\n\nCREATE POLICY \"Users can delete their own files\"\n  ON storage.objects FOR DELETE\n  USING (\n    bucket_id = 'avatars' AND\n    (storage.foldername(name))[1] = auth.uid()::text\n  );\n```\n\n## Edge Functions\n\n### Basic Edge Function\n\n```typescript\n// supabase/functions/hello/index.ts\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\n\nserve(async (req) => {\n  const { name } = await req.json()\n\n  return new Response(\n    JSON.stringify({ message: `Hello ${name}!` }),\n    {\n      headers: { 'Content-Type': 'application/json' },\n    },\n  )\n})\n```\n\n### Edge Function with Supabase Client\n\n```typescript\n// supabase/functions/create-post/index.ts\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\nimport { createClient } from 'https://esm.sh/@supabase/supabase-js@2'\n\nserve(async (req) => {\n  try {\n    const supabaseClient = createClient(\n      Deno.env.get('SUPABASE_URL') ?? '',\n      Deno.env.get('SUPABASE_ANON_KEY') ?? '',\n      {\n        global: {\n          headers: { Authorization: req.headers.get('Authorization')! },\n        },\n      }\n    )\n\n    // Get authenticated user\n    const { data: { user }, error: userError } = await supabaseClient.auth.getUser()\n    if (userError || !user) {\n      return new Response(JSON.stringify({ error: 'Unauthorized' }), {\n        status: 401,\n      })\n    }\n\n    const { title, content } = await req.json()\n\n    const { data, error } = await supabaseClient\n      .from('posts')\n      .insert({\n        title,\n        content,\n        user_id: user.id,\n      })\n      .select()\n      .single()\n\n    if (error) throw error\n\n    return new Response(JSON.stringify(data), {\n      headers: { 'Content-Type': 'application/json' },\n    })\n  } catch (error) {\n    return new Response(JSON.stringify({ error: error.message }), {\n      status: 500,\n    })\n  }\n})\n```\n\n### Scheduled Edge Function (Cron)\n\n```typescript\n// supabase/functions/cleanup-old-data/index.ts\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\nimport { createClient } from 'https://esm.sh/@supabase/supabase-js@2'\n\nserve(async (req) => {\n  // Verify request is from Supabase Cron\n  const authHeader = req.headers.get('Authorization')\n  if (authHeader !== `Bearer ${Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')}`) {\n    return new Response('Unauthorized', { status: 401 })\n  }\n\n  const supabase = createClient(\n    Deno.env.get('SUPABASE_URL')!,\n    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n  )\n\n  // Delete old data\n  const thirtyDaysAgo = new Date()\n  thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30)\n\n  const { error } = await supabase\n    .from('temporary_data')\n    .delete()\n    .lt('created_at', thirtyDaysAgo.toISOString())\n\n  if (error) {\n    return new Response(JSON.stringify({ error: error.message }), {\n      status: 500,\n    })\n  }\n\n  return new Response(JSON.stringify({ success: true }))\n})\n\n// Configure in Dashboard: Database > Cron Jobs\n// Schedule: 0 2 * * * (2am daily)\n// HTTP Request: https://your-project.supabase.co/functions/v1/cleanup-old-data\n```\n\n### Invoke Edge Function from Client\n\n```typescript\n// Client-side\nconst { data, error } = await supabase.functions.invoke('hello', {\n  body: { name: 'World' },\n})\n\n// With auth headers automatically included\nconst { data: { session } } = await supabase.auth.getSession()\n\nconst { data, error } = await supabase.functions.invoke('create-post', {\n  body: {\n    title: 'My Post',\n    content: 'Content here',\n  },\n  headers: {\n    Authorization: `Bearer ${session?.access_token}`,\n  },\n})\n```\n\n## Vector Search (AI/RAG)\n\n### Enable pgvector\n\n```sql\n-- Enable vector extension\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Create table with embedding column\nCREATE TABLE documents (\n  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n  content TEXT NOT NULL,\n  metadata JSONB,\n  embedding vector(1536),  -- For OpenAI ada-002 (1536 dimensions)\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Create index for fast similarity search\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)\nWITH (lists = 100);\n\n-- Or use HNSW for better performance (Postgres 15+)\nCREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops);\n```\n\n### Generate and Store Embeddings\n\n```typescript\n// lib/embeddings.ts\nimport { OpenAI } from 'openai'\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n})\n\nexport async function generateEmbedding(text: string): Promise<number[]> {\n  const response = await openai.embeddings.create({\n    model: 'text-embedding-ada-002',\n    input: text,\n  })\n\n  return response.data[0].embedding\n}\n\n// Store document with embedding\nexport async function storeDocument(content: string, metadata: any) {\n  const supabase = createClient()\n\n  const embedding = await generateEmbedding(content)\n\n  const { data, error } = await supabase\n    .from('documents')\n    .insert({\n      content,\n      metadata,\n      embedding,\n    })\n    .select()\n    .single()\n\n  if (error) throw error\n\n  return data\n}\n```\n\n### Semantic Search\n\n```typescript\n// Search similar documents\nexport async function searchSimilarDocuments(query: string, limit = 5) {\n  const supabase = createClient()\n\n  // Generate embedding for query\n  const queryEmbedding = await generateEmbedding(query)\n\n  // Search with RPC function\n  const { data, error } = await supabase.rpc('match_documents', {\n    query_embedding: queryEmbedding,\n    match_threshold: 0.78,  // Minimum similarity\n    match_count: limit,\n  })\n\n  if (error) throw error\n\n  return data\n}\n\n// Create the RPC function\n```\n\n```sql\nCREATE OR REPLACE FUNCTION match_documents (\n  query_embedding vector(1536),\n  match_threshold float,\n  match_count int\n)\nRETURNS TABLE (\n  id UUID,\n  content TEXT,\n  metadata JSONB,\n  similarity float\n)\nLANGUAGE SQL STABLE\nAS $$\n  SELECT\n    documents.id,\n    documents.content,\n    documents.metadata,\n    1 - (documents.embedding <=> query_embedding) AS similarity\n  FROM documents\n  WHERE 1 - (documents.embedding <=> query_embedding) > match_threshold\n  ORDER BY documents.embedding <=> query_embedding\n  LIMIT match_count;\n$$;\n```\n\n### RAG (Retrieval Augmented Generation)\n\n```typescript\nexport async function ragQuery(question: string) {\n  // 1. Search for relevant documents\n  const relevantDocs = await searchSimilarDocuments(question, 5)\n\n  // 2. Build context from relevant documents\n  const context = relevantDocs\n    .map(doc => doc.content)\n    .join('\\n\\n')\n\n  // 3. Generate answer with GPT\n  const completion = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [\n      {\n        role: 'system',\n        content: 'You are a helpful assistant. Answer questions based on the provided context.',\n      },\n      {\n        role: 'user',\n        content: `Context:\\n${context}\\n\\nQuestion: ${question}`,\n      },\n    ],\n  })\n\n  return {\n    answer: completion.choices[0].message.content,\n    sources: relevantDocs,\n  }\n}\n```\n\n## Database Functions & Triggers\n\n### Custom Functions\n\n```sql\n-- Get user's post count\nCREATE OR REPLACE FUNCTION get_user_post_count(user_id UUID)\nRETURNS INTEGER AS $$\n  SELECT COUNT(*)::INTEGER\n  FROM posts\n  WHERE posts.user_id = get_user_post_count.user_id;\n$$ LANGUAGE SQL STABLE;\n\n-- Call from TypeScript\nconst { data, error } = await supabase.rpc('get_user_post_count', {\n  user_id: userId,\n})\n```\n\n### Triggers\n\n```sql\n-- Auto-update updated_at column\nCREATE OR REPLACE FUNCTION handle_updated_at()\nRETURNS TRIGGER AS $$\nBEGIN\n  NEW.updated_at = NOW();\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER set_updated_at\n  BEFORE UPDATE ON posts\n  FOR EACH ROW\n  EXECUTE FUNCTION handle_updated_at();\n\n-- Update post count when post is created/deleted\nCREATE OR REPLACE FUNCTION update_post_count()\nRETURNS TRIGGER AS $$\nBEGIN\n  IF TG_OP = 'INSERT' THEN\n    UPDATE profiles\n    SET post_count = post_count + 1\n    WHERE id = NEW.user_id;\n    RETURN NEW;\n  ELSIF TG_OP = 'DELETE' THEN\n    UPDATE profiles\n    SET post_count = post_count - 1\n    WHERE id = OLD.user_id;\n    RETURN OLD;\n  END IF;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER update_post_count_trigger\n  AFTER INSERT OR DELETE ON posts\n  FOR EACH ROW\n  EXECUTE FUNCTION update_post_count();\n```\n\n## Performance Optimization\n\n### Query Optimization\n\n```typescript\n// Bad: N+1 query problem\nconst { data: posts } = await supabase.from('posts').select('*')\nfor (const post of posts) {\n  const { data: author } = await supabase\n    .from('profiles')\n    .select('*')\n    .eq('id', post.user_id)\n    .single()\n}\n\n// Good: Join in single query\nconst { data: posts } = await supabase\n  .from('posts')\n  .select(`\n    *,\n    profiles (\n      id,\n      name,\n      avatar_url\n    )\n  `)\n\n// Good: Use specific columns\nconst { data: posts } = await supabase\n  .from('posts')\n  .select('id, title, created_at, profiles(name)')  // Only what you need\n  .eq('published', true)\n  .order('created_at', { ascending: false })\n  .limit(20)\n```\n\n### Indexes\n\n```sql\n-- Index on frequently filtered columns\nCREATE INDEX posts_user_id_idx ON posts(user_id);\nCREATE INDEX posts_created_at_idx ON posts(created_at DESC);\n\n-- Partial index (filtered)\nCREATE INDEX posts_published_idx ON posts(published)\nWHERE published = true;\n\n-- Composite index\nCREATE INDEX posts_user_published_idx ON posts(user_id, published, created_at DESC);\n\n-- Full-text search index\nCREATE INDEX posts_content_idx ON posts\nUSING GIN (to_tsvector('english', content));\n```\n\n### Connection Pooling\n\n```typescript\n// Use connection pooler for serverless (Supavisor)\n// Connection string: postgresql://postgres.[project-ref]:[password]@aws-0-us-east-1.pooler.supabase.com:6543/postgres\n\nimport { createClient } from '@supabase/supabase-js'\n\nconst supabase = createClient(\n  process.env.SUPABASE_URL!,\n  process.env.SUPABASE_ANON_KEY!,\n  {\n    db: {\n      schema: 'public',\n    },\n    global: {\n      headers: { 'x-my-custom-header': 'my-app-name' },\n    },\n  }\n)\n```\n\n### Caching\n\n```typescript\n// Next.js cache with revalidation\nimport { unstable_cache } from 'next/cache'\nimport { createClient } from '@/lib/supabase/server'\n\nexport const getCachedPosts = unstable_cache(\n  async () => {\n    const supabase = createClient()\n    const { data } = await supabase\n      .from('posts')\n      .select('*')\n      .eq('published', true)\n    return data\n  },\n  ['posts'],\n  {\n    revalidate: 300,  // 5 minutes\n    tags: ['posts'],\n  }\n)\n\n// Revalidate on mutation\nimport { revalidateTag } from 'next/cache'\n\nexport async function createPost(data: any) {\n  const supabase = createClient()\n  await supabase.from('posts').insert(data)\n  revalidateTag('posts')\n}\n```\n\n## Local Development\n\n### Setup Local Supabase\n\n```bash\n# Install Supabase CLI\nbrew install supabase/tap/supabase\n\n# Initialize project\nsupabase init\n\n# Start local Supabase (Docker required)\nsupabase start\n\n# This starts:\n# - PostgreSQL\n# - GoTrue (Auth)\n# - Realtime\n# - Storage\n# - Kong (API Gateway)\n# - Studio (Dashboard)\n```\n\n### Local Development URLs\n\n```bash\n# After supabase start:\nAPI URL: http://localhost:54321\nStudio URL: http://localhost:54323\nInbucket URL: http://localhost:54324  # Email testing\n```\n\n### Migration Workflow\n\n```bash\n# Create migration\nsupabase migration new add_posts_table\n\n# Edit migration file in supabase/migrations/\n\n# Apply migration locally\nsupabase db reset\n\n# Push to production\nsupabase db push\n\n# Pull remote schema\nsupabase db pull\n```\n\n### Generate Types from Local DB\n\n```bash\n# Generate TypeScript types\nsupabase gen types typescript --local > types/database.ts\n```\n\n## Testing\n\n### Testing RLS Policies\n\n```sql\n-- Test as specific user\nSET LOCAL ROLE authenticated;\nSET LOCAL \"request.jwt.claims\" TO '{\"sub\": \"user-uuid-here\"}';\n\n-- Test query\nSELECT * FROM posts;\n\n-- Reset\nRESET ROLE;\n```\n\n### Testing with Supabase Test Helpers\n\n```typescript\n// tests/posts.test.ts\nimport { createClient } from '@supabase/supabase-js'\n\nconst supabase = createClient(\n  process.env.SUPABASE_URL!,\n  process.env.SUPABASE_SERVICE_ROLE_KEY!  // For testing\n)\n\ndescribe('Posts', () => {\n  beforeEach(async () => {\n    // Clean up\n    await supabase.from('posts').delete().neq('id', '00000000-0000-0000-0000-000000000000')\n  })\n\n  it('should create post', async () => {\n    const { data, error } = await supabase\n      .from('posts')\n      .insert({ title: 'Test', content: 'Test' })\n      .select()\n      .single()\n\n    expect(error).toBeNull()\n    expect(data.title).toBe('Test')\n  })\n})\n```\n\n## Error Handling\n\n### Comprehensive Error Handler\n\n```typescript\nimport { PostgrestError } from '@supabase/supabase-js'\n\nexport function handleSupabaseError(error: PostgrestError | null) {\n  if (!error) return null\n\n  // Common error codes\n  const errorMessages: Record<string, string> = {\n    '23505': 'This record already exists',  // Unique violation\n    '23503': 'Related record not found',    // Foreign key violation\n    '42P01': 'Table does not exist',\n    '42501': 'Permission denied',\n    'PGRST116': 'No rows found',\n  }\n\n  const userMessage = errorMessages[error.code] || error.message\n\n  console.error('Supabase error:', {\n    code: error.code,\n    message: error.message,\n    details: error.details,\n    hint: error.hint,\n  })\n\n  return userMessage\n}\n\n// Usage\nconst { data, error } = await supabase.from('posts').insert(postData)\n\nif (error) {\n  const message = handleSupabaseError(error)\n  toast.error(message)\n  return\n}\n```\n\n## Best Practices\n\n1. **Row Level Security:**\n   - Enable RLS on ALL tables\n   - Never rely on client-side checks alone\n   - Test policies thoroughly\n   - Use service role key sparingly (server-side only)\n\n2. **Query Optimization:**\n   - Use `.select()` to specify needed columns\n   - Add database indexes for filtered/sorted columns\n   - Use `.limit()` to cap results\n   - Consider pagination for large datasets\n\n3. **Real-time Subscriptions:**\n   - Always unsubscribe when component unmounts\n   - Use RLS policies to filter events\n   - Use broadcast for ephemeral data\n   - Limit number of simultaneous subscriptions\n\n4. **Authentication:**\n   - Store JWT in httpOnly cookies when possible\n   - Refresh tokens before expiry\n   - Handle auth state changes\n   - Validate user on server-side\n\n5. **Storage:**\n   - Set appropriate bucket policies\n   - Use image transformations for optimization\n   - Consider storage limits\n   - Clean up unused files\n\n6. **Error Handling:**\n   - Always check `error` object\n   - Provide user-friendly error messages\n   - Log errors for debugging\n   - Handle network failures gracefully\n\n## Common Patterns\n\n### Auto-create Profile on Signup\n\n```sql\n-- Create profiles table\nCREATE TABLE profiles (\n  id UUID REFERENCES auth.users PRIMARY KEY,\n  name TEXT,\n  avatar_url TEXT,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Trigger function\nCREATE OR REPLACE FUNCTION public.handle_new_user()\nRETURNS TRIGGER AS $$\nBEGIN\n  INSERT INTO public.profiles (id, name, avatar_url)\n  VALUES (\n    NEW.id,\n    NEW.raw_user_meta_data->>'name',\n    NEW.raw_user_meta_data->>'avatar_url'\n  );\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql SECURITY DEFINER;\n\n-- Trigger\nCREATE TRIGGER on_auth_user_created\n  AFTER INSERT ON auth.users\n  FOR EACH ROW EXECUTE FUNCTION public.handle_new_user();\n```\n\n## Documentation Quick Reference\n\n**Need to find something specific?**\n\nSearch the 2,616 documentation files:\n\n```bash\n# Search all docs\ngrep -r \"search term\" docs/supabase_com/\n\n# Find guides\nls docs/supabase_com/guides_*\n\n# Find API reference\nls docs/supabase_com/reference_*\n```\n\n**Common doc locations:**\n- Guides: `docs/supabase_com/guides_*`\n- JavaScript Reference: `docs/supabase_com/reference_javascript_*`\n- Database: `docs/supabase_com/guides_database_*`\n- Auth: `docs/supabase_com/guides_auth_*`\n- Storage: `docs/supabase_com/guides_storage_*`\n\n## Resources\n\n- **Dashboard:** https://supabase.com/dashboard\n- **Docs:** https://supabase.com/docs\n- **Status:** https://status.supabase.com\n- **CLI Docs:** https://supabase.com/docs/guides/cli\n\n## Implementation Checklist\n\n- [ ] Create Supabase project\n- [ ] Install: `npm install @supabase/supabase-js`\n- [ ] Set environment variables\n- [ ] Design database schema\n- [ ] Create migrations\n- [ ] Enable RLS and create policies\n- [ ] Set up authentication\n- [ ] Implement auth state management\n- [ ] Create CRUD operations\n- [ ] Add real-time subscriptions (if needed)\n- [ ] Configure storage buckets (if needed)\n- [ ] Test RLS policies\n- [ ] Add database indexes\n- [ ] Deploy edge functions (if needed)\n- [ ] Test in production\n",
        "templates/.claude/skills/toon-formatter/docs/INSTALL.md": "# Installing and Using the Zig TOON Enforcer\n\n## Quick Start (Prebuilt Binaries)\n\nPrebuilt binaries are included in `bin/` for supported platforms:\n\n```\nbin/\n├── toon-darwin-arm64    # macOS Apple Silicon\n├── toon-darwin-x64      # macOS Intel\n├── toon-linux-x64       # Linux x64\n└── toon-linux-arm64     # Linux ARM\n```\n\nThe `claude-starter init` command automatically symlinks the correct binary for your platform.\n\n```bash\n# Test the binary directly\n./bin/toon-darwin-arm64 --help\n```\n\n## Building from Source\n\nIf you need to rebuild or your platform isn't supported:\n\n### Prerequisites\n\nInstall Zig (version 0.13.0 or later):\n\n**macOS:**\n```bash\nbrew install zig\n```\n\n**Linux:**\n```bash\n# Using snap\nsudo snap install zig --classic --beta\n\n# Or download from https://ziglang.org/download/\nwget https://ziglang.org/download/0.13.0/zig-linux-x86_64-0.13.0.tar.xz\ntar xf zig-linux-x86_64-0.13.0.tar.xz\nsudo mv zig-linux-x86_64-0.13.0 /usr/local/zig\nexport PATH=$PATH:/usr/local/zig\n```\n\n**Windows:**\n```powershell\n# Using Scoop\nscoop install zig\n\n# Or download from https://ziglang.org/download/\n```\n\n### Build for Current Platform\n\n```bash\ncd .claude/skills/toon-formatter\n\n# Build optimized binary\nzig build -Doptimize=ReleaseFast\n\n# Binary is at zig-out/bin/toon\n./zig-out/bin/toon --help\n```\n\n### Build for All Platforms (Maintainers)\n\n```bash\n# Build binaries for all supported platforms\n./build-all.sh\n\n# Output in bin/ directory:\n#   toon-darwin-arm64\n#   toon-darwin-x64\n#   toon-linux-x64\n#   toon-linux-arm64\n```\n\n### Run Tests\n\n```bash\nzig build test\n```\n\n## Usage\n\n### 1. Check if JSON should use TOON\n\n```bash\n$ echo '[{\"id\":1,\"name\":\"Alice\"},{\"id\":2,\"name\":\"Bob\"}]' > test.json\n$ ./zig-out/bin/toon check test.json\n✗ JSON format recommended (keep as-is)\n# Exit code 1 because <5 items\n```\n\n```bash\n$ cat large.json\n[\n  {\"id\":1,\"name\":\"Alice\",\"role\":\"admin\"},\n  {\"id\":2,\"name\":\"Bob\",\"role\":\"user\"},\n  {\"id\":3,\"name\":\"Carol\",\"role\":\"user\"},\n  {\"id\":4,\"name\":\"Dan\",\"role\":\"user\"},\n  {\"id\":5,\"name\":\"Eve\",\"role\":\"admin\"}\n]\n\n$ ./zig-out/bin/toon check large.json\n✓ TOON format recommended (≥5 items, ≥60% uniformity)\n# Exit code 0\n```\n\n### 2. Convert JSON to TOON\n\n```bash\n$ ./zig-out/bin/toon encode large.json\n[5]{id,name,role}:\n  1,Alice,admin\n  2,Bob,user\n  3,Carol,user\n  4,Dan,user\n  5,Eve,admin\n```\n\n### 3. Convert TOON back to JSON\n\n```bash\n$ ./zig-out/bin/toon encode large.json > output.toon\n$ ./zig-out/bin/toon decode output.toon\n[{\"id\":\"1\",\"name\":\"Alice\",\"role\":\"admin\"},{\"id\":\"2\",\"name\":\"Bob\",\"role\":\"user\"}, ...]\n```\n\n### 4. Enforce TOON in Documentation\n\n```bash\n# Scan all .claude/docs/toon/*.md files for JSON that should use TOON\n$ ./enforce-toon.sh\n\nEnforcing TOON format in documentation...\n\nChecking: ../../docs/toon/examples/api-endpoints.md\n  ⚠ Line 42: JSON array should use TOON format\n    Suggested conversion:\n    [15]{method,path,auth,rateLimit}:\n      GET,/api/users,required,100/min\n      ...\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nChecked 10 files\n⚠ 3 file(s) have JSON that should use TOON\n```\n\n## Integration with Claude Code\n\nAfter building, you can use the Zig binary in your workflows:\n\n### Add to PATH (Optional)\n\n```bash\n# Add to ~/.zshrc or ~/.bashrc\nexport PATH=\"$PATH:/Users/zach/Documents/claude-starter/.claude/skills/toon-formatter/zig-out/bin\"\n\n# Now you can run from anywhere:\ntoon check data.json\ntoon encode data.json\n```\n\n### Use in Commands\n\nCreate `.claude/commands/check-toon.md`:\n\n```markdown\n# Check TOON\n\nCheck if documentation uses appropriate TOON format.\n\nUsage: /check-toon\n\nExecute the following workflow:\n\n1. **Run TOON Enforcement**\n   ```bash\n   .claude/skills/toon-formatter/enforce-toon.sh\n   ```\n```\n\n### Use in Hooks\n\nCreate `.claude/hooks/toon_check.sh`:\n\n```bash\n#!/usr/bin/env bash\n# Hook that runs after editing TOON docs\n\nif [[ \"$CLAUDE_FILE_PATH\" == *\".claude/docs/toon\"* ]]; then\n    .claude/skills/toon-formatter/enforce-toon.sh\nfi\n```\n\n## Performance\n\nThe Zig implementation is **~20x faster** than TypeScript alternatives:\n\n```bash\n# Benchmark with 1000-item array\n$ hyperfine \\\n    './zig-out/bin/toon encode large.json' \\\n    'node typescript-encoder.js large.json'\n\nBenchmark 1: ./zig-out/bin/toon encode large.json\n  Time (mean ± σ):       2.1 ms ±   0.3 ms    [User: 1.2 ms, System: 0.5 ms]\n  Range (min … max):     1.8 ms …   3.4 ms    500 runs\n\nBenchmark 2: node typescript-encoder.js large.json\n  Time (mean ± σ):      47.3 ms ±   2.1 ms    [User: 42.1 ms, System: 8.2 ms]\n  Range (min … max):    44.2 ms …  54.8 ms    100 runs\n\nSummary\n  './zig-out/bin/toon encode large.json' ran\n   22.52 ± 3.12 times faster than 'node typescript-encoder.js large.json'\n```\n\n## Next Steps\n\n1. **Build the binary**: `zig build -Doptimize=ReleaseFast`\n2. **Test it**: `./zig-out/bin/toon check test.json`\n3. **Run enforcement**: `./enforce-toon.sh`\n4. **Integrate with your workflow**: Add to commands/hooks\n\nSee [README_ZIG.md](./README_ZIG.md) for complete documentation.\n",
        "templates/.claude/skills/toon-formatter/docs/toon-guide.md": "# TOON Format v2.0 Complete Guide\n\n**Version:** 2.0 (2025-11-10)\n**Specification:** https://github.com/toon-format/spec\n**Status:** 100% Compliant Implementation\n\nToken-Oriented Object Notation (TOON) is a compact, human-readable encoding of JSON that reduces token consumption by **30-60%** for tabular data.\n\n## Quick Start\n\n### JSON to TOON Conversion\n\n**JSON (120 tokens):**\n```json\n[\n  {\"id\": 1, \"name\": \"Alice\", \"role\": \"admin\"},\n  {\"id\": 2, \"name\": \"Bob\", \"role\": \"user\"}\n]\n```\n\n**TOON (70 tokens - 40% savings):**\n```\n[2]{id,name,role}:\n  1,Alice,admin\n  2,Bob,user\n```\n\n### When to Use TOON\n\n✅ **Use TOON for:**\n- Arrays with ≥5 items\n- ≥60% field uniformity across objects\n- Tabular/structured data (APIs, logs, metrics, databases)\n- Flat object structures\n- Token efficiency matters\n\n❌ **Keep JSON for:**\n- Small arrays (<5 items)\n- Deeply nested structures (>3 levels)\n- Non-uniform data (<60% same fields)\n- Prose or free-form text\n- When human editing is priority\n\n## TOON v2.0 Features\n\n### 1. Three Array Types\n\nTOON v2.0 supports three array formats, automatically selected based on data structure:\n\n#### Inline Primitive Arrays\n\n**For:** Homogeneous primitive arrays (strings, numbers, booleans), ≤10 items\n\n**Syntax:** `arrayName[count]: value1,value2,value3`\n\n**Example:**\n```\nfriends[3]: ana,luis,sam\nscores[5]: 95,87,92,88,91\ntags[4]: urgent,reviewed,approved,final\n```\n\n**JSON equivalent:**\n```json\n{\n  \"friends\": [\"ana\", \"luis\", \"sam\"],\n  \"scores\": [95, 87, 92, 88, 91],\n  \"tags\": [\"urgent\", \"reviewed\", \"approved\", \"final\"]\n}\n```\n\n#### Tabular Arrays\n\n**For:** Arrays of objects with uniform fields, ≥2 items, ≥60% uniformity\n\n**Syntax:** `[count]{field1,field2,...}:\\n  value1,value2,...`\n\n**Example:**\n```\n[3]{id,name,role}:\n  1,Alice,admin\n  2,Bob,user\n  3,Carol,user\n```\n\n**JSON equivalent:**\n```json\n[\n  {\"id\": 1, \"name\": \"Alice\", \"role\": \"admin\"},\n  {\"id\": 2, \"name\": \"Bob\", \"role\": \"user\"},\n  {\"id\": 3, \"name\": \"Carol\", \"role\": \"user\"}\n]\n```\n\n#### Expanded List Arrays\n\n**For:** Non-uniform arrays, complex nested structures, mixed types\n\n**Syntax:** `arrayName[count]:\\n  - item1\\n  - item2`\n\n**Example:**\n```\nitems[2]:\n  - {id: 1, name: \"Simple\", count: 5}\n  - {id: 2, data: [1,2,3], meta: {tag: \"complex\"}}\n```\n\n**JSON equivalent:**\n```json\n{\n  \"items\": [\n    {\"id\": 1, \"name\": \"Simple\", \"count\": 5},\n    {\"id\": 2, \"data\": [1,2,3], \"meta\": {\"tag\": \"complex\"}}\n  ]\n}\n```\n\n### 2. Multiple Delimiters\n\nTOON v2.0 supports three delimiters for maximum flexibility:\n\n| Delimiter | Character | Use When | Declare |\n|-----------|-----------|----------|---------|\n| **Comma** | `,` | General use (default) | `[N]{...}:` or `[N,]{...}:` |\n| **Tab** | `\\t` | TSV-like data, columnar alignment | `[N\\t]{...}:` |\n| **Pipe** | `\\|` | Data with commas, Markdown tables | `[N\\|]{...}:` |\n\n#### Comma Delimiter (Default)\n\n```\n[2]{id,name,score}:\n  1,Alice,95\n  2,Bob,87\n```\n\n#### Tab Delimiter\n\n```\n[2\\t]{id,name,score}:\n  1\tAlice\t95\n  2\tBob\t87\n```\n\n#### Pipe Delimiter\n\n```\n[2|]{id,name,score}:\n  1|Alice|95\n  2|Bob|87\n```\n\n**Choosing a delimiter:**\n- **Comma:** Most compact, default choice\n- **Tab:** When data contains many commas, better columnar alignment\n- **Pipe:** Markdown compatibility, visual clarity\n\n### 3. Key Folding (v1.5+)\n\nKey folding flattens nested objects using dotted notation for significant token savings.\n\n**Without key folding:**\n```json\n{\n  \"server\": {\n    \"host\": \"localhost\",\n    \"port\": 8080,\n    \"ssl\": {\n      \"enabled\": true,\n      \"cert\": \"/path/to/cert\"\n    }\n  }\n}\n```\n\n**With key folding:**\n```\nserver.host: localhost\nserver.port: 8080\nserver.ssl.enabled: true\nserver.ssl.cert: /path/to/cert\n```\n\n**Token savings:** ~35% on nested objects\n\n**Rules:**\n- Each segment must be a valid identifier: `^[A-Za-z_][A-Za-z0-9_]*$`\n- Only fold if no collision with sibling keys\n- Don't fold arrays or primitives\n- Safe mode: only fold when unambiguous\n\n**Example - Collision Detection:**\n```json\n{\n  \"server\": \"primary\",\n  \"server.host\": \"localhost\"  // Collision!\n}\n```\n\nThis **cannot** use key folding because `server` exists as both a string and an object path.\n\n### 4. Path Expansion (Decoder Feature)\n\nPath expansion is the reverse of key folding - it expands dotted keys back into nested objects during decoding.\n\n**TOON input:**\n```\napi.base: https://example.com\napi.timeout: 5000\napi.retry.attempts: 3\napi.retry.delay: 1000\n```\n\n**JSON output (with path expansion enabled):**\n```json\n{\n  \"api\": {\n    \"base\": \"https://example.com\",\n    \"timeout\": 5000,\n    \"retry\": {\n      \"attempts\": 3,\n      \"delay\": 1000\n    }\n  }\n}\n```\n\n**Configuration:**\n```bash\n# Enable path expansion (default: true)\n./zig-out/bin/toon decode data.toon --expand-paths\n\n# Disable (keep dotted keys as-is)\n./zig-out/bin/toon decode data.toon --no-expand-paths\n```\n\n### 5. Strict Mode Validation\n\nStrict mode enforces rigorous formatting rules for production environments.\n\n**Validation rules:**\n1. **Indentation alignment:** Must be exact multiples of `indentSize` (default: 2)\n2. **No tabs in indentation:** Only spaces allowed\n3. **Array count matches:** Header count must equal actual rows\n4. **Field width consistency:** All rows must have same number of fields\n5. **No blank lines:** Within arrays or data rows\n\n**Example - Valid strict mode:**\n```\n[2]{id,name}:\n  1,Alice\n  2,Bob\n```\n\n**Example - Invalid (indentation error):**\n```\n[2]{id,name}:\n   1,Alice    ← 3 spaces (not multiple of 2)\n  2,Bob\n```\n\n**Example - Invalid (count mismatch):**\n```\n[3]{id,name}:\n  1,Alice\n  2,Bob      ← Only 2 rows, header says 3\n```\n\n**Enable strict mode:**\n```bash\n./zig-out/bin/toon decode data.toon --strict\n./zig-out/bin/toon validate data.toon --strict\n```\n\n### 6. Canonical Number Format\n\nTOON v2.0 requires canonical number formatting:\n\n**Rules:**\n- No exponent notation: `1e3` → `1000`\n- No trailing zeros: `1.50` → `1.5`\n- No leading zeros: `01` → `1` (except standalone `0`)\n- Negative zero becomes zero: `-0` → `0`\n- No decimal point if integer: `1.0` → `1`\n- `NaN` and `Infinity` → `null`\n\n**Examples:**\n```\n// Valid\n1\n1.5\n-42\n0.001\n1000\n\n// Invalid (will be normalized)\n1.0      → 1\n1.50     → 1.5\n01       → 1\n1e3      → 1000\n-0       → 0\n```\n\n### 7. Complete Escape Sequences\n\nTOON v2.0 supports exactly **five** escape sequences:\n\n| Escape | Meaning | Use |\n|--------|---------|-----|\n| `\\\\` | Backslash | Literal backslash |\n| `\\\"` | Quote | Quoted strings |\n| `\\n` | Newline | Line breaks in values |\n| `\\r` | Carriage return | Windows line endings |\n| `\\t` | Tab | Tab characters in values |\n\n**All other backslashes are literal.** Invalid escapes like `\\x` or `\\u` are errors.\n\n**Example:**\n```\n[2]{path,description}:\n  C:\\\\Users\\\\Alice,Windows path with backslashes\n  \"Line 1\\nLine 2\",Multi-line description\n```\n\n### 8. Complete Quoting Rules\n\nValues **must** be quoted if they contain:\n\n1. The active delimiter (`,` `\\t` or `|`)\n2. Colon (`:`)\n3. Brackets (`[` `]`)\n4. Control characters (ASCII < 32 or 127)\n5. Start with hyphen (`-`) - conflicts with list marker\n6. Match reserved words: `true`, `false`, `null`\n\n**Examples:**\n```\n[3]{name,value}:\n  simple,no quotes needed\n  \"has,comma\",must quote (contains delimiter)\n  \"starts-with-hyphen\",must quote\n  \"true\",must quote (reserved word)\n```\n\n### 9. Nested Objects\n\nTOON supports multi-level nesting via indentation:\n\n**Example:**\n```\nserver:\n  host: localhost\n  port: 8080\n  ssl:\n    enabled: true\n    cert: /path/to/cert\n\ndatabase:\n  primary:\n    host: db1.example.com\n    port: 5432\n  replica:\n    host: db2.example.com\n    port: 5432\n```\n\n**JSON equivalent:**\n```json\n{\n  \"server\": {\n    \"host\": \"localhost\",\n    \"port\": 8080,\n    \"ssl\": {\n      \"enabled\": true,\n      \"cert\": \"/path/to/cert\"\n    }\n  },\n  \"database\": {\n    \"primary\": {\n      \"host\": \"db1.example.com\",\n      \"port\": 5432\n    },\n    \"replica\": {\n      \"host\": \"db2.example.com\",\n      \"port\": 5432\n    }\n  }\n}\n```\n\n**Rules:**\n- Each indentation level = 2 spaces (configurable)\n- Consistent indentation required\n- Can be combined with key folding for maximum compression\n\n### 10. Root Form Detection\n\nTOON supports three root forms:\n\n#### Array Root\n```\n[2]{id,name}:\n  1,Alice\n  2,Bob\n```\n\n#### Object Root\n```\nname: MyApp\nversion: 1.0.0\nconfig:\n  debug: true\n```\n\n#### Primitive Root\n```\n42\n```\nor\n```\n\"Hello, World!\"\n```\n\n## Configuration Options\n\n### Encoder Configuration\n\n```bash\n./zig-out/bin/toon encode data.json \\\n  --delimiter comma|tab|pipe \\\n  --indent-size 2 \\\n  --key-folding \\\n  --flatten-depth 3\n```\n\n**Options:**\n- `--delimiter`: Choose comma (default), tab, or pipe\n- `--indent-size`: Spaces per indentation level (default: 2)\n- `--key-folding`: Enable automatic key folding (default: true)\n- `--flatten-depth`: Max nesting depth to fold (default: unlimited)\n\n### Decoder Configuration\n\n```bash\n./zig-out/bin/toon decode data.toon \\\n  --strict \\\n  --expand-paths \\\n  --indent-size 2\n```\n\n**Options:**\n- `--strict`: Enable strict validation (default: false)\n- `--expand-paths`: Expand dotted keys to nested objects (default: true)\n- `--indent-size`: Expected indentation size (default: 2)\n\n## Real-World Examples\n\n### API Endpoints (40% savings)\n\n**JSON (892 tokens):**\n```json\n[\n  {\"method\": \"GET\", \"path\": \"/api/users\", \"auth\": \"required\", \"rateLimit\": \"100/min\"},\n  {\"method\": \"POST\", \"path\": \"/api/users\", \"auth\": \"required\", \"rateLimit\": \"50/min\"},\n  {\"method\": \"GET\", \"path\": \"/api/users/:id\", \"auth\": \"required\", \"rateLimit\": \"100/min\"},\n  {\"method\": \"PUT\", \"path\": \"/api/users/:id\", \"auth\": \"required\", \"rateLimit\": \"50/min\"},\n  {\"method\": \"DELETE\", \"path\": \"/api/users/:id\", \"auth\": \"required\", \"rateLimit\": \"20/min\"}\n]\n```\n\n**TOON (534 tokens - 40.1% savings):**\n```\n[5]{method,path,auth,rateLimit}:\n  GET,/api/users,required,100/min\n  POST,/api/users,required,50/min\n  GET,/api/users/:id,required,100/min\n  PUT,/api/users/:id,required,50/min\n  DELETE,/api/users/:id,required,20/min\n```\n\n### Transaction Logs (39.6% savings)\n\n**TOON with pipe delimiter (Markdown-friendly):**\n```\n[5|]{id,timestamp,amount,merchant,status}:\n  tx_001|2024-01-15T10:30:00Z|42.50|Starbucks|completed\n  tx_002|2024-01-15T11:15:00Z|15.99|Amazon|completed\n  tx_003|2024-01-15T14:20:00Z|128.00|Apple Store|pending\n  tx_004|2024-01-15T16:45:00Z|8.50|Starbucks|completed\n  tx_005|2024-01-15T18:00:00Z|65.30|Restaurant|completed\n```\n\n### Nested Configuration (35% savings with key folding)\n\n**TOON:**\n```\napp.name: MyService\napp.version: 2.1.0\nserver.host: 0.0.0.0\nserver.port: 8080\nserver.ssl.enabled: true\nserver.ssl.cert: /etc/ssl/cert.pem\ndatabase.primary.host: db1.example.com\ndatabase.primary.port: 5432\ndatabase.replica.host: db2.example.com\ndatabase.replica.port: 5432\n```\n\n## Token Savings Data\n\n| Use Case | Items | JSON Tokens | TOON Tokens | Savings |\n|----------|-------|-------------|-------------|---------|\n| API Endpoints | 15 | 892 | 534 | 40.1% |\n| Transaction Logs | 250 | 4,545 | 2,744 | 39.6% |\n| Performance Metrics | 50 | 1,124 | 628 | 44.1% |\n| Database Results | 100 | 2,315 | 1,389 | 40.0% |\n| Config Files | 25 keys | 456 | 296 | 35.1% |\n\n## ABNF Grammar Reference\n\nFor the complete formal grammar, see: https://github.com/toon-format/spec/blob/main/SPEC.md\n\n**Summary:**\n```abnf\ntoon-document = value\nvalue = object / array / primitive\nobject = *( key-value )\nkey-value = key SP* \":\" SP* value\narray = inline-array / tabular-array / expanded-array\n```\n\nFull ABNF is RFC 5234 compliant and available in `.claude/skills/toon-formatter/references/abnf-grammar.md`.\n\n## Commands\n\n### Available Commands\n\n```bash\n# Analyze token savings\n/analyze-tokens <file> [--detailed] [--estimate-cost]\n\n# Convert JSON to TOON\n/convert-to-toon <file> [--delimiter comma|tab|pipe] [--key-folding] [--strict]\n\n# Validate TOON format\n/validate-toon <file> [--strict]\n\n# Migrate from v1.x to v2.0\n/toon-migrate <file>\n```\n\n### Zig Binary (20x Faster)\n\n```bash\n# Check if TOON is recommended\n./zig-out/bin/toon check data.json\n\n# Encode JSON to TOON\n./zig-out/bin/toon encode data.json --delimiter tab --key-folding\n\n# Decode TOON to JSON\n./zig-out/bin/toon decode data.toon --strict --expand-paths\n\n# Validate TOON format\n./zig-out/bin/toon validate data.toon --strict\n```\n\n## Troubleshooting\n\n### Low Token Savings\n\n**Problem:** Converting to TOON only saves 5-10% tokens.\n\n**Solutions:**\n1. Check uniformity: `/analyze-tokens data.json --detailed`\n2. Ensure ≥60% field overlap across objects\n3. Try key folding for nested objects: `--key-folding`\n4. Use tab delimiter if data has many commas: `--delimiter tab`\n\n### TOON Formatter Not Activating\n\n**Problem:** Skill doesn't auto-detect tabular data.\n\n**Solutions:**\n1. Use trigger keywords: \"optimize tokens\", \"TOON format\", \"tabular data\"\n2. Call explicitly: \"Please convert this to TOON format\"\n3. Use command directly: `/convert-to-toon data.json`\n\n### Strict Mode Validation Failures\n\n**Problem:** `--strict` mode reports errors.\n\n**Common issues:**\n1. **Indentation not aligned:** Must be exact multiples of 2 (or configured size)\n2. **Tabs in indentation:** Use spaces only\n3. **Count mismatch:** Header `[N]` must match actual row count\n4. **Field width mismatch:** All rows must have same number of fields\n\n**Fix:** Use Zig encoder to generate valid TOON:\n```bash\n./zig-out/bin/toon encode data.json --strict\n```\n\n## Resources\n\n- **Official Specification:** https://github.com/toon-format/spec\n- **TOON Website:** https://toonformat.dev\n- **Local Spec:** `.claude/skills/toon-formatter/README.md`\n- **Examples:** `.claude/skills/toon-formatter/examples/`\n- **Guides:** `.claude/skills/toon-formatter/guides/`\n- **References:** `.claude/skills/toon-formatter/references/`\n\n## Summary\n\nTOON v2.0 provides:\n\n✅ **30-60% token savings** on tabular data\n✅ **Three array types** for different data structures\n✅ **Three delimiters** for flexibility\n✅ **Key folding** for nested objects\n✅ **Strict mode** for production validation\n✅ **100% JSON compatibility** (lossless round-trip)\n✅ **Zero dependencies** - instruction-based\n✅ **20x performance** with optional Zig implementation\n\n**When in doubt:** Use `/analyze-tokens` to check if TOON is beneficial!\n",
        "templates/.claude/skills/toon-formatter/skill.md": "---\nname: toon-formatter\ndescription: AGGRESSIVELY use TOON v2.0 format for biggish regular data (≥5 items, ≥60% uniform). Auto-applies to tables, logs, events, transactions, analytics, API responses, database results. Supports 3 array types (inline, tabular, expanded), 3 delimiters (comma, tab, pipe), key folding for nested objects. Triggers on structured data, arrays, repeated patterns. Use TOON by default when tokens matter - RAG pipelines, tool calls, agents, benchmarks. Keywords \"data\", \"array\", \"list\", \"table\", \"log\", \"transaction\", \"metric\", \"analytics\", \"API\", \"database\", \"query\", \"TOON\".\nallowed-tools: Read, Write, Edit, Bash\nmodel: sonnet\n---\n\n# TOON v2.0 Formatter\n\n**AGGRESSIVELY** apply TOON format to save 30-60% tokens on structured data.\n\n## When to Use\n\n**YES - Use automatically:**\n- Arrays with ≥5 similar items\n- Tables, logs, events, transactions, analytics\n- API responses with ≥60% field uniformity\n- Database results, metrics, benchmarks\n\n**NO - Keep as JSON:**\n- Small arrays (<5 items)\n- Deeply nested or non-uniform data\n- Narrative text, instructions\n\n## Quick Reference\n\n**Tabular** (uniform objects):\n```\n[3]{id,name,role}:\n  1,Alice,admin\n  2,Bob,user\n  3,Carol,user\n```\n\n**Inline** (primitives ≤10):\n```\ntags[4]: js,react,node,api\n```\n\n**Delimiters:** comma (default), tab `[N\\t]`, pipe `[N|]`\n\n**Key folding** (nested objects):\n```\nserver.host: localhost\nserver.port: 8080\n```\n\n## Binary Encoder (20x faster)\n\n```bash\n# Encode JSON to TOON\n.claude/skills/toon-formatter/bin/toon encode data.json\n\n# With options\n.claude/skills/toon-formatter/bin/toon encode data.json --delimiter tab --key-folding\n\n# Check if TOON recommended\n.claude/skills/toon-formatter/bin/toon check data.json\n\n# Decode TOON to JSON\n.claude/skills/toon-formatter/bin/toon decode data.toon\n```\n\n## Commands\n\n- `/toon-encode <file>` - JSON to TOON\n- `/toon-decode <file>` - TOON to JSON  \n- `/toon-validate <file>` - Validate TOON\n- `/analyze-tokens <file>` - Compare savings\n- `/convert-to-toon <file>` - Full conversion workflow\n\n## Documentation\n\n- **Complete Guide:** `docs/toon-guide.md`\n- **Build Instructions:** `docs/INSTALL.md`\n- **Source Code:** `src/toon.zig`\n- **Spec:** https://github.com/toon-format/spec\n",
        "templates/.claude/skills/whop/skill.md": "---\nname: whop-expert\ndescription: Comprehensive Whop platform expert with access to 212 official documentation files covering memberships, payments, products, courses, experiences, forums, webhooks, and app development. Invoke when user mentions Whop, digital products, memberships, course platforms, or community monetization.\nallowed-tools: Read, Write, Edit, Grep, Glob, Bash, WebFetch\nmodel: sonnet\n---\n\n# Whop Platform Expert\n\n## Purpose\n\nProvide comprehensive, accurate guidance for building on the Whop platform based on 212+ official Whop documentation files. Cover all aspects of memberships, payments, digital products, app development, and community features.\n\n## Documentation Coverage\n\n**Full access to official Whop documentation (when available):**\n- **Location:** `docs/whop/`\n- **Files:** 212 markdown files\n- **Coverage:** Complete API reference, guides, integrations, and best practices\n\n**Note:** Documentation must be pulled separately:\n```bash\npipx install docpull\ndocpull https://docs.whop.com -o .claude/skills/whop/docs\n```\n\n**Major Areas:**\n- Products & Plans (membership management)\n- Payments, Refunds & Disputes\n- Memberships & Access Control\n- Courses & Educational Content\n- Experiences & Communities\n- Forums & Chat Channels\n- Apps & Integrations\n- Webhooks & Events\n- Affiliates & Revenue Sharing\n\n## When to Use\n\nInvoke when user mentions:\n- **Platform:** Whop, membership platform, digital products, monetization\n- **Memberships:** subscriptions, access passes, licenses, member management\n- **Products:** digital products, courses, communities, experiences\n- **Courses:** educational content, lessons, chapters, students\n- **Communities:** forums, chat, Discord integration\n- **Payments:** checkout, billing, refunds, disputes, transfers\n- **Apps:** Whop apps, B2B integrations, OAuth, API integrations\n- **Webhooks:** payment events, membership events, notifications\n\n## How to Use Documentation\n\nWhen answering questions:\n\n1. **Search for specific topics:**\n   ```bash\n   # Use Grep to find relevant docs\n   grep -r \"memberships\" .claude/skills/api/whop/docs/ --include=\"*.md\"\n   ```\n\n2. **Read specific API references:**\n   ```bash\n   # API docs are organized by resource\n   cat .claude/skills/api/whop/docs/docs_whop_com/api-reference_memberships_list-memberships.md\n   ```\n\n3. **Find integration guides:**\n   ```bash\n   # Developer guides\n   ls .claude/skills/api/whop/docs/docs_whop_com/developer*\n   ```\n\n## Core Authentication\n\n### API Keys\n\n**Two types of API keys:**\n\n1. **Company API Keys** - Access your own company data\n   - Found in: Dashboard → Settings → API Keys\n   - Use for: Your own payments, memberships, products\n\n2. **App API Keys** - Access data across multiple companies\n   - Found in: Dashboard → Developer → Your App → API Keys\n   - Use for: Multi-tenant apps, B2B integrations\n\n**Authentication:**\n```typescript\n// Server-side only\nconst headers = {\n  'Authorization': `Bearer ${process.env.WHOP_API_KEY}`,\n  'Content-Type': 'application/json',\n};\n\nconst response = await fetch('https://api.whop.com/api/v5/me', {\n  headers,\n});\n```\n\n**Security:**\n- NEVER expose API keys client-side\n- Use environment variables\n- Rotate keys if exposed\n- Use scoped permissions when possible\n\n## Quick Start Integration\n\n### 1. Create a Product\n\n```typescript\nconst product = await fetch('https://api.whop.com/api/v5/products', {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${API_KEY}`,\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    title: 'Premium Membership',\n    description: 'Access to all content',\n    visibility: 'visible',\n  }),\n});\n```\n\n### 2. Create a Plan\n\n```typescript\nconst plan = await fetch('https://api.whop.com/api/v5/plans', {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${API_KEY}`,\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    product_id: 'prod_xxx',\n    billing_period: 1,\n    billing_period_unit: 'month',\n    price: 2999, // $29.99 in cents\n    currency: 'usd',\n  }),\n});\n```\n\n### 3. List Memberships\n\n```typescript\nconst memberships = await fetch(\n  'https://api.whop.com/api/v5/memberships?valid=true',\n  {\n    headers: {\n      'Authorization': `Bearer ${API_KEY}`,\n    },\n  }\n);\n```\n\n## Membership Management\n\n### Check User Access\n\n```typescript\n// Using the Me endpoints (user's access token)\nasync function checkUserAccess(userAccessToken: string) {\n  const response = await fetch('https://api.whop.com/api/v5/me/memberships', {\n    headers: {\n      'Authorization': `Bearer ${userAccessToken}`,\n    },\n  });\n\n  const memberships = await response.json();\n\n  // Check if user has active membership\n  const hasAccess = memberships.data.some(\n    (m: any) => m.status === 'active' && m.valid\n  );\n\n  return hasAccess;\n}\n```\n\n### Validate License Key\n\n```typescript\nasync function validateLicense(licenseKey: string) {\n  const response = await fetch(\n    `https://api.whop.com/api/v5/memberships?license_key=${licenseKey}`,\n    {\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n      },\n    }\n  );\n\n  const data = await response.json();\n  return data.data.length > 0 && data.data[0].valid;\n}\n```\n\n### Cancel Membership\n\n```typescript\nasync function cancelMembership(membershipId: string) {\n  const response = await fetch(\n    `https://api.whop.com/api/v5/memberships/${membershipId}/cancel`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n        'Content-Type': 'application/json',\n      },\n    }\n  );\n\n  return response.json();\n}\n```\n\n## Payment Integration\n\n### Create Checkout Session\n\n```typescript\nasync function createCheckout(planId: string, userId: string) {\n  const response = await fetch(\n    'https://api.whop.com/api/v5/checkout-configurations',\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        plan_id: planId,\n        success_url: 'https://yourapp.com/success',\n        cancel_url: 'https://yourapp.com/cancel',\n        metadata: {\n          user_id: userId,\n        },\n      }),\n    }\n  );\n\n  const checkout = await response.json();\n  return checkout.url; // Redirect user to this URL\n}\n```\n\n### Retrieve Payment\n\n```typescript\nasync function getPayment(paymentId: string) {\n  const response = await fetch(\n    `https://api.whop.com/api/v5/payments/${paymentId}`,\n    {\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n      },\n    }\n  );\n\n  return response.json();\n}\n```\n\n### Refund Payment\n\n```typescript\nasync function refundPayment(paymentId: string, amount?: number) {\n  const response = await fetch(\n    `https://api.whop.com/api/v5/payments/${paymentId}/refund`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        amount, // Optional: partial refund in cents\n      }),\n    }\n  );\n\n  return response.json();\n}\n```\n\n## Course Management\n\n### Create Course\n\n```typescript\nasync function createCourse(productId: string) {\n  const response = await fetch('https://api.whop.com/api/v5/courses', {\n    method: 'POST',\n    headers: {\n      'Authorization': `Bearer ${API_KEY}`,\n      'Content-Type': 'application/json',\n    },\n    body: JSON.stringify({\n      product_id: productId,\n      title: 'Complete Web Development Course',\n      description: 'Learn fullstack development from scratch',\n      visibility: 'visible',\n    }),\n  });\n\n  return response.json();\n}\n```\n\n### Create Chapter\n\n```typescript\nasync function createChapter(courseId: string) {\n  const response = await fetch(\n    'https://api.whop.com/api/v5/course-chapters',\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        course_id: courseId,\n        title: 'Introduction to JavaScript',\n        order: 1,\n      }),\n    }\n  );\n\n  return response.json();\n}\n```\n\n### Create Lesson\n\n```typescript\nasync function createLesson(chapterId: string) {\n  const response = await fetch(\n    'https://api.whop.com/api/v5/course-lessons',\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        chapter_id: chapterId,\n        title: 'Variables and Data Types',\n        content: 'Lesson content in markdown...',\n        type: 'video', // or 'text', 'quiz', 'assignment'\n        video_url: 'https://youtube.com/watch?v=...',\n        order: 1,\n      }),\n    }\n  );\n\n  return response.json();\n}\n```\n\n### Track Lesson Completion\n\n```typescript\nasync function markLessonComplete(lessonId: string, userAccessToken: string) {\n  const response = await fetch(\n    `https://api.whop.com/api/v5/course-lessons/${lessonId}/mark-as-completed`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${userAccessToken}`,\n      },\n    }\n  );\n\n  return response.json();\n}\n```\n\n## Community Features\n\n### Create Forum Post\n\n```typescript\nasync function createForumPost(\n  forumId: string,\n  title: string,\n  content: string,\n  userAccessToken: string\n) {\n  const response = await fetch(\n    'https://api.whop.com/api/v5/forum-posts',\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${userAccessToken}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        forum_id: forumId,\n        title,\n        content,\n      }),\n    }\n  );\n\n  return response.json();\n}\n```\n\n### Create Chat Message\n\n```typescript\nasync function sendChatMessage(\n  channelId: string,\n  content: string,\n  userAccessToken: string\n) {\n  const response = await fetch(\n    'https://api.whop.com/api/v5/messages',\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${userAccessToken}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        channel_id: channelId,\n        content,\n      }),\n    }\n  );\n\n  return response.json();\n}\n```\n\n## Webhook Implementation\n\n### Setup Webhook Endpoint\n\n```typescript\nimport { headers } from 'next/headers';\nimport crypto from 'crypto';\n\nexport async function POST(req: Request) {\n  const body = await req.text();\n  const signature = headers().get('x-whop-signature');\n\n  // Verify webhook signature\n  const webhookSecret = process.env.WHOP_WEBHOOK_SECRET!;\n  const hash = crypto\n    .createHmac('sha256', webhookSecret)\n    .update(body)\n    .digest('hex');\n\n  if (hash !== signature) {\n    return new Response('Invalid signature', { status: 401 });\n  }\n\n  const event = JSON.parse(body);\n\n  // Handle webhook events\n  switch (event.action) {\n    case 'payment.succeeded':\n      await handlePaymentSuccess(event.data);\n      break;\n\n    case 'payment.failed':\n      await handlePaymentFailure(event.data);\n      break;\n\n    case 'membership.activated':\n      await handleMembershipActivated(event.data);\n      break;\n\n    case 'membership.deactivated':\n      await handleMembershipDeactivated(event.data);\n      break;\n\n    case 'dispute.created':\n      await handleDispute(event.data);\n      break;\n\n    default:\n      console.log(`Unhandled event: ${event.action}`);\n  }\n\n  return Response.json({ received: true });\n}\n```\n\n**Critical webhook events:**\n- `payment.succeeded` - Payment completed\n- `payment.failed` - Payment failed\n- `payment.pending` - Payment pending\n- `membership.activated` - Membership started\n- `membership.deactivated` - Membership ended/cancelled\n- `invoice.paid` - Recurring payment succeeded\n- `invoice.past_due` - Payment failed, retry in progress\n- `dispute.created` - Customer disputed payment\n\n## App Development\n\n### OAuth Integration\n\n**1. Redirect user to Whop OAuth:**\n```typescript\nconst authUrl = new URL('https://whop.com/oauth');\nauthUrl.searchParams.set('client_id', process.env.WHOP_CLIENT_ID!);\nauthUrl.searchParams.set('redirect_uri', 'https://yourapp.com/callback');\nauthUrl.searchParams.set('scope', 'memberships:read payments:read');\n\n// Redirect user\nwindow.location.href = authUrl.toString();\n```\n\n**2. Handle callback and exchange code:**\n```typescript\nexport async function GET(req: Request) {\n  const url = new URL(req.url);\n  const code = url.searchParams.get('code');\n\n  // Exchange code for access token\n  const response = await fetch('https://api.whop.com/api/v5/oauth/token', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n    },\n    body: JSON.stringify({\n      client_id: process.env.WHOP_CLIENT_ID!,\n      client_secret: process.env.WHOP_CLIENT_SECRET!,\n      code,\n      grant_type: 'authorization_code',\n      redirect_uri: 'https://yourapp.com/callback',\n    }),\n  });\n\n  const { access_token, refresh_token } = await response.json();\n\n  // Store tokens securely\n  await storeTokens(userId, access_token, refresh_token);\n\n  return Response.redirect('/dashboard');\n}\n```\n\n### Create Notification\n\n```typescript\nasync function sendNotification(userId: string, message: string) {\n  const response = await fetch(\n    'https://api.whop.com/api/v5/notifications',\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        user_id: userId,\n        title: 'New Update',\n        message,\n        type: 'info', // or 'success', 'warning', 'error'\n      }),\n    }\n  );\n\n  return response.json();\n}\n```\n\n## Affiliate System\n\n### Create Promo Code\n\n```typescript\nasync function createPromoCode(\n  productId: string,\n  code: string,\n  discount: number\n) {\n  const response = await fetch(\n    'https://api.whop.com/api/v5/promo-codes',\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        product_id: productId,\n        code,\n        discount_type: 'percentage',\n        discount_value: discount, // e.g., 20 for 20% off\n        max_uses: 100,\n      }),\n    }\n  );\n\n  return response.json();\n}\n```\n\n## Next.js Integration Example\n\n### Middleware for Access Control\n\n```typescript\n// middleware.ts\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\nexport async function middleware(request: NextRequest) {\n  const accessToken = request.cookies.get('whop_user_token')?.value;\n\n  if (!accessToken) {\n    return NextResponse.redirect(new URL('/login', request.url));\n  }\n\n  // Check if user has valid membership\n  const response = await fetch('https://api.whop.com/api/v5/me/memberships', {\n    headers: {\n      'Authorization': `Bearer ${accessToken}`,\n    },\n  });\n\n  const memberships = await response.json();\n  const hasAccess = memberships.data.some(\n    (m: any) => m.status === 'active' && m.valid\n  );\n\n  if (!hasAccess) {\n    return NextResponse.redirect(new URL('/subscribe', request.url));\n  }\n\n  return NextResponse.next();\n}\n\nexport const config = {\n  matcher: ['/dashboard/:path*', '/premium/:path*'],\n};\n```\n\n### Server Action for Checkout\n\n```typescript\n// app/actions/whop.ts\n'use server';\n\nimport { auth } from '@/lib/auth';\n\nexport async function createCheckoutSession(planId: string) {\n  const session = await auth();\n  if (!session?.user?.id) {\n    throw new Error('Unauthorized');\n  }\n\n  const response = await fetch(\n    'https://api.whop.com/api/v5/checkout-configurations',\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${process.env.WHOP_API_KEY}`,\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        plan_id: planId,\n        success_url: `${process.env.NEXT_PUBLIC_URL}/dashboard?success=true`,\n        cancel_url: `${process.env.NEXT_PUBLIC_URL}/pricing?canceled=true`,\n        metadata: {\n          user_id: session.user.id,\n        },\n      }),\n    }\n  );\n\n  const data = await response.json();\n  return { url: data.url };\n}\n```\n\n## Testing\n\n### Test Mode\n\nWhop provides a test/sandbox environment:\n- Use test API keys from Dashboard\n- No real charges\n- Simulate memberships and payments\n\n### Test Webhooks Locally\n\n```bash\n# Use ngrok or similar for local testing\nngrok http 3000\n\n# Configure webhook URL in Whop Dashboard:\n# https://your-ngrok-url.ngrok.io/api/webhooks\n```\n\n## Security Best Practices\n\n1. **API Keys:**\n   - Never expose API keys client-side\n   - Use environment variables\n   - Rotate keys periodically\n   - Use scoped keys when possible\n\n2. **Webhooks:**\n   - ALWAYS verify webhook signatures\n   - Use HTTPS endpoints only\n   - Return 200 immediately, process async\n   - Handle retries gracefully\n\n3. **Access Tokens:**\n   - Store user tokens encrypted\n   - Refresh tokens when expired\n   - Never log sensitive data\n\n4. **Validation:**\n   - Validate membership status server-side\n   - Don't trust client-sent data\n   - Check license keys before granting access\n\n## Common Patterns\n\n### Membership Gating Pattern\n\n```typescript\n// lib/whop/check-access.ts\nexport async function checkMembershipAccess(\n  userId: string,\n  productId: string\n): Promise<boolean> {\n  const response = await fetch(\n    `https://api.whop.com/api/v5/memberships?user_id=${userId}&product_id=${productId}&valid=true`,\n    {\n      headers: {\n        'Authorization': `Bearer ${process.env.WHOP_API_KEY}`,\n      },\n    }\n  );\n\n  const data = await response.json();\n  return data.data.length > 0;\n}\n\n// Use in API routes or Server Components\nexport async function GET(req: Request) {\n  const userId = await getCurrentUserId();\n  const hasAccess = await checkMembershipAccess(userId, 'prod_xxx');\n\n  if (!hasAccess) {\n    return Response.json({ error: 'No access' }, { status: 403 });\n  }\n\n  // Return protected content\n  return Response.json({ data: 'Premium content' });\n}\n```\n\n## TypeScript Types\n\n```typescript\n// types/whop.ts\nexport interface WhopMembership {\n  id: string;\n  user_id: string;\n  product_id: string;\n  plan_id: string;\n  status: 'active' | 'paused' | 'canceled' | 'expired';\n  valid: boolean;\n  license_key: string;\n  quantity: number;\n  renews_at: string | null;\n  expires_at: string | null;\n  cancel_at_period_end: boolean;\n  created_at: string;\n}\n\nexport interface WhopProduct {\n  id: string;\n  title: string;\n  description: string;\n  visibility: 'visible' | 'hidden';\n  created_at: string;\n}\n\nexport interface WhopPayment {\n  id: string;\n  amount: number;\n  currency: string;\n  status: 'succeeded' | 'pending' | 'failed';\n  user_id: string;\n  product_id: string;\n  created_at: string;\n}\n\nexport interface WhopWebhookEvent {\n  action: string;\n  data: any;\n  timestamp: string;\n}\n```\n\n## Documentation Quick Reference\n\n**Need to find something specific?**\n\nUse Grep to search the 212 documentation files:\n\n```bash\n# Search all docs\ngrep -r \"search term\" .claude/skills/api/whop/docs/\n\n# Search API references only\ngrep -r \"memberships\" .claude/skills/api/whop/docs/docs_whop_com/api-reference*\n\n# List all endpoints\nls .claude/skills/api/whop/docs/docs_whop_com/api-reference*/\n```\n\n**Common doc locations:**\n- API Reference: `api-reference_*/`\n- Developer Guides: `developer_*/`\n- Affiliates: `affiliates_*/`\n\n## Resources\n\n- **Dashboard:** https://whop.com/dashboard\n- **Developer Portal:** https://whop.com/dashboard/developer\n- **API Base:** https://api.whop.com/api/v5\n- **Documentation:** https://docs.whop.com\n\n## Implementation Checklist\n\n**Setup:**\n- [ ] Create Whop account and company\n- [ ] Get API keys from Dashboard → Settings → API Keys\n- [ ] Set environment variables (WHOP_API_KEY, WHOP_WEBHOOK_SECRET)\n- [ ] Install fetch or axios for API calls\n\n**Core Features:**\n- [ ] Create products and plans\n- [ ] Implement checkout flow\n- [ ] Add membership access checking\n- [ ] Set up webhook endpoint with signature verification\n- [ ] Handle payment succeeded/failed events\n- [ ] Handle membership activated/deactivated events\n- [ ] Test in development environment\n\n**Advanced (if needed):**\n- [ ] Build Whop App with OAuth\n- [ ] Implement course management\n- [ ] Set up community features (forums, chat)\n- [ ] Configure affiliate system\n- [ ] Add notification system\n- [ ] Implement analytics tracking\n\n## Error Handling\n\n```typescript\nasync function safeWhopRequest(\n  url: string,\n  options: RequestInit\n): Promise<any> {\n  try {\n    const response = await fetch(url, options);\n\n    if (!response.ok) {\n      const error = await response.json();\n      throw new Error(error.message || 'Whop API error');\n    }\n\n    return response.json();\n  } catch (error) {\n    console.error('Whop API error:', error);\n    throw error;\n  }\n}\n```\n\n**Common errors:**\n- `401 Unauthorized` - Invalid or expired API key\n- `403 Forbidden` - Insufficient permissions\n- `404 Not Found` - Resource doesn't exist\n- `429 Too Many Requests` - Rate limit exceeded\n- `500 Internal Server Error` - Whop service issue\n\n## Rate Limiting\n\nWhop implements rate limiting. Best practices:\n- Cache membership status where appropriate\n- Use webhooks instead of polling\n- Implement exponential backoff on failures\n- Batch operations when possible\n"
      },
      "plugins": [
        {
          "name": "ai-skills",
          "source": "./templates/.claude/skills/anthropic",
          "description": "Claude Code expertise: skills, commands, hooks, MCP, settings (7 skills)",
          "category": "ai",
          "categories": [
            "ai"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install ai-skills@claude-starter-complete"
          ]
        },
        {
          "name": "stripe-payments",
          "source": "./templates/.claude/skills/stripe",
          "description": "Complete Stripe API integration (payments, subscriptions, webhooks)",
          "category": "api",
          "categories": [
            "api"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install stripe-payments@claude-starter-complete"
          ]
        },
        {
          "name": "supabase-backend",
          "source": "./templates/.claude/skills/supabase",
          "description": "Supabase backend (PostgreSQL, Auth, Storage, Edge Functions)",
          "category": "backend",
          "categories": [
            "backend"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install supabase-backend@claude-starter-complete"
          ]
        },
        {
          "name": "plaid-banking",
          "source": "./templates/.claude/skills/plaid",
          "description": "Plaid banking API (Auth, Transactions, Identity, Accounts - 5 skills)",
          "category": "api",
          "categories": [
            "api"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install plaid-banking@claude-starter-complete"
          ]
        },
        {
          "name": "expo-mobile",
          "source": "./templates/.claude/skills/expo",
          "description": "Expo/React Native mobile development (EAS Build, Update, Router - 4 skills)",
          "category": "mobile",
          "categories": [
            "mobile"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install expo-mobile@claude-starter-complete"
          ]
        },
        {
          "name": "aptos-blockchain",
          "source": "./templates/.claude/skills/aptos",
          "description": "Aptos blockchain development (Move language, Shelby Protocol, Decibel - 17 skills)",
          "category": "blockchain",
          "categories": [
            "blockchain"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install aptos-blockchain@claude-starter-complete"
          ]
        },
        {
          "name": "shopify-ecommerce",
          "source": "./templates/.claude/skills/shopify",
          "description": "Shopify e-commerce platform integration",
          "category": "ecommerce",
          "categories": [
            "ecommerce"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install shopify-ecommerce@claude-starter-complete"
          ]
        },
        {
          "name": "whop-platform",
          "source": "./templates/.claude/skills/whop",
          "description": "Whop digital products and memberships platform",
          "category": "ecommerce",
          "categories": [
            "ecommerce"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install whop-platform@claude-starter-complete"
          ]
        },
        {
          "name": "ios-development",
          "source": "./templates/.claude/skills/ios",
          "description": "iOS/Swift development",
          "category": "mobile",
          "categories": [
            "mobile"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install ios-development@claude-starter-complete"
          ]
        },
        {
          "name": "toon-formatter",
          "source": "./templates/.claude/skills/toon-formatter",
          "description": "TOON format for 30-60% token savings on tabular data",
          "category": "optimization",
          "categories": [
            "optimization"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install toon-formatter@claude-starter-complete"
          ]
        },
        {
          "name": "meta-commands",
          "source": "./templates/.claude/commands/meta",
          "description": "Meta-commands for creating custom commands from templates",
          "category": "automation",
          "categories": [
            "automation"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install meta-commands@claude-starter-complete"
          ]
        },
        {
          "name": "debug-tools",
          "source": "./templates/.claude/commands/debug",
          "description": "Debugging tools (skill-graph, explain-ranking, workflow-debug, command-validate)",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install debug-tools@claude-starter-complete"
          ]
        },
        {
          "name": "workflows",
          "source": "./templates/.claude/workflows",
          "description": "YAML workflow automation (production-release, ci-pipeline, daily-maintenance, hotfix)",
          "category": "automation",
          "categories": [
            "automation"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install workflows@claude-starter-complete"
          ]
        },
        {
          "name": "orchestration",
          "source": "./templates/.claude/orchestration",
          "description": "Skill orchestration engine for multi-skill collaboration",
          "category": "automation",
          "categories": [
            "automation"
          ],
          "install_commands": [
            "/plugin marketplace add raintree-technology/claude-starter",
            "/plugin install orchestration@claude-starter-complete"
          ]
        }
      ]
    }
  ]
}