{
  "author": {
    "id": "robbyt",
    "display_name": "RT",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/58051?u=ca7429390c41ac5be0fdb0f8309944a64decb063&v=4",
    "url": "https://github.com/robbyt",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 11,
      "total_commands": 0,
      "total_skills": 23,
      "total_stars": 38,
      "total_forks": 2
    }
  },
  "marketplaces": [
    {
      "name": "robbyt-claude-skills",
      "version": null,
      "description": "Collection of Claude Code skills for developer workflows",
      "owner_info": {
        "name": "robbyt",
        "email": "robbyt@robbyt.net"
      },
      "keywords": [],
      "repo_full_name": "robbyt/claude-skills",
      "repo_url": "https://github.com/robbyt/claude-skills",
      "repo_description": "Skills and Plugins for Claude Code",
      "homepage": null,
      "signals": {
        "stars": 38,
        "forks": 2,
        "pushed_at": "2026-01-24T01:17:32Z",
        "created_at": "2025-11-20T20:57:51Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 5605
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-md",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-md/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-md/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 274
        },
        {
          "path": "plugins/claude-md/README.md",
          "type": "blob",
          "size": 1436
        },
        {
          "path": "plugins/claude-md/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-md/skills/condense",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-md/skills/condense/SKILL.md",
          "type": "blob",
          "size": 2861
        },
        {
          "path": "plugins/claude-md/skills/reflect",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-md/skills/reflect/SKILL.md",
          "type": "blob",
          "size": 3033
        },
        {
          "path": "plugins/claude-md/skills/reflect/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-md/skills/reflect/references/anti-patterns.md",
          "type": "blob",
          "size": 3722
        },
        {
          "path": "plugins/claude-md/skills/reflect/references/memory-locations.md",
          "type": "blob",
          "size": 3035
        },
        {
          "path": "plugins/codex",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 386
        },
        {
          "path": "plugins/codex/README.md",
          "type": "blob",
          "size": 1038
        },
        {
          "path": "plugins/codex/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex/skills/codebase-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex/skills/codebase-analysis/SKILL.md",
          "type": "blob",
          "size": 3544
        },
        {
          "path": "plugins/codex/skills/diff-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex/skills/diff-review/SKILL.md",
          "type": "blob",
          "size": 3903
        },
        {
          "path": "plugins/codex/skills/plan-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/codex/skills/plan-review/SKILL.md",
          "type": "blob",
          "size": 4413
        },
        {
          "path": "plugins/gemini",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 390
        },
        {
          "path": "plugins/gemini/README.md",
          "type": "blob",
          "size": 784
        },
        {
          "path": "plugins/gemini/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini/skills/codebase-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini/skills/codebase-analysis/SKILL.md",
          "type": "blob",
          "size": 2924
        },
        {
          "path": "plugins/gemini/skills/diff-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini/skills/diff-review/SKILL.md",
          "type": "blob",
          "size": 3481
        },
        {
          "path": "plugins/gemini/skills/plan-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini/skills/plan-review/SKILL.md",
          "type": "blob",
          "size": 3142
        },
        {
          "path": "plugins/gemini/skills/web-search",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini/skills/web-search/SKILL.md",
          "type": "blob",
          "size": 2146
        },
        {
          "path": "plugins/gh-cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-cli/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-cli/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 358
        },
        {
          "path": "plugins/gh-cli/README.md",
          "type": "blob",
          "size": 2495
        },
        {
          "path": "plugins/gh-cli/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-cli/skills/actions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-cli/skills/actions/SKILL.md",
          "type": "blob",
          "size": 1455
        },
        {
          "path": "plugins/gh-cli/skills/issues",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-cli/skills/issues/SKILL.md",
          "type": "blob",
          "size": 1438
        },
        {
          "path": "plugins/gh-cli/skills/pr",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-cli/skills/pr/SKILL.md",
          "type": "blob",
          "size": 2052
        },
        {
          "path": "plugins/gh-cli/skills/repo",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-cli/skills/repo/SKILL.md",
          "type": "blob",
          "size": 1388
        },
        {
          "path": "plugins/gh-cli/skills/view-file",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gh-cli/skills/view-file/SKILL.md",
          "type": "blob",
          "size": 1564
        },
        {
          "path": "plugins/go-formatter",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/go-formatter/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/go-formatter/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 307
        },
        {
          "path": "plugins/go-formatter/README.md",
          "type": "blob",
          "size": 584
        },
        {
          "path": "plugins/go-formatter/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/go-formatter/hooks/hooks.json",
          "type": "blob",
          "size": 259
        },
        {
          "path": "plugins/go-style-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/go-style-guide/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/go-style-guide/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 276
        },
        {
          "path": "plugins/go-style-guide/README.md",
          "type": "blob",
          "size": 1711
        },
        {
          "path": "plugins/go-style-guide/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/go-style-guide/skills/go-style-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/go-style-guide/skills/go-style-guide/SKILL.md",
          "type": "blob",
          "size": 8648
        },
        {
          "path": "plugins/go-style-guide/skills/go-style-guide/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/go-style-guide/skills/go-style-guide/references/api-design.md",
          "type": "blob",
          "size": 18708
        },
        {
          "path": "plugins/go-style-guide/skills/go-style-guide/references/concurrency.md",
          "type": "blob",
          "size": 9411
        },
        {
          "path": "plugins/go-style-guide/skills/go-style-guide/references/errors.md",
          "type": "blob",
          "size": 7783
        },
        {
          "path": "plugins/go-style-guide/skills/go-style-guide/references/review-checklist.md",
          "type": "blob",
          "size": 18245
        },
        {
          "path": "plugins/go-style-guide/skills/go-style-guide/references/style.md",
          "type": "blob",
          "size": 12752
        },
        {
          "path": "plugins/go-style-guide/skills/go-style-guide/references/testing.md",
          "type": "blob",
          "size": 8834
        },
        {
          "path": "plugins/multimedia",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 282
        },
        {
          "path": "plugins/multimedia/README.md",
          "type": "blob",
          "size": 2423
        },
        {
          "path": "plugins/multimedia/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/skills/artifact-detect",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/skills/artifact-detect/SKILL.md",
          "type": "blob",
          "size": 5900
        },
        {
          "path": "plugins/multimedia/skills/format-explain",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/skills/format-explain/SKILL.md",
          "type": "blob",
          "size": 5807
        },
        {
          "path": "plugins/multimedia/skills/framerate-audit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/skills/framerate-audit/SKILL.md",
          "type": "blob",
          "size": 5061
        },
        {
          "path": "plugins/multimedia/skills/hdr-audit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/skills/hdr-audit/SKILL.md",
          "type": "blob",
          "size": 4872
        },
        {
          "path": "plugins/multimedia/skills/source-compare",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/skills/source-compare/SKILL.md",
          "type": "blob",
          "size": 5134
        },
        {
          "path": "plugins/multimedia/skills/subtitle-audit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/skills/subtitle-audit/SKILL.md",
          "type": "blob",
          "size": 4989
        },
        {
          "path": "plugins/multimedia/skills/telecine-detect",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/skills/telecine-detect/SKILL.md",
          "type": "blob",
          "size": 4788
        },
        {
          "path": "plugins/multimedia/skills/video-audit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/multimedia/skills/video-audit/SKILL.md",
          "type": "blob",
          "size": 4772
        },
        {
          "path": "plugins/python-formatter-black",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-formatter-black/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-formatter-black/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 315
        },
        {
          "path": "plugins/python-formatter-black/README.md",
          "type": "blob",
          "size": 751
        },
        {
          "path": "plugins/python-formatter-black/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-formatter-black/hooks/hooks.json",
          "type": "blob",
          "size": 263
        },
        {
          "path": "plugins/python-formatter-ruff",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-formatter-ruff/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-formatter-ruff/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 312
        },
        {
          "path": "plugins/python-formatter-ruff/README.md",
          "type": "blob",
          "size": 788
        },
        {
          "path": "plugins/python-formatter-ruff/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/python-formatter-ruff/hooks/hooks.json",
          "type": "blob",
          "size": 263
        },
        {
          "path": "plugins/swift-formatter",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/swift-formatter/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/swift-formatter/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 320
        },
        {
          "path": "plugins/swift-formatter/README.md",
          "type": "blob",
          "size": 956
        },
        {
          "path": "plugins/swift-formatter/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/swift-formatter/hooks/hooks.json",
          "type": "blob",
          "size": 262
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"robbyt-claude-skills\",\n  \"owner\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"metadata\": {\n    \"description\": \"Collection of Claude Code skills for developer workflows\",\n    \"version\": \"1.1.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"claude-md\",\n      \"description\": \"Tools for managing CLAUDE.md memory files. Includes reflect skill for analyzing conversation history to improve memory files.\",\n      \"source\": \"./plugins/claude-md\",\n      \"version\": \"2.1.0\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"claude-md\",\n        \"memory\",\n        \"reflection\",\n        \"documentation\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"go-formatter\",\n      \"description\": \"Automatically format Go files with gofmt after Write/Edit/MultiEdit operations\",\n      \"source\": \"./plugins/go-formatter\",\n      \"version\": \"1.0.1\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"go\",\n        \"golang\",\n        \"formatter\",\n        \"gofmt\",\n        \"hooks\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"python-formatter-black\",\n      \"description\": \"Automatically format Python files with Black after Write/Edit/MultiEdit operations\",\n      \"source\": \"./plugins/python-formatter-black\",\n      \"version\": \"1.0.1\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"python\",\n        \"black\",\n        \"formatter\",\n        \"hooks\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"python-formatter-ruff\",\n      \"description\": \"Automatically format Python files with Ruff after Write/Edit/MultiEdit operations\",\n      \"source\": \"./plugins/python-formatter-ruff\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"python\",\n        \"ruff\",\n        \"formatter\",\n        \"hooks\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"swift-formatter\",\n      \"description\": \"Automatically format Swift files with swift-format after Write/Edit/MultiEdit operations\",\n      \"source\": \"./plugins/swift-formatter\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"swift\",\n        \"formatter\",\n        \"swift-format\",\n        \"hooks\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"go-style-guide\",\n      \"description\": \"Review Go code for adherence to Go Style Guide. Use when the user requests a code review of completed work, pull requests, or feature branches in Go projects.\",\n      \"source\": \"./plugins/go-style-guide\",\n      \"version\": \"1.1.0\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"go\",\n        \"golang\",\n        \"style-guide\",\n        \"code-review\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"gemini\",\n      \"description\": \"Gemini CLI integration with focused skills for web search, plan review, code review, and architecture analysis\",\n      \"source\": \"./plugins/gemini\",\n      \"version\": \"2.0.1\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"gemini\",\n        \"ai\",\n        \"code-review\",\n        \"web-search\",\n        \"architecture\",\n        \"plan-review\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"gh-cli\",\n      \"description\": \"GitHub CLI integration with focused skills for pull requests, issues, GitHub Actions, and viewing GitHub file URLs\",\n      \"source\": \"./plugins/gh-cli\",\n      \"version\": \"2.0.0\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"github\",\n        \"gh\",\n        \"cli\",\n        \"pull-requests\",\n        \"issues\",\n        \"actions\",\n        \"api\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"codex\",\n      \"description\": \"OpenAI Codex CLI integration with MCP server and skills for web search, plan review, code review, and codebase analysis\",\n      \"source\": \"./plugins/codex\",\n      \"version\": \"1.2.2\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"codex\",\n        \"openai\",\n        \"ai\",\n        \"code-review\",\n        \"web-search\",\n        \"codebase-analysis\",\n        \"plan-review\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"apple-dev-docs\",\n      \"description\": \"On-demand Apple Developer Documentation access via JSON-RPC. Search docs, browse frameworks, access WWDC videos with bundled offline content.\",\n      \"source\": \"./plugins/apple-dev-docs\",\n      \"version\": \"0.1.0\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"apple\",\n        \"ios\",\n        \"macos\",\n        \"swiftui\",\n        \"uikit\",\n        \"swift\",\n        \"wwdc\",\n        \"documentation\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"multimedia\",\n      \"description\": \"Video and audio file analysis, quality auditing, and encoding guidance. Includes skills for video audit, artifact detection, telecine detection, HDR validation, source comparison, and subtitle analysis.\",\n      \"source\": \"./plugins/multimedia\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"robbyt\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"video\",\n        \"audio\",\n        \"encoding\",\n        \"ffmpeg\",\n        \"mediainfo\",\n        \"quality\",\n        \"hdr\",\n        \"telecine\",\n        \"subtitles\"\n      ],\n      \"strict\": true\n    }\n  ]\n}\n",
        "plugins/claude-md/.claude-plugin/plugin.json": "{\n  \"name\": \"claude-md\",\n  \"description\": \"Tools for managing CLAUDE.md memory files\",\n  \"version\": \"2.1.0\",\n  \"author\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"claude-md\", \"memory\", \"reflection\", \"documentation\"]\n}\n",
        "plugins/claude-md/README.md": "# claude-md\n\nTools for managing CLAUDE.md memory files.\n\n## Skills\n\n### reflect\n\nAnalyzes recent conversation history to identify improvements for CLAUDE.md memory files.\n\n**Trigger phrases:**\n- \"reflect on this session\"\n- \"reflect on this conversation\"\n- \"reflect on this code\"\n- \"improve CLAUDE.md by reflecting on this session\"\n\n**What it does:**\n1. Finds all CLAUDE.md files in the project\n2. Analyzes conversation for repeated corrections, misunderstandings, or missing context\n3. Proposes specific changes with user approval\n4. Updates the appropriate memory file\n\n### condense\n\nDeduplicates and consolidates CLAUDE.md memory files to remove redundancy.\n\n**Trigger phrases:**\n- \"condense my CLAUDE.md files\"\n- \"deduplicate CLAUDE.md\"\n- \"clean up my memory files\"\n- \"consolidate my instructions\"\n\n**What it does:**\n1. Finds all CLAUDE.md files in the project\n2. Identifies duplication within and across files\n3. Detects misplaced instructions (subdirectory content that belongs in root or vice versa)\n4. Proposes consolidation with user approval\n5. Removes duplicates and reorganizes content\n\n## Resources\n\n- `skills/reflect/references/memory-locations.md` - Memory file hierarchy and placement guide\n- `skills/reflect/references/anti-patterns.md` - Common mistakes to avoid\n- `skills/reflect/scripts/find_claude_md.py` - Locate all CLAUDE.md files in a directory tree\n\n## Requirements\n\n- Python 3.x (for find_claude_md.py script)\n",
        "plugins/claude-md/skills/condense/SKILL.md": "---\nname: condense\ndescription: Deduplicate and consolidate CLAUDE.md memory files. Trigger when user says \"condense my CLAUDE.md files\", \"deduplicate CLAUDE.md\", \"clean up my memory files\", or \"consolidate my instructions\". Removes redundancy within files and across the file hierarchy.\n---\n\n# CLAUDE.md Condensation\n\nDeduplicate and consolidate CLAUDE.md memory files to remove redundancy.\n\n## Workflow\n\n### Phase 1: Discovery\n\n**Find all CLAUDE.md files:**\n```bash\npython ../reflect/scripts/find_claude_md.py\n```\n\n**Read all discovered files and analyze for:**\n1. Intra-file duplication (same instruction repeated within a file)\n2. Cross-file duplication (same instruction in multiple files)\n3. Misplaced instructions (subdirectory files containing project-wide content)\n\n### Phase 2: Analysis\n\n**Intra-file duplication:**\n- Identify repeated bullet points or instructions\n- Find semantically similar content (different wording, same meaning)\n\n**Cross-file duplication:**\n- Root CLAUDE.md should contain project-wide instructions\n- Subdirectory CLAUDE.md should only contain directory-specific instructions\n- If an instruction appears in both root and subdirectory, keep only in root\n- If an instruction in subdirectory applies to whole project, move to root\n\n**Misplaced instructions:**\n- Subdirectory file contains instructions that apply project-wide → move to root\n- Root file contains instructions only relevant to one directory → move to subdirectory\n\n### Phase 3: Interaction\n\nPresent findings using AskUserQuestion with checkboxes.\n\n**For each issue found:**\n1. Show the duplicated or misplaced content\n2. Identify which files are affected\n3. Propose the consolidation (delete, move, or merge)\n\n**Example:**\n```\nIssue: \"Use 2-space indentation\" appears in both ./CLAUDE.md and ./src/CLAUDE.md\nProposal: Remove from ./src/CLAUDE.md (already covered by root)\n```\n\nWait for user approval before implementing.\n\n### Phase 4: Implementation\n\nFor approved changes:\n\n1. **Remove duplicates** - Delete redundant entries, keeping the most appropriate location\n2. **Move misplaced content** - Transfer instructions to correct hierarchy level\n3. **Merge similar items** - Combine semantically similar instructions into one\n\n**Hierarchy rules:**\n- `./CLAUDE.md` - Project-wide instructions (highest priority)\n- `./.claude/rules/*.md` - Topic-specific rules (modular)\n- `./subdir/CLAUDE.md` - Only instructions specific to that subdirectory\n- `~/.claude/CLAUDE.md` - Personal preferences across all projects\n- `./CLAUDE.local.md` - Personal project-specific (not shared)\n\n## Resources\n\nUses shared resources from the reflect skill:\n- `../reflect/scripts/find_claude_md.py` - Locate all CLAUDE.md files\n- `../reflect/references/memory-locations.md` - Memory hierarchy details\n- `../reflect/references/anti-patterns.md` - What to avoid when writing instructions\n",
        "plugins/claude-md/skills/reflect/SKILL.md": "---\nname: reflect\ndescription: Analyze recent conversation to identify improvements for CLAUDE.md memory files. Trigger when user says \"reflect on this session\", \"reflect on this conversation\", \"reflect on this code\", or \"improve CLAUDE.md by reflecting on this session\". The verb \"reflect\" combined with a session/conversation/code subject is the key trigger.\n---\n\n# CLAUDE.md Reflection\n\nAnalyze recent conversation history to identify improvements for CLAUDE.md memory files.\n\n## Workflow\n\n### Phase 1: Analysis\n\n**Find all CLAUDE.md files:**\n```bash\npython scripts/find_claude_md.py\n```\n\n**Analyze conversation for:**\n- Repeated corrections from the user\n- Misunderstandings of user requests\n- Missing context that caused confusion\n- Inconsistencies between responses and user expectations\n- Instructions that belong in a different memory file\n\n**Read reference materials:**\n- `references/memory-locations.md` - Memory file hierarchy and placement\n- `references/anti-patterns.md` - Common mistakes to avoid\n\n### Phase 2: Interaction\n\nPresent findings using AskUserQuestion with checkboxes.\n\n**For each suggestion:**\n1. Explain the issue found in conversation\n2. Propose specific change (add, update, or delete)\n3. Identify which memory file to update\n4. Show the exact text to add/modify\n\n**Example:**\n```\nIssue: Repeatedly used 'var' instead of 'const' in JavaScript\nProposal: Add \"NEVER use var in JavaScript, use const or let\"\nFile: ./frontend/CLAUDE.md\n```\n\nWait for user approval before implementing.\n\n### Phase 3: Implementation\n\nFor approved changes:\n\n1. **Select correct memory file:**\n   - `./CLAUDE.md` - Project-wide, shared with team\n   - `./.claude/CLAUDE.md` - Alternative project location\n   - `./.claude/rules/*.md` - Modular topic-specific rules\n   - `~/.claude/CLAUDE.md` - Personal preferences (all projects)\n   - `./CLAUDE.local.md` - Personal project-specific (gitignored)\n\n2. **Apply changes:**\n   - Keep minimal and targeted\n   - Use bullet points, not paragraphs\n   - Be specific (\"Use 2-space indentation\" not \"Format code properly\")\n   - Add emphasis (NEVER, ALWAYS) only where critical\n   - Maintain existing structure\n\n3. **Avoid anti-patterns:**\n   - NO hyperbolic language (crucial, proper, robust)\n   - NO vague instructions (write good code, use best practices)\n   - NO historical comments (we used to do X)\n   - NO redundant information\n\n## Memory File Hierarchy\n\n| Type | Location | Purpose | Shared |\n|------|----------|---------|--------|\n| Project | `./CLAUDE.md` | Team instructions | Yes (git) |\n| Rules | `./.claude/rules/*.md` | Modular topic rules | Yes (git) |\n| User | `~/.claude/CLAUDE.md` | Personal preferences | No |\n| Local | `./CLAUDE.local.md` | Personal project prefs | No (gitignored) |\n\nFiles higher in hierarchy take precedence. Use `@path/to/file` syntax to import files.\n\n## Resources\n\n- `scripts/find_claude_md.py` - Locate all CLAUDE.md files\n- `references/memory-locations.md` - Detailed memory hierarchy docs\n- `references/anti-patterns.md` - Common mistakes to avoid\n",
        "plugins/claude-md/skills/reflect/references/anti-patterns.md": "# CLAUDE.md Anti-Patterns\n\nCommon mistakes to avoid when writing or improving CLAUDE.md files. You should AVOID the following:\n\n## Vague or Subjective Instructions\n\n**Bad:**\n- Write proper code\n- Use best practices\n- Make it simple\n- Write robust error handling\n- Create clean functions\n\n**Why:** These are subjective and provide no actionable guidance.\n\n**Good:**\n- Use 2-space indentation\n- Prefix test files with `test_`\n- Return errors instead of panicking\n- Functions should not exceed 50 lines\n\n## Hyperbolic or Fluff Language\n\n**Bad:**\n- The API client plays a crucial role in our architecture\n- It's very important to follow these guidelines\n- Make sure to write excellent documentation\n- Always use proper naming conventions\n\n**Why:** Words like \"crucial\", \"very important\", \"excellent\", \"proper\" add noise without meaning.\n\n**Good:**\n- Use the API client in `pkg/client` for all external requests\n- Document all exported functions\n- Use snake_case for variable names\n\n## Long Narrative Paragraphs\n\n**Bad:**\n```\nWhen you're working on this project, you should be aware that we have a\nspecific way of handling errors. In the past, we used to just return nil\nand log errors, but we've since moved to a better approach where we return\nthe actual error values so that callers can handle them appropriately.\n```\n\n**Good:**\n```\n# Error handling\n- Return errors to callers, don't just log them\n- Use errors.Join for multiple errors\n```\n\n## Redundant Information\n\n**Bad:**\n```\n# Build commands\n- npm run build: This command builds the project\n- npm run test: This command runs the tests\n- npm run lint: This command runs the linter\n\n# Building\nTo build the project, run `npm run build`\n```\n\n**Why:** Information appears multiple times.\n\n**Good:**\n```\n# Commands\n- npm run build: Build the project\n- npm run test: Run test suite\n- npm run lint: Check code style\n```\n\n## Outdated Instructions\n\n**Bad:**\n```\n# Database\n- Use MySQL 5.7\n- Run migrations with migrate.sh (NOTE: This script was removed, use db-migrate now)\n```\n\n**Why:** Contains outdated information and historical comments.\n\n**Good:**\n```\n# Database\n- Use PostgreSQL 15+\n- Run migrations with `npm run db:migrate`\n```\n\n## Unclear Scope\n\n**Bad:**\n```\nDon't use global variables\n```\n\n**Why:** Unclear if this applies to all files, tests, or specific contexts.\n\n**Good:**\n```\n# Code style\n- Avoid global variables in src/ (tests can use them for fixtures)\n```\n\n## Missing Context for Commands\n\n**Bad:**\n```\nRun: make proto\n```\n\n**Why:** No explanation of when or why to run this.\n\n**Good:**\n```\n# After editing .proto files\n- make proto: Regenerate protobuf code\n```\n\n## Over-Capitalization for Emphasis\n\n**Bad:**\n```\nNEVER EVER USE VAR IN JAVASCRIPT CODE!!!\nYOU MUST ALWAYS USE CONST OR LET!!!\n```\n\n**Why:** Excessive caps and punctuation create noise.\n\n**Good:**\n```\n# JavaScript style\n- NEVER use var, use const or let\n```\n\n## Instructions That Should Be Code\n\n**Bad:**\n```\nWhen rotating PDFs, make sure to:\n1. Load the PDF using pdfplumber\n2. Iterate through each page\n3. Rotate each page by the specified degrees\n4. Save the output\n```\n\n**Why:** Repetitive procedural code should be in a script, not instructions.\n\n**Good:**\n```\n# PDF manipulation\n- Rotate PDFs: python scripts/rotate_pdf.py <input> <degrees> <output>\n```\n\n## Mixing Different Concerns\n\n**Bad:**\n```\n# Instructions\n- Use ES modules\n- The build takes about 5 minutes\n- John prefers arrow functions\n- Run tests before committing\n- I fixed a bug last week where...\n```\n\n**Why:** Mixes style guidelines, historical notes, timing info, and personal preferences.\n\n**Good:**\n```\n# Code style\n- Use ES modules\n- Prefer arrow functions\n\n# Development workflow\n- Run `npm test` before committing\n```\n",
        "plugins/claude-md/skills/reflect/references/memory-locations.md": "# Memory File Locations\n\nReference: https://code.claude.com/docs/en/memory\n\n## Memory Types\n\n| Type | Location | Purpose | Shared |\n|------|----------|---------|--------|\n| Enterprise policy | macOS: `/Library/Application Support/ClaudeCode/CLAUDE.md`<br/>Linux: `/etc/claude-code/CLAUDE.md` | Organization-wide instructions | All users in org |\n| Project memory | `./CLAUDE.md` or `./.claude/CLAUDE.md` | Team-shared project instructions | Team (git) |\n| Project rules | `./.claude/rules/*.md` | Modular topic-specific rules | Team (git) |\n| User memory | `~/.claude/CLAUDE.md` | Personal preferences (all projects) | Just you |\n| Project local | `./CLAUDE.local.md` | Personal project-specific prefs | Just you (gitignored) |\n\nFiles higher in hierarchy take precedence and load first.\n\n## File Lookup\n\nMemory files are read recursively from the current working directory up to (but not including) the root directory. Useful for monorepos where you run from `foo/bar/` and have memories in both `foo/CLAUDE.md` and `foo/bar/CLAUDE.md`.\n\nNested CLAUDE.md files in subtrees are discovered but only loaded when files in those subtrees are read.\n\n## Imports\n\nCLAUDE.md files can import additional files using `@path/to/import` syntax:\n\n```\nSee @README for project overview and @package.json for available npm commands.\n\n# Additional Instructions\n- git workflow @docs/git-instructions.md\n- @~/.claude/my-project-instructions.md\n```\n\n- Both relative and absolute paths allowed\n- Imports not evaluated inside code spans or blocks\n- Max depth: 5 hops\n\n## Modular Rules (.claude/rules/)\n\nOrganize instructions into multiple files:\n\n```\nyour-project/\n├── .claude/\n│   ├── CLAUDE.md\n│   └── rules/\n│       ├── code-style.md\n│       ├── testing.md\n│       └── security.md\n```\n\nAll `.md` files in `.claude/rules/` are loaded as project memory.\n\n### Path-Specific Rules\n\nUse YAML frontmatter to scope rules to specific files:\n\n```markdown\n---\npaths: src/api/**/*.ts\n---\n\n# API Development Rules\n- All API endpoints must include input validation\n```\n\nRules without `paths` apply to all files.\n\n### Glob Patterns\n\n| Pattern | Matches |\n|---------|---------|\n| `**/*.ts` | All TypeScript files |\n| `src/**/*` | All files under src/ |\n| `*.md` | Markdown files in project root |\n| `src/components/*.tsx` | React components in specific dir |\n\nUse braces for multiple patterns: `src/**/*.{ts,tsx}` or `{src,lib}/**/*.ts`\n\n## Quick Memory Commands\n\n- `#` prefix: Add quick memory (prompts for file selection)\n- `/memory`: Open memory file in editor\n- `/init`: Bootstrap CLAUDE.md for codebase\n\n## Placement Guidelines\n\n**Root CLAUDE.md:**\n- Project-wide guidelines\n- Common commands and workflows\n- Links to subdirectory files\n\n**Subdirectory CLAUDE.md:**\n- Component-specific guidance\n- Referenced from root CLAUDE.md\n\n**~/.claude/CLAUDE.md:**\n- Personal preferences\n- Tooling shortcuts\n- Cross-project defaults\n\n**CLAUDE.local.md:**\n- Machine-specific config\n- Personal sandbox URLs\n- Test data preferences\n",
        "plugins/codex/.claude-plugin/plugin.json": "{\n  \"name\": \"codex\",\n  \"description\": \"OpenAI Codex CLI integration with MCP server and skills for plan review, code review, and codebase analysis\",\n  \"version\": \"1.2.2\",\n  \"author\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"codex\",\n    \"openai\",\n    \"ai\",\n    \"code-review\",\n    \"codebase-analysis\",\n    \"plan-review\"\n  ]\n}\n",
        "plugins/codex/README.md": "# Codex CLI Plugin\n\nOpenAI Codex CLI integration for code review, plan review, and codebase analysis.\n\n## MCP Server\n\nThis plugin includes an MCP server that starts automatically when the plugin is enabled. The server provides `codex` and `codex-reply` tools for native integration. Skills prefer MCP when available and fall back to Bash commands.\n\n## Skills\n\n### `codex:diff-review`\n\nCode review of git changes. Uses `codex review` or manual diff review.\n\n**Triggers:** \"have Codex review my changes\", \"get code review from Codex\", \"review this diff with Codex\"\n\n### `codex:plan-review`\n\nReview and critique implementation plans before execution.\n\n**Triggers:** \"have Codex review this plan\", \"get second opinion from Codex\", \"critique this plan with Codex\"\n\n### `codex:codebase-analysis`\n\nCodebase and architecture analysis with read-only sandbox.\n\n**Triggers:** \"analyze this codebase with Codex\", \"have Codex map dependencies\"\n\n## Setup\n\nCodex CLI must be pre-configured with API keys or OAuth. See `references/setup.md` for details.\n",
        "plugins/codex/skills/codebase-analysis/SKILL.md": "---\nname: codebase-analysis\ndescription: Codebase analysis using Codex CLI with read-only sandbox. Trigger when user needs architecture overview (\"analyze this codebase with Codex\", \"have Codex map dependencies\"), onboarding to unfamiliar code, understanding legacy systems, or identifying technical debt.\n---\n\n# Codebase Analysis via Codex\n\nUse Codex for codebase analysis with read-only sandbox.\n\n## CRITICAL: Default Model\n\n**ALWAYS use `model: \"gpt-5.2\"`** unless the user explicitly requests a different model. Do NOT choose `o3` or other models on your own.\n\n## CRITICAL: Instruct Codex\n\nEvery prompt sent to Codex MUST include these instructions:\n\n> \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete analysis immediately.\"\n\nCodex is a consultant. Claude Code handles all file modifications.\n\n## Quick Start (MCP)\n\nIf the `codex` MCP tool is available, use it directly:\n\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete analysis immediately.\\n\\nAnalyze this project structure and architecture.\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n## Fallback (Bash)\n\nIf MCP is unavailable, use shell command:\n\n```bash\ncodex exec \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete analysis immediately.\n\nAnalyze this project structure and architecture.\" --sandbox read-only -m gpt-5.2-codex 2>&1\n```\n\n## When to Use\n\n- Onboarding to unfamiliar codebases\n- Understanding legacy systems\n- Mapping component relationships\n- Finding hidden dependencies\n- Architecture documentation\n- Technical debt assessment\n\n## Examples\n\n**Full project analysis:**\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete analysis immediately.\\n\\nAnalyze this project. Report on:\\n- Overall architecture\\n- Key dependencies\\n- Component relationships\\n- Potential issues\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n**Flow mapping:**\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete analysis immediately.\\n\\nMap the authentication flow in this codebase. Identify all components involved.\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n**Dependency analysis:**\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete analysis immediately.\\n\\nAnalyze dependencies in this project:\\n- Direct vs transitive\\n- Outdated packages\\n- Circular dependencies\\n- Bundle size impact\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n## Performance\n\n- MCP simple analysis: ~5-30 seconds\n- MCP with many files: ~1-2 minutes\n- Bash fallback: ~2-3 minutes\n\n## Notes\n\n- **Always use `sandbox: \"read-only\"`** to prevent file modifications\n- **NEVER use `sandbox: \"danger-full-access\"`** - this is forbidden\n- Tool name may vary by installation. Check available tools for exact name.\n- MCP is preferred; Bash fallback requires `dangerouslyDisableSandbox: true`\n- See `references/setup.md` for troubleshooting\n",
        "plugins/codex/skills/diff-review/SKILL.md": "---\nname: diff-review\ndescription: Get Codex's code review of git changes after Claude makes edits. Trigger when user wants a second opinion on code changes (\"have Codex review my changes\", \"get code review from Codex\", \"review this diff with Codex\"), or as a final check before committing.\n---\n\n# Diff Review via Codex\n\nHave Codex review git changes for a second perspective on code quality.\n\n## CRITICAL: Default Model\n\n**ALWAYS use `model: \"gpt-5.2\"`** unless the user explicitly requests a different model. Do NOT choose `o3` or other models on your own.\n\n## CRITICAL: Instruct Codex\n\nEvery prompt sent to Codex MUST include these instructions:\n\n> \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\"\n\nCodex is a consultant. Claude Code handles all file modifications.\n\n## Quick Start (MCP)\n\nIf the `codex` MCP tool is available, first save the diff then review:\n\n```bash\ngit diff --cached > codex-review.diff\n```\n\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\\n\\nReview the code changes at codex-review.diff for bugs, security issues, and style problems.\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n```bash\nrm codex-review.diff\n```\n\n## Fallback (Bash)\n\nIf MCP is unavailable, use shell commands:\n\n```bash\ngit diff --cached > codex-review.diff\ncodex exec \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\n\nReview the code changes at codex-review.diff for issues.\" --sandbox read-only -m gpt-5.2-codex 2>&1\nrm codex-review.diff\n```\n\nOr use the built-in review command:\n\n```bash\ncodex review --uncommitted 2>&1\n```\n\nNote: The review command is scoped to diffs and doesn't support `--sandbox`.\n\n## Patterns\n\n**Staged changes:**\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\\n\\nReview codex-review.diff for:\\n1. Bugs or logic errors\\n2. Security vulnerabilities\\n3. Style inconsistencies\\n4. Missing error handling\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n**Security focus:**\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\\n\\nSecurity review of codex-review.diff. Check for:\\n- XSS vulnerabilities\\n- SQL/command injection\\n- Sensitive data exposure\\n- Authentication/authorization issues\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n**Performance focus:**\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\\n\\nPerformance review of codex-review.diff. Check for:\\n- Inefficient algorithms\\n- N+1 queries\\n- Memory leaks\\n- Blocking operations\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n## Performance\n\n- MCP diff review: ~5-30 seconds\n- MCP with source context: ~1-2 minutes\n- Bash fallback: ~2-3 minutes\n\n## Notes\n\n- **Always use `sandbox: \"read-only\"`** to prevent file modifications\n- **NEVER use `sandbox: \"danger-full-access\"`** - this is forbidden\n- Tool name may vary by installation. Check available tools for exact name.\n- Save diff to project root before review (Codex can read project files)\n- Clean up diff file after review\n- MCP is preferred; Bash fallback requires `dangerouslyDisableSandbox: true`\n- See `references/setup.md` for troubleshooting\n",
        "plugins/codex/skills/plan-review/SKILL.md": "---\nname: plan-review\ndescription: Get Codex's review of Claude's implementation plans. Trigger when user wants a second opinion on a plan (\"have Codex review this plan\", \"get second opinion from Codex\", \"critique this plan with Codex\"), or after Claude creates a plan file that needs validation before implementation.\n---\n\n# Plan Review via Codex\n\nHave Codex critique Claude's implementation plans for a second perspective.\n\n## CRITICAL: Default Model\n\n**ALWAYS use `model: \"gpt-5.2\"`** unless the user explicitly requests a different model. Do NOT choose `o3` or other models on your own.\n\n## CRITICAL: Instruct Codex\n\nEvery prompt sent to Codex MUST include these instructions:\n\n> \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\"\n\nCodex is a consultant. Claude Code handles all file modifications.\n\n## Quick Start (MCP)\n\nIf the `codex` MCP tool is available, read the plan and pass it to Codex:\n\nFirst, read the plan file content, then:\n\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\\n\\nReview this implementation plan:\\n\\n[PLAN CONTENT HERE]\\n\\nConsider:\\n1. Are there gaps or missing steps?\\n2. Are there risks not addressed?\\n3. Is the approach optimal?\\n4. What alternatives should be considered?\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n## Fallback (Bash)\n\nIf MCP is unavailable, tell Codex to read the file directly:\n\n```bash\ncodex exec \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\n\nReview the implementation plan at path/to/plan.md\n\nConsider:\n1. Are there gaps or missing steps?\n2. Are there risks not addressed?\n3. Is the approach optimal?\" --sandbox read-only -m gpt-5.2-codex 2>&1\n```\n\n**Note:** Do NOT use stdin piping with `$(cat)` - Codex doesn't expand shell command substitution. Instead, provide file paths in the prompt and let Codex read them directly.\n\n## With Source Context\n\nInclude source files for context in the prompt:\n\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\\n\\nReview this implementation plan:\\n\\n[PLAN CONTENT]\\n\\nAlso read these source files for context:\\n- src/auth/login.ts\\n- src/middleware/session.ts\\n\\nEvaluate if the plan addresses the actual codebase structure.\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n## Focused Reviews\n\n**Risk assessment:**\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\\n\\nReview this plan for risks:\\n\\n[PLAN CONTENT]\\n\\nEvaluate:\\n- Breaking changes\\n- Data loss potential\\n- Rollback complexity\\n- Dependencies that could fail\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n**Completeness check:**\n```\nmcp__plugin_codex_cli__codex({\n  \"prompt\": \"You are running non-interactively as part of a script. Do not ask questions or wait for input. Do not make any changes. Provide your complete feedback immediately.\\n\\nReview this plan for completeness:\\n\\n[PLAN CONTENT]\\n\\nEvaluate:\\n- Are all edge cases covered?\\n- Is testing addressed?\\n- Are there missing steps?\",\n  \"sandbox\": \"read-only\",\n  \"model\": \"gpt-5.2\"\n})\n```\n\n## Recommended Pattern\n\n1. Use Claude's Read tool to get plan file content\n2. Embed content directly in the Codex prompt\n3. List additional source files for Codex to read from project\n\n## Performance\n\n- MCP plan review: ~5-30 seconds\n- MCP with source files: ~1-2 minutes\n- Bash fallback: ~2-3 minutes\n\n## Notes\n\n- **Always use `sandbox: \"read-only\"`** to prevent file modifications\n- **NEVER use `sandbox: \"danger-full-access\"`** - this is forbidden\n- Tool name may vary by installation. Check available tools for exact name.\n- Read plan file content first, then include in prompt (Codex may not access ~/.claude/plans/)\n- MCP is preferred; Bash fallback requires `dangerouslyDisableSandbox: true`\n- See `references/setup.md` for troubleshooting\n",
        "plugins/gemini/.claude-plugin/plugin.json": "{\n  \"name\": \"gemini\",\n  \"description\": \"Gemini CLI integration with focused skills for web search, plan review, code review, and architecture analysis\",\n  \"version\": \"2.1.3\",\n  \"author\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"gemini\",\n    \"ai\",\n    \"code-review\",\n    \"web-search\",\n    \"architecture\",\n    \"plan-review\"\n  ]\n}\n",
        "plugins/gemini/README.md": "# Gemini CLI Plugin\n\n## Skills\n\n### `gemini:web-search`\n\nReal-time web research using Gemini's Google Search integration.\n\n**Triggers:** \"search with Gemini\", \"find current info about X\", \"what's the latest on Y\"\n\n### `gemini:diff-review`\n\nCode review of git changes for a second perspective.\n\n**Triggers:** \"have Gemini review my changes\", \"get code review from Gemini\", \"review this diff\"\n\n### `gemini:plan-review`\n\nReview and critique implementation plans.\n\n**Triggers:** \"have Gemini review this plan\", \"get second opinion\", \"critique this plan\"\n\n### `gemini:codebase-analysis`\n\nArchitectural analysis using Gemini's `codebase_investigator` tool.\n\n**Triggers:** \"analyze this codebase\", \"map dependencies\"\n\n## Setup\n\nSee `references/setup.md` for installation and authentication.\n",
        "plugins/gemini/skills/codebase-analysis/SKILL.md": "---\nname: codebase-analysis\ndescription: Deep architectural analysis using Gemini's codebase_investigator tool. Trigger when user needs architecture overview (\"analyze this codebase\", \"map dependencies\"), onboarding to unfamiliar code, understanding legacy systems, or identifying technical debt.\n---\n\n# Codebase Analysis via Gemini\n\nUse Gemini's `codebase_investigator` tool for deep architectural analysis.\n\n## Quick Start\n\n```bash\ngemini \"Use codebase_investigator to analyze this project. Do not make any changes. Respond with analysis only.\" --allowed-tools codebase_investigator -o text 2>&1\n```\n\n## When to Use\n\n- Onboarding to unfamiliar codebases\n- Understanding legacy systems\n- Mapping component relationships\n- Finding hidden dependencies\n- Architecture documentation\n- Technical debt assessment\n\n## Examples\n\n**Full project analysis:**\n```bash\ngemini \"Use codebase_investigator to analyze this project. Report on:\n- Overall architecture\n- Key dependencies\n- Component relationships\n- Potential issues\nDo not make any changes. Respond with analysis only.\" --allowed-tools codebase_investigator -o text\n```\n\n**Flow mapping:**\n```bash\ngemini \"Use codebase_investigator to map the authentication flow. Identify all components involved. Do not make any changes. Respond with analysis only.\" --allowed-tools codebase_investigator -o text\n```\n\n**Dependency analysis:**\n```bash\ngemini \"Use codebase_investigator to analyze dependencies:\n- Direct vs transitive\n- Outdated packages\n- Circular dependencies\n- Bundle size impact\nDo not make any changes. Respond with analysis only.\" --allowed-tools codebase_investigator -o text\n```\n\n**Technical debt:**\n```bash\ngemini \"Use codebase_investigator to identify technical debt:\n- Deprecated patterns\n- Inconsistent conventions\n- Missing documentation\n- Complex dependency chains\nDo not make any changes. Respond with analysis only.\" --allowed-tools codebase_investigator -o text\n```\n\n## Iterative Analysis\n\nUse sessions for multi-turn investigation:\n\n```bash\n# Initial analysis\ngemini \"Use codebase_investigator to analyze this project. Do not make any changes. Respond with analysis only.\" --allowed-tools codebase_investigator -o text\n\n# Follow-up (continues session)\necho \"What patterns did you find in the auth module? Do not make any changes. Respond with analysis only.\" | gemini --allowed-tools codebase_investigator -r 1 -o text\n\n# Deeper dive\necho \"Are there security concerns with that pattern? Do not make any changes. Respond with analysis only.\" | gemini --allowed-tools codebase_investigator -r 1 -o text\n```\n\n## Notes\n\n- **Gemini must not make any changes, provide feedback ONLY.**\n- Gemini respects `.gitignore` - it cannot read files matching gitignore patterns\n- Can take 5-10 minutes for large codebases\n- Requires sandbox bypass: use `dangerouslyDisableSandbox: true`\n- Use sessions for iterative exploration\n- See `references/setup.md` for troubleshooting\n",
        "plugins/gemini/skills/diff-review/SKILL.md": "---\nname: diff-review\ndescription: Get Gemini's code review of git changes after Claude makes edits. Trigger when user wants a second opinion on code changes (\"have Gemini review my changes\", \"get code review from Gemini\", \"review this diff\"), or as a final check before committing.\n---\n\n# Diff Review via Gemini\n\nHave Gemini review git changes for a second perspective on code quality.\n\n## Quick Start\n\nSave diff to project root, have Gemini review, then clean up:\n\n```bash\ngit diff --cached > gemini-review.diff\ngemini \"Review the code changes at gemini-review.diff for issues. Do not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\nrm gemini-review.diff\n```\n\n## Patterns\n\n**Staged changes:**\n```bash\ngit diff --cached > gemini-review.diff\ngemini \"Review gemini-review.diff for:\n1. Bugs or logic errors\n2. Security vulnerabilities\n3. Style inconsistencies\n4. Missing error handling\nDo not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\nrm gemini-review.diff\n```\n\n**All uncommitted changes:**\n```bash\ngit diff HEAD > gemini-review.diff\ngemini \"Review gemini-review.diff. Do not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\nrm gemini-review.diff\n```\n\n**Specific commit:**\n```bash\ngit show abc123 > gemini-review.diff\ngemini \"Review the commit at gemini-review.diff. Do not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\nrm gemini-review.diff\n```\n\n## Focused Reviews\n\n**Security focus:**\n```bash\ngit diff --cached > gemini-review.diff\ngemini \"Security review of gemini-review.diff. Check for:\n- XSS vulnerabilities\n- SQL/command injection\n- Sensitive data exposure\n- Authentication/authorization issues\nDo not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\nrm gemini-review.diff\n```\n\n**Performance focus:**\n```bash\ngit diff --cached > gemini-review.diff\ngemini \"Performance review of gemini-review.diff. Check for:\n- Inefficient algorithms\n- N+1 queries\n- Memory leaks\n- Blocking operations\nDo not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\nrm gemini-review.diff\n```\n\n## With File Context\n\nAsk Gemini to read full files for better context:\n\n```bash\ngit diff --cached > gemini-review.diff\ngemini \"Review gemini-review.diff. Also read the full files:\n- src/auth/login.ts\n- src/utils/validate.ts\nto understand the broader context. Do not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\nrm gemini-review.diff\n```\n\n## Notes\n\n- **Gemini must not make any changes, provide feedback ONLY.**\n- Gemini respects `.gitignore` - it cannot read files matching gitignore patterns\n- Gemini can only read files in the workspace directory (project root)\n- Requires `dangerouslyDisableSandbox: true` for Bash calls\n- May take 1-2 minutes for thorough review\n- See `references/setup.md` for troubleshooting\n",
        "plugins/gemini/skills/plan-review/SKILL.md": "---\nname: plan-review\ndescription: Get Gemini's review of Claude's implementation plans. Trigger when user wants a second opinion on a plan (\"have Gemini review this plan\", \"get second opinion\", \"critique this plan\"), or after Claude creates a plan file that needs validation before implementation.\n---\n\n# Plan Review via Gemini\n\nHave Gemini critique Claude's implementation plans for a second perspective.\n\n## Quick Start\n\n```bash\ncat ~/.claude/plans/example-plan.md | gemini \"Review this implementation plan:\n\n\\$(cat)\n\nDo not make any changes. Provide critique and feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\n```\n\n## Pattern\n\nClaude writes plans to `~/.claude/plans/`. Pipe plan content via stdin since Gemini cannot read files outside the project directory:\n\n```bash\ncat ~/.claude/plans/example-plan.md | gemini \"Review this implementation plan:\n\n\\$(cat)\n\nConsider:\n1. Are there gaps or missing steps?\n2. Are there risks not addressed?\n3. Is the approach optimal?\n4. What alternatives should be considered?\n\nDo not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\n```\n\n## With Source Context\n\nPipe the plan via stdin and let Gemini read source files from the project:\n\n```bash\ncat ~/.claude/plans/auth-refactor.md | gemini \"Review this implementation plan:\n\n\\$(cat)\n\nAlso read these source files for context:\n- src/auth/login.ts\n- src/middleware/session.ts\n\nEvaluate if the plan addresses the actual codebase structure. Do not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\n```\n\n## Focused Reviews\n\n**Risk assessment:**\n```bash\ncat ~/.claude/plans/migration.md | gemini \"Review this plan for risks:\n\n\\$(cat)\n\nEvaluate:\n- Breaking changes\n- Data loss potential\n- Rollback complexity\n- Dependencies that could fail\n\nDo not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\n```\n\n**Completeness check:**\n```bash\ncat ~/.claude/plans/feature.md | gemini \"Review this plan for completeness:\n\n\\$(cat)\n\nEvaluate:\n- Are all edge cases covered?\n- Is testing addressed?\n- Are there missing steps?\n\nDo not make any changes. Respond with feedback only.\" --allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos -o text 2>&1\n```\n\n## Notes\n\n- **Gemini must not make any changes, provide feedback ONLY.**\n- Pipe plan content via stdin using `$(cat)` - Gemini cannot read `~/.claude/plans/` directly\n- Gemini can explore the project using `--allowed-tools read_file,codebase_investigator,glob,search_file_content,list_directory,write_todos`\n- Gemini respects `.gitignore` - it cannot read files matching gitignore patterns\n- Requires `dangerouslyDisableSandbox: true` for Bash calls\n- May take 2-3 minutes for thorough review with source analysis\n- See `references/setup.md` for troubleshooting\n",
        "plugins/gemini/skills/web-search/SKILL.md": "---\nname: web-search\ndescription: Real-time web research using Gemini's Google Search. Trigger when user needs current information (\"search with Gemini\", \"find current info about X\", \"what's the latest on Y\"), library/API research, security vulnerability lookups, or comparisons requiring recent data.\n---\n\n# Web Search via Gemini\n\nUse Gemini's `google_web_search` tool for real-time internet research.\n\n## Quick Start\n\n```bash\ngemini \"Use Google Search to find [topic]. Do not make any changes. Respond with the results only.\" --allowed-tools google_web_search -o text 2>&1\n```\n\nUse `-m gemini-2.5-flash` for faster searches:\n```bash\ngemini \"Latest version of React? Do not make any changes. Respond with the results only.\" --allowed-tools google_web_search -m gemini-2.5-flash -o text 2>&1\n```\n\n## When to Use\n\n- Current events and news\n- Latest library versions and documentation\n- Security vulnerabilities (CVEs)\n- Community opinions and benchmarks\n- Best practices research\n- Comparison research\n\n## Examples\n\n**Current info:**\n```bash\ngemini \"What are the latest Next.js 15 features? Use Google Search. Do not make any changes. Respond with the results only.\" --allowed-tools google_web_search -o text\n```\n\n**Vulnerability research:**\n```bash\ngemini \"What are known CVEs for lodash 4.x? Use Google Search. Do not make any changes. Respond with the results only.\" --allowed-tools google_web_search -o text\n```\n\n**Comparison:**\n```bash\ngemini \"Compare Zustand vs Jotai for React state management. Use Google Search for recent benchmarks. Do not make any changes. Respond with the results only.\" --allowed-tools google_web_search -o text\n```\n\n**Best practices:**\n```bash\ngemini \"Current best practices for Node.js 22 error handling? Use Google Search. Do not make any changes. Respond with the results only.\" --allowed-tools google_web_search -o text\n```\n\n## Notes\n\n- **Gemini must not make any changes, provide feedback ONLY.**\n- Requires sandbox bypass: use `dangerouslyDisableSandbox: true`\n- May take 1-2 minutes for comprehensive searches\n- Validate findings against official documentation\n- See `references/setup.md` for troubleshooting\n",
        "plugins/gh-cli/.claude-plugin/plugin.json": "{\n  \"name\": \"gh-cli\",\n  \"description\": \"GitHub CLI integration with focused skills for pull requests, issues, GitHub Actions, and viewing GitHub file URLs\",\n  \"version\": \"2.0.0\",\n  \"author\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"github\", \"gh\", \"cli\", \"pull-requests\", \"issues\", \"actions\", \"api\"]\n}\n",
        "plugins/gh-cli/README.md": "# gh-cli\n\nGitHub CLI integration with focused skills for pull requests, issues, GitHub Actions, repository info, and viewing GitHub file URLs.\n\n## Setup\n\nInstall and authenticate the GitHub CLI before using this plugin. See the [official installation guide](https://cli.github.com/).\n\n```bash\ngh auth login\ngh auth status  # verify authentication\n```\n\n## Skills\n\n### pr\n\nPull request operations.\n\n**Triggers:** \"show open PRs\", \"view PR 123\", \"create a pull request\", \"merge PR\", \"approve PR\", \"show PR diff\", \"what files changed in PR\"\n\n**Examples:**\n```\n\"Show me all open PRs\" → gh pr list --state open\n\"Create a PR for my current branch\" → gh pr create --fill\n\"What files changed in PR #123?\" → view_pr_files.py 123 --list\n```\n\n### issues\n\nIssue management.\n\n**Triggers:** \"show open issues\", \"view issue 456\", \"create an issue\", \"file a bug\", \"close issue\", \"add label to issue\"\n\n**Examples:**\n```\n\"Create an issue for the login bug\" → gh issue create --title \"...\" --body \"...\"\n\"Show me all bugs assigned to me\" → gh issue list --label bug --assignee @me\n```\n\n### actions\n\nGitHub Actions workflow management.\n\n**Triggers:** \"check CI\", \"is the build passing\", \"show recent runs\", \"view action logs\", \"watch CI\", \"rerun the build\"\n\n**Examples:**\n```\n\"Check if CI is passing\" → gh run list --limit 5\n\"Show me the failed workflow logs\" → gh run view 789 --log-failed\n```\n\n### repo\n\nRepository information (read-only).\n\n**Triggers:** \"show repo info\", \"what's this repo about\", \"list my repos\", \"show repos for user\", \"stars\", \"languages\", \"topics\"\n\n**Examples:**\n```\n\"How many stars does this repo have?\" → gh repo view --json stargazersCount\n\"List all Go repos for user X\" → gh repo list X --language go\n```\n\n### view-file\n\nFetch raw file content from GitHub URLs without HTML/JS clutter.\n\n**Triggers:** User shares a GitHub file URL like `https://github.com/user/repo/blob/main/file.go`\n\n**Examples:**\n```\n\"View this file https://github.com/golang/go/blob/master/README.md\"\n→ view_github_file.py https://github.com/golang/go/blob/master/README.md\n```\n\n## Helper Scripts\n\n### `pr/scripts/view_pr_files.py`\n\nAnalyzes files changed in pull requests.\n- Lists all files changed in a PR\n- Shows diff for specific files\n- Displays file content from PR branch\n\n### `view-file/scripts/view_github_file.py`\n\nFetches raw file content from GitHub URLs.\n- Parses GitHub URLs to extract owner, repo, ref, and file path\n- Uses `gh api` to fetch content\n- Decodes base64-encoded content\n",
        "plugins/gh-cli/skills/actions/SKILL.md": "---\nname: actions\ndescription: GitHub Actions workflow management using GitHub CLI. Trigger when user wants to check CI status (\"check CI\", \"is the build passing\"), view workflow runs (\"show recent runs\", \"view action logs\"), watch running workflows (\"watch CI\"), or rerun failed workflows (\"rerun the build\").\n---\n\n# GitHub Actions\n\nMonitor and manage GitHub Actions workflows with the `gh` CLI.\n\n## Prerequisites\n\nGitHub CLI must be installed and authenticated:\n```bash\ngh auth status\n```\n\n## Quick Reference\n\n```bash\ngh run list                         # List recent runs\ngh run view 789                     # View run details\ngh run view 789 --log               # View run logs\ngh run watch 789                    # Watch running workflow\ngh run rerun 789                    # Rerun workflow\n```\n\n## List Workflow Runs\n\n```bash\ngh run list\ngh run list --limit 10\ngh run list --workflow ci.yml\ngh run list --status failure\ngh run list --branch main\n```\n\n## View Run Details\n\n```bash\ngh run view 789\ngh run view 789 --json status,conclusion,jobs\n```\n\n## View Run Logs\n\n```bash\ngh run view 789 --log\ngh run view 789 --log-failed  # Only failed job logs\n```\n\n## Watch Running Workflow\n\n```bash\ngh run watch 789\ngh run watch 789 --exit-status  # Exit with workflow status code\n```\n\n## Rerun Workflows\n\n```bash\ngh run rerun 789\ngh run rerun 789 --failed  # Rerun only failed jobs\n```\n\n## List Workflows\n\n```bash\ngh workflow list\ngh workflow view ci.yml\n```\n",
        "plugins/gh-cli/skills/issues/SKILL.md": "---\nname: issues\ndescription: GitHub issue management using GitHub CLI. Trigger when user wants to list issues (\"show open issues\"), view issue details (\"view issue 456\"), create issues (\"create an issue\", \"file a bug\"), or edit/close issues (\"close issue\", \"add label to issue\").\n---\n\n# Issue Management\n\nManage GitHub issues with the `gh` CLI.\n\n## Prerequisites\n\nGitHub CLI must be installed and authenticated:\n```bash\ngh auth status\n```\n\n## Quick Reference\n\n```bash\ngh issue list                       # List open issues\ngh issue view 456                   # View issue details\ngh issue create --title \"Bug\"       # Create issue\ngh issue close 456                  # Close issue\n```\n\n## List Issues\n\n```bash\ngh issue list\ngh issue list --state open\ngh issue list --label bug\ngh issue list --assignee @me\ngh issue list --state all --limit 50\n```\n\n## View Issue\n\n```bash\ngh issue view 456\ngh issue view 456 --json title,body,labels,state\n```\n\n## Create Issue\n\n```bash\ngh issue create --title \"Bug report\" --body \"Details\"\ngh issue create --label bug --assignee @me\ngh issue create --title \"Feature\" --label enhancement\n```\n\n## Edit Issue\n\n```bash\ngh issue edit 456 --title \"Updated title\"\ngh issue edit 456 --add-label \"needs-review\"\ngh issue edit 456 --remove-label \"in-progress\"\ngh issue edit 456 --add-assignee @me\n```\n\n## Close and Reopen\n\n```bash\ngh issue close 456\ngh issue close 456 --reason \"not planned\"\ngh issue reopen 456\n```\n",
        "plugins/gh-cli/skills/pr/SKILL.md": "---\nname: pr\ndescription: Pull request operations using GitHub CLI. Trigger when user wants to list PRs (\"show open PRs\"), view PR details (\"view PR 123\"), create PRs (\"create a pull request\"), review/merge PRs (\"merge PR\", \"approve PR\"), or view PR diffs (\"show PR diff\", \"what files changed in PR\").\n---\n\n# Pull Request Operations\n\nManage pull requests with the `gh` CLI.\n\n## Prerequisites\n\nGitHub CLI must be installed and authenticated:\n```bash\ngh auth status\n```\n\n## Quick Reference\n\n```bash\ngh pr list                          # List open PRs\ngh pr view 123                      # View PR details\ngh pr create --fill                 # Create PR from commits\ngh pr merge 123 --squash            # Merge PR\ngh pr diff 123                      # View diff\n```\n\n## List PRs\n\n```bash\ngh pr list --state open\ngh pr list --author @me\ngh pr list --label \"needs-review\"\n```\n\n## View PR Details\n\n```bash\ngh pr view 123\ngh pr view 123 --json title,body,state,files\n```\n\n## Create PR\n\n```bash\ngh pr create --title \"Feature\" --body \"Description\"\ngh pr create --fill  # Use commit messages\n```\n\n## Review and Merge\n\n```bash\ngh pr review 123 --approve\ngh pr review 123 --approve --body \"LGTM\"\ngh pr merge 123 --squash\ngh pr merge 123 --merge\n```\n\n## View PR Diff\n\n```bash\ngh pr diff 123\ngh pr diff 123 -- path/to/file.go   # Specific file\n```\n\n## Helper Script: View PR Files\n\nList or view files changed in a PR:\n\n```bash\n# List changed files\npython3 scripts/view_pr_files.py 123 --list\npython3 scripts/view_pr_files.py https://github.com/user/repo/pull/123 --list\n\n# View full diff\npython3 scripts/view_pr_files.py 123 --diff\n\n# View specific file content from PR branch\npython3 scripts/view_pr_files.py 123 --file path/to/file.go\n```\n\n### Fallback (if script fails)\n\n```bash\n# List changed files\ngh pr view 123 --json files --jq '.files[].path'\n\n# View diff\ngh pr diff 123\n\n# Get file content from PR branch\ngh pr view 123 --json headRefName --jq '.headRefName'\ngh api repos/{owner}/{repo}/contents/{path}?ref={head_ref} --jq '.content' | base64 --decode\n```\n",
        "plugins/gh-cli/skills/repo/SKILL.md": "---\nname: repo\ndescription: Repository information using GitHub CLI. Trigger when user wants to view repository details (\"show repo info\", \"what's this repo about\"), list repositories (\"list my repos\", \"show repos for user\"), or check repository metadata (\"stars\", \"languages\", \"topics\").\n---\n\n# Repository Information\n\nView repository details and list repositories with the `gh` CLI. Read-only operations only.\n\n## Prerequisites\n\nGitHub CLI must be installed and authenticated:\n```bash\ngh auth status\n```\n\n## Quick Reference\n\n```bash\ngh repo view                        # View current repo\ngh repo view owner/repo             # View specific repo\ngh repo list owner                  # List repos for owner\n```\n\n## View Repository\n\n```bash\ngh repo view\ngh repo view owner/repo\ngh repo view owner/repo --json name,description,stargazersCount\ngh repo view owner/repo --web  # Open in browser\n```\n\n**Common JSON fields:**\n```bash\ngh repo view owner/repo --json name,description,url,stargazersCount,forkCount,isPrivate,languages,topics\n```\n\n## List Repositories\n\n```bash\ngh repo list\ngh repo list owner\ngh repo list owner --limit 50\ngh repo list owner --json name,description\ngh repo list owner --language go\ngh repo list owner --source  # Non-forks only\n```\n\n## Repository Search\n\n```bash\ngh search repos \"query\"\ngh search repos \"language:go stars:>100\"\ngh search repos \"org:anthropics\"\n```\n",
        "plugins/gh-cli/skills/view-file/SKILL.md": "---\nname: view-file\ndescription: Fetch raw file content from GitHub URLs. Trigger when user shares a GitHub file link (https://github.com/user/repo/blob/main/file.go) and wants to view the source code. Avoids HTML/JS clutter from WebFetch.\n---\n\n# View GitHub Files\n\nFetch raw file content from GitHub URLs without HTML/JS clutter.\n\n## Prerequisites\n\nGitHub CLI must be installed and authenticated:\n```bash\ngh auth status\n```\n\n## When to Use\n\nWhen a user shares a GitHub file URL like:\n- `https://github.com/user/repo/blob/main/src/file.go`\n- `https://github.com/user/repo/tree/v1.0/docs/README.md`\n\nUse this skill instead of WebFetch to get clean source code.\n\n## Helper Script\n\n```bash\npython3 scripts/view_github_file.py https://github.com/user/repo/blob/main/path/to/file.go\n```\n\nThe script:\n- Parses the GitHub URL to extract owner, repo, ref, and path\n- Uses `gh api` to fetch file content\n- Decodes base64-encoded response\n- Returns clean source code\n\n## Supported URL Formats\n\n```\nhttps://github.com/{owner}/{repo}/blob/{ref}/{path}\nhttps://github.com/{owner}/{repo}/tree/{ref}/{path}\n```\n\nWhere `{ref}` can be a branch name, tag, or commit SHA.\n\n## Fallback (if script fails)\n\nParse the URL manually and use `gh api`:\n\n```bash\n# From URL: https://github.com/golang/go/blob/master/README.md\n# Extract: owner=golang, repo=go, ref=master, path=README.md\ngh api repos/golang/go/contents/README.md?ref=master --jq '.content' | base64 --decode\n```\n\nOr use raw.githubusercontent.com:\n\n```bash\ncurl https://raw.githubusercontent.com/golang/go/master/README.md\n```\n",
        "plugins/go-formatter/.claude-plugin/plugin.json": "{\n  \"name\": \"go-formatter\",\n  \"description\": \"Automatically format Go files with gofmt after Write/Edit/MultiEdit operations\",\n  \"version\": \"1.0.1\",\n  \"author\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"go\", \"golang\", \"formatter\", \"gofmt\", \"hooks\"]\n}\n",
        "plugins/go-formatter/README.md": "# go-formatter\n\nFormats Go files with `gofmt` after Write/Edit/MultiEdit operations.\n\n## Usage\n\nRuns as a PostToolUse hook. No manual invocation needed.\n\n## How it works\n\nThe plugin uses a PostToolUse hook that:\n1. Receives tool execution data via stdin\n2. Extracts the file path using `jq`\n3. Checks if the file ends with `.go`\n4. Runs `gofmt -w` to format the file in-place\n\n## Version\n\n`gofmt` is bundled with Go. No separate version pinning needed.\n\n## Note\n\nOnce installed, remove any existing gofmt PostToolUse hooks from `~/.claude/settings.json` to avoid running gofmt twice.\n",
        "plugins/go-formatter/hooks/hooks.json": "{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit|MultiEdit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/format-go.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/go-style-guide/.claude-plugin/plugin.json": "{\n  \"name\": \"go-style-guide\",\n  \"description\": \"Review Go code for adherence to Go Style Guide\",\n  \"version\": \"1.1.0\",\n  \"author\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"go\", \"golang\", \"style-guide\", \"code-review\"]\n}\n",
        "plugins/go-style-guide/README.md": "# go-style-guide\n\nReviews Go code against a style guide combining Google and Uber Go style guides.\n\n## Requirements\n\nGo 1.25.0 or later. Uses features from Go 1.22-1.25:\n- Automatic loop variable scoping (Go 1.22+)\n- Generic slice/map functions and iterators (Go 1.21-1.23)\n- JSON `omitzero` and `os.Root` filesystem safety (Go 1.24)\n- Testing improvements (`testing/synctest`, `b.Loop()`, `t.Context()`) (Go 1.24-1.25)\n\n## Triggers\n\n- \"review this Go code\"\n- \"check against style guide\"\n- \"review my PR\"\n- \"review the work done so far\"\n\n## Workflow\n\n1. **Scope Identification** - Determines what to review (PR diff, specific files, recent commits)\n2. **Code Analysis** - Reviews against bundled Go Style Guide\n3. **Report Findings** - Structured output with Critical and Important issues\n\n## Focus Areas\n\n### Critical (Architecture & Safety)\n- Fire-and-forget goroutines (lifecycle)\n- Mutex races (requires race detector)\n- Panics in production code\n\n### Important (Design & Patterns)\n- Error handling strategy\n- Data ownership boundaries\n- Concurrency patterns\n- API design\n- Testing strategy\n\n## Linter Integration\n\nThis guide complements automated tools. Use with:\n\n```bash\ngolangci-lint run --enable=errcheck,staticcheck,govet,gocritic,perfsprint,goimports,gci,nilslice,prealloc,thelper\n```\n\nThe skill focuses on what linters cannot catch: architectural decisions, ownership semantics, and context-aware patterns.\n\n## Resources\n\n- `references/go-style-guide.md` - Go Style Guide\n- `references/review-checklist.md` - Common violations\n\n## Source\n\nBased on the [Google Go Style Guide](https://google.github.io/styleguide/go/) and [Uber Go Style Guide](https://github.com/uber-go/guide/blob/master/style.md).\n",
        "plugins/go-style-guide/skills/go-style-guide/SKILL.md": "---\nname: go-style-guide\ndescription: Review Go code for adherence to Go Style Guide. Use when the user requests a code review of completed work, pull requests, or feature branches in Go projects. Focuses on critical bugs, race conditions, and important maintainability issues. Trigger phrases include \"review this Go code\", \"check against style guide\", \"review my PR\", or \"review the work done so far\".\n---\n\n# Go Style Guide Reviewer\n\nReview Go code against comprehensive style guide, focusing on critical bugs and important maintainability issues.\n\n**This guide assumes Go 1.25+ and does not consider backwards compatibility.** All patterns use modern Go best practices.\n\n## Reference Files\n\nLoad references based on code patterns found during review:\n\n| Code Pattern | Reference File |\n|--------------|----------------|\n| `go func()`, `sync.Mutex`, `sync.`, channels, atomic | `references/concurrency.md` |\n| `err`, `error`, `panic`, `Must` functions | `references/errors.md` |\n| `interface`, embedding, receivers, `func New`, `init()` | `references/api-design.md` |\n| slice/map return, `slices.Clone`, `maps.Clone` | `references/api-design.md` |\n| `_test.go`, `t.`, `b.` | `references/testing.md` |\n| naming, comments, logging | `references/style.md` |\n\n**Quick reference**: `references/review-checklist.md` - critical patterns with code examples\n\n**Note**: \"Copying at boundaries\" lives in api-design.md but relates to concurrency safety - check both when ownership/encapsulation is the concern.\n\n## Quick Style Reference\n\nBasic rules that don't need a file lookup:\n\n- **Package names**: lowercase, no underscores, singular (`net/url` not `net/urls`)\n- **Receiver names**: 1-2 letters, consistent across all methods (`func (c *Client) Connect()`)\n- **Error strings**: lowercase, no punctuation (`errors.New(\"connection failed\")`)\n- **Variable names**: short for small scopes (`i`, `v`), longer for large scopes (`requestTimeout`)\n\n## Review Process\n\nFollow this 3-phase workflow for systematic code review:\n\n### Phase 1: Scope Identification\n\nDetermine what code to review based on the user's request:\n\n**Pull Request / Branch Review**:\n```bash\n# Get all changed files in branch\ngit diff --name-only main...HEAD | grep '\\.go$'\n\n# Get diff with context\ngit diff main...HEAD\n```\n\n**Specific Files**:\n```bash\n# User specifies file(s) directly\n# Read the files using the Read tool\n```\n\n**Recent Work**:\n```bash\n# Review recent commits\ngit log --oneline -n 10\ngit diff HEAD~5..HEAD\n```\n\n**Output**: List of Go files and changes to review.\n\n### Phase 2: Code Analysis\n\nReview the code systematically using the bundled references:\n\n1. **Start with critical issues** (load `references/review-checklist.md` for quick patterns):\n   - Unhandled errors\n   - Type assertions without check\n   - Panics in production code\n   - Fire-and-forget goroutines\n   - Mutex races and missing defers\n   - Nil pointer dereferences\n\n2. **Check important patterns**:\n   - Error handling (wrapping, naming, handling once)\n   - Boundary safety (copying slices/maps)\n   - Struct design (embedding, initialization)\n   - Concurrency lifecycle (goroutine management)\n   - Exit handling (os.Exit only in main)\n\n3. **Consult topic files as needed** (see Reference Files table above):\n   - For detailed explanations of specific patterns\n   - When encountering unfamiliar idioms\n   - To verify best practices for specific scenarios\n\n**Grep patterns for architectural issues**:\n```bash\n# Find panic usage (context-dependent - init/main vs library)\nrg 'panic\\(' --type go\n\n# Find goroutine launches (check lifecycle management)\nrg '\\bgo\\s+' --type go\n\n# Find os.Exit or log.Fatal (should only be in main)\nrg '(os\\.Exit|log\\.Fatal)' --type go\n\n# Find global var declarations (check for mutable state)\nrg '^var\\s+\\w+\\s*=' --type go\n```\n\n### Phase 3: Report Findings\n\nStructure the review with **Critical** and **Important** issues only (skip Minor issues per user preference).\n\n**Format**:\n\n```markdown\n## Code Review Summary\n\n[1-2 sentence overview of code quality and adherence]\n\n**Note**: This review focuses on architectural and semantic issues that require human judgment. For syntax, formatting, and common bugs (unhandled errors, type assertions, etc.), ensure `golangci-lint` is run separately.\n\n## Critical Issues\n\n[Issues that could cause bugs, panics, or data races - MUST fix]\n\n### [Issue Title]\n**Location**: `file.go:123` or `functionName()`\n**Severity**: Critical\n\n**Current Code**:\n```go\n[problematic code snippet]\n```\n\n**Issue**: [What's wrong and why it's critical]\n\n**Recommended**:\n```go\n[corrected code]\n```\n\n**Guideline**: [Reference to style guide section, e.g., \"Error Handling > Type Assertions\"]\n\n---\n\n## Important Issues\n\n[Issues affecting maintainability, performance, or style - Should fix]\n\n[Use same format as Critical Issues]\n\n---\n\n## Positive Observations\n\n[Acknowledge good practices: proper error wrapping, clean concurrency patterns, good test structure]\n\n---\n\n## Recommendations\n\n1. [Prioritized action items]\n2. [Suggest running golangci-lint skill if not already done]\n```\n\n## Optional: Automated Linting Integration\n\nBefore or after manual review, suggest running automated linters for complementary coverage:\n\n**If golangci-lint skill is available**:\n\"Consider running the `golangci-lint` skill for automated static analysis. Say 'run golangci-lint' to execute.\"\n\n**Manual linting**:\n```bash\n# Run staticcheck\nstaticcheck ./...\n\n# Run golangci-lint\ngolangci-lint run\n```\n\n## Key Focus Areas\n\n### Critical (Architecture & Safety)\n- Goroutine lifecycle issues (fire-and-forget)\n- Race conditions (requires race detector, not linter)\n- Panics in production (context-dependent: library vs main)\n\n### Important (Design & Patterns)\n- Error handling strategy (when/where to handle, observability boundaries)\n- Data ownership (boundaries, copying, shared state)\n- Concurrency patterns (channel sizing, context propagation)\n- API design (embedding, evolution, encapsulation)\n- Testing strategy (table-driven, parallel, time mocking)\n\n### Skip (Handled by Linters or Out of Scope)\n\n**Linter-Caught Issues**:\n- Unhandled errors (errcheck)\n- Type assertions without checks (staticcheck)\n- Missing struct field names (govet)\n- Import grouping (goimports/gci)\n- Formatting issues (gofmt)\n- Common bugs (staticcheck, govet)\n\n**Subjective Preferences (Do Not Flag)**:\n- **Assertion library choice**: Use what codebase uses; don't add to new projects unless requested. Both manual checks and assertion libraries (testify, assert) are valid.\n- **Line length variations**: Focus on refactoring opportunities, not mechanical line breaking\n- **Test helper patterns**: Either `testing.T` parameter or returning errors acceptable; ensure `t.Helper()` used\n- **Performance nitpicks**: Only flag when profiling data shows actual impact\n\n## Review Principles\n\nWhen evaluating code, apply Google's Core Principles in order (from `references/style.md`):\n1. **Clarity**: Is the purpose and rationale clear?\n2. **Simplicity**: Does it accomplish goals in the straightforward manner?\n3. **Concision**: High signal-to-noise ratio?\n4. **Maintainability**: Can future programmers modify it correctly?\n5. **Consistency**: Aligns with broader codebase patterns?\n\nThen apply specific review guidance:\n1. **Be specific**: Quote exact code, provide exact fixes\n2. **Cite guidelines**: Reference specific sections of the style guide\n3. **Explain impact**: Why does this matter? (correctness, maintainability, performance)\n4. **Prioritize**: Critical issues first, important second\n5. **Acknowledge good code**: Recognize patterns that follow the core principles\n\n## When to Load Reference Files\n\nLoad `references/review-checklist.md` first for quick architectural patterns - it focuses on semantic issues requiring judgment.\n\nLoad topic-specific files based on code patterns (see Reference Files table):\n- `concurrency.md` - goroutines, mutexes, races, channels\n- `errors.md` - error types, wrapping, panic avoidance\n- `api-design.md` - interfaces, function design, data boundaries\n- `testing.md` - table tests, parallel tests, benchmarks\n- `style.md` - naming, documentation, code style\n\n**Important**: Skip reporting issues that golangci-lint would catch. The agent should focus on design, architecture, and context-dependent patterns that require human understanding.\n\n## Context Matters\n\nSome patterns have exceptions:\n- `init()` acceptable for database driver registration\n- `panic()` acceptable in tests (use `t.Fatal` or `t.FailNow`)\n- Global constants acceptable\n- Embedding in private structs sometimes acceptable for composition\n\nApply judgment based on context and domain.\n",
        "plugins/go-style-guide/skills/go-style-guide/references/api-design.md": "# API Design\n\nPatterns for interfaces, function design, data management, and struct organization.\n\n---\n\n## Interfaces Belong in Consumer Packages\n\nInterfaces generally belong in packages that consume interface values, not packages that implement them.\n\n**Bad - producer defines interface**:\n```go\npackage producer\n\n// Wrong - interface defined where it's implemented\ntype Reader interface {\n  Read() []byte\n}\n\ntype FileReader struct{}\n\nfunc (f *FileReader) Read() []byte {\n  // implementation\n}\n```\n\n**Good - consumer defines interface**:\n```go\npackage consumer\n\n// Interface defined where it's needed\ntype Reader interface {\n  Read() []byte\n}\n\nfunc Process(r Reader) {\n  data := r.Read()\n  // use data\n}\n```\n\n```go\npackage producer\n\n// Returns concrete type\ntype FileReader struct{}\n\nfunc (f *FileReader) Read() []byte {\n  // implementation\n}\n```\n\n**Why**: This pattern:\n- Allows adding new implementations without modifying the original package\n- Keeps interfaces minimal (only methods actually needed)\n- Prevents premature abstraction\n- Enables better API evolution\n\n**Exceptions exist**: Sometimes producer-defined interfaces make sense (e.g., `io.Reader`, plugin systems). Use judgment based on your use case.\n\n---\n\n## Return Concrete Types\n\nFunctions should return concrete types, not interfaces, unless there's a compelling reason to hide the implementation.\n\n**Bad**:\n```go\nfunc NewUserStore() UserStore {\n  return &userStoreImpl{}\n}\n```\n\n**Good**:\n```go\nfunc NewUserStore() *UserStore {\n  return &UserStore{}\n}\n```\n\n**Why**: Returning concrete types allows adding methods later without breaking callers. Only return interfaces when you need to enforce abstraction boundaries.\n\n---\n\n## Avoid Premature Interface Definitions\n\nDon't define interfaces before you have realistic usage. Interfaces should emerge from actual needs.\n\n**Bad**:\n```go\n// No consumers yet - premature abstraction\ntype DataProcessor interface {\n  Process(data []byte) error\n  Validate() bool\n  Transform() Result\n}\n```\n\n**Good**:\n```go\n// Start with concrete implementation\ntype DataProcessor struct {\n  // fields\n}\n\nfunc (d *DataProcessor) Process(data []byte) error {\n  // implementation\n}\n\n// Later, when you have multiple implementations, extract interface\n```\n\n**Why**: Interfaces defined without real usage tend to be too large or poorly designed. Let usage patterns guide interface design.\n\n---\n\n## Verify Interface Compliance\n\n**Bad**:\n```go\ntype Handler struct {\n  // ...\n}\n\nfunc (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n  // ...\n}\n\n// No compile-time verification\n```\n\n**Good**:\n```go\ntype Handler struct {\n  // ...\n}\n\n// Compile-time verification\nvar _ http.Handler = (*Handler)(nil)\n\nfunc (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n  // ...\n}\n```\n\n**Why**: Compile-time verification catches interface compliance issues immediately rather than at runtime.\n\n---\n\n## Receivers and Interfaces\n\nMethods with value receivers work on both pointers and values. Methods with pointer receivers only work on pointers or addressable values.\n\n**Example**:\n```go\ntype S struct {\n  data string\n}\n\nfunc (s S) Read() string {\n  return s.data\n}\n\nfunc (s *S) Write(str string) {\n  s.data = str\n}\n\n// Maps store non-addressable values\nsVals := map[int]S{1: {\"A\"}}\n\n// You can call Read on values\nsVals[1].Read()\n\n// COMPILE ERROR: cannot call pointer-receiver method on non-addressable value\nsVals[1].Write(\"test\")\n```\n\n**Why**: Understanding addressability prevents runtime errors and API design issues.\n\n---\n\n## Receiver Type Choice\n\nChoose receiver types based on correctness, not performance optimization.\n\n**Use pointer receivers when**:\n- Method mutates the receiver\n- Receiver contains non-copyable fields (mutexes, channels)\n- Receiver is very large (but profile first)\n- Some methods already have pointer receivers (consistency)\n\n**Use value receivers when**:\n- Method doesn't mutate receiver\n- Receiver is a small struct or primitive type\n- Receiver is a copyable value type (like `time.Time`)\n\n**Mixing**: Avoid mixing pointer and value receivers for the same type (except for specific performance needs identified through profiling).\n\n---\n\n## Don't Create Custom Context Types\n\nAlways use `context.Context` from the standard library. Custom context types fragment the ecosystem.\n\n**Bad**:\n```go\ntype AppContext struct {\n  context.Context\n  UserID string\n}\n\nfunc ProcessRequest(ctx AppContext) {\n  // ...\n}\n```\n\n**Good**:\n```go\nfunc ProcessRequest(ctx context.Context) {\n  userID := ctx.Value(userIDKey).(string)\n  // ...\n}\n```\n\n**Why**: Custom context types prevent interoperability with standard library functions and third-party code expecting `context.Context`.\n\n---\n\n## Prefer Synchronous Functions\n\nPrefer synchronous functions over asynchronous ones. Keep goroutine management localized to callers.\n\n**Bad**:\n```go\nfunc ProcessData(data []byte) {\n  go func() {\n    // Hidden concurrency - caller can't control it\n    result := process(data)\n    store(result)\n  }()\n}\n```\n\n**Good**:\n```go\nfunc ProcessData(data []byte) Result {\n  result := process(data)\n  return result\n}\n\n// Caller controls concurrency\ngo func() {\n  result := ProcessData(data)\n  store(result)\n}()\n```\n\n**Why**: Synchronous functions give callers control over concurrency, making goroutine lifetimes clear and testability easier.\n\n---\n\n## Make Goroutine Lifetimes Clear\n\nWhen functions do spawn goroutines, make it obvious when or whether they exit.\n\n**Bad**:\n```go\nfunc StartMonitor() {\n  go monitor()  // When does this stop? How?\n}\n```\n\n**Good**:\n```go\ntype Monitor struct {\n  stop chan struct{}\n  done chan struct{}\n}\n\nfunc (m *Monitor) Start() {\n  go m.run()\n}\n\nfunc (m *Monitor) Stop() {\n  close(m.stop)\n  <-m.done  // Wait for completion\n}\n```\n\n**Why**: Clear goroutine lifetimes prevent leaks and enable graceful shutdown.\n\n---\n\n## Context Should Be First Parameter\n\nContext should be the first parameter of functions (except HTTP handlers and streaming RPC methods where it's implicit).\n\n**Good**:\n```go\nfunc FetchUser(ctx context.Context, userID string) (*User, error) {\n  // ...\n}\n\nfunc ProcessBatch(ctx context.Context, items []Item, opts *Options) error {\n  // ...\n}\n```\n\n**Exception - HTTP handlers**:\n```go\nfunc HandleRequest(w http.ResponseWriter, r *http.Request) {\n  ctx := r.Context()  // Context from request\n  // ...\n}\n```\n\n**Why**: Consistent parameter order improves API discoverability and follows ecosystem conventions.\n\n---\n\n## Pass Values, Not Pointers (Usually)\n\nPass values unless the function needs to mutate the argument or the type is non-copyable.\n\n**Prefer values**:\n```go\nfunc FormatTimestamp(t time.Time) string {\n  return t.Format(time.RFC3339)\n}\n```\n\n**Use pointers when**:\n```go\n// 1. Function mutates the argument\nfunc UpdateUser(u *User) {\n  u.LastModified = time.Now()\n}\n\n// 2. Type contains non-copyable fields (sync.Mutex, etc.)\ntype Config struct {\n  mu sync.Mutex\n  data map[string]string\n}\n\nfunc LoadConfig(c *Config) error {\n  // Must use pointer - Config contains mutex\n}\n\n// 3. Type is very large and copying would be expensive (profile first!)\n```\n\n**Why**: Value parameters prevent accidental mutations and make data flow clearer. Only use pointers when necessary for correctness.\n\n---\n\n## Copy Slices and Maps at Boundaries\n\n**Bad**:\n```go\nfunc (d *Driver) SetTrips(trips []Trip) {\n  d.trips = trips  // Caller can mutate\n}\n\ntrips := ...\nd1.SetTrips(trips)\n\ntrips[0] = ...  // Modifies d1.trips!\n```\n\n**Good**:\n```go\nfunc (d *Driver) SetTrips(trips []Trip) {\n  d.trips = slices.Clone(trips)  // Defensive copy\n}\n\ntrips := ...\nd1.SetTrips(trips)\n\ntrips[0] = ...  // Does not affect d1.trips\n```\n\n**Why**: Prevents unintended mutations and maintains encapsulation.\n\nSimilarly, return copies of internal slices/maps:\n\n**Bad**:\n```go\ntype Stats struct {\n  mu sync.Mutex\n  counters []int\n}\n\nfunc (s *Stats) Snapshot() []int {\n  s.mu.Lock()\n  defer s.mu.Unlock()\n  return s.counters  // Caller can mutate without lock!\n}\n```\n\n**Good**:\n```go\nfunc (s *Stats) Snapshot() []int {\n  s.mu.Lock()\n  defer s.mu.Unlock()\n  return slices.Clone(s.counters)\n}\n```\n\n---\n\n## Generic Slice and Map Functions\n\nUse the `slices` and `maps` packages (Go 1.21+) for common operations instead of manual implementations.\n\n**Slices**:\n```go\nimport \"slices\"\n\n// Clone - replaces manual copy\noriginal := []int{1, 2, 3}\ncopy := slices.Clone(original)\n\n// Sort - generic sorting\nitems := []string{\"c\", \"a\", \"b\"}\nslices.Sort(items)\n\n// Compact - remove consecutive duplicates\ndata := []int{1, 1, 2, 2, 3}\nunique := slices.Compact(data)\n```\n\n**Maps**:\n```go\nimport \"maps\"\n\n// Clone\nm := map[string]int{\"a\": 1}\ncopy := maps.Clone(m)\n\n// Equal\nm1 := map[string]int{\"a\": 1}\nm2 := map[string]int{\"a\": 1}\nif maps.Equal(m1, m2) {\n  // ...\n}\n\n// DeleteFunc (Go 1.21+)\nmaps.DeleteFunc(m, func(k string, v int) bool {\n  return v%2 == 0\n})\n```\n\n**Important**: Modification functions in `slices` (Go 1.22+) \"clear the tail\" - zeroing obsolete elements. Always use the returned slice value:\n\n```go\n// Correct - use returned value\nitems = slices.Delete(items, 0, 1)\n\n// Bug - original slice may have stale tail elements\nslices.Delete(items, 0, 1)  // Don't ignore return value\n```\n\n---\n\n## JSON omitzero\n\nUse the `omitzero` struct tag (Go 1.24+) to omit zero values during marshaling, replacing error-prone `omitempty` pointer patterns.\n\n**Bad**:\n```go\ntype User struct {\n  // Pointer used only to allow omitting zero value (0)\n  Age *int `json:\"age,omitempty\"`\n}\n```\n\n**Good**:\n```go\ntype User struct {\n  // Clearer intent, no pointer needed\n  Age int `json:\"age,omitzero\"`\n}\n```\n\n---\n\n## Safe File System Access\n\nUse `os.Root` (Go 1.24+) for traversal-resistant file access within a directory.\n\n**Bad**:\n```go\n// Vulnerable to \"../\" traversal\nf, err := os.Open(filepath.Join(dir, filename))\n```\n\n**Good**:\n```go\nroot, err := os.OpenRoot(dir)\nif err != nil {\n  return err\n}\ndefer root.Close()\n\n// Safe: errors if path escapes root\nf, err := root.Open(filename)\n```\n\n---\n\n## Range Functions & Iterators\n\nGo 1.23+ supports custom iterators using `iter.Seq` for range loops.\n\n**When to provide iterators**: For container types that benefit from idiomatic `for range` syntax.\n\n**Example**:\n```go\nimport \"iter\"\n\ntype Set[E comparable] struct {\n  m map[E]struct{}\n}\n\n// Provide iterator for range loops\nfunc (s *Set[E]) All() iter.Seq[E] {\n  return func(yield func(E) bool) {\n    for v := range s.m {\n      if !yield(v) {\n        return\n      }\n    }\n  }\n}\n\n// Usage - idiomatic for/range\nfor v := range s.All() {\n  fmt.Println(v)\n}\n```\n\n**Replaces**: Channel-based iterators and callback patterns.\n\n**Benefits**:\n- Standard `for range` syntax\n- Compiler-optimized iteration\n- Early termination with `break`\n- Compatible with range-over-function patterns\n\n---\n\n## Avoid Embedding in Public Structs\n\n**Bad**:\n```go\ntype AbstractList struct{}\n\nfunc (l *AbstractList) Add(e Entity) {\n  // ...\n}\n\ntype ConcreteList struct {\n  AbstractList  // Exposes Add as public API\n}\n```\n\n**Good**:\n```go\ntype AbstractList struct{}\n\nfunc (l *AbstractList) Add(e Entity) {\n  // ...\n}\n\ntype ConcreteList struct {\n  list *AbstractList  // Private field\n}\n\nfunc (c *ConcreteList) Add(e Entity) {\n  c.list.Add(e)  // Explicit delegation\n}\n```\n\n**Why**: Embedding leaks implementation details and inhibits evolution.\n\n---\n\n## Struct Literal Field Names\n\nUse field names in struct literals for types from other packages. Omitting names is fragile.\n\n**Bad**:\n```go\n// Fragile - breaks if fields reordered\nuser := User{\"alice\", 30, \"alice@example.com\"}\n```\n\n**Good**:\n```go\nuser := User{\n  Name:  \"alice\",\n  Age:   30,\n  Email: \"alice@example.com\",\n}\n```\n\n**Exception**: Field names optional for same-package types when field order is stable (e.g., test tables).\n\n---\n\n## Type Alias vs Type Definition\n\nUse type definitions (`type T1 T2`) for creating new types. Reserve type aliases (`type T1 = T2`) only for migration scenarios.\n\n**Type definition** (creates new type):\n```go\ntype UserID int  // New type - not assignable to int\n\nvar id UserID = 42\nvar n int = id  // Compile error - different types\n```\n\n**Type alias** (same type, different name):\n```go\ntype StringAlias = string  // Alias - same type as string\n\nvar s StringAlias = \"hello\"\nvar str string = s  // OK - same type\n```\n\n**When to use aliases**:\n```go\n// During API migration only\npackage oldpkg\n\nimport \"newpkg\"\n\n// Temporary alias during migration period\ntype OldUserID = newpkg.UserID\n\n// Deprecated: Use newpkg.UserID instead\n```\n\n**Why**: Type definitions provide type safety. Aliases are rarely needed and create confusion.\n\n---\n\n## Avoid init()\n\nMake code deterministic and testable. Only use `init()` for specific scenarios. Most initialization should happen explicitly.\n\n**Avoid** in init():\n- I/O operations\n- Environment variable access\n- Global state manipulation\n- Anything that can fail\n\n**Bad**:\n```go\nvar config Config\n\nfunc init() {\n  config = loadConfig()  // I/O in init - can fail, hard to test\n}\n```\n\n**Good**:\n```go\nvar defaultConfig = Config{\n  Timeout: 10 * time.Second,\n}\n\nfunc NewConfig() (*Config, error) {\n  return loadConfig()  // Explicit, testable, can handle errors\n}\n```\n\n### Acceptable init() Uses\n\n**1. Database driver registration** (pluggable hooks):\n```go\npackage postgres\n\nimport (\n  \"database/sql\"\n  _ \"github.com/lib/pq\"  // Registers postgres driver in init()\n)\n\n// The imported package's init() registers the driver:\n// func init() {\n//   sql.Register(\"postgres\", &Driver{})\n// }\n```\n\n**2. Deterministic precomputation** (no I/O, no failures):\n```go\npackage math\n\nvar powersOfTwo [64]int\n\nfunc init() {\n  // Pure computation, deterministic, cannot fail\n  for i := range powersOfTwo {\n    powersOfTwo[i] = 1 << i\n  }\n}\n```\n\n**3. Complex expressions requiring loops**:\n```go\npackage constants\n\nvar httpStatusText = map[int]string{}\n\nfunc init() {\n  // Can't use map literal for computed values\n  for code := 200; code < 600; code++ {\n    httpStatusText[code] = computeStatusText(code)\n  }\n}\n```\n\n**Why these are acceptable**: Deterministic, cannot fail, no external dependencies, improve performance by computing once at startup.\n\n---\n\n## Functional Options\n\nFor APIs with optional parameters that may expand over time.\n\n**Pattern**:\n```go\ntype options struct {\n  cache  bool\n  logger *zap.Logger\n}\n\ntype Option interface {\n  apply(*options)\n}\n\ntype cacheOption bool\n\nfunc (c cacheOption) apply(opts *options) {\n  opts.cache = bool(c)\n}\n\nfunc WithCache(c bool) Option {\n  return cacheOption(c)\n}\n\nfunc Open(addr string, opts ...Option) (*Connection, error) {\n  options := options{\n    cache:  defaultCache,\n    logger: zap.NewNop(),\n  }\n\n  for _, o := range opts {\n    o.apply(&options)\n  }\n\n  // Use options\n}\n```\n\n**Benefits**:\n- Optional parameters only when needed\n- Future extensibility without breaking changes\n- Self-documenting API\n\n---\n\n## Option Struct Pattern\n\nFor functions with many optional parameters where most have sensible defaults, consider option structs as a simpler alternative to functional options.\n\n**When to use**:\n- Many optional parameters (3+)\n- Most fields have sensible defaults\n- Callers typically specify only 1-2 options\n- Simpler than functional options for straightforward cases\n\n**Pattern**:\n```go\ntype ClientOptions struct {\n  Timeout     time.Duration\n  Retries     int\n  Logger      *log.Logger\n  EnableCache bool\n}\n\nfunc NewClient(addr string, opts *ClientOptions) (*Client, error) {\n  // Apply defaults for nil options\n  if opts == nil {\n    opts = &ClientOptions{\n      Timeout:     30 * time.Second,\n      Retries:     3,\n      Logger:      log.Default(),\n      EnableCache: true,\n    }\n  }\n\n  // Use opts fields\n  return &Client{\n    addr:    addr,\n    timeout: opts.Timeout,\n    retries: opts.Retries,\n    logger:  opts.Logger,\n    cache:   opts.EnableCache,\n  }, nil\n}\n```\n\n**Usage**:\n```go\n// Use defaults\nclient, _ := NewClient(\"localhost:8080\", nil)\n\n// Override specific options\nclient, _ := NewClient(\"localhost:8080\", &ClientOptions{\n  Retries: 5,  // Other fields use defaults\n})\n```\n\n**Comparison with Functional Options**:\n\n| Aspect | Option Struct | Functional Options |\n|--------|--------------|-------------------|\n| Simplicity | Simpler, less code | More complex |\n| Extensibility | Requires version management | Seamlessly extensible |\n| Discovery | IDE autocomplete shows all options | Must know function names |\n| Best for | Stable APIs, many defaults | Evolving APIs, few overrides |\n\n---\n\n## Generic Interface Patterns\n\nUse generic interfaces (Go 1.18+) for type-safe constraints and self-referential patterns.\n\n**Self-referential constraints**:\n```go\n// Constraint where types must compare with themselves\ntype Comparer[T any] interface {\n  Compare(T) int\n}\n\n// Generic function using the constraint\nfunc BinarySearch[E Comparer[E]](items []E, target E) int {\n  low, high := 0, len(items)-1\n\n  for low <= high {\n    mid := (low + high) / 2\n    cmp := target.Compare(items[mid])\n\n    if cmp == 0 {\n      return mid\n    } else if cmp < 0 {\n      high = mid - 1\n    } else {\n      low = mid + 1\n    }\n  }\n\n  return -1\n}\n```\n\n**Type-safe builder pattern**:\n```go\ntype Builder[T any] interface {\n  Build() T\n}\n\nfunc BuildAll[T any, B Builder[T]](builders []B) []T {\n  results := make([]T, len(builders))\n  for i, b := range builders {\n    results[i] = b.Build()\n  }\n  return results\n}\n```\n\n**When to use**:\n- Types that need to reference themselves in method signatures\n- Abstracting operations across varied types with different constraints\n- Type-safe collections and algorithms\n\n**Benefits**:\n- Eliminates `interface{}` and type assertions\n- Compile-time type safety\n- Clearer API contracts\n\n---\n\n## Package Organization\n\n### Group Related Types\n\nGroup related types in the same package when client code typically needs both. Use godoc grouping as a guide for package boundaries.\n\n**Good**:\n```go\npackage user\n\n// Related types together\ntype User struct { }\ntype UserRepository interface { }\ntype UserService struct { }\n```\n\n**Consider splitting when**:\n- Package has thousands of lines in a single file\n- Types have distinct responsibilities with separate clients\n- Clear separation improves testability\n\n### Package Size\n\nAvoid single-file packages with thousands of lines. Split into multiple files by:\n- Responsibility (handlers.go, models.go, repository.go)\n- Type groupings (user.go, account.go, payment.go)\n\nNo strict line limits, but consider splitting when navigation becomes difficult.\n\n### Package Names as Context\n\nPackage names provide context. Don't repeat package name in type names.\n\n**Bad**:\n```go\npackage user\n\ntype UserService struct { }  // Redundant\n```\n\n**Good**:\n```go\npackage user\n\ntype Service struct { }  // Used as user.Service\n```\n",
        "plugins/go-style-guide/skills/go-style-guide/references/concurrency.md": "# Concurrency\n\nPatterns for goroutines, mutexes, channels, and race condition prevention.\n\n---\n\n## Atomic Operations\n\nUse `sync/atomic` types for type-safe atomic operations (Go 1.19+).\n\n**Bad**:\n```go\nimport \"sync/atomic\"\n\ntype foo struct {\n  running int32  // atomic\n}\n\nfunc (f *foo) start() {\n  if atomic.SwapInt32(&f.running, 1) == 1 {\n    return  // already running\n  }\n}\n```\n\n**Good**:\n```go\nimport \"sync/atomic\"\n\ntype foo struct {\n  running atomic.Bool\n}\n\nfunc (f *foo) start() {\n  if f.running.Swap(true) {\n    return  // already running\n  }\n}\n```\n\n**Why**: Type safety and convenience methods reduce errors. The `sync/atomic` package provides `Bool`, `Int32`, `Int64`, `Uint32`, `Uint64`, `Uintptr`, `Pointer[T]`, and `Value` types.\n\n---\n\n## Avoid Mutable Globals\n\n**Bad**:\n```go\nvar db *sql.DB\n\nfunc init() {\n  db = connectDB()  // Mutable global state\n}\n\nfunc GetDB() *sql.DB {\n  return db\n}\n```\n\n**Good**:\n```go\ntype Config struct {\n  DB *sql.DB\n}\n\nfunc New() (*Config, error) {\n  db, err := connectDB()\n  if err != nil {\n    return nil, err\n  }\n  return &Config{DB: db}, nil\n}\n```\n\n**Why**: Dependency injection improves testability by allowing mock substitution.\n\n---\n\n## Don't Fire-and-Forget Goroutines\n\nEvery spawned goroutine needs:\n- A predictable stop time, OR\n- A signaling mechanism to request stopping\n- A way to wait for completion\n\n**Bad**:\n```go\ngo func() {\n  for {\n    flush()\n    time.Sleep(delay)\n  }\n}()  // No way to stop this\n```\n\n**Good**:\n```go\ntype Worker struct {\n  stop chan struct{}\n  done chan struct{}\n}\n\nfunc (w *Worker) Start() {\n  go func() {\n    defer close(w.done)\n    ticker := time.NewTicker(delay)\n    defer ticker.Stop()\n\n    for {\n      select {\n      case <-ticker.C:\n        flush()\n      case <-w.stop:\n        return\n      }\n    }\n  }()\n}\n\nfunc (w *Worker) Stop() {\n  close(w.stop)\n  <-w.done\n}\n```\n\n**Test with `go.uber.org/goleak`**:\n```go\nfunc TestWorker(t *testing.T) {\n  defer goleak.VerifyNone(t)\n\n  w := &Worker{\n    stop: make(chan struct{}),\n    done: make(chan struct{}),\n  }\n  w.Start()\n  w.Stop()\n}\n```\n\n---\n\n## No Goroutines in init()\n\n**Bad**:\n```go\nfunc init() {\n  go monitor()  // Can't control lifecycle\n}\n```\n\n**Good**:\n```go\ntype Monitor struct {\n  stop chan struct{}\n}\n\nfunc (m *Monitor) Start() {\n  go m.run()\n}\n\nfunc (m *Monitor) Close() error {\n  close(m.stop)\n  return nil\n}\n```\n\n**Why**: Objects should have explicit lifecycle methods like `Close()` or `Shutdown()`.\n\n---\n\n## Closure Variable Capture\n\nClosures capture variables from their enclosing scope by reference. When multiple goroutines write to the same captured variable, this causes a data race.\n\n**Bad** (captures outer variable):\n```go\nfunc Run() error {\n  err := setup()\n  if err != nil {\n    return err\n  }\n\n  var wg sync.WaitGroup\n  wg.Go(func() {\n    err = taskA()  // Race: writes to captured outer err\n  })\n  wg.Go(func() {\n    err = taskB()  // Race: writes to captured outer err\n  })\n  wg.Wait()\n  return err\n}\n```\n\n**Good** (local variable):\n```go\nwg.Go(func() {\n  err := taskA()  // New local variable\n  // handle err locally\n})\n```\n\n**Good** (named return):\n```go\nwg.Go(func() (err error) {\n  err = taskA()  // Named return is local to closure\n  return\n})\n```\n\n**Why**: The one-character difference between `err =` and `err :=` determines whether a closure captures an outer variable or creates a new local one.\n\n**Debugging**: Use `go build -gcflags='-d closure=1'` to print captured variables.\n\n**Note**: Go 1.22+ fixed range loop variable capture, but general closure capture remains a manual concern.\n\n---\n\n## Stdlib Concurrent Safety Caveats\n\nTypes documented as \"safe for concurrent use\" (like `http.Client`) typically mean **some methods** are safe - not that all fields or operations are thread-safe. Modifying struct fields concurrently causes data races.\n\n**Bad** (modifying shared client fields concurrently):\n```go\ntype Fetcher struct {\n  client *http.Client\n}\n\nfunc (f *Fetcher) FetchWithRedirects(ctx context.Context, url string) (*http.Response, error) {\n  f.client.CheckRedirect = customPolicy  // Race if called concurrently!\n  return f.client.Get(url)\n}\n\nfunc (f *Fetcher) FetchNoRedirects(ctx context.Context, url string) (*http.Response, error) {\n  f.client.CheckRedirect = nil  // Race!\n  return f.client.Get(url)\n}\n```\n\n**Good** (inject pre-configured clients):\n```go\ntype Fetcher struct {\n  clientWithRedirects *http.Client\n  clientNoRedirects   *http.Client\n}\n\nfunc NewFetcher() *Fetcher {\n  return &Fetcher{\n    clientWithRedirects: &http.Client{CheckRedirect: customPolicy},\n    clientNoRedirects:   &http.Client{CheckRedirect: func(*http.Request, []*http.Request) error {\n      return http.ErrUseLastResponse\n    }},\n  }\n}\n\nfunc (f *Fetcher) FetchWithRedirects(ctx context.Context, url string) (*http.Response, error) {\n  return f.clientWithRedirects.Get(url)\n}\n\nfunc (f *Fetcher) FetchNoRedirects(ctx context.Context, url string) (*http.Response, error) {\n  return f.clientNoRedirects.Get(url)\n}\n```\n\n**Why**: \"Safe for concurrent use\" means method calls (Get, Do) are synchronized internally. Field modification is not protected and requires external synchronization or separate instances.\n\n**Common types affected**: `http.Client`, `http.Transport`, `sql.DB` configuration fields\n\n**Rule**: Configure stdlib types at construction time. If goroutines need different configurations, inject separate pre-configured instances.\n\n**Note**: Most stdlib types (`bytes.Buffer`, slices, maps) are NOT thread-safe. When passing `io.Writer`/`io.Reader` to libraries you don't control, wrap with a synchronized adapter that locks in `Write()`/`Read()`.\n\n---\n\n## Mutex and Data Scope Mismatch\n\nA mutex only synchronizes access when all goroutines share the **same** mutex instance. Creating a new mutex per-request while sharing the underlying data provides no synchronization.\n\n**Bad** (per-request mutex, shared data):\n```go\nvar globalData = map[string]int{\"a\": 1}\n\ntype Service struct {\n  data map[string]int\n  mu   sync.Mutex\n}\n\nfunc NewService() *Service {\n  return &Service{\n    data: globalData,  // Shallow copy - shares underlying map!\n    mu:   sync.Mutex{},  // New mutex per call - no shared synchronization\n  }\n}\n\nfunc Handler(w http.ResponseWriter, r *http.Request) {\n  svc := NewService()  // Each request gets own mutex\n  svc.mu.Lock()\n  defer svc.mu.Unlock()\n  svc.data[\"key\"] = 42  // Race! Different mutexes, same map\n}\n```\n\n**Good** (Option 1 - global mutex for global data):\n```go\nvar (\n  globalData = map[string]int{\"a\": 1}\n  globalMu   sync.Mutex\n)\n\nfunc Handler(w http.ResponseWriter, r *http.Request) {\n  globalMu.Lock()\n  defer globalMu.Unlock()\n  globalData[\"key\"] = 42  // All handlers share same mutex\n}\n```\n\n**Good** (Option 2 - deep copy for per-request isolation):\n```go\nfunc NewService() *Service {\n  return &Service{\n    data: maps.Clone(globalData),  // Deep copy - isolated data\n    mu:   sync.Mutex{},  // Own mutex for own data\n  }\n}\n```\n\n**Why**: Go's struct assignment is shallow - maps and slices copy the pointer, not the data. The mutex and data must have matching scope.\n\n**Rule**: Mutex scope must match data scope. 1 mutex for N goroutines accessing shared data, or N mutexes for N isolated copies.\n\n---\n\n## Specify Channel Direction\n\nAlways specify channel direction (`<-chan`, `chan<-`) in function signatures to prevent accidental misuse and document intent.\n\n**Bad** (bidirectional allows misuse):\n```go\nfunc process(ch chan int) {\n  // Could accidentally send when should only receive\n  val := <-ch\n}\n```\n\n**Good** (direction constraints):\n```go\n// Send-only parameter\nfunc produce(ch chan<- int) {\n  ch <- 42\n}\n\n// Receive-only parameter\nfunc consume(ch <-chan int) {\n  val := <-ch\n}\n\n// Bidirectional only when truly needed\nfunc bridge(in <-chan int, out chan<- int) {\n  for v := range in {\n    out <- v\n  }\n}\n```\n\n**Why**: Channel direction constraints:\n- Prevent accidental misuse (sending on receive-only channel)\n- Document function intent clearly\n- Enable compile-time safety\n\n---\n\n## Channel Size\n\nUse buffer sizes of **zero** (unbuffered) or **one** only.\n\n**Bad**:\n```go\nc := make(chan int, 64)  // Why 64? What happens at 65?\n```\n\n**Good**:\n```go\nc := make(chan int)      // Unbuffered - synchronous\nc := make(chan int, 1)   // Buffered by 1 - specific use case\n```\n\n**Why**: Larger buffer sizes require extensive justification regarding overflow prevention and blocking behavior.\n\n---\n\n## Zero-value Mutexes\n\n**Bad**:\n```go\nmu := new(sync.Mutex)\nmu.Lock()\n```\n\n**Good**:\n```go\nvar mu sync.Mutex\nmu.Lock()\n```\n\n**Why**: `sync.Mutex` and `sync.RWMutex` have valid zero values. Use `var` declaration for clarity.\n\n---\n\n## Don't Copy Types with Sync Primitives\n\nDon't copy types containing synchronization primitives (`sync.Mutex`, `sync.Cond`, etc.) or types with pointer-only methods.\n\n**Bad**:\n```go\ntype Counter struct {\n  mu    sync.Mutex\n  count int\n}\n\nfunc (c Counter) Inc() {  // Value receiver copies mutex!\n  c.mu.Lock()\n  defer c.mu.Unlock()\n  c.count++\n}\n\n// Copying the struct copies the mutex\nc1 := Counter{}\nc2 := c1  // Bug - copies mutex in locked/unlocked state\n```\n\n**Good**:\n```go\ntype Counter struct {\n  mu    sync.Mutex\n  count int\n}\n\nfunc (c *Counter) Inc() {  // Pointer receiver - no copy\n  c.mu.Lock()\n  defer c.mu.Unlock()\n  c.count++\n}\n```\n\n**Why**: Copying a `sync.Mutex` or similar types breaks synchronization guarantees and causes undefined behavior.\n",
        "plugins/go-style-guide/skills/go-style-guide/references/errors.md": "# Error Handling\n\nPatterns for error types, wrapping, aggregation, and panic avoidance.\n\n---\n\n## Error Types\n\nChoose error approach based on needs:\n\n| Error matching needed? | Error has dynamic message? | Approach |\n|------------------------|----------------------------|----------|\n| No | No | `errors.New` |\n| No | Yes | `fmt.Errorf` |\n| Yes | No | Top-level `var` with `errors.New` |\n| Yes | Yes | Custom error type |\n\n**Examples**:\n\n```go\n// No matching, static\nerr := errors.New(\"timeout\")\n\n// No matching, dynamic\nerr := fmt.Errorf(\"connection to %s failed\", host)\n\n// Matching, static\nvar ErrTimeout = errors.New(\"timeout\")\n\n// Matching, dynamic\ntype ConfigError struct {\n  Path string\n  Err  error\n}\n\nfunc (e *ConfigError) Error() string {\n  return fmt.Sprintf(\"config error at %s: %v\", e.Path, e.Err)\n}\n```\n\n---\n\n## Error Wrapping\n\nUse `%w` when callers should access underlying errors. Use `%v` to obfuscate.\n\n**Bad**:\n```go\nreturn fmt.Errorf(\"failed to create new store: %w\", err)\n```\n\n**Good**:\n```go\nreturn fmt.Errorf(\"new store: %w\", err)\n```\n\n**Why**: Avoid redundant \"failed to\" phrases. Error chains already show the failure path.\n\n### Error Chain Structure\n\nPlace `%w` at the end of error strings to mirror the error chain structure (newest to oldest):\n\n```go\n// Good - %w at end mirrors chain structure\nreturn fmt.Errorf(\"read config: %w\", err)\n// Error chain: \"read config: open file: permission denied\"\n//              [newest]    [middle]   [oldest/root cause]\n```\n\nError chains form newest-to-oldest hierarchies. Placing `%w` at the end makes the chain structure clear when reading error messages.\n\n### Error Translation at Boundaries\n\nAt system boundaries (RPC, IPC, storage), use `%v` instead of `%w` to translate errors into your canonical error space:\n\n```go\n// At RPC boundary - translate to gRPC status\nfunc (s *Server) GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.User, error) {\n  user, err := s.db.FindUser(req.Id)\n  if err != nil {\n    // Use %v to prevent exposing internal error types across RPC\n    return nil, status.Errorf(codes.NotFound, \"user %s: %v\", req.Id, err)\n  }\n  return user, nil\n}\n\n// Within service - preserve error chain with %w\nfunc (db *DB) FindUser(id string) (*User, error) {\n  user, err := db.query(id)\n  if err != nil {\n    // Use %w to maintain error chain for internal inspection\n    return nil, fmt.Errorf(\"query user %s: %w\", id, err)\n  }\n  return user, nil\n}\n```\n\n**Why**: System boundaries need canonical error representations. Internal code preserves error chains for debugging.\n\n---\n\n## Error Naming\n\n- **Exported errors**: Use `Err` prefix (e.g., `ErrCouldNotOpen`)\n- **Unexported errors**: Use `err` prefix (e.g., `errInvalidInput`)\n- **Custom error types**: Use `Error` suffix (e.g., `NotFoundError`)\n\n**Examples**:\n```go\nvar (\n  ErrNotFound     = errors.New(\"not found\")\n  errInvalidInput = errors.New(\"invalid input\")\n)\n\ntype ValidationError struct {\n  Field string\n}\n```\n\n---\n\n## Error String Format\n\nError strings should not be capitalized (unless beginning with proper nouns or acronyms) and should not end with punctuation. Errors typically appear within larger context where they're interpolated into other messages.\n\n**Bad**:\n```go\nreturn errors.New(\"Something bad happened.\")\nreturn errors.New(\"Configuration failed\")\n```\n\n**Good**:\n```go\nreturn errors.New(\"something bad happened\")\nreturn errors.New(\"configuration failed\")\n```\n\n**Why**: Error messages appear in larger context:\n```go\nfmt.Printf(\"operation failed: %v\", err)\n// Produces: \"operation failed: something bad happened\"\n// Not: \"operation failed: Something bad happened.\"\n```\n\n**Exception**: Proper nouns and acronyms maintain their casing:\n```go\nreturn errors.New(\"GitHub API unavailable\")\nreturn fmt.Errorf(\"failed to connect to PostgreSQL: %w\", err)\n```\n\n---\n\n## Handle Errors Once\n\nEach error should be handled at **one** point in the call stack.\n\n**Bad**:\n```go\nfunc writeFile(path string, data []byte) error {\n  if err := os.WriteFile(path, data, 0644); err != nil {\n    log.Printf(\"write failed: %v\", err)  // Logs AND returns\n    return fmt.Errorf(\"write %s: %w\", path, err)\n  }\n  return nil\n}\n```\n\n**Good**:\n```go\nfunc writeFile(path string, data []byte) error {\n  if err := os.WriteFile(path, data, 0644); err != nil {\n    return fmt.Errorf(\"write %s: %w\", path, err)  // Return with context\n  }\n  return nil\n}\n\n// Caller decides to log\nif err := writeFile(path, data); err != nil {\n  log.Printf(\"failed: %v\", err)\n}\n```\n\n**Why**: Handling errors at multiple levels creates redundant logging and makes control flow unclear.\n\n---\n\n## Error Aggregation\n\nUse `errors.Join` (Go 1.20+) to combine multiple errors.\n\n**Example**:\n```go\nfunc processAll(items []Item) error {\n  var errs []error\n\n  for _, item := range items {\n    if err := process(item); err != nil {\n      errs = append(errs, fmt.Errorf(\"process %s: %w\", item.ID, err))\n    }\n  }\n\n  return errors.Join(errs...)  // Returns nil if errs is empty\n}\n```\n\n**Why**: `errors.Join` automatically returns `nil` for empty slices and properly wraps multiple errors for inspection with `errors.Is` and `errors.As`.\n\n**Checking aggregated errors**:\n```go\nerr := processAll(items)\nif errors.Is(err, ErrNotFound) {\n  // Returns true if any joined error is ErrNotFound\n}\n```\n\n---\n\n## Don't Panic\n\nProduction code must avoid panics. Return errors instead and let callers decide handling strategy.\n\n**Exceptions**:\n- Program initialization (main package)\n- Test failures using `t.Fatal` or `t.FailNow`\n\n**Bad**:\n```go\nfunc run(args []string) {\n  if len(args) == 0 {\n    panic(\"no arguments\")  // Don't panic in production\n  }\n}\n```\n\n**Good**:\n```go\nfunc run(args []string) error {\n  if len(args) == 0 {\n    return errors.New(\"no arguments\")\n  }\n  return nil\n}\n\nfunc main() {\n  if err := run(os.Args[1:]); err != nil {\n    log.Fatal(err)  // Only panic-equivalent in main\n  }\n}\n```\n\n---\n\n## Must Functions\n\nReserve the `MustXYZ` naming pattern for setup helpers that terminate the program on failure. These functions should only be called early in program startup, never in library code or at runtime.\n\n**Acceptable - program initialization**:\n```go\nvar defaultConfig = MustLoadConfig(\"config.yaml\")\n\nfunc MustLoadConfig(path string) *Config {\n  cfg, err := LoadConfig(path)\n  if err != nil {\n    log.Fatalf(\"failed to load config: %v\", err)\n  }\n  return cfg\n}\n\nfunc main() {\n  // defaultConfig available here\n}\n```\n\n**Bad - library function**:\n```go\npackage parser\n\n// Wrong - library functions shouldn't panic\nfunc MustParseJSON(data []byte) *Object {\n  obj, err := ParseJSON(data)\n  if err != nil {\n    panic(err)  // Forces panic on caller\n  }\n  return obj\n}\n```\n\n**Good - library function**:\n```go\npackage parser\n\n// Return error - let caller decide how to handle\nfunc ParseJSON(data []byte) (*Object, error) {\n  // ...\n}\n```\n\n**Why**: `MustXYZ` functions are appropriate only for initialization code where failure prevents meaningful execution. Library code should always return errors.\n\n---\n\n## Exit in Main\n\nCall `os.Exit` or `log.Fatal` **only in `main()`**.\n\n**Bad**:\n```go\nfunc run() {\n  if err := setup(); err != nil {\n    log.Fatal(err)  // Bypasses defers in caller\n  }\n}\n\nfunc main() {\n  defer cleanup()\n  run()\n}\n```\n\n**Good**:\n```go\nfunc run() error {\n  if err := setup(); err != nil {\n    return err\n  }\n  return nil\n}\n\nfunc main() {\n  defer cleanup()\n  if err := run(); err != nil {\n    log.Fatal(err)  // Only in main\n  }\n}\n```\n\n**Why**: Preserves `defer` cleanup and improves testability.\n\n---\n\n## Exit Once\n\nRefactor business logic into a separate function returning errors.\n\n**Pattern**:\n```go\nfunc main() {\n  if err := run(); err != nil {\n    log.Fatal(err)\n  }\n}\n\nfunc run() error {\n  // All business logic here\n  // Return errors instead of exiting\n}\n```\n",
        "plugins/go-style-guide/skills/go-style-guide/references/review-checklist.md": "# Review Checklist - Architectural & Semantic Patterns\n\nQuick reference for patterns requiring human judgment. Linter-caught issues (unhandled errors, type assertions, formatting, etc.) are handled by `golangci-lint`.\n\n**Focus**: Architecture, ownership, lifecycle, strategy - not syntax or common bugs.\n\n---\n\n## Critical Issues (Architecture & Safety)\n\n### Fire-and-Forget Goroutines\n```go\n// BAD - No lifecycle management\ngo func() {\n  for {\n    doWork()\n    time.Sleep(interval)\n  }\n}()\n\n// GOOD - Managed lifecycle with stop channel\ntype Worker struct {\n  stop chan struct{}\n  done chan struct{}\n}\n\nfunc (w *Worker) Start() {\n  go func() {\n    defer close(w.done)\n    ticker := time.NewTicker(interval)\n    defer ticker.Stop()\n\n    for {\n      select {\n      case <-ticker.C:\n        doWork()\n      case <-w.stop:\n        return\n      }\n    }\n  }()\n}\n\nfunc (w *Worker) Stop() {\n  close(w.stop)\n  <-w.done  // Wait for completion\n}\n```\n\n**Why critical**: Goroutines without lifecycle control leak resources and prevent graceful shutdown.\n\n---\n\n### Mutex Races\n```go\n// BAD - Not holding lock during access\ns.mu.Lock()\ns.mu.Unlock()\nreturn s.data  // Race!\n\n// GOOD\ns.mu.Lock()\ndefer s.mu.Unlock()\nreturn s.data\n```\n\n**Why critical**: Race conditions cause non-deterministic bugs. Run with `go test -race` to detect.\n\n**Note**: This requires runtime race detector, not caught by static linters.\n\n---\n\n### Closure Variable Capture\n```go\n// BAD - Captures outer variable\nfunc Run() error {\n  err := setup()\n  if err != nil {\n    return err\n  }\n\n  var wg sync.WaitGroup\n  wg.Go(func() {\n    err = taskA()  // Race: writes to captured outer err\n  })\n  wg.Go(func() {\n    err = taskB()  // Race: writes to captured outer err\n  })\n  wg.Wait()\n  return err\n}\n\n// GOOD - Local variable\nwg.Go(func() {\n  err := taskA()  // New local variable\n  // handle err locally\n})\n\n// GOOD - Named return\nwg.Go(func() (err error) {\n  err = taskA()  // Named return is local to closure\n  return\n})\n```\n\n**Why critical**: The one-character difference between `err =` and `err :=` determines whether a closure captures an outer variable or creates a new local one. Multiple goroutines writing to the same captured variable causes a data race.\n\n**Debugging**: `go build -gcflags='-d closure=1'` prints captured variables.\n\n**Note**: Go 1.22+ fixed range loop variable capture, but general closure capture remains a manual concern.\n\n---\n\n### Stdlib Concurrent Safety Caveats\n\nTypes documented as \"safe for concurrent use\" (like `http.Client`) typically mean **some methods** are safe - not that all fields or operations are thread-safe.\n\n```go\n// BAD - Modifying shared client fields concurrently\ntype Fetcher struct {\n  client *http.Client\n}\n\nfunc (f *Fetcher) FetchWithRedirects(ctx context.Context, url string) (*http.Response, error) {\n  f.client.CheckRedirect = customPolicy  // Race if called concurrently!\n  return f.client.Get(url)\n}\n\nfunc (f *Fetcher) FetchNoRedirects(ctx context.Context, url string) (*http.Response, error) {\n  f.client.CheckRedirect = nil  // Race!\n  return f.client.Get(url)\n}\n\n// GOOD - Inject pre-configured clients\ntype Fetcher struct {\n  clientWithRedirects *http.Client\n  clientNoRedirects   *http.Client\n}\n\nfunc NewFetcher() *Fetcher {\n  return &Fetcher{\n    clientWithRedirects: &http.Client{CheckRedirect: customPolicy},\n    clientNoRedirects:   &http.Client{CheckRedirect: func(*http.Request, []*http.Request) error {\n      return http.ErrUseLastResponse\n    }},\n  }\n}\n```\n\n**Why critical**: \"Safe for concurrent use\" means method calls are synchronized internally. Field modification is not protected.\n\n**Common types**: `http.Client`, `http.Transport`, `sql.DB` configuration fields\n\n**Rule**: Configure stdlib types at construction time. If goroutines need different configurations, use separate instances.\n\n**Note**: Most stdlib types (`bytes.Buffer`, slices, maps) are NOT thread-safe. When passing `io.Writer`/`io.Reader` to libraries you don't control, wrap with a synchronized adapter that locks in `Write()`/`Read()`.\n\n---\n\n### Mutex and Data Scope Mismatch\n\nA mutex only synchronizes access when all goroutines share the **same** mutex instance. Creating a new mutex per-request while sharing the underlying data provides no synchronization.\n\n```go\n// BAD - Per-request mutex, shared data\nvar globalData = map[string]int{\"a\": 1}\n\ntype Service struct {\n  data map[string]int\n  mu   sync.Mutex\n}\n\nfunc NewService() *Service {\n  return &Service{\n    data: globalData,  // Shallow copy - shares underlying map!\n    mu:   sync.Mutex{},  // New mutex per call - no shared synchronization\n  }\n}\n\nfunc Handler(w http.ResponseWriter, r *http.Request) {\n  svc := NewService()  // Each request gets own mutex\n  svc.mu.Lock()\n  defer svc.mu.Unlock()\n  svc.data[\"key\"] = 42  // Race! Different mutexes, same map\n}\n\n// FIX Option 1 - Global mutex for global data\nvar (\n  globalData = map[string]int{\"a\": 1}\n  globalMu   sync.Mutex\n)\n\nfunc Handler(w http.ResponseWriter, r *http.Request) {\n  globalMu.Lock()\n  defer globalMu.Unlock()\n  globalData[\"key\"] = 42  // All handlers share same mutex\n}\n\n// FIX Option 2 - Deep copy for per-request isolation\nfunc NewService() *Service {\n  return &Service{\n    data: maps.Clone(globalData),  // Deep copy - isolated data\n    mu:   sync.Mutex{},  // Own mutex for own data\n  }\n}\n```\n\n**Why critical**: Go's struct assignment is shallow - maps and slices copy the pointer, not the data. N mutexes guarding 1 map = no synchronization.\n\n**Rule**: Mutex scope must match data scope. 1 mutex for N goroutines accessing shared data, or N mutexes for N isolated copies.\n\n---\n\n### Panics in Production Code\n```go\n// BAD - Library code\nfunc ParseConfig(data []byte) *Config {\n  if len(data) == 0 {\n    panic(\"empty config\")  // Never panic in libraries!\n  }\n  ...\n}\n\n// GOOD - Return error\nfunc ParseConfig(data []byte) (*Config, error) {\n  if len(data) == 0 {\n    return nil, errors.New(\"empty config\")\n  }\n  ...\n}\n\n// ACCEPTABLE - main() or init() only\nfunc main() {\n  if len(os.Args) < 2 {\n    log.Fatal(\"missing argument\")  // OK in main\n  }\n}\n```\n\n**Why critical**: Panics in library code prevent callers from recovering. Only acceptable in `main()` and `init()`.\n\n---\n\n## Important Issues (Design & Patterns)\n\n### Concurrency\n\n#### Goroutines in init()\n```go\n// BAD\nfunc init() {\n  go monitor()  // Can't control lifecycle\n}\n\n// GOOD - Explicit lifecycle\ntype Monitor struct {\n  stop chan struct{}\n}\n\nfunc (m *Monitor) Start() {\n  go m.run()\n}\n\nfunc (m *Monitor) Close() error {\n  close(m.stop)\n  return nil\n}\n```\n\n**Why**: `init()` goroutines have no shutdown mechanism, affecting testability and predictability.\n\n---\n\n#### Channel Buffer Size > 1\n```go\n// BAD - Unjustified magic number\nc := make(chan int, 64)  // Why 64? What happens at 65?\n\n// GOOD\nc := make(chan int)      // Unbuffered - explicit synchronization\nc := make(chan int, 1)   // Buffered by 1 - specific use case\n```\n\n**Why**: Buffer sizes >1 require justification. How is overflow prevented? What are blocking semantics?\n\n---\n\n#### Manual Context Cancellation in Tests\n```go\n// BAD - Manual lifecycle\nfunc TestOperation(t *testing.T) {\n  ctx, cancel := context.WithCancel(context.Background())\n  defer cancel()\n\n  // test code\n}\n\n// GOOD (Go 1.24+) - Automatic cleanup\nfunc TestOperation(t *testing.T) {\n  ctx := t.Context()  // Auto-canceled when test ends\n\n  // test code\n}\n```\n\n**Why**: `t.Context()` ensures proper cleanup ordering and integrates with test lifecycle.\n\n---\n\n### Data Ownership\n\n#### Not Copying Slices/Maps at Boundaries\n```go\n// BAD\nfunc (d *Driver) SetTrips(trips []Trip) {\n  d.trips = trips  // Caller can mutate!\n}\n\nfunc (d *Driver) GetTrips() []Trip {\n  return d.trips  // Caller can mutate!\n}\n\n// GOOD (Go 1.21+)\nimport \"slices\"\n\nfunc (d *Driver) SetTrips(trips []Trip) {\n  d.trips = slices.Clone(trips)  // Defensive copy\n}\n\nfunc (d *Driver) GetTrips() []Trip {\n  return slices.Clone(d.trips)  // Defensive copy\n}\n```\n\n**Why**: Shared slice/map references violate encapsulation. Copy at API boundaries to maintain ownership.\n\n**Judgment required**: When performance matters, document shared ownership explicitly.\n\n---\n\n#### Global Mutable State\n```go\n// BAD\nvar cache = make(map[string]string)\n\nfunc Get(key string) string {\n  return cache[key]  // Hard to test, race-prone\n}\n\n// GOOD - Dependency injection\ntype Cache struct {\n  mu   sync.RWMutex\n  data map[string]string\n}\n\nfunc NewCache() *Cache {\n  return &Cache{data: make(map[string]string)}\n}\n\nfunc (c *Cache) Get(key string) string {\n  c.mu.RLock()\n  defer c.mu.RUnlock()\n  return c.data[key]\n}\n```\n\n**Why**: Global mutable state prevents testability and causes race conditions. Use dependency injection.\n\n---\n\n### API Design\n\n#### Embedded Types in Public Structs\n```go\n// BAD - Leaks implementation, prevents evolution\ntype ConcreteList struct {\n  AbstractList  // Exposes all AbstractList methods publicly!\n}\n\n// GOOD - Explicit delegation\ntype ConcreteList struct {\n  list *AbstractList  // Private field\n}\n\nfunc (c *ConcreteList) Add(e Entity) {\n  c.list.Add(e)  // Explicit method\n}\n```\n\n**Why**: Embedding in public structs couples API to implementation, preventing evolution. Use composition with explicit methods.\n\n---\n\n#### os.Exit or log.Fatal Outside main()\n```go\n// BAD - Library function\nfunc SaveConfig(cfg Config) {\n  if err := write(cfg); err != nil {\n    log.Fatal(err)  // Bypasses caller's defers!\n  }\n}\n\n// GOOD - Return error\nfunc SaveConfig(cfg Config) error {\n  if err := write(cfg); err != nil {\n    return fmt.Errorf(\"save config: %w\", err)\n  }\n  return nil\n}\n\nfunc main() {\n  if err := SaveConfig(cfg); err != nil {\n    log.Fatal(err)  // Only in main()\n  }\n}\n```\n\n**Why**: `log.Fatal()` and `os.Exit()` bypass `defer` statements and prevent callers from cleanup. Only use in `main()`.\n\n---\n\n### Error Handling\n\n#### Handling Errors Multiple Times\n```go\n// BAD - Logs AND returns (doubles observability)\nfunc processFile(path string) error {\n  data, err := os.ReadFile(path)\n  if err != nil {\n    log.Printf(\"read failed: %v\", err)  // Logged here\n    return fmt.Errorf(\"read %s: %w\", path, err)  // AND returned\n  }\n  return process(data)\n}\n\n// GOOD - Return with context, let caller decide\nfunc processFile(path string) error {\n  data, err := os.ReadFile(path)\n  if err != nil {\n    return fmt.Errorf(\"read %s: %w\", path, err)  // Caller logs if needed\n  }\n  return process(data)\n}\n\n// Caller handles observability\nif err := processFile(path); err != nil {\n  log.Printf(\"process failed: %v\", err)  // Logged once, at boundary\n}\n```\n\n**Why**: Handling errors at multiple levels creates redundant logging and makes observability boundaries unclear.\n\n**Judgment required**: Decide observability boundaries - where to log vs where to wrap and return.\n\n---\n\n#### Manual Error Aggregation\n```go\n// BAD - Manual collection\nvar errs []error\nfor _, item := range items {\n  if err := process(item); err != nil {\n    errs = append(errs, err)\n  }\n}\nif len(errs) > 0 {\n  return fmt.Errorf(\"errors: %v\", errs)\n}\n\n// GOOD - Use errors.Join (Go 1.20+)\nvar errs []error\nfor _, item := range items {\n  if err := process(item); err != nil {\n    errs = append(errs, fmt.Errorf(\"process %s: %w\", item.ID, err))\n  }\n}\nreturn errors.Join(errs...)  // Returns nil if empty\n```\n\n**Why**: `errors.Join()` handles nil slices correctly and enables `errors.Is`/`errors.As` on joined errors.\n\n**Judgment required**: Decide when to aggregate (batch processing) vs fail-fast (validation).\n\n---\n\n### Testing\n\n#### Complex Table Tests\n```go\n// BAD - Too many conditionals\ntests := []struct{\n  name        string\n  input       string\n  shouldErr   bool\n  shouldCall1 bool\n  shouldCall2 bool\n  check1      func()\n  check2      func()\n}{\n  // Complex branching logic in test table\n}\n\n// GOOD - Split into focused tests\nfunc TestSuccess(t *testing.T) {\n  // Simple, clear success case\n}\n\nfunc TestError(t *testing.T) {\n  // Simple, clear error case\n}\n\nfunc TestEdgeCase(t *testing.T) {\n  // Specific edge case\n}\n```\n\n**Why**: Table tests with excessive conditionals are hard to understand and maintain.\n\n**Judgment required**: When to use table-driven vs separate tests depends on test similarity and complexity.\n\n---\n\n#### time.Sleep in Tests\n```go\n// BAD - Slow, flaky test\nfunc TestTimeout(t *testing.T) {\n  done := make(chan bool)\n  go func() {\n    time.Sleep(5 * time.Second)  // Slow! Flaky!\n    done <- true\n  }()\n\n  select {\n  case <-done:\n    // success\n  case <-time.After(6 * time.Second):\n    t.Fatal(\"timeout\")\n  }\n}\n\n// GOOD - Deterministic with synctest (Go 1.25+)\nimport \"testing/synctest\"\n\nfunc TestTimeout(t *testing.T) {\n  synctest.Run(func() {\n    done := make(chan bool)\n    go func() {\n      time.Sleep(5 * time.Second)  // Executes instantly\n      done <- true\n    }()\n\n    synctest.Wait()  // Wait for goroutines to block\n    <-done           // Completes instantly\n  })\n}\n```\n\n**Why**: `time.Sleep()` in tests makes them slow and timing-dependent (flaky).\n\n**Judgment required**: When to use `synctest` vs real time depends on whether you're testing timing behavior or business logic.\n\n---\n\n#### Manual b.N Loop\n```go\n// BAD - Easy to forget timer management\nfunc BenchmarkOperation(b *testing.B) {\n  data := setupExpensive()\n\n  b.ResetTimer()  // Easy to forget!\n\n  for i := 0; i < b.N; i++ {\n    operation(data)\n  }\n\n  b.StopTimer()  // Also easy to forget\n  cleanup()\n}\n\n// GOOD - Automatic timer management (Go 1.24+)\nfunc BenchmarkOperation(b *testing.B) {\n  data := setupExpensive()  // Not timed\n\n  for b.Loop() {\n    operation(data)  // Automatically timed\n  }\n\n  cleanup()  // Not timed\n}\n```\n\n**Why**: `b.Loop()` handles timer management automatically and prevents measurement errors.\n\n---\n\n### Modernization (High-Value)\n\n#### Unsafe Path Joining\n```go\n// BAD - Path traversal vulnerability\nfunc serveFile(dir, name string) (*os.File, error) {\n  path := filepath.Join(dir, name)  // User can pass \"../../../etc/passwd\"\n  return os.Open(path)\n}\n\n// GOOD - Safe filesystem root (Go 1.24+)\nfunc serveFile(dir, name string) (*os.File, error) {\n  root, err := os.OpenRoot(dir)\n  if err != nil {\n    return nil, err\n  }\n  return root.Open(name)  // Enforces dir boundary\n}\n```\n\n**Why**: `filepath.Join()` doesn't prevent path traversal. `os.Root` provides filesystem-level safety.\n\n**Judgment required**: Use `os.Root` for security-critical file serving, not general path manipulation.\n\n---\n\n#### Manual Slice and Map Operations\n```go\n// BAD - Manual operations when stdlib provides\n// Slice copy\ncopied := make([]int, len(original))\ncopy(copied, original)\n\n// Map copy\nm2 := make(map[string]int, len(m))\nfor k, v := range m {\n  m2[k] = v\n}\n\n// GOOD - Use slices/maps package (Go 1.21+)\nimport (\n  \"maps\"\n  \"slices\"\n)\n\ncopied := slices.Clone(original)\nslices.Sort(items)\nm2 := maps.Clone(m)\n```\n\n**Why**: `slices`/`maps` packages provide tested, optimized operations.\n\n**Judgment required**: Use when it improves clarity. Manual operations are fine for performance-critical code with profiling justification.\n\n---\n\n#### Map/Slice Reallocation\n```go\n// BAD - Reallocates, loses capacity\nfor i := 0; i < iterations; i++ {\n  m = make(map[string]int)  // Allocates every iteration\n  // use m\n}\n\n// GOOD - Clear in place, retains capacity (Go 1.21+)\nm := make(map[string]int)\nfor i := 0; i < iterations; i++ {\n  clear(m)  // Clears but keeps allocated capacity\n  // use m\n}\n```\n\n**Why**: `clear()` avoids allocation overhead when reusing containers.\n\n**Judgment required**: Only optimize when profiling shows allocation pressure.\n\n---\n\n## Review Workflow\n\n1. **Critical Issues First**: Goroutine lifecycle, race conditions, panics in libraries\n2. **Important Issues**: Error handling strategy, data ownership, API design\n3. **Context Matters**: Some patterns are acceptable in specific contexts (panic in `main()`, global constants)\n4. **Defer to Linters**: Don't report issues that `golangci-lint` catches (unhandled errors, type assertions, formatting)\n\n## Common False Positives\n\n- `init()` for database driver registration (acceptable)\n- Panic in test code using `t.Fatal()` (acceptable)\n- Global constants (acceptable - only mutable globals are problematic)\n- Embedding in private structs for composition (sometimes acceptable)\n- Shared slices when explicitly documented (acceptable with justification)\n\n## Google vs Uber Style Differences\n\nThis guide synthesizes both Google and Uber Go style guides. Note these differences:\n\n### Assertion Libraries\n- **Our approach**: Use what codebase uses; don't add to new projects unless requested\n- **Google**: Recommends against assertion libraries (prefer manual checks)\n- **Uber**: Examples use assertion libraries (testify)\n- **Review guidance**: Don't flag either approach; focus on test logic, not assertion style\n\n### Line Length\n- **Our approach**: No fixed maximum; prefer refactoring over mechanical breaking\n- **Google**: No fixed maximum; prefer refactoring\n- **Uber (historically)**: 99 character soft limit\n- **Review guidance**: Focus on whether code could be refactored, not line counts\n\n### Test Helpers\n- **Our approach**: Helpers take `testing.T` and call `t.Helper()`\n- **Google**: Recommends helpers return errors instead of taking `testing.T`\n- **Uber**: Helpers take `testing.T`\n- **Review guidance**: Either pattern acceptable; ensure `t.Helper()` is used when taking `testing.T`\n\n### Interface Placement\n- **Both agree**: Interfaces generally belong in consumer packages\n- **Review guidance**: Flag only when interface placement prevents evolution; exceptions exist\n\n**Reference**:\n- [Google Go Style Guide](https://google.github.io/styleguide/go/)\n- [Uber Go Style Guide](https://github.com/uber-go/guide/blob/master/style.md)\n\n---\n\n## When in Doubt\n\nReference topic-specific files for detailed explanations:\n- `concurrency.md` - goroutines, mutexes, races, channels\n- `errors.md` - error types, wrapping, panic avoidance\n- `api-design.md` - interfaces, function design, data boundaries\n- `testing.md` - table tests, parallel tests, benchmarks\n- `style.md` - naming, documentation, code style\n\n**Remember**: Focus on design decisions that require understanding of intent, ownership, lifecycle, and architecture. Let linters handle syntax and common bugs.\n",
        "plugins/go-style-guide/skills/go-style-guide/references/style.md": "# Style Guide\n\nCore principles, naming conventions, documentation standards, and code style.\n\n---\n\n## Core Principles\n\nWhen writing Go code, apply these five principles in hierarchical order. Earlier principles take precedence over later ones:\n\n1. **Clarity**: Code should be understandable. The purpose and rationale should be clear to readers.\n2. **Simplicity**: Code should accomplish its goals in the most straightforward way possible.\n3. **Concision**: Code should have a high signal-to-noise ratio with minimal redundancy.\n4. **Maintainability**: Code should be easy for future programmers to modify correctly and safely.\n5. **Consistency**: Code should align with broader patterns in the codebase and ecosystem.\n\n### Using These Principles\n\nThese principles form a decision-making framework for situations not explicitly covered by the guide:\n\n- When code patterns conflict, apply these principles in order to determine the better approach\n- When multiple valid implementations exist, choose the one that best satisfies these principles\n- When making trade-offs, explain which principle takes precedence and why\n\n**Example**: A function could be made more concise by using clever shortcuts, but doing so would reduce clarity. In this case, **clarity takes precedence** - write the clearer version even if it's slightly longer.\n\n**Example**: Code could be made more consistent with outdated patterns in an old codebase, but modern Go has better approaches. Here **simplicity and maintainability** (using modern patterns) may outweigh strict consistency with legacy code.\n\n---\n\n## Documentation Standards\n\n### Document Non-Obvious Behavior\n\nDocument error-prone or non-obvious fields and behaviors. Don't restate what's already clear from the code.\n\n**Bad** (restates obvious):\n```go\n// Name is the user's name\nName string\n```\n\n**Good** (documents non-obvious behavior):\n```go\n// Name is the user's display name. May be empty if user hasn't set one.\n// In that case, use Email as fallback for display purposes.\nName string\n```\n\n### Document Concurrent Safety\n\nExplicitly document whether types or functions are safe for concurrent use.\n\n**Good**:\n```go\n// Cache is safe for concurrent use by multiple goroutines.\ntype Cache struct {\n  mu sync.RWMutex\n  data map[string]interface{}\n}\n\n// Get retrieves a value. Safe for concurrent use.\nfunc (c *Cache) Get(key string) interface{} {\n  c.mu.RLock()\n  defer c.mu.RUnlock()\n  return c.data[key]\n}\n```\n\n**Also good** (documenting NOT safe):\n```go\n// Buffer is NOT safe for concurrent use. Callers must synchronize access.\ntype Buffer struct {\n  data []byte\n}\n```\n\n### Document Resource Cleanup\n\nExplicitly document cleanup requirements for resources.\n\n**Good**:\n```go\n// Open returns a connection to the database.\n// Callers must call Close() when done to release resources.\nfunc Open(dsn string) (*DB, error) {\n  // ...\n}\n\n// Close releases database resources.\n// It's safe to call Close multiple times.\nfunc (db *DB) Close() error {\n  // ...\n}\n```\n\n### Document Error Conditions\n\nSpecify what error types are returned and under what conditions.\n\n**Bad**:\n```go\n// Parse parses the input.\nfunc Parse(input string) (*Result, error)\n```\n\n**Good**:\n```go\n// Parse parses the input string.\n// Returns ErrInvalidSyntax if input has syntax errors.\n// Returns ErrTooLarge if input exceeds maximum size.\nfunc Parse(input string) (*Result, error)\n```\n\n**When using errors.Is**:\n```go\n// FetchUser retrieves a user by ID.\n// Returns ErrNotFound if user doesn't exist (use errors.Is to check).\n// Returns ErrPermission if caller lacks access (use errors.Is to check).\nfunc FetchUser(ctx context.Context, id string) (*User, error)\n```\n\n### Context Cancellation Semantics\n\nContext cancellation semantics are usually implied. Only document non-standard behavior.\n\n**Don't document** (standard behavior):\n```go\n// Fetch retrieves data. Respects ctx cancellation.\nfunc Fetch(ctx context.Context) (*Data, error)\n```\n\n**Do document** (non-standard behavior):\n```go\n// Fetch retrieves data. Even if ctx is canceled, the fetch completes\n// and resources are cleaned up before returning ctx.Err().\nfunc Fetch(ctx context.Context) (*Data, error)\n```\n\n### Comments Should Explain WHY\n\nComments should explain why code does something, not what it does. The code itself shows what.\n\n**Bad** (explains what):\n```go\n// Loop through users\nfor _, user := range users {\n  // Check if user is active\n  if user.Active {\n    // Process the user\n    process(user)\n  }\n}\n```\n\n**Good** (explains why):\n```go\n// Only process active users to avoid sending notifications\n// to users who have disabled their accounts\nfor _, user := range users {\n  if user.Active {\n    process(user)\n  }\n}\n```\n\n---\n\n## Logging and Configuration\n\n### Logging Best Practices\n\nUse `log.Info(v)` over formatting functions when no string manipulation is needed.\n\n**Good**:\n```go\nlog.Info(\"processing started\")  // No formatting needed\nlog.Infof(\"processing %d items\", count)  // Formatting needed\n```\n\nUse `log.V()` levels for development tracing that should be disabled in production.\n\n**Example**:\n```go\nif log.V(2) {\n  log.Info(\"detailed debug information\")\n}\n```\n\nAvoid calling expensive functions when verbose logging is disabled:\n\n**Bad**:\n```go\nlog.V(2).Infof(\"state: %s\", expensiveDebugString())  // Always calls function\n```\n\n**Good**:\n```go\nif log.V(2) {\n  log.Infof(\"state: %s\", expensiveDebugString())  // Only calls when enabled\n}\n```\n\n### Configuration Flags\n\nDefine flags only in `package main`. Don't export flags as package side effects.\n\n**Bad**:\n```go\npackage config\n\nimport \"flag\"\n\n// Bad - package exports flags as side effect\nvar Port = flag.Int(\"port\", 8080, \"server port\")\n```\n\n**Good**:\n```go\npackage main\n\nimport \"flag\"\n\nfunc main() {\n  port := flag.Int(\"port\", 8080, \"server port\")\n  flag.Parse()\n  // use *port\n}\n```\n\n**Flag naming**: Use `snake_case` for flag names, `camelCase` for variable names.\n\n```go\nvar (\n  maxConnections = flag.Int(\"max_connections\", 100, \"maximum connections\")\n  readTimeout    = flag.Duration(\"read_timeout\", 30*time.Second, \"read timeout\")\n)\n```\n\n---\n\n## Performance Guidelines\n\n### Avoid Repeated String-to-Byte Conversions\n\n**Bad**:\n```go\nfor i := 0; i < b.N; i++ {\n  w.Write([]byte(\"Hello world\"))  // Repeated conversion\n}\n```\n\n**Good**:\n```go\ndata := []byte(\"Hello world\")\nfor i := 0; i < b.N; i++ {\n  w.Write(data)  // Convert once\n}\n```\n\n### Clear Built-in\n\nUse the `clear()` built-in (Go 1.21+) to efficiently clear maps and slices in place.\n\n**Maps**:\n```go\n// Old - reallocates, loses capacity\nm = make(map[string]int)\n\n// Modern - clears in place, retains capacity\nclear(m)\n```\n\n**Slices**:\n```go\n// Zeros all elements in place\ns := make([]int, 10)\ns[0] = 5\nclear(s)  // All elements now zero\n```\n\n**Performance benefit**: Retains allocated memory, avoiding GC pressure and reallocation costs.\n\n**When to use**:\n- Reusing maps/slices across iterations\n- Pooled objects that need clearing\n- Performance-sensitive code where allocation matters\n\n---\n\n## Code Style Standards\n\n### Line Length\n\nNo fixed maximum line length exists. If a line feels too long, prefer refactoring the code instead of mechanically splitting it.\n\nLong lines often indicate that code is doing too much:\n- Extract complex expressions into well-named variables\n- Break large functions into smaller, focused ones\n- Simplify nested logic\n\n**When line breaks are necessary**, indent continuation lines clearly to distinguish them from subsequent lines of code.\n\n### Switch and Break\n\nDon't use `break` at the end of switch clauses - Go automatically breaks. Use comments for empty clauses.\n\n**Good**:\n```go\nswitch x {\ncase 1:\n  doSomething()\n  // No break needed - automatic\ncase 2:\n  doOtherThing()\ncase 3:\n  // Intentionally empty\ndefault:\n  doDefault()\n}\n```\n\n### Variable Shadowing\n\nDistinguish between \"stomping\" (reassigning) and \"shadowing\" (creating new variable in inner scope). Prefer clear names over implicit shadowing.\n\n**Shadowing** (new variable in inner scope):\n```go\nfunc process() error {\n  err := firstOperation()\n\n  if err != nil {\n    // This 'err' shadows outer 'err'\n    err := wrapError(err)\n    log.Print(err)\n  }\n\n  return err  // Returns outer err, not wrapped one!\n}\n```\n\n**Better** (clear names):\n```go\nfunc process() error {\n  err := firstOperation()\n\n  if err != nil {\n    wrappedErr := wrapError(err)\n    log.Print(wrappedErr)\n  }\n\n  return err\n}\n```\n\n### Consistency\n\nMaintain uniform style within packages. Apply conventions at package level or larger.\n\n### Package Names\n\n- All lowercase\n- No underscores\n- Short, succinct\n- Singular (e.g., `net/url` not `net/urls`)\n- Avoid generic names: \"common,\" \"util,\" \"shared,\" \"lib\"\n\n### Function Names\n\n- Use `MixedCaps`\n- Tests may contain underscores for grouping: `TestFunc_Condition`\n- Don't use `Get` prefix for getters unless the concept inherently uses \"get\"\n\n**Good**:\n```go\nfunc Count() int { }       // Not GetCount()\nfunc User(id string) *User { }  // Not GetUser()\n```\n\n**Acceptable** (concept inherently uses \"get\"):\n```go\nfunc GetPage(url string) (*Page, error) { }  // HTTP GET\n```\n\n### Receiver Names\n\nReceiver names should be short (1-2 letters), abbreviate the type, and be consistent across all methods.\n\n**Good**:\n```go\ntype Client struct{}\n\nfunc (c *Client) Connect() { }\nfunc (c *Client) Disconnect() { }\n```\n\n**Bad**:\n```go\ntype Client struct{}\n\nfunc (client *Client) Connect() { }  // Too long\nfunc (cl *Client) Disconnect() { }   // Inconsistent\n```\n\n**Convention**: Use first letter(s) of type name, always the same across all methods.\n\n### Variable Names\n\nVariable name length should scale with scope size and inverse to usage frequency:\n- **Short names** for small scopes and frequently used variables: `i`, `c`, `buf`\n- **Longer names** for large scopes and infrequently used variables: `requestTimeout`, `maxRetryAttempts`\n\n**Good**:\n```go\n// Short scope, frequent use\nfor i, v := range items {\n  process(v)\n}\n\n// Large scope, infrequent use\nvar requestTimeout = 30 * time.Second\n```\n\n### Initialism Casing\n\nInitialisms should maintain consistent casing - all uppercase or all lowercase, never mixed.\n\n**Good**:\n```go\nvar url string              // All lowercase\nvar userID int             // ID all uppercase\ntype URLParser struct { }  // URL all uppercase\ntype HTTPClient struct { } // HTTP all uppercase\n```\n\n**Bad**:\n```go\nvar Url string             // Never Url\nvar userId int             // Never Id\ntype UrlParser struct { }  // Never Url\n```\n\n**Special cases** - preserve standard prose formatting:\n- iOS not IOS or Ios\n- gRPC not GRPC or Grpc\n\n### Group Similar Declarations\n\n**Good**:\n```go\nconst (\n  a = 1\n  b = 2\n)\n\nvar (\n  x = 1\n  y = 2\n)\n\ntype (\n  Area float64\n  Volume float64\n)\n```\n\nOnly group related items.\n\n### Top-level Variables\n\nOmit types if they match the expression.\n\n**Bad**:\n```go\nvar _s string = F()\n```\n\n**Good**:\n```go\nvar _s = F()\n```\n\n### Prefix Unexported Globals\n\nUse underscore prefix.\n\n**Example**:\n```go\nvar (\n  _defaultPort = 8080\n  _maxRetries  = 3\n)\n```\n\n**Exception**: Unexported error values use `err` prefix without underscore.\n\n### Local Variables\n\nChoose declaration form based on clarity and intent.\n\n**Prefer `:=`** with non-zero values:\n```go\nname := \"Alice\"\ncount := 42\nresult := process()\n```\n\n**Use `var`** for zero-value initialization when values are \"ready for later use\":\n```go\nvar filtered []int  // Will be populated later\nvar buf bytes.Buffer  // Zero value is ready to use\nvar mu sync.Mutex  // Zero value is ready to use\n```\n\n**Prefer `new()`** over empty composite literals for pointer-to-zero-value:\n```go\n// When you need *T with zero value\np := new(Person)  // Clearer than &Person{}\n```\n\n**Size hints**: Preallocate capacity only when final size is known through empirical analysis (profiling):\n```go\n// Don't guess\nitems := make([]Item, 0)  // Let it grow\n\n// Only if profiling shows benefit AND size is known\nitems := make([]Item, 0, expectedSize)\n```\n\n### Reduce Nesting\n\nHandle error cases first, returning early.\n\n**Bad**:\n```go\nif condition {\n  // Deep nesting\n  if anotherCondition {\n    // More nesting\n    if yetAnother {\n      // Success case buried\n    }\n  }\n}\n```\n\n**Good**:\n```go\nif !condition {\n  return err\n}\n\nif !anotherCondition {\n  return err\n}\n\nif !yetAnother {\n  return err\n}\n\n// Success case at top level\n```\n\n### Map Initialization\n\n- Use `make(map[T1]T2)` for empty maps\n- Use literals for fixed elements\n\n**Examples**:\n```go\nvar m map[string]int  // nil map - read-only\n\nm := make(map[string]int)  // Empty map - can write\n\nm := map[string]int{\n  \"a\": 1,\n  \"b\": 2,\n}\n```\n\n### Raw String Literals\n\nUse backticks to avoid escaping.\n\n**Bad**:\n```go\nmsg := \"unknown error:\\\"test\\\"\"\n```\n\n**Good**:\n```go\nmsg := `unknown error:\"test\"`\n```\n",
        "plugins/go-style-guide/skills/go-style-guide/references/testing.md": "# Testing Patterns\n\nPatterns for table-driven tests, parallel tests, benchmarks, and test helpers.\n\n---\n\n## CRITICAL: Don't Call t.Fatal from Goroutines\n\nCalling `t.Fatal()`, `t.FailNow()`, or `t.Skip()` from goroutines causes immediate panic and corrupted test state. These functions must only be called from the goroutine running the test function.\n\n**CRITICAL BUG - causes panic**:\n```go\nfunc TestConcurrent(t *testing.T) {\n  go func() {\n    result, err := fetchData()\n    if err != nil {\n      t.Fatal(err)  // PANIC! Called from wrong goroutine\n    }\n  }()\n}\n```\n\n**Correct - use t.Error and coordinate with main goroutine**:\n```go\nfunc TestConcurrent(t *testing.T) {\n  errCh := make(chan error, 1)\n\n  go func() {\n    result, err := fetchData()\n    if err != nil {\n      errCh <- err  // Send error to main goroutine\n      return\n    }\n    errCh <- nil\n  }()\n\n  if err := <-errCh; err != nil {\n    t.Fatalf(\"fetchData failed: %v\", err)  // Called from test goroutine\n  }\n}\n```\n\n**Alternative - use t.Error from goroutine**:\n```go\nfunc TestConcurrent(t *testing.T) {\n  var wg sync.WaitGroup\n  wg.Add(1)\n\n  go func() {\n    defer wg.Done()\n    result, err := fetchData()\n    if err != nil {\n      t.Error(err)  // Safe - doesn't terminate immediately\n      return\n    }\n  }()\n\n  wg.Wait()\n}\n```\n\n**Why**: `t.Fatal()` calls `runtime.Goexit()`, which is only safe from the test's main goroutine. From other goroutines, it causes panics and prevents proper test cleanup.\n\n---\n\n## Table-Driven Tests\n\nUse when testing against multiple input/output conditions.\n\n**Example**:\n```go\nfunc TestParseURL(t *testing.T) {\n  tests := []struct{\n    name     string\n    give     string\n    wantHost string\n    wantErr  bool\n  }{\n    {\n      name:     \"simple\",\n      give:     \"http://example.com\",\n      wantHost: \"example.com\",\n    },\n    {\n      name:    \"invalid\",\n      give:    \"://invalid\",\n      wantErr: true,\n    },\n  }\n\n  for _, tt := range tests {\n    t.Run(tt.name, func(t *testing.T) {\n      u, err := ParseURL(tt.give)\n\n      if tt.wantErr {\n        assert.Error(t, err)\n        return\n      }\n\n      assert.NoError(t, err)\n      assert.Equal(t, tt.wantHost, u.Host)\n    })\n  }\n}\n```\n\n**Benefits**:\n- Reduces redundancy\n- Easy to add new cases\n- Clear test data structure\n\n---\n\n## Avoid Test Complexity\n\nSplit table tests with excessive conditionals into separate test functions.\n\n**Bad**:\n```go\ntests := []struct{\n  give        string\n  shouldErr   bool\n  shouldCall1 bool\n  shouldCall2 bool\n  check1      func()\n  check2      func()\n}{\n  // Complex logic in table\n}\n```\n\n**Good**:\n```go\nfunc TestSuccess(t *testing.T) {\n  // Simple, focused test\n}\n\nfunc TestError(t *testing.T) {\n  // Simple, focused test\n}\n```\n\n---\n\n## Parallel Tests\n\nGo 1.22+ automatically scopes loop variables per-iteration, eliminating the need for manual capture.\n\n**Example**:\n```go\ntests := []struct{ give string }{{give: \"A\"}, {give: \"B\"}}\nfor _, tt := range tests {\n  t.Run(tt.give, func(t *testing.T) {\n    t.Parallel()\n    // tt is automatically per-iteration in Go 1.22+\n  })\n}\n```\n\n---\n\n## Context in Tests\n\nUse `t.Context()` (Go 1.24+) to obtain a context that is automatically canceled when the test completes.\n\n**Bad**:\n```go\nfunc TestService(t *testing.T) {\n  ctx, cancel := context.WithCancel(context.Background())\n  defer cancel()  // Manual cleanup\n\n  result, err := service.Run(ctx)\n  // ...\n}\n```\n\n**Good**:\n```go\nfunc TestService(t *testing.T) {\n  ctx := t.Context()  // Auto-canceled on cleanup\n\n  result, err := service.Run(ctx)\n  // ...\n}\n```\n\n---\n\n## Testing Time\n\nUse `testing/synctest` (Go 1.25+) for fast, deterministic testing of time-dependent code.\n\n**Problem**: Tests using `time.Sleep` or `time.After` are slow and can be flaky.\n\n**Old approach**:\n```go\nfunc TestTimeout(t *testing.T) {\n  done := make(chan bool)\n\n  go func() {\n    time.Sleep(5 * time.Second)  // Slow!\n    done <- true\n  }()\n\n  <-done\n}\n```\n\n**Modern approach with synctest**:\n```go\nimport \"testing/synctest\"\n\nfunc TestTimeout(t *testing.T) {\n  synctest.Run(func() {\n    done := make(chan bool)\n\n    go func() {\n      time.Sleep(5 * time.Second)  // Executes instantly\n      done <- true\n    }()\n\n    synctest.Wait()  // Wait for goroutines to block\n    <-done           // Completes instantly\n  })\n}\n```\n\n**Benefits**:\n- Tests run instantly (no actual sleeping)\n- Deterministic timing behavior\n- No modifications to production code\n- Detects deadlocks and timing bugs\n\n**When to use**: Any test involving `time.Sleep`, `time.After`, `time.NewTimer`, or `time.NewTicker`.\n\n---\n\n## Benchmark Loop Pattern\n\nUse `b.Loop()` (Go 1.24+) for cleaner benchmark code.\n\n**Old pattern**:\n```go\nfunc BenchmarkOperation(b *testing.B) {\n  // Expensive setup\n  data := setupData()\n\n  b.ResetTimer()  // Easy to forget!\n\n  for i := 0; i < b.N; i++ {\n    operation(data)\n  }\n\n  b.StopTimer()  // Also easy to forget\n  // Cleanup\n}\n```\n\n**Modern pattern**:\n```go\nfunc BenchmarkOperation(b *testing.B) {\n  // Expensive setup - timer not running yet\n  data := setupData()\n\n  for b.Loop() {\n    operation(data)  // Automatically measured\n  }\n\n  // Cleanup - timer already stopped\n}\n```\n\n**Benefits**:\n- Eliminates forgotten `ResetTimer`/`StopTimer` calls\n- Prevents dead-code elimination issues\n- Cleaner, less error-prone API\n- Setup/cleanup automatically excluded from timing\n\n---\n\n## Test Helper Patterns\n\nTest helpers should call `t.Helper()` to improve failure line reporting. Helpers take `testing.T` as a parameter, allowing them to report failures directly.\n\n**Pattern**:\n```go\nfunc setupUser(t *testing.T, name string) *User {\n  t.Helper()  // Failure reports point to caller, not this line\n\n  user, err := createUser(name)\n  if err != nil {\n    t.Fatalf(\"failed to setup user: %v\", err)\n  }\n  return user\n}\n\nfunc TestUserWorkflow(t *testing.T) {\n  user := setupUser(t, \"alice\")  // Failure points here, not inside helper\n  // ... test logic\n}\n```\n\n**Why**: `t.Helper()` marks the function as a test helper, causing failure messages to report the caller's location instead of the line inside the helper.\n\n**Benefits**:\n- Clear failure locations in test output\n- Helpers can fail tests directly\n- Simplified test code\n\n---\n\n## Test Failure Messages\n\nFormat test failure messages to include function name, inputs, actual value, and expected value.\n\n**Pattern**: `FunctionName(inputs) = actual, want expected`\n\n**Good**:\n```go\nfunc TestParseInt(t *testing.T) {\n  got, err := ParseInt(\"invalid\")\n  if err == nil {\n    t.Errorf(\"ParseInt(%q) succeeded, want error\", \"invalid\")\n  }\n\n  got, err = ParseInt(\"42\")\n  want := 42\n  if got != want {\n    t.Errorf(\"ParseInt(%q) = %d, want %d\", \"42\", got, want)\n  }\n}\n```\n\n**Conventions**:\n- Include function name\n- Include inputs if short\n- Show actual value BEFORE expected value\n- Use \"got\" for actual, \"want\" for expected\n- Be specific about what failed\n\n**Bad**:\n```go\nt.Errorf(\"wrong value\")  // What value? What was it? What was expected?\nt.Errorf(\"expected %d but got %d\", want, got)  // Backwards (expected first)\n```\n\n**Why**: Consistent, informative failure messages make test output easier to parse and debug.\n\n---\n\n## t.Error vs t.Fatal Choice\n\nChoose between `t.Error` and `t.Fatal` based on whether subsequent checks are meaningful.\n\n**Prefer t.Error** to reveal all failures in one run:\n```go\nfunc TestValidation(t *testing.T) {\n  result := Validate(input)\n\n  if result.Name == \"\" {\n    t.Error(\"Name should not be empty\")  // Continue checking\n  }\n\n  if result.Email == \"\" {\n    t.Error(\"Email should not be empty\")  // Shows both failures\n  }\n}\n```\n\n**Use t.Fatal** when subsequent checks would panic or be meaningless:\n```go\nfunc TestDatabase(t *testing.T) {\n  db, err := OpenDB()\n  if err != nil {\n    t.Fatalf(\"OpenDB failed: %v\", err)  // Can't continue without DB\n  }\n  defer db.Close()\n\n  // These would panic if db is nil\n  result := db.Query(\"SELECT * FROM users\")\n}\n```\n\n**In table-driven tests**:\n- Use `t.Fatal()` in subtests (per-entry failures)\n- Use `t.Error()` + `continue` in non-subtest loops\n\n**Why**: `t.Error` reveals multiple issues; `t.Fatal` prevents cascading failures.\n\n---\n\n## Test Assertions\n\n**Simple rule**: If the codebase already uses an assertion library (testify, etc.), continue using it for consistency. For new projects, use standard library testing patterns unless an assertion library is explicitly requested.\n\n**Standard library pattern**:\n```go\nfunc TestAdd(t *testing.T) {\n  got := Add(2, 3)\n  want := 5\n  if got != want {\n    t.Errorf(\"Add(2, 3) = %d, want %d\", got, want)\n  }\n}\n```\n\n**With assertion library** (if already in codebase):\n```go\nfunc TestAdd(t *testing.T) {\n  got := Add(2, 3)\n  assert.Equal(t, 5, got)\n}\n```\n\n**Why**: Consistency within a project matters more than the specific assertion style. Avoid adding dependencies to new projects without explicit need.\n",
        "plugins/multimedia/.claude-plugin/plugin.json": "{\n  \"name\": \"multimedia\",\n  \"description\": \"Video and audio file analysis, quality auditing, and encoding guidance\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"rterhaar\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"video\", \"audio\", \"encoding\", \"ffmpeg\", \"mediainfo\", \"quality\"]\n}\n",
        "plugins/multimedia/README.md": "# Multimedia Plugin\n\nVideo and audio file analysis, quality auditing, and encoding guidance for Claude Code.\n\n## Skills\n\n| Skill | Purpose |\n|-------|---------|\n| `video-audit` | Analyze video files to understand properties and recommend actions |\n| `artifact-detect` | Identify quality issues like compression artifacts, sharpening damage, upscaling problems |\n| `format-explain` | Educational explanations of video concepts (container vs codec, color space, etc.) |\n| `telecine-detect` | Detect telecined vs interlaced content and advise on proper handling |\n| `source-compare` | Compare multiple video sources to determine which has better quality |\n| `hdr-audit` | Analyze HDR metadata and detect fake/invalid HDR content |\n| `framerate-audit` | Analyze frame rate characteristics, CFR vs VFR, duplicate frames |\n| `subtitle-audit` | Analyze subtitle tracks for missing fonts, timing, format issues |\n\n## Prerequisites\n\nInstall these tools for full functionality:\n\n```bash\n# macOS\nbrew install mediainfo ffmpeg mkvtoolnix\n\n# Linux\napt install mediainfo ffmpeg mkvtoolnix\n\n# mpv (recommended media player)\nbrew install mpv  # macOS\napt install mpv   # Linux\n```\n\nOptional for advanced work:\n- Aegisub (subtitle editing)\n- Wobbly (telecine analysis)\n- VapourSynth (frame analysis)\n\n## Usage Examples\n\n- \"Audit this video file to check its quality\"\n- \"What's wrong with this video? It looks bad\"\n- \"Explain what BT.709 means\"\n- \"Is this video interlaced or telecined?\"\n- \"Compare these two video sources\"\n- \"Is this real HDR or fake?\"\n- \"Check if this video is CFR or VFR\"\n- \"Check the subtitle tracks in this file\"\n\n## References\n\nShared reference files in `references/`:\n\n| File | Content |\n|------|---------|\n| `quality-myths.md` | Common misconceptions about video quality |\n| `color-space.md` | Color matrices, range, chroma location |\n| `artifacts.md` | Artifact identification guide |\n| `tools.md` | Recommended and discouraged tools |\n| `encoding-commands.md` | ffmpeg/mpv command templates |\n| `telecine.md` | Telecine patterns and IVTC workflow |\n| `hdr.md` | HDR metadata and fake HDR detection |\n\n## External Resources\n\n- Source article: https://gist.github.com/arch1t3cht/b5b9552633567fa7658deee5aec60453\n- Interlacing guide: https://fieldbased.media\n- Wobbly IVTC: https://wobbly.encode.moe\n- JET encoding guide: https://jaded-encoding-thaumaturgy.github.io/JET-guide/\n- Frame comparisons: https://slow.pics\n",
        "plugins/multimedia/skills/artifact-detect/SKILL.md": "---\nname: artifact-detect\ndescription: This skill should be used when the user asks \"what's wrong with this video\", \"why does this video look bad\", \"detect video artifacts\", \"find quality issues\", \"video has artifacts\", \"identify compression artifacts\", or wants to diagnose specific quality problems in a video file.\n---\n\n# Video Artifact Detection\n\nIdentify and diagnose specific quality issues in video files. Focus on visual inspection techniques and artifact recognition, not fixing the issues.\n\n## Artifact Categories\n\n### 1. Compression Artifacts\n\n**Blocking (macroblocking)**\n- Square or rectangular blocks visible in motion or gradients\n- Caused by: Low bitrate encoding, high quantization\n- Where to look: Fast motion scenes, areas with subtle color gradients\n- Severity indicator: More visible = worse quality\n\n**Banding**\n- Visible steps/bands in color gradients instead of smooth transitions\n- Caused by: Low bit depth, aggressive quantization, source issues\n- Where to look: Sky gradients, dark shadows, smooth surfaces\n- Note: May be source issue, not encoding issue\n\n**Mosquito noise**\n- Fuzzy, shifting noise around sharp edges\n- Caused by: Lossy compression struggling with hard edges\n- Where to look: Text overlays, high-contrast edges, animation line art\n\n**DCT ringing**\n- Wavy patterns near sharp edges\n- Caused by: JPEG-based compression (H.264/H.265 use DCT)\n- Where to look: Around text, logos, sharp transitions\n\n### 2. Post-Processing Damage\n\n**Haloing/Ringing from sharpening**\n- Bright or dark outlines around edges\n- Caused by: Oversharpening filters (unsharp mask, warp sharp)\n- Where to look: Around character outlines, text, any hard edges\n- Key sign: Consistent glow on one side of lines throughout video\n\n**Line warping**\n- Straight lines appear wavy or distorted\n- Caused by: Aggressive sharpening algorithms\n- Where to look: Architecture, text, grid patterns\n\n**Oversharpened texture**\n- Unnatural crispness, \"crunchy\" appearance\n- Caused by: Excessive sharpening combined with compression\n- Comparison: Should look natural, not like HDR photo filter\n\n### 3. Upscaling Artifacts\n\n**AI hallucinations**\n- Invented details that weren't in source\n- Caused by: AI upscaling (Real-ESRGAN, Topaz, etc.)\n- Where to look: Fine textures (hair, fabric), text, repeating patterns\n- Key sign: Details that seem too sharp or don't match surrounding areas\n\n**Interpolation blur**\n- Soft, smeared appearance on edges\n- Caused by: Bilinear/bicubic upscaling\n- Where to look: Diagonal lines, fine details\n- Comparison: Native resolution looks sharper\n\n**Stairstepping (aliasing)**\n- Jagged edges on diagonal lines\n- Caused by: Poor upscaling algorithm, no anti-aliasing\n- Where to look: Diagonal lines, curved edges\n\n### 4. Color Issues\n\n**Wrong color matrix symptoms**\n- Overall color shift (too green, too magenta)\n- Caused by: BT.601/BT.709 mismatch\n- Key sign: Colors look \"off\" but evenly across entire frame\n- Verification: Compare to known-good source\n\n**Chroma shift**\n- Color misaligned with luminance\n- Caused by: Wrong chroma location, processing error\n- Where to look: Hard edges - look for color fringing on one side\n- Key sign: Consistent colored glow on same side of all edges\n\n**Double range compression**\n- Crushed blacks, blown highlights\n- Caused by: Limited range treated as full (or vice versa) multiple times\n- Where to look: Dark shadows (crushed to pure black), bright areas (clipped)\n\n### 5. Frame Rate Issues\n\n**Judder/stutter**\n- Uneven motion, frames \"jumping\"\n- Caused by: Frame rate mismatch, bad pulldown\n- Where to look: Panning shots, steady motion\n\n**Duplicate frames**\n- Same frame appears twice (or more)\n- Caused by: Frame rate conversion errors\n- Detection: Step frame-by-frame, compare adjacent frames\n\n**Telecine combing**\n- Horizontal lines across moving objects\n- Caused by: Improper deinterlacing of telecined content\n- Where to look: Horizontal edges during motion\n- Note: Not all combing = interlaced. May be telecined (3:2 pulldown)\n\n## Visual Inspection Checklist\n\nWhen examining a video for artifacts, inspect these areas in order:\n\n### High-Priority Areas\n\n1. **Dark gradients and shadows** - Reveals banding, blocking, crushed blacks\n2. **Strong colors (especially dark reds)** - Shows compression stress, color issues\n3. **Around sharp edges** - Reveals haloing, ringing, mosquito noise, chroma shift\n4. **On-screen text and logos** - Shows sharpness issues, compression artifacts clearly\n5. **Image borders** - Often reveals encoding problems first\n\n### Motion-Specific Checks\n\n6. **Panning shots** - Reveals judder, duplicate frames, motion blur issues\n7. **Fast action scenes** - Shows blocking, motion compensation failures\n8. **Static grain/texture areas** - Reveals whether grain was filtered or preserved\n\n### Comparison Technique\n\nTo spot artifacts effectively:\n- View at 100% zoom (1:1 pixel mapping)\n- Compare against known-good source if available\n- Use frame-by-frame stepping for temporal artifacts\n- Toggle between source and processed version rapidly\n\n## Reporting Findings\n\nWhen reporting detected artifacts, include:\n\n1. **Artifact type** - Name from categories above\n2. **Severity** - Subtle, moderate, severe\n3. **Location** - Where in frame, what type of content affected\n4. **Likely cause** - Based on artifact characteristics\n5. **Fixability** - Can be fixed (retag, re-encode from source) or permanent damage\n\n## Tools for Detection\n\n- **mpv**: Best playback, press `i` for technical info, frame step with `.` and `,`\n- **vs-preview**: Frame-by-frame comparison with VapourSynth\n- **SlowPics**: For sharing comparison screenshots (not imgsli - converts to JPEG)\n\n## Additional Resources\n\nFor detailed artifact catalogs with visual examples:\n- **`${CLAUDE_PLUGIN_ROOT}/references/artifacts.md`** - Comprehensive artifact guide\n- **`${CLAUDE_PLUGIN_ROOT}/references/color-space.md`** - Color issue diagnosis\n",
        "plugins/multimedia/skills/format-explain/SKILL.md": "---\nname: format-explain\ndescription: This skill should be used when the user asks \"what does this mediainfo mean\", \"explain this video format\", \"what is BT.709\", \"what is H.264\", \"container vs codec\", \"why is this 10bit\", \"what does limited range mean\", or wants educational explanations of video technical concepts.\n---\n\n# Video Format Explanations\n\nProvide clear, educational explanations of video technical concepts. Route to detailed references for in-depth information.\n\n## Core Concepts\n\n### Container vs Codec\n\n**Container formats** (mkv, mp4, webm, avi, mov):\n- Package that holds video, audio, subtitles, metadata\n- Like a ZIP file for media streams\n- Changing container = remuxing (fast, no quality loss)\n- Different containers support different features (mkv supports more subtitle formats than mp4)\n\n**Video codecs** (H.264/AVC, H.265/HEVC, VP9, AV1):\n- Algorithm that compresses/decompresses video data\n- The actual encoding that determines quality and file size\n- Changing codec = reencoding (slow, causes quality loss)\n\n**Key insight**: A file's extension (.mkv, .mp4) tells you the container, not the codec. An mkv and mp4 can contain identical H.264 video.\n\n### Codec vs Encoder\n\n**Codec/Format** (H.264, H.265): The specification/standard for how video is encoded\n**Encoder** (x264, x265, NVENC): Software that creates video in that format\n\nDifferent encoders produce different quality at same bitrate:\n- x264/x265: Best quality, slow (CPU-based)\n- NVENC/QuickSync: Fast, larger files for same quality (GPU-based)\n- Apple VideoToolbox: Middle ground on macOS\n\n### Resolution\n\n**What it is**: Pixel dimensions (1920x1080, 3840x2160)\n**What it's not**: Quality\n\nResolution indicates the pixel grid size, not visual fidelity. A badly encoded 4K video can look worse than a well-encoded 1080p video.\n\n**Native vs upscaled**: Many \"4K\" releases are upscaled from lower resolution masters. The file has 4K pixels but no more detail than the source.\n\n### Frame Rate\n\n**Common rates**:\n- 23.976 fps (24000/1001): Film, most streaming content\n- 24 fps: True film rate (rare in digital distribution)\n- 29.97 fps (30000/1001): NTSC video\n- 25 fps: PAL video\n- 59.94/60 fps: High frame rate, gaming content\n\n**23.976 vs 24**: Nearly identical to viewers. The fractional rate exists for NTSC compatibility reasons from analog TV era.\n\n**CFR vs VFR**:\n- CFR (Constant Frame Rate): Every frame has same duration\n- VFR (Variable Frame Rate): Frame durations vary (common in screen recordings, phone videos)\n\n### Bit Depth\n\n**8-bit**: Standard, 256 levels per color channel, 16.7 million colors\n**10-bit**: HDR standard, 1024 levels per channel, 1 billion+ colors\n**12-bit**: Professional/mastering, rarely seen in distribution\n\n**10-bit for 8-bit content**: Encoders sometimes use 10-bit encoding for 8-bit sources because it improves compression efficiency (fewer banding artifacts).\n\n### Bitrate\n\n**What it measures**: Data per second (Mbps, kbps)\n**What it indicates**: How much data the encoder used, not necessarily quality\n\nHigher bitrate ≠ better quality. An inefficient encode at 20 Mbps can look worse than an efficient encode at 8 Mbps.\n\n**VBR vs CBR**:\n- VBR (Variable): Allocates bits where needed, better quality\n- CBR (Constant): Fixed rate, required for some streaming, wastes bits on simple scenes\n\n### CRF (Constant Rate Factor)\n\nQuality-based encoding setting used by x264/x265:\n- Lower CRF = higher quality, larger file\n- Higher CRF = lower quality, smaller file\n- Range: 0 (lossless) to 51 (worst)\n- Typical: 18-23 for high quality, 23-28 for smaller files\n\nCRF automatically adjusts bitrate based on scene complexity.\n\n## Color Space Concepts\n\n### Color Matrix (BT.601 vs BT.709)\n\nDefines how YCbCr values convert to/from RGB:\n- **BT.601**: Legacy standard for SD content (DVD era)\n- **BT.709**: Modern standard for HD content (Blu-ray, streaming)\n\nWrong matrix = incorrect colors. HD content played as BT.601 will have shifted hues.\n\n### Color Range (Limited vs Full)\n\n**Limited range** (16-235): Standard for video, leaves headroom\n**Full range** (0-255): Uses entire value range\n\nMost video content is Limited range. Playing Limited as Full = washed out. Playing Full as Limited = crushed blacks/clipped whites.\n\n### Chroma Subsampling\n\n**4:4:4**: Full color resolution (rare, large files)\n**4:2:2**: Half horizontal color resolution (professional)\n**4:2:0**: Quarter color resolution (standard for distribution)\n\nHuman eyes are less sensitive to color detail than brightness, so 4:2:0 looks nearly identical to 4:4:4 for most content.\n\n### HDR vs SDR\n\n**SDR** (Standard Dynamic Range): Traditional brightness levels, ~100 nits peak\n**HDR** (High Dynamic Range): Extended brightness, 1000+ nits peak, wider color gamut\n\nHDR requires:\n- 10-bit color depth (minimum)\n- HDR metadata (PQ or HLG transfer function)\n- Compatible display\n\nNot all \"HDR\" releases are legitimate. Some are inverse tonemapped from SDR sources.\n\n## Quick Answers\n\n| Question | Short Answer |\n|----------|--------------|\n| \"Is mkv better than mp4?\" | Different containers, neither is \"better\" - mkv supports more features |\n| \"Should I reencode to H.265?\" | Only if you need smaller files and accept quality loss |\n| \"Is 4K always better than 1080p?\" | No - depends on source quality and encoding |\n| \"Why is my file so large?\" | Check codec (NVENC vs x264), bitrate settings, or CRF value |\n| \"What's the best format?\" | Depends on use case. H.264/mkv for compatibility, H.265 for size |\n\n## Additional Resources\n\nFor detailed explanations:\n- **`${CLAUDE_PLUGIN_ROOT}/references/quality-myths.md`** - Common misconceptions debunked\n- **`${CLAUDE_PLUGIN_ROOT}/references/color-space.md`** - Full color space details\n- **`${CLAUDE_PLUGIN_ROOT}/references/encoding-commands.md`** - Practical command examples\n",
        "plugins/multimedia/skills/framerate-audit/SKILL.md": "---\nname: framerate-audit\ndescription: This skill should be used when the user asks \"check frame rate\", \"is this CFR or VFR\", \"video has duplicate frames\", \"video stutters\", \"frame rate issues\", \"why does video judder\", or wants to analyze frame rate characteristics and detect timing problems.\n---\n\n# Frame Rate Audit\n\nAnalyze video frame rate characteristics, detect CFR vs VFR, find duplicate frames, and diagnose timing issues.\n\n## Key Concepts\n\n### CFR vs VFR\n\n**CFR (Constant Frame Rate)**:\n- Every frame has identical duration\n- Required by most video editors\n- Preferred for distribution\n\n**VFR (Variable Frame Rate)**:\n- Frame durations vary\n- Common in screen recordings, phone videos\n- Causes editor sync issues\n\n### Common Frame Rates\n\n| Rate | Actual Value | Use Case |\n|------|--------------|----------|\n| 23.976 fps | 24000/1001 | Film, most streaming |\n| 24 fps | 24/1 | True film rate (rare digital) |\n| 25 fps | 25/1 | PAL video |\n| 29.97 fps | 30000/1001 | NTSC video |\n| 30 fps | 30/1 | Some digital content |\n| 59.94 fps | 60000/1001 | High frame rate NTSC |\n| 60 fps | 60/1 | Gaming, high frame rate |\n\n**23.976 vs 24**: Nearly identical visually. The fractional rate exists for NTSC broadcast compatibility from analog TV era.\n\n## Audit Workflow\n\n### Step 1: Check MediaInfo\n\n```bash\nmediainfo video.mkv\n```\n\nLook for:\n```\nFrame rate mode    : Constant or Variable\nFrame rate         : 23.976 (24000/1001) FPS\n```\n\n**Frame rate mode tells you CFR vs VFR**.\n\n### Step 2: Detailed Frame Analysis\n\nFor deeper analysis:\n```bash\n# Check frame rate mode specifically\nmediainfo --Inform=\"Video;%FrameRate_Mode%\" video.mkv\n\n# Get frame count\nffprobe -v error -count_packets -select_streams v:0 \\\n  -show_entries stream=nb_read_packets -of csv=p=0 video.mkv\n```\n\n### Step 3: Detect Duplicate Frames\n\n**Using ffmpeg**:\n```bash\n# Detect duplicate frames (mpdecimate filter)\nffmpeg -i video.mkv -vf \"mpdecimate,setpts=N/FRAME_RATE/TB\" \\\n  -f null - 2>&1 | grep \"drop\"\n```\n\n**Manual inspection with mpv**:\n- Open video in mpv\n- Step frame-by-frame with `.` and `,`\n- Compare adjacent frames visually\n- Look for identical frames in sequence\n\n### Step 4: Check for Judder\n\n**Judder causes**:\n- Frame rate mismatch with display\n- Incorrect pulldown handling\n- Duplicate frames creating uneven motion\n\n**Visual test**:\n- Watch panning shots\n- Motion should be smooth\n- Stuttering indicates timing issues\n\n## Common Issues\n\n### VFR Causing Editor Problems\n\n**Symptoms**:\n- Audio/video desync in editors\n- Random stuttering\n- Export timing issues\n\n**Solution**:\nConvert to CFR before editing:\n```bash\nffmpeg -i vfr_video.mkv -vsync cfr -r 30 cfr_video.mkv\n```\n\n### Duplicate Frames\n\n**Symptoms**:\n- Motion appears to stutter\n- Some frames are identical to previous\n- Uneven playback\n\n**Causes**:\n- Bad frame rate conversion\n- Screen recording with drops\n- Encoding errors\n\n**Detection**:\nStep through video frame-by-frame. If two adjacent frames are identical, you have duplicates.\n\n### Wrong Frame Rate Tag\n\n**Symptoms**:\n- Video plays too fast or slow\n- Audio sync issues\n- Duration doesn't match expected\n\n**Solution**:\nRetag without reencoding:\n```bash\n# Change frame rate metadata only\nffmpeg -i video.mkv -c copy -video_track_timescale 24000 output.mkv\n```\n\n### Interpolation Damage\n\n**Symptoms**:\n- \"Soap opera effect\" smoothness\n- Morphing/warping on motion\n- Ghosting around moving objects\n\n**Cause**:\nFrame interpolation (RIFE, SVP, TV motion smoothing) generating fake frames.\n\n**Why it's bad**:\n- Invented frames don't match source\n- Artistic intent is 24fps, not 60fps\n- Creates unnatural motion\n\n## Editor Compatibility\n\n### Premiere Pro / DaVinci / Vegas\n\n**Requirements**:\n- CFR strongly preferred\n- Fractional rates (23.976) can cause issues\n- Some editors handle VFR poorly\n\n**Before importing**:\n1. Check if source is CFR\n2. If VFR, convert to CFR first\n3. Use MkvToMp4 or ffmpeg for conversion\n\n### MKV to MP4 for Editors\n\nStandard remux may preserve VFR timestamps:\n```bash\n# May not fix VFR\nffmpeg -i input.mkv -c copy output.mp4\n```\n\nForce true CFR:\n```bash\n# Step 1: Set timescale\nffmpeg -i input.mkv -c copy -video_track_timescale 24000 temp.mp4\n\n# Step 2: Round timestamps (for 23.976 fps)\nffmpeg -i temp.mp4 -c copy \\\n  -bsf:v \"setts=dts=1001*round(DTS/1001):pts=1001*round(PTS/1001)\" output.mp4\n```\n\n## Quick Reference\n\n| Issue | MediaInfo Shows | Solution |\n|-------|-----------------|----------|\n| VFR video | Frame rate mode: Variable | Convert to CFR |\n| Duplicate frames | N/A (visual inspection) | Re-encode or accept |\n| Wrong rate tag | Unexpected frame rate | Retag with ffmpeg |\n| Interpolated | Higher fps than source | Use original |\n\n## Tools\n\n- **MediaInfo**: Frame rate mode detection\n- **ffprobe**: Detailed timing analysis\n- **mpv**: Frame stepping (`.` and `,` keys)\n- **MkvToMp4**: VFR to CFR conversion for editors\n\n## Additional Resources\n\n- **`${CLAUDE_PLUGIN_ROOT}/references/encoding-commands.md`** - Frame rate conversion commands\n- **`${CLAUDE_PLUGIN_ROOT}/references/tools.md`** - Tool recommendations\n",
        "plugins/multimedia/skills/hdr-audit/SKILL.md": "---\nname: hdr-audit\ndescription: This skill should be used when the user asks \"is this real HDR\", \"check HDR metadata\", \"fake HDR\", \"is this Dolby Vision legitimate\", \"HDR vs SDR\", \"check HDR peak brightness\", or wants to verify whether HDR content is genuine or inverse tonemapped from SDR.\n---\n\n# HDR Content Audit\n\nAnalyze HDR metadata and detect fake/invalid HDR content. HDR is not automatically better than SDR—many \"HDR\" releases are fake.\n\n## What Makes HDR Real\n\nLegitimate HDR requires:\n1. **10-bit color depth** (minimum)\n2. **HDR transfer function** (PQ or HLG)\n3. **Actual extended brightness** (content above 100 nits)\n4. **Mastered in HDR** from HDR source or graded for HDR\n\n## Fake HDR Indicators\n\n**Inverse tonemapped SDR**:\n- SDR content artificially expanded to HDR range\n- No actual detail gained in highlights\n- Often looks flat or washed out on HDR displays\n- Content never officially released in HDR\n\n**HDR container, SDR content**:\n- HDR metadata present but peak brightness never exceeds ~100 nits\n- Tagged as HDR but visually identical to SDR\n\n**Suspicious Dolby Vision**:\n- Content only officially released in SDR\n- DV metadata on old catalog titles\n- Random releases that don't match official offerings\n\n## MediaInfo HDR Fields\n\nKey fields to check in Video section:\n\n```\nHDR format                   : SMPTE ST 2086 / SMPTE ST 2094...\nColor primaries              : BT.2020\nTransfer characteristics     : PQ (SMPTE 2084) or HLG\nMatrix coefficients          : BT.2020 non-constant\nMastering display color prim : BT.2020 or DCI-P3\nMastering display luminance  : min: 0.0050 cd/m2, max: 1000 cd/m2\nMaxCLL                       : 1000 cd/m2\nMaxFALL                      : 400 cd/m2\n```\n\n## Audit Workflow\n\n### Step 1: Check Basic HDR Presence\n\nRun MediaInfo and look for:\n- Transfer characteristics: Should be PQ or HLG (not BT.709)\n- Bit depth: Should be 10 or 12 bit\n- Color primaries: Should be BT.2020 (wide color gamut)\n\n**If any are missing/wrong**: Not HDR, or broken HDR.\n\n### Step 2: Check Mastering Metadata\n\nLook for mastering display info:\n- **MaxCLL** (Content Light Level): Peak brightness in content\n- **MaxFALL** (Frame Average Light Level): Average peak per frame\n- **Mastering display luminance**: Range used in grading\n\n**Red flags**:\n- MaxCLL ≤ 100 nits (SDR range only)\n- No mastering metadata at all\n- Generic/placeholder values\n\n### Step 3: Verify Official HDR Release Exists\n\nResearch whether the content was officially released in HDR:\n- Check streaming services (Netflix, Disney+, etc.)\n- Check UHD Blu-ray releases\n- Search for official announcements\n\n**If no official HDR exists**: The \"HDR\" version is almost certainly fake.\n\n### Step 4: Visual Inspection (If Possible)\n\nOn an HDR display:\n- Does the content use the extended brightness range?\n- Are highlights noticeably brighter than SDR?\n- Does it look natural or artificially boosted?\n\n## HDR Formats\n\n### HDR10\n\n- Static metadata (one set of values for whole video)\n- Most common format\n- Open standard\n\n**MediaInfo shows**:\n```\nHDR format: SMPTE ST 2086\n```\n\n### HDR10+\n\n- Dynamic metadata (per-scene or per-frame optimization)\n- Samsung/Amazon developed\n- Not widely supported\n\n**MediaInfo shows**:\n```\nHDR format: SMPTE ST 2094 App 4\n```\n\n### Dolby Vision\n\n- Dynamic metadata\n- Profile-based system (various capabilities)\n- Requires licensing\n\n**MediaInfo shows**:\n```\nHDR format: Dolby Vision, dvhe.05.06\n```\n\n**Dolby Vision Profiles**:\n- Profile 5: Single layer, common for streaming\n- Profile 7: Dual layer with base HDR10\n- Profile 8: Similar to Profile 5 with HLG base\n\n## Quick Reference\n\n| Check | Expected for Real HDR | Fake HDR Indicator |\n|-------|----------------------|-------------------|\n| MaxCLL | >100 nits | ≤100 nits |\n| Transfer | PQ or HLG | BT.709 tagged as HDR |\n| Bit depth | 10+ bit | 8 bit |\n| Official release | Yes | No HDR release exists |\n| Visual brightness | Extended range used | Looks like SDR |\n\n## Common Fake HDR Scenarios\n\n**Older movies \"remastered\" in HDR**:\n- If studio didn't announce HDR remaster, it's fake\n- Many pirate releases add fake HDR to old content\n\n**Anime in HDR**:\n- Very few anime are produced in HDR\n- Most \"HDR anime\" is inverse tonemapped\n\n**Random Dolby Vision releases**:\n- DV requires licensing and official mastering\n- Unauthorized DV releases are synthetic conversions\n\n## Recommendations\n\n**When to use HDR version**:\n- Official HDR release from studio\n- MaxCLL shows actual extended brightness\n- Content mastered specifically for HDR\n\n**When to use SDR version**:\n- No official HDR release\n- HDR shows fake indicators\n- SDR version has better encoding quality\n- HDR version has conversion artifacts\n\n## Additional Resources\n\n- **`${CLAUDE_PLUGIN_ROOT}/references/hdr.md`** - Detailed HDR metadata guide\n- **`${CLAUDE_PLUGIN_ROOT}/references/color-space.md`** - Color space fundamentals\n",
        "plugins/multimedia/skills/source-compare/SKILL.md": "---\nname: source-compare\ndescription: This skill should be used when the user asks \"compare these videos\", \"which source is better\", \"compare blu-ray vs web\", \"which release should I use\", \"compare video quality\", or needs to evaluate multiple versions of the same content to determine which has better quality.\n---\n\n# Video Source Comparison\n\nCompare multiple video sources to determine which has better quality. Higher bitrate or resolution does not automatically mean better quality.\n\n## Core Principle\n\nQuality = how closely video resembles original master.\n\nWhen comparing sources:\n- Both derive from some common master\n- The better source is closer to that master\n- Technical specs (bitrate, resolution) are secondary to actual visual quality\n\n## Comparison Workflow\n\n### Step 1: Gather Metadata\n\nRun MediaInfo on both sources:\n```bash\nmediainfo source_a.mkv > source_a_info.txt\nmediainfo source_b.mkv > source_b_info.txt\n```\n\nCompare:\n- Resolution\n- Bitrate\n- Encoder used\n- Color space parameters\n- HDR metadata (if applicable)\n\n**Note**: Higher numbers don't guarantee better quality.\n\n### Step 2: Extract Comparison Frames\n\nChoose frames that reveal quality differences:\n\n**Good comparison frames**:\n- Dark scenes with gradients (reveals banding)\n- Scenes with fine texture (hair, fabric, foliage)\n- High motion scenes (reveals compression)\n- Scenes with text or sharp edges (reveals filtering)\n- Areas with strong colors (reveals color handling)\n\n**Extract frames with ffmpeg**:\n```bash\n# Extract frame at specific timestamp\nffmpeg -ss 00:15:30 -i source.mkv -frames:v 1 frame_15m30s.png\n\n# Extract multiple frames\nffmpeg -ss 00:10:00 -i source.mkv -frames:v 1 frame1.png\nffmpeg -ss 00:25:00 -i source.mkv -frames:v 1 frame2.png\nffmpeg -ss 00:45:00 -i source.mkv -frames:v 1 frame3.png\n```\n\n### Step 3: Visual Comparison\n\n**Using SlowPics** (https://slow.pics):\n1. Upload comparison frames from each source\n2. Uncheck \"Show border\" and \"Smooth scaling\"\n3. Use clicker mode (not slider)\n4. Press number keys (1/2/3) to switch rapidly between sources\n\n**What to look for**:\n\n| Area | Better Source Shows |\n|------|---------------------|\n| Dark gradients | Smoother transitions, less banding |\n| Edges | Clean edges, no haloing/ringing |\n| Textures | Preserved grain/detail, not smeared |\n| Colors | Natural, not oversaturated or shifted |\n| Compression | Less blocking, less mosquito noise |\n\n### Step 4: Check for Filtering\n\n**Signs of harmful filtering**:\n\n**Lowpassing (blur)**:\n- Soft, smeared appearance\n- Fine details less defined than other source\n- Some Blu-ray authoring studios apply this\n\n**Sharpening**:\n- Haloing around edges (bright/dark outlines)\n- Unnatural crispness\n- Line warping on straight edges\n\n**Color manipulation**:\n- Oversaturated colors\n- Crushed blacks or blown highlights\n- Different color grading than other source\n\n**Upscaling**:\n- Details too sharp for claimed resolution\n- AI hallucinations (invented details)\n- Native resolution lower than file resolution\n\n## Common Scenarios\n\n### Blu-ray vs Web\n\n**Blu-ray usually better when**:\n- Same master, Blu-ray has more bitrate\n- No filtering applied during authoring\n- Proper color space handling\n\n**Web can be better when**:\n- Blu-ray has lowpass filter applied\n- Web has newer/better master\n- Blu-ray is poor upscale from SD source\n- Web version has better color grading\n\n### Multiple Blu-ray Releases\n\nDifferent authoring studios, regions, or editions can have different quality:\n- Check authoring studio (some are known for lowpassing)\n- Compare frame-by-frame for filtering\n- Look for remastered vs original master\n\n### SD vs HD\n\n**HD version better when**:\n- Truly native HD content\n- Good upscale from clean SD master\n\n**SD version better when**:\n- HD is poorly upscaled from SD source\n- HD has additional filtering damage\n- SD is closer to original DVD master\n\n## Red Flags\n\n**Higher bitrate source might be worse if**:\n- Encoded with inferior encoder (NVENC vs x264)\n- Has filtering applied (sharpening, lowpass)\n- Is upscaled from lower resolution\n- Has wrong color space applied\n\n**Higher resolution source might be worse if**:\n- Upscaled from lower resolution original\n- Has AI upscaling artifacts\n- Native production resolution was lower\n\n## Quick Decision Guide\n\n| Comparison | Check First | Likely Winner |\n|------------|-------------|---------------|\n| Blu-ray vs Web | Filtering, authoring | Depends on studio |\n| 4K vs 1080p | Native resolution | If native, 4K |\n| HEVC vs AVC | Encoder, settings | Neither inherently |\n| High vs low bitrate | Encoder efficiency | Higher if same encoder |\n| HDR vs SDR | HDR legitimacy | SDR if HDR is fake |\n\n## Tools Reference\n\n- **SlowPics** (slow.pics) - Frame comparison hosting\n- **vs-preview** - VapourSynth comparison tool\n- **mpv** - Frame extraction, playback comparison\n- **MediaInfo** - Metadata comparison\n\n## Additional Resources\n\n- **JET Guide**: https://jaded-encoding-thaumaturgy.github.io/JET-guide/\n- **`${CLAUDE_PLUGIN_ROOT}/references/quality-myths.md`** - Why specs don't equal quality\n- **`${CLAUDE_PLUGIN_ROOT}/references/artifacts.md`** - What quality issues look like\n",
        "plugins/multimedia/skills/subtitle-audit/SKILL.md": "---\nname: subtitle-audit\ndescription: This skill should be used when the user asks \"check subtitles\", \"audit subs\", \"subtitle font issues\", \"ASS vs SRT\", \"missing fonts\", \"subtitle timing\", or wants to analyze subtitle tracks for issues like missing fonts, timing problems, or format limitations.\n---\n\n# Subtitle Track Audit\n\nAnalyze subtitle tracks for issues including missing fonts, timing problems, format capabilities, and proper tagging.\n\n## Subtitle Formats\n\n### ASS (Advanced SubStation Alpha)\n\n**Capabilities**:\n- Full styling (fonts, colors, positioning)\n- Complex typesetting (signs, karaoke)\n- Multiple styles per file\n- Precise positioning\n\n**Requirements**:\n- Fonts must be available (embedded or installed)\n- Only MKV container fully supports ASS\n- MP4 does not support ASS (must hardsub)\n\n### SRT (SubRip)\n\n**Capabilities**:\n- Basic text with timing\n- Limited styling (some players support HTML tags)\n- Wide compatibility\n\n**Limitations**:\n- No font specification\n- No precise positioning\n- No complex typesetting\n\n### PGS (Presentation Graphic Stream)\n\n**Capabilities**:\n- Blu-ray subtitle format\n- Image-based (pre-rendered)\n- Perfect styling preservation\n\n**Limitations**:\n- Cannot be edited easily\n- Large file size\n- Text not searchable/selectable\n\n## Audit Workflow\n\n### Step 1: List Subtitle Tracks\n\n```bash\nmediainfo video.mkv\n```\n\nLook for Subtitle sections showing:\n- Format (ASS, SRT, PGS, etc.)\n- Language\n- Title/name\n- Default/forced flags\n\n**Using ffprobe**:\n```bash\nffprobe -v error -select_streams s -show_entries \\\n  stream=index,codec_name:stream_tags=language,title \\\n  -of csv=p=0 video.mkv\n```\n\n### Step 2: Extract for Inspection\n\n```bash\n# Extract first subtitle track\nffmpeg -i video.mkv -map 0:s:0 subtitles.ass\n\n# Extract by language\nffmpeg -i video.mkv -map 0:s:m:language:eng output.srt\n```\n\n### Step 3: Check ASS Font Requirements\n\n**Using Aegisub**:\n1. Open ASS file in Aegisub\n2. Go to View → Fonts Collector\n3. Lists all fonts used and availability\n\n**Manual inspection**:\nOpen ASS file in text editor, look for Style lines:\n```\nStyle: Default,Arial,48,&H00FFFFFF,...\n```\nThe second field after style name is the font.\n\n### Step 4: Verify Embedded Fonts\n\nFor MKV files, check attachments:\n```bash\nmediainfo video.mkv | grep -A 20 \"Attachment\"\n```\n\nShould show font files (TTF, OTF) if fonts are embedded.\n\n**Extract embedded fonts**:\n```bash\nmkvextract attachments video.mkv 1:font1.ttf 2:font2.ttf\n```\n\n### Step 5: Timing Analysis\n\n**Check for issues**:\n- Subtitles appearing too early/late\n- Overlapping subtitle events\n- Very short display times\n\n**Using Aegisub**:\n- Open subtitle file\n- Check timing column for irregularities\n- Look for negative durations or overlaps\n\n## Common Issues\n\n### Missing Fonts\n\n**Symptoms**:\n- Subtitles render in wrong font\n- Missing characters (boxes or blanks)\n- Wrong styling/positioning\n\n**Solutions**:\n1. Install required fonts system-wide\n2. Use font manager (FontBase) for temporary activation\n3. Embed fonts when muxing MKV\n\n### Wrong Language Tag\n\n**Symptoms**:\n- Player selects wrong subtitle track\n- Language filtering doesn't work\n\n**Fix with MKVToolNix**:\n```bash\nmkvpropedit video.mkv --edit track:s1 --set language=eng\n```\n\n### Timing Offset\n\n**Symptoms**:\n- All subtitles early or late by same amount\n\n**Fix**:\n```bash\n# Delay subtitles by 2 seconds\nffmpeg -i input.srt -itsoffset 2 -i input.srt -c copy output.srt\n```\n\nOr use Aegisub: Timing → Shift Times\n\n### Container Compatibility\n\n**MKV**: Full ASS support, can embed fonts\n**MP4**: No ASS support, SRT only (basic), or must hardsub\n**WebM**: Limited subtitle support\n\n## Muxing Subtitles\n\n### Add to MKV\n\n```bash\n# With MKVToolNix\nmkvmerge -o output.mkv input.mkv subtitles.ass\n\n# With ffmpeg\nffmpeg -i video.mkv -i subtitles.ass -map 0 -map 1 -c copy output.mkv\n```\n\n### Set Language and Default\n\n```bash\nmkvmerge -o output.mkv input.mkv \\\n  --language 0:eng --default-track 0:yes subtitles.ass\n```\n\n### Embed Fonts\n\n```bash\nmkvmerge -o output.mkv input.mkv subtitles.ass \\\n  --attach-file font1.ttf --attach-file font2.ttf\n```\n\n## Quick Reference\n\n| Format | Styling | Fonts | Container Support |\n|--------|---------|-------|-------------------|\n| ASS | Full | Required | MKV only |\n| SRT | Basic/None | N/A | Most containers |\n| PGS | Image-based | N/A | MKV, M2TS |\n| VobSub | Image-based | N/A | MKV, AVI |\n\n## Font Collection\n\nWhen distributing ASS subtitles:\n\n1. **Collect fonts** using Aegisub's Font Collector\n2. **Embed in MKV** as attachments\n3. **Test on clean system** without fonts installed\n\n## Tools\n\n- **Aegisub**: Subtitle editing, font collection, timing\n- **MKVToolNix**: Muxing, track properties, font embedding\n- **ffmpeg**: Extraction, basic manipulation\n- **FontBase**: Font management without installation\n\n## Additional Resources\n\n- **`${CLAUDE_PLUGIN_ROOT}/references/tools.md`** - Tool recommendations\n- **`${CLAUDE_PLUGIN_ROOT}/references/encoding-commands.md`** - Subtitle extraction/muxing commands\n",
        "plugins/multimedia/skills/telecine-detect/SKILL.md": "---\nname: telecine-detect\ndescription: This skill should be used when the user asks \"is this interlaced\", \"detect telecine\", \"video has combing\", \"should I deinterlace\", \"3:2 pulldown\", \"inverse telecine\", or sees horizontal lines/combing artifacts in video and wants to understand if the content is truly interlaced or telecined.\n---\n\n# Telecine and Interlacing Detection\n\nDetect whether video content is telecined (reversible) or truly interlaced (not reversible), and advise on proper handling. Getting this wrong destroys video quality.\n\n## Critical Distinction\n\n**Telecine (3:2 Pulldown)**:\n- 24fps film content converted to 30fps for NTSC broadcast\n- Creates repeating pattern of combing across frames\n- Can be reversed nearly losslessly via IVTC (inverse telecine)\n- Original 24fps frames can be recovered\n\n**True Interlacing**:\n- Content actually captured at 60 interlaced fields per second\n- Each field is unique temporal information\n- Cannot be reversed without quality loss\n- Deinterlacing merges or discards fields\n\n## The Common Mistake\n\nSeeing combing → Immediately running a deinterlacer\n\n**Why this is wrong for telecined content**:\n- Deinterlacers throw away half the vertical resolution\n- They cannot distinguish telecine from true interlacing\n- The original 24 progressive frames are lost\n- Result is a blurry, lower-quality video\n\n**Correct approach**:\n1. First detect if content is telecined or truly interlaced\n2. If telecined → use IVTC to recover original frames\n3. If truly interlaced → only then consider deinterlacing\n\n## Detection Workflow\n\n### Step 1: Check MediaInfo\n\nLook for these fields:\n```\nScan type: Interlaced or Progressive\nScan order: Top Field First (TFF) or Bottom Field First (BFF)\n```\n\n**Note**: MediaInfo only shows container/stream flags. Content can be flagged as interlaced but actually be telecined, or flagged progressive but have telecine applied.\n\n### Step 2: Visual Inspection\n\nOpen in mpv and step through frames (`.` and `,` keys):\n\n**Telecine indicators**:\n- Combing appears on some frames but not others\n- Pattern repeats (e.g., every 5th frame has combing)\n- Combing only on specific parts of frame where motion occurred\n- 3:2 pattern: 2 clean frames, then 3 frames with combing alternating\n\n**True interlacing indicators**:\n- Every frame shows combing on all motion\n- No repeating pattern\n- Combing appears uniformly across frame\n\n### Step 3: Confirm with Frame Analysis\n\nFor telecine, look for the 3:2 pulldown pattern:\n```\nFrame 1: Progressive (A)\nFrame 2: Progressive (A)\nFrame 3: Interlaced (A top + B bottom)\nFrame 4: Interlaced (B top + C bottom)\nFrame 5: Progressive (C)\n... pattern repeats\n```\n\nCount frames between clean (progressive) frames. If pattern is consistent (typically 2-3-2-3 or similar), it's telecine.\n\n## Handling Telecined Content\n\n### Tools for IVTC\n\n**Wobbly** (Recommended):\n- GUI tool for manual telecine pattern matching\n- Handles irregular patterns and scene changes\n- Guide: https://wobbly.encode.moe\n\n**VapourSynth VIVTC**:\n- Automated IVTC for consistent patterns\n- Part of VapourSynth toolchain\n\n**avisynth/vapoursynth TFM+TDecimate**:\n- Classic IVTC combination\n- TFM matches fields, TDecimate removes duplicates\n\n### IVTC Process\n\n1. Identify the telecine cadence (pattern)\n2. Match fields to reconstruct original frames\n3. Decimate (remove) duplicate frames\n4. Result: Original 24fps progressive content\n\n## Handling True Interlacing\n\nOnly when content is confirmed truly interlaced:\n\n**Deinterlacing options** (quality varies):\n- QTGMC (VapourSynth/Avisynth) - highest quality, slow\n- yadif - fast, moderate quality\n- bwdif - improved yadif\n\n**Doubling options** (60fps output):\n- Bob deinterlacing - fast, doubles frame rate\n- QTGMC bob - higher quality bob\n\n## Quick Reference\n\n| Symptom | Likely Cause | Action |\n|---------|--------------|--------|\n| Combing on some frames only | Telecine | Use IVTC |\n| Repeating pattern of combing | Telecine | Use IVTC |\n| Every frame has combing | True interlacing | Deinterlace |\n| DVD/NTSC source, 29.97fps | Often telecine | Check pattern |\n| Sports/news content | Often true interlacing | Deinterlace |\n| Film/movie content | Often telecine | Use IVTC |\n\n## Red Flags\n\n**Don't deinterlace if**:\n- Source is a movie or scripted TV show (likely telecine)\n- Combing appears in a pattern\n- MediaInfo shows 23.976fps original rate\n- Source is from DVD/Blu-ray of film content\n\n**Consider deinterlacing if**:\n- Live broadcast content (sports, news)\n- Home video footage\n- Every single frame shows combing\n- No discernible pattern\n\n## Additional Resources\n\n- **fieldbased.media** - Comprehensive interlacing guide\n- **https://wobbly.encode.moe** - Wobbly IVTC guide\n- **`${CLAUDE_PLUGIN_ROOT}/references/telecine.md`** - Detailed telecine patterns\n",
        "plugins/multimedia/skills/video-audit/SKILL.md": "---\nname: video-audit\ndescription: This skill should be used when the user asks to \"audit this video\", \"analyze video quality\", \"check this video file\", \"is this video good quality\", \"should I reencode this\", \"what format is this video\", or wants to understand a video file's technical properties and quality before working with it.\n---\n\n# Video File Audit\n\nAnalyze video files to understand their technical properties, identify quality issues, and recommend appropriate next steps (remux, reencode, or leave unchanged).\n\n## Prerequisites\n\nRequires `mediainfo` CLI tool. Install via:\n- macOS: `brew install mediainfo`\n- Linux: `apt install mediainfo` or equivalent\n- Windows: Download from mediaarea.net\n\n## Audit Workflow\n\n### Step 1: Gather File Metadata\n\nRun MediaInfo to get comprehensive file information:\n\n```bash\nmediainfo --Full \"/path/to/video.mkv\"\n```\n\nFor quick summary:\n```bash\nmediainfo \"/path/to/video.mkv\"\n```\n\n### Step 2: Identify Container vs Codec\n\nExtract and report separately:\n\n**Container format** (file extension): mkv, mp4, webm, avi, mov, m2ts\n- Containers store and package video/audio streams\n- Changing container = remuxing (fast, lossless)\n\n**Video codec** (actual encoding): H.264 (AVC), H.265 (HEVC), VP9, AV1, ProRes\n- Codecs compress and encode the actual video data\n- Changing codec = reencoding (slow, quality loss)\n\nKey MediaInfo fields:\n- `Format` under \"General\" section = container\n- `Format` under \"Video\" section = codec\n\n### Step 3: Check Color Space Parameters\n\nExamine these fields in MediaInfo's Video section:\n\n| Parameter | Expected Value | Problem Indicator |\n|-----------|---------------|-------------------|\n| Color primaries | BT.709 | BT.601 on HD content |\n| Matrix coefficients | BT.709 | BT.601 on HD content |\n| Transfer characteristics | BT.709 | Mismatched values |\n| Color range | Limited | \"Full\" on broadcast content |\n| Chroma subsampling | 4:2:0 | Unusual values |\n\n**Red flags:**\n- HD video (720p+) tagged as BT.601 instead of BT.709\n- Mismatched matrix/primaries/transfer values\n- \"Full\" range on content that should be \"Limited\"\n\nFor detailed color space information, consult `${CLAUDE_PLUGIN_ROOT}/references/color-space.md`.\n\n### Step 4: Identify Quality Red Flags\n\nCheck MediaInfo for these warning signs:\n\n**Encoding quality indicators:**\n- `Encoded_Library`: Look for x264/x265 (good) vs NVENC/QuickSync (lower quality for archival)\n- `Writing application`: Identify source (ffmpeg, Handbrake, etc.)\n- `Bit rate mode`: VBR preferred over CBR for quality\n\n**Suspicious characteristics:**\n- Resolution not matching source (e.g., 4K from 1080p source = upscaled)\n- Frame rate interpolated (e.g., 60fps from 24fps source)\n- Very high or very low bitrate for resolution\n- 10-bit encoding of 8-bit source content (acceptable, often more efficient)\n\n**Content red flags:**\n- HDR metadata on content never released in HDR (fake HDR)\n- Dolby Vision on SDR-only content\n- Unusual aspect ratios suggesting cropping or stretching\n\n### Step 5: Provide Recommendations\n\nBased on findings, recommend one of:\n\n**Leave unchanged** when:\n- Container and codec match intended use\n- Color space parameters are correct\n- No quality red flags detected\n- File works with target application/device\n\n**Remux only** when:\n- Need different container (e.g., mkv→mp4 for compatibility)\n- Container change doesn't require reencoding\n- Color space just needs retagging (not conversion)\n\n**Reencode required** when:\n- Codec incompatible with target device\n- Source has fixable quality issues (wrong matrix applied, not just mistagged)\n- File size reduction needed (with quality tradeoff warning)\n\n**Avoid/warn** when:\n- Video already heavily compressed (reencoding will compound quality loss)\n- User wants to upscale resolution (quality loss, not gain)\n- User wants to interpolate frame rate (quality loss)\n\n## Quick Reference: Container vs Codec\n\n| If user says... | They probably mean... | Actual operation |\n|-----------------|----------------------|------------------|\n| \"Convert mkv to mp4\" | Change container | Remux (fast, lossless) |\n| \"Compress this video\" | Reduce file size | Reencode (slow, quality loss) |\n| \"Make it H.265\" | Change codec | Reencode (slow, quality loss) |\n| \"Fix the colors\" | Depends on issue | Retag (fast) or reencode (slow) |\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed technical information, consult:\n- **`${CLAUDE_PLUGIN_ROOT}/references/quality-myths.md`** - Why resolution/bitrate don't equal quality\n- **`${CLAUDE_PLUGIN_ROOT}/references/color-space.md`** - Color matrices, range, chroma location\n- **`${CLAUDE_PLUGIN_ROOT}/references/tools.md`** - Recommended and discouraged tools\n- **`${CLAUDE_PLUGIN_ROOT}/references/encoding-commands.md`** - ffmpeg/mpv command templates\n",
        "plugins/python-formatter-black/.claude-plugin/plugin.json": "{\n  \"name\": \"python-formatter-black\",\n  \"description\": \"Automatically format Python files with Black after Write/Edit/MultiEdit operations\",\n  \"version\": \"1.0.1\",\n  \"author\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"python\", \"black\", \"formatter\", \"hooks\"]\n}\n",
        "plugins/python-formatter-black/README.md": "# python-formatter-black\n\nFormats Python files with `black` after Write/Edit/MultiEdit operations.\n\n## Setup\n\n`black` must be installed and available in your PATH. See main README for installation options.\n\n## Usage\n\nRuns as a PostToolUse hook. No manual invocation needed.\n\n## How it works\n\nThe plugin uses a PostToolUse hook that:\n1. Receives tool execution data via stdin\n2. Extracts the file path using `jq`\n3. Checks if the file ends with `.py`\n4. Runs `black` to format the file\n\n## Configuration\n\nCustomize Black's formatting in `pyproject.toml`:\n\n```toml\n[tool.black]\nline-length = 100\ntarget-version = [\"py311\"]\n```\n\n## Note\n\nThis plugin runs alongside other formatter plugins (like go-formatter). Each plugin filters for its own file types.\n",
        "plugins/python-formatter-black/hooks/hooks.json": "{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit|MultiEdit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/format-python.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/python-formatter-ruff/.claude-plugin/plugin.json": "{\n  \"name\": \"python-formatter-ruff\",\n  \"description\": \"Automatically format Python files with Ruff after Write/Edit/MultiEdit operations\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"python\", \"ruff\", \"formatter\", \"hooks\"]\n}\n",
        "plugins/python-formatter-ruff/README.md": "# python-formatter-ruff\n\nFormats Python files with `ruff` after Write/Edit/MultiEdit operations.\n\n## Setup\n\n`ruff` must be installed and available in your PATH. See main README for installation options.\n\n## Usage\n\nRuns as a PostToolUse hook. No manual invocation needed.\n\n## How it works\n\nThe plugin uses a PostToolUse hook that:\n1. Receives tool execution data via stdin\n2. Extracts the file path using `jq`\n3. Checks if the file ends with `.py`\n4. Runs `ruff format` to format the file\n\n## Configuration\n\nCustomize Ruff's formatting in `pyproject.toml`:\n\n```toml\n[tool.ruff]\nline-length = 100\ntarget-version = \"py311\"\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\n```\n\n## Note\n\nDo not enable both `python-formatter-ruff` and `python-formatter-black` simultaneously.\n",
        "plugins/python-formatter-ruff/hooks/hooks.json": "{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit|MultiEdit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/format-python.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/swift-formatter/.claude-plugin/plugin.json": "{\n  \"name\": \"swift-formatter\",\n  \"description\": \"Automatically format Swift files with swift-format after Write/Edit/MultiEdit operations\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"robbyt\",\n    \"email\": \"robbyt@robbyt.net\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"swift\", \"formatter\", \"swift-format\", \"hooks\"]\n}\n",
        "plugins/swift-formatter/README.md": "# swift-formatter\n\nFormats Swift files with `swift-format` after Write/Edit/MultiEdit operations.\n\n## Usage\n\nRuns as a PostToolUse hook. No manual invocation needed.\n\n## How it works\n\nThe plugin uses a PostToolUse hook that:\n1. Receives tool execution data via stdin\n2. Extracts the file path using `jq`\n3. Checks if the file ends with `.swift`\n4. Formats the file using the first available formatter\n\n## Formatter Priority\n\n1. **swiftformat** - [Nick Lockwood's SwiftFormat](https://github.com/nicklockwood/SwiftFormat) (`brew install swiftformat`). More configurable, widely adopted.\n2. **swift format** - Built into Swift toolchain. Fallback if swiftformat is not installed.\n\n## Requirements\n\n- `swiftformat` or Swift toolchain - At least one must be available\n- `jq` - JSON processor for parsing tool input\n\n## Note\n\nOnce installed, remove any existing swift-format PostToolUse hooks from `~/.claude/settings.json` to avoid running swift-format twice.\n",
        "plugins/swift-formatter/hooks/hooks.json": "{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit|MultiEdit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/format-swift.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n"
      },
      "plugins": [
        {
          "name": "claude-md",
          "description": "Tools for managing CLAUDE.md memory files. Includes reflect skill for analyzing conversation history to improve memory files.",
          "source": "./plugins/claude-md",
          "version": "2.1.0",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "claude-md",
            "memory",
            "reflection",
            "documentation"
          ],
          "strict": true,
          "categories": [
            "claude-md",
            "documentation",
            "memory",
            "reflection"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install claude-md@robbyt-claude-skills"
          ]
        },
        {
          "name": "go-formatter",
          "description": "Automatically format Go files with gofmt after Write/Edit/MultiEdit operations",
          "source": "./plugins/go-formatter",
          "version": "1.0.1",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "go",
            "golang",
            "formatter",
            "gofmt",
            "hooks"
          ],
          "strict": true,
          "categories": [
            "formatter",
            "go",
            "gofmt",
            "golang",
            "hooks"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install go-formatter@robbyt-claude-skills"
          ]
        },
        {
          "name": "python-formatter-black",
          "description": "Automatically format Python files with Black after Write/Edit/MultiEdit operations",
          "source": "./plugins/python-formatter-black",
          "version": "1.0.1",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "python",
            "black",
            "formatter",
            "hooks"
          ],
          "strict": true,
          "categories": [
            "black",
            "formatter",
            "hooks",
            "python"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install python-formatter-black@robbyt-claude-skills"
          ]
        },
        {
          "name": "python-formatter-ruff",
          "description": "Automatically format Python files with Ruff after Write/Edit/MultiEdit operations",
          "source": "./plugins/python-formatter-ruff",
          "version": "1.0.0",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "python",
            "ruff",
            "formatter",
            "hooks"
          ],
          "strict": true,
          "categories": [
            "formatter",
            "hooks",
            "python",
            "ruff"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install python-formatter-ruff@robbyt-claude-skills"
          ]
        },
        {
          "name": "swift-formatter",
          "description": "Automatically format Swift files with swift-format after Write/Edit/MultiEdit operations",
          "source": "./plugins/swift-formatter",
          "version": "1.0.0",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "swift",
            "formatter",
            "swift-format",
            "hooks"
          ],
          "strict": true,
          "categories": [
            "formatter",
            "hooks",
            "swift",
            "swift-format"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install swift-formatter@robbyt-claude-skills"
          ]
        },
        {
          "name": "go-style-guide",
          "description": "Review Go code for adherence to Go Style Guide. Use when the user requests a code review of completed work, pull requests, or feature branches in Go projects.",
          "source": "./plugins/go-style-guide",
          "version": "1.1.0",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "go",
            "golang",
            "style-guide",
            "code-review"
          ],
          "strict": true,
          "categories": [
            "code-review",
            "go",
            "golang",
            "style-guide"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install go-style-guide@robbyt-claude-skills"
          ]
        },
        {
          "name": "gemini",
          "description": "Gemini CLI integration with focused skills for web search, plan review, code review, and architecture analysis",
          "source": "./plugins/gemini",
          "version": "2.0.1",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "gemini",
            "ai",
            "code-review",
            "web-search",
            "architecture",
            "plan-review"
          ],
          "strict": true,
          "categories": [
            "ai",
            "architecture",
            "code-review",
            "gemini",
            "plan-review",
            "web-search"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install gemini@robbyt-claude-skills"
          ]
        },
        {
          "name": "gh-cli",
          "description": "GitHub CLI integration with focused skills for pull requests, issues, GitHub Actions, and viewing GitHub file URLs",
          "source": "./plugins/gh-cli",
          "version": "2.0.0",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "github",
            "gh",
            "cli",
            "pull-requests",
            "issues",
            "actions",
            "api"
          ],
          "strict": true,
          "categories": [
            "actions",
            "api",
            "cli",
            "gh",
            "github",
            "issues",
            "pull-requests"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install gh-cli@robbyt-claude-skills"
          ]
        },
        {
          "name": "codex",
          "description": "OpenAI Codex CLI integration with MCP server and skills for web search, plan review, code review, and codebase analysis",
          "source": "./plugins/codex",
          "version": "1.2.2",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "codex",
            "openai",
            "ai",
            "code-review",
            "web-search",
            "codebase-analysis",
            "plan-review"
          ],
          "strict": true,
          "categories": [
            "ai",
            "code-review",
            "codebase-analysis",
            "codex",
            "openai",
            "plan-review",
            "web-search"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install codex@robbyt-claude-skills"
          ]
        },
        {
          "name": "apple-dev-docs",
          "description": "On-demand Apple Developer Documentation access via JSON-RPC. Search docs, browse frameworks, access WWDC videos with bundled offline content.",
          "source": "./plugins/apple-dev-docs",
          "version": "0.1.0",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "apple",
            "ios",
            "macos",
            "swiftui",
            "uikit",
            "swift",
            "wwdc",
            "documentation"
          ],
          "strict": true,
          "categories": [
            "apple",
            "documentation",
            "ios",
            "macos",
            "swift",
            "swiftui",
            "uikit",
            "wwdc"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install apple-dev-docs@robbyt-claude-skills"
          ]
        },
        {
          "name": "multimedia",
          "description": "Video and audio file analysis, quality auditing, and encoding guidance. Includes skills for video audit, artifact detection, telecine detection, HDR validation, source comparison, and subtitle analysis.",
          "source": "./plugins/multimedia",
          "version": "1.0.0",
          "author": {
            "name": "robbyt"
          },
          "license": "MIT",
          "keywords": [
            "video",
            "audio",
            "encoding",
            "ffmpeg",
            "mediainfo",
            "quality",
            "hdr",
            "telecine",
            "subtitles"
          ],
          "strict": true,
          "categories": [
            "audio",
            "encoding",
            "ffmpeg",
            "hdr",
            "mediainfo",
            "quality",
            "subtitles",
            "telecine",
            "video"
          ],
          "install_commands": [
            "/plugin marketplace add robbyt/claude-skills",
            "/plugin install multimedia@robbyt-claude-skills"
          ]
        }
      ]
    }
  ]
}