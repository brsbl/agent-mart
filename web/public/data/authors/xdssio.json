{
  "author": {
    "id": "xdssio",
    "display_name": "xdssio",
    "avatar_url": "https://avatars.githubusercontent.com/u/37710064?u=013b4a714f6203a59a431f5653a7672092a25967&v=4"
  },
  "marketplaces": [
    {
      "name": "xetrack",
      "version": null,
      "description": "ML experiment tracking and benchmarking tools for Claude Code",
      "repo_full_name": "xdssio/xetrack",
      "repo_url": "https://github.com/xdssio/xetrack",
      "repo_description": "A lightweight tool to track parameters using duckdb",
      "signals": {
        "stars": 6,
        "forks": 1,
        "pushed_at": "2026-02-09T15:23:11Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"xetrack\",\n  \"owner\": {\n    \"name\": \"XDSS\"\n  },\n  \"metadata\": {\n    \"description\": \"ML experiment tracking and benchmarking tools for Claude Code\",\n    \"version\": \"0.5.2\",\n    \"pluginRoot\": \"./skills\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"benchmark\",\n      \"source\": \"./skills/benchmark\",\n      \"description\": \"Comprehensive ML/AI benchmarking skill that guides users through rigorous experiment design, execution, and analysis using xetrack\",\n      \"version\": \"0.5.2\",\n      \"author\": {\n        \"name\": \"XDSS\"\n      },\n      \"homepage\": \"https://github.com/xdssio/xetrack\",\n      \"repository\": \"https://github.com/xdssio/xetrack\",\n      \"license\": \"MIT\",\n      \"category\": \"ml-tools\",\n      \"keywords\": [\n        \"benchmarking\",\n        \"machine-learning\",\n        \"experiments\",\n        \"tracking\",\n        \"xetrack\",\n        \"ml-ops\",\n        \"data-science\"\n      ]\n    },\n    {\n      \"name\": \"git-versioning\",\n      \"source\": \"./skills/git-versioning\",\n      \"description\": \"ML experiment versioning with Git + DVC + xetrack. Manage sequential/parallel/worktree workflows, merge/rebase experiment artifacts, model promotion, and experiment exploration\",\n      \"version\": \"0.5.2\",\n      \"author\": {\n        \"name\": \"XDSS\"\n      },\n      \"homepage\": \"https://github.com/xdssio/xetrack\",\n      \"repository\": \"https://github.com/xdssio/xetrack\",\n      \"license\": \"MIT\",\n      \"category\": \"ml-tools\",\n      \"keywords\": [\n        \"versioning\",\n        \"dvc\",\n        \"git\",\n        \"experiments\",\n        \"tracking\",\n        \"xetrack\",\n        \"ml-ops\",\n        \"reproducibility\"\n      ]\n    }\n  ]\n}\n",
        "README.md": "<p align=\"center\">\n   <img src=\"https://raw.githubusercontent.com/xdssio/xetrack/main/docs/images/logo.jpg\" alt=\"logo\" width=\"400\" />\n</p>\n\n\n<p align=\"center\">\n    <a href=\"https://github.com/xdssio/xetrack/actions/workflows/ci.yml\">\n        <img src=\"https://github.com/xdssio/xetrack/actions/workflows/ci.yml/badge.svg\" alt=\"CI Status\" />\n    </a>\n    <a href=\"https://pypi.org/project/xetrack/\">\n        <img src=\"https://img.shields.io/pypi/v/xetrack.svg\" alt=\"PyPI version\" />\n    </a>\n    <a href=\"https://pypi.org/project/xetrack/\">\n        <img src=\"https://img.shields.io/pypi/pyversions/xetrack.svg\" alt=\"Python versions\" />\n    </a>\n    <a href=\"https://github.com/xdssio/xetrack/blob/main/LICENSE\">\n        <img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"License: MIT\" />\n    </a>\n    <a href=\"https://github.com/xdssio/xetrack/issues\">\n        <img src=\"https://img.shields.io/github/issues/xdssio/xetrack.svg\" alt=\"GitHub issues\" />\n    </a>\n    <a href=\"https://github.com/xdssio/xetrack/network/members\">\n        <img src=\"https://img.shields.io/github/forks/xdssio/xetrack.svg\" alt=\"GitHub forks\" />\n    </a>\n    <a href=\"https://github.com/xdssio/xetrack/stargazers\">\n        <img src=\"https://img.shields.io/github/stars/xdssio/xetrack.svg\" alt=\"GitHub stars\" />\n    </a>\n</p>\n\n# xetrack\n\nLightweight, local-first experiment tracker and benchmark store built on [SQLite](https://sqlite.org/index) and [duckdb](https://duckdb.org).\n\n\n### Why xetrack Exists\nMost experiment trackers â€” like Weights & Biases â€” rely on cloud servers...\nxetrack is a lightweight package to track benchmarks, experiments, and monitor structured data.   \nIt is focused on simplicity and flexibility.\nYou create a \"Tracker\", and let it track benchmark results, model training and inference monitoring. later retrieve as pandas or connect to it directly as a database.\n\n\n## Features\n\n* Simple\n* Embedded\n* Fast\n* Pandas-like\n* SQL-like\n* Object store with deduplication\n* CLI for basic functions\n* Multiprocessing reads and writes\n* Loguru logs integration\n* Experiment tracking\n* Model monitoring\n\n## Installation\n\n```bash\npip install xetrack\npip install xetrack[duckdb] # to use duckdb as engine\npip install xetrack[assets] # to be able to use the assets manager to save objects\npip install xetrack[cache] # to enable function result caching\n```\n\n## Examples\n\n**Complete examples for every feature** are available in the `examples/` directory:\n\n```bash\n# Run all examples\npython examples/run_all.py\n\n# Run individual examples\npython examples/01_quickstart.py\npython examples/02_track_functions.py\n# ... etc\n```\n\nSee [`examples/README.md`](examples/README.md) for full documentation of all 9+ examples.\n\n## Quickstart\n\n```python\nfrom xetrack import Tracker\n\ntracker = Tracker('database_db', \n                  params={'model': 'resnet18'}\n                  )\ntracker.log({\"accuracy\":0.9, \"loss\":0.1, \"epoch\":1}) # All you really need\n\ntracker.latest\n{'accuracy': 0.9, 'loss': 0.1, 'epoch': 1, 'model': 'resnet18', 'timestamp': '18-08-2023 11:02:35.162360',\n 'track_id': 'cd8afc54-5992-4828-893d-a4cada28dba5'}\n\n\ntracker.to_df(all=True)  # retrieve all the runs as dataframe\n                    timestamp                              track_id     model  loss  epoch  accuracy\n0  26-09-2023 12:17:00.342814  398c985a-dc15-42da-88aa-6ac6cbf55794  resnet18   0.1      1       0.9\n```\n\n**Multiple experiment types**: Use different table names to organize different types of experiments in the same database.\n\n```python\nmodel_tracker = Tracker('experiments_db', table='model_experiments')\ndata_tracker = Tracker('experiments_db', table='data_experiments')\n```\n\n**Params** are values which are added to every future row:\n\n```python\n$ tracker.set_params({'model': 'resnet18', 'dataset': 'cifar10'})\n$ tracker.log({\"accuracy\":0.9, \"loss\":0.1, \"epoch\":2})\n\n{'accuracy': 0.9, 'loss': 0.1, 'epoch': 2, 'model': 'resnet18', 'dataset': 'cifar10', \n 'timestamp': '26-09-2023 12:18:40.151756', 'track_id': '398c985a-dc15-42da-88aa-6ac6cbf55794'}\n\n```\n\nYou can also set a value to an entire run with *set_value* (\"back in time\"):\n\n```python\ntracker.set_value('test_accuracy', 0.9) # Only known at the end of the experiment\ntracker.to_df()\n\n                    timestamp                              track_id     model  loss  epoch  accuracy  dataset  test_accuracy\n0  26-09-2023 12:17:00.342814  398c985a-dc15-42da-88aa-6ac6cbf55794  resnet18   0.1      1       0.9      NaN            0.9\n2  26-09-2023 12:18:40.151756  398c985a-dc15-42da-88aa-6ac6cbf55794  resnet18   0.1      2       0.9  cifar10            0.9\n\n```\n\n## Track functions\n\nYou can track any function.\n\n* The return value is logged before returned\n\n```python\ntracker = Tracker('database_db', \n    log_system_params=True, \n    log_network_params=True, \n    measurement_interval=0.1)\nimage = tracker.track(read_image, *args, **kwargs)\ntracker.latest\n{'result': 571084, 'name': 'read_image', 'time': 0.30797290802001953, 'error': '', 'disk_percent': 0.6,\n 'p_memory_percent': 0.496507, 'cpu': 0.0, 'memory_percent': 32.874608, 'bytes_sent': 0.0078125,\n 'bytes_recv': 0.583984375}\n```\n\nOr with a wrapper:\n\n```python\n\n@tracker.wrap(params={'name':'foofoo'})\ndef foo(a: int, b: str):\n    return a + len(b)\n\nresult = foo(1, 'hello')\ntracker.latest\n{'function_name': 'foo', 'args': \"[1, 'hello']\", 'kwargs': '{}', 'error': '', 'function_time': 4.0531158447265625e-06, \n 'function_result': 6, 'name': 'foofoo', 'timestamp': '26-09-2023 12:21:02.200245', 'track_id': '398c985a-dc15-42da-88aa-6ac6cbf55794'}\n```\n\n### Automatic Dataclass and Pydantic BaseModel Unpacking\n\n**NEW**: When tracking functions, xetrack automatically unpacks frozen dataclasses and Pydantic BaseModels into individual tracked fields with dot-notation prefixes.\n\nThis is especially useful for ML experiments where you have complex configuration objects:\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass TrainingConfig:\n    learning_rate: float\n    batch_size: int\n    epochs: int\n    optimizer: str = \"adam\"\n\n@tracker.wrap()\ndef train_model(config: TrainingConfig):\n    # Your training logic here\n    return {\"accuracy\": 0.95, \"loss\": 0.05}\n\nconfig = TrainingConfig(learning_rate=0.001, batch_size=32, epochs=10)\nresult = train_model(config)\n\n# All config fields are automatically unpacked and tracked!\ntracker.latest\n{\n    'function_name': 'train_model',\n    'config_learning_rate': 0.001,      # â† Unpacked from dataclass\n    'config_batch_size': 32,            # â† Unpacked from dataclass\n    'config_epochs': 10,                # â† Unpacked from dataclass\n    'config_optimizer': 'adam',         # â† Unpacked from dataclass\n    'accuracy': 0.95,\n    'loss': 0.05,\n    'timestamp': '...',\n    'track_id': '...'\n}\n```\n\n**Works with multiple dataclasses:**\n\n```python\n@dataclass(frozen=True)\nclass ModelConfig:\n    model_type: str\n    num_layers: int\n\n@dataclass(frozen=True)\nclass DataConfig:\n    dataset: str\n    batch_size: int\n\ndef experiment(model_cfg: ModelConfig, data_cfg: DataConfig):\n    return {\"score\": 0.92}\n\nresult = tracker.track(\n    experiment,\n    args=[\n        ModelConfig(model_type=\"transformer\", num_layers=12),\n        DataConfig(dataset=\"cifar10\", batch_size=64)\n    ]\n)\n\n# Result includes: model_cfg_model_type, model_cfg_num_layers, \n#                  data_cfg_dataset, data_cfg_batch_size, score\n```\n\n**Also works with Pydantic BaseModel:**\n\n```python\nfrom pydantic import BaseModel\n\nclass ExperimentConfig(BaseModel):\n    experiment_name: str\n    seed: int\n    use_gpu: bool = True\n\n@tracker.wrap()\ndef run_experiment(cfg: ExperimentConfig):\n    return {\"status\": \"completed\"}\n\nconfig = ExperimentConfig(experiment_name=\"exp_001\", seed=42)\nresult = run_experiment(config)\n\n# Automatically tracks: cfg.experiment_name, cfg.seed, cfg.use_gpu, status\n```\n\n**Benefits:**\n- Clean function signatures (one config object instead of many parameters)\n- All config values automatically tracked individually for easy filtering/analysis\n- Works with both `tracker.track()` and `@tracker.wrap()` decorator\n- Supports both frozen and non-frozen dataclasses\n- Compatible with Pydantic BaseModel via `model_dump()`\n\n## Track assets (Oriented for ML models)\n\nRequirements: `pip install xetrack[assets]` (installs sqlitedict)\n\nWhen you attempt to track a non primitive value which is not a list or a dict - xetrack saves it as assets with deduplication and log the object hash:\n\n* Tips: If you plan to log the same object many times over, after the first time you log it, just insert the hash instead for future values to save time on encoding and hashing.\n\n```python\n$ tracker = Tracker('database_db', params={'model': 'logistic regression'})\n$ lr = Logisticregression().fit(X_train, y_train)\n$ tracker.log({'accuracy': float(lr.score(X_test, y_test)), 'lr': lr})\n{'accuracy': 0.9777777777777777, 'lr': '53425a65a40a49f4',  # <-- this is the model hash\n    'dataset': 'iris', 'model': 'logistic regression', 'timestamp': '2023-12-27 12:21:00.727834', 'track_id': 'wisteria-turkey-4392'}\n\n$ model = tracker.get('53425a65a40a49f4') # retrieve an object\n$ model.score(X_test, y_test)\n0.9777777777777777\n```\n\nYou can retrieve the model in CLI if you need only the model in production and mind carring the rest of the file\n\n```bash\n# bash\nxt assets export database.db 53425a65a40a49f4 model.cloudpickle\n```\n\n```python\n# python\nimport cloudpickle\nwith open(\"model_cloudpickle\", 'rb') as f:\n    model = cloudpickle.loads(f.read())\n# LogisticRegression()\n```\n\n## Function Result Caching\n\nXetrack provides transparent disk-based caching for expensive function results using [diskcache](https://grantjenks.com/docs/diskcache/). When enabled, results are automatically cached based on function name, arguments, and keyword arguments.\n\n### Installation\n\n```bash\npip install xetrack[cache]\n```\n\n### Basic Usage\n\nSimply provide a `cache` parameter with a directory path to enable automatic caching:\n\n```python\nfrom xetrack import Tracker\n\ntracker = Tracker(db='track_db', cache='cache_dir')\n\ndef expensive_computation(x: int, y: int) -> int:\n    \"\"\"Simulate expensive computation\"\"\"\n    return x ** y\n\n# First call - executes function\nresult1 = tracker.track(expensive_computation, args=[2, 10])  # Computes 2^10 = 1024\n\n# Second call with same args - returns cached result instantly\nresult2 = tracker.track(expensive_computation, args=[2, 10])  # Cache hit!\n\n# Different args - executes function again\nresult3 = tracker.track(expensive_computation, args=[3, 10])  # Computes 3^10 = 59049\n\n# Tracker params also affect cache keys\nresult4 = tracker.track(expensive_computation, args=[2, 10], params={\"model\": \"v2\"})  # Computes (different params)\nresult5 = tracker.track(expensive_computation, args=[2, 10], params={\"model\": \"v2\"})  # Cache hit!\n```\n\n### Cache Observability & Lineage Tracking\n\nCache behavior is tracked in the database with the `cache` field for full lineage tracking:\n\n```python\nfrom xetrack import Reader\n\ndf = Reader(db='track_db').to_df()\nprint(df[['function_name', 'function_time', 'cache', 'track_id']])\n#   function_name           function_time  cache           track_id\n# 0 expensive_computation   2.345          \"\"              abc123      # Computed (cache miss)\n# 1 expensive_computation   0.000          \"abc123\"        def456      # Cache hit - traces back to abc123\n# 2 expensive_computation   2.891          \"\"              ghi789      # Different args (computed)\n```\n\nThe `cache` field provides lineage:\n- **Empty string (\"\")**: Result was computed (cache miss or no cache)\n- **track_id value**: Result came from cache (cache hit), references the original execution's track_id\n\n### Reading Cache Directly\n\nYou can inspect cached values without re-running functions. Cache stores dicts with \"result\" and \"cache\" keys:\n\n```python\nfrom xetrack import Reader\n\n# Read specific cached value by key\n# Note: _generate_cache_key is a private method for advanced usage\ncache_key = tracker._generate_cache_key(expensive_computation, [2, 10], {}, {})\nif cache_key is not None:  # Will be None if any arg is unhashable\n    cached_data = Reader.read_cache('cache_dir', cache_key)\n    print(f\"Result: {cached_data['result']}, Original execution: {cached_data['cache']}\")\n    # Result: 1024, Original execution: abc123\n\n# Scan all cached entries\nfor key, cached_data in Reader.scan_cache('cache_dir'):\n    print(f\"{key}: result={cached_data['result']}, from={cached_data['cache']}\")\n```\n\n### Use Cases\n\n- **ML Model Inference**: Cache predictions for repeated inputs\n- **Data Processing**: Cache expensive transformations or aggregations\n- **API Calls**: Cache external API responses (with appropriate TTL considerations)\n- **Scientific Computing**: Cache results of long-running simulations\n\n### Important Notes\n\n- **Cache keys** are generated from tuples of (function name, args, kwargs, **tracker params**)\n- Different tracker params create separate cache entries (e.g., different model versions)\n- Exceptions are **not cached** - failed calls will retry on next invocation\n- Cache is persistent across Python sessions\n- Lineage tracking: the `cache` field links cached results to their original execution via track_id\n\n### Handling Objects in Cache Keys\n\nXetrack intelligently handles different types of arguments:\n\n- **Primitives** (int, float, str, bool, bytes): Used as-is in cache keys\n- **Hashable objects** (custom classes with `__hash__`): Uses `hash()` for consistent keys across runs\n- **Unhashable objects** (list, dict, sets): **Caching skipped entirely** for that call (warning issued once per type)\n\n```python\n# Hashable custom objects work great\nclass Config:\n    def __init__(self, value):\n        self.value = value\n    def __hash__(self):\n        return hash(self.value)\n    def __eq__(self, other):\n        return isinstance(other, Config) and self.value == other.value\n\n# Cache hits work across different object instances with same hash\nconfig1 = Config(\"production\")\nconfig2 = Config(\"production\")\ntracker.track(process, args=[config1])  # Computed, cached\ntracker.track(process, args=[config2])  # Cache hit! (same hash)\n\n# Unhashable objects skip caching entirely\ntracker.track(process, args=[[1, 2, 3]])  # Computed, NOT cached (warning issued)\ntracker.track(process, args=[[1, 2, 3]])  # Computed again, still NOT cached\n\n# Make objects hashable to enable caching\nclass HashableList:\n    def __init__(self, items):\n        self.items = tuple(items)  # Use tuple for hashability\n    def __hash__(self):\n        return hash(self.items)\n    def __eq__(self, other):\n        return isinstance(other, HashableList) and self.items == other.items\n\ntracker.track(process, args=[HashableList([1, 2, 3])])  # âœ… Cached!\n```\n\n### Using Frozen Dataclasses for Complex Configurations\n\n**Recommended Pattern**: When your function has many parameters or complex configurations, use frozen dataclasses to enable caching. This is especially useful for ML experiments with multiple hyperparameters.\n\n```python\nfrom dataclasses import dataclass\n\n# âœ… RECOMMENDED: frozen=True makes dataclass hashable automatically, slots efficient in memory\n@dataclass(frozen=True, slots=True)\nclass TrainingConfig:\n    learning_rate: float\n    batch_size: int\n    epochs: int\n    model_name: str\n    optimizer: str = \"adam\"\n\ndef train_model(config: TrainingConfig) -> dict:\n    \"\"\"Complex training function with many parameters\"\"\"\n    # ... training logic ...\n    return {\"accuracy\": 0.95, \"loss\": 0.05}\n\n# Caching works seamlessly with frozen dataclasses\nconfig1 = TrainingConfig(learning_rate=0.001, batch_size=32, epochs=10, model_name=\"bert\")\nresult1 = tracker.track(train_model, args=[config1])  # Computed, cached\n\nconfig2 = TrainingConfig(learning_rate=0.001, batch_size=32, epochs=10, model_name=\"bert\")\nresult2 = tracker.track(train_model, args=[config2])  # Cache hit! (identical config)\n\n# Different config computes again\nconfig3 = TrainingConfig(learning_rate=0.002, batch_size=32, epochs=10, model_name=\"bert\")\nresult3 = tracker.track(train_model, args=[config3])  # Computed (different learning_rate)\n```\n\n**Benefits:**\n- Clean, readable function signatures (one config object instead of many parameters)\n- Type safety with automatic validation\n- Automatic hashability with `frozen=True` \n- Cache works across different object instances with identical values\n- Easier to version and serialize configurations\n\n\n### Tips and Tricks\n\n* ```Tracker(Tracker.IN_MEMORY, logs_path='logs/') ``` Let you run only in memory - great for debugging or working with logs only\n\n### Pandas-like\n\n```python\nprint(tracker)\n                                    _id                              track_id                 date    b    a  accuracy\n0  48154ec7-1fe4-4896-ac66-89db54ddd12a  fd0bfe4f-7257-4ec3-8c6f-91fe8ae67d20  16-08-2023 00:21:46  2.0  1.0       NaN\n1  8a43000a-03a4-4822-98f8-4df671c2d410  fd0bfe4f-7257-4ec3-8c6f-91fe8ae67d20  16-08-2023 00:24:21  NaN  NaN       1.0\n\ntracker['accuracy'] # get accuracy column\ntracker.to_df() # get pandas dataframe of current run\n\n```\n\n### SQL-like\nYou can filter the data using SQL-like syntax using [duckdb](https://duckdb.org/docs):\n* The sqlite database is attached as **db** and the table is **events**. Assts are in the **assets** table.   \n* To use the duckdb as backend, `pip install xetrack[duckdb]` (installs duckdb) and add the parameter engine=\"duckdb\" to Tracker like so:\n\n```python\nTracker(..., engine='duckdb')\n```\n\n\n\n#### Python\n```python\ntracker.conn.execute(f\"SELECT * FROM db.events WHERE accuracy > 0.8\").fetchall()\n```\n\n#### Duckdb CLI\n* Install: `curl https://install.duckdb.org | sh`   \n* If duckdb>=1.2.2, you can use [duckdb local ui](https://duckdb.org/2025/03/12/duckdb-ui.html)    \n\n```bash\n$ duckdb -ui\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                result                â”‚\nâ”‚               varchar                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ UI started at http://localhost:4213/ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nD INSTALL sqlite; LOAD sqlite; ATTACH 'database_db' AS db (TYPE sqlite);\n# navigate browser to http://localhost:4213/\n\n# or run directly in terminal\nD SELECT * FROM db.events;\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         timestamp          â”‚     track_id     â”‚  model   â”‚ epoch â”‚ accuracy â”‚  loss  â”‚\nâ”‚          varchar           â”‚     varchar      â”‚ varchar  â”‚ int64 â”‚  double  â”‚ double â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 2023-12-27 11:25:59.244003 â”‚ fierce-pudu-1649 â”‚ resnet18 â”‚     1 â”‚      0.9 â”‚    0.1 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### Logger integration\nThis is very useful in an environment where you can use normal logs, and don't want to manage a separate logger or file.\nOn great use-case is **model monitoring**.\n\n`logs_stdout=true` print to stdout every tracked event\n`logs_path='logs'` writes logs to a file\n\n```python\n$ Tracker(db=Tracker.IN_MEMORY, logs_path='logs',logs_stdout=True).log({\"accuracy\":0.9})\n2023-12-14 21:46:55.290 | TRACKING | xetrack.logging:log:176!ğŸ“!{\"accuracy\": 0.9, \"timestamp\": \"2023-12-14 21:46:55.290098\", \"track_id\": \"marvellous-stork-4885\"}\n\n$ Reader.read_logs(path='logs')\n   accuracy                   timestamp                track_id\n0       0.9  2023-12-14 21:47:48.375258  unnatural-polecat-1380\n```\n\n### JSONL Logging for Data Synthesis and GenAI Datasets\n\nJSONL (JSON Lines) format is ideal for building machine learning datasets, data synthesis, and GenAI training data. Each tracking event is written as a single-line JSON with structured metadata.\n\n**Use Cases:**\n- Building datasets for LLM fine-tuning\n- Creating synthetic data for model training\n- Structured data collection for data synthesis pipelines\n- Easy integration with data processing tools\n\n```python\n# Enable JSONL logging\ntracker = Tracker(\n    db='database_db',\n    jsonl='logs/data.jsonl'  # Write structured logs to JSONL\n)\n\n# Every log call writes structured JSON\ntracker.log({\"subject\": \"taxes\", \"prompt\": \"Help me with my taxes\"})\ntracker.log({\"subject\": \"dance\", \"prompt\": \"Help me with my moves\"})\n\n# Read JSONL data into pandas DataFrame\ndf = Reader.read_jsonl('logs/data.jsonl')\nprint(df)\n#                          timestamp     level  subject                    prompt                track_id\n# 0  2024-01-15T10:30:00.123456+00:00  TRACKING    taxes  Help me with my taxes  ancient-falcon-1234\n# 1  2024-01-15T10:35:00.234567+00:00  TRACKING    dance  Help me with my moves  ancient-falcon-1234\n\n# Or use pandas directly (JSONL is standard format)\nimport pandas as pd\ndf = pd.read_json('logs/data.jsonl', lines=True)\n```\n\n**JSONL Entry Format:**\nEach line contains flattened structured data suitable for ML pipelines:\n```json\n{\"timestamp\": \"2024-01-15T10:30:00.123456+00:00\", \"level\": \"TRACKING\", \"accuracy\": 0.95, \"loss\": 0.05, \"epoch\": 1, \"model\": \"test-model\", \"track_id\": \"xyz-123\"}\n```\n\nNote: Timestamp is in ISO 8601 format with timezone for maximum compatibility.\n\n**Reading Data:**\n```python\n# From JSONL file\ndf = Reader.read_jsonl('logs/tracking.jsonl')\n\n# From database (class method for convenience)\ndf = Reader.read_db('database_db', engine='sqlite', table='default')\n\n# From database with filtering\ndf = Reader.read_db('database_db', track_id='specific-run-id', head=100)\n```\n\n## Analysis\nTo get the data of all runs in the database for analysis:   \nUse this for further analysis and plotting.\n\n* This works even while a another process is writing to the database.\n\n```python\nfrom xetrack import Reader\ndf = Reader('database_db').to_df() \n```\n\n### Model Monitoring\n\nHere is how we can save logs on any server and monitor them with xetrack:    \nWe want to print logs to a file or *stdout* to be captured normally.   \nWe save memory by not inserting the data to the database (even though it's fine).\nLater we can read the logs and do fancy visualisation, online/offline analysis, build dashboards etc.\n\n```python\ntracker = Tracker(db=Tracker.SKIP_INSERT, logs_path='logs', logs_stdout=True)\ntracker.logger.monitor(\"<dict or pandas DataFrame>\") # -> write to logs in a structured way, consistent by schema, no database file needed\n\n\ndf = Reader.read_logs(path='logs')\n\"\"\"\nRun drift analysis and outlier detection on your logs: \n\"\"\"\n```\n\n### ML Tracking\n\n```python\ntracker.logger.experiment(<model evaluation and params>) # -> prettily write to logs\n\ndf = Reader.read_logs(path='logs')\n\"\"\"\nRun fancy visualisation, online/offline analysis, build dashboards etc.\n\"\"\"\n```\n\n## CLI\n\nFor basic and repetative needs.\n\n```bash\n$ xt head database.db --n=2\n|    | timestamp                  | track_id                 | model    |   accuracy | data   | params           |\n|---:|:---------------------------|:-------------------------|:---------|-----------:|:-------|:-----------------|\n|  0 | 2023-12-27 11:36:45.859668 | crouching-groundhog-5046 | xgboost  |        0.9 | mnist  | 1b5b2294fc521d12 |\n|  1 | 2023-12-27 11:36:45.863888 | crouching-groundhog-5046 | xgboost  |        0.9 | mnist  | 1b5b2294fc521d12 |\n...\n\n\n$ xt tail database.db --n=1\n|    | timestamp                  | track_id        | model    |   accuracy | data   | params           |\n|---:|:---------------------------|:----------------|:---------|-----------:|:-------|:-----------------|\n|  0 | 2023-12-27 11:37:30.627189 | ebony-loon-6720 | lightgbm |        0.9 | mnist  | 1b5b2294fc521d12 |\n\n$ xt set database.db accuracy 0.8 --where-key params --where-value 1b5b2294fc521d12 --track-id ebony-loon-6720\n\n$ xt delete database.db ebony-loon-6720 # delete experiments with a given track_id\n\n# run any other SQL in a oneliner\n$ xt sql database.db \"SELECT * FROM db.events;\"\n\n# retrieve a model (any object) which was saved into a file using cloudpickle\n$ xt assets export database.db hash output \n\n# remove an object from the assets\n$ xt assets delete database.db hash \n\n# If you have two databases, and you want to merge one to the other\n# Only works with duckdb at this moment \n$ xt copy source.db target.db --assets/--no-assets --table=<table>\n\n\n# Stats\n$ xt stats describe database.db --columns=x,y,z\n\n$ xt stats top/bottom database.db x # print the entry with the top/bottom result of a value\n\n# bashplotlib (`pip install bashplotlib` is required)\n$ xt plot hist database.db x\n    ----------------------\n    |    x histogram     |\n    ----------------------\n\n 225|      o\n 200|     ooo\n 175|     ooo\n 150|     ooo\n 125|     ooo\n 100|    ooooo\n  75|    ooooo\n  50|    ooooo\n  25|   ooooooo\n   1| oooooooooo\n     ----------\n\n-----------------------------------\n|             Summary             |\n-----------------------------------\n|        observations: 1000       |\n|      min value: -56.605967      |\n|         mean : 2.492545         |\n|       max value: 75.185944      |\n-----------------------------------\n$ xt plot scatter database.db x y\n\n```\n# SQLite vs Duckdb\n1. Dynamic Typing & Column Affinity\n    * Quirk: SQLite columns have affinity (preference) rather than strict types.\n    * Impact: \"42\" (str) will happily go into an INTEGER column without complaint.\n    * Mitigation: As youâ€™ve done, use explicit Python casting based on expected dtype.\n\n2. Booleans Are Integers\n    * Quirk: SQLite doesnâ€™t have a native BOOLEAN type. True becomes 1, False becomes 0.\n    * Impact: Any boolean stored/retrieved will behave like an integer.\n    * Mitigation: Handle boolean â†” integer conversion in code if you care about type fidelity.\n\n3. NULLs Can Be Inserted into ANY Column\n    * Quirk: Unless a column is explicitly declared NOT NULL, SQLite allows NULL in any field â€” even primary keys.\n    * Impact: Can result in partially complete or duplicate-prone rows if youâ€™re not strict.\n    * Mitigation: Add NOT NULL constraints and enforce required fields at the application level.\n\n# Tests for development\n```bash\npip install pytest-testmon pytest\npytest -x -q -p no:warnings --testmon  tests\n```\n\n---\n\n# Benchmark Skill for Claude Code\n\nxetrack includes a comprehensive **benchmark skill** for Claude Code that guides you through rigorous ML/AI benchmarking experiments.\n\n## What is the Benchmark Skill?\n\nThe benchmark skill is an AI agent guide that helps you:\n- **Design experiments** following best practices (single-execution, caching, reproducibility)\n- **Track predictions & metrics** with the two-table pattern\n- **Validate results** for data leaks, duplicate executions, and missing params\n- **Analyze with DuckDB** using powerful SQL queries\n- **Version experiments** with git tags and DVC\n- **Avoid common pitfalls** (multiprocessing issues, cache problems, etc.)\n\n> The 7-phase workflow is **genuinely well-structured**. The \"design end-to-start\" principle and single-execution principle are real insights that save people from common mistakes [...] The two-table pattern [...] is a concrete, opinionated design that **eliminates decision paralysis** [...] 8+ pitfalls discovered through actual simulations â€” **this is rare and valuable**. Most skills are written from theory; yours was battle-tested with real databases [...] The engine decision matrix [...] with multiprocessing gotchas is **genuinely useful** â€” this is a pitfall that costs hours to debug [...] Validation scripts [...] are actionable â€” they produce real recommendations, not just data [...] Scripts are functional, not just documentation [...] The experiment explorer [...] is a serious tool â€” auto-detection of retrieval strategy [...] side-by-side diff, disposable worktrees for exploration [...] The model manager with the candidates pattern **solves a real organizational problem** [...] The artifact merger using DuckDB for schema-flexible merges is clever [...] The 14 use cases [...] are concrete and map directly to real workflow decisions [...] The workflow decision matrix is **the killer feature** â€” exactly the kind of decision that's hard to make and easy to get wrong [...] The merge vs rebase semantics for each artifact type is **genuinely novel**; nobody has codified this for ML experiments before [...] The two skills complement each other perfectly â€” one runs experiments, the other versions them [...] Safety checklists [...] prevent data loss [...] Deep DuckDB integration for analysis is a differentiator [...] Local-first philosophy means **zero infrastructure to start**.\n>\n> â€” Claude, on first review of the benchmark & git-versioning skills\n\n## Installation\n\n### Option 1: Install from Plugin Marketplace (Recommended)\n\nThe easiest way to install the benchmark skill is directly from the xetrack repository using Claude Code's plugin marketplace:\n\n```bash\n# In Claude Code, add the xetrack marketplace\n/plugin marketplace add xdssio/xetrack\n\n# Install the benchmark skill\n/plugin install benchmark@xetrack\n```\n\nThat's it! Claude Code will automatically download and configure the skill.\n\n**Update to latest version:**\n```bash\n/plugin marketplace update\n```\n\n### Option 2: Manual Installation\n\n```bash\n# Clone the xetrack repository\ngit clone https://github.com/xdssio/xetrack.git\n\n# Copy the benchmark skill to Claude's skills directory\ncp -r xetrack/skills/benchmark ~/.claude/skills/benchmark\n\n# Verify installation\nls ~/.claude/skills/benchmark/SKILL.md\n```\n\n## Usage with Claude Code\n\nOnce installed, simply ask Claude to help with benchmarking:\n\n**Example prompts:**\n\n```\n\"Help me benchmark 3 embedding models on my classification task\"\n\n\"Set up a benchmark comparing prompt variations for my LLM classifier\"\n\n\"I want to benchmark different sklearn models with hyperparameter search\"\n\n\"Debug my benchmark - I'm getting inconsistent results\"\n```\n\nClaude will automatically use the benchmark skill and guide you through:\n\n0. **Phase 0**: Planning what to track (ideation)\n1. **Phase 1**: Understanding your goals and designing the experiment\n2. **Phase 2**: Building a robust single-execution function\n3. **Phase 3**: Adding caching for efficiency\n4. **Phase 4**: Parallelizing (if needed)\n5. **Phase 5**: Running the full benchmark loop\n6. **Phase 6**: Validating results for common pitfalls\n7. **Phase 7**: Analyzing results with DuckDB\n\n## Features\n\n### Two-Table Pattern\n\nThe skill teaches the recommended pattern of storing data in two tables:\n\n- **Predictions table**: Every single prediction/execution (detailed)\n- **Metrics table**: Aggregated results per experiment (summary)\n\n```python\n# Predictions table - granular data\npredictions_tracker = Tracker(\n    db='benchmark.db',\n    engine='duckdb',\n    table='predictions',\n    cache='cache_dir'\n)\n\n# Metrics table - aggregated results\nmetrics_tracker = Tracker(\n    db='benchmark.db',\n    engine='duckdb',\n    table='metrics'\n)\n```\n\n### Git Tag-Based Versioning\n\nAutomatic experiment versioning with git tags:\n\n```python\n# Skill helps you run experiments with versioned tags\n# e0.0.1 â†’ e0.0.2 â†’ e0.0.3\n\n# View experiment history:\ngit tag -l 'e*' -n9\n# e0.0.1  model=logistic | lr=0.001 | acc=0.8200 | data=3a2f1b\n# e0.0.2  model=bert-base | lr=0.0001 | acc=0.8500 | data=3a2f1b\n# e0.0.3  model=bert-base | lr=0.0001 | acc=0.8900 | data=7c4e2a\n```\n\n### DVC Integration\n\nBuilt-in guidance for data and database versioning with DVC:\n\n```bash\n# Skill recommends DVC for reproducibility\ndvc add data/\ndvc add benchmark.db\n\ngit add data.dvc benchmark.db.dvc\ngit commit -m \"experiment: e0.0.3 results\"\ngit tag -a e0.0.3 -m \"model=bert-base | acc=0.8900\"\n```\n\n### Validation Scripts\n\nHelper scripts to catch common issues:\n\n```bash\n# Check for data leaks, duplicates, missing params\npython skills/benchmark/scripts/validate_benchmark.py benchmark.db predictions\n\n# Analyze cache effectiveness\npython skills/benchmark/scripts/analyze_cache_hits.py benchmark.db predictions\n\n# Export markdown summary\npython skills/benchmark/scripts/export_summary.py benchmark.db predictions > RESULTS.md\n```\n\n### Common Pitfalls Documented\n\nThe skill warns you about:\n- âš ï¸ DuckDB + multiprocessing = database locks (use SQLite instead)\n- âš ï¸ System monitoring incompatible with multiprocessing\n- âš ï¸ Dataclass unpacking only works with `.track()`, not `.log()`\n- âš ï¸ Model objects can bloat database (use assets)\n- âš ï¸ Float parameters need rounding for consistent caching\n\n## Example Templates\n\nThe skill includes complete examples for common scenarios:\n\n```bash\n# sklearn model comparison\npython skills/benchmark/assets/sklearn_benchmark_template.py\n\n# LLM finetuning simulation\npython skills/benchmark/assets/llm_finetuning_template.py\n\n# Load testing / throughput benchmark\npython skills/benchmark/assets/throughput_benchmark_template.py\n```\n\n## Documentation\n\nFull documentation is in the skill itself:\n\n- **SKILL.md**: Complete workflow and guidance\n- **references/methodology.md**: Core benchmarking principles\n- **references/duckdb-analysis.md**: SQL query recipes\n- **scripts/**: Helper validation and analysis scripts\n- **assets/**: Complete example templates\n\n## When to Use the Skill\n\n**Use the benchmark skill when:**\n- Comparing multiple models or hyperparameters\n- Testing expensive APIs (LLMs, cloud services)\n- Results will be shared or published\n- Reproducibility is critical\n- Running experiments that take > 10 minutes\n\n**Skip for:**\n- Quick one-off comparisons (< 5 minutes to rerun)\n- Early prototyping (speed > reproducibility)\n- Solo throwaway analysis\n\n## Troubleshooting\n\n**\"Database is locked\" errors with DuckDB:**\n- **Cause**: DuckDB doesn't handle concurrent writes from multiple processes\n- **Solution**: Switch to SQLite engine if using multiprocessing\n- **Details**: See `references/build-and-cache.md` Pitfall 2 for full explanation\n\n**Cache not working:**\n- **Check installation**: Ensure `pip install xetrack[cache]` was run\n- **Check dataclass**: Must be frozen: `@dataclass(frozen=True, slots=True)`\n- **Float parameters**: Need rounding for consistent hashing (see `references/build-and-cache.md` Pitfall 6)\n- **Verify cache directory**: Check that cache path is writable\n\n**Import errors:**\n- **xetrack not found**: Run `pip install xetrack`\n- **DuckDB features**: Run `pip install xetrack[duckdb]`\n- **Asset management**: Run `pip install xetrack[assets]`\n- **Caching support**: Run `pip install xetrack[cache]`\n\n**\"Dataclass not unpacking\" issues:**\n- **Check method**: Auto-unpacking only works with `.track()`, not `.log()`\n- **Verify frozen**: Dataclass must have `frozen=True`\n- **See `references/build-and-cache.md`**: Pitfall 1 for detailed explanation\n\n## Git Versioning Skill\n\nThe **git-versioning** skill is a companion to the benchmark skill. While the benchmark skill runs experiments, the git-versioning skill handles versioning, merging, and retrieval of experiment artifacts.\n\n### When to Use\n\nUse the git-versioning skill when you need to:\n- Version experiments with git tags and DVC\n- Merge or rebase experiment results across branches\n- Promote models from candidates to production\n- Set up parallel experiments with git worktrees\n- Retrieve models or data from past experiments\n- Compare historical experiments side by side\n\n### Installation\n\n```bash\n# Plugin marketplace\n/plugin install git-versioning@xetrack\n\n# Manual\ncp -r xetrack/skills/git-versioning ~/.claude/skills/git-versioning\n```\n\n### Core Concepts\n\n**Workflow selection** â€” The skill helps you choose the right approach:\n\n| Scenario | Workflow | DB Engine | Branching |\n|----------|---------|-----------|-----------|\n| Single experiment | Sequential | SQLite | Main branch |\n| Param sweep, same code/data | Parallel | DuckDB | Main branch |\n| Different code or data per exp | Worktree | SQLite | Branch per exp |\n\n**Merge vs Rebase** â€” A novel decision framework for ML artifacts:\n- **Databases**: Merge (append rows) vs Rebase (replace when schema changed)\n- **Data files**: Merge (add samples) vs Rebase (preprocessing overhaul)\n- **Models**: Merge (keep as candidate) vs Rebase (promote to production)\n\n**Candidates pattern** â€” Keep models organized:\n- `models/production/model.bin` â€” current best (DVC tracked)\n- `models/candidates/` â€” runner-ups for A/B tests and ensembles\n\n### Scripts\n\n| Script | Purpose |\n|--------|---------|\n| `setup_worktree.sh` | Create worktree with shared DVC cache (prevents the #1 pitfall) |\n| `experiment_explorer.py` | Browse, compare, and retrieve past experiments |\n| `merge_artifacts.py` | DuckDB-powered merge/rebase for databases and parquet files |\n| `version_tag.py` | Create annotated tags with metric descriptions |\n| `model_manager.py` | Promote/prune models, manage candidates |\n\n### Example Prompts\n\n```\n\"Help me version my experiment and create a git tag\"\n\n\"Set up parallel experiments using git worktrees\"\n\n\"Merge results from my experiment branch back to main\"\n\n\"Retrieve the model from experiment e0.2.0\"\n\n\"Compare experiments e0.1.0 and e0.2.0 side by side\"\n```\n\n### How the Skills Work Together\n\n```\nBenchmark Skill                    Git Versioning Skill\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nPhase 0-3: Design & Build    â†’    (not needed yet)\nPhase 4-5: Run experiments   â†’    Choose workflow (sequential/parallel/worktree)\nPhase 6-7: Validate & Analyze â†’   Tag experiment, push artifacts\n                                   Merge results, promote models\n                                   Explore & compare past experiments\n```\n\n## Contributing\n\nFound an issue or want to improve the skills? Please open an issue or PR!\n\nThe skills were developed by running real simulations and discovering pitfalls, so real-world feedback is valuable.\n"
      },
      "plugins": [
        {
          "name": "benchmark",
          "source": "./skills/benchmark",
          "description": "Comprehensive ML/AI benchmarking skill that guides users through rigorous experiment design, execution, and analysis using xetrack",
          "version": "0.5.2",
          "author": {
            "name": "XDSS"
          },
          "homepage": "https://github.com/xdssio/xetrack",
          "repository": "https://github.com/xdssio/xetrack",
          "license": "MIT",
          "category": "ml-tools",
          "keywords": [
            "benchmarking",
            "machine-learning",
            "experiments",
            "tracking",
            "xetrack",
            "ml-ops",
            "data-science"
          ],
          "categories": [
            "benchmarking",
            "data-science",
            "experiments",
            "machine-learning",
            "ml-ops",
            "ml-tools",
            "tracking",
            "xetrack"
          ],
          "install_commands": [
            "/plugin marketplace add xdssio/xetrack",
            "/plugin install benchmark@xetrack"
          ]
        },
        {
          "name": "git-versioning",
          "source": "./skills/git-versioning",
          "description": "ML experiment versioning with Git + DVC + xetrack. Manage sequential/parallel/worktree workflows, merge/rebase experiment artifacts, model promotion, and experiment exploration",
          "version": "0.5.2",
          "author": {
            "name": "XDSS"
          },
          "homepage": "https://github.com/xdssio/xetrack",
          "repository": "https://github.com/xdssio/xetrack",
          "license": "MIT",
          "category": "ml-tools",
          "keywords": [
            "versioning",
            "dvc",
            "git",
            "experiments",
            "tracking",
            "xetrack",
            "ml-ops",
            "reproducibility"
          ],
          "categories": [
            "dvc",
            "experiments",
            "git",
            "ml-ops",
            "ml-tools",
            "reproducibility",
            "tracking",
            "versioning",
            "xetrack"
          ],
          "install_commands": [
            "/plugin marketplace add xdssio/xetrack",
            "/plugin install git-versioning@xetrack"
          ]
        }
      ]
    }
  ]
}