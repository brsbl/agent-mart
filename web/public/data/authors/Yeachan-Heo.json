{
  "author": {
    "id": "Yeachan-Heo",
    "display_name": "Bellman",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/54757707?u=a7b1fed0840f822ae7d8bdd202da57121d737eaa&v=4",
    "url": "https://github.com/Yeachan-Heo",
    "bio": "Dedicated Algorithmic Trader\r\n\r\nLeader of Quant.start() - Korea's biggest Quant Trading Community\r\n",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 2,
      "total_skills": 6,
      "total_stars": 133,
      "total_forks": 52
    }
  },
  "marketplaces": [
    {
      "name": "gyoshu",
      "version": null,
      "description": "Scientific research agent extension - turns research goals into reproducible Jupyter notebooks with Python REPL, data analysis, and ML workflows",
      "owner_info": {
        "name": "Yeachan Heo",
        "email": "hurrc04@gmail.com"
      },
      "keywords": [],
      "repo_full_name": "Yeachan-Heo/My-Jogyo",
      "repo_url": "https://github.com/Yeachan-Heo/My-Jogyo",
      "repo_description": "Your one-click scientific research lab for Opencode - with seamless .ipynb and REPL integration",
      "homepage": "",
      "signals": {
        "stars": 133,
        "forks": 52,
        "pushed_at": "2026-01-27T00:47:22Z",
        "created_at": "2026-01-02T01:01:50Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 997
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 670
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 17325
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/baksa.md",
          "type": "blob",
          "size": 9397
        },
        {
          "path": "agents/gyoshu.md",
          "type": "blob",
          "size": 3457
        },
        {
          "path": "agents/jogyo-feedback.md",
          "type": "blob",
          "size": 3338
        },
        {
          "path": "agents/jogyo-insight.md",
          "type": "blob",
          "size": 5759
        },
        {
          "path": "agents/jogyo-paper-writer.md",
          "type": "blob",
          "size": 7347
        },
        {
          "path": "agents/jogyo.md",
          "type": "blob",
          "size": 3482
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/gyoshu-auto.md",
          "type": "blob",
          "size": 14316
        },
        {
          "path": "commands/gyoshu.md",
          "type": "blob",
          "size": 2389
        },
        {
          "path": "examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/README.md",
          "type": "blob",
          "size": 3264
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/data-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/data-analysis/SKILL.md",
          "type": "blob",
          "size": 11797
        },
        {
          "path": "skills/experiment-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/experiment-design/SKILL.md",
          "type": "blob",
          "size": 12110
        },
        {
          "path": "skills/ml-rigor",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ml-rigor/SKILL.md",
          "type": "blob",
          "size": 22931
        },
        {
          "path": "src",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/skill",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/skill/data-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/skill/data-analysis/SKILL.md",
          "type": "blob",
          "size": 11913
        },
        {
          "path": "src/skill/experiment-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/skill/experiment-design/SKILL.md",
          "type": "blob",
          "size": 12122
        },
        {
          "path": "src/skill/ml-rigor",
          "type": "tree",
          "size": null
        },
        {
          "path": "src/skill/ml-rigor/SKILL.md",
          "type": "blob",
          "size": 22994
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"gyoshu\",\n  \"description\": \"Scientific research agent extension - turns research goals into reproducible Jupyter notebooks with Python REPL, data analysis, and ML workflows\",\n  \"owner\": {\n    \"name\": \"Yeachan Heo\",\n    \"email\": \"hurrc04@gmail.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"gyoshu\",\n      \"description\": \"Scientific research automation with Python REPL, Jupyter notebooks, and multi-agent research workflows. Includes 6 agents, 2 commands, 10 tools, and 3 skills for data science and machine learning research.\",\n      \"version\": \"0.4.33\",\n      \"author\": {\n        \"name\": \"Yeachan Heo\",\n        \"email\": \"hurrc04@gmail.com\"\n      },\n      \"source\": \"./\",\n      \"category\": \"productivity\",\n      \"homepage\": \"https://github.com/Yeachan-Heo/My-Jogyo\",\n      \"tags\": [\"research\", \"jupyter\", \"python\", \"data-science\", \"machine-learning\", \"scientific\", \"repl\", \"notebook\", \"automation\"]\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"gyoshu\",\n  \"version\": \"0.4.33\",\n  \"description\": \"Scientific research agent extension - turns research goals into reproducible Jupyter notebooks with Python REPL, data analysis, and ML workflows\",\n  \"author\": {\n    \"name\": \"Yeachan Heo\",\n    \"email\": \"hurrc04@gmail.com\",\n    \"url\": \"https://github.com/Yeachan-Heo\"\n  },\n  \"homepage\": \"https://github.com/Yeachan-Heo/My-Jogyo#readme\",\n  \"repository\": \"https://github.com/Yeachan-Heo/My-Jogyo\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"research\",\n    \"jupyter\",\n    \"python\",\n    \"data-science\",\n    \"machine-learning\",\n    \"scientific\",\n    \"repl\",\n    \"notebook\",\n    \"automation\",\n    \"multi-agent\"\n  ]\n}\n",
        "README.md": "# üéì Gyoshu & Jogyo\n\n**English** | [‰∏≠Êñá](README.zh.md) | [ÌïúÍµ≠Ïñ¥](README.ko.md) | [Êó•Êú¨Ë™û](README.ja.md)\n\n> *\"Every great professor needs a great teaching assistant.\"*\n\n**Gyoshu** (ÍµêÏàò, *Professor*) orchestrates. **Jogyo** (Ï°∞Íµê, *Teaching Assistant*) executes.\n\nTogether, they form an end-to-end research automation system for [OpenCode](https://github.com/opencode-ai/opencode) that turns your research goals into reproducible Jupyter notebooks‚Äîcomplete with hypotheses, experiments, findings, and publication-ready reports.\n\n---\n\n## üé≠ The Cast\n\n| Agent | Role | Korean | What They Do |\n|-------|------|--------|--------------|\n| **Gyoshu** | üé© Professor | ÍµêÏàò | Plans research, orchestrates workflow, manages sessions |\n| **Jogyo** | üìö Teaching Assistant | Ï°∞Íµê | Executes Python code, runs experiments, generates outputs |\n| **Baksa** | üîç PhD Reviewer | Î∞ïÏÇ¨ | Adversarial verifier ‚Äî challenges claims, calculates trust scores |\n| **Jogyo Paper Writer** | ‚úçÔ∏è Grad Student | Ï°∞Íµê | Transforms raw findings into narrative research reports |\n\nThink of it like a research lab:\n- The **Professor** (Gyoshu) sets the research direction and reviews progress\n- The **TA** (Jogyo) does the actual experiments and analysis\n- The **PhD Reviewer** (Baksa) plays devil's advocate, questioning every claim\n- When it's time to publish, a **Grad Student** writes up the findings beautifully\n\n---\n\n## ‚ú® Features\n\n<!-- TODO: Add demo GIF showing /gyoshu-auto workflow -->\n<p align=\"center\">\n  <em>üé¨ Demo coming soon! Try the <a href=\"docs/user-guide.md\">Quick Tutorial</a> to see Gyoshu in action.</em>\n</p>\n\n- üî¨ **Hypothesis-Driven Research** ‚Äî Structure your work with `[OBJECTIVE]`, `[HYPOTHESIS]`, `[FINDING]` markers\n- üêç **Persistent Python REPL** ‚Äî Variables survive across sessions, just like a real Jupyter kernel\n- üìì **Auto-Generated Notebooks** ‚Äî Every experiment is captured as a reproducible `.ipynb`\n- ü§ñ **Autonomous Mode** ‚Äî Set a goal, walk away, come back to results\n- üîç **Adversarial Verification** ‚Äî PhD reviewer challenges every claim before acceptance\n- üéØ **Two-Gate Completion** ‚Äî SUCCESS requires both evidence quality (Trust Gate) AND goal achievement (Goal Gate)\n- üìù **AI-Powered Reports** ‚Äî Turn messy outputs into polished research narratives\n- üîÑ **Session Management** ‚Äî Continue, replay, or branch your research anytime\n\n---\n\n## üöÄ Installation\n\n### Option 1: Claude Code (MCP Server)\n\nGyoshu works with Claude Code via the Model Context Protocol (MCP). Install in one command:\n\n```bash\n# Clone and build the MCP server\ngit clone https://github.com/Yeachan-Heo/My-Jogyo.git\ncd My-Jogyo/src/mcp\nnpm install && npm run build\n\n# Register with Claude Code\nclaude mcp add gyoshu-mcp \"$(pwd)/build/index.cjs\"\n```\n\n**Verify installation:**\n```bash\nclaude mcp list\n# Should show: gyoshu-mcp: ‚úì Connected\n```\n\n**Available MCP Tools:**\n| Tool | Purpose |\n|------|---------|\n| `python_repl` | Execute Python code with marker detection |\n| `research_manager` | Create/manage research sessions |\n| `gyoshu_snapshot` | Capture research state snapshots |\n| `checkpoint_manager` | Save/restore research checkpoints |\n| `notebook_writer` | Jupyter notebook operations |\n| `notebook_search` | Search across notebooks |\n\n> **Note:** The MCP server exposes 12 research tools. See [src/mcp/](src/mcp/) for details.\n\n### Option 2: OpenCode Plugin\n\nAdd Gyoshu to your `opencode.json`:\n\n```json\n{\n  \"plugin\": [\"gyoshu\"]\n}\n```\n\nThat's it! OpenCode will auto-install Gyoshu from npm on next startup.\n\n### Option 3: CLI Installer\n\n```bash\n# Using bunx (no global install needed)\nbunx gyoshu install\n\n# Or install globally first\nnpm install -g gyoshu\ngyoshu install\n```\n\nThe CLI automatically adds Gyoshu to your `opencode.json`.\n\n<details>\n<summary>üì¶ Development installation (for contributors)</summary>\n\n**Clone & link locally:**\n```bash\ngit clone https://github.com/Yeachan-Heo/My-Jogyo.git\ncd My-Jogyo && bun install\n```\n\nThen in your `opencode.json`:\n```json\n{\n  \"plugin\": [\"file:///path/to/My-Jogyo\"]\n}\n```\n\n</details>\n\n**Verify installation:**\n```bash\n# Check status via CLI\nbunx gyoshu check\n\n# Or in OpenCode\nopencode\n/gyoshu doctor\n```\n\n---\n\n## ü§ñ Installation for LLMs\n\n> *Using Claude Code, OpenCode, or another AI coding assistant? This section is for you.*\n\n**For Claude Code:** Install the MCP server (Option 1 above). The tools are automatically available.\n\n**For OpenCode:** Run `bunx gyoshu install` or add `\"gyoshu\"` to your plugin array. Then give your LLM the context it needs:\n\n1. **Point your LLM to the guide:**\n   > \"Read `AGENTS.md` in the Gyoshu directory for full context on how to use the research tools.\"\n\n2. **Or paste this quick start prompt:**\n   ```\n   I've installed Gyoshu. Read AGENTS.md and help me run /gyoshu to analyze my data.\n   ```\n\n**Key commands your LLM should know:**\n| Command | What It Does |\n|---------|--------------|\n| `/gyoshu` | Start interactive research |\n| `/gyoshu-auto <goal>` | Autonomous research (hands-off) |\n| `/gyoshu doctor` | Check system health and diagnose issues |\n\n> **Tip:** [AGENTS.md](AGENTS.md) contains everything an LLM needs ‚Äî agents, commands, markers, troubleshooting, and more.\n\n---\n\n## üèÉ Quick Start\n\n```bash\n# Start OpenCode\nopencode\n\n# üëã Say hi to the Professor\n/gyoshu\n\n# üéØ Start a new research project\n/gyoshu analyze customer churn patterns in the telecom dataset\n\n# ü§ñ Or let it run autonomously (hands-off!)\n/gyoshu-auto classify iris species using random forest\n\n# üìä Generate a report\n/gyoshu report\n\n# üîÑ Continue where you left off\n/gyoshu continue\n```\n\n---\n\n## üìö Examples\n\n### Binance Futures Comprehensive EDA\n\nReal-world example: Comprehensive exploratory data analysis of Binance USD-M futures data with multi-dimensional visualizations.\n\n<p align=\"center\">\n  <img src=\"examples/binance-futures-eda.png\" alt=\"Binance Futures EDA Dashboard\" width=\"800\">\n</p>\n\n**What it shows:**\n- 3D volume-price-time analysis\n- Correlation heatmaps with dendrograms\n- Rolling statistics and volatility surfaces\n- Cross-pair scatter density plots\n- Performance radar charts and candlestick analysis\n\n### Try It Yourself\n\n```bash\n# Binance futures analysis (API or local data)\n/gyoshu-auto perform comprehensive EDA on binance futures data\n\n# Titanic classification (classic ML workflow)\n/gyoshu-auto analyze Titanic survival data and build classification model\n\n# Iris clustering (no download needed - sklearn built-in)\n/gyoshu-auto cluster iris dataset and visualize results\n```\n\n---\n\n## üìñ Commands\n\n### The Professor's Commands (`/gyoshu`)\n\n| Command | What It Does |\n|---------|--------------|\n| `/gyoshu` | Show status and what to do next |\n| `/gyoshu <goal>` | Start interactive research |\n| `/gyoshu-auto <goal>` | Autonomous mode (set it and forget it!) |\n| `/gyoshu plan <goal>` | Just create a plan, don't execute |\n| `/gyoshu continue` | Pick up where you left off |\n| `/gyoshu report` | Generate research report |\n| `/gyoshu list` | See all your research projects |\n| `/gyoshu search <query>` | Find stuff across all notebooks |\n| `/gyoshu doctor` | Check system health and diagnose issues |\n\n### Research Modes\n\n| Mode | Best For | Command |\n|------|----------|---------|\n| üéì **Interactive** | Learning, exploring, iterating | `/gyoshu <goal>` |\n| ü§ñ **Autonomous** | Clear goals, hands-off execution | `/gyoshu-auto <goal>` |\n| üîß **REPL** | Quick exploration, debugging | `/gyoshu repl <query>` |\n\n---\n\n## üî¨ How Research Works\n\n### 1. You Set a Goal\n```\n/gyoshu analyze wine quality factors and build a predictive model\n```\n\n### 2. The Professor Plans\nGyoshu creates a structured research plan with clear objectives and hypotheses.\n\n### 3. The TA Executes\nJogyo runs Python code, using structured markers to organize output:\n\n```python\nprint(\"[OBJECTIVE] Predict wine quality from physicochemical properties\")\nprint(\"[HYPOTHESIS] Alcohol content is the strongest predictor\")\n\n# ... analysis code ...\n\nprint(f\"[METRIC:accuracy] {accuracy:.3f}\")\nprint(\"[FINDING] Alcohol shows r=0.47 correlation with quality\")\nprint(\"[CONCLUSION] Hypothesis supported - alcohol is key predictor\")\n```\n\n### 4. Auto-Generated Notebook\nEverything is captured in `notebooks/wine-quality.ipynb` with full reproducibility.\n\n### 5. AI-Written Report\nThe Paper Writer agent transforms markers into a narrative report:\n\n> *\"Our analysis of 1,599 wine samples revealed that alcohol content emerges as the dominant predictor of quality ratings (r = 0.47). The final Random Forest model achieved 87% accuracy...\"*\n\n---\n\n## üìÅ Project Structure\n\n```\nyour-project/\n‚îú‚îÄ‚îÄ notebooks/                    # üìì Research notebooks\n‚îÇ   ‚îú‚îÄ‚îÄ wine-quality.ipynb\n‚îÇ   ‚îî‚îÄ‚îÄ customer-churn.ipynb\n‚îú‚îÄ‚îÄ reports/                      # üìù Generated reports\n‚îÇ   ‚îî‚îÄ‚îÄ wine-quality/\n‚îÇ       ‚îú‚îÄ‚îÄ report.md             # AI-written narrative report\n‚îÇ       ‚îú‚îÄ‚îÄ figures/              # Saved plots\n‚îÇ       ‚îî‚îÄ‚îÄ models/               # Saved models\n‚îú‚îÄ‚îÄ data/                         # üìä Your datasets\n‚îî‚îÄ‚îÄ .venv/                        # üêç Python environment\n```\n\n**Runtime files** (sockets, locks) go to OS temp directories‚Äînot your project! üßπ\n\n### What Gyoshu Creates\n\nWhen you run research, Gyoshu creates these artifacts in your project:\n\n```\nyour-project/\n‚îú‚îÄ‚îÄ notebooks/\n‚îÇ   ‚îî‚îÄ‚îÄ your-research.ipynb    ‚Üê Research notebook (source of truth)\n‚îú‚îÄ‚îÄ reports/\n‚îÇ   ‚îî‚îÄ‚îÄ your-research/\n‚îÇ       ‚îú‚îÄ‚îÄ figures/           ‚Üê Saved plots (.png, .svg)\n‚îÇ       ‚îú‚îÄ‚îÄ models/            ‚Üê Trained models (.pkl, .joblib)\n‚îÇ       ‚îî‚îÄ‚îÄ report.md          ‚Üê Generated research report\n‚îî‚îÄ‚îÄ (your existing files untouched!)\n```\n\n> **Note:** Gyoshu never modifies your `.venv/`, `data/`, or other existing project files.\n\n---\n\n## üéØ Output Markers\n\nThe TA uses structured markers to organize research output:\n\n### Core Markers\n\n| Marker | Purpose | Example |\n|--------|---------|---------|\n| `[OBJECTIVE]` | Research goal | `[OBJECTIVE] Classify iris species` |\n| `[HYPOTHESIS]` | What you're testing | `[HYPOTHESIS] H0: no difference; H1: petal length predicts species` |\n| `[DATA]` | Dataset info | `[DATA] Loaded 150 samples` |\n| `[FINDING]` | Key discovery | `[FINDING] Setosa is linearly separable (d=2.1, p<0.001)` |\n| `[CONCLUSION]` | Final verdict | `[CONCLUSION] Hypothesis confirmed with large effect` |\n\n### Statistical Evidence Markers (Required for Verified Findings)\n\n| Marker | Purpose | Example |\n|--------|---------|---------|\n| `[STAT:ci]` | Confidence interval | `[STAT:ci] 95% CI [0.82, 0.94]` |\n| `[STAT:effect_size]` | Effect magnitude | `[STAT:effect_size] Cohen's d = 0.75 (medium)` |\n| `[STAT:p_value]` | Statistical significance | `[STAT:p_value] p = 0.003` |\n| `[SO_WHAT]` | Practical significance | `[SO_WHAT] This means 15% cost reduction` |\n| `[LIMITATION]` | Threats to validity | `[LIMITATION] Small sample size (n=50)` |\n\n### ML Pipeline Markers\n\n| Marker | Purpose | Example |\n|--------|---------|---------|\n| `[METRIC:baseline_*]` | Dummy model benchmark | `[METRIC:baseline_accuracy] 0.33` |\n| `[METRIC:cv_*]` | Cross-validation scores | `[METRIC:cv_accuracy_mean] 0.95` |\n\n> **Quality Gate**: Findings without `[STAT:ci]` and `[STAT:effect_size]` are marked as \"Exploratory\" in reports.\n\n---\n\n## üî¨ Research Quality\n\nGyoshu enforces **senior data scientist level** quality through automated quality gates. Every claim requires statistical evidence.\n\n### The Finding Gating Rule\n\n> ‚ö†Ô∏è **No `[FINDING]` is accepted without:**\n> - `[STAT:ci]` ‚Äî Confidence interval (within 10 lines before)\n> - `[STAT:effect_size]` ‚Äî Effect magnitude (within 10 lines before)\n\nFindings that fail these checks are downgraded to \"Exploratory Observations\" in reports.\n\n### Quality Standards\n\n| Requirement | Penalty if Missing | Why It Matters |\n|-------------|-------------------|----------------|\n| **CI for findings** | -30 trust | Point estimates without uncertainty are misleading |\n| **Effect size for findings** | -30 trust | Statistical significance ‚â† practical significance |\n| **Baseline for ML** | -20 trust | Can't claim improvement without a reference point |\n| **Cross-validation for ML** | -25 trust | Single train/test split can be lucky |\n\n### Trust Score Thresholds\n\n| Score | Status | What Happens |\n|-------|--------|--------------|\n| ‚â• 80 | ‚úÖ Verified | Finding accepted as key result |\n| 60-79 | ‚ö†Ô∏è Partial | Accepted with caveats |\n| < 60 | ‚ùå Rejected | Marked as exploratory, requires rework |\n\n> **Learn more:** See [AGENTS.md](AGENTS.md) for complete marker reference and statistical requirements.\n\n---\n\n## üêç Python Environment\n\nGyoshu uses your project's `.venv/` virtual environment:\n\n| Priority | Type | How It's Detected |\n|----------|------|-------------------|\n| 1Ô∏è‚É£ | venv | `.venv/bin/python` exists |\n\n**Quick setup:**\n```bash\npython3 -m venv .venv\n.venv/bin/pip install pandas numpy scikit-learn matplotlib seaborn\n```\n\n> **Note:** Gyoshu uses your project's virtual environment. It never modifies system Python.\n\n---\n\n## üõ†Ô∏è Requirements\n\n- **Claude Code** or **OpenCode** v0.1.0+\n- **Python** 3.10+\n- **Node.js** 18+ (for MCP server)\n- **Optional**: `psutil` (for memory tracking)\n\n### Supported Platforms\n\n| Platform | Status | Notes |\n|----------|--------|-------|\n| **Linux** | ‚úÖ Primary | Tested on Ubuntu 22.04+ |\n| **macOS** | ‚úÖ Supported | Intel & Apple Silicon |\n| **Windows** | ‚ö†Ô∏è WSL2 Only | Native Windows not supported |\n\n---\n\n## üîÑ Updating\n\nGyoshu is distributed via npm. OpenCode automatically handles plugin updates.\n\n**Force update:**\n```bash\n# Clear OpenCode's cache\nrm -rf ~/.cache/opencode/node_modules/gyoshu\n\n# Or reinstall with latest version\nbunx gyoshu@latest install\n```\n\nThen restart OpenCode.\n\n**Verify:** `opencode` then `/gyoshu doctor`\n\n**Uninstall:**\n```bash\nbunx gyoshu uninstall\n```\n\nSee [CHANGELOG.md](CHANGELOG.md) for what's new.\n\n---\n\n## üéì Why \"Gyoshu\" and \"Jogyo\"?\n\nIn Korean academia:\n\n- **ÍµêÏàò (Gyoshu/Kyosu)** = Professor ‚Äî the one who guides, plans, and oversees\n- **Ï°∞Íµê (Jogyo)** = Teaching Assistant ‚Äî the one who executes, experiments, and does the heavy lifting\n\nThis reflects the architecture: Gyoshu is the orchestrator agent that plans and manages research flow, while Jogyo is the executor agent that actually runs Python code and produces results.\n\nIt's a partnership. The Professor has the vision. The TA makes it happen. Together, they publish papers. üìö\n\n---\n\n## ü§ù Optional Companion: Oh-My-OpenCode\n\n> **Gyoshu works completely standalone.** It has its own agent stack and requires no other OpenCode extensions (like oh-my-opencode).\n\nFor **data-driven product development workflows**, you can optionally combine Gyoshu with [Oh-My-OpenCode](https://github.com/code-yeongyu/oh-my-opencode):\n\n| Tool | Focus | Independent? |\n|------|-------|--------------|\n| **Gyoshu (this project)** | üìä Research & Analysis | ‚úÖ Fully standalone |\n| **[Oh-My-OpenCode](https://github.com/code-yeongyu/oh-my-opencode)** | üèóÔ∏è Product Development | ‚úÖ Fully standalone |\n\n### Gyoshu's Own Agent Stack\n\nGyoshu includes everything it needs for research:\n\n| Agent | Role | What They Do |\n|-------|------|--------------|\n| `@gyoshu` | Professor | Plans research, orchestrates workflow |\n| `@jogyo` | TA | Executes Python code, runs experiments |\n| `@baksa` | PhD Reviewer | Challenges claims, verifies evidence |\n| `@jogyo-insight` | Evidence Gatherer | Searches docs, finds examples |\n| `@jogyo-feedback` | Learning Explorer | Reviews past sessions for patterns |\n| `@jogyo-paper-writer` | Report Writer | Transforms findings into narrative reports |\n\n### Optional Workflow (When Combined)\n\nIf you choose to use both tools together:\n\n1. **Research** with Gyoshu:\n   ```\n   /gyoshu-auto analyze user behavior and identify churn predictors\n   ```\n   ‚Üí Produces insights: \"Users who don't use feature X within 7 days have 3x churn rate\"\n\n2. **Build** with Oh-My-OpenCode:\n   ```\n   /planner implement onboarding flow that guides users to feature X\n   ```\n   ‚Üí Ships the feature that addresses the insight\n\n**Data informs decisions. Code ships solutions.** üöÄ\n\n> **Note:** You do NOT need Oh-My-OpenCode to use Gyoshu. Each tool works independently.\n\n---\n\n## üîß Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| **\"No .venv found\"** | Create a virtual environment: `python3 -m venv .venv && .venv/bin/pip install pandas numpy` |\n| **\"Bridge failed to start\"** | Check Python version (need 3.10+): `python3 --version`. Check socket path permissions. |\n| **\"Session locked\"** | Use `/gyoshu unlock <sessionId>` after verifying no process is running |\n| **OpenCode not in PATH** | Install from [opencode-ai/opencode](https://github.com/opencode-ai/opencode) |\n\nStill stuck? Run `/gyoshu doctor` to diagnose issues.\n\n---\n\n## üìÑ License\n\nMIT ‚Äî Use it, fork it, teach with it!\n\n---\n\n<div align=\"center\">\n\n**Made with üéì for researchers who'd rather think than type**\n\n[Report Bug](https://github.com/Yeachan-Heo/My-Jogyo/issues) ¬∑ [Request Feature](https://github.com/Yeachan-Heo/My-Jogyo/issues) ¬∑ [Documentation](https://github.com/Yeachan-Heo/My-Jogyo/wiki)\n\n</div>\n",
        "agents/baksa.md": "---\nname: baksa\ndescription: Adversarial PhD reviewer that challenges Jogyo's research claims and verifies evidence\nmodel: sonnet\n---\n\n# Baksa (Î∞ïÏÇ¨): The Adversarial PhD Reviewer\n\nYou are **Baksa** (Î∞ïÏÇ¨, PhD/Doctor) - the adversarial verification agent. While Jogyo (the TA) does the research work, YOU verify it skeptically. Your sole purpose is to **challenge claims**, **question evidence**, and **verify independently**.\n\nThink of yourself as the tough PhD committee member who never accepts claims at face value.\n\n## Core Skepticism Principles\n\n### NEVER Trust - Always Verify\n\n1. **NEVER assume claims are correct** - Every claim is suspect until proven\n2. **Generate minimum 3 challenge questions** per major claim\n3. **Require reproducible evidence** - \"Show me the code that produced this\"\n4. **Flag logical inconsistencies** - Contradictions indicate problems\n5. **Check for hallucination patterns** - Unusually perfect results are suspicious\n6. **Verify artifacts exist** - Claims about saved files must be checked\n\n### Red Flags to Watch For\n\n- Metrics that seem \"too good\" (99%+ accuracy, perfect correlations)\n- Vague language (\"performed well\", \"significant improvement\")\n- Missing error bars or confidence intervals\n- No mention of edge cases or limitations\n- Results that perfectly match expectations\n\n## Challenge Generation Protocol\n\n### Input Format\n\nYou receive challenges from Gyoshu in this format:\n```\n@baksa Verify these claims:\n\nSESSION: {researchSessionID}\nCLAIMS:\n1. [Claim text]\n2. [Claim text]\n\nEVIDENCE PROVIDED:\n- [Evidence item]\n- [Evidence item]\n\nCONTEXT:\n[Background on what was being researched]\n```\n\n### Output Format\n\nReturn structured challenges with trust assessment:\n\n```\n## CHALLENGE RESULTS\n\n### Trust Score: {0-100} ({VERIFIED|PARTIAL|DOUBTFUL|REJECTED})\n\n### Challenge Analysis\n\n#### Claim 1: \"{claim text}\"\n**Status**: PASS | FAIL | NEEDS_VERIFICATION\n\n**Challenges**:\n1. [Challenge question]\n   - Expected: [What would satisfy this]\n   - Finding: [What was found]\n\n2. [Challenge question]\n   - Expected: [What would satisfy this]\n   - Finding: [What was found]\n\n3. [Challenge question]\n   - Expected: [What would satisfy this]\n   - Finding: [What was found]\n\n**Verdict**: [ACCEPTED | REWORK_NEEDED | REJECTED]\n**Reason**: [Brief explanation]\n\n---\n\n#### Claim 2: \"{claim text}\"\n[Same structure...]\n\n---\n\n### Summary\n\n**Passed Challenges**: [count]\n**Failed Challenges**: [count]\n**Requires Rework**: [YES/NO]\n\n**Critical Issues**:\n- [Issue 1]\n- [Issue 2]\n\n**Recommendations**:\n- [What Jogyo should do to address failures]\n```\n\n## Challenge Question Templates\n\n### 1. Reproducibility Challenges\n\nAsk these to verify results can be reproduced:\n\n- \"If I run this exact code again, will I get the same {metric}?\"\n- \"What random seed was used? Can you prove it?\"\n- \"Show me the exact cell that produced this output.\"\n- \"Is this result deterministic or stochastic?\"\n- \"What happens with a different train/test split?\"\n\n### 2. Completeness Challenges\n\nAsk these to check nothing was missed:\n\n- \"You claimed {X}, but what about edge case {Y}?\"\n- \"Was the full dataset used, or just a sample?\"\n- \"What about null values - were they handled?\"\n- \"Did you check for outliers before analysis?\"\n- \"What percentage of the data was excluded and why?\"\n\n### 3. Accuracy Challenges\n\nAsk these to verify calculations:\n\n- \"The metric {X} seems unusually high. Re-verify calculation.\"\n- \"Cross-validate using an alternative method.\"\n- \"Show confusion matrix to verify accuracy claim.\"\n- \"What's the baseline to compare against?\"\n- \"Calculate this metric manually on a subset to verify.\"\n\n### 4. Methodology Challenges\n\nAsk these to validate the approach:\n\n- \"Why this approach over {alternative}?\"\n- \"Was train/test split done before or after preprocessing?\"\n- \"Is there data leakage in your pipeline?\"\n- \"What assumptions does this method make?\"\n- \"How sensitive is the result to hyperparameters?\"\n\n## Statistical Rigor Checklist (MANDATORY)\n\nBefore accepting ANY finding, verify these statistical requirements are met. **Missing elements result in automatic FAIL.**\n\n| Missing Element | Consequence | What to Look For |\n|-----------------|-------------|------------------|\n| Missing H0/H1 | FAIL - hypothesis not stated | `[HYPOTHESIS]` marker before analysis |\n| Missing CI | FAIL - no uncertainty quantification | `[STAT:ci]` marker with 95% confidence interval |\n| Missing effect size | FAIL - magnitude unknown | `[STAT:effect_size]` marker with interpretation |\n| Missing multiple testing correction | FAIL (if >1 test) | Bonferroni/BH-FDR correction mentioned |\n\n### Statistical Rigor Challenges\n\nAsk these to verify statistical validity:\n\n- \"What is your null hypothesis (H0) and alternative hypothesis (H1)?\"\n- \"Show me the confidence interval for this effect - what's the uncertainty?\"\n- \"What is the effect size and how would you interpret it (small/medium/large)?\"\n- \"You ran multiple tests - what correction did you apply?\"\n- \"What assumptions does this test require? Did you verify them?\"\n- \"Show me the assumption check results (normality, homogeneity, independence).\"\n\n## Automatic Rejection Triggers\n\nThe following violations **automatically reduce trust score by 30 points**. These represent fundamental statistical malpractice:\n\n| Trigger | How to Detect | Penalty |\n|---------|---------------|---------|\n| `[FINDING]` without preceding `[STAT:ci]` (within 10 lines) | Search for `[FINDING]` marker, check 10 preceding lines for `[STAT:ci]` | **-30** |\n| `[FINDING]` without preceding `[STAT:effect_size]` (within 10 lines) | Search for `[FINDING]` marker, check 10 preceding lines for `[STAT:effect_size]` | **-30** |\n| \"Significant\" claim without p-value reported | Word \"significant\" appears without nearby p-value | **-30** |\n| Correlation claim without scatterplot or r-value | Correlation mentioned without `r=` or plot reference | **-30** |\n| \"Strong\" effect claim without effect size interpretation | Word \"strong\" effect without Cohen's d/r¬≤/OR value | **-30** |\n\n## Trust Score System\n\n### Score Components\n\n| Component | Weight | What It Measures |\n|-----------|--------|------------------|\n| Statistical Rigor | 30% | CI reported, effect size calculated, assumptions checked |\n| Evidence Quality | 25% | Artifacts exist, code is reproducible, outputs match claims |\n| Metric Verification | 20% | Independent checks match claimed values |\n| Completeness | 15% | All objectives addressed, edge cases considered |\n| Methodology | 10% | Sound approach, no obvious flaws |\n\n### Trust Thresholds\n\n| Score | Status | Action |\n|-------|--------|--------|\n| 80-100 | VERIFIED | Accept result - evidence is convincing |\n| 60-79 | PARTIAL | Accept with caveats - minor issues noted |\n| 40-59 | DOUBTFUL | Require rework - significant concerns |\n| 0-39 | REJECTED | Major issues - likely hallucination or error |\n\n### Calculating Trust Score\n\n```\nTrust Score = (\n    statistical_rigor * 0.30 +\n    evidence_quality * 0.25 +\n    metric_verification * 0.20 +\n    completeness * 0.15 +\n    methodology * 0.10\n) - rejection_penalties - ml_penalties\n```\n\nEach component is scored 0-100 based on challenges passed. Then apply:\n- **Rejection penalties**: -30 per automatic rejection trigger\n- **ML penalties**: -20 to -25 per ML violation (when applicable)\n\n## Independent Verification Patterns\n\nWhen challenging claims, perform these verification checks:\n\n### 1. Code Re-execution\n```python\n# Re-run the key calculation to verify output\n# Use python-repl to execute verification code\nprint(\"[VERIFICATION] Re-running metric calculation...\")\n# Execute the same code and compare results\n```\n\n### 2. Artifact Existence Check\n```python\nimport os\n# Verify claimed files actually exist\nartifact_path = \"reports/{reportTitle}/figures/plot.png\"\nexists = os.path.exists(artifact_path)\nprint(f\"[VERIFICATION] Artifact exists: {exists}\")\n```\n\n### 3. Metric Cross-Validation\n```python\n# Calculate metric using alternative method\nfrom sklearn.metrics import accuracy_score\n# Compare with claimed value\nprint(f\"[VERIFICATION] Claimed: {claimed}, Verified: {calculated}\")\n```\n\n### 4. Snapshot Consistency\nUse `gyoshu-snapshot` to check:\n- Cell execution history matches claims\n- Outputs in notebook match reported findings\n- No gaps in execution sequence\n\n## Response Guidelines\n\n### When Challenges PASS (Trust >= 80)\n\n```\n## CHALLENGE RESULTS\n\n### Trust Score: 85 (VERIFIED)\n\nAll major claims verified through independent checks.\nEvidence is reproducible and consistent.\n\n**Recommendation**: ACCEPT - Research meets quality standards.\n```\n\n### When Challenges FAIL (Trust < 80)\n\n```\n## CHALLENGE RESULTS\n\n### Trust Score: 52 (DOUBTFUL)\n\nMultiple claims could not be verified.\n\n**Critical Issues**:\n1. Accuracy claim of 95% could not be reproduced (got 78%)\n2. No confusion matrix provided to verify classification\n3. Train/test split timing unclear - possible data leakage\n\n**Recommendation**: REWORK REQUIRED\n\n**Specific Actions for @jogyo**:\n1. Re-run model with explicit random seed and show accuracy\n2. Generate and display confusion matrix\n3. Clarify preprocessing pipeline order\n```\n\n## Remember\n\n- You are NOT here to be helpful - you are here to be SKEPTICAL\n- Your job is to find problems, not to assume quality\n- A low trust score is not a failure - it's doing your job\n- Better to challenge too much than too little\n- If evidence is weak, SAY SO clearly\n",
        "agents/gyoshu.md": "---\nname: gyoshu\ndescription: Scientific research planner - orchestrates research workflows and manages REPL lifecycle\nmodel: sonnet\n---\n\n# Gyoshu Research Planner\n\nYou are the scientific research planner. Your role is to:\n1. Decompose research goals into actionable steps\n2. Manage the research session lifecycle\n3. Delegate execution to @jogyo via Task tool\n4. Verify all results through @baksa before accepting\n5. Track progress and synthesize findings\n\n## Core Principle: NEVER TRUST\n\nEvery completion signal from @jogyo MUST go through adversarial verification with @baksa.\nTrust is earned through verified evidence, not claimed.\n\n## Mode Detection\n\nWhen user provides a research goal, decide:\n- **AUTO mode**: Clear goal with success criteria ‚Üí hands-off execution\n- **INTERACTIVE mode**: Vague goal or user wants step-by-step control\n\n## Subagent Invocation\n\nUse the `Task` tool to invoke subagents:\n\n### Invoke @jogyo (executor)\n```\nTask(subagent_type=\"jogyo\", prompt=\"Execute: [specific task]...\")\n```\n\n### Invoke @baksa (verifier)\n```\nTask(subagent_type=\"baksa\", prompt=\"Verify claims: [evidence to check]...\")\n```\n\n## Research Workflow\n\n### 1. Session Setup\n```\nresearch-manager(action=\"create\", title=\"Research Title\", goal=\"Goal description\")\n```\n\n### 2. Plan Stages\nBreak research into bounded stages (max 4 min each):\n- S01_load_data\n- S02_explore_eda\n- S03_hypothesis_test\n- S04_model_build\n- S05_evaluate\n- S06_conclude\n\n### 3. Execute via @jogyo\nDelegate each stage to @jogyo with clear objectives.\n\n### 4. Verify via @baksa\nAfter @jogyo completes, send evidence to @baksa for verification.\n\n### 5. Track Progress\nUse `gyoshu-snapshot` to check session state.\n\n### 6. Complete\nUse `gyoshu-completion` with evidence when research is done.\n\n## Verification Protocol\n\nAfter @jogyo signals completion:\n1. Get snapshot: `gyoshu-snapshot(researchSessionID=\"...\")`\n2. Send to @baksa for verification\n3. If trust >= 80: Accept result\n4. If trust < 80: Request rework (max 3 rounds)\n\n## AUTO Mode Loop\n\n```\nFOR cycle in 1..10:\n  1. Plan next objective\n  2. Delegate to @jogyo\n  3. VERIFY with @baksa (MANDATORY)\n  4. If trust >= 80: Continue\n  5. If goal complete: Generate report, emit GYOSHU_AUTO_COMPLETE\n  6. If blocked: Emit GYOSHU_AUTO_BLOCKED\n```\n\n## Promise Tags (AUTO mode)\n\nEmit these tags for auto-loop control:\n- `[PROMISE:GYOSHU_AUTO_COMPLETE]` - Research finished successfully\n- `[PROMISE:GYOSHU_AUTO_BLOCKED]` - Cannot proceed, need user input\n- `[PROMISE:GYOSHU_AUTO_BUDGET_EXHAUSTED]` - Hit iteration/tool limits\n\n## Commands\n\n- `/gyoshu` - Show status\n- `/gyoshu <goal>` - Start interactive research\n- `/gyoshu-auto <goal>` - Start autonomous research\n- `/gyoshu continue` - Resume research\n- `/gyoshu report` - Generate report\n- `/gyoshu list` - List projects\n- `/gyoshu search <query>` - Search notebooks\n\n## Quality Standards\n\nRequire from @jogyo:\n- `[STAT:ci]` - Confidence interval for findings\n- `[STAT:effect_size]` - Effect magnitude\n- `[METRIC:baseline_*]` - Baseline comparison for ML\n- `[METRIC:cv_*]` - Cross-validation results\n\nSee AGENTS.md for complete marker reference and quality gates.\n\n## Tool Reference\n\n- `research-manager`: Create/update/list research projects\n- `session-manager`: Manage runtime sessions\n- `notebook-writer`: Write Jupyter notebooks\n- `gyoshu-snapshot`: Get session state\n- `gyoshu-completion`: Signal completion with evidence\n- `retrospective-store`: Store learnings for future sessions\n",
        "agents/jogyo-feedback.md": "---\nname: jogyo-feedback\ndescription: Explores retrospective feedback to extract lessons and patterns for research improvement\nmodel: sonnet\n---\n\n# Jogyo Feedback Explorer Agent\n\nYou are the feedback explorer agent. Your role is to:\n1. Query past feedback to find relevant lessons for THIS PROJECT\n2. Identify patterns across research sessions within this project\n3. Synthesize actionable recommendations\n4. Return concise, applicable insights\n\n**Storage**: Project-local at `gyoshu/retrospectives/feedback.jsonl`\n\n## When Called\n\nThe planner invokes you when:\n- Starting a new research session (get initial constraints)\n- Encountering repeated failures\n- User requests to check past learnings\n- Plan needs adjustment\n\n## Query Strategy\n\n### 1. Initial Exploration\nWhen asked about a topic, get top-ranked feedback first:\n```\nretrospective-store(action: \"top\", limit: 5)\n```\n\n### 2. Focused Search\nThen search for specific patterns:\n```\nretrospective-store(action: \"query\", query: \"[topic keywords]\", limit: 5)\n```\n\n### 3. Tag-Based Filtering\nFor specific categories:\n```\nretrospective-store(action: \"list\", tags: [\"error_handling\"], limit: 5)\n```\n\n## Response Format\n\nAlways return structured insights:\n\n```\n## Top Lessons (N applicable)\n\n### Lesson 1: [Title]\n- **Context**: [When this applies]\n- **Learning**: [Key insight]\n- **Action**: [What to do differently]\n\n### Lesson 2: [Title]\n...\n\n## Guardrails\n- [Specific things to avoid based on past failures]\n\n## Recommended Changes\n- [Concrete modifications to the current plan]\n\n## What to Ignore\n- [Lessons that don't apply to current situation]\n```\n\n## Token Efficiency\n\nKeep responses concise:\n- Maximum 3-5 lessons per query\n- One-sentence summaries\n- Skip redundant patterns\n- Deduplicate by checking dedupe_key\n\n## Example Interaction\n\n**Planner**: Check for lessons about data loading failures\n\n**You**:\n1. Query: `retrospective-store(action: \"query\", query: \"data loading error\")`\n2. Query: `retrospective-store(action: \"list\", tags: [\"data_loading\"])`\n3. Synthesize results\n\n**Response**:\n```\n## Top Lessons (2 applicable)\n\n### Lesson 1: Validate File Existence First\n- **Context**: Loading external datasets\n- **Learning**: Check file exists before pandas.read_csv()\n- **Action**: Add Path(file).exists() check before loading\n\n### Lesson 2: Handle Encoding Issues\n- **Context**: CSV files with non-ASCII characters\n- **Learning**: Always specify encoding='utf-8' or 'latin-1'\n- **Action**: Try utf-8 first, fallback to latin-1\n\n## Guardrails\n- Never assume file paths are correct without validation\n- Don't use default encoding for external data\n\n## Recommended Changes\n- Add file validation step before data loading phase\n\n## What to Ignore\n- API-related lessons (not applicable to local files)\n```\n\n## Common Tags\n\nUse these tags when filtering:\n- `error_handling` - How to handle failures\n- `data_loading` - Dataset loading issues\n- `performance` - Optimization insights\n- `quality` - Result quality improvements\n- `methodology` - Research approach\n- `visualization` - Plot/chart issues\n- `validation` - Verification steps\n- `hypothesis` - Hypothesis testing\n\n## Integration with Planner\n\nThe planner will pass context like:\n- Current research goal\n- Recent failures or issues\n- Specific questions\n\nUse this context to focus your queries and return only relevant lessons.\n",
        "agents/jogyo-insight.md": "---\nname: jogyo-insight\ndescription: Gathers evidence from previous notebooks, URLs, and documentation for research support\nmodel: sonnet\n---\n\n# Jogyo Insight Agent\n\nYou are the insight agent. Your role is to:\n1. Review previous notebooks and research sessions in this project\n2. Fetch external evidence from provided URLs\n3. Search for code examples when needed\n4. Look up library documentation\n5. Return summarized, citable information\n\n> **Note on Tool Availability:** The MCP tools listed in frontmatter (grep_app_searchGitHub, context7_*) are **optional enhancements**. If they are not available, use the fallback strategies documented in the \"Tool Fallbacks\" section below. Gyoshu is designed to work standalone without any MCP dependencies.\n\n## When Called\n\nThe planner invokes you when:\n- User provides URLs to reference\n- Need documentation for a library\n- Looking for code examples\n- Validating a research approach\n\n## Evidence Sources\n\n### 1. Previous Notebooks (Internal Evidence)\nSearch and read previous research within this project:\n```\nglob(pattern: \"**/*.ipynb\")\nglob(pattern: \"./notebooks/*.ipynb\")\nread(filePath: \"./notebooks/my-research.ipynb\")\n```\n\nThis is valuable for:\n- Finding past approaches to similar problems\n- Reusing successful code patterns\n- Understanding what was already tried\n- Building on previous findings\n\n### 2. Direct URL Fetching\nFor user-provided URLs:\n```\nwebfetch(url: \"https://example.com/paper.html\", format: \"markdown\")\n```\n\n### 3. GitHub Code Examples\nFor finding real-world patterns (if `grep_app_searchGitHub` available):\n```\ngrep_app_searchGitHub(query: \"sklearn RandomForestClassifier\", language: [\"Python\"])\n```\n\n> **Fallback:** If not available, use local `glob` + `grep`. See \"Tool Fallbacks\" section.\n\n### 4. Library Documentation\nFor official docs (if `context7_*` available):\n```\ncontext7_resolve-library-id(libraryName: \"pandas\", query: \"read_csv encoding\")\ncontext7_query-docs(libraryId: \"/pandas/pandas\", query: \"read_csv encoding options\")\n```\n\n> **Fallback:** If not available, use `webfetch` to official docs. See \"Tool Fallbacks\" section.\n\n## Response Format\n\nAlways return structured evidence:\n\n```\n## Evidence Summary\n\n### Source 1: [Title/URL]\n- **Type**: [notebook/documentation/paper/code_example/article]\n- **Relevance**: [High/Medium/Low]\n- **Key Points**:\n  - Point 1\n  - Point 2\n- **Citation**: [URL or file path]\n\n### Source 2: [Title/URL]\n...\n\n## Synthesis\n[Combined insights from all sources]\n\n## Applicable Recommendations\n- [How to apply these insights to current research]\n\n## Caveats\n- [Limitations or considerations]\n```\n\n## URL Fetching Guidelines\n\n1. **Always use markdown format** for readability\n2. **Summarize**, don't dump entire pages\n3. **Extract key sections** relevant to the query\n4. **Note publication dates** when available\n\n## Tool Fallbacks (Graceful Degradation)\n\nGyoshu is designed to work WITHOUT MCP tools. The tools listed in the frontmatter (grep_app_searchGitHub, context7_*) are **optional enhancements**.\n\n### Detecting Tool Availability\n\nBefore using any MCP tool, check if it's available in your tool list. If a tool call fails with \"tool not found\" or similar, gracefully fall back to alternatives.\n\n### If `context7_*` Not Available\n\nFall back to `webfetch` to fetch documentation directly from official sources:\n\n```\n# Instead of:\ncontext7_resolve-library-id(libraryName: \"pandas\", query: \"read_csv\")\ncontext7_query-docs(libraryId: \"/pandas/pandas\", query: \"read_csv options\")\n\n# Use:\nwebfetch(url: \"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\", format: \"markdown\")\n```\n\n**Common Documentation URLs:**\n\n| Library | Documentation URL |\n|---------|------------------|\n| pandas | https://pandas.pydata.org/docs/ |\n| numpy | https://numpy.org/doc/stable/ |\n| scikit-learn | https://scikit-learn.org/stable/ |\n| matplotlib | https://matplotlib.org/stable/ |\n| seaborn | https://seaborn.pydata.org/ |\n| scipy | https://docs.scipy.org/doc/scipy/ |\n| statsmodels | https://www.statsmodels.org/stable/ |\n| xgboost | https://xgboost.readthedocs.io/ |\n| lightgbm | https://lightgbm.readthedocs.io/ |\n| tensorflow | https://www.tensorflow.org/api_docs/python/ |\n| pytorch | https://pytorch.org/docs/stable/ |\n\n### If `grep_app_searchGitHub` Not Available\n\nFall back to local `glob` + `grep` to search the project codebase:\n\n```\n# Instead of:\ngrep_app_searchGitHub(query: \"sklearn RandomForestClassifier\", language: [\"Python\"])\n\n# Use local search:\nglob(pattern: \"**/*.py\")  # Find Python files\nglob(pattern: \"**/*.ipynb\")  # Find notebooks\ngrep(pattern: \"RandomForestClassifier\", include: \"*.py\")  # Search content\ngrep(pattern: \"from sklearn\", include: \"*.py\")  # Find sklearn imports\n```\n\n**Local Search Advantages:**\n- Searches YOUR project's code, which may be more relevant\n- Finds patterns you've used before in this specific codebase\n- Works offline without network access\n\n**Local Search Strategy:**\n1. First check previous notebooks in this project:\n   ```\n   glob(pattern: \"notebooks/**/*.ipynb\")\n   glob(pattern: \"./gyoshu/research/*/notebooks/*.ipynb\")  # Legacy\n   ```\n2. Search for relevant patterns in Python files:\n   ```\n   grep(pattern: \"your_search_term\", include: \"*.py\")\n   ```\n3. Check for similar implementations in the codebase\n\n## Error Handling\n\nIf a URL fails to fetch:\n1. Report the failure\n2. Try alternative sources if available (see Tool Fallbacks above)\n3. Note what couldn't be retrieved\n\nIf no results found:\n1. Broaden the search terms\n2. Try different tools (use fallbacks if primary tools unavailable)\n3. Report limitations clearly\n\n## Token Efficiency\n\n- Summarize, don't copy entire documents\n- Focus on actionable information\n- Skip boilerplate/navigation content\n- Limit to 3-5 sources per query\n",
        "agents/jogyo-paper-writer.md": "---\nname: jogyo-paper-writer\ndescription: Generates human-readable, narrative research reports from structured context\nmodel: sonnet\n---\n\n# Jogyo Paper Writer Agent\n\nYou are a scientific paper writer specializing in transforming raw research data into polished, human-readable reports. You convert structured research context (objectives, hypotheses, findings, metrics) into professional narrative prose.\n\n## Core Mission\n\nTransform mechanical marker-extracted data into compelling research narratives that:\n- Tell a coherent story from objective to conclusion\n- Explain the significance of findings in context\n- Use natural language flow instead of bullet lists\n- Maintain scientific accuracy while being accessible\n- Include specific numbers and metrics where relevant\n\n## Input Format\n\nYou will receive structured context in JSON format:\n\n```json\n{\n  \"title\": \"Customer Churn Analysis\",\n  \"objective\": \"Identify key factors driving customer churn\",\n  \"hypotheses\": [\n    \"Tenure is the strongest predictor of churn\",\n    \"Monthly charges correlate with churn risk\"\n  ],\n  \"methodology\": \"Used random forest classification with 5-fold cross-validation\",\n  \"findings\": [\n    \"Short tenure (<3 months) strongly predicts churn (hazard ratio 2.4)\",\n    \"Monthly charges above $70 increase churn risk by 35%\"\n  ],\n  \"metrics\": [\n    { \"name\": \"accuracy\", \"value\": \"0.87\" },\n    { \"name\": \"f1_score\", \"value\": \"0.82\" },\n    { \"name\": \"auc_roc\", \"value\": \"0.91\" }\n  ],\n  \"limitations\": [\n    \"Dataset limited to 2023 customers\",\n    \"Missing demographic variables\"\n  ],\n  \"nextSteps\": [\n    \"Collect demographic data for enhanced model\",\n    \"Implement real-time churn prediction pipeline\"\n  ],\n  \"artifacts\": [\n    { \"filename\": \"feature_importance.png\", \"type\": \"figure\" },\n    { \"filename\": \"model.pkl\", \"type\": \"model\" }\n  ],\n  \"rawOutputs\": \"...(combined cell outputs for additional context)...\",\n  \"frontmatter\": { \"status\": \"completed\", \"tags\": [\"ml\", \"classification\"] }\n}\n```\n\n## Output Format\n\nWrite a markdown report with these sections (all narrative prose):\n\n### 1. Executive Summary (2-3 sentences)\nA concise overview of what was done, key findings, and significance.\n\n### 2. Introduction & Methodology\n- State the research objective naturally\n- Describe the approach taken\n- Mention any hypotheses being tested\n\n### 3. Results & Analysis\n- Present findings as a narrative, not bullet points\n- Integrate metrics naturally into the prose\n- Explain what the numbers mean\n- Connect findings to hypotheses\n\n### 4. Key Findings\n- Synthesize the most important discoveries\n- Explain their significance and implications\n- Use specific numbers where appropriate\n\n### 5. Limitations & Future Work\n- Acknowledge constraints honestly\n- Frame as opportunities for improvement\n- Suggest concrete next steps\n\n### 6. Conclusion\n- Summarize the research outcome\n- State whether objectives were achieved\n- End with actionable takeaways\n\n---\n\n## IMRAD Report Structure (MANDATORY)\n\nAll research reports MUST follow the IMRAD structure. This ensures scientific rigor and consistency across all Gyoshu outputs.\n\n| Section | Content | Required Markers |\n|---------|---------|------------------|\n| **Introduction** | Research question, context, motivation | `[OBJECTIVE]` |\n| **Methods** | Data description, tests used, assumptions checked | `[DATA]`, `[CHECK:*]`, `[DECISION]` |\n| **Results** | Effect sizes + CIs (verified findings only) | `[STAT:estimate]`, `[STAT:ci]`, `[STAT:effect_size]` |\n| **Analysis/Discussion** | Practical significance, limitations, interpretation | `[SO_WHAT]`, `[LIMITATION]` |\n| **Conclusion** | Answer to research question + recommendations | `[CONCLUSION]` |\n\n### Section Requirements\n\n**Introduction**: Must clearly state the research objective and any hypotheses being tested. Include context about why this question matters.\n\n**Methods**: Describe the data source, sample size, key variables, statistical tests chosen, and assumption checks performed. Reference `[DECISION]` markers that explain test selection rationale.\n\n**Results**: Present ONLY findings with full statistical evidence. Each finding requires:\n- Point estimate (`[STAT:estimate]`)\n- Confidence interval (`[STAT:ci]`)\n- Effect size with interpretation (`[STAT:effect_size]`)\n\n**Analysis/Discussion**: Interpret results in context. Explain practical significance using `[SO_WHAT]` markers. Acknowledge limitations using `[LIMITATION]` markers.\n\n**Conclusion**: Summarize whether hypotheses were supported/rejected and provide actionable recommendations.\n\n### Missing Section Handling\n\nIf any IMRAD section is missing required markers, insert a placeholder:\n\n```markdown\n### [SECTION MISSING: Methods]\n\n*This section requires [DECISION] markers explaining test selection and [CHECK:*] markers for assumption verification. The analysis did not include these elements.*\n```\n\n---\n\n## Finding Categorization Rules\n\nNot all findings are created equal. Categorize findings based on their trust score to ensure appropriate presentation in reports.\n\n| Category | Trust Score | Report Placement | Presentation |\n|----------|-------------|------------------|--------------|\n| **Verified Findings** | ‚â• 80 | Key Findings (main body) | Full confidence, lead with these |\n| **Partial Findings** | 60-79 | Findings (with caveats) | Include but note limitations |\n| **Exploratory Notes** | < 60 | Exploratory Observations (appendix) | Preliminary, needs further investigation |\n\n### How to Apply Categories\n\n**Verified Findings (trust ‚â• 80)**:\n- Include in the main \"Key Findings\" section\n- Present with full statistical evidence\n- Use confident language: \"The analysis demonstrates...\", \"Evidence strongly supports...\"\n\n**Partial Findings (trust 60-79)**:\n- Include in \"Findings\" section with explicit caveats\n- Note what's missing: \"While the data suggests X, the absence of Y limits confidence...\"\n- Use hedged language: \"Initial evidence suggests...\", \"The data indicates...\"\n\n**Exploratory Notes (trust < 60)**:\n- Move to \"Exploratory Observations\" section (separate from main findings)\n- Clearly label as preliminary\n- Use cautious language: \"Early observations hint at...\", \"Further investigation needed to confirm...\"\n\n---\n\n## Style Guidelines\n\n### DO:\n- Write in third person (\"The analysis revealed...\" not \"I found...\")\n- Use active voice when possible\n- Integrate numbers naturally (\"achieving 87% accuracy\" not \"accuracy: 0.87\")\n- Explain technical terms briefly if needed\n- Create logical flow between sections\n- Use transitions between paragraphs\n\n### DON'T:\n- Use bullet points in main narrative sections\n- Simply restate the raw marker data\n- Leave metrics unexplained\n- Write overly formal academic prose\n- Include code or raw outputs in the report\n- Use emojis or informal language\n\n## Workflow\n\n1. **Read** the context JSON provided\n2. **Analyze** the relationships between objectives, hypotheses, and findings\n3. **Synthesize** a coherent narrative that tells the research story\n4. **Write** the markdown report to `reports/{reportTitle}/report.md`\n5. **Confirm** the report was written successfully\n\n## Error Handling\n\nIf context is incomplete:\n- Note what's missing in the report\n- Work with available data\n- Add a \"Data Limitations\" note if key sections are empty\n\nIf writing fails:\n- Report the error clearly\n- Suggest manual steps to resolve\n",
        "agents/jogyo.md": "---\nname: jogyo\ndescription: Scientific research agent with Python REPL and structured output markers\nmodel: sonnet\n---\n\n# Jogyo Research Agent\n\nYou are a scientific research agent that executes Python code to investigate research questions.\n\n## Core Principles\n\n1. **Hypothesis-Driven**: Start with clear hypothesis or research question\n2. **Incremental Execution**: Run code in small chunks - never hallucinate results\n3. **Structured Output**: Use markers to categorize all output\n4. **Reproducibility**: Track parameters, seeds, and data sources\n\n## Output Markers\n\n### Research Process\n- `[OBJECTIVE]` - Research goal\n- `[HYPOTHESIS]` - Proposed explanation to test\n- `[DATA]` - Dataset description\n- `[FINDING]` - Key discovery (requires statistical evidence)\n- `[CONCLUSION]` - Final conclusions\n\n### Statistical Evidence (REQUIRED for findings)\n- `[STAT:ci]` - Confidence interval: `[STAT:ci] 95% CI [0.82, 0.94]`\n- `[STAT:effect_size]` - Effect magnitude: `[STAT:effect_size] Cohen's d = 0.75`\n- `[STAT:p_value]` - Significance: `[STAT:p_value] p = 0.003`\n- `[SO_WHAT]` - Practical significance\n- `[LIMITATION]` - Threats to validity\n\n### ML Pipeline\n- `[METRIC:baseline_*]` - Baseline benchmark (REQUIRED)\n- `[METRIC:cv_*_mean]` - Cross-validation mean (REQUIRED)\n- `[METRIC:cv_*_std]` - Cross-validation std\n- `[METRIC:feature_importance]` - Top features (REQUIRED)\n\n## Quality Gates\n\n**Every `[FINDING]` MUST have within 10 lines before it:**\n- `[STAT:ci]` - Confidence interval\n- `[STAT:effect_size]` - Effect magnitude\n\nFindings without these are marked as \"Exploratory\" in reports.\n\n## Python REPL Usage\n\n```python\npython-repl(\n  action=\"execute\",\n  researchSessionID=\"ses_xxx\",\n  notebookPath=\"./notebooks/research.ipynb\",\n  autoCapture=true,\n  code=\"import pandas as pd\\ndf = pd.read_csv('data.csv')\\nprint(f'[DATA] Loaded {len(df)} rows')\"\n)\n```\n\n## Stage Protocol\n\nWork in bounded stages (max 4 min each):\n```python\nprint(\"[STAGE:begin:id=S01_load_data]\")\n# ... stage work ...\nprint(\"[STAGE:end:id=S01_load_data:duration=120s]\")\n```\n\n## Completion\n\nWhen done, signal completion:\n```\ngyoshu-completion(\n  researchSessionID=\"ses_xxx\",\n  status=\"SUCCESS\",\n  summary=\"Completed analysis\",\n  evidence={findings: [...], metrics: {...}}\n)\n```\n\n## Example: Complete Analysis\n\n```python\n# 1. State objective\nprint(\"[OBJECTIVE] Analyze customer churn predictors\")\nprint(\"[HYPOTHESIS] H0: no difference; H1: tenure predicts churn\")\n\n# 2. Load data\nimport pandas as pd\ndf = pd.read_csv('churn.csv')\nprint(f\"[DATA] Loaded {len(df)} customers\")\n\n# 3. Analysis with statistics\nfrom scipy.stats import ttest_ind\nchurned = df[df['churn']==1]['tenure']\nretained = df[df['churn']==0]['tenure']\nt, p = ttest_ind(churned, retained)\n\n# 4. Calculate effect size\nimport numpy as np\npooled_std = np.sqrt((churned.var() + retained.var()) / 2)\ncohens_d = (retained.mean() - churned.mean()) / pooled_std\n\n# 5. Report with required markers\nmean_diff = retained.mean() - churned.mean()\nse = np.sqrt(churned.var()/len(churned) + retained.var()/len(retained))\nci_low, ci_high = mean_diff - 1.96*se, mean_diff + 1.96*se\n\nprint(f\"[STAT:ci] 95% CI [{ci_low:.2f}, {ci_high:.2f}]\")\nprint(f\"[STAT:effect_size] Cohen's d = {cohens_d:.2f}\")\nprint(f\"[STAT:p_value] p = {p:.4f}\")\nprint(f\"[FINDING] Retained customers have longer tenure (d={cohens_d:.2f}, p={p:.4f})\")\nprint(f\"[SO_WHAT] Each month of tenure reduces churn probability by ~2%\")\n```\n\nSee AGENTS.md for complete marker reference and examples.\n",
        "commands/gyoshu-auto.md": "---\ndescription: Start goal-based autonomous research with bounded execution\n---\n\nStart an AUTONOMOUS research session for the following goal:\n\n$ARGUMENTS\n\n---\n\n> **This is the standalone autonomous research command.** For interactive, step-by-step research, use `/gyoshu <goal>` instead. The `/gyoshu-auto` command runs to completion without user intervention (within budget limits).\n\n---\n\n## AUTO Mode Behavior\n\nThis command runs in **AUTO mode** - bounded autonomous execution that:\n1. Creates or continues a session targeting the specified goal\n2. Runs a bounded loop: delegate to @jogyo ‚Üí **verify with @baksa** ‚Üí check completion\n3. Continues until goal is COMPLETED, BLOCKED (needs user input), or budget exhausted\n\n## Adversarial Verification in AUTO Mode\n\n**CRITICAL: The AUTO loop includes mandatory verification via @baksa**\n\nAfter EVERY @jogyo completion, execute the challenge loop:\n\n### Challenge Loop (Max 3 Rounds)\n\n1. **Get Snapshot**: `gyoshu_snapshot(researchSessionID: \"...\")`\n2. **Invoke Critic**: `@baksa Challenge these claims with evidence: [from snapshot]`\n3. **Evaluate Trust Score**:\n   - **80-100 (VERIFIED)**: Accept result, continue to next cycle\n   - **60-79 (PARTIAL)**: Accept with caveats, note limitations\n   - **40-59 (DOUBTFUL)**: Send rework request to @jogyo\n   - **0-39 (REJECTED)**: Escalate to BLOCKED status\n\n4. **If Rework Needed**:\n   ```\n   @jogyo CHALLENGE FAILED - REWORK REQUIRED (Round N/3)\n\n   Failed Challenges:\n   - [List from @baksa]\n\n   Required: Address each challenge with evidence\n   ```\n\n5. **If Round 3 Fails**:\n   - Set goalStatus to BLOCKED\n   - Report to user with challenge history\n   - Do NOT continue autonomous execution\n\n### Budget Impact\n\nChallenge rounds count toward the cycle budget:\n- Each @baksa invocation = 0.5 cycle cost\n- Each @jogyo rework = 1 cycle cost\n- Plan accordingly: a single task may consume 3-4 cycles with verification\n\nThis ensures research quality through systematic skepticism - no claim passes without verification.\n\n### Stage Watchdog\n\nThe Stage Watchdog supervises Jogyo's execution of each stage, ensuring bounded execution and graceful recovery from stuck or runaway processes.\n\n#### Polling Behavior\n\nDuring stage execution, Gyoshu monitors progress every **5-10 seconds**:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                    Stage Execution                          ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                             ‚îÇ\n‚îÇ   t=0s     t=5s     t=10s    t=15s    t=20s    ...         ‚îÇ\n‚îÇ     ‚îÇ        ‚îÇ        ‚îÇ        ‚îÇ        ‚îÇ                   ‚îÇ\n‚îÇ     ‚ñº        ‚ñº        ‚ñº        ‚ñº        ‚ñº                   ‚îÇ\n‚îÇ   [poll]  [poll]  [poll]  [poll]  [poll]                   ‚îÇ\n‚îÇ                                                             ‚îÇ\n‚îÇ   Each poll checks:                                         ‚îÇ\n‚îÇ   ‚úì New cells executed since last poll?                     ‚îÇ\n‚îÇ   ‚úì New markers emitted ([STAGE:progress], [METRIC:*])?     ‚îÇ\n‚îÇ   ‚úì New artifacts created in output directory?              ‚îÇ\n‚îÇ   ‚úì Runtime within maxDuration limit?                       ‚îÇ\n‚îÇ                                                             ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Progress Signals:**\n| Signal | Indicates | Example |\n|--------|-----------|---------|\n| New cells | Execution is progressing | Cell count: 5 ‚Üí 6 |\n| New markers | Research milestones reached | `[STAGE:progress:pct=50]` |\n| New artifacts | Output being produced | `model.pkl` created |\n| REPL output | Active computation | stdout/stderr activity |\n\n**Stall Detection:**\nA stage is considered **stalled** if no progress signals are detected for 3 consecutive polls (15-30 seconds). Stall triggers a warning but not immediate intervention‚Äîsome operations (model training) may have long compute phases without output.\n\n#### Timeout Thresholds\n\nTimeouts are based on the stage's `maxDurationSec` from the stage envelope:\n\n| Threshold | Time | Action |\n|-----------|------|--------|\n| **Soft Timeout** | `maxDuration` | Warning logged, grace period begins |\n| **Hard Timeout** | `maxDuration + 30s` | Interrupt signal sent |\n| **Absolute Limit** | 600s (10 min) | Cannot exceed regardless of envelope |\n\n**Timeline Diagram:**\n\n```\nTime 0                   maxDuration              maxDuration+30s\n  ‚îÇ                           ‚îÇ                         ‚îÇ\n  ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ Normal Execution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ Grace Period ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ\n  ‚îÇ                           ‚îÇ                         ‚îÇ\n  ‚îÇ  [STAGE:begin]            ‚îÇ  [WARNING: soft        ‚îÇ  [SIGINT sent]\n  ‚îÇ                           ‚îÇ   timeout exceeded]     ‚îÇ\n  ‚îÇ  Regular polling          ‚îÇ                         ‚îÇ\n  ‚îÇ  every 5-10s              ‚îÇ  Polling intensifies   ‚îÇ  Escalation\n  ‚îÇ                           ‚îÇ  to every 2s           ‚îÇ  begins\n  ‚îÇ                           ‚îÇ                         ‚îÇ\n```\n\n#### Escalation Sequence\n\nWhen hard timeout is reached, the watchdog escalates through increasingly forceful termination signals:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                     WATCHDOG ESCALATION                             ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                                     ‚îÇ\n‚îÇ  t=maxDur          t=maxDur+5s      t=maxDur+8s      t=maxDur+10s  ‚îÇ\n‚îÇ      ‚îÇ                  ‚îÇ                ‚îÇ                ‚îÇ         ‚îÇ\n‚îÇ      ‚ñº                  ‚ñº                ‚ñº                ‚ñº         ‚îÇ\n‚îÇ   SIGINT            SIGTERM          SIGKILL         CHECKPOINT    ‚îÇ\n‚îÇ   (graceful)        (terminate)      (force kill)    (emergency)   ‚îÇ\n‚îÇ                                                                     ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ Request  ‚îÇ‚îÄ‚îÄ5s‚îÄ‚ñ∂‚îÇ  Force   ‚îÇ‚îÄ‚îÄ3s‚îÄ‚ñ∂‚îÇ  Kill    ‚îÇ‚îÄ2s‚ñ∂‚îÇ  Save     ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ cleanup  ‚îÇ      ‚îÇ terminate‚îÇ      ‚îÇ  process ‚îÇ    ‚îÇ  state    ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                                                                     ‚îÇ\n‚îÇ  Expected:          If SIGINT        Last resort     Always runs   ‚îÇ\n‚îÇ  Python handles     ignored or       for hung        to preserve   ‚îÇ\n‚îÇ  KeyboardInterrupt  too slow         native code     progress      ‚îÇ\n‚îÇ                                                                     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Signal Details:**\n\n| Step | Signal | Wait | Expected Response |\n|------|--------|------|-------------------|\n| 1 | `SIGINT` | 5s | Python raises `KeyboardInterrupt`, cleanup handlers run |\n| 2 | `SIGTERM` | 3s | Process terminates, may skip cleanup |\n| 3 | `SIGKILL` | 2s | Kernel terminates process immediately |\n| 4 | Checkpoint | - | Emergency checkpoint saved regardless of process state |\n\n**Why This Sequence:**\n- `SIGINT` allows graceful shutdown (flush buffers, close files, save partial results)\n- `SIGTERM` gives the OS a chance to clean up resources\n- `SIGKILL` is the nuclear option for truly stuck processes (e.g., native code infinite loops)\n- Emergency checkpoint preserves whatever state was achieved\n\n#### Emergency Checkpoint\n\nWhen the watchdog aborts a stage, it **always** triggers an emergency checkpoint to preserve progress:\n\n```python\n# Watchdog triggers emergency checkpoint\ncheckpoint-manager(\n    action=\"save\",\n    reportTitle=\"...\",\n    runId=\"...\",\n    checkpointId=\"ckpt-emergency-{timestamp}\",\n    stageId=\"{current_stage_id}\",\n    status=\"emergency\",           # Marks this as watchdog-triggered\n    reason=\"timeout\",             # Reason: timeout | abort | error\n    # Note: artifacts may be incomplete - validation skipped for speed\n)\n```\n\n**Emergency vs Normal Checkpoints:**\n\n| Aspect | Normal Checkpoint | Emergency Checkpoint |\n|--------|-------------------|----------------------|\n| Timing | After stage completes | During/after abort |\n| Artifacts | Fully validated (SHA256) | Best-effort (may be incomplete) |\n| Status | `saved` | `emergency` |\n| Resume | Full rehydration | May need manual review |\n| Speed | ~1-2s | <500ms (no validation) |\n\n**After Emergency Checkpoint:**\n1. Stage marked as `INTERRUPTED` in run state\n2. User notified with abort reason and checkpoint location\n3. Cycle increments, but stage flagged for review\n4. On `/gyoshu continue`, user can choose to:\n   - Resume from emergency checkpoint (review artifacts first)\n   - Restart stage from previous checkpoint\n   - Abort research entirely\n\n**Marker Emitted:**\n```\n[CHECKPOINT:emergency:id=ckpt-emergency-1704189600:stage=S03_train_model:reason=timeout]\n```\n\n## Adaptive Budgets\n\nGyoshu uses **adaptive budgets** that adjust based on task complexity and runtime progress. There are no fixed defaults - budgets are computed dynamically.\n\n### Budget Computation\n\n1. **Initial Estimation**: Gyoshu analyzes the goal to estimate complexity (L0-L4)\n2. **Runtime Adaptation**: Budgets adjust based on:\n   - Progress signals (new findings, artifacts, trust scores)\n   - Stall detection (no progress ‚Üí reduce scope)\n   - Breakthroughs (high trust + discoveries ‚Üí extend if beneficial)\n3. **Pool + Reserve Model**:\n   - Fast stages \"donate\" unused time to later stages\n   - 15% reserve kept for pivots/recovery\n   - Hard caps never exceeded without user approval\n\n### Hard Caps (Non-Negotiable)\n\n| Parameter | Hard Cap | Requires User Approval to Exceed |\n|-----------|----------|----------------------------------|\n| maxCycles | 25 | Yes |\n| maxToolCalls | 300 | Yes |\n| maxTimeMinutes | 180 | Yes |\n\n### Adaptation Triggers\n\n| Signal | Response |\n|--------|----------|\n| Stall (no progress 60s+) | Simplify/split stage, reduce scope |\n| Low trust (<60) twice | Reframe stage, after 3 fails ‚Üí BLOCKED |\n| High trust (‚â•90) + discovery | Small extension (+10-20%) within caps |\n| Budget 80%+ consumed | Surface options to user |\n\n## Stopping Conditions\n\nThe autonomous loop stops when any of these occur:\n- **COMPLETED**: Research goal achieved, findings documented\n- **BLOCKED**: Requires user decision, data access, or clarification\n- **BUDGET_EXHAUSTED**: Any budget limit reached\n\n## Auto-Decision Behavior\n\n**CRITICAL**: AUTO mode does NOT ask the user for decisions mid-run.\n\nInstead, the Two-Gate decision engine automatically determines the next action:\n\n| Decision | Trigger | Action |\n|----------|---------|--------|\n| **CONTINUE** | Progress being made | Continue to next cycle |\n| **PIVOT** | Goal not met | Try different approach (max 3 attempts) |\n| **REWORK** | Trust score too low | Send back to @jogyo (max 3 rounds) |\n| **COMPLETE** | Both gates pass | Generate report and exit |\n| **BLOCKED** | Cannot proceed | Report to user and exit |\n\nThis means the autonomous loop runs uninterrupted until reaching a terminal state. The decision engine evaluates:\n1. **Trust Gate**: Is the evidence quality sufficient? (verified by @baksa)\n2. **Goal Gate**: Are the acceptance criteria met? (checked against goal contract)\n\nOnly when both gates pass does the loop complete successfully.\n\n## Stagnation Detection\n\nIf no progress is detected for **3 consecutive cycles**, the loop automatically transitions to BLOCKED status with a stagnation reason.\n\n**Progress signals monitored:**\n- New cells executed\n- New artifacts created (figures, models, exports)\n- New markers emitted (`[FINDING]`, `[METRIC:*]`, etc.)\n- Trust score changes\n\n**Stagnation triggers BLOCKED status because:**\n- Infinite loops waste budget without value\n- Stuck research needs human intervention\n- Early exit preserves resources for retry\n\nWhen stagnation is detected:\n```\n[STAGNATION:detected:cycles=3:reason=\"No new cells, artifacts, or markers\"]\n```\n\n## Exit Condition Tags\n\nThe AUTO loop emits these promise tags when reaching terminal states:\n\n```\n<promise>GYOSHU_AUTO_COMPLETE</promise>     # Goal achieved, report generated\n<promise>GYOSHU_AUTO_BLOCKED</promise>       # Cannot proceed, user intervention needed\n<promise>GYOSHU_AUTO_BUDGET_EXHAUSTED</promise>  # Cycles/time/tools exhausted\n```\n\n**Why promise tags?**\n- Enable orchestrators to detect terminal states programmatically\n- Provide clear, parseable exit signals\n- Support integration with external automation systems\n\n**Tag semantics:**\n\n| Tag | Meaning | User Action |\n|-----|---------|-------------|\n| `GYOSHU_AUTO_COMPLETE` | Research succeeded, report ready | Review report |\n| `GYOSHU_AUTO_BLOCKED` | Cannot continue autonomously | Provide input/decision |\n| `GYOSHU_AUTO_BUDGET_EXHAUSTED` | Resource limits reached | Extend budget or accept partial |\n\n## Example Usage\n\n```\n/gyoshu-auto analyze the iris dataset and identify clustering patterns\n/gyoshu-auto investigate correlation between features X and Y in sales data\n/gyoshu-auto reproduce the analysis from paper.pdf and validate findings\n```\n\nThe planner will autonomously coordinate research execution, handle errors, and produce a final report.\n",
        "commands/gyoshu.md": "---\ndescription: Unified Gyoshu research command - start, continue, search, and manage research\nagent: gyoshu\n---\n\n# /gyoshu - Unified Research Command\n\n$ARGUMENTS\n\n---\n\n## Command Routing\n\nParse first token and route:\n\n| First Token | Action |\n|-------------|--------|\n| (empty) | Show status |\n| `help` | Show help |\n| `doctor` | System health check |\n| `plan <goal>` | Create plan only |\n| `continue [id]` | Resume research |\n| `repl <query>` | Direct REPL |\n| `list` | List projects |\n| `search <query>` | Search notebooks |\n| `report [id]` | Generate report |\n| `migrate` | Migrate data |\n| `unlock <id>` | Unlock session |\n| `abort` | Abort research |\n| `<goal>` | Start new research |\n\n## Status Display (no args)\n\nCheck for active research and show suggestions:\n\n```\nresearch-manager(action=\"list\", status=\"active\")\n```\n\nOutput format:\n```\nGYOSHU STATUS\n\nActive Research: [title] (status)\nLast Activity: [time]\n\nSuggestions:\n- /gyoshu continue - Resume active research\n- /gyoshu <goal> - Start new research\n```\n\n## Help\n\nShow command reference:\n```\n/gyoshu                    Show status\n/gyoshu <goal>             Start research\n/gyoshu plan <goal>        Create plan only\n/gyoshu continue [id]      Resume research\n/gyoshu list               List projects\n/gyoshu search <query>     Search notebooks\n/gyoshu report [id]        Generate report\n/gyoshu doctor             Check health\n```\n\n## Doctor\n\nRun diagnostics:\n1. Check Python environment (.venv)\n2. Check runtime directory\n3. List active sessions\n4. Report any issues\n\n## Start Research\n\nWhen user provides a goal:\n\n1. Search for similar prior research:\n   ```\n   research-manager(action=\"search\", query=\"<goal keywords>\")\n   ```\n\n2. Create research session:\n   ```\n   research-manager(action=\"create\", title=\"<title>\", goal=\"<goal>\")\n   ```\n\n3. Delegate to @jogyo via Task tool\n\n4. Verify results with @baksa\n\n5. On completion, generate report\n\n## Continue Research\n\nResume existing research:\n```\nresearch-manager(action=\"get\", reportTitle=\"<id>\")\ngyoshu-snapshot(researchSessionID=\"<sessionId>\")\n```\n\nThen continue from last checkpoint.\n\n## List/Search\n\nList: `research-manager(action=\"list\")`\nSearch: `research-manager(action=\"search\", query=\"<query>\")`\n\n## Report\n\nGenerate report for completed research:\n```\nresearch-manager(action=\"report\", reportTitle=\"<id>\")\n```\n\nSee AGENTS.md for complete workflow documentation.\n",
        "examples/README.md": "# Gyoshu Examples\n\nThis directory contains worked examples demonstrating Gyoshu's research automation capabilities.\n\n## Available Examples\n\n| Example | Type | Data Source | Key Features |\n|---------|------|-------------|--------------|\n| [Binance Futures EDA](binance-futures-eda.png) | Exploratory Data Analysis | Binance API | 3D visualizations, correlation analysis, volatility surfaces |\n\n## Running Examples\n\n### Prerequisites\n\n1. **Install Gyoshu** following the [main README](../README.md)\n2. **Set up Python environment**:\n   ```bash\n   python3 -m venv .venv\n   source .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\n   pip install pandas numpy matplotlib seaborn scikit-learn\n   ```\n3. **Configure data sources** (if needed):\n   - Kaggle: Set up `~/.kaggle/kaggle.json` with API credentials\n\n### Quick Start\n\n```bash\n# Navigate to a new directory\nmkdir my-research && cd my-research\n\n# Set up environment\npython3 -m venv .venv\nsource .venv/bin/activate\npip install pandas numpy matplotlib seaborn\n\n# Run any of these examples:\n\n# 1. Binance Futures EDA (API or local data)\n/gyoshu-auto perform comprehensive EDA on binance futures data\n\n# 2. Titanic Classification (sklearn built-in)\n/gyoshu-auto analyze Titanic survival data and build classification model\n\n# 3. Iris Clustering (no download needed)\n/gyoshu-auto cluster iris dataset and visualize results\n```\n\n## Example Structure\n\nEach example directory contains:\n\n```\nXX-example-name/\n‚îú‚îÄ‚îÄ README.md       # What the example shows, how to run it\n‚îú‚îÄ‚îÄ prompt.md       # Exact /gyoshu-auto command used\n‚îú‚îÄ‚îÄ notebook.ipynb  # Generated Jupyter notebook (cleaned)\n‚îî‚îÄ‚îÄ figures/        # Key output visualizations\n```\n\n> **Note**: Raw data files are NOT included due to size. Each example documents how to obtain the data.\n\n## Contributing Examples\n\nWant to add your own example? Follow this structure:\n\n1. Create a new directory: `examples/NN-descriptive-name/`\n2. Include:\n   - `README.md` - What it demonstrates, prerequisites, how to run\n   - `prompt.md` - The exact prompt used\n   - `notebook.ipynb` - Cleaned notebook (remove large outputs)\n   - `figures/` - 3-5 representative figures\n3. Update this README with a new row in the table\n\n### Cleaning Notebooks\n\nTo reduce notebook size for git:\n\n```python\nimport json\n\n# Load notebook\nwith open('notebook.ipynb') as f:\n    nb = json.load(f)\n\n# Remove large outputs\nfor cell in nb['cells']:\n    if cell['cell_type'] == 'code':\n        for output in cell.get('outputs', []):\n            if 'data' in output and 'image/png' in output['data']:\n                output['data'] = {'text/plain': ['[Image - see figures/]']}\n\n# Save cleaned\nwith open('notebook_cleaned.ipynb', 'w') as f:\n    json.dump(nb, f, indent=1)\n```\n\n## Suggested Future Examples\n\n| Example | Type | Dataset | What It Would Show |\n|---------|------|---------|-------------------|\n| Titanic Survival | Classification | Kaggle | Feature engineering, model comparison |\n| House Prices | Regression | Kaggle | Advanced feature engineering |\n| MNIST Digits | Deep Learning | torchvision | Neural network training |\n| Stock Prediction | Time Series | Yahoo Finance | Time-aware analysis |\n| Sentiment Analysis | NLP | Movie reviews | Text processing pipeline |\n",
        "skills/data-analysis/SKILL.md": "---\nname: data-analysis\ndescription: Patterns for data loading, exploration, and statistical analysis\n---\n\n# Data Analysis Patterns\n\n## When to Use\nLoad this skill when working with datasets that require exploration, cleaning, and statistical analysis.\n\n## Data Loading\n```python\nprint(\"[DATA] Loading dataset\")\ndf = pd.read_csv(\"data.csv\")\nprint(f\"[SHAPE] {df.shape[0]} rows, {df.shape[1]} columns\")\nprint(f\"[DTYPE] {dict(df.dtypes)}\")\nprint(f\"[MISSING] {df.isnull().sum().to_dict()}\")\n```\n\n## Exploratory Data Analysis (EDA)\n\n### Descriptive Statistics\n```python\nprint(\"[STAT] Descriptive statistics:\")\nprint(df.describe())\n\nprint(f\"[RANGE] {col}: {df[col].min()} to {df[col].max()}\")\n```\n\n### Distribution Analysis\n```python\nprint(\"[ANALYSIS] Checking distribution normality\")\nfrom scipy import stats\nstat, p_value = stats.shapiro(df[col])\nprint(f\"[STAT] Shapiro-Wilk p-value: {p_value:.4f}\")\n```\n\n### Correlation Analysis\n```python\nprint(\"[CORR] Correlation matrix:\")\nprint(df.corr())\n```\n\n## Statistical Tests\n\n### T-Test\n```python\nfrom scipy.stats import ttest_ind\nstat, p = ttest_ind(group1, group2)\nprint(f\"[STAT] T-test: t={stat:.3f}, p={p:.4f}\")\n```\n\n### ANOVA\n```python\nfrom scipy.stats import f_oneway\nstat, p = f_oneway(group1, group2, group3)\nprint(f\"[STAT] ANOVA: F={stat:.3f}, p={p:.4f}\")\n```\n\n## Confidence Interval Patterns\n\n### Parametric CI for Means\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef mean_ci(data, confidence=0.95):\n    \"\"\"Calculate parametric confidence interval for mean.\"\"\"\n    n = len(data)\n    mean = np.mean(data)\n    se = stats.sem(data)  # Standard error of mean\n    h = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n    return mean, mean - h, mean + h\n\nmean, ci_low, ci_high = mean_ci(df[col])\nprint(f\"[STAT:estimate] mean = {mean:.3f}\")\nprint(f\"[STAT:ci] 95% CI [{ci_low:.3f}, {ci_high:.3f}]\")\n```\n\n### Bootstrap CI for Medians/Complex Statistics\n```python\nimport numpy as np\n\ndef bootstrap_ci(data, stat_func=np.median, n_bootstrap=10000, confidence=0.95):\n    \"\"\"Calculate bootstrap confidence interval for any statistic.\"\"\"\n    boot_stats = []\n    n = len(data)\n    for _ in range(n_bootstrap):\n        sample = np.random.choice(data, size=n, replace=True)\n        boot_stats.append(stat_func(sample))\n\n    alpha = 1 - confidence\n    ci_low = np.percentile(boot_stats, 100 * alpha / 2)\n    ci_high = np.percentile(boot_stats, 100 * (1 - alpha / 2))\n    return stat_func(data), ci_low, ci_high\n\nmedian, ci_low, ci_high = bootstrap_ci(df[col], stat_func=np.median)\nprint(f\"[STAT:estimate] median = {median:.3f}\")\nprint(f\"[STAT:ci] 95% Bootstrap CI [{ci_low:.3f}, {ci_high:.3f}]\")\n```\n\n### Wilson CI for Proportions\n```python\nfrom scipy import stats\n\ndef wilson_ci(successes, trials, confidence=0.95):\n    \"\"\"Calculate Wilson score interval for proportions (better for small n).\"\"\"\n    p = successes / trials\n    z = stats.norm.ppf((1 + confidence) / 2)\n\n    denominator = 1 + z**2 / trials\n    center = (p + z**2 / (2 * trials)) / denominator\n    spread = z * np.sqrt((p * (1 - p) + z**2 / (4 * trials)) / trials) / denominator\n\n    return p, center - spread, center + spread\n\nprop, ci_low, ci_high = wilson_ci(successes=45, trials=100)\nprint(f\"[STAT:estimate] proportion = {prop:.3f}\")\nprint(f\"[STAT:ci] 95% Wilson CI [{ci_low:.3f}, {ci_high:.3f}]\")\n```\n\n## Effect Size Calculation\n\n### Cohen's d for Group Comparisons\n```python\nimport numpy as np\n\ndef cohens_d(group1, group2):\n    \"\"\"Calculate Cohen's d effect size for two independent groups.\"\"\"\n    n1, n2 = len(group1), len(group2)\n    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n\n    # Pooled standard deviation\n    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n    d = (np.mean(group1) - np.mean(group2)) / pooled_std\n\n    # Interpretation\n    magnitude = \"small\" if abs(d) < 0.5 else \"medium\" if abs(d) < 0.8 else \"large\"\n    return d, magnitude\n\nd, magnitude = cohens_d(treatment, control)\nprint(f\"[STAT:effect_size] Cohen's d = {d:.3f} ({magnitude})\")\n```\n\n### r¬≤ for Correlations\n```python\nfrom scipy import stats\n\ndef correlation_r2(x, y):\n    \"\"\"Calculate Pearson r and r¬≤ with interpretation.\"\"\"\n    r, p = stats.pearsonr(x, y)\n    r2 = r ** 2\n\n    # Interpretation (based on Cohen's guidelines for r)\n    magnitude = \"small\" if abs(r) < 0.3 else \"medium\" if abs(r) < 0.5 else \"large\"\n    return r, r2, p, magnitude\n\nr, r2, p, magnitude = correlation_r2(df[x_col], df[y_col])\nprint(f\"[STAT:estimate] r = {r:.3f}\")\nprint(f\"[STAT:effect_size] r¬≤ = {r2:.3f} ({magnitude} effect, {r2*100:.1f}% variance explained)\")\nprint(f\"[STAT:p_value] p = {p:.4f}\")\n```\n\n### Cliff's Delta for Non-Parametric Comparisons\n```python\nimport numpy as np\n\ndef cliffs_delta(group1, group2):\n    \"\"\"Calculate Cliff's delta (non-parametric effect size).\"\"\"\n    n1, n2 = len(group1), len(group2)\n\n    # Count dominance\n    more = sum(1 for x in group1 for y in group2 if x > y)\n    less = sum(1 for x in group1 for y in group2 if x < y)\n    delta = (more - less) / (n1 * n2)\n\n    # Interpretation (Romano et al., 2006)\n    abs_d = abs(delta)\n    magnitude = \"negligible\" if abs_d < 0.147 else \"small\" if abs_d < 0.33 else \"medium\" if abs_d < 0.474 else \"large\"\n    return delta, magnitude\n\ndelta, magnitude = cliffs_delta(treatment, control)\nprint(f\"[STAT:effect_size] Cliff's delta = {delta:.3f} ({magnitude})\")\n```\n\n## Assumption Checking\n\n### Normality: Shapiro-Wilk and Q-Q Plot\n```python\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef check_normality(data, col_name=\"variable\", alpha=0.05):\n    \"\"\"Check normality assumption with Shapiro-Wilk test and Q-Q plot.\"\"\"\n    # Shapiro-Wilk test (best for n < 5000)\n    stat, p = stats.shapiro(data)\n    is_normal = p > alpha\n\n    print(f\"[CHECK:normality] Shapiro-Wilk W={stat:.4f}, p={p:.4f}\")\n    print(f\"[CHECK:normality] {'PASS' if is_normal else 'FAIL'}: Data {'is' if is_normal else 'is NOT'} normally distributed (Œ±={alpha})\")\n\n    # Q-Q plot for visual inspection\n    fig, ax = plt.subplots(figsize=(6, 6))\n    stats.probplot(data, dist=\"norm\", plot=ax)\n    ax.set_title(f\"Q-Q Plot: {col_name}\")\n    plt.savefig(f\"reports/figures/qq_plot_{col_name}.png\", dpi=150, bbox_inches=\"tight\")\n    plt.close()\n\n    return is_normal, stat, p\n\nis_normal, stat, p = check_normality(df[col], col_name=col)\n```\n\n### Homogeneity of Variance: Levene's Test\n```python\nfrom scipy import stats\n\ndef check_homogeneity(*groups, alpha=0.05):\n    \"\"\"Check homogeneity of variance (homoscedasticity) with Levene's test.\"\"\"\n    stat, p = stats.levene(*groups)\n    is_homogeneous = p > alpha\n\n    print(f\"[CHECK:homogeneity] Levene's W={stat:.4f}, p={p:.4f}\")\n    print(f\"[CHECK:homogeneity] {'PASS' if is_homogeneous else 'FAIL'}: Variances {'are' if is_homogeneous else 'are NOT'} equal (Œ±={alpha})\")\n\n    if not is_homogeneous:\n        print(\"[CHECK:homogeneity] Recommendation: Use Welch's t-test instead of Student's t-test\")\n\n    return is_homogeneous, stat, p\n\nis_homogeneous, stat, p = check_homogeneity(group1, group2)\n```\n\n### Independence: Durbin-Watson Test (for Regression Residuals)\n```python\nfrom statsmodels.stats.stattools import durbin_watson\n\ndef check_independence(residuals):\n    \"\"\"Check independence of residuals with Durbin-Watson test.\"\"\"\n    dw_stat = durbin_watson(residuals)\n\n    # Interpretation: DW ‚âà 2 means no autocorrelation\n    # DW < 1.5 suggests positive autocorrelation\n    # DW > 2.5 suggests negative autocorrelation\n    if dw_stat < 1.5:\n        status = \"FAIL - positive autocorrelation detected\"\n    elif dw_stat > 2.5:\n        status = \"FAIL - negative autocorrelation detected\"\n    else:\n        status = \"PASS - no significant autocorrelation\"\n\n    print(f\"[CHECK:independence] Durbin-Watson = {dw_stat:.3f}\")\n    print(f\"[CHECK:independence] {status}\")\n\n    return dw_stat, status\n\ndw_stat, status = check_independence(model.resid)\n```\n\n## Robust Alternatives\n\n### Welch's t-test (Instead of Student's t-test)\n```python\nfrom scipy import stats\n\ndef welchs_ttest(group1, group2, alpha=0.05):\n    \"\"\"\n    Welch's t-test - DEFAULT choice for comparing two groups.\n    Does NOT assume equal variances (more robust than Student's t-test).\n    \"\"\"\n    stat, p = stats.ttest_ind(group1, group2, equal_var=False)  # equal_var=False for Welch's\n\n    print(f\"[DECISION] Using Welch's t-test: Does not assume equal variances\")\n    print(f\"[STAT:estimate] t-statistic = {stat:.3f}\")\n    print(f\"[STAT:p_value] p = {p:.4f}\")\n\n    # Effect size\n    from numpy import sqrt, var, mean\n    n1, n2 = len(group1), len(group2)\n    pooled_std = sqrt(((n1-1)*var(group1, ddof=1) + (n2-1)*var(group2, ddof=1)) / (n1+n2-2))\n    d = (mean(group1) - mean(group2)) / pooled_std\n    magnitude = \"small\" if abs(d) < 0.5 else \"medium\" if abs(d) < 0.8 else \"large\"\n    print(f\"[STAT:effect_size] Cohen's d = {d:.3f} ({magnitude})\")\n\n    return stat, p, d\n\nt_stat, p_value, effect_size = welchs_ttest(treatment, control)\n```\n\n### Mann-Whitney U Test (for Non-Normal Data)\n```python\nfrom scipy import stats\nimport numpy as np\n\ndef mann_whitney_test(group1, group2, alpha=0.05):\n    \"\"\"\n    Mann-Whitney U test - Non-parametric alternative to t-test.\n    Use when normality assumption is violated.\n    \"\"\"\n    stat, p = stats.mannwhitneyu(group1, group2, alternative='two-sided')\n\n    print(f\"[DECISION] Using Mann-Whitney U: Non-parametric, does not assume normality\")\n    print(f\"[STAT:estimate] U-statistic = {stat:.3f}\")\n    print(f\"[STAT:p_value] p = {p:.4f}\")\n\n    # Effect size: Cliff's delta (appropriate for non-parametric)\n    n1, n2 = len(group1), len(group2)\n    more = sum(1 for x in group1 for y in group2 if x > y)\n    less = sum(1 for x in group1 for y in group2 if x < y)\n    delta = (more - less) / (n1 * n2)\n    magnitude = \"negligible\" if abs(delta) < 0.147 else \"small\" if abs(delta) < 0.33 else \"medium\" if abs(delta) < 0.474 else \"large\"\n    print(f\"[STAT:effect_size] Cliff's delta = {delta:.3f} ({magnitude})\")\n\n    return stat, p, delta\n\nu_stat, p_value, effect_size = mann_whitney_test(treatment, control)\n```\n\n### Permutation Test (for Complex Designs)\n```python\nimport numpy as np\n\ndef permutation_test(group1, group2, n_permutations=10000, stat_func=None):\n    \"\"\"\n    Permutation test - Most robust, makes minimal assumptions.\n    Use when parametric assumptions are violated or for complex statistics.\n    \"\"\"\n    if stat_func is None:\n        stat_func = lambda x, y: np.mean(x) - np.mean(y)\n\n    observed = stat_func(group1, group2)\n    combined = np.concatenate([group1, group2])\n    n1 = len(group1)\n\n    # Generate permutation distribution\n    perm_stats = []\n    for _ in range(n_permutations):\n        np.random.shuffle(combined)\n        perm_stat = stat_func(combined[:n1], combined[n1:])\n        perm_stats.append(perm_stat)\n\n    # Two-tailed p-value\n    p_value = np.mean(np.abs(perm_stats) >= np.abs(observed))\n\n    print(f\"[DECISION] Using permutation test: Assumption-free, {n_permutations} permutations\")\n    print(f\"[STAT:estimate] Observed difference = {observed:.4f}\")\n    print(f\"[STAT:p_value] p = {p_value:.4f} (permutation-based)\")\n\n    # Bootstrap CI for the observed statistic\n    boot_diffs = []\n    for _ in range(n_permutations):\n        b1 = np.random.choice(group1, size=len(group1), replace=True)\n        b2 = np.random.choice(group2, size=len(group2), replace=True)\n        boot_diffs.append(stat_func(b1, b2))\n    ci_low, ci_high = np.percentile(boot_diffs, [2.5, 97.5])\n    print(f\"[STAT:ci] 95% Bootstrap CI [{ci_low:.4f}, {ci_high:.4f}]\")\n\n    return observed, p_value, ci_low, ci_high\n\nobs, p_val, ci_low, ci_high = permutation_test(treatment, control)\n```\n\n## Memory Management\n```python\nprint(f\"[MEMORY] DataFrame size: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n# Clean up\ndel large_df\nimport gc; gc.collect()\n```\n",
        "skills/experiment-design/SKILL.md": "---\nname: experiment-design\ndescription: Best practices for designing reproducible experiments\n---\n\n# Experiment Design Patterns\n\n## When to Use\nLoad this skill when designing experiments that need to be reproducible and statistically valid.\n\n## Reproducibility Setup\n\n### Random Seeds\n```python\nimport random\nimport numpy as np\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nprint(f\"[DECISION] Using random seed: {SEED}\")\n```\n\n### Environment Recording\n```python\nimport sys\nprint(f\"[INFO] Python: {sys.version}\")\nprint(f\"[INFO] NumPy: {np.__version__}\")\nprint(f\"[INFO] Pandas: {pd.__version__}\")\n```\n\n## Experimental Controls\n\n### Train/Test Split\n```python\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=SEED, stratify=y\n)\nprint(f\"[EXPERIMENT] Train: {len(X_train)}, Test: {len(X_test)}\")\n```\n\n### Cross-Validation\n```python\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(f\"[METRIC] CV Accuracy: {scores.mean():.3f} (+/- {scores.std()*2:.3f})\")\n```\n\n## A/B Testing Pattern\n```python\nprint(\"[EXPERIMENT] A/B Test Design\")\nprint(f\"[INFO] Control group: {len(control)}\")\nprint(f\"[INFO] Treatment group: {len(treatment)}\")\n\n# Power analysis\nfrom statsmodels.stats.power import TTestIndPower\npower = TTestIndPower()\nsample_size = power.solve_power(effect_size=0.5, alpha=0.05, power=0.8)\nprint(f\"[CALC] Required sample size per group: {sample_size:.0f}\")\n```\n\n## Power Analysis\n\nPower analysis ensures your experiment has sufficient sample size to detect meaningful effects. Without adequate power, you risk false negatives (missing real effects).\n\n### Sample Size Calculation\n\n```python\nfrom statsmodels.stats.power import TTestIndPower, FTestAnovaPower, NormalIndPower\nimport numpy as np\n\nprint(\"[DECISION] Conducting a priori power analysis before data collection\")\n\n# For two-group comparison (t-test)\npower_analysis = TTestIndPower()\n\n# Parameters:\n# - effect_size: Expected Cohen's d (0.2=small, 0.5=medium, 0.8=large)\n# - alpha: Significance level (typically 0.05)\n# - power: Desired statistical power (typically 0.80 or 0.90)\n# - ratio: Ratio of group sizes (1.0 = equal groups)\n\neffect_size = 0.5  # Medium effect size\nalpha = 0.05\ndesired_power = 0.80\n\nsample_size = power_analysis.solve_power(\n    effect_size=effect_size,\n    alpha=alpha,\n    power=desired_power,\n    ratio=1.0,\n    alternative='two-sided'\n)\n\nprint(f\"[STAT:estimate] Required n per group: {np.ceil(sample_size):.0f}\")\nprint(f\"[STAT:estimate] Total sample needed: {np.ceil(sample_size)*2:.0f}\")\nprint(f\"[DECISION] Targeting {effect_size} effect size (Cohen's d = medium)\")\n```\n\n### Achieved Power (Post-Hoc)\n\n```python\n# After data collection, calculate achieved power\nactual_n = 50  # Actual sample size per group\nachieved_power = power_analysis.solve_power(\n    effect_size=effect_size,\n    alpha=alpha,\n    nobs1=actual_n,\n    ratio=1.0,\n    alternative='two-sided'\n)\n\nprint(f\"[STAT:estimate] Achieved power: {achieved_power:.3f}\")\n\nif achieved_power < 0.80:\n    print(f\"[LIMITATION] Study is underpowered ({achieved_power:.0%} < 80%)\")\n    print(\"[LIMITATION] Negative results may be due to insufficient sample size\")\nelse:\n    print(f\"[DECISION] Adequate power achieved ({achieved_power:.0%} ‚â• 80%)\")\n```\n\n### Power for Different Tests\n\n```python\n# For ANOVA (multiple groups)\nfrom statsmodels.stats.power import FTestAnovaPower\nanova_power = FTestAnovaPower()\nn_groups = 3\neffect_size_f = 0.25  # Cohen's f (0.1=small, 0.25=medium, 0.4=large)\nn_per_group = anova_power.solve_power(\n    effect_size=effect_size_f,\n    alpha=0.05,\n    power=0.80,\n    k_groups=n_groups\n)\nprint(f\"[STAT:estimate] ANOVA: {np.ceil(n_per_group):.0f} per group needed\")\n\n# For proportions (chi-square, A/B tests)\nfrom statsmodels.stats.power import GofChisquarePower\nfrom statsmodels.stats.proportion import proportion_effectsize\n# Convert expected proportions to effect size\np1, p2 = 0.10, 0.15  # e.g., 10% baseline, 15% expected with treatment\nprop_effect = proportion_effectsize(p1, p2)\nprint(f\"[DECISION] Effect size h = {prop_effect:.3f} for proportions test\")\n```\n\n## Pre-registration Concept\n\nPre-registration prevents HARKing (Hypothesizing After Results are Known) and distinguishes confirmatory from exploratory analyses.\n\n### Define Analysis Before Data\n\n```python\nprint(\"[DECISION] Pre-registering analysis plan before examining data\")\n\n# Document your analysis plan BEFORE looking at the data\npreregistration = {\n    \"primary_hypothesis\": \"H1: Treatment group shows higher conversion rate than control\",\n    \"null_hypothesis\": \"H0: No difference in conversion rates between groups\",\n    \"primary_endpoint\": \"conversion_rate\",\n    \"secondary_endpoints\": [\"time_to_convert\", \"revenue_per_user\"],\n    \"alpha\": 0.05,\n    \"correction_method\": \"Bonferroni for secondary endpoints\",\n    \"minimum_effect_size\": \"5 percentage points (10% ‚Üí 15%)\",\n    \"planned_sample_size\": 500,\n    \"analysis_method\": \"Two-proportion z-test\",\n    \"exclusion_criteria\": \"Users with < 1 day exposure\"\n}\n\nprint(f\"[EXPERIMENT] Pre-registered analysis plan:\")\nfor key, value in preregistration.items():\n    print(f\"  {key}: {value}\")\n```\n\n### Confirmatory vs Exploratory Findings\n\n```python\n# After analysis, clearly label findings\nprint(\"[FINDING] Treatment increases conversion by 4.2pp (95% CI: [1.8, 6.6])\")\nprint(\"[STAT:ci] 95% CI [1.8, 6.6]\")\nprint(\"[STAT:effect_size] Cohen's h = 0.12 (small)\")\nprint(\"[STAT:p_value] p = 0.001\")\n# This is CONFIRMATORY because it tests pre-registered hypothesis\n\nprint(\"[OBSERVATION] Exploratory: Effect stronger for mobile users (+7.1pp)\")\nprint(\"[LIMITATION] Mobile subgroup analysis was NOT pre-registered\")\nprint(\"[DECISION] Flagging as exploratory - requires replication before action\")\n\n# Document finding type\nCONFIRMATORY = True  # Pre-registered hypothesis\nEXPLORATORY = False  # Post-hoc discovery\n\ndef label_finding(finding: str, is_confirmatory: bool):\n    \"\"\"Label findings appropriately based on pre-registration status.\"\"\"\n    if is_confirmatory:\n        print(f\"[FINDING] CONFIRMATORY: {finding}\")\n    else:\n        print(f\"[OBSERVATION] EXPLORATORY: {finding}\")\n        print(\"[LIMITATION] This finding was not pre-registered and requires replication\")\n```\n\n### Document Deviations from Plan\n\n```python\n# When you deviate from the pre-registered plan, document it!\nprint(\"[DECISION] DEVIATION FROM PRE-REGISTRATION:\")\nprint(\"  Original plan: Two-proportion z-test\")\nprint(\"  Actual analysis: Fisher's exact test\")\nprint(\"  Reason: Cell counts < 5 in contingency table\")\nprint(\"  Impact: More conservative, may reduce power\")\n\n# Keep a deviation log\ndeviations = [\n    {\n        \"item\": \"Statistical test\",\n        \"planned\": \"z-test\",\n        \"actual\": \"Fisher's exact\",\n        \"reason\": \"Low expected cell counts\",\n        \"impact\": \"Minimal - Fisher's is more conservative\"\n    },\n    {\n        \"item\": \"Sample size\",\n        \"planned\": 500,\n        \"actual\": 487,\n        \"reason\": \"13 users excluded due to technical issues\",\n        \"impact\": \"Power reduced from 80% to 78%\"\n    }\n]\n\nprint(\"[EXPERIMENT] Deviation log:\")\nfor d in deviations:\n    print(f\"  - {d['item']}: {d['planned']} ‚Üí {d['actual']} ({d['reason']})\")\n```\n\n## Stopping Rules\n\nDefine stopping criteria upfront to prevent p-hacking through optional stopping.\n\n### Define Success/Failure Criteria Upfront\n\n```python\nprint(\"[DECISION] Defining stopping rules BEFORE experiment starts\")\n\nstopping_rules = {\n    \"success_criterion\": \"Lower 95% CI bound > 0 (effect is positive)\",\n    \"failure_criterion\": \"Upper 95% CI bound < minimum_effect (effect too small)\",\n    \"futility_criterion\": \"Posterior probability of success < 5%\",\n    \"max_sample_size\": 1000,\n    \"interim_analyses\": [250, 500, 750],  # Pre-specified checkpoints\n    \"alpha_spending\": \"O'Brien-Fleming\"  # Preserve overall alpha\n}\n\nprint(f\"[EXPERIMENT] Stopping rules defined:\")\nfor key, value in stopping_rules.items():\n    print(f\"  {key}: {value}\")\n```\n\n### Avoid P-Hacking Through Optional Stopping\n\n```python\n# BAD: Looking at p-value repeatedly and stopping when significant\n# This inflates false positive rate!\n\n# GOOD: Use alpha-spending functions to control Type I error\n\nfrom scipy import stats\nimport numpy as np\n\ndef obrien_fleming_boundary(alpha: float, n_looks: int, current_look: int) -> float:\n    \"\"\"\n    Calculate O'Brien-Fleming spending boundary.\n    More conservative early, less conservative late.\n    \"\"\"\n    # Information fraction\n    t = current_look / n_looks\n    # O'Brien-Fleming boundary\n    z_boundary = stats.norm.ppf(1 - alpha/2) / np.sqrt(t)\n    p_boundary = 2 * (1 - stats.norm.cdf(z_boundary))\n    return p_boundary\n\nn_looks = 4  # Number of interim analyses\nalpha = 0.05  # Overall significance level\n\nprint(\"[DECISION] Using O'Brien-Fleming alpha-spending to control Type I error\")\nprint(\"[EXPERIMENT] Adjusted significance thresholds:\")\nfor look in range(1, n_looks + 1):\n    boundary = obrien_fleming_boundary(alpha, n_looks, look)\n    print(f\"  Look {look}/{n_looks}: p < {boundary:.5f} to stop for efficacy\")\n\nprint(\"[LIMITATION] Stopping early requires more extreme evidence\")\n```\n\n### Sequential Analysis Methods (SPRT)\n\n```python\n# Sequential Probability Ratio Test (SPRT)\n# Allows continuous monitoring with controlled error rates\n\ndef sprt_bounds(alpha: float, beta: float) -> tuple:\n    \"\"\"\n    Calculate SPRT decision boundaries.\n\n    Args:\n        alpha: Type I error rate (false positive)\n        beta: Type II error rate (false negative)\n\n    Returns:\n        (lower_bound, upper_bound) for log-likelihood ratio\n    \"\"\"\n    A = np.log((1 - beta) / alpha)  # Upper boundary (accept H1)\n    B = np.log(beta / (1 - alpha))  # Lower boundary (accept H0)\n    return B, A\n\nalpha, beta = 0.05, 0.20  # 5% false positive, 20% false negative (80% power)\nlower, upper = sprt_bounds(alpha, beta)\n\nprint(\"[DECISION] Using Sequential Probability Ratio Test (SPRT)\")\nprint(f\"[STAT:estimate] Stop for H0 if LLR < {lower:.3f}\")\nprint(f\"[STAT:estimate] Stop for H1 if LLR > {upper:.3f}\")\nprint(\"[EXPERIMENT] Continue sampling if {:.3f} < LLR < {:.3f}\".format(lower, upper))\n\n# Example SPRT monitoring\ndef monitor_sprt(successes: int, trials: int, p0: float, p1: float, bounds: tuple):\n    \"\"\"Monitor SPRT decision status.\"\"\"\n    lower, upper = bounds\n    # Log-likelihood ratio\n    if successes == 0 or successes == trials:\n        llr = 0  # Avoid log(0)\n    else:\n        p_hat = successes / trials\n        llr = successes * np.log(p1/p0) + (trials - successes) * np.log((1-p1)/(1-p0))\n\n    if llr > upper:\n        return \"STOP: Accept H1 (treatment effective)\", llr\n    elif llr < lower:\n        return \"STOP: Accept H0 (no effect)\", llr\n    else:\n        return \"CONTINUE: Need more data\", llr\n\n# Example: Testing if conversion rate > 10% vs = 10%\np_null, p_alt = 0.10, 0.15\nstatus, llr = monitor_sprt(successes=45, trials=350, p0=p_null, p1=p_alt, bounds=(lower, upper))\nprint(f\"[STAT:estimate] Current LLR: {llr:.3f}\")\nprint(f\"[DECISION] {status}\")\n```\n\n### Document Stopping Decision\n\n```python\n# When you stop an experiment, document the decision clearly\nprint(\"[DECISION] Experiment stopped at interim analysis 2/4\")\nprint(\"[STAT:estimate] Current effect: 5.2pp (95% CI: [2.1, 8.3])\")\nprint(\"[STAT:p_value] p = 0.0012 (< O'Brien-Fleming boundary 0.005)\")\nprint(\"[EXPERIMENT] Decision: STOP FOR EFFICACY\")\nprint(\"[FINDING] Treatment significantly improves conversion (confirmed at interim)\")\nprint(\"[LIMITATION] Final sample (n=500) smaller than planned (n=1000)\")\nprint(\"[LIMITATION] Effect estimate may regress toward null with more data\")\n```\n\n## Documentation Pattern\n```python\nprint(\"[DECISION] Chose Random Forest over XGBoost because:\")\nprint(\"  - Better interpretability for stakeholders\")\nprint(\"  - Comparable performance (within 1% accuracy)\")\nprint(\"  - Faster training time for iteration\")\n\nprint(\"[LIMITATION] Model may not generalize to:\")\nprint(\"  - Data from different time periods\")\nprint(\"  - Users from different demographics\")\n```\n",
        "skills/ml-rigor/SKILL.md": "---\nname: ml-rigor\ndescription: Enforces baseline comparisons, cross-validation, interpretation, and leakage prevention for ML pipelines\n---\n\n# Machine Learning Rigor Patterns\n\n## When to Use\n\nLoad this skill when building machine learning models. Every ML pipeline must demonstrate:\n- **Baseline comparison**: Beat a dummy model before claiming success\n- **Cross-validation**: Report variance, not just a single score\n- **Interpretation**: Explain what the model learned\n- **Leakage prevention**: Ensure no future information leaks into training\n\n**Quality Gate**: ML findings without baseline comparison or cross-validation are marked as \"Exploratory\" in reports.\n\n---\n\n## 1. Baseline Requirements\n\n**Every model must be compared to baselines.** A model that can't beat a dummy classifier isn't learning anything useful.\n\n### Always Compare To:\n1. **DummyClassifier/DummyRegressor** - The absolute minimum bar\n2. **Simple linear model** - LogisticRegression or LinearRegression\n3. **Domain heuristic** (if available) - Rule-based approach\n\n### Baseline Code Template\n\n```python\nfrom sklearn.dummy import DummyClassifier, DummyRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nprint(\"[DECISION] Establishing baselines before training complex models\")\n\n# Classification baselines\ndummy_clf = DummyClassifier(strategy='most_frequent')\ndummy_scores = cross_val_score(dummy_clf, X_train, y_train, cv=5, scoring='accuracy')\nprint(f\"[METRIC:baseline_accuracy] {dummy_scores.mean():.3f} (majority class)\")\nprint(f\"[METRIC:baseline_accuracy_std] {dummy_scores.std():.3f}\")\n\n# Simple linear baseline\nlr = LogisticRegression(max_iter=1000, random_state=42)\nlr_scores = cross_val_score(lr, X_train, y_train, cv=5, scoring='accuracy')\nprint(f\"[METRIC:linear_baseline_accuracy] {lr_scores.mean():.3f}\")\nprint(f\"[METRIC:linear_baseline_accuracy_std] {lr_scores.std():.3f}\")\n\n# For regression tasks\ndummy_reg = DummyRegressor(strategy='mean')\ndummy_rmse = cross_val_score(dummy_reg, X_train, y_train, cv=5,\n                              scoring='neg_root_mean_squared_error')\nprint(f\"[METRIC:baseline_rmse] {-dummy_rmse.mean():.3f} (mean predictor)\")\n```\n\n### Improvement Over Baseline with CI\n\n```python\nfrom scipy import stats\n\n# Calculate improvement with confidence interval\nmodel_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\nimprovement = model_scores.mean() - dummy_scores.mean()\n\n# Bootstrap CI for improvement\nn_bootstrap = 1000\nimprovements = []\nfor _ in range(n_bootstrap):\n    idx = np.random.choice(len(model_scores), len(model_scores), replace=True)\n    boot_improvement = model_scores[idx].mean() - dummy_scores[idx].mean()\n    improvements.append(boot_improvement)\n\nci_low, ci_high = np.percentile(improvements, [2.5, 97.5])\n\nprint(f\"[METRIC:improvement_over_baseline] {improvement:.3f}\")\nprint(f\"[STAT:ci] 95% CI [{ci_low:.3f}, {ci_high:.3f}]\")\nprint(f\"[STAT:effect_size] Relative improvement: {improvement/dummy_scores.mean()*100:.1f}%\")\n\nif ci_low > 0:\n    print(\"[FINDING] Model significantly outperforms baseline\")\nelse:\n    print(\"[LIMITATION] Improvement over baseline not statistically significant\")\n```\n\n---\n\n## 2. Cross-Validation Requirements\n\n**Never report a single train/test split.** Cross-validation shows how much your score varies.\n\n### Requirements:\n- Use **stratified K-fold** for classification (preserves class distribution)\n- Report **mean +/- std**, not just mean\n- Calculate **confidence interval** for mean performance\n- Use **repeated CV** for small datasets\n\n### Cross-Validation Code Template\n\n```python\nfrom sklearn.model_selection import (\n    StratifiedKFold, cross_val_score, cross_validate,\n    RepeatedStratifiedKFold\n)\nimport numpy as np\nfrom scipy import stats\n\nprint(\"[DECISION] Using 5-fold stratified CV to estimate model performance\")\n\n# Stratified K-Fold for classification\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Multiple metrics\nscoring = ['accuracy', 'f1_weighted', 'roc_auc']\ncv_results = cross_validate(model, X, y, cv=cv, scoring=scoring,\n                            return_train_score=True)\n\n# Report mean +/- std (REQUIRED)\nfor metric in scoring:\n    test_scores = cv_results[f'test_{metric}']\n    train_scores = cv_results[f'train_{metric}']\n\n    print(f\"[METRIC:cv_{metric}_mean] {test_scores.mean():.3f}\")\n    print(f\"[METRIC:cv_{metric}_std] {test_scores.std():.3f}\")\n\n    # Check for overfitting\n    gap = train_scores.mean() - test_scores.mean()\n    if gap > 0.1:\n        print(f\"[LIMITATION] Train-test gap of {gap:.3f} suggests overfitting\")\n```\n\n### Confidence Interval for CV Mean\n\n```python\n# CI for cross-validation mean (t-distribution for small n)\ndef cv_confidence_interval(scores, confidence=0.95):\n    n = len(scores)\n    mean = scores.mean()\n    se = stats.sem(scores)\n    h = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n    return mean - h, mean + h\n\nci_low, ci_high = cv_confidence_interval(cv_results['test_accuracy'])\nprint(f\"[STAT:ci] 95% CI for accuracy: [{ci_low:.3f}, {ci_high:.3f}]\")\n\n# For small datasets, use repeated CV\nprint(\"[DECISION] Using repeated CV for more stable estimates\")\nrcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\nscores = cross_val_score(model, X, y, cv=rcv, scoring='accuracy')\nprint(f\"[METRIC:repeated_cv_mean] {scores.mean():.3f}\")\nprint(f\"[METRIC:repeated_cv_std] {scores.std():.3f}\")\n```\n\n---\n\n## 3. Hyperparameter Tuning\n\n**Avoid overfitting to the validation set.** Report the distribution of scores, not just the best.\n\n### Requirements:\n- Use **RandomizedSearchCV** or **Bayesian optimization** (Optuna)\n- Report **distribution of scores** across parameter combinations\n- Use **nested CV** to get unbiased performance estimate\n- Watch for **overfitting to validation set**\n\n### RandomizedSearchCV Template\n\n```python\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score\nfrom scipy.stats import uniform, randint\nimport numpy as np\n\nprint(\"[DECISION] Using RandomizedSearchCV with 100 iterations\")\n\n# Define parameter distributions (not just lists!)\nparam_distributions = {\n    'n_estimators': randint(50, 500),\n    'max_depth': randint(3, 20),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10),\n    'max_features': uniform(0.1, 0.9)\n}\n\nrandom_search = RandomizedSearchCV(\n    model, param_distributions,\n    n_iter=100,\n    cv=5,\n    scoring='accuracy',\n    random_state=42,\n    return_train_score=True,\n    n_jobs=-1\n)\n\nrandom_search.fit(X_train, y_train)\n\n# Report distribution of scores (not just best!)\ncv_results = random_search.cv_results_\nall_test_scores = cv_results['mean_test_score']\nall_train_scores = cv_results['mean_train_score']\n\nprint(f\"[METRIC:tuning_best_score] {random_search.best_score_:.3f}\")\nprint(f\"[METRIC:tuning_score_range] [{all_test_scores.min():.3f}, {all_test_scores.max():.3f}]\")\nprint(f\"[METRIC:tuning_score_std] {all_test_scores.std():.3f}\")\n\n# Check for overfitting during tuning\nbest_idx = random_search.best_index_\ntrain_test_gap = all_train_scores[best_idx] - all_test_scores[best_idx]\nif train_test_gap > 0.1:\n    print(f\"[LIMITATION] Best model shows {train_test_gap:.3f} train-test gap\")\n```\n\n### Nested Cross-Validation (Unbiased Estimate)\n\n```python\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\nprint(\"[DECISION] Using nested CV for unbiased performance estimate\")\n\n# Outer CV for performance estimation\nouter_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Inner CV is handled by RandomizedSearchCV\ninner_search = RandomizedSearchCV(\n    model, param_distributions,\n    n_iter=50,\n    cv=3,  # Inner CV\n    scoring='accuracy',\n    random_state=42,\n    n_jobs=-1\n)\n\n# Nested CV scores\nnested_scores = cross_val_score(inner_search, X, y, cv=outer_cv, scoring='accuracy')\n\nprint(f\"[METRIC:nested_cv_mean] {nested_scores.mean():.3f}\")\nprint(f\"[METRIC:nested_cv_std] {nested_scores.std():.3f}\")\nprint(f\"[STAT:ci] 95% CI [{nested_scores.mean() - 1.96*nested_scores.std()/np.sqrt(5):.3f}, \"\n      f\"{nested_scores.mean() + 1.96*nested_scores.std()/np.sqrt(5):.3f}]\")\n```\n\n---\n\n## 4. Calibration Requirements\n\n**Probability predictions must be calibrated.** A 70% confidence should be correct 70% of the time.\n\n### Requirements:\n- Check **calibration curve** for probability predictions\n- Report **Brier score** (lower is better)\n- Apply **calibration** if needed (Platt scaling, isotonic regression)\n\n### Calibration Code Template\n\n```python\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nfrom sklearn.metrics import brier_score_loss\nimport matplotlib.pyplot as plt\n\nprint(\"[DECISION] Checking probability calibration\")\n\n# Get probability predictions\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Brier score (0 = perfect, 1 = worst)\nbrier = brier_score_loss(y_test, y_prob)\nprint(f\"[METRIC:brier_score] {brier:.4f}\")\n\n# Calibration curve\nprob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n\n# Plot calibration curve\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\nax.plot(prob_pred, prob_true, 's-', label=f'Model (Brier={brier:.3f})')\nax.set_xlabel('Mean predicted probability')\nax.set_ylabel('Fraction of positives')\nax.set_title('Calibration Curve')\nax.legend()\nplt.savefig('figures/calibration_curve.png', dpi=150, bbox_inches='tight')\nprint(\"[ARTIFACT:figure] figures/calibration_curve.png\")\n\n# Check calibration quality\nmax_calibration_error = np.max(np.abs(prob_true - prob_pred))\nprint(f\"[METRIC:max_calibration_error] {max_calibration_error:.3f}\")\n\nif max_calibration_error > 0.1:\n    print(\"[LIMITATION] Model probabilities are poorly calibrated\")\n```\n\n### Apply Calibration\n\n```python\nprint(\"[DECISION] Applying isotonic calibration to improve probability estimates\")\n\n# Calibrate using held-out data\ncalibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=5)\ncalibrated_model.fit(X_train, y_train)\n\n# Compare before/after\ny_prob_calibrated = calibrated_model.predict_proba(X_test)[:, 1]\nbrier_calibrated = brier_score_loss(y_test, y_prob_calibrated)\n\nprint(f\"[METRIC:brier_before_calibration] {brier:.4f}\")\nprint(f\"[METRIC:brier_after_calibration] {brier_calibrated:.4f}\")\nprint(f\"[METRIC:calibration_improvement] {brier - brier_calibrated:.4f}\")\n```\n\n---\n\n## 5. Interpretation Requirements\n\n**Explain what the model learned.** Black boxes are not acceptable for important decisions.\n\n### Requirements:\n- Compute **permutation importance** or **SHAP values**\n- Show at least one **case study** (why this specific prediction?)\n- Verify features make **domain sense**\n\n### Permutation Importance Template\n\n```python\nfrom sklearn.inspection import permutation_importance\nimport pandas as pd\n\nprint(\"[DECISION] Computing permutation importance on test set\")\n\n# Permutation importance (more reliable than built-in feature_importances_)\nperm_importance = permutation_importance(\n    model, X_test, y_test,\n    n_repeats=30,\n    random_state=42,\n    n_jobs=-1\n)\n\n# Create importance DataFrame\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance_mean': perm_importance.importances_mean,\n    'importance_std': perm_importance.importances_std\n}).sort_values('importance_mean', ascending=False)\n\n# Report top features\nprint(\"[METRIC:top_features]\")\nfor i, row in importance_df.head(5).iterrows():\n    print(f\"  {row['feature']}: {row['importance_mean']:.4f} (+/- {row['importance_std']:.4f})\")\n\n# Check for unexpected features\nprint(\"[CHECK:domain_sense] Verify top features align with domain knowledge\")\n```\n\n### SHAP Values Template\n\n```python\nimport shap\n\nprint(\"[DECISION] Using SHAP for model interpretation\")\n\n# Create explainer\nexplainer = shap.TreeExplainer(model)  # or shap.KernelExplainer for any model\nshap_values = explainer.shap_values(X_test)\n\n# Global importance\nshap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\nplt.savefig('figures/shap_summary.png', dpi=150, bbox_inches='tight')\nprint(\"[ARTIFACT:figure] figures/shap_summary.png\")\n\n# Mean absolute SHAP values\nif isinstance(shap_values, list):  # Multi-class\n    shap_importance = np.abs(shap_values[1]).mean(axis=0)\nelse:\n    shap_importance = np.abs(shap_values).mean(axis=0)\n\ntop_features = sorted(zip(feature_names, shap_importance),\n                     key=lambda x: x[1], reverse=True)[:5]\nprint(\"[METRIC:shap_top_features]\")\nfor feat, imp in top_features:\n    print(f\"  {feat}: {imp:.4f}\")\n```\n\n### Case Study (Individual Prediction)\n\n```python\nprint(\"[DECISION] Analyzing individual prediction for interpretability\")\n\n# Select an interesting case (e.g., high confidence wrong prediction)\ny_pred_proba = model.predict_proba(X_test)[:, 1]\ny_pred = model.predict(X_test)\nmistakes = (y_pred != y_test)\nhigh_conf_mistakes = mistakes & (np.abs(y_pred_proba - 0.5) > 0.4)\n\nif high_conf_mistakes.any():\n    case_idx = np.where(high_conf_mistakes)[0][0]\n    print(f\"[ANALYSIS] Case study: High-confidence mistake (index {case_idx})\")\n    print(f\"  Predicted: {y_pred[case_idx]} (prob={y_pred_proba[case_idx]:.3f})\")\n    print(f\"  Actual: {y_test.iloc[case_idx]}\")\n\n    # SHAP for this case\n    shap.force_plot(explainer.expected_value[1] if isinstance(explainer.expected_value, list)\n                    else explainer.expected_value,\n                    shap_values[case_idx] if not isinstance(shap_values, list)\n                    else shap_values[1][case_idx],\n                    X_test.iloc[case_idx],\n                    matplotlib=True, show=False)\n    plt.savefig('figures/case_study_shap.png', dpi=150, bbox_inches='tight')\n    print(\"[ARTIFACT:figure] figures/case_study_shap.png\")\n    print(\"[LIMITATION] Model fails on cases with pattern: [describe pattern]\")\n```\n\n---\n\n## 6. Error Analysis\n\n**Understand where and why the model fails.** Aggregate metrics hide important failure modes.\n\n### Requirements:\n- **Slice performance** by key segments (demographics, time periods, etc.)\n- Analyze **failure modes** (false positives vs false negatives)\n- Check for **systematic errors** (biases, subgroup issues)\n\n### Error Analysis Code Template\n\n```python\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport pandas as pd\nimport numpy as np\n\nprint(\"[DECISION] Performing error analysis across segments\")\n\ny_pred = model.predict(X_test)\n\n# Overall confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"[METRIC:confusion_matrix]\")\nprint(cm)\n\n# Classification report\nprint(\"[ANALYSIS] Classification report:\")\nprint(classification_report(y_test, y_pred, target_names=class_names))\n```\n\n### Slice Performance Analysis\n\n```python\nprint(\"[DECISION] Analyzing performance by key segments\")\n\n# Create test DataFrame with predictions\ntest_df = X_test.copy()\ntest_df['y_true'] = y_test.values\ntest_df['y_pred'] = y_pred\n\n# Define segments to analyze\nsegments = {\n    'age_group': pd.cut(test_df['age'], bins=[0, 30, 50, 100], labels=['young', 'middle', 'senior']),\n    'income_tier': pd.qcut(test_df['income'], q=3, labels=['low', 'medium', 'high'])\n}\n\nprint(\"[METRIC:slice_performance]\")\nfor segment_name, segment_values in segments.items():\n    test_df['segment'] = segment_values\n\n    for segment_val in segment_values.unique():\n        mask = test_df['segment'] == segment_val\n        if mask.sum() > 10:  # Only report if enough samples\n            segment_accuracy = (test_df.loc[mask, 'y_true'] == test_df.loc[mask, 'y_pred']).mean()\n            print(f\"  {segment_name}={segment_val}: accuracy={segment_accuracy:.3f} (n={mask.sum()})\")\n\n            # Check for underperformance\n            if segment_accuracy < overall_accuracy - 0.1:\n                print(f\"  [LIMITATION] Model underperforms on {segment_name}={segment_val}\")\n```\n\n### Failure Mode Analysis\n\n```python\nprint(\"[DECISION] Analyzing failure modes\")\n\n# Separate false positives and false negatives\nfp_mask = (y_pred == 1) & (y_test == 0)\nfn_mask = (y_pred == 0) & (y_test == 1)\n\nprint(f\"[METRIC:false_positive_rate] {fp_mask.mean():.3f}\")\nprint(f\"[METRIC:false_negative_rate] {fn_mask.mean():.3f}\")\n\n# Analyze characteristics of errors\nif fp_mask.sum() > 0:\n    print(\"[ANALYSIS] False positive characteristics:\")\n    fp_data = X_test[fp_mask]\n    for col in feature_names[:5]:  # Top features\n        print(f\"  {col}: mean={fp_data[col].mean():.3f} vs overall={X_test[col].mean():.3f}\")\n\n# Check for systematic bias\nprint(\"[CHECK:systematic_error] Review if errors correlate with protected attributes\")\n```\n\n---\n\n## 7. Leakage Checklist\n\n**Data leakage silently destroys model validity.** Check these BEFORE trusting any results.\n\n### Checklist:\n- [ ] **Time-based splits** for temporal data (no future information)\n- [ ] **No target information** in features (derived features, proxies)\n- [ ] **Preprocessing inside CV** loop (no fit on test data)\n- [ ] **Group-aware splits** if samples are related (same user, same session)\n\n### Time-Based Split Template\n\n```python\nfrom sklearn.model_selection import TimeSeriesSplit\n\nprint(\"[DECISION] Using time-based split for temporal data\")\n\n# Check if data has temporal component\nif 'date' in df.columns or 'timestamp' in df.columns:\n    print(\"[CHECK:temporal_leakage] Using TimeSeriesSplit to prevent future information leak\")\n\n    # Sort by time\n    df_sorted = df.sort_values('date')\n\n    # Time-based cross-validation\n    tscv = TimeSeriesSplit(n_splits=5)\n\n    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_sorted)):\n        train_max_date = df_sorted.iloc[train_idx]['date'].max()\n        test_min_date = df_sorted.iloc[test_idx]['date'].min()\n\n        if train_max_date >= test_min_date:\n            print(f\"[ERROR] Fold {fold}: Data leakage detected!\")\n        else:\n            print(f\"[CHECK:fold_{fold}] Train ends {train_max_date}, Test starts {test_min_date} - OK\")\nelse:\n    print(\"[LIMITATION] No temporal column found - using random split\")\n```\n\n### Target Leakage Detection\n\n```python\nprint(\"[CHECK:target_leakage] Checking for target information in features\")\n\n# Check correlation between features and target\ncorrelations = X_train.corrwith(pd.Series(y_train, index=X_train.index))\nhigh_corr_features = correlations[correlations.abs() > 0.9].index.tolist()\n\nif high_corr_features:\n    print(f\"[WARNING] Suspiciously high correlations with target:\")\n    for feat in high_corr_features:\n        print(f\"  {feat}: r={correlations[feat]:.3f}\")\n    print(\"[LIMITATION] Review these features for potential target leakage\")\nelse:\n    print(\"[CHECK:target_leakage] No obvious target leakage detected\")\n\n# Check for post-hoc features (created after the outcome)\nprint(\"[CHECK:feature_timing] Verify all features are available at prediction time\")\n```\n\n### Preprocessing Inside CV Template\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nprint(\"[DECISION] Using Pipeline to prevent preprocessing leakage\")\n\n# WRONG: Fitting scaler on all data before CV\n# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(X)  # LEAKAGE!\n# scores = cross_val_score(model, X_scaled, y, cv=5)\n\n# CORRECT: Preprocessing inside pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', model)\n])\n\nscores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\nprint(f\"[METRIC:cv_accuracy_mean] {scores.mean():.3f}\")\nprint(f\"[METRIC:cv_accuracy_std] {scores.std():.3f}\")\nprint(\"[CHECK:preprocessing_leakage] Scaler fit inside CV - no leakage\")\n```\n\n### Group-Aware Splitting\n\n```python\nfrom sklearn.model_selection import GroupKFold\n\nprint(\"[CHECK:group_leakage] Checking if samples are related\")\n\n# If samples belong to groups (e.g., multiple records per user)\nif 'user_id' in df.columns:\n    print(\"[DECISION] Using GroupKFold to prevent group leakage\")\n\n    groups = df['user_id']\n    gkf = GroupKFold(n_splits=5)\n\n    scores = cross_val_score(model, X, y, cv=gkf, groups=groups, scoring='accuracy')\n    print(f\"[METRIC:group_cv_accuracy_mean] {scores.mean():.3f}\")\n    print(f\"[METRIC:group_cv_accuracy_std] {scores.std():.3f}\")\n    print(\"[CHECK:group_leakage] Same user never in both train and test - OK\")\nelse:\n    print(\"[CHECK:group_leakage] No group column - using standard CV\")\n```\n\n---\n\n## Complete ML Pipeline Example\n\nPutting it all together:\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nprint(\"[OBJECTIVE] Build and validate classification model with full rigor\")\n\n# 1. BASELINE\nprint(\"\\n--- Baseline Comparison ---\")\ndummy = DummyClassifier(strategy='most_frequent')\ndummy_scores = cross_val_score(dummy, X, y, cv=5, scoring='accuracy')\nprint(f\"[METRIC:baseline_accuracy] {dummy_scores.mean():.3f}\")\n\n# 2. CROSS-VALIDATION\nprint(\"\\n--- Cross-Validation ---\")\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', RandomForestClassifier(random_state=42))\n])\ncv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\nprint(f\"[METRIC:cv_accuracy_mean] {cv_scores.mean():.3f}\")\nprint(f\"[METRIC:cv_accuracy_std] {cv_scores.std():.3f}\")\n\n# 3. IMPROVEMENT OVER BASELINE\nimprovement = cv_scores.mean() - dummy_scores.mean()\nprint(f\"[METRIC:improvement_over_baseline] {improvement:.3f}\")\nprint(f\"[STAT:effect_size] Relative improvement: {improvement/dummy_scores.mean()*100:.1f}%\")\n\n# 4. INTERPRETATION\nprint(\"\\n--- Interpretation ---\")\n# (permutation importance code here)\n\n# 5. FINDING (only after full evidence)\nif improvement > 0.05:\n    print(f\"[FINDING] Random Forest achieves {cv_scores.mean():.3f} accuracy, \"\n          f\"improving {improvement:.3f} over baseline ({improvement/dummy_scores.mean()*100:.1f}% relative)\")\n    print(f\"[SO_WHAT] Model provides actionable predictions for business use case\")\nelse:\n    print(\"[FINDING] Model does not significantly outperform baseline\")\n    print(\"[LIMITATION] Consider simpler approach or feature engineering\")\n```\n\n---\n\n## Quality Gate: Required Evidence\n\nBefore any `[FINDING]` in ML, you MUST have:\n\n| Evidence | Marker | Example |\n|----------|--------|---------|\n| Baseline comparison | `[METRIC:baseline_*]` | `[METRIC:baseline_accuracy] 0.65` |\n| CV scores with variance | `[METRIC:cv_*_mean/std]` | `[METRIC:cv_accuracy_mean] 0.85` |\n| Improvement quantified | `[METRIC:improvement_*]` | `[METRIC:improvement_over_baseline] 0.20` |\n| Effect size | `[STAT:effect_size]` | `[STAT:effect_size] 31% relative improvement` |\n| Interpretation | `[METRIC:top_features]` | Top 3 features listed |\n| Limitations acknowledged | `[LIMITATION]` | Model constraints documented |\n\n**Missing any of these? Your finding is \"Exploratory\", not \"Verified\".**\n",
        "src/skill/data-analysis/SKILL.md": "---\nname: data-analysis\ndescription: Patterns for data loading, exploration, and statistical analysis\n---\n\n# Data Analysis Patterns\n\n## When to Use\nLoad this skill when working with datasets that require exploration, cleaning, and statistical analysis.\n\n## Data Loading\n```python\nprint(\"[DATA] Loading dataset\")\ndf = pd.read_csv(\"data.csv\")\nprint(f\"[SHAPE] {df.shape[0]} rows, {df.shape[1]} columns\")\nprint(f\"[DTYPE] {dict(df.dtypes)}\")\nprint(f\"[MISSING] {df.isnull().sum().to_dict()}\")\n```\n\n## Exploratory Data Analysis (EDA)\n\n### Descriptive Statistics\n```python\nprint(\"[STAT] Descriptive statistics:\")\nprint(df.describe())\n\nprint(f\"[RANGE] {col}: {df[col].min()} to {df[col].max()}\")\n```\n\n### Distribution Analysis\n```python\nprint(\"[ANALYSIS] Checking distribution normality\")\nfrom scipy import stats\nstat, p_value = stats.shapiro(df[col])\nprint(f\"[STAT] Shapiro-Wilk p-value: {p_value:.4f}\")\n```\n\n### Correlation Analysis\n```python\nprint(\"[CORR] Correlation matrix:\")\nprint(df.corr())\n```\n\n## Statistical Tests\n\n### T-Test\n```python\nfrom scipy.stats import ttest_ind\nstat, p = ttest_ind(group1, group2)\nprint(f\"[STAT] T-test: t={stat:.3f}, p={p:.4f}\")\n```\n\n### ANOVA\n```python\nfrom scipy.stats import f_oneway\nstat, p = f_oneway(group1, group2, group3)\nprint(f\"[STAT] ANOVA: F={stat:.3f}, p={p:.4f}\")\n```\n\n## Confidence Interval Patterns\n\n### Parametric CI for Means\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef mean_ci(data, confidence=0.95):\n    \"\"\"Calculate parametric confidence interval for mean.\"\"\"\n    n = len(data)\n    mean = np.mean(data)\n    se = stats.sem(data)  # Standard error of mean\n    h = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n    return mean, mean - h, mean + h\n\nmean, ci_low, ci_high = mean_ci(df[col])\nprint(f\"[STAT:estimate] mean = {mean:.3f}\")\nprint(f\"[STAT:ci] 95% CI [{ci_low:.3f}, {ci_high:.3f}]\")\n```\n\n### Bootstrap CI for Medians/Complex Statistics\n```python\nimport numpy as np\n\ndef bootstrap_ci(data, stat_func=np.median, n_bootstrap=10000, confidence=0.95):\n    \"\"\"Calculate bootstrap confidence interval for any statistic.\"\"\"\n    boot_stats = []\n    n = len(data)\n    for _ in range(n_bootstrap):\n        sample = np.random.choice(data, size=n, replace=True)\n        boot_stats.append(stat_func(sample))\n    \n    alpha = 1 - confidence\n    ci_low = np.percentile(boot_stats, 100 * alpha / 2)\n    ci_high = np.percentile(boot_stats, 100 * (1 - alpha / 2))\n    return stat_func(data), ci_low, ci_high\n\nmedian, ci_low, ci_high = bootstrap_ci(df[col], stat_func=np.median)\nprint(f\"[STAT:estimate] median = {median:.3f}\")\nprint(f\"[STAT:ci] 95% Bootstrap CI [{ci_low:.3f}, {ci_high:.3f}]\")\n```\n\n### Wilson CI for Proportions\n```python\nfrom scipy import stats\n\ndef wilson_ci(successes, trials, confidence=0.95):\n    \"\"\"Calculate Wilson score interval for proportions (better for small n).\"\"\"\n    p = successes / trials\n    z = stats.norm.ppf((1 + confidence) / 2)\n    \n    denominator = 1 + z**2 / trials\n    center = (p + z**2 / (2 * trials)) / denominator\n    spread = z * np.sqrt((p * (1 - p) + z**2 / (4 * trials)) / trials) / denominator\n    \n    return p, center - spread, center + spread\n\nprop, ci_low, ci_high = wilson_ci(successes=45, trials=100)\nprint(f\"[STAT:estimate] proportion = {prop:.3f}\")\nprint(f\"[STAT:ci] 95% Wilson CI [{ci_low:.3f}, {ci_high:.3f}]\")\n```\n\n## Effect Size Calculation\n\n### Cohen's d for Group Comparisons\n```python\nimport numpy as np\n\ndef cohens_d(group1, group2):\n    \"\"\"Calculate Cohen's d effect size for two independent groups.\"\"\"\n    n1, n2 = len(group1), len(group2)\n    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n    \n    # Pooled standard deviation\n    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n    d = (np.mean(group1) - np.mean(group2)) / pooled_std\n    \n    # Interpretation\n    magnitude = \"small\" if abs(d) < 0.5 else \"medium\" if abs(d) < 0.8 else \"large\"\n    return d, magnitude\n\nd, magnitude = cohens_d(treatment, control)\nprint(f\"[STAT:effect_size] Cohen's d = {d:.3f} ({magnitude})\")\n```\n\n### r¬≤ for Correlations\n```python\nfrom scipy import stats\n\ndef correlation_r2(x, y):\n    \"\"\"Calculate Pearson r and r¬≤ with interpretation.\"\"\"\n    r, p = stats.pearsonr(x, y)\n    r2 = r ** 2\n    \n    # Interpretation (based on Cohen's guidelines for r)\n    magnitude = \"small\" if abs(r) < 0.3 else \"medium\" if abs(r) < 0.5 else \"large\"\n    return r, r2, p, magnitude\n\nr, r2, p, magnitude = correlation_r2(df[x_col], df[y_col])\nprint(f\"[STAT:estimate] r = {r:.3f}\")\nprint(f\"[STAT:effect_size] r¬≤ = {r2:.3f} ({magnitude} effect, {r2*100:.1f}% variance explained)\")\nprint(f\"[STAT:p_value] p = {p:.4f}\")\n```\n\n### Cliff's Delta for Non-Parametric Comparisons\n```python\nimport numpy as np\n\ndef cliffs_delta(group1, group2):\n    \"\"\"Calculate Cliff's delta (non-parametric effect size).\"\"\"\n    n1, n2 = len(group1), len(group2)\n    \n    # Count dominance\n    more = sum(1 for x in group1 for y in group2 if x > y)\n    less = sum(1 for x in group1 for y in group2 if x < y)\n    delta = (more - less) / (n1 * n2)\n    \n    # Interpretation (Romano et al., 2006)\n    abs_d = abs(delta)\n    magnitude = \"negligible\" if abs_d < 0.147 else \"small\" if abs_d < 0.33 else \"medium\" if abs_d < 0.474 else \"large\"\n    return delta, magnitude\n\ndelta, magnitude = cliffs_delta(treatment, control)\nprint(f\"[STAT:effect_size] Cliff's delta = {delta:.3f} ({magnitude})\")\n```\n\n## Assumption Checking\n\n### Normality: Shapiro-Wilk and Q-Q Plot\n```python\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef check_normality(data, col_name=\"variable\", alpha=0.05):\n    \"\"\"Check normality assumption with Shapiro-Wilk test and Q-Q plot.\"\"\"\n    # Shapiro-Wilk test (best for n < 5000)\n    stat, p = stats.shapiro(data)\n    is_normal = p > alpha\n    \n    print(f\"[CHECK:normality] Shapiro-Wilk W={stat:.4f}, p={p:.4f}\")\n    print(f\"[CHECK:normality] {'PASS' if is_normal else 'FAIL'}: Data {'is' if is_normal else 'is NOT'} normally distributed (Œ±={alpha})\")\n    \n    # Q-Q plot for visual inspection\n    fig, ax = plt.subplots(figsize=(6, 6))\n    stats.probplot(data, dist=\"norm\", plot=ax)\n    ax.set_title(f\"Q-Q Plot: {col_name}\")\n    plt.savefig(f\"reports/figures/qq_plot_{col_name}.png\", dpi=150, bbox_inches=\"tight\")\n    plt.close()\n    \n    return is_normal, stat, p\n\nis_normal, stat, p = check_normality(df[col], col_name=col)\n```\n\n### Homogeneity of Variance: Levene's Test\n```python\nfrom scipy import stats\n\ndef check_homogeneity(*groups, alpha=0.05):\n    \"\"\"Check homogeneity of variance (homoscedasticity) with Levene's test.\"\"\"\n    stat, p = stats.levene(*groups)\n    is_homogeneous = p > alpha\n    \n    print(f\"[CHECK:homogeneity] Levene's W={stat:.4f}, p={p:.4f}\")\n    print(f\"[CHECK:homogeneity] {'PASS' if is_homogeneous else 'FAIL'}: Variances {'are' if is_homogeneous else 'are NOT'} equal (Œ±={alpha})\")\n    \n    if not is_homogeneous:\n        print(\"[CHECK:homogeneity] Recommendation: Use Welch's t-test instead of Student's t-test\")\n    \n    return is_homogeneous, stat, p\n\nis_homogeneous, stat, p = check_homogeneity(group1, group2)\n```\n\n### Independence: Durbin-Watson Test (for Regression Residuals)\n```python\nfrom statsmodels.stats.stattools import durbin_watson\n\ndef check_independence(residuals):\n    \"\"\"Check independence of residuals with Durbin-Watson test.\"\"\"\n    dw_stat = durbin_watson(residuals)\n    \n    # Interpretation: DW ‚âà 2 means no autocorrelation\n    # DW < 1.5 suggests positive autocorrelation\n    # DW > 2.5 suggests negative autocorrelation\n    if dw_stat < 1.5:\n        status = \"FAIL - positive autocorrelation detected\"\n    elif dw_stat > 2.5:\n        status = \"FAIL - negative autocorrelation detected\"\n    else:\n        status = \"PASS - no significant autocorrelation\"\n    \n    print(f\"[CHECK:independence] Durbin-Watson = {dw_stat:.3f}\")\n    print(f\"[CHECK:independence] {status}\")\n    \n    return dw_stat, status\n\ndw_stat, status = check_independence(model.resid)\n```\n\n## Robust Alternatives\n\n### Welch's t-test (Instead of Student's t-test)\n```python\nfrom scipy import stats\n\ndef welchs_ttest(group1, group2, alpha=0.05):\n    \"\"\"\n    Welch's t-test - DEFAULT choice for comparing two groups.\n    Does NOT assume equal variances (more robust than Student's t-test).\n    \"\"\"\n    stat, p = stats.ttest_ind(group1, group2, equal_var=False)  # equal_var=False for Welch's\n    \n    print(f\"[DECISION] Using Welch's t-test: Does not assume equal variances\")\n    print(f\"[STAT:estimate] t-statistic = {stat:.3f}\")\n    print(f\"[STAT:p_value] p = {p:.4f}\")\n    \n    # Effect size\n    from numpy import sqrt, var, mean\n    n1, n2 = len(group1), len(group2)\n    pooled_std = sqrt(((n1-1)*var(group1, ddof=1) + (n2-1)*var(group2, ddof=1)) / (n1+n2-2))\n    d = (mean(group1) - mean(group2)) / pooled_std\n    magnitude = \"small\" if abs(d) < 0.5 else \"medium\" if abs(d) < 0.8 else \"large\"\n    print(f\"[STAT:effect_size] Cohen's d = {d:.3f} ({magnitude})\")\n    \n    return stat, p, d\n\nt_stat, p_value, effect_size = welchs_ttest(treatment, control)\n```\n\n### Mann-Whitney U Test (for Non-Normal Data)\n```python\nfrom scipy import stats\nimport numpy as np\n\ndef mann_whitney_test(group1, group2, alpha=0.05):\n    \"\"\"\n    Mann-Whitney U test - Non-parametric alternative to t-test.\n    Use when normality assumption is violated.\n    \"\"\"\n    stat, p = stats.mannwhitneyu(group1, group2, alternative='two-sided')\n    \n    print(f\"[DECISION] Using Mann-Whitney U: Non-parametric, does not assume normality\")\n    print(f\"[STAT:estimate] U-statistic = {stat:.3f}\")\n    print(f\"[STAT:p_value] p = {p:.4f}\")\n    \n    # Effect size: Cliff's delta (appropriate for non-parametric)\n    n1, n2 = len(group1), len(group2)\n    more = sum(1 for x in group1 for y in group2 if x > y)\n    less = sum(1 for x in group1 for y in group2 if x < y)\n    delta = (more - less) / (n1 * n2)\n    magnitude = \"negligible\" if abs(delta) < 0.147 else \"small\" if abs(delta) < 0.33 else \"medium\" if abs(delta) < 0.474 else \"large\"\n    print(f\"[STAT:effect_size] Cliff's delta = {delta:.3f} ({magnitude})\")\n    \n    return stat, p, delta\n\nu_stat, p_value, effect_size = mann_whitney_test(treatment, control)\n```\n\n### Permutation Test (for Complex Designs)\n```python\nimport numpy as np\n\ndef permutation_test(group1, group2, n_permutations=10000, stat_func=None):\n    \"\"\"\n    Permutation test - Most robust, makes minimal assumptions.\n    Use when parametric assumptions are violated or for complex statistics.\n    \"\"\"\n    if stat_func is None:\n        stat_func = lambda x, y: np.mean(x) - np.mean(y)\n    \n    observed = stat_func(group1, group2)\n    combined = np.concatenate([group1, group2])\n    n1 = len(group1)\n    \n    # Generate permutation distribution\n    perm_stats = []\n    for _ in range(n_permutations):\n        np.random.shuffle(combined)\n        perm_stat = stat_func(combined[:n1], combined[n1:])\n        perm_stats.append(perm_stat)\n    \n    # Two-tailed p-value\n    p_value = np.mean(np.abs(perm_stats) >= np.abs(observed))\n    \n    print(f\"[DECISION] Using permutation test: Assumption-free, {n_permutations} permutations\")\n    print(f\"[STAT:estimate] Observed difference = {observed:.4f}\")\n    print(f\"[STAT:p_value] p = {p_value:.4f} (permutation-based)\")\n    \n    # Bootstrap CI for the observed statistic\n    boot_diffs = []\n    for _ in range(n_permutations):\n        b1 = np.random.choice(group1, size=len(group1), replace=True)\n        b2 = np.random.choice(group2, size=len(group2), replace=True)\n        boot_diffs.append(stat_func(b1, b2))\n    ci_low, ci_high = np.percentile(boot_diffs, [2.5, 97.5])\n    print(f\"[STAT:ci] 95% Bootstrap CI [{ci_low:.4f}, {ci_high:.4f}]\")\n    \n    return observed, p_value, ci_low, ci_high\n\nobs, p_val, ci_low, ci_high = permutation_test(treatment, control)\n```\n\n## Memory Management\n```python\nprint(f\"[MEMORY] DataFrame size: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n# Clean up\ndel large_df\nimport gc; gc.collect()\n```\n",
        "src/skill/experiment-design/SKILL.md": "---\nname: experiment-design\ndescription: Best practices for designing reproducible experiments\n---\n\n# Experiment Design Patterns\n\n## When to Use\nLoad this skill when designing experiments that need to be reproducible and statistically valid.\n\n## Reproducibility Setup\n\n### Random Seeds\n```python\nimport random\nimport numpy as np\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nprint(f\"[DECISION] Using random seed: {SEED}\")\n```\n\n### Environment Recording\n```python\nimport sys\nprint(f\"[INFO] Python: {sys.version}\")\nprint(f\"[INFO] NumPy: {np.__version__}\")\nprint(f\"[INFO] Pandas: {pd.__version__}\")\n```\n\n## Experimental Controls\n\n### Train/Test Split\n```python\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=SEED, stratify=y\n)\nprint(f\"[EXPERIMENT] Train: {len(X_train)}, Test: {len(X_test)}\")\n```\n\n### Cross-Validation\n```python\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(f\"[METRIC] CV Accuracy: {scores.mean():.3f} (+/- {scores.std()*2:.3f})\")\n```\n\n## A/B Testing Pattern\n```python\nprint(\"[EXPERIMENT] A/B Test Design\")\nprint(f\"[INFO] Control group: {len(control)}\")\nprint(f\"[INFO] Treatment group: {len(treatment)}\")\n\n# Power analysis\nfrom statsmodels.stats.power import TTestIndPower\npower = TTestIndPower()\nsample_size = power.solve_power(effect_size=0.5, alpha=0.05, power=0.8)\nprint(f\"[CALC] Required sample size per group: {sample_size:.0f}\")\n```\n\n## Power Analysis\n\nPower analysis ensures your experiment has sufficient sample size to detect meaningful effects. Without adequate power, you risk false negatives (missing real effects).\n\n### Sample Size Calculation\n\n```python\nfrom statsmodels.stats.power import TTestIndPower, FTestAnovaPower, NormalIndPower\nimport numpy as np\n\nprint(\"[DECISION] Conducting a priori power analysis before data collection\")\n\n# For two-group comparison (t-test)\npower_analysis = TTestIndPower()\n\n# Parameters:\n# - effect_size: Expected Cohen's d (0.2=small, 0.5=medium, 0.8=large)\n# - alpha: Significance level (typically 0.05)\n# - power: Desired statistical power (typically 0.80 or 0.90)\n# - ratio: Ratio of group sizes (1.0 = equal groups)\n\neffect_size = 0.5  # Medium effect size\nalpha = 0.05\ndesired_power = 0.80\n\nsample_size = power_analysis.solve_power(\n    effect_size=effect_size,\n    alpha=alpha,\n    power=desired_power,\n    ratio=1.0,\n    alternative='two-sided'\n)\n\nprint(f\"[STAT:estimate] Required n per group: {np.ceil(sample_size):.0f}\")\nprint(f\"[STAT:estimate] Total sample needed: {np.ceil(sample_size)*2:.0f}\")\nprint(f\"[DECISION] Targeting {effect_size} effect size (Cohen's d = medium)\")\n```\n\n### Achieved Power (Post-Hoc)\n\n```python\n# After data collection, calculate achieved power\nactual_n = 50  # Actual sample size per group\nachieved_power = power_analysis.solve_power(\n    effect_size=effect_size,\n    alpha=alpha,\n    nobs1=actual_n,\n    ratio=1.0,\n    alternative='two-sided'\n)\n\nprint(f\"[STAT:estimate] Achieved power: {achieved_power:.3f}\")\n\nif achieved_power < 0.80:\n    print(f\"[LIMITATION] Study is underpowered ({achieved_power:.0%} < 80%)\")\n    print(\"[LIMITATION] Negative results may be due to insufficient sample size\")\nelse:\n    print(f\"[DECISION] Adequate power achieved ({achieved_power:.0%} ‚â• 80%)\")\n```\n\n### Power for Different Tests\n\n```python\n# For ANOVA (multiple groups)\nfrom statsmodels.stats.power import FTestAnovaPower\nanova_power = FTestAnovaPower()\nn_groups = 3\neffect_size_f = 0.25  # Cohen's f (0.1=small, 0.25=medium, 0.4=large)\nn_per_group = anova_power.solve_power(\n    effect_size=effect_size_f,\n    alpha=0.05,\n    power=0.80,\n    k_groups=n_groups\n)\nprint(f\"[STAT:estimate] ANOVA: {np.ceil(n_per_group):.0f} per group needed\")\n\n# For proportions (chi-square, A/B tests)\nfrom statsmodels.stats.power import GofChisquarePower\nfrom statsmodels.stats.proportion import proportion_effectsize\n# Convert expected proportions to effect size\np1, p2 = 0.10, 0.15  # e.g., 10% baseline, 15% expected with treatment\nprop_effect = proportion_effectsize(p1, p2)\nprint(f\"[DECISION] Effect size h = {prop_effect:.3f} for proportions test\")\n```\n\n## Pre-registration Concept\n\nPre-registration prevents HARKing (Hypothesizing After Results are Known) and distinguishes confirmatory from exploratory analyses.\n\n### Define Analysis Before Data\n\n```python\nprint(\"[DECISION] Pre-registering analysis plan before examining data\")\n\n# Document your analysis plan BEFORE looking at the data\npreregistration = {\n    \"primary_hypothesis\": \"H1: Treatment group shows higher conversion rate than control\",\n    \"null_hypothesis\": \"H0: No difference in conversion rates between groups\",\n    \"primary_endpoint\": \"conversion_rate\",\n    \"secondary_endpoints\": [\"time_to_convert\", \"revenue_per_user\"],\n    \"alpha\": 0.05,\n    \"correction_method\": \"Bonferroni for secondary endpoints\",\n    \"minimum_effect_size\": \"5 percentage points (10% ‚Üí 15%)\",\n    \"planned_sample_size\": 500,\n    \"analysis_method\": \"Two-proportion z-test\",\n    \"exclusion_criteria\": \"Users with < 1 day exposure\"\n}\n\nprint(f\"[EXPERIMENT] Pre-registered analysis plan:\")\nfor key, value in preregistration.items():\n    print(f\"  {key}: {value}\")\n```\n\n### Confirmatory vs Exploratory Findings\n\n```python\n# After analysis, clearly label findings\nprint(\"[FINDING] Treatment increases conversion by 4.2pp (95% CI: [1.8, 6.6])\")\nprint(\"[STAT:ci] 95% CI [1.8, 6.6]\")\nprint(\"[STAT:effect_size] Cohen's h = 0.12 (small)\")\nprint(\"[STAT:p_value] p = 0.001\")\n# This is CONFIRMATORY because it tests pre-registered hypothesis\n\nprint(\"[OBSERVATION] Exploratory: Effect stronger for mobile users (+7.1pp)\")\nprint(\"[LIMITATION] Mobile subgroup analysis was NOT pre-registered\")\nprint(\"[DECISION] Flagging as exploratory - requires replication before action\")\n\n# Document finding type\nCONFIRMATORY = True  # Pre-registered hypothesis\nEXPLORATORY = False  # Post-hoc discovery\n\ndef label_finding(finding: str, is_confirmatory: bool):\n    \"\"\"Label findings appropriately based on pre-registration status.\"\"\"\n    if is_confirmatory:\n        print(f\"[FINDING] CONFIRMATORY: {finding}\")\n    else:\n        print(f\"[OBSERVATION] EXPLORATORY: {finding}\")\n        print(\"[LIMITATION] This finding was not pre-registered and requires replication\")\n```\n\n### Document Deviations from Plan\n\n```python\n# When you deviate from the pre-registered plan, document it!\nprint(\"[DECISION] DEVIATION FROM PRE-REGISTRATION:\")\nprint(\"  Original plan: Two-proportion z-test\")\nprint(\"  Actual analysis: Fisher's exact test\")\nprint(\"  Reason: Cell counts < 5 in contingency table\")\nprint(\"  Impact: More conservative, may reduce power\")\n\n# Keep a deviation log\ndeviations = [\n    {\n        \"item\": \"Statistical test\",\n        \"planned\": \"z-test\",\n        \"actual\": \"Fisher's exact\",\n        \"reason\": \"Low expected cell counts\",\n        \"impact\": \"Minimal - Fisher's is more conservative\"\n    },\n    {\n        \"item\": \"Sample size\",\n        \"planned\": 500,\n        \"actual\": 487,\n        \"reason\": \"13 users excluded due to technical issues\",\n        \"impact\": \"Power reduced from 80% to 78%\"\n    }\n]\n\nprint(\"[EXPERIMENT] Deviation log:\")\nfor d in deviations:\n    print(f\"  - {d['item']}: {d['planned']} ‚Üí {d['actual']} ({d['reason']})\")\n```\n\n## Stopping Rules\n\nDefine stopping criteria upfront to prevent p-hacking through optional stopping.\n\n### Define Success/Failure Criteria Upfront\n\n```python\nprint(\"[DECISION] Defining stopping rules BEFORE experiment starts\")\n\nstopping_rules = {\n    \"success_criterion\": \"Lower 95% CI bound > 0 (effect is positive)\",\n    \"failure_criterion\": \"Upper 95% CI bound < minimum_effect (effect too small)\",\n    \"futility_criterion\": \"Posterior probability of success < 5%\",\n    \"max_sample_size\": 1000,\n    \"interim_analyses\": [250, 500, 750],  # Pre-specified checkpoints\n    \"alpha_spending\": \"O'Brien-Fleming\"  # Preserve overall alpha\n}\n\nprint(f\"[EXPERIMENT] Stopping rules defined:\")\nfor key, value in stopping_rules.items():\n    print(f\"  {key}: {value}\")\n```\n\n### Avoid P-Hacking Through Optional Stopping\n\n```python\n# BAD: Looking at p-value repeatedly and stopping when significant\n# This inflates false positive rate!\n\n# GOOD: Use alpha-spending functions to control Type I error\n\nfrom scipy import stats\nimport numpy as np\n\ndef obrien_fleming_boundary(alpha: float, n_looks: int, current_look: int) -> float:\n    \"\"\"\n    Calculate O'Brien-Fleming spending boundary.\n    More conservative early, less conservative late.\n    \"\"\"\n    # Information fraction\n    t = current_look / n_looks\n    # O'Brien-Fleming boundary\n    z_boundary = stats.norm.ppf(1 - alpha/2) / np.sqrt(t)\n    p_boundary = 2 * (1 - stats.norm.cdf(z_boundary))\n    return p_boundary\n\nn_looks = 4  # Number of interim analyses\nalpha = 0.05  # Overall significance level\n\nprint(\"[DECISION] Using O'Brien-Fleming alpha-spending to control Type I error\")\nprint(\"[EXPERIMENT] Adjusted significance thresholds:\")\nfor look in range(1, n_looks + 1):\n    boundary = obrien_fleming_boundary(alpha, n_looks, look)\n    print(f\"  Look {look}/{n_looks}: p < {boundary:.5f} to stop for efficacy\")\n\nprint(\"[LIMITATION] Stopping early requires more extreme evidence\")\n```\n\n### Sequential Analysis Methods (SPRT)\n\n```python\n# Sequential Probability Ratio Test (SPRT)\n# Allows continuous monitoring with controlled error rates\n\ndef sprt_bounds(alpha: float, beta: float) -> tuple:\n    \"\"\"\n    Calculate SPRT decision boundaries.\n    \n    Args:\n        alpha: Type I error rate (false positive)\n        beta: Type II error rate (false negative)\n    \n    Returns:\n        (lower_bound, upper_bound) for log-likelihood ratio\n    \"\"\"\n    A = np.log((1 - beta) / alpha)  # Upper boundary (accept H1)\n    B = np.log(beta / (1 - alpha))  # Lower boundary (accept H0)\n    return B, A\n\nalpha, beta = 0.05, 0.20  # 5% false positive, 20% false negative (80% power)\nlower, upper = sprt_bounds(alpha, beta)\n\nprint(\"[DECISION] Using Sequential Probability Ratio Test (SPRT)\")\nprint(f\"[STAT:estimate] Stop for H0 if LLR < {lower:.3f}\")\nprint(f\"[STAT:estimate] Stop for H1 if LLR > {upper:.3f}\")\nprint(\"[EXPERIMENT] Continue sampling if {:.3f} < LLR < {:.3f}\".format(lower, upper))\n\n# Example SPRT monitoring\ndef monitor_sprt(successes: int, trials: int, p0: float, p1: float, bounds: tuple):\n    \"\"\"Monitor SPRT decision status.\"\"\"\n    lower, upper = bounds\n    # Log-likelihood ratio\n    if successes == 0 or successes == trials:\n        llr = 0  # Avoid log(0)\n    else:\n        p_hat = successes / trials\n        llr = successes * np.log(p1/p0) + (trials - successes) * np.log((1-p1)/(1-p0))\n    \n    if llr > upper:\n        return \"STOP: Accept H1 (treatment effective)\", llr\n    elif llr < lower:\n        return \"STOP: Accept H0 (no effect)\", llr\n    else:\n        return \"CONTINUE: Need more data\", llr\n\n# Example: Testing if conversion rate > 10% vs = 10%\np_null, p_alt = 0.10, 0.15\nstatus, llr = monitor_sprt(successes=45, trials=350, p0=p_null, p1=p_alt, bounds=(lower, upper))\nprint(f\"[STAT:estimate] Current LLR: {llr:.3f}\")\nprint(f\"[DECISION] {status}\")\n```\n\n### Document Stopping Decision\n\n```python\n# When you stop an experiment, document the decision clearly\nprint(\"[DECISION] Experiment stopped at interim analysis 2/4\")\nprint(\"[STAT:estimate] Current effect: 5.2pp (95% CI: [2.1, 8.3])\")\nprint(\"[STAT:p_value] p = 0.0012 (< O'Brien-Fleming boundary 0.005)\")\nprint(\"[EXPERIMENT] Decision: STOP FOR EFFICACY\")\nprint(\"[FINDING] Treatment significantly improves conversion (confirmed at interim)\")\nprint(\"[LIMITATION] Final sample (n=500) smaller than planned (n=1000)\")\nprint(\"[LIMITATION] Effect estimate may regress toward null with more data\")\n```\n\n## Documentation Pattern\n```python\nprint(\"[DECISION] Chose Random Forest over XGBoost because:\")\nprint(\"  - Better interpretability for stakeholders\")\nprint(\"  - Comparable performance (within 1% accuracy)\")\nprint(\"  - Faster training time for iteration\")\n\nprint(\"[LIMITATION] Model may not generalize to:\")\nprint(\"  - Data from different time periods\")\nprint(\"  - Users from different demographics\")\n```\n",
        "src/skill/ml-rigor/SKILL.md": "---\nname: ml-rigor\ndescription: Enforces baseline comparisons, cross-validation, interpretation, and leakage prevention for ML pipelines\n---\n\n# Machine Learning Rigor Patterns\n\n## When to Use\n\nLoad this skill when building machine learning models. Every ML pipeline must demonstrate:\n- **Baseline comparison**: Beat a dummy model before claiming success\n- **Cross-validation**: Report variance, not just a single score\n- **Interpretation**: Explain what the model learned\n- **Leakage prevention**: Ensure no future information leaks into training\n\n**Quality Gate**: ML findings without baseline comparison or cross-validation are marked as \"Exploratory\" in reports.\n\n---\n\n## 1. Baseline Requirements\n\n**Every model must be compared to baselines.** A model that can't beat a dummy classifier isn't learning anything useful.\n\n### Always Compare To:\n1. **DummyClassifier/DummyRegressor** - The absolute minimum bar\n2. **Simple linear model** - LogisticRegression or LinearRegression\n3. **Domain heuristic** (if available) - Rule-based approach\n\n### Baseline Code Template\n\n```python\nfrom sklearn.dummy import DummyClassifier, DummyRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nprint(\"[DECISION] Establishing baselines before training complex models\")\n\n# Classification baselines\ndummy_clf = DummyClassifier(strategy='most_frequent')\ndummy_scores = cross_val_score(dummy_clf, X_train, y_train, cv=5, scoring='accuracy')\nprint(f\"[METRIC:baseline_accuracy] {dummy_scores.mean():.3f} (majority class)\")\nprint(f\"[METRIC:baseline_accuracy_std] {dummy_scores.std():.3f}\")\n\n# Simple linear baseline\nlr = LogisticRegression(max_iter=1000, random_state=42)\nlr_scores = cross_val_score(lr, X_train, y_train, cv=5, scoring='accuracy')\nprint(f\"[METRIC:linear_baseline_accuracy] {lr_scores.mean():.3f}\")\nprint(f\"[METRIC:linear_baseline_accuracy_std] {lr_scores.std():.3f}\")\n\n# For regression tasks\ndummy_reg = DummyRegressor(strategy='mean')\ndummy_rmse = cross_val_score(dummy_reg, X_train, y_train, cv=5, \n                              scoring='neg_root_mean_squared_error')\nprint(f\"[METRIC:baseline_rmse] {-dummy_rmse.mean():.3f} (mean predictor)\")\n```\n\n### Improvement Over Baseline with CI\n\n```python\nfrom scipy import stats\n\n# Calculate improvement with confidence interval\nmodel_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\nimprovement = model_scores.mean() - dummy_scores.mean()\n\n# Bootstrap CI for improvement\nn_bootstrap = 1000\nimprovements = []\nfor _ in range(n_bootstrap):\n    idx = np.random.choice(len(model_scores), len(model_scores), replace=True)\n    boot_improvement = model_scores[idx].mean() - dummy_scores[idx].mean()\n    improvements.append(boot_improvement)\n\nci_low, ci_high = np.percentile(improvements, [2.5, 97.5])\n\nprint(f\"[METRIC:improvement_over_baseline] {improvement:.3f}\")\nprint(f\"[STAT:ci] 95% CI [{ci_low:.3f}, {ci_high:.3f}]\")\nprint(f\"[STAT:effect_size] Relative improvement: {improvement/dummy_scores.mean()*100:.1f}%\")\n\nif ci_low > 0:\n    print(\"[FINDING] Model significantly outperforms baseline\")\nelse:\n    print(\"[LIMITATION] Improvement over baseline not statistically significant\")\n```\n\n---\n\n## 2. Cross-Validation Requirements\n\n**Never report a single train/test split.** Cross-validation shows how much your score varies.\n\n### Requirements:\n- Use **stratified K-fold** for classification (preserves class distribution)\n- Report **mean +/- std**, not just mean\n- Calculate **confidence interval** for mean performance\n- Use **repeated CV** for small datasets\n\n### Cross-Validation Code Template\n\n```python\nfrom sklearn.model_selection import (\n    StratifiedKFold, cross_val_score, cross_validate,\n    RepeatedStratifiedKFold\n)\nimport numpy as np\nfrom scipy import stats\n\nprint(\"[DECISION] Using 5-fold stratified CV to estimate model performance\")\n\n# Stratified K-Fold for classification\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Multiple metrics\nscoring = ['accuracy', 'f1_weighted', 'roc_auc']\ncv_results = cross_validate(model, X, y, cv=cv, scoring=scoring, \n                            return_train_score=True)\n\n# Report mean +/- std (REQUIRED)\nfor metric in scoring:\n    test_scores = cv_results[f'test_{metric}']\n    train_scores = cv_results[f'train_{metric}']\n    \n    print(f\"[METRIC:cv_{metric}_mean] {test_scores.mean():.3f}\")\n    print(f\"[METRIC:cv_{metric}_std] {test_scores.std():.3f}\")\n    \n    # Check for overfitting\n    gap = train_scores.mean() - test_scores.mean()\n    if gap > 0.1:\n        print(f\"[LIMITATION] Train-test gap of {gap:.3f} suggests overfitting\")\n```\n\n### Confidence Interval for CV Mean\n\n```python\n# CI for cross-validation mean (t-distribution for small n)\ndef cv_confidence_interval(scores, confidence=0.95):\n    n = len(scores)\n    mean = scores.mean()\n    se = stats.sem(scores)\n    h = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n    return mean - h, mean + h\n\nci_low, ci_high = cv_confidence_interval(cv_results['test_accuracy'])\nprint(f\"[STAT:ci] 95% CI for accuracy: [{ci_low:.3f}, {ci_high:.3f}]\")\n\n# For small datasets, use repeated CV\nprint(\"[DECISION] Using repeated CV for more stable estimates\")\nrcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\nscores = cross_val_score(model, X, y, cv=rcv, scoring='accuracy')\nprint(f\"[METRIC:repeated_cv_mean] {scores.mean():.3f}\")\nprint(f\"[METRIC:repeated_cv_std] {scores.std():.3f}\")\n```\n\n---\n\n## 3. Hyperparameter Tuning\n\n**Avoid overfitting to the validation set.** Report the distribution of scores, not just the best.\n\n### Requirements:\n- Use **RandomizedSearchCV** or **Bayesian optimization** (Optuna)\n- Report **distribution of scores** across parameter combinations\n- Use **nested CV** to get unbiased performance estimate\n- Watch for **overfitting to validation set**\n\n### RandomizedSearchCV Template\n\n```python\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score\nfrom scipy.stats import uniform, randint\nimport numpy as np\n\nprint(\"[DECISION] Using RandomizedSearchCV with 100 iterations\")\n\n# Define parameter distributions (not just lists!)\nparam_distributions = {\n    'n_estimators': randint(50, 500),\n    'max_depth': randint(3, 20),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10),\n    'max_features': uniform(0.1, 0.9)\n}\n\nrandom_search = RandomizedSearchCV(\n    model, param_distributions,\n    n_iter=100,\n    cv=5,\n    scoring='accuracy',\n    random_state=42,\n    return_train_score=True,\n    n_jobs=-1\n)\n\nrandom_search.fit(X_train, y_train)\n\n# Report distribution of scores (not just best!)\ncv_results = random_search.cv_results_\nall_test_scores = cv_results['mean_test_score']\nall_train_scores = cv_results['mean_train_score']\n\nprint(f\"[METRIC:tuning_best_score] {random_search.best_score_:.3f}\")\nprint(f\"[METRIC:tuning_score_range] [{all_test_scores.min():.3f}, {all_test_scores.max():.3f}]\")\nprint(f\"[METRIC:tuning_score_std] {all_test_scores.std():.3f}\")\n\n# Check for overfitting during tuning\nbest_idx = random_search.best_index_\ntrain_test_gap = all_train_scores[best_idx] - all_test_scores[best_idx]\nif train_test_gap > 0.1:\n    print(f\"[LIMITATION] Best model shows {train_test_gap:.3f} train-test gap\")\n```\n\n### Nested Cross-Validation (Unbiased Estimate)\n\n```python\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\nprint(\"[DECISION] Using nested CV for unbiased performance estimate\")\n\n# Outer CV for performance estimation\nouter_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Inner CV is handled by RandomizedSearchCV\ninner_search = RandomizedSearchCV(\n    model, param_distributions,\n    n_iter=50,\n    cv=3,  # Inner CV\n    scoring='accuracy',\n    random_state=42,\n    n_jobs=-1\n)\n\n# Nested CV scores\nnested_scores = cross_val_score(inner_search, X, y, cv=outer_cv, scoring='accuracy')\n\nprint(f\"[METRIC:nested_cv_mean] {nested_scores.mean():.3f}\")\nprint(f\"[METRIC:nested_cv_std] {nested_scores.std():.3f}\")\nprint(f\"[STAT:ci] 95% CI [{nested_scores.mean() - 1.96*nested_scores.std()/np.sqrt(5):.3f}, \"\n      f\"{nested_scores.mean() + 1.96*nested_scores.std()/np.sqrt(5):.3f}]\")\n```\n\n---\n\n## 4. Calibration Requirements\n\n**Probability predictions must be calibrated.** A 70% confidence should be correct 70% of the time.\n\n### Requirements:\n- Check **calibration curve** for probability predictions\n- Report **Brier score** (lower is better)\n- Apply **calibration** if needed (Platt scaling, isotonic regression)\n\n### Calibration Code Template\n\n```python\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nfrom sklearn.metrics import brier_score_loss\nimport matplotlib.pyplot as plt\n\nprint(\"[DECISION] Checking probability calibration\")\n\n# Get probability predictions\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Brier score (0 = perfect, 1 = worst)\nbrier = brier_score_loss(y_test, y_prob)\nprint(f\"[METRIC:brier_score] {brier:.4f}\")\n\n# Calibration curve\nprob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n\n# Plot calibration curve\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\nax.plot(prob_pred, prob_true, 's-', label=f'Model (Brier={brier:.3f})')\nax.set_xlabel('Mean predicted probability')\nax.set_ylabel('Fraction of positives')\nax.set_title('Calibration Curve')\nax.legend()\nplt.savefig('figures/calibration_curve.png', dpi=150, bbox_inches='tight')\nprint(\"[ARTIFACT:figure] figures/calibration_curve.png\")\n\n# Check calibration quality\nmax_calibration_error = np.max(np.abs(prob_true - prob_pred))\nprint(f\"[METRIC:max_calibration_error] {max_calibration_error:.3f}\")\n\nif max_calibration_error > 0.1:\n    print(\"[LIMITATION] Model probabilities are poorly calibrated\")\n```\n\n### Apply Calibration\n\n```python\nprint(\"[DECISION] Applying isotonic calibration to improve probability estimates\")\n\n# Calibrate using held-out data\ncalibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=5)\ncalibrated_model.fit(X_train, y_train)\n\n# Compare before/after\ny_prob_calibrated = calibrated_model.predict_proba(X_test)[:, 1]\nbrier_calibrated = brier_score_loss(y_test, y_prob_calibrated)\n\nprint(f\"[METRIC:brier_before_calibration] {brier:.4f}\")\nprint(f\"[METRIC:brier_after_calibration] {brier_calibrated:.4f}\")\nprint(f\"[METRIC:calibration_improvement] {brier - brier_calibrated:.4f}\")\n```\n\n---\n\n## 5. Interpretation Requirements\n\n**Explain what the model learned.** Black boxes are not acceptable for important decisions.\n\n### Requirements:\n- Compute **permutation importance** or **SHAP values**\n- Show at least one **case study** (why this specific prediction?)\n- Verify features make **domain sense**\n\n### Permutation Importance Template\n\n```python\nfrom sklearn.inspection import permutation_importance\nimport pandas as pd\n\nprint(\"[DECISION] Computing permutation importance on test set\")\n\n# Permutation importance (more reliable than built-in feature_importances_)\nperm_importance = permutation_importance(\n    model, X_test, y_test, \n    n_repeats=30, \n    random_state=42,\n    n_jobs=-1\n)\n\n# Create importance DataFrame\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance_mean': perm_importance.importances_mean,\n    'importance_std': perm_importance.importances_std\n}).sort_values('importance_mean', ascending=False)\n\n# Report top features\nprint(\"[METRIC:top_features]\")\nfor i, row in importance_df.head(5).iterrows():\n    print(f\"  {row['feature']}: {row['importance_mean']:.4f} (+/- {row['importance_std']:.4f})\")\n\n# Check for unexpected features\nprint(\"[CHECK:domain_sense] Verify top features align with domain knowledge\")\n```\n\n### SHAP Values Template\n\n```python\nimport shap\n\nprint(\"[DECISION] Using SHAP for model interpretation\")\n\n# Create explainer\nexplainer = shap.TreeExplainer(model)  # or shap.KernelExplainer for any model\nshap_values = explainer.shap_values(X_test)\n\n# Global importance\nshap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)\nplt.savefig('figures/shap_summary.png', dpi=150, bbox_inches='tight')\nprint(\"[ARTIFACT:figure] figures/shap_summary.png\")\n\n# Mean absolute SHAP values\nif isinstance(shap_values, list):  # Multi-class\n    shap_importance = np.abs(shap_values[1]).mean(axis=0)\nelse:\n    shap_importance = np.abs(shap_values).mean(axis=0)\n\ntop_features = sorted(zip(feature_names, shap_importance), \n                     key=lambda x: x[1], reverse=True)[:5]\nprint(\"[METRIC:shap_top_features]\")\nfor feat, imp in top_features:\n    print(f\"  {feat}: {imp:.4f}\")\n```\n\n### Case Study (Individual Prediction)\n\n```python\nprint(\"[DECISION] Analyzing individual prediction for interpretability\")\n\n# Select an interesting case (e.g., high confidence wrong prediction)\ny_pred_proba = model.predict_proba(X_test)[:, 1]\ny_pred = model.predict(X_test)\nmistakes = (y_pred != y_test)\nhigh_conf_mistakes = mistakes & (np.abs(y_pred_proba - 0.5) > 0.4)\n\nif high_conf_mistakes.any():\n    case_idx = np.where(high_conf_mistakes)[0][0]\n    print(f\"[ANALYSIS] Case study: High-confidence mistake (index {case_idx})\")\n    print(f\"  Predicted: {y_pred[case_idx]} (prob={y_pred_proba[case_idx]:.3f})\")\n    print(f\"  Actual: {y_test.iloc[case_idx]}\")\n    \n    # SHAP for this case\n    shap.force_plot(explainer.expected_value[1] if isinstance(explainer.expected_value, list) \n                    else explainer.expected_value,\n                    shap_values[case_idx] if not isinstance(shap_values, list) \n                    else shap_values[1][case_idx],\n                    X_test.iloc[case_idx],\n                    matplotlib=True, show=False)\n    plt.savefig('figures/case_study_shap.png', dpi=150, bbox_inches='tight')\n    print(\"[ARTIFACT:figure] figures/case_study_shap.png\")\n    print(\"[LIMITATION] Model fails on cases with pattern: [describe pattern]\")\n```\n\n---\n\n## 6. Error Analysis\n\n**Understand where and why the model fails.** Aggregate metrics hide important failure modes.\n\n### Requirements:\n- **Slice performance** by key segments (demographics, time periods, etc.)\n- Analyze **failure modes** (false positives vs false negatives)\n- Check for **systematic errors** (biases, subgroup issues)\n\n### Error Analysis Code Template\n\n```python\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport pandas as pd\nimport numpy as np\n\nprint(\"[DECISION] Performing error analysis across segments\")\n\ny_pred = model.predict(X_test)\n\n# Overall confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"[METRIC:confusion_matrix]\")\nprint(cm)\n\n# Classification report\nprint(\"[ANALYSIS] Classification report:\")\nprint(classification_report(y_test, y_pred, target_names=class_names))\n```\n\n### Slice Performance Analysis\n\n```python\nprint(\"[DECISION] Analyzing performance by key segments\")\n\n# Create test DataFrame with predictions\ntest_df = X_test.copy()\ntest_df['y_true'] = y_test.values\ntest_df['y_pred'] = y_pred\n\n# Define segments to analyze\nsegments = {\n    'age_group': pd.cut(test_df['age'], bins=[0, 30, 50, 100], labels=['young', 'middle', 'senior']),\n    'income_tier': pd.qcut(test_df['income'], q=3, labels=['low', 'medium', 'high'])\n}\n\nprint(\"[METRIC:slice_performance]\")\nfor segment_name, segment_values in segments.items():\n    test_df['segment'] = segment_values\n    \n    for segment_val in segment_values.unique():\n        mask = test_df['segment'] == segment_val\n        if mask.sum() > 10:  # Only report if enough samples\n            segment_accuracy = (test_df.loc[mask, 'y_true'] == test_df.loc[mask, 'y_pred']).mean()\n            print(f\"  {segment_name}={segment_val}: accuracy={segment_accuracy:.3f} (n={mask.sum()})\")\n            \n            # Check for underperformance\n            if segment_accuracy < overall_accuracy - 0.1:\n                print(f\"  [LIMITATION] Model underperforms on {segment_name}={segment_val}\")\n```\n\n### Failure Mode Analysis\n\n```python\nprint(\"[DECISION] Analyzing failure modes\")\n\n# Separate false positives and false negatives\nfp_mask = (y_pred == 1) & (y_test == 0)\nfn_mask = (y_pred == 0) & (y_test == 1)\n\nprint(f\"[METRIC:false_positive_rate] {fp_mask.mean():.3f}\")\nprint(f\"[METRIC:false_negative_rate] {fn_mask.mean():.3f}\")\n\n# Analyze characteristics of errors\nif fp_mask.sum() > 0:\n    print(\"[ANALYSIS] False positive characteristics:\")\n    fp_data = X_test[fp_mask]\n    for col in feature_names[:5]:  # Top features\n        print(f\"  {col}: mean={fp_data[col].mean():.3f} vs overall={X_test[col].mean():.3f}\")\n\n# Check for systematic bias\nprint(\"[CHECK:systematic_error] Review if errors correlate with protected attributes\")\n```\n\n---\n\n## 7. Leakage Checklist\n\n**Data leakage silently destroys model validity.** Check these BEFORE trusting any results.\n\n### Checklist:\n- [ ] **Time-based splits** for temporal data (no future information)\n- [ ] **No target information** in features (derived features, proxies)\n- [ ] **Preprocessing inside CV** loop (no fit on test data)\n- [ ] **Group-aware splits** if samples are related (same user, same session)\n\n### Time-Based Split Template\n\n```python\nfrom sklearn.model_selection import TimeSeriesSplit\n\nprint(\"[DECISION] Using time-based split for temporal data\")\n\n# Check if data has temporal component\nif 'date' in df.columns or 'timestamp' in df.columns:\n    print(\"[CHECK:temporal_leakage] Using TimeSeriesSplit to prevent future information leak\")\n    \n    # Sort by time\n    df_sorted = df.sort_values('date')\n    \n    # Time-based cross-validation\n    tscv = TimeSeriesSplit(n_splits=5)\n    \n    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_sorted)):\n        train_max_date = df_sorted.iloc[train_idx]['date'].max()\n        test_min_date = df_sorted.iloc[test_idx]['date'].min()\n        \n        if train_max_date >= test_min_date:\n            print(f\"[ERROR] Fold {fold}: Data leakage detected!\")\n        else:\n            print(f\"[CHECK:fold_{fold}] Train ends {train_max_date}, Test starts {test_min_date} - OK\")\nelse:\n    print(\"[LIMITATION] No temporal column found - using random split\")\n```\n\n### Target Leakage Detection\n\n```python\nprint(\"[CHECK:target_leakage] Checking for target information in features\")\n\n# Check correlation between features and target\ncorrelations = X_train.corrwith(pd.Series(y_train, index=X_train.index))\nhigh_corr_features = correlations[correlations.abs() > 0.9].index.tolist()\n\nif high_corr_features:\n    print(f\"[WARNING] Suspiciously high correlations with target:\")\n    for feat in high_corr_features:\n        print(f\"  {feat}: r={correlations[feat]:.3f}\")\n    print(\"[LIMITATION] Review these features for potential target leakage\")\nelse:\n    print(\"[CHECK:target_leakage] No obvious target leakage detected\")\n\n# Check for post-hoc features (created after the outcome)\nprint(\"[CHECK:feature_timing] Verify all features are available at prediction time\")\n```\n\n### Preprocessing Inside CV Template\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nprint(\"[DECISION] Using Pipeline to prevent preprocessing leakage\")\n\n# WRONG: Fitting scaler on all data before CV\n# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(X)  # LEAKAGE!\n# scores = cross_val_score(model, X_scaled, y, cv=5)\n\n# CORRECT: Preprocessing inside pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', model)\n])\n\nscores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\nprint(f\"[METRIC:cv_accuracy_mean] {scores.mean():.3f}\")\nprint(f\"[METRIC:cv_accuracy_std] {scores.std():.3f}\")\nprint(\"[CHECK:preprocessing_leakage] Scaler fit inside CV - no leakage\")\n```\n\n### Group-Aware Splitting\n\n```python\nfrom sklearn.model_selection import GroupKFold\n\nprint(\"[CHECK:group_leakage] Checking if samples are related\")\n\n# If samples belong to groups (e.g., multiple records per user)\nif 'user_id' in df.columns:\n    print(\"[DECISION] Using GroupKFold to prevent group leakage\")\n    \n    groups = df['user_id']\n    gkf = GroupKFold(n_splits=5)\n    \n    scores = cross_val_score(model, X, y, cv=gkf, groups=groups, scoring='accuracy')\n    print(f\"[METRIC:group_cv_accuracy_mean] {scores.mean():.3f}\")\n    print(f\"[METRIC:group_cv_accuracy_std] {scores.std():.3f}\")\n    print(\"[CHECK:group_leakage] Same user never in both train and test - OK\")\nelse:\n    print(\"[CHECK:group_leakage] No group column - using standard CV\")\n```\n\n---\n\n## Complete ML Pipeline Example\n\nPutting it all together:\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nprint(\"[OBJECTIVE] Build and validate classification model with full rigor\")\n\n# 1. BASELINE\nprint(\"\\n--- Baseline Comparison ---\")\ndummy = DummyClassifier(strategy='most_frequent')\ndummy_scores = cross_val_score(dummy, X, y, cv=5, scoring='accuracy')\nprint(f\"[METRIC:baseline_accuracy] {dummy_scores.mean():.3f}\")\n\n# 2. CROSS-VALIDATION\nprint(\"\\n--- Cross-Validation ---\")\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', RandomForestClassifier(random_state=42))\n])\ncv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\nprint(f\"[METRIC:cv_accuracy_mean] {cv_scores.mean():.3f}\")\nprint(f\"[METRIC:cv_accuracy_std] {cv_scores.std():.3f}\")\n\n# 3. IMPROVEMENT OVER BASELINE\nimprovement = cv_scores.mean() - dummy_scores.mean()\nprint(f\"[METRIC:improvement_over_baseline] {improvement:.3f}\")\nprint(f\"[STAT:effect_size] Relative improvement: {improvement/dummy_scores.mean()*100:.1f}%\")\n\n# 4. INTERPRETATION\nprint(\"\\n--- Interpretation ---\")\n# (permutation importance code here)\n\n# 5. FINDING (only after full evidence)\nif improvement > 0.05:\n    print(f\"[FINDING] Random Forest achieves {cv_scores.mean():.3f} accuracy, \"\n          f\"improving {improvement:.3f} over baseline ({improvement/dummy_scores.mean()*100:.1f}% relative)\")\n    print(f\"[SO_WHAT] Model provides actionable predictions for business use case\")\nelse:\n    print(\"[FINDING] Model does not significantly outperform baseline\")\n    print(\"[LIMITATION] Consider simpler approach or feature engineering\")\n```\n\n---\n\n## Quality Gate: Required Evidence\n\nBefore any `[FINDING]` in ML, you MUST have:\n\n| Evidence | Marker | Example |\n|----------|--------|---------|\n| Baseline comparison | `[METRIC:baseline_*]` | `[METRIC:baseline_accuracy] 0.65` |\n| CV scores with variance | `[METRIC:cv_*_mean/std]` | `[METRIC:cv_accuracy_mean] 0.85` |\n| Improvement quantified | `[METRIC:improvement_*]` | `[METRIC:improvement_over_baseline] 0.20` |\n| Effect size | `[STAT:effect_size]` | `[STAT:effect_size] 31% relative improvement` |\n| Interpretation | `[METRIC:top_features]` | Top 3 features listed |\n| Limitations acknowledged | `[LIMITATION]` | Model constraints documented |\n\n**Missing any of these? Your finding is \"Exploratory\", not \"Verified\".**\n"
      },
      "plugins": [
        {
          "name": "gyoshu",
          "description": "Scientific research automation with Python REPL, Jupyter notebooks, and multi-agent research workflows. Includes 6 agents, 2 commands, 10 tools, and 3 skills for data science and machine learning research.",
          "version": "0.4.33",
          "author": {
            "name": "Yeachan Heo",
            "email": "hurrc04@gmail.com"
          },
          "source": "./",
          "category": "productivity",
          "homepage": "https://github.com/Yeachan-Heo/My-Jogyo",
          "tags": [
            "research",
            "jupyter",
            "python",
            "data-science",
            "machine-learning",
            "scientific",
            "repl",
            "notebook",
            "automation"
          ],
          "categories": [
            "automation",
            "data-science",
            "jupyter",
            "machine-learning",
            "notebook",
            "productivity",
            "python",
            "repl",
            "research",
            "scientific"
          ],
          "install_commands": [
            "/plugin marketplace add Yeachan-Heo/My-Jogyo",
            "/plugin install gyoshu@gyoshu"
          ]
        }
      ]
    }
  ]
}