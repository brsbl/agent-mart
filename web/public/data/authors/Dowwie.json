{
  "author": {
    "id": "Dowwie",
    "display_name": "Darin Gordon",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/2601236?u=39694f69b26ca428d9f6687a482393a53814acb0&v=4",
    "url": "https://github.com/Dowwie",
    "bio": "\"Do not go where the path may lead, go instead where there is no path and leave a trail\". -- R.W. Emerson",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 8,
      "total_skills": 6,
      "total_stars": 8,
      "total_forks": 1
    }
  },
  "marketplaces": [
    {
      "name": "tasker-marketplace",
      "version": null,
      "description": "Local marketplace for tasker plugin development",
      "owner_info": {
        "name": "Dowwie"
      },
      "keywords": [],
      "repo_full_name": "Dowwie/tasker",
      "repo_url": "https://github.com/Dowwie/tasker",
      "repo_description": "Agentic Development Framework using Spec-driven Planning and Execution with Claude Code",
      "homepage": "",
      "signals": {
        "stars": 8,
        "forks": 1,
        "pushed_at": "2026-01-29T18:15:20Z",
        "created_at": "2025-11-25T21:50:51Z",
        "license": "Apache-2.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 469
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 443
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 1846
        },
        {
          "path": "agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "agents/logic-architect.md",
          "type": "blob",
          "size": 9426
        },
        {
          "path": "agents/physical-architect.md",
          "type": "blob",
          "size": 4927
        },
        {
          "path": "agents/plan-auditor.md",
          "type": "blob",
          "size": 2809
        },
        {
          "path": "agents/spec-reviewer.md",
          "type": "blob",
          "size": 11634
        },
        {
          "path": "agents/task-author.md",
          "type": "blob",
          "size": 5611
        },
        {
          "path": "agents/task-executor.md",
          "type": "blob",
          "size": 11099
        },
        {
          "path": "agents/task-plan-verifier.md",
          "type": "blob",
          "size": 23649
        },
        {
          "path": "agents/task-verifier.md",
          "type": "blob",
          "size": 7595
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/execute.md",
          "type": "blob",
          "size": 377
        },
        {
          "path": "commands/plan.md",
          "type": "blob",
          "size": 437
        },
        {
          "path": "commands/specify.md",
          "type": "blob",
          "size": 190
        },
        {
          "path": "hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/close-tui.sh",
          "type": "blob",
          "size": 500
        },
        {
          "path": "hooks/detect-workflow.sh",
          "type": "blob",
          "size": 1465
        },
        {
          "path": "hooks/hooks.json",
          "type": "blob",
          "size": 782
        },
        {
          "path": "hooks/launch-tui.sh",
          "type": "blob",
          "size": 1294
        },
        {
          "path": "hooks/post-task-commit.sh",
          "type": "blob",
          "size": 4139
        },
        {
          "path": "hooks/subagent_stop.sh",
          "type": "blob",
          "size": 106
        },
        {
          "path": "internal",
          "type": "tree",
          "size": null
        },
        {
          "path": "internal/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "internal/commands/executor-status.md",
          "type": "blob",
          "size": 285
        },
        {
          "path": "internal/commands/status.md",
          "type": "blob",
          "size": 648
        },
        {
          "path": "internal/commands/tasker-to-beads.md",
          "type": "blob",
          "size": 2365
        },
        {
          "path": "internal/commands/tui.md",
          "type": "blob",
          "size": 662
        },
        {
          "path": "internal/commands/verify-plan.md",
          "type": "blob",
          "size": 748
        },
        {
          "path": "internal/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "internal/skills/_archived",
          "type": "tree",
          "size": null
        },
        {
          "path": "internal/skills/_archived/orchestrator",
          "type": "tree",
          "size": null
        },
        {
          "path": "internal/skills/_archived/orchestrator/SKILL.md",
          "type": "blob",
          "size": 58103
        },
        {
          "path": "internal/skills/runtime-logger",
          "type": "tree",
          "size": null
        },
        {
          "path": "internal/skills/runtime-logger/SKILL.md",
          "type": "blob",
          "size": 4094
        },
        {
          "path": "internal/skills/tasker-to-beads",
          "type": "tree",
          "size": null
        },
        {
          "path": "internal/skills/tasker-to-beads/SKILL.md",
          "type": "blob",
          "size": 11200
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/execute",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/execute/SKILL.md",
          "type": "blob",
          "size": 14699
        },
        {
          "path": "skills/plan",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/plan/SKILL.md",
          "type": "blob",
          "size": 16721
        },
        {
          "path": "skills/specify",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/specify/SKILL.md",
          "type": "blob",
          "size": 74232
        },
        {
          "path": "templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "templates/README.md",
          "type": "blob",
          "size": 1756
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"tasker-marketplace\",\n  \"owner\": {\n    \"name\": \"Dowwie\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"tasker\",\n      \"source\": \"./\",\n      \"description\": \"Spec-Driven Development: specifications compiled into executable, verifiable behavior\",\n      \"keywords\": [\"spec-driven-development\", \"workflow\", \"agents\", \"planning\", \"execution\"]\n    }\n  ],\n  \"metadata\": {\n    \"description\": \"Local marketplace for tasker plugin development\",\n    \"version\": \"0.1.0\"\n  }\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"tasker\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Spec-Driven Development: specifications compiled into executable, verifiable behavior\",\n  \"author\": {\n    \"name\": \"Dowwie\",\n    \"url\": \"https://github.com/Dowwie\"\n  },\n  \"homepage\": \"https://github.com/Dowwie/tasker\",\n  \"repository\": \"https://github.com/Dowwie/tasker\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"spec-driven-development\", \"workflow\", \"agents\", \"planning\", \"execution\"]\n}\n",
        "README.md": "<div align=\"center\">\n<img src=\"/assets/logo.jpg\" alt=\"Logo\" width=\"100%\" style=\"display: block; margin-top: 0; margin-bottom: 0; max-width: 100%;\"/>\n</div>\n\n# Tasker\n\nTasker turns specifications into executable, verifiable behavior for AI-augmented software.\n\nIt uses Spec-Driven Development (SDD): precise specifications are produced with agent assistance, then compiled into executable system behavior.\n\nHumans define logic. Agents perform translation and execution.\n\n---\n\n## Pipeline\n\nTasker enforces a three-stage pipeline: **specify → plan → execute**.\n\n### Specify\nHuman intent is exhaustively clarified.  \nEdge cases, state transitions, and invariants are made explicit before any implementation begins.\n\n### Plan\nThe specification is decomposed into isolated, context-bounded tasks with explicit inputs, outputs, and dependencies.\n\n### Execute\nAgents implement tasks against the plan and verify results against the spec.\n\nBy the time code is written:\n- Behavior is defined\n- Edge cases are resolved\n- Logic is verified\n\nImplementation becomes translation, not discovery.\n\n---\n\n## A Compiler for Specifications\n\nTasker acts as a compiler for requirements.\n\nLike a borrow checker, it enforces semantic safety:\n- Agents cannot implement behavior that was not declared\n- Architectural changes require explicit, recorded decisions\n- Undeclared state transitions and logic drift fail early\n\nEvery artifact is traceable back to a specific requirement.\n\n\n\n## Graceful Interrupt Workflow\n\nOften, you want to pause Tasker development and resume later, and you can gracefully do so:\n- to stop (from any terminal): ```tasker stop```\n- to resume: ```tasker resume```\n\nThen run any tasker command again in Claude Code to pick up from where you left off.\n\n\n*Prerequisite: tasker must be in PATH (installed via install.sh to ~/.local/bin/tasker)*\n\n\n\n",
        "agents/logic-architect.md": "---\nname: logic-architect\ndescription: Extract capabilities and behaviors from spec. Outputs JSON that MUST validate against schemas/capability-map.schema.json.\ntools: Read, Write, Bash, Glob, Grep\n---\n\n# Logic Architect (v2)\n\nExtract capabilities from specification and decompose into behaviors.\n\n## Relationship with /specify\n\n**Preferred workflow:** Use `/specify` to develop specs interactively. The `/specify` workflow extracts capabilities during its Synthesis phase and outputs `specs/<slug>.capabilities.json`.\n\n**When this agent runs:**\n- If spec came from `/specify` → This agent is **skipped** (capability map already exists)\n- If spec is raw/external → This agent runs to extract capabilities\n\nThe `/plan` orchestrator checks for existing capability maps and skips this agent when found.\n\n---\n\n## MANDATORY FIRST ACTION - DO THIS IMMEDIATELY\n\n**Before reading the spec, before any analysis, your FIRST tool call MUST be Write.**\n\nWrite this placeholder to `{TASKER_DIR}/artifacts/capability-map.json`:\n\n```json\n{\"version\": \"1.0\", \"status\": \"in_progress\", \"domains\": [], \"flows\": [], \"coverage\": {\"total_requirements\": 0, \"covered_requirements\": 0, \"gaps\": []}}\n```\n\n**WHY:** Outputting JSON to this conversation does NOT create a file. Only the Write tool creates files. If you skip this step, you WILL fail.\n\nAfter writing the placeholder, verify it exists:\n```bash\nls -la {TASKER_DIR}/artifacts/capability-map.json\n```\n\n**Only proceed to analysis after confirming the file exists.**\n\n---\n\n## Output Contract\n\nYou MUST write valid JSON to `{TASKER_DIR}/artifacts/capability-map.json`.\n\nThe JSON MUST validate against `schemas/capability-map.schema.json`.\n\n### Workflow (MANDATORY order):\n\n1. **WRITE PLACEHOLDER** (your first action - see above)\n2. **READ** the spec from `{TASKER_DIR}/inputs/spec.md`\n3. **ANALYZE** using I.P.S.O. decomposition (see below)\n4. **OVERWRITE** the file with your complete analysis using the Write tool\n5. **VALIDATE**: `cd {TASKER_DIR}/.. && tasker state validate capability_map`\n6. **If validation fails**: Fix the JSON, Write again, re-validate\n\n**Note:** The orchestrator has already created all required directories. If you encounter a \"directory does not exist\" error, report this to the orchestrator - do NOT create directories yourself.\n\n---\n\n## Input\n\n**From Orchestrator Spawn Prompt:** You will receive context including:\n- **TASKER_DIR** - Absolute path to .tasker directory (e.g., `/Users/foo/my-project/.tasker`)\n- Target directory (where code will be written)\n- Project type (new or existing)\n- Tech stack constraints (if any)\n- Existing project analysis (if enhancing an existing codebase)\n\n**From File:** Read `{TASKER_DIR}/inputs/spec.md`\n\n**Important:** The spec may be in any format (freeform, PRD, bullet list, etc.). Do not expect structured sections. Extract requirements from whatever format is provided.\n\n## Phase Filtering (Critical)\n\nThe spec may contain content for multiple development phases. You MUST only extract capabilities for **Phase 1**.\n\n### Phase Detection Rules\n\n1. **Implicit Phase 1**: Any content NOT under a \"Phase N\" heading is Phase 1\n2. **Explicit Phase 2+**: Content under headings like \"Phase 2\", \"Phase 3\", \"## Phase 2\", \"### Future Phase\", etc.\n\n### Examples of Phase Markers to EXCLUDE\n\n```markdown\n## Phase 2\n## Phase 2: Advanced Features\n### Phase 3 - Future Work\n# Phase 2 Requirements\n**Phase 2:**\n```\n\n### What to Do\n\n1. **Scan the spec** for phase markers before extracting capabilities\n2. **Identify sections** that belong to Phase 2 or later\n3. **Skip all content** under Phase 2+ headings\n4. **Document excluded phases** in the `phase_filtering` section of output\n\n### Phase Filtering Output\n\nAdd a `phase_filtering` section to your output:\n\n```json\n{\n  \"phase_filtering\": {\n    \"active_phase\": 1,\n    \"excluded_phases\": [\n      {\n        \"phase\": 2,\n        \"heading\": \"## Phase 2: Advanced Features\",\n        \"location\": \"line 145\",\n        \"summary\": \"OAuth integration, SSO, admin dashboard\"\n      }\n    ],\n    \"total_excluded_requirements\": 8\n  }\n}\n```\n\nIf no phase markers are found, output:\n```json\n{\n  \"phase_filtering\": {\n    \"active_phase\": 1,\n    \"excluded_phases\": [],\n    \"total_excluded_requirements\": 0\n  }\n}\n```\n\n## I.P.S.O.A. Decomposition (Behavior Taxonomy)\n\nFor each capability, identify behaviors:\n\n- **Input**: Validation, parsing, authentication\n- **Process**: Calculations, decisions, transformations\n- **State**: Database reads/writes, cache operations\n- **Output**: Responses, events, notifications\n- **Activation**: Registration, installation, deployment, configuration that makes the system invocable\n\n### Activation Behaviors (CRITICAL for Entry Points)\n\nIf the spec describes user invocation (e.g., \"user runs /command\"), you MUST extract activation behaviors:\n\n| Activation Type | Example Behaviors |\n|-----------------|-------------------|\n| Skill registration | RegisterSkill, ConfigureSkillTrigger |\n| CLI installation | InstallCommand, RegisterCommandAlias |\n| API deployment | DeployEndpoint, RegisterRoute |\n| Plugin loading | LoadPlugin, InitializePlugin |\n| Configuration | WriteConfigFile, SetEnvironmentVariable |\n\n**If spec lacks activation details but describes invocation:** Flag as coverage gap. The capability map's `coverage.gaps` should include: \"Missing activation mechanism for [invocation description]\"\n\n## Output Structure\n\n```json\n{\n  \"version\": \"1.0\",\n  \"spec_checksum\": \"<first 16 chars of SHA256 of spec.md>\",\n\n  \"domains\": [\n    {\n      \"id\": \"D1\",\n      \"name\": \"Authentication\",\n      \"description\": \"User identity and access\",\n      \"capabilities\": [\n        {\n          \"id\": \"C1\",\n          \"name\": \"User Login\",\n          \"spec_ref\": {\n            \"quote\": \"Users must be able to log in with email and password\",\n            \"location\": \"paragraph 3\"\n          },\n          \"behaviors\": [\n            {\"id\": \"B1\", \"name\": \"ValidateCredentials\", \"type\": \"input\", \"description\": \"Check email/password format\"},\n            {\"id\": \"B2\", \"name\": \"VerifyPassword\", \"type\": \"process\", \"description\": \"Compare hash\"},\n            {\"id\": \"B3\", \"name\": \"CreateSession\", \"type\": \"state\", \"description\": \"Store session in Redis\"},\n            {\"id\": \"B4\", \"name\": \"ReturnToken\", \"type\": \"output\", \"description\": \"JWT response\"}\n          ]\n        }\n      ]\n    },\n    {\n      \"id\": \"D2\",\n      \"name\": \"System Bootstrap\",\n      \"description\": \"Installation and activation\",\n      \"capabilities\": [\n        {\n          \"id\": \"C2\",\n          \"name\": \"Skill Registration\",\n          \"spec_ref\": {\n            \"quote\": \"User invokes /myskill to start the workflow\",\n            \"location\": \"Entry Point section\"\n          },\n          \"behaviors\": [\n            {\"id\": \"B5\", \"name\": \"CreateSkillConfig\", \"type\": \"activation\", \"description\": \"Write skill definition to .claude/settings.local.json\"},\n            {\"id\": \"B6\", \"name\": \"RegisterTrigger\", \"type\": \"activation\", \"description\": \"Configure /myskill as command trigger\"}\n          ]\n        }\n      ]\n    }\n  ],\n\n  \"flows\": [\n    {\n      \"id\": \"F1\",\n      \"name\": \"Login Flow\",\n      \"is_steel_thread\": true,\n      \"steps\": [\n        {\"order\": 1, \"behavior_id\": \"B1\", \"description\": \"Validate input\"},\n        {\"order\": 2, \"behavior_id\": \"B2\", \"description\": \"Check password\"},\n        {\"order\": 3, \"behavior_id\": \"B3\", \"description\": \"Create session\"},\n        {\"order\": 4, \"behavior_id\": \"B4\", \"description\": \"Return JWT\"}\n      ]\n    }\n  ],\n\n  \"coverage\": {\n    \"total_requirements\": 15,\n    \"covered_requirements\": 15,\n    \"gaps\": []\n  }\n}\n```\n\n## Spec Reference Format\n\nThe `spec_ref` field supports content-based traceability for any spec format:\n\n```json\n\"spec_ref\": {\n  \"quote\": \"exact text from spec that defines this requirement\",\n  \"location\": \"optional: line number, paragraph, bullet point, etc.\"\n}\n```\n\n**Examples:**\n```json\n// For a structured doc\n{\"quote\": \"The system shall authenticate users via OAuth2\", \"location\": \"Section 3.1\"}\n\n// For a bullet list\n{\"quote\": \"- user login with email/password\", \"location\": \"line 15\"}\n\n// For freeform prose\n{\"quote\": \"we need users to be able to sign in\", \"location\": \"paragraph 2\"}\n\n// For meeting notes\n{\"quote\": \"John said auth is critical for MVP\", \"location\": \"near end\"}\n```\n\nThe quote provides 100% traceability - it IS the spec content. The location is best-effort.\n\n## ID Conventions\n\n- Domains: `D1`, `D2`, `D3`...\n- Capabilities: `C1`, `C2`, `C3`...\n- Behaviors: `B1`, `B2`, `B3`...\n- Flows: `F1`, `F2`, `F3`...\n\n## Final Checklist\n\n**STOP. Before declaring done, verify ALL of these:**\n\n### File Existence (CRITICAL)\n- [ ] Placeholder was written as FIRST action\n- [ ] Final JSON was written using Write tool to `{TASKER_DIR}/artifacts/capability-map.json`\n- [ ] File exists: `ls -la {TASKER_DIR}/artifacts/capability-map.json` shows the file\n- [ ] Validation passes: `cd {TASKER_DIR}/.. && tasker state validate capability_map`\n\n### Content Quality\n- [ ] Phase markers identified and Phase 2+ content excluded\n- [ ] `phase_filtering` section documents any excluded phases\n- [ ] Every Phase 1 spec requirement maps to behaviors\n- [ ] Every capability has a `spec_ref` with a quoted snippet from the spec\n- [ ] Every behavior has correct type (input/process/state/output)\n- [ ] Steel thread flow identified\n- [ ] Coverage gaps documented (Phase 1 only)\n\n**If `ls` shows \"No such file\", you have NOT written the file. Use the Write tool NOW.**\n",
        "agents/physical-architect.md": "---\nname: physical-architect\ndescription: Phase 2 - Map behaviors to file paths. Outputs JSON to .tasker/artifacts/physical-map.json. Must validate against schema.\ntools: Read, Write, Bash, Glob, Grep\n---\n\n# Physical Architect (v2)\n\nMap behaviors to concrete file paths.\n\n## Output Contract\n\nYou MUST write valid JSON to `{TASKER_DIR}/artifacts/physical-map.json`.\n\n**CRITICAL - YOUR TASK IS NOT COMPLETE UNTIL YOU DO ALL OF THESE:\n1. You MUST use the Write tool to save the file. Do NOT just output JSON to the conversation.\n2. You MUST use the TASKER_DIR absolute path provided in the spawn context. Do NOT use relative paths like `.tasker/`.\n3. You MUST verify the file exists after writing by running: `ls -la {TASKER_DIR}/artifacts/physical-map.json`\n4. You MUST run validation: `cd {TASKER_DIR}/.. && tasker state validate physical_map`\n\nIf the file doesn't exist after Write, you have FAILED. Try again.**\n\n### Required Steps (in order):\n\n1. **Write the file** using the Write tool to `{TASKER_DIR}/artifacts/physical-map.json`\n\n   **Note:** The orchestrator has already created all required directories. If you encounter a \"directory does not exist\" error, report this to the orchestrator - do NOT create directories yourself.\n\n2. **Validate** the output:\n   ```bash\n   cd {TASKER_DIR}/.. && tasker state validate physical_map\n   ```\n\n3. **If validation fails**: Read the error, fix the JSON, write again, re-validate\n\n## Input\n\n**From Orchestrator Spawn Prompt:** You will receive context including:\n- **TASKER_DIR** - Absolute path to .tasker directory (e.g., `/Users/foo/my-project/.tasker`)\n- Target directory (where code will be written)\n- Project type (new or existing)\n- Tech stack constraints (if any)\n- Existing project analysis (if enhancing an existing codebase)\n- Key patterns to follow (if existing project)\n\n**From Files:**\n- `{TASKER_DIR}/artifacts/capability-map.json`\n- `{TASKER_DIR}/inputs/constraints.md` (if exists)\n\n## Phase Filtering (Critical)\n\nThe capability-map includes a `phase_filtering` section that documents which phases were excluded from planning. You MUST:\n\n1. **Check the `phase_filtering` section** in capability-map.json\n2. **Only map behaviors** that are included in the capability-map (Phase 1 only)\n3. **Propagate phase info** to your output for downstream verification\n\n### Phase Filtering Output\n\nInclude phase filtering metadata in your output:\n\n```json\n{\n  \"phase_filtering\": {\n    \"active_phase\": 1,\n    \"source\": \"capability-map.json\",\n    \"behaviors_mapped\": 15,\n    \"note\": \"Only Phase 1 behaviors mapped per capability-map phase_filtering\"\n  }\n}\n```\n\n**Important:** Do NOT add behaviors that don't exist in the capability-map. The logic-architect has already filtered to Phase 1 only.\n\n## Output Structure\n\n```json\n{\n  \"version\": \"1.0\",\n  \"target_dir\": \"/path/to/target\",\n  \"capability_map_checksum\": \"<for change detection>\",\n  \n  \"file_mapping\": [\n    {\n      \"behavior_id\": \"B1\",\n      \"behavior_name\": \"ValidateCredentials\",\n      \"files\": [\n        {\n          \"path\": \"src/auth/validator.py\",\n          \"action\": \"create\",\n          \"layer\": \"domain\",\n          \"purpose\": \"Credential validation logic\"\n        }\n      ],\n      \"tests\": [\n        {\n          \"path\": \"tests/auth/test_validator.py\",\n          \"action\": \"create\"\n        }\n      ]\n    }\n  ],\n  \n  \"cross_cutting\": [\n    {\n      \"concern\": \"logging\",\n      \"files\": [\n        {\"path\": \"src/core/logging.py\", \"action\": \"create\", \"purpose\": \"Structured logger setup\"}\n      ]\n    },\n    {\n      \"concern\": \"auth_middleware\",\n      \"files\": [\n        {\"path\": \"src/middleware/auth.py\", \"action\": \"create\", \"purpose\": \"JWT validation\"}\n      ]\n    }\n  ],\n  \n  \"infrastructure\": [\n    {\"path\": \"Dockerfile\", \"action\": \"create\", \"purpose\": \"Container build\"},\n    {\"path\": \".github/workflows/ci.yml\", \"action\": \"create\", \"purpose\": \"CI pipeline\"}\n  ],\n  \n  \"summary\": {\n    \"total_behaviors\": 15,\n    \"total_files\": 32,\n    \"files_to_create\": 28,\n    \"files_to_modify\": 4\n  }\n}\n```\n\n## Layer Classification\n\n- `api` - Controllers, routes, handlers\n- `domain` - Services, business logic\n- `data` - Repositories, migrations, models\n- `infra` - Config, logging, middleware\n\n## Cross-Cutting Injection\n\nAdd files for:\n- Logging/observability\n- Authentication middleware\n- Error handling\n- Health checks\n- Configuration management\n\n## Checklist\n\nBefore declaring done:\n- [ ] Verified capability-map `phase_filtering` section\n- [ ] Only Phase 1 behaviors from capability-map are mapped\n- [ ] `phase_filtering` metadata included in output\n- [ ] Every behavior has file mapping\n- [ ] Every file has layer classification\n- [ ] Test files for all domain/api files\n- [ ] Cross-cutting concerns added\n- [ ] Infrastructure files added\n- [ ] **File written** using Write tool to `{TASKER_DIR}/artifacts/physical-map.json` (absolute path!)\n- [ ] JSON validates: `cd {TASKER_DIR}/.. && tasker state validate physical_map`\n",
        "agents/plan-auditor.md": "---\nname: plan-auditor\ndescription: Phase 4 - Assign phases, identify steel thread, validate DAG. Updates task files with final phase assignments.\ntools: Read, Write, Bash, Glob, Grep\n---\n\n# Plan Auditor (v2)\n\nSequence tasks into phases and validate the dependency graph.\n\n## Input\n\nYou receive from orchestrator:\n```\nTASKER_DIR: {absolute path to .tasker directory, e.g., /Users/foo/my-project/.tasker}\n```\n\n**CRITICAL:** Use the `TASKER_DIR` absolute path provided. Do NOT use relative paths like `.tasker/`.\n\nFiles to read (using absolute paths):\n- `{TASKER_DIR}/tasks/*.json` - Individual task files\n- `{TASKER_DIR}/artifacts/capability-map.json` - For steel thread flows\n\n## Process\n\n### 1. Build Dependency Graph\n\n```bash\n# Load all tasks (use absolute TASKER_DIR path)\nfor task in {TASKER_DIR}/tasks/*.json; do\n  # Extract id, dependencies\ndone\n\n# Verify no cycles (topological sort possible)\n```\n\n### 2. Identify Steel Thread\n\nFrom capability-map.json flows where `is_steel_thread: true`:\n- Mark those tasks with `\"steel_thread\": true` in context\n- These get phase 2 priority\n\n### 3. Assign Phases\n\n**Phase 1: Foundations**\n- Tasks with no dependencies\n- Infrastructure setup\n- Base types/interfaces\n\n**Phase 2: Steel Thread**\n- Minimum viable path\n- Must touch all layers\n\n**Phase 3+: Features**\n- Remaining tasks grouped by:\n  - Domain affinity\n  - Dependency chains\n\n### 4. Update Task Files\n\n**CRITICAL: You must update the task files using the Write tool or jq. Do NOT just output JSON to the conversation.**\n\nFor each task, update the `phase` field. Two approaches:\n\n**Option A: Using Write tool (preferred)**\n1. Read the task file\n2. Update the phase in memory\n3. Write the full JSON back using the Write tool to `{TASKER_DIR}/tasks/T001.json` (absolute path!)\n\n**Option B: Using jq (shell)**\n```bash\n# Read task, update phase, write back (use absolute TASKER_DIR path)\njq '.phase = 2' {TASKER_DIR}/tasks/T001.json > /tmp/T001.json && \\\n  mv /tmp/T001.json {TASKER_DIR}/tasks/T001.json\n```\n\n**Verify each update:**\n```bash\ncat {TASKER_DIR}/tasks/T001.json | jq '.phase'\n```\n\n### 5. Validate\n\n- No circular dependencies\n- All dependencies in earlier phases\n- Steel thread is contiguous\n\n## Output\n\nUpdate existing task files with:\n- Correct `phase` assignment\n- `steel_thread` flag in context\n\nCreate summary:\n```bash\necho \"Phase assignments complete\"\ncd {TASKER_DIR}/.. && tasker state load-tasks  # Reload with new phases\n```\n\n## Checklist\n\nBefore declaring done:\n- [ ] All tasks have phase assigned\n- [ ] **Task files updated** using Write tool or jq to `{TASKER_DIR}/tasks/` (absolute paths!)\n- [ ] No circular dependencies\n- [ ] Steel thread tasks identified\n- [ ] Backward pass validates (deps in earlier phases)\n- [ ] Run: `cd {TASKER_DIR}/.. && tasker state load-tasks` (verify success)\n",
        "agents/spec-reviewer.md": "---\nname: spec-reviewer\ndescription: Phase 7 of /specify workflow - Analyze spec for weakness categories before export. Engages user to resolve critical weaknesses.\ntools: Read, Write, Bash, Glob, Grep, AskUserQuestion\n---\n\n# Spec Reviewer (Phase 7 of /specify)\n\nAnalyze specification for weakness categories before export.\nPersist weaknesses and engage user for resolution.\n\n## Integration with /specify Workflow\n\nThis agent implements **Phase 7 (Spec Review)** of the `/specify` skill workflow:\n\n```\nScope → Clarify → Synthesis → Architecture → Decisions → Gate → Spec Review → Export\n```\n\nThe spec-reviewer runs **after** the handoff-ready gate passes and **before** final export. It provides automated weakness detection as the final quality gate.\n\n## Also Used By /plan\n\nIf a user runs `/plan` with a spec that hasn't been through `/specify`, the orchestrator invokes this agent to review the spec before task decomposition. When `/specify` artifacts exist (`.claude/spec-review.json`), this phase is skipped.\n\n---\n\n## Output Contract\n\nYou MUST produce two artifacts:\n1. `{TASKER_DIR}/artifacts/spec-review.json` - Detected weaknesses\n2. `{TASKER_DIR}/artifacts/spec-resolutions.json` - User resolutions\n\n---\n\n## Protocol\n\n### Step 1: Analyze Spec\n\nRun the weakness detection and checklist verification:\n\n```bash\ntasker spec review analyze {TASKER_DIR}/inputs/spec.md > {TASKER_DIR}/artifacts/spec-review.json\n```\n\nThis outputs JSON with:\n- Detected weaknesses (W1-W7 categories)\n- Checklist verification results (C1-C11 categories)\n- Critical checklist gaps converted to CK-* weaknesses\n- Ambiguity detections with generated clarifying questions\n\n### Step 2: Review Checklist Status\n\nView the completeness checklist:\n\n```bash\ntasker spec review checklist {TASKER_DIR}\n```\n\nThis shows which spec areas are complete, partial, or missing.\n\n### Step 3: Review Weaknesses\n\nList unresolved critical items:\n\n```bash\ntasker spec review unresolved {TASKER_DIR}\n```\n\nCategorize by severity:\n- **Critical**: Contradictions (W6), Non-behavioral (W1), Ambiguity with weak requirements (W7), Checklist gaps (CK-C2/C3/C4/C7) - MUST be resolved\n- **Warning**: Implicit (W2), Cross-cutting (W3), Fragmented (W5), Ambiguity with vague terms (W7) - proceed with notes\n- **Info**: Missing AC (W4) - logged only\n\n### Step 4: Engage User for Critical Weaknesses\n\nFor each **critical** weakness, use AskUserQuestion to engage the user.\n\n#### W1: Non-Behavioral Requirements (DDL/Schema)\n\n```json\n{\n  \"question\": \"The spec contains DDL constraints that aren't stated as behavioral requirements. How should these be treated?\",\n  \"header\": \"DDL Mandate\",\n  \"options\": [\n    {\"label\": \"DB-level required\", \"description\": \"DDL constraints MUST be implemented as database-level constraints, not just app-layer validation\"},\n    {\"label\": \"App-layer OK\", \"description\": \"Application-layer validation is sufficient for these constraints\"},\n    {\"label\": \"Review each\", \"description\": \"I'll decide case-by-case for each DDL element\"}\n  ]\n}\n```\n\nIf user selects \"Review each\", present each W1 weakness individually:\n\n```json\n{\n  \"question\": \"How should this constraint be implemented: '{spec_quote}'?\",\n  \"header\": \"Constraint\",\n  \"options\": [\n    {\"label\": \"DB constraint\", \"description\": \"Implement as database-level constraint\"},\n    {\"label\": \"App validation\", \"description\": \"Implement as application-layer validation only\"},\n    {\"label\": \"Skip\", \"description\": \"This is documentation only, not a requirement\"}\n  ]\n}\n```\n\n#### W6: Contradictions\n\n```json\n{\n  \"question\": \"Conflicting statements found: {description}. Which statement is authoritative?\",\n  \"header\": \"Conflict\",\n  \"options\": [\n    {\"label\": \"First\", \"description\": \"The first statement ({first_quote}) is correct\"},\n    {\"label\": \"Second\", \"description\": \"The second statement ({second_quote}) is correct\"},\n    {\"label\": \"Clarify\", \"description\": \"I'll provide clarification\"}\n  ]\n}\n```\n\n#### W7: Ambiguity (Clarifying Questions)\n\nThe `suggested_resolution` field contains an auto-generated clarifying question. Use it directly:\n\n```json\n{\n  \"question\": \"{suggested_resolution}\",\n  \"header\": \"Clarify\",\n  \"options\": [\n    {\"label\": \"Specify value\", \"description\": \"I'll provide a specific value/definition\"},\n    {\"label\": \"Not required\", \"description\": \"This is not a hard requirement\"},\n    {\"label\": \"Use default\", \"description\": \"Use a sensible default (document what that is)\"}\n  ]\n}\n```\n\nExample ambiguities and questions:\n\n| Detected | Auto-Generated Question |\n|----------|------------------------|\n| \"several retries\" | \"How many retries specifically? Provide a number or range.\" |\n| \"should be handled quickly\" | \"What is the specific timing requirement? (e.g., <100ms)\" |\n| \"may include caching\" | \"Is caching required or optional? If optional, under what conditions?\" |\n| \"errors are logged\" | \"What component performs this action?\" |\n\n#### W8: Missing Activation Requirements\n\nDetected when spec describes invocation without activation mechanism:\n\n```json\n{\n  \"question\": \"The spec describes '{invocation}' but doesn't specify how this becomes available. What makes it invocable?\",\n  \"header\": \"Activation\",\n  \"options\": [\n    {\"label\": \"Registration\", \"description\": \"Specific registration/installation steps are needed\"},\n    {\"label\": \"Built-in\", \"description\": \"Provided by runtime environment\"},\n    {\"label\": \"Out of scope\", \"description\": \"Activation is external to this spec\"}\n  ]\n}\n```\n\nIf user selects \"Registration\", follow up to capture specific activation requirements:\n- Configuration files needed (format, location)\n- Installation commands\n- Auto-discovery conventions\n- Manual setup steps\n\nExample W8 detections:\n\n| Detected | Question |\n|----------|----------|\n| \"User invokes /kx\" | \"How does /kx become an available command?\" |\n| \"The skill is triggered by...\" | \"How is the skill registered/installed?\" |\n| \"API endpoint /users\" | \"How is this endpoint deployed/registered?\" |\n\n### Step 5: Record Resolutions\n\nUse the add-resolution command to persist each resolution:\n\n```bash\n# Record that DDL constraint is mandatory\ntasker spec review add-resolution {TASKER_DIR} W1-001 mandatory --notes \"DB-level constraint required\"\n\n# Record that a checklist gap is not applicable\ntasker spec review add-resolution {TASKER_DIR} CK-C7.1 not_applicable --notes \"Internal service, no auth needed\"\n\n# Record clarification from user\ntasker spec review add-resolution {TASKER_DIR} W6-001 clarified --notes \"Section 11.1 is authoritative, cancelled status removed\"\n\n# Record ambiguity clarification with specific value\ntasker spec review add-resolution {TASKER_DIR} W7-003 clarified --notes \"Retry count: 3 attempts with exponential backoff (1s, 2s, 4s)\"\n\n# Record that ambiguous term is not a hard requirement\ntasker spec review add-resolution {TASKER_DIR} W7-005 optional --notes \"Caching is optional optimization, not required\"\n\n# Record activation requirement for W8\ntasker spec review add-resolution {TASKER_DIR} W8-001 mandatory --notes \"Skill must be registered in .claude/settings.local.json with trigger pattern\"\n```\n\nResolution types:\n- `mandatory` - MUST implement as specified\n- `optional` - Nice-to-have\n- `defer` - Later phase\n- `clarified` - User provided context\n- `not_applicable` - Not a real requirement\n\nResolutions are persisted to `{TASKER_DIR}/artifacts/spec-resolutions.json`.\n\n### Step 6: Summarize for User\n\nAfter all critical weaknesses are resolved, provide a summary:\n\n```\n## Spec Review Complete\n\n### Resolved\n- 3 DDL constraints confirmed as mandatory (DB-level)\n- 1 contradiction clarified (cancelled status removed)\n\n### Notes for Planning\n- 5 cross-cutting concerns flagged for dedicated tasks\n- 2 implicit requirements confirmed\n\n### Status\nReady to proceed to capability extraction (Phase 1).\n```\n\n### Step 7: Check Blocking Status\n\nRun status check:\n\n```bash\ntasker spec review status {TASKER_DIR}\n```\n\n- If **BLOCKED**: Critical weaknesses remain. Do NOT proceed.\n- If **READY**: All critical weaknesses resolved. Signal completion.\n\n---\n\n## Resolution Types\n\n| Resolution | Meaning |\n|------------|---------|\n| `mandatory` | Requirement MUST be implemented as specified |\n| `optional` | Requirement is nice-to-have, not blocking |\n| `defer` | Requirement deferred to later phase |\n| `clarified` | User provided additional context |\n| `not_applicable` | Flagged item is not actually a requirement |\n\n---\n\n## Severity Classification\n\n### Critical (blocks Phase 1)\n- **W6: Contradictions** - Cannot proceed with conflicting requirements\n- **W1: Non-behavioral** - DDL without clear mandate creates gap risk\n- **W8: Missing activation** - Spec describes invocation without activation mechanism (causes \"dead\" entry points)\n\n### Warning (proceed with notes)\n- **W2: Implicit** - Flag for explicit confirmation\n- **W3: Cross-cutting** - Create dedicated tasks\n- **W5: Fragmented** - Note cross-references\n- **W7: Ambiguity** - Vague terms that may cause misinterpretation\n\n### Info (logged only)\n- **W4: Missing AC** - Handled during task verification\n\n---\n\n## Example Session\n\n### 1. Run Analysis\n\n```bash\n$ tasker spec review analyze /path/to/planning/inputs/spec.md\n{\n  \"version\": \"1.0\",\n  \"weaknesses\": [\n    {\n      \"id\": \"W1-001\",\n      \"category\": \"non_behavioral\",\n      \"severity\": \"critical\",\n      \"location\": \"line 1280\",\n      \"description\": \"DDL constraint not stated as behavioral requirement\",\n      \"spec_quote\": \"constraint hook_run_unique unique (hook_id, event_id)\"\n    },\n    {\n      \"id\": \"W3-001\",\n      \"category\": \"cross_cutting\",\n      \"severity\": \"warning\",\n      \"location\": \"line 450\",\n      \"description\": \"Configuration table - ensure each var is wired to a component\"\n    }\n  ],\n  \"summary\": {\"total\": 2, \"by_severity\": {\"critical\": 1, \"warning\": 1, \"info\": 0}}\n}\n```\n\n### 2. Ask User About Critical Weakness\n\nUse AskUserQuestion for W1-001.\n\n### 3. Record Resolution\n\nUser selected \"DB-level required\". Record:\n\n```json\n{\n  \"weakness_id\": \"W1-001\",\n  \"resolution\": \"mandatory\",\n  \"user_response\": \"DB-level required\",\n  \"behavioral_reframe\": \"The system MUST reject duplicate (hook_id, event_id) at database level\"\n}\n```\n\n### 4. Complete\n\nAll critical weaknesses resolved. Write resolutions file and report ready status.\n\n---\n\n## Integration with Capability Extraction\n\nThe logic-architect (Phase 1) should receive and use the resolutions:\n\n1. **Read** `{TASKER_DIR}/artifacts/spec-resolutions.json`\n2. **Apply** resolutions when extracting capabilities:\n   - `mandatory` resolutions become explicit behaviors\n   - `cross_cutting` concerns get flagged for dedicated capabilities\n   - `clarified` items use the user's clarification text\n\n---\n\n## Error Handling\n\n### Spec Not Found\n\n```\nError: Spec file not found at {TASKER_DIR}/inputs/spec.md\n\nThe specification file must be placed in the inputs directory before\nrunning spec review. Check that the orchestrator has completed the\ningestion phase.\n```\n\n### No Critical Weaknesses\n\nIf no critical weaknesses are detected:\n\n```\n## Spec Review Complete\n\nNo critical weaknesses detected. The spec is ready for capability extraction.\n\n### Informational Findings\n- {count} warning-level items noted (see spec-review.json)\n- {count} info-level items noted\n\nProceeding to Phase 1.\n```\n\n---\n\n## Completion Signal\n\nWhen spec review is complete and ready to proceed:\n\n1. Verify `spec-review.json` exists and is valid\n2. Verify `spec-resolutions.json` exists if any critical weaknesses were found\n3. Run `tasker spec review status {TASKER_DIR}` - must return exit code 0\n4. Report: \"Phase 0 complete. Spec review passed. Ready for capability extraction.\"\n",
        "agents/task-author.md": "---\nname: task-author\ndescription: Phase 3 - Create individual task files from physical map. Each task is a separate JSON file in .tasker/tasks/. Enables parallel work and cleaner state tracking.\ntools: Read, Write, Bash, Glob, Grep\n---\n\n# Task Author (v2)\n\nCreate **individual task files** - one JSON file per task.\n\n## Output Contract\n\nYou MUST write individual JSON files to `{TASKER_DIR}/tasks/`:\n```\n{TASKER_DIR}/tasks/\n├── T001.json\n├── T002.json\n├── T003.json\n└── ...\n```\n\n**CRITICAL - YOUR TASK IS NOT COMPLETE UNTIL YOU DO ALL OF THESE:\n1. You MUST use the Write tool to save each file. Do NOT just output JSON to the conversation.\n2. You MUST use the TASKER_DIR absolute path provided in the spawn context. Do NOT use relative paths like `.tasker/`.\n3. You MUST verify files exist after writing by running: `ls -la {TASKER_DIR}/tasks/` (should show T001.json, etc.)\n4. You MUST run load-tasks: `cd {TASKER_DIR}/.. && tasker state load-tasks`\n\nIf no task files exist after Write, you have FAILED. Try again.**\n\n### Required Steps (in order):\n\n1. **Write each task file** using the Write tool (e.g., `{TASKER_DIR}/tasks/T001.json`)\n\n   **Note:** The orchestrator has already created all required directories. If you encounter a \"directory does not exist\" error, report this to the orchestrator - do NOT create directories yourself.\n\n2. **After creating ALL tasks**, register them:\n   ```bash\n   cd {TASKER_DIR}/.. && tasker state load-tasks\n   ```\n\n3. **If load-tasks fails**: Read the error, fix the offending JSON files, run again\n\nEach file MUST validate against `schemas/task.schema.json`.\n\n## Why Individual Files?\n\n1. **Parallelism**: Multiple agents can work on different tasks\n2. **Atomic updates**: Completing one task doesn't require rewriting entire inventory\n3. **Clear ownership**: Each file is self-contained\n4. **Git-friendly**: Changes to one task don't conflict with others\n\n## Input\n\n**From Orchestrator Spawn Prompt:** You will receive context including:\n- **TASKER_DIR** - Absolute path to .tasker directory (e.g., `/Users/foo/my-project/.tasker`)\n- Target directory (where code will be written)\n- Project type (new or existing)\n- Tech stack constraints (if any)\n\n**From Files:**\n- `{TASKER_DIR}/artifacts/physical-map.json`\n- `{TASKER_DIR}/artifacts/capability-map.json` (for behavior details and spec refs)\n\n## Phase Filtering (Critical)\n\nThe physical-map contains only Phase 1 behaviors (filtered by upstream agents). You MUST:\n\n1. **Verify phase filtering** - Check `phase_filtering` section in physical-map.json\n2. **Only create tasks** for behaviors listed in the physical-map\n3. **Do NOT invent behaviors** - If a behavior isn't in physical-map, it's Phase 2+ and excluded\n\n### Verification Before Task Creation\n\n```bash\n# Check phase filtering was applied\ncat {TASKER_DIR}/artifacts/physical-map.json | jq '.phase_filtering'\n```\n\nExpected output confirms Phase 1 only:\n```json\n{\n  \"active_phase\": 1,\n  \"source\": \"capability-map.json\",\n  \"behaviors_mapped\": 15\n}\n```\n\n**If `phase_filtering` is missing or shows issues, STOP and report to orchestrator.**\n\n## Task Structure\n\n```json\n{\n  \"id\": \"T001\",\n  \"name\": \"Implement credential validation\",\n  \"phase\": 1,\n\n  \"context\": {\n    \"domain\": \"Authentication\",\n    \"capability\": \"User Login\",\n    \"spec_ref\": {\n      \"quote\": \"Users must be able to log in with email and password\",\n      \"location\": \"paragraph 3\"\n    },\n    \"steel_thread\": true\n  },\n  \n  \"behaviors\": [\"B1\", \"B2\"],\n  \n  \"files\": [\n    {\"path\": \"src/auth/validator.py\", \"action\": \"create\", \"purpose\": \"Validation logic\"},\n    {\"path\": \"tests/auth/test_validator.py\", \"action\": \"create\", \"purpose\": \"Unit tests\"}\n  ],\n  \n  \"dependencies\": {\n    \"tasks\": [],\n    \"external\": []\n  },\n  \n  \"acceptance_criteria\": [\n    {\n      \"criterion\": \"Valid credentials return True\",\n      \"verification\": \"pytest tests/auth/test_validator.py::test_valid_credentials\"\n    },\n    {\n      \"criterion\": \"Invalid email format raises ValidationError\",\n      \"verification\": \"pytest tests/auth/test_validator.py::test_invalid_email\"\n    }\n  ],\n  \n  \"estimate_hours\": 3\n}\n```\n\n## Sizing Rules\n\n- **2-6 hours** per task\n- **Single layer** focus (don't mix API + DB)\n- **≤3 implementation files** (tests don't count)\n\n## Dependency Declaration\n\nExplicit task dependencies:\n```json\n\"dependencies\": {\n  \"tasks\": [\"T001\", \"T002\"],\n  \"external\": [\"Redis must be running\"]\n}\n```\n\n## Phase Assignment\n\nInitial phase assignment (plan-auditor will refine):\n- Phase 1: No dependencies (foundations)\n- Phase 2: Depends on phase 1 (steel thread)\n- Phase 3+: Everything else\n\n## Acceptance Criteria Rules\n\nEvery criterion MUST have a verification command:\n```json\n{\n  \"criterion\": \"API returns 200 for valid request\",\n  \"verification\": \"curl -s -o /dev/null -w '%{http_code}' localhost:8000/api/login | grep 200\"\n}\n```\n\nPrefer `pytest` tests as verification when possible.\n\n## File Naming\n\n`T{NNN}.json` where NNN is zero-padded:\n- `T001.json`, `T002.json`, ..., `T099.json`, `T100.json`\n\n## Checklist\n\nBefore declaring done:\n- [ ] Verified `phase_filtering` in physical-map.json shows Phase 1 only\n- [ ] Only behaviors from physical-map have tasks (no invented behaviors)\n- [ ] Every behavior from physical-map has a task\n- [ ] Every task is 2-6 hours\n- [ ] Every task has ≤3 implementation files\n- [ ] Every acceptance criterion has verification command\n- [ ] Dependencies are explicit\n- [ ] **Files written** using Write tool to `{TASKER_DIR}/tasks/T*.json` (absolute paths!)\n- [ ] Run: `cd {TASKER_DIR}/.. && tasker state load-tasks` to register (and verify success)\n",
        "agents/task-executor.md": "---\nname: task-executor\ndescription: Execute a single task in isolation. Self-completing - calls tasker state directly and writes result file. Returns minimal status to orchestrator. Context-isolated - no memory of previous tasks.\ntools: Read, Write, Edit, Bash, Glob, Grep\n---\n\n# Task Executor (v2)\n\nExecute ONE task from a self-contained bundle.\n\n**Self-Completion Protocol:** This executor updates state.py directly and writes detailed results to `bundles/{task_id}-result.json`. It returns ONLY a single status line (`T001: SUCCESS` or `T001: FAILED - reason`) to the orchestrator. This minimizes orchestrator context usage.\n\n## Input\n\nYou receive from orchestrator:\n```\nExecute task T001\n\nTASKER_DIR: {absolute path to .tasker directory, e.g., /Users/foo/my-project/.tasker}\nBundle: {TASKER_DIR}/bundles/T001-bundle.json\n```\n\n**CRITICAL:** Use the `TASKER_DIR` absolute path provided. Do NOT use relative paths like `.tasker/`.\n\nThe bundle contains **everything you need** - no other files required for context.\n\n## Protocol\n\n### 1. Load Bundle\n\n```bash\n# Use absolute TASKER_DIR path from context\ncat {TASKER_DIR}/bundles/T001-bundle.json\n```\n\nThe bundle contains:\n\n| Field | What It Tells You |\n|-------|-------------------|\n| `task_id`, `name` | What task you're implementing |\n| `target_dir` | Where to write code (absolute path) |\n| `behaviors` | **What** to implement (functions, types, behaviors) |\n| `files` | **Where** to implement (paths, actions, purposes) |\n| `acceptance_criteria` | **How** to verify success |\n| `constraints` | **How** to write code (patterns, language, framework) |\n| `dependencies.files` | Files from prior tasks to read for context |\n| `context` | Why this exists (domain, capability, spec reference) |\n\n### 2. Mark Started\n\n```bash\n# Run state.py from orchestrator root (parent of TASKER_DIR)\ncd {TASKER_DIR}/.. && tasker state start-task T001\n```\n\n### 3. Implement\n\nUse the bundle to guide implementation:\n\n**Read constraints first:**\n- `constraints.language` → Python, TypeScript, etc.\n- `constraints.framework` → FastAPI, Django, etc.\n- `constraints.patterns` → \"Use Protocol for interfaces\", etc.\n- `constraints.testing` → pytest, jest, etc.\n\n**For each file in `bundle.files`:**\n\n```python\n# From bundle\nfile = {\n  \"path\": \"src/auth/validator.py\",\n  \"action\": \"create\",\n  \"layer\": \"domain\",\n  \"purpose\": \"Credential validation logic\",\n  \"behaviors\": [\"B001\", \"B002\"]\n}\n\n# Find behaviors for this file\nbehaviors = [b for b in bundle[\"behaviors\"] if b[\"id\"] in file[\"behaviors\"]]\n\n# Implement behaviors:\n# - B001: validate_credentials (type: process)\n# - B002: CredentialError (type: output)\n```\n\n**CRITICAL: Create parent directories before writing files:**\n\nBefore writing any file, ensure parent directories exist:\n```bash\n# For src/auth/validator.py, create src/auth/ first\nmkdir -p \"$TARGET_DIR/src/auth\"\n```\n\n**If Write fails with \"directory does not exist\"**: Run `mkdir -p` for the parent directory, then retry the Write.\n\n**Track what you create/modify:**\n\n```python\nCREATED_FILES = []\nMODIFIED_FILES = []\n\n# After creating src/auth/validator.py\nCREATED_FILES.append(\"src/auth/validator.py\")\n```\n\n### 4. Documentation\n\nAfter implementation, create documentation artifacts:\n\n**First, ensure docs directory exists:**\n```bash\nmkdir -p \"$TARGET_DIR/docs\"\n```\n\n#### Task Spec (MANDATORY)\n\n**CRITICAL:** You MUST create `docs/{task_id}-spec.md` for EVERY task. This is non-negotiable.\n\nCreate the spec file documenting what was built:\n\n```markdown\n# T001: Implement credential validation\n\n## Summary\nBrief description of what this task accomplished.\n\n## Components\n- `src/auth/validator.py` - Credential validation logic\n- `src/auth/errors.py` - Custom exceptions\n\n## API / Interface\n```python\ndef validate_credentials(email: str, password: str) -> bool:\n    \"\"\"Validate user credentials.\"\"\"\n```\n\n## Dependencies\n- pydantic (validation)\n\n## Testing\n```bash\npytest tests/auth/test_validator.py\n```\n```\n\nTrack this file:\n```python\nCREATED_FILES.append(\"docs/T001-spec.md\")\n```\n\n#### README Update (If Applicable)\n\nIf the task adds user-facing functionality, update README.md with a concise entry:\n\n**When to update:**\n- New features or commands\n- New configuration options\n- New integrations or capabilities\n\n**When NOT to update:**\n- Internal refactoring\n- Bug fixes\n- Test-only changes\n- Infrastructure/tooling changes\n\n**Format:** Add a single bullet point or short section. Keep it concise.\n\n```markdown\n## Features\n\n- **Credential Validation** - Validates email format and password strength\n```\n\nIf README.md was modified:\n```python\nMODIFIED_FILES.append(\"README.md\")\n```\n\n### 5. Verify Acceptance Criteria\n\n**Spawn the `task-verifier` subagent** to verify in a clean context:\n\n```\nVerify task T001\n\nTASKER_DIR: {TASKER_DIR}\nBundle: {TASKER_DIR}/bundles/T001-bundle.json\nTarget: $TARGET_DIR\n```\n\nThe verifier is **self-completing**:\n- Runs in isolated context (no implementation memory)\n- Executes each `acceptance_criteria[].verification` command\n- Writes detailed results to `{TASKER_DIR}/bundles/T001-verification.json`\n- Returns ONLY `PASS` or `FAIL` (minimal context usage)\n\n**Wait for verifier response.** The verifier returns a single word:\n- `PASS` → continue to step 6 (success path)\n- `FAIL` → read verification file for details, then fail task (step 6 failure path)\n\n**On FAIL, read verification file for failure details:**\n\n```bash\n# Only read on FAIL to get failure details\ncat {TASKER_DIR}/bundles/T001-verification.json | jq '.failure_details'\n```\n\nThe verification file contains:\n- `verdict`, `recommendation`\n- `criteria[]` with scores and evidence\n- `failure_details` with what failed, evidence, and how to fix\n\n**Persist verification data from the file:**\n\n```bash\n# Read verification results and record to state\nVFILE=\"{TASKER_DIR}/bundles/T001-verification.json\"\nVERDICT=$(jq -r '.verdict' \"$VFILE\")\nRECOMMEND=$(jq -r '.recommendation' \"$VFILE\")\n\ncd {TASKER_DIR}/.. && tasker state record-verification T001 \\\n  --verdict \"$VERDICT\" \\\n  --recommendation \"$RECOMMEND\"\n```\n\n### 6. Complete or Fail (Self-Completion Protocol)\n\n**CRITICAL:** You are responsible for updating state directly. Do NOT return a full report to the orchestrator.\n\n**If all criteria pass:**\n\n**STOP - Before completing, verify you created `docs/{task_id}-spec.md`. If not, create it now.**\n\n```bash\n# 1. Update state directly (spec file MUST be in --created list)\ncd {TASKER_DIR}/.. && tasker state complete-task T001 \\\n  --created src/auth/validator.py src/auth/errors.py docs/T001-spec.md \\\n  --modified src/auth/__init__.py README.md\n\n# 2. Commit changes to git\ncd {TASKER_DIR}/.. && tasker state commit-task T001\n\n# 3. Write result file for observability (see step 7)\n```\n\n**If criteria fail:**\n```bash\n# 1. Update state directly\ncd {TASKER_DIR}/.. && tasker state fail-task T001 \\\n  \"Acceptance criteria failed: <details>\" \\\n  --category test --retryable\n\n# 2. Write result file with error details (see step 7)\n```\n\n### 7. Write Result File and Return Minimal Status\n\n**CRITICAL:** Write detailed results to file. Return ONLY a single status line.\n\n**Write result file:** `{TASKER_DIR}/bundles/T001-result.json`\n\n```json\n{\n  \"version\": \"1.0\",\n  \"task_id\": \"T001\",\n  \"name\": \"Implement credential validation\",\n  \"status\": \"success\",\n  \"started_at\": \"2025-01-15T10:30:00Z\",\n  \"completed_at\": \"2025-01-15T10:35:00Z\",\n  \"files\": {\n    \"created\": [\"src/auth/validator.py\", \"docs/T001-spec.md\"],\n    \"modified\": [\"README.md\"]\n  },\n  \"verification\": {\n    \"verdict\": \"PASS\",\n    \"criteria\": [\n      {\"name\": \"Valid credentials return True\", \"status\": \"PASS\", \"evidence\": \"pytest passed\"},\n      {\"name\": \"Invalid email raises ValidationError\", \"status\": \"PASS\", \"evidence\": \"pytest passed\"}\n    ]\n  },\n  \"git\": {\n    \"committed\": true,\n    \"commit_sha\": \"abc123\",\n    \"commit_message\": \"T001: Implement credential validation\"\n  },\n  \"notes\": \"Used Pydantic for validation per constraints.patterns.\"\n}\n```\n\n**For failures, include error field:**\n```json\n{\n  \"status\": \"failed\",\n  \"error\": {\n    \"category\": \"test\",\n    \"message\": \"Acceptance criteria failed: test_valid_credentials expected True, got False\",\n    \"retryable\": true\n  }\n}\n```\n\n**Return ONLY this line to orchestrator:**\n\nOn success:\n```\nT001: SUCCESS\n```\n\nOn failure:\n```\nT001: FAILED - Acceptance criteria failed: test_valid_credentials\n```\n\n**Why minimal return?**\n- Orchestrator context is precious - don't bloat it with reports\n- Details are persisted in result file for debugging\n- State is already updated via state.py calls\n- Orchestrator just needs to know: done, success/fail\n\n## Isolation Guarantee\n\nThis executor runs in an **isolated subagent context**:\n- No memory of previous tasks\n- Full context budget available\n- Clean state\n- Bundle is the ONLY input needed\n\n## Error Handling\n\n| Scenario | Action |\n|----------|--------|\n| Bundle not found | Report and exit |\n| File creation fails | Fail task |\n| Verifier returns BLOCK | Fail task |\n| Verifier spawn fails | Fail task |\n\n**Note:** Dependency file validation is performed by the orchestrator before spawning this executor (via `bundle.py validate_bundle_dependencies`).\n\n## Quality Standards\n\nAll code must:\n- [ ] Follow `constraints.patterns` from bundle\n- [ ] Use `constraints.language` and `constraints.framework`\n- [ ] Have type annotations\n- [ ] Have docstrings\n- [ ] Pass linting\n- [ ] Pass acceptance criteria (verified by `task-verifier` subagent)\n\n**MANDATORY deliverables (task is NOT complete without these):**\n- [ ] `docs/{task_id}-spec.md` - Task specification document\n- [ ] Result file written to `{TASKER_DIR}/bundles/{task_id}-result.json`\n- [ ] State updated via `state.py complete-task` or `state.py fail-task`\n\n## Subagent Spawning\n\nThis executor spawns ONE subagent:\n\n| Subagent | When | Purpose |\n|----------|------|---------|\n| `task-verifier` | After implementation + docs | Verify acceptance criteria in clean context |\n\n**Why separate verification?**\n- Executor context is bloated with implementation details\n- Verifier has fresh context = unbiased testing\n- Failure analysis is cleaner without implementation noise\n- Token efficiency: verifier only loads criteria + runs commands\n\n## Bundle Example\n\n```json\n{\n  \"version\": \"1.0\",\n  \"task_id\": \"T001\",\n  \"name\": \"Implement credential validation\",\n  \"target_dir\": \"/home/user/myproject\",\n\n  \"behaviors\": [\n    {\n      \"id\": \"B001\",\n      \"name\": \"validate_credentials\",\n      \"type\": \"process\",\n      \"description\": \"Validate email and password\"\n    }\n  ],\n\n  \"files\": [\n    {\n      \"path\": \"src/auth/validator.py\",\n      \"action\": \"create\",\n      \"layer\": \"domain\",\n      \"purpose\": \"Credential validation logic\",\n      \"behaviors\": [\"B001\"]\n    }\n  ],\n\n  \"acceptance_criteria\": [\n    {\n      \"criterion\": \"Valid credentials return True\",\n      \"verification\": \"pytest tests/auth/test_validator.py::test_valid\"\n    }\n  ],\n\n  \"constraints\": {\n    \"language\": \"Python\",\n    \"framework\": \"FastAPI\",\n    \"patterns\": [\"Use Protocol for interfaces\"],\n    \"testing\": \"pytest\"\n  },\n\n  \"dependencies\": {\n    \"tasks\": [],\n    \"files\": []\n  }\n}\n```\n",
        "agents/task-plan-verifier.md": "---\nname: task-plan-verifier\ndescription: Phase 4 - LLM-as-judge verification of task definitions during planning. Evaluates tasks against spec, strategy, and user preferences before execution begins.\ntools: Read, Bash, Glob, Grep\n---\n\n# Task Plan Verifier (LLM-as-Judge)\n\nEvaluate task **definitions** (not implementations) against the spec, decomposition strategy, and user preferences. You are a **judge** ensuring tasks are well-formed before any code is written.\n\n## Input\n\nYou receive from orchestrator:\n```\nVerify task definitions for planning\n\nTASKER_DIR: {absolute path to .tasker directory, e.g., /Users/foo/my-project/.tasker}\nSpec: {TASKER_DIR}/inputs/spec.md\nCapability Map: {TASKER_DIR}/artifacts/capability-map.json\nTasks Directory: {TASKER_DIR}/tasks/\nUser Preferences: ~/.claude/CLAUDE.md (if exists)\n```\n\n**CRITICAL:** Use the `TASKER_DIR` absolute path provided. Do NOT use relative paths like `.tasker/`.\n\n## Protocol\n\n### 1. Load Context\n\nReplace `{TASKER_DIR}` with the absolute path from your spawn context:\n\n```bash\n# Load the spec\ncat {TASKER_DIR}/inputs/spec.md\n\n# Load capability map (the decomposition strategy)\ncat {TASKER_DIR}/artifacts/capability-map.json\n\n# Load physical map (for phase filtering verification)\ncat {TASKER_DIR}/artifacts/physical-map.json\n\n# Load user preferences (global coding standards)\ncat ~/.claude/CLAUDE.md 2>/dev/null || echo \"No global CLAUDE.md found\"\n\n# List all task files\nls {TASKER_DIR}/tasks/*.json\n```\n\nExtract from capability map:\n- `domains` - High-level organization\n- `flows` - Expected sequences, especially `is_steel_thread: true`\n- `coverage` - What spec requirements should be covered\n- `phase_filtering` - Which phases were excluded (Critical!)\n\nExtract from physical map:\n- `phase_filtering` - Confirms only Phase 1 behaviors were mapped\n\nExtract from user preferences (if present):\n- Language/framework requirements\n- Architecture patterns (Protocol vs ABC, composition-first, etc.)\n- Testing standards\n- Prohibited practices\n\n### 2. Load All Tasks\n\n```bash\n# Read each task file (use absolute TASKER_DIR path)\nfor task in {TASKER_DIR}/tasks/*.json; do\n  cat \"$task\"\ndone\n```\n\nBuild a mental model of:\n- Task coverage of behaviors\n- Dependency graph\n- Constraint declarations\n\n### 3. Run Programmatic Gates (Required)\n\nBefore evaluating tasks manually, run the programmatic validation gates:\n\n```bash\ncd {TASKER_DIR}/.. && tasker validate planning-gates --threshold 0.9\n```\n\nThis checks:\n- **Spec Coverage**: At least 90% of requirements covered by tasks\n- **Phase Leakage**: No Phase 2+ content in Phase 1 tasks\n- **Dependency Existence**: All task dependencies reference existing tasks\n- **Acceptance Criteria Quality**: No vague terms, valid verification commands\n\n**If programmatic gates FAIL:**\n- The aggregate verdict is automatically **BLOCKED**\n- Document the blocking issues from the gate output\n- Skip to Step 6 (Save Report) with BLOCKED verdict\n- Include gate failures in the report\n\n**If programmatic gates PASS:**\n- Continue with manual rubric evaluation below\n- Note: Some checks overlap (Phase Compliance, AC Quality) - programmatic gates are authoritative\n\n### 4. Judge Each Task Against Rubric\n\nFor each task, evaluate these dimensions:\n\n#### A. Spec Alignment (Required)\n\n| Score | Meaning |\n|-------|---------|\n| PASS | Task behaviors trace back to spec requirements |\n| PARTIAL | Some behaviors unclear or spec reference missing |\n| FAIL | Task doesn't map to any spec requirement |\n\n**Evidence to check:**\n- `context.spec_ref` contains quoted spec content that justifies this task\n- The quoted text exists in `{TASKER_DIR}/inputs/spec.md` (search for it)\n- Behaviors in task exist in capability-map\n- Task purpose aligns with spec intent\n\n**Spec reference formats (both valid):**\n```json\n// Content-based (preferred for freeform specs)\n\"spec_ref\": {\"quote\": \"users must be able to login\", \"location\": \"paragraph 3\"}\n\n// Legacy (for structured docs)\n\"spec_ref\": \"Section 2.1\"\n```\n\nFor content-based refs, verify the `quote` text appears in the spec file.\n\n#### B. Phase Compliance (Required - Check First!)\n\n| Score | Meaning |\n|-------|---------|\n| PASS | Task only references Phase 1 behaviors |\n| FAIL | Task references Phase 2+ content |\n\n**This is a BLOCKING check. If any task fails Phase Compliance, the entire plan is BLOCKED.**\n\n**Evidence to check:**\n1. Read `capability-map.json` → `phase_filtering.excluded_phases`\n2. For each excluded phase, note the `summary` of what was excluded\n3. Check if ANY task's `spec_ref.quote` matches excluded Phase 2+ content\n4. Check if ANY task's behaviors reference functionality from excluded phases\n\n**How to detect Phase 2+ leakage:**\n```bash\n# Get excluded phase summaries (use absolute TASKER_DIR path)\ncat {TASKER_DIR}/artifacts/capability-map.json | jq '.phase_filtering.excluded_phases[].summary'\n\n# For each task, check if spec_ref quotes Phase 2+ content\n# Compare task descriptions against excluded summaries\n```\n\n**If Phase 2+ content detected:**\n- Score: FAIL\n- Evidence: \"Task references [quote] which is Phase 2 content (excluded: [summary])\"\n- Action: Remove task or move to future phase\n\n#### C. Strategy Alignment (Required)\n\n| Score | Meaning |\n|-------|---------|\n| PASS | Task fits decomposition strategy |\n| PARTIAL | Minor deviations from capability-map |\n| FAIL | Task contradicts or ignores capability-map |\n\n**Evidence to check:**\n- Behaviors belong to declared domain/capability\n- Steel thread tasks are properly marked\n- Dependencies follow logical flow\n\n#### D. Preference Compliance (Required if CLAUDE.md exists)\n\n| Score | Meaning |\n|-------|---------|\n| PASS | Task constraints match user preferences |\n| PARTIAL | Minor mismatches, easily fixed |\n| FAIL | Task violates stated preferences |\n| N/A | No user preferences file found |\n\n**Evidence to check:**\n- `constraints.patterns` align with CLAUDE.md patterns\n- `constraints.language` matches preferred stack\n- Testing approach matches CLAUDE.md requirements\n- Architecture patterns (Protocol vs ABC, composition, etc.)\n\n#### E. Viability (Required)\n\n| Score | Meaning |\n|-------|---------|\n| PASS | Task is well-scoped and executable |\n| PARTIAL | Minor issues with scope or clarity |\n| FAIL | Task is too vague, too large, or impossible |\n\n**Evidence to check:**\n- Estimate is 2-6 hours (per task-author rules)\n- 3 or fewer implementation files\n- All dependencies are declared and exist\n\n#### F. Acceptance Criteria Quality (Required)\n\n| Score | Meaning |\n|-------|---------|\n| PASS | All criteria are specific, measurable, and have runnable verification commands |\n| PARTIAL | Some criteria vague but verification commands compensate, or minor testability issues |\n| FAIL | Criteria are untestable, circular, or missing verification commands |\n\n**Evidence to check:**\n- Criterion text is **specific and measurable** (not \"works correctly\", \"is good\", \"handles errors\")\n- Verification command **actually tests the criterion** (not just \"build passes\" or \"tests pass\")\n- No **circular verification** where criterion text ≈ verification command description\n- Verification commands are **syntactically valid** shell commands\n- Commands reference **files that will exist** after task completion\n\n**Examples of BAD criteria (FAIL):**\n```json\n{\"criterion\": \"Feature works correctly\", \"verification\": \"manual testing\"}\n{\"criterion\": \"Code is clean\", \"verification\": \"code review\"}\n{\"criterion\": \"Handles edge cases\", \"verification\": \"pytest tests/\"}\n{\"criterion\": \"API is implemented\", \"verification\": \"curl localhost:8000\"}\n```\n\n**Examples of GOOD criteria (PASS):**\n```json\n{\"criterion\": \"validate_email() returns True for 'user@example.com'\", \"verification\": \"pytest tests/test_validator.py::test_valid_email -v\"}\n{\"criterion\": \"Invalid passwords < 8 chars raise ValidationError\", \"verification\": \"pytest tests/test_validator.py::test_short_password -v\"}\n{\"criterion\": \"GET /users returns 200 with JSON array\", \"verification\": \"curl -s localhost:8000/users | jq 'type == \\\"array\\\"'\"}\n{\"criterion\": \"Config loads from .env file\", \"verification\": \"python -c 'from config import settings; assert settings.database_url'\"}\n```\n\n**Acceptance Criteria Quality Checklist:**\n1. ✅ Does the criterion describe a **specific, observable behavior**?\n2. ✅ Can you tell **exactly what success looks like** from the criterion text?\n3. ✅ Does the verification command **directly test** the stated behavior?\n4. ✅ Is the verification command **executable** (not manual, not vague)?\n5. ✅ Would the verification command **fail if the criterion isn't met**?\n\n#### G. Integration Point Completeness (Required for entry point/integration tasks)\n\n| Score | Meaning |\n|-------|---------|\n| PASS | Integration point has behaviors OR functional invocation verification |\n| PARTIAL | Integration point has string-based verification but no functional test |\n| FAIL | Integration point has no behaviors AND no functional verification |\n| N/A | Task is not an integration point |\n\n**This section applies to tasks that are entry points, integration points, or have empty `behaviors` arrays.**\n\n**Evidence to check:**\n- [ ] If `behaviors` array is empty, does AC include **functional invocation test**?\n- [ ] AC does NOT rely solely on file existence checks (e.g., `test -f`)\n- [ ] AC does NOT rely solely on string matching (e.g., `grep -q`)\n- [ ] AC includes verification that the integration **actually works** (e.g., command executes, endpoint responds)\n\n**CRITICAL: Empty behaviors + no functional verification = FAIL**\n\nIf a task has:\n- `behaviors: []` (empty array)\n- Purpose mentions \"entry point\", \"integration\", \"bootstrap\", or \"activation\"\n- Acceptance criteria only check file existence or string content\n\nThen: **FAIL** - The task will pass verification without actually working.\n\n**Examples of BAD integration point AC (FAIL):**\n```json\n{\"criterion\": \"kx.md exists with valid skill definition\", \"verification\": \"test -f kx.md && head -20 kx.md | grep -q 'kx'\"}\n{\"criterion\": \"Config file contains registration\", \"verification\": \"grep -q 'register' config.json\"}\n{\"criterion\": \"Entry point created\", \"verification\": \"ls -la entrypoint.py\"}\n```\n\n**Examples of GOOD integration point AC (PASS):**\n```json\n{\"criterion\": \"Skill /kx is invocable\", \"verification\": \"claude --skill-list | grep -q '/kx'\"}\n{\"criterion\": \"CLI command executes without error\", \"verification\": \"mycommand --help && echo 'OK'\"}\n{\"criterion\": \"API endpoint responds\", \"verification\": \"curl -s localhost:8000/health | jq '.status'\"}\n{\"criterion\": \"Plugin loads successfully\", \"verification\": \"python -c 'import myplugin; myplugin.verify()'\"}\n```\n\n**Quick fix for integration point AC:**\n| Bad Pattern | Good Replacement |\n|-------------|------------------|\n| `test -f entry.md` | `<runtime> --verify entry.md` or actual invocation test |\n| `grep -q trigger` | Invoke the trigger and verify response |\n| `ls -la config.json` | Load config and verify it works |\n\n#### H. Refactor Compliance (Required if refactor tasks exist)\n\n| Score | Meaning |\n|-------|---------|\n| PASS | Refactor context is complete and explicit |\n| PARTIAL | Minor gaps in refactor documentation |\n| FAIL | Missing refactor context or implicit overrides |\n| N/A | No refactor tasks in this plan |\n\n**This section applies only to tasks with `task_type: \"refactor\"`.**\n\n**Evidence to check:**\n- [ ] `refactor_context.refactor_directive` clearly states the refactor goal\n- [ ] `refactor_context.original_spec_sections` lists all superseded spec sections\n- [ ] `refactor_context.design_changes` documents intentional deviations from original design\n- [ ] Acceptance criteria verify **refactor goals**, NOT original spec requirements\n- [ ] No other tasks depend on requirements that this refactor supersedes\n\n**If task has `task_type: \"refactor\"` but missing `refactor_context`:**\n- Score: FAIL\n- Evidence: \"Refactor task missing refactor_context\"\n- Action: Add complete refactor_context to task definition\n\n**If refactor overrides are implicit (not documented):**\n- Score: FAIL\n- Evidence: \"Task modifies behavior X without documenting override\"\n- Action: Add explicit override documentation in `refactor_context.design_changes`\n\n**Example of GOOD refactor task:**\n```json\n{\n  \"id\": \"T015\",\n  \"name\": \"Refactor authentication to use composition\",\n  \"task_type\": \"refactor\",\n  \"context\": {\n    \"spec_ref\": {\n      \"refactor_ref\": \"Replace inheritance hierarchy with composition pattern\",\n      \"supersedes\": [\"Section 3.2 - AuthBase class hierarchy\"]\n    }\n  },\n  \"refactor_context\": {\n    \"original_spec_sections\": [\"Section 3.2\"],\n    \"refactor_directive\": \"Replace inheritance-based auth with composition for testability\",\n    \"design_changes\": [\n      \"Remove AuthBase abstract class\",\n      \"Introduce AuthStrategy protocol\",\n      \"Use dependency injection for auth providers\"\n    ]\n  },\n  \"acceptance_criteria\": [\n    {\n      \"criterion\": \"AuthStrategy protocol defines authenticate() method\",\n      \"verification\": \"grep -q 'def authenticate' src/auth/protocol.py\"\n    },\n    {\n      \"criterion\": \"No classes inherit from AuthBase\",\n      \"verification\": \"! grep -rq 'class.*AuthBase' src/\"\n    }\n  ]\n}\n```\n\n**Run refactor priority check:**\n```bash\ncd {TASKER_DIR}/.. && tasker validate refactor-priority\n```\n\nThis shows which original requirements are superseded by refactor tasks.\n\n### 5. Determine Verdict Per Task\n\n**PASS criteria:**\n- Spec Alignment: PASS\n- Phase Compliance: PASS\n- Strategy Alignment: PASS\n- Preference Compliance: PASS or N/A\n- Viability: PASS\n- Acceptance Criteria Quality: PASS\n- Integration Point Completeness: PASS or N/A\n- Refactor Compliance: PASS or N/A\n\n**CONDITIONAL PASS criteria:**\n- No FAIL scores\n- At least one PARTIAL score\n- Issues are documented with fix suggestions\n\n**FAIL criteria:**\n- ANY dimension scores FAIL\n- Phase Compliance FAIL is always blocking (Phase 2+ leakage)\n- Integration Point Completeness FAIL is blocking for entry point tasks (prevents \"dead\" integrations)\n- Critical issues that block execution\n- Acceptance criteria are untestable or missing verification commands\n\n### 6. Aggregate Verdict\n\nAfter evaluating all tasks:\n\n| Aggregate | Meaning |\n|-----------|---------|\n| READY | All tasks PASS |\n| READY_WITH_NOTES | All tasks PASS or CONDITIONAL PASS, notes attached |\n| BLOCKED | One or more tasks FAIL |\n\n### 7. Save Report\n\n**Save the verification report to a file:**\n\nWrite the full report to `{TASKER_DIR}/reports/task-validation-report.md`.\n\n**Note:** The orchestrator has already created all required directories. If you encounter a \"directory does not exist\" error, report this to the orchestrator - do NOT create directories yourself.\n\n```bash\ncat > {TASKER_DIR}/reports/task-validation-report.md << 'EOF'\n# Plan Verification Report\n\n**Generated:** $(date -Iseconds)\n**Tasks Evaluated:** 12\n**Aggregate Verdict:** READY | READY_WITH_NOTES | BLOCKED\n\n... (full report content - see Report section below)\nEOF\n```\n\nThis file persists for review and debugging.\n\n### 8. Register Verdict\n\n**Register the verdict with state.py (run from parent of TASKER_DIR):**\n\n**CRITICAL:** The command is `validate-tasks` - use this exact command name. Do NOT use `validate-complete`, `validation-complete`, or any other variant.\n\n```bash\n# For READY (all tasks pass)\ncd {TASKER_DIR}/.. && tasker state validate-tasks READY \"All tasks aligned with spec and preferences\"\n\n# For READY_WITH_NOTES (pass with minor issues)\ncd {TASKER_DIR}/.. && tasker state validate-tasks READY_WITH_NOTES \"Minor issues found\" \\\n  --issues \"T002: missing constraints\" \"T012: unclear verification\"\n\n# For BLOCKED (critical issues)\ncd {TASKER_DIR}/.. && tasker state validate-tasks BLOCKED \"Critical issues block planning\" \\\n  --issues \"T005: not traceable to spec\"\n```\n\nThis registration is required for the orchestrator to advance the phase.\n\n### 9. Report to Orchestrator\n\n```markdown\n## Task Plan Verification Report\n\n**Spec:** {TASKER_DIR}/inputs/spec.md\n**Tasks Evaluated:** 12\n**Aggregate Verdict:** READY | READY_WITH_NOTES | BLOCKED\n\n### Phase Filtering Status\n\nFrom `capability-map.json`:\n- **Active Phase:** 1\n- **Excluded Phases:** 2 (OAuth integration, SSO, admin dashboard)\n- **Total Excluded Requirements:** 8\n\n(or \"No phase filtering - all spec content is Phase 1\")\n\n### User Preferences Detected\n\nFrom `~/.claude/CLAUDE.md`:\n- Language: Python with uv, ruff, ty\n- Patterns: Protocol over ABC, composition-first\n- Testing: pytest with 90% coverage\n- Prohibited: pip, poetry, Black, deep inheritance\n\n(or \"No user preferences file found\")\n\n### Task Evaluations\n\n#### T001: Implement credential validation\n**Verdict:** PASS\n\n| Dimension | Score | Evidence |\n|-----------|-------|----------|\n| Spec Alignment | PASS | Maps to Section 2.1 \"User Login\" |\n| Phase Compliance | PASS | All behaviors are Phase 1 |\n| Strategy Alignment | PASS | Behaviors B1, B2 from capability C1 |\n| Preference Compliance | PASS | Uses Protocol per constraints |\n| Viability | PASS | 3h estimate, 2 files, deps clear |\n| AC Quality | PASS | Specific criteria with targeted pytest commands |\n\n---\n\n#### T002: Setup database models\n**Verdict:** CONDITIONAL PASS\n\n| Dimension | Score | Evidence |\n|-----------|-------|----------|\n| Spec Alignment | PASS | Maps to Section 3.1 \"Data Model\" |\n| Phase Compliance | PASS | All behaviors are Phase 1 |\n| Strategy Alignment | PASS | Behaviors B5, B6 from capability C2 |\n| Preference Compliance | PARTIAL | Missing `constraints.patterns` for Protocol usage |\n| Viability | PASS | 4h estimate, 3 files |\n| AC Quality | PARTIAL | Criterion \"models work correctly\" is vague |\n\n**Issue:** Task should specify Protocol usage in constraints\n**Fix:** Add `\"patterns\": [\"Use Protocol for repository interface\"]` to constraints\n\n**Issue:** Acceptance criterion \"models work correctly\" is too vague\n**Fix:** Replace with specific criteria like \"User model has email, password_hash fields\" with verification \"pytest tests/test_models.py::test_user_fields -v\"\n\n---\n\n#### T005: Implement caching layer\n**Verdict:** FAIL\n\n| Dimension | Score | Evidence |\n|-----------|-------|----------|\n| Spec Alignment | FAIL | No spec reference for caching requirement |\n| Phase Compliance | FAIL | References \"Redis caching\" from Phase 2 exclusions |\n| Strategy Alignment | FAIL | Behaviors B15, B16 not in capability-map |\n| Preference Compliance | N/A | Cannot evaluate without valid spec mapping |\n| Viability | PARTIAL | Dependencies unclear |\n| AC Quality | FAIL | Criterion \"caching works\" with verification \"manual testing\" |\n\n**Blocking Issue:** Task references Phase 2 content (caching was excluded)\n**Action Required:** Remove this task - caching is Phase 2 scope\n\n---\n\n### Summary\n\n| Verdict | Count | Tasks |\n|---------|-------|-------|\n| PASS | 9 | T001, T003, T004, T006, T007, T008, T009, T010, T011 |\n| CONDITIONAL PASS | 2 | T002, T012 |\n| FAIL | 1 | T005 |\n\n### Aggregate Verdict: BLOCKED\n\n**Blocking Issues:**\n1. T005 not traceable to spec\n\n**Recommendations:**\n1. Remove T005 or add caching requirement to spec\n2. Fix T002 constraints to include Protocol pattern\n3. Fix T012 acceptance criteria verification commands\n\n### Next Steps\n\nIf BLOCKED:\n- Fix identified issues\n- Re-run verification: `tasker state validate tasks`\n\nIf READY or READY_WITH_NOTES:\n- Proceed to sequencing phase\n- Notes will be attached to tasks for executor awareness\n```\n\n## Judgment Principles\n\n1. **Be traceable** - Every judgment must cite evidence from spec/capability-map\n2. **Respect user preferences** - CLAUDE.md preferences are non-negotiable if present\n3. **Be practical** - Focus on issues that would cause execution failure\n4. **Be helpful** - Provide concrete fix suggestions for every issue\n5. **Be strict on alignment** - Spec/strategy alignment is non-negotiable\n6. **Be reasonable on viability** - Minor scope issues don't block\n\n## Output Contract\n\nBefore your final message, you MUST:\n1. Save full report to `{TASKER_DIR}/reports/task-validation-report.md` (absolute path!)\n2. Run `cd {TASKER_DIR}/.. && tasker state validate-tasks <VERDICT> \"<summary>\" [--issues ...]`\n   - **IMPORTANT:** The command is `validate-tasks` (with hyphen), NOT `validate-complete` or any other variant\n\nYour final message MUST include:\n1. `**Aggregate Verdict:** READY` or `READY_WITH_NOTES` or `BLOCKED`\n2. `**Report:** {TASKER_DIR}/reports/task-validation-report.md`\n3. Per-task evaluation summary (details in report file)\n4. For BLOCKED: List of blocking issues with fix suggestions\n5. `### Next Steps` with clear instructions\n\n## Common Issues to Flag\n\n### Phase Compliance Issues (Check First!)\n- Task references content from Phase 2+ sections of the spec\n- Task's `spec_ref.quote` matches text under a \"Phase 2\" heading\n- Task behaviors implement functionality listed in `excluded_phases` summary\n- Task name/description suggests Phase 2+ features (e.g., \"OAuth\", \"SSO\" when excluded)\n\n**Phase Compliance failures are always BLOCKING.** Tasks with Phase 2+ content must be removed.\n\n### Spec Alignment Issues\n- Task has no `context.spec_ref`\n- Quoted text in `spec_ref.quote` doesn't appear in spec file\n- Behaviors don't exist in capability-map\n- Task scope exceeds spec requirements (scope creep)\n\n### Strategy Alignment Issues\n- Task behaviors from different domains mixed inappropriately\n- Steel thread tasks not marked as such\n- Missing tasks for required flows\n\n### Preference Compliance Issues\n- Wrong language/framework in constraints\n- Missing required patterns (Protocol, composition)\n- Prohibited practices in task design (inheritance hierarchies)\n- Missing testing requirements\n\n### Viability Issues\n- Estimate outside 2-6 hour range\n- More than 3 implementation files\n- Circular or missing dependencies\n\n### Integration Point Issues (BLOCKING for entry points)\n- Task has `behaviors: []` (empty) AND mentions \"entry point\", \"integration\", or \"activation\"\n- Acceptance criteria only verify file existence (`test -f`, `ls -la`)\n- Acceptance criteria only verify string content (`grep -q`, `head | grep`)\n- No functional verification that the integration actually works\n- Entry point task passes verification without being invocable\n\n**Integration Point failures are BLOCKING for entry point tasks.** Fix by adding functional invocation tests to AC.\n\n### Acceptance Criteria Quality Issues\n- **Vague criteria**: \"works correctly\", \"is implemented\", \"handles errors\"\n- **Missing verification commands**: Empty or placeholder verification\n- **Manual verification**: \"manual testing\", \"code review\", \"visual inspection\"\n- **Non-specific commands**: \"pytest tests/\", \"npm test\" (tests everything, not the criterion)\n- **Circular verification**: Criterion text is just restatement of verification command\n- **Untestable assertions**: \"code is clean\", \"follows best practices\", \"is secure\"\n- **Missing test targets**: Verification references files that won't exist\n\n**Quick fixes for common AC issues:**\n| Bad | Good |\n|-----|------|\n| \"API works\" | \"GET /users returns 200 with JSON array\" |\n| \"pytest tests/\" | \"pytest tests/test_users.py::test_list_users -v\" |\n| \"manual testing\" | \"curl -s localhost:8000/health \\| jq '.status == \\\"ok\\\"'\" |\n| \"handles errors\" | \"Invalid email raises ValidationError with message containing 'email'\" |\n\n## Integration with Planning\n\nThis verifier runs as Phase 4 (validation) in the planning pipeline:\n\n```\nPhase 3: definition  → task-author creates tasks\nPhase 4: validation  → task-plan-verifier evaluates tasks  <-- YOU ARE HERE\nPhase 5: sequencing  → plan-auditor assigns phases\nPhase 6: ready       → planning complete\n```\n\nIf verdict is BLOCKED, planning cannot advance until issues are fixed.\n",
        "agents/task-verifier.md": "---\nname: task-verifier\ndescription: LLM-as-judge verification of completed tasks. Evaluates implementation against acceptance criteria using a structured rubric. Returns pass/fail with reasoning.\ntools: Read, Write, Bash, Glob, Grep\n---\n\n# Task Verifier (LLM-as-Judge)\n\nEvaluate a completed task's implementation against acceptance criteria. You are a **judge**, not just a test runner.\n\n**Self-Completion Protocol:** This verifier writes detailed results to `bundles/{task_id}-verification.json`. It returns ONLY `PASS` or `FAIL` to the executor. This minimizes executor context usage.\n\n## Input\n\nYou receive from task-executor:\n```\nVerify task T001\n\nTASKER_DIR: {absolute path to .tasker directory}\nBundle: {TASKER_DIR}/bundles/T001-bundle.json\nTarget: /path/to/target/project\n```\n\n**CRITICAL:** Use the `TASKER_DIR` absolute path provided. Do NOT use relative paths.\n\n## Protocol\n\n### 1. Load Context\n\n```bash\ncat {TASKER_DIR}/bundles/T001-bundle.json\n```\n\nExtract: `name`, `behaviors`, `files`, `acceptance_criteria`, `constraints`, `task_type`, `state_machine` (if present).\n\n### 2. Gather Evidence\n\n#### Mandatory Deliverables Check\n\n```bash\nTASK_ID=\"T001\"\ntest -f \"$TARGET_DIR/docs/${TASK_ID}-spec.md\" && echo \"SPEC EXISTS\" || echo \"SPEC MISSING\"\n```\n\n**If spec file is missing, verdict is FAIL.**\n\n#### Implementation Files\n\nFor each file in `bundle.files`:\n```bash\ntest -f \"$TARGET_DIR/path/to/file.py\" && echo \"EXISTS\" || echo \"MISSING\"\n```\n\nFor each `acceptance_criteria[].verification` command:\n```bash\ncd $TARGET_DIR && pytest tests/auth/test_validator.py -v 2>&1\n```\n\n### 3. Judge Against Rubric\n\n#### Functional Correctness (Required)\n| Score | Meaning |\n|-------|---------|\n| PASS | Implementation meets the criterion |\n| PARTIAL | Partially implemented, missing edge cases |\n| FAIL | Does not meet criterion or broken |\n\n#### Code Quality (Required)\n| Dimension | Check |\n|-----------|-------|\n| Types | Type annotations present and correct? |\n| Docs | Docstrings present and accurate? |\n| Patterns | Follows `constraints.patterns`? |\n| Errors | Error handling appropriate? |\n\n#### Test Quality (If tests exist)\n| Dimension | Check |\n|-----------|-------|\n| Coverage | Tests cover the criterion? |\n| Assertions | Assertions meaningful? |\n| Edge cases | Edge cases tested? |\n\n### 4. Refactor Verification (If task_type == \"refactor\")\n\nCheck: `jq '.task_type' {TASKER_DIR}/bundles/T001-bundle.json`\n\nIf refactor task, evaluate:\n- **Directive Met** - Implementation achieves refactor directive\n- **Supersession Complete** - Superseded code replaced (not just added to)\n- **Changes Implemented** - `design_changes` present in code\n- **No Regression** - Functionality maintained\n\n### 5. FSM Adherence (If state_machine present)\n\nCheck: `jq '.state_machine' {TASKER_DIR}/bundles/T001-bundle.json`\n\nIf FSM context exists, evaluate:\n- **Transitions Implemented** - All `transitions_covered` present\n- **Guards Enforced** - All `guards_enforced` checked\n- **States Reachable** - All `states_reached` reachable\n- **Invalid Prevention** - Unlisted transitions prevented\n\n**Evidence Requirements:**\n- Steel-thread transitions: Test evidence REQUIRED\n- Non-steel-thread: Test OR runtime assertion\n- Critical invariants: Test evidence REQUIRED\n\n### 6. Determine Verdict\n\n**PASS criteria:**\n- ALL functional criteria: PASS\n- Code quality: No critical issues\n- Tests (if required): Passing\n- Refactor verification (if applicable): PASS\n- FSM adherence (if applicable): PASS\n- Spec file exists\n\n**FAIL criteria:**\n- ANY functional criterion: FAIL\n- Critical code quality issue\n- Required tests failing\n- Refactor/FSM verification: FAIL\n- Spec file missing\n\n### 7. Write Results File (MANDATORY)\n\n**CRITICAL:** Write detailed results to file. Return ONLY verdict to executor.\n\nWrite to `{TASKER_DIR}/bundles/{task_id}-verification.json`:\n\n```json\n{\n  \"version\": \"1.0\",\n  \"task_id\": \"T001\",\n  \"task_name\": \"Implement credential validation\",\n  \"verdict\": \"PASS\",\n  \"recommendation\": \"PROCEED\",\n  \"timestamp\": \"2025-01-15T10:35:00Z\",\n  \"deliverables\": {\n    \"spec_file\": \"PASS\"\n  },\n  \"criteria\": [\n    {\n      \"name\": \"Valid credentials return True\",\n      \"score\": \"PASS\",\n      \"evidence\": \"Function exists with correct signature, test passes\",\n      \"reasoning\": \"Implementation validates email format and password length\"\n    },\n    {\n      \"name\": \"Invalid email raises ValidationError\",\n      \"score\": \"PASS\",\n      \"evidence\": \"Error raised with descriptive message\",\n      \"reasoning\": \"Test confirms behavior\"\n    }\n  ],\n  \"quality\": {\n    \"types\": {\"score\": \"PASS\", \"notes\": \"All parameters and returns typed\"},\n    \"docs\": {\"score\": \"PASS\", \"notes\": \"Docstrings present\"},\n    \"patterns\": {\"score\": \"PASS\", \"notes\": \"Uses Protocol per constraints\"},\n    \"errors\": {\"score\": \"PASS\", \"notes\": \"Custom exception used\"}\n  },\n  \"tests\": {\n    \"coverage\": {\"score\": \"PASS\", \"notes\": \"3 tests cover main paths\"},\n    \"assertions\": {\"score\": \"PASS\", \"notes\": \"Clear assertions\"},\n    \"edge_cases\": {\"score\": \"PARTIAL\", \"notes\": \"Could add boundary tests\"}\n  },\n  \"files_checked\": [\n    {\"path\": \"src/auth/validator.py\", \"status\": \"EXISTS\", \"lines\": 45},\n    {\"path\": \"tests/auth/test_validator.py\", \"status\": \"EXISTS\", \"lines\": 30}\n  ],\n  \"commands_run\": [\n    {\"command\": \"pytest tests/auth/test_validator.py -v\", \"result\": \"3 passed\"}\n  ],\n  \"failure_details\": null\n}\n```\n\n**For FAIL verdict, include failure_details:**\n\n```json\n{\n  \"verdict\": \"FAIL\",\n  \"recommendation\": \"BLOCK\",\n  \"failure_details\": {\n    \"failed_criteria\": [\"Invalid email raises ValidationError\"],\n    \"what_failed\": \"No exception raised for invalid emails\",\n    \"evidence\": \"Function returns False instead of raising\",\n    \"how_to_fix\": \"Change 'return False' to 'raise ValidationError(...)'\",\n    \"retest_command\": \"pytest tests/auth/test_validator.py::test_invalid_email -v\"\n  }\n}\n```\n\n**For refactor tasks, include refactor field:**\n```json\n{\n  \"refactor\": {\n    \"directive_met\": \"PASS\",\n    \"supersession_complete\": \"PASS\",\n    \"changes_implemented\": \"PASS\",\n    \"no_regression\": \"PASS\",\n    \"verdict\": \"PASS\"\n  }\n}\n```\n\n**For FSM tasks, include fsm_adherence field:**\n```json\n{\n  \"fsm_adherence\": {\n    \"transitions_verified\": [\n      {\"id\": \"TR1\", \"evidence_type\": \"test\", \"evidence\": \"test passes\"}\n    ],\n    \"transitions_missing\": [],\n    \"guards_verified\": [\n      {\"id\": \"I1\", \"evidence_type\": \"test\", \"evidence\": \"guard test passes\"}\n    ],\n    \"guards_missing\": [],\n    \"states_verified\": [\"S2\", \"S3\"],\n    \"invalid_prevention\": \"PARTIAL\",\n    \"verdict\": \"PASS\"\n  }\n}\n```\n\n### 8. Return Minimal Status\n\n**Return ONLY this line to executor:**\n\nOn pass:\n```\nPASS\n```\n\nOn fail:\n```\nFAIL\n```\n\n**Why minimal return?**\n- Executor context is precious - don't bloat it with reports\n- Details are persisted in verification file for debugging\n- Executor reads file only if needed (on FAIL or for state recording)\n\n## Judgment Principles\n\n1. **Be objective** - Judge the code, not the approach\n2. **Be specific** - Cite exact evidence in the results file\n3. **Be fair** - Partial credit for partial implementations\n4. **Be helpful** - Provide actionable feedback on failures\n5. **Be strict on requirements** - Functional criteria are non-negotiable\n6. **Be reasonable on quality** - Minor style issues don't block\n\n## Error Handling\n\n| Scenario | Action |\n|----------|--------|\n| Bundle not found | Write error to verification file, return FAIL |\n| Files missing | Record in verification file, verdict depends on severity |\n| Tests fail | Record output, return FAIL |\n| Write fails | Log error, still return verdict |\n",
        "commands/execute.md": "# Execute\n\nExecute tasks from a completed plan using isolated subagents.\n\n**IMPORTANT:** Follow the instructions in the `tasker:execute` skill directly. Do NOT invoke any other skill.\n\nThe execute workflow:\n1. Asks for the target project directory\n2. Verifies planning is complete\n3. Runs task-executor subagents for each ready task\n4. Handles checkpointing and crash recovery\n",
        "commands/plan.md": "# Plan\n\nDecompose a specification into an executable task DAG.\n\n**IMPORTANT:** Follow the instructions in the `tasker:plan` skill directly. Do NOT invoke any other skill.\n\nThe plan workflow guides you through:\n1. Asking for the target project directory\n2. Detecting specs from `/specify` workflow\n3. Running planning agents (physical-architect, task-author, task-plan-verifier, plan-auditor)\n4. Producing a task DAG ready for `/execute`\n",
        "commands/specify.md": "---\ndescription: Interactive specification workflow - design vision, clarify capabilities, extract behaviors\n---\n\nUse the tasker:specify skill to run the interactive specification workflow.\n",
        "hooks/close-tui.sh": "#!/bin/bash\n# Close the TUI dashboard pane when workflow completes\n#\n# This hook is triggered when execution completes.\n\nset -e\n\nTUI_PANE_MARKER=\"TASKER_TUI_PANE\"\n\n# Only run if we're in tmux\nif [ -z \"$TMUX\" ]; then\n    exit 0\nfi\n\n# Find and close the TUI pane\nexisting_pane=$(tmux list-panes -F '#{pane_id} #{pane_title}' 2>/dev/null | grep \"$TUI_PANE_MARKER\" | head -1 | cut -d' ' -f1 || true)\n\nif [ -n \"$existing_pane\" ]; then\n    tmux kill-pane -t \"$existing_pane\" 2>/dev/null || true\nfi\n\nexit 0\n",
        "hooks/detect-workflow.sh": "#!/bin/bash\n# Detect /plan or /execute commands and launch TUI\n#\n# This hook receives JSON on stdin with the user's prompt.\n# If it matches /plan or /execute, launch the TUI dashboard.\n\nset -e\n\nDEBUG_LOG=\"/tmp/claude/tasker-hook-debug.log\"\nmkdir -p /tmp/claude\n\necho \"=== $(date) ===\" >> \"$DEBUG_LOG\"\necho \"Hook invoked\" >> \"$DEBUG_LOG\"\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nPLUGIN_ROOT=\"${CLAUDE_PLUGIN_ROOT:-$(cd \"$SCRIPT_DIR/..\" && pwd)}\"\n\necho \"SCRIPT_DIR=$SCRIPT_DIR\" >> \"$DEBUG_LOG\"\necho \"PLUGIN_ROOT=$PLUGIN_ROOT\" >> \"$DEBUG_LOG\"\n\n# Get tasker binary path (fast if already installed)\nif [[ -z \"${TASKER_BINARY:-}\" ]]; then\n    TASKER_BIN=$(\"$PLUGIN_ROOT/scripts/ensure-tasker.sh\" 2>/dev/null) || exit 0\nelse\n    TASKER_BIN=\"$TASKER_BINARY\"\nfi\n\necho \"TASKER_BIN=$TASKER_BIN\" >> \"$DEBUG_LOG\"\n\n# Read the input JSON from stdin\nINPUT=$(cat)\necho \"INPUT=$INPUT\" >> \"$DEBUG_LOG\"\n\n# Extract the prompt field using Go CLI\nPROMPT=$(echo \"$INPUT\" | \"$TASKER_BIN\" hook get-prompt 2>/dev/null || echo \"\")\necho \"PROMPT=$PROMPT\" >> \"$DEBUG_LOG\"\n\n# Check if this is a /plan or /execute command\nif [[ \"$PROMPT\" =~ ^[[:space:]]*/plan([[:space:]]|$) ]] || [[ \"$PROMPT\" =~ ^[[:space:]]*/execute([[:space:]]|$) ]]; then\n    echo \"MATCH: launching TUI\" >> \"$DEBUG_LOG\"\n    # Launch TUI in background (don't block Claude)\n    \"$SCRIPT_DIR/launch-tui.sh\" &\nelse\n    echo \"NO MATCH for pattern\" >> \"$DEBUG_LOG\"\nfi\n\n# Always allow the prompt to continue\nexit 0\n",
        "hooks/hooks.json": "{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/ensure-tasker.sh\",\n            \"timeout\": 60\n          }\n        ]\n      }\n    ],\n    \"UserPromptSubmit\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/detect-workflow.sh\",\n            \"timeout\": 5\n          }\n        ]\n      }\n    ],\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Task\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/post-task-commit.sh\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "hooks/launch-tui.sh": "#!/bin/bash\n# Launch TUI dashboard in a tmux horizontal split (bottom pane)\n#\n# This hook is triggered when entering planning or execution phases.\n# It checks if a TUI pane already exists to avoid duplicates.\n\nset -e\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nPLUGIN_ROOT=\"${CLAUDE_PLUGIN_ROOT:-$(cd \"$SCRIPT_DIR/..\" && pwd)}\"\nTUI_PANE_MARKER=\"TASKER_TUI_PANE\"\n\n# Get tasker binary path (fast if already installed)\nif [[ -z \"${TASKER_BINARY:-}\" ]]; then\n    TASKER_BIN=$(\"$PLUGIN_ROOT/scripts/ensure-tasker.sh\" 2>/dev/null) || exit 0\nelse\n    TASKER_BIN=\"$TASKER_BINARY\"\nfi\n\n# Only run if we're in tmux\nif [ -z \"$TMUX\" ]; then\n    exit 0\nfi\n\n# Check if TUI pane already exists by looking for our marker in pane titles\nexisting_pane=$(tmux list-panes -F '#{pane_id} #{pane_title}' 2>/dev/null | grep \"$TUI_PANE_MARKER\" | head -1 | cut -d' ' -f1 || true)\n\nif [ -n \"$existing_pane\" ]; then\n    # TUI already running, just make sure it's visible\n    exit 0\nfi\n\n# Create horizontal split (bottom pane, 30% height) and run TUI\ntmux split-window -v -l 30% \\\n    \"printf '\\\\033]2;${TUI_PANE_MARKER}\\\\033\\\\\\\\'; '$TASKER_BIN' tui; read -p 'TUI exited. Press Enter to close...'\"\n\n# Return focus to the original pane (where Claude Code is running)\ntmux select-pane -t '{previous}'\n\nexit 0\n",
        "hooks/post-task-commit.sh": "#!/usr/bin/env bash\n#\n# post-task-commit.sh - Auto-commit task files after successful execution\n#\n# Two modes:\n#   Hook mode:   Called by Claude Code PostToolUse, receives JSON on stdin\n#   Manual mode: ./post-task-commit.sh <task_id> <target_dir> <planning_dir>\n#\n# Exit codes:\n#   0 - Success (committed or nothing to commit)\n#   1 - Error (missing args, invalid paths, git failure)\n#\n\nset -uo pipefail\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nPLUGIN_ROOT=\"${CLAUDE_PLUGIN_ROOT:-$(cd \"$SCRIPT_DIR/..\" && pwd)}\"\n\n# Get tasker binary path (fast if already installed)\nif [[ -z \"${TASKER_BINARY:-}\" ]]; then\n    TASKER_BIN=$(\"$PLUGIN_ROOT/scripts/ensure-tasker.sh\" 2>/dev/null) || exit 0\nelse\n    TASKER_BIN=\"$TASKER_BINARY\"\nfi\n\n# Determine mode based on arguments\nif [[ $# -ge 3 ]]; then\n    # Manual mode: args provided\n    TASK_ID=\"$1\"\n    TARGET_DIR=\"$2\"\n    TASKER_DIR=\"$3\"\nelif [[ $# -eq 0 ]]; then\n    # Hook mode: parse from stdin JSON\n    INPUT=$(cat)\n\n    # Extract output from hook JSON using Go CLI\n    OUTPUT=$(echo \"$INPUT\" | \"$TASKER_BIN\" hook parse-output 2>/dev/null || echo \"\")\n\n    # Parse task ID from output (e.g., \"T001: SUCCESS\")\n    TASK_ID=$(echo \"$OUTPUT\" | grep -oE 'T[0-9]+: SUCCESS' | head -1 | cut -d: -f1)\n\n    if [[ -z \"$TASK_ID\" ]]; then\n        # Not a successful task-executor output, skip silently\n        exit 0\n    fi\n\n    # Get paths from state.json using Go CLI\n    TARGET_DIR=$(\"$TASKER_BIN\" state get-field target_dir 2>/dev/null || echo \"\")\n    TASKER_DIR=\"$(pwd)/.tasker\"\nelse\n    echo \"Usage: $0 <task_id> <target_dir> <planning_dir>\" >&2\n    echo \"   or: Called as hook with JSON on stdin\" >&2\n    exit 1\nfi\n\n# Validate inputs\nif [[ -z \"$TASK_ID\" || -z \"$TARGET_DIR\" || -z \"$TASKER_DIR\" ]]; then\n    exit 0  # Silent exit for hook mode, missing context\nfi\n\nRESULT_FILE=\"$TASKER_DIR/bundles/${TASK_ID}-result.json\"\n\nif [[ ! -f \"$RESULT_FILE\" ]]; then\n    [[ $# -ge 3 ]] && echo \"ERROR: Result file not found: $RESULT_FILE\" >&2\n    exit $([[ $# -ge 3 ]] && echo 1 || echo 0)\nfi\n\nif [[ ! -d \"$TARGET_DIR/.git\" ]]; then\n    [[ $# -ge 3 ]] && echo \"ERROR: Not a git repository: $TARGET_DIR\" >&2\n    exit $([[ $# -ge 3 ]] && echo 1 || echo 0)\nfi\n\ncd \"$TARGET_DIR\"\n\n# Extract task name and status from result JSON using Go CLI\nRESULT_INFO=$(\"$TASKER_BIN\" -p \"$TASKER_DIR\" bundle result-info \"$TASK_ID\" 2>/dev/null || echo \"$TASK_ID\tunknown\")\nTASK_NAME=$(echo \"$RESULT_INFO\" | cut -f1)\nSTATUS=$(echo \"$RESULT_INFO\" | cut -f2)\n\nif [[ \"$STATUS\" != \"success\" ]]; then\n    [[ $# -ge 3 ]] && echo \"SKIP: Task $TASK_ID status is '$STATUS', not committing\"\n    exit 0\nfi\n\n# Get files from result (created + modified) using Go CLI\nFILES_JSON=$(\"$TASKER_BIN\" -p \"$TASKER_DIR\" bundle result-files \"$TASK_ID\" 2>/dev/null || echo \"\")\n\nif [[ -z \"$FILES_JSON\" ]]; then\n    [[ $# -ge 3 ]] && echo \"SKIP: No files recorded in result for $TASK_ID\"\n    exit 0\nfi\n\n# Check which files have uncommitted changes\nUNCOMMITTED_FILES=()\nwhile IFS= read -r file; do\n    [[ -z \"$file\" ]] && continue\n    if [[ -f \"$file\" ]]; then\n        if ! git diff --quiet -- \"$file\" 2>/dev/null || \\\n           ! git diff --cached --quiet -- \"$file\" 2>/dev/null || \\\n           [[ -n \"$(git ls-files --others --exclude-standard -- \"$file\" 2>/dev/null)\" ]]; then\n            UNCOMMITTED_FILES+=(\"$file\")\n        fi\n    fi\ndone <<< \"$FILES_JSON\"\n\nif [[ ${#UNCOMMITTED_FILES[@]} -eq 0 ]]; then\n    [[ $# -ge 3 ]] && echo \"OK: All files for $TASK_ID already committed\"\n    exit 0\nfi\n\n[[ $# -ge 3 ]] && echo \"COMMIT: Found ${#UNCOMMITTED_FILES[@]} uncommitted files for $TASK_ID\"\n\n# Stage and commit\nfor file in \"${UNCOMMITTED_FILES[@]}\"; do\n    [[ $# -ge 3 ]] && echo \"  + $file\"\n    git add \"$file\"\ndone\n\nCOMMIT_MSG=\"${TASK_ID}: ${TASK_NAME}\"\ngit commit -m \"$COMMIT_MSG\" --no-verify\n\nCOMMIT_SHA=$(git rev-parse HEAD)\n[[ $# -ge 3 ]] && echo \"COMMITTED: $COMMIT_SHA - $COMMIT_MSG\"\n\n# Update result file with commit info using Go CLI\n\"$TASKER_BIN\" -p \"$TASKER_DIR\" bundle update-git \"$TASK_ID\" --sha=\"$COMMIT_SHA\" --msg=\"$COMMIT_MSG\" 2>/dev/null || true\n\n[[ $# -ge 3 ]] && echo \"OK: Updated $RESULT_FILE with commit info\"\nexit 0\n",
        "hooks/subagent_stop.sh": "#!/bin/bash\n# SubagentStop hook - delegates to Go binary for token logging\nexec tasker hook subagent-stop\n",
        "internal/commands/executor-status.md": "---\ndescription: Display executor status dashboard with task progress and recent activity\n---\n\nRun the dashboard command to display the current executor status:\n\n```bash\ntasker state status\n```\n\nIf the user requests compact output, use `--compact`. If they request JSON, use `--json`.\n",
        "internal/commands/status.md": "---\ndescription: Display executor status dashboard with task progress and recent activity\n---\n\nRun the status command to display a workflow dashboard:\n\n```bash\ntasker state status --once\n```\n\nThis shows:\n- Current phase and target directory\n- Progress (completed/total tasks, current phase)\n- Status breakdown (completed, running, failed, blocked, pending)\n- Health checks (DAG validation, steel thread, verification commands)\n- Verifier calibration score\n- Cost metrics (tokens, USD)\n- Active tasks currently running\n- Recent failures\n\nFor interactive TUI mode, use:\n```bash\ntasker tui\n```\n\nFor JSON output:\n```bash\ntasker state status --json\n```\n",
        "internal/commands/tasker-to-beads.md": "# Tasker to Beads\n\nTransform Tasker task definitions into rich, self-contained Beads issues.\n\nUse the `tasker-to-beads` skill: `Skill(\"tasker-to-beads\")`\n\n## What This Does\n\n1. **Asks for target directory** — Where development will happen (may differ from planning project)\n2. **Initializes beads** in the target directory if not already done\n3. Extracts structural data from Tasker task files\n4. Uses LLM comprehension to enrich with spec context, architecture narrative, and human-readable descriptions\n5. Creates Beads issues with proper labels, priorities, and dependencies in the target project\n\n## Interactive Workflow\n\nWhen invoked, this command will:\n\n1. **First**, ask: \"Where would you like to create the Beads issues? This should be the directory where the actual development will take place.\"\n   - Options: Current directory, specify path, or common locations\n2. **Check** if beads is initialized in that directory\n3. **Initialize beads** if needed (with option to customize issue prefix)\n4. **Process tasks** according to mode selected\n\n## Usage\n\n- `/tasker-to-beads` — Interactive mode, asks for target directory and processes all tasks\n- `/tasker-to-beads T001` — Process single task by ID (still asks for target)\n- `/tasker-to-beads --batch` — Process all tasks and create manifest for batch import\n- `/tasker-to-beads --target /path/to/project` — Skip target prompt, use specified directory\n\n## CLI Commands\n\nThe tasker binary includes transform commands for mechanical transformations:\n\n```bash\n# Check status of source and target\ntasker transform status -t /path/to/target\n\n# Initialize beads in target with custom prefix\n# Runs: bd init <PREFIX> && bd onboard\ntasker transform init-target /path/to/target FATHOM\n\n# Prepare context for all tasks\ntasker transform context --all\n\n# Create issue in target directory\ntasker transform create T001 .tasker/beads-export/T001-enriched.json -t /path/to/target\n```\n\n## Example Session\n\n```\nUser: /tasker-to-beads\n\nClaude: I'll help you transform Tasker tasks into Beads issues.\n\nFirst, where would you like to create the Beads issues?\n- [ ] Current directory (/Users/dev/tasker)\n- [ ] Specify a different path\n\nUser: /Users/dev/fathom\n\nClaude: Checking /Users/dev/fathom...\n  - Directory exists: Yes\n  - Beads initialized: No\n\nI'll initialize Beads with prefix \"FATHOM\". Continue? [Y/n]\n\n...\n```\n",
        "internal/commands/tui.md": "---\ndescription: Launch the TUI status dashboard in a tmux split pane\n---\n\nLaunch the TUI dashboard:\n\n```bash\n.claude/hooks/launch-tui.sh\n```\n\nThis opens a horizontal split at the bottom of your tmux window showing:\n- Current phase and progress\n- Task status breakdown\n- Health checks (DAG, steel thread, verification)\n- Verifier calibration\n- Cost metrics\n- Active and recently completed tasks\n\nThe TUI auto-refreshes every 5 seconds.\n\n**Keybindings in TUI:**\n- `r` - Manual refresh\n- `a` - Toggle auto-refresh\n- `d` - Toggle dark/light mode\n- `q` - Quit\n\nTo close the TUI pane:\n```bash\n.claude/hooks/close-tui.sh\n```\n\nOr just press `q` in the TUI, then Enter.\n",
        "internal/commands/verify-plan.md": "# Verify Plan\n\nSpawn the `task-plan-verifier` subagent to evaluate task definitions.\n\n## Instruction\n\nSpawn the **task-plan-verifier** agent with:\n\n```\nVerify task definitions for planning\n\nSpec: .tasker/inputs/spec.md\nCapability Map: .tasker/artifacts/capability-map.json\nTasks Directory: .tasker/tasks/\nUser Preferences: ~/.claude/CLAUDE.md (if exists)\n```\n\n## When to Use\n\n- After editing task files to check alignment\n- To re-run validation after fixing BLOCKED issues\n- To verify tasks match your `~/.claude/CLAUDE.md` coding preferences\n\n## Verdicts\n\n| Verdict | Meaning |\n|---------|---------|\n| READY | All tasks pass, can proceed |\n| READY_WITH_NOTES | Pass with minor issues documented |\n| BLOCKED | Critical issues must be fixed first |\n",
        "internal/skills/_archived/orchestrator/SKILL.md": "---\nname: orchestrator\ndescription: Thin orchestrator for Task Decomposition Protocol v2. Supports two modes - /plan (decompose spec into tasks) and /execute (run tasks via subagents). Delegates all state management to the tasker CLI.\ntools:\n  - agent\n  - bash\n  - file_read\n  - file_write\n---\n\n# Orchestrator v2\n\nA **thin coordination layer** with two distinct modes:\n- **Plan Mode** (`/plan`) - Decompose spec into task DAG\n- **Execute Mode** (`/execute`) - Run tasks via isolated subagents\n\n## Workflow Integration\n\nThe full workflow from requirements to implementation:\n\n```\n/specify → /plan → /execute\n```\n\n| Skill | Purpose | Input | Output |\n|-------|---------|-------|--------|\n| `/specify` | Design vision & capabilities | User requirements | `{TARGET}/docs/specs/<slug>.md` + `.capabilities.json` + ADRs |\n| `/plan` | DAG construction (files → tasks) | Spec + capability map | Task DAG in `.tasker/tasks/` |\n| `/execute` | Implementation | Task DAG | Working code |\n\n**Entry points:**\n- **From scratch** → Run `/specify` first to develop spec and extract capabilities\n- **Existing spec** → Run `/plan {TARGET}/docs/specs/<slug>.md` directly (may need capability extraction)\n- **From /specify** → Run `/plan {TARGET}/docs/specs/<slug>.md` (skips to physical mapping)\n\n## Philosophy\n\nThe orchestrator does NOT:\n- Track state itself (tasker CLI does this)\n- Validate artifacts (tasker CLI does this)\n- Compute ready tasks (tasker CLI does this)\n\nThe orchestrator ONLY:\n- Queries state via `tasker state`\n- Dispatches agents based on mode/phase\n- Handles user interaction\n- **Ensures commits happen** via `.claude/hooks/post-task-commit.sh` (defense in depth)\n\n---\n\n## Directory Initialization (After target_dir is known)\n\n**CRITICAL:** After obtaining target_dir from the user, the orchestrator MUST initialize the `.tasker/` directory structure. This is the ONLY place where directories are created - sub-agents assume directories already exist.\n\n### Setup (Run after target_dir is confirmed)\n\nInitialize the `.tasker/` directory structure:\n\n```bash\nTASKER_DIR=\"$TARGET_DIR/.tasker\"\nmkdir -p \"$TASKER_DIR\"/{artifacts,inputs,tasks,reports,bundles,logs}\n```\n\nThis creates:\n- `$TARGET_DIR/.tasker/artifacts/` - For capability-map.json, physical-map.json\n- `$TARGET_DIR/.tasker/inputs/` - For spec.md\n- `$TARGET_DIR/.tasker/tasks/` - For T001.json, T002.json, etc.\n- `$TARGET_DIR/.tasker/reports/` - For task-validation-report.md\n- `$TARGET_DIR/.tasker/bundles/` - For execution bundles\n- `$TARGET_DIR/.tasker/logs/` - For activity logging\n\nSet TASKER_DIR as the working directory for all operations:\n```bash\nTASKER_DIR=\"$TARGET_DIR/.tasker\"\necho \"TASKER_DIR: $TASKER_DIR\"\n```\n\n**Why centralized initialization?**\n1. **Reliability**: Sub-agents run in isolated contexts and directory creation has been unreliable\n2. **Single responsibility**: Orchestrator owns the directory structure, sub-agents own the content\n3. **Fail-fast**: If directory creation fails, we catch it immediately rather than mid-workflow\n\n**IMPORTANT:** Sub-agents must NOT create directories. They assume the directory structure already exists. If a sub-agent encounters a \"directory does not exist\" error, it indicates the orchestrator failed to initialize properly.\n\n## Runtime Logging (MANDATORY)\n\nAll orchestrator activity and sub-agent activity MUST be logged using `./scripts/log-activity.sh`.\n\n### Logging Script Usage\n\n```bash\n./scripts/log-activity.sh <LEVEL> <AGENT> <EVENT> \"<MESSAGE>\"\n```\n\n**Parameters:**\n- `LEVEL`: INFO, WARN, ERROR\n- `AGENT`: orchestrator, logic-architect, physical-architect, task-author, etc.\n- `EVENT`: start, decision, tool, complete, spawn, spawn-complete, phase-transition, validation\n- `MESSAGE`: Description of the activity\n\n### Orchestrator Logging Examples\n\n```bash\n./scripts/log-activity.sh INFO orchestrator phase-transition \"moving from logical to physical\"\n./scripts/log-activity.sh INFO orchestrator spawn \"launching logic-architect for capability extraction\"\n./scripts/log-activity.sh INFO orchestrator spawn-complete \"logic-architect finished with SUCCESS\"\n./scripts/log-activity.sh INFO orchestrator validation \"capability_map - PASSED\"\n```\n\n### Sub-Agent Logging Instructions\n\n**CRITICAL:** Include these logging instructions in EVERY sub-agent spawn prompt. Sub-agents are context-isolated and will not see this skill file - they must receive logging instructions explicitly.\n\nAdd this block to every spawn prompt:\n\n```\n## Logging (MANDATORY)\n\nLog your activity using the logging script:\n\n\\`\\`\\`bash\n./scripts/log-activity.sh INFO $AGENT_NAME start \"Starting task description\"\n./scripts/log-activity.sh INFO $AGENT_NAME decision \"What decision and why\"\n./scripts/log-activity.sh INFO $AGENT_NAME complete \"Outcome description\"\n./scripts/log-activity.sh WARN $AGENT_NAME warning \"Warning message\"\n./scripts/log-activity.sh ERROR $AGENT_NAME error \"Error message\"\n\\`\\`\\`\n\nReplace $AGENT_NAME with your agent name (e.g., logic-architect, task-executor).\n```\n\n## CRITICAL: Path Management\n\n**TASKER_DIR must be an absolute path.** Sub-agents run in isolated contexts and cannot resolve relative paths correctly.\n\nAfter obtaining target_dir from user, compute and store:\n```bash\n# Get absolute path to .tasker directory within target project\nTARGET_DIR=\"<user-provided-absolute-path>\"\nTASKER_DIR=\"$TARGET_DIR/.tasker\"\necho \"TASKER_DIR: $TASKER_DIR\"\n```\n\nThis `TASKER_DIR` value (e.g., `/Users/foo/my-project/.tasker`) MUST be passed to every sub-agent spawn. Do NOT use relative paths like `.tasker/` in spawn prompts.\n\n**NOTE:** Throughout this document, `TASKER_DIR` replaces the old `PLANNING_DIR` variable. They serve the same purpose.\n\n---\n\n# Plan Mode\n\nTriggered by `/plan`. Runs phases 0-6 (spec review through ready).\n\n## MANDATORY FIRST STEP: Ask for Target Project Directory\n\n**ALWAYS ask for target_dir FIRST before anything else.** No guessing, no inference from CWD.\n\n### Step 1: Ask for Target Directory\n\nUse AskUserQuestion to ask:\n```\nWhat is the target project directory?\n```\nFree-form text input. User must provide an absolute or relative path.\n\n**Validation:**\n```bash\nTARGET_DIR=\"<user-provided-path>\"\n# Convert to absolute path\nTARGET_DIR=$(cd \"$TARGET_DIR\" 2>/dev/null && pwd || echo \"$TARGET_DIR\")\n\nif [ ! -d \"$TARGET_DIR\" ]; then\n    # For new projects, check parent exists\n    PARENT=$(dirname \"$TARGET_DIR\")\n    if [ -d \"$PARENT\" ]; then\n        echo \"Directory will be created: $TARGET_DIR\"\n        mkdir -p \"$TARGET_DIR\"\n    else\n        echo \"Error: Parent directory does not exist: $PARENT\"\n        # Re-ask for target_dir\n    fi\nfi\n```\n\n### Step 2: Check for Existing Session and Initialize\n\nAfter target_dir is confirmed, check for existing `.tasker/` state:\n\n```bash\nTASKER_DIR=\"$TARGET_DIR/.tasker\"\nif [ -f \"$TASKER_DIR/state.json\" ]; then\n    echo \"Found existing tasker session at $TASKER_DIR\"\n    echo \"Resuming from saved state...\"\n    # Read phase from state.json and resume\n    tasker state status\nelse\n    echo \"No existing session. Initializing...\"\n    # Initialize directory structure (see Directory Initialization below)\nfi\n```\n\n### Step 3: Detect Specs (Automatic)\n\n**Specs are REQUIRED for /plan to proceed.** Check `$TARGET_DIR/docs/specs/` for specs from /specify:\n\n```bash\nSPEC_DIR=\"$TARGET_DIR/docs/specs\"\n\n# Find spec files (from /specify workflow)\nSPEC_FILES=$(find \"$SPEC_DIR\" -maxdepth 1 -name \"*.md\" 2>/dev/null)\nCAP_MAPS=$(find \"$SPEC_DIR\" -maxdepth 1 -name \"*.capabilities.json\" 2>/dev/null)\n\nif [ -n \"$SPEC_FILES\" ]; then\n    echo \"=== Specs found ===\"\n    echo \"$SPEC_FILES\"\n\n    # Use the first spec (or only spec)\n    SPEC_PATH=$(echo \"$SPEC_FILES\" | head -1)\n    SPEC_SLUG=$(basename \"$SPEC_PATH\" .md)\n\n    # Check for matching capability map\n    CAP_MAP=\"$SPEC_DIR/${SPEC_SLUG}.capabilities.json\"\n    if [ -f \"$CAP_MAP\" ]; then\n        echo \"Capability map found: $CAP_MAP\"\n        echo \"Can skip logic-architect phase\"\n    fi\nelse\n    echo \"No specs found in $SPEC_DIR\"\n    # BLOCK - see below\nfi\n```\n\n### If spec found:\n\nProceed to Step 4. No user question needed.\n\n### If NO spec found — BLOCK and offer /specify:\n\n**/plan CANNOT proceed without specs.** Present this to the user:\n\n```markdown\n## Specs Required\n\nPlanning requires a specification to decompose into tasks. Without a spec, there's nothing to plan.\n\n**Would you like to create a spec now using `/specify`?**\n\nThe `/specify` workflow will guide you through:\n1. Defining goals and scope\n2. Clarifying requirements through structured questions\n3. Extracting capabilities and behaviors\n4. Producing a spec ready for `/plan`\n```\n\nAsk using AskUserQuestion:\n```\nWould you like to run /specify to create a spec now?\n```\nOptions:\n- **Yes, start /specify** — Begin the specification workflow\n- **No, I'll provide a spec later** — Exit /plan for now\n\n**If user chooses \"Yes, start /specify\":**\n1. Invoke the `tasker:specify` skill with the already-collected `target_dir`\n2. Pass context: `TARGET_DIR` is already known, skip Step 1 of /specify\n3. After /specify completes, automatically return to /plan\n\n**If user chooses \"No\":**\nExit gracefully with message: \"Run `/plan` again when you have a spec ready, or use `/specify` to create one.\"\n\n### Step 4: Detect Project Type (Automatic)\n\n**Do not ask the user** — infer project type from target directory:\n\n```bash\n# Check if target directory has source code\nSOURCE_FILES=$(find \"$TARGET_DIR\" \\( -name \"*.py\" -o -name \"*.ts\" -o -name \"*.js\" -o -name \"*.go\" -o -name \"*.rs\" -o -name \"*.java\" \\) \\\n    -not -path \"*node_modules*\" -not -path \"*__pycache__*\" -not -path \"*.venv*\" -not -path \"*/.git/*\" 2>/dev/null | head -5)\n\nif [ -n \"$SOURCE_FILES\" ]; then\n    PROJECT_TYPE=\"existing\"\n    echo \"Detected existing project with source files\"\nelse\n    PROJECT_TYPE=\"new\"\n    echo \"New project (no existing source files)\"\nfi\n```\n\n- **Existing project** → Proceed to Step 5 (project analysis)\n- **New project** → Skip to Step 6 (tech stack)\n\n### Step 5: Existing Project Analysis (if PROJECT_TYPE=existing)\n\n**If enhancing an existing project**, you MUST analyze the target directory **BEFORE proceeding to ingestion**. This analysis is CRITICAL - sub-agents cannot see the codebase, so you must extract and pass this context to them.\n\n```bash\n# Check directory exists\nif [ ! -d \"$TARGET_DIR\" ]; then\n    echo \"Error: Target directory does not exist\"\n    exit 1\nfi\n\n# Analyze structure (capture output for context)\necho \"=== Project Structure ===\"\ntree -L 3 -I 'node_modules|__pycache__|.git|venv|.venv|dist|build|.pytest_cache' \"$TARGET_DIR\" 2>/dev/null || \\\n    find \"$TARGET_DIR\" -maxdepth 3 -type f | head -50\n\n# Identify key configuration files\necho \"=== Key Configuration Files ===\"\nfor f in package.json pyproject.toml Cargo.toml go.mod Makefile requirements.txt setup.py tsconfig.json; do\n    [ -f \"$TARGET_DIR/$f\" ] && echo \"Found: $f\"\ndone\n\n# Detect source layout patterns\necho \"=== Source Layout ===\"\nfor d in src lib app pkg cmd internal; do\n    [ -d \"$TARGET_DIR/$d\" ] && echo \"Found directory: $d/\"\ndone\n\n# Detect test layout\necho \"=== Test Layout ===\"\nfor d in tests test spec __tests__; do\n    [ -d \"$TARGET_DIR/$d\" ] && echo \"Found test directory: $d/\"\ndone\n\n# Sample existing code files to understand patterns\necho \"=== Code Samples ===\"\nfind \"$TARGET_DIR\" \\( -name \"*.py\" -o -name \"*.ts\" -o -name \"*.js\" -o -name \"*.go\" -o -name \"*.rs\" \\) \\\n    -not -path \"*node_modules*\" -not -path \"*__pycache__*\" -not -path \"*.venv*\" | head -10\n```\n\n**Read key files to understand patterns:**\n```bash\n# Read config files to understand dependencies and structure\n[ -f \"$TARGET_DIR/pyproject.toml\" ] && cat \"$TARGET_DIR/pyproject.toml\"\n[ -f \"$TARGET_DIR/package.json\" ] && cat \"$TARGET_DIR/package.json\"\n\n# Sample a few source files to understand coding patterns\n# (naming conventions, import style, architecture patterns)\n```\n\n**Present findings and store context:**\n```markdown\n## Existing Project Analysis\n\n**Directory:** /path/to/project\n**Stack Detected:** Python 3.11+ (pyproject.toml with uv)\n**Source Layout:** src/ with module structure\n**Test Layout:** tests/ mirroring src/\n\n**Key Configuration:**\n- pyproject.toml: dependencies include fastapi, pydantic, loguru\n- Uses ruff for linting, pytest for testing\n\n**Discovered Patterns:**\n- Naming: snake_case for files and functions\n- Imports: absolute imports from src root\n- Architecture: Protocol-based interfaces in src/interfaces/\n- Testing: pytest with fixtures in conftest.py\n\n**Key Files:**\n- src/main.py (entry point)\n- src/interfaces/ (Protocol definitions)\n- src/services/ (business logic)\n- tests/conftest.py (shared fixtures)\n\n**Integration Considerations:**\n- New code must follow existing module structure\n- Must use existing Protocols for interfaces\n- Tests should extend existing fixtures\n- Must pass ruff and existing test suite\n\nProceed with planning? (y/n)\n```\n\n**Tech stack comes from the spec.** A well-developed spec (from /specify) includes tech stack requirements. Do not ask for tech stack separately — read it from the spec.\n\n### Step 6: Store Discovery Context\n\n**CRITICAL:** You must retain this analysis for passing to sub-agents. Store it as a structured context block:\n\n```\nPROJECT_CONTEXT = \"\"\"\nDirectory: {TARGET_DIR}\nProject Type: existing\nStack: {detected stack}\nSource Layout: {layout pattern}\nTest Layout: {test pattern}\n\nKey Patterns:\n- {pattern 1}\n- {pattern 2}\n- {pattern 3}\n\nIntegration Requirements:\n- {requirement 1}\n- {requirement 2}\n\"\"\"\n```\n\nThis `PROJECT_CONTEXT` MUST be included in every sub-agent spawn prompt (logic-architect, physical-architect, task-author). Without it, sub-agents will design solutions that conflict with existing code.\n\n## Ingestion: Storing the Spec\n\nFirst, check if the spec already exists (directory was created during initialization):\n\n```bash\nif [ -f \"$TASKER_DIR/inputs/spec.md\" ]; then\n    echo \"Spec found, proceeding to planning...\"\nfi\n```\n\n**If spec already exists:** Skip ingestion, proceed to logical phase.\n\n**If spec doesn't exist**, ask user for specification, then:\n\n- **User provides a file path** → `cp /path/to/spec \"$TASKER_DIR/inputs/spec.md\"`\n- **User pastes content** → Write it to `$TASKER_DIR/inputs/spec.md`\n\n**Important:** Store the spec exactly as provided - no transformation, summarization, or normalization.\n\n## Skip Phases for /specify-Generated Specs\n\nIf the spec came from `/specify` workflow, it has already been reviewed AND capabilities have been extracted. Check for artifacts:\n\n```bash\n# Derive capability map path from spec path\n# e.g., /path/to/project/docs/specs/my-feature.md → /path/to/project/docs/specs/my-feature.capabilities.json\nSPEC_DIR=$(dirname \"$SPEC_PATH\")\nSPEC_SLUG=$(basename \"$SPEC_PATH\" .md)\nCAPABILITY_MAP=\"${SPEC_DIR}/${SPEC_SLUG}.capabilities.json\"\n\n# Check for /specify artifacts (spec review may be in tasker's .claude/)\nSPEC_REVIEW=\".claude/spec-review.json\"\n\nif [ -f \"$CAPABILITY_MAP\" ]; then\n    echo \"Capability map found from /specify workflow: $CAPABILITY_MAP\"\n    echo \"Copying artifacts and skipping to physical phase...\"\n\n    # Copy artifacts to planning directory\n    cp \"$CAPABILITY_MAP\" \"$TASKER_DIR/artifacts/capability-map.json\"\n    [ -f \"$SPEC_REVIEW\" ] && cp \"$SPEC_REVIEW\" \"$TASKER_DIR/artifacts/spec-review.json\"\n\n    # Check for FSM artifacts from /specify workflow\n    FSM_DIR=\"${SPEC_DIR}/../fsm/${SPEC_SLUG}\"\n    if [ -d \"$FSM_DIR\" ]; then\n        echo \"FSM artifacts found from /specify workflow: $FSM_DIR\"\n        mkdir -p \"$TASKER_DIR/artifacts/fsm\"\n        cp -r \"$FSM_DIR\"/* \"$TASKER_DIR/artifacts/fsm/\"\n\n        # Validate FSM artifacts\n        tasker fsm validate validate \"$TASKER_DIR/artifacts/fsm\"\n        if [ $? -ne 0 ]; then\n            echo \"WARNING: FSM validation failed. Review artifacts before proceeding.\"\n        fi\n    fi\n\n    # Skip spec_review AND logical phases - advance directly to physical\n    tasker state set-phase physical\nfi\n```\n\n**Skip phases when `/specify` artifacts exist:**\n- `spec_review` - Skipped (already done by `/specify` Phase 7)\n- `logical` - Skipped (capability map already exists from `/specify` Phase 3)\n\n## Plan Phase Dispatch\n\n```bash\n# Initialize if no state exists\nif [ ! -f \"$TASKER_DIR/state.json\" ]; then\n    tasker state init \"$TARGET_DIR\"\nfi\n\n# Check current phase\ntasker state status\n```\n\n| Phase | Agent | Output | Validation | Skip if |\n|-------|-------|--------|------------|---------|\n| `ingestion` | (none) | `inputs/spec.md` (verbatim) | File exists | — |\n| `spec_review` | **spec-reviewer** | `artifacts/spec-review.json` | All critical resolved | `/specify` artifacts exist |\n| `logical` | **logic-architect** | `artifacts/capability-map.json` | `validate capability_map` | `/specify` artifacts exist |\n| `physical` | **physical-architect** | `artifacts/physical-map.json` | `validate physical_map` | — |\n| `definition` | **task-author** | `tasks/*.json` | `load-tasks` | — |\n| `validation` | **task-plan-verifier** | Validation report | `validate-tasks <verdict>` | — |\n| `sequencing` | **plan-auditor** | Updated task phases | DAG is valid | — |\n| `ready` | (done) | Planning complete | — | — |\n\n**Entry points based on artifacts:**\n- **No artifacts** → Start at `ingestion` (raw spec provided)\n- **Has `.capabilities.json`** → Start at `physical` (came from `/specify`)\n\n**Note:** When starting at `physical`, the capability-map.json is copied from `{TARGET}/docs/specs/<slug>.capabilities.json` to `artifacts/capability-map.json`.\n\n## Plan Loop\n\n```python\nwhile phase not in [\"ready\", \"executing\", \"complete\"]:\n    1. Query current phase\n    2. Spawn appropriate agent WITH FULL CONTEXT (see spawn templates below)\n    3. Wait for agent to complete\n    4. **VERIFY OUTPUT EXISTS** (critical - see below)\n       - DO NOT log spawn-complete until file verified\n       - DO NOT proceed to validation until file verified\n    5. If file missing: RE-SPAWN agent immediately (see recovery below)\n    6. Validate output:\n       - For artifacts: state.py validate <artifact>\n       - For task validation: state.py validate-tasks <verdict>\n    7. If valid: state.py advance\n    8. If invalid: Tell agent to fix, re-validate\n```\n\n**CRITICAL: Never log \"spawn-complete: SUCCESS\" until the output file is verified to exist!**\n\n## CRITICAL: Output Verification Before Validation\n\n**MANDATORY STEP:** After each agent completes, you MUST verify its output file exists before attempting validation.\n\nNote: `$TASKER_DIR` below refers to the absolute path you passed to the agent (e.g., `/Users/foo/my-project/.tasker`).\n\n```bash\n# After logic-architect completes:\nif [ ! -f $TASKER_DIR/artifacts/capability-map.json ]; then\n    echo \"ERROR: capability-map.json not written. Agent must retry.\"\n    # Re-spawn the agent with explicit reminder to use Write tool\nfi\n\n# After physical-architect completes:\nif [ ! -f $TASKER_DIR/artifacts/physical-map.json ]; then\n    echo \"ERROR: physical-map.json not written. Agent must retry.\"\n    # Re-spawn the agent with explicit reminder to use Write tool\nfi\n\n# After task-author completes:\ntask_count=$(ls $TASKER_DIR/tasks/*.json 2>/dev/null | wc -l)\nif [ \"$task_count\" -eq 0 ]; then\n    echo \"ERROR: No task files written. Agent must retry.\"\n    # Re-spawn the agent with explicit reminder to use Write tool\nfi\n```\n\n**Why this matters:** Sub-agents may fail silently (e.g., output JSON to conversation instead of writing to file, or write to wrong directory). The orchestrator MUST verify files exist at the correct absolute path before calling `state.py validate`, otherwise validation will fail with \"Artifact not found\" which is confusing.\n\n**Recovery procedure:** If file doesn't exist:\n1. Check if directory exists: `ls -la $TASKER_DIR/artifacts/`\n2. Re-spawn the agent with this explicit reminder:\n   > \"IMPORTANT: You must use the Write tool to save the file to the absolute path {TASKER_DIR}/artifacts/. Simply outputting JSON to the conversation is NOT sufficient. Do NOT use relative paths like .tasker/.\"\n\n## Agent Spawn Templates\n\n**CRITICAL:** Each sub-agent is context-isolated. They CANNOT see the orchestrator's conversation or any information you've gathered from the user. You MUST pass ALL relevant context explicitly in the spawn prompt.\n\n**MANDATORY:** For existing projects, you MUST include the full PROJECT_CONTEXT from the discovery phase. Sub-agents have no visibility into the target codebase - they rely entirely on the context you provide.\n\n### Spec Review Phase: spec-reviewer\n\n**Spawn prompt for spec-reviewer:**\n\n```\nAnalyze the specification for weaknesses before capability extraction.\n\n## Logging (MANDATORY)\n\nLog your activity using the logging script:\n\n./scripts/log-activity.sh INFO spec-reviewer start \"Analyzing spec for weaknesses\"\n./scripts/log-activity.sh INFO spec-reviewer decision \"What decision and why\"\n./scripts/log-activity.sh INFO spec-reviewer complete \"Outcome description\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory, e.g., /Users/foo/my-project/.tasker}\n\n## Your Task\n\n1. Run weakness detection:\n   tasker spec review analyze {TASKER_DIR}/inputs/spec.md\n\n2. Save results to {TASKER_DIR}/artifacts/spec-review.json\n\n3. For CRITICAL weaknesses (W1: Non-behavioral, W6: Contradictions):\n   - Use AskUserQuestion tool to engage user for resolution\n   - Record resolutions to {TASKER_DIR}/artifacts/spec-resolutions.json\n\n4. Check status:\n   tasker spec review status {TASKER_DIR}\n\nCRITICAL WEAKNESS CATEGORIES:\n- W1: Non-behavioral (DDL/schema not stated as behavior) - Ask: \"Should DDL be DB-level or app-layer?\"\n- W6: Contradictions (conflicting statements) - Ask: \"Which statement is authoritative?\"\n\nWARNING CATEGORIES (proceed with notes):\n- W2: Implicit requirements\n- W3: Cross-cutting concerns\n- W5: Fragmented requirements\n\nINFO CATEGORIES (log only):\n- W4: Missing acceptance criteria\n\nIMPORTANT:\n- You MUST resolve all CRITICAL weaknesses before signaling completion\n- Use AskUserQuestion for each critical weakness requiring user input\n- Save resolutions in spec-resolutions.json for the logic-architect to consume\n\n**Note:** The orchestrator has already created all required directories. Do NOT create directories yourself.\n```\n\n### Logical Phase: logic-architect\n\n**Spawn prompt for logic-architect:**\n\n```\nExtract capabilities and behaviors from the specification.\n\n## Logging (MANDATORY)\n\nLog your activity using the logging script:\n\n./scripts/log-activity.sh INFO logic-architect start \"Extracting capabilities from spec\"\n./scripts/log-activity.sh INFO logic-architect decision \"What decision and why\"\n./scripts/log-activity.sh INFO logic-architect complete \"Outcome description\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory, e.g., /Users/foo/my-project/.tasker}\nTarget Directory: {TARGET_DIR}\nProject Type: {new | existing}\nTech Stack: {user-provided constraints or \"none specified\"}\n\n## Project Context (CRITICAL for existing projects)\n\n{INSERT FULL PROJECT_CONTEXT HERE - this is MANDATORY for existing projects}\n\nExample for existing project:\n\"\"\"\nDirectory: /Users/foo/my-app\nProject Type: existing\nStack: Python 3.11+ with FastAPI, managed by uv\nSource Layout: src/ with module packages\nTest Layout: tests/ mirroring src structure\n\nKey Patterns:\n- Naming: snake_case for files and functions\n- Imports: absolute imports from src root\n- Architecture: Protocol-based interfaces in src/interfaces/\n- Error handling: Custom exceptions in src/exceptions.py\n- Logging: loguru with structured logging\n\nIntegration Requirements:\n- New capabilities must define Protocols in src/interfaces/\n- Implementations go in src/services/ or src/domain/\n- Must not duplicate existing functionality\n- Must integrate with existing error handling patterns\n\"\"\"\n\nFor new projects, state: \"New project - no existing patterns to follow\"\n\n## Specification Location\n\nThe full specification is in: {TASKER_DIR}/inputs/spec.md\n\nRead that file for the complete requirements. The spec has already been stored verbatim.\n\n## Your Task\n\n1. Read {TASKER_DIR}/inputs/spec.md\n2. Read {TASKER_DIR}/artifacts/spec-resolutions.json if it exists\n   - Resolutions marked \"mandatory\" MUST become explicit behaviors\n   - Non-behavioral requirements (W1) should be tagged for explicit tasks\n   - Cross-cutting concerns (W3) should be flagged for dedicated capabilities\n3. **For existing projects:** Consider how new capabilities integrate with existing structure\n4. Extract capabilities using I.P.S.O. decomposition\n5. Apply phase filtering (Phase 1 only)\n6. **CRITICAL: Use the Write tool** to save to {TASKER_DIR}/artifacts/capability-map.json\n7. **Verify file exists**: `ls -la {TASKER_DIR}/artifacts/capability-map.json`\n8. Validate with: `cd {TASKER_DIR}/.. && tasker state validate capability_map`\n\nIMPORTANT - YOUR TASK IS NOT COMPLETE UNTIL:\n1. You MUST use the Write tool to save the file. Simply outputting JSON to the conversation is NOT sufficient.\n2. Use the TASKER_DIR absolute path provided above. Do NOT use relative paths.\n3. After Write, you MUST verify: `ls -la {TASKER_DIR}/artifacts/capability-map.json` - if file doesn't exist, Write again!\n4. You MUST run validation and confirm it passes.\n5. For existing projects, ensure capabilities don't duplicate what already exists in the codebase.\n\n**Note:** The orchestrator has already created all required directories. Do NOT create directories yourself. If you encounter \"directory does not exist\" errors, report this to the orchestrator.\n\nIf you complete without the file existing at the absolute path, you have FAILED.\n```\n\n### Physical Phase: physical-architect\n\n**Spawn prompt for physical-architect:**\n\n```\nMap behaviors to concrete file paths.\n\n## Logging (MANDATORY)\n\nLog your activity using the logging script:\n\n./scripts/log-activity.sh INFO physical-architect start \"Mapping behaviors to file paths\"\n./scripts/log-activity.sh INFO physical-architect decision \"What decision and why\"\n./scripts/log-activity.sh INFO physical-architect complete \"Outcome description\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory, e.g., /Users/foo/my-project/.tasker}\nTarget Directory: {TARGET_DIR}\nProject Type: {new | existing}\nTech Stack: {user-provided constraints or \"infer from capability-map\"}\n\n## Project Context (CRITICAL for existing projects)\n\n{INSERT FULL PROJECT_CONTEXT HERE - this is MANDATORY for existing projects}\n\nThis context tells you:\n- Where source files should go (e.g., src/services/, src/domain/)\n- Where tests should go (e.g., tests/ mirroring src/)\n- Naming conventions to follow\n- Existing patterns to integrate with\n- Files/modules that already exist (don't recreate them)\n\nFor new projects, state: \"New project - establish sensible conventions\"\n\n## Your Task\n\n1. Read {TASKER_DIR}/artifacts/capability-map.json\n2. **For existing projects:** Map behaviors to paths that FIT the existing structure\n   - Use existing directories (don't create parallel structures)\n   - Follow established naming conventions\n   - Integrate with existing modules where appropriate\n3. For new projects: Establish clean, conventional structure\n4. Add cross-cutting concerns and infrastructure\n5. **CRITICAL: Use the Write tool** to save to {TASKER_DIR}/artifacts/physical-map.json\n6. **Verify file exists**: `ls -la {TASKER_DIR}/artifacts/physical-map.json`\n7. Validate with: `cd {TASKER_DIR}/.. && tasker state validate physical_map`\n\nIMPORTANT - YOUR TASK IS NOT COMPLETE UNTIL:\n1. You MUST use the Write tool to save the file. Simply outputting JSON to the conversation is NOT sufficient.\n2. Use the TASKER_DIR absolute path provided above. Do NOT use relative paths.\n3. After Write, you MUST verify: `ls -la {TASKER_DIR}/artifacts/physical-map.json` - if file doesn't exist, Write again!\n4. You MUST run validation and confirm it passes.\n5. For existing projects, respect the established structure - don't fight it.\n\n**Note:** The orchestrator has already created all required directories. Do NOT create directories yourself. If you encounter \"directory does not exist\" errors, report this to the orchestrator.\n\nIf you complete without the file existing at the absolute path, you have FAILED.\n```\n\n### Definition Phase: task-author\n\n**Spawn prompt for task-author:**\n\n```\nCreate individual task files from the physical map.\n\n## Logging (MANDATORY)\n\nLog your activity using the logging script:\n\n./scripts/log-activity.sh INFO task-author start \"Creating task files from physical map\"\n./scripts/log-activity.sh INFO task-author decision \"What decision and why\"\n./scripts/log-activity.sh INFO task-author complete \"Outcome description\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory, e.g., /Users/foo/my-project/.tasker}\nTarget Directory: {TARGET_DIR}\nProject Type: {new | existing}\n\n## Project Context (for existing projects)\n\n{INSERT FULL PROJECT_CONTEXT HERE if existing project}\n\nKey information for task definitions:\n- Testing patterns (what framework, where fixtures live)\n- Linting/formatting requirements (ruff, eslint, etc.)\n- Build/run commands (make test, uv run pytest, npm test)\n- Integration points with existing code\n\n## Tech Stack Constraints\n{user-provided constraints that affect implementation}\n\n## Your Task\n\n1. Read {TASKER_DIR}/artifacts/physical-map.json\n2. Read {TASKER_DIR}/artifacts/capability-map.json (for behavior details)\n3. **Check for FSM artifacts**: If {TASKER_DIR}/artifacts/fsm/index.json exists:\n   - Read the FSM index and transitions files\n   - For each task, identify which FSM transitions it covers\n   - Add `state_machine` field to task definitions (see FSM Integration below)\n   - Generate FSM-aware acceptance criteria\n4. **For existing projects:** Include acceptance criteria that verify integration:\n   - Tests pass with existing test suite\n   - Linting passes (ruff, eslint, etc.)\n   - New code follows established patterns\n5. **CRITICAL: Use the Write tool** to save each task file to {TASKER_DIR}/tasks/T001.json, etc.\n6. **Verify files exist**: `ls -la {TASKER_DIR}/tasks/`\n7. Load tasks with: `cd {TASKER_DIR}/.. && tasker state load-tasks`\n\n## FSM Integration (if FSM artifacts exist)\n\nWhen FSM artifacts are present at {TASKER_DIR}/artifacts/fsm/:\n\n### Add state_machine field to tasks\n```json\n{\n  \"id\": \"T001\",\n  \"name\": \"Implement credential validation\",\n  ...\n  \"state_machine\": {\n    \"transitions_covered\": [\"TR1\", \"TR2\"],\n    \"guards_enforced\": [\"I1\"],\n    \"states_reached\": [\"S2\", \"S3\"]\n  }\n}\n```\n\n### Generate FSM-aware acceptance criteria\nFor each transition covered:\n- \"Transition TR1 (S1→S2) fires when trigger X occurs\"\n- \"Guard I1 prevents invalid transition when condition Y fails\"\n\nExample:\n```json\n{\n  \"criterion\": \"Transition TR1: Unauthenticated→Validating fires on login request\",\n  \"verification\": \"pytest tests/auth/test_fsm.py::test_tr1_login_triggers_validation\"\n},\n{\n  \"criterion\": \"Guard I1: Invalid email format prevents authentication\",\n  \"verification\": \"pytest tests/auth/test_fsm.py::test_guard_invalid_email_blocked\"\n}\n```\n\n### Coverage validation (HARD GATE)\n\nAfter creating tasks, verify FSM transition coverage. **This is a HARD PLANNING GATE**:\n\n```bash\n# Generate coverage report (for observability)\ntasker fsm validate coverage-report \\\n    {TASKER_DIR}/artifacts/fsm/index.json \\\n    {TASKER_DIR}/tasks \\\n    --output {TASKER_DIR}/artifacts/fsm-coverage.plan.json\n\n# Validate coverage meets thresholds (HARD GATE)\ntasker fsm validate task-coverage \\\n    {TASKER_DIR}/artifacts/fsm/index.json \\\n    {TASKER_DIR}/tasks \\\n    --steel-threshold 1.0 \\\n    --other-threshold 0.9\n```\n\n**Coverage Requirements:**\n- **Steel-thread transitions: 100% coverage required** - Every transition in the primary machine MUST have at least one task covering it\n- **Non-steel-thread transitions: 90% coverage required** (configurable)\n\n**If coverage gate fails:**\n1. Planning BLOCKS until coverage is achieved\n2. Task-author must add tasks to cover missing transitions\n3. Each transition should have:\n   - Implementation task(s) that implement the transition\n   - Verification in acceptance criteria that the transition works\n\nIMPORTANT - YOUR TASK IS NOT COMPLETE UNTIL:\n1. You MUST use the Write tool to save each file. Simply outputting JSON to the conversation is NOT sufficient.\n2. Use the TASKER_DIR absolute path provided above. Do NOT use relative paths.\n3. After Write, you MUST verify: `ls {TASKER_DIR}/tasks/*.json | wc -l` - if count is 0, Write again!\n4. You MUST run load-tasks and confirm it succeeds.\n5. For existing projects, tasks must include verification that new code integrates cleanly.\n6. **If FSM artifacts exist**: You MUST run FSM coverage validation:\n   ```bash\n   tasker fsm validate task-coverage \\\n       {TASKER_DIR}/artifacts/fsm/index.json \\\n       {TASKER_DIR}/tasks\n   ```\n   This is a **HARD GATE** - planning cannot proceed if steel-thread coverage < 100%.\n\n**Note:** The orchestrator has already created all required directories. Do NOT create directories yourself. If you encounter \"directory does not exist\" errors, report this to the orchestrator.\n\nIf you complete without task files existing at the absolute path, you have FAILED.\n```\n\n### Validation Phase Details\n\n**Automated Planning Gates (Pre-Check)**\n\nBefore advancing from `definition` to `validation` phase, the system automatically runs programmatic validation gates:\n\n```bash\n# This happens automatically when calling: tasker state advance\n# The following gates are checked:\n# - Spec coverage ≥ 90% (tasks cover requirements)\n# - No phase leakage (Phase 2+ content not in Phase 1 tasks)\n# - All task dependencies reference existing tasks\n# - Acceptance criteria quality (no vague terms like \"works correctly\")\n```\n\nIf any gate fails, phase advancement is blocked:\n```\nPlanning gates failed: Spec coverage 75.0% below threshold 90.0%; Acceptance criteria quality issues: 3 problem(s)\nRun 'tasker validate gates' for details\n```\n\nResults are stored in `state.json → artifacts.validation_results` for observability.\n\n**LLM-as-Judge Verification**\n\nAfter programmatic gates pass, the `validation` phase runs **task-plan-verifier** to evaluate task definitions.\n\n**Spawn prompt for task-plan-verifier:**\n\n```\nVerify task definitions for planning\n\n## Logging (MANDATORY)\n\nLog your activity using the logging script:\n\n./scripts/log-activity.sh INFO task-plan-verifier start \"Verifying task definitions\"\n./scripts/log-activity.sh INFO task-plan-verifier decision \"What decision and why\"\n./scripts/log-activity.sh INFO task-plan-verifier complete \"Outcome description\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory}\nSpec: {TASKER_DIR}/inputs/spec.md\nCapability Map: {TASKER_DIR}/artifacts/capability-map.json\nTasks Directory: {TASKER_DIR}/tasks/\nUser Preferences: ~/.claude/CLAUDE.md (if exists)\n\n## Required Command\n\nRegister verdict using: tasker state validate-tasks <VERDICT> \"<summary>\"\n(IMPORTANT: The command is validate-tasks, NOT validate-complete or any other variant)\n```\n\nThe verifier:\n1. Evaluates all tasks against spec, strategy, and user preferences\n2. Registers its verdict via `tasker state validate-tasks <VERDICT> ...`\n3. Returns a detailed report\n\nVerdicts:\n- `READY` - All tasks pass, proceed to sequencing\n- `READY_WITH_NOTES` - Tasks pass with minor issues noted\n- `BLOCKED` - Critical issues found, must fix before continuing\n\nAfter the verifier completes, the orchestrator:\n```bash\n# Check if we can advance (verifier already registered verdict)\ntasker state advance\n```\n\nIf BLOCKED, the orchestrator:\n1. Displays the verifier's summary to user\n2. Points user to full report: `{TASKER_DIR}/reports/task-validation-report.md`\n3. Waits for user to fix task files\n4. Re-runs task-plan-verifier (or user runs `/verify-plan`)\n5. Repeats until READY or READY_WITH_NOTES\n\n### Sequencing Phase: plan-auditor\n\n**Spawn prompt for plan-auditor:**\n\n```\nAssign phases to tasks and validate the dependency graph.\n\n## Logging (MANDATORY)\n\nLog your activity using the logging script:\n\n./scripts/log-activity.sh INFO plan-auditor start \"Assigning phases and validating DAG\"\n./scripts/log-activity.sh INFO plan-auditor decision \"What decision and why\"\n./scripts/log-activity.sh INFO plan-auditor complete \"Outcome description\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory, e.g., /Users/foo/my-project/.tasker}\n\n## Your Task\n\n1. Read {TASKER_DIR}/tasks/*.json\n2. Read {TASKER_DIR}/artifacts/capability-map.json (for steel thread flows)\n3. Build dependency graph\n4. Assign phases (1: foundations, 2: steel thread, 3+: features)\n5. **CRITICAL: Update task files** using Write tool to {TASKER_DIR}/tasks/T001.json etc.\n6. Validate DAG (no cycles, deps in earlier phases)\n7. Run: cd {TASKER_DIR}/.. && tasker state load-tasks\n\nIMPORTANT:\n- You MUST update task files using the Write tool or jq.\n- Use the TASKER_DIR absolute path provided above. Do NOT use relative paths.\n```\n\nThe report file contains:\n- Per-task evaluations with evidence\n- Specific issues and fix suggestions\n- User preference violations (from ~/.claude/CLAUDE.md)\n\n## Plan Completion\n\nWhen phase reaches \"ready\":\n```markdown\n## Planning Complete ✓\n\n**Tasks:** 24\n**Phases:** 4\n**Steel Thread:** T001 → T003 → T007 → T012\n\nRun `/execute` to begin implementation.\n```\n\n### Archive Planning Artifacts (Automatic)\n\nAfter planning completes, archive the artifacts for post-hoc analysis:\n\n```bash\n# Archive planning artifacts\ntasker archive planning {project_name}\n```\n\nThis creates:\n```\narchive/{project_name}/planning/{timestamp}/\n├── inputs/         # spec.md (verbatim)\n├── artifacts/      # capability-map.json, physical-map.json, spec-review.json\n├── tasks/          # T001.json, T002.json, ...\n├── reports/        # task-validation-report.md\n├── state.json      # State snapshot\n└── archive-manifest.json\n```\n\nThe archive preserves full context for:\n- Gap analysis after implementation\n- Debugging spec coverage issues\n- Auditing planning decisions\n- Restoring planning state if needed\n\n---\n\n# Execute Mode\n\nTriggered by `/execute`. Runs task execution phase, followed by optional compliance check.\n\n## Execute Inputs\n\n**ALWAYS ask for target_dir FIRST before anything else.** No guessing, no inference from CWD.\n\nUse AskUserQuestion to ask:\n```\nWhat is the target project directory?\n```\nFree-form text input. User must provide an absolute or relative path.\n\nAfter target_dir is confirmed:\n```bash\nTARGET_DIR=\"<user-provided-path>\"\n# Convert to absolute path\nTARGET_DIR=$(cd \"$TARGET_DIR\" 2>/dev/null && pwd)\n\nTASKER_DIR=\"$TARGET_DIR/.tasker\"\n\n# Verify .tasker/ exists and has state\nif [ ! -f \"$TASKER_DIR/state.json\" ]; then\n    echo \"Error: No tasker session found at $TASKER_DIR\"\n    echo \"Run /plan first to create a task plan.\"\n    exit 1\nfi\n```\n\n## Execute Prerequisites\n\n```bash\n# Verify planning is complete\ntasker state status\n# Phase must be: ready, executing, or have tasks\n```\n\n## Git Repository Initialization (MANDATORY)\n\n**Before any implementation begins**, check if the target repository has git initialized. If not, initialize it:\n\n```\nBash: ./scripts/ensure-git.sh \"$TARGET_DIR\"\n```\n\n**Why this is required:**\n- Enables automatic commit hooks to track changes per task\n- Provides audit trail of all implementation changes\n- Required for the post-task-commit hook to function\n\n## Recovery on Start (CRITICAL)\n\n**Before starting the execute loop**, always check for and recover from a previous crash:\n\n```bash\n# Check for existing checkpoint from previous run\ntasker state checkpoint status\n\n# If checkpoint exists and is active, recover\ntasker state checkpoint recover\n\n# This will:\n# 1. Find tasks that completed (have result files) but weren't acknowledged\n# 2. Identify orphaned tasks (still \"running\" with no result file)\n# 3. Update checkpoint state accordingly\n```\n\nIf orphaned tasks are found, ask user:\n```markdown\nFound 3 orphaned tasks from previous run:\n- T019, T011, T006\n\nOptions:\n1. Retry orphaned tasks (reset to pending)\n2. Skip orphaned tasks (mark failed)\n3. Abort and investigate\n```\n\nTo retry orphaned tasks:\n```bash\ntasker state task retry T019\ntasker state task retry T011\ntasker state task retry T006\ntasker state checkpoint clear\n```\n\n## Execute Loop\n\n**CRITICAL CONSTRAINTS:**\n- **Max 3 parallel executors** - More causes orchestrator context exhaustion\n- **Task-executors are self-completing** - They call `state.py complete-task` directly\n- **Checkpoint before spawning** - Track batch for crash recovery\n- **Minimal returns** - Executors return only `T001: SUCCESS` or `T001: FAILED - reason`\n\n```bash\nPARALLEL_LIMIT=3\n\nwhile true; do\n    # 0. CHECK FOR HALT\n    tasker state check-halt\n    if [ $? -ne 0 ]; then\n        echo \"Halt requested. Stopping gracefully.\"\n        tasker state checkpoint complete\n\n        # Generate evaluation report even on halt\n        tasker evaluate --output $TASKER_DIR/reports/evaluation-report.txt\n        tasker evaluate --format json --output $TASKER_DIR/reports/evaluation-report.json\n\n        tasker state confirm-halt\n        break\n    fi\n\n    # 1. Get ready tasks\n    READY=$(tasker state ready)\n\n    if [ -z \"$READY\" ]; then\n        tasker state advance\n        if [ $? -eq 0 ]; then\n            echo \"All tasks complete!\"\n            tasker state checkpoint clear\n\n            # Generate evaluation report (MANDATORY)\n            tasker evaluate --output $TASKER_DIR/reports/evaluation-report.txt\n            tasker evaluate --format json --output $TASKER_DIR/reports/evaluation-report.json\n\n            break\n        else\n            echo \"No ready tasks. Check for blockers.\"\n            tasker state status\n            break\n        fi\n    fi\n\n    # 2. Select batch (up to PARALLEL_LIMIT tasks)\n    BATCH=$(echo \"$READY\" | head -$PARALLEL_LIMIT | cut -d: -f1)\n    BATCH_ARRAY=($BATCH)\n    echo \"Batch: ${BATCH_ARRAY[@]}\"\n\n    # 3. Generate and validate bundles for all tasks in batch\n    VALID_TASKS=()\n    for TASK_ID in ${BATCH_ARRAY[@]}; do\n        tasker bundle generate $TASK_ID\n        tasker bundle validate-integrity $TASK_ID\n        INTEGRITY_CODE=$?\n\n        if [ $INTEGRITY_CODE -eq 0 ]; then\n            # Bundle valid - add to execution batch\n            VALID_TASKS+=($TASK_ID)\n        elif [ $INTEGRITY_CODE -eq 2 ]; then\n            # WARNING: Artifacts changed since bundle creation - regenerate\n            ./scripts/log-activity.sh WARN orchestrator validation \"$TASK_ID: Bundle drift detected, regenerating\"\n            tasker bundle generate $TASK_ID\n            VALID_TASKS+=($TASK_ID)\n        else\n            # CRITICAL: Missing dependencies or validation failure\n            ./scripts/log-activity.sh ERROR orchestrator validation \"$TASK_ID: Bundle integrity FAILED\"\n            tasker state task fail $TASK_ID \"Bundle integrity validation failed\" --category dependency\n            # Task will be skipped from this batch\n        fi\n    done\n\n    # Update batch to only include valid tasks\n    if [ ${#VALID_TASKS[@]} -eq 0 ]; then\n        ./scripts/log-activity.sh ERROR orchestrator batch \"No valid tasks in batch - all failed integrity check\"\n        continue  # Skip to next batch\n    fi\n    BATCH_ARRAY=(${VALID_TASKS[@]})\n\n    # 4. CREATE CHECKPOINT before spawning (crash recovery)\n    tasker state checkpoint create ${BATCH_ARRAY[@]}\n\n    # 5. Mark all tasks as started\n    for TASK_ID in ${BATCH_ARRAY[@]}; do\n        tasker state task start $TASK_ID\n    done\n\n    # 6. SPAWN EXECUTORS IN PARALLEL\n    # Use Task tool with multiple invocations in single message\n    # Each executor gets: TASKER_DIR and Bundle path\n    # Each executor returns: \"T001: SUCCESS\" or \"T001: FAILED - reason\"\n\n    # 7. AS EACH EXECUTOR RETURNS, update checkpoint\n    # Parse the minimal return: \"T001: SUCCESS\" -> task_id=T001, status=success\n    # NOTE: Commits are handled automatically by PostToolUse hook (.claude/hooks/post-task-commit.sh)\n    for TASK_ID in ${BATCH_ARRAY[@]}; do\n        # Executor returned - check result file exists\n        if [ -f \"$TASKER_DIR/bundles/${TASK_ID}-result.json\" ]; then\n            STATUS=$(tasker bundle result-info \"$TASKER_DIR/bundles/${TASK_ID}-result.json\" | grep \"^status:\" | cut -d: -f2)\n            tasker state checkpoint update $TASK_ID $STATUS\n            ./scripts/log-activity.sh INFO orchestrator task-result \"$TASK_ID: $STATUS\"\n        else\n            ./scripts/log-activity.sh WARN orchestrator task-result \"$TASK_ID: no result file\"\n        fi\n    done\n\n    # 8. COMPLETE CHECKPOINT for this batch\n    tasker state checkpoint complete\n\n    # 9. Check for halt AFTER batch\n    tasker state check-halt\n    if [ $? -ne 0 ]; then\n        echo \"Halt requested after batch. Stopping gracefully.\"\n\n        # Generate evaluation report even on halt\n        tasker evaluate --output $TASKER_DIR/reports/evaluation-report.txt\n        tasker evaluate --format json --output $TASKER_DIR/reports/evaluation-report.json\n\n        tasker state confirm-halt\n        break\n    fi\n\n    # 10. Continue to next batch (or ask user in interactive mode)\n    # In batch mode: continue automatically\n    # In interactive mode: read -p \"Continue? (y/n): \" CONTINUE\ndone\n```\n\n## Post-Execution Commit (Defense in Depth)\n\nTask file commits are handled **automatically** by a Claude Code hook, ensuring commits happen regardless of executor behavior.\n\n### Hook Configuration\n\nConfigured in `.claude/settings.local.json`:\n```json\n\"PostToolUse\": [\n  {\n    \"matcher\": \"Task\",\n    \"hooks\": [\n      {\n        \"type\": \"command\",\n        \"command\": \".claude/hooks/post-task-commit.sh\",\n        \"timeout\": 30\n      }\n    ]\n  }\n]\n```\n\n### How It Works\n\n1. **Hook** (`.claude/hooks/post-task-commit.sh`) - Triggered after every Task tool completion\n   - Parses task ID from output (e.g., \"T001: SUCCESS\")\n   - Reads `target_dir` from `state.json`\n   - Calls the commit script if task succeeded\n\n2. **Script** (`.claude/hooks/post-task-commit.sh`) - Does the actual commit work\n   - Reads `bundles/<task_id>-result.json` for file list\n   - Checks if files have uncommitted changes\n   - If uncommitted: stages and commits with message `<task_id>: <task_name>`\n   - If already committed: no-op (idempotent)\n   - Updates result file with commit SHA\n\n### Manual Usage\n\n```bash\n./.claude/hooks/post-task-commit.sh <task_id> <target_dir> <planning_dir>\n```\n\n**Why hook-based (not orchestrator-called):**\n- **Automatic** - No orchestrator code to forget/break\n- **Single point of control** - Hook config is the source of truth\n- **Resilient** - Works regardless of orchestrator implementation\n- **Idempotent** - Safe to run multiple times\n\n## Checkpoint Commands Reference\n\n```bash\n# Create checkpoint before spawning batch\ntasker state checkpoint create T001 T002 T003\n\n# Update after each executor returns\ntasker state checkpoint update T001 success\ntasker state checkpoint update T002 failed\n\n# Mark batch complete\ntasker state checkpoint complete\n\n# Check current checkpoint status\ntasker state checkpoint status\n\n# Recover from crash (finds orphans, updates from result files)\ntasker state checkpoint recover\n\n# Clear checkpoint (after successful completion or manual cleanup)\ntasker state checkpoint clear\n```\n\n## Graceful Halt and Resume\n\nThe executor supports graceful halt via two mechanisms:\n\n### 1. STOP File (Recommended for External Control)\n\nCreate a `STOP` file in the `.tasker/` directory:\n\n```bash\ntouch .tasker/STOP\n```\n\nThe executor checks for this file before starting each new task and after completing each task. When detected:\n1. Current task (if running) completes normally\n2. No new tasks are started\n3. State is saved with halt information\n4. Clean exit with instructions to resume\n\n### 2. User Message (For Interactive Sessions)\n\nIf a user sends \"STOP\" during an interactive `/execute` session, the orchestrator should:\n1. Call `tasker state halt user_message`\n2. Allow current task to complete\n3. Exit gracefully\n\n### Resuming Execution\n\nTo resume after a halt:\n\n```bash\n# Check current halt status\ntasker state halt-status\n\n# Clear halt and resume\ntasker state resume\n\n# Then run /execute again\n```\n\nThe resume command:\n- Removes the STOP file if present\n- Clears the halt flag in state\n- Logs the resume event\n\n### Halt Status Commands\n\n```bash\n# Request halt (called by orchestrator when user sends STOP)\ntasker state halt [reason]\n\n# Check if halted (exit code 1 = halted, 0 = ok)\ntasker state check-halt\n\n# Confirm halt completed (after executor stops)\ntasker state confirm-halt\n\n# Show detailed halt status\ntasker state halt-status [--format json]\n\n# Clear halt and resume\ntasker state resume\n```\n\n## Subagent Spawn\n\nSpawn task-executor with self-contained bundle. **Executors are self-completing** - they update state and write results directly, returning only a minimal status line to the orchestrator.\n\n```\nExecute task [TASK_ID]\n\n## Logging (MANDATORY)\n\nLog your activity using the logging script:\n\n```bash\n./scripts/log-activity.sh INFO task-executor start \"Executing task [TASK_ID]\"\n./scripts/log-activity.sh INFO task-executor decision \"What decision and why\"\n./scripts/log-activity.sh INFO task-executor tool \"Write - creating src/auth/validator.py\"\n./scripts/log-activity.sh INFO task-executor complete \"Outcome description\"\n./scripts/log-activity.sh ERROR task-executor error \"Error message if any\"\n```\n\nTASKER_DIR: {absolute path to .tasker directory, e.g., /Users/foo/my-project/.tasker}\nBundle: {TASKER_DIR}/bundles/[TASK_ID]-bundle.json\n\nThe bundle contains everything you need:\n- Task definition and acceptance criteria\n- Expanded behavior details (what to implement)\n- File paths and purposes\n- Target directory\n- Constraints and patterns to follow\n- Dependencies (files from prior tasks)\n\n## Self-Completion Protocol (CRITICAL)\n\nYou are responsible for updating state and persisting results. Do NOT rely on the orchestrator.\n\n### On Success:\n1. Track all files you created/modified\n2. Call: `tasker state task complete [TASK_ID] --created file1 file2 --modified file3`\n3. Write result file: `{TASKER_DIR}/bundles/[TASK_ID]-result.json` (see schema below)\n4. Return ONLY this line: `[TASK_ID]: SUCCESS`\n\n### On Failure:\n1. Call: `tasker state task fail [TASK_ID] \"error message\" --category <cat> --retryable`\n2. Write result file with error details\n3. Return ONLY this line: `[TASK_ID]: FAILED - <one-line reason>`\n\n### Result File Schema\nWrite to `{TASKER_DIR}/bundles/[TASK_ID]-result.json`:\n```json\n{\n  \"version\": \"1.0\",\n  \"task_id\": \"[TASK_ID]\",\n  \"name\": \"Task name from bundle\",\n  \"status\": \"success|failed\",\n  \"started_at\": \"ISO timestamp\",\n  \"completed_at\": \"ISO timestamp\",\n  \"files\": {\n    \"created\": [\"path1\", \"path2\"],\n    \"modified\": [\"path3\"]\n  },\n  \"verification\": {\n    \"verdict\": \"PASS|FAIL\",\n    \"criteria\": [\n      {\"name\": \"criterion\", \"status\": \"PASS|FAIL\", \"evidence\": \"...\"}\n    ]\n  },\n  \"error\": {\n    \"category\": \"dependency|compilation|test|validation|runtime\",\n    \"message\": \"...\",\n    \"retryable\": true\n  },\n  \"notes\": \"Any decisions or observations\"\n}\n```\n\n## Workflow Summary\n1. Read the bundle file - it has ALL context\n2. Implement behaviors in specified files\n3. Run acceptance criteria verification\n4. Call state.py to update task status\n5. Write detailed result to bundles/[TASK_ID]-result.json\n6. Return ONE LINE status to orchestrator\n\nIMPORTANT: Use the TASKER_DIR absolute path provided above. Do NOT use relative paths.\n```\n\nThe subagent:\n- Has NO memory of previous tasks\n- Gets full context budget\n- Reads ONE file (the bundle) for complete context\n- **Calls state.py directly** to mark completion\n- **Writes result file** for observability\n- Returns **minimal status line** (not full report)\n\n## Bundle Contents\n\nThe bundle (`{TASKER_DIR}/bundles/T001-bundle.json`) includes:\n\n| Field | Purpose |\n|-------|---------|\n| `task_id`, `name` | Task identification |\n| `target_dir` | Where to write code |\n| `behaviors` | Expanded behavior details (not just IDs) |\n| `files` | Paths, actions, purposes, layers |\n| `acceptance_criteria` | Verification commands |\n| `constraints` | Tech stack, patterns, testing |\n| `dependencies.files` | Files from prior tasks to read |\n| `context` | Domain, capability, spec reference |\n| `state_machine` | FSM context for adherence verification (if present) |\n\n### FSM Context in Bundle (when present)\n\nIf the task has `state_machine` field, the bundle includes expanded FSM details:\n\n```json\n{\n  \"state_machine\": {\n    \"transitions_covered\": [\"TR1\", \"TR2\"],\n    \"guards_enforced\": [\"I1\"],\n    \"states_reached\": [\"S2\", \"S3\"],\n    \"transitions_detail\": [\n      {\n        \"id\": \"TR1\",\n        \"from_state\": \"S1\",\n        \"to_state\": \"S2\",\n        \"trigger\": \"validate_credentials\",\n        \"guards\": [{\"condition\": \"email_valid\", \"invariant_id\": \"I1\"}]\n      }\n    ]\n  }\n}\n```\n\nThe task-executor uses this for:\n1. Understanding the state transitions being implemented\n2. Ensuring guards are properly enforced\n3. Writing tests that verify FSM behavior\n\nGenerate bundles with:\n```bash\ntasker bundle generate T001       # Single task\ntasker bundle generate-ready      # All ready tasks\n```\n\n## Execute Options\n\n| Command | Behavior |\n|---------|----------|\n| `/execute` | Interactive, one task at a time |\n| `/execute T005` | Execute specific task only |\n| `/execute --batch` | All ready tasks, no prompts |\n| `/execute --parallel 3` | Up to 3 tasks simultaneously |\n\n## FSM Coverage Report (After Execution Completes)\n\nAfter all tasks complete, generate the execution coverage report:\n\n```bash\n# Generate execute phase coverage report with verification evidence\ntasker fsm validate execute-coverage-report \\\n    {TASKER_DIR}/artifacts/fsm/index.json \\\n    {TASKER_DIR}/bundles \\\n    --output {TASKER_DIR}/artifacts/fsm-coverage.execute.json\n```\n\nThis report includes:\n- Which transitions were verified during execution\n- Evidence type for each transition (test, runtime_assertion, manual)\n- Which invariants were enforced\n- Pointers to tasks and acceptance criteria that provide evidence\n\nUse this report for post-execution compliance auditing.\n\n## Evaluation Report (After Completion)\n\n**MANDATORY**: After all tasks complete (or execution halts), generate the evaluation report:\n\n```bash\ntasker evaluate --output {TASKER_DIR}/reports/evaluation-report.txt\ntasker evaluate --format json --output {TASKER_DIR}/reports/evaluation-report.json\n```\n\nDisplay the report to the user. The report includes:\n- Planning quality (verdict from task-plan-verifier)\n- Execution summary (completed/failed/blocked/skipped counts)\n- First-attempt success rate (measures spec quality)\n- Verification breakdown (criteria pass/partial/fail, code quality)\n- Cost analysis (total tokens, total cost, per-task cost)\n- Failure analysis (which tasks failed and why)\n- Improvement patterns (common issues for process improvement)\n\nFor metrics-only summary:\n```bash\ntasker evaluate --metrics-only\n```\n\n## Archive Execution Artifacts (After Completion)\n\nAfter execution completes (all tasks done or halted), archive execution artifacts:\n\n```bash\n# Archive execution artifacts\ntasker archive execution {project_name}\n```\n\nThis creates:\n```\narchive/{project_name}/execution/{timestamp}/\n├── bundles/        # Task bundles and result files\n├── logs/           # Activity logs\n├── state.json      # State snapshot\n└── archive-manifest.json\n```\n\nThe archive preserves:\n- Task execution results and timing\n- Bundle contents (full context for each task)\n- Logs for debugging\n- State for resumption analysis\n\n### Archive Commands Reference\n\n```bash\n# Archive planning artifacts\ntasker archive planning {project_name}\n\n# Archive execution artifacts\ntasker archive execution {project_name}\n\n# List all archives\ntasker archive list\n\n# List archives for specific project\ntasker archive list --project {project_name}\n\n# Restore planning state from archive\ntasker archive restore {archive_id} --project {project_name}\n```\n\n---\n\n# State Commands Reference\n\n```bash\n# General\ntasker state status          # Current phase, task counts\ntasker state advance         # Try to advance phase\n\n# Planning\ntasker state init <dir>      # Initialize new plan\ntasker state validate <art>  # Validate artifact\ntasker state validate-tasks <verdict> [summary] [--issues ...]\n                                         # Register task validation result\n\n# Execution\ntasker state ready     # List ready tasks\ntasker state task start <id> # Mark running\ntasker state task complete <id>  # Mark done\ntasker state task fail <id> <e>  # Mark failed\ntasker state load-tasks      # Reload from files\n\n# Halt / Resume\ntasker state halt [reason]   # Request graceful halt\ntasker state check-halt      # Check if halted (exit 1 = halted)\ntasker state confirm-halt    # Confirm halt completed\ntasker state halt-status     # Show halt status\ntasker state resume          # Clear halt, resume execution\n\n# Checkpoint (Crash Recovery)\ntasker state checkpoint create <t1> [t2 ...]  # Create batch checkpoint\ntasker state checkpoint update <id> <status>  # Update task (success|failed)\ntasker state checkpoint complete              # Mark batch done\ntasker state checkpoint status                # Show current checkpoint\ntasker state checkpoint recover               # Recover orphaned tasks\ntasker state checkpoint clear                 # Remove checkpoint\n\n# Bundles\ntasker bundle generate <id>   # Generate bundle for task\ntasker bundle generate-ready  # Generate all ready bundles\ntasker bundle validate <id>   # Validate bundle against schema\ntasker bundle validate-integrity <id>  # Check deps + checksums\ntasker bundle list            # List existing bundles\ntasker bundle clean           # Remove all bundles\n\n# Observability\ntasker state log-tokens <s> <i> <o> <c>  # Log usage\n```\n\n---\n\n# Error Recovery\n\n## Planning Errors\n\nIf agent produces invalid output:\n1. Validation fails (`state.py validate` returns non-zero)\n2. Report errors to agent\n3. Agent fixes and re-outputs\n4. Re-validate\n5. Only advance on success\n\n## Execution Errors\n\nIf task fails:\n1. `state.py fail-task` marks it failed\n2. Rollback triggered (created files deleted, modified files restored)\n3. Dependent tasks auto-blocked\n4. Other ready tasks can continue\n5. User can retry later: fix issue, then `state.py start-task` again\n",
        "internal/skills/runtime-logger/SKILL.md": "---\nname: runtime-logger\ndescription: Emit info-level logs to file during task execution, similar to logger.info() calls in code\n---\n\n# Runtime Activity Logger\n\nLog key activities to `.claude/logs/activity.log` using bash as you work. This creates a trace of agent reasoning and actions analogous to info-level logging in deterministic code.\n\n## Setup\n\nBefore starting any task, ensure the log directory exists:\n\n```bash\nmkdir -p .claude/logs\n```\n\n## Log Format\n\nUse this consistent format for all log entries:\n\n```bash\necho \"[$(date -Iseconds)] [LEVEL] phase: message\" >> .claude/logs/activity.log\n```\n\n### Log Levels\n\n| Level | Use For |\n|-------|---------|\n| `INFO` | Normal operations, progress updates, decisions |\n| `DEBUG` | Detailed technical information when troubleshooting |\n| `WARN` | Unexpected situations that don't block progress |\n| `ERROR` | Failures that affect task completion |\n\n## When to Log\n\n### Task Lifecycle\n- **Task start**: Log the goal and planned approach\n- **Task completion**: Log summary, outcome, and any artifacts created\n\n### Tool Usage\n- **Before significant tool use**: Log what tool and why\n- **After tool use**: Log outcome (success/failure, key findings)\n\n### Decision Points\n- **Architectural choices**: Log reasoning for design decisions\n- **Branch points**: Log why one approach was chosen over alternatives\n- **Assumptions**: Log any assumptions being made\n\n### Subagent Activity\n- **Delegation**: Log task being delegated and to which subagent\n- **Completion**: Log subagent results when they return\n\n## Examples\n\n### Starting a Task\n```bash\necho \"[$(date -Iseconds)] [INFO] start: Beginning refactoring task for payment module\" >> .claude/logs/activity.log\necho \"[$(date -Iseconds)] [INFO] planning: Will analyze current structure, identify patterns, then apply Strategy pattern\" >> .claude/logs/activity.log\n```\n\n### Tool Usage\n```bash\necho \"[$(date -Iseconds)] [INFO] tool: Reading src/payments/processor.py to understand current implementation\" >> .claude/logs/activity.log\necho \"[$(date -Iseconds)] [INFO] tool-result: Found 3 payment types with duplicated validation logic\" >> .claude/logs/activity.log\n```\n\n### Decision Points\n```bash\necho \"[$(date -Iseconds)] [INFO] decision: Using Strategy pattern - cleaner than inheritance for 3 payment types\" >> .claude/logs/activity.log\necho \"[$(date -Iseconds)] [INFO] decision: Keeping backward compatibility by wrapping legacy interface\" >> .claude/logs/activity.log\n```\n\n### Subagent Delegation\n```bash\necho \"[$(date -Iseconds)] [INFO] delegate: Spawning test-writer subagent for unit test coverage\" >> .claude/logs/activity.log\necho \"[$(date -Iseconds)] [INFO] delegate-complete: test-writer created 12 unit tests, all passing\" >> .claude/logs/activity.log\n```\n\n### Warnings and Errors\n```bash\necho \"[$(date -Iseconds)] [WARN] Found deprecated API usage in line 142, will need migration\" >> .claude/logs/activity.log\necho \"[$(date -Iseconds)] [ERROR] Build failed: missing dependency 'stripe-sdk', attempting to resolve\" >> .claude/logs/activity.log\n```\n\n### Task Completion\n```bash\necho \"[$(date -Iseconds)] [INFO] complete: Refactoring finished - 3 files modified, 12 tests added\" >> .claude/logs/activity.log\necho \"[$(date -Iseconds)] [INFO] artifacts: Created src/payments/strategies/*.py, updated tests/test_payments.py\" >> .claude/logs/activity.log\n```\n\n## Log File Management\n\n### Viewing Logs in Real-Time\n```bash\ntail -f .claude/logs/activity.log\n```\n\n### Rotating Logs\nFor long-running projects, consider date-based log files:\n```bash\necho \"[$(date -Iseconds)] [INFO] message\" >> .claude/logs/activity-$(date +%Y-%m-%d).log\n```\n\n## Integration with Subagents\n\nSubagents inherit access to bash and should follow the same logging conventions. When delegating tasks, instruct subagents to:\n\n1. Log their start and completion\n2. Use the same log file path\n3. Prefix messages with their agent name for traceability\n\nExample subagent log entry:\n```bash\necho \"[$(date -Iseconds)] [INFO] [test-writer] start: Generating unit tests for PaymentStrategy classes\" >> .claude/logs/activity.log\n```\n",
        "internal/skills/tasker-to-beads/SKILL.md": "---\nname: tasker-to-beads\ndescription: Transform Tasker task definitions into rich, self-contained Beads issues. Bridges ephemeral execution units with persistent planning artifacts.\ntools:\n  - bash\n  - file_read\n  - file_write\n  - ask_user\n---\n\n# Tasker to Beads Transformer\n\nConverts Tasker task definitions into enriched Beads issues that are **self-contained** and **human-readable**. This skill performs the \"neural\" work of understanding spec context, synthesizing narratives, and creating issues that stand alone.\n\n## When to Use\n\n- After completing Tasker `/plan` phase (state is \"ready\")\n- When you want to track implementation progress in Beads\n- When multiple people/agents will work on tasks and need persistent context\n\n## Prerequisites\n\n1. Tasker planning complete (`.tasker/state.json` exists, phase is \"ready\")\n2. A target directory for development (beads will be initialized there if needed)\n\n## Target Directory Concept\n\nThe **source project** (where tasker planning lives) may be different from the **target project** (where development happens). For example:\n\n- **Source**: `~/projects/tasker/` - contains spec, planning artifacts, task definitions\n- **Target**: `~/projects/fathom/` - where the actual Elixir project will be built\n\nWhen invoked, this skill will:\n1. **Ask for the target directory** if not provided\n2. **Initialize beads** in the target directory if not already done\n3. **Create issues** in the target project's beads system\n\n---\n\n## Workflow\n\n### Step 0: Determine Target Directory\n\n**IMPORTANT:** Before proceeding, ask the user where development will happen:\n\n> \"Where would you like to create the Beads issues? This should be the directory where the actual development will take place (e.g., the project repository being built).\"\n\nCommon scenarios:\n- **Same directory**: Use current project (e.g., adding features to existing codebase)\n- **New directory**: Create issues in a separate project (e.g., building a new project from a spec)\n\n### Step 1: Initialize Target (if needed)\n\n```bash\n# Check status and see if target needs initialization\ntasker transform status -t /path/to/target\n\n# Initialize beads in target directory with custom prefix\n# This runs: bd init <PREFIX> && bd onboard\ntasker transform init-target /path/to/target FATHOM\n```\n\nThe `init-target` command runs two steps:\n1. `bd init <PREFIX>` - Creates the `.beads` directory structure\n2. `bd onboard` - Sets up project configuration and initial state\n\n### Step 2: Prepare Context\n\n```bash\ntasker transform context --all -t /path/to/target\n```\n\nThis extracts structural data from task files and saves context bundles to `.tasker/beads-export/`.\n\n### Step 3: Enrich Each Task (LLM Work)\n\nFor each task, read the context file and generate an enriched issue description.\n\n**Read the context:**\n```bash\ncat .tasker/beads-export/{TASK_ID}-context.json\n```\n\n**Generate enriched content using the template below**, then save:\n```bash\n# Save to .tasker/beads-export/{TASK_ID}-enriched.json\n```\n\n### Step 4: Create Beads Issues\n\nAfter enrichment, create issues in the target directory:\n```bash\ntasker transform create {TASK_ID} .tasker/beads-export/{TASK_ID}-enriched.json -t /path/to/target\n```\n\nOr batch create from manifest:\n```bash\ntasker transform batch-create .tasker/beads-export/manifest.json -t /path/to/target\n```\n\n---\n\n## Enrichment Template\n\nWhen generating enriched issue content, produce JSON with this structure:\n\n```json\n{\n  \"task_id\": \"T001\",\n  \"title\": \"Initialize Mix project with OTP dependencies\",\n  \"priority\": \"critical\",\n  \"labels\": [\"domain:infrastructure\", \"phase:1\", \"steel-thread\"],\n  \"dependencies\": [],\n  \"description\": \"... rich markdown description ...\"\n}\n```\n\n### Description Format\n\nThe description should be **self-contained** markdown with these sections:\n\n```markdown\n## Overview\n\n[1-2 sentences explaining WHAT this task accomplishes and WHY it matters to the project]\n\n## Spec Context\n\n[Relevant excerpts from the specification that drive this task. Include enough context that someone can understand the requirement without reading the full spec.]\n\n> \"Direct quote from spec if available\"\n\n[Your synthesis of what the spec requires]\n\n## Architecture Context\n\n**Domain:** [domain name]\n**Capability:** [capability name]\n\n[Explain how this fits into the system architecture. What role does it play? What does it enable?]\n\n## Implementation Approach\n\n[Narrative description of how to implement this. Not just file paths, but the approach and key decisions.]\n\n### Files to Create/Modify\n\n| File | Purpose |\n|------|---------|\n| `path/to/file.ex` | Brief purpose |\n\n## Dependencies\n\n[If this task depends on others, explain WHAT it needs from them, not just the ID]\n\n- **T003 (Core data schemas):** Provides the `Session` and `Message` structs used here\n- **T005 (Tool behaviour):** Defines the `Tool` behaviour this implements\n\n## Acceptance Criteria\n\nHuman-readable checklist (not raw verification commands):\n\n- [ ] Project compiles without warnings\n- [ ] All required dependencies are declared in mix.exs\n- [ ] Basic module structure is in place\n- [ ] Tests pass (if applicable)\n\n## Notes\n\n[Any additional context, gotchas, or considerations]\n```\n\n---\n\n## Enrichment Prompts\n\nWhen processing each task context, use this reasoning approach:\n\n### 1. Understand the Task's Purpose\n\nRead the task name, context, and behaviors. Ask:\n- What problem does this solve?\n- Why does this exist in the system?\n- What would be missing without it?\n\n### 2. Extract Spec Relevance\n\nFrom `relevant_spec_sections` in the context:\n- Find the most relevant 1-2 sections\n- Extract key quotes that define the requirement\n- Synthesize the requirement in your own words\n\n### 3. Explain Architecture Fit\n\nFrom `capability_context`:\n- Describe the domain and capability this belongs to\n- Explain how the behaviors contribute to the capability\n- Connect to the broader system design\n\n### 4. Narrate Dependencies\n\nFrom `dependency_context`:\n- For each dependency, explain what this task USES from it\n- Make the dependency relationship meaningful, not just structural\n\n### 5. Humanize Acceptance Criteria\n\nFrom `task.acceptance_criteria`:\n- Convert verification commands to human-readable checks\n- Add implicit criteria (code quality, tests, documentation)\n- Make them checkable by a human reviewer\n\n---\n\n## Single-Task Mode\n\nTo transform a single task:\n\n```bash\n# 1. Prepare context\ntasker transform context T001\n\n# 2. Read and understand\ncat .tasker/beads-export/T001-context.json\n\n# 3. Generate enriched content (you do this)\n# ... apply the enrichment template ...\n# Save to .tasker/beads-export/T001-enriched.json\n\n# 4. Create the issue in target directory\ntasker transform create T001 .tasker/beads-export/T001-enriched.json -t /path/to/target\n```\n\n---\n\n## Batch Mode\n\nTo transform all tasks:\n\n```bash\n# 1. Check status and determine target\ntasker transform status -t /path/to/target\n\n# 2. Prepare all contexts\ntasker transform context --all\n\n# 3. For each context file, generate enriched content\n# This is the neural loop - process each T*-context.json\n\n# 4. Create manifest with all enriched issues\n# Save to .tasker/beads-export/manifest.json with structure:\n# { \"issues\": [ {...enriched issue 1...}, {...enriched issue 2...}, ... ] }\n\n# 5. Batch create in target directory\ntasker transform batch-create .tasker/beads-export/manifest.json -t /path/to/target\n```\n\n---\n\n## Manifest Format\n\nFor batch creation, the manifest should be:\n\n```json\n{\n  \"created_at\": \"2025-01-15T10:00:00Z\",\n  \"source\": \"tasker\",\n  \"task_count\": 47,\n  \"issues\": [\n    {\n      \"task_id\": \"T001\",\n      \"title\": \"Initialize Mix project with OTP dependencies\",\n      \"priority\": \"critical\",\n      \"labels\": [\"domain:infrastructure\", \"phase:1\", \"steel-thread\"],\n      \"dependencies\": [],\n      \"description\": \"## Overview\\n\\n...\"\n    },\n    {\n      \"task_id\": \"T002\",\n      \"title\": \"...\",\n      ...\n    }\n  ]\n}\n```\n\n---\n\n## Quality Checklist\n\nBefore creating an issue, verify:\n\n- [ ] Title is concise but descriptive (not just the task name)\n- [ ] Overview explains WHY, not just WHAT\n- [ ] Spec context includes actual quotes/references\n- [ ] Architecture context explains the system role\n- [ ] Dependencies are explained narratively\n- [ ] Acceptance criteria are human-checkable\n- [ ] Description stands alone (no external context needed)\n\n---\n\n## Example: T001 Enrichment\n\n**Input context (abbreviated):**\n```json\n{\n  \"task_id\": \"T001\",\n  \"task\": {\n    \"name\": \"Mix project initialization\",\n    \"phase\": 1,\n    \"context\": {\n      \"domain\": \"Infrastructure\",\n      \"capability\": \"Project Setup\",\n      \"steel_thread\": true\n    },\n    \"files\": [{\"path\": \"mix.exs\", \"action\": \"create\"}],\n    \"acceptance_criteria\": [\n      {\"criterion\": \"mix compile succeeds\", \"verification\": \"mix compile\"}\n    ]\n  },\n  \"relevant_spec_sections\": [\n    \"## Tech Stack\\n\\nThe agent framework uses Elixir with OTP patterns...\"\n  ]\n}\n```\n\n**Output enriched (abbreviated):**\n```json\n{\n  \"task_id\": \"T001\",\n  \"title\": \"Initialize Elixir/OTP project foundation\",\n  \"priority\": \"critical\",\n  \"labels\": [\"domain:infrastructure\", \"phase:1\", \"steel-thread\"],\n  \"dependencies\": [],\n  \"description\": \"## Overview\\n\\nEstablish the foundational Mix project structure for the Fathom agent framework. This is the critical first step that all other tasks depend on, providing the build system, dependency management, and basic project layout.\\n\\n## Spec Context\\n\\n> \\\"The agent framework uses Elixir with OTP patterns for fault-tolerant, distributed agent coordination.\\\"\\n\\nThe specification mandates Elixir/OTP as the implementation technology, leveraging its supervision trees, GenServers, and distribution capabilities for building resilient agent systems.\\n\\n## Architecture Context\\n\\n**Domain:** Infrastructure\\n**Capability:** Project Setup\\n\\nThis task creates the skeleton that all other infrastructure and application code builds upon. It establishes:\\n- Dependency versions (ecto, phoenix_pubsub, telemetry, etc.)\\n- Compilation settings and warnings configuration\\n- Project metadata and structure\\n\\n## Implementation Approach\\n\\nCreate a standard Mix project with umbrella-ready structure. Key dependencies include:\\n- `ecto` + `postgrex` for persistence\\n- `phoenix_pubsub` for inter-process messaging\\n- `telemetry` + `opentelemetry` for observability\\n- `libcluster` for node discovery\\n\\n### Files to Create/Modify\\n\\n| File | Purpose |\\n|------|---------|\\n| `mix.exs` | Project definition, dependencies, compilation settings |\\n| `.formatter.exs` | Code formatting rules |\\n| `.gitignore` | Standard Elixir ignores |\\n| `README.md` | Project documentation |\\n\\n## Dependencies\\n\\nNone - this is the foundation task.\\n\\n## Acceptance Criteria\\n\\n- [ ] `mix deps.get` fetches all dependencies successfully\\n- [ ] `mix compile --warnings-as-errors` passes\\n- [ ] `mix format --check-formatted` passes\\n- [ ] All required dependencies declared (ecto, postgrex, phoenix_pubsub, telemetry, libcluster, etc.)\\n\\n## Notes\\n\\nThis is on the **steel thread** path - it must complete before any other work can proceed. Keep the initial setup minimal but complete; don't add optional dependencies that aren't immediately needed.\"\n}\n```\n",
        "skills/execute/SKILL.md": "---\nname: execute\ndescription: EXECUTION PHASE - Run tasks via isolated subagents. Requires completed task DAG from plan phase.\ntools:\n  - agent\n  - bash\n  - file_read\n  - file_write\n---\n\n# Execute Workflow\n\n**CRITICAL: This is the EXECUTE skill. Only use this after /plan has completed and task files exist in .tasker/tasks/. If no tasks exist, tell the user to run /tasker:plan first.**\n\n**IMPORTANT: All tasker working files are in `$TARGET_DIR/.tasker/`. Do NOT create or use any other directories like `project-planning/`, `planning/`, or `schemas/` at the target project root. The `.tasker/` directory is the ONLY location for tasker artifacts (including `.tasker/schemas/` for JSON schemas).**\n\nExecute a task DAG via isolated subagents. This is Phase 3 of the tasker workflow:\n\n```\n/specify → /plan → /execute\n```\n\n## Input Requirements\n\n- **Task DAG** from `/plan`: `.tasker/tasks/T001.json`, `T002.json`, etc.\n- **State file**: `.tasker/state.json` with phase = \"ready\" or \"executing\"\n\n## Output\n\n- **Working implementation** in target directory\n- **Task results**: `.tasker/bundles/T001-result.json`, etc.\n- **Evaluation report**: `.tasker/reports/evaluation-report.txt`\n\n---\n\n## MANDATORY FIRST STEP: Ask for Target Project Directory\n\n**ALWAYS ask for target_dir FIRST before anything else.** No guessing, no inference from CWD.\n\nUse AskUserQuestion to ask:\n```\nWhat is the target project directory?\n```\nFree-form text input. User must provide an absolute or relative path.\n\nAfter target_dir is confirmed:\n```bash\nTARGET_DIR=\"<user-provided-path>\"\n# Convert to absolute path\nTARGET_DIR=$(cd \"$TARGET_DIR\" 2>/dev/null && pwd)\n\nTASKER_DIR=\"$TARGET_DIR/.tasker\"\n\n# Verify .tasker/ exists and has state\nif [ ! -f \"$TASKER_DIR/state.json\" ]; then\n    echo \"Error: No tasker session found at $TASKER_DIR\"\n    echo \"Run /plan first to create a task plan.\"\n    exit 1\nfi\n```\n\n---\n\n## Execute Prerequisites\n\n```bash\n# Verify planning is complete\ntasker state status\n# Phase must be: ready, executing, or have tasks\n```\n\n---\n\n## Git Repository Initialization (MANDATORY)\n\n**Before any implementation begins**, check if the target repository has git initialized. If not, initialize it:\n\n```bash\n./scripts/ensure-git.sh \"$TARGET_DIR\"\n```\n\n**Why this is required:**\n- Enables automatic commit hooks to track changes per task\n- Provides audit trail of all implementation changes\n- Required for the post-task-commit hook to function\n\n---\n\n## Recovery on Start (CRITICAL)\n\n**Before starting the execute loop**, always check for and recover from a previous crash:\n\n```bash\n# Check for existing checkpoint from previous run\ntasker state checkpoint status\n\n# If checkpoint exists and is active, recover\ntasker state checkpoint recover\n\n# This will:\n# 1. Find tasks that completed (have result files) but weren't acknowledged\n# 2. Identify orphaned tasks (still \"running\" with no result file)\n# 3. Update checkpoint state accordingly\n```\n\nIf orphaned tasks are found, ask user:\n```markdown\nFound 3 orphaned tasks from previous run:\n- T019, T011, T006\n\nOptions:\n1. Retry orphaned tasks (reset to pending)\n2. Skip orphaned tasks (mark failed)\n3. Abort and investigate\n```\n\nTo retry orphaned tasks:\n```bash\ntasker state task retry T019\ntasker state task retry T011\ntasker state task retry T006\ntasker state checkpoint clear\n```\n\n---\n\n## Execute Loop\n\n**CRITICAL CONSTRAINTS:**\n- **Max 3 parallel executors** - More causes orchestrator context exhaustion\n- **Task-executors are self-completing** - They update state and write results directly\n- **Checkpoint before spawning** - Track batch for crash recovery\n- **Minimal returns** - Executors return only `T001: SUCCESS` or `T001: FAILED - reason`\n\n### Loop Steps\n\nRepeat until no more tasks or halt requested:\n\n**Step 0: Check for Halt**\n- Use Bash: `tasker state check-halt`\n- If exit code is non-zero, halt was requested:\n  - Run `tasker state checkpoint complete`\n  - Generate and display evaluation report (see \"Evaluation Report\" section)\n  - Run `tasker state confirm-halt`\n  - Exit loop\n\n**Step 1: Get Ready Tasks**\n- Use Bash: `tasker state ready`\n- If no ready tasks:\n  - Run `tasker state advance`\n  - If successful: all tasks complete! Clear checkpoint, generate and display evaluation report, exit loop\n  - If no progress: check for blockers with `tasker state status`, exit loop\n\n**Step 2: Select Batch**\n- Take up to 3 task IDs from ready tasks\n\n**Step 3: Generate and Validate Bundles**\nFor each task in batch:\n- Use Bash: `tasker bundle generate {TASK_ID}`\n- Use Bash: `tasker bundle validate-integrity {TASK_ID}`\n- If integrity fails (exit code 1): fail the task with `tasker state task fail {TASK_ID} \"Bundle integrity validation failed\" --category dependency`\n- If integrity warns (exit code 2): regenerate bundle\n\n**Step 4: Create Checkpoint**\n- Use Bash: `tasker state checkpoint create {TASK_IDs...}`\n\n**Step 5: Mark Tasks Started**\nFor each valid task:\n- Use Bash: `tasker state task start {TASK_ID}`\n\n**Step 6: Spawn Executors in Parallel**\nUse the Task tool to spawn `task-executor` subagents for ALL valid tasks in a SINGLE message (parallel execution):\n\n```\nExecute task {TASK_ID}\n\nTASKER_DIR: {absolute path to .tasker directory}\nBundle: {TASKER_DIR}/bundles/{TASK_ID}-bundle.json\n```\n\nWait for all executors to return. Each returns only: `{TASK_ID}: SUCCESS` or `{TASK_ID}: FAILED - reason`\n\n**Step 7: Update Checkpoint**\nFor each task in batch:\n- Use Bash: `tasker state checkpoint update {TASK_ID} {status}`\n\n**Step 8: Complete Checkpoint**\n- Use Bash: `tasker state checkpoint complete`\n\n**Step 9: Check for Halt After Batch**\n- Use Bash: `tasker state check-halt`\n- If halted: generate and display evaluation report, confirm halt, exit loop\n\n**Step 10: Continue to Next Batch**\n\n---\n\n## Post-Execution Commit (Defense in Depth)\n\nTask file commits are handled **automatically** by a Claude Code hook, ensuring commits happen regardless of executor behavior.\n\n### Hook Configuration\n\nConfigured in `.claude/settings.local.json`:\n```json\n\"PostToolUse\": [\n  {\n    \"matcher\": \"Task\",\n    \"hooks\": [\n      {\n        \"type\": \"command\",\n        \"command\": \".claude/hooks/post-task-commit.sh\",\n        \"timeout\": 30\n      }\n    ]\n  }\n]\n```\n\n---\n\n## Subagent Spawn Template\n\nSpawn task-executor with self-contained bundle. **Executors are self-completing** - they update state and write results directly, returning only a minimal status line.\n\n```\nExecute task [TASK_ID]\n\n## Logging (MANDATORY)\n\n```bash\n./scripts/log-activity.sh INFO task-executor start \"Executing task [TASK_ID]\"\n./scripts/log-activity.sh INFO task-executor decision \"What decision and why\"\n./scripts/log-activity.sh INFO task-executor complete \"Outcome description\"\n```\n\nTASKER_DIR: {absolute path to .tasker directory}\nBundle: {TASKER_DIR}/bundles/[TASK_ID]-bundle.json\n\nThe bundle contains everything you need:\n- Task definition and acceptance criteria\n- Expanded behavior details (what to implement)\n- File paths and purposes\n- Target directory\n- Constraints and patterns to follow\n- Dependencies (files from prior tasks)\n\n## Self-Completion Protocol (CRITICAL)\n\nYou are responsible for updating state and persisting results. Do NOT rely on the orchestrator.\n\n### On Success:\n1. Track all files you created/modified\n2. Call: `tasker state task complete [TASK_ID] --created file1 file2 --modified file3`\n3. Write result file: `{TASKER_DIR}/bundles/[TASK_ID]-result.json`\n4. Return ONLY this line: `[TASK_ID]: SUCCESS`\n\n### On Failure:\n1. Call: `tasker state task fail [TASK_ID] \"error message\" --category <cat> --retryable`\n2. Write result file with error details\n3. Return ONLY this line: `[TASK_ID]: FAILED - <one-line reason>`\n\n### Result File Schema\nWrite to `{TASKER_DIR}/bundles/[TASK_ID]-result.json`:\n```json\n{\n  \"version\": \"1.0\",\n  \"task_id\": \"[TASK_ID]\",\n  \"name\": \"Task name from bundle\",\n  \"status\": \"success|failed\",\n  \"started_at\": \"ISO timestamp\",\n  \"completed_at\": \"ISO timestamp\",\n  \"files\": {\n    \"created\": [\"path1\", \"path2\"],\n    \"modified\": [\"path3\"]\n  },\n  \"verification\": {\n    \"verdict\": \"PASS|FAIL\",\n    \"criteria\": [\n      {\"name\": \"criterion\", \"status\": \"PASS|FAIL\", \"evidence\": \"...\"}\n    ]\n  },\n  \"error\": {\n    \"category\": \"dependency|compilation|test|validation|runtime\",\n    \"message\": \"...\",\n    \"retryable\": true\n  },\n  \"notes\": \"Any decisions or observations\"\n}\n```\n\n## Workflow Summary\n1. Read the bundle file - it has ALL context\n2. Implement behaviors in specified files\n3. Run acceptance criteria verification\n4. Call tasker state to update task status\n5. Write detailed result to bundles/[TASK_ID]-result.json\n6. Return ONE LINE status to orchestrator\n\nIMPORTANT: Use the TASKER_DIR absolute path provided. Do NOT use relative paths.\n```\n\n---\n\n## Bundle Contents\n\nThe bundle (`{TASKER_DIR}/bundles/T001-bundle.json`) includes:\n\n| Field | Purpose |\n|-------|---------|\n| `task_id`, `name` | Task identification |\n| `target_dir` | Where to write code |\n| `behaviors` | Expanded behavior details (not just IDs) |\n| `files` | Paths, actions, purposes, layers |\n| `acceptance_criteria` | Verification commands |\n| `constraints` | Tech stack, patterns, testing |\n| `dependencies.files` | Files from prior tasks to read |\n| `context` | Domain, capability, spec reference |\n| `state_machine` | FSM context for adherence verification (if present) |\n\nGenerate bundles with:\n```bash\ntasker bundle generate T001       # Single task\ntasker bundle generate-ready      # All ready tasks\n```\n\n---\n\n## Execute Options\n\n| Command | Behavior |\n|---------|----------|\n| `/execute` | Interactive, one task at a time |\n| `/execute T005` | Execute specific task only |\n| `/execute --batch` | All ready tasks, no prompts |\n| `/execute --parallel 3` | Up to 3 tasks simultaneously |\n\n---\n\n## Graceful Halt and Resume\n\nThe executor supports graceful halt via two mechanisms:\n\n### 1. STOP File (Recommended for External Control)\n\nCreate a `STOP` file in the `.tasker/` directory:\n\n```bash\ntouch .tasker/STOP\n```\n\nThe executor checks for this file before starting each new task and after completing each task. When detected:\n1. Current task (if running) completes normally\n2. No new tasks are started\n3. State is saved with halt information\n4. Clean exit with instructions to resume\n\n### 2. User Message (For Interactive Sessions)\n\nIf a user sends \"STOP\" during an interactive `/execute` session, the orchestrator should:\n1. Call `tasker state halt user_message`\n2. Allow current task to complete\n3. Exit gracefully\n\n### Resuming Execution\n\nTo resume after a halt:\n\n```bash\n# Check current halt status\ntasker state halt-status\n\n# Clear halt and resume\ntasker state resume\n\n# Then run /execute again\n```\n\n---\n\n## FSM Coverage Report (After Execution Completes)\n\nAfter all tasks complete, generate the execution coverage report:\n\n```bash\ntasker fsm validate execute-coverage-report \\\n    {TASKER_DIR}/artifacts/fsm/index.json \\\n    {TASKER_DIR}/bundles \\\n    --output {TASKER_DIR}/artifacts/fsm-coverage.execute.json\n```\n\nThis report includes:\n- Which transitions were verified during execution\n- Evidence type for each transition (test, runtime_assertion, manual)\n- Which invariants were enforced\n- Pointers to tasks and acceptance criteria that provide evidence\n\n---\n\n## Evaluation Report (After Completion)\n\n**MANDATORY**: After all tasks complete (or execution halts), generate AND DISPLAY the evaluation report.\n\n### Step 1: Generate the report\n\nUse Bash tool:\n- `tasker evaluate --output {TASKER_DIR}/reports/evaluation-report.txt`\n- `tasker evaluate --format json --output {TASKER_DIR}/reports/evaluation-report.json`\n\n### Step 2: Read and display the report\n\n**CRITICAL:** Use the Read tool to read `{TASKER_DIR}/reports/evaluation-report.txt` and display its full contents to the user. Do NOT just say \"report generated\" - show the actual report.\n\nThe report includes:\n- **Planning Quality** - plan verdict and issues at planning time\n- **Execution Summary** - completed/failed/blocked/skipped counts\n- **First-Attempt Success Rate** - measures spec quality\n- **Verification Breakdown** - criteria pass/partial/fail, code quality scores\n- **Cost Analysis** - total tokens, total cost, per-task cost\n- **Failure Analysis** - which tasks failed and why (if any)\n- **Improvement Patterns** - common issues for process improvement\n\n### Step 3: Summarize key outcomes\n\nAfter displaying the report, provide a brief summary:\n- Total tasks completed vs failed\n- Any notable failures or issues requiring attention\n- Suggested next steps (e.g., \"run tests\", \"review changes\", \"check logs\")\n\n---\n\n## Archive Execution Artifacts (After Completion)\n\nAfter execution completes (all tasks done or halted), archive execution artifacts:\n\n```bash\ntasker archive execution {project_name}\n```\n\nThis creates:\n```\narchive/{project_name}/execution/{timestamp}/\n├── bundles/        # Task bundles and result files\n├── logs/           # Activity logs\n├── state.json      # State snapshot\n└── archive-manifest.json\n```\n\n---\n\n## State Commands Reference\n\n```bash\n# Execution\ntasker state ready               # List ready tasks\ntasker state task start <id>     # Mark running\ntasker state task complete <id>  # Mark done\ntasker state task fail <id> <e>  # Mark failed\ntasker state load-tasks          # Reload from files\n\n# Halt / Resume\ntasker state halt [reason]       # Request graceful halt\ntasker state check-halt          # Check if halted (exit 1 = halted)\ntasker state confirm-halt        # Confirm halt completed\ntasker state halt-status         # Show halt status\ntasker state resume              # Clear halt, resume execution\n\n# Checkpoint (Crash Recovery)\ntasker state checkpoint create <t1> [t2 ...]  # Create batch checkpoint\ntasker state checkpoint update <id> <status>  # Update task\ntasker state checkpoint complete              # Mark batch done\ntasker state checkpoint status                # Show current checkpoint\ntasker state checkpoint recover               # Recover orphaned tasks\ntasker state checkpoint clear                 # Remove checkpoint\n\n# Bundles\ntasker bundle generate <id>      # Generate bundle for task\ntasker bundle generate-ready     # Generate all ready bundles\ntasker bundle validate <id>      # Validate bundle against schema\ntasker bundle validate-integrity <id>  # Check deps + checksums\ntasker bundle list               # List existing bundles\ntasker bundle clean              # Remove all bundles\n```\n\n---\n\n## Error Recovery\n\nIf task fails:\n1. `tasker state task fail` marks it failed\n2. Rollback triggered (created files deleted, modified files restored)\n3. Dependent tasks auto-blocked\n4. Other ready tasks can continue\n5. User can retry later: fix issue, then `tasker state task retry` again\n",
        "skills/plan/SKILL.md": "---\nname: plan\ndescription: PLANNING PHASE - Decompose specification into task DAG with dependencies and phases. DO NOT invoke execute skill.\ntools:\n  - agent\n  - bash\n  - file_read\n  - file_write\n---\n\n# Plan Workflow\n\n**CRITICAL: This is the PLAN skill. Do NOT call Skill(execute) or Skill(tasker:execute). Follow the instructions below directly.**\n\n**IMPORTANT: All tasker working files go in `$TARGET_DIR/.tasker/`. Do NOT create any other directories like `project-planning/`, `planning/`, or `schemas/` at the target project root. The `.tasker/` directory is the ONLY location for tasker artifacts (including `.tasker/schemas/` for JSON schemas).**\n\nDecompose a specification into an executable task DAG. This is Phase 2 of the tasker workflow:\n\n```\n/specify → /plan → /execute\n```\n\n## Input Requirements\n\n- **Spec** from `/specify`: `{TARGET}/docs/specs/<slug>.md`\n- **Capability Map**: `{TARGET}/docs/specs/<slug>.capabilities.json`\n- **FSM Artifacts** (optional): `{TARGET}/docs/state-machines/<slug>/`\n\n## Output\n\n- **Task DAG**: `.tasker/tasks/T001.json`, `T002.json`, etc.\n- **Physical Map**: `.tasker/artifacts/physical-map.json`\n- **Validation Report**: `.tasker/reports/task-validation-report.md`\n\n---\n\n## MANDATORY FIRST STEP: Ask for Target Project Directory\n\n**ALWAYS ask for target_dir FIRST before anything else.** No guessing, no inference from CWD.\n\n### Step 1: Ask for Target Directory\n\nUse AskUserQuestion to ask:\n```\nWhat is the target project directory?\n```\nFree-form text input. User must provide an absolute or relative path.\n\n**Validation:**\n```bash\nTARGET_DIR=\"<user-provided-path>\"\n# Convert to absolute path\nTARGET_DIR=$(cd \"$TARGET_DIR\" 2>/dev/null && pwd || echo \"$TARGET_DIR\")\n\nif [ ! -d \"$TARGET_DIR\" ]; then\n    # For new projects, check parent exists\n    PARENT=$(dirname \"$TARGET_DIR\")\n    if [ -d \"$PARENT\" ]; then\n        echo \"Directory will be created: $TARGET_DIR\"\n        mkdir -p \"$TARGET_DIR\"\n    else\n        echo \"Error: Parent directory does not exist: $PARENT\"\n        # Re-ask for target_dir\n    fi\nfi\n```\n\n### Step 2: Check for Existing Session and Initialize\n\nAfter target_dir is confirmed, check for existing `.tasker/` state:\n\n```bash\nTASKER_DIR=\"$TARGET_DIR/.tasker\"\nif [ -f \"$TASKER_DIR/state.json\" ]; then\n    echo \"Found existing tasker session at $TASKER_DIR\"\n    echo \"Resuming from saved state...\"\n    # Read phase from state.json and resume\n    tasker state status\nelse\n    echo \"No existing session. Initializing...\"\n    # Initialize directory structure\nfi\n```\n\n### Step 3: Detect Specs (Automatic)\n\n**Specs are REQUIRED for /plan to proceed.** Check `$TARGET_DIR/docs/specs/` for specs from /specify:\n\n```bash\nSPEC_DIR=\"$TARGET_DIR/docs/specs\"\n\n# Find spec files (from /specify workflow)\nSPEC_FILES=$(find \"$SPEC_DIR\" -maxdepth 1 -name \"*.md\" 2>/dev/null)\nCAP_MAPS=$(find \"$SPEC_DIR\" -maxdepth 1 -name \"*.capabilities.json\" 2>/dev/null)\n\nif [ -n \"$SPEC_FILES\" ]; then\n    echo \"=== Specs found ===\"\n    echo \"$SPEC_FILES\"\n\n    # Use the first spec (or only spec)\n    SPEC_PATH=$(echo \"$SPEC_FILES\" | head -1)\n    SPEC_SLUG=$(basename \"$SPEC_PATH\" .md)\n\n    # Check for matching capability map\n    CAP_MAP=\"$SPEC_DIR/${SPEC_SLUG}.capabilities.json\"\n    if [ -f \"$CAP_MAP\" ]; then\n        echo \"Capability map found: $CAP_MAP\"\n        echo \"Can skip logic-architect phase\"\n    fi\nelse\n    echo \"No specs found in $SPEC_DIR\"\n    # BLOCK - see below\nfi\n```\n\n### If spec found:\n\nProceed to Step 4. No user question needed.\n\n### If NO spec found — BLOCK and offer /specify:\n\n**/plan CANNOT proceed without specs.** Present this to the user:\n\n```markdown\n## Specs Required\n\nPlanning requires a specification to decompose into tasks. Without a spec, there's nothing to plan.\n\n**Would you like to create a spec now using `/specify`?**\n\nThe `/specify` workflow will guide you through:\n1. Defining goals and scope\n2. Clarifying requirements through structured questions\n3. Extracting capabilities and behaviors\n4. Producing a spec ready for `/plan`\n```\n\nAsk using AskUserQuestion:\n```\nWould you like to run /specify to create a spec now?\n```\nOptions:\n- **Yes, start /specify** — Begin the specification workflow\n- **No, I'll provide a spec later** — Exit /plan for now\n\n### Step 4: Detect Project Type (Automatic)\n\n**Do not ask the user** — infer project type from target directory:\n\n```bash\n# Check if target directory has source code\nSOURCE_FILES=$(find \"$TARGET_DIR\" \\( -name \"*.py\" -o -name \"*.ts\" -o -name \"*.js\" -o -name \"*.go\" -o -name \"*.rs\" -o -name \"*.java\" \\) \\\n    -not -path \"*node_modules*\" -not -path \"*__pycache__*\" -not -path \"*.venv*\" -not -path \"*/.git/*\" 2>/dev/null | head -5)\n\nif [ -n \"$SOURCE_FILES\" ]; then\n    PROJECT_TYPE=\"existing\"\n    echo \"Detected existing project with source files\"\nelse\n    PROJECT_TYPE=\"new\"\n    echo \"New project (no existing source files)\"\nfi\n```\n\n- **Existing project** → Proceed to Step 5 (project analysis)\n- **New project** → Skip to Step 6 (tech stack)\n\n### Step 5: Existing Project Analysis (if PROJECT_TYPE=existing)\n\n**If enhancing an existing project**, you MUST analyze the target directory **BEFORE proceeding to ingestion**. This analysis is CRITICAL - sub-agents cannot see the codebase, so you must extract and pass this context to them.\n\n```bash\n# Check directory exists\nif [ ! -d \"$TARGET_DIR\" ]; then\n    echo \"Error: Target directory does not exist\"\n    exit 1\nfi\n\n# Analyze structure (capture output for context)\necho \"=== Project Structure ===\"\ntree -L 3 -I 'node_modules|__pycache__|.git|venv|.venv|dist|build|.pytest_cache' \"$TARGET_DIR\" 2>/dev/null || \\\n    find \"$TARGET_DIR\" -maxdepth 3 -type f | head -50\n\n# Identify key configuration files\necho \"=== Key Configuration Files ===\"\nfor f in package.json pyproject.toml Cargo.toml go.mod Makefile requirements.txt setup.py tsconfig.json; do\n    [ -f \"$TARGET_DIR/$f\" ] && echo \"Found: $f\"\ndone\n\n# Detect source layout patterns\necho \"=== Source Layout ===\"\nfor d in src lib app pkg cmd internal; do\n    [ -d \"$TARGET_DIR/$d\" ] && echo \"Found directory: $d/\"\ndone\n\n# Detect test layout\necho \"=== Test Layout ===\"\nfor d in tests test spec __tests__; do\n    [ -d \"$TARGET_DIR/$d\" ] && echo \"Found test directory: $d/\"\ndone\n\n# Sample existing code files to understand patterns\necho \"=== Code Samples ===\"\nfind \"$TARGET_DIR\" \\( -name \"*.py\" -o -name \"*.ts\" -o -name \"*.js\" -o -name \"*.go\" -o -name \"*.rs\" \\) \\\n    -not -path \"*node_modules*\" -not -path \"*__pycache__*\" -not -path \"*.venv*\" | head -10\n```\n\n**Read key files to understand patterns:**\n```bash\n# Read config files to understand dependencies and structure\n[ -f \"$TARGET_DIR/pyproject.toml\" ] && cat \"$TARGET_DIR/pyproject.toml\"\n[ -f \"$TARGET_DIR/package.json\" ] && cat \"$TARGET_DIR/package.json\"\n\n# Sample a few source files to understand coding patterns\n# (naming conventions, import style, architecture patterns)\n```\n\n### Step 6: Store Discovery Context\n\n**CRITICAL:** You must retain this analysis for passing to sub-agents. Store it as a structured context block:\n\n```\nPROJECT_CONTEXT = \"\"\"\nDirectory: {TARGET_DIR}\nProject Type: existing\nStack: {detected stack}\nSource Layout: {layout pattern}\nTest Layout: {test pattern}\n\nKey Patterns:\n- {pattern 1}\n- {pattern 2}\n- {pattern 3}\n\nIntegration Requirements:\n- {requirement 1}\n- {requirement 2}\n\"\"\"\n```\n\nThis `PROJECT_CONTEXT` MUST be included in every sub-agent spawn prompt (logic-architect, physical-architect, task-author). Without it, sub-agents will design solutions that conflict with existing code.\n\n---\n\n## Directory Initialization\n\n**CRITICAL:** After obtaining target_dir, initialize the `.tasker/` directory structure. Sub-agents assume directories already exist.\n\n```bash\nTASKER_DIR=\"$TARGET_DIR/.tasker\"\nmkdir -p \"$TASKER_DIR\"/{artifacts,inputs,tasks,reports,bundles,logs}\n```\n\nThis creates:\n- `$TARGET_DIR/.tasker/artifacts/` - For capability-map.json, physical-map.json\n- `$TARGET_DIR/.tasker/inputs/` - For spec.md\n- `$TARGET_DIR/.tasker/tasks/` - For T001.json, T002.json, etc.\n- `$TARGET_DIR/.tasker/reports/` - For task-validation-report.md\n- `$TARGET_DIR/.tasker/bundles/` - For execution bundles\n- `$TARGET_DIR/.tasker/logs/` - For activity logging\n\n---\n\n## Runtime Logging (MANDATORY)\n\nAll orchestrator activity and sub-agent activity MUST be logged using `./scripts/log-activity.sh`.\n\n```bash\n./scripts/log-activity.sh <LEVEL> <AGENT> <EVENT> \"<MESSAGE>\"\n```\n\n**Parameters:**\n- `LEVEL`: INFO, WARN, ERROR\n- `AGENT`: orchestrator, logic-architect, physical-architect, task-author, etc.\n- `EVENT`: start, decision, tool, complete, spawn, spawn-complete, phase-transition, validation\n- `MESSAGE`: Description of the activity\n\n---\n\n## Skip Phases for /specify-Generated Specs\n\nIf the spec came from `/specify` workflow, it has already been reviewed AND capabilities have been extracted. Check for artifacts:\n\n```bash\n# Derive capability map path from spec path\nSPEC_DIR=$(dirname \"$SPEC_PATH\")\nSPEC_SLUG=$(basename \"$SPEC_PATH\" .md)\nCAPABILITY_MAP=\"${SPEC_DIR}/${SPEC_SLUG}.capabilities.json\"\n\nif [ -f \"$CAPABILITY_MAP\" ]; then\n    echo \"Capability map found from /specify workflow: $CAPABILITY_MAP\"\n    echo \"Copying artifacts and skipping to physical phase...\"\n\n    # Copy artifacts to .tasker/ directory\n    cp \"$CAPABILITY_MAP\" \"$TASKER_DIR/artifacts/capability-map.json\"\n\n    # Check for FSM artifacts from /specify workflow\n    FSM_DIR=\"${SPEC_DIR}/../state-machines/${SPEC_SLUG}\"\n    if [ -d \"$FSM_DIR\" ]; then\n        echo \"FSM artifacts found from /specify workflow: $FSM_DIR\"\n        mkdir -p \"$TASKER_DIR/artifacts/fsm\"\n        cp -r \"$FSM_DIR\"/* \"$TASKER_DIR/artifacts/fsm/\"\n    fi\n\n    # Skip spec_review AND logical phases - advance directly to physical\n    tasker state set-phase physical\nfi\n```\n\n**Skip phases when `/specify` artifacts exist:**\n- `spec_review` - Skipped (already done by `/specify` Phase 7)\n- `logical` - Skipped (capability map already exists from `/specify` Phase 3)\n\n---\n\n## Plan Phase Dispatch\n\n```bash\n# Initialize if no state exists\nif [ ! -f \"$TASKER_DIR/state.json\" ]; then\n    tasker state init \"$TARGET_DIR\"\nfi\n\n# Check current phase\ntasker state status\n```\n\n| Phase | Agent | Output | Validation | Skip if |\n|-------|-------|--------|------------|---------|\n| `ingestion` | (none) | `inputs/spec.md` (verbatim) | File exists | — |\n| `spec_review` | **spec-reviewer** | `artifacts/spec-review.json` | All critical resolved | `/specify` artifacts exist |\n| `logical` | **logic-architect** | `artifacts/capability-map.json` | `validate capability_map` | `/specify` artifacts exist |\n| `physical` | **physical-architect** | `artifacts/physical-map.json` | `validate physical_map` | — |\n| `definition` | **task-author** | `tasks/*.json` | `load-tasks` | — |\n| `validation` | **task-plan-verifier** | Validation report | `validate-tasks <verdict>` | — |\n| `sequencing` | **plan-auditor** | Updated task phases | DAG is valid | — |\n| `ready` | (done) | Planning complete | — | — |\n\n---\n\n## Plan Loop\n\n```python\nwhile phase not in [\"ready\", \"executing\", \"complete\"]:\n    1. Query current phase\n    2. Spawn appropriate agent WITH FULL CONTEXT (see spawn templates below)\n    3. Wait for agent to complete\n    4. **VERIFY OUTPUT EXISTS** (critical - see below)\n       - DO NOT log spawn-complete until file verified\n       - DO NOT proceed to validation until file verified\n    5. If file missing: RE-SPAWN agent immediately (see recovery below)\n    6. Validate output:\n       - For artifacts: tasker state validate <artifact>\n       - For task validation: tasker state validate-tasks <verdict>\n    7. If valid: tasker state advance\n    8. If invalid: Tell agent to fix, re-validate\n```\n\n**CRITICAL: Never log \"spawn-complete: SUCCESS\" until the output file is verified to exist!**\n\n---\n\n## Output Verification Before Validation\n\n**MANDATORY STEP:** After each agent completes, you MUST verify its output file exists before attempting validation.\n\n```bash\n# After logic-architect completes:\nif [ ! -f $TASKER_DIR/artifacts/capability-map.json ]; then\n    echo \"ERROR: capability-map.json not written. Agent must retry.\"\nfi\n\n# After physical-architect completes:\nif [ ! -f $TASKER_DIR/artifacts/physical-map.json ]; then\n    echo \"ERROR: physical-map.json not written. Agent must retry.\"\nfi\n\n# After task-author completes:\ntask_count=$(ls $TASKER_DIR/tasks/*.json 2>/dev/null | wc -l)\nif [ \"$task_count\" -eq 0 ]; then\n    echo \"ERROR: No task files written. Agent must retry.\"\nfi\n```\n\n**Recovery procedure:** If file doesn't exist:\n1. Check if directory exists: `ls -la $TASKER_DIR/artifacts/`\n2. Re-spawn the agent with explicit reminder to use Write tool\n\n---\n\n## Agent Spawn Templates\n\n**CRITICAL:** Each sub-agent is context-isolated. They CANNOT see the orchestrator's conversation. You MUST pass ALL relevant context explicitly in the spawn prompt.\n\n### Physical Phase: physical-architect\n\n```\nMap behaviors to concrete file paths.\n\n## Logging (MANDATORY)\n\n./scripts/log-activity.sh INFO physical-architect start \"Mapping behaviors to file paths\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory}\nTarget Directory: {TARGET_DIR}\nProject Type: {new | existing}\nTech Stack: {from spec or inferred}\n\n## Project Context (CRITICAL for existing projects)\n\n{INSERT FULL PROJECT_CONTEXT HERE}\n\n## Your Task\n\n1. Read {TASKER_DIR}/artifacts/capability-map.json\n2. **For existing projects:** Map behaviors to paths that FIT the existing structure\n3. For new projects: Establish clean, conventional structure\n4. **CRITICAL: Use the Write tool** to save to {TASKER_DIR}/artifacts/physical-map.json\n5. **Verify file exists**: `ls -la {TASKER_DIR}/artifacts/physical-map.json`\n6. Validate with: `cd {TASKER_DIR}/.. && tasker state validate physical_map`\n```\n\n### Definition Phase: task-author\n\n```\nCreate individual task files from the physical map.\n\n## Logging (MANDATORY)\n\n./scripts/log-activity.sh INFO task-author start \"Creating task files\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory}\nTarget Directory: {TARGET_DIR}\nProject Type: {new | existing}\n\n## Project Context (for existing projects)\n\n{INSERT FULL PROJECT_CONTEXT HERE}\n\n## Your Task\n\n1. Read {TASKER_DIR}/artifacts/physical-map.json\n2. Read {TASKER_DIR}/artifacts/capability-map.json (for behavior details)\n3. **Check for FSM artifacts**: If {TASKER_DIR}/artifacts/fsm/index.json exists, add FSM context to tasks\n4. **CRITICAL: Use the Write tool** to save each task file to {TASKER_DIR}/tasks/T001.json, etc.\n5. **Verify files exist**: `ls -la {TASKER_DIR}/tasks/`\n6. Load tasks with: `cd {TASKER_DIR}/.. && tasker state load-tasks`\n```\n\n### Validation Phase: task-plan-verifier\n\n```\nVerify task definitions for planning\n\n## Logging (MANDATORY)\n\n./scripts/log-activity.sh INFO task-plan-verifier start \"Verifying task definitions\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory}\nSpec: {TASKER_DIR}/inputs/spec.md\nCapability Map: {TASKER_DIR}/artifacts/capability-map.json\nTasks Directory: {TASKER_DIR}/tasks/\n\n## Required Command\n\nRegister verdict using: tasker state validate-tasks <VERDICT> \"<summary>\"\n```\n\n### Sequencing Phase: plan-auditor\n\n```\nAssign phases to tasks and validate the dependency graph.\n\n## Logging (MANDATORY)\n\n./scripts/log-activity.sh INFO plan-auditor start \"Assigning phases and validating DAG\"\n\n## Context\n\nTASKER_DIR: {absolute path to .tasker directory}\n\n## Your Task\n\n1. Read {TASKER_DIR}/tasks/*.json\n2. Read {TASKER_DIR}/artifacts/capability-map.json (for steel thread flows)\n3. Build dependency graph\n4. Assign phases (1: foundations, 2: steel thread, 3+: features)\n5. **CRITICAL: Update task files** using Write tool\n6. Validate DAG (no cycles, deps in earlier phases)\n7. Run: cd {TASKER_DIR}/.. && tasker state load-tasks\n```\n\n---\n\n## Plan Completion\n\nWhen phase reaches \"ready\":\n```markdown\n## Planning Complete ✓\n\n**Tasks:** 24\n**Phases:** 4\n**Steel Thread:** T001 → T003 → T007 → T012\n\nRun `/execute` to begin implementation.\n```\n\n### Archive Planning Artifacts (Automatic)\n\nAfter planning completes, archive the artifacts:\n\n```bash\ntasker archive planning {project_name}\n```\n\n---\n\n## State Commands Reference\n\n```bash\n# General\ntasker state status          # Current phase, task counts\ntasker state advance         # Try to advance phase\n\n# Planning\ntasker state init <dir>      # Initialize new plan\ntasker state validate <art>  # Validate artifact\ntasker state validate-tasks <verdict> [summary]  # Register task validation result\ntasker state load-tasks      # Reload from files\n```\n\n---\n\n## Error Recovery\n\nIf agent produces invalid output:\n1. Validation fails (`tasker state validate` returns non-zero)\n2. Report errors to agent\n3. Agent fixes and re-outputs\n4. Re-validate\n5. Only advance on success\n",
        "skills/specify/SKILL.md": "---\nname: specify\ndescription: Interactive specification workflow - design vision, clarify capabilities, extract behaviors. Produces spec packets, capability maps, and ADRs for /plan consumption.\n---\n\n# Specify Workflow\n\n**IMPORTANT: All tasker working files go in `$TARGET_DIR/.tasker/`. Do NOT create any other directories like `project-planning/`, `planning/`, or `schemas/` at the target project root. The `.tasker/` directory is the ONLY location for tasker artifacts (including `.tasker/schemas/` for JSON schemas).**\n\nAn **agent-driven interactive workflow** that transforms ideas into actionable specifications with extracted capabilities, ready for `/plan` to decompose into tasks.\n\n## Core Principles\n\n1. **Workflows and invariants before architecture** - Never discuss implementation until behavior is clear\n2. **Decision-dense, not prose-dense** - Bullet points over paragraphs\n3. **No guessing** — Uncertainty becomes Open Questions\n4. **Minimal facilitation** — Decide only when required\n5. **Specs are living; ADRs are historical**\n6. **Less is more** — Avoid ceremony\n\n## Artifacts\n\n### Required Outputs (in TARGET project)\n- **README** — `{TARGET}/README.md` (project overview - what it is, how to use it)\n- **Spec Packet** — `{TARGET}/docs/specs/<slug>.md` (human-readable)\n- **Capability Map** — `{TARGET}/docs/specs/<slug>.capabilities.json` (machine-readable, for `/plan`)\n- **Behavior Model (FSM)** — `{TARGET}/docs/state-machines/<slug>/` (state machine artifacts, for `/plan` and `/execute`)\n- **ADR files** — `{TARGET}/docs/adrs/ADR-####-<slug>.md` (0..N)\n\n### Working Files (in target project's .tasker/)\n- **Session state** — `$TARGET_DIR/.tasker/state.json` (persistent, primary resume source)\n- **Spec draft** — `$TARGET_DIR/.tasker/spec-draft.md` (working draft, written incrementally)\n- **Discovery file** — `$TARGET_DIR/.tasker/clarify-session.md` (append-only log)\n- **Stock-takes** — `$TARGET_DIR/.tasker/stock-takes.md` (append-only log of vision evolution)\n- **Decision registry** — `$TARGET_DIR/.tasker/decisions.json` (index of decisions/ADRs)\n- **Spec Review** — `$TARGET_DIR/.tasker/spec-review.json` (weakness analysis)\n\n### Archive\nAfter completion, artifacts can be archived using `tasker archive` for post-hoc analysis.\n\n---\n\n## MANDATORY: Phase Order\n\n```\nInitialization → Scope → Clarification Loop (Discovery) → Synthesis → Architecture Sketch → Decisions/ADRs → Gate → Spec Review → Export\n```\n\n**NEVER skip or reorder these phases.**\n\n---\n\n# Phase 0 — Initialization\n\n## Goal\nEstablish project context and session state before specification work begins.\n\n## STEP 1: Auto-Detect Session State (MANDATORY FIRST)\n\n**Before asking the user anything**, check for existing session state files.\n\n### 1a. Determine Target Directory\n\nCheck in order:\n1. If user provided a path in their message, use that\n2. If CWD contains `.tasker/state.json`, use CWD\n3. Otherwise, ask:\n\n```\nWhat is the target project directory?\n```\n\n### 1b. Check for Existing Session\n\n```bash\nTARGET_DIR=\"<determined-path>\"\nSTATE_FILE=\"$TARGET_DIR/.tasker/state.json\"\n\nif [ -f \"$STATE_FILE\" ]; then\n    # Read state to determine if session is in progress\n    PHASE=$(jq -r '.phase.current' \"$STATE_FILE\")\n    if [ \"$PHASE\" != \"complete\" ] && [ \"$PHASE\" != \"null\" ]; then\n        echo \"RESUME: Found active session at phase '$PHASE'\"\n        # AUTO-RESUME - skip to Step 1c\n    else\n        echo \"NEW: Previous session completed. Starting fresh.\"\n        # Proceed to Step 2 (new session)\n    fi\nelse\n    echo \"NEW: No existing session found.\"\n    # Proceed to Step 2 (new session)\nfi\n```\n\n### 1c. Auto-Resume Protocol (if active session found)\n\n**If `.tasker/state.json` exists and `phase.current != \"complete\"`:**\n\n1. **Read state.json** to get current phase and step\n2. **Inform user** (no question needed):\n   ```\n   Resuming specification session for \"{spec_session.spec_slug}\"\n   Current phase: {phase.current}, step: {phase.step}\n   ```\n3. **Read required working files** for the current phase (see Resume Protocol section)\n4. **Jump directly to the current phase** - do NOT re-run earlier phases\n\n**This is automatic. Do not ask the user whether to resume.**\n\n---\n\n## STEP 2: New Session Setup (only if no active session)\n\n### 2a. No Guessing on Reference Materials\n\n**You MUST NOT:**\n- Scan directories to infer what files exist\n- Guess spec locations from directory structure\n- Read files to detect existing specs\n- Make any assumptions about what the user has\n\n**The user tells you everything. You ask, they answer.**\n\n### 2b. Ask About Reference Materials\n\nAsk using AskUserQuestion:\n```\nDo you have existing specification reference materials (PRDs, requirements docs, design docs, etc.)?\n```\nOptions:\n- **No reference materials** — Starting from scratch\n- **Yes, I have reference materials** — I'll provide the location(s)\n\n### If \"Yes, I have reference materials\":\nAsk for the location(s):\n```\nWhere are your reference materials located? (Provide path(s) - can be files or directories)\n```\nFree-form text input. User provides path(s) (e.g., `docs/specs/`, `requirements.md`, `PRD.pdf`).\n\n**Validate path exists:**\n```bash\nEXISTING_SPEC_PATH=\"<user-provided-path>\"\nif [ ! -e \"$TARGET_DIR/$EXISTING_SPEC_PATH\" ] && [ ! -e \"$EXISTING_SPEC_PATH\" ]; then\n    echo \"Warning: Path not found. Please verify the path.\"\nfi\n```\n\n### 2c. Initialize Session State\n\nCreate `.tasker/` directory structure in target project:\n\n```bash\nTASKER_DIR=\"$TARGET_DIR/.tasker\"\nmkdir -p \"$TASKER_DIR\"/{inputs,artifacts,tasks,bundles,reports,fsm-draft,adrs-draft}\n```\n\nCreate `$TARGET_DIR/.tasker/state.json`:\n\n```json\n{\n  \"version\": \"3.0\",\n  \"target_dir\": \"<absolute-path>\",\n  \"phase\": {\n    \"current\": \"initialization\",\n    \"completed\": [],\n    \"step\": null\n  },\n  \"created_at\": \"<timestamp>\",\n  \"updated_at\": \"<timestamp>\",\n  \"spec_session\": {\n    \"project_type\": \"new|existing\",\n    \"existing_spec_path\": \"<path-from-step-2-or-null>\",\n    \"spec_slug\": \"<slug>\",\n    \"spec_path\": \"<target>/docs/specs/<slug>.md\",\n    \"started_at\": \"<timestamp>\",\n    \"resumed_from\": null\n  },\n  \"scope\": null,\n  \"clarify\": null,\n  \"synthesis\": null,\n  \"architecture\": null,\n  \"decisions\": null,\n  \"review\": null\n}\n```\n\n**CRITICAL: Update state.json after EVERY significant action.** This enables resume from any point.\n\nThe phase-specific state objects are populated as each phase progresses (see phase definitions below).\n\n**If user provided existing spec path in Step 2b**, store it in `spec_session.existing_spec_path` for reference during Scope phase.\n\n## Output (New Session Only)\n\nFor **new sessions** (Step 2 path):\n- `.tasker/` directory structure created in target project\n- Session state initialized in `$TARGET_DIR/.tasker/state.json`\n- Existing spec path captured (if provided)\n- Proceed to Phase 1 (Scope)\n\nFor **resumed sessions** (Step 1c path):\n- State already exists - no initialization needed\n- Jump directly to `phase.current` phase\n- Read working files as specified in Resume Protocol\n\n---\n\n# Phase 1 — Scope\n\n## Goal\nEstablish bounds before discovery.\n\n## Pre-Scope: Load Existing Spec (if provided)\n\nIf `spec_session.existing_spec_path` was set during initialization:\n\n1. **Read the existing spec file** to understand prior context\n2. **Extract initial answers** for the scope questions below (Goal, Non-goals, Done means)\n3. **Present extracted context** to user for confirmation/refinement rather than asking from scratch\n\n```bash\nif [ -n \"$EXISTING_SPEC_PATH\" ]; then\n    echo \"Loading existing spec from: $EXISTING_SPEC_PATH\"\n    # Read and analyze existing spec\n    # Pre-fill scope questions with extracted information\nfi\n```\n\n## Required Questions (AskUserQuestion)\n\nAsk these questions using AskUserQuestion tool with structured options.\n**If existing spec was loaded**, present extracted answers for confirmation rather than blank questions:\n\n### Question 1: Goal\n```\nWhat are we building?\n```\nFree-form text input.\n\n### Question 2: Non-goals\n```\nWhat is explicitly OUT of scope?\n```\nFree-form text input (allow multiple items).\n\n### Question 3: Done Means\n```\nWhat are the acceptance bullets? (When is this \"done\"?)\n```\nFree-form text input (allow multiple items).\n\n### Question 4: Tech Stack\n```\nWhat tech stack should be used?\n```\nFree-form text input. Examples:\n- \"Python 3.12+ with FastAPI, PostgreSQL, Redis\"\n- \"TypeScript, Next.js, Prisma, Supabase\"\n- \"Go with Chi router, SQLite\"\n- \"Whatever fits best\" (let /specify recommend based on requirements)\n\n**If user says \"whatever fits best\" or similar:**\n- Note this for Phase 2 (Clarify) to recommend based on gathered requirements\n- Ask clarifying questions: \"Any language preferences?\", \"Cloud provider constraints?\", \"Team expertise?\"\n\n### Question 5: Entry Point (CRITICAL for W8/I6 compliance)\n```\nHow will users invoke this? What makes it available?\n```\n\nOptions to present:\n- **CLI command** — User runs a command (specify command name)\n- **API endpoint** — User calls an HTTP endpoint (specify URL pattern)\n- **Claude Code skill** — User invokes /skillname (specify trigger)\n- **Library/module** — No direct invocation, imported by other code\n- **Other** — Custom activation mechanism\n\n**If user selects CLI/API/Skill:**\nFollow up: \"What specific steps are needed to make this available to users?\"\n\n**If user selects Library/module:**\nNote in spec: \"Installation & Activation: N/A - library/module only\"\n\n**Why this matters:** Specs that describe invocation without activation mechanism cause W8 weakness and I6 invariant failure. Capturing this early prevents dead entry points.\n\n## Output\n\n### 1. Update State (MANDATORY)\n\nUpdate `$TARGET_DIR/.tasker/state.json`:\n```json\n{\n  \"phase\": {\n    \"current\": \"scope\",\n    \"completed\": [\"initialization\"],\n    \"step\": \"complete\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"scope\": {\n    \"goal\": \"<user-provided-goal>\",\n    \"non_goals\": [\"<item1>\", \"<item2>\"],\n    \"done_means\": [\"<criterion1>\", \"<criterion2>\"],\n    \"tech_stack\": \"<tech-stack-or-TBD>\",\n    \"entry_point\": {\n      \"type\": \"cli|api|skill|library|other\",\n      \"trigger\": \"<command-name or /skillname or endpoint>\",\n      \"activation_steps\": [\"<step1>\", \"<step2>\"]\n    },\n    \"completed_at\": \"<timestamp>\"\n  }\n}\n```\n\n### 2. Write Spec Draft (MANDATORY)\n\nWrite initial spec sections to `$TARGET_DIR/.tasker/spec-draft.md`:\n\n```markdown\n# Spec: {Title}\n\n## Goal\n{goal from scope}\n\n## Non-goals\n{non_goals from scope}\n\n## Done means\n{done_means from scope}\n\n## Tech Stack\n{tech_stack from scope}\n\n## Installation & Activation\n**Entry Point:** {entry_point.trigger from scope}\n**Type:** {entry_point.type from scope}\n\n**Activation Steps:**\n{entry_point.activation_steps from scope, numbered list}\n\n**Verification:**\n<!-- To be filled in during Clarify or Synthesis -->\n\n<!-- Remaining sections will be added by subsequent phases -->\n```\n\n**IMPORTANT:** All spec content is built in this file, NOT in conversation context. Read from this file when you need prior spec content.\n\n---\n\n# Phase 2 — Clarify (Ralph Iterative Discovery Loop)\n\n## Purpose\nExhaustively gather requirements via structured questioning.\n\n## Setup\n\n### 1. Initialize Clarify State in state.json (MANDATORY)\n\nUpdate `$TARGET_DIR/.tasker/state.json`:\n```json\n{\n  \"phase\": {\n    \"current\": \"clarify\",\n    \"completed\": [\"initialization\", \"scope\"],\n    \"step\": \"starting\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"clarify\": {\n    \"current_category\": \"core_requirements\",\n    \"current_round\": 1,\n    \"categories\": {\n      \"core_requirements\": { \"status\": \"not_started\", \"rounds\": 0 },\n      \"users_context\": { \"status\": \"not_started\", \"rounds\": 0 },\n      \"integrations\": { \"status\": \"not_started\", \"rounds\": 0 },\n      \"edge_cases\": { \"status\": \"not_started\", \"rounds\": 0 },\n      \"quality_attributes\": { \"status\": \"not_started\", \"rounds\": 0 },\n      \"existing_patterns\": { \"status\": \"not_started\", \"rounds\": 0 },\n      \"preferences\": { \"status\": \"not_started\", \"rounds\": 0 }\n    },\n    \"pending_followups\": [],\n    \"requirements_count\": 0,\n    \"stock_takes_count\": 0,\n    \"started_at\": \"<timestamp>\"\n  }\n}\n```\n\n### 2. Create Discovery File\n\nCreate `$TARGET_DIR/.tasker/clarify-session.md`:\n\n```markdown\n# Discovery: {TOPIC}\nStarted: {timestamp}\n\n## Category Status\n\n| Category | Status | Rounds | Notes |\n|----------|--------|--------|-------|\n| Core requirements | ○ Not Started | 0 | — |\n| Users & context | ○ Not Started | 0 | — |\n| Integrations | ○ Not Started | 0 | — |\n| Edge cases | ○ Not Started | 0 | — |\n| Quality attributes | ○ Not Started | 0 | — |\n| Existing patterns | ○ Not Started | 0 | — |\n| Preferences | ○ Not Started | 0 | — |\n\n## Discovery Rounds\n\n```\n\n### 3. Create Stock-Takes File\n\nCreate `$TARGET_DIR/.tasker/stock-takes.md`:\n\n```markdown\n# Stock-Takes: {TOPIC}\nStarted: {timestamp}\n\nThis file tracks how the vision evolves as discovery progresses.\n\n---\n\n```\n\n## CRITICAL: Resume Capability\n\n**On resume (after compaction or restart):**\n1. Read `$TARGET_DIR/.tasker/state.json` to get `clarify` state\n2. Read `$TARGET_DIR/.tasker/clarify-session.md` to get discovery history\n3. Resume from `clarify.current_category` and `clarify.current_round`\n4. If `clarify.pending_followups` is non-empty, continue follow-up loop first\n\n**DO NOT rely on conversation context for clarify progress. Always read from files.**\n\n## Loop Rules\n\n- **No iteration cap** - Continue until goals are met\n- **Category Focus Mode** - Work on ONE category at a time until it's complete or explicitly deferred\n- Each iteration:\n  1. Read discovery file\n  2. Select ONE incomplete category to focus on (priority: Core requirements → Users & context → Integrations → Edge cases → Quality attributes → Existing patterns → Preferences)\n  3. Ask **2–4 questions** within that focused category\n  4. Get user answers\n  5. **Run Follow-up Sub-loop** (see below) - validate and drill down on answers\n  6. Only after follow-ups are complete: update discovery file, extract requirements, update category status\n  7. Repeat within same category until goal is met OR user says \"move on from this category\"\n\n- **Clarity Before Progress** - If user response is anything except a direct answer (counter-question, confusion, pushback, tangential), provide clarification FIRST. Do NOT present new questions until prior questions have direct answers.\n\n- **Stop ONLY when:**\n  - ALL category goals are met (see checklist), OR\n  - User says \"enough\", \"stop\", \"move on\", or similar\n\n## Follow-up Sub-loop (MANDATORY)\n\nAfter receiving answers to a question round, **DO NOT immediately move to the next round**. First, validate each answer:\n\n### Answer Validation Triggers\n\nFor each answer, check if follow-up is required:\n\n| Trigger | Example | Required Follow-up |\n|---------|---------|-------------------|\n| **Vague quantifier** | \"several users\", \"a few endpoints\" | \"How many specifically?\" |\n| **Undefined scope** | \"and so on\", \"etc.\", \"things like that\" | \"Can you list all items explicitly?\" |\n| **Weak commitment** | \"probably\", \"maybe\", \"I think\" | \"Is this confirmed or uncertain?\" |\n| **Missing specifics** | \"fast response\", \"secure\" | \"What's the specific target? (e.g., <100ms)\" |\n| **Deferred knowledge** | \"I'm not sure\", \"don't know yet\" | \"Should we make a default assumption, or is this blocking?\" |\n| **Contradicts earlier answer** | Conflicts with prior round | \"Earlier you said X, now Y. Which is correct?\" |\n\n### Sub-loop Process\n\n```\nFor each answer in current round:\n  1. Check against validation triggers\n  2. If trigger found:\n     a. Add to pending_followups in state.json\n     b. Ask ONE follow-up question (not batched)\n     c. Wait for response\n     d. Remove from pending_followups, re-validate the new response\n     e. Repeat until answer is concrete OR user explicitly defers\n  3. Only after ALL answers validated → proceed to next round\n```\n\n### MANDATORY: Persist Follow-up State\n\nBefore asking a follow-up question, update `state.json`:\n```json\n{\n  \"clarify\": {\n    \"pending_followups\": [\n      {\n        \"question_id\": \"Q3.2\",\n        \"original_answer\": \"<user's vague answer>\",\n        \"trigger\": \"vague_quantifier\",\n        \"followup_question\": \"<the follow-up question being asked>\"\n      }\n    ]\n  }\n}\n```\n\nAfter receiving follow-up response, remove from `pending_followups` and update the round in `clarify-session.md`.\n\n### Follow-up Question Format\n\nUse AskUserQuestion with context from the original answer:\n\n```json\n{\n  \"question\": \"You mentioned '{user_quote}'. {specific_follow_up_question}\",\n  \"header\": \"Clarify\",\n  \"options\": [\n    {\"label\": \"Specify\", \"description\": \"I'll provide a specific answer\"},\n    {\"label\": \"Not critical\", \"description\": \"This detail isn't important for the spec\"},\n    {\"label\": \"Defer\", \"description\": \"I don't know yet, note as open question\"}\n  ]\n}\n```\n\n### Handling Non-Direct Responses (MANDATORY)\n\nIf the user's response is **anything other than a direct answer**, assume clarification is required. Do NOT present new questions until the original question is resolved.\n\n| Response Type | Example | Required Action |\n|---------------|---------|-----------------|\n| **Counter-question** | \"What do you mean by X?\" | Answer their question, then re-ask yours |\n| **Confusion** | \"I'm not sure what you're asking\" | Rephrase the question with more context |\n| **Pushback** | \"Why do you need to know that?\" | Explain why this matters for the spec |\n| **Tangential** | Talks about something else | Acknowledge, then redirect to the question |\n| **Partial answer** | Answers part, ignores rest | Note the partial, ask about the unanswered part |\n| **Meta-comment** | \"This is getting detailed\" | Acknowledge, offer to simplify or defer |\n\n**Process:**\n```\n1. Detect non-direct response\n2. Address the user's concern/question FIRST\n3. Only after clarity achieved → re-present the original question (or confirm it's now answered)\n4. Do NOT batch new questions until all prior questions have direct answers\n```\n\n**Example:**\n```\nAgent: \"What authentication method should users use?\"\nUser: \"What options are there?\"\n\n[NON-DIRECT: User asked a counter-question]\n\nAgent: \"Common options are: (1) Email/password, (2) OAuth (Google, GitHub),\n       (3) Magic link, (4) SSO/SAML. Which fits your use case?\"\nUser: \"Email/password for now, maybe OAuth later.\"\n\n[NOW DIRECT: Can proceed]\n```\n\n### When NOT to Follow Up\n\n- User explicitly says \"that's all I know\" or \"let's move on\"\n- The answer is already concrete and specific\n- The detail is genuinely non-blocking (implementation detail, not spec-level)\n- You've already asked 2 follow-ups on the same answer (avoid interrogation)\n\n### Example Interaction\n\n```\nAgent: \"What integrations are required?\"\nUser: \"We need to connect to a few external services and maybe some APIs.\"\n\n[FOLLOW-UP REQUIRED: \"a few\" is vague, \"maybe\" is weak commitment]\n\nAgent: \"You mentioned 'a few external services'. Can you list all the external systems this will integrate with?\"\nUser: \"Stripe for payments, SendGrid for email, and our internal user service.\"\n\n[ANSWER NOW CONCRETE: specific services named]\n\nAgent: \"You said 'maybe some APIs'. Are there additional API integrations beyond Stripe, SendGrid, and the user service?\"\nUser: \"No, that's all.\"\n\n[CATEGORY GOAL PROGRESS: Integrations now has concrete list]\n```\n\n## Category Checklist (Goal-Driven Coverage)\n\nEach category has concrete \"done\" criteria. Track completion in the discovery file.\n\n| Category | Goal (Done When) |\n|----------|------------------|\n| **Core requirements** | Primary workflows enumerated with inputs, outputs, and happy path steps |\n| **Users & context** | User roles identified, expertise levels known, access patterns clear |\n| **Integrations / boundaries** | All external systems named, data flows mapped, API contracts sketched |\n| **Edge cases / failures** | Error handling defined for each workflow step, retry/fallback behavior specified |\n| **Quality attributes** | Performance targets have numbers (or explicit \"not critical\"), security requirements stated |\n| **Existing patterns** | Relevant prior art identified OR confirmed none exists, conventions to follow listed |\n| **Preferences / constraints** | Tech stack decided, deployment target known, timeline/resource constraints stated |\n\n### Tracking Format\n\nUpdate discovery file with completion status:\n\n```markdown\n## Category Status\n\n| Category | Status | Notes |\n|----------|--------|-------|\n| Core requirements | ✓ Complete | 3 workflows defined |\n| Users & context | ✓ Complete | 2 roles: admin, user |\n| Integrations | ⋯ In Progress | DB confirmed, auth TBD |\n| Edge cases | ○ Not Started | — |\n| Quality attributes | ○ Not Started | — |\n| Existing patterns | ✓ Complete | Follow auth module pattern |\n| Preferences | ⋯ In Progress | Python confirmed, framework TBD |\n```\n\n### Completion Criteria\n\nA category is **complete** when:\n1. The goal condition is satisfied (see table above)\n2. User has confirmed or provided the information\n3. **All answers have passed follow-up validation** (no vague quantifiers, no weak commitments, no undefined scope)\n4. No obvious follow-up questions remain for that category\n5. User has explicitly confirmed or the agent has verified understanding\n\n**Do NOT mark complete** if:\n- User said \"I don't know\" without a fallback decision\n- Information is vague (e.g., \"fast\" instead of \"<100ms\")\n- Dependencies on other categories are unresolved\n- **Follow-up validation has not been run on all answers**\n- **Any answer contains unresolved triggers** (vague quantifiers, weak commitments, etc.)\n\n### Category Transition Rules\n\nBefore moving to a new category:\n1. **Summarize** what was learned in the current category\n2. **Confirm** with user: \"I've captured X, Y, Z for [category]. Does that cover everything, or is there more?\"\n3. **Run Stock-Take** (see below)\n4. **Only then** move to the next incomplete category\n\nThis prevents the feeling of being \"rushed\" through categories.\n\n## Stock-Taking (Big Picture Synthesis)\n\n### Purpose\n\nAs questions are answered and categories complete, periodically synthesize the \"big picture\" - what's emerging, the shape of the vision. This helps users see how their answers are building toward something coherent and provides calibration moments.\n\n### Trigger\n\nStock-take is triggered **after each category completes** (before transitioning to the next category). This creates a natural rhythm of ~5-7 stock-takes during Phase 2.\n\n### Content\n\nA stock-take is NOT a list of answers. It's a **synthesis of meaning** - what's taking shape:\n\n1. **What we're building** (1-2 sentences, evolving as understanding deepens)\n2. **Key constraints/boundaries** that have emerged from answers\n3. **The shape becoming visible** (patterns, tensions, tradeoffs surfacing)\n4. **Direction check** - light confirmation the vision still feels right\n\n### Format\n\n```markdown\n**Taking stock** (after {category_name}):\n\n{1-3 sentence synthesis of what's emerging - not a summary of answers, but the picture forming}\n\n{Any notable patterns, tensions, or tradeoffs becoming visible}\n\nDoes this still capture where we're heading?\n```\n\n### Example\n\nAfter completing \"Integrations\" category:\n\n> **Taking stock** (after Integrations):\n>\n> We're building a CLI skill system where specs drive task decomposition. The emphasis is on preventing incomplete handoffs - every behavior must trace back to stated requirements. The system is self-contained except for Git (for state persistence) and Claude Code (as the execution runtime).\n>\n> There's tension between thoroughness and workflow friction that keeps surfacing - users want comprehensive specs but not interrogation.\n>\n> Does this still capture where we're heading?\n\n### Tone\n\n- **Reflective**, not interrogative\n- **Synthesizing**, not summarizing\n- **Calibrating**, not gate-checking\n\nThe question at the end is light - \"Does this still feel right?\" not \"Please confirm items 1-7.\"\n\n### Process\n\nAfter category completion:\n\n1. **Read accumulated state** from `spec-draft.md` (scope), `clarify-session.md` (discovery so far)\n2. **Synthesize** the emerging picture (not regurgitate answers)\n3. **Present** the stock-take to user\n4. **Listen** for any course correction or \"that's not quite right\"\n5. **Append** to `$TARGET_DIR/.tasker/stock-takes.md`\n6. **Update** `state.json` with `stock_takes_count`\n\n### Stock-Takes File Format\n\nAppend each stock-take to `$TARGET_DIR/.tasker/stock-takes.md`:\n\n```markdown\n---\n\n## Stock-Take {N} — After {Category Name}\n*{timestamp}*\n\n{The synthesis content}\n\n**User response:** {confirmed | adjusted: brief note}\n\n---\n```\n\n### State Update\n\nAfter each stock-take, update `state.json`:\n```json\n{\n  \"clarify\": {\n    \"stock_takes_count\": N\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n### When NOT to Stock-Take\n\n- **Early exit**: If user says \"move on\" mid-category, skip stock-take for that category\n- **Minor category**: If a category yielded very little new information, stock-take can be brief or combined with next\n- **User impatience**: If user explicitly wants to skip calibration, respect that\n\n## AskUserQuestion Format\n\n### Primary Questions (Category-Focused)\n\nUse AskUserQuestion with 2-4 questions per iteration, **all within the same category**:\n\n```\nquestions:\n  - question: \"How should the system handle [specific scenario]?\"\n    header: \"Edge case\"  # Keep headers consistent within a round\n    options:\n      - label: \"Option A\"\n        description: \"Description of approach A\"\n      - label: \"Option B\"\n        description: \"Description of approach B\"\n    multiSelect: false\n```\n\n**IMPORTANT:** Do NOT mix categories in a single question batch. If you're asking about \"Edge cases\", all 2-4 questions should be about edge cases.\n\n### Follow-up Questions (Single Question)\n\nFor follow-ups during the validation sub-loop, ask **ONE question at a time**:\n\n```\nquestions:\n  - question: \"You mentioned '{user_quote}'. Can you be more specific about X?\"\n    header: \"Clarify\"\n    options:\n      - label: \"Specify\"\n        description: \"I'll provide details\"\n      - label: \"Not critical\"\n        description: \"This isn't spec-relevant\"\n      - label: \"Defer\"\n        description: \"Note as open question\"\n    multiSelect: false\n```\n\n### Open-ended Questions\n\nFor open-ended questions, use free-form with context:\n```\nquestions:\n  - question: \"What integrations are required?\"\n    header: \"Integrations\"\n    options:\n      - label: \"REST API\"\n        description: \"Standard HTTP/JSON endpoints\"\n      - label: \"Database direct\"\n        description: \"Direct database access\"\n      - label: \"Message queue\"\n        description: \"Async via queue (Kafka, RabbitMQ, etc.)\"\n    multiSelect: true\n```\n\n## Updating Discovery File AND State (MANDATORY)\n\nAfter each Q&A round AND its follow-ups are complete:\n\n### 1. Append to Discovery File\n\nAppend to `$TARGET_DIR/.tasker/clarify-session.md`:\n\n```markdown\n### Round N — [Category Name]\n\n**Questions:**\n1. [Question text]\n2. [Question text]\n\n**Answers:**\n1. [User's answer]\n2. [User's answer]\n\n**Follow-ups:**\n- Q1 follow-up: \"[follow-up question]\" → \"[user response]\"\n- Q2: No follow-up needed (answer was specific)\n\n**Requirements Discovered:**\n- REQ-NNN: [Req 1]\n- REQ-NNN: [Req 2]\n\n**Category Status:** [✓ Complete | ⋯ In Progress | User deferred]\n```\n\n### 2. Update State (MANDATORY after every round)\n\nUpdate `$TARGET_DIR/.tasker/state.json`:\n```json\n{\n  \"phase\": {\n    \"step\": \"round_N_complete\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"clarify\": {\n    \"current_category\": \"<category>\",\n    \"current_round\": N+1,\n    \"categories\": {\n      \"<category>\": { \"status\": \"in_progress|complete\", \"rounds\": N }\n    },\n    \"pending_followups\": [],\n    \"requirements_count\": <total REQ count>\n  }\n}\n```\n\n### 3. Update Category Status Table\n\nAlso update the Category Status table at the top of `clarify-session.md` to reflect current state.\n\n**NOTE:** Do NOT proceed to next round until both files are updated. This ensures resumability.\n\n## Completion Signal\n\nWhen ALL category goals are met:\n\n1. Verify all categories show \"✓ Complete\" in the status table\n2. Confirm no blocking questions remain\n3. Update state.json:\n```json\n{\n  \"phase\": {\n    \"current\": \"clarify\",\n    \"completed\": [\"initialization\", \"scope\"],\n    \"step\": \"complete\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"clarify\": {\n    \"status\": \"complete\",\n    \"completed_at\": \"<timestamp>\",\n    \"categories\": { /* all marked complete */ },\n    \"requirements_count\": <final count>\n  }\n}\n```\n4. Output:\n```\n<promise>CLARIFIED</promise>\n```\n\n**If user requests early exit:** Accept it, mark incomplete categories in state.json with `status: \"deferred\"`, and note in discovery file for Phase 3 to flag as assumptions.\n\n---\n\n# Phase 3 — Synthesis (Derived, Not Asked)\n\n## Purpose\nDerive structured requirements AND capabilities from discovery. **No new information introduced here.**\n\nThis phase produces TWO outputs:\n1. **Spec sections** (human-readable) - Workflows, invariants, interfaces\n2. **Capability map** (machine-readable) - For `/plan` to consume\n\n## CRITICAL: State-Driven, Not Context-Driven\n\n**On entry to Phase 3:**\n1. Read `$TARGET_DIR/.tasker/state.json` to confirm `clarify.status == \"complete\"`\n2. Read `$TARGET_DIR/.tasker/clarify-session.md` for ALL discovery content\n3. Read `$TARGET_DIR/.tasker/spec-draft.md` for existing spec sections (Goal, Non-goals, etc.)\n\n**DO NOT rely on conversation context for discovery content. Read from files.**\n\n## Initialize Synthesis State\n\nUpdate `$TARGET_DIR/.tasker/state.json`:\n```json\n{\n  \"phase\": {\n    \"current\": \"synthesis\",\n    \"completed\": [\"initialization\", \"scope\", \"clarify\"],\n    \"step\": \"starting\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"synthesis\": {\n    \"status\": \"in_progress\",\n    \"spec_sections\": {\n      \"workflows\": false,\n      \"invariants\": false,\n      \"interfaces\": false,\n      \"open_questions\": false\n    },\n    \"capability_map\": {\n      \"domains_count\": 0,\n      \"capabilities_count\": 0,\n      \"behaviors_count\": 0,\n      \"steel_thread_identified\": false\n    },\n    \"fsm\": {\n      \"machines_count\": 0,\n      \"states_count\": 0,\n      \"transitions_count\": 0,\n      \"invariants_validated\": false\n    }\n  }\n}\n```\n\n## Process\n\n1. Read `$TARGET_DIR/.tasker/clarify-session.md` completely\n2. Extract and organize into spec sections (update spec-draft.md after each)\n3. Decompose into capabilities using I.P.S.O.A. taxonomy\n4. Everything must trace to a specific discovery answer\n\n---\n\n## Part A: Spec Sections\n\n### Workflows\nNumbered steps with variants and failures:\n```markdown\n## Workflows\n\n### 1. [Primary Workflow Name]\n1. User initiates X\n2. System validates Y\n3. System performs Z\n4. System returns result\n\n**Variants:**\n- If [condition], then [alternative flow]\n\n**Failures:**\n- If [error], then [error handling]\n\n**Postconditions:**\n- [State after completion]\n```\n\n### Invariants\nBulleted rules that must ALWAYS hold:\n```markdown\n## Invariants\n- [Rule that must never be violated]\n- [Another invariant]\n```\n\n### Interfaces\nOnly if a boundary exists:\n```markdown\n## Interfaces\n- [Interface description]\n\n(or \"No new/changed interfaces\" if none)\n```\n\n### Open Questions\nClassified by blocking status:\n```markdown\n## Open Questions\n\n### Blocking\n- [Question that affects workflows/invariants/interfaces]\n\n### Non-blocking\n- [Question about internal preferences only]\n```\n\n### Part A Output: Update Files (MANDATORY)\n\nAfter synthesizing each spec section:\n\n**1. Append section to `$TARGET_DIR/.tasker/spec-draft.md`:**\n```markdown\n## Workflows\n[Synthesized workflows content]\n\n## Invariants\n[Synthesized invariants content]\n\n## Interfaces\n[Synthesized interfaces content]\n\n## Open Questions\n[Synthesized open questions content]\n```\n\n**2. Update state.json after EACH section:**\n```json\n{\n  \"synthesis\": {\n    \"spec_sections\": {\n      \"workflows\": true,\n      \"invariants\": true,\n      \"interfaces\": false,\n      \"open_questions\": false\n    }\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n---\n\n## Part A.5: Behavior Model Compilation (FSM)\n\nAfter synthesizing Workflows, Invariants, and Interfaces, compile the Behavior Model (state machine).\n\n### Purpose\n\nThe FSM serves two purposes:\n1. **QA during implementation** - Shapes acceptance criteria, enables transition/guard coverage verification\n2. **Documentation** - Human-readable diagrams for ongoing system understanding\n\n### CRITICAL INVARIANT: Canonical Truth\n\n> **FSM JSON is canonical; Mermaid is generated. `/plan` and `/execute` must fail if required transitions and invariants lack coverage evidence.**\n\n- **Canonical artifacts**: `*.states.json`, `*.transitions.json`, `index.json`\n- **Derived artifacts**: `*.mmd` (Mermaid diagrams) - generated ONLY from canonical JSON\n- **NEVER** manually edit `.mmd` files - regenerate from JSON using `fsm-mermaid.py`\n- If Mermaid is ever edited manually, the system loses machine trust\n\n### Compilation Steps\n\n1. **Identify Steel Thread Flow**: The primary end-to-end workflow\n2. **Derive States**: Convert workflow steps to business states\n   - Initial state from workflow trigger\n   - Normal states from step postconditions\n   - Success terminal from workflow completion\n   - Failure terminals from failure clauses\n3. **Derive Transitions**: Convert step sequences, variants, and failures\n   - Happy path: step N → step N+1\n   - Variants: conditional branches with guards\n   - Failures: error transitions to failure states\n4. **Link Guards to Invariants**: Map spec invariants to transition guards\n5. **Validate Completeness**: Run I1-I6 checks (see below)\n6. **Resolve Ambiguity**: Use AskUserQuestion for any gaps\n\n### Completeness Invariants\n\nThe FSM MUST satisfy these invariants:\n\n| ID | Invariant | Check |\n|----|-----------|-------|\n| I1 | Steel Thread FSM mandatory | At least one machine for primary workflow |\n| I2 | Behavior-first | No architecture dependencies required |\n| I3 | Completeness | Initial state, terminals, no dead ends |\n| I4 | Guard-Invariant linkage | Every guard links to an invariant ID |\n| I5 | No silent ambiguity | Vague terms resolved or flagged as Open Questions |\n| I6 | Precondition reachability | If initial transition requires external trigger (e.g., \"user invokes X\"), the preconditions for that trigger must be specified in the spec |\n\n**I6 Detailed Check:**\nIf the first transition's trigger describes user invocation (e.g., \"user runs /command\", \"user invokes skill\"):\n1. Check if spec has \"Installation & Activation\" section\n2. Verify the activation mechanism makes the trigger possible\n3. If missing, flag as W8 weakness (missing activation requirements)\n\nExample I6 failure:\n- FSM starts: `Idle --[user invokes /kx]--> Running`\n- Spec has NO section explaining how `/kx` becomes available\n- **I6 FAILS**: \"Precondition for initial transition 'user invokes /kx' not reachable - no activation mechanism specified\"\n\n### Complexity Triggers (Splitting Rules)\n\nCreate additional machines based on **structural heuristics**, not just state count:\n\n**State Count Triggers:**\n- Steel Thread exceeds 12 states → split into domain-level sub-machines\n- Any machine exceeds 20 states → mandatory split\n\n**Structural Triggers (split even with fewer states):**\n- **Divergent user journeys**: Two or more distinct journeys that share only an initial prefix, then branch into unrelated flows → separate machines for each journey\n- **Unrelated failure clusters**: Multiple failure states that handle different categories of errors (e.g., validation errors vs. system errors vs. business rule violations) → group related failures into their own machines\n- **Mixed abstraction levels**: Machine combines business lifecycle states (e.g., Order: Created → Paid → Shipped) with UI microstates (e.g., Modal: Open → Editing → Validating) → separate business lifecycle from UI state machines\n- **Cross-boundary workflows**: Workflow that spans multiple bounded contexts or domains → domain-level machine for each context\n\n**Hierarchy Guidelines:**\n- `steel_thread` level: Primary end-to-end flow\n- `domain` level: Sub-flows within a bounded context\n- `entity` level: Lifecycle states for a specific entity\n\n### Ambiguity Resolution\n\nIf the compiler detects ambiguous workflow language, use AskUserQuestion:\n\n```json\n{\n  \"question\": \"The workflow step '{step}' has ambiguous outcome. What business state results?\",\n  \"header\": \"FSM State\",\n  \"options\": [\n    {\"label\": \"Define state\", \"description\": \"I'll provide the state name\"},\n    {\"label\": \"Same as previous\", \"description\": \"Remains in current state\"},\n    {\"label\": \"Terminal success\", \"description\": \"Workflow completes successfully\"},\n    {\"label\": \"Terminal failure\", \"description\": \"Workflow fails with error\"}\n  ]\n}\n```\n\n### FSM Working Files (Written Incrementally)\n\nDuring synthesis, write FSM drafts to `$TARGET_DIR/.tasker/fsm-draft/`:\n- `index.json` - Machine list, hierarchy, primary machine\n- `steel-thread.states.json` - State definitions (S1, S2, ...)\n- `steel-thread.transitions.json` - Transition definitions (TR1, TR2, ...)\n- `steel-thread.notes.md` - Ambiguity resolutions and rationale\n\n**Update state.json after each FSM file:**\n```json\n{\n  \"synthesis\": {\n    \"fsm\": {\n      \"machines_count\": 1,\n      \"states_count\": 8,\n      \"transitions_count\": 12,\n      \"files_written\": [\"index.json\", \"steel-thread.states.json\"],\n      \"invariants_validated\": false\n    }\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n### FSM Final Output Structure\n\nFinal FSM artifacts are exported to `{TARGET}/docs/state-machines/<slug>/` in Phase 8:\n- `index.json` - Machine list, hierarchy, primary machine\n- `steel-thread.states.json` - State definitions (S1, S2, ...)\n- `steel-thread.transitions.json` - Transition definitions (TR1, TR2, ...)\n- `steel-thread.mmd` - Mermaid stateDiagram-v2 for visualization (DERIVED from JSON)\n- `steel-thread.notes.md` - Ambiguity resolutions and rationale\n\n### ID Conventions (FSM-specific)\n\n- Machines: `M1`, `M2`, `M3`...\n- States: `S1`, `S2`, `S3`...\n- Transitions: `TR1`, `TR2`, `TR3`...\n\n### Traceability (Spec Provenance - MANDATORY)\n\nEvery state and transition MUST have a `spec_ref` pointing to the specific workflow step, variant, or failure that defined it. This prevents \"FSM hallucination\" where states/transitions are invented without spec basis.\n\n**Required for each state:**\n- `spec_ref.quote` - Verbatim text from the spec that defines this state\n- `spec_ref.location` - Section reference (e.g., \"Workflow 1, Step 3\")\n\n**Required for each transition:**\n- `spec_ref.quote` - Verbatim text from the spec that defines this transition\n- `spec_ref.location` - Section reference (e.g., \"Workflow 1, Variant 2\")\n\n**If no spec text exists for an element:**\n1. The element should NOT be created (likely FSM hallucination)\n2. Or, use AskUserQuestion to get clarification and document the decision\n\n---\n\n## Part B: Capability Extraction\n\nExtract capabilities from the synthesized workflows using **I.P.S.O.A. decomposition**.\n\n### I.P.S.O.A. Behavior Taxonomy\n\nFor each capability, identify behaviors by type:\n\n| Type | Description | Examples |\n|------|-------------|----------|\n| **Input** | Validation, parsing, authentication | Validate email format, parse JSON body |\n| **Process** | Calculations, decisions, transformations | Calculate total, apply discount rules |\n| **State** | Database reads/writes, cache operations | Save order, fetch user profile |\n| **Output** | Responses, events, notifications | Return JSON, emit event, send email |\n| **Activation** | Registration, installation, deployment | Register skill, deploy endpoint, write config |\n\n### Activation Behaviors\n\nIf the spec describes user invocation (e.g., \"user runs /command\"), extract activation behaviors:\n- **Registration**: Skill/plugin registration with runtime\n- **Installation**: CLI or package installation steps\n- **Deployment**: API endpoint or service deployment\n- **Configuration**: Config files or environment setup\n\n**Missing activation = coverage gap.** If spec says \"user invokes X\" but doesn't specify how X becomes available, add to `coverage.gaps`.\n\n### Domain Grouping\n\nGroup related capabilities into domains:\n- **Authentication** - Login, logout, session management\n- **User Management** - Profile, preferences, settings\n- **Core Feature** - The primary business capability\n- etc.\n\n### Steel Thread Identification\n\nIdentify the **steel thread** - the minimal end-to-end flow that proves the system works:\n- Mark one flow as `is_steel_thread: true`\n- This becomes the critical path for Phase 1 implementation\n\n### Capability Map Working File\n\nDuring synthesis, write capability map draft to `$TARGET_DIR/.tasker/capability-map-draft.json`.\n\n**Update state.json as you build the map:**\n```json\n{\n  \"synthesis\": {\n    \"capability_map\": {\n      \"domains_count\": 3,\n      \"capabilities_count\": 8,\n      \"behaviors_count\": 24,\n      \"steel_thread_identified\": true,\n      \"draft_written\": true\n    }\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n### Capability Map Final Output\n\nIn Phase 8, write final to `{TARGET}/docs/specs/<slug>.capabilities.json`:\n\n```json\n{\n  \"version\": \"1.0\",\n  \"spec_checksum\": \"<first 16 chars of SHA256 of spec>\",\n\n  \"domains\": [\n    {\n      \"id\": \"D1\",\n      \"name\": \"Authentication\",\n      \"description\": \"User identity and access\",\n      \"capabilities\": [\n        {\n          \"id\": \"C1\",\n          \"name\": \"User Login\",\n          \"discovery_ref\": \"Round 3, Q2: User confirmed email/password auth\",\n          \"behaviors\": [\n            {\"id\": \"B1\", \"name\": \"ValidateCredentials\", \"type\": \"input\", \"description\": \"Check email/password format\"},\n            {\"id\": \"B2\", \"name\": \"VerifyPassword\", \"type\": \"process\", \"description\": \"Compare hash\"},\n            {\"id\": \"B3\", \"name\": \"CreateSession\", \"type\": \"state\", \"description\": \"Store session\"},\n            {\"id\": \"B4\", \"name\": \"ReturnToken\", \"type\": \"output\", \"description\": \"JWT response\"}\n          ]\n        }\n      ]\n    }\n  ],\n\n  \"flows\": [\n    {\n      \"id\": \"F1\",\n      \"name\": \"Login Flow\",\n      \"is_steel_thread\": true,\n      \"steps\": [\n        {\"order\": 1, \"behavior_id\": \"B1\", \"description\": \"Validate input\"},\n        {\"order\": 2, \"behavior_id\": \"B2\", \"description\": \"Check password\"},\n        {\"order\": 3, \"behavior_id\": \"B3\", \"description\": \"Create session\"},\n        {\"order\": 4, \"behavior_id\": \"B4\", \"description\": \"Return JWT\"}\n      ]\n    }\n  ],\n\n  \"invariants\": [\n    {\"id\": \"I1\", \"description\": \"Passwords must never be logged\", \"discovery_ref\": \"Round 5, Q1\"}\n  ],\n\n  \"coverage\": {\n    \"total_requirements\": 15,\n    \"covered_requirements\": 15,\n    \"gaps\": []\n  }\n}\n```\n\n### ID Conventions\n\n- Domains: `D1`, `D2`, `D3`...\n- Capabilities: `C1`, `C2`, `C3`...\n- Behaviors: `B1`, `B2`, `B3`...\n- Flows: `F1`, `F2`, `F3`...\n- Invariants: `I1`, `I2`, `I3`...\n\n### Traceability\n\nEvery capability and invariant MUST have a `discovery_ref` pointing to the specific round and question in `$TARGET_DIR/.tasker/clarify-session.md` that established it.\n\n## Synthesis Complete: Update State\n\nAfter all synthesis outputs are complete, update state.json:\n```json\n{\n  \"phase\": {\n    \"current\": \"synthesis\",\n    \"completed\": [\"initialization\", \"scope\", \"clarify\"],\n    \"step\": \"complete\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"synthesis\": {\n    \"status\": \"complete\",\n    \"completed_at\": \"<timestamp>\",\n    \"spec_sections\": { \"workflows\": true, \"invariants\": true, \"interfaces\": true, \"open_questions\": true },\n    \"capability_map\": { \"domains_count\": N, \"capabilities_count\": N, \"behaviors_count\": N, \"steel_thread_identified\": true, \"draft_written\": true },\n    \"fsm\": { \"machines_count\": N, \"states_count\": N, \"transitions_count\": N, \"invariants_validated\": true }\n  }\n}\n```\n\n## CRITICAL: Resume From Synthesis\n\n**On resume (after compaction or restart):**\n1. Read `$TARGET_DIR/.tasker/state.json` to get `synthesis` state\n2. If `synthesis.spec_sections` shows incomplete sections, read `spec-draft.md` and continue\n3. If `synthesis.capability_map.draft_written` is false, continue building from `capability-map-draft.json`\n4. If `synthesis.fsm.invariants_validated` is false, continue FSM work from `fsm-draft/`\n\n---\n\n# Phase 4 — Architecture Sketch\n\n## Rule\nArchitecture MUST come **AFTER** workflows, invariants, interfaces.\n\n## Initialize Architecture State\n\nUpdate `$TARGET_DIR/.tasker/state.json`:\n```json\n{\n  \"phase\": {\n    \"current\": \"architecture\",\n    \"completed\": [\"initialization\", \"scope\", \"clarify\", \"synthesis\"],\n    \"step\": \"starting\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"architecture\": {\n    \"status\": \"in_progress\",\n    \"user_provided\": false,\n    \"agent_proposed\": false\n  }\n}\n```\n\n## CRITICAL: Read Prior State\n\n**On entry:**\n1. Read `$TARGET_DIR/.tasker/spec-draft.md` to understand synthesized workflows\n2. Architecture sketch should align with the workflows defined\n\n## Process\n\nUse AskUserQuestion to either:\n\n**Option A: Ask for sketch**\n```\nquestions:\n  - question: \"Can you provide a brief architecture sketch for this feature?\"\n    header: \"Architecture\"\n    options:\n      - label: \"I'll describe it\"\n        description: \"User provides architecture overview\"\n      - label: \"Propose one\"\n        description: \"Agent proposes architecture for review\"\n```\n\n**Option B: Propose and confirm**\nPresent a brief sketch and ask for confirmation/edits.\n\n## Output\n\n### 1. Update spec-draft.md\n\nAppend **Architecture sketch** section to `$TARGET_DIR/.tasker/spec-draft.md`:\n```markdown\n## Architecture sketch\n- **Components touched:** [list]\n- **Responsibilities:** [brief description]\n- **Failure handling:** [brief description]\n```\n\n**Keep this SHORT. No essays.**\n\n### 2. Update State\n\nUpdate `$TARGET_DIR/.tasker/state.json`:\n```json\n{\n  \"phase\": {\n    \"step\": \"complete\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"architecture\": {\n    \"status\": \"complete\",\n    \"completed_at\": \"<timestamp>\",\n    \"user_provided\": true|false,\n    \"agent_proposed\": true|false\n  }\n}\n```\n\n---\n\n# Phase 5 — Decisions & ADRs\n\n## Initialize Decisions State\n\nUpdate `$TARGET_DIR/.tasker/state.json`:\n```json\n{\n  \"phase\": {\n    \"current\": \"decisions\",\n    \"completed\": [\"initialization\", \"scope\", \"clarify\", \"synthesis\", \"architecture\"],\n    \"step\": \"starting\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"decisions\": {\n    \"status\": \"in_progress\",\n    \"count\": 0,\n    \"pending\": 0\n  }\n}\n```\n\n## CRITICAL: Read Prior State\n\n**On entry:**\n1. Read `$TARGET_DIR/.tasker/spec-draft.md` to identify decision points from workflows/invariants\n2. Read `$TARGET_DIR/.tasker/clarify-session.md` for context on requirements\n\n## ADR Trigger\n\n**Create ADR if ANY of these are true:**\n- Hard to reverse\n- Reusable standard\n- Tradeoff-heavy\n- Cross-cutting\n- NFR-impacting\n\n**If none apply → record decision in spec only.**\n\n## Decision Facilitation Rules\n\n### FACILITATE a decision ONLY IF ALL are true:\n1. ADR-worthy (meets trigger above)\n2. Not already decided (no existing ADR, no explicit user preference)\n3. Blocking workflows, invariants, or interfaces\n\n### DO NOT FACILITATE if ANY are true:\n- Already decided\n- Local / reversible / implementation detail\n- Non-blocking\n- User not ready to decide\n- Too many options (>3)\n- Premature (behavior not defined yet)\n\n## Facilitation Format\n\nIf facilitation is allowed:\n\n```\nquestions:\n  - question: \"How should we approach [decision topic]?\"\n    header: \"Decision\"\n    options:\n      - label: \"Option A: [Name]\"\n        description: \"Consequences: [1], [2]\"\n      - label: \"Option B: [Name]\"\n        description: \"Consequences: [1], [2]\"\n      - label: \"Need more info\"\n        description: \"Defer decision, add to Open Questions\"\n```\n\n## Outcomes\n\n- **User chooses option** → Write decision to spec-draft.md + create ADR (Accepted)\n- **User says \"need more info\"** → Add as Blocking Open Question (no ADR yet)\n\n**Update state.json after each decision:**\n```json\n{\n  \"decisions\": {\n    \"count\": 2,\n    \"pending\": 1\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n### Decision Registry\n\nWrite decision index to `$TARGET_DIR/.tasker/decisions.json`:\n```json\n{\n  \"decisions\": [\n    { \"id\": \"DEC-001\", \"title\": \"Authentication method\", \"adr\": \"ADR-0001-auth-method.md\" },\n    { \"id\": \"DEC-002\", \"title\": \"Database choice\", \"adr\": \"ADR-0002-database-choice.md\" },\n    { \"id\": \"DEC-003\", \"title\": \"Error handling strategy\", \"adr\": null }\n  ]\n}\n```\n\n- `adr` is the filename if ADR-worthy, `null` if inline decision only\n- ADR files contain full decision context and rationale\n\n**Usage pattern:** Always consult `decisions.json` first to find relevant decisions by title. Only read a specific ADR file when you need the full details. Never scan `adrs-draft/` directory to discover what decisions exist.\n\n### ADR Working Files\n\nWrite ADR drafts to `$TARGET_DIR/.tasker/adrs-draft/`:\n- `ADR-0001-auth-method.md`\n- `ADR-0002-database-choice.md`\n\nFinal ADRs are exported to `{TARGET}/docs/adrs/` in Phase 8.\n\n## ADR Template\n\nWrite ADRs to `{TARGET}/docs/adrs/ADR-####-<slug>.md`:\n\n```markdown\n# ADR-####: {Title}\n\n**Status:** Accepted\n**Date:** {YYYY-MM-DD}\n\n## Applies To\n- [Spec: Feature A](../specs/feature-a.md)\n- [Spec: Feature B](../specs/feature-b.md)\n\n## Context\n[Why this decision was needed - reference specific discovery round if applicable]\n\n## Decision\n[What was decided]\n\n## Alternatives Considered\n| Alternative | Pros | Cons | Why Not Chosen |\n|-------------|------|------|----------------|\n| [Alt 1] | ... | ... | ... |\n| [Alt 2] | ... | ... | ... |\n\n## Consequences\n- [Consequence 1]\n- [Consequence 2]\n\n## Related\n- Supersedes: (none | ADR-XXXX)\n- Related ADRs: (none | ADR-XXXX, ADR-YYYY)\n```\n\n**ADR Rules:**\n- ADRs are **immutable** once Accepted\n- Changes require a **new ADR** that supersedes the old\n- ADRs can apply to multiple specs (many-to-many relationship)\n- When creating a new spec that uses an existing ADR, update the ADR's \"Applies To\" section\n\n## Decisions Complete: Update State\n\nAfter all decisions are resolved, update state.json:\n```json\n{\n  \"phase\": {\n    \"step\": \"complete\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"decisions\": {\n    \"status\": \"complete\",\n    \"completed_at\": \"<timestamp>\",\n    \"count\": 3,\n    \"pending\": 0\n  }\n}\n```\n\nAlso append Decisions section to `$TARGET_DIR/.tasker/spec-draft.md`.\n\n---\n\n# Phase 6 — Handoff-Ready Gate\n\n## CRITICAL: State-Driven Gate Check\n\n**On entry to Phase 6:**\n1. Read `$TARGET_DIR/.tasker/state.json` to verify all prior phases complete\n2. Read `$TARGET_DIR/.tasker/spec-draft.md` to verify all sections exist\n3. Read `$TARGET_DIR/.tasker/fsm-draft/` to verify FSM compiled\n4. Read `$TARGET_DIR/.tasker/capability-map-draft.json` to verify capability map exists\n\n**DO NOT rely on conversation context. All gate checks use persisted files.**\n\nUpdate state.json:\n```json\n{\n  \"phase\": {\n    \"current\": \"gate\",\n    \"completed\": [\"initialization\", \"scope\", \"clarify\", \"synthesis\", \"architecture\", \"decisions\"],\n    \"step\": \"checking\"\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n## Preliminary Check (ALL must pass)\n\n| Check | Requirement | Verify By |\n|-------|-------------|-----------|\n| Phases complete | All phases 1-5 completed in order | `state.json` phase.completed array |\n| No blocking questions | Zero Blocking Open Questions | `spec-draft.md` Open Questions section |\n| Interfaces present | Interfaces section exists (even if \"none\") | `spec-draft.md` |\n| Decisions present | Decisions section exists | `spec-draft.md` |\n| Workflows defined | At least one workflow with variants/failures | `spec-draft.md` |\n| Invariants stated | At least one invariant | `spec-draft.md` |\n| FSM compiled | Steel Thread FSM compiled with I1-I6 passing | `fsm-draft/index.json` |\n\n## Spec Completeness Check (Checklist C1-C11)\n\nRun the checklist verification against the current spec draft:\n\n```bash\ntasker spec checklist /tmp/claude/spec-draft.md\n```\n\nThis verifies the spec contains all expected sections:\n\n| Category | Critical Items (must pass) |\n|----------|---------------------------|\n| C2: Data Model | Tables defined, fields typed, constraints stated |\n| C3: API | Endpoints listed, request/response schemas, auth requirements |\n| C4: Behavior | Observable behaviors, business rules |\n| C7: Security | Authentication mechanism, authorization rules |\n\n### Handling Checklist Gaps\n\nFor **critical missing items** (marked with *):\n\n1. If the spec SHOULD have this content → return to Phase 2 (Clarify) to gather requirements\n2. If the spec legitimately doesn't need this → document as N/A with rationale\n\nExample checklist failure:\n```\n## Gate FAILED - Incomplete Spec\n\nChecklist verification found critical gaps:\n\n- [✗] C2.4*: Constraints stated (UNIQUE, CHECK, FK)?\n  → Data model section exists but no constraints defined\n\n- [✗] C3.2*: Request schemas defined?\n  → API endpoints listed but request bodies not specified\n\nAction: Return to Phase 2 to clarify data constraints and API request formats.\n```\n\n## Gate Result: Update State\n\n### If Gate PASSES:\n```json\n{\n  \"phase\": {\n    \"current\": \"gate\",\n    \"step\": \"passed\"\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n### If Gate FAILS:\n\nUpdate state.json with blockers:\n```json\n{\n  \"phase\": {\n    \"current\": \"gate\",\n    \"step\": \"failed\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"gate_blockers\": [\n    { \"type\": \"blocking_question\", \"detail\": \"Rate limiting across tenants\" },\n    { \"type\": \"missing_section\", \"detail\": \"Interfaces\" },\n    { \"type\": \"checklist_gap\", \"detail\": \"C2.4: No database constraints\" }\n  ]\n}\n```\n\n1. List exact blockers\n2. **STOP** - do not proceed to spec review\n3. Tell user what must be resolved\n\nExample:\n```\n## Gate FAILED\n\nCannot proceed. The following must be resolved:\n\n1. **Blocking Open Questions:**\n   - How should rate limiting work across tenants?\n   - What is the retry policy for failed webhooks?\n\n2. **Missing Sections:**\n   - Interfaces section not present\n\n3. **Checklist Gaps (Critical):**\n   - C2.4: No database constraints defined\n   - C3.2: API request schemas missing\n```\n\n---\n\n# Phase 7 — Spec Review (MANDATORY)\n\n## Purpose\nRun automated weakness detection to catch issues before export. This is the final quality gate.\n\n## Initialize Review State\n\nUpdate `$TARGET_DIR/.tasker/state.json`:\n```json\n{\n  \"phase\": {\n    \"current\": \"review\",\n    \"completed\": [\"initialization\", \"scope\", \"clarify\", \"synthesis\", \"architecture\", \"decisions\", \"gate\"],\n    \"step\": \"starting\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"review\": {\n    \"status\": \"in_progress\",\n    \"weaknesses_found\": 0,\n    \"weaknesses_resolved\": 0,\n    \"critical_remaining\": 0\n  }\n}\n```\n\n## CRITICAL: Read From Files\n\n**On entry to Phase 7:**\n1. The spec draft is already in `$TARGET_DIR/.tasker/spec-draft.md` - use this file\n2. Do NOT build spec from conversation context\n\n## Process\n\n### Step 1: Copy Spec Draft for Analysis\n\nThe spec draft already exists at `$TARGET_DIR/.tasker/spec-draft.md`. Copy to temp for analysis:\n\n```bash\ncp \"$TARGET_DIR/.tasker/spec-draft.md\" /tmp/claude/spec-draft.md\n```\n\n### Step 2: Run Weakness Detection\n\n```bash\ntasker spec review /tmp/claude/spec-draft.md\n```\n\nThis detects:\n- **W1: Non-behavioral requirements** - DDL/schema not stated as behavior\n- **W2: Implicit requirements** - Constraints assumed but not explicit\n- **W3: Cross-cutting concerns** - Config, observability, lifecycle\n- **W4: Missing acceptance criteria** - Qualitative terms without metrics\n- **W5: Fragmented requirements** - Cross-references needing consolidation\n- **W6: Contradictions** - Conflicting statements\n- **W7: Ambiguity** - Vague quantifiers, undefined scope, weak requirements, passive voice\n- **W8: Missing activation requirements** - Spec describes invocation without specifying how to make it invocable (e.g., \"user runs /command\" without defining how the command becomes available)\n- **CK-*: Checklist gaps** - Critical missing content from C1-C11 categories\n\nW7 Ambiguity patterns include:\n- Vague quantifiers (\"some\", \"many\", \"several\")\n- Undefined scope (\"etc.\", \"and so on\")\n- Vague conditionals (\"if applicable\", \"when appropriate\")\n- Weak requirements (\"may\", \"might\", \"could\")\n- Passive voice hiding actor (\"is handled\", \"will be processed\")\n- Vague timing (\"quickly\", \"soon\", \"eventually\")\n- Subjective qualifiers (\"reasonable\", \"appropriate\")\n- Unquantified limits (\"large\", \"fast\", \"slow\")\n\n### Step 3: Handle Critical Weaknesses\n\nFor **CRITICAL** weaknesses (W1, W6, W7 with weak requirements), engage user:\n\n#### W1: Non-Behavioral Requirements\n\n```json\n{\n  \"question\": \"The spec contains DDL/schema that isn't stated as behavioral requirement: '{spec_quote}'. How should this be treated?\",\n  \"header\": \"DDL Mandate\",\n  \"options\": [\n    {\"label\": \"DB-level required\", \"description\": \"MUST be implemented as database-level constraint\"},\n    {\"label\": \"App-layer OK\", \"description\": \"Application-layer validation is sufficient\"},\n    {\"label\": \"Documentation only\", \"description\": \"This is reference documentation, not a requirement\"}\n  ]\n}\n```\n\n#### W6: Contradictions\n\n```json\n{\n  \"question\": \"Conflicting statements found: {description}. Which is authoritative?\",\n  \"header\": \"Conflict\",\n  \"options\": [\n    {\"label\": \"First statement\", \"description\": \"{first_quote}\"},\n    {\"label\": \"Second statement\", \"description\": \"{second_quote}\"},\n    {\"label\": \"Clarify\", \"description\": \"I'll provide clarification\"}\n  ]\n}\n```\n\n#### W7: Ambiguity\n\nEach W7 weakness includes a clarifying question. Use AskUserQuestion with the auto-generated question:\n\n```json\n{\n  \"question\": \"{weakness.question or weakness.suggested_resolution}\",\n  \"header\": \"Clarify\",\n  \"options\": [\n    {\"label\": \"Specify value\", \"description\": \"I'll provide a specific value/definition\"},\n    {\"label\": \"Not required\", \"description\": \"This is not a hard requirement\"},\n    {\"label\": \"Use default\", \"description\": \"Use a sensible default\"}\n  ]\n}\n```\n\nExample clarifying questions by ambiguity type:\n- Vague quantifier: \"How many specifically? Provide a number or range.\"\n- Weak requirement: \"Is this required or optional? If optional, under what conditions?\"\n- Vague timing: \"What is the specific timing? (e.g., <100ms, every 5 minutes)\"\n- Passive voice: \"What component/system performs this action?\"\n\n#### W8: Missing Activation Requirements\n\nDetected when the spec describes invocation (e.g., \"user runs /command\", \"user invokes the skill\") without specifying how that invocation becomes possible.\n\n```json\n{\n  \"question\": \"The spec describes '{invocation_description}' but doesn't specify how this becomes invocable. What makes this available to users?\",\n  \"header\": \"Activation\",\n  \"options\": [\n    {\"label\": \"Registration required\", \"description\": \"I'll specify what registration/installation is needed\"},\n    {\"label\": \"Built-in\", \"description\": \"This is provided by the runtime environment (document which)\"},\n    {\"label\": \"Documentation only\", \"description\": \"Activation is out of scope - add to Non-goals\"}\n  ]\n}\n```\n\nW8 patterns to detect:\n- \"User invokes X\" or \"User runs X\" without installation/registration steps\n- Entry points described without activation mechanism\n- Commands or APIs referenced without deployment/registration\n- Skills or plugins described without installation instructions\n\nIf user selects \"Registration required\", follow up:\n```json\n{\n  \"question\": \"What specific steps or files are needed to make '{invocation}' available?\",\n  \"header\": \"Activation Steps\",\n  \"options\": [\n    {\"label\": \"Config file\", \"description\": \"A configuration file registers this (specify format/location)\"},\n    {\"label\": \"CLI install\", \"description\": \"A CLI command installs this (specify command)\"},\n    {\"label\": \"Auto-discovery\", \"description\": \"The runtime auto-discovers this (specify convention)\"},\n    {\"label\": \"Manual setup\", \"description\": \"Manual steps are required (I'll document them)\"}\n  ]\n}\n```\n\n#### CK-*: Checklist Gaps\n\nFor critical checklist gaps that weren't caught in Phase 6 Gate:\n\n```json\n{\n  \"question\": \"The spec is missing {checklist_item}. Should this be added?\",\n  \"header\": \"Missing Content\",\n  \"options\": [\n    {\"label\": \"Add it\", \"description\": \"I'll provide the missing information\"},\n    {\"label\": \"N/A\", \"description\": \"This spec doesn't need this (document why)\"},\n    {\"label\": \"Defer\", \"description\": \"Address in a follow-up spec\"}\n  ]\n}\n```\n\n### Step 4: Record and Apply Resolutions\n\nFor each resolved weakness:\n\n1. **Record the resolution** for downstream consumers (logic-architect):\n```bash\ntasker spec add-resolution {weakness_id} {resolution_type} \\\n    --response \"{user_response}\" \\\n    --notes \"{context}\"\n```\n\nResolution types:\n- `mandatory` - MUST be implemented as specified (W1 DDL requirements, W8 activation requirements)\n- `clarified` - User provided specific value/definition (W7 ambiguity, W8 activation mechanism)\n- `not_applicable` - Doesn't apply to this spec (checklist gaps)\n- `defer` - Address in follow-up work\n\n2. **Update the spec content** to address the issue:\n   - If W1 resolved as \"mandatory\", add explicit behavioral statement\n   - If W6 resolved, remove contradictory statement\n   - If W7 resolved, replace ambiguous language with specific terms\n   - If W8 resolved as \"mandatory\" or \"clarified\", add Installation & Activation section to spec\n   - If CK-* resolved as \"not_applicable\", document rationale\n\n### Step 5: Re-run Until Clean\n\n```bash\n# Re-run analysis\ntasker spec review /tmp/claude/spec-draft.md\n```\n\n**Update state.json after each resolution:**\n```json\n{\n  \"review\": {\n    \"weaknesses_found\": 6,\n    \"weaknesses_resolved\": 4,\n    \"critical_remaining\": 2\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n**Continue until:**\n- Zero critical weaknesses remain, OR\n- All critical weaknesses have been explicitly accepted by user\n\n### Step 6: Save Review Results\n\nSave the final review results:\n\n```bash\ntasker spec review /tmp/claude/spec-draft.md > $TARGET_DIR/.tasker/spec-review.json\n```\n\nUpdate state.json:\n```json\n{\n  \"phase\": {\n    \"step\": \"complete\"\n  },\n  \"review\": {\n    \"status\": \"complete\",\n    \"completed_at\": \"<timestamp>\",\n    \"weaknesses_found\": 6,\n    \"weaknesses_resolved\": 6,\n    \"critical_remaining\": 0\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n## Spec Review Gate\n\n| Check | Requirement |\n|-------|-------------|\n| No critical weaknesses | All W1, W6, critical W7, CK-* resolved or accepted |\n| Resolutions recorded | `spec-resolutions.json` contains all resolution decisions |\n| Review file saved | `$TARGET_DIR/.tasker/spec-review.json` exists |\n\nCheck resolution status:\n```bash\ntasker spec unresolved\n```\n\nIf critical weaknesses remain unresolved, **STOP** and ask user to resolve.\n\n---\n\n# Phase 8 — Export\n\n## CRITICAL: Export From Working Files\n\n**On entry to Phase 8:**\nAll content comes from working files, NOT conversation context:\n- Spec content: `$TARGET_DIR/.tasker/spec-draft.md`\n- Capability map: `$TARGET_DIR/.tasker/capability-map-draft.json`\n- FSM artifacts: `$TARGET_DIR/.tasker/fsm-draft/`\n- Decision registry: `$TARGET_DIR/.tasker/decisions.json`\n- ADR drafts: `$TARGET_DIR/.tasker/adrs-draft/`\n\nUpdate state.json:\n```json\n{\n  \"phase\": {\n    \"current\": \"export\",\n    \"completed\": [\"initialization\", \"scope\", \"clarify\", \"synthesis\", \"architecture\", \"decisions\", \"gate\", \"review\"],\n    \"step\": \"starting\"\n  },\n  \"updated_at\": \"<timestamp>\"\n}\n```\n\n## Write Files\n\nOnly after spec review passes. All permanent artifacts go to the **TARGET project**.\n\n### 1. Ensure Target Directory Structure\n\n```bash\nmkdir -p {TARGET}/docs/specs {TARGET}/docs/adrs {TARGET}/docs/state-machines/<slug>\n```\n\n### 2. Spec Packet\nCopy and finalize `$TARGET_DIR/.tasker/spec-draft.md` to `{TARGET}/docs/specs/<slug>.md`:\n\n```markdown\n# Spec: {Title}\n\n## Related ADRs\n- [ADR-0001: Decision Title](../adrs/ADR-0001-decision-title.md)\n- [ADR-0002: Another Decision](../adrs/ADR-0002-another-decision.md)\n\n## Goal\n[From Phase 1]\n\n## Non-goals\n[From Phase 1]\n\n## Done means\n[From Phase 1]\n\n## Tech Stack\n[From Phase 1 - summarized]\n\n**Language & Runtime:**\n- [e.g., Python 3.12+, Node.js 20+, Go 1.22+]\n\n**Frameworks:**\n- [e.g., FastAPI, Next.js, Chi]\n\n**Data:**\n- [e.g., PostgreSQL, Redis, SQLite]\n\n**Infrastructure:**\n- [e.g., Docker, AWS Lambda, Kubernetes]\n\n**Testing:**\n- [e.g., pytest, Jest, go test]\n\n(Remove sections that don't apply)\n\n## Installation & Activation\n[If spec describes user invocation, this section is REQUIRED]\n\n**Entry Point:** [e.g., `/myskill`, `mycommand`, `POST /api/start`]\n\n**Activation Mechanism:**\n- [e.g., \"Skill registration in .claude/settings.local.json\"]\n- [e.g., \"CLI installation via pip install\"]\n- [e.g., \"API deployment to AWS Lambda\"]\n\n**Activation Steps:**\n1. [Step to make the entry point available]\n2. [Step to verify it works]\n\n**Verification Command:**\n```bash\n[Command to verify the system is properly activated and invocable]\n```\n\n(If no user invocation is described, this section can be \"N/A - library/module only\")\n\n## Workflows\n[From Phase 3]\n\n## Invariants\n[From Phase 3]\n\n## Interfaces\n[From Phase 3]\n\n## Architecture sketch\n[From Phase 4]\n\n## Decisions\nSummary of key decisions made during specification:\n\n| Decision | Rationale | ADR |\n|----------|-----------|-----|\n| [Decision 1] | [Why] | [ADR-0001](../adrs/ADR-0001-slug.md) |\n| [Decision 2] | [Why] | (inline - not ADR-worthy) |\n\n## Open Questions\n\n### Blocking\n(none - gate passed)\n\n### Non-blocking\n- [Any remaining non-blocking questions]\n\n## Agent Handoff\n- **What to build:** [Summary]\n- **Must preserve:** [Key constraints]\n- **Blocking conditions:** None\n\n## Artifacts\n- **Capability Map:** [<slug>.capabilities.json](./<slug>.capabilities.json)\n- **Behavior Model (FSM):** [state-machines/<slug>/](../state-machines/<slug>/)\n- **Discovery Log:** Archived in tasker project\n```\n\n### 3. Capability Map\nCopy and validate `$TARGET_DIR/.tasker/capability-map-draft.json` to `{TARGET}/docs/specs/<slug>.capabilities.json`.\n\nValidate against schema:\n```bash\ntasker state validate capability_map --file {TARGET}/docs/specs/<slug>.capabilities.json\n```\n\n### 4. Behavior Model (FSM)\n\nCopy FSM drafts from `$TARGET_DIR/.tasker/fsm-draft/` to `{TARGET}/docs/state-machines/<slug>/`:\n\n```bash\n# Copy FSM drafts to final location\ncp -r \"$TARGET_DIR/.tasker/fsm-draft/\"* \"{TARGET}/docs/state-machines/<slug>/\"\n\n# Generate Mermaid diagrams from canonical JSON\ntasker fsm mermaid {TARGET}/docs/state-machines/<slug>\n\n# Validate FSM artifacts (I1-I6 invariants)\ntasker fsm validate {TARGET}/docs/state-machines/<slug>\n```\n\nValidate against schemas:\n```bash\ntasker fsm validate {TARGET}/docs/state-machines/<slug>\n```\n\n### 6. ADR Files (0..N)\nCopy ADR drafts from `$TARGET_DIR/.tasker/adrs-draft/` to `{TARGET}/docs/adrs/`:\n\n```bash\ncp \"$TARGET_DIR/.tasker/adrs-draft/\"*.md \"{TARGET}/docs/adrs/\"\n```\n\n### 7. Spec Review Results\nVerify `$TARGET_DIR/.tasker/spec-review.json` is saved.\n\n### 8. Generate README.md\n\n**Purpose:** Ensure anyone encountering the project immediately understands what it is and how to use it.\n\nGenerate `{TARGET}/README.md` with:\n\n```markdown\n# {Project Title}\n\n{One-sentence description of what this system does}\n\n## What It Does\n\n{2-3 bullet points explaining the core functionality}\n\n## Installation\n\n{From Installation & Activation section of spec}\n\n## Usage\n\n{Primary entry point and basic usage example}\n\n## How It Works\n\n{Brief explanation of the architecture/workflow - 3-5 bullet points}\n\n## Project Structure\n\n```\n{key directories and files with one-line descriptions}\n```\n\n## License\n\n{License from spec or \"TBD\"}\n```\n\n**Content Sources:**\n- Title/description: From spec Goal section\n- Installation: From spec Installation & Activation section\n- Usage: From spec Entry Point and Workflows\n- How It Works: From spec Architecture Sketch\n- Project Structure: From capability map domains/files\n\n**IMPORTANT:** The README is the first thing anyone sees. It must clearly answer:\n1. What is this? (one sentence)\n2. What problem does it solve?\n3. How do I use it?\n\n## Final State Update\n\nUpdate `$TARGET_DIR/.tasker/state.json`:\n```json\n{\n  \"phase\": {\n    \"current\": \"complete\",\n    \"completed\": [\"initialization\", \"scope\", \"clarify\", \"synthesis\", \"architecture\", \"decisions\", \"gate\", \"review\", \"export\"],\n    \"step\": \"done\"\n  },\n  \"updated_at\": \"<timestamp>\",\n  \"completed_at\": \"<timestamp>\"\n}\n```\n\n## Completion Message\n\n```markdown\n## Specification Complete\n\n### What Was Designed\n\n**{Project Title}** — {One-sentence description}\n\n{2-3 sentences explaining what this system does, who it's for, and the key value it provides}\n\n**Entry Point:** {e.g., `/kx`, `mycommand`, `POST /api/start`}\n\n---\n\n**Exported to {TARGET}/:**\n- `README.md` (project overview - start here)\n\n**Exported to {TARGET}/docs/:**\n- `specs/<slug>.md` (human-readable spec)\n- `specs/<slug>.capabilities.json` (machine-readable for /plan)\n- `state-machines/<slug>/` (behavior model - state machine)\n  - `index.json`, `steel-thread.states.json`, `steel-thread.transitions.json`\n  - `steel-thread.mmd` (Mermaid diagram)\n- `adrs/ADR-####-*.md` (N ADRs)\n\n**Working files (in $TARGET_DIR/.tasker/):**\n- `clarify-session.md` (discovery log)\n- `spec-review.json` (weakness analysis)\n- `state.json` (session state)\n\n**Capabilities Extracted:**\n- Domains: N\n- Capabilities: N\n- Behaviors: N\n- Steel Thread: F1 (name)\n\n**Behavior Model (FSM) Summary:**\n- Machines: N (primary: M1 Steel Thread)\n- States: N\n- Transitions: N\n- Guards linked to invariants: N\n\n**Spec Review Summary:**\n- Total weaknesses detected: X\n- Critical resolved: Y\n- Warnings noted: Z\n\n**Next steps:**\n- Review exported spec, capability map, and FSM diagrams\n- Run `/plan {TARGET}/docs/specs/<slug>.md` to begin task decomposition\n```\n\n---\n\n# Non-Goals (Skill-Level)\n\n- No Git automation (user commits manually)\n- No project management (no Jira/Linear integration)\n- No runtime ops/runbooks\n- No over-facilitation (don't ask unnecessary questions)\n- No architectural debates before behavior is defined\n- No file/task mapping (that's `/plan`'s job)\n\n---\n\n# Commands\n\n| Command | Action |\n|---------|--------|\n| `/specify` | Start or resume specification workflow (auto-detects from state files) |\n| `/specify status` | Show current phase and progress |\n| `/specify reset` | Discard current session and start fresh |\n\n---\n\n# Context Engineering: Persistence Over Memory\n\n## Design Principle\n\nThis skill is designed to **survive context compaction**. All significant state is persisted to files, not held in conversation memory.\n\n## Working Files Summary\n\n| File | Purpose | Lifecycle |\n|------|---------|-----------|\n| `state.json` | Phase progress, granular step tracking | Updated after every significant action |\n| `spec-draft.md` | Accumulated spec sections | Appended after each phase |\n| `clarify-session.md` | Discovery Q&A log | Append-only during Phase 2 |\n| `stock-takes.md` | Vision evolution log | Append-only during Phase 2 (after each category) |\n| `capability-map-draft.json` | Capability extraction working copy | Written during Phase 3 |\n| `fsm-draft/` | FSM working files | Written during Phase 3 |\n| `decisions.json` | Decision registry (index of ADRs) | Updated during Phase 5 |\n| `adrs-draft/` | ADR working files (full decision details) | Written during Phase 5 |\n| `spec-review.json` | Weakness analysis results | Written during Phase 7 |\n\n## Resume Protocol\n\n**On any `/specify` invocation (including after compaction):**\n\nThe skill automatically detects and resumes active sessions. This is NOT optional.\n\n### Detection (Phase 0, Step 1)\n1. Check for `$TARGET_DIR/.tasker/state.json`\n2. If exists and `phase.current != \"complete\"` → **auto-resume**\n3. If not exists or `phase.current == \"complete\"` → **new session**\n\n### Resume Steps (when auto-resume triggered)\n1. **Read `state.json`** to get current phase and step\n2. **Inform user** of resume (no confirmation needed)\n3. **Read the appropriate working files** for that phase's context:\n\n| Phase | Files to Read |\n|-------|---------------|\n| scope | `state.json` only |\n| clarify | `clarify-session.md`, `stock-takes.md`, `state.json` (category status, pending followups, stock_takes_count) |\n| synthesis | `clarify-session.md`, `stock-takes.md`, `spec-draft.md`, `capability-map-draft.json`, `fsm-draft/` |\n| architecture | `spec-draft.md` |\n| decisions | `spec-draft.md`, `decisions.json` |\n| gate | `spec-draft.md`, `capability-map-draft.json`, `fsm-draft/` |\n| review | `spec-draft.md`, `spec-review.json` |\n| export | All working files |\n\n4. **Jump to the current phase** - do NOT re-run earlier phases\n5. **Resume from `phase.step`** within that phase\n\n## Anti-Patterns (AVOID)\n\n- **DO NOT** accumulate requirements in conversation context during Phase 2 - write to `clarify-session.md`\n- **DO NOT** build spec text in conversation - write sections to `spec-draft.md` immediately\n- **DO NOT** assume prior phase content is in context - always read from files\n- **DO NOT** batch state updates - update `state.json` after every significant action\n- **DO NOT** scan `adrs-draft/` to find decisions - use `decisions.json` as the index, then read specific ADR files only when full details are needed\n\n## State Update Frequency\n\nUpdate `state.json` after:\n- Completing any user question round\n- Completing any follow-up sub-loop\n- Changing categories in Phase 2\n- Completing any stock-take in Phase 2\n- Completing any spec section in Phase 3\n- Each decision outcome in Phase 5\n- Each weakness resolution in Phase 7\n- Completing any phase\n\nThis ensures the skill can resume from any point with minimal context loss.\n\n---\n\n# Integration with /plan\n\nAfter `/specify` completes, user runs:\n```\n/plan {TARGET}/docs/specs/<slug>.md\n```\n\nBecause `/specify` already produced a capability map and FSM, `/plan` can **skip** these phases:\n- Spec Review (already done)\n- Capability Extraction (already done)\n\n`/plan` starts directly at **Physical Mapping** (mapping capabilities to files).\n\nAdditionally, `/plan` will:\n- Load FSM artifacts from `{TARGET}/docs/state-machines/<slug>/`\n- Validate transition coverage (every FSM transition → ≥1 task)\n- Generate FSM-aware acceptance criteria for tasks\n\n# Integration with /execute\n\nWhen executing tasks, `/execute` will:\n- Load FSM artifacts for adherence verification\n- For each task with FSM context:\n  - Verify transitions are implemented\n  - Verify guards are enforced\n  - Verify states are reachable\n- Include FSM verification results in task completion\n",
        "templates/README.md": "# Templates\n\nThis directory contains example files for the Task Decomposition Protocol v2.\n\n## Quick Start\n\nNo setup required. Just run `/plan` and the orchestrator will ask for:\n\n1. **Your specification** - paste it or provide a file path\n2. **Target directory** - where to write the code\n3. **Tech stack** (optional) - any constraints\n\nThe spec is stored verbatim. Any format works.\n\n## Template Files\n\n### example-spec.md\n\nAn example specification showing one possible format. **You don't need to follow this format.**\nThe planner accepts any input:\n- Freeform requirements\n- PRDs or design docs\n- Bullet lists\n- Meeting notes\n\nThis file is purely for inspiration.\n\n### task.json.example\n\nExample of an individual task file. Shows the JSON schema with:\n- Task identification (id, name, phase)\n- Context (domain, capability, spec reference)\n- Behaviors to implement\n- Files to create/modify\n- Dependencies (tasks and external)\n- Acceptance criteria with verification commands\n- Time estimate\n\nTask files are created by the task-author agent during Phase 3 (Definition).\n\n### constraints.md.example\n\nExample constraints file. **Optional.** You can provide tech stack constraints conversationally when running `/plan` instead of creating this file.\n\n## Validation\n\nAll artifacts are validated against JSON schemas:\n- `schemas/capability-map.schema.json`\n- `schemas/physical-map.schema.json`\n- `schemas/task.schema.json`\n- `schemas/state.schema.json`\n\nRun validation manually:\n```bash\ntasker state validate capability_map\ntasker state validate physical_map\n```\n\n## Workflow\n\n1. Run `/plan` - provide spec, target dir, optional constraints\n2. Review generated tasks in `$TARGET_DIR/.tasker/tasks/`\n3. Run `/execute` to implement tasks via isolated subagents\n"
      },
      "plugins": [
        {
          "name": "tasker",
          "source": "./",
          "description": "Spec-Driven Development: specifications compiled into executable, verifiable behavior",
          "keywords": [
            "spec-driven-development",
            "workflow",
            "agents",
            "planning",
            "execution"
          ],
          "categories": [
            "agents",
            "execution",
            "planning",
            "spec-driven-development",
            "workflow"
          ],
          "install_commands": [
            "/plugin marketplace add Dowwie/tasker",
            "/plugin install tasker@tasker-marketplace"
          ]
        }
      ]
    }
  ]
}