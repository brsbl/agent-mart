{
  "author": {
    "id": "dnvriend",
    "display_name": "Dennis Vriend",
    "avatar_url": "https://avatars.githubusercontent.com/u/4494623?u=e818279b41fd2175b7e52829dd96677a1f7f18a7&v=4"
  },
  "marketplaces": [
    {
      "name": "kokoro-tts-tool",
      "version": null,
      "description": "Marketplace for kokoro-tts-tool CLI plugin - A CLI that provides text-to-speech using kokoro",
      "repo_full_name": "dnvriend/kokoro-tts-tool",
      "repo_url": "https://github.com/dnvriend/kokoro-tts-tool",
      "repo_description": "A CLI that provides local text-to-speech using Kokoro TTS on Apple Silicon",
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2025-12-07T07:41:11Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"kokoro-tts-tool\",\n    \"owner\": {\n        \"name\": \"Dennis Vriend\",\n        \"email\": \"dvriend@ilionx.com\",\n        \"url\": \"https://github.com/dnvriend\"\n    },\n    \"metadata\": {\n        \"description\": \"Marketplace for kokoro-tts-tool CLI plugin - A CLI that provides text-to-speech using kokoro\",\n        \"version\": \"0.1.0\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"kokoro-tts-tool\",\n            \"source\": \"./plugins/kokoro-tts-tool\",\n            \"description\": \"A CLI that provides text-to-speech using kokoro\"\n        }\n    ]\n}\n",
        "README.md": "<p align=\"center\">\n  <img src=\".github/assets/logo.png\" alt=\"kokoro-tts-tool logo\" width=\"128\">\n</p>\n\n# kokoro-tts-tool\n\n[![Python Version](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](https://github.com/python/mypy)\n[![AI Generated](https://img.shields.io/badge/AI-Generated-blueviolet.svg)](https://www.anthropic.com/claude)\n[![Built with Claude Code](https://img.shields.io/badge/Built_with-Claude_Code-5A67D8.svg)](https://www.anthropic.com/claude/code)\n\nA CLI that provides local text-to-speech using Kokoro TTS on Apple Silicon. No API keys required.\n\n## Table of Contents\n\n- [About](#about)\n- [Features](#features)\n- [Installation](#installation)\n- [Quick Start](#quick-start)\n- [Usage](#usage)\n- [Infinite Streaming](#infinite-streaming)\n- [Available Voices](#available-voices)\n- [Multi-Level Verbosity Logging](#multi-level-verbosity-logging)\n- [Shell Completion](#shell-completion)\n- [Development](#development)\n- [Testing](#testing)\n- [Security](#security)\n- [Contributing](#contributing)\n- [License](#license)\n- [Author](#author)\n\n## About\n\n`kokoro-tts-tool` is a Python CLI tool for local text-to-speech synthesis using the Kokoro-82M model. It runs entirely on your machine with no cloud dependencies, optimized for Apple Silicon Macs.\n\n**Key highlights:**\n- **Local inference**: Uses ONNX runtime for fast, CPU-optimized synthesis\n- **60+ voices**: Multiple languages and accents (English, Japanese, Mandarin, etc.)\n- **Near real-time**: Fast enough for interactive use on Apple Silicon\n- **Infinite streaming**: Continuous TTS for long documents without audio artifacts\n- **No API keys**: Everything runs locally, completely free\n\n## Features\n\n- Local TTS with Kokoro-82M (82 million parameters)\n- 60+ voices across 8 languages\n- Near real-time synthesis on Apple Silicon\n- Auto-download of model files (~350MB)\n- WAV output or direct speaker playback\n- Infinite streaming for long documents (books, articles)\n- Seamless audio without pop artifacts between chunks\n- Fast offline rendering (20-50x real-time on M4)\n- Type-safe with mypy strict mode\n- Tested with pytest\n- Multi-level verbosity logging (-v/-vv/-vvv)\n- Shell completion for bash, zsh, and fish\n- Security scanning with bandit, pip-audit, and gitleaks\n\n## Installation\n\n### Prerequisites\n\n- Python 3.14 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- Apple Silicon Mac (recommended) or any platform with Python 3.14+\n\n### Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/dnvriend/kokoro-tts-tool.git\ncd kokoro-tts-tool\n\n# Install globally with uv\nuv tool install .\n```\n\n### Install with mise (recommended for development)\n\n```bash\ncd kokoro-tts-tool\nmise trust\nmise install\nuv sync\nuv tool install .\n```\n\n### Verify installation\n\n```bash\nkokoro-tts-tool --version\n```\n\n## Quick Start\n\n```bash\n# 1. Initialize (downloads models on first run, ~350MB)\nkokoro-tts-tool init\n\n# 2. Synthesize text to speakers\nkokoro-tts-tool synthesize \"Hello world!\"\n\n# 3. Save to file\nkokoro-tts-tool synthesize \"Hello world!\" --output hello.wav\n\n# 4. Use different voice\nkokoro-tts-tool synthesize \"This is Adam.\" --voice am_adam\n\n# 5. List available voices\nkokoro-tts-tool list-voices\n```\n\n## Usage\n\n### Commands\n\n```bash\n# Show all commands\nkokoro-tts-tool --help\n\n# Download/update models\nkokoro-tts-tool init\n\n# Synthesize text\nkokoro-tts-tool synthesize \"Your text here\"\nkokoro-tts-tool synthesize \"Your text\" --output speech.wav\nkokoro-tts-tool synthesize \"Your text\" --voice bf_emma --speed 1.2\n\n# Read from stdin\necho \"Hello from stdin\" | kokoro-tts-tool synthesize --stdin\n\n# List voices\nkokoro-tts-tool list-voices\nkokoro-tts-tool list-voices --language English\nkokoro-tts-tool list-voices --gender Female\nkokoro-tts-tool list-voices --json\n\n# Show configuration\nkokoro-tts-tool info\n```\n\n### Synthesize Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--voice`, `-v` | Voice ID (e.g., af_heart, am_adam) | af_heart |\n| `--output`, `-o` | Output WAV file path | (plays to speakers) |\n| `--speed` | Speech speed (0.5 to 2.0) | 1.0 |\n| `--stdin`, `-s` | Read text from stdin | false |\n\n## Infinite Streaming\n\nStream long documents (books, articles, study materials) without audio artifacts:\n\n```bash\n# Stream a markdown file to speakers\nkokoro-tts-tool infinite --input book.md\n\n# Render to WAV file (fast offline mode, 20-50x real-time on M4)\nkokoro-tts-tool infinite --input book.md --output audiobook.wav\n\n# Pipe from stdin\ncat chapter.md | kokoro-tts-tool infinite --stdin\n\n# With custom voice and speed\nkokoro-tts-tool infinite --input notes.md --voice am_adam --speed 1.2\n```\n\n### Infinite Streaming Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--input`, `-i` | Input text/markdown file | - |\n| `--stdin`, `-s` | Read text from stdin | false |\n| `--output`, `-o` | Save to WAV file (fast offline mode) | (plays to speakers) |\n| `--voice` | Voice ID | af_heart |\n| `--speed` | Speech speed (0.5 to 2.0) | 1.0 |\n| `--chunk-size` | Target words per chunk (50-1000) | 200 |\n| `--pause` | Pause between chunks in ms (0-2000) | 150 |\n| `--no-markdown` | Treat input as plain text | false |\n\n## Available Voices\n\nThe tool includes 60+ voices across 8 languages:\n\n### American English (20 voices)\n| Voice ID | Gender | Grade | Description |\n|----------|--------|-------|-------------|\n| `af_heart` | Female | A | Default, emotional, soft (highest quality) |\n| `af_bella` | Female | A- | Expressive, dynamic range |\n| `am_adam` | Male | A- | Deep narrator (audiobooks) |\n| `am_michael` | Male | B+ | Natural, casual |\n\n### British English (8 voices)\n| Voice ID | Gender | Grade | Description |\n|----------|--------|-------|-------------|\n| `bf_emma` | Female | B+ | Polished, formal (education) |\n| `bm_george` | Male | B+ | Resonant, classic (history) |\n\n### Other Languages\n- **Japanese**: jf_alpha, jm_kumo, and more\n- **Mandarin**: zf_xiaobei, zm_yunjian, and more\n- **Spanish**: ef_dora, em_alex\n- **French**: ff_siwis\n- **Hindi**: hf_alpha, hm_omega\n- **Italian**: if_sara, im_nicola\n- **Portuguese (Brazilian)**: pf_dora, pm_alex\n\nRun `kokoro-tts-tool list-voices` for the complete list.\n\n### Voice Quality Grades\n- **A/A-**: Highest quality, recommended for production\n- **B+/B**: Good quality\n- **B-**: Acceptable quality\n\n## Multi-Level Verbosity Logging\n\nThe CLI supports progressive verbosity levels for debugging:\n\n| Flag | Level | Output | Use Case |\n|------|-------|--------|----------|\n| (none) | WARNING | Errors and warnings only | Production |\n| `-v` | INFO | + High-level operations | Normal debugging |\n| `-vv` | DEBUG | + Detailed info | Development |\n| `-vvv` | TRACE | + Library internals | Deep debugging |\n\n```bash\n# Quiet mode\nkokoro-tts-tool synthesize \"Hello\"\n\n# With debug output\nkokoro-tts-tool -vv synthesize \"Hello\"\n```\n\n## Shell Completion\n\nThe CLI provides native shell completion for bash, zsh, and fish:\n\n```bash\n# Bash - add to ~/.bashrc\necho 'eval \"$(kokoro-tts-tool completion bash)\"' >> ~/.bashrc\n\n# Zsh - add to ~/.zshrc\necho 'eval \"$(kokoro-tts-tool completion zsh)\"' >> ~/.zshrc\n\n# Fish - save to completions\nmkdir -p ~/.config/fish/completions\nkokoro-tts-tool completion fish > ~/.config/fish/completions/kokoro-tts-tool.fish\n```\n\n## Development\n\n### Setup Development Environment\n\n```bash\ngit clone https://github.com/dnvriend/kokoro-tts-tool.git\ncd kokoro-tts-tool\nmake install\nmake help\n```\n\n### Available Make Commands\n\n```bash\nmake install         # Install dependencies\nmake format          # Format code\nmake lint            # Run linting\nmake typecheck       # Type checking\nmake test            # Run tests\nmake security        # Security scans\nmake check           # All checks\nmake pipeline        # Full pipeline\n```\n\n### Project Structure\n\n```\nkokoro-tts-tool/\n‚îú‚îÄ‚îÄ kokoro_tts_tool/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ cli.py              # CLI entry point\n‚îÇ   ‚îú‚îÄ‚îÄ engine.py           # TTS engine wrapper\n‚îÇ   ‚îú‚îÄ‚îÄ models.py           # Model management\n‚îÇ   ‚îú‚îÄ‚îÄ voices.py           # Voice definitions\n‚îÇ   ‚îú‚îÄ‚îÄ splitter.py         # Text chunking for long documents\n‚îÇ   ‚îú‚îÄ‚îÄ streaming.py        # Audio streaming for speaker playback\n‚îÇ   ‚îú‚îÄ‚îÄ utils.py            # Utilities\n‚îÇ   ‚îú‚îÄ‚îÄ logging_config.py   # Logging setup\n‚îÇ   ‚îú‚îÄ‚îÄ completion.py       # Shell completion\n‚îÇ   ‚îî‚îÄ‚îÄ commands/           # CLI commands\n‚îÇ       ‚îú‚îÄ‚îÄ synthesize_commands.py\n‚îÇ       ‚îú‚îÄ‚îÄ voice_commands.py\n‚îÇ       ‚îú‚îÄ‚îÄ init_commands.py\n‚îÇ       ‚îú‚îÄ‚îÄ info_commands.py\n‚îÇ       ‚îî‚îÄ‚îÄ infinite_commands.py\n‚îú‚îÄ‚îÄ tests/\n‚îú‚îÄ‚îÄ references/             # Research documentation\n‚îú‚îÄ‚îÄ plugins/                # Claude Code plugin\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ Makefile\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ CLAUDE.md\n```\n\n## Testing\n\n```bash\n# Run all tests\nmake test\n\n# Run tests with verbose output\nuv run pytest tests/ -v\n```\n\n## Security\n\nThe project includes security scanning:\n\n```bash\n# Run all security checks\nmake security\n\n# Individual scans\nmake security-bandit       # Python security linting\nmake security-pip-audit    # Dependency CVE scanning\nmake security-gitleaks     # Secret detection\n```\n\n### Prerequisites\n\n```bash\n# Install gitleaks (macOS)\nbrew install gitleaks\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Run `make pipeline`\n5. Submit a Pull Request\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.\n\n## Author\n\n**Dennis Vriend** - [@dnvriend](https://github.com/dnvriend)\n\n## Acknowledgments\n\n- [Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M) - The TTS model\n- [kokoro-onnx](https://github.com/thewh1teagle/kokoro-onnx) - ONNX implementation\n- [Click](https://click.palletsprojects.com/) - CLI framework\n- [uv](https://github.com/astral-sh/uv) - Fast Python tooling\n\n---\n\n**Generated with AI**\n\nThis project was generated using [Claude Code](https://www.anthropic.com/claude/code).\n\nMade with Python 3.14\n"
      },
      "plugins": [
        {
          "name": "kokoro-tts-tool",
          "source": "./plugins/kokoro-tts-tool",
          "description": "A CLI that provides text-to-speech using kokoro",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add dnvriend/kokoro-tts-tool",
            "/plugin install kokoro-tts-tool@kokoro-tts-tool"
          ]
        }
      ]
    },
    {
      "name": "gemini-file-search-tool",
      "version": null,
      "description": "Manage Gemini File Search stores and query with RAG",
      "repo_full_name": "dnvriend/gemini-file-search-tool",
      "repo_url": "https://github.com/dnvriend/gemini-file-search-tool",
      "repo_description": "Production-ready CLI and Python library for Google's Gemini File Search API - A fully managed RAG system for document search and question-answering",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-12-31T23:41:21Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"gemini-file-search-tool\",\n    \"owner\": {\n        \"name\": \"Dennis Vriend\",\n        \"email\": \"dvriend@ilionx.com\",\n        \"url\": \"https://github.com/dnvriend/gemini-file-search-tool\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"gemini-file-search-tool\",\n            \"source\": \"./plugins/gemini-file-search-tool\",\n            \"description\": \"Manage Gemini File Search stores and query with RAG\"\n        }\n    ]\n}\n",
        "README.md": "# gemini-file-search-tool\n\n<div align=\"center\">\n  <img src=\".github/assets/rag-icon.png\" alt=\"RAG Icon\" width=\"200\" />\n  <br>\n  <br>\n\n[![Python Version](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](https://github.com/python/mypy)\n[![AI Generated](https://img.shields.io/badge/AI-Generated-blueviolet.svg)](https://www.anthropic.com/claude)\n[![Built with Claude Code](https://img.shields.io/badge/Built_with-Claude_Code-5A67D8.svg)](https://www.anthropic.com/claude/code)\n\n**Gemini File Search Tool** - Production-ready CLI & Python library for Google's fully managed RAG system\n\n</div>\n\n## Table of Contents\n\n- [About](#about)\n- [Use Cases](#use-cases)\n- [Features](#features)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Usage](#usage)\n- [Development](#development)\n- [Testing](#testing)\n- [Known Issues](#known-issues)\n- [Contributing](#contributing)\n- [License](#license)\n- [Author](#author)\n- [Resources](#resources)\n\n## About\n\n`gemini-file-search-tool` is a production-ready CLI and Python library for [Google's Gemini File Search API](https://ai.google.dev/gemini-api/docs/file-search), a fully managed Retrieval-Augmented Generation (RAG) system that eliminates the operational complexity of vector databases, embeddings, and retrieval infrastructure.\n\n### What is Gemini File Search?\n\nGemini File Search is Google's fully managed RAG solution that automatically handles document ingestion, chunking, embedding generation, and semantic retrieval. As announced in [Google's developer blog](https://blog.google/technology/developers/file-search-gemini-api/), it provides enterprise-grade document search capabilities with zero infrastructure overhead, allowing developers to focus on building intelligent applications rather than managing search infrastructure.\n\n### Why This Tool?\n\nThis tool provides a **CLI-first interface** to the Gemini File Search API, designed specifically for integration with AI agents, automation workflows, and human operators:\n\n- **Agent-Friendly Design**: CLIs provide structured commands with built-in documentation and rich error messages that AI agents can parse and act upon in ReAct (Reasoning and Acting) loops, making them superior to standalone scripts for agentic workflows\n- **Composable Architecture**: JSON output to stdout and logs to stderr enable seamless piping and integration with other tools, perfect for complex automation pipelines\n- **Reusable Building Blocks**: Commands can be composed into larger workflows, used in skills for Claude Code, or integrated into custom automation without modification\n- **Dual-Mode Operation**: Functions as both a CLI tool for command-line operations and a Python library for programmatic integration\n- **Production Quality**: Type-safe, thoroughly tested, with comprehensive error handling that provides actionable feedback for both humans and agents\n\nWhether you're building AI-powered document search, integrating RAG into agentic workflows, or automating knowledge base operations, this tool provides the foundation for reliable, maintainable solutions.\n\n## Use Cases\n\n- **üìö Knowledge Base Management**: Index documentation, research papers, wikis, and technical specifications into searchable stores for instant retrieval\n- **üíª Code-RAG (Retrieval-Augmented Generation for Code)**: Upload entire codebases to enable semantic code search and natural language querying. Ask questions like \"how does authentication work?\", \"where is error handling implemented?\", or \"explain the database architecture\". Perfect for onboarding, code reviews, architecture discovery, and building AI coding assistants.\n- **üîç Semantic Search**: Query your document stores with natural language questions and receive contextually relevant answers with automatic citation\n- **üéØ RAG Applications**: Build production-ready retrieval-augmented generation systems with JSON-formatted responses including grounding metadata and source attribution\n\n### Code-RAG Example\n\nUpload a codebase and query it with natural language:\n\n```bash\n# Upload your entire codebase\ngemini-file-search-tool upload \"src/**/*.py\" --store \"my-project-code\" -v\n\n# Query with natural language\ngemini-file-search-tool query \"How does the authentication system work?\" \\\n  --store \"my-project-code\" --show-cost -v\n\n# Ask architectural questions\ngemini-file-search-tool query \"What design patterns are used in this codebase?\" \\\n  --store \"my-project-code\" --query-model pro\n\n# Find implementations\ngemini-file-search-tool query \"Where is error handling for API calls implemented?\" \\\n  --store \"my-project-code\"\n```\n\n**Meta Note**: This tool itself was built using Code-RAG! We uploaded the codebase to a Gemini File Search store and used it to answer questions during development. The tool enables the very functionality it provides.\n\n## Features\n\n- ‚úÖ **Fully Managed RAG**: Automatic chunking, embeddings, and retrieval without infrastructure management\n- ‚úÖ **Multi-Format Support**: PDF, DOCX, TXT, JSON, CSV, HTML, and source code files\n- ‚úÖ **Code-RAG Enabled**: Upload codebases and query with natural language for semantic code search\n- ‚úÖ **Intelligent Caching**: Local mtime-based cache (O(1) performance) prevents unnecessary re-uploads\n- ‚úÖ **Async Uploads**: `--no-wait` flag for fire-and-forget uploads with manual sync capability\n- ‚úÖ **Cache Management**: sync-cache, flush-cache, and cache-report commands for operation tracking\n- ‚úÖ **Query Enhancement**: LLM-powered query optimization for better RAG retrieval (generic, code-rag, obsidian modes)\n- ‚úÖ **Natural Language Queries**: Ask questions in plain language and get contextual answers\n- ‚úÖ **Automatic Citations**: Built-in source attribution and grounding metadata\n- ‚úÖ **Multi-Level Verbosity**: Progressive logging detail with `-v` (INFO), `-vv` (DEBUG), `-vvv` (TRACE)\n- ‚úÖ **Cost Tracking**: Token usage monitoring and cost estimation for both enhancement and query operations\n- ‚úÖ **Composable CLI**: JSON output for easy integration with other tools and scripts\n- ‚úÖ **Python Library**: Import and use programmatically in your applications\n- ‚úÖ **Type-Safe**: Strict mypy type checking and modern Python 3.14+ syntax\n- ‚úÖ **Production-Ready**: Comprehensive testing, linting, and quality checks\n\n## Installation\n\n### Prerequisites\n\n- Python 3.14 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- Google Gemini API access (see [Configuration](#configuration))\n\n### Core Dependencies\n\nThis tool uses the [Google Generative AI Python SDK](https://github.com/googleapis/python-genai) (`google-genai`) for interacting with Google's Gemini API.\n\n### Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/dnvriend/gemini-file-search-tool.git\ncd gemini-file-search-tool\n\n# Install globally with uv\nuv tool install .\n```\n\n### Install with mise (recommended for development)\n\n```bash\ncd gemini-file-search-tool\nmise trust\nmise install\nuv sync\nuv tool install .\n```\n\n### Verify installation\n\n```bash\ngemini-file-search-tool --version\n```\n\n## Configuration\n\n### Environment Variables\n\nThe CLI automatically detects and supports both authentication methods. Configuration depends on whether you're using the Gemini Developer API or the Gemini API in Vertex AI.\n\n#### Gemini Developer API (Recommended)\n\nSet `GEMINI_API_KEY` or `GOOGLE_API_KEY`. The client will automatically pick up these variables. If both are set, `GOOGLE_API_KEY` takes precedence.\n\n```bash\nexport GEMINI_API_KEY='your-api-key'\n```\n\n**Get your API key:**\n1. Visit [Google AI Studio](https://aistudio.google.com/app/apikey)\n2. Create or select a project\n3. Generate an API key\n4. Set the environment variable\n\n**Example:**\n```bash\nexport GEMINI_API_KEY='AIza...'\ngemini-file-search-tool list-stores\n```\n\n#### Gemini API on Vertex AI\n\nFor Vertex AI, set the following environment variables:\n\n```bash\nexport GOOGLE_GENAI_USE_VERTEXAI=true\nexport GOOGLE_CLOUD_PROJECT='your-project-id'\nexport GOOGLE_CLOUD_LOCATION='us-central1'\n```\n\n**Prerequisites:**\n- Google Cloud project with Vertex AI API enabled\n- Proper IAM permissions for Vertex AI\n- Authenticated with `gcloud auth application-default login`\n\n**Example:**\n```bash\nexport GOOGLE_GENAI_USE_VERTEXAI=true\nexport GOOGLE_CLOUD_PROJECT='my-project'\nexport GOOGLE_CLOUD_LOCATION='us-central1'\n\n# Authenticate with Google Cloud\ngcloud auth application-default login\n\n# Use the CLI (automatically uses Vertex AI)\ngemini-file-search-tool list-stores\n```\n\n**Note:** The CLI automatically detects which authentication method to use based on the `GOOGLE_GENAI_USE_VERTEXAI` environment variable.\n\nFor more information, see the [Google Generative AI Python SDK documentation](https://github.com/googleapis/python-genai).\n\n## Usage\n\n### Basic Usage\n\n```bash\n# Show help\ngemini-file-search-tool --help\n\n# Show version\ngemini-file-search-tool --version\n\n# Get help for specific commands\ngemini-file-search-tool create-store --help\ngemini-file-search-tool upload --help\n```\n\n### Store Management\n\n```bash\n# Create a new store\ngemini-file-search-tool create-store \"my-documents\"\n\n# List all stores\ngemini-file-search-tool list-stores\n\n# Get store details\ngemini-file-search-tool get-store \"my-documents\"\n\n# Update store display name\ngemini-file-search-tool update-store \"my-documents\" --display-name \"My Documents 2024\"\n\n# Delete a store\ngemini-file-search-tool delete-store \"my-documents\" --force\n```\n\n### Document Management\n\n```bash\n# Upload a single file\ngemini-file-search-tool upload document.pdf --store \"my-documents\"\n\n# Upload multiple files with glob pattern\ngemini-file-search-tool upload \"*.pdf\" --store \"my-documents\"\n\n# Recursive upload\ngemini-file-search-tool upload \"docs/**/*.md\" --store \"my-documents\"\n\n# Upload with metadata\ngemini-file-search-tool upload document.pdf --store \"my-documents\" \\\n  --title \"Important Document\" \\\n  --url \"https://example.com/doc\"\n\n# Upload with custom chunking\ngemini-file-search-tool upload document.pdf --store \"my-documents\" \\\n  --max-tokens 300 \\\n  --max-overlap 25\n\n# Upload respecting .gitignore (default behavior)\ngemini-file-search-tool upload \"**/*.py\" --store \"my-codebase\" -v\n\n# Upload ignoring .gitignore patterns\ngemini-file-search-tool upload \"**/*.py\" --store \"my-codebase\" --ignore-gitignore\n\n# Dry-run to preview files before uploading\ngemini-file-search-tool upload \"**/*.py\" --store \"my-codebase\" --dry-run -v\n\n# Upload with verbose logging (see what's happening)\ngemini-file-search-tool upload \"*.pdf\" --store \"my-documents\" -v\n\n# Upload with debug logging (see detailed operations)\ngemini-file-search-tool upload \"*.pdf\" --store \"my-documents\" -vv\n\n# Upload with trace logging (see full API calls)\ngemini-file-search-tool upload \"*.pdf\" --store \"my-documents\" -vvv\n\n# List documents in a store\ngemini-file-search-tool list-documents --store \"my-documents\"\n\n# List documents with verbose output\ngemini-file-search-tool list-documents --store \"my-documents\" -v\n```\n\n### Querying\n\n**‚ö° Recommended Configuration (Based on Benchmarks)**\n\nFor optimal cost-quality balance, use the default configuration with no enhancement:\n\n```bash\n# Recommended: No enhancement + Flash model (best value)\ngemini-file-search-tool query \"What are the key findings?\" \\\n  --store \"my-documents\"\n\n# With cost tracking and grounding metadata\ngemini-file-search-tool query \"Explain the methodology\" \\\n  --store \"my-documents\" --show-cost --query-grounding-metadata -v\n```\n\n**Performance Benchmarks** (30 queries across 6 configurations):\n- **Quality Score**: 97.7/100\n- **Cost**: ~$0.00013 per query\n- **Success Rate**: 100% (vs 20-80% with enhancement)\n- **Value**: 156.1 quality points per $0.001 (6.6x better than enhancement)\n\nSee `references/benchmark-model-comparison-2025-11-16.md` for detailed analysis.\n\n**Other Query Options:**\n\n```bash\n# Query with Pro model (complex analytical questions)\ngemini-file-search-tool query \"Analyze the technical architecture\" \\\n  --store \"my-documents\" --query-model pro\n\n# Query with metadata filter\ngemini-file-search-tool query \"Tell me about the book\" \\\n  --store \"my-documents\" --metadata-filter \"author=Robert Graves\"\n\n# Query with enhancement (only for vague/exploratory queries)\ngemini-file-search-tool query \"authentication stuff\" \\\n  --store \"my-documents\" --enhance-mode code-rag --show-enhancement\n\n# Query with debug logging (see API operations)\ngemini-file-search-tool query \"What are the findings?\" \\\n  --store \"my-documents\" --show-cost -vv\n\n# Query with trace logging (see full HTTP requests)\ngemini-file-search-tool query \"Analyze this\" \\\n  --store \"my-documents\" --show-cost -vvv\n```\n\n**‚ö†Ô∏è Query Enhancement Notes:**\n\nBased on comprehensive benchmarks, **query enhancement is disabled by default** and should only be used for:\n- Vague or poorly worded queries (\"authentication stuff\", \"that database thing\")\n- Exploratory queries where you're unsure what you're looking for\n- Cases where simple queries consistently fail to retrieve relevant documents\n\n**Why?** Enhancement makes queries too specific, reducing retrieval success rates from 100% to 20-80%. Simple, natural language queries work best for RAG systems (see [Lewis et al., 2020](https://arxiv.org/abs/2005.11401)).\n\n### Cost Tracking\n\nThe CLI automatically tracks token usage for query operations and can estimate costs based on current Gemini API pricing:\n\n```bash\n# View token usage (verbose mode)\ngemini-file-search-tool query \"What is this about?\" \\\n  --store \"my-documents\" -v\n\n# Output:\n# [INFO] Token usage: 150 prompt + 320 candidates = 470 total\n\n# Show estimated cost\ngemini-file-search-tool query \"What is this about?\" \\\n  --store \"my-documents\" --show-cost -v\n\n# Output:\n# [INFO] Token usage: 150 prompt + 320 candidates = 470 total\n# [INFO] Estimated cost: $0.00010725 USD\n```\n\n**Query Response with Cost Information:**\n\n```json\n{\n  \"response_text\": \"The document discusses...\",\n  \"usage_metadata\": {\n    \"prompt_token_count\": 150,\n    \"candidates_token_count\": 320,\n    \"total_token_count\": 470\n  },\n  \"estimated_cost\": {\n    \"input_cost_usd\": 0.00001125,\n    \"output_cost_usd\": 0.000096,\n    \"total_cost_usd\": 0.00010725,\n    \"currency\": \"USD\",\n    \"model\": \"gemini-2.5-flash\",\n    \"note\": \"Estimated cost based on current pricing. Subject to change.\"\n  }\n}\n```\n\n**Current Pricing (as of 2025-01):**\n- **gemini-2.5-flash**: $0.075 input / $0.30 output per 1M tokens\n- **gemini-2.5-pro**: $1.25 input / $5.00 output per 1M tokens\n\n**Note**: Pricing is subject to change. Verify current rates at [Google AI Pricing](https://ai.google.dev/pricing).\n\n**Limitations:**\n- Token usage is only available for query operations\n- Upload costs (document embedding) are not tracked by the API\n- Cost estimates are calculated locally using published pricing\n\n### Upload Features\n\n**Intelligent Caching**:\n- **Local Cache**: Automatically tracks uploaded files at `~/.config/gemini-file-search-tool/stores/`\n- **Per-Store Isolation**: Each store maintains its own cache file (e.g., `fileSearchStores__my-store.json`)\n- **mtime Optimization**: O(n) ‚Üí O(1) performance - checks file modification time before computing expensive SHA256 hash\n- **Smart Re-uploads**: Only uploads files that have actually changed (skips identical files automatically)\n- **Cache Structure**: Stores hash, mtime, remote_id, and last_uploaded timestamp for each file\n- **Performance Impact**:\n  - **Large codebases (1000 files)**: Cache check ~0.1 seconds vs ~5-10 seconds without cache\n  - **Network I/O remains the bottleneck**: Upload time (5-10s per file) dominates hash calculation (500ms)\n  - See `docs/cache-design.md` for detailed performance analysis and architecture\n\n**Automatic File Validation**:\n- **Empty Files**: 0-byte files automatically skipped with warning\n- **File Size**: 50MB limit enforced\n- **Base64 Images**: Detects base64-encoded images in text files (can cause upload failures)\n- **System Files**: Auto-skips `__pycache__`, `.pyc`, `.DS_Store`, etc.\n- **Gitignore Support**: Automatically respects `.gitignore` patterns (use `--ignore-gitignore` to disable)\n- **MIME Types**: Automatic registration for `.toml`, `.env`, `.txt`, `.md` files\n\n**Duplicate Detection**:\n- Automatically detects existing files by name and size\n- Skips unchanged files (no re-upload)\n- Updates files when size changes (deletes old, uploads new)\n\n**Preview & Control**:\n- **Dry-Run**: Use `--dry-run` to preview which files would be uploaded without actually uploading\n- **Skip Validation**: Use `--skip-validation` to bypass checks for faster uploads\n- **Ignore Gitignore**: Use `--ignore-gitignore` to upload files normally excluded by .gitignore\n\n### Dry-Run Mode\n\nPreview which files would be uploaded without actually uploading:\n\n```bash\n# Preview files with sizes\ngemini-file-search-tool upload \"**/*.py\" --store \"code\" --dry-run -v\n\n# Output (to stderr):\n# [INFO] Loaded 38 patterns from .gitignore\n# [INFO] DRY-RUN: Would upload 13 file(s)\n\n# Output (to stdout - JSON):\n[\n  {\n    \"file\": \"/path/to/file1.py\",\n    \"size_bytes\": 1809,\n    \"size_mb\": 0.0\n  },\n  {\n    \"file\": \"/path/to/file2.py\",\n    \"size_bytes\": 2691,\n    \"size_mb\": 0.0\n  }\n]\n```\n\n**Benefits**:\n- **Preview Files**: See exactly which files match your glob pattern\n- **Verify Gitignore**: Confirm .gitignore filtering is working correctly\n- **Check Sizes**: Review file sizes before uploading\n- **No API Calls**: Completely safe, no interaction with Gemini API\n\n### Verbosity Levels\n\nAll commands support multi-level verbosity for controlling log output detail:\n\n| Flag | Level | Description | Use Case |\n|------|-------|-------------|----------|\n| (none) | WARNING | Only critical errors | Production, clean output |\n| `-v` | INFO | High-level operations | See progress, identify failures |\n| `-vv` | DEBUG | Detailed operations | Debug issues, see API calls |\n| `-vvv` | TRACE | Full API details | Deep debugging, HTTP traces |\n\n**Examples:**\n\n```bash\n# INFO level: See which files are uploaded/failed\ngemini-file-search-tool upload \"*.pdf\" --store \"docs\" -v\n\n# DEBUG level: See validation, polling, API operations\ngemini-file-search-tool upload \"*.pdf\" --store \"docs\" -vv\n\n# TRACE level: See full HTTP requests/responses from SDK\ngemini-file-search-tool upload \"*.pdf\" --store \"docs\" -vvv\n```\n\n**Output:**\n\n```bash\n# With -v (INFO)\n[INFO] Starting upload operation\n[INFO] Uploading: document.pdf (2.50MB) to store 'fileSearchStores/docs-123'\n[INFO] Upload completed successfully: document.pdf\n\n# With -vv (DEBUG)\n[INFO] Starting upload operation\n[INFO] Uploading: document.pdf (2.50MB) to store 'fileSearchStores/docs-123'\n[DEBUG] Validating file: document.pdf\n[DEBUG] File validation passed: document.pdf\n[DEBUG] Starting upload operation for: document.pdf\n[DEBUG] Operation started: operations/upload-abc123\n[DEBUG] Polling operation operations/upload-abc123 (attempt 1) - waiting 2.0s\n[DEBUG] Polling operation operations/upload-abc123 (attempt 2) - waiting 3.0s\n[INFO] Upload completed successfully: document.pdf\n\n# With -vvv (TRACE) - includes all above plus:\n[DEBUG] (httpx) HTTP Request: POST https://...\n[DEBUG] (httpx) HTTP Response: 200 OK\n```\n\n**Benefits:**\n- **Real-time Feedback**: See progress and failures as they happen (not just in final JSON)\n- **Progressive Detail**: Choose the right level of verbosity for your needs\n- **Clean stdout**: All logs go to stderr, keeping JSON output clean for piping\n- **Library Logging**: At `-vvv`, see internals from `httpx`, `google-api-core`, etc.\n\n### Cache Management\n\nThe CLI provides commands to manage the local cache for asynchronous upload operations and cache maintenance.\n\n#### Asynchronous Uploads\n\nUse `--no-wait` to skip operation polling and return immediately after initiating uploads:\n\n```bash\n# Fast async uploads (don't wait for completion)\ngemini-file-search-tool upload \"*.pdf\" --store \"docs\" --no-wait -v\n\n# Output: Returns immediately with \"pending\" status\n[\n  {\"file\": \"doc1.pdf\", \"status\": \"pending\", \"operation\": \"operations/...\"},\n  {\"file\": \"doc2.pdf\", \"status\": \"pending\", \"operation\": \"operations/...\"}\n]\n```\n\n**Benefits**:\n- **Faster Returns**: No polling overhead (~2-10s per file saved)\n- **Bulk Operations**: Initiate thousands of uploads quickly\n- **Fire-and-Forget**: Useful for known-working file types where immediate feedback isn't needed\n- **Last-One-Wins**: Re-uploading a file automatically overwrites previous pending operations\n\n**Trade-offs**:\n- No immediate status (success/failure unknown until synced)\n- Requires manual `sync-cache` to check final status\n\n#### Sync Cache\n\nCheck status of pending operations and update cache with final results:\n\n```bash\n# Sync all pending operations for a store\ngemini-file-search-tool sync-cache --store \"docs\" -v\n\n# With custom number of workers (default: 4)\ngemini-file-search-tool sync-cache --store \"docs\" --num-workers 8 -v\n\n# Output (JSON - default):\n{\n  \"status\": \"success\",\n  \"total\": 10,\n  \"synced\": 8,\n  \"failed\": 1,\n  \"still_pending\": 1,\n  \"operations\": [\n    {\"file\": \"doc1.pdf\", \"status\": \"synced\", \"remote_id\": \"documents/...\"},\n    {\"file\": \"doc2.pdf\", \"status\": \"failed\", \"error\": {\"message\": \"...\"}}\n  ]\n}\n\n# Human-readable text output\ngemini-file-search-tool sync-cache --store \"docs\" --text\n\n# Output (text):\nSync Summary:\n  Total operations: 10\n  Synced: 8\n  Failed: 1\n  Still pending: 1\n```\n\n**Features**:\n- **Parallel Processing**: Fetches operation status concurrently with configurable workers (default: 4)\n- **Batch Cache Writes**: Collects all updates and writes cache once at the end (not per-operation)\n- **Progress Bar**: Visual feedback with tqdm during sync\n- **Error Details**: Captures and stores error messages from failed operations\n- **Automatic Updates**: Updates cache with remote_id when operations complete successfully\n- **Idempotent**: Safe to run multiple times (only updates changed operations)\n\n#### Cache Report\n\nGenerate reports on cache status with filtering options:\n\n```bash\n# Default report (summary + pending operations)\ngemini-file-search-tool cache-report --store \"docs\"\n\n# Show only failed operations\ngemini-file-search-tool cache-report --store \"docs\" --errors-only\n\n# Show only completed uploads\ngemini-file-search-tool cache-report --store \"docs\" --completed-only\n\n# Show all cached files\ngemini-file-search-tool cache-report --store \"docs\" --all\n\n# Human-readable text output\ngemini-file-search-tool cache-report --store \"docs\" --text\n```\n\n**Output Example (JSON)**:\n```json\n{\n  \"store\": \"docs\",\n  \"stats\": {\n    \"total_files\": 100,\n    \"completed\": 95,\n    \"pending_operations\": 3,\n    \"failed_operations\": 2\n  },\n  \"files\": [\n    {\n      \"file\": \"/path/to/doc.pdf\",\n      \"status\": \"pending\",\n      \"operation\": \"operations/...\",\n      \"hash\": \"abc123...\",\n      \"mtime\": 1731969000.0\n    }\n  ]\n}\n```\n\n**Filters**:\n- `--pending-only`: Show only files with pending operations\n- `--errors-only`: Show only files with errors\n- `--completed-only`: Show only successfully uploaded files\n- `--all`: Show all cached files (overrides other filters)\n- `--text`: Human-readable text format instead of JSON\n\n#### Flush Cache\n\nDelete cache file for a specific store:\n\n```bash\n# Flush with confirmation prompt\ngemini-file-search-tool flush-cache --store \"docs\"\n\n# Output:\nCache statistics for 'docs':\n  Total files: 100\n  Completed: 95\n  Pending operations: 3\n  Failed operations: 2\n\nAre you sure you want to delete this cache? [y/N]:\n\n# Force flush without confirmation\ngemini-file-search-tool flush-cache --store \"docs\" --force\n```\n\n**Use Cases**:\n- **Clean Slate**: Start fresh after major changes\n- **Rebuild Cache**: Use with `upload --rebuild-cache` to re-upload everything\n- **Troubleshooting**: Clear corrupted cache data\n\n#### Cache with Store Deletion\n\nWhen deleting a store, cache statistics are shown and the cache is automatically removed:\n\n```bash\n# Delete store with cache cleanup\ngemini-file-search-tool delete-store \"docs\" --force\n\n# Output shows cache stats before deletion:\n[INFO] Cache found for store 'docs':\n[INFO]   Total files: 100\n[INFO]   Completed: 95\n[INFO]   Pending operations: 3\n[INFO]   Failed operations: 2\n[INFO] Deleting store: fileSearchStores/docs-123\n[INFO] Removing cache file...\n[INFO] Cache removed successfully\n```\n\n#### Typical Workflows\n\n**Async Upload + Sync Pattern:**\n```bash\n# 1. Fast upload without waiting\ngemini-file-search-tool upload \"docs/**/*.pdf\" --store \"my-docs\" --no-wait -v\n\n# 2. Continue working on other tasks...\n\n# 3. Later, check status\ngemini-file-search-tool sync-cache --store \"my-docs\" -v\n\n# 4. Review any failures\ngemini-file-search-tool cache-report --store \"my-docs\" --errors-only\n```\n\n**Re-upload Changed Files:**\n```bash\n# Upload with last-one-wins strategy\n# If files change and you re-upload, previous pending operations are overwritten\ngemini-file-search-tool upload \"docs/*.pdf\" --store \"my-docs\" --no-wait\n\n# The cache automatically tracks the latest operation-id per file\n```\n\n**Cache Inspection:**\n```bash\n# Quick overview\ngemini-file-search-tool cache-report --store \"my-docs\" --text\n\n# Detailed analysis\ngemini-file-search-tool cache-report --store \"my-docs\" --all | jq .\n```\n\n### Library Usage\n\nYou can also use this package as a Python library:\n\n```python\nfrom pathlib import Path\nfrom gemini_file_search_tool import (\n    create_store,\n    upload_file,\n    query_store,\n    list_documents,\n)\n\n# Create a store\nstore = create_store(\"my-documents\")\nprint(f\"Created store: {store['name']}\")\n\n# Upload a file\nresult = upload_file(\n    file_path=Path(\"document.pdf\"),\n    store_name=store[\"name\"],\n    title=\"Important Document\"\n)\nprint(f\"Upload status: {result['status']}\")\n\n# Query the store\nresponse = query_store(\n    store_name=store[\"name\"],\n    prompt=\"What is in this document?\",\n    model=\"gemini-2.5-flash\"\n)\nprint(response[\"response_text\"])\n\n# List documents\ndocuments = list_documents(store[\"name\"])\nprint(f\"Found {len(documents)} documents\")\n```\n\n## Development\n\n### Setup Development Environment\n\n```bash\n# Clone repository\ngit clone https://github.com/dnvriend/gemini-file-search-tool.git\ncd gemini-file-search-tool\n\n# Install dependencies\nmake install\n\n# Show available commands\nmake help\n```\n\n### Available Make Commands\n\n```bash\nmake install          # Install dependencies\nmake format           # Format code with ruff\nmake lint             # Run linting with ruff\nmake typecheck        # Run type checking with mypy\nmake test             # Run tests with pytest\nmake check            # Run all checks (lint, typecheck, test)\nmake pipeline         # Run full pipeline (format, lint, typecheck, test, build, install-global)\nmake build            # Build package\nmake run ARGS=\"...\"   # Run gemini-file-search-tool locally\nmake clean            # Remove build artifacts\n```\n\n### Project Structure\n\n```\ngemini-file-search-tool/\n‚îú‚îÄ‚îÄ gemini_file_search_tool/    # Main package\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ cli.py          # CLI entry point\n‚îÇ   ‚îî‚îÄ‚îÄ utils.py        # Utility functions\n‚îú‚îÄ‚îÄ tests/              # Test suite\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n‚îú‚îÄ‚îÄ pyproject.toml      # Project configuration\n‚îú‚îÄ‚îÄ Makefile            # Development commands\n‚îú‚îÄ‚îÄ README.md           # This file\n‚îú‚îÄ‚îÄ LICENSE             # MIT License\n‚îî‚îÄ‚îÄ CLAUDE.md           # Development documentation\n```\n\n## Testing\n\nRun the test suite:\n\n```bash\n# Run all tests\nmake test\n\n# Run tests with verbose output\nuv run pytest tests/ -v\n\n# Run specific test file\nuv run pytest tests/test_utils.py\n\n# Run with coverage\nuv run pytest tests/ --cov=gemini_file_search_tool\n```\n\n## Known Issues\n\n### SDK Bug #1661 - Document Listing with Vertex AI\n\n**Issue**: The Google Generative AI Python SDK has a bug ([#1661](https://github.com/googleapis/python-genai/issues/1661)) where `documents.list()` requires a 'parent' parameter that causes failures.\n\n**Impact**: The `list-documents` command cannot be used with Vertex AI authentication.\n\n**Workaround**: We use the REST API directly instead of the SDK's `documents.list()` method. This workaround only works with the Developer API (requires `GEMINI_API_KEY` or `GOOGLE_API_KEY`).\n\n**Status**: Waiting for upstream fix in the Google Generative AI Python SDK.\n\n**Affected Commands**:\n- `list-documents` - Only works with Developer API, not Vertex AI\n\n**Code Location**: `gemini_file_search_tool/core/documents.py:list_documents()`\n\n## Contributing\n\nContributions are welcome! Please follow these guidelines:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run the full pipeline (`make pipeline`)\n5. Commit your changes (`git commit -m 'Add amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\n### Code Style\n\n- Follow PEP 8 guidelines\n- Use type hints for all functions\n- Write docstrings for public functions\n- Format code with `ruff`\n- Pass all linting and type checks\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Author\n\n**Dennis Vriend**\n\n- GitHub: [@dnvriend](https://github.com/dnvriend)\n\n## Acknowledgments\n\n- Built with [Click](https://click.palletsprojects.com/) for CLI framework\n- Developed with [uv](https://github.com/astral-sh/uv) for fast Python tooling\n\n## Resources\n\n### Gemini File Search Documentation\n\n- **[Gemini File Search API Documentation](https://ai.google.dev/gemini-api/docs/file-search)** - Official API documentation and guides\n- **[Gemini API File Search Stores Reference](https://ai.google.dev/api/file-search/file-search-stores)** - API reference for file search stores\n- **[Introducing File Search for the Gemini API](https://blog.google/technology/developers/file-search-gemini-api/)** - Official announcement and overview from Google\n\n### Related Tools\n\n- **[Google Generative AI Python SDK](https://github.com/googleapis/python-genai)** - Python SDK for Google's Gemini API\n\n---\n\n**Generated with AI**\n\nThis project was generated using [Claude Code](https://www.anthropic.com/claude/code), an AI-powered development tool by [Anthropic](https://www.anthropic.com/). Claude Code assisted in creating the project structure, implementation, tests, documentation, and development tooling.\n\nMade with ‚ù§Ô∏è using Python 3.14\n"
      },
      "plugins": [
        {
          "name": "gemini-file-search-tool",
          "source": "./plugins/gemini-file-search-tool",
          "description": "Manage Gemini File Search stores and query with RAG",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add dnvriend/gemini-file-search-tool",
            "/plugin install gemini-file-search-tool@gemini-file-search-tool"
          ]
        }
      ]
    },
    {
      "name": "obsidian-search-tool",
      "version": null,
      "description": "Search Obsidian vault via Dataview and JsonLogic",
      "repo_full_name": "dnvriend/obsidian-search-tool",
      "repo_url": "https://github.com/dnvriend/obsidian-search-tool",
      "repo_description": "CLI tool for searching Obsidian vaults through the Local REST API using Dataview DQL and JsonLogic queries",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-12-15T08:32:46Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"obsidian-search-tool\",\n    \"owner\": {\n        \"name\": \"Dennis Vriend\",\n        \"email\": \"dvriend@ilionx.com\",\n        \"url\": \"https://github.com/dnvriend/obsidian-search-tool\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"obsidian-search-tool\",\n            \"source\": \"./plugins/obsidian-search-tool\",\n            \"description\": \"Search Obsidian vault via Dataview and JsonLogic\"\n        }\n    ]\n}\n",
        "README.md": "<div align=\"center\">\n  <img src=\".github/assets/logo.png\" alt=\"Obsidian Search Tool Logo\" width=\"200\"/>\n</div>\n\n# Obsidian Search Tool\n\n[![CI](https://github.com/dnvriend/obsidian-search-tool/actions/workflows/ci.yml/badge.svg)](https://github.com/dnvriend/obsidian-search-tool/actions/workflows/ci.yml)\n[![Python Version](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](https://github.com/python/mypy)\n[![AI Generated](https://img.shields.io/badge/AI-Generated-blueviolet.svg)](https://www.anthropic.com/claude)\n[![Built with Claude Code](https://img.shields.io/badge/Built_with-Claude_Code-5A67D8.svg)](https://www.anthropic.com/claude/code)\n\nA CLI tool that searches an Obsidian vault through the Obsidian Local REST API using Dataview Query Language (DQL) - TABLE queries only - and JsonLogic queries.\n\n## Table of Contents\n\n- [About](#about)\n- [Use Cases](#use-cases)\n- [Features](#features)\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Usage](#usage)\n- [Library Usage](#library-usage)\n- [API Limitations and Known Issues](#api-limitations-and-known-issues)\n- [Development](#development)\n- [Resources](#resources)\n- [License](#license)\n\n## About\n\n### What is Obsidian Search Tool?\n\n`obsidian-search-tool` is a command-line interface for searching through [Obsidian](https://obsidian.md/) vaults using the [Obsidian Local REST API](https://github.com/coddingtonbear/obsidian-local-rest-api) plugin. It provides **search-only** operations using:\n\n- **Dataview DQL**: TABLE queries with powerful filtering, sorting, and grouping\n- **JsonLogic**: JSON-based programmatic queries for tags, frontmatter, and content\n\n### Why This CLI Tool?\n\n**Agent-Friendly Design** - Built specifically for integration with AI agents (Claude Code), automation pipelines, and reusable building blocks:\n- **ReAct Loops**: Structured commands and error messages enable AI agents to reason and act effectively\n- **Composable Architecture**: JSON stdout and logs stderr allow easy piping and integration\n- **Reusable Building Blocks**: Commands serve as primitives for Claude Code skills, MCP servers, or custom workflows\n- **Dual-Mode Operation**: Use as CLI or importable Python library\n- **Production Quality**: Type-safe, tested, with comprehensive error handling\n\n## Use Cases\n\n- üìö **Knowledge Base Management** - Search and organize notes by tags, topics, or metadata\n- üíª **Source Code Intelligence** - Query code snippets and technical documentation\n- üîç **Semantic Search** - Find notes by content, frontmatter fields, or relationships\n- üéØ **RAG Applications** - Retrieve context for AI-powered applications\n- ü§ñ **Agent Integration** - Build Claude Code skills and MCP servers for vault access\n\n## Features\n\n- **Search Operations**:\n  - Dataview DQL TABLE queries (FROM, WHERE, SORT, LIMIT)\n  - JsonLogic queries for programmatic access (content search with \"in\" operator)\n  - Tag search, frontmatter field search, content search, file path search\n  - Access to implicit fields (file.name, file.mtime, file.size, file.tags, etc.)\n  - **Note**: GROUP BY, FLATTEN not supported by Obsidian Local REST API\n\n- **Multiple Output Formats**:\n  - JSON (default) - Machine-readable for automation\n  - Markdown text - Human-readable with metadata\n  - Pretty-printed tables - Visual inspection of structured data\n\n- **CLI-First Design**:\n  - Flat command structure for simplicity\n  - Stdin support for piping queries\n  - Rich error messages with solutions\n  - Comprehensive help with examples\n\n- **Production Quality**:\n  - Type-safe with strict mypy\n  - Tested with pytest\n  - Linted with ruff\n  - Comprehensive error handling\n  - Environment-based configuration\n\n## Prerequisites\n\n### Required\n\n1. **Obsidian** must be running\n2. **[Local REST API](https://github.com/coddingtonbear/obsidian-local-rest-api)** plugin by Adam Coddington installed and enabled\n3. **[Dataview](https://github.com/blacksmithgu/obsidian-dataview)** plugin by Michael Brenan installed and enabled\n4. **OBSIDIAN_API_KEY** environment variable set with API key from plugin settings\n5. Python 3.14+ and [uv](https://github.com/astral-sh/uv) package manager\n\n## Installation\n\n### From Source\n\n```bash\n# Clone repository\ngit clone https://github.com/dnvriend/obsidian-search-tool.git\ncd obsidian-search-tool\n\n# Install globally with uv\nuv tool install .\n\n# Verify installation\nobsidian-search-tool --version\n```\n\n### With mise (Recommended for Development)\n\n```bash\ncd obsidian-search-tool\nmise trust\nmise install\nuv sync\nuv tool install .\n```\n\n## Configuration\n\n### Get API Key\n\n1. Open Obsidian\n2. Go to Settings ‚Üí Community Plugins\n3. Find \"Local REST API\" plugin\n4. Click plugin settings\n5. Copy the API key from the settings page\n\n### Set Environment Variables\n\n```bash\n# Required: API key from plugin settings\nexport OBSIDIAN_API_KEY=\"your-api-key-here\"\n\n# Optional: API base URL (default: http://127.0.0.1:27123)\nexport OBSIDIAN_BASE_URL=\"http://127.0.0.1:27123\"\n\n# Optional: Request timeout in seconds (default: 30)\nexport OBSIDIAN_TIMEOUT=\"30\"\n\n# Optional: Enable verbose logging (default: false)\nexport OBSIDIAN_VERBOSE=\"false\"\n```\n\n## Usage\n\n### Quick Start\n\n```bash\n# Set API key\nexport OBSIDIAN_API_KEY=\"your-api-key-here\"\n\n# Check connectivity\nobsidian-search-tool status\n\n# Validate authentication\nobsidian-search-tool auth\n\n# Search with Dataview DQL (default)\nobsidian-search-tool search 'TABLE file.name FROM #project'\n\n# Search with JsonLogic\nobsidian-search-tool search --type jsonlogic \\\\\n    '{\"in\": [{\"var\": \"frontmatter.tags\"}, \"project\"]}'\n```\n\n### Status and Authentication\n\n```bash\n# Check API connectivity\nobsidian-search-tool status\n\n# Check with text output\nobsidian-search-tool status --text\n\n# Validate authentication\nobsidian-search-tool auth\n\n# Verbose logging\nobsidian-search-tool auth --verbose\n```\n\n### Dataview DQL Search\n\n```bash\n# Basic TABLE query\nobsidian-search-tool search 'TABLE file.name FROM #project'\n\n# With WHERE clause and sorting\nobsidian-search-tool search \\\\\n    'TABLE file.name, author WHERE author SORT file.mtime DESC'\n\n# Recent notes (modified in last 7 days)\nobsidian-search-tool search \\\\\n    'TABLE file.name, file.mtime WHERE file.mtime >= date(today) - dur(7 days) SORT file.mtime DESC'\n\n# Notes with specific field using contains()\nobsidian-search-tool search \\\\\n    'TABLE file.name, author WHERE contains(author, \"Ben\") LIMIT 5'\n\n# Files in specific folder\nobsidian-search-tool search \\\\\n    'TABLE file.name, file.folder WHERE contains(file.folder, \"ai-ml\") LIMIT 5'\n\n# Large files sorted by size\nobsidian-search-tool search \\\\\n    'TABLE file.name, file.size WHERE file.size > 10000 SORT file.size DESC'\n\n# Complex query with multiple conditions\nobsidian-search-tool search \\\\\n    'TABLE file.name, file.mtime FROM \"reference\" WHERE file.size > 5000 AND contains(file.tags, \"#docs\") SORT file.mtime DESC LIMIT 10'\n\n# From stdin\necho 'TABLE file.name FROM #meeting' | obsidian-search-tool search --stdin\n\n# Text output\nobsidian-search-tool search 'TABLE file.name' --text\n\n# Table output (best for structured data)\nobsidian-search-tool search 'TABLE file.name, status, file.mtime' --table\n```\n\n### JsonLogic Search\n\n```bash\n# Content search\nobsidian-search-tool search --type jsonlogic \\\\\n    '{\"in\": [\"Claude\", {\"var\": \"content\"}]}'\n\n# File path contains string\nobsidian-search-tool search --type jsonlogic \\\\\n    '{\"in\": [\"daily\", {\"var\": \"filename\"}]}'\n\n# Search by tag\nobsidian-search-tool search --type jsonlogic \\\\\n    '{\"in\": [{\"var\": \"frontmatter.tags\"}, \"project\"]}'\n\n# Multiple tags (AND)\nobsidian-search-tool search --type jsonlogic \\\\\n    '{\"and\": [{\"in\": [{\"var\": \"frontmatter.tags\"}, \"project\"]}, \\\\\n              {\"in\": [{\"var\": \"frontmatter.tags\"}, \"aws\"]}]}'\n\n# Has frontmatter field\nobsidian-search-tool search --type jsonlogic \\\\\n    '{\"exists\": {\"var\": \"frontmatter.status\"}}'\n```\n\n### Output Formats\n\n```bash\n# JSON output (default)\nobsidian-search-tool search 'TABLE file.name'\n\n# Markdown text output\nobsidian-search-tool search 'TABLE file.name' --text\n\n# Pretty-printed table output\nobsidian-search-tool search 'TABLE file.name, author' --table\n```\n\n## Library Usage\n\nUse as a Python library for programmatic access:\n\n```python\nfrom obsidian_search_tool import ObsidianClient\n\n# Create client (reads environment variables)\nclient = ObsidianClient()\n\n# Or with explicit configuration\nclient = ObsidianClient(\n    base_url=\"http://127.0.0.1:27123\",\n    api_key=\"your-api-key\",\n    timeout=30\n)\n\n# Check status\nstatus = client.status()\nprint(f\"Connected: {status.status}\")\n\n# Validate authentication\nauth = client.check_auth()\nprint(f\"Authenticated: {auth.status}\")\n\n# Dataview DQL search\nresponse = client.search_dataview('TABLE file.name FROM #project')\nif response.success:\n    print(f\"Found {response.result_count} results\")\n    for result in response.results:\n        print(result)\n\n# JsonLogic search\nresponse = client.search_jsonlogic('{\"in\": [{\"var\": \"frontmatter.tags\"}, \"aws\"]}')\nif response.success:\n    print(f\"Results: {response.results}\")\n```\n\n### Error Handling\n\n```python\nfrom obsidian_search_tool import (\n    ObsidianClient,\n    ObsidianAuthError,\n    ObsidianConnectionError,\n    ObsidianAPIError,\n)\n\ntry:\n    client = ObsidianClient()\n    response = client.search_dataview('TABLE file.name')\nexcept ObsidianAuthError as e:\n    print(f\"Authentication failed: {e}\")\nexcept ObsidianConnectionError as e:\n    print(f\"Connection error: {e}\")\nexcept ObsidianAPIError as e:\n    print(f\"API error [{e.status_code}]: {e}\")\n```\n\n## API Limitations and Known Issues\n\nThis section documents tested functionality and known limitations of the Obsidian Local REST API.\n\n### ‚úÖ Supported Dataview DQL Features\n\n**Data Commands** (tested and verified):\n- ‚úÖ **FROM** - Source selection by tags, folders, files, links\n- ‚úÖ **WHERE** - Filtering with all comparison operators\n- ‚úÖ **SORT** - Single and multiple field sorting (ASC/DESC)\n- ‚úÖ **LIMIT** - Result count restriction\n\n**Functions** (tested and verified):\n- ‚úÖ **date()** - Date parsing and construction: `date(today)`, `date(\"2024-01-01\")`\n- ‚úÖ **dur()** - Duration construction: `dur(7 days)`, `dur(2 hours)`\n- ‚úÖ **contains()** - String/array contains checks\n- ‚úÖ **Comparison operators** - `>`, `<`, `>=`, `<=`, `=`, `!=`\n- ‚úÖ **Logical operators** - `AND`, `OR`, `!`\n- ‚úÖ **Arithmetic** - Date math: `date(today) - dur(7 days)`\n\n**Implicit Fields** (all work correctly):\n- ‚úÖ `file.name`, `file.path`, `file.folder`, `file.size`, `file.ext`\n- ‚úÖ `file.mtime`, `file.mday`, `file.ctime`, `file.cday`\n- ‚úÖ `file.tags`, `file.etags`, `file.inlinks`, `file.outlinks`\n\n### ‚ùå Unsupported Features\n\n**Dataview DQL Commands:**\n- ‚ùå **GROUP BY** - Returns error: `TABLE WITHOUT ID queries are not supported`\n- ‚ùå **FLATTEN** - Returns error: `TABLE WITHOUT ID queries are not supported`\n- ‚ùå **LIST queries** - Only TABLE queries supported by API\n- ‚ùå **TASK queries** - Only TABLE queries supported by API\n- ‚ùå **CALENDAR queries** - Only TABLE queries supported by API\n\n**JsonLogic Operators:**\n- ‚ùå **startsWith** - Returns error: `Unrecognized operation startsWith`\n- ‚ùå **endsWith** - Returns error: `Unrecognized operation endsWith`\n- ‚úÖ **in** operator - Works for content, filename, and frontmatter searches\n\n### üí° Workarounds\n\n**Folder filtering** (since GROUP BY doesn't work):\n```bash\n# Use contains() to filter by folder path\nobsidian-search-tool search \\\n    'TABLE file.name, file.folder WHERE contains(file.folder, \"reference\")'\n```\n\n**Path pattern matching** (since startsWith doesn't work):\n```bash\n# Use JsonLogic \"in\" operator for substring matching\nobsidian-search-tool search --type jsonlogic \\\n    '{\"in\": [\"daily\", {\"var\": \"filename\"}]}'\n```\n\n**Counting results**:\n```bash\n# Get count via jq\nobsidian-search-tool search 'TABLE file.name FROM \"daily\"' | jq '.data.results | length'\n```\n\n### üêõ Common Errors\n\n| Error Message | Cause | Solution |\n|---------------|-------|----------|\n| `OBSIDIAN_API_KEY environment variable is required` | Missing API key | Set `OBSIDIAN_API_KEY` from plugin settings |\n| `Connection failed` | Obsidian not running or API plugin disabled | Start Obsidian and enable Local REST API plugin |\n| `Only TABLE dataview queries are supported` | Used LIST/TASK/CALENDAR query | Use TABLE queries only |\n| `TABLE WITHOUT ID queries are not supported` | Used GROUP BY or FLATTEN | Remove GROUP BY/FLATTEN from query |\n| `Unrecognized operation startsWith` | Used unsupported JsonLogic operator | Use `in` operator for substring matching |\n\n## Development\n\n### Setup\n\n```bash\n# Clone and install\ngit clone https://github.com/dnvriend/obsidian-search-tool.git\ncd obsidian-search-tool\nmake install\n\n# Show available commands\nmake help\n```\n\n### Available Make Commands\n\n```bash\nmake install       # Install dependencies\nmake format        # Format code with ruff\nmake lint          # Run linting with ruff\nmake typecheck     # Run type checking with mypy\nmake test          # Run tests with pytest\nmake check         # Run all checks (lint, typecheck, test)\nmake pipeline      # Full pipeline (format, check, build, install-global)\nmake build         # Build package\nmake clean         # Remove build artifacts\n```\n\n### Project Structure\n\n```\nobsidian-search-tool/\n‚îú‚îÄ‚îÄ obsidian_search_tool/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Public API exports\n‚îÇ   ‚îú‚îÄ‚îÄ cli.py             # CLI entry point\n‚îÇ   ‚îú‚îÄ‚îÄ core/              # Core library\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.py      # ObsidianClient\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py      # Data models\n‚îÇ   ‚îú‚îÄ‚îÄ commands/          # CLI commands\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_commands.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ status_commands.py\n‚îÇ   ‚îî‚îÄ‚îÄ utils.py           # Formatters and logging\n‚îú‚îÄ‚îÄ tests/                 # Test suite\n‚îú‚îÄ‚îÄ pyproject.toml         # Project configuration\n‚îú‚îÄ‚îÄ Makefile              # Development commands\n‚îú‚îÄ‚îÄ README.md             # This file\n‚îî‚îÄ‚îÄ CLAUDE.md             # Developer guide\n```\n\n## Resources\n\n### Official Documentation\n\n- [Obsidian](https://obsidian.md/) - Knowledge base application\n- [Obsidian Local REST API](https://github.com/coddingtonbear/obsidian-local-rest-api) - REST API plugin\n- [Dataview Plugin](https://github.com/blacksmithgu/obsidian-dataview) - Query language for Obsidian\n- [JsonLogic](https://jsonlogic.com/) - JSON-based logic expression format\n\n### Related Tools\n\n- [Python SDK](https://github.com/coddingtonbear/python-obsidian-rest) - Python client for Obsidian REST API\n- [Dataview Documentation](https://blacksmithgu.github.io/obsidian-dataview/) - Complete Dataview reference\n\n### Reference Files\n\nThe `references/` directory contains curated documentation for quick reference:\n\n- **[JsonLogic Search Examples](references/jsonlogic-search-examples.md)** - JsonLogic query patterns and examples\n- **[Dataview Query Types](references/dataview-ql-query-types.md)** - TABLE, LIST, TASK, and CALENDAR query types\n- **[Dataview Data Commands](references/dataview-ql-data-commands.md)** - FROM, WHERE, SORT, GROUP BY, LIMIT reference\n- **[Dataview Functions](references/dataview-ql-functions.md)** - Complete function reference for DQL expressions\n- **[Dataview Page Metadata](references/dataview-metadata-on-pages.md)** - Available file.* fields and implicit metadata\n\nThese files provide offline reference for common query patterns and Dataview capabilities.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Author\n\n**Dennis Vriend**\n- GitHub: [@dnvriend](https://github.com/dnvriend)\n- Email: dvriend@ilionx.com\n\n## Acknowledgments\n\n- Built with [Click](https://click.palletsprojects.com/) for CLI framework\n- Built with [Requests](https://requests.readthedocs.io/) for HTTP client\n- Built with [Rich](https://rich.readthedocs.io/) for terminal formatting\n- Developed with [uv](https://github.com/astral-sh/uv) for fast Python tooling\n- Type-checked with [mypy](https://mypy.readthedocs.io/)\n- Linted and formatted with [ruff](https://github.com/astral-sh/ruff)\n\n---\n\n**Generated with AI**\n\nThis project was generated using [Claude Code](https://www.anthropic.com/claude/code), an AI-powered development tool by [Anthropic](https://www.anthropic.com/). Claude Code assisted in creating the project structure, implementation, tests, documentation, and development tooling.\n\nMade with ‚ù§Ô∏è using Python 3.14\n"
      },
      "plugins": [
        {
          "name": "obsidian-search-tool",
          "source": "./plugins/obsidian-search-tool",
          "description": "Search Obsidian vault via Dataview and JsonLogic",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add dnvriend/obsidian-search-tool",
            "/plugin install obsidian-search-tool@obsidian-search-tool"
          ]
        }
      ]
    },
    {
      "name": "gemini-google-search-tool",
      "version": null,
      "description": "Query Gemini with Google Search grounding",
      "repo_full_name": "dnvriend/gemini-google-search-tool",
      "repo_url": "https://github.com/dnvriend/gemini-google-search-tool",
      "repo_description": "A professional CLI tool and Python library for querying Gemini with Google Search grounding",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-12-05T06:45:37Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"gemini-google-search-tool\",\n    \"owner\": {\n        \"name\": \"Dennis Vriend\",\n        \"email\": \"dvriend@ilionx.com\",\n        \"url\": \"https://github.com/dnvriend/gemini-google-search-tool\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"gemini-google-search-tool\",\n            \"source\": \"./plugins/gemini-google-search-tool\",\n            \"description\": \"Query Gemini with Google Search grounding\"\n        }\n    ]\n}\n",
        "README.md": "# gemini-google-search-tool\n\n[![Python Version](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](https://github.com/python/mypy)\n[![AI Generated](https://img.shields.io/badge/AI-Generated-blueviolet.svg)](https://www.anthropic.com/claude)\n[![Built with Claude Code](https://img.shields.io/badge/Built_with-Claude_Code-5A67D8.svg)](https://www.anthropic.com/claude/code)\n\nA professional CLI tool and Python library for querying Gemini with Google Search grounding, connecting AI responses to real-time web content with automatic citations.\n\n## Table of Contents\n\n- [About](#about)\n- [Use Cases](#use-cases)\n- [Features](#features)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Usage](#usage)\n  - [CLI Usage](#cli-usage)\n  - [Library Usage](#library-usage)\n- [Development](#development)\n- [Testing](#testing)\n- [Resources](#resources)\n- [Contributing](#contributing)\n- [License](#license)\n\n## About\n\n### What is Google Search Grounding?\n\n[Google Search grounding](https://ai.google.dev/gemini-api/docs/grounding) connects Gemini models to real-time web content through Google Search. When you query Gemini with grounding enabled, the model:\n\n1. Automatically determines if a search would improve the answer\n2. Generates and executes appropriate search queries\n3. Processes search results and synthesizes information\n4. Returns a grounded response with verifiable sources\n\nThis reduces hallucinations and provides up-to-date information beyond the model's training cutoff.\n\n### Why This CLI-First Tool?\n\nThis tool provides a **CLI-first** architecture designed for both humans and AI agents:\n\n- **Agent-Friendly Design**: Structured commands and rich error messages enable AI agents (like Claude Code) to reason and act effectively in ReAct loops\n- **Composable Architecture**: JSON output to stdout, logs to stderr‚Äîperfect for piping and automation\n- **Reusable Building Blocks**: Commands serve as primitives for Claude Code skills, MCP servers, shell scripts, and custom workflows\n- **Dual-Mode Operation**: Use it as a CLI tool or import as a Python library\n- **Production Quality**: Type-safe (strict mypy), tested (pytest), with comprehensive error handling\n\n## Use Cases\n\n- üìö **Real-Time Information**: Access current events, news, and recent developments\n- üîç **Fact Verification**: Ground responses in verifiable web sources\n- üí° **Research Assistance**: Gather information from multiple sources automatically\n- ü§ñ **Agent Integration**: Build AI workflows with reliable, up-to-date information\n- üîó **Citation Management**: Track and display source attributions automatically\n\n## Features\n\n### Core Capabilities\n\n- **Real-Time Web Search**: Automatic search execution with multi-query support\n- **Automatic Citations**: Inline citations with source URIs and titles\n- **Model Selection**: Choose between gemini-2.5-flash (fast) or gemini-2.5-pro (powerful)\n- **Flexible Input**: Positional arguments or stdin for automation\n- **Multiple Output Formats**: JSON (default) or markdown with citations\n- **Verbose Metadata**: Full grounding details including search queries and supports\n\n### CLI-First Design\n\n- **Type Safety**: Strict mypy checking, comprehensive type hints\n- **Error Handling**: Rich error messages with suggested fixes\n- **Composability**: JSON to stdout, logs to stderr for piping\n- **Documentation**: Agent-friendly help with inline examples\n- **Testing**: Comprehensive test suite with pytest\n\n## Installation\n\n### Prerequisites\n\n- Python 3.14 or higher\n- [uv](https://github.com/astral-sh/uv) package manager (recommended)\n- Google Gemini API key ([get one free](https://aistudio.google.com/app/apikey))\n\n### Install Globally with uv\n\n```bash\n# Install from source\ngit clone https://github.com/dnvriend/gemini-google-search-tool.git\ncd gemini-google-search-tool\nuv tool install .\n\n# Verify installation\ngemini-google-search-tool --version\n```\n\n### Install with pip\n\n```bash\npip install gemini-google-search-tool\n```\n\n### Install with mise (recommended for development)\n\n```bash\ncd gemini-google-search-tool\nmise trust\nmise install\nuv sync\nuv tool install .\n```\n\n### Shell Completion\n\nEnable tab completion for your shell to improve the CLI experience:\n\n**Bash (add to `~/.bashrc`):**\n```bash\neval \"$(gemini-google-search-tool completion bash)\"\n```\n\n**Zsh (add to `~/.zshrc`):**\n```bash\neval \"$(gemini-google-search-tool completion zsh)\"\n```\n\n**Fish (run once):**\n```bash\nmkdir -p ~/.config/fish/completions\ngemini-google-search-tool completion fish > ~/.config/fish/completions/gemini-google-search-tool.fish\n```\n\nFor more details:\n```bash\ngemini-google-search-tool completion --help\n```\n\n## Configuration\n\n### API Key Setup\n\nThe tool requires a `GEMINI_API_KEY` environment variable.\n\n**Option 1: Environment Variable**\n\n```bash\nexport GEMINI_API_KEY='your-api-key-here'\n```\n\n**Option 2: macOS Keychain** (secure storage)\n\n```bash\n# Store in keychain\nsecurity add-generic-password -a \"production\" -s \"GEMINI_API_KEY\" -w \"your-api-key\"\n\n# Retrieve and export\nexport GEMINI_API_KEY=$(security find-generic-password -a \"production\" -s \"GEMINI_API_KEY\" -w)\n```\n\n**Get a free API key**: [Google AI Studio](https://aistudio.google.com/app/apikey)\n\n## Usage\n\n### CLI Usage\n\n#### Basic Query\n\n```bash\ngemini-google-search-tool query \"Who won euro 2024?\"\n```\n\n**Output:**\n```json\n{\n  \"response_text\": \"Spain won Euro 2024, defeating England 2-1 in the final...\",\n  \"citations\": [\n    {\"index\": 1, \"uri\": \"https://...\", \"title\": \"youtube.com\"},\n    {\"index\": 2, \"uri\": \"https://...\", \"title\": \"wikipedia.org\"}\n  ]\n}\n```\n\nCitations are always included in JSON output by default.\n\n#### Query with Inline Citations\n\nThe `--add-citations` flag adds inline citation links directly into the response text.\n\n```bash\ngemini-google-search-tool query \"Latest AI developments\" --add-citations\n```\n\n**Output:**\n```json\n{\n  \"response_text\": \"Recent AI developments include...[1](https://...), [2](https://...)\",\n  \"citations\": [\n    {\"index\": 1, \"uri\": \"https://...\", \"title\": \"Source Title\"},\n    {\"index\": 2, \"uri\": \"https://...\", \"title\": \"Another Source\"}\n  ]\n}\n```\n\n#### Read from stdin\n\n```bash\necho \"Climate change updates\" | gemini-google-search-tool query --stdin\n```\n\n#### Markdown Output\n\n```bash\ngemini-google-search-tool query \"Quantum computing news\" --text\n```\n\n**Output:**\n```markdown\nRecent advances in quantum computing include...\n\n## Citations\n\n1. [MIT Technology Review](https://...)\n2. [Nature](https://...)\n```\n\n#### Verbose Output with Grounding Metadata\n\n```bash\ngemini-google-search-tool query \"Latest tech trends\" --verbose\n```\n\n**Output includes:**\n- Response text\n- Web search queries executed\n- Grounding chunks with URIs\n- Grounding supports (which parts of text are supported by which sources)\n\n#### Use Pro Model\n\n```bash\ngemini-google-search-tool query \"Analyze quantum computing impact\" --pro\n```\n\n#### All Options Combined\n\n```bash\ngemini-google-search-tool query \"Machine learning breakthroughs\" \\\n  --add-citations \\\n  --pro \\\n  --verbose\n```\n\n### Library Usage\n\nImport and use as a Python library:\n\n```python\nfrom gemini_google_search_tool import (\n    GeminiClient,\n    query_with_grounding,\n    add_inline_citations,\n)\n\n# Initialize client\nclient = GeminiClient()  # Reads GEMINI_API_KEY from environment\n\n# Query with grounding\nresponse = query_with_grounding(\n    client=client,\n    prompt=\"Who won euro 2024?\",\n    model=\"gemini-2.5-flash\",  # or \"gemini-2.5-pro\"\n)\n\n# Access response\nprint(response.response_text)\nprint(f\"Citations: {len(response.citations)}\")\n\n# Access citations\nfor citation in response.citations:\n    print(f\"[{citation.index}] {citation.title}: {citation.uri}\")\n\n# Access metadata\nif response.web_search_queries:\n    print(f\"Search queries: {response.web_search_queries}\")\n\n# Add inline citations\nif response.grounding_segments:\n    text_with_citations = add_inline_citations(\n        response.response_text,\n        response.grounding_segments,\n        response.citations,\n    )\n    print(text_with_citations)\n```\n\n### Error Handling in Library\n\n```python\nfrom gemini_google_search_tool import GeminiClient, GeminiClientError, SearchError\n\ntry:\n    client = GeminiClient()\n    response = query_with_grounding(client, \"Your query\")\nexcept GeminiClientError as e:\n    print(f\"Client error: {e}\")\n    # Handle authentication or configuration errors\nexcept SearchError as e:\n    print(f\"Search error: {e}\")\n    # Handle query execution errors\n```\n\n## Development\n\n### Setup Development Environment\n\n```bash\n# Clone repository\ngit clone https://github.com/dnvriend/gemini-google-search-tool.git\ncd gemini-google-search-tool\n\n# Install dependencies\nmake install\n\n# Show available commands\nmake help\n```\n\n### Available Make Commands\n\n```bash\nmake install          # Install dependencies\nmake format           # Format code with ruff\nmake lint             # Run linting with ruff\nmake typecheck        # Run type checking with mypy\nmake test             # Run tests with pytest\nmake check            # Run all checks (lint, typecheck, test)\nmake pipeline         # Full pipeline: format, check, build, install-global\nmake build            # Build package\nmake clean            # Remove build artifacts\n```\n\n### Project Structure\n\n```\ngemini-google-search-tool/\n‚îú‚îÄ‚îÄ gemini_google_search_tool/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Public API exports\n‚îÇ   ‚îú‚îÄ‚îÄ cli.py                   # CLI entry point (Click group)\n‚îÇ   ‚îú‚îÄ‚îÄ core/                    # Core library (importable, CLI-independent)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.py           # Gemini client management\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ search.py           # Search and citation logic\n‚îÇ   ‚îú‚îÄ‚îÄ commands/                # CLI command implementations\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query_commands.py   # Query command with Click decorators\n‚îÇ   ‚îî‚îÄ‚îÄ utils.py                 # Shared utilities (output, validation)\n‚îú‚îÄ‚îÄ tests/                       # Test suite\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n‚îú‚îÄ‚îÄ pyproject.toml               # Project configuration\n‚îú‚îÄ‚îÄ Makefile                     # Development commands\n‚îú‚îÄ‚îÄ README.md                    # User documentation (this file)\n‚îú‚îÄ‚îÄ CLAUDE.md                    # Developer guide\n‚îî‚îÄ‚îÄ LICENSE                      # MIT License\n```\n\n### Architecture Principles\n\n- **Separation of Concerns**: Core library (`core/`) is independent of CLI\n- **Exception-Based Errors**: Core functions raise exceptions (not `sys.exit`), CLI handles formatting\n- **Importable Library**: Public API exposed via `__init__.py` for programmatic use\n- **Type Safety**: Strict mypy checks, comprehensive type hints\n- **Composability**: JSON to stdout, logs to stderr for piping\n\n## Testing\n\nRun the test suite:\n\n```bash\n# Run all tests\nmake test\n\n# Run with verbose output\nuv run pytest tests/ -v\n\n# Run specific test file\nuv run pytest tests/test_utils.py\n\n# Run with coverage\nuv run pytest tests/ --cov=gemini_google_search_tool\n```\n\n## Resources\n\n### Official Documentation\n\n- [Google Search Grounding Documentation](https://ai.google.dev/gemini-api/docs/grounding)\n- [Gemini API Documentation](https://ai.google.dev/gemini-api/docs)\n- [Google AI Studio](https://aistudio.google.com/) - Get API keys and test models\n\n### Related Tools\n\n- [google-genai Python SDK](https://github.com/googleapis/python-genai) - Official Gemini Python client\n- [Click](https://click.palletsprojects.com/) - CLI framework\n- [uv](https://github.com/astral-sh/uv) - Fast Python package manager\n\n## Contributing\n\nContributions are welcome! Please follow these guidelines:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run the full pipeline (`make pipeline`)\n5. Commit your changes (`git commit -m 'Add amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\n### Code Style\n\n- Follow PEP 8 guidelines (enforced by ruff)\n- Use type hints for all functions (strict mypy)\n- Write docstrings for public functions (Google style)\n- Format code with `ruff format`\n- Pass all checks: `make check`\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Author\n\n**Dennis Vriend**\n\n- GitHub: [@dnvriend](https://github.com/dnvriend)\n- Email: dvriend@ilionx.com\n\n## Acknowledgments\n\n- Built with [Click](https://click.palletsprojects.com/) for CLI framework\n- Developed with [uv](https://github.com/astral-sh/uv) for fast Python tooling\n- Uses [google-genai](https://github.com/googleapis/python-genai) SDK for Gemini API access\n- Quality assurance with [ruff](https://github.com/astral-sh/ruff) and [mypy](https://github.com/python/mypy)\n\n---\n\n**Generated with AI**\n\nThis project was generated using [Claude Code](https://www.anthropic.com/claude/code), an AI-powered development tool by [Anthropic](https://www.anthropic.com/). Claude Code assisted in creating the project structure, implementation, tests, documentation, and development tooling.\n\nMade with ‚ù§Ô∏è using Python 3.14\n"
      },
      "plugins": [
        {
          "name": "gemini-google-search-tool",
          "source": "./plugins/gemini-google-search-tool",
          "description": "Query Gemini with Google Search grounding",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add dnvriend/gemini-google-search-tool",
            "/plugin install gemini-google-search-tool@gemini-google-search-tool"
          ]
        }
      ]
    },
    {
      "name": "paste-image-to-file-tool",
      "version": null,
      "description": "Marketplace for paste-image-to-file-tool CLI plugin - A Python CLI tool",
      "repo_full_name": "dnvriend/paste-image-to-file-tool",
      "repo_url": "https://github.com/dnvriend/paste-image-to-file-tool",
      "repo_description": "paste-image-to-file-tool",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-15T20:58:33Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"paste-image-to-file-tool\",\n    \"owner\": {\n        \"name\": \"Dennis Vriend\",\n        \"email\": \"dvriend@ilionx.com\",\n        \"url\": \"https://github.com/dnvriend\"\n    },\n    \"metadata\": {\n        \"description\": \"Marketplace for paste-image-to-file-tool CLI plugin - A Python CLI tool\",\n        \"version\": \"0.1.0\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"paste-image-to-file-tool\",\n            \"source\": \"./plugins/paste-image-to-file-tool\",\n            \"description\": \"A Python CLI tool\"\n        }\n    ]\n}\n",
        "README.md": "# paste-image-to-file-tool\n\n<p align=\"center\">\n  <img src=\".github/assets/logo.png\" alt=\"paste-image-to-file-tool logo\" width=\"128\">\n</p>\n\n[![Python Version](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](https://github.com/python/mypy)\n\nA CLI tool that saves clipboard images to numbered PNG files. Copy a screenshot, run the tool, and images are saved as `1.png`, `2.png`, etc.\n\nThis tool pairs beautifully with the [gemini-ocr-tool](https://github.com/dnvriend/gemini-ocr-tool) where you can first create bulk screenshots with this tool, then bulk ocr these screenshots to text for postprocessing, eg. if you want to make quizzes with the [markdown-quiz-exporter-tool](https://github.com/dnvriend/markdown-quiz-exporter-tool), but there are many other uses as well.\n\n## Quick Start\n\n```bash\n# Install\nuv tool install .\n\n# Run (auto mode - polls clipboard for new images)\npaste-image-to-file-tool\n\n# Output:\n# waiting for paste (auto mode, Ctrl+C to stop)\n# writing 1.png\n# waiting for paste\n# writing 2.png\n```\n\n## Features\n\n- Save clipboard images to numbered PNG files (`1.png`, `2.png`, ...)\n- Auto mode: polls clipboard, saves on change (default)\n- Manual mode: press Enter to save current clipboard (`-m`)\n- Multi-level verbosity logging (`-v`/`-vv`/`-vvv`)\n- OpenTelemetry observability (traces, metrics, logs)\n- Shell completion for bash, zsh, and fish\n\n## Installation\n\n### Prerequisites\n\n- Python 3.14+\n- [uv](https://github.com/astral-sh/uv) package manager\n\n### Install from source\n\n```bash\ngit clone https://github.com/dnvriend/paste-image-to-file-tool.git\ncd paste-image-to-file-tool\nuv tool install .\n```\n\n### Verify installation\n\n```bash\npaste-image-to-file-tool --version\n```\n\n## Usage\n\n```bash\n# Auto mode (default)\npaste-image-to-file-tool\n\n# Manual mode - press Enter to save\npaste-image-to-file-tool -m\n\n# With verbose logging\npaste-image-to-file-tool -v      # INFO level\npaste-image-to-file-tool -vv     # DEBUG level\n\n# With telemetry\npaste-image-to-file-tool --telemetry\n\n# Show help\npaste-image-to-file-tool --help\n```\n\n## Documentation\n\n| Topic | Description |\n|-------|-------------|\n| [Logging](references/logging.md) | Multi-level verbosity and file logging |\n| [Telemetry](references/telemetry.md) | OpenTelemetry observability setup |\n| [Shell Completion](references/shell-completion.md) | Bash, zsh, and fish completion |\n| [Security](references/security.md) | Security scanning tools |\n| [Development](references/development.md) | Development setup and commands |\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Run the full pipeline (`make pipeline`)\n4. Commit and push your changes\n5. Open a Pull Request\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.\n\n## Author\n\n**Dennis Vriend** - [@dnvriend](https://github.com/dnvriend)\n\n---\n\nBuilt with [Typer](https://typer.tiangolo.com/) and [uv](https://github.com/astral-sh/uv)\n"
      },
      "plugins": [
        {
          "name": "paste-image-to-file-tool",
          "source": "./plugins/paste-image-to-file-tool",
          "description": "A Python CLI tool",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add dnvriend/paste-image-to-file-tool",
            "/plugin install paste-image-to-file-tool@paste-image-to-file-tool"
          ]
        }
      ]
    },
    {
      "name": "aws-transcribe-tool",
      "version": null,
      "description": "Marketplace for aws-transcribe-tool CLI plugin - A Python CLI tool",
      "repo_full_name": "dnvriend/aws-transcribe-tool",
      "repo_url": "https://github.com/dnvriend/aws-transcribe-tool",
      "repo_description": "aws-transcribe-tool",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-12T20:37:24Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"aws-transcribe-tool\",\n    \"owner\": {\n        \"name\": \"Dennis Vriend\",\n        \"email\": \"dvriend@ilionx.com\",\n        \"url\": \"https://github.com/dnvriend\"\n    },\n    \"metadata\": {\n        \"description\": \"Marketplace for aws-transcribe-tool CLI plugin - A Python CLI tool\",\n        \"version\": \"0.1.0\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"aws-transcribe-tool\",\n            \"source\": \"./plugins/aws-transcribe-tool\",\n            \"description\": \"A Python CLI tool\"\n        }\n    ]\n}\n",
        "README.md": "# aws-transcribe-tool\n\n<p align=\"center\">\n  <img src=\".github/assets/logo.png\" alt=\"AWS Transcribe Tool Logo\" width=\"400\"/>\n</p>\n\n[![Python](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA CLI tool that transcribes audio files to text using Amazon Transcribe with support for 60+ languages.\n\n## Installation\n\n```bash\ngit clone https://github.com/dnvriend/aws-transcribe-tool.git\ncd aws-transcribe-tool\nuv tool install .\n```\n\n**Requirements**: Python 3.14+, uv, AWS credentials, S3 bucket.\n\n## Usage\n\n```bash\n# Transcribe audio\naws-transcribe-tool transcribe interview.mp3 transcript.txt my-s3-bucket\n\n# Specify language (default: en-US)\naws-transcribe-tool transcribe audio.wav output.txt my-bucket -l nl-NL\n\n# List supported languages\naws-transcribe-tool list-languages\n```\n\n## Options\n\n| Flag | Description |\n|------|-------------|\n| `-l, --language` | Language code (default: en-US) |\n| `-v` | Verbose output (-v INFO, -vv DEBUG, -vvv TRACE) |\n| `--telemetry` | Enable OpenTelemetry |\n| `--version` | Show version |\n\n## License\n\nMIT\n"
      },
      "plugins": [
        {
          "name": "aws-transcribe-tool",
          "source": "./plugins/aws-transcribe-tool",
          "description": "A Python CLI tool",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add dnvriend/aws-transcribe-tool",
            "/plugin install aws-transcribe-tool@aws-transcribe-tool"
          ]
        }
      ]
    },
    {
      "name": "concat-glob-tool",
      "version": null,
      "description": "Concatenate files with glob patterns and separators",
      "repo_full_name": "dnvriend/concat-glob-tool",
      "repo_url": "https://github.com/dnvriend/concat-glob-tool",
      "repo_description": "A CLI tool that concatenates files matching glob patterns with intelligent separators",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-12-05T06:43:41Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"concat-glob-tool\",\n    \"owner\": {\n        \"name\": \"Dennis Vriend\",\n        \"email\": \"dvriend@ilionx.com\",\n        \"url\": \"https://github.com/dnvriend/concat-glob-tool\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"concat-glob-tool\",\n            \"source\": \"./plugins/concat-glob-tool\",\n            \"description\": \"Concatenate files with glob patterns and separators\"\n        }\n    ]\n}\n",
        "README.md": "# concat-glob-tool\n\n<div align=\"center\">\n  <img src=\".github/assets/logo-web.png\" alt=\"concat-glob-tool logo\" width=\"200\"/>\n  <p><em>Concatenate files with intelligent separators</em></p>\n</div>\n\n[![Python Version](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](https://github.com/python/mypy)\n[![AI Generated](https://img.shields.io/badge/AI-Generated-blueviolet.svg)](https://www.anthropic.com/claude)\n[![Built with Claude Code](https://img.shields.io/badge/Built_with-Claude_Code-5A67D8.svg)](https://www.anthropic.com/claude/code)\n\nA CLI tool that concatenates files matching glob patterns with intelligent separators and agent-friendly design.\n\n## Table of Contents\n\n- [About](#about)\n- [Use Cases](#use-cases)\n- [Features](#features)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Usage](#usage)\n- [Library Usage](#library-usage)\n- [Multi-Level Verbosity Logging](#multi-level-verbosity-logging)\n- [Shell Completion](#shell-completion)\n- [Development](#development)\n- [Testing](#testing)\n- [Security](#security)\n- [Contributing](#contributing)\n- [License](#license)\n- [Author](#author)\n\n## About\n\n`concat-glob-tool` is a CLI-first utility that concatenates multiple files into a single output file with intelligent separators. Built with modern Python tooling and designed for both human use and AI agent integration.\n\n### What is concat-glob-tool?\n\n`concat-glob-tool` provides a simple yet powerful way to combine multiple files into a single file with clear separators between each file. It's particularly useful for:\n\n- Creating context files for Large Language Models (LLMs)\n- Combining source code for documentation or analysis\n- Aggregating log files or configuration files\n- Building single-file distributions\n\n### Why CLI-First?\n\nThis tool follows a CLI-first design philosophy, making it:\n\n- **ü§ñ Agent-Friendly**: Designed for AI agents (Claude Code) with structured commands, clear error messages, and working examples in help text\n- **üîó Composable**: JSON output to stdout, logs to stderr for easy piping and integration with automation tools\n- **üß± Reusable**: Commands serve as building blocks for Claude Code skills, MCP servers, shell scripts, or custom workflows\n- **‚úÖ Reliable**: Type-safe with comprehensive testing ensures predictable behavior in automated systems\n- **üìö Self-Documenting**: Rich help messages guide both humans and agents through usage patterns\n- **üîß Maintainable**: Modular architecture makes it easy to extend, debug, and evolve\n\n### Dual-Mode Operation\n\n- **CLI Mode**: Use as a standalone command-line tool with all features\n- **Library Mode**: Import core functions for programmatic integration in Python scripts\n\n## Use Cases\n\n- üìö **LLM Context Generation**: Combine multiple source files into a single context file for feeding to Large Language Models\n- üíª **Code Documentation**: Aggregate source code files for documentation or code review\n- üîç **Log Analysis**: Merge multiple log files with clear file separators for easier analysis\n- üì¶ **Single-File Distribution**: Create self-contained bundles of configuration or template files\n- üéØ **RAG Preparation**: Prepare documents for Retrieval-Augmented Generation systems\n- ü§ñ **Agent Integration**: Build reusable file processing pipelines for AI automation workflows\n\n## Features\n\n- üéØ **Glob Pattern Matching**: Support for multiple glob patterns including recursive patterns (`**/*.py`)\n- üì• **Stdin Support**: Read file paths from stdin for integration with other tools (e.g., `find`)\n- üîÄ **Intelligent Separators**: Files are separated with headers including the filename\n- üîç **Dry-Run Mode**: Preview operations before execution (enabled by default)\n- üí™ **Force Overwrite**: Safely overwrite existing files with explicit flag\n- üé® **Custom Separators**: Configurable separator text between files\n- ‚úÖ Type-safe with mypy strict mode\n- ‚úÖ Linted with ruff\n- ‚úÖ Tested with pytest\n- üìä Multi-level verbosity logging (-v/-vv/-vvv)\n- üêö Shell completion for bash, zsh, and fish\n- üîí Security scanning with bandit, pip-audit, and gitleaks\n- ‚úÖ Modern Python tooling (uv, mise, click)\n- üì¶ Dual-mode: CLI tool + importable Python library\n\n## Installation\n\n### Prerequisites\n\n- Python 3.14 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n\n### Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/dnvriend/concat-glob-tool.git\ncd concat-glob-tool\n\n# Install globally with uv\nuv tool install .\n```\n\n### Install with mise (recommended for development)\n\n```bash\ncd concat-glob-tool\nmise trust\nmise install\nuv sync\nuv tool install .\n```\n\n### Verify installation\n\n```bash\nconcat-glob-tool --version\n```\n\n## Configuration\n\nNo configuration files are required. All settings are controlled via command-line flags.\n\n### Environment Variable Expansion\n\nGlob patterns support:\n- **Home directory expansion**: `~` expands to your home directory\n- **Environment variables**: `$VAR` or `${VAR}` expand to environment variable values\n- **Relative paths**: Automatically resolved from current working directory\n\n## Usage\n\n### Quick Start\n\n```bash\n# Preview concatenation (dry-run mode, default)\nconcat-glob-tool '*.py' --output-file result.txt\n\n# Execute concatenation\nconcat-glob-tool '*.py' --output-file result.txt --no-dry-run\n\n# Multiple patterns\nconcat-glob-tool '*.py' '*.md' -o combined.txt --no-dry-run\n\n# Recursive patterns\nconcat-glob-tool 'src/**/*.py' -o all-code.txt --no-dry-run\n```\n\n### Complete CLI Reference\n\n```bash\nconcat-glob-tool [PATTERNS...] --output-file FILE [OPTIONS]\nconcat-glob-tool --stdin --output-file FILE [OPTIONS]\n```\n\n#### Arguments\n\n- `PATTERNS...` - One or more glob patterns (e.g., `*.py`, `src/**/*.md`)\n\n#### Options\n\n| Option | Short | Description | Default |\n|--------|-------|-------------|---------|\n| `--output-file` | `-o` | Output file path (required) | - |\n| `--stdin` | `-s` | Read file paths from stdin | False |\n| `--separator` | - | Separator text between files | `---` |\n| `--dry-run` | `-n` | Preview without writing | True |\n| `--no-dry-run` | - | Execute the concatenation | False |\n| `--force` | `-f` | Overwrite existing output file | False |\n| `--verbose` | `-v` | Enable verbose output (repeatable) | 0 |\n| `--version` | - | Show version | - |\n| `--help` | - | Show help message | - |\n\n### Examples\n\n#### Basic Concatenation\n\n```bash\n# Concatenate all Python files in current directory\nconcat-glob-tool '*.py' -o output.txt --no-dry-run\n\n# Concatenate multiple file types\nconcat-glob-tool '*.py' '*.md' '*.txt' -o combined.txt --no-dry-run\n```\n\n#### Recursive Patterns\n\n```bash\n# All Python files in src/ and subdirectories\nconcat-glob-tool 'src/**/*.py' -o all-code.txt --no-dry-run\n\n# Multiple recursive patterns\nconcat-glob-tool 'src/**/*.py' 'tests/**/*.py' -o codebase.txt --no-dry-run\n```\n\n#### Stdin Integration\n\n```bash\n# Use find to locate files\nfind . -name '*.py' -type f | concat-glob-tool --stdin -o output.txt --no-dry-run\n\n# Use fd (faster find alternative)\nfd -e py | concat-glob-tool --stdin -o output.txt --no-dry-run\n\n# Filter with grep\nfind . -name '*.py' | grep -v test | concat-glob-tool --stdin -o output.txt --no-dry-run\n```\n\n#### Custom Separators\n\n```bash\n# Custom separator text\nconcat-glob-tool '*.py' -o output.txt --separator '====' --no-dry-run\n\n# Simple separator\nconcat-glob-tool '*.py' -o output.txt --separator '---' --no-dry-run\n```\n\n#### Dry-Run and Force\n\n```bash\n# Preview without writing (default)\nconcat-glob-tool '*.py' -o output.txt\n\n# Preview with verbose output\nconcat-glob-tool '*.py' -o output.txt -v\n\n# Execute concatenation\nconcat-glob-tool '*.py' -o output.txt --no-dry-run\n\n# Overwrite existing file\nconcat-glob-tool '*.py' -o existing.txt --force --no-dry-run\n```\n\n#### LLM Context Generation\n\n```bash\n# Create context file for Claude Code\nconcat-glob-tool 'src/**/*.py' 'tests/**/*.py' '*.md' \\\n    -o llm-context.txt \\\n    --separator '---' \\\n    --no-dry-run\n\n# Create documentation bundle\nconcat-glob-tool 'docs/**/*.md' 'README.md' 'CLAUDE.md' \\\n    -o full-docs.txt \\\n    --no-dry-run\n```\n\n### Output Format\n\nFiles are concatenated with this separator format between each file (except before the first file):\n\n```\n---\n# /path/to/file.py\n---\n[file contents]\n```\n\nThe separator includes:\n- Blank line before separator\n- Separator line (`---` by default)\n- Comment line with full file path\n- Separator line\n- Blank line before file contents\n\n### Error Handling\n\nThe tool provides clear error messages with solutions:\n\n```bash\n# No matches\n$ concat-glob-tool '*.nonexistent' -o out.txt --no-dry-run\nError: No files matched the patterns: *.nonexistent\n\nSolution: Verify glob patterns are correct. Examples:\n  - '*.py' for Python files in current directory\n  - '**/*.py' for Python files recursively\n\n# Output exists\n$ concat-glob-tool '*.py' -o existing.txt --no-dry-run\nError: Output file already exists: existing.txt\n\nSolution: Use --force to overwrite or choose a different output file.\n\n# Invalid stdin usage\n$ concat-glob-tool '*.py' --stdin -o out.txt\nError: Cannot use both --stdin and glob patterns.\n\nSolution: Use either --stdin OR provide glob patterns, not both.\n```\n\n## Library Usage\n\n`concat-glob-tool` can also be used as a Python library for programmatic integration.\n\n### Installation as Library\n\n```bash\npip install concat-glob-tool\n# or\nuv add concat-glob-tool\n```\n\n### Core API\n\n```python\nfrom concat_glob_tool import (\n    expand_glob_patterns,\n    concatenate_files,\n    format_separator,\n    read_paths_from_stdin,\n    ConcatError,\n    NoMatchesError,\n    OutputExistsError,\n)\nfrom pathlib import Path\n\n# Expand glob patterns\nfiles = expand_glob_patterns([\"*.py\", \"src/**/*.md\"])\nprint(f\"Found {len(files)} files\")\n\n# Concatenate files\nresult = concatenate_files(\n    files=files,\n    output_file=Path(\"output.txt\"),\n    separator=\"---\",\n    force=False,\n    dry_run=False,\n)\n\nprint(f\"Concatenated {result['files_count']} files\")\nprint(f\"Wrote {result['bytes_written']} bytes to {result['output_file']}\")\n```\n\n### Exception Handling\n\n```python\nfrom pathlib import Path\nfrom concat_glob_tool import (\n    concatenate_files,\n    expand_glob_patterns,\n    NoMatchesError,\n    OutputExistsError,\n    ConcatError,\n)\n\ntry:\n    # Expand patterns\n    files = expand_glob_patterns([\"*.py\"])\n\n    # Concatenate\n    result = concatenate_files(\n        files=files,\n        output_file=Path(\"output.txt\"),\n        force=False,\n        dry_run=False,\n    )\n\n    print(f\"Success: {result}\")\n\nexcept NoMatchesError as e:\n    print(f\"No files found: {e}\")\nexcept OutputExistsError as e:\n    print(f\"Output exists: {e}\")\nexcept ConcatError as e:\n    print(f\"Concatenation error: {e}\")\n```\n\n### Custom Separator Formatting\n\n```python\nfrom concat_glob_tool import format_separator\n\n# Format separator with filename\nsep = format_separator(\"myfile.py\", \"---\")\nprint(sep)\n# Output:\n# \\n---\\n# myfile.py\\n---\\n\n```\n\n### Reading from Stdin (Programmatic)\n\n```python\nimport sys\nfrom concat_glob_tool import read_paths_from_stdin\nfrom pathlib import Path\n\n# Simulate stdin with file paths\nsys.stdin = open(\"file_list.txt\", \"r\")\npaths = read_paths_from_stdin()\n\nfor path in paths:\n    print(f\"File: {path}\")\n```\n\n### Integration Example\n\n```python\n#!/usr/bin/env python3\n\"\"\"Example: Concatenate Python files with custom logic.\"\"\"\n\nfrom pathlib import Path\nfrom concat_glob_tool import expand_glob_patterns, concatenate_files\n\ndef main():\n    # Find all Python files\n    files = expand_glob_patterns([\"src/**/*.py\", \"tests/**/*.py\"])\n\n    # Filter files (e.g., exclude __init__.py)\n    filtered_files = [f for f in files if f.name != \"__init__.py\"]\n\n    # Concatenate\n    result = concatenate_files(\n        files=filtered_files,\n        output_file=Path(\"codebase.txt\"),\n        separator=\"===\",\n        force=True,\n        dry_run=False,\n    )\n\n    print(f\"‚úÖ Concatenated {result['files_count']} files\")\n    print(f\"üìù Output: {result['output_file']}\")\n    print(f\"üíæ Size: {result['bytes_written']} bytes\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### API Reference\n\n#### `expand_glob_patterns(patterns: list[str]) -> list[Path]`\n\nExpand glob patterns to list of file paths.\n\n**Parameters:**\n- `patterns`: List of glob patterns (e.g., `[\"*.py\", \"**/*.md\"]`)\n\n**Returns:**\n- Sorted list of unique file paths\n\n**Raises:**\n- `NoMatchesError`: If no files match any patterns\n\n#### `concatenate_files(files, output_file, separator=\"---\", force=False, dry_run=True) -> dict`\n\nConcatenate files to output file with separator.\n\n**Parameters:**\n- `files`: List of `Path` objects to concatenate\n- `output_file`: Output file path (`Path` object)\n- `separator`: Separator text (default: `\"---\"`)\n- `force`: Overwrite existing output file (default: `False`)\n- `dry_run`: Preview without writing (default: `True`)\n\n**Returns:**\n- Dictionary with keys:\n  - `files_count`: Number of files processed\n  - `output_file`: Output file path as string\n  - `bytes_written`: Total bytes written (0 in dry-run mode)\n\n**Raises:**\n- `OutputExistsError`: If output exists and `force=False`\n\n#### `format_separator(filename: str, separator_text: str) -> str`\n\nFormat the separator with filename.\n\n**Parameters:**\n- `filename`: Name of the file\n- `separator_text`: Base separator text\n\n**Returns:**\n- Formatted separator string: `\\n---\\n# FILENAME\\n---\\n`\n\n#### `read_paths_from_stdin() -> list[Path]`\n\nRead file paths from stdin (one per line).\n\n**Returns:**\n- List of file paths\n\n**Raises:**\n- `NoMatchesError`: If no valid paths provided\n- `ConcatError`: If a path doesn't exist or isn't a file\n\n## Multi-Level Verbosity Logging\n\nThe CLI supports progressive verbosity levels for debugging and troubleshooting. All logs output to stderr, keeping stdout clean for data piping.\n\n### Logging Levels\n\n| Flag | Level | Output | Use Case |\n|------|-------|--------|----------|\n| (none) | WARNING | Errors and warnings only | Production, quiet mode |\n| `-v` | INFO | + High-level operations | Normal debugging |\n| `-vv` | DEBUG | + Detailed info, full tracebacks | Development, troubleshooting |\n| `-vvv` | TRACE | + Library internals | Deep debugging |\n\n### Examples\n\n```bash\n# Quiet mode - only errors and warnings\nconcat-glob-tool\n\n# INFO - see operations and progress\nconcat-glob-tool -v\n# Output:\n# [INFO] concat-glob-tool started\n# [INFO] concat-glob-tool completed\n\n# DEBUG - see detailed information\nconcat-glob-tool -vv\n# Output:\n# [INFO] concat-glob-tool started\n# [DEBUG] Running with verbose level: 2\n# [INFO] concat-glob-tool completed\n\n# TRACE - see library internals (configure in logging_config.py)\nconcat-glob-tool -vvv\n```\n\n### Customizing Library Logging\n\nTo enable DEBUG logging for third-party libraries at TRACE level (-vvv), edit `concat_glob_tool/logging_config.py`:\n\n```python\n# Configure dependent library loggers at TRACE level (-vvv)\nif verbose_count >= 3:\n    logging.getLogger(\"requests\").setLevel(logging.DEBUG)\n    logging.getLogger(\"urllib3\").setLevel(logging.DEBUG)\n    # Add your project-specific library loggers here\n```\n\n## Shell Completion\n\nThe CLI provides native shell completion for bash, zsh, and fish shells.\n\n### Supported Shells\n\n| Shell | Version Requirement | Status |\n|-------|-------------------|--------|\n| **Bash** | ‚â• 4.4 | ‚úÖ Supported |\n| **Zsh** | Any recent version | ‚úÖ Supported |\n| **Fish** | ‚â• 3.0 | ‚úÖ Supported |\n| **PowerShell** | Any version | ‚ùå Not Supported |\n\n### Installation\n\n#### Quick Setup (Temporary)\n\n```bash\n# Bash - active for current session only\neval \"$(concat-glob-tool completion bash)\"\n\n# Zsh - active for current session only\neval \"$(concat-glob-tool completion zsh)\"\n\n# Fish - active for current session only\nconcat-glob-tool completion fish | source\n```\n\n#### Permanent Setup (Recommended)\n\n```bash\n# Bash - add to ~/.bashrc\necho 'eval \"$(concat-glob-tool completion bash)\"' >> ~/.bashrc\nsource ~/.bashrc\n\n# Zsh - add to ~/.zshrc\necho 'eval \"$(concat-glob-tool completion zsh)\"' >> ~/.zshrc\nsource ~/.zshrc\n\n# Fish - save to completions directory\nmkdir -p ~/.config/fish/completions\nconcat-glob-tool completion fish > ~/.config/fish/completions/concat-glob-tool.fish\n```\n\n#### File-based Installation (Better Performance)\n\nFor better shell startup performance, generate completion scripts to files:\n\n```bash\n# Bash\nconcat-glob-tool completion bash > ~/.concat-glob-tool-complete.bash\necho 'source ~/.concat-glob-tool-complete.bash' >> ~/.bashrc\n\n# Zsh\nconcat-glob-tool completion zsh > ~/.concat-glob-tool-complete.zsh\necho 'source ~/.concat-glob-tool-complete.zsh' >> ~/.zshrc\n\n# Fish (automatic loading from completions directory)\nmkdir -p ~/.config/fish/completions\nconcat-glob-tool completion fish > ~/.config/fish/completions/concat-glob-tool.fish\n```\n\n### Usage\n\nOnce installed, completion works automatically:\n\n```bash\n# Tab completion for commands\nconcat-glob-tool <TAB>\n# Shows: completion\n\n# Tab completion for options\nconcat-glob-tool --<TAB>\n# Shows: --verbose --version --help\n\n# Tab completion for shell types\nconcat-glob-tool completion <TAB>\n# Shows: bash zsh fish\n```\n\n### Getting Help\n\n```bash\n# View completion installation instructions\nconcat-glob-tool completion --help\n```\n\n## Development\n\n### Setup Development Environment\n\n```bash\n# Clone repository\ngit clone https://github.com/dnvriend/concat-glob-tool.git\ncd concat-glob-tool\n\n# Install dependencies\nmake install\n\n# Show available commands\nmake help\n```\n\n### Available Make Commands\n\n```bash\nmake install                 # Install dependencies\nmake format                  # Format code with ruff\nmake lint                    # Run linting with ruff\nmake typecheck               # Run type checking with mypy\nmake test                    # Run tests with pytest\nmake security-bandit         # Python security linter\nmake security-pip-audit      # Dependency vulnerability scanner\nmake security-gitleaks       # Secret/API key detection\nmake security                # Run all security checks\nmake check                   # Run all checks (lint, typecheck, test, security)\nmake pipeline                # Run full pipeline (format, lint, typecheck, test, security, build, install-global)\nmake build                   # Build package\nmake run ARGS=\"...\"          # Run concat-glob-tool locally\nmake clean                   # Remove build artifacts\n```\n\n### Project Structure\n\n```\nconcat-glob-tool/\n‚îú‚îÄ‚îÄ concat_glob_tool/    # Main package\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ cli.py          # CLI entry point\n‚îÇ   ‚îî‚îÄ‚îÄ utils.py        # Utility functions\n‚îú‚îÄ‚îÄ tests/              # Test suite\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n‚îú‚îÄ‚îÄ pyproject.toml      # Project configuration\n‚îú‚îÄ‚îÄ Makefile            # Development commands\n‚îú‚îÄ‚îÄ README.md           # This file\n‚îú‚îÄ‚îÄ LICENSE             # MIT License\n‚îî‚îÄ‚îÄ CLAUDE.md           # Development documentation\n```\n\n## Testing\n\nRun the test suite:\n\n```bash\n# Run all tests\nmake test\n\n# Run tests with verbose output\nuv run pytest tests/ -v\n\n# Run specific test file\nuv run pytest tests/test_utils.py\n\n# Run with coverage\nuv run pytest tests/ --cov=concat_glob_tool\n```\n\n## Security\n\nThe project includes lightweight security tools providing 80%+ coverage with fast scan times:\n\n### Security Tools\n\n| Tool | Purpose | Speed | Coverage |\n|------|---------|-------|----------|\n| **bandit** | Python code security linting | ‚ö°‚ö° Fast | SQL injection, hardcoded secrets, unsafe functions |\n| **pip-audit** | Dependency vulnerability scanning | ‚ö°‚ö° Fast | Known CVEs in dependencies |\n| **gitleaks** | Secret and API key detection | ‚ö°‚ö°‚ö° Very Fast | Secrets in code and git history |\n\n### Running Security Scans\n\n```bash\n# Run all security checks (~5-8 seconds)\nmake security\n\n# Or run individually\nmake security-bandit       # Python security linting\nmake security-pip-audit    # Dependency CVE scanning\nmake security-gitleaks     # Secret detection\n```\n\n### Prerequisites\n\ngitleaks must be installed separately:\n\n```bash\n# macOS\nbrew install gitleaks\n\n# Linux\n# See: https://github.com/gitleaks/gitleaks#installation\n```\n\nSecurity checks run automatically in `make check` and `make pipeline`.\n\n### What's Protected\n\n- ‚úÖ AWS credentials (AKIA*, ASIA*, etc.)\n- ‚úÖ GitHub tokens (ghp_*, gho_*, etc.)\n- ‚úÖ API keys and secrets\n- ‚úÖ Private keys\n- ‚úÖ Slack tokens\n- ‚úÖ 100+ other secret types\n\n## Contributing\n\nContributions are welcome! Please follow these guidelines:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run the full pipeline (`make pipeline`)\n5. Commit your changes (`git commit -m 'Add amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\n### Code Style\n\n- Follow PEP 8 guidelines\n- Use type hints for all functions\n- Write docstrings for public functions\n- Format code with `ruff`\n- Pass all linting and type checks\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Author\n\n**Dennis Vriend**\n\n- GitHub: [@dnvriend](https://github.com/dnvriend)\n\n## Acknowledgments\n\n- Built with [Click](https://click.palletsprojects.com/) for CLI framework\n- Developed with [uv](https://github.com/astral-sh/uv) for fast Python tooling\n\n---\n\n**Generated with AI**\n\nThis project was generated using [Claude Code](https://www.anthropic.com/claude/code), an AI-powered development tool by [Anthropic](https://www.anthropic.com/). Claude Code assisted in creating the project structure, implementation, tests, documentation, and development tooling.\n\nMade with ‚ù§Ô∏è using Python 3.14\n"
      },
      "plugins": [
        {
          "name": "concat-glob-tool",
          "source": "./plugins/concat-glob-tool",
          "description": "Concatenate files with glob patterns and separators",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add dnvriend/concat-glob-tool",
            "/plugin install concat-glob-tool@concat-glob-tool"
          ]
        }
      ]
    },
    {
      "name": "pdf-to-pptx-tool",
      "version": null,
      "description": "Convert PDF documents to PowerPoint presentations",
      "repo_full_name": "dnvriend/pdf-to-pptx-tool",
      "repo_url": "https://github.com/dnvriend/pdf-to-pptx-tool",
      "repo_description": "Professional CLI tool that converts PDF documents into PowerPoint presentations",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2025-12-05T06:47:23Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"pdf-to-pptx-tool\",\n    \"owner\": {\n        \"name\": \"Dennis Vriend\",\n        \"email\": \"dvriend@ilionx.com\",\n        \"url\": \"https://github.com/dnvriend/pdf-to-pptx-tool\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"pdf-to-pptx-tool\",\n            \"source\": \"./plugins/pdf-to-pptx-tool\",\n            \"description\": \"Convert PDF documents to PowerPoint presentations\"\n        }\n    ]\n}\n",
        "README.md": "# pdf-to-pptx-tool\n\n<div align=\"center\">\n  <img src=\".github/assets/logo-web.png\" alt=\"pdf-to-pptx-tool logo\" width=\"200\"/>\n\n  [![Python Version](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)\n  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n  [![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n  [![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](https://github.com/python/mypy)\n  [![AI Generated](https://img.shields.io/badge/AI-Generated-blueviolet.svg)](https://www.anthropic.com/claude)\n  [![Built with Claude Code](https://img.shields.io/badge/Built_with-Claude_Code-5A67D8.svg)](https://www.anthropic.com/claude/code)\n</div>\n\nA professional CLI tool that converts PDF documents into PowerPoint presentations\n\n## Table of Contents\n\n- [About](#about)\n- [Features](#features)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Convert Command](#convert-command)\n- [Multi-Level Verbosity Logging](#multi-level-verbosity-logging)\n- [Shell Completion](#shell-completion)\n- [Development](#development)\n- [Testing](#testing)\n- [Security](#security)\n- [Contributing](#contributing)\n- [License](#license)\n- [Author](#author)\n\n## About\n\n`pdf-to-pptx-tool` is a Python CLI tool built with modern tooling and best practices.\n\n## Features\n\n- üìÑ **PDF to PowerPoint Conversion**: Convert PDF documents to PPTX format\n- üé® **Customizable Quality**: Adjustable DPI (72-600) for quality vs file size\n- üìê **16:9 Slides**: Professional widescreen format (10\" √ó 5.625\")\n- üñºÔ∏è  **Full-Page Images**: Each PDF page becomes a full-slide image\n- üìä **Multi-level Verbosity**: Progressive logging (-v/-vv/-vvv) for debugging\n- üêö **Shell Completion**: Native completion for bash, zsh, and fish\n- ‚úÖ **Type-Safe**: Strict mypy checking for reliability\n- üîí **Security Scanned**: bandit, pip-audit, and gitleaks\n- ‚ö° **Modern Tooling**: Built with Click, uv, and Python 3.14+\n\n## Installation\n\n### Prerequisites\n\n- Python 3.14 or higher\n- [uv](https://github.com/astral-sh/uv) package manager\n- **poppler** system library (for PDF rendering)\n  ```bash\n  # macOS\n  brew install poppler\n\n  # Ubuntu/Debian\n  sudo apt-get install poppler-utils\n\n  # Fedora\n  sudo dnf install poppler-utils\n  ```\n\n### Install from source\n\n```bash\n# Clone the repository\ngit clone https://github.com/dnvriend/pdf-to-pptx-tool.git\ncd pdf-to-pptx-tool\n\n# Install globally with uv\nuv tool install .\n```\n\n### Install with mise (recommended for development)\n\n```bash\ncd pdf-to-pptx-tool\nmise trust\nmise install\nuv sync\nuv tool install .\n```\n\n### Verify installation\n\n```bash\npdf-to-pptx-tool --version\n```\n\n## Usage\n\n### Convert PDF to PowerPoint\n\n```bash\n# Basic conversion (default 200 DPI)\npdf-to-pptx-tool convert document.pdf slides.pptx\n\n# High quality conversion (300 DPI)\npdf-to-pptx-tool convert report.pdf presentation.pptx --dpi 300\n\n# With verbose logging to see progress\npdf-to-pptx-tool -v convert input.pdf output.pptx\n\n# With debug logging for troubleshooting\npdf-to-pptx-tool -vv convert problematic.pdf fixed.pptx\n```\n\n### Show Help\n\n```bash\n# General help\npdf-to-pptx-tool --help\n\n# Convert command help\npdf-to-pptx-tool convert --help\n\n# Completion command help\npdf-to-pptx-tool completion --help\n\n# Show version\npdf-to-pptx-tool --version\n```\n\n## Convert Command\n\nThe `convert` command transforms PDF documents into PowerPoint presentations.\n\n### Syntax\n\n```bash\npdf-to-pptx-tool convert INPUT_PDF OUTPUT_PPTX [OPTIONS]\n```\n\n### Arguments\n\n- `INPUT_PDF`: Path to the input PDF file (required)\n- `OUTPUT_PPTX`: Path to the output PowerPoint file (required)\n- `--dpi INTEGER`: Resolution for conversion (default: 200)\n  - Range: 72-600 DPI\n  - Higher DPI = better quality but larger files\n  - Recommended: 200-300 for most presentations\n\n### DPI Quality Guidelines\n\n| DPI | Quality | File Size | Best For |\n|-----|---------|-----------|----------|\n| 72 | Low | Smallest | Quick previews, draft slides |\n| 150 | Medium | Small | Web presentations, email |\n| **200** | **Good** | **Medium** | **Default - recommended for most** |\n| 300 | High | Large | Print quality, detailed diagrams |\n| 600 | Very High | Very Large | Professional print, posters |\n\n### Examples\n\n```bash\n# Basic conversion with default 200 DPI\npdf-to-pptx-tool convert quarterly-report.pdf q4-presentation.pptx\n\n# High quality for detailed diagrams\npdf-to-pptx-tool convert technical-doc.pdf slides.pptx --dpi 300\n\n# Quick preview with lower quality\npdf-to-pptx-tool convert draft.pdf preview.pptx --dpi 150\n\n# Batch conversion\nfor pdf in *.pdf; do\n  pdf-to-pptx-tool convert \"$pdf\" \"${pdf%.pdf}.pptx\"\ndone\n```\n\n### Output Format\n\n- **Aspect Ratio**: 16:9 widescreen\n- **Slide Size**: 10 inches √ó 5.625 inches\n- **Layout**: One full-slide image per PDF page\n- **Image Format**: PNG embedded in slides\n- **Compatibility**: PowerPoint 2007+ (Windows/Mac/Online)\n\n## Multi-Level Verbosity Logging\n\nThe CLI supports progressive verbosity levels for debugging and troubleshooting. All logs output to stderr, keeping stdout clean for data piping.\n\n### Logging Levels\n\n| Flag | Level | Output | Use Case |\n|------|-------|--------|----------|\n| (none) | WARNING | Errors and warnings only | Production, quiet mode |\n| `-v` | INFO | + High-level operations | Normal debugging |\n| `-vv` | DEBUG | + Detailed info, full tracebacks | Development, troubleshooting |\n| `-vvv` | TRACE | + Library internals | Deep debugging |\n\n### Examples\n\n```bash\n# Quiet mode - only errors and warnings\npdf-to-pptx-tool\n\n# INFO - see operations and progress\npdf-to-pptx-tool -v\n# Output:\n# [INFO] pdf-to-pptx-tool started\n# [INFO] pdf-to-pptx-tool completed\n\n# DEBUG - see detailed information\npdf-to-pptx-tool -vv\n# Output:\n# [INFO] pdf-to-pptx-tool started\n# [DEBUG] Running with verbose level: 2\n# [INFO] pdf-to-pptx-tool completed\n\n# TRACE - see library internals (configure in logging_config.py)\npdf-to-pptx-tool -vvv\n```\n\n### Customizing Library Logging\n\nTo enable DEBUG logging for third-party libraries at TRACE level (-vvv), edit `pdf_to_pptx_tool/logging_config.py`:\n\n```python\n# Configure dependent library loggers at TRACE level (-vvv)\nif verbose_count >= 3:\n    logging.getLogger(\"requests\").setLevel(logging.DEBUG)\n    logging.getLogger(\"urllib3\").setLevel(logging.DEBUG)\n    # Add your project-specific library loggers here\n```\n\n## Shell Completion\n\nThe CLI provides native shell completion for bash, zsh, and fish shells.\n\n### Supported Shells\n\n| Shell | Version Requirement | Status |\n|-------|-------------------|--------|\n| **Bash** | ‚â• 4.4 | ‚úÖ Supported |\n| **Zsh** | Any recent version | ‚úÖ Supported |\n| **Fish** | ‚â• 3.0 | ‚úÖ Supported |\n| **PowerShell** | Any version | ‚ùå Not Supported |\n\n### Installation\n\n#### Quick Setup (Temporary)\n\n```bash\n# Bash - active for current session only\neval \"$(pdf-to-pptx-tool completion bash)\"\n\n# Zsh - active for current session only\neval \"$(pdf-to-pptx-tool completion zsh)\"\n\n# Fish - active for current session only\npdf-to-pptx-tool completion fish | source\n```\n\n#### Permanent Setup (Recommended)\n\n```bash\n# Bash - add to ~/.bashrc\necho 'eval \"$(pdf-to-pptx-tool completion bash)\"' >> ~/.bashrc\nsource ~/.bashrc\n\n# Zsh - add to ~/.zshrc\necho 'eval \"$(pdf-to-pptx-tool completion zsh)\"' >> ~/.zshrc\nsource ~/.zshrc\n\n# Fish - save to completions directory\nmkdir -p ~/.config/fish/completions\npdf-to-pptx-tool completion fish > ~/.config/fish/completions/pdf-to-pptx-tool.fish\n```\n\n#### File-based Installation (Better Performance)\n\nFor better shell startup performance, generate completion scripts to files:\n\n```bash\n# Bash\npdf-to-pptx-tool completion bash > ~/.pdf-to-pptx-tool-complete.bash\necho 'source ~/.pdf-to-pptx-tool-complete.bash' >> ~/.bashrc\n\n# Zsh\npdf-to-pptx-tool completion zsh > ~/.pdf-to-pptx-tool-complete.zsh\necho 'source ~/.pdf-to-pptx-tool-complete.zsh' >> ~/.zshrc\n\n# Fish (automatic loading from completions directory)\nmkdir -p ~/.config/fish/completions\npdf-to-pptx-tool completion fish > ~/.config/fish/completions/pdf-to-pptx-tool.fish\n```\n\n### Usage\n\nOnce installed, completion works automatically:\n\n```bash\n# Tab completion for commands\npdf-to-pptx-tool <TAB>\n# Shows: completion\n\n# Tab completion for options\npdf-to-pptx-tool --<TAB>\n# Shows: --verbose --version --help\n\n# Tab completion for shell types\npdf-to-pptx-tool completion <TAB>\n# Shows: bash zsh fish\n```\n\n### Getting Help\n\n```bash\n# View completion installation instructions\npdf-to-pptx-tool completion --help\n```\n\n## Development\n\n### Setup Development Environment\n\n```bash\n# Clone repository\ngit clone https://github.com/dnvriend/pdf-to-pptx-tool.git\ncd pdf-to-pptx-tool\n\n# Install dependencies\nmake install\n\n# Show available commands\nmake help\n```\n\n### Available Make Commands\n\n```bash\nmake install                 # Install dependencies\nmake format                  # Format code with ruff\nmake lint                    # Run linting with ruff\nmake typecheck               # Run type checking with mypy\nmake test                    # Run tests with pytest\nmake security-bandit         # Python security linter\nmake security-pip-audit      # Dependency vulnerability scanner\nmake security-gitleaks       # Secret/API key detection\nmake security                # Run all security checks\nmake check                   # Run all checks (lint, typecheck, test, security)\nmake pipeline                # Run full pipeline (format, lint, typecheck, test, security, build, install-global)\nmake build                   # Build package\nmake run ARGS=\"...\"          # Run pdf-to-pptx-tool locally\nmake clean                   # Remove build artifacts\n```\n\n### Project Structure\n\n```\npdf-to-pptx-tool/\n‚îú‚îÄ‚îÄ pdf_to_pptx_tool/    # Main package\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ cli.py          # CLI entry point\n‚îÇ   ‚îî‚îÄ‚îÄ utils.py        # Utility functions\n‚îú‚îÄ‚îÄ tests/              # Test suite\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py\n‚îú‚îÄ‚îÄ pyproject.toml      # Project configuration\n‚îú‚îÄ‚îÄ Makefile            # Development commands\n‚îú‚îÄ‚îÄ README.md           # This file\n‚îú‚îÄ‚îÄ LICENSE             # MIT License\n‚îî‚îÄ‚îÄ CLAUDE.md           # Development documentation\n```\n\n## Testing\n\nRun the test suite:\n\n```bash\n# Run all tests\nmake test\n\n# Run tests with verbose output\nuv run pytest tests/ -v\n\n# Run specific test file\nuv run pytest tests/test_utils.py\n\n# Run with coverage\nuv run pytest tests/ --cov=pdf_to_pptx_tool\n```\n\n## Security\n\nThe project includes lightweight security tools providing 80%+ coverage with fast scan times:\n\n### Security Tools\n\n| Tool | Purpose | Speed | Coverage |\n|------|---------|-------|----------|\n| **bandit** | Python code security linting | ‚ö°‚ö° Fast | SQL injection, hardcoded secrets, unsafe functions |\n| **pip-audit** | Dependency vulnerability scanning | ‚ö°‚ö° Fast | Known CVEs in dependencies |\n| **gitleaks** | Secret and API key detection | ‚ö°‚ö°‚ö° Very Fast | Secrets in code and git history |\n\n### Running Security Scans\n\n```bash\n# Run all security checks (~5-8 seconds)\nmake security\n\n# Or run individually\nmake security-bandit       # Python security linting\nmake security-pip-audit    # Dependency CVE scanning\nmake security-gitleaks     # Secret detection\n```\n\n### Prerequisites\n\ngitleaks must be installed separately:\n\n```bash\n# macOS\nbrew install gitleaks\n\n# Linux\n# See: https://github.com/gitleaks/gitleaks#installation\n```\n\nSecurity checks run automatically in `make check` and `make pipeline`.\n\n### What's Protected\n\n- ‚úÖ AWS credentials (AKIA*, ASIA*, etc.)\n- ‚úÖ GitHub tokens (ghp_*, gho_*, etc.)\n- ‚úÖ API keys and secrets\n- ‚úÖ Private keys\n- ‚úÖ Slack tokens\n- ‚úÖ 100+ other secret types\n\n## Contributing\n\nContributions are welcome! Please follow these guidelines:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run the full pipeline (`make pipeline`)\n5. Commit your changes (`git commit -m 'Add amazing feature'`)\n6. Push to the branch (`git push origin feature/amazing-feature`)\n7. Open a Pull Request\n\n### Code Style\n\n- Follow PEP 8 guidelines\n- Use type hints for all functions\n- Write docstrings for public functions\n- Format code with `ruff`\n- Pass all linting and type checks\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Author\n\n**Dennis Vriend**\n\n- GitHub: [@dnvriend](https://github.com/dnvriend)\n\n## Acknowledgments\n\n- Built with [Click](https://click.palletsprojects.com/) for CLI framework\n- Developed with [uv](https://github.com/astral-sh/uv) for fast Python tooling\n\n---\n\n**Generated with AI**\n\nThis project was generated using [Claude Code](https://www.anthropic.com/claude/code), an AI-powered development tool by [Anthropic](https://www.anthropic.com/). Claude Code assisted in creating the project structure, implementation, tests, documentation, and development tooling.\n\nMade with ‚ù§Ô∏è using Python 3.14\n"
      },
      "plugins": [
        {
          "name": "pdf-to-pptx-tool",
          "source": "./plugins/pdf-to-pptx-tool",
          "description": "Convert PDF documents to PowerPoint presentations",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add dnvriend/pdf-to-pptx-tool",
            "/plugin install pdf-to-pptx-tool@pdf-to-pptx-tool"
          ]
        }
      ]
    },
    {
      "name": "slack-messaging-tool",
      "version": null,
      "description": "Marketplace for slack-messaging-tool CLI plugin - A Python CLI tool",
      "repo_full_name": "dnvriend/slack-messaging-tool",
      "repo_url": "https://github.com/dnvriend/slack-messaging-tool",
      "repo_description": "slack-messaging-tool",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-26T07:42:46Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"slack-messaging-tool\",\n    \"owner\": {\n        \"name\": \"Dennis Vriend\",\n        \"email\": \"dvriend@ilionx.com\",\n        \"url\": \"https://github.com/dnvriend\"\n    },\n    \"metadata\": {\n        \"description\": \"Marketplace for slack-messaging-tool CLI plugin - A Python CLI tool\",\n        \"version\": \"0.1.0\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"slack-messaging-tool\",\n            \"source\": \"./plugins/slack-messaging-tool\",\n            \"description\": \"A Python CLI tool\"\n        }\n    ]\n}\n",
        "README.md": "# slack-messaging-tool\n\n<p align=\"center\">\n  <img src=\".github/assets/logo.png\" alt=\"slack-messaging-tool logo\" width=\"128\">\n</p>\n\n[![Python Version](https://img.shields.io/badge/python-3.14+-blue.svg)](https://www.python.org/downloads/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Code style: ruff](https://img.shields.io/badge/code%20style-ruff-000000.svg)](https://github.com/astral-sh/ruff)\n[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](https://github.com/python/mypy)\n\nA CLI tool that provides Slack messaging capabilities via the Slack API.\n\n## Features\n\n- Send messages to Slack channels (by name or ID)\n- Support for Slack Block Kit rich layouts\n- List available channels in workspace\n- Bot token authentication\n- Multi-level verbosity logging\n- OpenTelemetry observability\n- Shell completion (bash, zsh, fish)\n\n## Installation\n\n```bash\n# Clone and install\ngit clone https://github.com/dnvriend/slack-messaging-tool.git\ncd slack-messaging-tool\nuv tool install .\n\n# Verify\nslack-messaging-tool --version\n```\n\n## Usage\n\n```bash\n# Set your bot token\nexport SLACK_BOT_TOKEN=xoxb-your-bot-token\n\n# List channels\nslack-messaging-tool channels\n\n# Send a message\nslack-messaging-tool send -c general -m \"Hello team!\"\n\n# Send with Block Kit\nslack-messaging-tool send -c general -m \"Fallback\" \\\n    --blocks '[{\"type\":\"section\",\"text\":{\"type\":\"mrkdwn\",\"text\":\"*Bold*\"}}]'\n\n# Enable verbose logging\nslack-messaging-tool -v channels\n```\n\n## Environment Variables\n\n| Variable | Required | Default | Description |\n|----------|----------|---------|-------------|\n| `SLACK_BOT_TOKEN` | Yes | - | Slack bot token (xoxb-...) |\n| `OTEL_ENABLED` | No | `false` | Enable OpenTelemetry |\n| `OTEL_EXPORTER_TYPE` | No | `console` | `console` or `otlp` |\n| `OTEL_EXPORTER_OTLP_ENDPOINT` | No | `http://localhost:4317` | OTLP endpoint |\n| `LOG_FILE` | No | - | Path to log file |\n\n## Documentation\n\n- [Logging](references/logging.md) - Multi-level verbosity logging\n- [Telemetry](references/telemetry.md) - OpenTelemetry observability\n- [Shell Completion](references/shell-completion.md) - Tab completion setup\n- [Development](references/development.md) - Contributing guide\n\n## Development\n\n```bash\nmake install    # Install dependencies\nmake test       # Run tests\nmake pipeline   # Full CI pipeline\n```\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.\n\n## Author\n\n**Dennis Vriend** - [@dnvriend](https://github.com/dnvriend)\n"
      },
      "plugins": [
        {
          "name": "slack-messaging-tool",
          "source": "./plugins/slack-messaging-tool",
          "description": "A Python CLI tool",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add dnvriend/slack-messaging-tool",
            "/plugin install slack-messaging-tool@slack-messaging-tool"
          ]
        }
      ]
    }
  ]
}