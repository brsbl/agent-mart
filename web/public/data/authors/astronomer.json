{
  "author": {
    "id": "astronomer",
    "display_name": "Astronomer",
    "avatar_url": "https://avatars.githubusercontent.com/u/12449437?v=4"
  },
  "marketplaces": [
    {
      "name": "astronomer",
      "version": null,
      "description": "Data engineering plugin - warehouse exploration, pipeline authoring, Airflow integration",
      "repo_full_name": "astronomer/agents",
      "repo_url": "https://github.com/astronomer/agents",
      "repo_description": "AI agent tooling for data engineering workflows.",
      "signals": {
        "stars": 132,
        "forks": 12,
        "pushed_at": "2026-01-31T14:55:25Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"astronomer\",\n  \"owner\": {\n    \"name\": \"Astronomer\",\n    \"email\": \"support@astronomer.io\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"data\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"description\": \"Data engineering plugin - warehouse exploration, pipeline authoring, Airflow integration\",\n      \"version\": \"0.1.0\",\n      \"author\": {\n        \"name\": \"Astronomer\",\n        \"email\": \"support@astronomer.io\"\n      },\n      \"homepage\": \"https://github.com/astronomer/agents\",\n      \"repository\": \"https://github.com/astronomer/agents\",\n      \"keywords\": [\"data-engineering\", \"airflow\", \"snowflake\", \"bigquery\", \"jupyter\", \"astronomer\"],\n      \"hooks\": {\n        \"UserPromptSubmit\": [\n          {\n            \"hooks\": [\n              {\n                \"type\": \"command\",\n                \"command\": \"${CLAUDE_PLUGIN_ROOT}/skills/airflow/hooks/airflow-skill-suggester.sh\"\n              }\n            ]\n          }\n        ]\n      }\n    }\n  ]\n}\n",
        "README.md": "# agents\n\nAI agent tooling for data engineering workflows. Extends code agents and IDEs like [Claude Code](https://docs.anthropic.com/en/docs/claude-code) and [Cursor](https://cursor.com) with specialized capabilities for working with Airflow and data warehouses.\n\nBuilt by [Astronomer](https://www.astronomer.io/).\n\n## Table of Contents\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [Installation](#installation)\n  - [Quick Start](#quick-start)\n  - [Compatibility](#compatibility)\n  - [Claude Code](#claude-code)\n  - [Cursor](#cursor)\n  - [Other MCP Clients](#other-mcp-clients)\n- [Features](#features)\n  - [MCP Server](#mcp-server)\n  - [Skills](#skills)\n  - [User Journeys](#user-journeys)\n- [Configuration](#configuration)\n  - [Warehouse Connections](#warehouse-connections)\n  - [Airflow](#airflow)\n- [Usage](#usage)\n  - [Getting Started](#getting-started)\n- [Development](#development)\n  - [Local Development Setup](#local-development-setup)\n  - [Adding Skills](#adding-skills)\n- [Troubleshooting](#troubleshooting)\n  - [Common Issues](#common-issues)\n- [Contributing](#contributing)\n- [License](#license)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Installation\n\n### Quick Start\n\n```bash\nnpx skills add astronomer/agents\n```\n\nThis installs Astronomer skills into your project via [skills.sh](https://skills.sh). Works with Claude Code, Cursor, and other AI coding tools.\n\n### Compatibility\n\n**Skills:** Works with [25+ AI coding agents](https://github.com/vercel-labs/add-skill?tab=readme-ov-file#available-agents) including Claude Code, Cursor, VS Code (GitHub Copilot), Windsurf, Cline, and more.\n\n**MCP Server:** Works with any [MCP-compatible client](https://modelcontextprotocol.io/clients) including Claude Desktop, VS Code, and others.\n\n### Claude Code\n\n```bash\n# Add the marketplace and install the plugin\nclaude plugin marketplace add astronomer/agents\nclaude plugin install data@astronomer\n```\n\nThe plugin includes the Airflow MCP server that runs via `uvx` from PyPI. Data warehouse queries are handled by the `analyzing-data` skill using a background Jupyter kernel.\n\n### Cursor\n\nCursor supports both MCP servers and skills.\n\n**MCP Server** - Click to install:\n\n<a href=\"https://cursor.com/en-US/install-mcp?name=astro-airflow-mcp&config=eyJjb21tYW5kIjoidXZ4IiwiYXJncyI6WyJhc3Ryby1haXJmbG93LW1jcCIsIi0tdHJhbnNwb3J0Iiwic3RkaW8iXX0\"><img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Add Airflow MCP to Cursor\" height=\"32\"></a>\n\n**Skills** - Install to your project:\n\n```bash\nnpx skills add astronomer/agents\n```\n\nThis installs skills to `.cursor/skills/` in your project.\n\n<details>\n<summary>Manual MCP configuration</summary>\n\nAdd to `~/.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"airflow\": {\n      \"command\": \"uvx\",\n      \"args\": [\"astro-airflow-mcp\", \"--transport\", \"stdio\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>Enable hooks (skill suggestions, session management)</summary>\n\nCreate `.cursor/hooks.json` in your project:\n\n```json\n{\n  \"version\": 1,\n  \"hooks\": {\n    \"beforeSubmitPrompt\": [\n      {\n        \"command\": \"$CURSOR_PROJECT_DIR/.cursor/skills/airflow/hooks/airflow-skill-suggester.sh\",\n        \"timeout\": 5\n      }\n    ],\n    \"stop\": [\n      {\n        \"command\": \"uv run $CURSOR_PROJECT_DIR/.cursor/skills/analyzing-data/scripts/cli.py stop\",\n        \"timeout\": 10\n      }\n    ]\n  }\n}\n```\n\n**What these hooks do:**\n- `beforeSubmitPrompt`: Suggests data skills when you mention Airflow keywords\n- `stop`: Cleans up kernel when session ends\n\n</details>\n\n### Other MCP Clients\n\nFor any MCP-compatible client (Claude Desktop, VS Code, etc.):\n\n```bash\n# Airflow MCP\nuvx astro-airflow-mcp --transport stdio\n\n# With remote Airflow\nAIRFLOW_API_URL=https://your-airflow.example.com \\\nAIRFLOW_USERNAME=admin \\\nAIRFLOW_PASSWORD=admin \\\nuvx astro-airflow-mcp --transport stdio\n```\n\n## Features\n\nThe `data` plugin bundles an MCP server and skills into a single installable package.\n\n### MCP Server\n\n| Server | Description |\n|--------|-------------|\n| **[Airflow](https://github.com/astronomer/agents/tree/main/astro-airflow-mcp)** | Full Airflow REST API integration via [astro-airflow-mcp](https://github.com/astronomer/agents/tree/main/astro-airflow-mcp): DAG management, triggering, task logs, system health |\n\n### Skills\n\n#### Data Discovery & Analysis\n\n| Skill | Description |\n|-------|-------------|\n| [init](./skills/init/) | Initialize schema discovery - generates `.astro/warehouse.md` for instant lookups |\n| [analyzing-data](./skills/analyzing-data/) | SQL-based analysis to answer business questions (uses background Jupyter kernel) |\n| [checking-freshness](./skills/checking-freshness/) | Check how current your data is |\n| [profiling-tables](./skills/profiling-tables/) | Comprehensive table profiling and quality assessment |\n\n#### Data Lineage\n\n| Skill | Description |\n|-------|-------------|\n| [tracing-downstream-lineage](./skills/tracing-downstream-lineage/) | Analyze what breaks if you change something |\n| [tracing-upstream-lineage](./skills/tracing-upstream-lineage/) | Trace where data comes from |\n\n#### DAG Development\n\n| Skill | Description |\n|-------|-------------|\n| [airflow](./skills/airflow/) | Main entrypoint - routes to specialized Airflow skills |\n| [setting-up-astro-project](./skills/setting-up-astro-project/) | Initialize and configure new Astro/Airflow projects |\n| [managing-astro-local-env](./skills/managing-astro-local-env/) | Manage local Airflow environment (start, stop, logs, troubleshoot) |\n| [authoring-dags](./skills/authoring-dags/) | Create and validate Airflow DAGs with best practices |\n| [testing-dags](./skills/testing-dags/) | Test and debug Airflow DAGs locally |\n| [debugging-dags](./skills/debugging-dags/) | Deep failure diagnosis and root cause analysis |\n\n#### Migration\n\n| Skill | Description |\n|-------|-------------|\n| [migrating-airflow-2-to-3](./skills/migrating-airflow-2-to-3/) | Migrate DAGs from Airflow 2.x to 3.x |\n\n### User Journeys\n\n#### Data Analysis Flow\n\n```mermaid\nflowchart LR\n    init[\"/data:init\"] --> analyzing[\"/data:analyzing-data\"]\n    analyzing --> profiling[\"/data:profiling-tables\"]\n    analyzing --> freshness[\"/data:checking-freshness\"]\n```\n\n1. **Initialize** (`/data:init`) - One-time setup to generate `warehouse.md` with schema metadata\n2. **Analyze** (`/data:analyzing-data`) - Answer business questions with SQL\n3. **Profile** (`/data:profiling-tables`) - Deep dive into specific tables for statistics and quality\n4. **Check freshness** (`/data:checking-freshness`) - Verify data is up to date before using\n\n#### DAG Development Flow\n\n```mermaid\nflowchart LR\n    setup[\"/data:setting-up-astro-project\"] --> authoring[\"/data:authoring-dags\"]\n    setup --> env[\"/data:managing-astro-local-env\"]\n    authoring --> testing[\"/data:testing-dags\"]\n    testing --> debugging[\"/data:debugging-dags\"]\n```\n\n1. **Setup** (`/data:setting-up-astro-project`) - Initialize project structure and dependencies\n2. **Environment** (`/data:managing-astro-local-env`) - Start/stop local Airflow for development\n3. **Author** (`/data:authoring-dags`) - Write DAG code following best practices\n4. **Test** (`/data:testing-dags`) - Run DAGs and fix issues iteratively\n5. **Debug** (`/data:debugging-dags`) - Deep investigation for complex failures\n\n## Configuration\n\n### Warehouse Connections\n\nConfigure data warehouse connections at `~/.astro/agents/warehouse.yml`:\n\n```yaml\nmy_warehouse:\n  type: snowflake\n  account: ${SNOWFLAKE_ACCOUNT}\n  user: ${SNOWFLAKE_USER}\n  auth_type: private_key\n  private_key_path: ~/.ssh/snowflake_key.p8\n  private_key_passphrase: ${SNOWFLAKE_PRIVATE_KEY_PASSPHRASE}\n  warehouse: COMPUTE_WH\n  role: ANALYST\n  databases:\n    - ANALYTICS\n    - RAW\n```\n\nStore credentials in `~/.astro/agents/.env`:\n\n```bash\nSNOWFLAKE_ACCOUNT=xyz12345\nSNOWFLAKE_USER=myuser\nSNOWFLAKE_PRIVATE_KEY_PASSPHRASE=your-passphrase-here  # Only required if using an encrypted private key\n```\n\n**Supported databases:**\n\n| Type | Package | Description |\n|------|---------|-------------|\n| `snowflake` | Built-in | Snowflake Data Cloud |\n| `postgres` | Built-in | PostgreSQL |\n| `bigquery` | Built-in | Google BigQuery |\n| `sqlalchemy` | Any SQLAlchemy driver | Auto-detects packages for 25+ databases (see below) |\n\n<details>\n<summary>Auto-detected SQLAlchemy databases</summary>\n\nThe connector automatically installs the correct driver packages for:\n\n| Database | Dialect URL |\n|----------|-------------|\n| PostgreSQL | `postgresql://` or `postgres://` |\n| MySQL | `mysql://` or `mysql+pymysql://` |\n| MariaDB | `mariadb://` |\n| SQLite | `sqlite:///` |\n| SQL Server | `mssql+pyodbc://` |\n| Oracle | `oracle://` |\n| Redshift | `redshift://` |\n| Snowflake | `snowflake://` |\n| BigQuery | `bigquery://` |\n| DuckDB | `duckdb:///` |\n| Trino | `trino://` |\n| ClickHouse | `clickhouse://` |\n| CockroachDB | `cockroachdb://` |\n| Databricks | `databricks://` |\n| Amazon Athena | `awsathena://` |\n| Cloud Spanner | `spanner://` |\n| Teradata | `teradata://` |\n| Vertica | `vertica://` |\n| SAP HANA | `hana://` |\n| IBM Db2 | `db2://` |\n\nFor unlisted databases, install the driver manually and use standard SQLAlchemy URLs.\n\n</details>\n\n<details>\n<summary>Example configurations</summary>\n\n```yaml\n# PostgreSQL\nmy_postgres:\n  type: postgres\n  host: localhost\n  port: 5432\n  user: analyst\n  password: ${POSTGRES_PASSWORD}\n  database: analytics\n\n# BigQuery\nmy_bigquery:\n  type: bigquery\n  project: my-gcp-project\n  credentials_path: ~/.config/gcloud/service_account.json\n\n# SQLAlchemy (any supported database)\nmy_duckdb:\n  type: sqlalchemy\n  url: duckdb:///path/to/analytics.duckdb\n  databases: [main]\n\n# Redshift (via SQLAlchemy)\nmy_redshift:\n  type: sqlalchemy\n  url: redshift+redshift_connector://${REDSHIFT_USER}:${REDSHIFT_PASSWORD}@${REDSHIFT_HOST}:5439/${REDSHIFT_DATABASE}\n  databases: [my_database]\n```\n\n</details>\n\n### Airflow\n\nThe Airflow MCP auto-discovers your project when you run Claude Code from an Airflow project directory (contains `airflow.cfg` or `dags/` folder).\n\nFor remote instances, set environment variables:\n\n| Variable | Description |\n|----------|-------------|\n| `AIRFLOW_API_URL` | Airflow webserver URL |\n| `AIRFLOW_USERNAME` | Username |\n| `AIRFLOW_PASSWORD` | Password |\n| `AIRFLOW_AUTH_TOKEN` | Bearer token (alternative to username/password) |\n\n## Usage\n\nSkills are invoked automatically based on what you ask. You can also invoke them directly with `/data:<skill-name>`.\n\n### Getting Started\n\n1. **Initialize your warehouse** (recommended first step):\n   ```\n   /data:init\n   ```\n   This generates `.astro/warehouse.md` with schema metadata for faster queries.\n\n2. **Ask questions naturally**:\n   - \"What tables contain customer data?\"\n   - \"Show me revenue trends by product\"\n   - \"Create a DAG that loads data from S3 to Snowflake daily\"\n   - \"Why did my etl_pipeline DAG fail yesterday?\"\n\n## Development\n\nSee [CLAUDE.md](./CLAUDE.md) for plugin development guidelines.\n\n### Local Development Setup\n\n```bash\n# Clone the repo\ngit clone https://github.com/astronomer/agents.git\ncd agents\n\n# Test with local plugin\nclaude --plugin-dir .\n\n# Or install from local marketplace\nclaude plugin marketplace add .\nclaude plugin install data@astronomer\n```\n\n### Adding Skills\n\nCreate a new skill in `skills/<name>/SKILL.md` with YAML frontmatter:\n\n```yaml\n---\nname: my-skill\ndescription: When to invoke this skill\n---\n\n# Skill instructions here...\n```\n\nAfter adding skills, reinstall the plugin:\n```bash\nclaude plugin uninstall data@astronomer && claude plugin install data@astronomer\n```\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| Skills not appearing | Reinstall plugin: `claude plugin uninstall data@astronomer && claude plugin install data@astronomer` |\n| Warehouse connection errors | Check credentials in `~/.astro/agents/.env` and connection config in `warehouse.yml` |\n| Airflow not detected | Ensure you're running from a directory with `airflow.cfg` or a `dags/` folder |\n\n## Contributing\n\nContributions welcome! See [CLAUDE.md](./CLAUDE.md) for development guidelines.\n\n## License\n\nApache 2.0\n\n---\n\nMade with :heart: by Astronomer\n"
      },
      "plugins": [
        {
          "name": "data",
          "source": "./",
          "strict": false,
          "description": "Data engineering plugin - warehouse exploration, pipeline authoring, Airflow integration",
          "version": "0.1.0",
          "author": {
            "name": "Astronomer",
            "email": "support@astronomer.io"
          },
          "homepage": "https://github.com/astronomer/agents",
          "repository": "https://github.com/astronomer/agents",
          "keywords": [
            "data-engineering",
            "airflow",
            "snowflake",
            "bigquery",
            "jupyter",
            "astronomer"
          ],
          "hooks": {
            "UserPromptSubmit": [
              {
                "hooks": [
                  {
                    "type": "command",
                    "command": "${CLAUDE_PLUGIN_ROOT}/skills/airflow/hooks/airflow-skill-suggester.sh"
                  }
                ]
              }
            ]
          },
          "categories": [
            "airflow",
            "astronomer",
            "bigquery",
            "data-engineering",
            "jupyter",
            "snowflake"
          ],
          "install_commands": [
            "/plugin marketplace add astronomer/agents",
            "/plugin install data@astronomer"
          ]
        }
      ]
    }
  ]
}