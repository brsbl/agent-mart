{
  "author": {
    "id": "geyang",
    "display_name": "Ge Yang",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/630490?u=7f72ce01d44526d72dab37bc64f8fe8ead7bbcd7&v=4",
    "url": "https://github.com/geyang",
    "bio": "I work on reinforcement learning and autonomous robots with an emerging focus on spatial perception. IAIFI Fellow, Postdoc at MIT CSAIL.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 1,
      "total_stars": 18,
      "total_forks": 2
    }
  },
  "marketplaces": [
    {
      "name": "params-proto",
      "version": null,
      "description": "Declarative hyperparameter management skills for ML/AI experiments",
      "owner_info": {
        "name": "Ge Yang"
      },
      "keywords": [],
      "repo_full_name": "geyang/params-proto",
      "repo_url": "https://github.com/geyang/params-proto",
      "repo_description": "params_proto, a collection of decorators that makes shell argument passing declarative",
      "homepage": "https://params-proto.readthedocs.io",
      "signals": {
        "stars": 18,
        "forks": 2,
        "pushed_at": "2026-01-28T03:05:21Z",
        "created_at": "2017-09-06T09:45:12Z",
        "license": "NOASSERTION"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 331
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 361
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 7230
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/params-proto",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/params-proto/SKILL.md",
          "type": "blob",
          "size": 5236
        },
        {
          "path": "skills/params-proto/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/params-proto/references/cli-and-types.md",
          "type": "blob",
          "size": 7581
        },
        {
          "path": "skills/params-proto/references/environment-vars.md",
          "type": "blob",
          "size": 4410
        },
        {
          "path": "skills/params-proto/references/patterns.md",
          "type": "blob",
          "size": 7886
        },
        {
          "path": "skills/params-proto/references/sweeps.md",
          "type": "blob",
          "size": 6383
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"params-proto\",\n  \"owner\": {\n    \"name\": \"Ge Yang\"\n  },\n  \"description\": \"Declarative hyperparameter management skills for ML/AI experiments\",\n  \"plugins\": [\n    {\n      \"name\": \"params-proto\",\n      \"source\": \"./\",\n      \"description\": \"params-proto skills for CLI generation and configuration management\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"params-proto\",\n  \"version\": \"3.0.0\",\n  \"description\": \"Declarative hyperparameter management skills for ML/AI experiments\",\n  \"author\": {\n    \"name\": \"Ge Yang\"\n  },\n  \"repository\": \"https://github.com/geyang/params-proto\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"machine-learning\", \"hyperparameters\", \"cli\", \"configuration\"],\n  \"skills\": \"./skills/\"\n}\n",
        "README.md": "# params-proto: Modern Declarative Parameters for Machine Learning\n\n[![Documentation Status](https://readthedocs.org/projects/params-proto/badge/?version=latest)](https://params-proto.readthedocs.io/en/latest/?badge=latest)\n[![GitHub Release](https://img.shields.io/github/release/geyang/params-proto.svg)](https://github.com/geyang/params-proto/releases)\n[![PyPI version](https://badge.fury.io/py/params-proto.svg)](https://badge.fury.io/py/params-proto)\n\n**params-proto** is a lightweight, decorator-based library for defining configurations in Python. Write your parameters once with type hints and inline documentation, and get automatic CLI parsing, validation, and help generation.\n\n> **Note**: This is v3 with a completely redesigned API. For the v2 API, see [params-proto-v2](https://github.com/geyang/params-proto-v2).\n\n## Why params-proto?\n\n**Stop fighting with argparse and click.** With params-proto, your configuration **is** your documentation:\n\n```python\nfrom params_proto import proto\n\n@proto.cli\ndef train_mnist(\n    batch_size: int = 128,  # Training batch size\n    lr: float = 0.001,  # Learning rate\n    epochs: int = 10,  # Number of training epochs\n):\n    \"\"\"Train an MLP on MNIST dataset.\"\"\"\n    print(f\"Training with lr={lr}, batch_size={batch_size}, epochs={epochs}\")\n    # Your training code here...\n\nif __name__ == \"__main__\":\n    train_mnist()\n```\n\n**That's it!** No argparse boilerplate, no manual help strings, no type conversion logic. Just pure Python functions with type hints and inline comments.\n\nRun it:\n```bash\n$ python train.py --help\nusage: train.py [-h] [--batch-size INT] [--lr FLOAT] [--epochs INT]\n\nTrain an MLP on MNIST dataset.\n\noptions:\n  -h, --help           show this help message and exit\n  --batch-size INT     Training batch size (default: 128)\n  --lr FLOAT           Learning rate (default: 0.001)\n  --epochs INT         Number of training epochs (default: 10)\n\n$ python train.py --lr 0.01 --batch-size 256\nTraining with lr=0.01, batch_size=256, epochs=10\n```\n\n> **Note**: The actual terminal output includes beautiful ANSI colors! See the demo below or check the [documentation](https://params-proto.readthedocs.io/) for colorized examples.\n\n## Try It Now\n\nWant to see the colorized help in action? Clone the repo and run the demo:\n\n```bash\n# Clone and setup\ngit clone https://github.com/geyang/params-proto.git\ncd params-proto\nuv sync\n\n# See the colorized help (with bright blue types, bold cyan defaults, bold red required)\nuv run python scratch/demo_v3.py --help\n\n# Try running without required --seed (shows error)\nuv run python scratch/demo_v3.py\n# Error: the following arguments are required: --seed\n\n# Run with required parameter (keyword syntax)\nuv run python scratch/demo_v3.py --seed 42\n\n# Or use positional syntax\nuv run python scratch/demo_v3.py 42\n```\n\n## Installation\n\n```bash\npip install params-proto==3.0.0-rc25\n```\n\n## Key Features\n\n### 1. Function-based Configs\nDefine parameters using type-annotated functions:\n\n```python\n@proto.cli\ndef train(\n    model: str = \"resnet50\",  # Model architecture\n    dataset: str = \"imagenet\",  # Dataset to use\n    gpu: bool = True,  # Enable GPU acceleration\n):\n    \"\"\"Train a model on a dataset.\"\"\"\n    print(f\"Training {model} on {dataset}\")\n```\n\n### 2. Class-based Configs\nOr use classes for more structure:\n\n```python\n@proto\nclass Params:    \"\"\"Training configuration.\"\"\"\n\n    # Model settings\n    model: str = \"resnet50\"\n    pretrained: bool = True  # Use pretrained weights\n\n    # Training settings\n    lr: float = 0.001  # Learning rate\n    batch_size: int = 32  # Batch size\n    epochs: int = 100  # Number of epochs\n```\n\n### 3. Singleton Prefixed Configs\nCreate modular, reusable configuration groups:\n\n```python\nfrom params_proto import proto\n\n@proto.prefix\nclass Environment:\n    \"\"\"Environment configuration.\"\"\"\n    domain: str = \"cartpole\"  # Domain name\n    task: str = \"swingup\"  # Task name\n    time_limit: float = 10.0  # Episode time limit\n\n@proto.prefix\nclass Agent:\n    \"\"\"Agent hyperparameters.\"\"\"\n    algorithm: str = \"SAC\"  # RL algorithm\n    lr: float = 3e-4  # Learning rate\n    gamma: float = 0.99  # Discount factor\n\n@proto.cli\ndef train_rl(\n    seed: int = 0,  # Random seed\n    total_steps: int = 1000000,  # Total training steps\n):\n    \"\"\"Train RL agent on dm_control.\"\"\"\n    print(f\"Training {Agent.algorithm} on {Environment.domain}-{Environment.task}\")\n    print(f\"Agent LR: {Agent.lr}, Gamma: {Agent.gamma}\")\n```\n\nCommand line:\n```bash\n$ python train_rl.py --Agent.lr 0.001 --Environment.domain walker --seed 42\nTraining SAC on walker-swingup\nAgent LR: 0.001, Gamma: 0.99\n```\n\n### 4. Multiple Override Patterns\n\nOverride parameters in multiple ways:\n\n```python\n# 1. Command line\n$ python train.py --lr 0.01\n\n# 2. Direct attribute assignment\nParams.lr = 0.01\n\n# 3. Function call with kwargs\ntrain(lr=0.01, batch_size=256)\n\n# 4. Using proto.bind() context manager\nwith proto.bind(lr=0.01, **{\"train.epochs\": 50}):\n    train()\n```\n\n### 5. Rich Type System\n\nSupport for complex types:\n\n```python\nfrom typing import Literal, Union\nfrom enum import Enum, auto\n\nclass Optimizer(Enum):\n    ADAM = auto()\n    SGD = auto()\n    RMSPROP = auto()\n\n@proto\nclass Params:    # Union types\n    precision: Literal[\"fp16\", \"fp32\", \"fp64\"] = \"fp32\"\n\n    # Enums\n    optimizer: Optimizer = Optimizer.ADAM\n\n    # Tuples\n    image_size: tuple[int, int] = (224, 224)\n\n    # Optional types\n    checkpoint: str | None = None\n```\n\n## Quick Start\n\n1. **Define your configuration** with a decorated function or class\n2. **Add type hints** for automatic validation\n3. **Add inline comments** for automatic documentation\n4. **Call your function** - params-proto handles the rest!\n\nSee our [Quick Start Guide](https://params-proto.readthedocs.io/en/latest/quick_start.html) for more.\n\n## Documentation\n\n- **[Quick Start](https://params-proto.readthedocs.io/en/latest/quick_start.html)** - Get started in 5 minutes\n- **[User Guide](https://params-proto.readthedocs.io/en/latest/guide/)** - Detailed documentation\n- **[Examples](https://params-proto.readthedocs.io/en/latest/examples/)** - Real-world usage patterns\n- **[API Reference](https://params-proto.readthedocs.io/en/latest/api/)** - Complete API documentation\n- **[Migration from v2](https://params-proto.readthedocs.io/en/latest/migration.html)** - Upgrade guide\n\n## What Changed in v3?\n\n**v2 (old)**: Class-based with inheritance\n```python\nfrom params_proto import ParamsProto\n\nclass Args(ParamsProto):\n    lr = 0.001\n    batch_size = 32\n```\n\n**v3 (new)**: Decorator-based with type hints\n```python\nfrom params_proto import proto\n\n@proto\nclass Args:\n    lr: float = 0.001  # Learning rate\n    batch_size: int = 32  # Batch size\n```\n\nKey improvements:\n- ✅ Cleaner decorator syntax (no inheritance needed)\n- ✅ Full IDE support with type hints\n- ✅ Inline documentation becomes automatic help text\n- ✅ Support for functions, not just classes\n- ✅ Better Union types and Enum support\n- ✅ Simplified singleton pattern with `@proto.prefix`\n\n## Contributing\n\n```bash\ngit clone https://github.com/episodeyang/params_proto.git\ncd params_proto\nmake dev test\n```\n\nTo publish:\n```bash\nmake publish\n```\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.\n",
        "skills/params-proto/SKILL.md": "---\nname: params-proto\ndescription: |\n  Declarative hyperparameter management for ML/AI experiments. Use when Claude needs to:\n  (1) Create CLI applications with type-hinted parameters and auto-generated help\n  (2) Configure ML training scripts with @proto.cli, @proto.prefix, or @proto decorators\n  (3) Set up multi-namespace configurations with namespaced CLI arguments\n  (4) Read configuration from environment variables using EnvVar\n  (5) Create hyperparameter sweeps using piter or Sweep\n  (6) Work with Union types for subcommand-like CLI patterns\n---\n\n# params-proto v3.1.1\n\nDeclarative hyperparameter management for ML experiments with automatic CLI generation.\n\n## Installation\n\n```bash\npip install params-proto==3.1.1\n```\n\n## Three Decorators\n\n| Decorator | Purpose | Access Pattern |\n|-----------|---------|----------------|\n| `@proto.cli` | CLI entry point | Parses sys.argv automatically |\n| `@proto.prefix` | Singleton config | `ClassName.attr` (class-level) |\n| `@proto` | Multi-instance | `instance.attr` (object-level) |\n\n## Quick Start\n\n### Simple CLI Script\n\n```python\nfrom params_proto import proto\n\n@proto.cli\ndef train(\n    lr: float = 0.001,  # Learning rate (inline comment = help text)\n    batch_size: int = 32,  # Batch size\n    epochs: int = 100,  # Number of epochs\n):\n    \"\"\"Train a model.\"\"\"  # Docstring = CLI description\n    print(f\"Training with lr={lr}\")\n\nif __name__ == \"__main__\":\n    train()\n```\n\n```bash\npython train.py --lr 0.01 --batch-size 64\npython train.py --help\n```\n\n### Multi-Namespace Configuration\n\n```python\n@proto.prefix\nclass Model:\n    name: str = \"resnet50\"  # Architecture\n    dropout: float = 0.5  # Dropout rate\n\n@proto.prefix\nclass Training:\n    lr: float = 0.001  # Learning rate\n    epochs: int = 100  # Epochs\n\n@proto.cli\ndef main(seed: int = 42):\n    \"\"\"Train with namespaced config.\"\"\"\n    print(f\"Model: {Model.name}, LR: {Training.lr}\")\n\n# CLI: python train.py --model.name vit --training.lr 0.01\n```\n\n### Environment Variables\n\n```python\nfrom params_proto import proto, EnvVar\n\n@proto.cli\ndef train(\n    lr: float = EnvVar @ \"LEARNING_RATE\" | 0.001,  # Env var with default\n    api_key: str = EnvVar @ \"API_KEY\",  # Required env var (no default)\n    # OR operation: try multiple env vars in order\n    token: str = EnvVar @ \"API_TOKEN\" @ \"AUTH_TOKEN\" | \"default\",\n): ...\n```\n\n### Union Types (Subcommand Pattern)\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Adam:\n    lr: float = 0.001\n    beta1: float = 0.9\n\n@dataclass\nclass SGD:\n    lr: float = 0.01\n    momentum: float = 0.9\n\n@proto.cli\ndef train(optimizer: Adam | SGD):\n    \"\"\"Train with selected optimizer.\"\"\"\n    print(f\"Using {type(optimizer).__name__}\")\n\n# CLI: python train.py adam --lr 0.001\n# CLI: python train.py sgd --momentum 0.95\n```\n\n### Hyperparameter Sweeps with piter\n\n```python\nfrom params_proto.hyper import piter\n\n# Zip (default): pairs values element-wise\nconfigs = piter @ {\"lr\": [0.001, 0.01], \"batch_size\": [32, 64]}\n# 2 configs: (0.001, 32), (0.01, 64)\n\n# Cartesian product with * (only first needs piter @)\nconfigs = piter @ {\"lr\": [0.001, 0.01]} * {\"batch_size\": [32, 64]}\n# 4 configs: all combinations\n\n# Override with fixed values using %\nconfigs = piter @ {\"lr\": [0.001, 0.01]} * {\"batch_size\": [32, 64]} % {\"seed\": 42}\n\n# Repeat for multiple trials using **\nconfigs = (piter @ {\"lr\": [0.001, 0.01]}) ** 3  # 2 configs x 3 trials\n\nfor config in configs:\n    train(**config)\n```\n\n## Type Annotations\n\n| Type | CLI Display | Example |\n|------|-------------|---------|\n| `int` | `INT` | `count: int = 10` |\n| `float` | `FLOAT` | `lr: float = 0.001` |\n| `str` | `STR` | `name: str = \"default\"` |\n| `bool` | `BOOL` | `debug: bool = False` |\n| `Enum` | `{A,B,C}` | `opt: Optimizer = Optimizer.ADAM` |\n| `Literal` | `VALUE` | `mode: Literal[\"a\", \"b\"] = \"a\"` |\n| `List[T]` | `VALUE` | `ids: List[int] = [1, 2]` |\n| `Tuple[T, ...]` | `VALUE` | `dims: Tuple[int, ...] = (224, 224)` |\n| `Optional[T]` | `VALUE` | `path: str \\| None = None` |\n\n## Boolean Flags\n\n```python\n@proto.cli\ndef train(\n    verbose: bool = False,  # --verbose sets True\n    cuda: bool = True,      # --no-cuda sets False\n): ...\n```\n\n## Override Priority (highest to lowest)\n\n1. CLI arguments\n2. Direct assignment (`Config.lr = 0.01`)\n3. Context manager (`with proto.bind(Config, lr=0.01): ...`)\n4. Environment variables\n5. Default values\n\n## Getting a Clean Dict\n\n```python\nConfig._dict      # → {'lr': 0.001, 'batch_size': 32}\ndict(Config)      # → same (works for classes and functions)\n```\n\n## Reference Files\n\nFor detailed documentation, see:\n\n- [cli-and-types.md](https://raw.githubusercontent.com/geyang/params-proto/main/skills/params-proto/references/cli-and-types.md) - @proto.cli, @proto.prefix, type system\n- [environment-vars.md](https://raw.githubusercontent.com/geyang/params-proto/main/skills/params-proto/references/environment-vars.md) - EnvVar with templates and inheritance\n- [sweeps.md](https://raw.githubusercontent.com/geyang/params-proto/main/skills/params-proto/references/sweeps.md) - piter and Sweep for hyperparameter search\n- [patterns.md](https://raw.githubusercontent.com/geyang/params-proto/main/skills/params-proto/references/patterns.md) - Common ML patterns and examples\n",
        "skills/params-proto/references/cli-and-types.md": "# CLI and Types Reference\n\n## Table of Contents\n\n- [@proto.cli Decorator](#protocli-decorator)\n- [@proto.prefix Decorator](#protoprefix-decorator)\n- [@proto Decorator](#proto-decorator)\n- [Type Annotations](#type-annotations)\n- [Help Text Generation](#help-text-generation)\n\n---\n\n## @proto.cli Decorator\n\nCreates CLI entry points from functions or classes.\n\n### Basic Usage\n\n```python\nfrom params_proto import proto\n\n@proto.cli\ndef train(\n    lr: float = 0.001,  # Learning rate\n    batch_size: int = 32,  # Batch size\n):\n    \"\"\"Train a neural network.\"\"\"\n    print(f\"Training with lr={lr}\")\n\nif __name__ == \"__main__\":\n    train()\n```\n\n### Parameters\n\n- `prog` - Override program name in help: `@proto.cli(prog=\"my-trainer\")`\n\n### CLI Argument Parsing\n\n```bash\n# Named arguments (underscore → hyphen)\npython train.py --learning-rate 0.01 --batch-size 64\n\n# Positional for required params (no default)\npython train.py 42  # First required param\n\n# Boolean flags\npython train.py --verbose      # Set to True\npython train.py --no-cuda      # Set to False\n\n# Prefix syntax for @proto.prefix classes\npython train.py --model.name resnet --training.lr 0.01\n```\n\n### Required vs Optional Parameters\n\n```python\n@proto.cli\ndef train(\n    seed: int,  # Required - no default, shows (required) in help\n    lr: float = 0.001,  # Optional - has default\n): ...\n```\n\n### Boolean Flags\n\n```python\n@proto.cli\ndef train(\n    verbose: bool = False,  # --verbose sets True\n    cuda: bool = True,      # --no-cuda sets False\n): ...\n```\n\n### With @classmethod and @staticmethod\n\nPlace `@proto.cli` on the OUTSIDE (applied last):\n\n```python\nclass Trainer:\n    @proto.cli          # OUTSIDE - receives the descriptor\n    @staticmethod\n    def evaluate(model_path: str, threshold: float = 0.5): ...\n\n    @proto.cli          # OUTSIDE\n    @classmethod\n    def train(cls, lr: float = 0.01): ...\n```\n\n---\n\n## @proto.prefix Decorator\n\nCreates singleton configuration classes with namespaced CLI arguments.\n\n### Basic Usage\n\n```python\n@proto.prefix\nclass Training:\n    \"\"\"Training hyperparameters.\"\"\"\n    lr: float = 0.001  # Learning rate\n    batch_size: int = 32  # Batch size\n\n# Access as class attributes (singleton)\nprint(Training.lr)  # 0.001\nTraining.lr = 0.01  # Direct modification\n```\n\n### With @proto.cli\n\n```python\n@proto.prefix\nclass Model:\n    name: str = \"resnet50\"\n    dropout: float = 0.5\n\n@proto.prefix\nclass Training:\n    lr: float = 0.001\n    epochs: int = 100\n\n@proto.cli\ndef main(seed: int = 42):\n    print(f\"Training {Model.name} with lr={Training.lr}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n```bash\npython train.py --model.name vit --training.lr 0.01\n```\n\n### Singleton Behavior\n\n```python\n@proto.prefix\nclass Config:\n    value: int = 0\n\nConfig.value = 42\nc1 = Config()\nc2 = Config()\nc1.value = 100\nprint(c2.value)  # 100 - same singleton\n```\n\n### Override Patterns\n\n```python\n# 1. Direct assignment\nTraining.lr = 0.01\n\n# 2. CLI arguments\n# python train.py --training.lr 0.01\n\n# 3. Context manager (scoped override)\nwith proto.bind(Training, lr=0.01):\n    train()  # Training.lr == 0.01\n# Training.lr restored after block\n\n# 4. proto.bind() without context manager\nproto.bind(**{\"training.lr\": 0.01, \"model.name\": \"vit\"})\n```\n\n### Getting a Clean Dict\n\n```python\n@proto.prefix\nclass Config:\n    lr: float = 0.001\n    batch_size: int = 32\n\nConfig.lr = 0.01  # Override\n\n# Two equivalent ways to get parameter values\nConfig._dict      # → {'lr': 0.01, 'batch_size': 32}\ndict(Config)      # → {'lr': 0.01, 'batch_size': 32}\n```\n\n### Custom Prefix Name\n\n```python\n@proto.prefix(\"train\")  # Custom prefix instead of class name\nclass TrainingConfig:\n    lr: float = 0.001\n\n# CLI: --train.lr 0.01 (not --trainingconfig.lr)\n```\n\n### Post-Initialization Hook\n\n```python\n@proto.prefix\nclass Config:\n    lr: float = 0.01\n    total: int = None\n\n    def __post_init__(self):\n        if self.lr > 1:\n            raise ValueError(\"lr too high\")\n        self.total = int(self.lr * 1000)\n\nc = Config()\nprint(c.total)  # 10\n```\n\n---\n\n## @proto Decorator\n\nCreates multi-instance configuration classes (not singletons).\n\n### Basic Usage\n\n```python\n@proto\nclass OptimizerConfig:\n    name: str = \"adam\"\n    lr: float = 0.001\n\n# Create multiple instances\nadam = OptimizerConfig(name=\"adam\", lr=0.001)\nsgd = OptimizerConfig(name=\"sgd\", lr=0.01)\n```\n\n### Inheritance\n\n```python\nclass BaseConfig:\n    lr: float = 0.001\n    batch_size: int = 32\n\n@proto\nclass TrainConfig(BaseConfig):\n    epochs: int = 100\n\nc = TrainConfig()\nvars(c)  # {'lr': 0.001, 'batch_size': 32, 'epochs': 100}\n```\n\n### Getting a Clean Dict\n\n```python\n@proto\nclass Config:\n    lr: float = 0.001\n    batch_size: int = 32\n\n# Class-level: get defaults + overrides\nConfig._dict      # → {'lr': 0.001, 'batch_size': 32}\ndict(Config)      # → {'lr': 0.001, 'batch_size': 32}\n\n# Instance-level: use vars()\nc = Config(lr=0.01)\nvars(c)           # → {'lr': 0.01, 'batch_size': 32}\n```\n\n### @proto vs @proto.prefix\n\n| Feature | `@proto.prefix` | `@proto` |\n|---------|-----------------|----------|\n| Singleton | Yes | No |\n| CLI integration | Auto with @proto.cli | Manual |\n| Access pattern | `Class.attr` | `instance.attr` |\n| Use case | Global config | Reusable templates |\n\n---\n\n## Type Annotations\n\n### Supported Types\n\n| Python Type | CLI Display | Conversion |\n|-------------|-------------|------------|\n| `int` | `INT` | `\"42\"` → `42` |\n| `float` | `FLOAT` | `\"0.01\"` → `0.01` |\n| `str` | `STR` | Pass through |\n| `bool` | `BOOL` | `\"true\"/\"1\"/\"yes\"` → `True` |\n| `Enum` | `{MEMBER,...}` | Member name lookup |\n| `Literal[\"a\",\"b\"]` | `VALUE` | Validated against choices |\n| `List[T]` | `VALUE` | Multiple args collected |\n| `Tuple[T, ...]` | `VALUE` | Variable-length tuple |\n| `Tuple[int, str]` | `VALUE` | Fixed-length typed tuple |\n| `Optional[T]` | `VALUE` | `None` if not provided |\n| `Path` | `VALUE` | String → `Path` object |\n\n### Union Types (Subcommand Pattern)\n\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass Train:\n    lr: float = 0.001\n    epochs: int = 100\n\n@dataclass\nclass Evaluate:\n    checkpoint: str = \"model.pt\"\n\n@proto.cli\ndef main(mode: Train | Evaluate):\n    if isinstance(mode, Train):\n        print(f\"Training: lr={mode.lr}\")\n    else:\n        print(f\"Evaluating: {mode.checkpoint}\")\n```\n\n```bash\npython main.py train --lr 0.01\npython main.py evaluate --checkpoint best.pt\n# Also: --mode:train, --mode:Train, --mode:perspective-camera\n```\n\n### Enum Types\n\n```python\nfrom enum import Enum\n\nclass Optimizer(Enum):\n    ADAM = \"adam\"\n    SGD = \"sgd\"\n\n@proto.cli\ndef train(optimizer: Optimizer = Optimizer.ADAM): ...\n```\n\n```bash\npython train.py --optimizer SGD\n```\n\n### List and Tuple Types\n\n```python\n@proto.cli\ndef train(\n    gpu_ids: List[int] = [0, 1],  # --gpu-ids 0 1 2 3\n    dims: Tuple[int, int] = (224, 224),  # --dims 256 256\n    scales: Tuple[float, ...] = (0.5, 1.0),  # --scales 0.5 0.75 1.0\n): ...\n```\n\n---\n\n## Help Text Generation\n\n### From Inline Comments\n\n```python\n@proto.cli\ndef train(\n    lr: float = 0.001,  # Learning rate for optimizer\n): ...\n```\n\n### From Docstrings\n\n```python\n@proto.cli\ndef train(lr: float = 0.001):\n    \"\"\"Train a model.\n\n    Args:\n        lr: Learning rate for the optimizer\n    \"\"\"\n```\n\n### Combined Output\n\n```\n--lr FLOAT    Learning rate for optimizer\n              Learning rate for the optimizer (default: 0.001)\n```\n\n### Help for @proto.prefix Classes\n\n```\nModel options:\n  Model configuration.\n\n  --model.name STR     Architecture (default: resnet50)\n  --model.dropout FLOAT\n                       Dropout rate (default: 0.5)\n```\n",
        "skills/params-proto/references/environment-vars.md": "# Environment Variables Reference\n\n## Table of Contents\n\n- [Basic Usage](#basic-usage)\n- [Syntax](#syntax)\n- [OR Operation](#or-operation-multiple-env-vars)\n- [Type Conversion](#type-conversion)\n- [Required Environment Variables](#required-environment-variables)\n- [Template Expansion](#template-expansion)\n- [Priority Order](#priority-order)\n- [Inheritance](#inheritance)\n- [Common Patterns](#common-patterns)\n\n---\n\n## Basic Usage\n\n```python\nfrom params_proto import proto, EnvVar\n\n@proto.cli\ndef train(\n    lr: float = EnvVar @ \"LEARNING_RATE\" | 0.001,  # From env or default\n    api_key: str = EnvVar @ \"API_KEY\",  # Required env var\n    host: str = EnvVar @ \"HOST\" | \"localhost\",\n): ...\n```\n\n---\n\n## Syntax\n\n```python\nEnvVar @ \"ENV_VAR_NAME\" | default_value\n```\n\n- `@` specifies the environment variable name\n- `|` provides a fallback default value\n- Without `|`, the env var is required\n\n**Note:** Use `|` (pipe) not `or` for defaults. The `or` keyword doesn't work because it evaluates truthiness.\n\n---\n\n## OR Operation (Multiple Env Vars)\n\nTry multiple environment variable names in order:\n\n```python\n@proto.cli\ndef deploy(\n    # Try API_KEY first, then SECRET_KEY, then use default\n    api_key: str = EnvVar @ \"API_KEY\" @ \"SECRET_KEY\" | \"default\",\n\n    # Function call syntax\n    token: str = EnvVar(\"AUTH_TOKEN\", \"ACCESS_TOKEN\", default=\"none\"),\n): ...\n```\n\nReturns the first env var that is set, or the default if none are set.\n\n---\n\n## Type Conversion\n\nValues are automatically converted based on type hint:\n\n```python\n@proto.cli\ndef train(\n    lr: float = EnvVar @ \"LR\" | 0.001,  # String → float\n    batch_size: int = EnvVar @ \"BATCH\" | 32,  # String → int\n    debug: bool = EnvVar @ \"DEBUG\" | False,  # String → bool\n): ...\n```\n\n### Boolean Conversion\n\n| True values | False values |\n|-------------|--------------|\n| `\"true\"`, `\"1\"`, `\"yes\"`, `\"on\"` | `\"false\"`, `\"0\"`, `\"no\"`, `\"off\"` |\n\nCase-insensitive.\n\n---\n\n## Required Environment Variables\n\n```python\n@proto.cli\ndef deploy(\n    api_key: str = EnvVar @ \"API_KEY\",  # No default = required\n): ...\n```\n\nMissing required env var raises error at import time.\n\n---\n\n## Template Expansion\n\nMultiple variables in one value:\n\n```python\n@proto.cli\ndef connect(\n    url: str = EnvVar @ \"PROTOCOL://$HOST:$PORT/api\",\n): ...\n```\n\n```bash\nPROTOCOL=https HOST=example.com PORT=443 python connect.py\n# url = \"https://example.com:443/api\"\n```\n\nSupported syntax: `$VAR` and `${VAR}`\n\n---\n\n## Priority Order\n\n1. CLI arguments (highest)\n2. Direct assignment\n3. Environment variables\n4. Default values (lowest)\n\n```python\n@proto.cli\ndef train(lr: float = EnvVar @ \"LR\" | 0.001): ...\n```\n\n```bash\n# Uses env var\nLR=0.01 python train.py\n# lr = 0.01\n\n# CLI overrides env var\nLR=0.01 python train.py --lr 0.1\n# lr = 0.1\n```\n\n---\n\n## Inheritance\n\nEnvVar fields are inherited and type-converted correctly:\n\n```python\nclass BaseConfig:\n    host: str = EnvVar @ \"HOST\" | \"localhost\"\n    port: int = EnvVar @ \"PORT\" | 8080\n    debug: bool = EnvVar @ \"DEBUG\" | False\n\n@proto.prefix\nclass AppConfig(BaseConfig):\n    timeout: int = EnvVar @ \"TIMEOUT\" | 30\n```\n\n```bash\nHOST=10.0.0.1 PORT=3000 DEBUG=true python app.py\n# AppConfig.host = \"10.0.0.1\" (str)\n# AppConfig.port = 3000 (int)\n# AppConfig.debug = True (bool)\n```\n\n---\n\n## Common Patterns\n\n### Database Configuration\n\n```python\n@proto.prefix\nclass Database:\n    host: str = EnvVar @ \"DB_HOST\" | \"localhost\"\n    port: int = EnvVar @ \"DB_PORT\" | 5432\n    user: str = EnvVar @ \"DB_USER\" | \"postgres\"\n    password: str = EnvVar @ \"DB_PASSWORD\"  # Required\n    name: str = EnvVar @ \"DB_NAME\" | \"myapp\"\n```\n\n### API Keys\n\n```python\n@proto.prefix\nclass API:\n    openai_key: str = EnvVar @ \"OPENAI_API_KEY\"\n    anthropic_key: str = EnvVar @ \"ANTHROPIC_API_KEY\"\n```\n\n### Feature Flags\n\n```python\n@proto.prefix\nclass Features:\n    enable_cache: bool = EnvVar @ \"ENABLE_CACHE\" | True\n    debug_mode: bool = EnvVar @ \"DEBUG\" | False\n    log_level: str = EnvVar @ \"LOG_LEVEL\" | \"INFO\"\n```\n\n---\n\n## Help Text Display\n\nEnvironment variables appear in help:\n\n```\n--lr FLOAT    Learning rate (default: $LEARNING_RATE or 0.001)\n```\n\n---\n\n## Best Practices\n\n1. **Use uppercase for env vars** - `LEARNING_RATE` not `learning_rate`\n2. **Provide defaults for optional** - `EnvVar @ \"VAR\" | default`\n3. **Use prefixes for grouping** - `DB_HOST`, `DB_PORT`, `API_KEY`\n4. **Don't commit secrets** - Use `.env` files, not code defaults\n",
        "skills/params-proto/references/patterns.md": "# Common Patterns Reference\n\n## Table of Contents\n\n- [Simple Training Script](#simple-training-script)\n- [Multi-Namespace ML Config](#multi-namespace-ml-config)\n- [Environment-Based Config](#environment-based-config)\n- [Union Types (Subcommand Pattern)](#union-types-subcommand-pattern)\n- [Hyperparameter Sweep](#hyperparameter-sweep)\n- [Context Manager Overrides](#context-manager-overrides)\n- [Reusable Config Class](#reusable-config-class)\n- [CLI with Validation](#cli-with-validation)\n- [Testing Pattern](#testing-pattern)\n- [Inheritance](#inheritance)\n\n---\n\n## Simple Training Script\n\n```python\nfrom params_proto import proto\n\n@proto.cli\ndef train(\n    lr: float = 0.001,  # Learning rate\n    batch_size: int = 32,  # Batch size\n    epochs: int = 100,  # Training epochs\n    seed: int = 42,  # Random seed\n):\n    \"\"\"Train a model.\"\"\"\n    print(f\"Training for {epochs} epochs with lr={lr}\")\n\nif __name__ == \"__main__\":\n    train()\n```\n\n---\n\n## Multi-Namespace ML Config\n\n```python\nfrom params_proto import proto\n\n@proto.prefix\nclass Data:\n    \"\"\"Data configuration.\"\"\"\n    path: str = \"./data\"  # Data directory\n    batch_size: int = 32  # Batch size\n    workers: int = 4  # Data loader workers\n\n@proto.prefix\nclass Model:\n    \"\"\"Model configuration.\"\"\"\n    name: str = \"resnet50\"  # Architecture\n    pretrained: bool = True  # Use pretrained weights\n    dropout: float = 0.5  # Dropout rate\n\n@proto.prefix\nclass Training:\n    \"\"\"Training hyperparameters.\"\"\"\n    lr: float = 0.001  # Learning rate\n    epochs: int = 100  # Number of epochs\n    weight_decay: float = 1e-4  # L2 regularization\n\n@proto.cli\ndef main(\n    seed: int = 42,  # Random seed\n    device: str = \"cuda\",  # Device (cuda/cpu)\n):\n    \"\"\"Train image classifier.\"\"\"\n    print(f\"Model: {Model.name}\")\n    print(f\"Data: {Data.path}\")\n    print(f\"Training: lr={Training.lr}, epochs={Training.epochs}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n```bash\npython train.py --model.name vit --training.lr 0.01 --data.batch-size 64\n```\n\n---\n\n## Environment-Based Config\n\n```python\nfrom params_proto import proto, EnvVar\n\n@proto.prefix\nclass Database:\n    host: str = EnvVar @ \"DB_HOST\" | \"localhost\"\n    port: int = EnvVar @ \"DB_PORT\" | 5432\n    user: str = EnvVar @ \"DB_USER\" | \"postgres\"\n    password: str = EnvVar @ \"DB_PASSWORD\"\n    name: str = EnvVar @ \"DB_NAME\" | \"myapp\"\n\n@proto.prefix\nclass API:\n    key: str = EnvVar @ \"API_KEY\"\n    base_url: str = EnvVar @ \"API_URL\" | \"https://api.example.com\"\n\n@proto.cli\ndef serve(\n    port: int = EnvVar @ \"PORT\" | 8080,\n    debug: bool = EnvVar @ \"DEBUG\" | False,\n):\n    \"\"\"Start the server.\"\"\"\n    print(f\"Connecting to {Database.host}:{Database.port}\")\n    print(f\"Serving on port {port}\")\n```\n\n---\n\n## Union Types (Subcommand Pattern)\n\n```python\nfrom dataclasses import dataclass\nfrom params_proto import proto\n\n@dataclass\nclass Train:\n    \"\"\"Training mode.\"\"\"\n    lr: float = 0.001\n    epochs: int = 100\n    batch_size: int = 32\n\n@dataclass\nclass Evaluate:\n    \"\"\"Evaluation mode.\"\"\"\n    checkpoint: str = \"model.pt\"\n    batch_size: int = 64\n\n@dataclass\nclass Export:\n    \"\"\"Export mode.\"\"\"\n    checkpoint: str = \"model.pt\"\n    format: str = \"onnx\"\n\n@proto.cli\ndef main(mode: Train | Evaluate | Export):\n    \"\"\"ML pipeline with different modes.\"\"\"\n    if isinstance(mode, Train):\n        print(f\"Training: lr={mode.lr}, epochs={mode.epochs}\")\n    elif isinstance(mode, Evaluate):\n        print(f\"Evaluating: {mode.checkpoint}\")\n    elif isinstance(mode, Export):\n        print(f\"Exporting to {mode.format}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n```bash\npython main.py train --lr 0.01 --epochs 50\npython main.py evaluate --checkpoint best.pt\npython main.py export --format torchscript\n```\n\n---\n\n## Hyperparameter Sweep\n\n### Using piter (Recommended)\n\n```python\nfrom params_proto import proto\nfrom params_proto.hyper import piter\n\n@proto.cli\ndef train(\n    lr: float = 0.001,\n    batch_size: int = 32,\n    model: str = \"resnet50\",\n    seed: int = 42,\n):\n    \"\"\"Train with given hyperparameters.\"\"\"\n    print(f\"Training {model} with lr={lr}, batch={batch_size}, seed={seed}\")\n    return {\"accuracy\": 0.95, \"loss\": 0.1}\n\n# Grid search with piter @ syntax\nconfigs = (\n    piter @ {\"lr\": [0.001, 0.01]}\n    * {\"batch_size\": [32, 64]}\n    * {\"model\": [\"resnet50\", \"vit\"]}\n) % {\"seed\": 42}\n\nresults = []\nfor config in configs:\n    metrics = train(**config)\n    results.append({**config, **metrics})\n```\n\n### Using Sweep (Class-based)\n\n```python\nfrom params_proto import proto, Sweep\n\n@proto.cli\ndef train(\n    lr: float = 0.001,\n    batch_size: int = 32,\n    model: str = \"resnet50\",\n    seed: int = 42,\n):\n    return {\"accuracy\": 0.95}\n\nsweep = Sweep(train).product(\n    lr=[0.001, 0.01],\n    batch_size=[32, 64],\n    model=[\"resnet50\", \"vit\"],\n).set(\n    seed=42,\n)\n\nfor config in sweep:\n    train(**config)\n```\n\n---\n\n## Context Manager Overrides\n\n```python\nfrom params_proto import proto\n\n@proto.prefix\nclass Config:\n    lr: float = 0.001\n    debug: bool = False\n\ndef train():\n    print(f\"lr={Config.lr}, debug={Config.debug}\")\n\n# Default values\ntrain()  # lr=0.001, debug=False\n\n# Override with context manager\nwith proto.bind(Config, lr=0.01, debug=True):\n    train()  # lr=0.01, debug=True\n\n# Back to defaults\ntrain()  # lr=0.001, debug=False\n```\n\n---\n\n## Reusable Config Class\n\n```python\nfrom params_proto import proto\n\n@proto\nclass OptimizerConfig:\n    \"\"\"Reusable optimizer configuration.\"\"\"\n    name: str = \"adam\"\n    lr: float = 0.001\n    weight_decay: float = 1e-4\n\n# Create multiple instances\nadam_config = OptimizerConfig(name=\"adam\", lr=0.001)\nsgd_config = OptimizerConfig(name=\"sgd\", lr=0.01)\n\ndef train(optimizer_config: OptimizerConfig):\n    print(f\"Using {optimizer_config.name} with lr={optimizer_config.lr}\")\n```\n\n---\n\n## CLI with Validation\n\n```python\nfrom params_proto import proto\nfrom enum import Enum\n\nclass Precision(Enum):\n    FP32 = \"fp32\"\n    FP16 = \"fp16\"\n    BF16 = \"bf16\"\n\n@proto.cli\ndef train(\n    lr: float = 0.001,  # Learning rate (0, 1)\n    batch_size: int = 32,  # Batch size (power of 2)\n    precision: Precision = Precision.FP32,  # Training precision\n):\n    \"\"\"Train with validation.\"\"\"\n    if not 0 < lr < 1:\n        raise ValueError(f\"lr must be in (0, 1), got {lr}\")\n    if batch_size & (batch_size - 1) != 0:\n        raise ValueError(f\"batch_size must be power of 2, got {batch_size}\")\n\n    print(f\"Training with lr={lr}, batch={batch_size}, precision={precision.value}\")\n\nif __name__ == \"__main__\":\n    train()\n```\n\n---\n\n## Testing Pattern\n\n```python\nfrom params_proto import proto\n\n@proto.cli(prog=\"train\")  # Fixed name for reproducible help text\ndef train(lr: float = 0.001):\n    \"\"\"Train a model.\"\"\"\n    return {\"lr\": lr}\n\n# Test programmatically (bypasses CLI parsing)\ndef test_train():\n    result = train(lr=0.01)\n    assert result[\"lr\"] == 0.01\n\n# Test help text\ndef test_help():\n    assert \"--lr FLOAT\" in train.__help_str__\n    assert \"(default: 0.001)\" in train.__help_str__\n```\n\n---\n\n## Inheritance\n\n### Basic Inheritance\n\n```python\nclass BaseConfig:\n    lr: float = 0.001\n    batch_size: int = 32\n\n@proto\nclass TrainConfig(BaseConfig):\n    epochs: int = 100\n\nc = TrainConfig()\nvars(c)  # {'lr': 0.001, 'batch_size': 32, 'epochs': 100}\n```\n\n### Inheritance with EnvVar\n\n```python\nclass BaseConfig:\n    host: str = EnvVar @ \"HOST\" | \"localhost\"\n    port: int = EnvVar @ \"PORT\" | 8080\n\n@proto.prefix\nclass AppConfig(BaseConfig):\n    debug: bool = EnvVar @ \"DEBUG\" | False\n```\n\n```bash\nHOST=10.0.0.1 PORT=3000 DEBUG=true python app.py\n# AppConfig.host = \"10.0.0.1\" (str)\n# AppConfig.port = 3000 (int)\n# AppConfig.debug = True (bool)\n```\n\n### Post-Init Hook\n\n```python\n@proto\nclass Config:\n    lr: float = 0.01\n    total: int = None\n\n    def __post_init__(self):\n        if self.lr > 1:\n            raise ValueError(\"lr too high\")\n        self.total = int(self.lr * 1000)\n\nc = Config(lr=0.5)\nprint(c.total)  # 500\n```\n",
        "skills/params-proto/references/sweeps.md": "# Hyperparameter Sweeps Reference\n\n## Table of Contents\n\n- [piter - Parameter Iterator (Recommended)](#piter---parameter-iterator-recommended)\n- [Sweep - Class-Based Sweeps](#sweep---class-based-sweeps)\n- [Comparison: piter vs Sweep](#comparison-piter-vs-sweep)\n\n---\n\n## piter - Parameter Iterator (Recommended)\n\nThe `piter` function creates parameter sweeps from plain dictionaries using a clean `@` syntax.\n\n### Basic Usage (Zip by Default)\n\n```python\nfrom params_proto.hyper import piter\n\n# Zips values element-wise (default behavior)\nconfigs = piter @ {\"lr\": [0.001, 0.01], \"batch_size\": [32, 64]}\n\nfor config in configs:\n    print(config)\n    # {'lr': 0.001, 'batch_size': 32}\n    # {'lr': 0.01, 'batch_size': 64}\n```\n\n### Cartesian Product with `*`\n\nUse `*` to create all combinations. Only the first dict needs `piter @`:\n\n```python\n# Grid search: 4 configs (2 x 2)\nconfigs = piter @ {\"lr\": [0.001, 0.01]} * {\"batch_size\": [32, 64]}\n\n# Output:\n# {'lr': 0.001, 'batch_size': 32}\n# {'lr': 0.001, 'batch_size': 64}\n# {'lr': 0.01, 'batch_size': 32}\n# {'lr': 0.01, 'batch_size': 64}\n```\n\n### Chaining Multiple Products\n\n```python\n# 3-way product: 8 configs (2 x 2 x 2)\nconfigs = piter @ {\"lr\": [0.001, 0.01]} * {\"batch_size\": [32, 64]} * {\"model\": [\"resnet\", \"vit\"]}\n\nfor config in configs:\n    train(**config)\n```\n\n### Override with `%`\n\nApply fixed parameters to all configurations:\n\n```python\n# Add seed to all configs\nconfigs = piter @ {\"lr\": [0.001, 0.01]} * {\"batch_size\": [32, 64]} % {\"seed\": 42}\n\n# All 4 configs have seed=42\n```\n\nCan also use another piter:\n\n```python\nconfigs = piter @ {\"batch_size\": [32, 64]} % (piter @ {\"lr\": 0.001, \"seed\": 200})\n```\n\n### Repeat with `**`\n\nRun multiple trials per config:\n\n```python\n# 2 configs x 3 trials = 6 runs\nconfigs = (piter @ {\"lr\": [0.001, 0.01]}) ** 3\n```\n\n### Complex Composition\n\n```python\n# Grid search with fixed seed and 3 trials\nexperiments = (\n    piter @ {\"lr\": [0.001, 0.01, 0.1]}\n    * {\"batch_size\": [32, 64]}\n    * {\"model\": [\"resnet\", \"vit\"]}\n) % {\"seed\": 42} ** 3\n\n# 12 configs x 3 trials = 36 runs\nfor config in experiments:\n    train(**config)\n```\n\n### With @proto.cli\n\n```python\nfrom params_proto import proto\nfrom params_proto.hyper import piter\n\n@proto.cli\ndef train(lr: float = 0.001, batch_size: int = 32, seed: int = 42):\n    print(f\"Training: lr={lr}, batch={batch_size}, seed={seed}\")\n\n# Run sweep\nfor config in piter @ {\"lr\": [0.001, 0.01]} * {\"batch_size\": [32, 64]}:\n    train(**config)\n```\n\n### With Prefixed Parameter Names\n\n```python\nconfigs = piter @ {\"model.depth\": [18, 50], \"training.lr\": [0.001, 0.01]}\n# Keys match --model.depth and --training.lr CLI syntax\n```\n\n### Methods\n\n```python\nconfigs = piter @ {\"lr\": [0.001, 0.01]}\n\n# Convert to list\nconfig_list = configs.to_list()  # or list(configs)\n\n# Get length\nlen(configs)  # 2\n\n# Iterate multiple times (results are cached)\nfor c in configs: ...\nfor c in configs: ...  # Works again\n```\n\n---\n\n## Sweep - Class-Based Sweeps\n\nFor integration with `@proto` decorated classes, use `Sweep`.\n\n### Basic Usage\n\n```python\nfrom params_proto import proto, Sweep\n\n@proto.cli\ndef train(lr: float = 0.001, batch_size: int = 32):\n    print(f\"Training with lr={lr}, batch={batch_size}\")\n\nsweep = Sweep(train)\n```\n\n### Grid Search (Product)\n\n```python\nsweep = Sweep(train).product(\n    lr=[0.001, 0.01, 0.1],\n    batch_size=[32, 64, 128],\n)\n\n# 3 x 3 = 9 configurations\nfor config in sweep:\n    train(**config)\n```\n\n### Context Manager Syntax\n\n```python\n@proto.prefix\nclass Config:\n    lr: float = 0.001\n    batch_size: int = 32\n\nwith Sweep(Config).product as sweep:\n    Config.lr = [0.001, 0.01, 0.1]\n    Config.batch_size = [32, 64]\n\nfor config in sweep:\n    # Config values are set automatically\n    train()\n```\n\n### Zip (Paired Values)\n\n```python\nwith Sweep(Config).zip as sweep:\n    Config.lr = [0.001, 0.01, 0.1]\n    Config.batch_size = [32, 64, 128]\n\n# 3 configurations (paired)\n# (0.001, 32), (0.01, 64), (0.1, 128)\n```\n\n### Nested Product and Zip\n\n```python\nwith Sweep(Config) as sweep:\n    with sweep.product:\n        Config.lr = [0.001, 0.01]\n        Config.epochs = [100, 200]\n\n        with sweep.zip:\n            Config.batch_size = [32, 64]\n            Config.workers = [4, 8]\n\n# 2 x 2 x 2 = 8 configs (lr x epochs x zipped(batch_size, workers))\n```\n\n### Set (Fixed Values)\n\n```python\nwith Sweep(Config) as sweep:\n    Config.seed = 42  # Fixed for all configs\n\n    with sweep.product:\n        Config.lr = [0.001, 0.01]\n```\n\n### Chain (Sequential)\n\n```python\nwith Sweep(Config) as sweep:\n    with sweep.chain:\n        with sweep.set:\n            Config.level = \"easy\"\n            with sweep.product:\n                Config.seed = range(5)\n\n        with sweep.set:\n            Config.level = \"hard\"\n            with sweep.product:\n                Config.seed = range(5)\n\n# 10 configs: 5 easy + 5 hard\n```\n\n### Each (Computed Parameters)\n\n```python\nwith Sweep(Config).product as sweep:\n    Config.seed = [10, 20, 30]\n\n@sweep.each\ndef each(Config):\n    Config.exp_name = f\"seed-{Config.seed}\"\n\n# Each config gets computed exp_name\n```\n\n### Sweep Operators\n\n```python\n# Multiplication = product\nresult = sweep * (piter @ {\"extra\": [1, 2]})\n\n# Power = repeat\nresult = sweep ** 3  # 3 repetitions\n\n# Modulo = override\nresult = sweep % {\"fixed_param\": 42}\n```\n\n### Saving and Loading\n\n```python\n# Save to file\nsweep.save(\"sweep.jsonl\", verbose=False)\n\n# Load from file\nsweep = Sweep(Config).load(\"sweep.jsonl\")\n```\n\n### Slicing\n\n```python\n# Get subset of configs\nsubset = list(sweep[:5])\nsubset = list(sweep[10:20:2])\nsubset = list(sweep[-10:])\n```\n\n### DataFrame Conversion\n\n```python\ndf = sweep.dataframe\n# Pandas DataFrame with config columns\n```\n\n---\n\n## Comparison: piter vs Sweep\n\n| Feature | `piter` | `Sweep` |\n|---------|---------|---------|\n| **Syntax** | `piter @ {\"lr\": [...]}` | `Sweep(Config).product(...)` |\n| **Cartesian product** | `piter @ {...} * {...}` | `.product(...)` |\n| **Input** | Plain dictionaries | `@proto` decorated classes |\n| **Type checking** | No | Yes (via `@proto`) |\n| **Save/Load** | Manual | Built-in |\n| **Use case** | Quick sweeps, scripting | Production configs |\n\n### When to Use\n\n**Use `piter @` for:**\n- Quick experiments\n- Scripting and notebooks\n- Simple parameter grids\n\n**Use `Sweep` for:**\n- Production pipelines\n- Type safety requirements\n- Complex nested sweeps\n- Saving/loading configurations\n"
      },
      "plugins": [
        {
          "name": "params-proto",
          "source": "./",
          "description": "params-proto skills for CLI generation and configuration management",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add geyang/params-proto",
            "/plugin install params-proto@params-proto"
          ]
        }
      ]
    }
  ]
}