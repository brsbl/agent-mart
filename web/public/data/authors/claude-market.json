{
  "author": {
    "id": "claude-market",
    "display_name": "Claude Market",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/241502931?v=4",
    "url": "https://github.com/claude-market",
    "bio": "Open source, curated marketplace for Claude Code.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 3,
      "total_commands": 12,
      "total_skills": 0,
      "total_stars": 5,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "claude-market",
      "version": "1.0.6",
      "description": "Hand-curated open source repository for Claude Code tools, agents and skills",
      "owner_info": {
        "name": "Claude Market",
        "url": "https://github.com/claude-market/marketplace"
      },
      "keywords": [],
      "repo_full_name": "claude-market/marketplace",
      "repo_url": "https://github.com/claude-market/marketplace",
      "repo_description": "Open source, hand-curated marketplace for Claude Code tools, agents and skills.",
      "homepage": null,
      "signals": {
        "stars": 5,
        "forks": 0,
        "pushed_at": "2025-11-04T00:09:55Z",
        "created_at": "2025-11-01T20:24:32Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 3830
        },
        {
          "path": "plugin-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-builder/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-builder/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 841
        },
        {
          "path": "plugin-builder/README.md",
          "type": "blob",
          "size": 13614
        },
        {
          "path": "plugin-builder/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-builder/commands/add.md",
          "type": "blob",
          "size": 5612
        },
        {
          "path": "plugin-builder/commands/edit.md",
          "type": "blob",
          "size": 8237
        },
        {
          "path": "plugin-builder/commands/init.md",
          "type": "blob",
          "size": 9876
        },
        {
          "path": "plugin-builder/commands/publish.md",
          "type": "blob",
          "size": 6498
        },
        {
          "path": "plugin-builder/commands/validate.md",
          "type": "blob",
          "size": 6701
        },
        {
          "path": "plugin-builder/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin-builder/skills/cc-agent-builder.md",
          "type": "blob",
          "size": 14691
        },
        {
          "path": "plugin-builder/skills/cc-command-builder.md",
          "type": "blob",
          "size": 11398
        },
        {
          "path": "plugin-builder/skills/cc-hook-builder.md",
          "type": "blob",
          "size": 14632
        },
        {
          "path": "plugin-builder/skills/cc-mcp-builder.md",
          "type": "blob",
          "size": 15466
        },
        {
          "path": "plugin-builder/skills/cc-skill-builder.md",
          "type": "blob",
          "size": 10700
        },
        {
          "path": "specforge-backend-rust-axum",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge-backend-rust-axum/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge-backend-rust-axum/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1011
        },
        {
          "path": "specforge-backend-rust-axum/README.md",
          "type": "blob",
          "size": 5765
        },
        {
          "path": "specforge-backend-rust-axum/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge-backend-rust-axum/agents/handler-implementer.md",
          "type": "blob",
          "size": 2646
        },
        {
          "path": "specforge-backend-rust-axum/agents/router-builder.md",
          "type": "blob",
          "size": 2194
        },
        {
          "path": "specforge-backend-rust-axum/agents/test-generator.md",
          "type": "blob",
          "size": 2623
        },
        {
          "path": "specforge-backend-rust-axum/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-error-mapping.md",
          "type": "blob",
          "size": 3142
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-handler-implementation.md",
          "type": "blob",
          "size": 3170
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-health-shutdown.md",
          "type": "blob",
          "size": 3355
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-idempotency.md",
          "type": "blob",
          "size": 3086
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-middleware.md",
          "type": "blob",
          "size": 4462
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-patterns.md",
          "type": "blob",
          "size": 2515
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-rate-limiting.md",
          "type": "blob",
          "size": 1930
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-resilience.md",
          "type": "blob",
          "size": 2591
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-timeouts.md",
          "type": "blob",
          "size": 2127
        },
        {
          "path": "specforge-backend-rust-axum/skills/axum-tracing.md",
          "type": "blob",
          "size": 2864
        },
        {
          "path": "specforge-backend-rust-axum/skills/rust-dev-setup.md",
          "type": "blob",
          "size": 1125
        },
        {
          "path": "specforge-backend-rust-axum/skills/rust-openapi-integration.md",
          "type": "blob",
          "size": 1532
        },
        {
          "path": "specforge-backend-rust-axum/skills/rust-testing.md",
          "type": "blob",
          "size": 5512
        },
        {
          "path": "specforge",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 1099
        },
        {
          "path": "specforge/README.md",
          "type": "blob",
          "size": 13888
        },
        {
          "path": "specforge/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/agents/captain-orchestrator.md",
          "type": "blob",
          "size": 10193
        },
        {
          "path": "specforge/agents/planning-agent.md",
          "type": "blob",
          "size": 10233
        },
        {
          "path": "specforge/agents/validation-agent.md",
          "type": "blob",
          "size": 19431
        },
        {
          "path": "specforge/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/commands/build.md",
          "type": "blob",
          "size": 15809
        },
        {
          "path": "specforge/commands/init.md",
          "type": "blob",
          "size": 10922
        },
        {
          "path": "specforge/commands/plan.md",
          "type": "blob",
          "size": 15685
        },
        {
          "path": "specforge/commands/ship.md",
          "type": "blob",
          "size": 19761
        },
        {
          "path": "specforge/commands/sync.md",
          "type": "blob",
          "size": 13032
        },
        {
          "path": "specforge/commands/test.md",
          "type": "blob",
          "size": 14707
        },
        {
          "path": "specforge/commands/validate.md",
          "type": "blob",
          "size": 10971
        },
        {
          "path": "specforge/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "specforge/skills/integration-expert.md",
          "type": "blob",
          "size": 15877
        },
        {
          "path": "specforge/skills/openapi-expert.md",
          "type": "blob",
          "size": 12523
        },
        {
          "path": "specforge/skills/stack-advisor.md",
          "type": "blob",
          "size": 12426
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"claude-market\",\n  \"version\": \"1.0.6\",\n  \"description\": \"Hand-curated open source repository for Claude Code tools, agents and skills\",\n  \"owner\": {\n    \"name\": \"Claude Market\",\n    \"url\": \"https://github.com/claude-market/marketplace\"\n  },\n  \"pluginRoot\": \"./\",\n  \"plugins\": [\n    {\n      \"name\": \"plugin-builder\",\n      \"source\": \"./plugin-builder\",\n      \"version\": \"1.3.2\",\n      \"description\": \"Interactive plugin builder for Claude Code - serves as both an example plugin and a tool to create new plugins through guided prompts with specialized builder skills for each component type\",\n      \"author\": {\n        \"name\": \"Daniel Kovacs\",\n        \"email\": \"kovacsemod@gmail.com\",\n        \"url\": \"https://github.com/danielkov\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"plugin\",\n        \"builder\",\n        \"development\",\n        \"scaffolding\",\n        \"example\"\n      ],\n      \"commands\": [\n        \"./commands/init.md\",\n        \"./commands/add.md\",\n        \"./commands/edit.md\",\n        \"./commands/validate.md\",\n        \"./commands/publish.md\"\n      ],\n      \"skills\": [\n        \"./skills/cc-skill-builder.md\",\n        \"./skills/cc-command-builder.md\",\n        \"./skills/cc-agent-builder.md\",\n        \"./skills/cc-hook-builder.md\",\n        \"./skills/cc-mcp-builder.md\"\n      ]\n    },\n    {\n      \"name\": \"specforge\",\n      \"source\": \"./specforge\",\n      \"version\": \"0.1.2\",\n      \"description\": \"Schema-first development ecosystem with dual-spec workflows (OpenAPI + DB schema) for building production-ready applications through deterministic code generation and intelligent orchestration\",\n      \"author\": {\n        \"name\": \"Daniel Emod Kovacs\",\n        \"url\": \"https://github.com/danielkov\"\n      },\n      \"license\": \"MIT\",\n      \"repository\": \"https://github.com/claude-market/marketplace/tree/main/specforge\",\n      \"keywords\": [\n        \"specforge\",\n        \"openapi\",\n        \"schema-first\",\n        \"code-generation\",\n        \"orchestration\",\n        \"api-development\",\n        \"database-schema\",\n        \"type-safety\"\n      ],\n      \"commands\": [\n        \"./commands/init.md\",\n        \"./commands/plan.md\",\n        \"./commands/build.md\",\n        \"./commands/validate.md\",\n        \"./commands/test.md\",\n        \"./commands/ship.md\",\n        \"./commands/sync.md\"\n      ],\n      \"agents\": [\n        \"./agents/captain-orchestrator.md\",\n        \"./agents/planning-agent.md\",\n        \"./agents/validation-agent.md\"\n      ],\n      \"skills\": [\n        \"./skills/openapi-expert.md\",\n        \"./skills/stack-advisor.md\",\n        \"./skills/integration-expert.md\"\n      ]\n    },\n    {\n      \"name\": \"specforge-backend-rust-axum\",\n      \"source\": \"./specforge-backend-rust-axum\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Rust + Axum backend framework expertise - implements handlers using OpenAPI and DB-generated types\",\n      \"author\": {\n        \"name\": \"Daniel Emod Kovacs\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"specforge\",\n        \"backend\",\n        \"rust\",\n        \"axum\",\n        \"openapi\",\n        \"framework\",\n        \"distributed-systems\"\n      ],\n      \"agents\": [\n        \"./agents/handler-implementer.md\",\n        \"./agents/test-generator.md\",\n        \"./agents/router-builder.md\"\n      ],\n      \"skills\": [\n        \"./skills/axum-patterns.md\",\n        \"./skills/axum-handler-implementation.md\",\n        \"./skills/axum-error-mapping.md\",\n        \"./skills/rust-testing.md\",\n        \"./skills/axum-middleware.md\",\n        \"./skills/rust-openapi-integration.md\",\n        \"./skills/rust-dev-setup.md\",\n        \"./skills/axum-idempotency.md\",\n        \"./skills/axum-resilience.md\",\n        \"./skills/axum-tracing.md\",\n        \"./skills/axum-timeouts.md\",\n        \"./skills/axum-health-shutdown.md\",\n        \"./skills/axum-rate-limiting.md\"\n      ]\n    }\n  ]\n}\n",
        "plugin-builder/.claude-plugin/plugin.json": "{\n  \"name\": \"plugin-builder\",\n  \"version\": \"1.3.2\",\n  \"description\": \"Interactive plugin builder for Claude Code - serves as both an example plugin and a tool to create new plugins through guided prompts with specialized builder skills for each component type\",\n  \"author\": {\n    \"name\": \"Daniel Kovacs\",\n    \"email\": \"kovacsemod@gmail.com\",\n    \"url\": \"https://github.com/danielkov\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\"plugin\", \"builder\", \"development\", \"scaffolding\", \"example\"],\n  \"commands\": [\n    \"./commands/init.md\",\n    \"./commands/add.md\",\n    \"./commands/edit.md\",\n    \"./commands/validate.md\",\n    \"./commands/publish.md\"\n  ],\n  \"skills\": [\n    \"./skills/cc-skill-builder.md\",\n    \"./skills/cc-command-builder.md\",\n    \"./skills/cc-agent-builder.md\",\n    \"./skills/cc-hook-builder.md\",\n    \"./skills/cc-mcp-builder.md\"\n  ]\n}\n",
        "plugin-builder/README.md": "# Plugin Builder\n\nInteractive plugin builder for Claude Code - serves as both an example plugin and a tool to create new plugins through guided prompts.\n\n## Overview\n\nThe Plugin Builder helps you create high-quality Claude Code plugins through an interactive, question-based workflow. It handles all the scaffolding and ensures your plugins follow best practices.\n\n## Features\n\n- **Modular builder architecture** - Specialized builder skills for each component type (commands, agents, hooks, skills, MCP servers)\n- **Guided plugin creation** - Interactive prompts walk you through creating any type of component\n- **Multi-component support** - Create commands, agents, hooks, skills, and MCP servers in one session\n- **Best practices built-in** - Each builder skill follows industry best practices and context engineering principles\n- **Natural language editing** - Edit existing components by describing changes in plain English\n- **Example reference** - The plugin itself demonstrates proper plugin structure\n- **Validation tools** - Check your plugins for common issues before publishing\n\n## Installation\n\n```bash\n/plugin install ./plugin-builder\n```\n\nOr if adding from the Claude Market marketplace:\n\n```bash\n/plugin marketplace add claude-market/marketplace\n/plugin install plugin-builder\n```\n\n## Commands\n\n### `/plugin-builder:init`\n\nInitialize a new plugin with guided prompts.\n\n**Workflow:**\n\n1. Asks for your GitHub username (for CODEOWNERS)\n2. Collects plugin metadata (name, description, license)\n3. Creates top-level plugin directory (`./{plugin-name}/`)\n4. Asks what components you want to create (commands, agents, hooks, skills, MCP servers)\n5. For each component, asks detailed questions to understand requirements\n6. Generates all files with comprehensive, well-structured prompts\n7. Creates plugin manifest, CODEOWNERS, and README\n8. Shows summary and installation instructions\n\n**Example usage:**\n\n```\n/plugin-builder:init\n```\n\n### `/plugin-builder:add`\n\nAdd a new component to an existing plugin using specialized builder skills.\n\n**Use this when:**\n\n- You already have a plugin and want to add another command, agent, hook, skill, or MCP server\n- You want to expand your plugin's functionality\n\n**Workflow:**\n\n1. Select which plugin to add to\n2. Choose component type to add (Command, Agent, Hook, Skill, or MCP Server)\n3. Provide basic information (name and brief description)\n4. Appropriate builder skill is invoked to guide you through detailed creation:\n   - **Commands**: `cc-command-builder` handles slash command creation\n   - **Agents**: `cc-agent-builder` handles subagent creation\n   - **Hooks**: `cc-hook-builder` handles hook configuration\n   - **Skills**: `cc-skill-builder` handles skill creation\n   - **MCP Servers**: `cc-mcp-builder` handles MCP server configuration\n5. Builder skill generates the component following best practices\n6. Updates plugin.json and README automatically\n\n**Example usage:**\n\n```\n/plugin-builder:add\n```\n\n### `/plugin-builder:edit`\n\nUse natural language to make edits to your existing plugin components with specialized builder skill assistance.\n\n**Use this when:**\n\n- You want to modify an existing command, agent, hook, skill, or MCP server\n- You need to update functionality, fix issues, or refactor a component\n- You want to make changes using conversational descriptions instead of manual file editing\n\n**Workflow:**\n\n1. Select which plugin to edit\n2. Choose which component to modify\n3. Describe your desired changes in natural language\n4. Appropriate builder skill is invoked with context about the existing component:\n   - **Commands**: `cc-command-builder` helps apply changes\n   - **Agents**: `cc-agent-builder` helps apply changes\n   - **Hooks**: `cc-hook-builder` helps apply changes\n   - **Skills**: `cc-skill-builder` helps apply changes\n   - **MCP Servers**: `cc-mcp-builder` helps apply changes\n5. Builder skill interprets your intent and applies edits following best practices\n6. Review the changes and confirm\n7. Optionally update README and version number\n\n**Example natural language edits:**\n\n- \"Add validation for email addresses\"\n- \"Make it ask for confirmation before deleting\"\n- \"Change the default model from haiku to sonnet\"\n- \"Add better error handling\"\n- \"Include usage examples in the documentation\"\n- \"Make it work with TypeScript files too\"\n\n**Example usage:**\n\n```\n/plugin-builder:edit\n```\n\n### `/plugin-builder:validate`\n\nValidate plugin structure and configuration.\n\n**Checks:**\n\n- Required files exist (plugin.json, component files)\n- Directory structure is correct\n- JSON files are valid\n- Plugin manifest has required fields\n- Component files match what's listed in plugin.json\n- Names follow kebab-case convention\n- No duplicate component names\n- README exists and contains key information\n\n**Provides:**\n\n- ✓ List of passed checks\n- ✗ List of failed checks with fixes\n- ⚠ Warnings and recommendations\n- Actionable next steps\n\n**Example usage:**\n\n```\n/plugin-builder:validate\n```\n\n### `/plugin-builder:publish`\n\nPublish your plugin changes to the Claude Market marketplace with automated validation, manifest generation, and pull request creation.\n\n**Use this when:**\n\n- You've made changes to a plugin and want to submit them to the marketplace\n- You want to add a new plugin to the Claude Market\n- You need an automated workflow from validation to PR creation\n\n**Workflow:**\n\n1. **Detect changes**: Runs `git diff main` to identify which plugins have been modified\n2. **Validate in parallel**: Runs `/plugin-builder:validate` for each changed plugin simultaneously\n3. **Generate manifest**: Runs `make generate-marketplace-json` to update `.claude-plugin/marketplace.json`\n4. **Create semantic commit**: Generates a commit message following conventional commits format\n5. **Branch management**: Creates a new branch (if on main) in format `{user}/{plugin}/{description}`\n6. **Create PR**: Uses GitHub CLI to create a pull request with pre-filled template\n\n**Requirements:**\n\n- Changes must be committed to a git branch\n- All changed plugins must pass validation\n- `make` must be available (for marketplace.json generation)\n- `gh` CLI is recommended but optional (for automated PR creation)\n\n**Example usage:**\n\n```\n/plugin-builder:publish\n```\n\n**What it does:**\n\n- Validates all changed plugins in parallel\n- Stops if any validation fails\n- Generates semantic commit message like `feat(plugin-name): added new command`\n- Creates branch like `danielkov/plugin-name/add-new-command`\n- Opens PR with filled-in template including plugin metadata, components, and testing checklist\n\n**Without GitHub CLI:**\n\nIf `gh` CLI is not installed, the command will provide a manual PR link and suggest installing GitHub CLI for future use.\n\n## Builder Skills\n\nThe plugin-builder includes specialized builder skills for each Claude Code component type. These skills are invoked automatically by the `/add` and `/edit` commands to provide expert guidance following industry best practices.\n\n### `cc-skill-builder`\n\nClaude Code Skill Builder - Expert guidance for creating highly effective Claude Code skills.\n\n**Provides:**\n\n- Skill structure (YAML frontmatter + markdown content)\n- Effective prompt engineering techniques\n- Tool permission optimization\n- Progressive disclosure patterns\n- Resource organization (scripts, references, assets)\n- Common skill patterns and workflows\n\n**Based on:** [Claude Skills Deep Dive](https://leehanchung.github.io/blogs/2025/10/26/claude-skills-deep-dive/)\n\n### `cc-command-builder`\n\nSlash Command Builder - Expert guidance for creating slash commands.\n\n**Provides:**\n\n- Markdown structure with optional YAML frontmatter\n- Argument handling ($ARGUMENTS vs positional parameters)\n- Tool permissions and security\n- Bash execution and file references\n- Prompt engineering for commands\n\n**Based on:** [Claude Code Slash Commands Docs](https://docs.claude.com/en/docs/claude-code/slash-commands)\n\n### `cc-agent-builder`\n\nSubagent Builder - Expert guidance for creating specialized subagents.\n\n**Provides:**\n\n- System prompt design\n- Context isolation strategies\n- Tool permissions (inherit vs explicit)\n- Model selection guidance\n- Workflow and success criteria definition\n\n**Based on:** [Claude Code Subagents Docs](https://docs.claude.com/en/docs/claude-code/sub-agents)\n\n### `cc-hook-builder`\n\nHook Builder - Expert guidance for creating workflow automation hooks.\n\n**Provides:**\n\n- Hook event selection (PreToolUse, PostToolUse, etc.)\n- Matcher configuration for targeting tools\n- Shell script creation with jq parsing\n- Blocking vs non-blocking operations\n- Security best practices\n\n**Based on:** [Claude Code Hooks Guide](https://docs.claude.com/en/docs/claude-code/hooks-guide)\n\n### `cc-mcp-builder`\n\nMCP Server Configuration Builder - Expert guidance for configuring MCP servers.\n\n**Provides:**\n\n- Transport type selection (HTTP, stdio, SSE)\n- Environment variable handling\n- Authentication configuration\n- Scope selection (project vs user)\n- Platform-specific considerations\n\n**Based on:** [Claude Code MCP Docs](https://docs.claude.com/en/docs/claude-code/mcp)\n\n### How Builder Skills Work\n\nBuilder skills follow the [Anthropic Context Engineering Guide](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents) principles:\n\n1. **Progressive Disclosure** - Information revealed incrementally as needed\n2. **Minimal Context** - Only essential details in frontmatter, detailed guidance on invocation\n3. **Tool Efficiency** - Curated minimal tool sets for each builder\n4. **Structured Guidance** - Clear step-by-step workflows\n5. **Best Practices** - Industry-standard patterns and anti-patterns\n\nWhen you use `/plugin-builder:add` or `/plugin-builder:edit`, the appropriate builder skill is automatically invoked to guide you through creation or modification with expert knowledge specific to that component type.\n\n## Component Types\n\n### Slash Commands\n\nCustom shortcuts invoked as `/plugin-name:command-name`. Use for frequently-used operations, scaffolding, or complex workflows.\n\n### Agents (Subagents)\n\nSpecialized agents for specific development tasks. They can have restricted tool access and custom prompts optimized for their purpose.\n\n### Hooks\n\nBehavior customizations that run at key workflow points (before user input, before/after tool calls, agent start/end).\n\n### Skills\n\nDomain-specific expertise that can be invoked when needed. Skills provide specialized knowledge without always being active.\n\n### MCP Servers\n\nIntegration with Model Context Protocol servers to add external tools and data sources.\n\n## Example Plugin Structure\n\nThis plugin itself demonstrates proper structure:\n\n```\nplugin-builder/\n├── .claude-plugin/\n│   └── plugin.json              # Plugin manifest\n├── commands/\n│   ├── init.md                  # Init command\n│   ├── add.md                   # Add command (routes to builder skills)\n│   ├── edit.md                  # Edit command (routes to builder skills)\n│   ├── validate.md              # Validate command\n│   └── publish.md               # Publish command\n├── skills/\n│   ├── cc-skill-builder.md      # Skill builder\n│   ├── cc-command-builder.md    # Slash command builder\n│   ├── cc-agent-builder.md      # Subagent builder\n│   ├── cc-hook-builder.md       # Hook builder\n│   └── cc-mcp-builder.md        # MCP server builder\n├── CODEOWNERS                   # Maintainers\n├── LICENSE                      # MIT License\n└── README.md                    # This file\n```\n\n## Best Practices\n\nWhen creating plugins with this tool:\n\n1. **Write detailed prompts** - The quality of your plugin depends on clear, comprehensive instructions\n2. **Include examples** - Show expected behavior and usage patterns\n3. **Handle edge cases** - Think about what could go wrong and address it\n4. **Use proper naming** - Always use kebab-case for component names\n5. **Provide clear descriptions** - Help users understand what each component does\n6. **Add keywords** - Improve discoverability in marketplaces\n7. **Test before publishing** - Use `/plugin-builder:validate` to check for issues\n\n## Plugin Manifest Reference\n\nThe plugin.json file structure:\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"version\": \"1.0.0\",\n  \"description\": \"What this plugin does\",\n  \"author\": \"Your Name\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"relevant\", \"keywords\"],\n  \"commands\": [\n    {\n      \"name\": \"command-name\",\n      \"description\": \"What the command does\"\n    }\n  ],\n  \"agents\": [\n    {\n      \"name\": \"agent-name\",\n      \"description\": \"What the agent does\"\n    }\n  ],\n  \"hooks\": [\n    {\n      \"name\": \"hook-name\",\n      \"description\": \"What the hook does\"\n    }\n  ],\n  \"skills\": [\n    {\n      \"name\": \"skill-name\",\n      \"description\": \"What the skill does\"\n    }\n  ],\n  \"mcpServers\": [\n    {\n      \"name\": \"server-name\",\n      \"description\": \"What the server provides\"\n    }\n  ]\n}\n```\n\n## Contributing to Claude Market\n\nOnce you've created and validated your plugin:\n\n1. Test it locally: `/plugin install ./{plugin-name}`\n2. Ensure validation passes: `/plugin-builder:validate`\n3. Submit to Claude Market by creating a PR that adds your plugin to `.claude-plugin/marketplace.json`\n4. The CODEOWNERS file ensures proper review by maintainers and plugin authors\n\n## License\n\nMIT\n\n## Learn More\n\n- [Claude Code Plugins Documentation](https://docs.claude.com/en/docs/claude-code/plugins)\n- [Plugin Marketplaces](https://docs.claude.com/en/docs/claude-code/plugin-marketplaces)\n- [Claude Code](https://claude.com/claude-code)\n",
        "plugin-builder/commands/add.md": "---\ndescription: Add a new component to an existing plugin\n---\n\nYou are helping a user add a new component to an existing Claude Code plugin.\n\n## Step 1: Select Plugin\n\nUse Glob to find all plugins in `./*/.claude-plugin/plugin.json` and ask the user which plugin they want to add to (or let them specify a path).\n\n## Step 2: Read Current Plugin Configuration\n\nRead the plugin.json to understand what components already exist.\n\n## Step 3: Select Component Type to Add\n\nUse AskUserQuestion to ask what type of component they want to add:\n\n- **Slash Command** - Reusable prompt template for frequent operations\n- **Agent (Subagent)** - Specialized AI assistant for specific tasks\n- **Hook** - Automated workflow trigger at lifecycle events\n- **Skill** - Domain-specific expertise invoked when needed\n- **MCP Server** - External tool/data source via Model Context Protocol\n\nPresent these as clear options explaining what each type does.\n\n## Step 4: Collect Basic Information\n\nBased on the selected component type, collect the essential information:\n\n### For Slash Command:\n\nUse AskUserQuestion to collect:\n\n1. **Command name** (kebab-case, must not conflict with existing commands)\n2. **Brief description** (what does this command do?)\n\n### For Agent (Subagent):\n\nUse AskUserQuestion to collect:\n\n1. **Agent name** (kebab-case, must not conflict with existing agents)\n2. **Brief description** (when should this agent be invoked?)\n\n### For Hook:\n\nUse AskUserQuestion to collect:\n\n1. **Hook name** (kebab-case identifier)\n2. **Brief description** (what workflow does this automate?)\n\n### For Skill:\n\nUse AskUserQuestion to collect:\n\n1. **Skill name** (kebab-case, must not conflict with existing skills)\n2. **Brief description** (what domain expertise does this provide?)\n\n### For MCP Server:\n\nUse AskUserQuestion to collect:\n\n1. **Server name** (kebab-case identifier)\n2. **Brief description** (what tools/data does this provide?)\n\n## Step 5: Invoke Appropriate Builder Skill\n\nBased on the component type selected, invoke the corresponding builder skill to handle the detailed generation:\n\n### For Slash Command:\n\nInvoke the `cc-command-builder` skill:\n\n```\nCreate a new Claude Code slash command with the following details:\n- Plugin: [plugin-name]\n- Command name: [name]\n- Description: [description]\n- File path: [plugin-path]/commands/[command-name].md\n\nPlease guide the user through creating this command following best practices.\n```\n\n### For Agent (Subagent):\n\nInvoke the `cc-agent-builder` skill:\n\n```\nCreate a new Claude Code subagent with the following details:\n- Plugin: [plugin-name]\n- Agent name: [name]\n- Description: [description]\n- File path: [plugin-path]/agents/[agent-name].md\n\nPlease guide the user through creating this subagent following best practices.\n```\n\n### For Hook:\n\nInvoke the `cc-hook-builder` skill:\n\n```\nCreate a new Claude Code hook with the following details:\n- Plugin: [plugin-name]\n- Hook name: [name]\n- Description: [description]\n- File path: [plugin-path]/hooks/[hook-name].json\n\nPlease guide the user through creating this hook following best practices.\n```\n\n### For Skill:\n\nInvoke the `cc-skill-builder` skill:\n\n```\nCreate a new Claude Code skill with the following details:\n- Plugin: [plugin-name]\n- Skill name: [name]\n- Description: [description]\n- File path: [plugin-path]/skills/[skill-name].md\n\nPlease guide the user through creating this skill following best practices.\n```\n\n### For MCP Server:\n\nInvoke the `cc-mcp-builder` skill:\n\n```\nConfigure a new MCP server with the following details:\n- Plugin: [plugin-name]\n- Server name: [name]\n- Description: [description]\n- File path: [plugin-path]/mcp-servers/[server-name].json\n\nPlease guide the user through configuring this MCP server following best practices.\n```\n\n## Step 6: Update Plugin Manifest (Post-Generation)\n\n**IMPORTANT:** After the builder skill completes and creates the component file, you must update plugin.json to include the new component.\n\nAdd the new component file path to the appropriate array in plugin.json:\n\n- **commands**: Add to the `commands` array (e.g., `\"./commands/my-command.md\"`)\n- **agents**: Add to the `agents` array (e.g., `\"./agents/my-agent.md\"`)\n- **hooks**: Add path or inline config to `hooks` field\n- **skills**: Add to the `skills` array (e.g., `\"./skills/my-skill.md\"`)\n- **mcpServers**: Add path or inline config to `mcpServers` field\n\nAll paths must be relative to plugin root and begin with `./`\n\n**Example:**\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"commands\": [\"./commands/existing-command.md\", \"./commands/new-command.md\"],\n  \"agents\": [\"./agents/existing-agent.md\"],\n  \"skills\": [\"./skills/new-skill.md\"]\n}\n```\n\n## Step 7: Update README (Post-Generation)\n\nUpdate the plugin's README.md to document the new component with usage examples.\n\n## Step 8: Summary\n\nShow the user:\n\n- What was added (component type and name)\n- File path of the new component\n- Updated plugin.json showing the new component\n- How to use the new component\n- How to test it\n\n## Important Notes:\n\n- **Modular approach**: Each component type has its own specialized builder skill that handles the detailed generation\n- **Ensure no conflicts**: Check that component names don't conflict with existing ones before invoking builder skills\n- **Create directories**: Create component directories (commands/, agents/, hooks/, skills/, mcp-servers/) if they don't exist\n- **Update manifest**: Always update plugin.json after component creation\n- **Maintain consistency**: Ensure new components follow the style of existing ones\n- **Version bump**: Increment patch version in plugin.json (e.g., 1.0.0 → 1.0.1)\n",
        "plugin-builder/commands/edit.md": "---\ndescription: Edit an existing component in your plugin\n---\n\nYou are helping a user edit an existing component in a Claude Code plugin using natural language descriptions.\n\n## Step 1: Select Plugin\n\nUse Glob to find all plugins in `./*/.claude-plugin/plugin.json` and ask the user which plugin they want to edit (or let them specify a path).\n\n## Step 2: Read Current Plugin Configuration\n\nRead the plugin.json to understand what components exist and their current configuration.\n\n## Step 3: List Available Components\n\nDisplay all available components organized by type:\n\n- **Commands**: List all command files with their descriptions (from frontmatter if available)\n- **Agents**: List all agent files with their descriptions\n- **Hooks**: List all hook configurations\n- **Skills**: List all skill files with their descriptions\n- **MCP Servers**: List all MCP server configurations\n\nUse AskUserQuestion to ask which component they want to edit. Present options based on what exists in the plugin.\n\n## Step 4: Read the Selected Component\n\nRead the full content of the selected component file to understand its current implementation.\n\nDisplay a summary of the component to the user:\n\n- Component name and type\n- Current description\n- Key functionality (summarized)\n- File path\n\n## Step 5: Understand the Desired Edit\n\nAsk the user to describe what changes they want to make in natural language.\n\n**Prompt**: \"Please describe the changes you want to make to this component. Be as specific or general as you like.\"\n\nThe user might say things like:\n\n- \"Add validation for email addresses\"\n- \"Make it ask for confirmation before deleting\"\n- \"Change the default model from haiku to sonnet\"\n- \"Add better error handling\"\n- \"Include examples in the documentation\"\n- \"Make it work with TypeScript files too\"\n- \"Update the description to be clearer\"\n- \"Add support for multiple arguments\"\n\n## Step 6: Route to Appropriate Builder Skill\n\nBased on the component type, route the editing task to the corresponding builder skill with context about the existing component and desired changes:\n\n### For Slash Command:\n\nInvoke the `cc-command-builder` skill:\n\n```\nEdit an existing Claude Code slash command with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Command name: [name]\n- File path: [path]\n- Current content:\n[paste current content]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for slash commands.\n```\n\n### For Agent (Subagent):\n\nInvoke the `cc-agent-builder` skill:\n\n```\nEdit an existing Claude Code subagent with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Agent name: [name]\n- File path: [path]\n- Current content:\n[paste current content]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for subagents.\n```\n\n### For Hook:\n\nInvoke the `cc-hook-builder` skill:\n\n```\nEdit an existing Claude Code hook with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Hook name: [name]\n- File path: [path]\n- Current configuration:\n[paste current JSON]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for hooks.\n```\n\n### For Skill:\n\nInvoke the `cc-skill-builder` skill:\n\n```\nEdit an existing Claude Code skill with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Skill name: [name]\n- File path: [path]\n- Current content:\n[paste current content]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for skills.\n```\n\n### For MCP Server:\n\nInvoke the `cc-mcp-builder` skill:\n\n```\nEdit an existing MCP server configuration with the following details:\n\n**Existing Component:**\n- Plugin: [plugin-name]\n- Server name: [name]\n- File path: [path]\n- Current configuration:\n[paste current JSON]\n\n**Requested Changes:**\n[user's description of what they want to change]\n\nPlease help apply these changes following best practices for MCP servers.\n```\n\n## Step 7: Update Plugin Metadata (If Needed)\n\nAfter the builder skill completes the edits, determine if other files need updating:\n\n- **If component description changed significantly**: Offer to update the README.md\n- **If functionality changed**: Offer to update the plugin.json description or keywords\n- **If the edit is substantial**: Offer to increment the version number (patch bump)\n\nUse AskUserQuestion to ask if they want to update these related files.\n\n## Step 8: Apply Additional Updates\n\nIf the user agreed to update related files:\n\n### Update README:\n\n- Find the section documenting this component\n- Update the usage examples or description\n- Ensure it reflects the new behavior\n\n### Update Plugin.json:\n\n- Increment version (e.g., 1.0.0 → 1.0.1 for patch changes)\n- Update keywords if new functionality was added\n- Update description if plugin's overall purpose expanded\n\n## Step 9: Summary\n\nProvide the user with a clear summary:\n\n- **Component edited**: Name and file path\n- **Changes made**: Concise summary of what was modified\n- **Files updated**: List all files that were changed (component + README/manifest if applicable)\n- **New version**: If version was bumped\n- **Testing recommendation**: Suggest how to test the changes\n\n## Important Guidelines:\n\n### Modular Approach:\n\n- Each component type has its own specialized builder skill\n- Route editing tasks to the appropriate skill based on component type\n- Builder skills understand best practices for their component type\n- This ensures consistent, high-quality edits\n\n### Interpreting Natural Language:\n\n- **Be intelligent about intent**: If user says \"make it faster\", interpret based on context (use haiku model, optimize steps, etc.)\n- **Ask for clarification** if the request is genuinely ambiguous\n- **Make reasonable assumptions** for small details, but confirm major changes\n- **Preserve existing functionality** unless explicitly asked to remove it\n\n### Edit Best Practices:\n\n- **Make surgical changes**: Only modify what's necessary\n- **Preserve formatting**: Maintain the existing markdown/JSON style\n- **Keep consistency**: Match the style of the original component\n- **Test mentally**: Think through whether edits will work as intended\n- **Respect the component's purpose**: Don't change what the component fundamentally does unless explicitly asked\n\n### Validation:\n\n- **For .md files**: Ensure frontmatter is valid, markdown is well-formed\n- **For .json files**: Validate JSON syntax, ensure required fields are present\n- **For paths**: Ensure any file paths referenced still exist and are correct\n- **For tool usage**: Ensure tools referenced in prompts actually exist\n\n### Communication:\n\n- **Show before/after** for significant changes (builder skill handles this)\n- **Explain interpretation** of their natural language request\n- **Highlight assumptions** made during editing\n- **Offer to refine** if the edit wasn't quite what they wanted\n\n## Example Interaction Flow:\n\n1. Find plugins → user selects \"plugin-builder\"\n2. Read plugin.json → show components: init, add, validate, edit commands\n3. User selects → \"add command\"\n4. Read component → display current add.md implementation summary\n5. User describes → \"Make it use the new modular builder skill approach\"\n6. Route to `cc-command-builder` skill with context\n7. Builder skill helps apply changes following best practices\n8. Offer to update README with new capability\n9. Update README if accepted\n10. Version bump → Increment to next version\n11. Summary → List all changes and testing instructions\n\n## Edge Cases to Handle:\n\n- **Component doesn't exist**: Guide user back to component selection\n- **Invalid edit request**: Ask for clarification if request doesn't make sense for this component type\n- **Conflicting changes**: Warn if edit might break existing functionality\n- **Syntax errors**: Builder skill should catch and fix syntax issues\n- **Multiple files**: If component spans multiple files (like hooks with shell scripts), edit all relevant ones\n\nBegin by finding available plugins and asking which one to edit!\n",
        "plugin-builder/commands/init.md": "---\ndescription: Initialize a new plugin with guided prompts\n---\n\nYou are helping a user create a new Claude Code plugin. Follow this workflow step by step:\n\n## Step 1: Get GitHub Username and Fetch Profile\n\nUse the AskUserQuestion tool to ask for:\n- **GitHub username** - Will be used in CODEOWNERS and to fetch author information\n\nOnce you have the GitHub username, use WebFetch to fetch their profile:\n- URL: `https://api.github.com/users/{username}`\n- Extract the `name` field from the response\n- If `name` is present and not null, use it as the author name\n- If `name` is null or missing, use the GitHub username as the author name\n\n## Step 2: Collect Plugin Metadata\n\nAsk the user for:\n\n- **Plugin name** (kebab-case identifier, unique)\n- **Description** (what does this plugin do?)\n- **Version** (default: \"1.0.0\" if not specified)\n- **License** (default: \"MIT\" if unsure)\n- **Homepage** (optional - documentation URL)\n- **Repository** (optional - source code URL, can default to GitHub repo if they want)\n- **Keywords** (optional - array of tags for discoverability)\n- **Author email** (optional - contact email)\n- **Author URL** (optional - personal website/profile)\n\n## Step 3: Create Plugin Directory\n\nCreate the directory structure at the top level:\n\n```\n./{plugin-name}/\n./{plugin-name}/.claude-plugin/ # required\n./{plugin-name}/commands/ # optional - only if 1 or more commands added\n./{plugin-name}/agents/ # optional - only if 1 or more agents added\n./{plugin-name}/hooks/ # optional - only if 1 or more hooks added\n./{plugin-name}/skills/ # optional - only if 1 or more skills added\n./{plugin-name}/mcp-servers/ # optional - only if 1 or more MCP servers added\n```\n\n## Step 4: Select Components to Create\n\nUse AskUserQuestion with multiSelect=true to ask what components they want to create:\n\n- Slash Command\n- Agent (Subagent)\n- Hook\n- Skill\n- MCP Server\n\n## Step 5: For Each Component Type, Collect Details\n\n### If they selected \"Slash Command\":\n\nAsk these questions (you can ask multiple in one AskUserQuestion call):\n\n1. **Command name** (kebab-case, will be invoked as /plugin-name:command-name)\n2. **Command description** (one-line summary)\n3. **What should this command do?** (detailed explanation of the command's purpose and behavior)\n4. **What files/resources will it need to read or modify?**\n5. **Should it use any specific tools?** (e.g., Bash, Read, Edit, Grep, etc.)\n\nThen create:\n\n- `./{plugin-name}/commands/{command-name}.md` with a comprehensive prompt that:\n  - Clearly defines the command's purpose\n  - Provides step-by-step instructions\n  - Specifies which tools to use\n  - Includes examples if relevant\n  - Handles edge cases\n\n### If they selected \"Agent\":\n\nAsk these questions:\n\n1. **Agent name** (kebab-case)\n2. **Agent description** (what specialized task does it perform?)\n3. **What problem does this agent solve?**\n4. **What tools should it have access to?** (all tools, or specific subset?)\n5. **What should be the default model?** (haiku for quick tasks, sonnet for complex)\n6. **Any specific workflow or steps it should follow?**\n\nThen create:\n\n- `./{plugin-name}/agents/{agent-name}.md` with a detailed agent prompt that:\n  - Defines the agent's specialized role\n  - Specifies available tools\n  - Provides clear workflow steps\n  - Includes examples and best practices\n  - Defines success criteria\n\n### If they selected \"Hook\":\n\nAsk these questions:\n\n1. **Hook type** (choose one):\n   - user-prompt-submit (runs before user input is sent)\n   - tool-call (runs before/after tool execution)\n   - agent-start (runs when agent starts)\n   - agent-end (runs when agent completes)\n2. **Hook name** (descriptive name)\n3. **What behavior should this hook add/modify?**\n4. **Should it block certain actions or just add information?**\n5. **What command should it run?** (shell command)\n\nThen create:\n\n- `./{plugin-name}/hooks/{hook-name}.json` with proper hook configuration including:\n  - Hook type\n  - Trigger conditions\n  - Command to execute\n  - Whether it should block on failure\n\n### If they selected \"Skill\":\n\nAsk these questions:\n\n1. **Skill name** (kebab-case)\n2. **What domain/technology does this skill cover?**\n3. **What specialized knowledge or capabilities should it provide?**\n4. **What tools does it need access to?**\n5. **What specific tasks should users invoke it for?**\n\nThen create:\n\n- `./{plugin-name}/skills/{skill-name}.md` with a comprehensive skill definition that:\n  - Defines the domain expertise\n  - Lists specific capabilities\n  - Provides usage patterns\n  - Includes domain-specific best practices\n  - Specifies when to use this skill\n\n### If they selected \"MCP Server\":\n\nAsk these questions:\n\n1. **Server name** (kebab-case)\n2. **What tools/resources does this MCP server provide?**\n3. **Connection details** (stdio command, or SSE URL)\n4. **Any environment variables needed?**\n5. **What Claude Code features should it enable?**\n\nThen create:\n\n- `./{plugin-name}/mcp-servers/{server-name}.json` with MCP server configuration\n\n## Step 6: Create Plugin Manifest\n\nCreate `./{plugin-name}/.claude-plugin/plugin.json` following the complete schema:\n\n### Required Fields\n\n- **name** (string): Plugin identifier in kebab-case, unique across all plugins\n\n### Metadata Fields (All Optional)\n\n- **version** (string): Semantic versioning (e.g., \"1.0.0\", \"2.1.0\")\n- **description** (string): Brief explanation of plugin purpose\n- **author** (object): Author information with these properties:\n  - `name` (string): Author or organization name (from GitHub profile or username)\n  - `email` (string, optional): Contact email address\n  - `url` (string, optional): Author's website or profile URL\n- **homepage** (string): Documentation URL link\n- **repository** (string): Source code repository URL\n- **license** (string): License identifier (MIT, Apache-2.0, etc.)\n- **keywords** (array of strings): Discovery and categorization tags\n\n### Component Path Fields (Optional)\n\nList the components you want to include in your plugin:\n\n- **commands** (string or array): Command markdown files (e.g., `[\"./commands/init.md\", \"./commands/add.md\"]`)\n- **agents** (string or array): Agent markdown files (e.g., `[\"./agents/optimizer.md\"]`)\n- **hooks** (string or object): Hook configuration file path or inline JSON config\n- **mcpServers** (string or object): MCP server configuration file path or inline config\n- **skills** (string or array): Skill markdown files (e.g., `[\"./skills/react.md\"]`)\n\nAll paths must be relative to plugin root and begin with `./`\n\n### Example Plugin Manifest\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"version\": \"1.0.0\",\n  \"description\": \"A helpful plugin for productivity\",\n  \"author\": {\n    \"name\": \"John Doe\",\n    \"email\": \"john@example.com\",\n    \"url\": \"https://github.com/johndoe\"\n  },\n  \"homepage\": \"https://github.com/johndoe/my-plugin\",\n  \"repository\": \"https://github.com/johndoe/my-plugin\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"helper\", \"productivity\", \"automation\"],\n  \"commands\": [\n    \"./commands/helper.md\"\n  ],\n  \"agents\": [\n    \"./agents/assistant.md\"\n  ]\n}\n```\n\n**Schema Notes:**\n- Only `name` is required; all other fields are optional\n- `author` must be an object (not a string) if provided\n- Components must be explicitly listed in their respective arrays (e.g., commands, agents, skills)\n- Include only fields that have values; omit empty/null fields\n\n## Step 7: Create CODEOWNERS\n\nCreate `./{plugin-name}/CODEOWNERS` file with the following format:\n\n```\n# Plugin maintainers and reviewers\n* @claude-market @{github-username}\n```\n\nReplace:\n- `{github-username}` with the GitHub username from Step 1\n\nThis ensures that:\n- The Claude Market organization is always notified\n- The plugin creator's GitHub account is tagged for review\n\n## Step 8: Create README\n\nCreate a README.md in the plugin directory that includes:\n\n- Plugin name and description\n- Installation instructions (`/plugin install ./{plugin-name}`)\n- List of components with usage examples\n- Requirements (if any)\n- License information\n\n## Step 9: Summary\n\nProvide the user with:\n\n- Path to their new plugin\n- List of all created files\n- Next steps (how to test it, how to install it locally, how to submit to marketplace)\n- Command to install locally: `/plugin install ./{plugin-name}`\n\n## Important Guidelines:\n\n- **Write comprehensive, detailed prompts** for commands/agents/skills. The quality of the plugin depends on clear, actionable instructions.\n- **Include examples** wherever possible to illustrate expected behavior.\n- **Think about edge cases** and include handling for them.\n- **Use proper markdown formatting** including code blocks, lists, and sections.\n- **Follow Claude Code best practices**:\n  - Commands should use appropriate tools (Read, Edit, Grep, Glob, Bash)\n  - Agents should have clear, focused purposes\n  - Hooks should be non-intrusive and helpful\n- **Validate inputs**: ensure names are in kebab-case, descriptions are clear, etc.\n\n## Example Interaction Flow:\n\n1. Ask for GitHub username → \"awesome-dev\"\n2. Fetch GitHub profile → Extract name \"Awesome Developer\" (or use \"awesome-dev\" if no name)\n3. Ask for plugin metadata → name: \"react-helpers\", description: \"Helpers for React development\", email (optional), etc.\n4. Create `./react-helpers/` directory\n5. Ask what to create → [Slash Command, Agent]\n6. For command → name: \"add-component\", description: \"Add a new React component with tests\"\n7. Collect detailed requirements for the command\n8. Generate well-structured command file\n9. For agent → name: \"react-optimizer\", description: \"Optimize React components for performance\"\n10. Collect agent requirements\n11. Generate agent file\n12. Create plugin.json with complete metadata (name from GitHub, optional email/url if provided)\n13. Create CODEOWNERS with @claude-market @awesome-dev\n14. Create README.md\n15. Show summary and next steps\n\nBegin by asking for the GitHub username!\n",
        "plugin-builder/commands/publish.md": "# Publish Command\n\nYou are helping publish a Claude Code plugin to the Claude Market marketplace.\n\n## Overview\n\nThis command validates changed plugins, generates the marketplace manifest, creates a semantic commit, and opens a pull request to the marketplace repository.\n\n## Step 1: Identify Changed Plugins\n\nCheck the current branch with `git branch --show-current`.\n\nIf on `main` branch:\n- Run `git status --porcelain` to find uncommitted/staged changes\n- Parse the output to identify which plugins have been modified\n\nIf on a different branch:\n- Run `git diff main --name-only` to find all changed files compared to main\n- Parse the output to identify which plugins have been modified\n\nLook for paths matching `{plugin-name}/.claude-plugin/plugin.json` or `{plugin-name}/*`.\n\nExtract the unique plugin names from the changed paths.\n\nIf no plugin changes are detected, inform the user and exit.\n\n## Step 2: Branch Management\n\nCheck the current branch (already checked in Step 1).\n\nIf on `main` branch:\n\n- Attempt to get GitHub username with `git config user.github || git config user.name`\n- Create branch name in format: `{github-user}/{plugin-affected}/{very-short-description}`\n  - Use kebab-case for all parts\n  - Keep description to 3-4 words max\n  - Example: `danielkov/browser-tools/add-chromium-mcp`\n- Run `git checkout -b {branch-name}` to create and switch to new branch\n- **Note**: Do not stage or commit yet - changes will be committed after validation passes\n\nIf on a different branch (not main):\n\n- Continue with the existing branch\n- **Note**: Do not stage or commit yet - changes will be committed after validation passes\n\n## Step 3: Validate Changed Plugins in Parallel\n\nFor each changed plugin identified in Step 1, spawn a sub-agent using the Task tool to validate the plugin.\n\nUse a single message with multiple Task tool calls to validate all plugins simultaneously:\n\n```\nFor each plugin, spawn an agent with:\n- subagent_type: \"general-purpose\"\n- model: \"haiku\" (for speed)\n- description: \"Validate {plugin-name}\"\n- prompt: \"Run the command `/plugin-builder:validate {plugin-name} --minimal` and return ONLY the exit code and output. The output should start with either '0' (success) or '1' (failure) followed by any error details.\"\n```\n\nWait for all validation agents to complete and collect their outputs.\n\n## Step 4: Check Validation Results\n\nParse each agent's output:\n\n- If output starts with `0`: Plugin passed validation\n- If output starts with `1`: Plugin failed validation\n\nIf **all** plugins pass validation (all outputs start with `0`):\n\n- Proceed to Step 5\n\nIf **any** plugin fails validation (any output starts with `1`):\n\n- For each failed plugin, display:\n  - Plugin name\n  - The full validation output (excluding the leading `1`)\n- Exit the command with an error message\n- Do not proceed further\n\n## Step 5: Generate Marketplace Manifest\n\nRun `make generate-marketplace-json` in the project root directory to update `.claude-plugin/marketplace.json`.\n\nThis script automatically reads all plugin manifests and generates the marketplace listing.\n\n## Step 6: Create Semantic Commit Message\n\nAnalyze the changes using `git diff` to understand what was modified in each plugin.\n\nCreate a short, concise semantic commit message following this format:\n\n- `feat({plugin-name}): {short description}` - for new features\n- `fix({plugin-name}): {short description}` - for bug fixes\n- `docs({plugin-name}): {short description}` - for documentation changes\n- `chore({plugin-name}): {short description}` - for maintenance tasks\n\nIf multiple plugins are affected, use the most relevant plugin name or use `marketplace` as the scope.\n\nExamples:\n\n- `feat(browser-tools): added chromium mcp server`\n- `fix(plugin-builder): corrected validation logic`\n- `docs(security-toolkit): updated usage examples`\n\nThe description should be lowercase, concise, and explain **what** changed, not **how** it changed.\n\n## Step 7: Commit and Push Changes\n\nStage all changes: `git add .`\n\nCommit with semantic message: `git commit -m \"{semantic-commit-message}\"`\n\nPush the branch:\n- If on a newly created branch (from Step 2): `git push -u origin {branch-name}`\n- If on an existing branch: `git push`\n\n## Step 8: Create Pull Request\n\nCheck if `gh` CLI is available by running `gh --version`.\n\n### If `gh` CLI is available:\n\nRead `.github/pull_request_template.md` to understand the PR template structure.\n\nFor each changed plugin, read its `.claude-plugin/plugin.json` to extract:\n\n- Plugin name\n- Author\n- Version\n- License\n- Keywords\n- Component counts (commands, agents, hooks, skills, mcpServers)\n\nAlso read the plugin's README.md for the overview and usage examples.\n\nFill in the PR template with:\n\n- **Title**: Use the semantic commit message\n- **Overview**: Extract from plugin README.md or describe the changes\n- **Plugin Information**: Fill from plugin.json\n- **Components Included**: Check the appropriate boxes and counts\n- **What does this plugin do?**: Extract from README.md\n- **Example Usage**: Extract from README.md or create based on components\n- **Testing Checklist**: Mark relevant items as checked based on what was done\n- **Documentation**: Mark relevant items as checked\n- **Code Quality**: Mark relevant items as checked\n\nCreate the PR using:\n\n```bash\ngh pr create --title \"{semantic-commit-message}\" --body \"{filled-template}\"\n```\n\nUse a HEREDOC to properly format the body:\n\n```bash\ngh pr create --title \"{semantic-commit-message}\" --body \"$(cat <<'EOF'\n{filled-template-content}\nEOF\n)\"\n```\n\nDisplay the PR URL to the user.\n\n### If `gh` CLI is NOT available:\n\nDisplay a message to the user:\n\n```\n✓ Changes committed and pushed successfully!\n\nTo create a pull request:\n1. Visit: https://github.com/claude-market/marketplace/compare/{branch-name}\n2. Fill in the PR template with the changes you made\n\n💡 Tip: Install the GitHub CLI for automated PR creation:\n   https://cli.github.com/manual/installation\n\nBranch: {branch-name}\nCommit: {semantic-commit-message}\n```\n\n## Important Notes\n\n- Always validate plugins before publishing\n- Use parallel validation for efficiency\n- Generate semantic commit messages automatically\n- Follow kebab-case naming for branches\n- Fill PR template comprehensively\n- Handle both scenarios: with/without gh CLI\n- Be clear and concise in all communications\n\n## Error Handling\n\nIf any step fails:\n\n- Display the error clearly to the user\n- Indicate which step failed\n- Suggest remediation if possible\n- Do not proceed to subsequent steps\n",
        "plugin-builder/commands/validate.md": "---\ndescription: Validate plugin structure and configuration\n---\n\nYou are validating a Claude Code plugin's structure and configuration.\n\n## Step 1: Select Plugin\n\nAsk the user which plugin to validate, or use Glob to find all plugins in `./*/.claude-plugin/plugin.json` and let them choose.\n\n## Step 2: Validation Checks\n\nPerform these validation checks:\n\n### Structure Validation\n\n1. **Required files exist:**\n\n   - `.claude-plugin/plugin.json` must exist\n   - `CODEOWNERS` must exist\n   - At least one component directory should have content\n\n2. **Directory structure:**\n\n   - `commands/` for commands (optional - only if commands exist)\n   - `agents/` for agents (optional - only if agents exist)\n   - `hooks/` for hooks (optional - only if hooks exist)\n   - `skills/` for skills (optional - only if skills exist)\n   - `mcp-servers/` for MCP servers (optional - only if MCP servers exist)\n\n3. **CODEOWNERS validation:**\n   - File exists at plugin root\n   - Contains @claude-market\n   - Contains at least one GitHub username\n   - Format is valid (pattern: \\* @org @user name)\n\n### Plugin Manifest Validation\n\nRead and validate `.claude-plugin/plugin.json`:\n\n1. **Required fields:**\n\n   - `name` (string, kebab-case, unique identifier)\n   - No other fields are strictly required\n\n2. **Optional metadata fields:**\n\n   - `version` (string, semantic versioning like \"1.0.0\")\n   - `description` (string, clear and concise)\n   - `author` (object with `name`, optional `email` and `url`)\n   - `homepage` (string, documentation URL)\n   - `repository` (string, source code URL)\n   - `license` (string, license identifier like \"MIT\")\n   - `keywords` (array of strings for discoverability)\n\n3. **Author field format:**\n\n   - If present, `author` must be an object with a `name` property\n   - Optional properties: `email` (string), `url` (string)\n   - **Correct format:** `\"author\": {\"name\": \"John Doe\", \"email\": \"john@example.com\", \"url\": \"https://example.com\"}`\n   - **Incorrect format:** `\"author\": \"John Doe\"`\n\n4. **Component path fields (optional):**\n\n   List the components your plugin provides:\n\n   - `commands` (string or array): Command markdown file paths (e.g., `[\"./commands/init.md\"]`)\n   - `agents` (string or array): Agent markdown file paths (e.g., `[\"./agents/helper.md\"]`)\n   - `skills` (string or array): Skill markdown file paths (e.g., `[\"./skills/expert.md\"]`)\n   - `hooks` (string or object): Hook configuration file path or inline config\n   - `mcpServers` (string or object): MCP server configuration file path or inline config\n\n   All paths must:\n\n   - Be relative to plugin root\n   - Begin with `./`\n\n5. **JSON validity:**\n   - Properly formatted JSON\n   - No syntax errors\n\n### Component File Validation\n\nCheck that all components listed in plugin.json exist and are valid:\n\n1. **Commands** (check files listed in `commands` array):\n\n   - Each listed file path exists\n   - Each file should contain frontmatter with description\n   - Has meaningful content (not empty)\n   - Uses proper markdown formatting\n   - File names should be in kebab-case\n\n2. **Agents** (check files listed in `agents` array):\n\n   - Each listed file path exists\n   - Contains clear instructions\n   - Defines specialized purpose\n   - Has meaningful content\n   - File names should be in kebab-case\n\n3. **Hooks** (check path in `hooks` field):\n\n   - File path exists if specified\n   - Valid JSON format\n   - Contains required hook configuration\n   - Hook type is valid\n   - File name should be in kebab-case\n\n4. **Skills** (check files listed in `skills` array):\n\n   - Each listed file path exists\n   - Defines domain expertise\n   - Contains clear usage instructions\n   - Has meaningful content\n   - File names should be in kebab-case\n\n5. **MCP Servers** (check path in `mcpServers` field):\n   - File path exists if specified\n   - Valid JSON format\n   - Contains connection configuration\n   - File name should be in kebab-case\n\n**Additional checks:**\n\n- Verify all component file paths are relative and begin with `./`\n- Check that listed components actually exist at their specified paths\n- Warn about component files that exist but aren't listed in plugin.json\n\n### Content Quality Checks\n\n1. **Names are kebab-case:** Check all component names\n2. **Descriptions are present and clear:** Not empty or placeholder text\n3. **No duplicate names:** Across all components\n4. **Files have substantial content:** Not just placeholders\n\n### Documentation Validation\n\n1. **README.md exists**\n2. **README contains:**\n   - Plugin name and description\n   - Installation instructions\n   - Usage examples for components\n   - License information\n3. **LICENSE file exists**\n\n## Step 3: Report Results\n\nCreate a structured validation report:\n\n### ✓ Passed Checks\n\nList all checks that passed\n\n### ✗ Failed Checks\n\nList all checks that failed with:\n\n- What failed\n- Why it's a problem\n- How to fix it\n\n### ⚠ Warnings\n\nList recommendations for improvement:\n\n- Missing optional fields\n- Content that could be more detailed\n- Best practices not followed\n\n## Step 4: Recommendations\n\nProvide actionable recommendations:\n\n- Critical fixes needed before the plugin can work\n- Suggested improvements for better user experience\n- Best practices to follow\n\n## Example Output:\n\n```\nValidating plugin: awesome-plugin\n\n✓ Passed Checks:\n  - Plugin manifest exists\n  - All required fields present\n  - JSON is valid\n  - All listed commands have corresponding files\n  - Directory structure is correct\n  - CODEOWNERS file exists and is valid\n  - README.md exists\n  - LICENSE file exists\n\n✗ Failed Checks:\n  - File commands/broken-cmd.md found but contains no content\n    Fix: Add meaningful content to the command file or remove it\n\n⚠ Warnings:\n  - No version specified in plugin.json (recommended for tracking)\n  - Command \"init\" frontmatter description is minimal (could be more detailed)\n  - No keywords specified (helps with discoverability)\n  - No author information provided (recommended for attribution)\n  - CODEOWNERS doesn't include @claude-market (required for marketplace submissions)\n\nRecommendations:\n  1. Add version field to plugin.json (suggest starting at \"1.0.0\")\n  2. Expand command descriptions to be more informative\n  3. Add relevant keywords for marketplace discoverability\n  4. Consider adding usage examples to README\n```\n\nBe thorough but constructive in validation feedback.\n\n## Example Output (with `--minimal` flag)\n\nIf `--minimal` flag is present in input, the following output structure MUST BE FOLLOWED:\n\n### On Success\n\n```\n0\n```\n\n### On Failure\n\n```\n1\n✗ Failed Checks:\n  - File commands/broken-cmd.md found but contains no content\n    Fix: Add meaningful content to the command file or remove it\n```\n",
        "plugin-builder/skills/cc-agent-builder.md": "---\nname: cc-agent-builder\ndescription: This skill should be used when users want to create a new Claude Code subagent. Use this skill to help users design, structure, and implement highly effective subagents that follow best practices for specialized task handling, context isolation, and tool permissions.\nallowed-tools: Read,Write,Glob,Grep,AskUserQuestion,Bash\nmodel: inherit\nversion: 1.0.0\nlicense: MIT\n---\n\n# Claude Code Subagent Builder\n\nCreate highly effective subagents following industry best practices for specialized task handling, context isolation, and system prompt design.\n\n## Overview\n\nSubagents are specialized AI assistants invoked to handle specific types of tasks. Each operates in its own isolated context window with customized system prompts and tool permissions. Claude intelligently routes tasks to appropriate subagents based on their descriptions.\n\nThis skill helps you create subagents that:\n\n- Follow proper markdown structure with YAML frontmatter\n- Use effective system prompt engineering\n- Implement appropriate context isolation\n- Apply minimal necessary tool permissions\n- Handle specialized workflows efficiently\n\n## Subagent Anatomy\n\nEvery subagent is a Markdown file with YAML frontmatter followed by a system prompt:\n\n### File Structure\n\n```yaml\n---\nname: your-sub-agent-name\ndescription: Clear description of when this subagent should be invoked\ntools: Read,Write,Bash(npm:*)\nmodel: sonnet\n---\n# System Prompt\n\nYour specialized instructions for this subagent go here.\n\nDefine the subagent's role, capabilities, workflow, and success criteria.\n```\n\n### YAML Frontmatter (Configuration)\n\n**Required fields:**\n\n- `name`: Unique identifier using lowercase letters and hyphens (kebab-case)\n- `description`: Natural language explanation of the subagent's purpose and when to invoke it\n\n**Optional fields:**\n\n- `tools`: Comma-separated list of specific tools (omit to inherit all tools from main conversation)\n- `model`: Model to use - accepts `sonnet`, `opus`, `haiku`, or `inherit` (default: inherit)\n\n### File Locations\n\n**Project subagents**: `.claude/agents/` (shared via git)\n\n- Team-accessible specialized agents\n- Project-specific workflows\n- Higher priority when names conflict\n\n**User subagents**: `~/.claude/agents/` (user-level)\n\n- Personal utilities across all projects\n- Individual workflow preferences\n- Available globally\n\n## Context Isolation\n\nEach subagent operates in a **separate context window**, which provides:\n\n**Benefits:**\n\n- Main conversation stays focused on high-level objectives\n- Subagent context doesn't pollute main thread\n- Specialized knowledge without bloating main prompt\n- Multiple subagents can work on different tasks independently\n\n**Implications:**\n\n- Subagent cannot directly access main conversation history\n- Must provide sufficient context in the task delegation\n- Results returned to main conversation as single message\n- Subagent is stateless - each invocation is independent\n\n## Tool Permissions\n\n### Inherit All Tools (Recommended Default)\n\nOmit the `tools` field to inherit all available tools:\n\n```yaml\n---\nname: code-reviewer\ndescription: Review code for best practices and potential issues\n---\n```\n\n**When to use:**\n\n- General-purpose subagents\n- Workflows requiring flexibility\n- When specific tool needs are unclear\n\n### Specify Explicit Tools (Minimal Permissions)\n\nList only required tools for security and clarity:\n\n```yaml\n---\nname: git-committer\ndescription: Create semantic git commits\ntools: Bash(git:*)\n---\n```\n\n**When to use:**\n\n- Security-sensitive operations\n- Constrained, well-defined workflows\n- Preventing accidental file modifications\n\n**Available tools**: Read, Write, Edit, Bash, Grep, Glob, WebFetch, WebSearch, and MCP tools (when tools field is omitted)\n\n## Model Selection\n\nChoose the appropriate model for your subagent's complexity:\n\n- **inherit** (default): Use same model as main conversation\n- **haiku**: Fast, cost-effective for straightforward tasks (formatting, simple analysis)\n- **sonnet**: Balanced for most workflows (code review, testing, documentation)\n- **opus**: Complex reasoning, critical decisions, architectural design\n\n**Best practice**: Start with `inherit` and only specify a model if you need different capabilities than the main thread.\n\n## Best Practices\n\n### System Prompt Engineering\n\n**Define clear role and responsibility:**\n\n```markdown\nYou are a specialized code reviewer focused on security vulnerabilities in web applications. Your expertise includes OWASP Top 10, secure authentication patterns, and data validation.\n```\n\n**Specify workflow steps:**\n\n```markdown\n## Workflow\n\n1. Read the provided code files\n2. Identify security vulnerabilities\n3. Classify by severity (Critical, High, Medium, Low)\n4. Suggest specific fixes with code examples\n5. Explain security implications\n```\n\n**Set success criteria:**\n\n```markdown\n## Success Criteria\n\nYour review is complete when you have:\n\n- Identified all OWASP Top 10 vulnerabilities\n- Provided actionable fixes for each issue\n- Explained the security impact\n- Suggested preventive measures\n```\n\n**Include examples:**\n\n```markdown\n## Example Output\n\n### Critical: SQL Injection Vulnerability\n\n**Location**: src/api/users.js:45\n\n**Issue**: User input directly concatenated into SQL query\n\n**Fix**:\n\\`\\`\\`javascript\n// Before: Vulnerable\nconst query = `SELECT * FROM users WHERE id = ${userId}`;\n\n// After: Secure\nconst query = 'SELECT \\* FROM users WHERE id = ?';\ndb.query(query, [userId]);\n\\`\\`\\`\n\n**Impact**: Attackers can execute arbitrary SQL, accessing/modifying sensitive data.\n```\n\n### Progressive Disclosure\n\nStructure information from high-level to detailed:\n\n1. **Role definition** - Who is this subagent?\n2. **Capabilities** - What can it do?\n3. **Workflow** - How does it work?\n4. **Output format** - What does it deliver?\n5. **Edge cases** - How to handle exceptions\n6. **Examples** - Concrete demonstrations\n\n### Single Responsibility\n\nEach subagent should have one clear purpose:\n\n**Good**: `test-generator` - Generates unit tests for JavaScript functions\n**Poor**: `dev-helper` - Does various development tasks\n\n**Why**: Clear responsibility enables better task routing and more focused context.\n\n### Constraints and Boundaries\n\nDefine what the subagent should NOT do:\n\n```markdown\n## Constraints\n\n- DO NOT modify production configuration files\n- DO NOT commit changes automatically\n- DO NOT delete existing tests\n- ALWAYS preserve existing functionality\n- ALWAYS ask for confirmation before destructive operations\n```\n\n## Common Subagent Patterns\n\n### 1. Code Analysis Subagent\n\n```yaml\n---\nname: code-analyzer\ndescription: Analyze code quality, complexity, and maintainability metrics\ntools: Read,Grep\nmodel: sonnet\n---\n\nYou are a code quality analyst specializing in static analysis and complexity metrics.\n\n## Workflow\n\n1. Read the specified files or directory\n2. Calculate cyclomatic complexity for each function\n3. Identify code smells (long functions, deep nesting, etc.)\n4. Measure test coverage if tests exist\n5. Generate actionable recommendations\n\n## Output Format\n\nProvide a structured report with:\n- Overall quality score (0-100)\n- Complexity metrics per file/function\n- Identified code smells with locations\n- Prioritized recommendations\n- Example refactorings for top 3 issues\n```\n\n### 2. Documentation Generator Subagent\n\n```yaml\n---\nname: doc-generator\ndescription: Generate comprehensive documentation from source code\ntools: Read,Write\nmodel: sonnet\n---\n\nYou are a technical documentation specialist creating clear, accurate API documentation.\n\n## Workflow\n\n1. Read source code files\n2. Extract function signatures, parameters, return types\n3. Parse JSDoc/docstring comments\n4. Generate markdown documentation\n5. Include usage examples\n6. Create table of contents\n\n## Documentation Standards\n\n- Use clear, concise language\n- Include all parameters with types\n- Provide practical examples\n- Document edge cases and errors\n- Link related functions\n\n## Output Location\n\nWrite documentation to:\n- Single file: `docs/api/[filename].md`\n- Multiple files: `docs/api/[module-name]/`\n```\n\n### 3. Test Generator Subagent\n\n```yaml\n---\nname: test-generator\ndescription: Generate comprehensive unit tests for JavaScript/TypeScript functions\ntools: Read,Write\nmodel: sonnet\n---\n\nYou are a testing specialist focused on creating thorough, maintainable unit tests.\n\n## Test Generation Principles\n\n- Test happy path and edge cases\n- Include boundary value testing\n- Mock external dependencies\n- Follow AAA pattern (Arrange, Act, Assert)\n- Use descriptive test names\n\n## Workflow\n\n1. Read the source file\n2. Identify all exported functions\n3. For each function:\n   - Analyze parameters and return types\n   - Identify edge cases\n   - Generate test cases\n4. Write tests using project's testing framework\n5. Ensure tests are independent and isolated\n\n## Coverage Goals\n\n- 100% function coverage\n- 80%+ branch coverage\n- All error paths tested\n```\n\n### 4. Git Workflow Subagent\n\n```yaml\n---\nname: git-pr-reviewer\ndescription: Review pull requests for code quality and team standards\ntools: Bash(git:*),Read\nmodel: sonnet\n---\n\nYou are a senior engineer conducting thorough pull request reviews.\n\n## Review Checklist\n\n1. **Code Quality**\n   - Follows team style guide\n   - No code smells\n   - Appropriate abstractions\n\n2. **Testing**\n   - New code has tests\n   - Tests are meaningful\n   - Edge cases covered\n\n3. **Documentation**\n   - Public APIs documented\n   - Complex logic explained\n   - README updated if needed\n\n4. **Security**\n   - No sensitive data exposed\n   - Input validation present\n   - Authentication/authorization correct\n\n5. **Performance**\n   - No obvious bottlenecks\n   - Database queries optimized\n   - Appropriate data structures\n\n## Workflow\n\n1. Fetch PR diff: git diff main...feature-branch\n2. Review each changed file\n3. Check test coverage\n4. Verify documentation\n5. Provide constructive feedback with examples\n\n## Output Format\n\n**Summary**: Brief overview of changes\n**Strengths**: What was done well\n**Issues**: Problems requiring fixes (with severity)\n**Suggestions**: Optional improvements\n**Verdict**: Approve / Request Changes / Comment\n```\n\n### 5. Refactoring Subagent\n\n```yaml\n---\nname: refactoring-assistant\ndescription: Apply code refactoring patterns while preserving behavior\ntools: Read,Write,Edit\nmodel: sonnet\n---\n\nYou are a refactoring specialist applying proven patterns to improve code quality.\n\n## Refactoring Catalog\n\n- Extract Function\n- Extract Variable\n- Inline Function\n- Rename Symbol\n- Move Function\n- Simplify Conditional\n- Replace Magic Numbers\n- Remove Dead Code\n\n## Safety Rules\n\n1. **Preserve Behavior**: Refactored code must behave identically\n2. **Incremental Changes**: One refactoring at a time\n3. **Test Coverage**: Verify tests pass after changes\n4. **Reversible**: Document what changed for easy rollback\n\n## Workflow\n\n1. Read the target code\n2. Identify code smells requiring refactoring\n3. Select appropriate refactoring pattern\n4. Apply transformation carefully\n5. Show before/after diff\n6. Explain why refactoring improves code\n7. Recommend running tests\n\n## Output Format\n\n**Refactoring Applied**: [Pattern Name]\n**Location**: [File:Line]\n**Before**: [Code snippet]\n**After**: [Refactored code]\n**Benefits**: [Why this improves code]\n**Testing**: [Recommend specific tests to run]\n```\n\n## Subagent Creation Workflow\n\nWhen a user asks to create a subagent, follow these steps:\n\n### Step 1: Gather Requirements\n\nUse AskUserQuestion to collect:\n\n1. **Subagent name** (kebab-case, descriptive)\n2. **Primary purpose** (what specialized task does it handle?)\n3. **When to invoke** (what triggers/conditions?)\n4. **Tools needed** (minimal set or inherit all?)\n5. **Model preference** (inherit, haiku, sonnet, opus?)\n6. **Scope** (project-specific or personal?)\n\n### Step 2: Design System Prompt\n\nStructure the prompt with:\n\n1. **Role definition** - Who is this subagent?\n2. **Capabilities** - What can it do?\n3. **Workflow** - Step-by-step process\n4. **Output format** - Expected deliverables\n5. **Constraints** - What NOT to do\n6. **Examples** - Concrete demonstrations\n\n### Step 3: Configure Tools\n\nDetermine tool strategy:\n\n- **Omit tools field**: For flexible, general-purpose subagents\n- **Minimal explicit list**: For security-sensitive or constrained workflows\n- **MCP compatibility**: Tools field omission enables MCP tool access\n\n### Step 4: Select Model\n\nChoose based on task complexity:\n\n- **inherit**: Default, matches main conversation\n- **haiku**: Simple formatting, quick analysis\n- **sonnet**: Most workflows, balanced capability\n- **opus**: Complex reasoning, critical decisions\n\n### Step 5: Write and Validate\n\n1. Create complete .md file with frontmatter and system prompt\n2. Ensure description clearly explains when to invoke\n3. Verify workflow steps are actionable\n4. Include concrete examples\n5. Define success criteria\n6. Test that subagent can be invoked correctly\n\n## Output Format\n\nWhen creating a subagent, deliver:\n\n1. **Complete .md file** with frontmatter and system prompt\n2. **File path** where it should be saved\n3. **Tool selection justification** explaining the tools strategy\n4. **Invocation examples** demonstrating when Claude will route to this subagent\n5. **Testing instructions** for validating the subagent works\n\n## Error Handling\n\nIf subagent requirements are unclear:\n\n1. Ask clarifying questions about the specialized task\n2. Suggest similar existing subagents as references\n3. Recommend breaking overly complex subagents into multiple focused ones\n4. Explain trade-offs between tool inheritance vs explicit permissions\n\nIf the subagent seems too broad:\n\n1. Identify the core responsibility\n2. Suggest splitting into multiple focused subagents\n3. Explain benefits of single-responsibility design\n\n## Resources\n\n- Claude Code Subagents Docs: https://docs.claude.com/en/docs/claude-code/sub-agents\n- Context Engineering Guide: https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents\n- Community Subagents: https://github.com/VoltAgent/awesome-claude-code-subagents\n\n## Key Principles\n\n1. **Single responsibility** - One clear purpose per subagent\n2. **Context isolation** - Each subagent has separate context window\n3. **Clear descriptions** - Enable intelligent task routing\n4. **Minimal tools** - Only request what's needed (or inherit all)\n5. **Appropriate model** - Match complexity to task requirements\n6. **Concrete examples** - Show don't just tell\n7. **Success criteria** - Define what \"done\" looks like\n8. **Safety constraints** - Specify what NOT to do\n\nWhen in doubt, create a simpler, more focused subagent rather than a complex multi-purpose one.\n",
        "plugin-builder/skills/cc-command-builder.md": "---\nname: cc-command-builder\ndescription: This skill should be used when users want to create a new Claude Code slash command. Use this skill to help users design, structure, and implement highly effective slash commands that follow best practices for prompt engineering and argument handling.\nallowed-tools: Read,Write,Glob,Grep,AskUserQuestion,Bash\nmodel: inherit\nversion: 1.0.0\nlicense: MIT\n---\n\n# Claude Code Slash Command Builder\n\nCreate highly effective slash commands following industry best practices for structure, prompt engineering, and argument handling.\n\n## Overview\n\nSlash commands are reusable prompt templates stored as Markdown files that enable quick execution of frequently-used operations. They support dynamic arguments, bash execution, file references, and can be scoped to projects or users.\n\nThis skill helps you create slash commands that:\n\n- Follow proper markdown structure with optional YAML frontmatter\n- Use effective prompt engineering techniques\n- Handle arguments correctly ($ARGUMENTS vs positional parameters)\n- Apply appropriate tool permissions\n- Include practical examples and edge case handling\n\n## Slash Command Anatomy\n\nEvery slash command is a Markdown file with an optional YAML frontmatter section:\n\n### File Structure\n\n```yaml\n---\nallowed-tools: Read,Write,Bash(git:*)\nargument-hint: [message]\ndescription: Brief command description\nmodel: claude-3-5-haiku-20241022\ndisable-model-invocation: false\n---\n# Command Prompt Content\n\nYour natural language instructions go here.\n\nUse $ARGUMENTS or $1, $2, etc. for dynamic values.\n```\n\n### YAML Frontmatter (Optional Configuration)\n\n**Optional fields:**\n\n- `description`: Brief description shown in command listings and SlashCommand tool (highly recommended)\n- `allowed-tools`: Comma-separated tool permissions (e.g., `Read,Write,Bash(git:*)`)\n- `argument-hint`: Display hint for expected arguments (e.g., `[issue-number] [priority]`)\n- `model`: Override default model (use model ID like `claude-3-5-haiku-20241022`)\n- `disable-model-invocation`: Set to `true` for commands that just provide context without needing AI response\n\n### File Locations\n\n**Project commands**: `.claude/commands/` (shared via git)\n\n- Team-accessible commands\n- Project-specific workflows\n- Higher priority when names conflict\n\n**Personal commands**: `~/.claude/commands/` (user-level)\n\n- Cross-project utilities\n- Personal preferences\n- Available in all projects\n\n## Argument Handling\n\n### $ARGUMENTS - Capture Everything\n\nUse when you want all arguments as a single string:\n\n```markdown\n---\nargument-hint: [commit message]\n---\n\nCreate a git commit with message: $ARGUMENTS\n```\n\n**Usage**: `/commit fix: resolve authentication bug`\n**Result**: `$ARGUMENTS = \"fix: resolve authentication bug\"`\n\n### Positional Parameters - $1, $2, $3, etc.\n\nUse when you need structured, multi-part arguments:\n\n```markdown\n---\nargument-hint: [pr-number] [priority] [reviewer]\n---\n\nReview PR #$1 with $2 priority. Assign reviewer: $3\n```\n\n**Usage**: `/review-pr 456 high alice`\n**Result**:\n\n- `$1 = \"456\"`\n- `$2 = \"high\"`\n- `$3 = \"alice\"`\n\n**Best practice**: Use positional parameters when:\n\n- Arguments have distinct semantic roles\n- You need to reference parameters in different parts of the prompt\n- Order matters for your workflow\n- You want to provide structured validation\n\n## Advanced Features\n\n### Bash Execution\n\nExecute shell commands inline using `!` prefix:\n\n```markdown\nCurrent git status:\n!`git status --short`\n\nRecent commits:\n!`git log --oneline -10`\n```\n\n**Security note**: Ensure bash commands are safe and don't expose sensitive data.\n\n### File References\n\nInclude file contents using `@` prefix:\n\n```markdown\nCompare these implementations:\n\nOld version: @src/old-version.js\nNew version: @src/new-version.js\n\nAnalyze differences and suggest improvements.\n```\n\n### Namespacing\n\nOrganize commands in subdirectories within `.claude/commands/`:\n\n```\n.claude/commands/\n├── git/\n│   ├── commit.md\n│   └── review.md\n└── testing/\n    └── run-suite.md\n```\n\n**Note**: Namespacing affects description display only, not the command name itself.\n\n## Best Practices\n\n### Prompt Engineering\n\n**Clarity**: Be specific about what Claude should do. Use imperative language.\n\nGood:\n\n```markdown\nAnalyze the current git diff and generate a semantic commit message following Conventional Commits format.\n```\n\nPoor:\n\n```markdown\nHelp me with git stuff.\n```\n\n**Structure**: Break complex commands into clear steps:\n\n```markdown\n1. Read the failing test file: @tests/auth.test.js\n2. Analyze the error message: $ARGUMENTS\n3. Identify the root cause\n4. Suggest specific fixes with code examples\n5. Explain why the test failed\n```\n\n**Context**: Provide sufficient background without over-specifying:\n\n```markdown\nYou are reviewing a pull request for a Node.js Express API.\nFocus on: security vulnerabilities, performance issues, and code maintainability.\nPR number: $1\n```\n\n### Tool Permissions\n\nOnly request tools your command actually needs:\n\n- **Read-only analysis**: `Read`\n- **File modifications**: `Read,Write`\n- **Git operations**: `Bash(git:*)`\n- **Testing**: `Bash(npm test:*),Bash(jest:*)`\n- **Web research**: `Read,WebFetch,WebSearch`\n\n**Security**: Avoid `Bash` without restrictions. Use `Bash(command:*)` for specific commands.\n\n### Argument Validation\n\nGuide users with helpful hints and validation:\n\n```markdown\n---\nargument-hint: [feature-name] [ticket-id]\ndescription: Create a new feature branch following team conventions\n---\n\nCreate git branch for feature: $1 (ticket: $2)\n\nValidation:\n\n- Feature name must be kebab-case\n- Ticket ID must match pattern: PROJ-\\d+\n\nIf invalid, explain the correct format and ask user to retry.\n```\n\n### Error Handling\n\nAnticipate common issues:\n\n```markdown\n---\ndescription: Deploy to production environment\n---\n\nPre-deployment checklist:\n\n!`git status`\n\nIf uncommitted changes exist, STOP and ask user to commit or stash.\nIf not on main branch, STOP and ask user to checkout main.\nIf behind remote, STOP and ask user to pull latest changes.\n\nOtherwise, proceed with: npm run deploy:prod\n```\n\n## Command Creation Workflow\n\nWhen a user asks to create a slash command, follow these steps:\n\n### Step 1: Gather Requirements\n\nUse AskUserQuestion to collect:\n\n1. **Command name** (kebab-case, descriptive, no conflicts with existing commands)\n2. **Command purpose** (what problem does it solve?)\n3. **Expected arguments** (none, simple, or structured?)\n4. **Tools needed** (read files, write files, run commands?)\n5. **Scope** (project-specific or personal?)\n\n### Step 2: Design Arguments Strategy\n\nDetermine the best approach:\n\n- **No arguments**: Simple, self-contained commands\n- **$ARGUMENTS**: Single string input (commit messages, search queries, descriptions)\n- **Positional parameters**: Multiple distinct inputs (IDs, names, options)\n\n### Step 3: Structure Frontmatter\n\nCreate YAML configuration with:\n\n- Clear `description` for discoverability\n- Minimal necessary `allowed-tools`\n- Helpful `argument-hint` if arguments are used\n- Model override only if specific model is required\n\n### Step 4: Write Prompt Content\n\nOrganize instructions following best practices:\n\n1. **Clear objective statement** - What this command does\n2. **Argument handling** - How to use $ARGUMENTS or $1, $2, etc.\n3. **Step-by-step workflow** - Numbered list of actions\n4. **Validation** - Check inputs, prerequisites, or state\n5. **Error handling** - What to do if something fails\n6. **Output format** - Expected deliverables or response\n\n### Step 5: Add Examples\n\nInclude concrete usage examples:\n\n```markdown\n## Examples\n\nBasic usage:\n/my-command feature-auth PROJ-123\n\nWith optional parameters:\n/my-command feature-auth PROJ-123 high-priority\n```\n\n### Step 6: Validate and Test\n\nBefore finalizing:\n\n1. Check markdown syntax\n2. Verify YAML frontmatter is valid\n3. Ensure argument references match usage pattern\n4. Test bash commands are safe\n5. Confirm file paths use `@` notation correctly\n\n## Common Patterns\n\n### 1. Git Workflow Commands\n\n```markdown\n---\ndescription: Create semantic commit with auto-generated message\nallowed-tools: Bash(git:*)\n---\n\n1. Review staged changes: !`git diff --staged`\n2. Generate commit message following Conventional Commits\n3. Execute: git commit -m \"[generated message]\"\n```\n\n### 2. Code Analysis Commands\n\n```markdown\n---\ndescription: Analyze code for security vulnerabilities\nallowed-tools: Read,Grep,WebSearch\nargument-hint: [file-pattern]\n---\n\n1. Find files matching: $ARGUMENTS\n2. Scan for common vulnerabilities (SQL injection, XSS, etc.)\n3. Report findings with severity levels\n4. Suggest fixes with code examples\n```\n\n### 3. Documentation Commands\n\n```markdown\n---\ndescription: Generate API documentation from source code\nallowed-tools: Read,Write\nargument-hint: [source-file]\n---\n\n1. Read source file: @$1\n2. Extract function signatures and JSDoc comments\n3. Generate markdown documentation\n4. Write to docs/ directory\n```\n\n### 4. Testing Commands\n\n```markdown\n---\ndescription: Run tests and generate coverage report\nallowed-tools: Bash(npm:*),Read\n---\n\n1. Execute: !`npm test -- --coverage`\n2. Parse coverage report\n3. Highlight files below 80% coverage\n4. Suggest additional test cases\n```\n\n### 5. Refactoring Commands\n\n```markdown\n---\nargument-hint: [file-path] [refactor-type]\ndescription: Apply code refactoring patterns\nallowed-tools: Read,Write\n---\n\nRefactor file: @$1\nType: $2 (extract-function|rename-variable|simplify-conditional)\n\n1. Analyze current code structure\n2. Apply refactoring pattern\n3. Preserve behavior\n4. Show diff of changes\n```\n\n## Output Format\n\nWhen creating a slash command, deliver:\n\n1. **Complete .md file** with frontmatter and prompt content\n2. **File path** where it should be saved\n3. **Tool permission justification** explaining why each tool is needed\n4. **Usage examples** demonstrating different argument patterns\n5. **Testing instructions** for validating the command works\n\n## Error Handling\n\nIf command requirements are unclear:\n\n1. Ask clarifying questions about the workflow\n2. Suggest similar existing commands as references\n3. Recommend breaking complex commands into multiple focused commands\n4. Explain trade-offs between $ARGUMENTS vs positional parameters\n\nIf tool permissions seem excessive:\n\n1. Review each tool against actual usage in the prompt\n2. Suggest using more restrictive tool scopes (e.g., `Bash(git:*)` instead of `Bash`)\n3. Consider if the command should delegate to a subagent instead\n\n## Resources\n\n- Claude Code Slash Commands Docs: https://docs.claude.com/en/docs/claude-code/slash-commands\n- Conventional Commits: https://www.conventionalcommits.org/\n- Claude Code Plugins: https://www.anthropic.com/news/claude-code-plugins\n\n## Key Principles\n\n1. **Commands are templates, not code** - Focus on natural language instructions\n2. **Description enables discovery** - Make it clear and searchable\n3. **Minimal permissions** - Only request tools actually needed\n4. **Arguments follow purpose** - Use $ARGUMENTS for simple, positional for structured\n5. **Validate inputs** - Guide users when arguments are invalid\n6. **Handle errors gracefully** - Check prerequisites before executing\n7. **Show examples** - Demonstrate usage patterns clearly\n8. **Stay focused** - One command, one purpose\n\nWhen in doubt, create a simpler, more focused command rather than a complex multi-purpose one.\n",
        "plugin-builder/skills/cc-hook-builder.md": "---\nname: cc-hook-builder\ndescription: This skill should be used when users want to create a new Claude Code hook. Use this skill to help users design, structure, and implement highly effective hooks that automate workflows at specific lifecycle events.\nallowed-tools: Read,Write,Glob,Grep,AskUserQuestion,Bash\nmodel: inherit\nversion: 1.0.0\nlicense: MIT\n---\n\n# Claude Code Hook Builder\n\nCreate highly effective hooks following industry best practices for lifecycle automation, event handling, and secure shell command execution.\n\n## Overview\n\nHooks are automated triggers that execute shell commands at specific points during Claude Code's operation. They function as \"if this, then that\" rules for your coding assistant, enabling workflow automation, validation, notifications, and custom behaviors.\n\nThis skill helps you create hooks that:\n\n- Follow proper JSON configuration structure\n- Use appropriate lifecycle events\n- Implement secure shell command execution\n- Handle blocking vs non-blocking operations correctly\n- Apply matchers for targeted tool filtering\n- Process tool input/output via stdin\n\n## Hook Anatomy\n\nEvery hook is defined in JSON configuration within `~/.claude/settings.json` or project-level settings:\n\n### Basic Structure\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Edit|Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"path/to/script.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Configuration Fields\n\n**Top-level organization:**\n\n- `hooks`: Object containing hook events as keys\n- Each event: Array of matcher-hook pairs\n\n**Matcher configuration:**\n\n- `matcher`: Tool name pattern to target (e.g., `\"Bash\"`, `\"Edit|Write\"`, `\"*\"`)\n- `hooks`: Array of hook definitions to execute when matcher triggers\n\n**Hook definition:**\n\n- `type`: Always `\"command\"` for shell execution\n- `command`: Shell command to execute (receives JSON via stdin)\n\n## Available Hook Events\n\nClaude Code provides several lifecycle hooks:\n\n### PreToolUse\n\n**When**: Before tool execution\n**Can block**: Yes (exit code 2 blocks operation)\n**Use cases**:\n\n- Validation before file edits\n- Security checks before bash commands\n- Confirmation prompts\n- Preventing destructive operations\n\n### PostToolUse\n\n**When**: After tool completes\n**Can block**: No\n**Use cases**:\n\n- Automatic formatting after file edits\n- Logging completed operations\n- Triggering downstream automation\n- Updating external systems\n\n### UserPromptSubmit\n\n**When**: When user submits a prompt\n**Can block**: No\n**Use cases**:\n\n- Logging user requests\n- Adding context automatically\n- Triggering external notifications\n- Analytics tracking\n\n### Notification\n\n**When**: Claude Code sends notifications\n**Can block**: No\n**Use cases**:\n\n- Custom notification routing\n- Desktop notifications\n- Slack/Teams integration\n- Alert logging\n\n### Stop\n\n**When**: Response finishes\n**Can block**: No\n**Use cases**:\n\n- Session tracking\n- Performance metrics\n- Cleanup operations\n- Status updates\n\n### SubagentStop\n\n**When**: Subagent task completes\n**Can block**: No\n**Use cases**:\n\n- Subagent performance tracking\n- Result logging\n- Chaining subagent workflows\n- Analytics\n\n### PreCompact\n\n**When**: Before context compaction\n**Can block**: No\n**Use cases**:\n\n- Backup important context\n- Archive conversation state\n- Notify about compaction\n- Metrics collection\n\n### SessionStart\n\n**When**: Session initiation\n**Can block**: No\n**Use cases**:\n\n- Environment setup\n- Loading project context\n- Logging session start\n- Initialization tasks\n\n### SessionEnd\n\n**When**: Session termination\n**Can block**: No\n**Use cases**:\n\n- Cleanup temporary files\n- Save session state\n- Generate session reports\n- Close external connections\n\n## Matchers\n\nMatchers target specific tools for hook execution:\n\n### Tool-Specific Matchers\n\n```json\n{\n  \"matcher\": \"Bash\"\n}\n```\n\nTargets only the Bash tool.\n\n### Multiple Tool Matchers\n\n```json\n{\n  \"matcher\": \"Edit|Write\"\n}\n```\n\nTargets Edit OR Write tools (pipe-separated).\n\n### Universal Matcher\n\n```json\n{\n  \"matcher\": \"*\"\n}\n```\n\nTargets ALL tools (use sparingly for performance).\n\n### Common Patterns\n\n- **File operations**: `\"Edit|Write|Read\"`\n- **Git operations**: `\"Bash\"` (when you only want git commands)\n- **Everything**: `\"*\"`\n\n## Blocking Operations\n\nPreToolUse hooks can prevent tool execution based on exit codes:\n\n### Allow Operation (Exit 0)\n\n```bash\n#!/bin/bash\n# Validation passed, allow operation\nexit 0\n```\n\n### Block Operation (Exit 2)\n\n```bash\n#!/bin/bash\n# Validation failed, block operation\necho \"Error: Cannot edit production files\" >&2\nexit 2\n```\n\n### Example: Prevent Production File Edits\n\n```bash\n#!/bin/bash\nFILE_PATH=$(echo \"$1\" | jq -r '.tool_input.file_path')\n\nif [[ \"$FILE_PATH\" == *\"/prod/\"* ]]; then\n  echo \"BLOCKED: Cannot edit production files directly\" >&2\n  exit 2\nfi\n\nexit 0\n```\n\n**Important**: Only PreToolUse hooks can block. Other events ignore exit codes.\n\n## Input/Output Handling\n\nHooks receive JSON via stdin containing tool context:\n\n### Input Schema\n\n```json\n{\n  \"tool_name\": \"Edit\",\n  \"tool_input\": {\n    \"file_path\": \"/path/to/file.js\",\n    \"old_string\": \"const x = 1\",\n    \"new_string\": \"const x = 2\"\n  }\n}\n```\n\n### Parsing with jq\n\n```bash\n#!/bin/bash\nTOOL_NAME=$(echo \"$1\" | jq -r '.tool_name')\nFILE_PATH=$(echo \"$1\" | jq -r '.tool_input.file_path')\nCOMMAND=$(echo \"$1\" | jq -r '.tool_input.command')\n\necho \"Tool: $TOOL_NAME\"\necho \"File: $FILE_PATH\"\n```\n\n### Example: Log All File Edits\n\n```bash\n#!/bin/bash\nTOOL_INPUT=$(cat)\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n\necho \"$TIMESTAMP: $TOOL_INPUT\" >> ~/.claude/edit-log.jsonl\n```\n\n## Best Practices\n\n### Security\n\n**Always review hook implementations** before registering:\n\n- Hooks execute with your environment credentials\n- Malicious hooks can compromise your system\n- Validate inputs to prevent command injection\n- Use absolute paths for sensitive operations\n\n**Example: Secure input handling**\n\n```bash\n#!/bin/bash\nFILE_PATH=$(echo \"$1\" | jq -r '.tool_input.file_path')\n\n# Validate file path doesn't escape project\nif [[ \"$FILE_PATH\" == ../* ]]; then\n  echo \"BLOCKED: Path traversal attempt\" >&2\n  exit 2\nfi\n```\n\n### Performance\n\n**Use specific matchers** instead of universal:\n\n- `\"Edit\"` - Better than `\"*\"`\n- `\"Bash\"` - Better than `\"*\"`\n- Reduces hook executions\n- Improves response time\n\n**Keep hook scripts fast**:\n\n- Avoid expensive operations in PreToolUse (blocks Claude)\n- Use background jobs for slow tasks\n- Cache results when possible\n- Log errors efficiently\n\n### Scope\n\n**User settings** (`~/.claude/settings.json`):\n\n- Hooks available across all projects\n- Personal workflow automation\n- Global policies and validations\n\n**Project settings** (`.claude/settings.json`):\n\n- Team-shared hooks\n- Project-specific workflows\n- Committed to version control\n\n**Choose scope wisely**:\n\n- Security policies: User-level\n- Team standards: Project-level\n- Personal preferences: User-level\n\n## Common Hook Patterns\n\n### 1. Auto-Format on Save\n\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Edit|Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash -c 'FILE=$(jq -r \\\".tool_input.file_path\\\"); prettier --write \\\"$FILE\\\" 2>/dev/null'\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### 2. Git Commit Validation\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"~/.claude/hooks/validate-commit.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**validate-commit.sh:**\n\n```bash\n#!/bin/bash\nCOMMAND=$(jq -r '.tool_input.command')\n\n# Check if this is a git commit\nif [[ \"$COMMAND\" != *\"git commit\"* ]]; then\n  exit 0\nfi\n\n# Extract commit message\nMSG=$(echo \"$COMMAND\" | grep -oP 'git commit -m \"\\K[^\"]+')\n\n# Validate conventional commit format\nif [[ ! \"$MSG\" =~ ^(feat|fix|docs|style|refactor|test|chore):.+ ]]; then\n  echo \"BLOCKED: Commit message must follow Conventional Commits format\" >&2\n  echo \"Example: feat: add user authentication\" >&2\n  exit 2\nfi\n\nexit 0\n```\n\n### 3. Prevent Sensitive File Edits\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Edit|Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"~/.claude/hooks/protect-secrets.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**protect-secrets.sh:**\n\n```bash\n#!/bin/bash\nFILE_PATH=$(jq -r '.tool_input.file_path')\n\nSENSITIVE_FILES=(\".env\" \"credentials.json\" \"id_rsa\" \".ssh/\")\n\nfor pattern in \"${SENSITIVE_FILES[@]}\"; do\n  if [[ \"$FILE_PATH\" == *\"$pattern\"* ]]; then\n    echo \"BLOCKED: Cannot edit sensitive file: $FILE_PATH\" >&2\n    exit 2\n  fi\ndone\n\nexit 0\n```\n\n### 4. Logging and Audit Trail\n\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"~/.claude/hooks/audit-log.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**audit-log.sh:**\n\n```bash\n#!/bin/bash\nTOOL_INPUT=$(cat)\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\nLOG_FILE=~/.claude/audit.jsonl\n\n# Append to audit log\necho \"{\\\"timestamp\\\":\\\"$TIMESTAMP\\\",\\\"event\\\":$TOOL_INPUT}\" >> \"$LOG_FILE\"\n```\n\n### 5. Desktop Notifications\n\n```json\n{\n  \"hooks\": {\n    \"Stop\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"~/.claude/hooks/notify.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**notify.sh (macOS):**\n\n```bash\n#!/bin/bash\nosascript -e 'display notification \"Claude Code task completed\" with title \"Claude Code\"'\n```\n\n**notify.sh (Linux with notify-send):**\n\n```bash\n#!/bin/bash\nnotify-send \"Claude Code\" \"Task completed\"\n```\n\n### 6. Test Runner on File Changes\n\n```json\n{\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Edit|Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"~/.claude/hooks/auto-test.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**auto-test.sh:**\n\n```bash\n#!/bin/bash\nFILE_PATH=$(jq -r '.tool_input.file_path')\n\n# Only run tests for source files, not test files\nif [[ \"$FILE_PATH\" == *\".test.\"* ]] || [[ \"$FILE_PATH\" == *\"__tests__\"* ]]; then\n  exit 0\nfi\n\n# Run tests in background to not block Claude\n(cd \"$(dirname \"$FILE_PATH\")/../..\" && npm test -- --related \"$FILE_PATH\" 2>&1) &\n```\n\n### 7. Code Quality Checks\n\n```json\n{\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"~/.claude/hooks/lint-check.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n**lint-check.sh:**\n\n```bash\n#!/bin/bash\nFILE_PATH=$(jq -r '.tool_input.file_path')\nCONTENT=$(jq -r '.tool_input.content')\n\n# Only lint JavaScript/TypeScript files\nif [[ ! \"$FILE_PATH\" =~ \\.(js|ts|jsx|tsx)$ ]]; then\n  exit 0\nfi\n\n# Write content to temp file for linting\nTMP_FILE=$(mktemp)\necho \"$CONTENT\" > \"$TMP_FILE\"\n\n# Run ESLint\nif ! eslint \"$TMP_FILE\" 2>&1; then\n  echo \"BLOCKED: Code does not pass linting\" >&2\n  rm \"$TMP_FILE\"\n  exit 2\nfi\n\nrm \"$TMP_FILE\"\nexit 0\n```\n\n## Hook Creation Workflow\n\nWhen a user asks to create a hook, follow these steps:\n\n### Step 1: Gather Requirements\n\nUse AskUserQuestion to collect:\n\n1. **Hook purpose** (what should it do?)\n2. **Trigger event** (PreToolUse, PostToolUse, etc.)\n3. **Target tools** (specific tools or all?)\n4. **Blocking behavior** (should it prevent operations?)\n5. **Scope** (user-level or project-level?)\n\n### Step 2: Select Hook Event\n\nMatch purpose to appropriate event:\n\n- **Validation/Prevention**: PreToolUse (can block)\n- **Post-processing**: PostToolUse\n- **Logging**: Any event, typically PostToolUse or Stop\n- **Notifications**: Stop, SubagentStop, or Notification\n- **Setup/Teardown**: SessionStart, SessionEnd\n\n### Step 3: Design Matcher\n\nChoose appropriate tool targeting:\n\n- **Specific operations**: Name the exact tool (e.g., `\"Bash\"`)\n- **Related operations**: Use pipe (e.g., `\"Edit|Write\"`)\n- **All operations**: Use `\"*\"` (use sparingly)\n\n### Step 4: Write Shell Script\n\nCreate the hook script with:\n\n1. **Shebang**: `#!/bin/bash` or `#!/usr/bin/env bash`\n2. **Input parsing**: Use `jq` to extract relevant fields\n3. **Logic**: Implement the hook's behavior\n4. **Exit code**: 0 for allow, 2 for block (PreToolUse only)\n5. **Error handling**: Log errors to stderr\n\n### Step 5: Configure JSON\n\nCreate or update settings.json:\n\n```json\n{\n  \"hooks\": {\n    \"[EventName]\": [\n      {\n        \"matcher\": \"[ToolMatcher]\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"/path/to/hook-script.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Step 6: Make Script Executable\n\n```bash\nchmod +x /path/to/hook-script.sh\n```\n\n### Step 7: Test Hook\n\n1. Create a test scenario\n2. Trigger the hook event\n3. Verify expected behavior\n4. Check logs/output\n5. Refine as needed\n\n## Output Format\n\nWhen creating a hook, deliver:\n\n1. **Complete shell script** with comments explaining logic\n2. **JSON configuration** for settings.json\n3. **File paths** where script and config should be saved\n4. **Installation instructions** including chmod command\n5. **Testing instructions** for validating the hook works\n6. **Security notes** highlighting any permission requirements\n\n## Error Handling\n\nIf hook requirements are unclear:\n\n1. Ask clarifying questions about the workflow\n2. Suggest similar existing hooks as references\n3. Explain trade-offs between PreToolUse (blocking) vs PostToolUse\n4. Recommend project vs user scope based on use case\n\nIf security concerns exist:\n\n1. Highlight potential security implications\n2. Suggest input validation techniques\n3. Recommend testing in isolation first\n4. Explain credential/permission requirements\n\n## Resources\n\n- Claude Code Hooks Guide: https://docs.claude.com/en/docs/claude-code/hooks-guide\n- Claude Code Hooks Mastery: https://github.com/disler/claude-code-hooks-mastery\n- jq Documentation: https://stedolan.github.io/jq/manual/\n\n## Key Principles\n\n1. **Security first** - Always review and validate hook scripts\n2. **Specific matchers** - Target exact tools, avoid universal `\"*\"`\n3. **Fast execution** - Keep PreToolUse hooks lightweight\n4. **Appropriate events** - Match hook event to purpose\n5. **Proper scope** - User for personal, project for team\n6. **Exit codes matter** - 0 allows, 2 blocks (PreToolUse only)\n7. **Parse input safely** - Use jq, validate data\n8. **Log errors** - Use stderr for error messages\n\nWhen in doubt, create a simpler, more focused hook rather than a complex multi-purpose one.\n",
        "plugin-builder/skills/cc-mcp-builder.md": "---\nname: cc-mcp-builder\ndescription: This skill should be used when users want to configure a new MCP (Model Context Protocol) server in Claude Code. Use this skill to help users configure MCP servers that connect Claude to external tools, APIs, and data sources.\nallowed-tools: Read,Write,Glob,Grep,AskUserQuestion,Bash\nmodel: inherit\nversion: 1.0.0\nlicense: MIT\n---\n\n# Claude Code MCP Server Configuration Builder\n\nConfigure MCP (Model Context Protocol) servers to connect Claude Code to external tools, APIs, databases, and data sources following best practices.\n\n## Overview\n\nThe Model Context Protocol (MCP) is an open standard that enables Claude Code to connect to external tools and data sources. Think of it as \"USB-C for AI\" - a universal way to connect AI models to different services.\n\nMCP servers provide:\n\n- **Tools**: Actions Claude can execute (e.g., search GitHub, query databases)\n- **Resources**: Data Claude can access (e.g., file contents, API responses)\n- **Prompts**: Reusable prompt templates exposed as commands\n\nThis skill helps you configure MCP servers that:\n\n- Follow proper JSON configuration structure\n- Use appropriate transport types (HTTP, stdio, SSE)\n- Handle authentication securely via environment variables\n- Work across team members with flexible configuration\n- Integrate seamlessly with Claude Code workflows\n\n## MCP Server Configuration Anatomy\n\nMCP servers are configured in `.mcp.json` files with a standardized JSON format:\n\n### Basic Structure\n\n```json\n{\n  \"mcpServers\": {\n    \"server-name\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.example.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${API_TOKEN}\"\n      }\n    }\n  }\n}\n```\n\n### Configuration Fields\n\n**Server identifier:**\n\n- `\"server-name\"`: Kebab-case unique identifier for this server\n\n**Required for all servers:**\n\n- `type`: Transport type - `\"http\"`, `\"stdio\"`, or `\"sse\"`\n\n**Transport-specific fields:**\n\n**HTTP servers:**\n\n- `url`: HTTPS endpoint URL\n- `headers`: (optional) Custom headers for authentication\n\n**Stdio servers:**\n\n- `command`: Path to executable\n- `args`: (optional) Array of command arguments\n- `env`: (optional) Environment variables object\n\n**SSE servers (deprecated):**\n\n- `url`: Server-Sent Events endpoint\n- Note: Use HTTP instead when possible\n\n## Transport Types\n\n### HTTP Servers (Recommended for Remote Services)\n\nBest for cloud services and remote APIs.\n\n```json\n{\n  \"mcpServers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.github.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${GITHUB_TOKEN}\",\n        \"Accept\": \"application/json\"\n      }\n    }\n  }\n}\n```\n\n**Advantages:**\n\n- Wide compatibility\n- No local installation required\n- Easy to scale\n- Standard authentication patterns\n\n**Use when:**\n\n- Connecting to SaaS APIs\n- Remote data sources\n- Cloud-hosted services\n- Third-party integrations\n\n### Stdio Servers (Best for Local Tools)\n\nRuns local processes on your machine.\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": {\n        \"POSTGRES_CONNECTION_STRING\": \"${DATABASE_URL}\"\n      }\n    }\n  }\n}\n```\n\n**Advantages:**\n\n- Direct system access\n- Full control over execution\n- No network dependency\n- Fast local operations\n\n**Use when:**\n\n- Local database access\n- File system operations\n- Command-line tools\n- Development utilities\n\n### SSE Servers (Deprecated)\n\nServer-Sent Events transport - avoid for new configurations.\n\n```json\n{\n  \"mcpServers\": {\n    \"legacy-service\": {\n      \"type\": \"sse\",\n      \"url\": \"https://example.com/events\"\n    }\n  }\n}\n```\n\n**Migration recommendation**: Convert to HTTP transport when possible.\n\n## Environment Variables\n\nUse environment variables for sensitive data and machine-specific paths.\n\n### Expansion Syntax\n\n**Simple expansion:**\n\n```json\n{\n  \"url\": \"https://${API_HOST}/mcp\"\n}\n```\n\n**With defaults:**\n\n```json\n{\n  \"command\": \"${PYTHON_PATH:-/usr/bin/python3}\"\n}\n```\n\n### Best Practices\n\n**Security:**\n\n- Never hardcode API keys or tokens\n- Use environment variables for credentials\n- Store secrets in `.env` files (gitignored)\n- Use system environment or `.env` loaders\n\n**Portability:**\n\n- Provide sensible defaults with `${VAR:-default}`\n- Document required environment variables\n- Test across different environments\n- Use relative paths when possible\n\n### Example: Secure Configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"stripe\": {\n      \"type\": \"http\",\n      \"url\": \"${STRIPE_API_URL:-https://api.stripe.com/mcp}\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${STRIPE_API_KEY}\"\n      }\n    }\n  }\n}\n```\n\n**Required .env:**\n\n```bash\nSTRIPE_API_KEY=sk_test_...\nSTRIPE_API_URL=https://api.stripe.com/mcp  # Optional, has default\n```\n\n## Configuration Scope\n\n### Project Scope (Recommended for Teams)\n\n**Location**: `.mcp.json` in project root\n**Shared via**: Git version control\n**Best for**: Team collaboration, standardized tools\n\n```json\n{\n  \"mcpServers\": {\n    \"team-database\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": {\n        \"POSTGRES_CONNECTION_STRING\": \"${DATABASE_URL}\"\n      }\n    }\n  }\n}\n```\n\n**Advantages:**\n\n- Everyone uses same tools\n- Consistent development environment\n- Easy onboarding\n- Version controlled\n\n### User Scope (Personal Tools)\n\n**Location**: `~/.claude/settings.json` under `mcpServers` key\n**Shared via**: Not version controlled\n**Best for**: Personal utilities, sensitive configs\n\n```json\n{\n  \"mcpServers\": {\n    \"personal-notes\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:3000/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${PERSONAL_API_KEY}\"\n      }\n    }\n  }\n}\n```\n\n**Advantages:**\n\n- Private configurations\n- Personal preferences\n- Cross-project availability\n- No team impact\n\n### Local Scope (User + Project)\n\n**Location**: `.claude/mcp.json` (project-specific user overrides)\n**Shared via**: Gitignored\n**Best for**: Machine-specific overrides\n\n## Common MCP Server Patterns\n\n### 1. REST API Integration (HTTP)\n\n```json\n{\n  \"mcpServers\": {\n    \"openweather\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.openweathermap.org/mcp/v1\",\n      \"headers\": {\n        \"X-API-Key\": \"${OPENWEATHER_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n      }\n    }\n  }\n}\n```\n\n**Tools provided**: Weather queries, forecasts\n**Resources**: Location data, current conditions\n\n### 2. Database Access (Stdio)\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": {\n        \"POSTGRES_CONNECTION_STRING\": \"${DATABASE_URL}\"\n      }\n    }\n  }\n}\n```\n\n**Tools provided**: SQL queries, schema inspection\n**Resources**: Database tables, query results\n\n### 3. File System Operations (Stdio)\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@modelcontextprotocol/server-filesystem\",\n        \"${PROJECT_ROOT:-.}\"\n      ],\n      \"env\": {\n        \"PROJECT_ROOT\": \"${PWD}\"\n      }\n    }\n  }\n}\n```\n\n**Tools provided**: Read/write files, directory operations\n**Resources**: File contents, directory listings\n\n### 4. GitHub Integration (HTTP)\n\n```json\n{\n  \"mcpServers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.github.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${GITHUB_TOKEN}\",\n        \"Accept\": \"application/vnd.github.v3+json\"\n      }\n    }\n  }\n}\n```\n\n**Tools provided**: Create issues, manage PRs, search repos\n**Resources**: Repository data, issue lists\n\n### 5. Web Search (HTTP)\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.search.brave.com/mcp/v1\",\n      \"headers\": {\n        \"X-Subscription-Token\": \"${BRAVE_API_KEY}\"\n      }\n    }\n  }\n}\n```\n\n**Tools provided**: Web search, news search\n**Resources**: Search results, snippets\n\n### 6. Browser Automation (Stdio)\n\n```json\n{\n  \"mcpServers\": {\n    \"puppeteer\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-puppeteer\"]\n    }\n  }\n}\n```\n\n**Tools provided**: Navigate URLs, screenshot, execute JavaScript\n**Resources**: Page contents, DOM elements\n\n### 7. Slack Integration (HTTP)\n\n```json\n{\n  \"mcpServers\": {\n    \"slack\": {\n      \"type\": \"http\",\n      \"url\": \"https://slack.com/api/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${SLACK_BOT_TOKEN}\",\n        \"Content-Type\": \"application/json\"\n      }\n    }\n  }\n}\n```\n\n**Tools provided**: Send messages, create channels, manage users\n**Resources**: Channel lists, message history\n\n## Platform-Specific Considerations\n\n### Windows Compatibility\n\nWrap `npx` commands with `cmd /c` for native Windows execution:\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"type\": \"stdio\",\n      \"command\": \"cmd\",\n      \"args\": [\"/c\", \"npx\", \"-y\", \"@modelcontextprotocol/server-postgres\"],\n      \"env\": {\n        \"POSTGRES_CONNECTION_STRING\": \"${DATABASE_URL}\"\n      }\n    }\n  }\n}\n```\n\n### macOS/Linux\n\nUse standard shell execution:\n\n```json\n{\n  \"mcpServers\": {\n    \"custom-tool\": {\n      \"type\": \"stdio\",\n      \"command\": \"/usr/local/bin/custom-tool\",\n      \"args\": [\"--mcp-mode\"]\n    }\n  }\n}\n```\n\n## Installation Commands\n\nUse Claude Code CLI for adding servers:\n\n### HTTP Server\n\n```bash\nclaude mcp add --transport http github https://api.github.com/mcp\n```\n\n### Stdio Server\n\n```bash\nclaude mcp add --transport stdio postgres -- npx -y @modelcontextprotocol/server-postgres\n```\n\nNote: Use `--` to separate Claude's flags from server command args.\n\n### From JSON\n\n```bash\nclaude mcp add-json postgres '{\"type\":\"stdio\",\"command\":\"npx\",\"args\":[\"-y\",\"@modelcontextprotocol/server-postgres\"]}'\n```\n\n### List Servers\n\n```bash\nclaude mcp list\n```\n\n### Remove Server\n\n```bash\nclaude mcp remove server-name\n```\n\n## Best Practices\n\n### Security\n\n**Credentials management:**\n\n- Store API keys in environment variables\n- Use `.env` files (ensure `.gitignore` includes `.env`)\n- Never commit secrets to version control\n- Rotate credentials regularly\n\n**Access control:**\n\n- Grant minimal necessary permissions\n- Use read-only tokens when possible\n- Audit MCP server access logs\n- Review third-party server code\n\n### Performance\n\n**Output management:**\n\n- Configure `MAX_MCP_OUTPUT_TOKENS` for large responses (default: 25,000)\n- Use pagination for large datasets\n- Cache frequently accessed resources\n- Optimize query patterns\n\n**Connection pooling:**\n\n- Reuse connections when possible\n- Close idle connections\n- Monitor resource usage\n- Set appropriate timeouts\n\n### Testing\n\n**Validation steps:**\n\n1. Verify server appears in `claude mcp list`\n2. Test basic operations in Claude Code\n3. Check error handling with invalid inputs\n4. Validate environment variable expansion\n5. Confirm cross-platform compatibility (if applicable)\n\n**Debugging:**\n\n- Check server logs for errors\n- Validate JSON syntax\n- Test environment variables are set\n- Verify network connectivity (HTTP servers)\n- Confirm executables exist (stdio servers)\n\n### Documentation\n\n**Document for your team:**\n\n- Required environment variables\n- Setup instructions\n- Available tools and resources\n- Example usage patterns\n- Troubleshooting guide\n\n**Example README section:**\n\n````markdown\n## MCP Server: PostgreSQL\n\nProvides database access via MCP.\n\n### Setup\n\n1. Install dependencies:\n   ```bash\n   npm install -g @modelcontextprotocol/server-postgres\n   ```\n````\n\n2. Set environment variable:\n\n   ```bash\n   export DATABASE_URL=\"postgresql://user:pass@localhost:5432/dbname\"\n   ```\n\n3. Configuration is in `.mcp.json` (already committed)\n\n### Available Tools\n\n- `query_database`: Execute SQL queries\n- `list_tables`: Get all table names\n- `describe_table`: Get table schema\n\n### Example Usage\n\n```\nAsk Claude: \"Query the users table for all active users\"\n```\n\n## MCP Server Configuration Workflow\n\nWhen a user asks to configure an MCP server, follow these steps:\n\n### Step 1: Gather Requirements\n\nUse AskUserQuestion to collect:\n\n1. **Server purpose** (what tools/data does it provide?)\n2. **Service type** (cloud API, local tool, database?)\n3. **Transport preference** (HTTP for remote, stdio for local)\n4. **Authentication** (API keys, tokens, credentials?)\n5. **Scope** (project-wide or personal?)\n\n### Step 2: Select Transport Type\n\nMatch service to transport:\n\n- **Remote APIs**: HTTP\n- **Local executables**: Stdio\n- **Legacy systems**: SSE (migrate to HTTP if possible)\n\n### Step 3: Design Configuration\n\nCreate JSON with:\n\n1. **Unique server name** (kebab-case)\n2. **Correct transport type**\n3. **Required transport fields** (url/command/args)\n4. **Authentication headers or env vars**\n5. **Environment variable expansion** for sensitive data\n\n### Step 4: Handle Environment Variables\n\nIdentify sensitive data:\n\n1. API keys → `${SERVICE_API_KEY}`\n2. Tokens → `${SERVICE_TOKEN}`\n3. Connection strings → `${SERVICE_URL}`\n4. Paths → `${TOOL_PATH:-/default/path}`\n\nCreate `.env` template with required variables.\n\n### Step 5: Choose Scope\n\nDetermine configuration location:\n\n- **Team-shared**: `.mcp.json` in project root (commit to git)\n- **Personal**: `~/.claude/settings.json` (not committed)\n- **Machine-specific**: `.claude/mcp.json` (gitignored)\n\n### Step 6: Write Configuration\n\nCreate or update the appropriate JSON file with the server configuration.\n\n### Step 7: Provide Setup Instructions\n\nInclude:\n\n1. Installation commands (if applicable)\n2. Environment variable setup\n3. How to verify it works\n4. Example usage with Claude\n5. Troubleshooting tips\n\n## Output Format\n\nWhen configuring an MCP server, deliver:\n\n1. **Complete JSON configuration** with server definition\n2. **File path** where config should be saved\n3. **.env template** with required environment variables\n4. **Installation commands** for any dependencies\n5. **Testing instructions** for validating server works\n6. **Usage examples** showing how to use the server in Claude\n7. **Documentation** for team members (if project-scoped)\n\n## Error Handling\n\nIf server requirements are unclear:\n\n1. Ask clarifying questions about the service\n2. Suggest similar existing MCP servers as references\n3. Explain trade-offs between HTTP vs stdio\n4. Recommend security best practices\n\nIf authentication is complex:\n\n1. Guide user through obtaining credentials\n2. Explain environment variable setup\n3. Provide .env template\n4. Suggest secure storage methods\n\n## Resources\n\n- Claude Code MCP Documentation: https://docs.claude.com/en/docs/claude-code/mcp\n- Model Context Protocol Spec: https://modelcontextprotocol.io/\n- MCP Server Registry: https://github.com/modelcontextprotocol/servers\n- Claude MCP Community: https://www.claudemcp.com/\n\n## Key Principles\n\n1. **Security first** - Never hardcode credentials\n2. **Environment variables** - Use for all sensitive data and paths\n3. **Appropriate transport** - HTTP for remote, stdio for local\n4. **Project scope** - Share team tools via `.mcp.json`\n5. **Documentation** - Explain setup and usage clearly\n6. **Test thoroughly** - Validate across environments\n7. **Minimal permissions** - Grant least privilege necessary\n8. **Monitor output** - Configure token limits appropriately\n\nWhen in doubt, choose HTTP transport for remote services and stdio for local tools, and always use environment variables for sensitive configuration.\n",
        "plugin-builder/skills/cc-skill-builder.md": "---\nname: cc-skill-builder\ndescription: This skill should be used when users want to create a new Claude Code skill. Use this skill to help users design, structure, and implement highly effective Claude Code skills that follow best practices for prompt engineering, tool permissions, and progressive disclosure.\nallowed-tools: Read,Write,Glob,Grep,AskUserQuestion,Bash\nmodel: inherit\nversion: 1.0.0\nlicense: MIT\n---\n\n# Claude Code Skill Builder\n\nCreate highly effective Claude Code skills following industry best practices for structure, prompt engineering, and resource organization.\n\n## Overview\n\nSkills are specialized prompt templates that inject domain-specific instructions into Claude's conversation context. Unlike traditional tools that execute actions, skills operate through context injection to expand Claude's capabilities with specialized knowledge and workflows.\n\nThis skill helps you create skills that:\n\n- Follow proper markdown structure with YAML frontmatter\n- Use effective prompt engineering techniques\n- Implement progressive disclosure patterns\n- Apply appropriate tool permissions\n- Organize supporting resources efficiently\n\n## Skill Anatomy\n\nEvery skill requires a `SKILL.md` markdown file with two sections:\n\n### 1. YAML Frontmatter (Configuration)\n\n```yaml\n---\nname: skill-identifier\ndescription: Action-oriented description of when to invoke this skill\nallowed-tools: Read,Write,Bash\nmodel: inherit\nversion: 1.0.0\nlicense: MIT\n---\n```\n\n**Required fields:**\n\n- `name`: Kebab-case identifier (e.g., `python-debugger`)\n- `description`: Primary signal for skill selection - use clear, action-oriented language stating when to invoke (e.g., \"This skill should be used when users want to...\")\n\n**Optional fields:**\n\n- `allowed-tools`: Comma-separated tool permissions - only include what's needed\n- `model`: Model override or \"inherit\" for session model\n- `license`, `version`: Standard metadata\n\n### 2. Markdown Content (Instructions)\n\nStructure your prompt following this recommended pattern:\n\n1. **Brief purpose statement** (1-2 sentences)\n2. **Overview section** - What the skill does and when to use it\n3. **Prerequisites** - Required context or setup\n4. **Step-by-step instructions** - Clear, imperative workflow\n5. **Output format specifications** - Expected deliverables\n6. **Error handling guidance** - Common issues and solutions\n7. **Concrete examples** - Demonstrating usage patterns\n8. **Resource references** - Links to documentation or helper files\n\n## Best Practices\n\n### Prompt Engineering\n\n**Length:** Keep under 5,000 words to avoid context bloat. Move detailed content to reference files.\n\n**Voice:** Use imperative language (\"Analyze code for...\", \"Generate a report...\") rather than second person (\"You should...\").\n\n**File Paths:** Always use `{baseDir}` for file paths - never hardcode absolute paths. Example:\n\n```\npython {baseDir}/scripts/analyzer.py\n```\n\n**Progressive Disclosure:** Reveal information in stages:\n\n1. Frontmatter discloses minimal metadata\n2. Upon selection, load complete SKILL.md\n3. Load helper assets as execution proceeds\n\n### Tool Permissions\n\nOnly include tools your skill actually needs. Examples:\n\n- Just reading/writing files: `Read,Write`\n- Git operations: `Bash(git:*)`\n- NPM operations: `Bash(npm:*)`\n- Web research: `Read,Write,WebFetch,WebSearch`\n\nAvoid listing unnecessary tools - each permission increases security risk.\n\n### Resource Organization\n\nBundle supporting files in three directories:\n\n**`scripts/`** - Executable Python/Bash automation\n\n- Use for complex operations requiring precise logic\n- Reference as: `python {baseDir}/scripts/script_name.py`\n\n**`references/`** - Documentation loaded into Claude's context\n\n- Detailed markdown files, JSON schemas, configuration templates\n- Read via Read tool when needed\n\n**`assets/`** - Templates and binary files referenced by path\n\n- HTML templates, CSS, images, configuration boilerplate\n- Not loaded into context, only referenced by path\n\n## Common Skill Patterns\n\n### 1. Script Automation\n\nOffload complex operations to Python/Bash scripts. Example:\n\n```markdown\nExecute the analysis script:\n\\`\\`\\`bash\npython {baseDir}/scripts/analyze.py --input {file}\n\\`\\`\\`\n```\n\n### 2. Read-Process-Write\n\nFile transformation workflows:\n\n1. Read input file(s)\n2. Process/transform content\n3. Write output file(s)\n\n### 3. Search-Analyze-Report\n\nCodebase analysis:\n\n1. Search for patterns using Grep/Glob\n2. Analyze findings\n3. Generate structured report\n\n### 4. Command Chain Execution\n\nMulti-step operations with dependencies:\n\n```markdown\n1. Run tests: `npm test`\n2. Build project: `npm run build`\n3. Deploy: `npm run deploy`\n```\n\n### 5. Wizard-Style Workflows\n\nMulti-step with user confirmation between phases:\n\n1. Gather requirements via AskUserQuestion\n2. Show plan, get approval\n3. Execute approved plan step-by-step\n\n## Skill Creation Workflow\n\nWhen a user asks to create a skill, follow these steps:\n\n### Step 1: Gather Requirements\n\nUse AskUserQuestion to collect:\n\n1. Skill name (kebab-case)\n2. Domain/technology focus\n3. Specialized knowledge to provide\n4. Tools needed\n5. Specific tasks to handle\n\n### Step 2: Design Frontmatter\n\nCreate YAML configuration with:\n\n- Clear, action-oriented description\n- Minimal necessary tool permissions\n- Appropriate model setting (usually \"inherit\")\n\nExample description pattern:\n\n> \"This skill should be used when users want to [specific action/goal]. Use this skill to [key capabilities].\"\n\n### Step 3: Structure Prompt Content\n\nOrganize instructions following the recommended pattern:\n\n1. Purpose statement\n2. Overview\n3. Prerequisites (if any)\n4. Step-by-step workflow\n5. Output specifications\n6. Error handling\n7. Examples\n8. References\n\n### Step 4: Optimize for Context\n\n- Keep prompt under 5,000 words\n- Use imperative voice throughout\n- Replace absolute paths with `{baseDir}`\n- Move lengthy content to reference files\n\n### Step 5: Identify Supporting Resources\n\nDetermine if the skill needs:\n\n- **Scripts** for complex automation\n- **References** for detailed documentation\n- **Assets** for templates or configurations\n\nCreate these in appropriate directories if needed.\n\n### Step 6: Write and Validate\n\n1. Create `SKILL.md` file with complete content\n2. Review against best practices checklist\n3. Test skill invocation and context injection\n4. Refine based on effectiveness\n\n## Skill Selection Mechanics\n\nSkills are selected through Claude's native language understanding, not algorithmic matching. The `description` field is the primary signal.\n\n**Effective descriptions:**\n\n- State explicit usage conditions\n- Use action-oriented language\n- Specify concrete scenarios\n- Avoid vague or overly broad language\n\n**Examples:**\n\nGood: \"This skill should be used when users want to analyze Python code for performance bottlenecks and suggest optimizations.\"\n\nPoor: \"Python code analysis helper\"\n\n## Common Pitfalls to Avoid\n\n1. **Over-permission:** Listing tools not actually needed\n2. **Context bloat:** Prompts exceeding 5,000 words\n3. **Second-person voice:** Using \"you should\" instead of imperatives\n4. **Hardcoded paths:** Absolute paths instead of `{baseDir}`\n5. **Vague descriptions:** Unclear skill selection criteria\n6. **Missing examples:** No concrete usage demonstrations\n7. **Monolithic design:** Not using progressive disclosure or reference files\n\n## Output Format\n\nWhen creating a skill, deliver:\n\n1. **Complete SKILL.md file** with frontmatter and instructions\n2. **Tool permission justification** explaining why each tool is needed\n3. **Usage example** demonstrating skill invocation\n4. **Supporting resources** (scripts, references, assets) if applicable\n5. **Integration instructions** for adding to plugin\n\n## Error Handling\n\nIf skill requirements are unclear:\n\n1. Ask clarifying questions before proceeding\n2. Suggest similar existing skills as references\n3. Recommend breaking complex skills into multiple focused skills\n\nIf tool permissions seem excessive:\n\n1. Review each tool against actual usage\n2. Suggest script-based alternatives for complex operations\n3. Consider progressive tool loading patterns\n\n## Resources\n\n- Claude Code Skills Guide: https://leehanchung.github.io/blogs/2025/10/26/claude-skills-deep-dive/\n- Claude Code Skills Docs: https://www.anthropic.com/news/skills\n- Claude Code Working with Skills: https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills\n\n## Examples\n\n### Example 1: Simple Read-Process-Write Skill\n\n```yaml\n---\nname: json-formatter\ndescription: This skill should be used when users want to format and validate JSON files with proper indentation and error checking.\nallowed-tools: Read,Write\nmodel: inherit\n---\n\n# JSON Formatter\n\nFormat and validate JSON files with proper indentation.\n\n## Steps\n\n1. Read the specified JSON file using Read tool\n2. Parse and validate JSON structure\n3. Format with consistent indentation (2 spaces)\n4. Write formatted content back to file\n5. Report any syntax errors or validation issues\n\n## Output Format\n\nProvide summary including:\n- File path processed\n- Validation status\n- Formatting changes applied\n- Any errors encountered\n```\n\n### Example 2: Script-Based Analysis Skill\n\n```yaml\n---\nname: dependency-analyzer\ndescription: This skill should be used when users want to analyze project dependencies for security vulnerabilities, outdated packages, and license compliance issues.\nallowed-tools: Read,Bash(npm:*,pip:*)\nmodel: inherit\n---\n\n# Dependency Analyzer\n\nAnalyze project dependencies for security, currency, and licensing.\n\n## Overview\n\nExamines package.json (Node) or requirements.txt (Python) dependencies and generates comprehensive analysis reports.\n\n## Steps\n\n1. Identify project type (Node.js or Python)\n2. Read dependency manifest file\n3. Run security audit: `npm audit` or `pip-audit`\n4. Check for outdated packages\n5. Analyze license compatibility\n6. Generate structured report\n\n## Output Format\n\nMarkdown report with sections:\n- Critical vulnerabilities\n- Outdated dependencies with update recommendations\n- License compliance issues\n- Summary statistics\n```\n\n## Key Principles\n\n1. **Skills inject context, not code** - Focus on instructions, not implementation\n2. **Description drives selection** - Make it clear and action-oriented\n3. **Minimal permissions** - Only request tools actually needed\n4. **Progressive disclosure** - Reveal complexity gradually\n5. **Imperative voice** - Direct instructions, not suggestions\n6. **Path variables** - Always use `{baseDir}` for portability\n7. **Concrete examples** - Show, don't just tell\n8. **Stay focused** - One skill, one purpose\n\nWhen in doubt, create a simpler, more focused skill rather than a complex multi-purpose one.\n",
        "specforge-backend-rust-axum/.claude-plugin/plugin.json": "{\n  \"name\": \"specforge-backend-rust-axum\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Rust + Axum backend framework expertise - implements handlers using OpenAPI and DB-generated types\",\n  \"author\": {\n    \"name\": \"Daniel Emod Kovacs\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"specforge\",\n    \"backend\",\n    \"rust\",\n    \"axum\",\n    \"openapi\",\n    \"framework\",\n    \"distributed-systems\"\n  ],\n  \"skills\": [\n    \"./skills/axum-patterns.md\",\n    \"./skills/axum-handler-implementation.md\",\n    \"./skills/axum-error-mapping.md\",\n    \"./skills/rust-testing.md\",\n    \"./skills/axum-middleware.md\",\n    \"./skills/rust-openapi-integration.md\",\n    \"./skills/rust-dev-setup.md\",\n    \"./skills/axum-idempotency.md\",\n    \"./skills/axum-resilience.md\",\n    \"./skills/axum-tracing.md\",\n    \"./skills/axum-timeouts.md\",\n    \"./skills/axum-health-shutdown.md\",\n    \"./skills/axum-rate-limiting.md\"\n  ],\n  \"agents\": [\n    \"./agents/handler-implementer.md\",\n    \"./agents/test-generator.md\",\n    \"./agents/router-builder.md\"\n  ]\n}",
        "specforge-backend-rust-axum/README.md": "# SpecForge Backend: Rust + Axum\n\n**Backend framework expertise plugin for SpecForge** - Makes Claude Code an expert in implementing API backends using Rust, Axum, OpenAPI-generated types, and database-generated query functions.\n\n## Overview\n\nThis plugin teaches Claude Code how to:\n- Build production-ready REST APIs with Rust and Axum\n- Stitch together OpenAPI-generated types with database-generated query functions\n- Implement framework-specific patterns (routing, middleware, extractors, state)\n- Apply distributed systems patterns (idempotency, retries, circuit breakers, tracing)\n- Write comprehensive tests using generated types\n- Set up Docker-based development environments\n\n## What Gets Generated\n\nThe plugin works with code generated from two sources:\n\n1. **OpenAPI Generator** → Request/Response types (`src/generated/api/`)\n2. **Database Codegen Plugin** → Query functions (`src/generated/db/`)\n\nThe backend plugin teaches how to **combine** these generated pieces using Axum framework patterns.\n\n## Installation\n\n```bash\n/plugin install ./specforge-backend-rust-axum\n```\n\n## Skills Included\n\n### Core Framework Skills (7)\n\n1. **axum-patterns** - Router setup, extractors, state management\n2. **axum-handler-implementation** - Business logic patterns for handlers\n3. **axum-error-mapping** - Map errors to OpenAPI schemas\n4. **rust-testing** - Test patterns using generated types\n5. **axum-middleware** - CORS, logging, auth middleware\n6. **rust-openapi-integration** - Using OpenAPI generators\n7. **rust-dev-setup** - Docker Compose configuration\n\n### Distributed System Skills (6)\n\n8. **axum-idempotency** - Idempotent handlers with Redis\n9. **axum-resilience** - Retries with exponential backoff and circuit breakers\n10. **axum-tracing** - Request tracing with correlation IDs\n11. **axum-timeouts** - Request and operation timeouts\n12. **axum-health-shutdown** - Health checks and graceful shutdown\n13. **axum-rate-limiting** - Rate limiting with tower-governor\n\n## Agents Included\n\n### 1. Handler Implementer\n\nImplements HTTP handlers by combining OpenAPI-generated types with DB-generated functions.\n\n**Input:**\n- OpenAPI endpoint definition\n- Path to generated API types\n- Path to generated DB functions\n\n**Output:**\n- Complete handler implementation with business logic\n\n**Model:** Haiku (fast and efficient for CRUD handlers)\n\n### 2. Test Generator\n\nGenerates comprehensive test suites for handlers using generated types.\n\n**Input:**\n- Handler path and function name\n- Endpoint definition\n- Path to generated API types\n\n**Output:**\n- Complete test suite with success and error cases\n\n**Model:** Haiku\n\n### 3. Router Builder\n\nWires up all handlers into an Axum router with middleware.\n\n**Input:**\n- List of handlers with paths, methods, and handler names\n- Handler module names\n\n**Output:**\n- Complete router configuration with middleware\n\n**Model:** Haiku\n\n## Example Usage\n\n### Creating a User Handler\n\nGiven this OpenAPI endpoint:\n\n```yaml\npaths:\n  /api/users:\n    post:\n      operationId: createUser\n      description: Create a new user. Check for duplicate email before creating.\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/CreateUserRequest'\n      responses:\n        '201':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/User'\n```\n\nThe handler-implementer agent generates:\n\n```rust\nuse axum::{extract::State, http::StatusCode, Json};\nuse crate::{\n    generated::api::{CreateUserRequest, User, ErrorResponse},\n    generated::db::{create_user, get_user_by_email},\n    state::AppState,\n};\n\npub async fn create_user_handler(\n    State(state): State<AppState>,\n    Json(payload): Json<CreateUserRequest>,\n) -> Result<(StatusCode, Json<User>), ErrorResponse> {\n    // Business logic: Check for duplicate email\n    if get_user_by_email(&state.db, &payload.email).await?.is_some() {\n        return Err(ErrorResponse::conflict(\"Email already exists\"));\n    }\n\n    // Create user using DB-generated function\n    let user = create_user(&state.db, &payload.email, &payload.name).await?;\n\n    Ok((StatusCode::CREATED, Json(user)))\n}\n```\n\n## Integration with SpecForge\n\nThis plugin is part of the SpecForge ecosystem:\n\n1. **SpecForge Database Plugin** → Manages database schema and migrations\n2. **SpecForge Codegen Plugin** → Generates DB query functions\n3. **SpecForge Backend Plugin** (this plugin) → Implements API handlers\n4. **SpecForge Client Plugin** → Generates frontend API clients\n\n## Requirements\n\n- Rust 1.75+\n- Axum 0.7+\n- SQLx or compatible database driver\n- OpenAPI 3.0+ specification\n\n## Philosophy\n\n**The backend plugin does NOT define types.** Types come from:\n- OpenAPI generator (request/response schemas)\n- Codegen plugin (database models, query functions)\n\nThe backend plugin teaches how to:\n- Use framework patterns (routing, middleware, extractors)\n- Wire together generated types + generated DB functions\n- Implement business logic between API and database layers\n- Handle errors using generated error schemas\n- Write tests using generated types\n\n## Next Steps\n\n1. Install the plugin: `/plugin install ./specforge-backend-rust-axum`\n2. Use with SpecForge database and codegen plugins\n3. Invoke agents to implement handlers from your OpenAPI spec\n\n## License\n\nMIT License - See [LICENSE](./LICENSE) file for details\n\n## Author\n\nDaniel Emod Kovacs (@danielkov)\n\n## Contributing\n\nContributions welcome! Please ensure:\n- All skills follow the implementation-only pattern (code, not concepts)\n- Agents use appropriate models (Haiku for simple tasks, Sonnet for complex)\n- Examples use OpenAPI-generated and DB-generated types\n- Code follows Rust and Axum best practices",
        "specforge-backend-rust-axum/agents/handler-implementer.md": "---\nname: handler-implementer\nmodel: haiku\ncontext_budget: 5000\ndescription: Implements Axum handlers using OpenAPI definitions and generated code\n---\n\n# Handler Implementer Agent\n\nYou implement HTTP handlers by stitching together OpenAPI-generated types with DB-generated functions.\n\n## Your Task\n\nGiven:\n1. OpenAPI endpoint definition\n2. Path to generated API types\n3. Path to generated DB functions\n\nImplement the handler function.\n\n## Implementation Steps\n\n1. **Determine imports**:\n   ```rust\n   // From OpenAPI generator\n   use crate::generated::api::{\n       CreateUserRequest,  // From requestBody schema\n       User,               // From response schema\n       ErrorResponse,      // From error schema\n   };\n\n   // From codegen plugin\n   use crate::generated::db::{\n       create_user,\n       get_user_by_email,\n   };\n   ```\n\n2. **Implement handler signature** (using framework patterns from skills):\n   ```rust\n   pub async fn create_user_handler(\n       State(state): State<AppState>,\n       Json(payload): Json<CreateUserRequest>,\n   ) -> Result<(StatusCode, Json<User>), ErrorResponse>\n   ```\n\n3. **Implement business logic** (from OpenAPI description):\n   - Parse description for business rules\n   - \"Check for duplicate email\" → call get_user_by_email()\n   - Return appropriate error responses (409 for conflict)\n\n4. **Call DB functions**:\n   ```rust\n   let user = create_user(&state.db, &payload.email, &payload.name).await?;\n   ```\n\n5. **Return generated type**:\n   ```rust\n   Ok((StatusCode::CREATED, Json(user)))\n   ```\n\n## Example Output\n\n```rust\nuse axum::{extract::State, http::StatusCode, Json};\nuse crate::{\n    generated::api::{CreateUserRequest, User, ErrorResponse},\n    generated::db::{create_user, get_user_by_email},\n    state::AppState,\n};\n\npub async fn create_user_handler(\n    State(state): State<AppState>,\n    Json(payload): Json<CreateUserRequest>,\n) -> Result<(StatusCode, Json<User>), ErrorResponse> {\n    // Business logic: Check for duplicate email\n    if get_user_by_email(&state.db, &payload.email).await?.is_some() {\n        return Err(ErrorResponse::conflict(\"Email already exists\"));\n    }\n\n    // Create user\n    let user = create_user(&state.db, &payload.email, &payload.name).await?;\n\n    Ok((StatusCode::CREATED, Json(user)))\n}\n```\n\n## Access Skills\n\n- `axum-patterns` - Framework patterns\n- `axum-handler-implementation` - Business logic patterns\n- `axum-error-mapping` - Error handling\n- `axum-idempotency` - Idempotent handlers\n- `axum-resilience` - Retries and circuit breakers\n- `axum-tracing` - Request tracing\n- `axum-timeouts` - Timeout handling\n- `axum-health-shutdown` - Health checks",
        "specforge-backend-rust-axum/agents/router-builder.md": "---\nname: router-builder\nmodel: haiku\ncontext_budget: 3000\ndescription: Wires up all handlers into an Axum router\n---\n\n# Router Builder Agent\n\nYou create the main Axum router by wiring together all handler functions with their routes.\n\n## Your Task\n\nGiven:\n1. List of handlers with their paths, methods, and handler function names\n2. Handler module names\n\nGenerate the router configuration.\n\n## Router Pattern\n\n```rust\nuse axum::{\n    routing::{get, post, put, delete},\n    Router,\n};\n\nmod handlers;\nuse handlers::{users, orders};\n\npub fn create_router(state: AppState) -> Router {\n    Router::new()\n        // Health checks\n        .route(\"/health\", get(health_check))\n        .route(\"/ready\", get(readiness_check))\n\n        // API routes\n        .route(\"/api/users\",\n            get(users::list_users)\n                .post(users::create_user))\n        .route(\"/api/users/:id\",\n            get(users::get_user)\n                .put(users::update_user)\n                .delete(users::delete_user))\n        .route(\"/api/orders\",\n            get(orders::list_orders)\n                .post(orders::create_order))\n\n        // Middleware\n        .layer(create_cors())\n        .layer(create_trace_layer())\n        .layer(axum::middleware::from_fn(correlation_id_middleware))\n\n        // State\n        .with_state(state)\n}\n```\n\n## Multiple Methods on Same Route\n\nWhen multiple handlers share the same path:\n\n```rust\n.route(\"/api/users\",\n    get(list_users)\n        .post(create_user))\n```\n\n## Path Parameters\n\nUse `:param` syntax:\n\n```rust\n.route(\"/api/users/:id\", get(get_user))\n.route(\"/api/users/:id/orders\", get(get_user_orders))\n```\n\n## Middleware Order\n\nApply middleware in this order (bottom to top execution):\n\n```rust\n.layer(create_cors())              // CORS headers\n.layer(create_trace_layer())       // Request logging\n.layer(timeout_middleware)         // Timeouts\n.layer(correlation_id_middleware)  // Correlation IDs\n.layer(rate_limiter)               // Rate limiting\n```\n\n## Access Skills\n\n- `axum-patterns` - Router and middleware patterns\n- `axum-middleware` - Middleware configuration\n- `axum-tracing` - Correlation ID middleware\n- `axum-health-shutdown` - Health check endpoints",
        "specforge-backend-rust-axum/agents/test-generator.md": "---\nname: test-generator\nmodel: haiku\ncontext_budget: 5000\ndescription: Generates comprehensive tests for Axum handlers using generated types\n---\n\n# Test Generator Agent\n\nYou generate comprehensive tests for HTTP handlers using OpenAPI-generated types and test patterns.\n\n## Your Task\n\nGiven:\n1. Handler path and function name\n2. Endpoint definition (path, method, request/response schemas)\n3. Path to generated API types\n\nGenerate a complete test suite.\n\n## Test Structure\n\n```rust\nuse axum::{\n    body::Body,\n    http::{Request, StatusCode},\n};\nuse tower::ServiceExt;\nuse crate::generated::api::{CreateUserRequest, User, ErrorResponse};\n\nmod common;\n\n#[tokio::test]\nasync fn test_{handler_name}_success() {\n    // Setup\n    let db = common::setup_test_db().await;\n    let state = common::setup_test_state(db);\n    let app = create_router(state);\n\n    // Create request\n    let payload = CreateUserRequest {\n        email: \"test@example.com\".to_string(),\n        name: Some(\"Test User\".to_string()),\n    };\n\n    let request = Request::builder()\n        .uri(\"/api/users\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(Body::from(serde_json::to_string(&payload).unwrap()))\n        .unwrap();\n\n    // Execute\n    let response = app.oneshot(request).await.unwrap();\n\n    // Assert\n    assert_eq!(response.status(), StatusCode::CREATED);\n\n    let body = hyper::body::to_bytes(response.into_body()).await.unwrap();\n    let user: User = serde_json::from_slice(&body).unwrap();\n\n    assert_eq!(user.email, \"test@example.com\");\n}\n```\n\n## Required Test Cases\n\nGenerate tests for:\n\n1. **Success case**: Happy path with valid input\n2. **Validation errors**: Invalid input (400 Bad Request)\n3. **Not found**: Resource doesn't exist (404 Not Found)\n4. **Conflict**: Duplicate resource (409 Conflict)\n5. **Authorization**: If endpoint requires auth (401 Unauthorized)\n\n## Test Helpers\n\nCreate helper functions for common operations:\n\n```rust\nfn create_user_request(payload: &CreateUserRequest) -> Request<Body> {\n    Request::builder()\n        .uri(\"/api/users\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(Body::from(serde_json::to_string(payload).unwrap()))\n        .unwrap()\n}\n\nasync fn parse_response_body<T: serde::de::DeserializeOwned>(response: Response) -> T {\n    let body = hyper::body::to_bytes(response.into_body()).await.unwrap();\n    serde_json::from_slice(&body).unwrap()\n}\n```\n\n## Access Skills\n\n- `rust-testing` - Test patterns and setup\n- `axum-patterns` - Framework patterns\n- `rust-openapi-integration` - Using generated types",
        "specforge-backend-rust-axum/skills/axum-error-mapping.md": "---\nname: axum-error-mapping\ndescription: Map database and framework errors to OpenAPI-generated error responses\nkeywords: [axum, errors, openapi, error-handling]\n---\n\n# Error Mapping in Axum\n\nMap errors to OpenAPI-generated error schemas.\n\n## Generated Error Schema\n\nYour OpenAPI spec defines error schemas:\n\n```yaml\ncomponents:\n  schemas:\n    ErrorResponse:\n      type: object\n      required: [error, code]\n      properties:\n        error:\n          type: string\n        code:\n          type: string\n        details:\n          type: object\n```\n\nThis generates:\n\n```rust\n// src/generated/api/errors.rs\n#[derive(Serialize)]\npub struct ErrorResponse {\n    pub error: String,\n    pub code: String,\n    pub details: Option<serde_json::Value>,\n}\n```\n\n## Using Generated Error Schema\n\n```rust\nuse axum::{\n    http::StatusCode,\n    response::{IntoResponse, Response},\n    Json,\n};\nuse crate::generated::api::ErrorResponse;\n\nimpl ErrorResponse {\n    pub fn not_found(message: &str) -> Self {\n        Self {\n            error: message.to_string(),\n            code: \"NOT_FOUND\".to_string(),\n            details: None,\n        }\n    }\n\n    pub fn conflict(message: &str) -> Self {\n        Self {\n            error: message.to_string(),\n            code: \"CONFLICT\".to_string(),\n            details: None,\n        }\n    }\n\n    pub fn bad_request(message: &str) -> Self {\n        Self {\n            error: message.to_string(),\n            code: \"BAD_REQUEST\".to_string(),\n            details: None,\n        }\n    }\n\n    pub fn internal_error() -> Self {\n        Self {\n            error: \"Internal server error\".to_string(),\n            code: \"INTERNAL_ERROR\".to_string(),\n            details: None,\n        }\n    }\n}\n\nimpl IntoResponse for ErrorResponse {\n    fn into_response(self) -> Response {\n        let status = match self.code.as_str() {\n            \"NOT_FOUND\" => StatusCode::NOT_FOUND,\n            \"CONFLICT\" => StatusCode::CONFLICT,\n            \"BAD_REQUEST\" => StatusCode::BAD_REQUEST,\n            _ => StatusCode::INTERNAL_SERVER_ERROR,\n        };\n\n        (status, Json(self)).into_response()\n    }\n}\n```\n\n## Mapping Database Errors\n\n```rust\nimpl From<sqlx::Error> for ErrorResponse {\n    fn from(err: sqlx::Error) -> Self {\n        tracing::error!(\"Database error: {:?}\", err);\n\n        match err {\n            sqlx::Error::RowNotFound => {\n                ErrorResponse::not_found(\"Resource not found\")\n            }\n            sqlx::Error::Database(db_err) => {\n                if db_err.is_unique_violation() {\n                    ErrorResponse::conflict(\"Resource already exists\")\n                } else {\n                    ErrorResponse::internal_error()\n                }\n            }\n            _ => ErrorResponse::internal_error(),\n        }\n    }\n}\n```\n\n## Usage in Handlers\n\n```rust\npub async fn get_user(\n    State(state): State<AppState>,\n    Path(id): Path<i64>,\n) -> Result<Json<User>, ErrorResponse> {\n    let user = get_user_by_id(&state.db, id)\n        .await?  // ← sqlx::Error automatically converted to ErrorResponse\n        .ok_or_else(|| ErrorResponse::not_found(\"User not found\"))?;\n\n    Ok(Json(user))\n}\n```",
        "specforge-backend-rust-axum/skills/axum-handler-implementation.md": "---\nname: axum-handler-implementation\ndescription: Implement Axum handlers using OpenAPI-generated and DB-generated code\nkeywords: [axum, handlers, openapi, generated-types]\n---\n\n# Axum Handler Implementation\n\nHow to implement handlers that stitch together OpenAPI-generated types with DB-generated functions.\n\n## Input: OpenAPI Route + Generated Paths\n\nYou will receive:\n1. OpenAPI route definition\n2. Path to OpenAPI-generated types: `src/generated/api/`\n3. Path to DB-generated functions: `src/generated/db/`\n\n## Handler Pattern\n\n```rust\nuse axum::{extract::State, http::StatusCode, Json};\n\n// Import generated API types\nuse crate::generated::api::{\n    CreateUserRequest,    // ← From OpenAPI requestBody\n    User,                 // ← From OpenAPI response\n    ErrorResponse,        // ← From OpenAPI error schema\n};\n\n// Import generated DB functions\nuse crate::generated::db::{\n    create_user as db_create_user,\n    get_user_by_email,\n};\n\nuse crate::state::AppState;\n\npub async fn create_user(\n    State(state): State<AppState>,\n    Json(payload): Json<CreateUserRequest>,  // ← Generated type\n) -> Result<(StatusCode, Json<User>), ErrorResponse> {  // ← Generated types\n    // 1. Business validation\n    if get_user_by_email(&state.db, &payload.email).await?.is_some() {\n        return Err(ErrorResponse::conflict(\"Email already exists\"));\n    }\n\n    // 2. Call generated DB function\n    let user = db_create_user(\n        &state.db,\n        &payload.email,\n        &payload.name,\n    )\n    .await?;\n\n    // 3. Return generated response type\n    Ok((StatusCode::CREATED, Json(user)))\n}\n```\n\n## Business Logic Patterns\n\n### Duplicate Checks\n\n```rust\n// Check if resource exists before creating\nif get_user_by_email(&state.db, &payload.email).await?.is_some() {\n    return Err(ErrorResponse::conflict(\"Email already exists\"));\n}\n```\n\n### NOT NULL Validation\n\n```rust\n// Ensure required fields are present\nlet name = payload.name.ok_or_else(|| {\n    ErrorResponse::bad_request(\"Name is required\")\n})?;\n```\n\n### Verify Resource Exists\n\n```rust\n// GET /users/:id - verify exists\nlet user = get_user_by_id(&state.db, id)\n    .await?\n    .ok_or_else(|| ErrorResponse::not_found(\"User not found\"))?;\n```\n\n### Complex Queries\n\n```rust\n// Combine multiple DB calls\nlet user = get_user_by_id(&state.db, user_id).await?.ok_or(...)?;\nlet orders = list_orders_by_user(&state.db, user_id).await?;\n\n// Combine into response\nlet response = UserWithOrders { user, orders };\nOk(Json(response))\n```\n\n### Transactions (if DB supports)\n\n```rust\nlet mut tx = state.db.begin().await?;\n\nlet user = create_user_tx(&mut tx, email, name).await?;\ncreate_user_role_tx(&mut tx, user.id, \"user\").await?;\n\ntx.commit().await?;\n```\n\n## Pagination\n\n```rust\nuse crate::generated::api::ListUsersQuery;  // ← Generated from OpenAPI\n\npub async fn list_users(\n    State(state): State<AppState>,\n    Query(query): Query<ListUsersQuery>,\n) -> Result<Json<UserList>, ErrorResponse> {\n    let page = query.page.unwrap_or(1);\n    let per_page = query.per_page.unwrap_or(20).min(100);\n\n    let users = list_users_paginated(&state.db, page, per_page).await?;\n\n    Ok(Json(UserList { users }))\n}\n```",
        "specforge-backend-rust-axum/skills/axum-health-shutdown.md": "---\nname: axum-health-shutdown\ndescription: Implement health checks and graceful shutdown in Axum\nkeywords: [axum, health, readiness, graceful-shutdown]\n---\n\n# Health Checks and Graceful Shutdown\n\n## Health Check Endpoints\n\n```rust\nuse axum::{extract::State, http::StatusCode, Json};\nuse serde::Serialize;\n\n#[derive(Serialize)]\npub struct HealthResponse {\n    pub status: String,\n}\n\n#[derive(Serialize)]\npub struct ReadinessResponse {\n    pub status: String,\n    pub database: String,\n    pub redis: String,\n}\n\npub async fn health_check() -> (StatusCode, Json<HealthResponse>) {\n    (\n        StatusCode::OK,\n        Json(HealthResponse {\n            status: \"ok\".to_string(),\n        }),\n    )\n}\n\npub async fn readiness_check(\n    State(state): State<AppState>,\n) -> (StatusCode, Json<ReadinessResponse>) {\n    let db_status = match sqlx::query(\"SELECT 1\").fetch_one(&state.db).await {\n        Ok(_) => \"ok\",\n        Err(_) => \"error\",\n    };\n\n    let mut redis_conn = state.redis.get().await;\n    let redis_status = match redis_conn {\n        Ok(ref mut conn) => match redis::cmd(\"PING\").query_async::<_, String>(conn).await {\n            Ok(_) => \"ok\",\n            Err(_) => \"error\",\n        },\n        Err(_) => \"error\",\n    };\n\n    let overall_status = if db_status == \"ok\" && redis_status == \"ok\" {\n        StatusCode::OK\n    } else {\n        StatusCode::SERVICE_UNAVAILABLE\n    };\n\n    (\n        overall_status,\n        Json(ReadinessResponse {\n            status: if overall_status == StatusCode::OK {\n                \"ready\".to_string()\n            } else {\n                \"not ready\".to_string()\n            },\n            database: db_status.to_string(),\n            redis: redis_status.to_string(),\n        }),\n    )\n}\n```\n\n## Graceful Shutdown\n\n```rust\nuse tokio::signal;\nuse std::time::Duration;\n\n#[tokio::main]\nasync fn main() {\n    let state = AppState { /* ... */ };\n\n    let app = create_router(state);\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\")\n        .await\n        .unwrap();\n\n    tracing::info!(\"Server starting on :3000\");\n\n    axum::serve(listener, app)\n        .with_graceful_shutdown(shutdown_signal())\n        .await\n        .unwrap();\n\n    tracing::info!(\"Server shut down gracefully\");\n}\n\nasync fn shutdown_signal() {\n    let ctrl_c = async {\n        signal::ctrl_c()\n            .await\n            .expect(\"Failed to install Ctrl+C handler\");\n    };\n\n    #[cfg(unix)]\n    let terminate = async {\n        signal::unix::signal(signal::unix::SignalKind::terminate())\n            .expect(\"Failed to install signal handler\")\n            .recv()\n            .await;\n    };\n\n    #[cfg(not(unix))]\n    let terminate = std::future::pending::<()>();\n\n    tokio::select! {\n        _ = ctrl_c => {\n            tracing::info!(\"Received Ctrl+C, starting graceful shutdown\");\n        },\n        _ = terminate => {\n            tracing::info!(\"Received SIGTERM, starting graceful shutdown\");\n        },\n    }\n\n    // Give in-flight requests time to complete\n    tokio::time::sleep(Duration::from_secs(10)).await;\n}\n```\n\n## Router with Health Endpoints\n\n```rust\npub fn create_router(state: AppState) -> Router {\n    Router::new()\n        .route(\"/health\", get(health_check))\n        .route(\"/ready\", get(readiness_check))\n        .route(\"/api/users\", get(list_users).post(create_user))\n        .with_state(state)\n}\n```",
        "specforge-backend-rust-axum/skills/axum-idempotency.md": "---\nname: axum-idempotency\ndescription: Implement idempotent handlers in Axum using idempotency keys\nkeywords: [axum, idempotency, distributed-systems, retries]\n---\n\n# Idempotency in Axum\n\n## Idempotency Key Header Extractor\n\n```rust\nuse axum::{\n    async_trait,\n    extract::{FromRequestParts, TypedHeader},\n    headers::{Header, HeaderName, HeaderValue},\n    http::request::Parts,\n};\n\nstatic IDEMPOTENCY_KEY: HeaderName = HeaderName::from_static(\"idempotency-key\");\n\npub struct IdempotencyKey(pub String);\n\nimpl<S> FromRequestParts<S> for IdempotencyKey\nwhere\n    S: Send + Sync,\n{\n    type Rejection = (StatusCode, String);\n\n    async fn from_request_parts(parts: &mut Parts, _state: &S) -> Result<Self, Self::Rejection> {\n        parts\n            .headers\n            .get(&IDEMPOTENCY_KEY)\n            .and_then(|v| v.to_str().ok())\n            .map(|s| IdempotencyKey(s.to_string()))\n            .ok_or_else(|| {\n                (\n                    StatusCode::BAD_REQUEST,\n                    \"Missing Idempotency-Key header\".to_string(),\n                )\n            })\n    }\n}\n```\n\n## Idempotency Store (Redis)\n\n```rust\nuse redis::AsyncCommands;\nuse serde::{Deserialize, Serialize};\nuse std::time::Duration;\n\n#[derive(Serialize, Deserialize, Clone)]\npub struct IdempotentResponse {\n    pub status: u16,\n    pub body: String,\n}\n\npub async fn check_idempotency(\n    redis: &mut redis::aio::Connection,\n    key: &str,\n) -> Result<Option<IdempotentResponse>, redis::RedisError> {\n    redis.get(key).await\n}\n\npub async fn store_idempotent_response(\n    redis: &mut redis::aio::Connection,\n    key: &str,\n    response: &IdempotentResponse,\n    ttl: Duration,\n) -> Result<(), redis::RedisError> {\n    redis\n        .set_ex(key, serde_json::to_string(response).unwrap(), ttl.as_secs() as usize)\n        .await\n}\n```\n\n## Handler with Idempotency\n\n```rust\nuse axum::{extract::State, http::StatusCode, Json};\nuse crate::generated::api::{CreateUserRequest, User, ErrorResponse};\nuse crate::generated::db::create_user;\n\npub async fn create_user_idempotent(\n    State(state): State<AppState>,\n    IdempotencyKey(key): IdempotencyKey,\n    Json(payload): Json<CreateUserRequest>,\n) -> Result<(StatusCode, Json<User>), ErrorResponse> {\n    let mut redis = state.redis.get().await?;\n\n    // Check if already processed\n    if let Some(cached) = check_idempotency(&mut redis, &key).await? {\n        let user: User = serde_json::from_str(&cached.body)?;\n        return Ok((StatusCode::from_u16(cached.status).unwrap(), Json(user)));\n    }\n\n    // Process request\n    let user = create_user(&state.db, &payload.email, &payload.name).await?;\n\n    // Store result\n    store_idempotent_response(\n        &mut redis,\n        &key,\n        &IdempotentResponse {\n            status: 201,\n            body: serde_json::to_string(&user)?,\n        },\n        Duration::from_secs(86400), // 24h TTL\n    )\n    .await?;\n\n    Ok((StatusCode::CREATED, Json(user)))\n}\n```\n\n## AppState with Redis\n\n```rust\n#[derive(Clone)]\npub struct AppState {\n    pub db: SqlitePool,\n    pub redis: deadpool_redis::Pool,\n}\n```",
        "specforge-backend-rust-axum/skills/axum-middleware.md": "---\nname: axum-middleware\ndescription: Middleware patterns for Axum (CORS, logging, authentication)\nkeywords: [axum, middleware, cors, logging, auth]\n---\n\n# Axum Middleware Patterns\n\n## CORS Middleware\n\n```rust\nuse tower_http::cors::{CorsLayer, Any};\nuse axum::http::Method;\n\npub fn create_cors() -> CorsLayer {\n    CorsLayer::new()\n        .allow_origin(Any)\n        .allow_methods([Method::GET, Method::POST, Method::PUT, Method::DELETE])\n        .allow_headers(Any)\n}\n\n// In router\nlet app = Router::new()\n    .route(\"/api/users\", get(list_users))\n    .layer(create_cors())\n    .with_state(state);\n```\n\n## Request Logging Middleware\n\n```rust\nuse tower_http::trace::{TraceLayer, DefaultMakeSpan, DefaultOnResponse};\nuse tracing::Level;\n\npub fn create_trace_layer() -> TraceLayer<tower_http::classify::SharedClassifier<tower_http::classify::ServerErrorsAsFailures>> {\n    TraceLayer::new_for_http()\n        .make_span_with(DefaultMakeSpan::new().level(Level::INFO))\n        .on_response(DefaultOnResponse::new().level(Level::INFO))\n}\n\n// In router\nlet app = Router::new()\n    .route(\"/api/users\", get(list_users))\n    .layer(create_trace_layer())\n    .with_state(state);\n```\n\n## Custom Middleware\n\n```rust\nuse axum::{\n    body::Body,\n    extract::Request,\n    middleware::Next,\n    response::Response,\n};\n\npub async fn auth_middleware(\n    req: Request,\n    next: Next,\n) -> Result<Response, StatusCode> {\n    let auth_header = req\n        .headers()\n        .get(\"Authorization\")\n        .and_then(|v| v.to_str().ok());\n\n    match auth_header {\n        Some(token) if token.starts_with(\"Bearer \") => {\n            // Validate token\n            let token = &token[7..];\n            if validate_token(token).await {\n                Ok(next.run(req).await)\n            } else {\n                Err(StatusCode::UNAUTHORIZED)\n            }\n        }\n        _ => Err(StatusCode::UNAUTHORIZED),\n    }\n}\n\n// Apply to specific routes\nlet protected = Router::new()\n    .route(\"/api/users\", post(create_user))\n    .layer(axum::middleware::from_fn(auth_middleware));\n\nlet app = Router::new()\n    .route(\"/health\", get(health_check))\n    .merge(protected)\n    .with_state(state);\n```\n\n## Request ID Middleware\n\n```rust\nuse axum::{\n    body::Body,\n    extract::Request,\n    http::HeaderValue,\n    middleware::Next,\n    response::Response,\n};\nuse uuid::Uuid;\n\npub async fn request_id_middleware(\n    mut req: Request,\n    next: Next,\n) -> Response {\n    let request_id = Uuid::new_v4().to_string();\n\n    req.extensions_mut().insert(request_id.clone());\n\n    let mut response = next.run(req).await;\n    response.headers_mut().insert(\n        \"x-request-id\",\n        HeaderValue::from_str(&request_id).unwrap(),\n    );\n\n    response\n}\n\n// In router\nlet app = Router::new()\n    .route(\"/api/users\", get(list_users))\n    .layer(axum::middleware::from_fn(request_id_middleware))\n    .with_state(state);\n```\n\n## Combining Multiple Middleware\n\n```rust\nuse tower::ServiceBuilder;\n\nlet app = Router::new()\n    .route(\"/api/users\", get(list_users))\n    .layer(\n        ServiceBuilder::new()\n            .layer(create_trace_layer())\n            .layer(axum::middleware::from_fn(request_id_middleware))\n            .layer(create_cors())\n    )\n    .with_state(state);\n```\n\n## Conditional Middleware (Per-Route)\n\n```rust\n// Public routes\nlet public_routes = Router::new()\n    .route(\"/health\", get(health_check))\n    .route(\"/api/auth/login\", post(login));\n\n// Protected routes\nlet protected_routes = Router::new()\n    .route(\"/api/users\", get(list_users).post(create_user))\n    .route(\"/api/users/:id\", get(get_user).put(update_user).delete(delete_user))\n    .layer(axum::middleware::from_fn(auth_middleware));\n\n// Combine\nlet app = Router::new()\n    .merge(public_routes)\n    .merge(protected_routes)\n    .layer(create_trace_layer())\n    .with_state(state);\n```\n\n## Rate Limiting Middleware\n\n```rust\nuse tower_governor::{\n    governor::GovernorConfigBuilder,\n    key_extractor::SmartIpKeyExtractor,\n    GovernorLayer,\n};\n\npub fn create_rate_limiter() -> GovernorLayer<SmartIpKeyExtractor> {\n    let governor_conf = Box::new(\n        GovernorConfigBuilder::default()\n            .per_second(10)\n            .burst_size(20)\n            .finish()\n            .unwrap(),\n    );\n\n    GovernorLayer {\n        config: Box::leak(governor_conf),\n    }\n}\n\n// In router\nlet app = Router::new()\n    .route(\"/api/users\", get(list_users))\n    .layer(create_rate_limiter())\n    .with_state(state);\n```",
        "specforge-backend-rust-axum/skills/axum-patterns.md": "---\nname: axum-patterns\ndescription: Axum framework patterns for routing, extractors, and state management\nkeywords: [axum, rust, patterns, http]\n---\n\n# Axum Framework Patterns\n\nLearn Axum patterns for implementing HTTP handlers.\n\n## Router Setup\n\n```rust\nuse axum::{\n    routing::{get, post, delete, put},\n    Router,\n};\n\npub fn create_router(state: AppState) -> Router {\n    Router::new()\n        .route(\"/health\", get(health_check))\n        .route(\"/api/users\", get(list_users).post(create_user))\n        .route(\"/api/users/{id}\", get(get_user).put(update_user).delete(delete_user))\n        .with_state(state)\n}\n```\n\n## Extractors\n\nAxum uses \"extractors\" to pull data from requests:\n\n### State (Dependency Injection)\n\n```rust\nuse axum::extract::State;\n\nasync fn handler(State(state): State<AppState>) -> Result<Json<Response>, ErrorResponse> {\n    // Access shared state (database pool, config, etc.)\n    let db = &state.db;\n    // ...\n}\n```\n\n### Path Parameters\n\n```rust\nuse axum::extract::Path;\n\nasync fn get_user(\n    Path(id): Path<i64>,\n) -> Result<Json<User>, ErrorResponse> {\n    // id extracted from /users/:id\n}\n```\n\n### Query Parameters\n\n```rust\nuse axum::extract::Query;\n\nasync fn list_users(\n    Query(params): Query<ListUsersQuery>,  // ← Generated from OpenAPI\n) -> Result<Json<UserList>, ErrorResponse> {\n    // params.page, params.per_page extracted from ?page=1&per_page=20\n}\n```\n\n### JSON Body\n\n```rust\nuse axum::Json;\n\nasync fn create_user(\n    Json(payload): Json<CreateUserRequest>,  // ← Generated from OpenAPI\n) -> Result<Json<User>, ErrorResponse> {\n    // payload is automatically deserialized\n}\n```\n\n### Combining Extractors\n\n**IMPORTANT**: Order matters!\n- `State` and `Path` must come BEFORE `Json`/`Form`\n\n```rust\nasync fn update_user(\n    State(state): State<AppState>,\n    Path(id): Path<i64>,\n    Json(payload): Json<UpdateUserRequest>,\n) -> Result<Json<User>, ErrorResponse> {\n    // Correct order: State, Path, then Json\n}\n```\n\n## Response Types\n\n```rust\nuse axum::{http::StatusCode, Json};\n\n// Simple JSON response\nasync fn handler() -> Json<User> {\n    Json(user)\n}\n\n// With status code\nasync fn handler() -> (StatusCode, Json<User>) {\n    (StatusCode::CREATED, Json(user))\n}\n\n// Error handling (using generated error type)\nasync fn handler() -> Result<Json<User>, ErrorResponse> {\n    // ...\n}\n```\n\n## State Management\n\n```rust\n#[derive(Clone)]\npub struct AppState {\n    pub db: SqlitePool,\n    pub config: Config,\n}\n\n// Pass to router\nlet app = create_router(state);\n```",
        "specforge-backend-rust-axum/skills/axum-rate-limiting.md": "---\nname: axum-rate-limiting\ndescription: Implement rate limiting in Axum using tower-governor\nkeywords: [axum, rate-limiting, throttling]\n---\n\n# Rate Limiting in Axum\n\n## Setup tower-governor\n\n```toml\n# Cargo.toml\n[dependencies]\ntower-governor = \"0.1\"\n```\n\n## Rate Limiter Configuration\n\n```rust\nuse tower_governor::{\n    governor::GovernorConfigBuilder, key_extractor::SmartIpKeyExtractor, GovernorLayer,\n};\nuse std::time::Duration;\n\npub fn create_rate_limiter() -> GovernorLayer<SmartIpKeyExtractor> {\n    let governor_conf = Box::new(\n        GovernorConfigBuilder::default()\n            .per_second(10) // 10 requests per second\n            .burst_size(20) // Allow bursts of 20\n            .finish()\n            .unwrap(),\n    );\n\n    GovernorLayer {\n        config: Box::leak(governor_conf),\n    }\n}\n```\n\n## Router with Rate Limiting\n\n```rust\npub fn create_router(state: AppState) -> Router {\n    Router::new()\n        .route(\"/api/users\", get(list_users).post(create_user))\n        .layer(create_rate_limiter())\n        .with_state(state)\n}\n```\n\n## Custom Key Extractor (by User ID)\n\n```rust\nuse tower_governor::key_extractor::KeyExtractor;\n\npub struct UserIdKeyExtractor;\n\nimpl KeyExtractor for UserIdKeyExtractor {\n    type Key = String;\n\n    fn extract<T>(&self, req: &axum::http::Request<T>) -> Result<Self::Key, tower_governor::GovernorError> {\n        req.headers()\n            .get(\"x-user-id\")\n            .and_then(|v| v.to_str().ok())\n            .map(String::from)\n            .ok_or(tower_governor::GovernorError::UnableToExtractKey)\n    }\n}\n\npub fn create_user_rate_limiter() -> GovernorLayer<UserIdKeyExtractor> {\n    let governor_conf = Box::new(\n        GovernorConfigBuilder::default()\n            .per_minute(100) // 100 requests per minute per user\n            .burst_size(20)\n            .finish()\n            .unwrap(),\n    );\n\n    GovernorLayer {\n        config: Box::leak(governor_conf),\n    }\n}\n```",
        "specforge-backend-rust-axum/skills/axum-resilience.md": "---\nname: axum-resilience\ndescription: Implement retries, exponential backoff, and circuit breakers in Axum\nkeywords: [axum, retry, circuit-breaker, resilience]\n---\n\n# Resilience Patterns in Axum\n\n## Retry with Exponential Backoff\n\n```rust\nuse tokio::time::{sleep, Duration};\nuse std::future::Future;\n\npub async fn retry_with_backoff<F, Fut, T, E>(\n    mut f: F,\n    max_retries: u32,\n    initial_delay: Duration,\n) -> Result<T, E>\nwhere\n    F: FnMut() -> Fut,\n    Fut: Future<Output = Result<T, E>>,\n{\n    let mut delay = initial_delay;\n\n    for attempt in 0..max_retries {\n        match f().await {\n            Ok(result) => return Ok(result),\n            Err(e) if attempt == max_retries - 1 => return Err(e),\n            Err(_) => {\n                sleep(delay).await;\n                delay *= 2; // Exponential backoff\n            }\n        }\n    }\n\n    unreachable!()\n}\n```\n\n## Using Retry in Handler\n\n```rust\npub async fn fetch_external_data(\n    State(state): State<AppState>,\n) -> Result<Json<ExternalData>, ErrorResponse> {\n    let data = retry_with_backoff(\n        || async {\n            state\n                .http_client\n                .get(\"https://api.example.com/data\")\n                .send()\n                .await?\n                .json::<ExternalData>()\n                .await\n        },\n        3,\n        Duration::from_millis(100),\n    )\n    .await\n    .map_err(|_| ErrorResponse::internal_error())?;\n\n    Ok(Json(data))\n}\n```\n\n## Circuit Breaker with `failsafe`\n\n```toml\n# Cargo.toml\n[dependencies]\nfailsafe = \"1.2\"\n```\n\n```rust\nuse failsafe::{CircuitBreaker, Config as CircuitBreakerConfig};\nuse std::time::Duration;\n\npub fn create_circuit_breaker() -> CircuitBreaker {\n    CircuitBreaker::new(\n        CircuitBreakerConfig::default()\n            .failure_threshold_capacity(5)\n            .success_threshold_capacity(2)\n            .timeout(Duration::from_secs(60)),\n    )\n}\n\n// In AppState\n#[derive(Clone)]\npub struct AppState {\n    pub db: SqlitePool,\n    pub external_api_cb: Arc<Mutex<CircuitBreaker>>,\n}\n\n// In handler\npub async fn call_external_api(\n    State(state): State<AppState>,\n) -> Result<Json<Data>, ErrorResponse> {\n    let cb = state.external_api_cb.lock().await;\n\n    let data = cb\n        .call(|| async {\n            state\n                .http_client\n                .get(\"https://api.example.com/data\")\n                .send()\n                .await?\n                .json::<Data>()\n                .await\n        })\n        .await\n        .map_err(|_| ErrorResponse::service_unavailable(\"External service down\"))?;\n\n    Ok(Json(data))\n}\n```",
        "specforge-backend-rust-axum/skills/axum-timeouts.md": "---\nname: axum-timeouts\ndescription: Implement request and operation timeouts in Axum\nkeywords: [axum, timeout, resilience]\n---\n\n# Timeouts in Axum\n\n## Request Timeout Middleware\n\n```rust\nuse axum::{\n    body::Body,\n    extract::Request,\n    http::StatusCode,\n    middleware::Next,\n    response::{IntoResponse, Response},\n};\nuse std::time::Duration;\nuse tokio::time::timeout;\n\npub async fn timeout_middleware(\n    req: Request,\n    next: Next,\n) -> Response {\n    match timeout(Duration::from_secs(30), next.run(req)).await {\n        Ok(response) => response,\n        Err(_) => (\n            StatusCode::REQUEST_TIMEOUT,\n            \"Request timed out after 30s\",\n        )\n            .into_response(),\n    }\n}\n```\n\n## Operation-Level Timeout\n\n```rust\nuse tokio::time::{timeout, Duration};\n\npub async fn fetch_with_timeout(\n    State(state): State<AppState>,\n) -> Result<Json<Data>, ErrorResponse> {\n    let data = timeout(\n        Duration::from_secs(5),\n        async {\n            state\n                .http_client\n                .get(\"https://api.example.com/data\")\n                .send()\n                .await?\n                .json::<Data>()\n                .await\n        }\n    )\n    .await\n    .map_err(|_| ErrorResponse::gateway_timeout(\"External API timeout\"))?\n    .map_err(|_| ErrorResponse::internal_error())?;\n\n    Ok(Json(data))\n}\n```\n\n## Database Query Timeout\n\n```rust\nuse sqlx::query_builder::Separated;\n\npub async fn get_user_with_timeout(\n    State(state): State<AppState>,\n    Path(id): Path<i64>,\n) -> Result<Json<User>, ErrorResponse> {\n    let user = timeout(\n        Duration::from_secs(10),\n        get_user_by_id(&state.db, id)\n    )\n    .await\n    .map_err(|_| ErrorResponse::gateway_timeout(\"Database query timeout\"))?\n    .map_err(|e| ErrorResponse::from(e))?\n    .ok_or_else(|| ErrorResponse::not_found(\"User not found\"))?;\n\n    Ok(Json(user))\n}\n```\n\n## Router with Timeout\n\n```rust\npub fn create_router(state: AppState) -> Router {\n    Router::new()\n        .route(\"/api/users\", get(list_users))\n        .layer(axum::middleware::from_fn(timeout_middleware))\n        .with_state(state)\n}\n```",
        "specforge-backend-rust-axum/skills/axum-tracing.md": "---\nname: axum-tracing\ndescription: Implement request tracing with correlation IDs in Axum\nkeywords: [axum, tracing, observability, correlation-id]\n---\n\n# Request Tracing in Axum\n\n## Correlation ID Middleware\n\n```rust\nuse axum::{\n    body::Body,\n    extract::Request,\n    http::HeaderValue,\n    middleware::Next,\n    response::Response,\n};\nuse uuid::Uuid;\n\npub static X_CORRELATION_ID: &str = \"x-correlation-id\";\n\npub async fn correlation_id_middleware(\n    mut req: Request,\n    next: Next,\n) -> Response {\n    let correlation_id = req\n        .headers()\n        .get(X_CORRELATION_ID)\n        .and_then(|v| v.to_str().ok())\n        .map(String::from)\n        .unwrap_or_else(|| Uuid::new_v4().to_string());\n\n    req.extensions_mut().insert(correlation_id.clone());\n\n    let mut response = next.run(req).await;\n    response.headers_mut().insert(\n        X_CORRELATION_ID,\n        HeaderValue::from_str(&correlation_id).unwrap(),\n    );\n\n    response\n}\n```\n\n## Correlation ID Extractor\n\n```rust\nuse axum::{\n    extract::FromRequestParts,\n    http::request::Parts,\n};\n\npub struct CorrelationId(pub String);\n\nimpl<S> FromRequestParts<S> for CorrelationId\nwhere\n    S: Send + Sync,\n{\n    type Rejection = (StatusCode, String);\n\n    async fn from_request_parts(parts: &mut Parts, _state: &S) -> Result<Self, Self::Rejection> {\n        parts\n            .extensions\n            .get::<String>()\n            .cloned()\n            .map(CorrelationId)\n            .ok_or_else(|| {\n                (\n                    StatusCode::INTERNAL_SERVER_ERROR,\n                    \"Missing correlation ID\".to_string(),\n                )\n            })\n    }\n}\n```\n\n## Using Correlation ID in Handlers\n\n```rust\npub async fn create_user_with_tracing(\n    State(state): State<AppState>,\n    CorrelationId(correlation_id): CorrelationId,\n    Json(payload): Json<CreateUserRequest>,\n) -> Result<(StatusCode, Json<User>), ErrorResponse> {\n    tracing::info!(\n        correlation_id = %correlation_id,\n        email = %payload.email,\n        \"Creating user\"\n    );\n\n    let user = create_user(&state.db, &payload.email, &payload.name).await?;\n\n    tracing::info!(\n        correlation_id = %correlation_id,\n        user_id = %user.id,\n        \"User created successfully\"\n    );\n\n    Ok((StatusCode::CREATED, Json(user)))\n}\n```\n\n## Router with Middleware\n\n```rust\nuse tower_http::trace::{TraceLayer, DefaultMakeSpan, DefaultOnResponse};\n\npub fn create_router(state: AppState) -> Router {\n    Router::new()\n        .route(\"/api/users\", post(create_user_with_tracing))\n        .layer(axum::middleware::from_fn(correlation_id_middleware))\n        .layer(\n            TraceLayer::new_for_http()\n                .make_span_with(DefaultMakeSpan::new().include_headers(true))\n                .on_response(DefaultOnResponse::new().include_headers(true))\n        )\n        .with_state(state)\n}\n```",
        "specforge-backend-rust-axum/skills/rust-dev-setup.md": "---\nname: rust-dev-setup\ndescription: Docker Compose setup for Rust + Axum backend\nkeywords: [rust, docker, setup, axum]\n---\n\n# Rust + Axum Development Setup\n\n## Docker Compose Service\n\n```yaml\nservices:\n  api:\n    build:\n      context: ./backend\n      dockerfile: Dockerfile\n    container_name: ${PROJECT_NAME:-app}-api\n    environment:\n      DATABASE_URL: ${DATABASE_URL}\n      RUST_LOG: ${RUST_LOG:-info}\n      PORT: 3000\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      postgres:\n        condition: service_healthy\n    volumes:\n      - ./backend:/app\n      - cargo_cache:/usr/local/cargo/registry\n\nvolumes:\n  cargo_cache:\n```\n\n## Dockerfile\n\n```dockerfile\nFROM rust:1.75-alpine AS builder\nWORKDIR /app\nRUN apk add --no-cache musl-dev openssl-dev\nCOPY Cargo.toml Cargo.lock ./\nCOPY src ./src\nRUN cargo build --release\n\nFROM alpine:3.18\nWORKDIR /app\nRUN apk add --no-cache libgcc\nCOPY --from=builder /app/target/release/app /app/app\nUSER 1000\nEXPOSE 3000\nCMD [\"./app\"]\n```\n\n## Environment Variables\n\n```bash\n# .env\nDATABASE_URL=postgresql://postgres:postgres@postgres:5432/app_dev\nRUST_LOG=info,axum=debug\nPORT=3000\n```",
        "specforge-backend-rust-axum/skills/rust-openapi-integration.md": "---\nname: rust-openapi-integration\ndescription: Using OpenAPI-generated Rust types with openapi-generator\nkeywords: [rust, openapi, code-generation]\n---\n\n# Rust OpenAPI Integration\n\n## OpenAPI Code Generation Tools\n\n### `openapi-generator`\n\n```bash\nopenapi-generator generate \\\n  -i spec/openapi.yaml \\\n  -g rust \\\n  -o backend/src/generated/api \\\n  --additional-properties=packageName=api\n```\n\n## Generated Structure\n\n```\nsrc/generated/api/\n├── mod.rs\n├── models.rs        # Request/response types\n└── errors.rs        # Error schemas\n```\n\n## Using Generated Types\n\n```rust\n// Import generated types\nuse crate::generated::api::{\n    CreateUserRequest,\n    User,\n    UserList,\n    ListUsersQuery,\n    ErrorResponse,\n};\n\n// Use in handler\npub async fn create_user(\n    Json(payload): Json<CreateUserRequest>,\n) -> Result<Json<User>, ErrorResponse> {\n    // payload has all fields from OpenAPI schema\n    // payload.email: String\n    // payload.name: Option<String>\n}\n```\n\n## Type Mapping\n\nOpenAPI → Rust:\n\n| OpenAPI Type | Rust Type |\n|--------------|-----------|\n| `string` | `String` |\n| `integer` | `i64` or `i32` |\n| `number` | `f64` |\n| `boolean` | `bool` |\n| `array` | `Vec<T>` |\n| `object` | `struct` |\n| nullable (`null` allowed) | `Option<T>` |\n\n## Validation\n\nGenerated types include serde validation:\n\n```rust\n#[derive(Deserialize)]\npub struct CreateUserRequest {\n    #[serde(validate = \"email\")]\n    pub email: String,\n\n    #[serde(validate = \"length(min = 2, max = 100)\")]\n    pub name: String,\n}\n```",
        "specforge-backend-rust-axum/skills/rust-testing.md": "---\nname: rust-testing\ndescription: Testing patterns for Axum handlers using generated types\nkeywords: [rust, testing, axum, integration-tests]\n---\n\n# Rust Testing for Axum Handlers\n\nTesting patterns using OpenAPI-generated and DB-generated types.\n\n## Test Setup\n\n```rust\n// tests/common/mod.rs\nuse sqlx::SqlitePool;\n\npub async fn setup_test_db() -> SqlitePool {\n    let pool = SqlitePool::connect(\":memory:\").await.unwrap();\n\n    // Run migrations\n    sqlx::migrate!(\"./migrations\")\n        .run(&pool)\n        .await\n        .unwrap();\n\n    pool\n}\n\npub fn setup_test_state(pool: SqlitePool) -> AppState {\n    AppState { db: pool }\n}\n```\n\n## Unit Testing Handlers\n\n```rust\n// tests/users_test.rs\nuse axum::{\n    body::Body,\n    http::{Request, StatusCode},\n};\nuse tower::ServiceExt;  // for `oneshot`\n\nuse crate::generated::api::{CreateUserRequest, User};\n\nmod common;\n\n#[tokio::test]\nasync fn test_create_user_success() {\n    // Setup\n    let db = common::setup_test_db().await;\n    let state = common::setup_test_state(db);\n    let app = create_router(state);\n\n    // Create request\n    let payload = CreateUserRequest {\n        email: \"test@example.com\".to_string(),\n        name: Some(\"Test User\".to_string()),\n    };\n\n    let request = Request::builder()\n        .uri(\"/api/users\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(Body::from(serde_json::to_string(&payload).unwrap()))\n        .unwrap();\n\n    // Execute\n    let response = app.oneshot(request).await.unwrap();\n\n    // Assert\n    assert_eq!(response.status(), StatusCode::CREATED);\n\n    let body = hyper::body::to_bytes(response.into_body()).await.unwrap();\n    let user: User = serde_json::from_slice(&body).unwrap();\n\n    assert_eq!(user.email, \"test@example.com\");\n    assert_eq!(user.name.as_deref(), Some(\"Test User\"));\n}\n\n#[tokio::test]\nasync fn test_create_user_duplicate_email() {\n    // Setup\n    let db = common::setup_test_db().await;\n    let state = common::setup_test_state(db.clone());\n    let app = create_router(state);\n\n    // Create first user\n    let payload = CreateUserRequest {\n        email: \"test@example.com\".to_string(),\n        name: Some(\"Test User\".to_string()),\n    };\n\n    create_user(&db, &payload.email, payload.name.as_deref()).await.unwrap();\n\n    // Try to create duplicate\n    let request = Request::builder()\n        .uri(\"/api/users\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(Body::from(serde_json::to_string(&payload).unwrap()))\n        .unwrap();\n\n    let response = app.oneshot(request).await.unwrap();\n\n    // Assert\n    assert_eq!(response.status(), StatusCode::CONFLICT);\n}\n```\n\n## Testing with Test Fixtures\n\n```rust\n// tests/fixtures.rs\nuse crate::generated::db::create_user;\n\npub async fn create_test_user(db: &SqlitePool) -> User {\n    create_user(db, \"test@example.com\", Some(\"Test User\"))\n        .await\n        .unwrap()\n}\n\npub async fn create_test_users(db: &SqlitePool, count: usize) -> Vec<User> {\n    let mut users = Vec::new();\n    for i in 0..count {\n        let user = create_user(\n            db,\n            &format!(\"user{}@example.com\", i),\n            Some(&format!(\"User {}\", i))\n        )\n        .await\n        .unwrap();\n        users.push(user);\n    }\n    users\n}\n\n// Use in tests\n#[tokio::test]\nasync fn test_get_user() {\n    let db = common::setup_test_db().await;\n    let user = create_test_user(&db).await;\n\n    // Test GET /api/users/:id\n    // ...\n}\n```\n\n## Testing Error Cases\n\n```rust\n#[tokio::test]\nasync fn test_get_nonexistent_user() {\n    let db = common::setup_test_db().await;\n    let state = common::setup_test_state(db);\n    let app = create_router(state);\n\n    let request = Request::builder()\n        .uri(\"/api/users/99999\")\n        .method(\"GET\")\n        .body(Body::empty())\n        .unwrap();\n\n    let response = app.oneshot(request).await.unwrap();\n\n    assert_eq!(response.status(), StatusCode::NOT_FOUND);\n\n    let body = hyper::body::to_bytes(response.into_body()).await.unwrap();\n    let error: ErrorResponse = serde_json::from_slice(&body).unwrap();\n\n    assert_eq!(error.code, \"NOT_FOUND\");\n}\n```\n\n## Integration Testing\n\n```rust\n// tests/integration_test.rs\n#[tokio::test]\nasync fn test_user_crud_workflow() {\n    let db = common::setup_test_db().await;\n    let state = common::setup_test_state(db);\n    let app = create_router(state);\n\n    // 1. Create user\n    let create_payload = CreateUserRequest {\n        email: \"workflow@example.com\".to_string(),\n        name: Some(\"Workflow User\".to_string()),\n    };\n\n    let response = app\n        .clone()\n        .oneshot(create_user_request(&create_payload))\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::CREATED);\n    let user: User = parse_response_body(response).await;\n\n    // 2. Get user\n    let response = app\n        .clone()\n        .oneshot(get_user_request(user.id))\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::OK);\n\n    // 3. Update user\n    let update_payload = UpdateUserRequest {\n        name: Some(\"Updated Name\".to_string()),\n    };\n\n    let response = app\n        .clone()\n        .oneshot(update_user_request(user.id, &update_payload))\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::OK);\n\n    // 4. Delete user\n    let response = app\n        .oneshot(delete_user_request(user.id))\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n}\n```",
        "specforge/.claude-plugin/plugin.json": "{\n  \"name\": \"specforge\",\n  \"version\": \"0.1.2\",\n  \"description\": \"Schema-first development ecosystem with dual-spec workflows (OpenAPI + DB schema) for building production-ready applications through deterministic code generation and intelligent orchestration\",\n  \"author\": {\n    \"name\": \"Daniel Emod Kovacs\",\n    \"url\": \"https://github.com/danielkov\"\n  },\n  \"repository\": \"https://github.com/claude-market/marketplace/tree/main/specforge\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"specforge\",\n    \"openapi\",\n    \"schema-first\",\n    \"code-generation\",\n    \"orchestration\",\n    \"api-development\",\n    \"database-schema\",\n    \"type-safety\"\n  ],\n  \"commands\": [\n    \"./commands/init.md\",\n    \"./commands/plan.md\",\n    \"./commands/build.md\",\n    \"./commands/validate.md\",\n    \"./commands/test.md\",\n    \"./commands/ship.md\",\n    \"./commands/sync.md\"\n  ],\n  \"agents\": [\n    \"./agents/captain-orchestrator.md\",\n    \"./agents/planning-agent.md\",\n    \"./agents/validation-agent.md\"\n  ],\n  \"skills\": [\n    \"./skills/openapi-expert.md\",\n    \"./skills/stack-advisor.md\",\n    \"./skills/integration-expert.md\"\n  ]\n}\n",
        "specforge/README.md": "# SpecForge\n\n**Schema-first development ecosystem with dual-spec workflows (OpenAPI + DB schema) for building production-ready applications through deterministic code generation and intelligent orchestration.**\n\n## Overview\n\nSpecForge is a multi-plugin ecosystem for Claude Code that implements complete **schema-first, DB-first** development workflows. The core plugin provides orchestration, planning, and validation, while modular plugins handle database tooling, code generation pipelines, and framework patterns.\n\n### Core Innovation\n\nDual-spec approach where both OpenAPI and database schemas drive code generation. Separate codegen pipeline plugins bridge the gap between backend frameworks and databases, ensuring type-safe, compile-time verified database operations.\n\n## Installation\n\n```bash\n# Install core plugin\n/plugin marketplace add claude-market/marketplace\n/plugin install specforge\n\n# Install stack plugins (THREE required: backend, database, codegen)\n/plugin install specforge-backend-rust-axum\n/plugin install specforge-db-sqlite\n/plugin install specforge-generate-rust-sql\n\n# Optional: frontend plugin\n/plugin install specforge-frontend-react-tanstack\n```\n\n## Commands\n\n### `/specforge:init`\n\nInitialize a SpecForge project with tech stack selection.\n\n**What it does:**\n\n- Discovers available stack plugins from the marketplace\n- Presents interactive stack selection (backend, database, codegen, frontend)\n- Generates project structure\n- Creates initial OpenAPI spec template\n- Configures Docker Compose\n- Updates CLAUDE.md with selected stack\n\n**Usage:**\n\n```bash\n/specforge:init\n```\n\n**Output:**\n\n- `spec/openapi.yaml` - OpenAPI specification\n- `backend/` - Backend code directory\n- `frontend/` - Frontend code directory (if selected)\n- `docker/docker-compose.yml` - Docker configuration\n- `CLAUDE.md` - Updated with SpecForge configuration\n\n### `/specforge:plan`\n\nGenerate implementation plan with dual-spec changes (OpenAPI + DB schema).\n\n**What it does:**\n\n- Analyzes feature requirements\n- Proposes coordinated changes to OpenAPI spec and database schema\n- Identifies required code generation\n- Plans handler implementation strategy\n- Estimates complexity and suggests appropriate agents\n\n**Usage:**\n\n```bash\n/specforge:plan\n```\n\n**Example:**\n\n```bash\n# After editing spec/openapi.yaml to add new endpoints\n/specforge:plan\n```\n\n**Output:**\n\n- Implementation plan with spec changes\n- Database migration files (proposed)\n- Code generation requirements\n- Handler implementation strategy\n- Testing plan\n\n### `/specforge:build`\n\nApply specs, generate code, implement handlers with test-driven iteration.\n\n**What it does:**\n\n- Applies database migrations\n- Validates OpenAPI spec\n- Runs code generation pipeline (DB schema → types)\n- Generates OpenAPI types\n- Implements endpoint handlers in parallel\n- Runs test-driven iteration until all tests pass\n- Wires handlers to router\n\n**Usage:**\n\n```bash\n/specforge:build\n```\n\n**Orchestration Flow:**\n\n1. **Phase 1**: Apply spec changes (migrations, OpenAPI validation)\n2. **Phase 2**: Code generation (DB types, API types, frontend client)\n3. **Phase 3**: Handler implementation (parallel execution)\n4. **Phase 4**: Test & iterate (behavior observation, diagnostics)\n5. **Phase 5**: Integration & verification\n\n### `/specforge:sync`\n\nSynchronize codebase after manual spec changes (OpenAPI + DB schema).\n\n**What it does:**\n\n- Detects changes to OpenAPI spec and database migrations\n- Validates updated specifications\n- Applies new database migrations (with confirmation)\n- Regenerates type-safe code from updated schemas\n- Identifies affected handlers that need updates\n- Compiles and tests to verify everything still works\n- Reports what changed and what needs attention\n\n**When to use:**\n\n- After manually editing `spec/openapi.yaml`\n- After pulling changes from version control\n- After adding/modifying database migrations\n- After resolving merge conflicts in specs\n\n**Usage:**\n\n```bash\n# Sync after spec changes\n/specforge:sync\n\n# Dry run (show what would happen)\n/specforge:sync --dry-run\n```\n\n**Example Workflow:**\n\n```bash\n# Teammate added new endpoints\ngit pull origin main\n\n# Sync to regenerate code\n/specforge:sync\n\n# Output shows:\n# - 1 migration applied\n# - Database models regenerated\n# - API types regenerated\n# - 2 handlers need updates\n#\n# Next: /specforge:build (to update affected handlers)\n```\n\n**Sync vs Build:**\n\n- **`/specforge:sync`**: Regenerates code from externally changed specs\n- **`/specforge:build`**: Applies specs + implements handlers in one go\n\n**Output:**\n\n- List of detected changes\n- Migration application results\n- Regenerated code files\n- Affected handlers report\n- Test results summary\n- Suggested next steps\n\n### `/specforge:validate`\n\nMulti-level validation (spec, schema, code, runtime) for SpecForge projects.\n\n**What it does:**\n\n- **Level 1**: OpenAPI spec validation (compliance, schema correctness)\n- **Level 2**: Database schema validation (migrations, constraints)\n- **Level 3**: Code generation validation (types, queries generated correctly)\n- **Level 4**: Compilation validation (no type errors, linting)\n- **Level 5**: Runtime validation (tests pass, contract testing)\n\n**Usage:**\n\n```bash\n# Quick validation (spec, schema, compile)\n/specforge:validate\n\n# Full validation including runtime tests\n/specforge:validate --full\n\n# Specific level validation\n/specforge:validate --level spec\n/specforge:validate --level schema\n/specforge:validate --level codegen\n/specforge:validate --level compile\n/specforge:validate --level runtime\n```\n\n**Output:**\n\n- Validation report with pass/fail status for each level\n- Detailed error messages for failures\n- Recommendations for fixes\n- Exit codes for CI/CD integration\n\n**Validation Levels:**\n\n1. **Spec Validation**: OpenAPI 3.x compliance, schema definitions, examples\n2. **Schema Validation**: Migration consistency, constraints, indexes\n3. **Codegen Validation**: Generated code exists, types match schemas\n4. **Compile Validation**: No compilation errors, linting passes\n5. **Runtime Validation**: Tests pass, API matches spec\n\n### `/specforge:test`\n\nTest orchestration with behavior observation and iterative fixing.\n\n**What it does:**\n\n- Runs unit tests with coverage reporting\n- Runs integration tests (end-to-end API flows)\n- Performs contract testing against OpenAPI spec\n- Observes test execution behavior to identify patterns\n- Diagnoses failures using specialized agents\n- Iteratively fixes issues (max 3 iterations)\n- Generates comprehensive test reports\n\n**Usage:**\n\n```bash\n# Quick test (unit + integration)\n/specforge:test\n\n# Full test including contract tests\n/specforge:test --full\n\n# Specific test suite\n/specforge:test --suite unit\n/specforge:test --suite integration\n/specforge:test --suite contract\n\n# Watch mode (run on file changes)\n/specforge:test --watch\n```\n\n**Test Levels:**\n\n1. **Unit Tests**: Test individual handlers and functions\n2. **Integration Tests**: Test API endpoints end-to-end with database\n3. **Contract Tests**: Verify API responses match OpenAPI spec using Schemathesis\n4. **Behavior Observation**: Monitor execution to identify failure patterns\n5. **Iterative Fixing**: Auto-diagnose and fix common issues\n\n**Test Report Includes:**\n\n- Pass/fail summary for each test suite\n- Code coverage metrics (line, branch, function)\n- Performance metrics (response times)\n- Detailed failure diagnostics\n- Iteration history (what was fixed and how)\n- Recommendations for additional tests\n\n**Example Output:**\n\n```\nSpecForge Test Report\n\nSummary:\n- ✓ Unit Tests: 45/45 passed (100%)\n- ✓ Integration Tests: 12/12 passed (100%)\n- ✓ Contract Tests: 8/8 passed (100%)\nTotal: 65/65 tests passed\n\nCoverage:\n- Line Coverage: 87%\n- Branch Coverage: 82%\n- Function Coverage: 95%\n\nIterations: 2\n- Iteration 1: Fixed 3 type mismatches\n- Iteration 2: All tests passed ✓\n```\n\n### `/specforge:ship`\n\nDeployment preparation.\n\n**What it does:**\n\n- Validates project is production-ready\n- Builds Docker images\n- Runs security scans\n- Generates deployment artifacts\n- Creates deployment documentation\n\n**Usage:**\n\n```bash\n/specforge:ship\n```\n\n**Coming soon:** Full implementation based on SPECFORGE_PLAN.md\n\n## Workflow\n\n### 1. Initialize Project\n\n```bash\n/specforge:init\n```\n\nSelect your tech stack (backend, database, codegen, frontend).\n\n### 2. Design API\n\nEdit `spec/openapi.yaml` to define your API endpoints, schemas, and business logic.\n\n### 3. Plan Implementation\n\n```bash\n/specforge:plan\n```\n\nReview the proposed implementation plan including database schema changes.\n\n### 4. Build Application\n\n```bash\n/specforge:build\n```\n\nSpecForge orchestrates code generation and handler implementation.\n\n### 5. Validate Everything\n\n```bash\n/specforge:validate\n```\n\nRun multi-level validation to ensure correctness.\n\n### 6. Test\n\n```bash\n/specforge:test\n```\n\nRun comprehensive tests with behavior observation.\n\n### 7. Deploy\n\n```bash\n/specforge:ship\n```\n\nPrepare for production deployment.\n\n## Architecture\n\n### Multi-Plugin Ecosystem\n\n```\n┌──────────────────────────────────────────────────────────┐\n│                   specforge (core)                        │\n│  Orchestration, Planning, Validation, Dual-Spec Workflow │\n└────────────────────┬─────────────────────────────────────┘\n                     │\n         ┌───────────┼───────────┬──────────────┐\n         ▼           ▼           ▼              ▼\n┌─────────────┐ ┌─────────────┐ ┌──────────┐ ┌─────────────┐\n│  Backend    │ │  Database   │ │ Codegen  │ │  Frontend   │\n│  Plugins    │ │  Plugins    │ │ Pipeline │ │  Plugins    │\n│ (patterns)  │ │ (tooling)   │ │ Plugins  │ │             │\n└─────────────┘ └─────────────┘ └──────────┘ └─────────────┘\n```\n\n### Plugin Categories\n\n1. **Backend Plugins** (`specforge-backend-{tech}-{framework}`)\n\n   - Framework-specific patterns, handlers, middleware, error handling\n   - Examples: `specforge-backend-rust-axum`, `specforge-backend-node-express`\n\n2. **Database Plugins** (`specforge-db-{database}`)\n\n   - Database tooling, migrations, health checks, Docker config\n   - Examples: `specforge-db-postgresql`, `specforge-db-sqlite`\n\n3. **Codegen Pipeline Plugins** (`specforge-generate-{tech}-{database}`)\n\n   - Type-safe DB access code generation from schemas\n   - Examples: `specforge-generate-rust-sql`, `specforge-generate-ts-prisma`\n\n4. **Frontend Plugins** (`specforge-frontend-{framework}-{variant}`)\n   - Frontend client generation, state management, components\n   - Examples: `specforge-frontend-react-tanstack`, `specforge-frontend-vue-pinia`\n\n## Agents\n\n### `captain-orchestrator` (Sonnet)\n\nMain coordination agent that:\n\n- Reads project configuration\n- Delegates to specialized agents\n- Manages parallel execution\n- Tracks overall state\n\n### `planning-agent` (Sonnet)\n\nStrategic planning agent that:\n\n- Analyzes feature requirements\n- Proposes dual-spec changes\n- Identifies implementation strategy\n- Estimates complexity\n\n### `validation-agent` (Sonnet)\n\nDeep debugging agent that:\n\n- Diagnoses validation failures\n- Analyzes error patterns\n- Proposes fixes\n- Verifies corrections\n\n## Skills\n\n### `openapi-expert`\n\nOpenAPI specification best practices:\n\n- Spec design patterns\n- Validation rules\n- Contract testing\n- Documentation generation\n\n### `stack-advisor`\n\nTech stack recommendations:\n\n- Plugin compatibility\n- Performance considerations\n- Best practices for stack combinations\n- Migration strategies\n\n### `integration-expert`\n\nPlugin composition strategies:\n\n- How plugins work together\n- Context optimization\n- Parallel execution patterns\n- State management\n\n## Benefits\n\n### 1. Type Safety Across the Stack\n\n```\nDatabase Schema (SQL)\n    ↓ [sql-gen / Prisma / sqlc]\nGenerated Types & Queries\n    ↓\nBusiness Logic Handlers\n    ↓\nOpenAPI-Generated Types\n    ↓\nAPI Responses\n```\n\nCompile-time guarantees from database to API response.\n\n### 2. Clear Separation of Concerns\n\n- **Backend Plugin**: Framework patterns, business logic, HTTP handling\n- **Database Plugin**: Database tooling, migrations, configuration\n- **Codegen Plugin**: Bridge between DB and backend with type safety\n\n### 3. Interoperability\n\nMix any backend + database + codegen plugin:\n\n- `rust-axum` + `postgresql` + `rust-sql` ✓\n- `node-express` + `sqlite` + `ts-prisma` ✓\n- `go-gin` + `mysql` + `go-sqlc` ✓\n\n### 4. Schema-First Approach\n\n- Single source of truth (database schema)\n- Verified at compile-time (no runtime SQL errors)\n- Refactoring safety (schema changes propagate)\n- Migration-driven (explicit, versioned evolution)\n\n## Configuration\n\nSpecForge configuration is stored in your project's `CLAUDE.md`:\n\n```markdown\n## SpecForge configuration (EDIT ONLY IF YOU ARE SPECFORGE AGENT)\n\n- backend: rust-axum\n- database: sqlite\n- codegen: rust-sql\n- frontend: react-tanstack (optional)\n```\n\n## Resources\n\n- [OpenAPI Specification](https://spec.openapis.org/oas/latest.html)\n- [OpenAPI Best Practices](https://learn.openapis.org/best-practices.html)\n- [SPECFORGE_PLAN.md](../../SPECFORGE_PLAN.md) - Full architectural specification\n\n## Support\n\n- **Issues**: Report bugs or request features at [GitHub Issues](https://github.com/claude-market/marketplace/issues)\n- **Discussions**: Join the community discussion\n\n## License\n\nMIT License - see [LICENSE](./LICENSE) file for details\n",
        "specforge/agents/captain-orchestrator.md": "---\nname: captain-orchestrator\ndescription: Main orchestration agent for SpecForge workflows. Coordinates dual-spec planning (OpenAPI + DB schema), delegates to specialized agents, manages build phases, and oversees test-driven iteration across backend, database, and codegen plugins.\nmodel: sonnet\n---\n\n# SpecForge Captain Orchestrator\n\nYou are the Captain Orchestrator for SpecForge - the main coordination agent responsible for orchestrating the entire schema-first development workflow. You coordinate dual-spec workflows (OpenAPI + Database schemas), delegate to specialized planning agents, backend handler agents, database agents, and codegen pipeline agents.\n\n## Your Role\n\nYou are the strategic coordinator and decision-maker for the SpecForge ecosystem. You:\n\n- **Orchestrate multi-phase workflows** (planning, building, validation, testing, shipping)\n- **Delegate to specialized agents** based on their expertise and context requirements\n- **Manage plugin discovery and composition** across backend, database, and codegen plugins\n- **Coordinate parallel execution** of multiple agents (up to 5 concurrent tasks)\n- **Monitor and track progress** through complex multi-step workflows\n- **Handle failures gracefully** with retry logic and diagnostics escalation\n\n## Core Responsibilities\n\n### 1. Workflow Orchestration\n\nCoordinate the complete SpecForge development lifecycle:\n\n**Phase 1: Planning**\n\n- Parse user requirements for features\n- Delegate to planning-agent for dual-spec analysis\n- Review and approve OpenAPI + DB schema change proposals\n- Estimate complexity and select appropriate models for implementation\n\n**Phase 2: Building**\n\n- Apply database migrations via database plugin agents\n- Run codegen pipeline to generate type-safe DB access code\n- Generate OpenAPI types for request/response models\n- Spawn backend handler agents in parallel (max 5 concurrent)\n- Coordinate frontend client generation if applicable\n\n**Phase 3: Testing & Iteration**\n\n- Delegate to test agents for each implemented handler\n- Observe behavior (compile errors, runtime errors, test failures)\n- Invoke diagnostics agents from codegen plugins to fix issues\n- Retry up to 3 times per handler before escalating\n- Track success/failure of each endpoint implementation\n\n**Phase 4: Validation**\n\n- Delegate to validation-agent for deep debugging\n- Run integration tests across full stack\n- Verify spec compliance (OpenAPI + DB schema)\n- Check type safety and compilation\n\n**Phase 5: Deployment**\n\n- Prepare Docker configurations\n- Generate deployment artifacts\n- Coordinate with infrastructure plugins\n\n### 2. Plugin Discovery & Composition\n\nUnderstand and coordinate the three-plugin architecture:\n\n**Required Plugins:**\n\n1. **Backend Plugin** (`specforge-backend-{tech}-{framework}`)\n\n   - Provides: Framework patterns, handler implementation\n   - Example: `specforge-backend-rust-axum`\n\n2. **Database Plugin** (`specforge-db-{database}`)\n\n   - Provides: Database tooling, migrations, Docker config\n   - Example: `specforge-db-sqlite`\n\n3. **Codegen Pipeline Plugin** (`specforge-generate-{tech}-{db}`)\n   - Provides: Type-safe DB access code generation\n   - Example: `specforge-generate-rust-sql`\n\n**Optional Plugins:** 4. **Frontend Plugin** (`specforge-frontend-{framework}-{variant}`)\n\n- Provides: Frontend client generation and components\n- Example: `specforge-frontend-react-tanstack`\n\n**Discovery Process:**\n\n- Read project's `CLAUDE.md` for configured tech stack\n- Validate all required plugins are installed\n- Install missing plugins if needed (with user confirmation)\n\n### 3. Parallel Agent Execution\n\nMaximize efficiency by running independent tasks in parallel:\n\n**Strategy:**\n\n```javascript\n// Example: Parallel handler implementation\nconst endpoints = parseEndpointsFromOpenAPI(spec);\nconst tasks = endpoints.map((endpoint) => ({\n  agent: `${backendPlugin}/handler-agent`,\n  model: endpoint.complexity === \"simple\" ? \"haiku\" : \"sonnet\",\n  context: prepareMinimalContext(endpoint),\n}));\n\n// Run up to 5 concurrent agent tasks\nawait executeParallel(tasks, { maxConcurrent: 5 });\n```\n\n**Context Budget Management:**\n\n- Main orchestrator: 15K tokens max\n- Planning agent: 10K tokens\n- Handler agents: 5K tokens each (Haiku) or 15K (Sonnet)\n- Total budget target: <100K tokens for 20-endpoint API\n\n### 4. State Management\n\nMaintain workflow state for tracking and recovery:\n\n**State File:** `.specforge/state.json`\n\n```json\n{\n  \"phase\": \"building\",\n  \"endpoints\": {\n    \"POST /users\": {\n      \"status\": \"completed\",\n      \"agent\": \"rust-handler-agent\",\n      \"tests\": \"passed\",\n      \"iterations\": 1\n    },\n    \"GET /users/:id\": {\n      \"status\": \"in_progress\",\n      \"agent\": \"rust-handler-agent\",\n      \"iterations\": 2\n    }\n  },\n  \"plugins\": {\n    \"backend\": \"specforge-backend-rust-axum\",\n    \"database\": \"specforge-db-sqlite\",\n    \"codegen\": \"specforge-generate-rust-sql\"\n  }\n}\n```\n\n### 5. Delegation Strategy\n\nChoose the right agent for each task:\n\n**Planning Tasks** → `planning-agent` (Sonnet)\n\n- Complex strategic decisions\n- Dual-spec coordination\n- Feature decomposition\n\n**Backend Handlers** → `{backend-plugin}/handler-agent` (Haiku or Sonnet)\n\n- Simple CRUD → Haiku\n- Complex business logic → Sonnet\n- Parallel execution when independent\n\n**Database Migrations** → `{database-plugin}/migration-agent` (Haiku)\n\n- Schema changes\n- Migration generation\n- Sequential execution\n\n**Code Generation** → `{codegen-plugin}/codegen-agent` (Haiku)\n\n- Run codegen tools (sql-gen, Prisma, sqlc)\n- Generate type-safe DB access code\n\n**Testing** → `{backend-plugin}/test-agent` (Haiku)\n\n- Generate unit tests\n- Run test suites\n- Report coverage\n\n**Diagnostics** → `{codegen-plugin}/diagnostics-agent` (Sonnet)\n\n- Fix compilation errors\n- Resolve type mismatches\n- Debug codegen issues\n\n**Deep Debugging** → `validation-agent` (Sonnet)\n\n- Complex runtime errors\n- Integration failures\n- Spec compliance issues\n\n## Workflow Examples\n\n### Example 1: /specforge:build Orchestration\n\n```markdown\n## Phase 1: Apply Spec Changes\n\n✓ Applied database migrations (3 new tables)\n✓ Validated OpenAPI spec changes\n\n## Phase 2: Code Generation\n\n✓ Generated type-safe DB models (User, Order, OrderItem)\n✓ Generated DB query functions (get_user_by_id, create_order)\n✓ Generated OpenAPI request/response types\n\n## Phase 3: Handler Implementation (5 endpoints, 3 parallel)\n\n→ Spawning handler agents:\n\n- POST /users (rust-handler-agent, Haiku) ✓ Complete\n- GET /users/:id (rust-handler-agent, Haiku) ✓ Complete\n- POST /orders (rust-handler-agent, Sonnet) → In progress...\n- GET /users/:id/orders (rust-handler-agent, Haiku) ✓ Complete\n- DELETE /orders/:id (rust-handler-agent, Haiku) ✓ Complete\n\n## Phase 4: Test & Iterate\n\n→ Testing POST /users\n✓ Unit tests passed (3/3)\n✓ Integration test passed\n\n→ Testing POST /orders\n✗ Compilation error: Type mismatch in order_items field\n→ Invoking rust-sql-diagnostics-agent...\n✓ Fixed: Updated generated query signature\n✓ Tests passed (iteration 2/3)\n\n## Summary\n\n✓ 5 endpoints implemented\n✓ 15 unit tests generated and passing\n✓ 5 integration tests passing\n✓ Type safety verified at compile-time\n✓ Ready for deployment\n```\n\n### Example 2: Error Recovery\n\n```markdown\n→ Handler implementation failed: POST /orders (iteration 3/3)\n\nERROR: rust-handler-agent encountered persistent compilation errors\n\n- Type mismatch between generated Order struct and handler signature\n- Diagnostics agent could not resolve after 3 iterations\n\nESCALATION DECISION:\n→ Delegating to validation-agent (Sonnet) for deep investigation\n→ Providing full context: OpenAPI spec, DB schema, generated code, error logs\n\n[validation-agent analysis reveals schema/spec mismatch]\n[validation-agent proposes DB migration fix]\n[Re-running codegen pipeline...]\n\n✓ Issue resolved: Added missing foreign key constraint\n✓ Codegen regenerated with correct types\n✓ Handler implementation succeeded\n```\n\n## Constraints\n\n- **DO NOT** modify code directly - always delegate to appropriate specialized agents\n- **DO NOT** run more than 5 concurrent agent tasks (context budget limit)\n- **DO NOT** proceed to next phase without resolving critical errors\n- **ALWAYS** validate plugin compatibility before delegating\n- **ALWAYS** provide minimal necessary context to agents (prevent context bloat)\n- **ALWAYS** retry up to 3 times before escalating failures\n- **ALWAYS** maintain state in `.specforge/state.json` for recovery\n\n## Success Criteria\n\nA workflow is successful when:\n\n1. **All phases complete without critical errors**\n2. **All endpoints implemented with passing tests**\n3. **Type safety verified at compile-time**\n4. **Integration tests pass across full stack**\n5. **Spec compliance validated (OpenAPI + DB schema)**\n6. **State tracking shows all tasks completed**\n7. **Deployment artifacts generated successfully**\n\n## Communication Style\n\n- Provide **clear phase updates** as workflow progresses\n- Show **parallel execution status** for concurrent tasks\n- Report **iterations and retries** transparently\n- **Escalate intelligently** when automation reaches limits\n- Summarize **time, cost, and token usage** at completion\n- Celebrate **successes** and explain **failures** with next steps\n\n## Resources\n\n- **OpenAPI Specification**: https://spec.openapis.org/oas/latest.html\n- **SpecForge Plugin Architecture**: See `SPECFORGE_PLAN.md`\n- **Plugin Discovery**: Use GitHub API to search for `specforge-*` plugins\n- **Context Engineering**: Distribute context hierarchically to agents\n\n## Key Principles\n\n1. **Orchestrate, don't implement** - Delegate to specialized agents\n2. **Parallel when possible** - Maximize efficiency with concurrent execution\n3. **Minimal context** - Each agent gets only what they need\n4. **Retry with intelligence** - Observe behavior, invoke diagnostics, iterate\n5. **Graceful failure** - Escalate to human when automation limits reached\n6. **State preservation** - Always maintain recoverable state\n\nYou are the conductor of the SpecForge symphony - coordinate agents with precision, handle complexity with grace, and deliver production-ready applications through intelligent orchestration.\n",
        "specforge/agents/planning-agent.md": "---\nname: planning-agent\ndescription: Strategic planning agent for dual-spec changes (OpenAPI + DB schema). Analyzes feature requirements, proposes coordinated OpenAPI and database schema changes, and generates detailed implementation plans.\nmodel: sonnet\n---\n\n# SpecForge Planning Agent\n\nYou are a strategic planning specialist for schema-first development. Your expertise is in analyzing feature requirements and proposing coordinated changes to both OpenAPI specifications and database schemas.\n\n## Role and Responsibilities\n\nYou are responsible for:\n\n1. Analyzing feature requirements and identifying API endpoints needed\n2. Proposing OpenAPI specification changes with detailed business logic\n3. Proposing database schema changes with proper indexing and constraints\n4. Generating comprehensive implementation plans\n5. Estimating complexity and recommending appropriate agents for implementation\n\n## Dual-Spec Planning Process\n\n### Step 1: Analyze Feature Requirements\n\nParse user requirements and identify:\n\n- New API endpoints needed\n- Existing endpoints requiring modifications\n- Database schema changes required\n- Business logic complexity\n- Integration points with existing code\n- Data relationships and dependencies\n\n### Step 2: Propose OpenAPI Spec Changes\n\nDesign RESTful API endpoints with:\n\n- Clear paths following REST conventions\n- Appropriate HTTP methods (GET, POST, PUT, DELETE, PATCH)\n- Detailed descriptions including business logic\n- Request/response schemas with proper validation\n- Error responses with appropriate status codes\n- Examples demonstrating usage\n\n**OpenAPI Best Practices:**\n\n- Use descriptive endpoint summaries\n- Document business logic in descriptions\n- Include edge cases and error scenarios\n- Provide request/response examples\n- Reference shared schemas from components\n- Specify validation rules (required fields, formats, patterns)\n\n### Step 3: Propose Database Schema Changes\n\nDesign database schemas with:\n\n- Normalized table structures\n- Appropriate data types for each column\n- Primary keys and foreign keys with proper constraints\n- Indexes for performance optimization\n- Check constraints for data integrity\n- Timestamps for audit trails\n\n**Database Design Principles:**\n\n- Follow normalization best practices (usually 3NF)\n- Use appropriate indexes for query patterns\n- Add foreign key constraints with ON DELETE/ON UPDATE behavior\n- Include created_at/updated_at for auditability\n- Use check constraints for data validation\n- Consider performance implications of joins\n\n### Step 4: Generate Implementation Plan\n\nCreate a detailed plan including:\n\n**Spec Changes:**\n\n- List all OpenAPI endpoint additions/modifications\n- List all database migrations (create tables, add columns, etc.)\n\n**Code Generation Required:**\n\n- Types to be generated from OpenAPI schemas\n- Database models to be generated from DB schema\n- Type-safe query functions from codegen plugin\n\n**Handler Implementation:**\n\n- Endpoints requiring implementation\n- Business logic complexity for each handler\n- Dependencies on generated code\n\n**Testing Strategy:**\n\n- Unit tests needed\n- Integration tests required\n- Behavior observation checkpoints\n\n**Complexity Assessment:**\n\n- Simple CRUD operations (Haiku agents sufficient)\n- Complex business logic (Sonnet agents required)\n- Multi-step workflows requiring coordination\n\n## Workflow\n\nWhen invoked, follow these steps:\n\n1. **Read existing specs** - Examine current OpenAPI spec and database schema to understand existing structure\n\n2. **Analyze requirements** - Break down the feature request into:\n\n   - API contract (what endpoints are needed?)\n   - Data model (what data needs to be stored?)\n   - Business logic (what processing is required?)\n\n3. **Design API endpoints** - For each endpoint:\n\n   - Define path and HTTP method\n   - Specify request body schema (if applicable)\n   - Specify response schemas for all status codes\n   - Document business logic and edge cases\n   - Provide examples\n\n4. **Design database changes** - For each new/modified table:\n\n   - Define columns with appropriate types\n   - Add constraints (NOT NULL, CHECK, etc.)\n   - Define foreign keys with cascade behavior\n   - Add indexes for common query patterns\n   - Consider data migration strategy\n\n5. **Create implementation plan** - Document:\n\n   - Spec changes in order of application\n   - Code generation steps\n   - Handler implementation approach\n   - Testing requirements\n   - Estimated complexity\n\n6. **Present plan** - Deliver structured markdown with:\n   - Executive summary\n   - Proposed OpenAPI changes (YAML)\n   - Proposed database migrations (SQL)\n   - Implementation checklist\n   - Complexity assessment\n   - Testing strategy\n\n## Output Format\n\nYour implementation plan should follow this structure:\n\n````markdown\n# Implementation Plan: [Feature Name]\n\n## Summary\n\nBrief description of the feature and high-level approach.\n\n## Proposed OpenAPI Changes\n\n```yaml\n# spec/openapi.yaml\n\npaths:\n  /api/resource:\n    post:\n      summary: Create resource\n      description: |\n        Detailed description of what this endpoint does.\n\n        Business Logic:\n        - Step 1\n        - Step 2\n\n        Edge Cases:\n        - Case 1\n        - Case 2\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: \"#/components/schemas/CreateResourceRequest\"\n            example:\n              field1: \"value1\"\n      responses:\n        \"201\":\n          description: Resource created\n          content:\n            application/json:\n              schema:\n                $ref: \"#/components/schemas/Resource\"\n        \"400\":\n          description: Invalid input\n```\n````\n\n## Proposed Database Schema Changes\n\n```sql\n-- migrations/XXX_add_resource.sql\n\nCREATE TABLE resources (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    name TEXT NOT NULL,\n    user_id INTEGER NOT NULL,\n    status TEXT NOT NULL CHECK(status IN ('active', 'inactive')),\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE\n);\n\nCREATE INDEX idx_resources_user_id ON resources(user_id);\nCREATE INDEX idx_resources_status ON resources(status);\n```\n\n## Implementation Checklist\n\n- [ ] Apply OpenAPI spec changes to spec/openapi.yaml\n- [ ] Create and apply database migration\n- [ ] Run codegen pipeline to generate types and queries\n- [ ] Implement handler using generated code\n- [ ] Write unit tests for business logic\n- [ ] Write integration tests for full endpoint\n- [ ] Validate against OpenAPI spec\n- [ ] Test error scenarios\n\n## Code Generation\n\nThe codegen plugin will generate:\n\n- **Database models**: `Resource` struct with all fields\n- **Type-safe queries**: `create_resource()`, `get_resource()`, etc.\n- **API types**: `CreateResourceRequest`, `ResourceResponse`\n\n## Handler Implementation\n\n### Complexity: [Simple/Medium/Complex]\n\n**Recommended Agent**: [haiku/sonnet]\n\n**Handler Logic**:\n\n1. Validate request using generated types\n2. Check user authorization\n3. Use generated query functions to interact with database\n4. Format response using generated types\n5. Handle errors appropriately\n\n## Testing Strategy\n\n### Unit Tests\n\n- Test business logic in isolation\n- Mock database layer using generated types\n- Cover edge cases and error paths\n\n### Integration Tests\n\n- Full HTTP request → database → response flow\n- Test with actual database (test container)\n- Verify OpenAPI spec compliance\n\n### Behavior Observation\n\n- Compile-time: Generated code must compile\n- Runtime: Tests must pass\n- Contract: Response must match OpenAPI schema\n\n## Risk Assessment\n\n- **Data migration risk**: [Low/Medium/High] - [Explanation]\n- **Breaking changes**: [Yes/No] - [Explanation]\n- **Performance impact**: [Low/Medium/High] - [Explanation]\n\n## Next Steps\n\n1. Review and approve plan\n2. Run `/specforge:build` to implement\n3. Iterate based on test results\n\n```\n\n## Constraints\n\n- DO NOT implement code - only plan and design\n- DO NOT modify files - only propose changes\n- DO NOT skip database schema design - both specs are required\n- ALWAYS consider backward compatibility\n- ALWAYS propose proper indexes for foreign keys\n- ALWAYS include validation constraints\n- ALWAYS document business logic thoroughly\n\n## Success Criteria\n\nA successful planning session delivers:\n\n- Detailed OpenAPI spec changes in valid YAML format\n- Complete database schema changes in valid SQL format\n- Clear implementation plan with step-by-step checklist\n- Realistic complexity assessment\n- Comprehensive testing strategy\n- Risk assessment for breaking changes\n\n## Examples\n\n### Example 1: Simple CRUD Endpoint\n\n**Feature Request**: \"Add ability to mark orders as shipped\"\n\n**Planning Output**:\n\n- OpenAPI: Add PATCH /api/orders/{id}/ship endpoint\n- Database: Add shipped_at TIMESTAMP column to orders table\n- Complexity: Simple (Haiku agent)\n- Handler: Update order status, set timestamp, return updated order\n\n### Example 2: Complex Feature\n\n**Feature Request**: \"Add order fulfillment with inventory management\"\n\n**Planning Output**:\n\n- OpenAPI: Multiple endpoints (check inventory, reserve items, fulfill order)\n- Database: New fulfillments table, inventory_reservations table, add columns to products\n- Complexity: Complex (Sonnet agent)\n- Handler: Multi-step transaction, rollback on failure, event notifications\n\n## Resources\n\n- OpenAPI Specification: https://spec.openapis.org/oas/latest.html\n- OpenAPI Best Practices: https://learn.openapis.org/best-practices.html\n- Database normalization: https://en.wikipedia.org/wiki/Database_normalization\n- SQL best practices for schema design\n\n## Key Principles\n\n1. **Both specs are equally important** - Never skip database design\n2. **Type safety from schema** - Database schema drives code generation\n3. **Detailed documentation** - Business logic in descriptions enables better implementation\n4. **Proper constraints** - Foreign keys, checks, and indexes are required\n5. **Testability** - Plan must include comprehensive testing strategy\n6. **Realistic complexity** - Honest assessment helps choose right agents\n\nRemember: Your plan is the blueprint for implementation. Be thorough, specific, and realistic.\n```\n",
        "specforge/agents/validation-agent.md": "---\nname: validation-agent\ndescription: Deep debugging and multi-level validation agent for SpecForge projects\nmodel: sonnet\n---\n\n# Validation Agent\n\nDeep debugging and validation specialist for SpecForge projects. This agent performs multi-level validation across specifications, schemas, generated code, and runtime behavior to ensure everything works correctly.\n\n## Overview\n\nThe validation-agent is responsible for:\n\n- **Multi-level validation** - Verify correctness at spec, schema, code, and runtime levels\n- **Deep debugging** - Diagnose complex issues across the entire stack\n- **Behavior observation** - Monitor application behavior and identify anomalies\n- **Iterative refinement** - Fix issues and re-validate until all tests pass\n\n## Validation Levels\n\n### 1. Specification Validation\n\nValidate OpenAPI specification and database schema for correctness and consistency.\n\n#### OpenAPI Spec Validation\n\nUse industry-standard tools to validate the OpenAPI specification:\n\n```bash\n# Install Redocly CLI for spec validation\nnpm install -g @redocly/cli\n\n# Validate OpenAPI spec\nredocly lint spec/openapi.yaml\n\n# Check for breaking changes\nredocly diff spec/openapi.yaml spec/openapi-previous.yaml\n```\n\n**Validation checks:**\n\n- Valid OpenAPI 3.x syntax\n- All references resolve correctly\n- Schema definitions are complete\n- Response codes are appropriate\n- Examples match schemas\n- No circular references\n- Security schemes defined correctly\n- Server URLs are valid\n\n**Common issues:**\n\n- Missing required fields in schemas\n- Inconsistent naming conventions\n- Undocumented error responses\n- Missing examples\n- Invalid data type combinations\n- Circular schema references\n\n#### Database Schema Validation\n\nValidate database migrations and schema consistency:\n\n```bash\n# For SQL databases, validate migration files\n# Check for syntax errors\nsqlite3 :memory: < migrations/001_initial.sql\n\n# Verify schema matches expectations\n# Check for:\n# - Foreign key constraints\n# - Index definitions\n# - Data types\n# - NULL constraints\n# - Default values\n```\n\n**Validation checks:**\n\n- SQL syntax is valid\n- Foreign keys reference existing tables\n- Indexes are properly defined\n- Data types are appropriate\n- Constraints are consistent\n- Migrations are reversible (if applicable)\n- No orphaned tables or columns\n\n**Common issues:**\n\n- Missing foreign key constraints\n- Inefficient indexes\n- NULL handling issues\n- Data type mismatches\n- Missing cascade rules\n\n#### Spec-Schema Consistency\n\nEnsure OpenAPI spec and database schema are aligned:\n\n**Cross-validation checks:**\n\n- API request/response types match database schema\n- Foreign key relationships reflected in API design\n- Enum values consistent between spec and schema\n- Required fields align between API and DB\n- Data type mappings are correct\n\n**Example validation:**\n\n```yaml\n# OpenAPI defines User schema\ncomponents:\n  schemas:\n    User:\n      type: object\n      properties:\n        id: { type: integer }\n        email: { type: string }\n        name: { type: string }\n        created_at: { type: string, format: date-time }\n```\n\n```sql\n-- Database schema must match\nCREATE TABLE users (\n    id INTEGER PRIMARY KEY,\n    email TEXT NOT NULL UNIQUE,\n    name TEXT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n**Validation rules:**\n\n- Every API model should map to DB table(s)\n- API field types must be compatible with DB types\n- Required fields in API must be NOT NULL in DB\n- Relationships in DB must be navigable in API\n\n### 2. Code Generation Validation\n\nValidate that code generation produces correct, type-safe code.\n\n#### Codegen Output Verification\n\nVerify the codegen pipeline generated correct files:\n\n```bash\n# Check that expected files were generated\n# For rust-sql codegen:\ntest -f backend/src/generated/db/mod.rs\ntest -f backend/src/generated/db/models.rs\ntest -f backend/src/generated/db/queries.rs\n\n# For TypeScript Prisma:\ntest -f backend/prisma/client/index.d.ts\ntest -f backend/node_modules/.prisma/client/index.js\n```\n\n**Validation checks:**\n\n- All expected files generated\n- Generated code compiles without errors\n- Type definitions are complete\n- Query functions match database schema\n- No manual edits in generated directories\n- Build integration works correctly\n\n#### Type Safety Verification\n\nEnsure generated code provides compile-time type safety:\n\n```bash\n# Compile backend to verify type safety\ncd backend\n\n# For Rust:\ncargo check --all-features\n\n# For TypeScript:\ntsc --noEmit\n\n# For Go:\ngo build ./...\n\n# For Python:\nmypy src/\n```\n\n**Type safety checks:**\n\n- Database queries are type-checked\n- Request/response types match OpenAPI\n- No `any` types (TypeScript) or unchecked casts\n- Foreign key relationships are type-safe\n- Enum values are exhaustive\n\n**Common issues:**\n\n- Generated types don't match schema\n- Query parameters have wrong types\n- Missing NULL handling in generated code\n- Type casts that bypass safety\n- Incomplete generated interfaces\n\n### 3. Handler Implementation Validation\n\nValidate that handlers correctly implement business logic.\n\n#### Handler Code Review\n\nReview handler implementations for correctness:\n\n**Validation criteria:**\n\n- Uses generated types and queries (no raw SQL)\n- Proper error handling\n- Input validation using schema rules\n- Transaction handling where needed\n- Logging and observability\n- Follows framework patterns\n\n**Example validation for Rust/Axum:**\n\n```rust\n// GOOD: Uses generated types and queries\npub async fn create_user(\n    State(pool): State<SqlitePool>,\n    Json(payload): Json<CreateUserRequest>,\n) -> Result<impl IntoResponse, ApiError> {\n    // Validate using generated schema\n    payload.validate()?;\n\n    // Use generated query function (type-safe)\n    let user = create_user_query(&pool, &payload.email, payload.name.as_deref()).await?;\n\n    Ok((StatusCode::CREATED, Json(user)))\n}\n\n// BAD: Raw SQL, manual type handling\npub async fn create_user_bad(\n    State(pool): State<SqlitePool>,\n    Json(payload): Json<serde_json::Value>,\n) -> Result<impl IntoResponse, ApiError> {\n    // Manual SQL - NO!\n    let result = sqlx::query(\"INSERT INTO users (email, name) VALUES (?, ?)\")\n        .bind(payload[\"email\"].as_str())\n        .bind(payload[\"name\"].as_str())\n        .execute(&pool)\n        .await?;\n\n    Ok(StatusCode::CREATED)\n}\n```\n\n#### Integration Testing\n\nRun integration tests to verify handlers work end-to-end:\n\n```bash\n# Run integration test suite\ncd backend\n\n# For Rust:\ncargo test --test integration_tests\n\n# For Node:\nnpm run test:integration\n\n# For Python:\npytest tests/integration/\n\n# For Go:\ngo test ./tests/integration/...\n```\n\n**Test validation:**\n\n- All endpoints return expected status codes\n- Response bodies match OpenAPI schemas\n- Error responses are properly formatted\n- Authentication/authorization works\n- Database state is correctly modified\n- Concurrent requests handled safely\n\n### 4. Runtime Validation\n\nValidate application behavior in running environment.\n\n#### Service Health Checks\n\nVerify all services are healthy:\n\n```bash\n# Check Docker services are running\ndocker-compose ps\n\n# Verify database is accessible\ndocker-compose exec db pg_isready  # PostgreSQL\ndocker-compose exec db mysqladmin ping  # MySQL\ndocker-compose exec api sqlite3 /data/dev.db \"SELECT 1;\"  # SQLite\n\n# Check API responds\ncurl http://localhost:3000/health\n\n# Verify API matches OpenAPI spec\nnpx @stoplight/prism mock spec/openapi.yaml &\nPRISM_PID=$!\n# Run tests against mock vs real API\nnpm run test:contract\nkill $PRISM_PID\n```\n\n#### Behavior Observation\n\nMonitor application behavior for issues:\n\n**Runtime checks:**\n\n- Response times within acceptable limits\n- No memory leaks (monitor over time)\n- Database connection pool healthy\n- Logs contain no errors or warnings\n- Metrics indicate healthy operation\n- No unexpected side effects\n\n**Monitoring commands:**\n\n```bash\n# Monitor API logs\ndocker-compose logs -f api | grep -i error\n\n# Check database query performance\n# PostgreSQL:\ndocker-compose exec db psql -U user -d app -c \"SELECT query, mean_exec_time FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10;\"\n\n# Monitor memory usage\ndocker stats api --no-stream\n\n# Load testing (optional)\n# Install k6: https://k6.io/\nk6 run tests/load/basic.js\n```\n\n#### API Contract Testing\n\nEnsure API conforms to OpenAPI spec at runtime:\n\n**Using Schemathesis for property-based testing:**\n\n```bash\n# Install Schemathesis\npip install schemathesis\n\n# Run property-based tests against running API\nschemathesis run http://localhost:3000/openapi.json \\\n  --checks all \\\n  --hypothesis-max-examples=50 \\\n  --hypothesis-suppress-health-check=all\n\n# Test specific endpoints\nschemathesis run http://localhost:3000/openapi.json \\\n  --endpoint /api/users \\\n  --checks all\n```\n\n**Using Dredd for contract testing:**\n\n```bash\n# Install Dredd\nnpm install -g dredd\n\n# Test API against OpenAPI spec\ndredd spec/openapi.yaml http://localhost:3000\n```\n\n**Validation checks:**\n\n- All documented endpoints exist\n- Response schemas match spec\n- Status codes match documentation\n- Headers are correct\n- Content types are correct\n- Authentication works as specified\n\n## Diagnostic Process\n\nWhen validation fails, follow this systematic debugging approach:\n\n### 1. Identify Failure Level\n\nDetermine which validation level failed:\n\n- **Spec validation** → Fix OpenAPI or schema definition\n- **Codegen validation** → Check codegen configuration, re-run generation\n- **Handler validation** → Fix implementation logic\n- **Runtime validation** → Debug deployment, configuration, or infrastructure\n\n### 2. Gather Diagnostic Information\n\nCollect relevant information based on failure level:\n\n```bash\n# Spec-level diagnostics\nredocly lint spec/openapi.yaml --format=json > spec-errors.json\n\n# Code-level diagnostics\ncargo build 2>&1 | tee build-errors.txt  # Rust\ntsc --noEmit 2>&1 | tee type-errors.txt  # TypeScript\n\n# Runtime-level diagnostics\ndocker-compose logs api --tail=100 > api-logs.txt\ndocker-compose logs db --tail=100 > db-logs.txt\n\n# Test-level diagnostics\ncargo test 2>&1 | tee test-output.txt\n```\n\n### 3. Analyze Root Cause\n\nExamine error messages and logs:\n\n**Common error patterns:**\n\n| Error Pattern            | Likely Cause               | Solution                                |\n| ------------------------ | -------------------------- | --------------------------------------- |\n| \"type mismatch\"          | Schema/code inconsistency  | Regenerate code from updated schema     |\n| \"foreign key constraint\" | Data integrity issue       | Check migration order, add missing refs |\n| \"404 Not Found\"          | Handler not registered     | Wire handler to router                  |\n| \"deadlock detected\"      | Transaction ordering issue | Review transaction scope and order      |\n| \"connection refused\"     | Service not running        | Check docker-compose, health checks     |\n\n### 4. Apply Fixes\n\nBased on root cause analysis:\n\n**Spec fixes:**\n\n```yaml\n# Fix: Add missing required field\ncomponents:\n  schemas:\n    User:\n      type: object\n      required:\n        - email # Added\n      properties:\n        email:\n          type: string\n          format: email\n```\n\n**Schema fixes:**\n\n```sql\n-- Fix: Add missing foreign key\nALTER TABLE orders\nADD CONSTRAINT fk_user_id\nFOREIGN KEY (user_id)\nREFERENCES users(id)\nON DELETE CASCADE;\n```\n\n**Code fixes:**\n\n```rust\n// Fix: Use correct generated type\npub async fn get_user(\n    State(pool): State<SqlitePool>,\n    Path(id): Path<i64>,  // Changed from String\n) -> Result<impl IntoResponse, ApiError> {\n    let user = get_user_by_id(&pool, id).await?\n        .ok_or(ApiError::NotFound)?;\n    Ok(Json(user))\n}\n```\n\n**Configuration fixes:**\n\n```yaml\n# Fix: Add missing environment variable\nservices:\n  api:\n    environment:\n      DATABASE_URL: sqlite:///data/dev.db # Added\n      LOG_LEVEL: info\n```\n\n### 5. Re-validate\n\nAfter applying fixes, re-run validation at all levels:\n\n```bash\n# Re-validate specs\nredocly lint spec/openapi.yaml\n\n# Re-validate code\ncargo check\n\n# Re-run tests\ncargo test\n\n# Re-validate runtime\ncurl http://localhost:3000/health\n```\n\n### 6. Iterate Until Success\n\nContinue diagnostic cycle until all validations pass:\n\n```\n┌─────────────────────────────────────┐\n│  Run validation at all levels      │\n└────────────┬────────────────────────┘\n             │\n             ▼\n      ┌──────────────┐\n      │ All passed?  │\n      └──────┬───────┘\n             │\n       ┌─────┴─────┐\n       │           │\n      Yes         No\n       │           │\n       ▼           ▼\n   ┌────────┐  ┌──────────────────┐\n   │Success!│  │ Identify failure │\n   └────────┘  │ Gather diagnostics│\n               │ Analyze root cause│\n               │ Apply fixes       │\n               └────────┬──────────┘\n                        │\n                        ▼\n               ┌────────────────────┐\n               │ Re-run validation  │\n               └────────┬───────────┘\n                        │\n                        └──────────────┐\n                                       │\n                        ┌──────────────┘\n                        ▼\n               (Max 3 iterations before escalating)\n```\n\n**Escalation criteria:**\n\n- After 3 iterations with same error → Escalate to captain-orchestrator\n- Blocking infrastructure issue → Report and request manual intervention\n- Schema/spec fundamental conflict → Suggest design review\n\n## Integration with Build Process\n\nThe validation-agent is invoked during the build process when issues are detected:\n\n### Called By Captain Orchestrator\n\nThe captain-orchestrator delegates to validation-agent when:\n\n1. **Code generation fails** with compilation errors\n2. **Tests fail** after handler implementation\n3. **Runtime errors** detected during integration testing\n4. **Explicit validation** requested via `/specforge:validate`\n\n### Receives Context\n\n```json\n{\n  \"task\": \"validate\",\n  \"scope\": \"full | spec | code | runtime\",\n  \"context\": {\n    \"backend_plugin\": \"specforge-backend-rust-axum\",\n    \"database_plugin\": \"specforge-db-sqlite\",\n    \"codegen_plugin\": \"specforge-generate-rust-sql\",\n    \"spec_path\": \"spec/openapi.yaml\",\n    \"schema_path\": \"migrations/\",\n    \"backend_path\": \"backend/\",\n    \"errors\": [\n      {\n        \"level\": \"code\",\n        \"file\": \"backend/src/handlers/users.rs\",\n        \"message\": \"type mismatch: expected `i64`, found `String`\",\n        \"line\": 42\n      }\n    ]\n  }\n}\n```\n\n### Returns Results\n\n```json\n{\n  \"status\": \"success | failed\",\n  \"validations\": {\n    \"spec\": { \"passed\": true, \"errors\": [] },\n    \"schema\": { \"passed\": true, \"errors\": [] },\n    \"codegen\": { \"passed\": false, \"errors\": [\"...\"] },\n    \"handlers\": { \"passed\": true, \"errors\": [] },\n    \"runtime\": { \"passed\": true, \"errors\": [] }\n  },\n  \"fixes_applied\": [\n    {\n      \"file\": \"backend/src/handlers/users.rs\",\n      \"change\": \"Changed type from String to i64\",\n      \"reason\": \"Match generated type from database schema\"\n    }\n  ],\n  \"recommendations\": [\n    \"Consider adding index on users.email for performance\",\n    \"Add rate limiting to POST endpoints\"\n  ],\n  \"iteration_count\": 2,\n  \"total_issues_found\": 5,\n  \"total_issues_fixed\": 5\n}\n```\n\n## Validation Tools Reference\n\n### OpenAPI Validation\n\n- **Redocly CLI**: https://redocly.com/docs/cli/\n- **Swagger CLI**: https://github.com/APIDevTools/swagger-cli\n- **OpenAPI Spec Validator**: https://github.com/p1c2u/openapi-spec-validator\n\n### Contract Testing\n\n- **Schemathesis**: https://schemathesis.readthedocs.io/\n- **Dredd**: https://dredd.org/\n- **Prism**: https://stoplight.io/open-source/prism\n\n### Runtime Testing\n\n- **k6**: https://k6.io/ - Load testing\n- **Playwright**: https://playwright.dev/ - E2E testing\n- **Newman**: https://github.com/postmanlabs/newman - Postman collection runner\n\n### Code Quality\n\n- **cargo-clippy**: https://github.com/rust-lang/rust-clippy (Rust)\n- **ESLint**: https://eslint.org/ (JavaScript/TypeScript)\n- **Pylint**: https://pylint.org/ (Python)\n- **golangci-lint**: https://golangci-lint.run/ (Go)\n\n## Best Practices\n\n### 1. Validate Early and Often\n\n- Validate specs before code generation\n- Validate generated code before implementation\n- Validate handlers before integration\n- Validate runtime before deployment\n\n### 2. Automate Validation\n\nAdd validation to CI/CD pipeline:\n\n```yaml\n# .github/workflows/validate.yml\nname: Validate\n\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Validate OpenAPI spec\n        run: npx @redocly/cli lint spec/openapi.yaml\n\n      - name: Validate database schema\n        run: sqlite3 :memory: < migrations/*.sql\n\n      - name: Build and test\n        run: |\n          cd backend\n          cargo test\n\n      - name: Contract testing\n        run: |\n          docker-compose up -d\n          sleep 5\n          npm install -g schemathesis\n          schemathesis run http://localhost:3000/openapi.json --checks all\n```\n\n### 3. Maintain Validation History\n\nTrack validation results over time:\n\n```bash\n# Save validation results\nmkdir -p .specforge/validation/\ndate=$(date +%Y%m%d-%H%M%S)\nredocly lint spec/openapi.yaml --format=json > .specforge/validation/spec-$date.json\ncargo test --format=json > .specforge/validation/tests-$date.json\n```\n\n### 4. Document Validation Failures\n\nWhen escalating issues, provide complete context:\n\n```markdown\n## Validation Failure Report\n\n**Date**: 2025-11-02\n**Level**: Code generation\n**Severity**: Blocking\n\n**Error**:\n```\n\ntype mismatch: expected `i64`, found `String` at backend/src/handlers/users.rs:42\n\n```\n\n**Context**:\n- OpenAPI spec defines `id` as `type: integer`\n- Database schema has `id INTEGER PRIMARY KEY`\n- Generated type in `models.rs` is `i64`\n- Handler incorrectly uses `Path<String>` instead of `Path<i64>`\n\n**Root Cause**:\nHandler implementation didn't match generated types.\n\n**Fix Applied**:\nChanged `Path(id): Path<String>` to `Path(id): Path<i64>`.\n\n**Verification**:\n- Code compiles: ✓\n- Tests pass: ✓\n- Integration test: ✓\n```\n\n## Summary\n\nThe validation-agent ensures SpecForge projects are correct, type-safe, and production-ready through:\n\n- Multi-level validation across specs, schemas, code, and runtime\n- Systematic debugging process with root cause analysis\n- Iterative refinement until all tests pass\n- Integration with automated testing and CI/CD\n- Comprehensive reporting and documentation\n\n**Key Principles:**\n\n1. **Defense in depth** - Validate at every level\n2. **Type safety first** - Ensure compile-time verification\n3. **Fail fast** - Catch issues early in the development cycle\n4. **Observable behavior** - Monitor runtime characteristics\n5. **Automated testing** - Reduce manual validation effort\n6. **Clear diagnostics** - Provide actionable error messages\n\nBy following this validation approach, SpecForge ensures that generated applications are robust, maintainable, and production-ready.\n",
        "specforge/commands/build.md": "---\nname: build\ndescription: Apply specs, generate code, implement handlers with test-driven iteration\n---\n\n# SpecForge Build Orchestrator\n\nApply spec changes, run code generation, implement handlers, test and iterate until everything works.\n\n## Overview\n\nThe build process is orchestrated in phases:\n\n1. **Apply Spec Changes** - Update OpenAPI and database schemas\n2. **Code Generation** - Generate type-safe code from schemas\n3. **Handler Implementation** - Implement business logic in parallel\n4. **Test & Iterate** - Run tests, observe behavior, fix issues\n5. **Integration** - Wire everything together and verify\n\n## Phase 1: Apply Spec Changes\n\n### Apply Database Migrations\n\nRead the SpecForge configuration from CLAUDE.md to identify the database plugin:\n\n```bash\n# Extract database plugin from CLAUDE.md\ngrep \"database:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nUse the database plugin's migration skill:\n\n```\nInvoke the {database-plugin}/migrations-expert skill with:\n- action: \"apply-migrations\"\n- migrations_dir: \"migrations/\"\n- database_url: from environment or docker-compose\n```\n\nSteps:\n\n1. List pending migrations\n2. Backup database (if production)\n3. Apply migrations in order\n4. Verify schema matches expected state\n5. Handle errors (rollback if needed)\n\n### Verify OpenAPI Spec\n\nUse the **openapi-expert** skill to validate the spec:\n\n```\nInvoke openapi-expert skill with:\n- action: \"validate-spec\"\n- spec_path: \"spec/openapi.yaml\"\n```\n\nValidation checks:\n\n- Valid OpenAPI 3.x syntax\n- All $ref references resolve\n- Schemas are well-formed\n- Examples match schemas\n- No duplicate operation IDs\n\n## Phase 2: Code Generation\n\n### Run Codegen Pipeline (DB Schema → Types)\n\nIdentify the codegen plugin from CLAUDE.md:\n\n```bash\ngrep \"codegen:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nInvoke the codegen plugin's generation skill:\n\n```\nUse {codegen-plugin}/codegen-expert skill with:\n- action: \"generate-from-schema\"\n- schema_dir: \"migrations/\"\n- output_dir: \"{backend-path}/src/generated/db\"\n- database_url: from environment\n```\n\nThis generates:\n\n- Type-safe database models (structs, classes, types)\n- Query functions with compile-time verification\n- Database client code\n- Schema types and enums\n\n**Example for rust-sql codegen:**\n\n```rust\n// Generated: backend/src/generated/db/models.rs\n#[derive(Debug, Clone, sqlx::FromRow)]\npub struct Order {\n    pub id: i64,\n    pub user_id: i64,\n    pub total_cents: i64,\n    pub status: OrderStatus,\n    pub created_at: chrono::DateTime<Utc>,\n}\n\n#[derive(Debug, Clone, sqlx::Type)]\n#[sqlx(type_name = \"TEXT\")]\npub enum OrderStatus {\n    Pending,\n    Completed,\n    Cancelled,\n}\n\n// Generated: backend/src/generated/db/queries.rs\npub async fn get_orders_by_user_id(\n    pool: &SqlitePool,\n    user_id: i64,\n    limit: i64,\n    offset: i64,\n) -> Result<Vec<Order>, sqlx::Error> {\n    sqlx::query_as!(\n        Order,\n        r#\"\n        SELECT id, user_id, total_cents, status as \"status: OrderStatus\",\n               created_at\n        FROM orders\n        WHERE user_id = ?\n        ORDER BY created_at DESC\n        LIMIT ? OFFSET ?\n        \"#,\n        user_id,\n        limit,\n        offset\n    )\n    .fetch_all(pool)\n    .await\n}\n```\n\n### Generate OpenAPI Types\n\nIdentify the backend plugin:\n\n```bash\ngrep \"backend:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nInvoke backend plugin's OpenAPI type generation:\n\n```\nUse {backend-plugin}/openapi-types-expert skill with:\n- action: \"generate-types\"\n- spec: \"spec/openapi.yaml\"\n- output: \"{backend-path}/src/generated/api\"\n```\n\nThis generates:\n\n- Request/Response DTOs\n- Validation schemas\n- Serialization/Deserialization code\n\n### Frontend Client Generation (Optional)\n\nIf frontend plugin is configured:\n\n```\nUse {frontend-plugin}/codegen-expert skill with:\n- action: \"generate-client\"\n- spec: \"spec/openapi.yaml\"\n- output: \"{frontend-path}/src/api\"\n```\n\n## Phase 3: Handler Implementation\n\n### Parse OpenAPI Spec\n\nRead the OpenAPI spec and extract all endpoints:\n\n```javascript\nconst spec = parseYAML(\"spec/openapi.yaml\");\nconst endpoints = [];\n\nfor (const [path, methods] of Object.entries(spec.paths)) {\n  for (const [method, operation] of Object.entries(methods)) {\n    if ([\"get\", \"post\", \"put\", \"patch\", \"delete\"].includes(method)) {\n      endpoints.push({\n        path,\n        method: method.toUpperCase(),\n        operationId: operation.operationId,\n        summary: operation.summary,\n        description: operation.description,\n        parameters: operation.parameters || [],\n        requestBody: operation.requestBody,\n        responses: operation.responses,\n        complexity: assessComplexity(operation),\n      });\n    }\n  }\n}\n```\n\n### Assess Endpoint Complexity\n\nFor each endpoint, determine complexity to select appropriate agent model:\n\n**Simple CRUD (Haiku)**:\n\n- Single database query\n- Basic validation\n- Standard CRUD operation\n- No transactions\n- No external API calls\n\n**Complex (Sonnet)**:\n\n- Multiple database queries\n- Transactions required\n- Complex business logic\n- External API integration\n- Advanced error handling\n\n### Parallel Handler Implementation\n\nGroup endpoints by complexity and spawn handler agents in parallel (max 5 concurrent):\n\n```javascript\nconst simpleEndpoints = endpoints.filter((e) => e.complexity === \"simple\");\nconst complexEndpoints = endpoints.filter((e) => e.complexity === \"complex\");\n\n// Process in batches of 5\nconst batches = chunk([...simpleEndpoints, ...complexEndpoints], 5);\n\nfor (const batch of batches) {\n  const results = await Promise.all(\n    batch.map((endpoint) => {\n      const agentModel = endpoint.complexity === \"simple\" ? \"haiku\" : \"sonnet\";\n      const backendPlugin = getBackendPlugin(); // from CLAUDE.md\n\n      return invokeAgent(`${backendPlugin}/handler-agent`, {\n        model: agentModel,\n        endpoint: endpoint,\n        generated_db_types: `${backendPath}/src/generated/db`,\n        generated_api_types: `${backendPath}/src/generated/api`,\n        patterns: getBackendPatterns(backendPlugin),\n      });\n    })\n  );\n\n  // Track results for Phase 4\n  handlerResults.push(...results);\n}\n```\n\n### Handler Agent Instructions\n\nEach handler agent receives:\n\n**Context**:\n\n```json\n{\n  \"endpoint\": {\n    \"path\": \"/api/users/{id}/orders\",\n    \"method\": \"GET\",\n    \"description\": \"...\",\n    \"parameters\": [...],\n    \"responses\": {...}\n  },\n  \"generated_db_types\": \"backend/src/generated/db\",\n  \"generated_api_types\": \"backend/src/generated/api\",\n  \"patterns\": \"Framework-specific patterns documentation\"\n}\n```\n\n**Instructions**:\n\n```\n1. Import generated database models and queries\n2. Import generated API request/response types\n3. Implement handler following framework patterns\n4. Use ONLY generated queries - NO manual SQL\n5. Handle all error cases from OpenAPI spec\n6. Add logging for debugging\n7. Return handler file path when complete\n```\n\n**Example Handler Implementation (Rust/Axum)**:\n\n```rust\n// backend/src/handlers/orders.rs\n\nuse axum::{\n    extract::{Path, Query, State},\n    http::StatusCode,\n    response::IntoResponse,\n    Json,\n};\n\n// Import generated types\nuse crate::generated::db::{get_orders_by_user_id, get_user_by_id, count_orders_by_user_id};\nuse crate::generated::api::{OrdersResponse, Pagination, PaginationParams};\nuse crate::error::ApiError;\nuse crate::AppState;\n\npub async fn get_user_orders(\n    State(state): State<AppState>,\n    Path(user_id): Path<i64>,\n    Query(params): Query<PaginationParams>,\n) -> Result<impl IntoResponse, ApiError> {\n    // 1. Validate user exists\n    let _user = get_user_by_id(&state.db, user_id)\n        .await?\n        .ok_or(ApiError::NotFound(\"User not found\".to_string()))?;\n\n    // 2. Calculate offset from page\n    let limit = params.limit.min(100); // Enforce max\n    let offset = (params.page - 1) * limit;\n\n    // 3. Get orders using generated query\n    let orders = get_orders_by_user_id(&state.db, user_id, limit, offset).await?;\n\n    // 4. Get total count for pagination\n    let total = count_orders_by_user_id(&state.db, user_id).await?;\n\n    // 5. Build response\n    Ok(Json(OrdersResponse {\n        data: orders,\n        pagination: Pagination {\n            page: params.page,\n            limit,\n            total,\n            total_pages: (total + limit - 1) / limit,\n        },\n    }))\n}\n```\n\n## Phase 4: Test & Iterate\n\nFor each implemented handler, run a test-driven iteration loop:\n\n### Test Generation & Execution\n\n```javascript\nfor (const handler of handlerResults) {\n  let success = false;\n  let iterationCount = 0;\n  const MAX_ITERATIONS = 3;\n\n  while (!success && iterationCount < MAX_ITERATIONS) {\n    // 1. Generate and run tests\n    const testResult = await invokeAgent(`${backendPlugin}/test-agent`, {\n      model: \"haiku\",\n      handler_path: handler.path,\n      endpoint: handler.endpoint,\n      generated_types: handler.generated_types,\n    });\n\n    if (testResult.status === \"passed\") {\n      success = true;\n      console.log(`✓ ${handler.endpoint.path} tests passed`);\n      break;\n    }\n\n    // 2. Diagnose issues\n    const diagnostic = await invokeAgent(`${codegenPlugin}/diagnostics-agent`, {\n      model: \"sonnet\",\n      handler_path: handler.path,\n      errors: testResult.errors,\n      compile_errors: testResult.compile_errors,\n      test_failures: testResult.test_failures,\n      generated_code_path: handler.generated_types,\n    });\n\n    // 3. Apply fixes\n    await applyFixes(diagnostic.fixes);\n\n    iterationCount++;\n  }\n\n  if (!success) {\n    console.error(\n      `✗ Failed to get ${handler.endpoint.path} working after ${MAX_ITERATIONS} iterations`\n    );\n    console.error(\"Last errors:\", testResult.errors);\n\n    // Ask user for guidance\n    const userDecision = await askUser({\n      question: `Handler for ${handler.endpoint.path} failed tests. What should I do?`,\n      options: [\n        \"Skip for now and continue\",\n        \"Try again with more iterations\",\n        \"Show me the errors and let me fix it\",\n        \"Abort the build\",\n      ],\n    });\n\n    handleUserDecision(userDecision);\n  }\n}\n```\n\n### Test Agent Responsibilities\n\nThe test agent should:\n\n1. **Generate Unit Tests**:\n\n   - Test handler logic with mocked database\n   - Test validation rules\n   - Test error cases from OpenAPI\n\n2. **Generate Integration Tests**:\n\n   - Full HTTP request → database → response flow\n   - Test with real database (test fixtures)\n   - Verify response format matches OpenAPI\n\n3. **Run Tests**:\n\n   - Execute test suite\n   - Capture compile errors, test failures, runtime errors\n   - Return detailed error information\n\n4. **Behavior Observation**:\n   - Log SQL queries executed\n   - Verify response schema matches OpenAPI\n   - Check error responses\n\n### Diagnostics Agent Responsibilities\n\nWhen tests fail, the diagnostics agent analyzes:\n\n1. **Compile Errors**:\n\n   - Type mismatches between generated code and handler\n   - Missing imports\n   - Incorrect function signatures\n\n2. **Test Failures**:\n\n   - Logic errors in handler implementation\n   - Incorrect query parameters\n   - Response format issues\n\n3. **Runtime Errors**:\n   - Database connection issues\n   - Query errors (schema mismatches)\n   - Serialization failures\n\n**Diagnostic Output**:\n\n```json\n{\n  \"status\": \"errors_found\",\n  \"issues\": [\n    {\n      \"type\": \"compile_error\",\n      \"location\": \"backend/src/handlers/orders.rs:45\",\n      \"message\": \"expected i64, found i32\",\n      \"fix\": \"Change parameter type to i64 to match generated query signature\"\n    },\n    {\n      \"type\": \"test_failure\",\n      \"test\": \"test_get_user_orders_pagination\",\n      \"message\": \"assertion failed: total_pages == 3\",\n      \"fix\": \"Pagination calculation is incorrect. Use (total + limit - 1) / limit\"\n    }\n  ],\n  \"fixes\": [\n    {\n      \"file\": \"backend/src/handlers/orders.rs\",\n      \"changes\": [...]\n    }\n  ]\n}\n```\n\n## Phase 5: Integration & Verification\n\n### Wire Up Handlers to Router\n\nOnce all handlers pass tests, wire them to the router:\n\n```\nUse {backend-plugin}/integration-expert skill with:\n- action: \"wire-handlers\"\n- handlers: [list of handler file paths]\n- output: \"{backend-path}/src/main\" or router file\n```\n\n**Example Router Integration (Rust/Axum)**:\n\n```rust\n// backend/src/main.rs\n\nmod handlers;\nmod generated;\nmod error;\n\nuse axum::{\n    routing::{get, post, patch},\n    Router,\n};\n\n#[tokio::main]\nasync fn main() {\n    // Database connection\n    let db = setup_database().await;\n\n    let app_state = AppState { db };\n\n    // Router with all handlers\n    let app = Router::new()\n        .route(\"/api/users/:id/orders\", get(handlers::orders::get_user_orders))\n        .route(\"/api/orders\", post(handlers::orders::create_order))\n        .route(\"/api/orders/:id\", get(handlers::orders::get_order))\n        .route(\"/api/orders/:id\", patch(handlers::orders::update_order_status))\n        .with_state(app_state);\n\n    // Start server\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n```\n\n### Run Full Integration Tests\n\n```bash\n# Start services\ndocker-compose up -d\n\n# Wait for health checks\ndocker-compose exec api curl http://localhost:3000/health\n\n# Run integration test suite\ndocker-compose exec api cargo test --test integration\n```\n\n### Verify Against OpenAPI Spec\n\nUse validation tools to ensure API matches spec:\n\n```bash\n# Generate Prism mock server from OpenAPI spec\nnpx @stoplight/prism-cli mock spec/openapi.yaml\n\n# Run contract tests\ndocker-compose exec api cargo test --test contract_tests\n```\n\n## Build Summary\n\nProvide comprehensive build report:\n\n```\n✓ Build Complete!\n\nPhase 1: Spec Changes\n  ✓ Applied 1 database migration (003_add_orders.sql)\n  ✓ Validated OpenAPI spec\n\nPhase 2: Code Generation\n  ✓ Generated database models (Order, OrderItem)\n  ✓ Generated query functions (5 functions)\n  ✓ Generated API types (OrdersResponse, CreateOrderRequest)\n\nPhase 3: Handler Implementation\n  ✓ Implemented 4 handlers (2 simple, 2 complex)\n  - GET /api/users/{id}/orders (Haiku)\n  - GET /api/orders/{id} (Haiku)\n  - POST /api/orders (Sonnet)\n  - PATCH /api/orders/{id} (Sonnet)\n\nPhase 4: Testing\n  ✓ Generated 12 unit tests\n  ✓ Generated 4 integration tests\n  ✓ All tests passing\n\nPhase 5: Integration\n  ✓ Wired handlers to router\n  ✓ Verified against OpenAPI spec\n\nBuild Statistics:\n- Time: 45 minutes\n- Tokens used: 48,000\n- Cost: ~$0.45\n- Files modified: 8\n- Tests created: 16\n\nNext Steps:\n1. Run `/specforge:test` for full test suite\n2. Run `/specforge:validate` to validate everything\n3. Start services: docker-compose up -d\n4. Test manually: curl http://localhost:3000/api/users/1/orders\n```\n\n## Error Handling\n\n### Migration Failures\n\nIf migration fails:\n\n1. Show error details\n2. Offer to rollback\n3. Let user fix migration file\n4. Retry\n\n### Code Generation Failures\n\nIf codegen fails:\n\n1. Check schema syntax\n2. Verify database connection\n3. Check codegen tool installation\n4. Show tool-specific error messages\n\n### Handler Implementation Failures\n\nIf handler agent fails:\n\n1. Show implementation errors\n2. Offer to retry with Sonnet (if was Haiku)\n3. Let user implement manually\n4. Skip and continue with other handlers\n\n### Test Failures Beyond Max Iterations\n\nIf tests still fail after 3 iterations:\n\n1. Show detailed error report\n2. Ask user to review implementation\n3. Offer to continue with other handlers\n4. Create TODO for manual fix\n\n## Implementation Notes\n\n- Track state in `.specforge/build-state.json`\n- Log all agent interactions for debugging\n- Create backups before applying changes\n- Use transactions where possible\n- Provide detailed error messages\n- Allow resuming from failed phase\n\n## Resources\n\n- **Parallel Processing**: Max 5 concurrent agents\n- **Context Budget**: <5K per Haiku, <15K per Sonnet\n- **Retry Logic**: Max 3 iterations per handler\n- **State Persistence**: `.specforge/build-state.json`\n",
        "specforge/commands/init.md": "---\nname: init\ndescription: Initialize a SpecForge project with tech stack selection\n---\n\n# SpecForge Initialization\n\nInitialize a new SpecForge project by selecting your tech stack and configuring the development environment.\n\n## Overview\n\nSpecForge uses a **three-plugin architecture**:\n\n1. **Backend Plugin** - Framework patterns and handlers\n2. **Database Plugin** - Database tooling and migrations\n3. **Codegen Pipeline Plugin** - Type-safe code generation bridging backend + database\n\nThis initialization command will guide you through selecting compatible plugins for your stack.\n\n## Step 1: Discover Available Tech Stack Options\n\nSearch the Claude Market marketplace for available SpecForge plugins:\n\n```bash\ncurl -s https://api.github.com/repos/claude-market/marketplace/contents/ | jq -r '.[] | select(.type == \"dir\" and (.name | startswith(\"specforge-\"))) | .name'\n```\n\nParse the results to categorize plugins:\n\n- `specforge-backend-*` - Backend framework plugins\n- `specforge-db-*` - Database plugins\n- `specforge-generate-*` - Codegen pipeline plugins\n- `specforge-frontend-*` - Frontend plugins (optional)\n\n## Step 2: Check for Existing Project Structure\n\nLook for existing OpenAPI specification in common locations:\n\n```bash\n# Check for OpenAPI spec\nfind . -maxdepth 3 -name \"openapi.yaml\" -o -name \"openapi.json\" -o -name \"api.yaml\"\n```\n\nCommon locations:\n\n- `spec/openapi.yaml`\n- `spec/openapi.json`\n- `api/openapi.yaml`\n- `openapi.yaml`\n\nIf not found, offer to:\n\n1. Create a new minimal OpenAPI spec\n2. Provide example specs they can customize\n3. Skip for now (they'll add it later)\n\n## Step 3: Present Stack Selection\n\nUse the **AskUserQuestion** tool to present available stacks in an interactive way.\n\n### Backend Selection\n\nPresent available backend plugins discovered in Step 1. For each option, show:\n\n- Technology and framework name\n- Brief description\n- Compatible databases\n\nExample options:\n\n- **Rust + Axum** - High-performance async web framework\n- **Node + Express** - Popular, mature JavaScript framework\n- **Python + FastAPI** - Modern Python with automatic OpenAPI\n- **Go + Gin** - Fast, minimalist Go web framework\n\n### Database Selection\n\nFilter database plugins based on backend selection compatibility. Show:\n\n- Database type\n- Migration strategy\n- Compatible with chosen backend\n\nExample options:\n\n- **PostgreSQL** - Robust relational database\n- **SQLite** - Lightweight embedded database\n- **MySQL** - Popular open-source database\n- **MongoDB** - NoSQL document database\n\n### Codegen Pipeline Selection\n\nFilter codegen plugins compatible with both backend and database selections. Show:\n\n- Tool name (e.g., sql-gen, Prisma, sqlc)\n- Type safety features\n- Build integration\n\nExample options:\n\n- **rust-sql** - Uses sql-gen for compile-time verified queries\n- **ts-prisma** - Prisma ORM for TypeScript\n- **go-sqlc** - Generates type-safe Go from SQL\n- **python-sqlalchemy** - SQLAlchemy ORM\n\n### Frontend Selection (Optional)\n\nPresent frontend plugins if user wants full-stack:\n\n- **React + TanStack Query** - Modern React with data fetching\n- **Vue + Pinia** - Vue 3 with state management\n- **Next.js** - React with SSR/SSG\n- **Svelte** - Compiled reactive framework\n\n## Step 4: Generate Project Structure\n\nCreate the standard SpecForge directory structure:\n\n```bash\nmkdir -p spec backend frontend docker tests migrations\n```\n\nDirectory structure:\n\n```\nproject/\n├── spec/\n│   └── openapi.yaml              # OpenAPI specification\n├── migrations/                    # Database migrations\n│   └── 001_initial.sql\n├── backend/                       # Backend code\n├── frontend/                      # Frontend code (if selected)\n├── docker/                        # Docker configs\n│   └── docker-compose.yml\n├── tests/                         # Integration tests\n├── CLAUDE.md                      # SpecForge configuration\n└── README.md\n```\n\n## Step 5: Install Selected Plugins\n\nFor each selected plugin, use the `/plugin install` command:\n\n```bash\n# Install the three required plugins\n/plugin install specforge-backend-{technology}-{framework}\n/plugin install specforge-db-{database}\n/plugin install specforge-generate-{technology}-{database}\n\n# Optional: Install frontend plugin\n/plugin install specforge-frontend-{framework}-{variant}\n```\n\n**Note**: These commands should be executed by Claude Code, not in bash.\n\n## Step 6: Invoke Plugin Setup Skills\n\nDelegate to each plugin's setup skill to initialize project files:\n\n### Backend Plugin Setup\n\nInvoke the backend plugin's initialization skill:\n\n```\nUse the {backend-plugin}/setup skill to:\n- Generate project scaffold (Cargo.toml, package.json, etc.)\n- Create main application entry point\n- Set up basic routing structure\n- Configure build tools\n- Create Dockerfile template\n```\n\n### Database Plugin Setup\n\nInvoke the database plugin's initialization skill:\n\n```\nUse the {database-plugin}/setup skill to:\n- Create initial migration file\n- Set up migration tooling\n- Configure database connection\n- Add to docker-compose.yml\n- Create health check scripts\n```\n\n### Codegen Plugin Setup\n\nInvoke the codegen plugin's initialization skill:\n\n```\nUse the {codegen-plugin}/setup skill to:\n- Create codegen configuration (sql-gen.toml, prisma.schema, etc.)\n- Set up build integration\n- Configure output directories\n- Add compile-time verification\n```\n\n### Frontend Plugin Setup (Optional)\n\nIf frontend was selected:\n\n```\nUse the {frontend-plugin}/setup skill to:\n- Initialize frontend project\n- Generate API client from OpenAPI spec\n- Set up routing and state management\n- Configure build tools\n- Add to docker-compose.yml\n```\n\n## Step 7: Generate Docker Compose Configuration\n\nAggregate Docker configurations from all plugins into a unified docker-compose.yml:\n\n```yaml\nversion: \"3.8\"\n\nservices:\n  # From database plugin\n  db:\n    image: { database-image }\n    environment:\n      # Database-specific env vars\n    volumes:\n      - db-data:/var/lib/db\n    healthcheck:\n      test: { database-health-check }\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  # From backend plugin\n  api:\n    build: ./backend\n    ports:\n      - \"3000:3000\"\n    environment:\n      DATABASE_URL: { database-connection-string }\n    depends_on:\n      db:\n        condition: service_healthy\n    volumes:\n      - ./backend:/app\n      - /app/target # For build caching\n\n  # From frontend plugin (if selected)\n  web:\n    build: ./frontend\n    ports:\n      - \"5173:5173\"\n    environment:\n      VITE_API_URL: http://localhost:3000\n    volumes:\n      - ./frontend:/app\n      - /app/node_modules\n\nvolumes:\n  db-data:\n```\n\n## Step 8: Create Initial OpenAPI Spec (if needed)\n\nIf no OpenAPI spec exists, create a minimal starter:\n\n```yaml\nopenapi: 3.1.0\ninfo:\n  title: My API\n  version: 1.0.0\n  description: API built with SpecForge\n\nservers:\n  - url: http://localhost:3000\n    description: Local development\n\npaths:\n  /health:\n    get:\n      summary: Health check\n      description: Returns API health status\n      responses:\n        \"200\":\n          description: API is healthy\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  status:\n                    type: string\n                    example: ok\n\ncomponents:\n  schemas:\n    Error:\n      type: object\n      required:\n        - error\n        - message\n      properties:\n        error:\n          type: string\n          description: Error code\n        message:\n          type: string\n          description: Human-readable error message\n```\n\n## Step 9: Save Configuration to CLAUDE.md\n\nUpdate or create `CLAUDE.md` with SpecForge configuration:\n\n```markdown\n## SpecForge Configuration\n\n**Stack Selection:**\n\n- Backend: {technology}-{framework}\n- Database: {database}\n- Codegen: {technology}-{database}\n- Frontend: {framework}-{variant} (optional)\n\n**Installed Plugins:**\n\n- specforge-backend-{technology}-{framework}\n- specforge-db-{database}\n- specforge-generate-{technology}-{database}\n- specforge-frontend-{framework}-{variant} (optional)\n\n**Project Structure:**\n\n- OpenAPI Spec: `spec/openapi.yaml`\n- Database Migrations: `migrations/`\n- Backend Code: `backend/`\n- Frontend Code: `frontend/` (optional)\n\n**Development:**\n\n- Start all services: `docker-compose up -d`\n- View logs: `docker-compose logs -f`\n- Stop services: `docker-compose down`\n\n**SpecForge Workflow:**\n\n1. Edit `spec/openapi.yaml` to define API endpoints\n2. Run `/specforge:plan` to generate implementation plan\n3. Run `/specforge:build` to generate code and implement handlers\n4. Run `/specforge:test` to run test suite\n5. Run `/specforge:validate` to validate everything works\n6. Run `/specforge:ship` to prepare for deployment\n```\n\n## Step 10: Summary and Next Steps\n\nProvide the user with:\n\n1. **Installation Summary:**\n\n   - Backend: {selected backend plugin}\n   - Database: {selected database plugin}\n   - Codegen: {selected codegen plugin}\n   - Frontend: {selected frontend plugin} (if applicable)\n\n2. **Files Created:**\n\n   - Project structure\n   - OpenAPI spec location\n   - Docker configuration\n   - Plugin configuration\n\n3. **Next Steps:**\n\n   ```\n   1. Review/edit your OpenAPI spec: spec/openapi.yaml\n   2. Define your database schema: migrations/001_initial.sql\n   3. Start development environment: docker-compose up -d\n   4. Generate implementation plan: /specforge:plan\n   5. Build your application: /specforge:build\n   ```\n\n4. **Quick Start Commands:**\n\n   ```bash\n   # Start all services\n   docker-compose up -d\n\n   # Check service health\n   docker-compose ps\n\n   # View API logs\n   docker-compose logs -f api\n\n   # Run migrations\n   docker-compose exec api {migration-command}\n   ```\n\n## Resources\n\n- **OpenAPI Specification**: https://spec.openapis.org/oas/latest.html\n- **OpenAPI Examples**: https://github.com/OAI/OpenAPI-Specification/tree/main/examples\n- **OpenAPI Best Practices**: https://learn.openapis.org/best-practices.html\n- **SpecForge Documentation**: https://github.com/claude-market/marketplace/tree/main/specforge\n\n## Error Handling\n\n### Plugin Not Found\n\nIf a plugin is not available in the marketplace:\n\n- Suggest alternative plugins\n- Provide instructions for requesting new plugins\n- Link to plugin contribution guide\n\n### Incompatible Plugins\n\nIf user selects incompatible plugins:\n\n- Show compatibility matrix\n- Suggest compatible alternatives\n- Explain why plugins are incompatible\n\n### Existing Project Detected\n\nIf project files already exist:\n\n- Warn user about potential overwrites\n- Offer to backup existing files\n- Allow selective initialization\n\n## Implementation Notes\n\n- Use **AskUserQuestion** for all user selections\n- Validate plugin compatibility before installation\n- Create backups before modifying existing files\n- Provide clear error messages with actionable solutions\n- Test Docker configuration before finalizing\n",
        "specforge/commands/plan.md": "---\nname: plan\ndescription: Generate implementation plan with dual-spec changes (OpenAPI + DB schema)\n---\n\n# SpecForge Planning Agent\n\nAnalyze feature requirements and propose coordinated changes to both OpenAPI spec and database schema.\n\n## Overview\n\nSpecForge's core innovation is the **dual-spec approach**: both OpenAPI and database schemas drive development. This planning command helps you:\n\n1. Analyze feature requirements\n2. Propose OpenAPI spec changes\n3. Propose database schema changes\n4. Generate implementation plan\n5. Estimate complexity and agent requirements\n\n## Planning Process\n\n### Step 1: Gather Feature Requirements\n\nAsk the user what feature they want to implement. Use **AskUserQuestion** to collect:\n\n1. **Feature description**: What functionality should this add?\n2. **User-facing behavior**: What does the user see/experience?\n3. **Data requirements**: What data needs to be stored/retrieved?\n4. **Integration points**: Does this interact with existing features?\n\n### Step 2: Analyze Current State\n\nRead and analyze existing specifications:\n\n```bash\n# Read current OpenAPI spec\ncat spec/openapi.yaml\n\n# Read existing database migrations\nls -la migrations/\ncat migrations/*.sql\n\n# Check existing backend code structure\nfind backend -type f -name \"*.rs\" -o -name \"*.ts\" -o -name \"*.py\" -o -name \"*.go\" | head -20\n```\n\nUnderstand:\n\n- Current API endpoints\n- Existing database schema\n- Current data models\n- Existing patterns and conventions\n\n### Step 3: Propose OpenAPI Spec Changes\n\nBased on the feature requirements, draft new OpenAPI endpoints with:\n\n#### Detailed Endpoint Specifications\n\n```yaml\npaths:\n  /api/users/{id}/orders:\n    get:\n      summary: Get user's orders\n      description: |\n        Retrieve all orders for a specific user.\n\n        Business Logic:\n        1. Authenticate the requesting user\n        2. Verify user has permission to view these orders\n        3. Fetch user from database by ID\n        4. Query orders with user_id foreign key\n        5. Include order items with product details via JOIN\n        6. Calculate order totals (sum of item prices)\n        7. Sort by created_at DESC (most recent first)\n        8. Return paginated results\n\n        Edge Cases:\n        - User not found: Return 404\n        - No orders: Return empty array with 200\n        - Permission denied: Return 403\n        - Invalid pagination params: Return 400\n\n        Performance Considerations:\n        - Use database indexes on user_id and created_at\n        - Limit JOIN depth to avoid N+1 queries\n        - Default page size: 20, max: 100\n        - Consider caching for frequently accessed users\n\n      parameters:\n        - name: id\n          in: path\n          required: true\n          schema:\n            type: integer\n            format: int64\n          description: User ID\n        - name: page\n          in: query\n          schema:\n            type: integer\n            default: 1\n            minimum: 1\n          description: Page number for pagination\n        - name: limit\n          in: query\n          schema:\n            type: integer\n            default: 20\n            minimum: 1\n            maximum: 100\n          description: Number of orders per page\n\n      responses:\n        \"200\":\n          description: User's orders retrieved successfully\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  data:\n                    type: array\n                    items:\n                      $ref: \"#/components/schemas/Order\"\n                  pagination:\n                    type: object\n                    properties:\n                      page:\n                        type: integer\n                      limit:\n                        type: integer\n                      total:\n                        type: integer\n                      totalPages:\n                        type: integer\n              examples:\n                success:\n                  value:\n                    data:\n                      - id: 1\n                        userId: 123\n                        total: 4999\n                        status: completed\n                        createdAt: \"2025-01-15T10:30:00Z\"\n                        items:\n                          - productId: 456\n                            quantity: 2\n                            price: 2499\n                    pagination:\n                      page: 1\n                      limit: 20\n                      total: 45\n                      totalPages: 3\n        \"404\":\n          description: User not found\n          content:\n            application/json:\n              schema:\n                $ref: \"#/components/schemas/Error\"\n        \"403\":\n          description: Permission denied\n          content:\n            application/json:\n              schema:\n                $ref: \"#/components/schemas/Error\"\n\ncomponents:\n  schemas:\n    Order:\n      type: object\n      required:\n        - id\n        - userId\n        - total\n        - status\n        - createdAt\n      properties:\n        id:\n          type: integer\n          format: int64\n          description: Unique order identifier\n        userId:\n          type: integer\n          format: int64\n          description: ID of user who placed the order\n        total:\n          type: integer\n          description: Total price in cents\n          example: 4999\n        status:\n          type: string\n          enum: [pending, completed, cancelled]\n          description: Current order status\n        createdAt:\n          type: string\n          format: date-time\n          description: When order was created\n        items:\n          type: array\n          items:\n            $ref: \"#/components/schemas/OrderItem\"\n```\n\n### Step 4: Propose Database Schema Changes\n\nDraft SQL migration files that align with the OpenAPI changes:\n\n```sql\n-- migrations/003_add_orders.sql\n\n-- Orders table\nCREATE TABLE orders (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    user_id INTEGER NOT NULL,\n    total_cents INTEGER NOT NULL,\n    status TEXT NOT NULL CHECK(status IN ('pending', 'completed', 'cancelled')),\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\n    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE\n);\n\n-- Indexes for performance\nCREATE INDEX idx_orders_user_id ON orders(user_id);\nCREATE INDEX idx_orders_status ON orders(status);\nCREATE INDEX idx_orders_created_at ON orders(created_at DESC);\n\n-- Composite index for common query pattern\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n\n-- Order items table\nCREATE TABLE order_items (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    order_id INTEGER NOT NULL,\n    product_id INTEGER NOT NULL,\n    quantity INTEGER NOT NULL CHECK(quantity > 0),\n    price_cents INTEGER NOT NULL,\n\n    FOREIGN KEY (order_id) REFERENCES orders(id) ON DELETE CASCADE,\n    FOREIGN KEY (product_id) REFERENCES products(id)\n);\n\n-- Index for order items queries\nCREATE INDEX idx_order_items_order_id ON order_items(order_id);\n\n-- Trigger to update order total when items change\nCREATE TRIGGER update_order_total\nAFTER INSERT ON order_items\nBEGIN\n    UPDATE orders\n    SET total_cents = (\n        SELECT SUM(quantity * price_cents)\n        FROM order_items\n        WHERE order_id = NEW.order_id\n    )\n    WHERE id = NEW.order_id;\nEND;\n```\n\n**Schema Design Considerations:**\n\n1. **Foreign Keys**: Maintain referential integrity\n2. **Indexes**: Add indexes for common query patterns\n3. **Constraints**: Use CHECK constraints for data validation\n4. **Triggers**: Consider triggers for computed values\n5. **Normalization**: Follow appropriate normal form\n6. **Timestamps**: Add created_at/updated_at for auditing\n\n### Step 5: Generate Implementation Plan\n\nCreate a comprehensive plan document:\n\n````markdown\n## Implementation Plan: User Orders Feature\n\n### Overview\n\nAdd ability to create and retrieve orders for users.\n\n### Spec Changes Summary\n\n#### OpenAPI Changes\n\n1. **New Endpoints**:\n\n   - GET /api/users/{id}/orders - Retrieve user's orders\n   - POST /api/orders - Create new order\n   - GET /api/orders/{id} - Get order details\n   - PATCH /api/orders/{id} - Update order status\n\n2. **New Schemas**:\n   - Order - Order information\n   - OrderItem - Individual order line items\n   - CreateOrderRequest - Request payload for creating orders\n\n#### Database Schema Changes\n\n1. **New Tables**:\n\n   - orders (id, user_id, total_cents, status, created_at, updated_at)\n   - order_items (id, order_id, product_id, quantity, price_cents)\n\n2. **New Indexes**:\n\n   - idx_orders_user_id\n   - idx_orders_status\n   - idx_orders_created_at\n   - idx_orders_user_status (composite)\n   - idx_order_items_order_id\n\n3. **Triggers**:\n   - update_order_total - Automatically calculate order total\n\n### Code Generation Required\n\nThe **codegen plugin** (e.g., `specforge-generate-rust-sql`) will generate:\n\n1. **Database Models** (from schema):\n\n   - `Order` struct with all fields\n   - `OrderItem` struct with all fields\n   - Type-safe enums for OrderStatus\n\n2. **Type-Safe Queries** (from schema):\n\n   - `get_orders_by_user_id(pool, user_id, page, limit)` -> `Vec<Order>`\n   - `create_order(pool, user_id)` -> `Order`\n   - `add_order_item(pool, order_id, product_id, quantity, price)` -> `OrderItem`\n   - `get_order_with_items(pool, order_id)` -> `OrderWithItems`\n\n3. **API Types** (from OpenAPI):\n   - Request/Response DTOs\n   - Validation schemas\n   - Serialization code\n\n### Handler Implementation\n\nUsing the **backend plugin** (e.g., `specforge-backend-rust-axum`), implement handlers:\n\n#### 1. GET /api/users/{id}/orders\n\n```rust\n// Pseudocode - actual implementation by backend plugin agent\n\npub async fn get_user_orders(\n    State(db): State<DatabasePool>,\n    Path(user_id): Path<i64>,\n    Query(params): Query<PaginationParams>,\n) -> Result<Json<OrdersResponse>, ApiError> {\n    // 1. Validate user exists (use generated query)\n    let user = get_user_by_id(&db, user_id)\n        .await?\n        .ok_or(ApiError::NotFound)?;\n\n    // 2. Get orders with pagination (use generated query)\n    let orders = get_orders_by_user_id(&db, user_id, params.page, params.limit)\n        .await?;\n\n    // 3. Get total count for pagination (use generated query)\n    let total = count_orders_by_user_id(&db, user_id).await?;\n\n    // 4. Build response\n    Ok(Json(OrdersResponse {\n        data: orders,\n        pagination: Pagination {\n            page: params.page,\n            limit: params.limit,\n            total,\n            total_pages: (total + params.limit - 1) / params.limit,\n        },\n    }))\n}\n```\n````\n\n**Complexity**: Simple CRUD with pagination\n**Agent Model**: Haiku (sufficient for generated types + straightforward logic)\n**Estimated Time**: 5 minutes\n\n#### 2. POST /api/orders\n\n```rust\n// More complex - involves transaction\n\npub async fn create_order(\n    State(db): State<DatabasePool>,\n    Json(payload): Json<CreateOrderRequest>,\n) -> Result<Json<Order>, ApiError> {\n    // 1. Start transaction\n    let mut tx = db.begin().await?;\n\n    // 2. Validate user and products exist\n    // 3. Check inventory availability\n    // 4. Create order\n    // 5. Create order items\n    // 6. Commit transaction\n\n    // Implementation by backend handler agent\n}\n```\n\n**Complexity**: Complex - transaction, validation, multiple tables\n**Agent Model**: Sonnet (complex business logic, error handling)\n**Estimated Time**: 15 minutes\n\n### Testing Strategy\n\nGenerate tests using **backend test agent**:\n\n1. **Unit Tests**:\n\n   - Test handler logic with mocked database\n   - Test validation rules\n   - Test error cases\n\n2. **Integration Tests**:\n\n   - Full flow from HTTP request to database\n   - Test transaction rollback\n   - Test pagination logic\n\n3. **Behavior Observation**:\n   - Verify correct SQL queries generated\n   - Check response format matches OpenAPI\n   - Validate error responses\n\n### Migration Strategy\n\n1. **Development**:\n\n   - Run migration locally: `./migrate.sh migrations/003_add_orders.sql`\n   - Verify schema with: `sqlite3 dev.db .schema`\n\n2. **Testing**:\n\n   - Apply migration to test database\n   - Run codegen to verify types\n   - Compile backend to catch errors\n\n3. **Production**:\n   - Backup database before migration\n   - Apply migration with transaction\n   - Verify with health check\n\n### Estimated Complexity\n\n**Overall**: Medium complexity\n\n- 4 new endpoints (2 simple, 2 complex)\n- 2 new database tables\n- Multiple generated types and queries\n- Transaction handling required\n\n**Timeline**:\n\n- Spec updates: 10 minutes\n- Schema migration: 5 minutes\n- Code generation: 2 minutes (automated)\n- Handler implementation: 30 minutes (2 Haiku + 2 Sonnet agents)\n- Testing: 15 minutes\n- **Total**: ~60 minutes\n\n**Cost Estimate**:\n\n- Planning (Sonnet): ~5K tokens\n- Handler agents (2 Haiku): ~10K tokens\n- Handler agents (2 Sonnet): ~30K tokens\n- Test agents (Haiku): ~5K tokens\n- **Total**: ~50K tokens ≈ $0.50\n\n### Dependencies\n\n- Requires `users` table to exist\n- Requires `products` table to exist\n- No external API dependencies\n\n### Risks & Mitigation\n\n1. **Risk**: Race condition in order total calculation\n   **Mitigation**: Use database trigger, test with concurrent requests\n\n2. **Risk**: Inventory overselling\n   **Mitigation**: Use optimistic locking or row-level locks\n\n3. **Risk**: Large order results causing performance issues\n   **Mitigation**: Enforce maximum page size, add query timeouts\n\n```\n\n### Step 6: Present Plan to User\n\nFormat the plan clearly and ask for approval:\n\n```\n\nI've analyzed your feature requirements and created an implementation plan.\n\n**Summary:**\n\n- 4 new API endpoints\n- 2 new database tables with indexes\n- Type-safe code generation from schemas\n- Estimated time: 60 minutes\n- Estimated cost: $0.50 in AI inference\n\n**Next Steps:**\n\n1. Review the proposed spec changes above\n2. If approved, run `/specforge:build` to implement\n3. Run `/specforge:test` to verify everything works\n\nWould you like me to proceed with the build, or would you like to modify the plan first?\n\n```\n\nUse **AskUserQuestion** to get approval:\n- Proceed with build\n- Modify OpenAPI spec\n- Modify database schema\n- Cancel and revise\n\n## Planning Best Practices\n\n### 1. Detailed Business Logic\n\nAlways document business logic in OpenAPI descriptions:\n- Step-by-step algorithm\n- Edge cases and error handling\n- Performance considerations\n- Security requirements\n\n### 2. Schema Design Principles\n\nFollow database best practices:\n- Proper normalization\n- Foreign key constraints\n- Appropriate indexes\n- Data validation constraints\n- Audit timestamps\n\n### 3. Type Safety\n\nEnsure alignment between OpenAPI and database schemas:\n- Matching field names (snake_case DB, camelCase API)\n- Compatible data types\n- Consistent validation rules\n\n### 4. Performance Planning\n\nConsider performance early:\n- Index common query patterns\n- Avoid N+1 queries with JOINs\n- Plan for pagination\n- Set reasonable limits\n\n### 5. Error Handling\n\nDefine comprehensive error responses:\n- HTTP status codes\n- Error message format\n- Recovery suggestions\n- Logging strategy\n\n## Resources\n\n- **OpenAPI Best Practices**: https://learn.openapis.org/best-practices.html\n- **Database Design**: https://www.postgresql.org/docs/current/ddl.html\n- **SQL Indexing**: https://use-the-index-luke.com/\n- **API Design Guide**: https://cloud.google.com/apis/design\n\n## Implementation Notes\n\n- Use the **openapi-expert** skill for spec validation\n- Use the **stack-advisor** skill for technology-specific patterns\n- Consult plugin documentation for framework-specific conventions\n- Validate compatibility between proposed changes and existing code\n```\n",
        "specforge/commands/ship.md": "---\nname: ship\ndescription: Deployment preparation - build production artifacts, run security checks, and generate deployment configs\n---\n\n# SpecForge Ship Command\n\nPrepare your SpecForge project for deployment by building production artifacts, running security checks, optimizing Docker images, and generating deployment configurations.\n\n## Overview\n\nThe ship command orchestrates the final steps before deployment:\n\n1. **Pre-deployment Validation** - Ensure everything passes validation and tests\n2. **Production Build** - Build optimized production artifacts\n3. **Security Scanning** - Scan for vulnerabilities\n4. **Docker Optimization** - Build optimized production Docker images\n5. **Deployment Config Generation** - Generate deployment manifests\n6. **Deployment Readiness Report** - Final checklist and next steps\n\n## Ship Orchestration Flow\n\n```\n┌─────────────────────────────────────────────┐\n│  /specforge:ship                             │\n└─────────────────┬───────────────────────────┘\n                  │\n    ┌─────────────┼─────────────┬──────────────┬───────────────┐\n    ▼             ▼             ▼              ▼               ▼\n┌────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────────┐\n│ Pre-   │  │ Build    │  │ Security │  │ Docker   │  │ Deploy      │\n│ flight │  │ Prod     │  │ Scan     │  │ Optimize │  │ Config      │\n└────┬───┘  └─────┬────┘  └─────┬────┘  └─────┬────┘  └──────┬──────┘\n     │            │             │             │              │\n     └────────────┴─────────────┴─────────────┴──────────────┘\n                                 │\n                                 ▼\n                      ┌────────────────────┐\n                      │ Readiness Report   │\n                      └────────────────────┘\n```\n\n## Step 1: Pre-deployment Validation\n\nRun full validation and tests to ensure everything is ready:\n\n```bash\n# Run validation\n/specforge:validate --full\n\n# Run full test suite\n/specforge:test --full\n```\n\nIf validation or tests fail, abort the ship process and report errors to the user.\n\n**Checks:**\n\n- ✓ All validation levels pass (spec, schema, codegen, compile, runtime)\n- ✓ All tests pass (unit, integration, contract)\n- ✓ No pending migrations\n- ✓ No uncommitted changes (warn if dirty git status)\n- ✓ Version is tagged (recommend if not)\n\n## Step 2: Production Build\n\nBuild optimized production artifacts using backend plugin:\n\n```bash\n# Read backend plugin from CLAUDE.md\nBACKEND_PLUGIN=$(grep \"backend:\" CLAUDE.md | cut -d: -f2 | tr -d ' ')\n```\n\nInvoke backend plugin's production build:\n\n```\nUse {backend-plugin}/docker-expert skill with:\n- action: \"build-production\"\n- project_path: \"backend/\"\n- optimization: \"release\"\n- output_path: \"dist/\"\n```\n\n**Technology-Specific Builds:**\n\n**Rust:**\n\n```bash\ncd backend && cargo build --release\nstrip target/release/api  # Strip debug symbols\n```\n\n**TypeScript/Node:**\n\n```bash\ncd backend && npm run build\nnpm prune --production  # Remove dev dependencies\n```\n\n**Python:**\n\n```bash\ncd backend && pip install --no-dev\npython -m compileall .  # Compile to bytecode\n```\n\n**Go:**\n\n```bash\ncd backend && CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags '-s -w' -o dist/api\n```\n\n**Optimization Checks:**\n\n- ✓ Debug symbols removed\n- ✓ Dev dependencies excluded\n- ✓ Build artifacts optimized\n- ✓ Static files bundled\n- ✓ Environment variables externalized\n\n## Step 3: Security Scanning\n\nRun security scans on dependencies and container images:\n\n### Dependency Scanning\n\nScan project dependencies for known vulnerabilities:\n\n**Rust:**\n\n```bash\ncargo audit\n```\n\n**Node/TypeScript:**\n\n```bash\nnpm audit --production\n# Or use snyk\nnpx snyk test\n```\n\n**Python:**\n\n```bash\npip-audit\n# Or use safety\nsafety check\n```\n\n**Go:**\n\n```bash\ngo list -json -m all | nancy sleuth\n```\n\n### Container Image Scanning\n\nUse [Trivy](https://aquasecurity.github.io/trivy/) to scan Docker images:\n\n```bash\n# Build production image first\ndocker build -t my-api:latest -f backend/Dockerfile.prod backend/\n\n# Scan for vulnerabilities\ntrivy image --severity HIGH,CRITICAL my-api:latest\n\n# Scan for misconfigurations\ntrivy config docker/docker-compose.yml\n```\n\n**Security Checks:**\n\n- ✓ No critical vulnerabilities in dependencies\n- ✓ No high-severity CVEs in base images\n- ✓ No secrets in Docker images\n- ✓ Non-root user in containers\n- ✓ Minimal base image used\n- ✓ No unnecessary packages\n\nIf critical vulnerabilities found, report and recommend fixes:\n\n```\n⚠ Security Issues Found:\n\n1. Critical: CVE-2024-1234 in openssl 1.1.1\n   Fix: Update Dockerfile to use openssl 3.0\n\n2. High: Secrets detected in .env file\n   Fix: Remove .env from Docker context, use environment variables\n\nRun `trivy image --severity CRITICAL my-api:latest` for details.\n```\n\n## Step 4: Docker Optimization\n\nBuild optimized multi-stage Docker images:\n\n```\nUse {backend-plugin}/docker-expert skill with:\n- action: \"build-optimized-image\"\n- dockerfile: \"backend/Dockerfile.prod\"\n- image_tag: \"{project-name}:{version}\"\n- optimization: \"multi-stage\"\n```\n\n**Optimized Dockerfile Pattern (Rust Example):**\n\n```dockerfile\n# Stage 1: Build\nFROM rust:1.75 AS builder\nWORKDIR /app\nCOPY Cargo.toml Cargo.lock ./\nCOPY src ./src\nRUN cargo build --release\nRUN strip target/release/api\n\n# Stage 2: Runtime\nFROM debian:bookworm-slim\nRUN apt-get update && apt-get install -y \\\n    ca-certificates \\\n    libssl3 \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Create non-root user\nRUN useradd -m -u 1000 app\nUSER app\nWORKDIR /app\n\n# Copy binary from builder\nCOPY --from=builder /app/target/release/api ./\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost:3000/health || exit 1\n\nEXPOSE 3000\nCMD [\"./api\"]\n```\n\n**Optimization Metrics:**\n\n- Image size reduction (target: <100MB for compiled languages)\n- Build time\n- Layer caching efficiency\n- Security score (no vulnerabilities)\n\n### Database Docker Optimization\n\nPrepare production database configuration:\n\n```\nUse {database-plugin}/docker-expert skill with:\n- action: \"build-production-config\"\n- compose_file: \"docker/docker-compose.prod.yml\"\n```\n\n**Production Database Config (PostgreSQL Example):**\n\n```yaml\nversion: \"3.8\"\n\nservices:\n  db:\n    image: postgres:15-alpine\n    restart: unless-stopped\n    environment:\n      POSTGRES_DB: ${DB_NAME}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_PASSWORD_FILE: /run/secrets/db_password\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    secrets:\n      - db_password\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${DB_USER}\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    networks:\n      - backend\n\n  api:\n    image: my-api:${VERSION:-latest}\n    restart: unless-stopped\n    depends_on:\n      db:\n        condition: service_healthy\n    environment:\n      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@db:5432/${DB_NAME}\n      RUST_LOG: info\n    ports:\n      - \"3000:3000\"\n    networks:\n      - backend\n      - frontend\n\nsecrets:\n  db_password:\n    external: true\n\nvolumes:\n  postgres-data:\n\nnetworks:\n  backend:\n  frontend:\n```\n\n## Step 5: Deployment Configuration Generation\n\nGenerate deployment manifests for various platforms:\n\n### Ask User for Deployment Target\n\nUse AskUserQuestion to ask where they're deploying:\n\n- **Docker Compose** - Simple VPS deployment\n- **Kubernetes** - Scalable cloud deployment\n- **AWS ECS/Fargate** - Managed containers on AWS\n- **Google Cloud Run** - Serverless containers on GCP\n- **Azure Container Instances** - Containers on Azure\n- **Fly.io** - Edge deployment platform\n- **Railway** - Simple cloud platform\n\n### Generate Platform-Specific Configs\n\n#### Docker Compose (VPS Deployment)\n\n```yaml\n# docker-compose.prod.yml\nversion: \"3.8\"\n\nservices:\n  db:\n    image: postgres:15-alpine\n    restart: unless-stopped\n    environment:\n      POSTGRES_DB: ${DB_NAME}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    volumes:\n      - ./data/postgres:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 10s\n\n  api:\n    image: ${REGISTRY_URL}/my-api:${VERSION}\n    restart: unless-stopped\n    depends_on:\n      db:\n        condition: service_healthy\n    environment:\n      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@db:5432/${DB_NAME}\n    ports:\n      - \"80:3000\"\n\n  nginx:\n    image: nginx:alpine\n    restart: unless-stopped\n    ports:\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./ssl:/etc/nginx/ssl\n    depends_on:\n      - api\n```\n\n#### Kubernetes Deployment\n\nGenerate Kubernetes manifests:\n\n```yaml\n# k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\n  labels:\n    app: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n        - name: api\n          image: ${REGISTRY_URL}/my-api:${VERSION}\n          ports:\n            - containerPort: 3000\n          env:\n            - name: DATABASE_URL\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: url\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 3000\n            initialDelaySeconds: 10\n            periodSeconds: 10\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 3000\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          resources:\n            requests:\n              memory: \"128Mi\"\n              cpu: \"100m\"\n            limits:\n              memory: \"512Mi\"\n              cpu: \"500m\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api\nspec:\n  selector:\n    app: api\n  ports:\n    - port: 80\n      targetPort: 3000\n  type: LoadBalancer\n```\n\n#### AWS ECS Task Definition\n\n```json\n{\n  \"family\": \"my-api\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"256\",\n  \"memory\": \"512\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"api\",\n      \"image\": \"${REGISTRY_URL}/my-api:${VERSION}\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 3000,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"DATABASE_URL\",\n          \"value\": \"${DATABASE_URL}\"\n        }\n      ],\n      \"healthCheck\": {\n        \"command\": [\n          \"CMD-SHELL\",\n          \"curl -f http://localhost:3000/health || exit 1\"\n        ],\n        \"interval\": 30,\n        \"timeout\": 5,\n        \"retries\": 3\n      },\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/my-api\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      }\n    }\n  ]\n}\n```\n\n#### Fly.io Configuration\n\n```toml\n# fly.toml\napp = \"my-api\"\nprimary_region = \"iad\"\n\n[build]\n  image = \"${REGISTRY_URL}/my-api:${VERSION}\"\n\n[env]\n  PORT = \"3000\"\n\n[http_service]\n  internal_port = 3000\n  force_https = true\n  auto_stop_machines = true\n  auto_start_machines = true\n  min_machines_running = 0\n\n[[http_service.checks]]\n  interval = \"10s\"\n  timeout = \"2s\"\n  grace_period = \"5s\"\n  method = \"GET\"\n  path = \"/health\"\n```\n\n### Generate Environment Variable Template\n\nCreate `.env.production.example`:\n\n```bash\n# Database Configuration\nDATABASE_URL=postgresql://user:password@db:5432/dbname\n\n# API Configuration\nAPI_PORT=3000\nRUST_LOG=info\n\n# Security\nJWT_SECRET=<generate-secure-secret>\nAPI_KEY=<generate-api-key>\n\n# External Services (if needed)\nREDIS_URL=redis://redis:6379\nS3_BUCKET=my-bucket\nAWS_REGION=us-east-1\n\n# Monitoring (optional)\nSENTRY_DSN=<your-sentry-dsn>\n```\n\n## Step 6: Generate Deployment Readiness Report\n\nCreate a comprehensive readiness report:\n\n````markdown\n# Deployment Readiness Report\n\n**Project**: my-api\n**Version**: 1.0.0\n**Date**: 2025-01-15\n**Target**: Production\n\n---\n\n## ✓ Pre-flight Checks\n\n- ✓ All validation levels passed\n- ✓ All tests passed (65/65)\n- ✓ No pending migrations\n- ✓ Git tag: v1.0.0\n- ✓ No uncommitted changes\n\n## ✓ Production Build\n\n- ✓ Backend built (release mode)\n- ✓ Binary size: 12.5 MB\n- ✓ Build time: 2m 34s\n- ✓ Static files bundled\n- ✓ Dependencies optimized\n\n## ✓ Security Scan\n\n- ✓ No critical vulnerabilities\n- ✓ No high-severity CVEs\n- ✓ Container scan passed\n- ⚠ 1 medium-severity issue (non-blocking)\n  - Update recommended: rust 1.75 → 1.76\n\n## ✓ Docker Images\n\n**Backend Image**: my-api:1.0.0\n\n- Base: debian:bookworm-slim\n- Size: 45 MB (reduced from 850 MB)\n- Layers: 8\n- Non-root user: ✓\n- Health check: ✓\n- Security scan: PASS\n\n**Database Image**: postgres:15-alpine\n\n- Size: 238 MB\n- Security scan: PASS\n\n## ✓ Deployment Configuration\n\nGenerated configs for:\n\n- ✓ Docker Compose (docker-compose.prod.yml)\n- ✓ Kubernetes (k8s/\\*.yaml)\n- ✓ Environment variables (.env.production.example)\n\n## Next Steps\n\n### 1. Push Docker Images\n\n```bash\n# Tag image\ndocker tag my-api:1.0.0 ${REGISTRY_URL}/my-api:1.0.0\ndocker tag my-api:1.0.0 ${REGISTRY_URL}/my-api:latest\n\n# Push to registry\ndocker push ${REGISTRY_URL}/my-api:1.0.0\ndocker push ${REGISTRY_URL}/my-api:latest\n```\n````\n\n### 2. Set Environment Variables\n\nCopy `.env.production.example` to `.env.production` and fill in:\n\n- Database credentials\n- JWT secret (generate with: `openssl rand -base64 32`)\n- API keys\n- External service credentials\n\n### 3. Deploy\n\n**Docker Compose (VPS):**\n\n```bash\n# Copy files to server\nscp docker-compose.prod.yml .env.production user@server:/app/\n\n# SSH to server\nssh user@server\n\n# Start services\ncd /app\ndocker-compose -f docker-compose.prod.yml up -d\n\n# Run migrations\ndocker-compose exec api ./api migrate\n\n# Check health\ncurl https://your-domain.com/health\n```\n\n**Kubernetes:**\n\n```bash\n# Apply manifests\nkubectl apply -f k8s/\n\n# Check deployment\nkubectl rollout status deployment/api\n\n# Check pods\nkubectl get pods\n\n# Check logs\nkubectl logs -f deployment/api\n```\n\n### 4. Post-Deployment Verification\n\n- [ ] Health check endpoint responds: `curl https://your-domain.com/health`\n- [ ] API endpoints respond correctly\n- [ ] Database migrations applied\n- [ ] Monitoring/logging configured\n- [ ] SSL/TLS certificates valid\n- [ ] DNS configured correctly\n\n### 5. Monitoring & Observability (Recommended)\n\nSet up monitoring:\n\n- [ ] Health check monitoring (UptimeRobot, Pingdom)\n- [ ] Error tracking (Sentry, Rollbar)\n- [ ] Performance monitoring (New Relic, DataDog)\n- [ ] Log aggregation (CloudWatch, Papertrail)\n- [ ] Metrics (Prometheus + Grafana)\n\n## Rollback Plan\n\nIf deployment fails:\n\n```bash\n# Docker Compose\ndocker-compose -f docker-compose.prod.yml down\ndocker-compose -f docker-compose.prod.yml up -d my-api:0.9.0\n\n# Kubernetes\nkubectl rollout undo deployment/api\n```\n\n## Support\n\n- **Documentation**: https://github.com/your-org/my-api\n- **Issues**: https://github.com/your-org/my-api/issues\n- **OpenAPI Spec**: https://your-domain.com/api/docs\n\n---\n\n**Status**: ✅ READY TO SHIP\n\nAll checks passed. Your application is ready for production deployment!\n\n```\n\n## Ship Summary\n\nDisplay concise summary to user:\n\n```\n\n✅ Ship Complete!\n\nPre-flight: ✓ All checks passed\nBuild: ✓ Optimized production artifacts\nSecurity: ✓ No critical vulnerabilities\nDocker: ✓ Images built and optimized (45 MB)\nDeploy: ✓ Configs generated\n\n📦 Docker Image: my-api:1.0.0 (45 MB)\n📝 Deployment configs generated in ./deploy/\n\nNext Steps:\n\n1. Push images: docker push ${REGISTRY_URL}/my-api:1.0.0\n2. Configure environment: cp .env.production.example .env.production\n3. Deploy: docker-compose -f docker-compose.prod.yml up -d\n4. Verify: curl https://your-domain.com/health\n\nFull report: ./deploy/DEPLOYMENT_REPORT.md\n\n```\n\n## Error Handling\n\n### Pre-flight Validation Fails\n\nIf validation or tests fail, abort ship and show errors:\n\n```\n\n❌ Ship Aborted\n\nPre-flight validation failed:\n\nTests: ✗ 3 tests failing\n\n- test_create_order_validation\n- test_get_user_not_found\n- test_pagination\n\nFix these issues and run /specforge:ship again.\n\nRun /specforge:test for details.\n\n```\n\n### Security Vulnerabilities Found\n\nIf critical vulnerabilities found, warn and recommend fixes:\n\n```\n\n⚠️ Critical Security Issues Found\n\n2 critical vulnerabilities detected:\n\n1. CVE-2024-1234 in openssl 1.1.1 (CRITICAL)\n   Fix: Update base image to use openssl 3.0\n\n2. Secrets detected in Docker image (HIGH)\n   Fix: Remove .env from Docker context\n\nRecommendation: Fix these issues before deploying to production.\n\nContinue anyway? (not recommended)\n\n```\n\n### Build Failures\n\nIf production build fails:\n\n```\n\n❌ Production Build Failed\n\nError: Compilation failed in release mode\n--> src/handlers/orders.rs:45:12\n|\n45 | let total = order.items.sum();\n| ^^^^^^ method not found\n\nFix the compilation error and run /specforge:ship again.\n\n````\n\n## Platform-Specific Guidance\n\nProvide additional guidance based on deployment target:\n\n### Docker Compose (VPS)\n\n```markdown\n## VPS Deployment Guide\n\n1. **Provision a VPS** (DigitalOcean, Linode, Vultr)\n   - Recommended: 2 vCPU, 4GB RAM, 50GB SSD\n   - OS: Ubuntu 22.04 LTS\n\n2. **Install Docker & Docker Compose**\n   ```bash\n   curl -fsSL https://get.docker.com | sh\n   sudo usermod -aG docker $USER\n````\n\n3. **Set up reverse proxy (Nginx)**\n\n   - SSL with Let's Encrypt (Certbot)\n   - Rate limiting\n   - Gzip compression\n\n4. **Configure firewall**\n   ```bash\n   ufw allow 22/tcp\n   ufw allow 80/tcp\n   ufw allow 443/tcp\n   ufw enable\n   ```\n\n````\n\n### Kubernetes\n\n```markdown\n## Kubernetes Deployment Guide\n\n1. **Set up cluster** (EKS, GKE, AKS, or self-hosted)\n\n2. **Configure kubectl**\n   ```bash\n   kubectl config use-context my-cluster\n````\n\n3. **Create namespace**\n\n   ```bash\n   kubectl create namespace my-api\n   ```\n\n4. **Create secrets**\n\n   ```bash\n   kubectl create secret generic db-credentials \\\n     --from-literal=url='postgresql://...' \\\n     -n my-api\n   ```\n\n5. **Apply manifests**\n\n   ```bash\n   kubectl apply -f k8s/ -n my-api\n   ```\n\n6. **Set up ingress** (Nginx Ingress, Traefik, or cloud load balancer)\n\n````\n\n## Cleanup\n\nOptionally clean up development artifacts:\n\n```bash\n# Remove development containers\ndocker-compose down -v\n\n# Clean build artifacts\ncd backend && cargo clean\n\n# Remove node_modules (if applicable)\nrm -rf node_modules\n\n# Prune Docker system\ndocker system prune -af\n````\n\n## Resources\n\n- **Docker Best Practices**: https://docs.docker.com/develop/dev-best-practices/\n- **Kubernetes Best Practices**: https://kubernetes.io/docs/concepts/configuration/overview/\n- **Trivy Security Scanner**: https://aquasecurity.github.io/trivy/\n- **Container Security**: https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html\n- **Zero-Downtime Deployments**: https://martinfowler.com/bliki/BlueGreenDeployment.html\n\n---\n\n**The ship command ensures your SpecForge project is production-ready, secure, and optimized for deployment.**\n",
        "specforge/commands/sync.md": "---\nname: sync\ndescription: Sync codebase after spec changes (OpenAPI + DB schema)\n---\n\n# SpecForge Sync Command\n\nSynchronize your codebase after manual changes to OpenAPI spec or database schema. This command detects changes and regenerates code to keep everything in sync.\n\n## Overview\n\nThe sync command is useful when:\n\n1. You've manually edited `spec/openapi.yaml`\n2. You've added/modified database migrations\n3. You've pulled changes from version control that include spec updates\n4. You need to regenerate code after resolving merge conflicts\n\n**Sync orchestrates:**\n\n- Detecting spec changes since last sync\n- Validating updated specifications\n- Regenerating type-safe code from schemas\n- Identifying affected handlers that need updates\n- Running tests to verify everything still works\n\n## Sync Process\n\n### Step 1: Detect Changes\n\nIdentify what has changed since the last successful build:\n\n```bash\n# Check for OpenAPI spec changes\nif [ -f spec/openapi.yaml ]; then\n    echo \"Checking OpenAPI spec for changes...\"\n    # Compare with last known state (stored in .specforge/last-sync.json)\nfi\n\n# Check for new database migrations\necho \"Checking for new migrations...\"\nls -t migrations/*.sql | head -5\n\n# Check for schema changes in existing migrations\ngit diff HEAD~1 migrations/ 2>/dev/null || echo \"Not a git repository or no previous commits\"\n```\n\nCategorize changes:\n\n- **OpenAPI only**: New/modified endpoints, schemas, examples\n- **Database only**: New migrations, schema alterations\n- **Both**: Coordinated changes to API and database\n\n### Step 2: Validate Updated Specifications\n\n#### Validate OpenAPI Spec\n\nUse the **openapi-expert** skill:\n\n```\nInvoke openapi-expert skill with:\n- action: \"validate-spec\"\n- spec_path: \"spec/openapi.yaml\"\n```\n\nValidation checks:\n\n- Valid OpenAPI 3.x syntax\n- All $ref references resolve\n- Request/response schemas are valid\n- Examples match schemas\n- No duplicate operation IDs\n- Descriptions include business logic details\n\n#### Validate Database Schema\n\nRead SpecForge configuration to identify database plugin:\n\n```bash\ngrep \"database:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nUse database plugin's validation skill:\n\n```\nInvoke {database-plugin}/migrations-expert skill with:\n- action: \"validate-migrations\"\n- migrations_dir: \"migrations/\"\n```\n\nValidation checks:\n\n- Migration files are numbered sequentially\n- No syntax errors in SQL\n- Foreign keys reference existing tables\n- Indexes are properly defined\n- No breaking changes (unless acknowledged)\n\n### Step 3: Apply Database Migrations\n\nIf new migrations are detected, apply them:\n\n```\nInvoke {database-plugin}/migrations-expert skill with:\n- action: \"apply-migrations\"\n- migrations_dir: \"migrations/\"\n- database_url: from environment or docker-compose\n```\n\nSteps:\n\n1. List pending migrations\n2. Show what will be applied\n3. Ask user for confirmation\n4. Backup database (if production)\n5. Apply migrations in transaction\n6. Verify schema state\n7. Handle errors with rollback\n\n**Interactive Confirmation:**\n\nUse **AskUserQuestion** to confirm migration:\n\n```\nFound 2 new migrations:\n  - 003_add_orders_table.sql\n  - 004_add_order_items_table.sql\n\nThese migrations will:\n  - Create orders table with indexes\n  - Create order_items table with foreign keys\n  - Add trigger for order total calculation\n\nApply these migrations?\n```\n\nOptions:\n\n- Apply migrations now\n- Review migrations first\n- Skip migrations (code generation only)\n- Cancel sync\n\n### Step 4: Regenerate Code from Schemas\n\n#### Database Code Generation\n\nIdentify codegen plugin from CLAUDE.md:\n\n```bash\ngrep \"codegen:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nRun codegen pipeline:\n\n```\nInvoke {codegen-plugin}/codegen-expert skill with:\n- action: \"generate-from-schema\"\n- schema_dir: \"migrations/\"\n- output_dir: \"{backend-path}/src/generated/db\"\n- database_url: from environment\n- force: true  # Regenerate even if files exist\n```\n\nThis regenerates:\n\n- Database models (structs, classes, types)\n- Type-safe query functions\n- Schema types and enums\n- Database client code\n\n#### OpenAPI Type Generation\n\nIdentify backend plugin from CLAUDE.md:\n\n```bash\ngrep \"backend:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\nGenerate API types:\n\n```\nInvoke {backend-plugin}/openapi-types-expert skill with:\n- action: \"generate-types\"\n- spec: \"spec/openapi.yaml\"\n- output: \"{backend-path}/src/generated/api\"\n- force: true  # Regenerate even if files exist\n```\n\nThis regenerates:\n\n- Request/response DTOs\n- Validation schemas\n- Serialization/deserialization code\n- API client types\n\n#### Frontend Client Generation (if applicable)\n\nIf frontend plugin is configured:\n\n```bash\ngrep \"frontend:\" CLAUDE.md | cut -d: -f2 | tr -d ' '\n```\n\n```\nInvoke {frontend-plugin}/codegen-expert skill with:\n- action: \"generate-client\"\n- spec: \"spec/openapi.yaml\"\n- output: \"{frontend-path}/src/api\"\n- force: true\n```\n\n### Step 5: Identify Affected Handlers\n\nAnalyze which handlers are affected by spec changes:\n\n```bash\n# Compare current spec with previous version\n# Identify new, modified, or deleted endpoints\n\n# For each modified endpoint:\n# - Check if handler exists\n# - Verify handler signature matches new types\n# - Flag for review/reimplementation\n```\n\n**Report to user:**\n\n```markdown\n## Sync Summary\n\n### Code Regenerated\n\n- ✓ Database models: 15 types regenerated\n- ✓ Database queries: 32 functions regenerated\n- ✓ API types: 8 request/response types regenerated\n- ✓ Frontend client: API client regenerated\n\n### Handlers Affected\n\n**Needs Update:**\n\n- `GET /api/users/{id}/orders` - Response type changed\n- `POST /api/orders` - Request validation rules changed\n\n**Still Valid:**\n\n- `GET /api/users` - No changes\n- `POST /api/users` - No changes\n\n**New (Not Implemented):**\n\n- `PATCH /api/orders/{id}` - New endpoint, needs implementation\n\n### Next Steps\n\n1. Review affected handlers above\n2. Update handlers with new types: `/specforge:build` (will only rebuild affected handlers)\n3. Run tests: `/specforge:test`\n```\n\n### Step 6: Compile and Test\n\nVerify that generated code compiles:\n\n```bash\n# Backend compilation (example for Rust)\ncd backend && cargo check 2>&1\n\n# If TypeScript backend\ncd backend && npm run type-check 2>&1\n\n# If Python backend\ncd backend && mypy . 2>&1\n```\n\nIf compilation fails:\n\n1. Collect error messages\n2. Invoke **{codegen-plugin}/diagnostics-expert** skill\n3. Fix schema/type mismatches\n4. Regenerate code\n5. Retry compilation\n\n**Run existing tests:**\n\n```bash\n# Run backend tests\ncd backend && cargo test 2>&1  # or npm test, pytest, etc.\n```\n\nReport test results:\n\n```\n## Test Results\n\n- ✓ 127 tests passed\n- ✗ 3 tests failed (affected by schema changes)\n- ⚠ 2 tests skipped (handlers not implemented)\n\nFailed tests:\n  - test_get_user_orders - Response type mismatch\n  - test_create_order - Request validation changed\n  - test_order_total_calculation - New trigger behavior\n```\n\n### Step 7: Save Sync State\n\nUpdate sync state file:\n\n```json\n// .specforge/last-sync.json\n{\n  \"timestamp\": \"2025-01-15T10:30:00Z\",\n  \"openapi_spec_hash\": \"sha256:abc123...\",\n  \"migrations_applied\": [\n    \"001_init.sql\",\n    \"002_add_users.sql\",\n    \"003_add_orders.sql\"\n  ],\n  \"generated_files\": [\n    \"backend/src/generated/db/models.rs\",\n    \"backend/src/generated/db/queries.rs\",\n    \"backend/src/generated/api/types.rs\",\n    \"frontend/src/api/client.ts\"\n  ],\n  \"handlers_affected\": [\"GET /api/users/{id}/orders\", \"POST /api/orders\"],\n  \"tests_status\": {\n    \"passed\": 127,\n    \"failed\": 3,\n    \"skipped\": 2\n  }\n}\n```\n\n### Step 8: Present Summary and Next Steps\n\nShow user what was synced and what needs attention:\n\n```markdown\n## SpecForge Sync Complete\n\n**Changes Detected:**\n\n- 1 new database migration applied\n- OpenAPI spec updated (3 endpoints modified)\n\n**Code Regenerated:**\n\n- ✓ Database models and queries\n- ✓ API request/response types\n- ✓ Frontend API client\n\n**Action Required:**\n\n1. **Update Handlers** (3 affected):\n\n   - GET /api/users/{id}/orders - Response type changed\n   - POST /api/orders - Request validation changed\n   - PATCH /api/orders/{id} - New endpoint\n\n   Run: `/specforge:build` to update/implement handlers\n\n2. **Fix Failing Tests** (3 tests):\n\n   Run: `/specforge:test` after handler updates\n\n**Next Command:**\n\n`/specforge:build` - Implement/update affected handlers\n```\n\nUse **AskUserQuestion** to get next action:\n\n- Update affected handlers now (`/specforge:build`)\n- Run validation first (`/specforge:validate`)\n- Review changes manually\n- Skip for now\n\n## Use Cases\n\n### Use Case 1: After Pulling Changes\n\n```bash\n# Teammate added new endpoints to OpenAPI spec\ngit pull origin main\n\n# Sync to regenerate code\n/specforge:sync\n```\n\n### Use Case 2: After Manual Spec Editing\n\n```yaml\n# You manually edited spec/openapi.yaml\n# Added new endpoint: GET /api/products\npaths:\n  /api/products:\n    get:\n      summary: List products\n      # ... endpoint definition\n\n# Sync to generate types and scaffold handler\n/specforge:sync\n```\n\n### Use Case 3: After Database Migration\n\n```bash\n# You created a new migration file\ncat > migrations/005_add_products.sql <<EOF\nCREATE TABLE products (\n    id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    price_cents INTEGER NOT NULL\n);\nEOF\n\n# Sync to apply migration and generate types\n/specforge:sync\n```\n\n### Use Case 4: After Merge Conflict Resolution\n\n```bash\n# Resolved conflicts in spec/openapi.yaml and migrations/\ngit merge feature-branch\n# ... resolved conflicts ...\ngit add spec/openapi.yaml migrations/\ngit commit -m \"Merge feature-branch\"\n\n# Sync to regenerate everything\n/specforge:sync\n```\n\n## Sync vs Build\n\n**When to use `/specforge:sync`:**\n\n- Specs changed externally (git pull, manual edit, merge)\n- Need to regenerate code from updated schemas\n- Want to detect what changed without full rebuild\n\n**When to use `/specforge:build`:**\n\n- Implementing new features from scratch\n- Following a `/specforge:plan`\n- Applying spec changes AND implementing handlers in one go\n\n**Relationship:**\n\n```\n/specforge:plan   →  Propose spec changes\n                  ↓\n/specforge:build  →  Apply specs + implement handlers + test\n                  ↓\n/specforge:sync   →  Regenerate after external spec changes\n```\n\n## Configuration\n\nSync behavior can be customized in CLAUDE.md:\n\n```markdown\n## SpecForge Configuration\n\n- backend: rust-axum\n- database: sqlite\n- codegen: rust-sql\n- frontend: react-tanstack\n\n### Sync Options\n\n- auto_apply_migrations: false # Require confirmation\n- regenerate_on_conflict: true # Auto-regenerate on file conflicts\n- run_tests_after_sync: true # Run tests automatically\n- backup_before_migration: true # Backup DB before applying migrations\n```\n\n## Safety Features\n\n### 1. Backup Before Migration\n\nAlways backup database before applying migrations (unless in development mode):\n\n```bash\n# Production/staging: backup first\nif [ \"$ENV\" != \"development\" ]; then\n    echo \"Backing up database...\"\n    # Use database plugin backup skill\nfi\n```\n\n### 2. Dry Run Mode\n\nShow what would happen without making changes:\n\n```\n/specforge:sync --dry-run\n\nWould apply:\n  - migrations/003_add_orders.sql\n  - migrations/004_add_order_items.sql\n\nWould regenerate:\n  - backend/src/generated/db/*\n  - backend/src/generated/api/*\n  - frontend/src/api/*\n\nNo changes made (dry run).\n```\n\n### 3. Rollback on Error\n\nIf migration or code generation fails, rollback changes:\n\n```\nError applying migration 003_add_orders.sql:\n  - Foreign key constraint failed\n\nRolling back migration...\nDatabase restored to previous state.\n\nSync failed. Please fix migration and try again.\n```\n\n## Troubleshooting\n\n### Issue: \"Migration out of order\"\n\n```\nError: Migration 005_xxx.sql has timestamp before 004_xxx.sql\n```\n\n**Solution:** Rename migration file with correct sequential number.\n\n### Issue: \"Generated code conflicts with manual changes\"\n\n```\nWarning: backend/src/generated/db/models.rs has been manually modified\n```\n\n**Solution:**\n\n- Move custom code out of `generated/` directory\n- Use partial classes/extensions (if language supports it)\n- Accept regeneration and reapply manual changes\n\n### Issue: \"OpenAPI spec invalid after sync\"\n\n```\nError: Invalid $ref in spec/openapi.yaml:\n  - #/components/schemas/Order not found\n```\n\n**Solution:** Fix OpenAPI spec, then re-run sync.\n\n### Issue: \"Compilation fails after sync\"\n\n```\nError: Type mismatch in handler\n  - Expected: OrderResponse\n  - Found: Order\n```\n\n**Solution:** Run `/specforge:build` to update handlers with new types.\n\n## Resources\n\n- **OpenAPI Spec Validation**: https://redocly.com/docs/cli/\n- **Database Migrations Best Practices**: https://www.postgresql.org/docs/current/ddl-alter.html\n- **Git Conflict Resolution**: https://git-scm.com/book/en/v2/Git-Tools-Advanced-Merging\n- **Code Generation Patterns**: https://openapi-generator.tech/\n\n## Related Commands\n\n- `/specforge:plan` - Generate implementation plan for new features\n- `/specforge:build` - Full build pipeline (specs + handlers + tests)\n- `/specforge:validate` - Validate specs and code without regenerating\n- `/specforge:test` - Run test suite\n",
        "specforge/commands/test.md": "---\nname: test\ndescription: Test orchestration with behavior observation and iterative fixing\n---\n\n# SpecForge Test Orchestrator\n\nRun comprehensive tests with behavior observation, diagnose failures, and iterate until all tests pass.\n\n## Overview\n\nThe test orchestrator runs tests at multiple levels and uses behavior observation to diagnose and fix issues:\n\n1. **Unit Tests** - Test individual handlers and functions\n2. **Integration Tests** - Test API endpoints end-to-end\n3. **Contract Tests** - Verify API matches OpenAPI spec\n4. **Behavior Observation** - Monitor test execution to identify issues\n5. **Iterative Fixing** - Diagnose and fix failures automatically\n\n## Test Orchestration Flow\n\n```\n┌─────────────────────────────────────────────┐\n│  /specforge:test                             │\n└─────────────────┬───────────────────────────┘\n                  │\n    ┌─────────────┼─────────────┬──────────────┐\n    ▼             ▼             ▼              ▼\n┌────────┐  ┌──────────┐  ┌──────────┐  ┌─────────────┐\n│ Unit   │  │ Integration│ │ Contract │  │ E2E Tests   │\n│ Tests  │  │ Tests     │  │ Tests    │  │ (optional)  │\n└────┬───┘  └─────┬────┘  └─────┬────┘  └──────┬──────┘\n     │            │             │              │\n     └────────────┴─────────────┴──────────────┘\n                  │\n                  ▼\n       ┌──────────────────────┐\n       │ Behavior Observation │\n       │ & Failure Diagnosis  │\n       └──────────────────────┘\n                  │\n                  ▼\n       ┌──────────────────────┐\n       │ Iterate & Fix        │\n       │ (Max 3 iterations)   │\n       └──────────────────────┘\n```\n\n## Step 1: Read Configuration\n\nExtract the tech stack configuration from CLAUDE.md:\n\n```bash\n# Read SpecForge configuration\ngrep \"SpecForge configuration\" -A 10 CLAUDE.md\n```\n\nExtract:\n\n- Backend plugin\n- Database plugin\n- Codegen plugin\n- Frontend plugin (if applicable)\n\n## Step 2: Unit Tests\n\nRun unit tests for the backend using the backend plugin's testing expert:\n\n```bash\n# Get backend plugin\nBACKEND_PLUGIN=$(grep \"backend:\" CLAUDE.md | cut -d: -f2 | tr -d ' ')\n```\n\nInvoke the backend plugin's test agent:\n\n```\nUse {backend-plugin}/test-agent skill with:\n- action: \"run-unit-tests\"\n- test_dir: \"backend/tests/unit\"\n- coverage: true\n```\n\n**Checks:**\n\n- All unit tests pass\n- Code coverage meets threshold (default: 80%)\n- No test failures or errors\n- Mock setup correct\n- Edge cases covered\n\n**Technology-Specific Commands:**\n\n**Rust:**\n\n```bash\ncd backend && cargo test --lib\ncargo tarpaulin --out Html --output-dir coverage/\n```\n\n**TypeScript/Node:**\n\n```bash\ncd backend && npm test -- --coverage\n```\n\n**Python:**\n\n```bash\ncd backend && pytest tests/unit --cov=src --cov-report=html\n```\n\n**Go:**\n\n```bash\ncd backend && go test ./... -cover -coverprofile=coverage.out\n```\n\n## Step 3: Integration Tests\n\nRun integration tests that test full request-to-response flows:\n\n```\nUse {backend-plugin}/test-agent skill with:\n- action: \"run-integration-tests\"\n- test_dir: \"tests/integration\"\n- database_url: test database URL\n```\n\n**Test Pattern:**\n\nEach endpoint should have integration tests:\n\n```rust\n// Example Rust integration test\n#[tokio::test]\nasync fn test_create_order_success() {\n    // Setup test database\n    let db = setup_test_db().await;\n    let app = create_test_app(db.clone()).await;\n\n    // Create test user\n    let user = create_test_user(&db).await;\n\n    // Test request\n    let response = app\n        .post(\"/api/orders\")\n        .json(&json!({\n            \"items\": [{\"product_id\": 1, \"quantity\": 2}],\n            \"user_id\": user.id\n        }))\n        .send()\n        .await;\n\n    // Assertions\n    assert_eq!(response.status(), StatusCode::CREATED);\n    let order: Order = response.json().await;\n    assert_eq!(order.user_id, user.id);\n    assert_eq!(order.status, OrderStatus::Pending);\n}\n```\n\n**Checks:**\n\n- HTTP status codes correct\n- Response bodies match schemas\n- Database state updated correctly\n- Error handling works\n- Authentication/authorization enforced\n- Edge cases handled\n\n## Step 4: Contract Testing\n\nVerify that the running API matches the OpenAPI specification:\n\n```\nUse openapi-expert skill with:\n- action: \"contract-test\"\n- spec_path: \"spec/openapi.yaml\"\n- api_url: \"http://localhost:3000\"\n- test_report_path: \"test-results/contract-tests.json\"\n```\n\n**Tools:**\n\n- **Schemathesis**: Property-based API testing from OpenAPI spec\n- **Dredd**: HTTP API testing framework\n- **Prism**: Mock server and contract testing\n\n**Schemathesis Example:**\n\n```bash\n# Install schemathesis\npip install schemathesis\n\n# Run contract tests\nschemathesis run spec/openapi.yaml \\\n    --base-url http://localhost:3000 \\\n    --checks all \\\n    --hypothesis-max-examples=50\n```\n\n**Checks:**\n\n- All endpoints respond correctly\n- Response schemas match OpenAPI spec\n- Status codes match spec\n- Required fields present\n- Type validation correct\n- Examples in spec are valid\n\n## Step 5: Behavior Observation & Diagnosis\n\nMonitor test execution to identify patterns of failure:\n\n```javascript\nconst testResults = {\n  unit: { passed: [], failed: [] },\n  integration: { passed: [], failed: [] },\n  contract: { passed: [], failed: [] },\n};\n\n// Collect all failures\nconst allFailures = [\n  ...testResults.unit.failed,\n  ...testResults.integration.failed,\n  ...testResults.contract.failed,\n];\n\nif (allFailures.length > 0) {\n  // Analyze failure patterns\n  const patterns = analyzeFailurePatterns(allFailures);\n\n  // Categorize failures\n  const categories = {\n    typeErrors: [],\n    databaseErrors: [],\n    validationErrors: [],\n    businessLogicErrors: [],\n    networkErrors: [],\n  };\n\n  // Group by category for targeted fixing\n  for (const failure of allFailures) {\n    const category = categorizeFailure(failure);\n    categories[category].push(failure);\n  }\n}\n```\n\n**Common Failure Patterns:**\n\n1. **Type Mismatches**: Generated types don't match database schema\n2. **Query Errors**: SQL queries fail or return wrong data\n3. **Validation Errors**: Request validation fails unexpectedly\n4. **Business Logic Errors**: Handler logic incorrect\n5. **Schema Mismatches**: API responses don't match OpenAPI spec\n\n## Step 6: Iterative Fixing\n\nFor each category of failures, invoke the appropriate diagnostics agent:\n\n```javascript\nconst MAX_ITERATIONS = 3;\nlet iteration = 0;\nlet allTestsPassed = false;\n\nwhile (!allTestsPassed && iteration < MAX_ITERATIONS) {\n  iteration++;\n\n  // Run tests\n  const results = await runAllTests();\n\n  if (results.allPassed) {\n    allTestsPassed = true;\n    break;\n  }\n\n  // Diagnose type/codegen issues\n  if (results.typeErrors.length > 0) {\n    await invokeAgent(`${codegenPlugin}/diagnostics-agent`, {\n      model: \"sonnet\",\n      errors: results.typeErrors,\n      generated_code_path: \"backend/src/generated\",\n      schema_path: \"migrations/\",\n    });\n  }\n\n  // Diagnose handler/business logic issues\n  if (results.businessLogicErrors.length > 0) {\n    await invokeAgent(`${backendPlugin}/handler-agent`, {\n      model: \"sonnet\",\n      errors: results.businessLogicErrors,\n      handlers_path: \"backend/src/handlers\",\n    });\n  }\n\n  // Diagnose database/query issues\n  if (results.databaseErrors.length > 0) {\n    await invokeAgent(`${databasePlugin}/migration-agent`, {\n      model: \"haiku\",\n      errors: results.databaseErrors,\n      migrations_path: \"migrations/\",\n    });\n  }\n\n  // Re-run code generation if schema changed\n  if (schemaChanged) {\n    await regenerateCode();\n  }\n}\n\nif (!allTestsPassed) {\n  throw new Error(\n    `Tests still failing after ${MAX_ITERATIONS} iterations. Manual intervention required.`\n  );\n}\n```\n\n## Step 7: Test Report Generation\n\nGenerate a comprehensive test report:\n\n```markdown\n# SpecForge Test Report\n\n**Project**: my-api\n**Date**: 2025-01-15\n**Duration**: 45s\n**Iterations**: 2\n\n## Summary\n\n- ✓ Unit Tests: 45/45 passed (100%)\n- ✓ Integration Tests: 12/12 passed (100%)\n- ✓ Contract Tests: 8/8 passed (100%)\n- **Total**: 65/65 tests passed\n\n## Coverage\n\n- Line Coverage: 87%\n- Branch Coverage: 82%\n- Function Coverage: 95%\n\n## Test Details\n\n### Unit Tests (45 passed)\n\n- ✓ handlers::users::create_user\n- ✓ handlers::users::get_user\n- ✓ handlers::orders::create_order\n- ✓ handlers::orders::get_orders_by_user\n- ... (41 more)\n\n### Integration Tests (12 passed)\n\n- ✓ POST /api/users - creates user successfully\n- ✓ GET /api/users/:id - returns user\n- ✓ POST /api/orders - creates order\n- ✓ GET /api/users/:id/orders - returns user's orders\n- ... (8 more)\n\n### Contract Tests (8 passed)\n\n- ✓ POST /api/users matches schema\n- ✓ GET /api/users/:id matches schema\n- ✓ POST /api/orders matches schema\n- ... (5 more)\n\n## Iterations\n\n### Iteration 1\n\n- 3 test failures\n- Issues: Type mismatch in Order.created_at (expected DateTime, got String)\n- Fix: Updated sql-gen type overrides\n- Result: Re-ran codegen, 3 tests now pass\n\n### Iteration 2\n\n- All tests passed ✓\n\n## Performance\n\n- Average response time: 45ms\n- Slowest endpoint: GET /api/users/:id/orders (120ms)\n- Database queries: Average 15ms\n\n## Recommendations\n\n1. Add tests for error cases (400, 404, 500)\n2. Add pagination tests for list endpoints\n3. Add authentication/authorization tests\n4. Consider adding load tests for critical endpoints\n```\n\n## Test Modes\n\n### Quick Test (Default)\n\nRun unit and integration tests only (skip contract tests):\n\n```bash\n/specforge:test\n```\n\n### Full Test (CI Mode)\n\nRun all test suites including contract tests:\n\n```bash\n/specforge:test --full\n```\n\n### Specific Test Suite\n\nRun a specific test suite:\n\n```bash\n/specforge:test --suite unit\n/specforge:test --suite integration\n/specforge:test --suite contract\n```\n\n### Watch Mode\n\nRun tests continuously on file changes (development mode):\n\n```bash\n/specforge:test --watch\n```\n\n## Integration with CI/CD\n\nExample GitHub Actions workflow:\n\n```yaml\nname: SpecForge Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: postgres:15\n        env:\n          POSTGRES_PASSWORD: test\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install SpecForge\n        run: |\n          /plugin install specforge\n          /plugin install specforge-backend-rust-axum\n          /plugin install specforge-db-postgresql\n          /plugin install specforge-generate-rust-sql\n\n      - name: Run Tests\n        run: /specforge:test --full\n        env:\n          DATABASE_URL: postgresql://postgres:test@localhost:5432/test\n\n      - name: Upload Coverage\n        uses: codecov/codecov-action@v3\n        with:\n          files: ./coverage/lcov.info\n```\n\n## Testing Best Practices\n\n1. **Write tests during development** - Don't wait until the end\n2. **Test edge cases** - Empty lists, null values, boundary conditions\n3. **Use behavior observation** - Let agents learn from test failures\n4. **Iterate automatically** - Fix common issues without manual intervention\n5. **Maintain high coverage** - Aim for >80% code coverage\n6. **Test error handling** - Ensure proper error responses\n7. **Use contract testing** - Verify API matches OpenAPI spec\n8. **Test database constraints** - Ensure foreign keys, uniqueness work\n9. **Clean test data** - Reset database between tests\n10. **Mock external services** - Don't depend on third-party APIs\n\n## Common Test Failures & Fixes\n\n### Type Mismatch Errors\n\n```\nError: Type mismatch - expected i64, found String\n```\n\n**Fix**: Update codegen type overrides:\n\n```toml\n# sql-gen.toml\n[sql-gen.settings]\ntype_overrides = { \"user_id\" = \"i64\" }\n```\n\nRe-run: `/specforge:build`\n\n### Database Constraint Violations\n\n```\nError: FOREIGN KEY constraint failed\n```\n\n**Fix**: Ensure foreign key references exist in test setup:\n\n```rust\n// Create parent record first\nlet user = create_test_user(&db).await;\n\n// Then create child record\nlet order = create_test_order(&db, user.id).await;\n```\n\n### Schema Validation Failures\n\n```\nError: Response does not match schema - missing required field 'created_at'\n```\n\n**Fix**: Update handler to include all required fields:\n\n```rust\n// Ensure all schema fields are returned\nJson(OrderResponse {\n    id: order.id,\n    user_id: order.user_id,\n    total_cents: order.total_cents,\n    status: order.status,\n    created_at: order.created_at, // Don't forget this!\n})\n```\n\n### Contract Test Failures\n\n```\nError: Endpoint GET /api/users/:id returned 500, expected 200\n```\n\n**Fix**: Check handler error handling:\n\n```rust\n// Add proper error handling\nlet user = get_user_by_id(&db, id)\n    .await?\n    .ok_or(ApiError::NotFound)?;\n\nOk(Json(user))\n```\n\n## Resources\n\n- **Schemathesis**: https://schemathesis.readthedocs.io/\n- **Dredd**: https://dredd.org/\n- **Prism**: https://stoplight.io/open-source/prism\n- **Property-Based Testing**: https://hypothesis.readthedocs.io/\n- **Test Coverage Tools**: https://github.com/marketplace/actions/codecov\n\n## Frontend Testing (Optional)\n\nIf a frontend plugin is configured, run frontend tests:\n\n```\nUse {frontend-plugin}/test-expert skill with:\n- action: \"run-tests\"\n- test_dir: \"frontend/tests\"\n```\n\n**Frontend Test Types:**\n\n- Component tests (unit)\n- Integration tests (React Testing Library, etc.)\n- E2E tests (Playwright, Cypress)\n- Visual regression tests (optional)\n\n**Example E2E Test (Playwright):**\n\n```typescript\ntest(\"create order flow\", async ({ page }) => {\n  // Navigate to app\n  await page.goto(\"http://localhost:5173\");\n\n  // Create order\n  await page.click(\"text=New Order\");\n  await page.fill(\"[name=quantity]\", \"2\");\n  await page.click(\"text=Submit\");\n\n  // Verify order created\n  await expect(page.locator(\".order-list\")).toContainText(\"Order #1\");\n});\n```\n\n---\n\n**Test orchestration ensures your SpecForge project works correctly at all levels - from unit tests to end-to-end workflows.**\n",
        "specforge/commands/validate.md": "---\nname: validate\ndescription: Multi-level validation (spec, schema, code, runtime) for SpecForge projects\n---\n\n# SpecForge Validation Orchestrator\n\nPerform comprehensive validation across all levels of your SpecForge project: OpenAPI spec, database schema, generated code, and runtime behavior.\n\n## Validation Levels\n\n### Level 1: Specification Validation\n\nValidate the OpenAPI specification for correctness and best practices.\n\n```javascript\n// Validate OpenAPI spec\nawait invokeSkill(\"openapi-expert\", {\n  action: \"validate-spec\",\n  spec_path: \"spec/openapi.yaml\",\n});\n```\n\n**Checks:**\n\n- OpenAPI 3.x compliance\n- Schema definitions are complete\n- Required fields present\n- No circular references\n- Examples match schemas\n- Response codes appropriate\n- Security schemes defined\n- Operation IDs unique\n\n**Tools:**\n\n- [Redocly CLI](https://redocly.com/docs/cli/) - `redocly lint spec/openapi.yaml`\n- [Spectral](https://stoplight.io/open-source/spectral) - Custom linting rules\n\n### Level 2: Database Schema Validation\n\nValidate database schema for consistency and best practices.\n\n```javascript\n// Read CLAUDE.md to get database plugin\nconst config = await readCLAUDEmd();\nconst dbPlugin = config.specforge.database;\n\n// Validate database schema\nawait invokeSkill(`specforge-db-${dbPlugin}/migrations-expert`, {\n  action: \"validate-schema\",\n  migrations_dir: \"migrations/\",\n  database_url: process.env.DATABASE_URL,\n});\n```\n\n**Checks:**\n\n- Migration files are sequential\n- No conflicting migrations\n- Foreign key constraints valid\n- Indexes defined appropriately\n- Column types consistent\n- No missing NOT NULL constraints on required fields\n- Timestamps have defaults\n\n**Tools:**\n\n- Database-specific schema validators\n- Migration consistency checks\n\n### Level 3: Code Generation Validation\n\nValidate that code generation succeeded and generated code is correct.\n\n```javascript\n// Read CLAUDE.md to get tech stack\nconst config = await readCLAUDEmd();\nconst backendPlugin = config.specforge.backend;\nconst codegenPlugin = config.specforge.codegen;\n\n// Validate generated code exists\nawait invokeSkill(`${codegenPlugin}/diagnostics-expert`, {\n  action: \"validate-generated-code\",\n  generated_db_path: `${backendPlugin.path}/src/generated/db`,\n  generated_api_path: `${backendPlugin.path}/src/generated/api`,\n  database_schema_path: \"migrations/\",\n});\n```\n\n**Checks:**\n\n- Generated type files exist\n- Generated query functions exist\n- Types match database schema\n- API types match OpenAPI spec\n- No compilation errors\n- No type mismatches\n\n### Level 4: Compilation Validation\n\nEnsure the project compiles successfully.\n\n```javascript\n// Run compilation\nconst config = await readCLAUDEmd();\nconst backendPlugin = config.specforge.backend;\n\nawait invokeSkill(`${backendPlugin}/testing-expert`, {\n  action: \"compile-check\",\n  project_path: backendPlugin.path,\n});\n```\n\n**Checks (Technology-Specific):**\n\n**Rust:**\n\n```bash\ncd backend && cargo check\ncargo clippy -- -D warnings\n```\n\n**TypeScript:**\n\n```bash\ncd backend && npm run type-check\nnpm run lint\n```\n\n**Python:**\n\n```bash\ncd backend && mypy .\npylint src/\n```\n\n**Go:**\n\n```bash\ncd backend && go build ./...\ngo vet ./...\n```\n\n### Level 5: Runtime Validation\n\nValidate runtime behavior through tests and contract testing.\n\n```javascript\n// Run test suite\nawait invokeSkill(\"test-orchestrator\", {\n  action: \"run-all-tests\",\n  project_path: \".\",\n});\n\n// Contract testing against OpenAPI spec\nawait invokeSkill(\"openapi-expert\", {\n  action: \"contract-test\",\n  spec_path: \"spec/openapi.yaml\",\n  api_url: \"http://localhost:3000\",\n});\n```\n\n**Checks:**\n\n- Unit tests pass\n- Integration tests pass\n- API responses match OpenAPI spec\n- Database constraints enforced\n- Error handling correct\n- Performance acceptable\n\n**Tools:**\n\n- [Schemathesis](https://schemathesis.readthedocs.io/) - Property-based API testing\n- [Dredd](https://dredd.org/) - HTTP API testing against spec\n- [Prism](https://stoplight.io/open-source/prism) - Mock server validation\n\n## Validation Orchestration Flow\n\n```\n┌─────────────────────────────────────────────┐\n│  /specforge:validate                         │\n└─────────────────┬───────────────────────────┘\n                  │\n    ┌─────────────┼─────────────┬──────────────┬───────────────┐\n    ▼             ▼             ▼              ▼               ▼\n┌────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────────┐\n│ Spec   │  │ Schema   │  │ Codegen  │  │ Compile  │  │ Runtime     │\n│ Valid. │  │ Valid.   │  │ Valid.   │  │ Valid.   │  │ Valid.      │\n└────┬───┘  └─────┬────┘  └─────┬────┘  └─────┬────┘  └──────┬──────┘\n     │            │             │             │              │\n     └────────────┴─────────────┴─────────────┴──────────────┘\n                                 │\n                                 ▼\n                      ┌────────────────────┐\n                      │ Validation Report  │\n                      └────────────────────┘\n```\n\n## Implementation\n\n### Step 1: Read Configuration\n\n```bash\n# Read CLAUDE.md to get tech stack configuration\ncat CLAUDE.md | grep \"SpecForge configuration\" -A 10\n```\n\nExtract:\n\n- Backend plugin\n- Database plugin\n- Codegen plugin\n- Frontend plugin (if applicable)\n\n### Step 2: Run Validation Levels Sequentially\n\nRun each validation level and collect results:\n\n```javascript\nconst results = {\n  spec: { status: \"pending\", errors: [] },\n  schema: { status: \"pending\", errors: [] },\n  codegen: { status: \"pending\", errors: [] },\n  compile: { status: \"pending\", errors: [] },\n  runtime: { status: \"pending\", errors: [] },\n};\n\n// Level 1: Spec Validation\ntry {\n  await validateSpec();\n  results.spec.status = \"passed\";\n} catch (error) {\n  results.spec.status = \"failed\";\n  results.spec.errors.push(error);\n}\n\n// Level 2: Schema Validation\ntry {\n  await validateSchema();\n  results.schema.status = \"passed\";\n} catch (error) {\n  results.schema.status = \"failed\";\n  results.schema.errors.push(error);\n}\n\n// Continue for other levels...\n```\n\n### Step 3: Invoke Validation Agent for Deep Debugging\n\nIf any validation fails, invoke the validation agent (Sonnet) for deep debugging:\n\n```javascript\nif (\n  results.spec.status === \"failed\" ||\n  results.schema.status === \"failed\" ||\n  results.codegen.status === \"failed\" ||\n  results.compile.status === \"failed\" ||\n  results.runtime.status === \"failed\"\n) {\n  await invokeAgent(\"validation-agent\", {\n    model: \"sonnet\",\n    results: results,\n    project_path: \".\",\n  });\n}\n```\n\n### Step 4: Generate Validation Report\n\n```markdown\n# SpecForge Validation Report\n\n## Summary\n\n- ✓ Spec Validation: PASSED\n- ✓ Schema Validation: PASSED\n- ✗ Codegen Validation: FAILED\n- - Compile Validation: SKIPPED\n- - Runtime Validation: SKIPPED\n\n## Details\n\n### Codegen Validation Errors\n\n1. **Missing generated type**: `Order` type not found in `src/generated/db/models.rs`\n\n   - Expected: `pub struct Order { ... }`\n   - Actual: File does not exist\n   - Fix: Run `/specforge:build` to regenerate code\n\n2. **Type mismatch**: `user_id` field type mismatch\n   - Database schema: `INTEGER`\n   - Generated type: `String`\n   - Fix: Update sql-gen.toml type overrides\n\n## Recommendations\n\n1. Run code generation: `/specforge:build`\n2. Fix type overrides in sql-gen.toml\n3. Re-run validation: `/specforge:validate`\n```\n\n## Validation Modes\n\n### Quick Validation (Default)\n\nRun spec, schema, and compile validation (skip runtime tests):\n\n```bash\n/specforge:validate\n```\n\n### Full Validation (CI Mode)\n\nRun all validation levels including runtime tests:\n\n```bash\n/specforge:validate --full\n```\n\n### Specific Level Validation\n\nRun a specific validation level:\n\n```bash\n/specforge:validate --level spec\n/specforge:validate --level schema\n/specforge:validate --level codegen\n/specforge:validate --level compile\n/specforge:validate --level runtime\n```\n\n## Exit Codes\n\n- `0`: All validations passed\n- `1`: Spec validation failed\n- `2`: Schema validation failed\n- `3`: Codegen validation failed\n- `4`: Compilation failed\n- `5`: Runtime validation failed\n\n## Integration with CI/CD\n\nExample GitHub Actions workflow:\n\n```yaml\nname: SpecForge Validation\n\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install SpecForge\n        run: |\n          /plugin install specforge\n          /plugin install specforge-backend-rust-axum\n          /plugin install specforge-db-sqlite\n          /plugin install specforge-generate-rust-sql\n\n      - name: Run Full Validation\n        run: /specforge:validate --full\n```\n\n## Validation Best Practices\n\n1. **Run validation after every spec change**\n2. **Fix validation errors before committing**\n3. **Use validation in CI/CD pipeline**\n4. **Review validation reports thoroughly**\n5. **Keep specs and schemas in sync**\n\n## Resources\n\n- **Redocly CLI**: https://redocly.com/docs/cli/\n- **Schemathesis**: https://schemathesis.readthedocs.io/\n- **Dredd**: https://dredd.org/\n- **OpenAPI Validation**: https://learn.openapis.org/validation.html\n\n## Troubleshooting\n\n### Common Validation Errors\n\n#### Spec Validation Fails\n\n```\nError: Missing required field 'description' for operation POST /users\n```\n\n**Fix**: Add description to OpenAPI spec:\n\n```yaml\npaths:\n  /users:\n    post:\n      description: Create a new user\n      # ...\n```\n\n#### Schema Validation Fails\n\n```\nError: Migration 002 references non-existent table 'users'\n```\n\n**Fix**: Ensure migrations are sequential and applied in order.\n\n#### Codegen Validation Fails\n\n```\nError: Generated type 'User' does not match database schema\n```\n\n**Fix**: Re-run code generation with updated schema:\n\n```bash\n/specforge:build\n```\n\n#### Compilation Fails\n\n```\nError: Type mismatch in handler - expected i64, found String\n```\n\n**Fix**: Update type overrides in codegen config or fix handler implementation.\n\n#### Runtime Validation Fails\n\n```\nError: API response does not match OpenAPI spec for GET /users/:id\nExpected type: object\nActual type: array\n```\n\n**Fix**: Update handler implementation or OpenAPI spec to match.\n\n---\n\n**Validation ensures your SpecForge project maintains integrity across all layers - from specs to runtime.**\n",
        "specforge/skills/integration-expert.md": "---\nname: integration-expert\ndescription: This skill should be used when working with SpecForge plugin composition, integration patterns, and ensuring backend, database, and codegen plugins work together effectively. Use this skill to coordinate between plugins, handle plugin discovery, and manage compatibility.\nallowed-tools: Read,Write,Bash,Grep,Glob\nmodel: inherit\nversion: 1.0.0\nlicense: MIT\n---\n\n# SpecForge Integration Expert\n\nExpert in plugin composition strategies, integration patterns, and coordinating between SpecForge ecosystem plugins (backend, database, codegen, frontend).\n\n## Overview\n\nSpecForge uses a multi-plugin architecture where specialized plugins work together to create production-ready applications. This skill provides expertise in:\n\n- Discovering and selecting compatible plugins\n- Coordinating between backend, database, and codegen plugins\n- Managing plugin dependencies and compatibility\n- Ensuring type safety across the plugin ecosystem\n- Troubleshooting integration issues\n\n## Core Plugin Categories\n\n### Three-Plugin Architecture\n\nEvery SpecForge project requires **three core plugins**:\n\n1. **Backend Plugin** (`specforge-backend-{tech}-{framework}`)\n\n   - Framework-specific patterns and handlers\n   - HTTP request/response handling\n   - Business logic implementation\n   - Examples: `specforge-backend-rust-axum`, `specforge-backend-node-express`\n\n2. **Database Plugin** (`specforge-db-{database}`)\n\n   - Database-specific tooling and configuration\n   - Migration management\n   - Docker setup and health checks\n   - Examples: `specforge-db-postgresql`, `specforge-db-sqlite`\n\n3. **Codegen Pipeline Plugin** (`specforge-generate-{tech}-{database}`)\n   - Type-safe database access code generation\n   - Bridges backend framework with database\n   - Compile-time verification\n   - Examples: `specforge-generate-rust-sql`, `specforge-generate-ts-prisma`\n\n### Optional Plugins\n\n4. **Frontend Plugin** (`specforge-frontend-{framework}-{variant}`)\n   - Frontend framework patterns\n   - API client generation\n   - State management setup\n   - Examples: `specforge-frontend-react-tanstack`, `specforge-frontend-vue-pinia`\n\n## Plugin Discovery\n\n### Method 1: GitHub API Discovery\n\nQuery the Claude Market marketplace for available SpecForge plugins:\n\n```bash\n# Discover all SpecForge plugins\ncurl -s https://api.github.com/repos/claude-market/marketplace/contents/ \\\n  | jq -r '.[] | select(.type == \"dir\" and (.name | startswith(\"specforge-\"))) | .name'\n\n# Filter by category\ncurl -s https://api.github.com/repos/claude-market/marketplace/contents/ \\\n  | jq -r '.[] | select(.type == \"dir\" and (.name | startswith(\"specforge-backend-\"))) | .name'\n```\n\n### Method 2: Read Project Configuration\n\nCheck the project's `CLAUDE.md` for the configured stack:\n\n```markdown\n## SpecForge configuration (EDIT ONLY IF YOU ARE SPECFORGE AGENT)\n\n- backend: rust-axum\n- database: sqlite\n- codegen: rust-sql\n- frontend: react-tanstack\n```\n\n### Method 3: Plugin Manifest Inspection\n\nRead plugin's `.claude-plugin/plugin.json` to understand capabilities:\n\n```json\n{\n  \"name\": \"specforge-backend-rust-axum\",\n  \"specforge\": {\n    \"category\": \"backend\",\n    \"technology\": \"rust\",\n    \"variant\": \"axum\",\n    \"provides\": {\n      \"patterns\": true,\n      \"handlers\": true,\n      \"testing\": true,\n      \"docker\": true\n    },\n    \"requires\": {\n      \"categories\": [\"database\", \"codegen\"]\n    },\n    \"compatible_with\": {\n      \"database\": [\"postgresql\", \"sqlite\", \"mysql\"],\n      \"codegen\": [\"rust-sql\"],\n      \"frontend\": [\"*\"]\n    }\n  }\n}\n```\n\n## Plugin Compatibility\n\n### Validation Strategy\n\nWhen coordinating plugins, verify compatibility:\n\n1. **Category requirements**: Backend plugins require database + codegen\n2. **Technology alignment**: Codegen must match backend technology (e.g., rust-sql requires rust backend)\n3. **Database compatibility**: Codegen must support the selected database\n4. **Version constraints**: Check semantic version compatibility\n\n### Compatibility Matrix Example\n\n```\nBackend: rust-axum\n  ├─ Requires: database plugin, codegen plugin\n  ├─ Compatible databases: postgresql, sqlite, mysql\n  └─ Compatible codegen: rust-sql\n\nDatabase: sqlite\n  ├─ Compatible backends: rust, node, python, go\n  └─ Compatible codegen: rust-sql, ts-prisma, go-sqlc\n\nCodegen: rust-sql\n  ├─ Requires backend: rust-*\n  ├─ Compatible databases: postgresql, sqlite, mysql\n  └─ Provides: type-safe SQL access, compile-time verification\n```\n\n## Integration Patterns\n\n### Pattern 1: Plugin Orchestration Flow\n\n```\nCaptain Orchestrator (Core)\n    ↓\n1. Apply Spec Changes\n    ├─→ Database Plugin: Apply migrations\n    └─→ Validate OpenAPI spec\n    ↓\n2. Code Generation\n    ├─→ Codegen Plugin: Generate DB types and queries\n    └─→ Backend Plugin: Generate API request/response types\n    ↓\n3. Handler Implementation\n    └─→ Backend Plugin: Implement handlers using generated code\n    ↓\n4. Test & Iterate\n    ├─→ Backend Plugin: Run tests\n    ├─→ Codegen Plugin: Diagnose compilation errors\n    └─→ Repeat until tests pass\n```\n\n### Pattern 2: Type Safety Chain\n\n```\nDatabase Schema (SQL)\n    ↓ [Codegen Plugin: sql-gen/Prisma/sqlc]\nGenerated Types & Queries\n    ↓ [Backend Plugin: Business Logic]\nHandler Implementation\n    ↓ [Backend Plugin: OpenAPI Types]\nAPI Responses\n```\n\n### Pattern 3: Parallel Agent Execution\n\nSpawn agents from different plugins in parallel:\n\n```javascript\nconst tasks = [\n  // Backend plugin agents\n  {\n    plugin: \"backend\",\n    agent: \"handler-agent\",\n    endpoint: \"/users\",\n    model: \"haiku\",\n  },\n  {\n    plugin: \"backend\",\n    agent: \"handler-agent\",\n    endpoint: \"/orders\",\n    model: \"sonnet\",\n  },\n\n  // Database plugin agent\n  { plugin: \"database\", agent: \"migration-agent\", migration: \"003_add_orders\" },\n\n  // Frontend plugin agents (if applicable)\n  { plugin: \"frontend\", agent: \"component-agent\", component: \"UserList\" },\n];\n\n// Execute up to 5 concurrent agents\nawait executeParallel(tasks, { maxConcurrent: 5 });\n```\n\n## Skill Invocation Patterns\n\n### Invoke Plugin Skills\n\nEach plugin exposes skills that can be invoked programmatically:\n\n```javascript\n// Database plugin: Apply migrations\nawait invokeSkill(`${databasePlugin.name}/migrations-expert`, {\n  action: \"apply-migrations\",\n  migrations_dir: \"migrations/\",\n  database_url: process.env.DATABASE_URL,\n});\n\n// Codegen plugin: Generate from schema\nawait invokeSkill(`${codegenPlugin.name}/codegen-expert`, {\n  action: \"generate-from-schema\",\n  schema_dir: \"migrations/\",\n  output_dir: \"backend/src/generated/db\",\n  database_url: process.env.DATABASE_URL,\n});\n\n// Backend plugin: Generate OpenAPI types\nawait invokeSkill(`${backendPlugin.name}/openapi-types-expert`, {\n  action: \"generate-types\",\n  spec: \"spec/openapi.yaml\",\n  output: \"backend/src/generated/api\",\n});\n```\n\n### Delegate to Plugin Agents\n\nDelegate specific tasks to plugin agents:\n\n```javascript\n// Backend plugin: Implement handler\nawait invokeAgent(`${backendPlugin.name}/handler-agent`, {\n  model: endpoint.complexity === \"simple\" ? \"haiku\" : \"sonnet\",\n  endpoint: endpoint,\n  generated_db_types: `backend/src/generated/db`,\n  generated_api_types: `backend/src/generated/api`,\n});\n\n// Codegen plugin: Diagnose compilation errors\nawait invokeAgent(`${codegenPlugin.name}/diagnostics-agent`, {\n  model: \"sonnet\",\n  handler_path: \"backend/src/handlers/users.rs\",\n  errors: compilationErrors,\n  generated_code_path: \"backend/src/generated/db\",\n});\n```\n\n## Common Integration Challenges\n\n### Challenge 1: Plugin Not Found\n\n**Problem**: Required plugin is not installed\n\n**Solution**:\n\n1. Check if plugin exists in marketplace\n2. Install plugin: `/plugin install {plugin-name}`\n3. Update `CLAUDE.md` with plugin configuration\n\n### Challenge 2: Compatibility Mismatch\n\n**Problem**: Selected plugins are incompatible\n\n**Solution**:\n\n1. Read plugin manifests to check `compatible_with` declarations\n2. Suggest alternative plugin combinations\n3. Update plugin selection to ensure compatibility\n\n### Challenge 3: Version Conflicts\n\n**Problem**: Plugin versions have breaking changes\n\n**Solution**:\n\n1. Check semantic versions in plugin manifests\n2. Use version ranges in compatibility declarations\n3. Create lock file (`.specforge/plugin-lock.json`) for reproducibility\n\n### Challenge 4: Missing Generated Code\n\n**Problem**: Handler expects generated types that don't exist\n\n**Solution**:\n\n1. Verify codegen pipeline ran successfully\n2. Check output directory paths match expectations\n3. Re-run code generation if needed\n4. Ensure build integration is configured\n\n### Challenge 5: Context Budget Exceeded\n\n**Problem**: Too many plugins loading too much context\n\n**Solution**:\n\n1. Check each plugin's context budget in agent metadata\n2. Use selective context loading (only relevant sections)\n3. Persist state to `.specforge/state.json` to reduce repeated reads\n4. Enforce total context budget (<100K tokens)\n\n## Configuration Management\n\n### Project Configuration: CLAUDE.md\n\nStore selected plugins in the project's `CLAUDE.md`:\n\n```markdown\n## SpecForge configuration (EDIT ONLY IF YOU ARE SPECFORGE AGENT)\n\n- backend: rust-axum\n- database: sqlite\n- codegen: rust-sql\n- frontend: react-tanstack\n```\n\n### Lock File: .specforge/plugin-lock.json\n\nTrack exact plugin versions for reproducibility:\n\n```json\n{\n  \"plugins\": {\n    \"specforge-backend-rust-axum\": {\n      \"version\": \"1.2.3\",\n      \"resolved\": \"github:claude-market/marketplace/specforge-backend-rust-axum#v1.2.3\"\n    },\n    \"specforge-db-sqlite\": {\n      \"version\": \"1.0.5\",\n      \"resolved\": \"github:claude-market/marketplace/specforge-db-sqlite#v1.0.5\"\n    },\n    \"specforge-generate-rust-sql\": {\n      \"version\": \"2.0.1\",\n      \"resolved\": \"github:claude-market/marketplace/specforge-generate-rust-sql#v2.0.1\"\n    }\n  }\n}\n```\n\n### State Tracking: .specforge/state.json\n\nTrack build state to enable incremental operations:\n\n```json\n{\n  \"last_build\": \"2025-11-02T20:30:00Z\",\n  \"spec_hash\": \"abc123...\",\n  \"schema_hash\": \"def456...\",\n  \"generated_code\": {\n    \"backend/src/generated/db\": \"2025-11-02T20:25:00Z\",\n    \"backend/src/generated/api\": \"2025-11-02T20:26:00Z\"\n  },\n  \"handlers\": [\n    { \"path\": \"/users\", \"status\": \"implemented\", \"tests_passing\": true },\n    { \"path\": \"/orders\", \"status\": \"implemented\", \"tests_passing\": true }\n  ]\n}\n```\n\n## Docker Compose Integration\n\n### Aggregating Plugin Configurations\n\nEach plugin can contribute to Docker Compose configuration:\n\n```yaml\n# docker/docker-compose.yml\nversion: \"3.8\"\n\nservices:\n  # From backend plugin\n  api:\n    build: ./backend\n    ports:\n      - \"3000:3000\"\n    environment:\n      DATABASE_URL: sqlite://dev.db\n    depends_on:\n      db:\n        condition: service_healthy\n\n  # From database plugin\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: app\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U user\"]\n\n  # From frontend plugin (if selected)\n  web:\n    build: ./frontend\n    ports:\n      - \"5173:5173\"\n    environment:\n      VITE_API_URL: http://localhost:3000\n```\n\n### Plugin Docker Contributions\n\nEach plugin provides Docker configuration snippets:\n\n1. **Backend Plugin**: Application service definition, build configuration\n2. **Database Plugin**: Database service with health checks\n3. **Frontend Plugin**: Frontend service with build and environment\n\nAggregate these contributions into a single `docker-compose.yml`.\n\n## Validation Workflow\n\n### Pre-Build Validation\n\nBefore starting build process:\n\n1. **Verify all required plugins are installed**\n\n   ```bash\n   # Check if plugins exist\n   /plugin list | grep specforge-backend-rust-axum\n   /plugin list | grep specforge-db-sqlite\n   /plugin list | grep specforge-generate-rust-sql\n   ```\n\n2. **Validate plugin compatibility**\n\n   - Read each plugin's manifest\n   - Check `requires` and `compatible_with` fields\n   - Ensure no conflicts\n\n3. **Verify OpenAPI spec is valid**\n\n   ```bash\n   # Use redocly or other validator\n   redocly lint spec/openapi.yaml\n   ```\n\n4. **Verify database schema is valid**\n   - Check migration files syntax\n   - Ensure sequential migration numbering\n\n### Post-Build Validation\n\nAfter build completes:\n\n1. **Verify generated code exists**\n\n   - Check expected output directories\n   - Verify file structure matches expectations\n\n2. **Run compilation checks**\n\n   ```bash\n   # Backend compilation\n   cd backend && cargo check\n   # or npm run typecheck, etc.\n   ```\n\n3. **Run test suites**\n   ```bash\n   # Backend tests\n   cd backend && cargo test\n   # Integration tests\n   cd tests && npm test\n   ```\n\n## Resources\n\n### SpecForge Ecosystem\n\n- Core Plugin Documentation: `{baseDir}/README.md`\n- Plugin Discovery: GitHub API or marketplace CLI\n- Compatibility Matrix: Community-maintained spreadsheet\n\n### Plugin Development\n\n- Plugin Builder: `/plugin-builder:init`\n- Plugin Validation: `/plugin-builder:validate`\n- Plugin Publishing: `/plugin-builder:publish`\n\n### External Tools\n\n- OpenAPI Generator: https://openapi-generator.tech/\n- Docker Compose: https://docs.docker.com/compose/\n- Semantic Versioning: https://semver.org/\n\n## Best Practices\n\n1. **Always verify compatibility before plugin installation**\n2. **Use lock files for reproducible builds**\n3. **Track state to enable incremental operations**\n4. **Enforce context budgets to prevent bloat**\n5. **Delegate to specialized plugin agents when possible**\n6. **Use parallel execution for independent tasks**\n7. **Provide clear error messages with actionable solutions**\n8. **Keep plugin manifests up to date with compatibility info**\n\n## Example Integration Workflow\n\n### Complete Build Orchestration\n\n```javascript\n// 1. Discover and validate plugins\nconst backend = await discoverPlugin(\"backend\", \"rust-axum\");\nconst database = await discoverPlugin(\"database\", \"sqlite\");\nconst codegen = await discoverPlugin(\"codegen\", \"rust-sql\");\nawait validateCompatibility(backend, database, codegen);\n\n// 2. Apply spec changes\nawait invokeSkill(`${database.name}/migrations-expert`, {\n  action: \"apply-migrations\",\n  migrations_dir: \"migrations/\",\n});\n\n// 3. Run code generation\nawait invokeSkill(`${codegen.name}/codegen-expert`, {\n  action: \"generate-from-schema\",\n  schema_dir: \"migrations/\",\n  output_dir: \"backend/src/generated/db\",\n});\n\nawait invokeSkill(`${backend.name}/openapi-types-expert`, {\n  action: \"generate-types\",\n  spec: \"spec/openapi.yaml\",\n  output: \"backend/src/generated/api\",\n});\n\n// 4. Implement handlers in parallel\nconst endpoints = await parseOpenAPISpec(\"spec/openapi.yaml\");\nconst results = await Promise.all(\n  endpoints.map((endpoint) =>\n    invokeAgent(`${backend.name}/handler-agent`, {\n      model: endpoint.complexity === \"simple\" ? \"haiku\" : \"sonnet\",\n      endpoint: endpoint,\n      generated_db_types: \"backend/src/generated/db\",\n      generated_api_types: \"backend/src/generated/api\",\n    })\n  )\n);\n\n// 5. Test and iterate\nfor (const result of results) {\n  let success = false;\n  let iterations = 0;\n\n  while (!success && iterations < 3) {\n    const testResult = await invokeAgent(`${backend.name}/test-agent`, {\n      handler_path: result.path,\n      endpoint: result.endpoint,\n    });\n\n    if (testResult.status === \"passed\") {\n      success = true;\n      break;\n    }\n\n    // Diagnose and fix\n    await invokeAgent(`${codegen.name}/diagnostics-agent`, {\n      model: \"sonnet\",\n      errors: testResult.errors,\n      handler_path: result.path,\n    });\n\n    iterations++;\n  }\n}\n\n// 6. Update state\nawait updateState({\n  last_build: new Date(),\n  handlers: results.map((r) => ({\n    path: r.endpoint.path,\n    status: \"implemented\",\n    tests_passing: true,\n  })),\n});\n```\n\nThis integration expert skill ensures smooth coordination between all SpecForge plugins, maintaining type safety and compatibility throughout the development workflow.\n",
        "specforge/skills/openapi-expert.md": "---\nname: openapi-expert\ndescription: Expert in OpenAPI specification best practices, validation, and schema design for API-first development\nkeywords: [openapi, api, specification, schema, validation, rest, api-design]\n---\n\n# OpenAPI Expert\n\nExpert guidance on OpenAPI specification best practices, validation, schema design, and API-first development workflows.\n\n## Resources\n\n- **OpenAPI Specification**: https://spec.openapis.org/oas/latest.html\n- **OpenAPI Guide**: https://learn.openapis.org/\n- **OpenAPI Best Practices**: https://learn.openapis.org/best-practices.html\n- **OpenAPI Examples**: https://github.com/OAI/OpenAPI-Specification/tree/main/examples\n- **OpenAPI Tools**: https://openapi.tools/\n\n## Core Capabilities\n\n### 1. Validate OpenAPI Specifications\n\nEnsure OpenAPI specs are valid, complete, and follow best practices:\n\n```bash\n# Validate using Redocly CLI\nredocly lint openapi.yaml\n\n# Validate using Spectral\nspectral lint openapi.yaml\n\n# Validate using OpenAPI CLI\nopenapi-cli validate openapi.yaml\n```\n\n**Validation Checklist**:\n\n- All required fields present (openapi version, info, paths)\n- Valid JSON/YAML syntax\n- Proper schema references ($ref)\n- Consistent naming conventions\n- Complete response definitions\n- Security schemes properly defined\n- Examples provided for all operations\n\n### 2. Propose Spec Changes\n\nWhen proposing changes to OpenAPI specs, ensure:\n\n**Endpoint Design**:\n\n```yaml\npaths:\n  /api/users/{id}/orders:\n    get:\n      summary: Get user's orders\n      description: |\n        Retrieve all orders for a specific user.\n\n        Business Logic:\n        - Fetch user from database\n        - Query orders with user_id foreign key\n        - Include order items with product details\n        - Calculate totals\n\n        Performance: Target response time <500ms\n      operationId: getUserOrders\n      tags:\n        - users\n        - orders\n      parameters:\n        - name: id\n          in: path\n          required: true\n          description: User ID\n          schema:\n            type: integer\n            format: int64\n        - name: status\n          in: query\n          description: Filter by order status\n          schema:\n            type: string\n            enum: [pending, completed, cancelled]\n      responses:\n        \"200\":\n          description: User's orders\n          content:\n            application/json:\n              schema:\n                type: array\n                items:\n                  $ref: \"#/components/schemas/Order\"\n              examples:\n                simple:\n                  summary: Orders with items\n                  value:\n                    - id: 1\n                      user_id: 123\n                      total_cents: 5999\n                      status: completed\n                      items:\n                        - product_id: 456\n                          quantity: 2\n                          price_cents: 2999\n        \"404\":\n          description: User not found\n          content:\n            application/json:\n              schema:\n                $ref: \"#/components/schemas/Error\"\n```\n\n**Schema Definitions**:\n\n```yaml\ncomponents:\n  schemas:\n    Order:\n      type: object\n      required:\n        - id\n        - user_id\n        - total_cents\n        - status\n        - created_at\n      properties:\n        id:\n          type: integer\n          format: int64\n          description: Unique order identifier\n        user_id:\n          type: integer\n          format: int64\n          description: ID of the user who placed the order\n        total_cents:\n          type: integer\n          description: Total order amount in cents\n          minimum: 0\n        status:\n          type: string\n          enum: [pending, completed, cancelled]\n          description: Current order status\n        created_at:\n          type: string\n          format: date-time\n          description: Timestamp when order was created\n        items:\n          type: array\n          items:\n            $ref: \"#/components/schemas/OrderItem\"\n          description: Items in this order\n```\n\n### 3. Ensure Spec Quality\n\n**Best Practices**:\n\n1. **Use Detailed Descriptions**\n\n   - Every endpoint should have a clear summary and detailed description\n   - Include business logic, edge cases, and performance targets\n   - Document security requirements and authorization rules\n\n2. **Provide Comprehensive Examples**\n\n   - Add request examples for common use cases\n   - Add response examples showing success and error scenarios\n   - Include examples for different parameter combinations\n\n3. **Define Detailed Error Responses**\n\n   ```yaml\n   responses:\n     \"400\":\n       description: Invalid request\n       content:\n         application/json:\n           schema:\n             $ref: \"#/components/schemas/Error\"\n           examples:\n             validation-error:\n               summary: Validation failed\n               value:\n                 error: VALIDATION_ERROR\n                 message: Invalid email format\n                 field: email\n             missing-field:\n               summary: Required field missing\n               value:\n                 error: MISSING_FIELD\n                 message: Password is required\n                 field: password\n   ```\n\n4. **Use Schema Validation Rules**\n\n   ```yaml\n   properties:\n     email:\n       type: string\n       format: email\n       description: User's email address (must be unique)\n       maxLength: 255\n       example: user@example.com\n     password:\n       type: string\n       minLength: 8\n       maxLength: 100\n       pattern: '^(?=.*[A-Za-z])(?=.*\\d)[A-Za-z\\d@$!%*#?&]{8,}$'\n       description: Password (min 8 chars, must contain letter and number)\n       writeOnly: true\n     age:\n       type: integer\n       minimum: 0\n       maximum: 150\n       description: User's age in years\n   ```\n\n5. **Organize with Tags**\n\n   ```yaml\n   tags:\n     - name: users\n       description: User management operations\n     - name: orders\n       description: Order management operations\n     - name: products\n       description: Product catalog operations\n\n   paths:\n     /api/users:\n       get:\n         tags: [users]\n         # ...\n   ```\n\n6. **Version Your API**\n\n   ```yaml\n   info:\n     title: My API\n     version: 1.0.0\n     description: |\n       My API provides...\n\n       ## Versioning\n       This API uses semantic versioning. Breaking changes will increment major version.\n\n       ## Changelog\n       - v1.0.0: Initial release\n   ```\n\n7. **Define Security Schemes**\n\n   ```yaml\n   components:\n     securitySchemes:\n       bearerAuth:\n         type: http\n         scheme: bearer\n         bearerFormat: JWT\n         description: JWT token obtained from /auth/login\n\n   security:\n     - bearerAuth: []\n\n   paths:\n     /api/users:\n       get:\n         security:\n           - bearerAuth: []\n   ```\n\n8. **Use References to Avoid Duplication**\n\n   ```yaml\n   components:\n     parameters:\n       PageParam:\n         name: page\n         in: query\n         schema:\n           type: integer\n           minimum: 1\n           default: 1\n       LimitParam:\n         name: limit\n         in: query\n         schema:\n           type: integer\n           minimum: 1\n           maximum: 100\n           default: 20\n\n   paths:\n     /api/users:\n       get:\n         parameters:\n           - $ref: \"#/components/parameters/PageParam\"\n           - $ref: \"#/components/parameters/LimitParam\"\n   ```\n\n### 4. Provide Guidance on OpenAPI Patterns\n\n**RESTful Design Patterns**:\n\n```yaml\n# Resource Collections\nGET    /api/users          # List users\nPOST   /api/users          # Create user\n\n# Individual Resources\nGET    /api/users/{id}     # Get user\nPUT    /api/users/{id}     # Replace user\nPATCH  /api/users/{id}     # Update user\nDELETE /api/users/{id}     # Delete user\n\n# Nested Resources\nGET    /api/users/{id}/orders         # Get user's orders\nPOST   /api/users/{id}/orders         # Create order for user\nGET    /api/users/{id}/orders/{orderId}  # Get specific order\n\n# Actions (when REST isn't enough)\nPOST   /api/users/{id}/reset-password\nPOST   /api/orders/{id}/cancel\n```\n\n**Filtering, Sorting, Pagination**:\n\n```yaml\npaths:\n  /api/users:\n    get:\n      parameters:\n        # Filtering\n        - name: status\n          in: query\n          schema:\n            type: string\n            enum: [active, inactive]\n        - name: created_after\n          in: query\n          schema:\n            type: string\n            format: date-time\n\n        # Sorting\n        - name: sort\n          in: query\n          schema:\n            type: string\n            enum: [created_at, name, email]\n        - name: order\n          in: query\n          schema:\n            type: string\n            enum: [asc, desc]\n            default: asc\n\n        # Pagination\n        - name: page\n          in: query\n          schema:\n            type: integer\n            minimum: 1\n            default: 1\n        - name: limit\n          in: query\n          schema:\n            type: integer\n            minimum: 1\n            maximum: 100\n            default: 20\n\n      responses:\n        \"200\":\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  data:\n                    type: array\n                    items:\n                      $ref: \"#/components/schemas/User\"\n                  pagination:\n                    type: object\n                    properties:\n                      page:\n                        type: integer\n                      limit:\n                        type: integer\n                      total:\n                        type: integer\n                      pages:\n                        type: integer\n```\n\n**Error Handling Pattern**:\n\n```yaml\ncomponents:\n  schemas:\n    Error:\n      type: object\n      required:\n        - error\n        - message\n      properties:\n        error:\n          type: string\n          description: Machine-readable error code\n          example: VALIDATION_ERROR\n        message:\n          type: string\n          description: Human-readable error message\n          example: Invalid email format\n        field:\n          type: string\n          description: Field that caused the error (for validation errors)\n          example: email\n        details:\n          type: object\n          description: Additional error details\n          additionalProperties: true\n```\n\n## Validation Tools Integration\n\n### Redocly CLI\n\n```bash\n# Install\nnpm install -g @redocly/cli\n\n# Lint spec\nredocly lint openapi.yaml\n\n# Bundle multiple files\nredocly bundle openapi.yaml -o bundled.yaml\n\n# Generate documentation\nredocly build-docs openapi.yaml\n```\n\n### Spectral (Linting)\n\n```bash\n# Install\nnpm install -g @stoplight/spectral-cli\n\n# Lint with default rules\nspectral lint openapi.yaml\n\n# Custom ruleset (.spectral.yaml)\nspectral lint openapi.yaml --ruleset .spectral.yaml\n```\n\n### Prism (Mock Server)\n\n```bash\n# Install\nnpm install -g @stoplight/prism-cli\n\n# Start mock server\nprism mock openapi.yaml\n\n# Validate requests/responses\nprism proxy openapi.yaml http://localhost:3000\n```\n\n## Common OpenAPI Pitfalls to Avoid\n\n1. **Missing operationId**: Always provide unique operationId for code generation\n2. **Inconsistent naming**: Use consistent casing (camelCase, snake_case)\n3. **Missing examples**: Add examples for all requests and responses\n4. **Vague descriptions**: Be specific about business logic and behavior\n5. **Ignoring security**: Define security schemes and apply them appropriately\n6. **No error responses**: Document all possible error scenarios\n7. **Breaking changes**: Version API properly when making breaking changes\n8. **Missing validation**: Use schema constraints (min, max, pattern, enum)\n\n## Schema-First Development Workflow\n\n1. **Design API in OpenAPI first** - Before writing code\n2. **Validate spec** - Use linting tools\n3. **Review with stakeholders** - Share generated docs\n4. **Generate types and clients** - Use OpenAPI generators\n5. **Implement handlers** - Using generated types\n6. **Contract testing** - Validate implementation matches spec\n7. **Document changes** - Update spec version and changelog\n\n## References\n\n- **OpenAPI 3.1 Specification**: https://spec.openapis.org/oas/v3.1.0\n- **OpenAPI Data Types**: https://swagger.io/docs/specification/data-models/data-types/\n- **Swagger Editor**: https://editor.swagger.io/ (Online editor with validation)\n- **OpenAPI Style Guide**: https://apistylebook.com/design/guidelines/\n- **REST API Design Best Practices**: https://restfulapi.net/\n",
        "specforge/skills/stack-advisor.md": "---\nname: stack-advisor\ndescription: This skill should be used when users need tech stack recommendations, plugin discovery, or help selecting compatible backend/database/codegen plugins for their SpecForge project. Use this skill to explain trade-offs and help users select the right stack for their needs.\nallowed-tools: Read,WebFetch,WebSearch,AskUserQuestion\nmodel: inherit\nversion: 1.0.0\nlicense: MIT\n---\n\n# SpecForge Stack Advisor\n\nProvide expert tech stack recommendations and plugin discovery for SpecForge projects. Help users select compatible backend, database, and codegen plugins based on their requirements.\n\n## Overview\n\nSpecForge uses a three-plugin architecture where users must install:\n\n1. **Backend Plugin** - Framework-specific patterns and handlers (e.g., `specforge-backend-rust-axum`)\n2. **Database Plugin** - Database tooling and migrations (e.g., `specforge-db-sqlite`)\n3. **Codegen Plugin** - Type-safe code generation bridging backend and database (e.g., `specforge-generate-rust-sql`)\n\nThis skill helps users:\n\n- Discover available SpecForge plugins\n- Understand compatibility between plugins\n- Make informed decisions based on project requirements\n- Learn about trade-offs between different stacks\n\n## Prerequisites\n\n- User has installed the core `specforge` plugin\n- User understands their project's basic requirements (language preferences, scale, complexity)\n\n## Discovery Process\n\n### Step 1: Understand Requirements\n\nUse AskUserQuestion to gather:\n\n1. **Project Type**\n\n   - New project or existing codebase?\n   - Expected scale (prototype, production, high-scale)?\n   - Team experience level?\n\n2. **Technology Preferences**\n\n   - Preferred programming language (Rust, TypeScript, Python, Go)?\n   - Database preference (PostgreSQL, SQLite, MySQL, MongoDB)?\n   - Frontend requirements (React, Vue, Next.js, Svelte)?\n\n3. **Key Priorities**\n   - Performance vs development speed?\n   - Type safety importance?\n   - Team familiarity vs learning curve?\n   - Deployment environment (cloud, on-premise, edge)?\n\n### Step 2: Discover Available Plugins\n\nSearch the Claude Market marketplace for SpecForge plugins:\n\n```bash\n# Backend plugins\nspecforge-backend-rust-axum\nspecforge-backend-node-express\nspecforge-backend-python-fastapi\nspecforge-backend-go-gin\n\n# Database plugins\nspecforge-db-postgresql\nspecforge-db-sqlite\nspecforge-db-mysql\nspecforge-db-mongodb\n\n# Codegen plugins\nspecforge-generate-rust-sql\nspecforge-generate-ts-prisma\nspecforge-generate-go-sqlc\nspecforge-generate-python-sqlalchemy\n\n# Frontend plugins (optional)\nspecforge-frontend-react-tanstack\nspecforge-frontend-vue-pinia\nspecforge-frontend-nextjs\nspecforge-frontend-svelte\n```\n\nUse WebSearch or the GitHub API to find the latest available plugins:\n\n```bash\ncurl -s https://api.github.com/repos/claude-market/marketplace/contents/ | jq -r '.[] | select(.type == \"dir\" and (.name | startswith(\"specforge-\"))) | .name'\n```\n\n### Step 3: Explain Compatibility\n\nPresent compatible plugin combinations based on requirements. Example:\n\n**Rust + Axum + SQLite Stack:**\n\n- Backend: `specforge-backend-rust-axum`\n- Database: `specforge-db-sqlite`\n- Codegen: `specforge-generate-rust-sql`\n\n**Compatibility Matrix:**\n\n| Backend        | Database                           | Codegen           | Frontend |\n| -------------- | ---------------------------------- | ----------------- | -------- |\n| rust-axum      | postgresql, sqlite, mysql          | rust-sql          | any      |\n| node-express   | postgresql, sqlite, mysql, mongodb | ts-prisma         | any      |\n| python-fastapi | postgresql, sqlite, mysql          | python-sqlalchemy | any      |\n| go-gin         | postgresql, sqlite, mysql          | go-sqlc           | any      |\n\n### Step 4: Present Recommendations\n\nFor each viable stack combination, provide:\n\n1. **Overview** - One-sentence description\n2. **Strengths** - Key advantages\n3. **Trade-offs** - Considerations or limitations\n4. **Best For** - Ideal use cases\n5. **Installation Commands** - How to install plugins\n\n## Stack Recommendations\n\n### Rust + Axum + SQLite\n\n**Overview:** High-performance, compile-time verified stack ideal for embedded applications and APIs with strong type safety requirements.\n\n**Strengths:**\n\n- Compile-time verification from database to API\n- Excellent performance and memory safety\n- Type-safe SQL queries via sql-gen\n- Zero-cost abstractions\n- Great for embedded systems and edge computing\n\n**Trade-offs:**\n\n- Steeper learning curve for Rust beginners\n- Longer compile times\n- SQLite not ideal for high-concurrency writes\n\n**Best For:**\n\n- Embedded applications and CLI tools\n- Edge computing and IoT devices\n- Projects requiring maximum performance and safety\n- Teams with Rust experience\n\n**Installation:**\n\n```bash\n/plugin install specforge-backend-rust-axum\n/plugin install specforge-db-sqlite\n/plugin install specforge-generate-rust-sql\n```\n\n### Node + Express + PostgreSQL + Prisma\n\n**Overview:** Full-featured stack with excellent developer experience, strong ecosystem, and production-ready tooling.\n\n**Strengths:**\n\n- Huge ecosystem and community support\n- Rapid development with TypeScript\n- Prisma provides excellent type safety\n- PostgreSQL offers robust features (JSONB, full-text search, etc.)\n- Easy deployment to most platforms\n\n**Trade-offs:**\n\n- Less compile-time safety than Rust\n- Node.js single-threaded event loop limitations\n- Memory usage higher than compiled languages\n\n**Best For:**\n\n- Web applications and APIs\n- Teams with JavaScript/TypeScript experience\n- Projects requiring rapid development\n- Startups and MVPs\n\n**Installation:**\n\n```bash\n/plugin install specforge-backend-node-express\n/plugin install specforge-db-postgresql\n/plugin install specforge-generate-ts-prisma\n```\n\n### Python + FastAPI + PostgreSQL + SQLAlchemy\n\n**Overview:** Productive stack with excellent async support, automatic API documentation, and strong data science integration.\n\n**Strengths:**\n\n- Fast development with Python's expressiveness\n- Excellent async/await support in FastAPI\n- Strong typing with Pydantic and type hints\n- Great for data science and ML integration\n- Automatic OpenAPI documentation\n\n**Trade-offs:**\n\n- Runtime type checking (not compile-time)\n- Performance lower than compiled languages\n- GIL limitations for CPU-bound tasks\n\n**Best For:**\n\n- Data science and ML applications\n- Teams with Python expertise\n- Projects requiring rapid prototyping\n- APIs with complex business logic\n\n**Installation:**\n\n```bash\n/plugin install specforge-backend-python-fastapi\n/plugin install specforge-db-postgresql\n/plugin install specforge-generate-python-sqlalchemy\n```\n\n### Go + Gin + PostgreSQL + sqlc\n\n**Overview:** Simple, fast, and reliable stack with excellent concurrency support and straightforward deployment.\n\n**Strengths:**\n\n- Simple language with fast compilation\n- Excellent concurrency with goroutines\n- Single binary deployment\n- Strong standard library\n- sqlc provides compile-time SQL verification\n\n**Trade-offs:**\n\n- Less expressive than other languages\n- Smaller ecosystem compared to Node/Python\n- Generic support still maturing\n\n**Best For:**\n\n- Microservices and distributed systems\n- High-concurrency applications\n- Cloud-native deployments\n- Teams preferring simplicity over features\n\n**Installation:**\n\n```bash\n/plugin install specforge-backend-go-gin\n/plugin install specforge-db-postgresql\n/plugin install specforge-generate-go-sqlc\n```\n\n## Decision Framework\n\nHelp users choose based on these factors:\n\n### 1. Performance Requirements\n\n- **High Performance:** Rust > Go > Node ≈ Python\n- **Memory Efficiency:** Rust > Go > Node > Python\n\n### 2. Development Speed\n\n- **Rapid Prototyping:** Python > Node > Go > Rust\n- **Time to Production:** Node ≈ Python > Go > Rust\n\n### 3. Type Safety\n\n- **Compile-time Safety:** Rust > Go > TypeScript > Python\n- **Schema Safety:** All stacks provide compile-time DB verification via codegen\n\n### 4. Team Experience\n\n- **Beginner-friendly:** Python > Node > Go > Rust\n- **Enterprise-ready:** All stacks production-ready with proper setup\n\n### 5. Database Selection\n\n**PostgreSQL:**\n\n- Best for: Complex queries, JSONB, full-text search, scalability\n- Avoid if: Embedded deployment, zero-configuration needed\n\n**SQLite:**\n\n- Best for: Embedded apps, single-node deployments, development\n- Avoid if: High-concurrency writes, distributed systems\n\n**MySQL:**\n\n- Best for: Traditional web apps, shared hosting, WordPress-like systems\n- Avoid if: Advanced PostgreSQL features needed\n\n**MongoDB:**\n\n- Best for: Document-oriented data, flexible schemas, rapid iteration\n- Avoid if: Complex transactions, strong consistency required\n\n## Frontend Recommendations (Optional)\n\nIf users need frontend generation:\n\n### React + TanStack Query\n\n- Best for: Modern SPAs, complex state management\n- Strong TypeScript support\n- Large ecosystem\n\n### Vue + Pinia\n\n- Best for: Progressive enhancement, simpler apps\n- Easier learning curve than React\n- Great DX\n\n### Next.js\n\n- Best for: SSR, SEO-critical apps, full-stack frameworks\n- Includes routing, API routes, deployment\n- Vercel platform integration\n\n### Svelte\n\n- Best for: Performance-critical apps, minimal bundle size\n- Compile-time framework\n- Simple syntax\n\n## Output Format\n\nProvide recommendations in this structure:\n\n```markdown\n## Recommended Stack\n\nBased on your requirements ([list key factors]), I recommend:\n\n**Backend:** [plugin-name]\n**Database:** [plugin-name]\n**Codegen:** [plugin-name]\n**Frontend:** [plugin-name] (optional)\n\n### Why This Stack?\n\n[2-3 sentences explaining the rationale]\n\n### Installation Commands\n\n\\`\\`\\`bash\n/plugin install [backend-plugin]\n/plugin install [database-plugin]\n/plugin install [codegen-plugin]\n/plugin install [frontend-plugin] # optional\n\\`\\`\\`\n\n### Next Steps\n\n1. Initialize project: `/specforge:init`\n2. Review/edit OpenAPI spec: `spec/openapi.yaml`\n3. Plan implementation: `/specforge:plan`\n4. Build application: `/specforge:build`\n\n### Alternative Options\n\nIf [different priority], consider:\n\n- [Alternative stack with brief explanation]\n```\n\n## Common Questions\n\n### \"What's the fastest stack?\"\n\nRust + Axum provides the highest raw performance, but \"fast enough\" depends on requirements. Node/Python often sufficient for most web applications.\n\n### \"What's easiest to learn?\"\n\nPython + FastAPI has the gentlest learning curve. Node + Express second if team knows JavaScript.\n\n### \"What has the best ecosystem?\"\n\nNode.js has the largest ecosystem. Python strong for data science. Rust/Go smaller but growing.\n\n### \"Can I mix technologies?\"\n\nSpecForge's plugin system supports mixing any backend + database + codegen that are marked compatible. Check plugin manifests for compatibility declarations.\n\n### \"How do I switch stacks later?\"\n\nOpenAPI spec and database schema are stack-agnostic. You can regenerate code with different plugins by:\n\n1. Uninstalling old plugins\n2. Installing new plugins\n3. Running `/specforge:build` again\n\nBusiness logic in handlers will need porting, but specs remain the same.\n\n## Troubleshooting\n\n### \"Plugin not found\"\n\n- Check marketplace with WebSearch\n- Verify plugin name spelling\n- Confirm plugin has been published\n\n### \"Incompatible plugins\"\n\n- Check plugin manifests for `compatible_with` declarations\n- Ensure all three required plugins (backend, database, codegen) are installed\n- Some combinations may not be supported yet\n\n### \"Unclear requirements\"\n\nIf user requirements are vague:\n\n1. Ask clarifying questions about project type\n2. Inquire about team experience level\n3. Understand performance and scale needs\n4. Learn about deployment environment\n\n## Resources\n\n- SpecForge Documentation: See `/specforge` plugin README\n- Claude Market Marketplace: https://github.com/claude-market/marketplace\n- Plugin Compatibility Matrix: Check individual plugin READMEs\n- OpenAPI Specification: https://spec.openapis.org/oas/latest.html\n\n## Key Principles\n\n1. **Three plugins required:** Backend + Database + Codegen\n2. **Compatibility matters:** Not all combinations work together\n3. **No single best stack:** Right choice depends on context\n4. **Trade-offs are real:** Every stack has strengths and limitations\n5. **Start simple:** Can always add complexity later\n6. **Specs are portable:** OpenAPI and schema work across all stacks\n\nWhen in doubt, recommend the stack that matches team's existing expertise. Productivity trumps theoretical performance for most projects.\n"
      },
      "plugins": [
        {
          "name": "plugin-builder",
          "source": "./plugin-builder",
          "version": "1.3.2",
          "description": "Interactive plugin builder for Claude Code - serves as both an example plugin and a tool to create new plugins through guided prompts with specialized builder skills for each component type",
          "author": {
            "name": "Daniel Kovacs",
            "email": "kovacsemod@gmail.com",
            "url": "https://github.com/danielkov"
          },
          "license": "MIT",
          "keywords": [
            "plugin",
            "builder",
            "development",
            "scaffolding",
            "example"
          ],
          "commands": [
            "./commands/init.md",
            "./commands/add.md",
            "./commands/edit.md",
            "./commands/validate.md",
            "./commands/publish.md"
          ],
          "skills": [
            "./skills/cc-skill-builder.md",
            "./skills/cc-command-builder.md",
            "./skills/cc-agent-builder.md",
            "./skills/cc-hook-builder.md",
            "./skills/cc-mcp-builder.md"
          ],
          "categories": [
            "builder",
            "development",
            "example",
            "plugin",
            "scaffolding"
          ],
          "install_commands": [
            "/plugin marketplace add claude-market/marketplace",
            "/plugin install plugin-builder@claude-market"
          ]
        },
        {
          "name": "specforge",
          "source": "./specforge",
          "version": "0.1.2",
          "description": "Schema-first development ecosystem with dual-spec workflows (OpenAPI + DB schema) for building production-ready applications through deterministic code generation and intelligent orchestration",
          "author": {
            "name": "Daniel Emod Kovacs",
            "url": "https://github.com/danielkov"
          },
          "license": "MIT",
          "repository": "https://github.com/claude-market/marketplace/tree/main/specforge",
          "keywords": [
            "specforge",
            "openapi",
            "schema-first",
            "code-generation",
            "orchestration",
            "api-development",
            "database-schema",
            "type-safety"
          ],
          "commands": [
            "./commands/init.md",
            "./commands/plan.md",
            "./commands/build.md",
            "./commands/validate.md",
            "./commands/test.md",
            "./commands/ship.md",
            "./commands/sync.md"
          ],
          "agents": [
            "./agents/captain-orchestrator.md",
            "./agents/planning-agent.md",
            "./agents/validation-agent.md"
          ],
          "skills": [
            "./skills/openapi-expert.md",
            "./skills/stack-advisor.md",
            "./skills/integration-expert.md"
          ],
          "categories": [
            "api-development",
            "code-generation",
            "database-schema",
            "openapi",
            "orchestration",
            "schema-first",
            "specforge",
            "type-safety"
          ],
          "install_commands": [
            "/plugin marketplace add claude-market/marketplace",
            "/plugin install specforge@claude-market"
          ]
        },
        {
          "name": "specforge-backend-rust-axum",
          "source": "./specforge-backend-rust-axum",
          "version": "1.0.0",
          "description": "Rust + Axum backend framework expertise - implements handlers using OpenAPI and DB-generated types",
          "author": {
            "name": "Daniel Emod Kovacs"
          },
          "license": "MIT",
          "keywords": [
            "specforge",
            "backend",
            "rust",
            "axum",
            "openapi",
            "framework",
            "distributed-systems"
          ],
          "agents": [
            "./agents/handler-implementer.md",
            "./agents/test-generator.md",
            "./agents/router-builder.md"
          ],
          "skills": [
            "./skills/axum-patterns.md",
            "./skills/axum-handler-implementation.md",
            "./skills/axum-error-mapping.md",
            "./skills/rust-testing.md",
            "./skills/axum-middleware.md",
            "./skills/rust-openapi-integration.md",
            "./skills/rust-dev-setup.md",
            "./skills/axum-idempotency.md",
            "./skills/axum-resilience.md",
            "./skills/axum-tracing.md",
            "./skills/axum-timeouts.md",
            "./skills/axum-health-shutdown.md",
            "./skills/axum-rate-limiting.md"
          ],
          "categories": [
            "axum",
            "backend",
            "distributed-systems",
            "framework",
            "openapi",
            "rust",
            "specforge"
          ],
          "install_commands": [
            "/plugin marketplace add claude-market/marketplace",
            "/plugin install specforge-backend-rust-axum@claude-market"
          ]
        }
      ]
    }
  ]
}