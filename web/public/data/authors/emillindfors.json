{
  "author": {
    "id": "EmilLindfors",
    "display_name": "Emil Tomson Lindfors",
    "avatar_url": "https://avatars.githubusercontent.com/u/4932954?u=c1d41076a9d9c213abaaedd192d67d63e868f8de&v=4"
  },
  "marketplaces": [
    {
      "name": "lf-marketplace",
      "version": null,
      "description": "Changelog management plugin that ensures all code commits include proper changelog entries. Provides hooks to prevent commits without changelog updates, commands for managing changelog entries, and agents for writing well-formatted changelog entries following Keep a Changelog format",
      "repo_full_name": "EmilLindfors/claude-marketplace",
      "repo_url": "https://github.com/EmilLindfors/claude-marketplace",
      "repo_description": null,
      "signals": {
        "stars": 2,
        "forks": 1,
        "pushed_at": "2025-11-14T17:46:35Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"lf-marketplace\",\n  \"owner\": {\n    \"name\": \"Emil Lindfors\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"changelog\",\n      \"source\": \"./plugins/changelog\",\n      \"description\": \"Changelog management plugin that ensures all code commits include proper changelog entries. Provides hooks to prevent commits without changelog updates, commands for managing changelog entries, and agents for writing well-formatted changelog entries following Keep a Changelog format\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Emil Lindfors\"\n      }\n    },\n    {\n      \"name\": \"rust-hexagonal\",\n      \"source\": \"./plugins/rust-hexagonal\",\n      \"description\": \"Hexagonal architecture plugin for Rust. Helps design and implement clean, maintainable Rust applications using the ports and adapters pattern. Includes commands for initializing project structure, adding ports and adapters, and an expert agent for architecture guidance\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Emil Lindfors\"\n      }\n    },\n    {\n      \"name\": \"rust-error-handling\",\n      \"source\": \"./plugins/rust-error-handling\",\n      \"description\": \"Error handling best practices plugin for Rust. Provides commands for creating custom error types with thiserror, refactoring panic-based code to Result-based error handling, and an expert agent for error handling guidance and code review\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Emil Lindfors\"\n      }\n    },\n    {\n      \"name\": \"rust-testing\",\n      \"source\": \"./plugins/rust-testing\",\n      \"description\": \"Testing best practices plugin for Rust. Includes commands for adding unit tests, integration tests, test analysis, and an expert agent for comprehensive testing strategies, mock implementations, and property-based testing\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Emil Lindfors\"\n      }\n    },\n    {\n      \"name\": \"rust-modern-patterns\",\n      \"source\": \"./plugins/rust-modern-patterns\",\n      \"description\": \"Modern Rust patterns for Rust 2024 Edition. Includes let chains, async closures, gen blocks, match ergonomics, const improvements. Commands for modernization, edition upgrade, and pattern checking. Expert agent for Rust 2024 migration and best practices. Requires Rust 1.85.0+\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Emil Lindfors\"\n      }\n    },\n    {\n      \"name\": \"rust-data-engineering\",\n      \"source\": \"./plugins/rust-data-engineering\",\n      \"description\": \"Data engineering plugin for Rust with object_store, Arrow, Parquet, DataFusion, and Iceberg. Build cloud-native data lakes, analytical query engines, and ETL pipelines. Commands for object storage, Parquet I/O, DataFusion queries, and Iceberg tables. Expert agent for data lake architecture and performance optimization\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Emil Lindfors\"\n      }\n    },\n    {\n      \"name\": \"rust-lambda\",\n      \"source\": \"./plugins/rust-lambda\",\n      \"description\": \"Comprehensive AWS Lambda development with Rust using cargo-lambda. Build, deploy, and optimize Lambda functions with support for IO-intensive, compute-intensive, and mixed workloads. Includes 12 commands for complete Lambda lifecycle management and an expert agent for architecture decisions\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Emil Lindfors\"\n      }\n    }\n  ]\n}\n",
        "plugins/changelog/README.md": "# AQC Changelog Plugin\n\nA comprehensive changelog management plugin for Claude Code that ensures all code commits include proper changelog entries.\n\n## Overview\n\nThe AQC Changelog plugin helps maintain a clean, well-documented changelog following the [Keep a Changelog](https://keepachangelog.com/en/1.0.0/) format and [Semantic Versioning](https://semver.org/spec/v2.0.0.html) standards. It prevents commits without changelog updates, provides convenient commands for managing changelog entries, and includes a specialized agent for writing professional changelog entries.\n\n## Features\n\n### üîí Changelog Enforcement Hook\n- **Automatic validation**: Checks every commit attempt to ensure CHANGELOG.md has been updated\n- **Smart detection**: Only blocks commits when code changes are being committed\n- **Clear messaging**: Provides helpful error messages with instructions on how to proceed\n- **Fail-safe design**: If the hook encounters an error, it won't block the user\n\n### üìù Slash Commands\n\n#### `/changelog-add`\nAdd a new entry to the CHANGELOG.md file with guided prompts and format validation.\n\n**Features**:\n- Interactive prompts for change type and description\n- Automatic formatting following Keep a Changelog standards\n- Auto-categorization (Added, Changed, Fixed, Removed, Security, Deprecated)\n- Automatic git staging of the updated file\n- Format validation and consistency checks\n\n#### `/changelog-view`\nView recent entries from the CHANGELOG.md file with formatted output.\n\n**Features**:\n- Display unreleased changes\n- Show recent versioned releases\n- Search for specific entries\n- Format validation\n- Statistics on change types\n\n#### `/changelog-init`\nInitialize a new CHANGELOG.md file with proper structure.\n\n**Features**:\n- Creates standard Keep a Changelog format\n- Detects project version from package files\n- Includes helpful comments and examples\n- Customizable initial structure\n- Version detection support for multiple project types\n\n### ü§ñ Changelog Writer Agent\n\nA specialized agent (`changelog-writer`) that can be invoked for complex changelog writing tasks.\n\n**Capabilities**:\n- Research changes from git diff and modified files\n- Write detailed, well-formatted changelog entries\n- Follow project-specific changelog style\n- Provide technical detail and context\n- Group related changes logically\n- Stage changes automatically\n\n## Installation\n\nThe plugin is already installed in this repository. To enable it in other projects:\n\n1. **Install the plugin**:\n   ```bash\n   # Copy the plugin to your project\n   cp -r plugins/aqc-changelog /path/to/your/project/plugins/\n   ```\n\n2. **Register in marketplace.json**:\n   ```json\n   {\n     \"plugins\": [\n       {\n         \"name\": \"aqc-changelog\",\n         \"source\": \"./plugins/aqc-changelog\",\n         \"description\": \"Changelog management plugin\",\n         \"version\": \"1.0.0\",\n         \"author\": {\n           \"name\": \"Aquacloud\"\n         }\n       }\n     ]\n   }\n   ```\n\n3. **Hooks are automatically enabled** when the plugin is enabled. No manual configuration needed!\n\n## Usage\n\n### Basic Workflow\n\n1. **Make code changes** in your project\n2. **Add changelog entry**:\n   ```\n   /changelog-add\n   ```\n   Follow the prompts to add your entry.\n\n3. **Stage your changes**:\n   ```bash\n   git add .\n   ```\n\n4. **Commit** (the hook will verify changelog was updated):\n   ```\n   Create a commit with message \"Add new feature\"\n   ```\n\n### Quick Tips\n\n- **View recent changes**: `/changelog-view`\n- **Initialize new project**: `/changelog-init`\n- **Complex entries**: Use the `changelog-writer` agent for detailed, multi-faceted changes\n- **Multiple related changes**: Group them under one main bullet with sub-bullets\n\n### Example Changelog Entry\n\n```markdown\n### Added\n- **System Metrics Collection**: Comprehensive system monitoring using dedicated PyIceberg table\n  - **Multi-Service Support**: Service identification with hostname and environment\n  - **Comprehensive Metrics**: CPU, memory, disk, network, and process metrics\n  - **Partitioned Storage**: Efficiently partitioned by service_name, date, and hour\n  - **Configurable**: Environment variables for intervals and enable/disable control\n```\n\n## Changelog Categories\n\n- **Added**: New features, endpoints, or functionality\n- **Changed**: Changes in existing functionality\n- **Deprecated**: Soon-to-be removed features\n- **Removed**: Removed features or functionality\n- **Fixed**: Bug fixes and error corrections\n- **Security**: Security improvements and vulnerability fixes\n\n## Hook Configuration\n\nThe changelog hook is automatically configured when the plugin is enabled. It uses the `PreToolUse` hook to intercept git commit commands and validate that:\n\n1. The CHANGELOG.md file has been modified when committing code changes\n2. All empty category sections are removed before committing\n\n**Plugin hooks configuration** (in `plugins/changelog/hooks/hooks.json`):\n```json\n{\n  \"description\": \"Changelog validation before commits\",\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"python3 ${CLAUDE_PLUGIN_ROOT}/hooks/check-changelog-before-commit.py\",\n            \"timeout\": 5\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nThe hook runs automatically before any Bash tool executions, specifically targeting git commit operations.\n\n## Troubleshooting\n\n### Hook not triggering\n- Check that the hook is registered in your settings.json\n- Verify the hook script is executable: `chmod +x plugins/aqc-changelog/hooks/check-changelog-before-commit.py`\n- Check Python 3 is available: `python3 --version`\n\n### Hook blocking incorrectly\n- Ensure CHANGELOG.md is staged: `git add CHANGELOG.md`\n- Check that changes are actually in the changelog file\n- Verify the changelog file name is correct (case-sensitive)\n\n### Commands not available\n- Check the plugin is registered in `.claude-plugin/marketplace.json`\n- Verify the plugin.json file exists in the plugin directory\n- Restart Claude Code to reload plugins\n\n## Development\n\n### Project Structure\n\n```\nplugins/aqc-changelog/\n‚îú‚îÄ‚îÄ .claude-plugin/\n‚îÇ   ‚îî‚îÄ‚îÄ plugin.json          # Plugin metadata\n‚îú‚îÄ‚îÄ commands/\n‚îÇ   ‚îú‚îÄ‚îÄ changelog-add.md     # Add entry command\n‚îÇ   ‚îú‚îÄ‚îÄ changelog-view.md    # View entries command\n‚îÇ   ‚îî‚îÄ‚îÄ changelog-init.md    # Initialize changelog command\n‚îú‚îÄ‚îÄ agents/\n‚îÇ   ‚îî‚îÄ‚îÄ changelog-writer.md  # Specialized changelog writing agent\n‚îú‚îÄ‚îÄ hooks/\n‚îÇ   ‚îî‚îÄ‚îÄ check-changelog-before-commit.py  # Enforcement hook\n‚îú‚îÄ‚îÄ CONTEXT.md               # Plugin context and best practices\n‚îî‚îÄ‚îÄ README.md               # This file\n```\n\n### Testing the Hook\n\nTo test the hook manually:\n\n```bash\n# Create a test input\necho '{\"prompt\": \"git commit -m test\", \"cwd\": \".\"}' | \\\n  python3 plugins/aqc-changelog/hooks/check-changelog-before-commit.py\n```\n\nExpected output when changelog is not updated:\n```json\n{\n  \"decision\": \"block\",\n  \"reason\": \"‚ö†Ô∏è Changelog update required!...\",\n  \"hookSpecificOutput\": {...}\n}\n```\n\n## Best Practices\n\n1. **Update changelog before committing**: Add entries as you make changes, not at the end\n2. **Be specific and technical**: Include file paths, endpoint names, function names\n3. **Group related changes**: Multiple related fixes can go under one main bullet\n4. **Include context**: Explain the \"why\" behind changes, especially for bug fixes\n5. **Use consistent formatting**: Match the existing changelog style\n6. **Reference breaking changes**: Clearly mark any breaking changes\n7. **Keep it current**: Always add to [Unreleased], versions are managed separately\n\n## Contributing\n\nTo contribute improvements to this plugin:\n\n1. Make changes to plugin files\n2. Update the version in `plugin.json`\n3. Update this README if adding features\n4. Test the hook and commands thoroughly\n5. Update CONTEXT.md with any new patterns or practices\n\n## License\n\nThis plugin is part of the AquaCloud Claude Code plugin collection.\n\n## Support\n\nFor issues or questions:\n- Check the troubleshooting section above\n- Review CONTEXT.md for detailed usage patterns\n- Contact the AquaCloud team at support@aquacloud.ai\n\n---\n\n**Remember**: A well-maintained changelog is a gift to your future self and your team! üìù‚ú®\n",
        "plugins/rust-hexagonal/README.md": "# Rust Hexagonal Architecture Plugin\n\nA comprehensive plugin for implementing hexagonal architecture (ports and adapters pattern) in Rust projects.\n\n## Overview\n\nThe Rust Hexagonal Architecture plugin helps you design and implement clean, maintainable Rust applications following the hexagonal architecture pattern. This pattern separates your domain logic from external concerns, making your code more testable, flexible, and maintainable.\n\n## Features\n\n### üìê Architecture Setup Commands\n\n#### `/rust-hex-init`\nInitialize a hexagonal architecture project structure for Rust.\n\n**Features**:\n- Creates proper directory structure for domain, ports, and adapters\n- Sets up initial module files with proper exports\n- Generates example port traits and adapters\n- Creates Cargo.toml workspace configuration\n- Includes best practice comments and documentation\n\n#### `/rust-hex-add-port`\nAdd a new port (interface) to your hexagonal architecture.\n\n**Features**:\n- Interactive prompts for port type (driving/driven) and name\n- Generates trait definition with proper documentation\n- Creates corresponding adapter stub\n- Updates module exports automatically\n- Follows Rust naming conventions\n\n#### `/rust-hex-add-adapter`\nAdd a new adapter implementation for an existing port.\n\n**Features**:\n- Lists available ports to implement\n- Generates adapter struct and implementation\n- Includes common patterns (async, error handling, configuration)\n- Creates test module scaffold\n- Auto-updates module exports\n\n### ü§ñ Hexagonal Architecture Agent\n\nA specialized agent (`rust-hex-architect`) for complex architecture tasks.\n\n**Capabilities**:\n- Analyze existing codebase and suggest hexagonal refactoring\n- Design port interfaces based on domain requirements\n- Generate complete adapter implementations\n- Review architecture for proper separation of concerns\n- Suggest improvements for testability and maintainability\n- Create integration examples between components\n\n## Hexagonal Architecture Concepts\n\n### What is Hexagonal Architecture?\n\nHexagonal architecture (also known as ports and adapters) is an architectural pattern that aims to create loosely coupled application components. In Rust, this pattern leverages:\n\n- **Traits** for defining ports (interfaces)\n- **Structs** for domain logic and adapters\n- **Compile-time polymorphism** for zero-cost abstractions\n- **Dependency injection** through function parameters or struct fields\n\n### Core Components\n\n1. **Domain** (The Hexagon)\n   - Core business logic\n   - Independent of external concerns\n   - Pure Rust with minimal dependencies\n\n2. **Ports** (Interfaces)\n   - **Driving Ports** (Primary): What the domain offers to the outside world\n   - **Driven Ports** (Secondary): What the domain needs from the outside world\n   - Defined as Rust traits\n\n3. **Adapters** (Implementations)\n   - **Driving Adapters**: REST API, CLI, gRPC handlers\n   - **Driven Adapters**: Database repositories, HTTP clients, file systems\n   - Implement port traits\n\n## Directory Structure\n\n```\nyour-project/\n‚îú‚îÄ‚îÄ Cargo.toml\n‚îî‚îÄ‚îÄ src/\n    ‚îú‚îÄ‚îÄ domain/\n    ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs\n    ‚îÇ   ‚îú‚îÄ‚îÄ models.rs         # Domain entities\n    ‚îÇ   ‚îî‚îÄ‚îÄ services.rs       # Business logic\n    ‚îú‚îÄ‚îÄ ports/\n    ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs\n    ‚îÇ   ‚îú‚îÄ‚îÄ driving.rs        # Primary ports\n    ‚îÇ   ‚îî‚îÄ‚îÄ driven.rs         # Secondary ports\n    ‚îî‚îÄ‚îÄ adapters/\n        ‚îú‚îÄ‚îÄ mod.rs\n        ‚îú‚îÄ‚îÄ driving/\n        ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs\n        ‚îÇ   ‚îú‚îÄ‚îÄ rest_api.rs   # HTTP adapter\n        ‚îÇ   ‚îî‚îÄ‚îÄ cli.rs        # CLI adapter\n        ‚îî‚îÄ‚îÄ driven/\n            ‚îú‚îÄ‚îÄ mod.rs\n            ‚îú‚îÄ‚îÄ postgres.rs   # Database adapter\n            ‚îî‚îÄ‚îÄ http_client.rs # External API adapter\n```\n\n## Installation\n\nThe plugin is installed in this repository. To enable it in other projects:\n\n1. **Install the plugin**:\n   ```bash\n   cp -r plugins/rust-hexagonal /path/to/your/project/plugins/\n   ```\n\n2. **Register in marketplace.json**:\n   ```json\n   {\n     \"plugins\": [\n       {\n         \"name\": \"rust-hexagonal\",\n         \"source\": \"./plugins/rust-hexagonal\",\n         \"description\": \"Hexagonal architecture plugin for Rust\",\n         \"version\": \"1.0.0\"\n       }\n     ]\n   }\n   ```\n\n## Usage\n\n### Quick Start\n\n1. **Initialize hexagonal structure**:\n   ```\n   /rust-hex-init\n   ```\n\n2. **Add a driven port** (e.g., user repository):\n   ```\n   /rust-hex-add-port\n   ```\n   Select \"driven\" and name it \"UserRepository\"\n\n3. **Add an adapter** (e.g., PostgreSQL implementation):\n   ```\n   /rust-hex-add-adapter\n   ```\n   Select the port and name the adapter\n\n4. **Use the architect agent** for complex refactoring:\n   ```\n   Ask the rust-hex-architect agent to help refactor my existing code to use hexagonal architecture\n   ```\n\n### Example: User Service\n\n**Domain Service** (`src/domain/user_service.rs`):\n```rust\nuse crate::ports::driven::UserRepository;\n\npub struct UserService<R: UserRepository> {\n    repository: R,\n}\n\nimpl<R: UserRepository> UserService<R> {\n    pub fn new(repository: R) -> Self {\n        Self { repository }\n    }\n\n    pub async fn get_user(&self, id: &str) -> Result<User, Error> {\n        self.repository.find_by_id(id).await\n    }\n}\n```\n\n**Driven Port** (`src/ports/driven.rs`):\n```rust\n#[async_trait]\npub trait UserRepository: Send + Sync {\n    async fn find_by_id(&self, id: &str) -> Result<User, Error>;\n    async fn save(&self, user: &User) -> Result<(), Error>;\n}\n```\n\n**Adapter** (`src/adapters/driven/postgres_user_repo.rs`):\n```rust\npub struct PostgresUserRepository {\n    pool: PgPool,\n}\n\n#[async_trait]\nimpl UserRepository for PostgresUserRepository {\n    async fn find_by_id(&self, id: &str) -> Result<User, Error> {\n        sqlx::query_as!(User, \"SELECT * FROM users WHERE id = $1\", id)\n            .fetch_one(&self.pool)\n            .await\n            .map_err(|e| Error::Database(e))\n    }\n}\n```\n\n## Best Practices\n\n1. **Keep domain pure**: No external dependencies in domain code\n2. **Use traits for ports**: Define clear contracts with traits\n3. **Prefer composition over inheritance**: Rust doesn't have inheritance, use composition\n4. **Make adapters swappable**: Easy to replace implementations\n5. **Test domain independently**: Mock adapters using test doubles\n6. **Use async traits**: For I/O operations, use `#[async_trait]`\n7. **Handle errors properly**: Define domain-specific error types\n8. **Document ports clearly**: Explain preconditions and postconditions\n\n## Benefits in Rust\n\n- **Zero-cost abstractions**: Traits compile to static dispatch by default\n- **Type safety**: Compiler ensures ports are properly implemented\n- **Testability**: Easy to create mock implementations\n- **Flexibility**: Swap adapters without changing domain code\n- **Clear boundaries**: Explicit separation of concerns\n\n## Troubleshooting\n\n### Commands not available\n- Verify plugin is registered in marketplace.json\n- Restart Claude Code to reload plugins\n\n### Generated code doesn't compile\n- Ensure you have required dependencies in Cargo.toml\n- Check that async-trait is added if using async\n- Verify all imports are correct\n\n## Contributing\n\nTo contribute improvements:\n1. Make changes to plugin files\n2. Test commands and agents\n3. Update README and CONTEXT.md\n4. Submit improvements\n\n## License\n\nPart of the Claude Code plugin collection.\n\n---\n\n**Clean architecture, powerful Rust** ü¶Äüî∑\n",
        "plugins/rust-error-handling/README.md": "# Rust Error Handling Plugin\n\nA comprehensive plugin for implementing robust error handling patterns in Rust applications.\n\n## Overview\n\nThis plugin helps you implement best practices for error handling in Rust using the Result type, custom error types, and proper error propagation patterns.\n\n## Features\n\n### üìù Error Handling Commands\n\n#### `/rust-error-add-type`\nCreate a new custom error type with proper error variants.\n\n**Features**:\n- Interactive prompts for error name and variants\n- Automatic `thiserror` or manual implementation\n- From trait implementations for error conversion\n- Display and Debug implementations\n- Documentation templates\n\n#### `/rust-error-refactor`\nRefactor code from panic-based error handling to Result-based.\n\n**Features**:\n- Scan code for unwrap() and expect() calls\n- Suggest Result-based alternatives\n- Convert panic-prone code to proper error handling\n- Add error propagation with ? operator\n- Update function signatures to return Result\n\n### ü§ñ Error Handling Agent\n\nA specialized agent (`rust-error-expert`) for complex error handling tasks.\n\n**Capabilities**:\n- Analyze error handling patterns in codebase\n- Design error type hierarchies\n- Refactor from panic to Result\n- Review error handling for best practices\n- Generate error types with proper conversions\n- Suggest error recovery strategies\n\n## Error Handling Patterns\n\n### Custom Error Types with thiserror\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum MyError {\n    #[error(\"IO error: {0}\")]\n    Io(#[from] std::io::Error),\n\n    #[error(\"Parse error: {0}\")]\n    Parse(String),\n\n    #[error(\"Not found: {0}\")]\n    NotFound(String),\n}\n```\n\n### Error Propagation with ?\n\n```rust\nfn process_data() -> Result<Data, MyError> {\n    let content = read_file()?; // Auto-converts using From trait\n    let parsed = parse_content(&content)?;\n    Ok(parsed)\n}\n```\n\n### Error Context with anyhow\n\n```rust\nuse anyhow::{Context, Result};\n\nfn load_config() -> Result<Config> {\n    let content = std::fs::read_to_string(\"config.toml\")\n        .context(\"Failed to read config file\")?;\n\n    toml::from_str(&content)\n        .context(\"Failed to parse config\")\n}\n```\n\n### Result Type Aliases\n\n```rust\n// Define a result type alias for your library\npub type Result<T> = std::result::Result<T, MyError>;\n\npub fn do_work() -> Result<Output> {\n    // Now you can use Result without the error type\n    Ok(output)\n}\n```\n\n## Best Practices\n\n1. **Use Result for recoverable errors**: Don't use panic! for expected errors\n2. **Use custom error types**: Define domain-specific errors with thiserror\n3. **Implement From traits**: Enable automatic error conversion with ?\n4. **Add context to errors**: Use anyhow or custom context methods\n5. **Don't overuse unwrap/expect**: Only use in tests or when panic is appropriate\n6. **Layer error types**: Different error types for different layers (domain, infra)\n7. **Document error conditions**: Explain when functions return which errors\n8. **Test error cases**: Write tests for error paths, not just happy paths\n\n## Installation\n\n```bash\ncp -r plugins/rust-error-handling /path/to/your/project/plugins/\n```\n\nRegister in marketplace.json:\n```json\n{\n  \"plugins\": [{\n    \"name\": \"rust-error-handling\",\n    \"source\": \"./plugins/rust-error-handling\",\n    \"description\": \"Error handling best practices for Rust\",\n    \"version\": \"1.0.0\"\n  }]\n}\n```\n\n## Usage\n\n### Quick Start\n\n1. **Add error handling dependencies**:\n   ```bash\n   cargo add thiserror anyhow\n   ```\n\n2. **Create a custom error type**:\n   ```\n   /rust-error-add-type\n   ```\n\n3. **Refactor existing code**:\n   ```\n   /rust-error-refactor\n   ```\n\n4. **Get expert help**:\n   ```\n   Ask rust-error-expert to review my error handling\n   ```\n\n## Common Dependencies\n\n```toml\n[dependencies]\n# For library error types\nthiserror = \"1.0\"\n\n# For application error handling\nanyhow = \"1.0\"\n\n# For error reporting\ncolor-eyre = \"0.6\"  # Alternative to anyhow\n```\n\n## Error Type Patterns\n\n### Simple Enum Errors\n```rust\n#[derive(Error, Debug)]\npub enum ConfigError {\n    #[error(\"Missing field: {0}\")]\n    MissingField(String),\n\n    #[error(\"Invalid value for {field}: {value}\")]\n    InvalidValue { field: String, value: String },\n}\n```\n\n### Layered Errors\n```rust\n// Domain errors\n#[derive(Error, Debug)]\npub enum DomainError {\n    #[error(\"Validation failed: {0}\")]\n    Validation(String),\n\n    #[error(\"Business rule violated: {0}\")]\n    BusinessRule(String),\n}\n\n// Infrastructure errors\n#[derive(Error, Debug)]\npub enum InfraError {\n    #[error(\"Database error\")]\n    Database(#[from] sqlx::Error),\n\n    #[error(\"Network error\")]\n    Network(#[from] reqwest::Error),\n}\n\n// Application errors (combines both)\n#[derive(Error, Debug)]\npub enum AppError {\n    #[error(\"Domain error: {0}\")]\n    Domain(#[from] DomainError),\n\n    #[error(\"Infrastructure error: {0}\")]\n    Infra(#[from] InfraError),\n}\n```\n\n## Testing Error Handling\n\n```rust\n#[test]\nfn test_error_handling() {\n    let result = parse_config(\"invalid\");\n\n    assert!(result.is_err());\n\n    match result {\n        Err(ConfigError::InvalidValue { field, .. }) => {\n            assert_eq!(field, \"port\");\n        }\n        _ => panic!(\"Expected InvalidValue error\"),\n    }\n}\n```\n\n## Troubleshooting\n\n### thiserror not working\n- Ensure `thiserror = \"1.0\"` is in Cargo.toml\n- Check that derive macro is enabled\n\n### Error conversion not working\n- Verify #[from] attribute is used\n- Check that From trait is in scope\n- Ensure error types are compatible\n\n## Resources\n\n- [Rust Error Handling Best Practices](https://rust10x.com/best-practices/error-handling)\n- [thiserror documentation](https://docs.rs/thiserror/)\n- [anyhow documentation](https://docs.rs/anyhow/)\n- [Error Handling in Rust (Rust Book)](https://doc.rust-lang.org/book/ch09-00-error-handling.html)\n\n---\n\n**Handle errors gracefully, write robust Rust** ü¶Ä‚ú®\n",
        "plugins/rust-testing/README.md": "# Rust Testing Plugin\n\nA comprehensive plugin for implementing testing best practices in Rust applications, covering unit tests, integration tests, and property-based testing.\n\n## Overview\n\nThis plugin helps you write comprehensive tests for Rust applications, following best practices for unit testing, integration testing, test organization, and test-driven development.\n\n## Features\n\n### üß™ Testing Commands\n\n#### `/rust-test-add-unit`\nAdd unit tests for a specific function or module.\n\n**Features**:\n- Generate test module with #[cfg(test)]\n- Create tests for success and error cases\n- Add test fixtures and helpers\n- Include mock implementations\n- Generate property-based tests\n\n#### `/rust-test-add-integration`\nCreate integration tests in the tests/ directory.\n\n**Features**:\n- Set up integration test file structure\n- Create test fixtures and setup/teardown\n- Add database test containers\n- Generate API test clients\n- Include common test utilities\n\n#### `/rust-test-analyze`\nAnalyze test coverage and suggest improvements.\n\n**Features**:\n- Identify untested functions\n- Find missing error case tests\n- Suggest edge cases to test\n- Check test organization\n- Recommend testing strategies\n\n### ü§ñ Testing Expert Agent\n\nA specialized agent (`rust-test-expert`) for comprehensive testing guidance.\n\n**Capabilities**:\n- Design test strategies for complex code\n- Generate comprehensive test suites\n- Create mock implementations\n- Set up integration test infrastructure\n- Review test quality and coverage\n- Suggest property-based testing approaches\n\n## Testing Patterns\n\n### Unit Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_success_case() {\n        let result = my_function(\"valid input\");\n        assert_eq!(result, expected_output);\n    }\n\n    #[test]\n    fn test_error_case() {\n        let result = my_function(\"invalid\");\n        assert!(result.is_err());\n    }\n\n    #[test]\n    #[should_panic(expected = \"panic message\")]\n    fn test_panic_case() {\n        my_function_that_panics();\n    }\n}\n```\n\n### Integration Tests\n\n```rust\n// tests/integration_test.rs\nuse my_crate::*;\n\n#[test]\nfn test_full_workflow() {\n    let app = setup_test_app();\n\n    let result = app.process(\"input\");\n\n    assert_eq!(result, expected);\n}\n\nfn setup_test_app() -> App {\n    // Setup test instance\n}\n```\n\n### Async Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_async_function() {\n        let result = async_operation().await;\n        assert!(result.is_ok());\n    }\n}\n```\n\n### Mock Implementations\n\n```rust\n#[cfg(test)]\nmod tests {\n    struct MockRepository {\n        data: HashMap<String, User>,\n    }\n\n    impl UserRepository for MockRepository {\n        async fn find(&self, id: &str) -> Result<User, Error> {\n            self.data.get(id)\n                .cloned()\n                .ok_or(Error::NotFound)\n        }\n    }\n\n    #[tokio::test]\n    async fn test_with_mock() {\n        let mut mock = MockRepository::new();\n        mock.data.insert(\"1\".to_string(), test_user());\n\n        let service = UserService::new(mock);\n        let user = service.get_user(\"1\").await.unwrap();\n\n        assert_eq!(user.id, \"1\");\n    }\n}\n```\n\n### Property-Based Testing\n\n```rust\nuse proptest::prelude::*;\n\nproptest! {\n    #[test]\n    fn test_roundtrip(value in any::<u32>()) {\n        let serialized = serialize(value);\n        let deserialized = deserialize(&serialized).unwrap();\n        prop_assert_eq!(value, deserialized);\n    }\n\n    #[test]\n    fn test_property(input in \"[a-z]{1,10}\") {\n        let result = process(&input);\n        prop_assert!(result.len() > 0);\n    }\n}\n```\n\n## Best Practices\n\n1. **Test Organization**: Use #[cfg(test)] modules in source files for unit tests\n2. **Integration Tests**: Put integration tests in tests/ directory\n3. **Test Naming**: Use descriptive names (test_function_scenario_expected)\n4. **Setup/Teardown**: Use helper functions for test setup\n5. **Assertions**: Use specific assertions (assert_eq!, assert_matches!)\n6. **Error Testing**: Test both success and error paths\n7. **Edge Cases**: Test boundary conditions and edge cases\n8. **Async Testing**: Use #[tokio::test] for async tests\n9. **Test Fixtures**: Create reusable test data\n10. **Mocking**: Use trait objects or dedicated mocking libraries\n\n## Installation\n\n```bash\ncp -r plugins/rust-testing /path/to/your/project/plugins/\n```\n\nRegister in marketplace.json:\n```json\n{\n  \"plugins\": [{\n    \"name\": \"rust-testing\",\n    \"source\": \"./plugins/rust-testing\",\n    \"description\": \"Testing best practices for Rust\",\n    \"version\": \"1.0.0\"\n  }]\n}\n```\n\n## Usage\n\n### Quick Start\n\n1. **Add testing dependencies**:\n   ```bash\n   cargo add --dev tokio-test\n   cargo add --dev proptest\n   cargo add --dev wiremock\n   ```\n\n2. **Add unit tests**:\n   ```\n   /rust-test-add-unit\n   ```\n\n3. **Create integration tests**:\n   ```\n   /rust-test-add-integration\n   ```\n\n4. **Analyze coverage**:\n   ```\n   /rust-test-analyze\n   ```\n\n5. **Get testing help**:\n   ```\n   Ask rust-test-expert to help design tests for my service\n   ```\n\n## Common Testing Dependencies\n\n```toml\n[dev-dependencies]\n# Async testing\ntokio-test = \"0.4\"\n\n# Property-based testing\nproptest = \"1.0\"\nquickcheck = \"1.0\"\n\n# Mocking\nmockall = \"0.12\"\nwiremock = \"0.6\"  # For HTTP mocking\n\n# Test fixtures\nrstest = \"0.18\"\n\n# Test containers\ntestcontainers = \"0.15\"\n\n# Assertions\nassert_matches = \"1.5\"\npretty_assertions = \"1.4\"\n```\n\n## Test Organization\n\n```\nmy-project/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ lib.rs\n‚îÇ   ‚îú‚îÄ‚îÄ module.rs        # Unit tests in #[cfg(test)] module\n‚îÇ   ‚îî‚îÄ‚îÄ another.rs\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ integration_test.rs\n‚îÇ   ‚îú‚îÄ‚îÄ api_tests.rs\n‚îÇ   ‚îî‚îÄ‚îÄ common/\n‚îÇ       ‚îî‚îÄ‚îÄ mod.rs       # Shared test utilities\n‚îî‚îÄ‚îÄ benches/\n    ‚îî‚îÄ‚îÄ benchmarks.rs    # Performance benchmarks\n```\n\n## Running Tests\n\n```bash\n# Run all tests\ncargo test\n\n# Run specific test\ncargo test test_name\n\n# Run with output\ncargo test -- --nocapture\n\n# Run integration tests only\ncargo test --test integration_test\n\n# Run with coverage\ncargo tarpaulin --out Html\n```\n\n## Testing Strategies\n\n### Test-Driven Development (TDD)\n1. Write failing test\n2. Implement minimum code to pass\n3. Refactor\n4. Repeat\n\n### Table-Driven Tests\n```rust\n#[test]\nfn test_multiple_cases() {\n    let test_cases = vec![\n        (\"input1\", \"expected1\"),\n        (\"input2\", \"expected2\"),\n        (\"input3\", \"expected3\"),\n    ];\n\n    for (input, expected) in test_cases {\n        assert_eq!(process(input), expected);\n    }\n}\n```\n\n### Snapshot Testing\n```rust\n#[test]\nfn test_output_snapshot() {\n    let output = generate_complex_output();\n    insta::assert_snapshot!(output);\n}\n```\n\n## Troubleshooting\n\n### Tests not running\n- Check #[cfg(test)] module\n- Verify test function has #[test] attribute\n- Ensure cargo test is used, not cargo run\n\n### Async tests failing\n- Use #[tokio::test] instead of #[test]\n- Add tokio to dev-dependencies\n- Check async runtime configuration\n\n### Integration tests not found\n- Put tests in tests/ directory at project root\n- Each file in tests/ is a separate crate\n- Use common/ directory for shared code\n\n## Resources\n\n- [Rust Book: Testing](https://doc.rust-lang.org/book/ch11-00-testing.html)\n- [Rust By Example: Testing](https://doc.rust-lang.org/rust-by-example/testing.html)\n- [proptest documentation](https://docs.rs/proptest/)\n- [mockall documentation](https://docs.rs/mockall/)\n\n---\n\n**Test thoroughly, ship confidently** ü¶Ä‚úÖ\n",
        "plugins/rust-modern-patterns/README.md": "# Rust Modern Patterns Plugin\n\nA comprehensive plugin for using the latest Rust features and patterns from Rust 2024 Edition and beyond.\n\n## Overview\n\nThis plugin helps you write modern, idiomatic Rust code using the latest language features, patterns, and best practices. Stay up-to-date with Rust 2024 Edition features including let chains, async closures, improved match ergonomics, and more.\n\n## Current Rust Version\n\n**Latest Stable:** Rust 1.91.0 (as of November 2025)\n**Edition:** Rust 2024 Edition (stabilized in 1.85.0)\n**Note:** Rust 2024 Edition features are available from Rust 1.85.0 onwards\n\n## Features\n\n### üöÄ Modernization Commands\n\n#### `/rust-modernize`\nAnalyze and modernize code to use latest Rust features.\n\n**Features**:\n- Convert old patterns to Rust 2024 Edition features\n- Refactor to use let chains instead of nested if-let\n- Update to async closures where appropriate\n- Apply modern match ergonomics\n- Suggest const improvements\n\n#### `/rust-upgrade-edition`\nUpgrade project to Rust 2024 Edition.\n\n**Features**:\n- Update Cargo.toml to edition = \"2024\"\n- Check for breaking changes\n- Suggest migration paths\n- Update deprecated patterns\n- Verify MSRV compatibility\n\n#### `/rust-pattern-check`\nCheck code for opportunities to use modern patterns.\n\n**Features**:\n- Identify nested if-let chains\n- Find async fn that could use async closures\n- Suggest better match patterns\n- Identify const-eligible code\n- Check for outdated idioms\n\n#### `/rust-async-traits`\nConvert async-trait macro usage to native async fn in traits.\n\n**Features**:\n- Detect async-trait usage\n- Convert to native async fn (Rust 1.75+)\n- Identify when async-trait is still needed (dyn Trait)\n- Remove unnecessary dependencies\n- Improve performance by removing boxing overhead\n\n#### `/rust-quality-check`\nRun comprehensive quality checks using modern tooling.\n\n**Features**:\n- Format verification (cargo fmt)\n- Linting with clippy (pedantic mode)\n- Test suite with cargo-nextest\n- Security audit with cargo-audit\n- Dependency validation with cargo-deny\n- SemVer checks for libraries\n\n#### `/rust-setup-tooling`\nSet up modern Rust development environment.\n\n**Features**:\n- Install essential tools (bacon, nextest, audit, deny)\n- Create configuration files (clippy.toml, rustfmt.toml, deny.toml)\n- Generate development scripts\n- Set up CI/CD workflows\n- Create developer documentation\n\n### ü§ñ Modern Rust Expert Agent\n\nA specialized agent (`rust-modern-expert`) for modern Rust patterns.\n\n**Capabilities**:\n- Teach latest Rust 2024 features\n- Refactor code to modern patterns\n- Review code for best practices\n- Design using latest features\n- Migration guidance to Rust 2024\n\n## Rust 2024 Edition Features\n\n### Let Chains (Stabilized in 1.88)\n\nChain multiple let patterns with `&&` in if/while expressions:\n\n```rust\n// ‚ùå Old way (nested)\nif let Some(user) = get_user() {\n    if let Some(email) = user.email {\n        if email.contains('@') {\n            send_email(&email);\n        }\n    }\n}\n\n// ‚úÖ Modern way (let chains)\nif let Some(user) = get_user()\n    && let Some(email) = user.email\n    && email.contains('@')\n{\n    send_email(&email);\n}\n```\n\n### Async Closures\n\nUse async closures with automatic trait bounds:\n\n```rust\n// ‚ùå Old way\nlet futures: Vec<_> = items\n    .iter()\n    .map(|item| {\n        let item = item.clone();\n        async move { process(item).await }\n    })\n    .collect();\n\n// ‚úÖ Modern way\nlet futures: Vec<_> = items\n    .iter()\n    .map(async |item| {\n        process(item).await\n    })\n    .collect();\n```\n\n### Async Functions in Traits (Native Support)\n\n**Important:** Since Rust 1.75, async functions in traits are natively supported. The `async-trait` crate is now **optional** and only needed for:\n- Supporting older Rust versions (< 1.75)\n- Traits that need to be object-safe (dyn Trait)\n- Specific edge cases with complex generic bounds\n\n```rust\n// ‚úÖ Modern: Native async fn in traits (Rust 1.75+)\ntrait UserRepository {\n    async fn find_user(&self, id: &str) -> Result<User, Error>;\n    async fn save_user(&self, user: &User) -> Result<(), Error>;\n}\n\n// Implementation\nimpl UserRepository for PostgresRepo {\n    async fn find_user(&self, id: &str) -> Result<User, Error> {\n        // Native async, no macro needed!\n        sqlx::query_as!(User, \"SELECT * FROM users WHERE id = $1\", id)\n            .fetch_one(&self.pool)\n            .await\n    }\n\n    async fn save_user(&self, user: &User) -> Result<(), Error> {\n        // ...\n    }\n}\n\n// ‚ùå Only use async-trait when you need dyn Trait\nuse async_trait::async_trait;\n\n#[async_trait]\ntrait DynamicRepository {\n    async fn fetch(&self) -> Result<Data, Error>;\n}\n\n// This is needed for:\nlet repo: Box<dyn DynamicRepository> = Box::new(my_repo);\n```\n\n### Improved Match Ergonomics\n\nBetter pattern matching with clearer semantics:\n\n```rust\n// ‚úÖ Rust 2024: mut doesn't force by-value binding\nmatch &data {\n    Some(mut x) => {\n        // x is &mut T, not T\n        x.modify();\n    }\n    None => {}\n}\n```\n\n### Const Improvements\n\nMore capabilities in const contexts:\n\n```rust\n// ‚úÖ Reference static items in const contexts\nconst CONFIG: &Config = &GLOBAL_CONFIG;\n\n// ‚úÖ More const-capable standard library functions\nconst fn parse_number(s: &str) -> Option<i32> {\n    // More operations allowed in const fn\n}\n```\n\n### MSRV-Aware Resolver\n\nAutomatic dependency resolution respecting MSRV:\n\n```toml\n[package]\nedition = \"2024\"\nrust-version = \"1.75\"  # MSRV\n\n# Resolver automatically picks compatible versions\n```\n\n### Gen Blocks (Rust 2024)\n\nGenerator blocks for iterators:\n\n```rust\nlet fibonacci = gen {\n    let (mut a, mut b) = (0, 1);\n    loop {\n        yield a;\n        (a, b) = (b, a + b);\n    }\n};\n```\n\n### Never Type `!`\n\nProper never type for functions that never return:\n\n```rust\nfn exit_program() -> ! {\n    std::process::exit(0)\n}\n\nfn parse_or_panic(s: &str) -> i32 {\n    s.parse().unwrap_or_else(|_| exit_program())\n}\n```\n\n## Modern Patterns\n\n### Pattern 1: Let Chains for Complex Conditions\n\n```rust\n// Checking multiple Option/Result values\nif let Ok(config) = load_config()\n    && let Some(db_url) = config.database_url\n    && let Ok(pool) = connect_to_db(&db_url)\n{\n    // All conditions met, use pool\n}\n```\n\n### Pattern 2: Async Closures for Concurrent Operations\n\n```rust\nasync fn process_items<F>(items: Vec<Item>, processor: F)\nwhere\n    F: AsyncFn(Item) -> Result<(), Error>,\n{\n    for item in items {\n        processor(item).await?;\n    }\n}\n\n// Usage\nprocess_items(items, async |item| {\n    validate(&item).await?;\n    save_to_db(&item).await\n}).await?;\n```\n\n### Pattern 3: Modern Match with Deref Patterns\n\n```rust\nmatch &option_box {\n    Some(deref!(value)) => {\n        // Use value directly\n    }\n    None => {}\n}\n```\n\n### Pattern 4: Const Functions for Compile-Time Computation\n\n```rust\nconst fn compute_table_size(items: usize) -> usize {\n    items * std::mem::size_of::<Item>()\n}\n\nconst TABLE_SIZE: usize = compute_table_size(1000);\nstatic TABLE: [u8; TABLE_SIZE] = [0; TABLE_SIZE];\n```\n\n### üéì Skills\n\n#### Modern Rust Tooling Guide\nExpert guidance on the 2025 Rust tooling ecosystem:\n- **Development Tools**: bacon, cargo-nextest, cargo-watch\n- **Code Quality**: clippy (pedantic mode), rustfmt\n- **Security**: cargo-audit, cargo-deny, cargo-semver-checks\n- **Performance**: cargo-flamegraph, profiling workflows\n- **CI/CD**: Best practices and pipeline templates\n\n#### Type-Driven Design Patterns\nLeverage Rust's type system for compile-time correctness:\n- **Newtype Pattern**: Type-safe domain identifiers and units\n- **Typestate Pattern**: Enforce state machines at compile time\n- **Builder Pattern**: Compile-time validation of required fields\n- **Phantom Types**: Generic type safety without runtime cost\n- **Session Types**: Protocol enforcement through types\n\n#### Let Chains Advisor\nMaster let chains for cleaner control flow:\n- Identify nested if-let patterns\n- Convert to flat let chains\n- Combine with guards and conditions\n\n#### Rust 2024 Migration\nGuide projects to Rust 2024 Edition:\n- Edition upgrade process\n- Breaking change detection\n- Modern pattern adoption\n\n#### Async Patterns Guide\nModern async Rust patterns:\n- Native async fn in traits (no async-trait needed for Rust 1.75+)\n- Async closures for cleaner async code\n- Runtime selection (Tokio vs smol)\n- **Note**: async-std is discontinued as of 2025\n\n## Modern Development Workflow\n\n### Essential Tooling Setup\n\n```bash\n# Install modern development tools\ncargo install bacon              # Background compiler\ncargo install cargo-nextest      # Fast test runner\ncargo install cargo-audit        # Security scanning\ncargo install cargo-deny         # Dependency linting\ncargo install cargo-semver-checks # API compatibility\ncargo install flamegraph         # Performance profiling\n```\n\n### Daily Development\n\n1. **Start bacon for continuous feedback**:\n   ```bash\n   bacon clippy\n   ```\n\n2. **Run tests with nextest**:\n   ```bash\n   cargo nextest run\n   ```\n\n3. **Before committing**:\n   ```bash\n   cargo fmt               # Format code\n   cargo clippy -- -D warnings  # Lint\n   cargo nextest run       # Test\n   cargo audit             # Security check\n   ```\n\n### CI/CD Integration\n\nSee the [Rust Tooling Guide skill](skills/rust-tooling-guide/SKILL.md) for complete CI/CD pipeline templates.\n\n## Best Practices\n\n### Language Features\n1. **Use Let Chains**: Reduce nesting with let chains in if/while\n2. **Async Closures**: Use async closures for better async ergonomics\n3. **Edition 2024**: Upgrade to edition = \"2024\" for latest features\n4. **Const When Possible**: Use const fn for compile-time computation\n5. **Match Ergonomics**: Leverage improved match patterns\n6. **MSRV Awareness**: Set rust-version in Cargo.toml\n7. **Never Type**: Use `!` for functions that never return\n8. **Gen Blocks**: Use generators for custom iterators\n\n### Type-Driven Design\n9. **Newtype Pattern**: Wrap primitives for type safety\n10. **Typestate Pattern**: Encode state machines in types\n11. **Builder with Typestate**: Compile-time required fields\n12. **Phantom Types**: Zero-cost generic safety\n\n### Modern Tooling\n13. **Bacon**: Continuous feedback during development\n14. **Cargo-nextest**: Faster, more reliable tests\n15. **Clippy Pedantic**: Catch issues early\n16. **Cargo-audit**: Weekly security scans\n17. **Cargo-deny**: License and dependency compliance\n18. **Flamegraph**: Profile before optimizing\n\n### Async Runtime Selection (2025)\n19. **Tokio**: Production-ready, largest ecosystem\n20. **smol**: Simpler, smaller, modular alternative\n21. **Avoid async-std**: Discontinued as of 2025\n\n## Installation\n\n```bash\ncp -r plugins/rust-modern-patterns /path/to/your/project/plugins/\n```\n\nRegister in marketplace.json:\n```json\n{\n  \"plugins\": [{\n    \"name\": \"rust-modern-patterns\",\n    \"source\": \"./plugins/rust-modern-patterns\",\n    \"description\": \"Modern Rust patterns for Rust 2024 Edition\",\n    \"version\": \"1.0.0\"\n  }]\n}\n```\n\n## Usage\n\n### Quick Start\n\n1. **Check your code for modernization opportunities**:\n   ```\n   /rust-pattern-check\n   ```\n\n2. **Upgrade to Rust 2024 Edition**:\n   ```\n   /rust-upgrade-edition\n   ```\n\n3. **Modernize specific code**:\n   ```\n   /rust-modernize\n   ```\n\n4. **Get expert guidance**:\n   ```\n   Ask rust-modern-expert to help modernize my code to use let chains\n   ```\n\n## Cargo.toml Configuration\n\n```toml\n[package]\nname = \"my-project\"\nversion = \"0.1.0\"\nedition = \"2024\"  # Use Rust 2024 Edition\nrust-version = \"1.85\"  # Minimum Rust version\n\n[dependencies]\n# Your dependencies\n```\n\n## Migration Guide\n\n### Upgrading to Rust 2024\n\n1. **Update Cargo.toml**:\n   ```toml\n   edition = \"2024\"\n   ```\n\n2. **Run cargo fix**:\n   ```bash\n   cargo fix --edition\n   ```\n\n3. **Review warnings**:\n   ```bash\n   cargo check\n   ```\n\n4. **Update patterns**:\n   - Convert nested if-let to let chains\n   - Use async closures where appropriate\n   - Update match patterns for new ergonomics\n\n## Feature Compatibility\n\n| Feature | Minimum Rust Version | Edition |\n|---------|---------------------|---------|\n| Let Chains | 1.88.0 | 2024 |\n| Async Closures | 1.85.0 | 2024 |\n| Match Ergonomics | 1.85.0 | 2024 |\n| Gen Blocks | 1.85.0 | 2024 |\n| MSRV Resolver | 1.84.0 | Any |\n| Const Improvements | 1.83.0+ | Any |\n\n## Resources\n\n- [Rust 2024 Edition Guide](https://doc.rust-lang.org/edition-guide/rust-2024/)\n- [Let Chains RFC](https://rust-lang.github.io/rfcs/2497-if-let-chains.html)\n- [Async Closures](https://blog.rust-lang.org/2025/02/20/Rust-1.85.0/)\n- [Match Ergonomics 2024](https://rust-lang.github.io/rfcs/3627-match-ergonomics-2024.html)\n- [Rust Release Notes](https://doc.rust-lang.org/releases.html)\n\n## Troubleshooting\n\n### Edition 2024 not recognized\n- Ensure Rust 1.85.0 or later: `rustup update`\n- Check `rustc --version`\n\n### Let chains not working\n- Verify edition = \"2024\" in Cargo.toml\n- Ensure Rust 1.88.0 or later\n\n### Async closures not compiling\n- Check Rust version (1.85.0+)\n- Verify proper trait bounds (AsyncFn/AsyncFnMut)\n\n## Version History\n\n- **1.0.0** - Initial release with Rust 2024 Edition features\n\n---\n\n**Write modern Rust, leverage the latest features** ü¶Ä‚ú®\n",
        "plugins/rust-data-engineering/README.md": "# Rust Data Engineering Plugin\n\nExpert guidance for building data pipelines, lakehouse architectures, and analytical workloads in Rust using modern data engineering tools.\n\n## Overview\n\nThis plugin provides comprehensive support for Rust-based data engineering workflows, focusing on cloud-native data lakes, analytical query engines, and efficient data formats. Perfect for teams building ETL pipelines, lakehouse architectures, or analytical applications.\n\n## Key Technologies\n\n- **object_store** - Unified cloud storage abstraction (S3, Azure Blob, GCS, local)\n- **Apache Arrow** - High-performance columnar memory format\n- **Apache Parquet** - Efficient columnar storage format\n- **DataFusion** - Fast SQL query engine built on Arrow\n- **Apache Iceberg** - Table format for large analytical datasets\n- **CSV/JSON** - Structured data parsing and writing\n\n## Features\n\n### Commands\n\n#### Cloud Storage & Object Store\n\n- `/data-object-store-setup` - Configure object_store for S3, Azure, GCS, or local storage\n- `/data-object-store-read` - Read objects with streaming, buffering, and retry strategies\n- `/data-object-store-write` - Write objects with multipart uploads and atomic operations\n\n#### Parquet Operations\n\n- `/data-parquet-read` - Read Parquet files with predicate pushdown and column projection\n- `/data-parquet-write` - Write Parquet files with compression, encoding, and row group tuning\n- `/data-parquet-schema` - Define and evolve Parquet schemas\n\n#### DataFusion Queries\n\n- `/data-datafusion-setup` - Create DataFusion execution context and register data sources\n- `/data-datafusion-query` - Execute SQL queries against Parquet, CSV, and in-memory data\n- `/data-datafusion-custom` - Build custom table providers and UDFs\n\n#### Iceberg Tables\n\n- `/data-iceberg-table` - Create and manage Iceberg tables\n- `/data-iceberg-query` - Query Iceberg tables with time travel\n- `/data-iceberg-evolution` - Handle schema evolution and partitioning\n\n#### Data Pipeline Patterns\n\n- `/data-pipeline-etl` - Design ETL pipelines with streaming and batch processing\n- `/data-partition-strategy` - Implement partitioning strategies (Hive, date-based, custom)\n- `/data-schema-evolution` - Handle schema changes safely across versions\n\n### Expert Agent\n\n- `@data-engineering-expert` - Specialized agent for data engineering architecture decisions, performance optimization, and data lake best practices\n\n## Installation\n\nAdd this plugin to your Claude Code marketplace:\n\n```bash\n# From the marketplace root\ncd .claude-plugin\n# Plugin will be auto-discovered from plugins/rust-data-engineering\n```\n\nThe plugin includes commands and an expert agent for data engineering workflows.\n\n## Quick Start\n\n### 1. Set Up Object Store\n\n```bash\n/data-object-store-setup\n```\n\nCreates object_store configuration for your cloud provider:\n\n```rust\nuse object_store::{aws::AmazonS3Builder, ObjectStore};\nuse std::sync::Arc;\n\nlet s3 = AmazonS3Builder::new()\n    .with_region(\"us-east-1\")\n    .with_bucket_name(\"my-data-lake\")\n    .with_access_key_id(access_key)\n    .with_secret_access_key(secret_key)\n    .build()?;\n\nlet store: Arc<dyn ObjectStore> = Arc::new(s3);\n```\n\n### 2. Read Parquet Files\n\n```bash\n/data-parquet-read\n```\n\nEfficiently read Parquet data with predicate pushdown:\n\n```rust\nuse parquet::arrow::async_reader::ParquetObjectReader;\nuse parquet::arrow::ParquetRecordBatchStreamBuilder;\nuse object_store::path::Path;\n\nlet path = Path::from(\"data/events/year=2024/month=01/data.parquet\");\nlet meta = store.head(&path).await?;\n\nlet reader = ParquetObjectReader::new(store.clone(), meta);\nlet builder = ParquetRecordBatchStreamBuilder::new(reader).await?;\n\n// Column projection\nlet builder = builder.with_projection(vec![0, 2, 5]);\n\n// Predicate pushdown\nlet builder = builder.with_row_filter(/* ... */);\n\nlet mut stream = builder.build()?;\nwhile let Some(batch) = stream.next().await {\n    let batch = batch?;\n    // Process RecordBatch\n}\n```\n\n### 3. Query with DataFusion\n\n```bash\n/data-datafusion-query\n```\n\nRun SQL queries against cloud-stored Parquet files:\n\n```rust\nuse datafusion::prelude::*;\nuse datafusion::execution::context::SessionContext;\n\nlet ctx = SessionContext::new();\n\n// Register object store\nlet url = \"s3://my-data-lake/\";\nctx.runtime_env()\n    .register_object_store(url, store.clone());\n\n// Register Parquet table\nctx.register_parquet(\n    \"events\",\n    \"s3://my-data-lake/data/events/\",\n    ParquetReadOptions::default(),\n).await?;\n\n// Execute SQL\nlet df = ctx.sql(\"\n    SELECT user_id, COUNT(*) as event_count\n    FROM events\n    WHERE date >= '2024-01-01'\n    GROUP BY user_id\n    ORDER BY event_count DESC\n    LIMIT 100\n\").await?;\n\ndf.show().await?;\n```\n\n### 4. Work with Iceberg Tables\n\n```bash\n/data-iceberg-table\n```\n\nCreate and manage Iceberg tables for ACID guarantees and schema evolution:\n\n```rust\nuse iceberg_rust::{Table, TableIdentifier};\nuse iceberg_rust::catalog::Catalog;\n\n// Open Iceberg table\nlet catalog = /* ... */;\nlet table = catalog.load_table(\n    &TableIdentifier::new(&[\"db\", \"events\"])\n).await?;\n\n// Time travel query\nlet snapshot_id = table.metadata()\n    .snapshots()\n    .get(5)\n    .snapshot_id();\n\nlet scan = table.scan()\n    .snapshot_id(snapshot_id)\n    .build()?;\n\n// Schema evolution\nlet mut update = table.update_schema();\nupdate.add_column(\"new_field\", DataType::String)?;\nupdate.commit().await?;\n```\n\n## Common Workflows\n\n### ETL Pipeline Architecture\n\nUse `/data-pipeline-etl` to design production ETL pipelines:\n\n1. **Extract** - Read from object_store with retry and backpressure\n2. **Transform** - Use DataFusion or Arrow compute kernels\n3. **Load** - Write Parquet with optimal row group sizes and compression\n\n```rust\n// Streaming ETL\nasync fn process_batch(\n    store: Arc<dyn ObjectStore>,\n    input_path: &Path,\n    output_path: &Path,\n) -> Result<()> {\n    // Read with streaming\n    let reader = ParquetObjectReader::new(/* ... */);\n    let stream = ParquetRecordBatchStreamBuilder::new(reader)\n        .await?\n        .build()?;\n\n    // Transform with Arrow compute\n    let transformed = stream.map(|batch| {\n        let batch = batch?;\n        apply_transformations(batch)\n    });\n\n    // Write with optimal settings\n    let props = WriterProperties::builder()\n        .set_compression(Compression::ZSTD(ZstdLevel::default()))\n        .set_max_row_group_size(1_000_000)\n        .build();\n\n    write_parquet_stream(store, output_path, transformed, props).await?;\n    Ok(())\n}\n```\n\n### Partitioning Strategy\n\nUse `/data-partition-strategy` for efficient data organization:\n\n```\ndata-lake/\n‚îî‚îÄ‚îÄ events/\n    ‚îú‚îÄ‚îÄ year=2024/\n    ‚îÇ   ‚îú‚îÄ‚îÄ month=01/\n    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day=01/\n    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ part-00000.parquet\n    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ part-00001.parquet\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ day=02/\n    ‚îÇ   ‚îî‚îÄ‚îÄ month=02/\n```\n\nEnables partition pruning in queries:\n```sql\nSELECT * FROM events\nWHERE year = 2024 AND month = 1 AND day >= 15\n-- Only scans relevant partitions\n```\n\n### Schema Evolution\n\nUse `/data-schema-evolution` to safely evolve schemas:\n\n```rust\n// Add optional fields (backward compatible)\nschema_update.add_column(\"new_metric\", DataType::Float64)?;\n\n// Rename fields (maintain compatibility)\nschema_update.rename_column(\"old_name\", \"new_name\")?;\n\n// Parquet handles missing fields gracefully\n// Old files return NULL for new columns\n```\n\n## Performance Best Practices\n\n### Object Store Optimization\n\n- Use `with_retry()` for resilient cloud operations\n- Enable `with_concurrent_request_limit()` for rate limiting\n- Configure multipart uploads for large files (>100MB)\n\n### Parquet Tuning\n\n- **Row group size**: 100MB-1GB for optimal S3 scanning\n- **Compression**: ZSTD for balanced compression/speed, Snappy for speed\n- **Page size**: 1MB default works well for most cases\n- **Column encoding**: Use dictionary encoding for low-cardinality columns\n\n### DataFusion Query Optimization\n\n- Enable predicate pushdown and column projection\n- Use `EXPLAIN` to inspect query plans\n- Partition data to enable partition pruning\n- Create statistics for better query planning\n\n### Iceberg Benefits\n\n- **ACID transactions**: Atomic commits prevent partial updates\n- **Time travel**: Query historical table states\n- **Schema evolution**: Add/rename/reorder columns safely\n- **Partition evolution**: Change partitioning without rewriting data\n- **Hidden partitioning**: Partition on derived values transparently\n\n## Integration Examples\n\n### S3 + Parquet + DataFusion\n\n```rust\nuse object_store::aws::AmazonS3Builder;\nuse datafusion::prelude::*;\n\n// S3 object store\nlet s3 = AmazonS3Builder::from_env()\n    .with_bucket_name(\"data-lake\")\n    .build()?;\n\n// DataFusion context\nlet ctx = SessionContext::new();\nctx.runtime_env()\n    .register_object_store(\"s3://data-lake/\", Arc::new(s3));\n\n// Query Parquet on S3\nctx.register_parquet(\"sales\", \"s3://data-lake/sales/\", Default::default()).await?;\nlet df = ctx.sql(\"SELECT product, SUM(amount) FROM sales GROUP BY product\").await?;\n```\n\n### Streaming ETL with Arrow\n\n```rust\nuse arrow::compute::kernels::filter;\nuse arrow::compute::kernels::aggregate;\n\n// Stream large datasets\nlet mut stream = parquet_stream;\nwhile let Some(batch) = stream.next().await {\n    let batch = batch?;\n\n    // Filter rows\n    let predicate = /* boolean array */;\n    let filtered = filter::filter_record_batch(&batch, &predicate)?;\n\n    // Compute aggregates\n    let values = filtered.column(0);\n    let sum = aggregate::sum(values)?;\n\n    // Write results\n}\n```\n\n## Troubleshooting\n\n### Large File Memory Issues\n\n**Problem**: OOM when reading large Parquet files\n\n**Solution**: Use streaming readers and limit batch size:\n```rust\nlet builder = ParquetRecordBatchStreamBuilder::new(reader)\n    .await?\n    .with_batch_size(8192); // Smaller batches\n```\n\n### S3 Rate Limiting\n\n**Problem**: 503 SlowDown errors from S3\n\n**Solution**: Configure retry and backoff:\n```rust\nlet s3 = AmazonS3Builder::new()\n    .with_retry(RetryConfig::default())\n    .build()?;\n```\n\n### Schema Mismatch Errors\n\n**Problem**: Incompatible schemas when reading multiple files\n\n**Solution**: Use schema evolution or unified schema:\n```rust\n// Read with schema adaptation\nlet options = ParquetReadOptions {\n    schema_force_view: Some(unified_schema),\n    ..Default::default()\n};\n```\n\n## Resources\n\n- [object_store crate](https://docs.rs/object_store/)\n- [Apache Arrow Rust](https://docs.rs/arrow/)\n- [Apache Parquet Rust](https://docs.rs/parquet/)\n- [DataFusion User Guide](https://arrow.apache.org/datafusion/)\n- [Apache Iceberg Rust](https://github.com/apache/iceberg-rust)\n- [Parquet Format Specification](https://parquet.apache.org/docs/)\n\n## Author\n\nEmil Lindfors\n\n## Version\n\n1.0.0\n",
        "plugins/rust-lambda/README.md": "# Rust Lambda Plugin\n\nA comprehensive plugin for building, deploying, and optimizing AWS Lambda functions using Rust with cargo-lambda and the lambda-runtime library.\n\n## Overview\n\nThis plugin provides guidance and best practices for developing AWS Lambda functions in Rust, covering everything from initial setup to advanced optimization strategies. Learn how to build both IO-intensive async lambdas and compute-intensive sync lambdas.\n\n## Features\n\n### üöÄ Development Commands\n\n#### `/lambda-new`\nCreate a new Rust Lambda function project.\n\n**Features**:\n- Initialize project with cargo-lambda\n- Set up proper project structure\n- Configure runtime and dependencies\n- Generate basic handler templates\n\n#### `/lambda-build`\nBuild Lambda function for deployment.\n\n**Features**:\n- Cross-compile for AWS Lambda runtime\n- Optimize binary size\n- Support for ARM64 and x86_64\n- Release builds with proper flags\n\n#### `/lambda-deploy`\nDeploy Lambda function to AWS.\n\n**Features**:\n- Deploy with cargo-lambda\n- Update existing functions\n- Configure environment variables\n- Set memory and timeout\n\n#### `/lambda-github-actions`\nSet up GitHub Actions CI/CD for Lambda deployment.\n\n**Features**:\n- Complete workflow templates\n- AWS credentials configuration\n- Build and deploy automation\n- Multi-architecture support\n\n#### `/lambda-optimize-io`\nOptimize Lambda for IO-intensive workloads.\n\n**Features**:\n- Async/await best practices\n- Concurrent request handling\n- Connection pooling strategies\n- Efficient async runtime usage\n\n#### `/lambda-optimize-compute`\nOptimize Lambda for compute-intensive workloads.\n\n**Features**:\n- Rayon parallel processing\n- spawn_blocking patterns\n- Mixed async/sync strategies\n- Thread pool configuration\n\n#### `/lambda-iac`\nSet up Infrastructure as Code with SAM, Terraform, or CDK.\n\n**Features**:\n- SAM templates for serverless deployments\n- Terraform modules for Lambda infrastructure\n- CDK examples in TypeScript/Python\n- LocalStack integration for local testing\n\n#### `/lambda-observability`\nImplement advanced observability with OpenTelemetry and X-Ray.\n\n**Features**:\n- AWS X-Ray integration with xray-lite\n- OpenTelemetry setup with lambda-otel-lite\n- Structured logging with tracing\n- Distributed tracing patterns\n- CloudWatch Logs Insights queries\n\n#### `/lambda-secrets`\nManage secrets and configuration securely.\n\n**Features**:\n- AWS Secrets Manager integration\n- Parameter Store usage patterns\n- Parameters and Secrets Lambda Extension\n- Caching strategies\n- Environment-specific configuration\n\n#### `/lambda-function-urls`\nSet up Lambda Function URLs and streaming responses.\n\n**Features**:\n- Direct HTTPS endpoints without API Gateway\n- REST API patterns with lambda_http\n- Response streaming for large payloads (up to 20MB)\n- CORS configuration\n- Authentication patterns\n\n#### `/lambda-cost`\nDeep dive into cost optimization strategies.\n\n**Features**:\n- AWS Lambda Power Tuning integration\n- Memory vs CPU allocation strategies\n- Rust-specific optimizations\n- ARM64 savings (20% cheaper)\n- Cost monitoring and alerts\n\n#### `/lambda-advanced`\nAdvanced topics: extensions, containers, and more.\n\n**Features**:\n- Custom Lambda Extensions in Rust\n- Container image deployments\n- VPC configuration\n- Reserved concurrency\n- Blue/green deployments\n- Multi-region strategies\n\n### ü§ñ Lambda Expert Agent\n\nA specialized agent (`rust-lambda-expert`) for Lambda development.\n\n**Capabilities**:\n- Design Lambda architectures\n- Optimize for cold start performance\n- Review Lambda code for best practices\n- Debug Lambda-specific issues\n- Guide on async vs sync patterns\n\n## Getting Started\n\n### Prerequisites\n\n1. **Install Rust** (1.75.0 or later recommended):\n   ```bash\n   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n   ```\n\n2. **Install cargo-lambda**:\n   ```bash\n   # macOS/Linux\n   brew tap cargo-lambda/cargo-lambda\n   brew install cargo-lambda\n\n   # Or using pip\n   pip install cargo-lambda\n\n   # Or from source\n   cargo install cargo-lambda\n   ```\n\n3. **Install Zig** (for cross-compilation):\n   ```bash\n   # macOS\n   brew install zig\n\n   # Linux\n   # Download from https://ziglang.org/download/\n   ```\n\n### Quick Start\n\n1. **Create a new Lambda project**:\n   ```bash\n   cargo lambda new my-lambda-function\n   cd my-lambda-function\n   ```\n\n2. **Test locally**:\n   ```bash\n   cargo lambda watch\n\n   # In another terminal\n   cargo lambda invoke --data-ascii '{\"key\": \"value\"}'\n   ```\n\n3. **Build for Lambda**:\n   ```bash\n   cargo lambda build --release\n   ```\n\n4. **Deploy to AWS**:\n   ```bash\n   cargo lambda deploy\n   ```\n\n## Core Dependencies\n\nAdd these to your `Cargo.toml`:\n\n```toml\n[package]\nname = \"my-lambda\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[dependencies]\n# Core Lambda runtime\nlambda_runtime = \"0.13\"\ntokio = { version = \"1\", features = [\"macros\"] }\n\n# Serialization\nserde = { version = \"1\", features = [\"derive\"] }\nserde_json = \"1\"\n\n# Error handling\nanyhow = \"1\"\nthiserror = \"1\"\n\n# Tracing/Logging\ntracing = { version = \"0.1\", features = [\"log\"] }\ntracing-subscriber = { version = \"0.3\", features = [\"env-filter\"] }\n\n# For compute-intensive tasks\nrayon = \"1.10\"\n\n# For async HTTP requests (IO-intensive)\nreqwest = { version = \"0.12\", features = [\"json\"] }\n\n[profile.release]\nopt-level = 'z'     # Optimize for size\nlto = true          # Enable Link Time Optimization\ncodegen-units = 1   # Reduce number of codegen units\nstrip = true        # Strip symbols from binary\n```\n\n## Lambda Patterns\n\n### Pattern 1: IO-Intensive Lambda (Async)\n\nFor lambdas that make many API calls, database queries, or external requests:\n\n```rust\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse serde::{Deserialize, Serialize};\nuse tracing::info;\n\n#[derive(Deserialize)]\nstruct Request {\n    user_ids: Vec<String>,\n}\n\n#[derive(Serialize)]\nstruct Response {\n    users: Vec<User>,\n}\n\n#[derive(Serialize)]\nstruct User {\n    id: String,\n    name: String,\n}\n\n/// IO-intensive lambda: maximize async concurrency\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    info!(\"Processing {} user IDs\", event.payload.user_ids.len());\n\n    // Process requests concurrently using async\n    let user_futures = event.payload.user_ids\n        .into_iter()\n        .map(|user_id| async move {\n            // Each request is async and non-blocking\n            fetch_user_from_api(&user_id).await\n        });\n\n    // Wait for all requests to complete concurrently\n    let users = futures::future::try_join_all(user_futures).await?;\n\n    Ok(Response { users })\n}\n\nasync fn fetch_user_from_api(user_id: &str) -> Result<User, Error> {\n    // Async HTTP request - doesn't block the runtime\n    let client = reqwest::Client::new();\n    let response = client\n        .get(format!(\"https://api.example.com/users/{}\", user_id))\n        .send()\n        .await?\n        .json::<User>()\n        .await?;\n\n    Ok(response)\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::INFO)\n        .with_target(false)\n        .without_time()\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n```\n\n**Key Points**:\n- Use async throughout for IO operations\n- Leverage `futures::future::try_join_all` for concurrency\n- Don't block the async runtime\n- Efficient for high-latency, low-CPU operations\n\n### Pattern 2: Compute-Intensive Lambda (Sync with Rayon)\n\nFor lambdas that do CPU-heavy processing (image processing, data transformation, etc.):\n\n```rust\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse rayon::prelude::*;\nuse serde::{Deserialize, Serialize};\nuse tokio::task;\nuse tracing::info;\n\n#[derive(Deserialize)]\nstruct Request {\n    numbers: Vec<i64>,\n}\n\n#[derive(Serialize)]\nstruct Response {\n    results: Vec<i64>,\n}\n\n/// Compute-intensive lambda: use sync processing with Rayon\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    info!(\"Processing {} numbers\", event.payload.numbers.len());\n\n    let numbers = event.payload.numbers;\n\n    // Async at the boundaries: spawn_blocking for CPU work\n    let results = task::spawn_blocking(move || {\n        // Use Rayon for parallel CPU-intensive work\n        numbers\n            .par_iter()\n            .map(|&num| expensive_computation(num))\n            .collect::<Vec<_>>()\n    })\n    .await?;\n\n    Ok(Response { results })\n}\n\n/// CPU-intensive synchronous function\nfn expensive_computation(n: i64) -> i64 {\n    // Simulate expensive computation\n    // (e.g., cryptographic hashing, image processing, etc.)\n    (0..1000).fold(n, |acc, _| {\n        acc.wrapping_mul(31).wrapping_add(17)\n    })\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::INFO)\n        .with_target(false)\n        .without_time()\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n```\n\n**Key Points**:\n- Use `tokio::task::spawn_blocking` to run sync code\n- Use Rayon for parallel CPU work within the blocking task\n- Keep async only at input/output boundaries\n- Efficient for CPU-bound operations\n\n### Pattern 3: Mixed Workload Lambda\n\nFor lambdas that combine IO and compute:\n\n```rust\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\nuse rayon::prelude::*;\nuse serde::{Deserialize, Serialize};\nuse tokio::task;\nuse tracing::info;\n\n#[derive(Deserialize)]\nstruct Request {\n    image_urls: Vec<String>,\n}\n\n#[derive(Serialize)]\nstruct Response {\n    processed_count: usize,\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    info!(\"Processing {} images\", event.payload.image_urls.len());\n\n    // Step 1: Async IO - Download all images concurrently\n    let download_futures = event.payload.image_urls\n        .into_iter()\n        .map(|url| async move {\n            let response = reqwest::get(&url).await?;\n            let bytes = response.bytes().await?;\n            Ok::<_, Error>(bytes.to_vec())\n        });\n\n    let images = futures::future::try_join_all(download_futures).await?;\n\n    // Step 2: Sync compute - Process images in parallel with Rayon\n    let processed = task::spawn_blocking(move || {\n        images\n            .par_iter()\n            .map(|image_data| {\n                // CPU-intensive image processing\n                process_image(image_data)\n            })\n            .collect::<Result<Vec<_>, _>>()\n    })\n    .await??;\n\n    // Step 3: Async IO - Upload results concurrently\n    let upload_futures = processed\n        .into_iter()\n        .enumerate()\n        .map(|(i, data)| async move {\n            upload_to_s3(&format!(\"processed-{}.jpg\", i), &data).await\n        });\n\n    futures::future::try_join_all(upload_futures).await?;\n\n    Ok(Response {\n        processed_count: event.payload.image_urls.len(),\n    })\n}\n\nfn process_image(data: &[u8]) -> Result<Vec<u8>, Error> {\n    // CPU-intensive synchronous processing\n    // (e.g., resize, filter, compress)\n    Ok(data.to_vec()) // Placeholder\n}\n\nasync fn upload_to_s3(key: &str, data: &[u8]) -> Result<(), Error> {\n    // Async S3 upload\n    Ok(())\n}\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    tracing_subscriber::fmt()\n        .with_max_level(tracing::Level::INFO)\n        .with_target(false)\n        .without_time()\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n```\n\n**Key Points**:\n- Async for all IO (downloads, uploads, API calls)\n- Sync with Rayon for CPU-intensive work\n- Use `spawn_blocking` to transition between async and sync\n- Maximizes both IO concurrency and CPU utilization\n\n## GitHub Actions CI/CD\n\n### Complete Workflow Example\n\nCreate `.github/workflows/deploy-lambda.yml`:\n\n```yaml\nname: Deploy Lambda\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\nenv:\n  CARGO_TERM_COLOR: always\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n\n    permissions:\n      id-token: write   # Required for AWS OIDC\n      contents: read\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Rust\n        uses: dtolnay/rust-toolchain@stable\n\n      - name: Cache cargo registry\n        uses: actions/cache@v4\n        with:\n          path: ~/.cargo/registry\n          key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}\n\n      - name: Cache cargo index\n        uses: actions/cache@v4\n        with:\n          path: ~/.cargo/git\n          key: ${{ runner.os }}-cargo-git-${{ hashFiles('**/Cargo.lock') }}\n\n      - name: Cache cargo build\n        uses: actions/cache@v4\n        with:\n          path: target\n          key: ${{ runner.os }}-cargo-build-target-${{ hashFiles('**/Cargo.lock') }}\n\n      - name: Install Zig\n        uses: goto-bus-stop/setup-zig@v2\n        with:\n          version: 0.11.0\n\n      - name: Install cargo-lambda\n        run: pip install cargo-lambda\n\n      - name: Run tests\n        run: cargo test --verbose\n\n      - name: Build Lambda (x86_64)\n        run: cargo lambda build --release --output-format zip\n\n      - name: Build Lambda (ARM64)\n        run: cargo lambda build --release --arm64 --output-format zip\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n          aws-region: us-east-1\n\n      - name: Deploy to AWS Lambda\n        if: github.ref == 'refs/heads/main'\n        run: |\n          cargo lambda deploy \\\n            --iam-role ${{ secrets.LAMBDA_EXECUTION_ROLE }} \\\n            --region us-east-1 \\\n            my-lambda-function\n```\n\n### Using AWS Credentials (Alternative)\n\nIf not using OIDC, you can use AWS credentials directly:\n\n```yaml\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n```\n\n### Multi-Function Deployment\n\nFor deploying multiple Lambda functions:\n\n```yaml\n      - name: Deploy all Lambda functions\n        if: github.ref == 'refs/heads/main'\n        run: |\n          for func in $(cargo lambda list); do\n            echo \"Deploying $func...\"\n            cargo lambda deploy --iam-role ${{ secrets.LAMBDA_EXECUTION_ROLE }} $func\n          done\n```\n\n## Performance Optimization\n\n### Cold Start Optimization\n\n1. **Minimize binary size**:\n   ```toml\n   [profile.release]\n   opt-level = 'z'       # Optimize for size\n   lto = true            # Link Time Optimization\n   codegen-units = 1     # Better optimization\n   strip = true          # Remove debug symbols\n   panic = 'abort'       # Smaller binary\n   ```\n\n2. **Use function-level initialization**:\n   ```rust\n   use std::sync::OnceLock;\n\n   static HTTP_CLIENT: OnceLock<reqwest::Client> = OnceLock::new();\n\n   async fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n       // Initialize once, reuse across invocations\n       let client = HTTP_CLIENT.get_or_init(|| {\n           reqwest::Client::builder()\n               .timeout(Duration::from_secs(10))\n               .build()\n               .unwrap()\n       });\n\n       // Use client...\n   }\n   ```\n\n3. **ARM64 for better performance**:\n   ```bash\n   cargo lambda build --release --arm64\n   ```\n\n### Memory and Timeout Configuration\n\n```bash\n# Set memory (more memory = more CPU)\ncargo lambda deploy --memory 512\n\n# Set timeout\ncargo lambda deploy --timeout 30\n\n# Set environment variables\ncargo lambda deploy --env-var KEY1=value1 --env-var KEY2=value2\n```\n\n## Tracing and Logging\n\n### Setup Tracing\n\n```rust\nuse tracing::{info, warn, error};\nuse tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};\n\n#[tokio::main]\nasync fn main() -> Result<(), Error> {\n    // Initialize tracing for CloudWatch\n    tracing_subscriber::registry()\n        .with(\n            tracing_subscriber::EnvFilter::try_from_default_env()\n                .unwrap_or_else(|_| \"info\".into()),\n        )\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    run(service_fn(function_handler)).await\n}\n\nasync fn function_handler(event: LambdaEvent<Request>) -> Result<Response, Error> {\n    info!(request_id = %event.context.request_id, \"Processing request\");\n\n    // Your logic here\n\n    Ok(response)\n}\n```\n\n### Environment Variables for Log Level\n\nSet `RUST_LOG` environment variable when deploying:\n\n```bash\ncargo lambda deploy --env-var RUST_LOG=debug\n```\n\n## Error Handling Best Practices\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\nenum LambdaError {\n    #[error(\"Database error: {0}\")]\n    Database(#[from] sqlx::Error),\n\n    #[error(\"HTTP error: {0}\")]\n    Http(#[from] reqwest::Error),\n\n    #[error(\"Invalid input: {0}\")]\n    InvalidInput(String),\n}\n\n// Lambda runtime requires errors to implement std::error::Error\nimpl From<LambdaError> for lambda_runtime::Error {\n    fn from(err: LambdaError) -> Self {\n        Box::new(err)\n    }\n}\n```\n\n## Common Event Types\n\n### API Gateway Event\n\n```rust\nuse aws_lambda_events::event::apigw::{ApiGatewayProxyRequest, ApiGatewayProxyResponse};\nuse http::StatusCode;\n\nasync fn function_handler(\n    event: LambdaEvent<ApiGatewayProxyRequest>,\n) -> Result<ApiGatewayProxyResponse, Error> {\n    let body = event.payload.body.unwrap_or_default();\n\n    Ok(ApiGatewayProxyResponse {\n        status_code: StatusCode::OK.as_u16() as i64,\n        body: Some(format!(\"Received: {}\", body)),\n        ..Default::default()\n    })\n}\n```\n\n### S3 Event\n\n```rust\nuse aws_lambda_events::event::s3::S3Event;\n\nasync fn function_handler(event: LambdaEvent<S3Event>) -> Result<(), Error> {\n    for record in event.payload.records {\n        let bucket = record.s3.bucket.name.unwrap();\n        let key = record.s3.object.key.unwrap();\n\n        info!(\"Processing s3://{}/{}\", bucket, key);\n    }\n\n    Ok(())\n}\n```\n\n### SQS Event\n\n```rust\nuse aws_lambda_events::event::sqs::SqsEvent;\n\nasync fn function_handler(event: LambdaEvent<SqsEvent>) -> Result<(), Error> {\n    for record in event.payload.records {\n        let body = record.body.unwrap_or_default();\n        info!(\"Processing message: {}\", body);\n\n        // Process message...\n    }\n\n    Ok(())\n}\n```\n\n## Testing\n\n### Unit Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_handler() {\n        let event = LambdaEvent::new(\n            Request { user_ids: vec![\"123\".to_string()] },\n            lambda_runtime::Context::default(),\n        );\n\n        let result = function_handler(event).await;\n        assert!(result.is_ok());\n    }\n}\n```\n\n### Integration Tests\n\n```bash\n# Start local Lambda emulator\ncargo lambda watch\n\n# Invoke with test data\ncargo lambda invoke --data-file tests/fixtures/event.json\n```\n\n## Best Practices\n\n1. **Use Async for IO**: Network calls, database queries, file operations\n2. **Use Sync for Compute**: CPU-intensive calculations, data processing\n3. **Reuse Connections**: Initialize clients once using `OnceLock` or `lazy_static`\n4. **Optimize Binary Size**: Use release profile optimizations\n5. **Structure Errors**: Use `thiserror` for clear error types\n6. **Add Tracing**: Use structured logging for observability\n7. **Test Locally**: Use `cargo lambda watch` for fast iteration\n8. **Monitor Cold Starts**: Use ARM64 and size optimizations\n9. **Set Appropriate Memory**: More memory = more CPU (and cost)\n10. **Use spawn_blocking**: For CPU work in async context\n\n## Troubleshooting\n\n### Binary too large\n- Enable `strip = true` in release profile\n- Use `opt-level = 'z'` for size optimization\n- Check dependencies with `cargo tree`\n\n### Cold starts too slow\n- Build for ARM64 architecture\n- Reduce binary size\n- Move initialization outside handler\n- Consider Lambda SnapStart (Java) or provisioned concurrency\n\n### Out of memory errors\n- Increase Lambda memory allocation\n- Use streaming for large files\n- Profile memory usage locally\n\n### Timeout errors\n- Increase timeout setting\n- Optimize async concurrency\n- Use spawn_blocking for CPU work\n- Profile with `cargo flamegraph`\n\n## Resources\n\n- [cargo-lambda Documentation](https://www.cargo-lambda.info/)\n- [lambda_runtime Crate](https://docs.rs/lambda_runtime)\n- [AWS Lambda Rust Guide](https://docs.aws.amazon.com/lambda/latest/dg/lambda-rust.html)\n- [AWS Lambda Events](https://docs.rs/aws_lambda_events)\n- [Tokio Documentation](https://tokio.rs/)\n- [Rayon Documentation](https://docs.rs/rayon)\n\n## Installation\n\n```bash\ncp -r plugins/rust-lambda /path/to/your/project/plugins/\n```\n\nRegister in marketplace.json:\n```json\n{\n  \"plugins\": [{\n    \"name\": \"rust-lambda\",\n    \"source\": \"./plugins/rust-lambda\",\n    \"description\": \"Build and deploy AWS Lambda functions with Rust\",\n    \"version\": \"1.0.0\"\n  }]\n}\n```\n\n## Usage\n\n### Quick Commands\n\n```bash\n# Create new Lambda\n/lambda-new\n\n# Build for deployment\n/lambda-build\n\n# Deploy to AWS\n/lambda-deploy\n\n# Setup CI/CD\n/lambda-github-actions\n\n# Optimize for IO workloads\n/lambda-optimize-io\n\n# Optimize for compute workloads\n/lambda-optimize-compute\n```\n\n### Get Expert Help\n\n```\nAsk rust-lambda-expert to help optimize my Lambda for cold starts\nAsk rust-lambda-expert to review my Lambda architecture\n```\n\n---\n\n**Build fast, efficient Lambda functions with Rust** ü¶Ä‚ö°\n"
      },
      "plugins": [
        {
          "name": "changelog",
          "source": "./plugins/changelog",
          "description": "Changelog management plugin that ensures all code commits include proper changelog entries. Provides hooks to prevent commits without changelog updates, commands for managing changelog entries, and agents for writing well-formatted changelog entries following Keep a Changelog format",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Emil Lindfors"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EmilLindfors/claude-marketplace",
            "/plugin install changelog@lf-marketplace"
          ]
        },
        {
          "name": "rust-hexagonal",
          "source": "./plugins/rust-hexagonal",
          "description": "Hexagonal architecture plugin for Rust. Helps design and implement clean, maintainable Rust applications using the ports and adapters pattern. Includes commands for initializing project structure, adding ports and adapters, and an expert agent for architecture guidance",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Emil Lindfors"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EmilLindfors/claude-marketplace",
            "/plugin install rust-hexagonal@lf-marketplace"
          ]
        },
        {
          "name": "rust-error-handling",
          "source": "./plugins/rust-error-handling",
          "description": "Error handling best practices plugin for Rust. Provides commands for creating custom error types with thiserror, refactoring panic-based code to Result-based error handling, and an expert agent for error handling guidance and code review",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Emil Lindfors"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EmilLindfors/claude-marketplace",
            "/plugin install rust-error-handling@lf-marketplace"
          ]
        },
        {
          "name": "rust-testing",
          "source": "./plugins/rust-testing",
          "description": "Testing best practices plugin for Rust. Includes commands for adding unit tests, integration tests, test analysis, and an expert agent for comprehensive testing strategies, mock implementations, and property-based testing",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Emil Lindfors"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EmilLindfors/claude-marketplace",
            "/plugin install rust-testing@lf-marketplace"
          ]
        },
        {
          "name": "rust-modern-patterns",
          "source": "./plugins/rust-modern-patterns",
          "description": "Modern Rust patterns for Rust 2024 Edition. Includes let chains, async closures, gen blocks, match ergonomics, const improvements. Commands for modernization, edition upgrade, and pattern checking. Expert agent for Rust 2024 migration and best practices. Requires Rust 1.85.0+",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Emil Lindfors"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EmilLindfors/claude-marketplace",
            "/plugin install rust-modern-patterns@lf-marketplace"
          ]
        },
        {
          "name": "rust-data-engineering",
          "source": "./plugins/rust-data-engineering",
          "description": "Data engineering plugin for Rust with object_store, Arrow, Parquet, DataFusion, and Iceberg. Build cloud-native data lakes, analytical query engines, and ETL pipelines. Commands for object storage, Parquet I/O, DataFusion queries, and Iceberg tables. Expert agent for data lake architecture and performance optimization",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Emil Lindfors"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EmilLindfors/claude-marketplace",
            "/plugin install rust-data-engineering@lf-marketplace"
          ]
        },
        {
          "name": "rust-lambda",
          "source": "./plugins/rust-lambda",
          "description": "Comprehensive AWS Lambda development with Rust using cargo-lambda. Build, deploy, and optimize Lambda functions with support for IO-intensive, compute-intensive, and mixed workloads. Includes 12 commands for complete Lambda lifecycle management and an expert agent for architecture decisions",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Emil Lindfors"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EmilLindfors/claude-marketplace",
            "/plugin install rust-lambda@lf-marketplace"
          ]
        }
      ]
    }
  ]
}