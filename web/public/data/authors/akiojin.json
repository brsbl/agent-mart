{
  "author": {
    "id": "akiojin",
    "display_name": "Akio Jinsenji",
    "avatar_url": "https://avatars.githubusercontent.com/u/965624?u=0b3ad9f4be59794614a90c8f2fd153a61fcb6d32&v=4"
  },
  "marketplaces": [
    {
      "name": "unity-mcp-server",
      "version": "1.0.0",
      "description": "Claude Code skills for Unity MCP Server. Provides workflow-oriented guidance for C# editing, Scene/GameObject management, PlayMode testing, and Asset management using 108+ Unity automation tools.",
      "repo_full_name": "akiojin/unity-mcp-server",
      "repo_url": "https://github.com/akiojin/unity-mcp-server",
      "repo_description": "Unity MCP server with OpenUPM/npm/LSP auto-release",
      "signals": {
        "stars": 17,
        "forks": 4,
        "pushed_at": "2026-02-17T10:16:40Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"unity-mcp-server\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Claude Code skills for Unity MCP Server. Provides workflow-oriented guidance for C# editing, Scene/GameObject management, PlayMode testing, and Asset management using 108+ Unity automation tools.\",\n  \"owner\": {\n    \"name\": \"akiojin\",\n    \"url\": \"https://github.com/akiojin\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"unity-mcp-server\",\n      \"description\": \"Claude Code skills for Unity MCP Server. Provides workflow-oriented guidance for C# editing, Scene/GameObject management, PlayMode testing, and Asset management using 108+ Unity automation tools.\",\n      \"version\": \"1.0.1\",\n      \"source\": \"./.claude-plugin/plugins/unity-mcp-server\",\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        "README.md": "# Unity MCP Server\n\nEnglish | [日本語](README.ja.md)\n\n## Overview\n\nUnity MCP Server lets LLM-based clients automate the Unity Editor. It focuses on reliable, scriptable workflows with a simple interface and zero or low-configuration setup.\n\n## What It Can Do\n\n- **Editor automation**: Create/modify scenes, GameObjects, components, prefabs, materials\n- **UI automation**: Locate and interact with UI, validate UI state\n- **Input simulation**: Keyboard/mouse/gamepad/touch for playmode testing (Input System only)\n- **Visual capture**: Deterministic screenshots from Game/Scene/Explorer/Window views\n- **Code base awareness**: Safe structured edits and accurate symbol/search powered by bundled C# LSP (no `.sln` file required)\n- **Project control**: Read/update project/editor settings; read logs, monitor compilation\n- **Addressables management**: Register/organize assets, manage groups, build automation\n\n## Performance\n\nCode index tools outperform standard file operations:\n\n| Operation        | Code Index Tool      | Standard Tool | Advantage                  |\n| ---------------- | -------------------- | ------------- | -------------------------- |\n| Symbol lookup    | `find_symbol` | `grep`        | **Instant** vs 350ms       |\n| Reference search | `find_refs`   | `grep`        | **Structured** results     |\n| Code search      | `search`      | `grep`        | **3-5x smaller** responses |\n\nKey benefits:\n\n- **128,040 files indexed** with 100% coverage\n- **Non-blocking** background index builds (Worker Threads)\n- **LLM-optimized** output with pagination and size limits\n\n> For detailed benchmarks, see [docs/benchmark-results-2025-12-13.md](docs/benchmark-results-2025-12-13.md)\n\n## Requirements\n\n- Unity 2020.3 LTS or newer\n- Node.js 18.x / 20.x / 22.x / 24.x LTS (25+ not supported)\n- Claude Desktop or another MCP-compatible client\n\n## Installation\n\n### Unity Package\n\nPackage Manager → Add from git URL:\n\n```\nhttps://github.com/akiojin/unity-mcp-server.git?path=UnityMCPServer/Packages/unity-mcp-server\n```\n\nOr via OpenUPM:\n\n```bash\nopenupm add com.akiojin.unity-mcp-server\n```\n\n### MCP Client Configuration\n\nConfigure your MCP client (Claude Desktop example):\n\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"unity-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"@akiojin/unity-mcp-server@latest\"]\n    }\n  }\n}\n```\n\n### HTTP Mode (for HTTP-only networks)\n\n```bash\nnpx @akiojin/unity-mcp-server --http 6401 --no-telemetry\ncurl http://localhost:6401/healthz\n```\n\n## Quick Start\n\n1. **Install the Unity package** (Git URL or OpenUPM)\n2. **Configure your MCP client** with the JSON above\n3. **Open your Unity project** (package starts TCP listener on port 6400)\n4. **Launch your MCP client** (it connects to the Node server)\n5. **Test the connection** with `ping`\n\n> Tip: `npx @akiojin/unity-mcp-server@latest` downloads and runs the latest build without cloning.\n\n## Architecture\n\n```\n┌────────────────┐        JSON-RPC (MCP)        ┌──────────────────────┐\n│  MCP Client    │ ───────────────────────────▶ │  Node MCP Server     │\n│ (Claude/Codex/ │ ◀─────────────────────────── │ (@akiojin/unity-     │\n│   Cursor …)    │        tool responses        │ mcp-server)          │\n└────────────────┘                              └──────────┬───────────┘\n                                                         TCP│6400\n                                                            ▼\n                                                   ┌───────────────────┐\n                                                   │  Unity Editor     │\n                                                   │  (Package opens   │\n                                                   │   TCP listener)   │\n                                                   └───────────────────┘\n```\n\n## Configuration\n\nConfiguration is optional; defaults work without any config file.\n\nNode-side configuration uses **environment variables only**, and Unity-side host/port lives in **Project Settings**.\n\nSee [docs/configuration.md](docs/configuration.md).\n\n## Tools\n\nUnity MCP Server ships **100+ tools**. Use the `search_tools` meta-tool to discover the right tool quickly.\n\nFor clients with strict tool-count limits, you can filter exposed tools by category:\n\n```bash\nexport UNITY_MCP_TOOL_INCLUDE_CATEGORIES=system,scene,gameobject,analysis,script\nexport UNITY_MCP_TOOL_EXCLUDE_CATEGORIES=ui,input,addressables,video,screenshot,profiler\n```\n\nSee [docs/tools.md](docs/tools.md) for discovery tips and the recommended Code Index workflow.\n\nNote: `find_refs` supports paging via `startAfter` and returns `cursor` when results are truncated.\n\n## Claude Code Skills\n\nThis package includes Claude Code skills that provide workflow-oriented guidance for effectively using the 108+ tools.\n\n### Available Skills\n\n| Skill                          | Description                                                                              | Triggers                                                      |\n| ------------------------------ | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------- |\n| `mcp-server-development`       | Build MCP servers (TypeScript SDK), implement tools/resources/prompts, JSON-RPC patterns | \"MCP server\", \"tool handler\", \"JSON-RPC\", \"TypeScript\"        |\n| `unity-csharp-editing`         | C# script editing, search, refactoring with TDD workflow                                 | \"C# edit\", \"script search\", \"refactoring\"                     |\n| `unity-scene-management`       | Scene, GameObject, Component management                                                  | \"scene create\", \"GameObject\", \"component add\"                 |\n| `unity-playmode-testing`       | PlayMode control, input simulation, UI automation                                        | \"playmode\", \"input simulate\", \"UI click\"                      |\n| `unity-asset-management`       | Prefab, Material, Addressables management                                                | \"prefab create\", \"material\", \"Addressables\"                   |\n| `unity-editor-imgui-design`    | Unity Editor IMGUI for EditorWindow/Inspector/PropertyDrawer (not for in-game UI)        | \"EditorWindow\", \"Custom Inspector\", \"PropertyDrawer\", \"IMGUI\" |\n| `unity-game-ugui-design`       | In-game uGUI (Canvas/RectTransform/Anchors) UI design                                    | \"uGUI\", \"Canvas\", \"RectTransform\", \"Anchors\", \"HUD\"           |\n| `unity-game-ui-toolkit-design` | In-game UI Toolkit (UXML/USS/Flexbox) UI design                                          | \"UI Toolkit\", \"UXML\", \"USS\", \"VisualElement\", \"Flexbox\"       |\n\n### Installation\n\nInstall as a Claude Code plugin from GitHub:\n\n```bash\n# Step 1: Add marketplace\n/plugin marketplace add akiojin/unity-mcp-server\n\n# Step 2: Install plugin\n/plugin install unity-mcp-server@unity-mcp-server\n```\n\nOr manually copy the `.claude/skills/` directory to your project.\n\n### Usage\n\nSkills activate automatically when you mention related keywords. You can also invoke them directly:\n\n```\n# Ask about C# editing workflow\n\"How do I edit Unity C# scripts?\"\n\n# Ask about scene management\n\"Create a new scene with basic lighting\"\n\n# Ask about testing\n\"How do I simulate keyboard input in playmode?\"\n```\n\n## Troubleshooting\n\nSee [docs/troubleshooting/README.md](docs/troubleshooting/README.md).\n\n## OpenUPM Scoped Registry\n\nTo use OpenUPM packages, add the scoped registry to your project:\n\n### Via Project Settings\n\n1. `Edit > Project Settings > Package Manager`\n2. Under **Scoped Registries**, click **+**\n3. Add:\n   - **Name**: `OpenUPM`\n   - **URL**: `https://package.openupm.com`\n   - **Scopes**: `com.akiojin`, `com.akiojin.unity-mcp-server`\n\n### Via manifest.json\n\n```json\n\"scopedRegistries\": [\n  {\n    \"name\": \"OpenUPM\",\n    \"url\": \"https://package.openupm.com\",\n    \"scopes\": [\"com.akiojin\", \"com.akiojin.unity-mcp-server\"]\n  }\n]\n```\n\n## Repository Structure\n\n```\n.unity/\n├── cache/           # Local caches (git-ignored)\n└── capture/         # Screenshots/videos (git-ignored)\n\nUnityMCPServer/\n├── Packages/unity-mcp-server/  # UPM package (source)\n└── Assets/                     # Samples only\n\nmcp-server/          # Node MCP server\n\ncsharp-lsp/          # Roslyn-based LSP tool\n```\n\n## Feature Documentation\n\nAll features are documented with SDD format: [`specs/`](specs/)\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for development setup, commit guidelines, and PR process.\n\n## Development Documentation\n\nFor internal development details (Spec Kit, release process, LLM optimization):\n\n- [docs/README.md](docs/README.md)\n- [docs/development.md](docs/development.md)\n- [CLAUDE.md](CLAUDE.md)\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file.\n"
      },
      "plugins": [
        {
          "name": "unity-mcp-server",
          "description": "Claude Code skills for Unity MCP Server. Provides workflow-oriented guidance for C# editing, Scene/GameObject management, PlayMode testing, and Asset management using 108+ Unity automation tools.",
          "version": "1.0.1",
          "source": "./.claude-plugin/plugins/unity-mcp-server",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add akiojin/unity-mcp-server",
            "/plugin install unity-mcp-server@unity-mcp-server"
          ]
        }
      ]
    },
    {
      "name": "akiojin-skills",
      "version": "1.0.0",
      "description": "Custom plugins for Claude Code by akiojin",
      "repo_full_name": "akiojin/skills",
      "repo_url": "https://github.com/akiojin/skills",
      "repo_description": "Claude Code skills and plugins collection",
      "signals": {
        "stars": 3,
        "forks": 2,
        "pushed_at": "2026-02-13T18:31:28Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"akiojin-skills\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Custom plugins for Claude Code by akiojin\",\n  \"owner\": {\n    \"name\": \"akiojin\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"cli-design\",\n      \"source\": \"./cli-design\",\n      \"description\": \"CLI UI design toolkit for terminal applications with Ink.js patterns, multi-screen navigation, animations, state management, and performance optimization\",\n      \"version\": \"1.0.0\",\n      \"category\": \"development\",\n      \"keywords\": [\"cli\", \"inkjs\", \"terminal\", \"react\", \"tui\", \"animation\", \"state-management\"]\n    },\n    {\n      \"name\": \"speckit\",\n      \"source\": \"./speckit\",\n      \"description\": \"Spec Kit skills: speckit-require and speckit-update (requirements workflow + upstream update).\",\n      \"version\": \"0.1.0\",\n      \"category\": \"development\",\n      \"keywords\": [\"spec-kit\", \"requirements\", \"specification\", \"update\", \"workflow\", \"tdd\"]\n    },\n    {\n      \"name\": \"github\",\n      \"source\": \"./github\",\n      \"description\": \"GitHub workflow skills: PR creation/update (gh-pr) and CI failure inspection/fix (gh-fix-ci)\",\n      \"version\": \"0.1.0\",\n      \"category\": \"development\",\n      \"keywords\": [\"github\", \"pr\", \"pull-request\", \"ci\", \"review\", \"actions\"]\n    },\n    {\n      \"name\": \"drawio\",\n      \"source\": \"./drawio\",\n      \"description\": \"Draw.io diagram creation and editing skill. Handles XML structure, styling, fonts, arrows, connectors, and PNG export.\",\n      \"version\": \"0.1.0\",\n      \"category\": \"development\",\n      \"keywords\": [\"draw.io\", \"diagram\", \"flowchart\", \"architecture\", \"xml\"]\n    },\n    {\n      \"name\": \"cli-inkjs\",\n      \"source\": \"./cli-inkjs\",\n      \"description\": \"Ink.js (React for CLI) design and implementation toolkit. Covers component patterns, input handling, screen navigation, key propagation prevention, performance optimization, and common gotchas.\",\n      \"version\": \"1.0.0\",\n      \"category\": \"development\",\n      \"keywords\": [\"cli\", \"inkjs\", \"terminal\", \"react\", \"tui\", \"input-handling\"]\n    },\n    {\n      \"name\": \"cli-opentui\",\n      \"source\": \"./cli-opentui\",\n      \"description\": \"OpenTUI (SolidJS for CLI) design and implementation toolkit. Covers component patterns, input handling, screen navigation, key propagation prevention, mouse handling, performance optimization, and common gotchas.\",\n      \"version\": \"1.0.0\",\n      \"category\": \"development\",\n      \"keywords\": [\"cli\", \"opentui\", \"solidjs\", \"terminal\", \"tui\", \"input-handling\"]\n    }\n  ]\n}\n",
        "README.md": "# Codex & Claude Code Skills\n\nA collection of skills and plugins for Codex and Claude Code.\n\r\n## Available Plugins\r\n\r\n### cli-design\r\n\r\nInk.js CLI UI design skill with:\r\n\r\n- **string-width handling**: Emoji/icon width calculation fixes for terminal alignment\r\n- **Component patterns**: Screen/Part/Common component classification\r\n- **Custom hooks**: useScreenState, useTerminalSize patterns\r\n- **React.memo optimization**: Custom comparison functions for performance\r\n- **Testing patterns**: ink-testing-library with Vitest\r\n\r\n### unity-development\r\n\r\nUnity C# development skill with:\r\n\r\n- **unity-mcp-server tools**: Symbol-based code exploration and editing\r\n- **VContainer DI**: Dependency injection patterns\r\n- **UniTask**: Async processing patterns\r\n- **Fail-Fast principle**: No null-check defensive coding\r\n\r\n### speckit\n\nSpec Kit skills (plugin) with:\n\n- **Requirements workflow**: spec.md → plan.md → tasks.md flow\n- **Upstream update**: GitHub Spec Kit version sync\n\n### github\n\nGitHub workflow skills with:\n\n- **PR automation**: gh-pr workflow and PR body templates\n- **CI diagnostics**: gh-fix-ci workflow and inspection scripts\n\n### drawio\n\r\nDraw.io diagram creation and editing skill with:\r\n\r\n- **XML structure**: Direct .drawio file manipulation\r\n- **Shape support**: Rectangle, ellipse, diamond, text, and connectors\r\n- **Japanese fonts**: Noto Sans JP with proper sizing\r\n- **Export options**: PNG, SVG, PDF via drawio-export CLI\r\n- **Style guide**: Color palette, layout rules, and best practices\r\n\r\n## Installation (Claude Code)\n\r\n### Add Marketplace\r\n\r\n```bash\r\n/plugin marketplace add akiojin/skills\r\n```\r\n\r\n### Install Plugin\r\n\r\n```bash\r\n/plugin install cli-design@akiojin-skills\r\n```\r\n\r\nOr:\r\n\r\n```bash\r\n/plugin install unity-development@akiojin-skills\n```\n\nOr:\n\n```bash\n/plugin install speckit@akiojin-skills\n```\n\nOr:\n\n```bash\n/plugin install github@akiojin-skills\n```\n\nOr:\n\n```bash\n/plugin install drawio@akiojin-skills\n```\n\r\nOr interactively:\r\n\r\n```bash\r\n/plugin\r\n# Select \"Browse Plugins\"\r\n# Choose cli-design, unity-development, speckit, github, or drawio\n```\n\r\n## Installation (Codex)\n\nCodex reads skills from folders under `.codex/skills` (repo or user scope).\nThis repo also ships packaged `.skill` files under `codex-skills/dist/`.\n\n### Option A: Install from GitHub with skill-installer\n\n```bash\n# From GitHub repo/path\nscripts/install-skill-from-github.py --repo akiojin/skills --path gh-pr --ref main\n```\n\n### Option B: Unzip .skill into $CODEX_HOME/skills\n\n```bash\n# Example (PowerShell)\n$dest = \"$env:USERPROFILE\\.codex\\skills\"\nNew-Item -ItemType Directory -Force -Path $dest | Out-Null\nExpand-Archive -Path .\\codex-skills\\dist\\gh-pr.skill -DestinationPath $dest -Force\n```\n\n```bash\n# Example (bash)\ndest=\"$HOME/.codex/skills\"\nmkdir -p \"$dest\"\nunzip -o ./codex-skills/dist/gh-pr.skill -d \"$dest\"\n```\n\n### Available Codex skills\n\n- gh-pr\n- gh-fix-ci\n- speckit-require\n- speckit-update\n- drawio\n- inkjs-design\n\nAfter installation, restart Codex to load new skills.\n\n## Usage (Codex)\n\n### gh-pr\n\nUse when creating or updating GitHub PRs with the gh CLI, or when you want a PR body generated from a template.\n\n### gh-fix-ci\n\nUse when debugging failing GitHub Actions checks for a PR and you want a fix plan + code changes.\n\n### speckit-require\n\nUse when creating or updating requirement specs with Spec Kit (specify/clarify/plan/tasks flow).\n\n### speckit-update\n\nUse when updating GitHub Spec Kit versions and syncing templates/scripts while preserving local rules.\n\n### drawio\n\nUse when creating or editing .drawio files (XML) and exporting diagrams.\n\n### inkjs-design\n\nUse when designing Ink.js CLI UIs (layout, input handling, string-width/emoji alignment, tests).\n\n## Usage (Claude Code)\n\r\n### cli-design\r\n\r\nAutomatically triggered when:\r\n\r\n- Creating Ink.js terminal UI components\r\n- Handling emoji/icon width issues (string-width)\r\n- Implementing keyboard input handling (useInput)\r\n- Designing responsive terminal layouts\r\n- Writing CLI component tests\r\n\r\n### unity-development\r\n\r\nAutomatically triggered when:\r\n\r\n- Editing Unity C# scripts\r\n- Working with GameObjects and components\r\n- Running Unity tests (EditMode/PlayMode)\r\n- Configuring VContainer DI\r\n- Implementing UniTask async methods\r\n\r\n### speckit-require\n\nAutomatically triggered when:\n\n- Creating or updating requirements specs\n- Drafting specifications (specification/spec doc authoring)\n- Running Spec Kit workflows (specify/clarify/plan/tasks)\n\n### speckit-update\n\nAutomatically triggered when:\n\n- Updating GitHub Spec Kit versions in a repo\n- Syncing Spec Kit templates/commands/scripts from upstream\n- Preserving local Spec Kit rules (Japanese, no branch ops, SPEC-UUID)\n\n### github\n\nSlash commands:\n\n- `/github:gh-pr`\n- `/github:gh-fix-ci`\n\n### speckit (slash commands)\n\n- `/speckit:speckit-require`\n- `/speckit:speckit-update`\n\r\n### drawio\r\n\r\nAutomatically triggered when:\r\n\r\n- Creating flowcharts, architecture diagrams, or sequence diagrams\r\n- Editing .drawio files in XML format\r\n- Setting up diagram styling and fonts\r\n- Exporting diagrams to PNG, SVG, or PDF\r\n\r\n## License\n\nMIT\n\n## Add a New Skill\n\n### 1) Create the skill folder\n\n- **Codex skill**: create a folder at repo root with `SKILL.md` (YAML frontmatter required).\n- **Claude Code plugin**: create a plugin folder at repo root. If it contains multiple skills, place them under `<plugin>/skills/<skill-name>/SKILL.md`.\n\nExample (plugin with multiple skills):\n\n```\nspeckit/\n└── skills/\n    ├── speckit-require/\n    │   └── SKILL.md\n    └── speckit-update/\n        └── SKILL.md\n```\n\n### 2) Register for Claude Code (plugin)\n\nAdd a new entry to `.claude-plugin/marketplace.json`:\n\n- `name`, `source`, `description`, `version`, `category`, `keywords`\n\nExample entry (plugin):\n\n```json\n{\n  \"name\": \"speckit\",\n  \"source\": \"./speckit\",\n  \"description\": \"Spec Kit skills: requirements workflow + upstream update.\",\n  \"version\": \"0.1.0\",\n  \"category\": \"development\",\n  \"keywords\": [\"spec-kit\", \"requirements\", \"specification\", \"update\", \"workflow\", \"tdd\"]\n}\n```\n\n### 3) Package for Codex\n\nPackage the skill into `codex-skills/dist/`:\n\n```bash\nexport PYTHONUTF8=1\ncodex_home=\"${CODEX_HOME:-$HOME/.codex}\"\npython \"$codex_home/skills/.system/skill-creator/scripts/package_skill.py\" \\\n  \"<repo-root>/<skill-folder>\" \\\n  \"<repo-root>/codex-skills/dist\"\n```\n\nExample (single skill package):\n\n```bash\nexport PYTHONUTF8=1\ncodex_home=\"${CODEX_HOME:-$HOME/.codex}\"\npython \"$codex_home/skills/.system/skill-creator/scripts/package_skill.py\" \\\n  \"$PWD/speckit/skills/speckit-update\" \\\n  \"$PWD/codex-skills/dist\"\n```\n\n### 4) Update this README\n\n- Add the skill under **Available Plugins**, **Available Codex skills**, and the **Usage** sections.\n\n### 5) Add resources (scripts / references / assets)\n\nCreate only the directories you need under the skill folder:\n\n- `scripts/` for executable automation (bash/python/etc.)\n- `references/` for docs you want the model to read on demand\n- `assets/` for templates or files to copy into outputs\n\nExample:\n\n```\nspeckit/skills/speckit-update/\n├── SKILL.md\n├── scripts/\n│   └── sync-upstream.sh\n├── references/\n│   └── upstream-diff-checklist.md\n└── assets/\n    └── templates/\n        └── spec-template.md\n```\n",
        "cli-design/README.md": "# CLI Design Plugin\n\nA comprehensive CLI UI design toolkit for terminal applications.\n\n## Overview\n\nThis plugin provides patterns, best practices, and utilities for building terminal user interfaces. Currently focused on **Ink.js** (React for CLI), with potential for future CLI framework support.\n\n## Available Skills\n\n### inkjs-design\n\nComplete Ink.js development toolkit covering:\n\n- **Component Patterns** - Screen/Part/Common component architecture\n- **Custom Hooks** - useInput, useApp, useTerminalSize patterns\n- **Multi-screen Navigation** - Stack-based screen management\n- **Animations** - Spinners, progress bars, frame-based animations\n- **State Management** - Map-based state, controlled/uncontrolled modes\n- **Responsive Layouts** - Terminal size handling, character width calculations\n- **Performance Optimization** - React.memo, useMemo patterns\n- **Input Handling** - Keyboard shortcuts, conflict avoidance\n- **Testing** - ink-testing-library patterns\n\n## Installation\n\n```bash\n# Using Claude Code plugin installer\n/plugin install cli-design@akiojin-skills\n```\n\n## Quick Start\n\n1. Use the `/cli-design:inkjs-cli` command to access Ink.js patterns\n2. Reference the `inkjs-design` skill for component development\n3. Check `references/` for detailed documentation on specific topics\n\n## Directory Structure\n\n```\ncli-design/\n├── .claude-plugin/\n│   └── plugin.json\n├── agents/\n│   └── inkjs-component-architect.md\n├── commands/\n│   └── inkjs-cli.md\n├── skills/\n│   └── inkjs-design/\n│       ├── SKILL.md\n│       ├── examples/\n│       └── references/\n└── README.md\n```\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/cli-design:inkjs-cli` | Invoke Ink.js design patterns and guidance |\n\n## Agents\n\n| Agent | Description |\n|-------|-------------|\n| `inkjs-component-architect` | Design Ink.js components with best practices |\n\n## Future Roadmap\n\n- **blessed-design** - Blessed/blessed-contrib patterns\n- **prompts-design** - Enquirer/Prompts patterns\n- **chalk-design** - Terminal styling patterns\n\n## License\n\nMIT\n",
        "cli-inkjs/README.md": "# CLI Design Plugin\n\nA comprehensive CLI UI design toolkit for terminal applications.\n\n## Overview\n\nThis plugin provides patterns, best practices, and utilities for building terminal user interfaces. Currently focused on **Ink.js** (React for CLI), with potential for future CLI framework support.\n\n## Available Skills\n\n### inkjs-design\n\nComplete Ink.js development toolkit covering:\n\n- **Component Patterns** - Screen/Part/Common component architecture\n- **Custom Hooks** - useInput, useApp, useTerminalSize patterns\n- **Multi-screen Navigation** - Stack-based screen management\n- **Animations** - Spinners, progress bars, frame-based animations\n- **State Management** - Map-based state, controlled/uncontrolled modes\n- **Responsive Layouts** - Terminal size handling, character width calculations\n- **Performance Optimization** - React.memo, useMemo patterns\n- **Input Handling** - Keyboard shortcuts, conflict avoidance\n- **Testing** - ink-testing-library patterns\n\n## Installation\n\n```bash\n# Using Claude Code plugin installer\n/plugin install cli-design@akiojin-skills\n```\n\n## Quick Start\n\n1. Use the `/cli-design:inkjs-cli` command to access Ink.js patterns\n2. Reference the `inkjs-design` skill for component development\n3. Check `references/` for detailed documentation on specific topics\n\n## Directory Structure\n\n```\ncli-design/\n├── .claude-plugin/\n│   └── plugin.json\n├── agents/\n│   └── inkjs-component-architect.md\n├── commands/\n│   └── inkjs-cli.md\n├── skills/\n│   └── inkjs-design/\n│       ├── SKILL.md\n│       ├── examples/\n│       └── references/\n└── README.md\n```\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/cli-design:inkjs-cli` | Invoke Ink.js design patterns and guidance |\n\n## Agents\n\n| Agent | Description |\n|-------|-------------|\n| `inkjs-component-architect` | Design Ink.js components with best practices |\n\n## Future Roadmap\n\n- **blessed-design** - Blessed/blessed-contrib patterns\n- **prompts-design** - Enquirer/Prompts patterns\n- **chalk-design** - Terminal styling patterns\n\n## License\n\nMIT\n",
        "cli-opentui/README.md": "# cli-opentui\n\nOpenTUI (SolidJS for CLI) design and implementation toolkit.\n\n## Overview\n\nThis skill provides patterns, best practices, and implementation guidance for building CLI applications with OpenTUI and SolidJS.\n\n## Contents\n\n- **agents/**: AI agent configurations for component architecture\n- **commands/**: Skill invocation commands\n- **skills/opentui-design/**: Core design patterns and references\n  - **references/**: Detailed documentation on specific topics\n  - **examples/**: Code examples and templates\n\n## Key Topics\n\n- Component patterns for OpenTUI\n- Input handling and key propagation prevention\n- Mouse handling and limitations\n- Screen navigation patterns\n- State management with SolidJS\n- Performance optimization\n- Common gotchas and workarounds\n\n## Usage\n\nThis skill is designed to be used with Claude Code when building CLI applications using OpenTUI/SolidJS stack.\n"
      },
      "plugins": [
        {
          "name": "cli-design",
          "source": "./cli-design",
          "description": "CLI UI design toolkit for terminal applications with Ink.js patterns, multi-screen navigation, animations, state management, and performance optimization",
          "version": "1.0.0",
          "category": "development",
          "keywords": [
            "cli",
            "inkjs",
            "terminal",
            "react",
            "tui",
            "animation",
            "state-management"
          ],
          "categories": [
            "animation",
            "cli",
            "development",
            "inkjs",
            "react",
            "state-management",
            "terminal",
            "tui"
          ],
          "install_commands": [
            "/plugin marketplace add akiojin/skills",
            "/plugin install cli-design@akiojin-skills"
          ]
        },
        {
          "name": "speckit",
          "source": "./speckit",
          "description": "Spec Kit skills: speckit-require and speckit-update (requirements workflow + upstream update).",
          "version": "0.1.0",
          "category": "development",
          "keywords": [
            "spec-kit",
            "requirements",
            "specification",
            "update",
            "workflow",
            "tdd"
          ],
          "categories": [
            "development",
            "requirements",
            "spec-kit",
            "specification",
            "tdd",
            "update",
            "workflow"
          ],
          "install_commands": [
            "/plugin marketplace add akiojin/skills",
            "/plugin install speckit@akiojin-skills"
          ]
        },
        {
          "name": "github",
          "source": "./github",
          "description": "GitHub workflow skills: PR creation/update (gh-pr) and CI failure inspection/fix (gh-fix-ci)",
          "version": "0.1.0",
          "category": "development",
          "keywords": [
            "github",
            "pr",
            "pull-request",
            "ci",
            "review",
            "actions"
          ],
          "categories": [
            "actions",
            "ci",
            "development",
            "github",
            "pr",
            "pull-request",
            "review"
          ],
          "install_commands": [
            "/plugin marketplace add akiojin/skills",
            "/plugin install github@akiojin-skills"
          ]
        },
        {
          "name": "drawio",
          "source": "./drawio",
          "description": "Draw.io diagram creation and editing skill. Handles XML structure, styling, fonts, arrows, connectors, and PNG export.",
          "version": "0.1.0",
          "category": "development",
          "keywords": [
            "draw.io",
            "diagram",
            "flowchart",
            "architecture",
            "xml"
          ],
          "categories": [
            "architecture",
            "development",
            "diagram",
            "draw.io",
            "flowchart",
            "xml"
          ],
          "install_commands": [
            "/plugin marketplace add akiojin/skills",
            "/plugin install drawio@akiojin-skills"
          ]
        },
        {
          "name": "cli-inkjs",
          "source": "./cli-inkjs",
          "description": "Ink.js (React for CLI) design and implementation toolkit. Covers component patterns, input handling, screen navigation, key propagation prevention, performance optimization, and common gotchas.",
          "version": "1.0.0",
          "category": "development",
          "keywords": [
            "cli",
            "inkjs",
            "terminal",
            "react",
            "tui",
            "input-handling"
          ],
          "categories": [
            "cli",
            "development",
            "inkjs",
            "input-handling",
            "react",
            "terminal",
            "tui"
          ],
          "install_commands": [
            "/plugin marketplace add akiojin/skills",
            "/plugin install cli-inkjs@akiojin-skills"
          ]
        },
        {
          "name": "cli-opentui",
          "source": "./cli-opentui",
          "description": "OpenTUI (SolidJS for CLI) design and implementation toolkit. Covers component patterns, input handling, screen navigation, key propagation prevention, mouse handling, performance optimization, and common gotchas.",
          "version": "1.0.0",
          "category": "development",
          "keywords": [
            "cli",
            "opentui",
            "solidjs",
            "terminal",
            "tui",
            "input-handling"
          ],
          "categories": [
            "cli",
            "development",
            "input-handling",
            "opentui",
            "solidjs",
            "terminal",
            "tui"
          ],
          "install_commands": [
            "/plugin marketplace add akiojin/skills",
            "/plugin install cli-opentui@akiojin-skills"
          ]
        }
      ]
    },
    {
      "name": "llmlb-skills",
      "version": "1.0.0",
      "description": "Claude Code plugin marketplace metadata for llmlb assistant CLI workflows.",
      "repo_full_name": "akiojin/llmlb",
      "repo_url": "https://github.com/akiojin/llmlb",
      "repo_description": "Distributed LLM router with load balancing and automatic model distribution",
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-02-18T06:51:48Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"llmlb-skills\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Claude Code plugin marketplace metadata for llmlb assistant CLI workflows.\",\n  \"owner\": {\n    \"name\": \"akiojin\",\n    \"url\": \"https://github.com/akiojin\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"llmlb-cli\",\n      \"description\": \"CLI-first assistant workflows for llmlb (curl/openapi/guide).\",\n      \"version\": \"0.1.0\",\n      \"source\": \"./.claude-plugin/plugins/llmlb-cli\",\n      \"category\": \"development\",\n      \"keywords\": [\"llmlb\", \"cli\", \"assistant\", \"openapi\", \"claude-code\"]\n    }\n  ]\n}\n",
        "README.md": "# LLM Load Balancer\n\nA centralized management system for coordinating LLM inference runtimes across multiple machines\n\nEnglish | [日本語](./README.ja.md)\n\n## Overview\n\nLLM Load Balancer is a powerful centralized system that provides unified management and a single API endpoint for multiple LLM inference runtimes running across different machines. It features intelligent load balancing, automatic failure detection, real-time monitoring capabilities, and seamless integration for enhanced scalability.\n\n### Vision\n\nLLM Load Balancer is designed to serve three primary use cases:\n\n1. **Private LLM Server** - For individuals and small teams who want to run their own LLM infrastructure with full control over their data and models\n2. **Enterprise Gateway** - For organizations requiring centralized management, access control, and monitoring of LLM resources across departments\n3. **Cloud Provider Integration** - Seamlessly route requests to OpenAI, Google, or Anthropic APIs through the same unified endpoint\n\n### Multi-Engine Architecture\n\nLLM Load Balancer uses a manager-based multi-engine architecture:\n\n| Engine | Status | Models | Hardware |\n|--------|--------|--------|----------|\n| **llama.cpp** | Production | GGUF format (LLaMA, Mistral, etc.) | CPU, CUDA, Metal |\n| **GPT-OSS** | Production (Metal/CUDA) | Safetensors (official GPU artifacts) | Apple Silicon, Windows |\n| **Whisper** | Production | Speech-to-Text (ASR) | CPU, CUDA, Metal |\n| **Stable Diffusion** | Production | Image Generation | CUDA, Metal |\n| **Nemotron** | Validation | Safetensors format | CUDA |\n\nManager-based runtimes replace the legacy plugin system. See `docs/manager-migration.md`\nfor migration steps.\n\n**Engine Selection Policy**:\n\n- **Models with GGUF available** → Use llama.cpp (Metal/CUDA ready)\n- **Models with safetensors only** → Implement built-in engine (Metal/CUDA support required)\n\n### Safetensors Architecture Support (Implementation-Aligned)\n\n| Architecture | Status | Notes |\n|-------------|--------|-------|\n| **gpt-oss (MoE + MXFP4)** | Implemented | Uses `mlp.router.*` and `mlp.experts.*_(blocks\\|scales\\|bias)` with MoE forward |\n| **nemotron3 (Mamba-Transformer MoE)** | Staged (not wired) | Not connected to the forward pass yet |\n\nSee <https://github.com/akiojin/xLLM>/blob/main/specs/SPEC-69549000/spec.md for the authoritative list and updates.\n\n### GGUF Architecture Coverage (llama.cpp, Examples)\n\nThese are representative examples of model families supported via GGUF/llama.cpp. This list is\nnon-exhaustive and follows upstream llama.cpp compatibility.\n\n| Architecture | Example models | Notes |\n|-------------|----------------|-------|\n| **llama** | Llama 3.1, Llama 3.2, Llama 3.3, DeepSeek-R1-Distill-Llama | Meta Llama family |\n| **mistral** | Mistral, Mistral-Nemo | Mistral AI family |\n| **gemma** | Gemma3, Gemma3n, Gemma3-QAT, FunctionGemma, EmbeddingGemma | Google Gemma family |\n| **qwen** | Qwen2.5, Qwen3, QwQ, Qwen3-VL, Qwen3-Coder, Qwen3-Embedding, Qwen3-Reranker | Alibaba Qwen family |\n| **phi** | Phi-4 | Microsoft Phi family |\n| **nemotron** | Nemotron | NVIDIA Nemotron family |\n| **deepseek** | DeepSeek-V3.2, DeepCoder-Preview | DeepSeek family |\n| **gpt-oss** | GPT-OSS, GPT-OSS-Safeguard | OpenAI GPT-OSS family |\n| **granite** | Granite-4.0-H-Small/Tiny/Micro, Granite-Docling | IBM Granite family |\n| **smollm** | SmolLM2, SmolLM3, SmolVLM | HuggingFace SmolLM family |\n| **kimi** | Kimi-K2 | Moonshot Kimi family |\n| **moondream** | Moondream2 | Moondream family |\n| **devstral** | Devstral-Small | Mistral derivative (coding-focused) |\n| **magistral** | Magistral-Small-3.2 | Mistral derivative (multimodal) |\n\n### Multimodal Support\n\nBeyond text generation, LLM Load Balancer provides OpenAI-compatible APIs for:\n\n- **Text-to-Speech (TTS)**: `/v1/audio/speech` - Generate natural speech from text\n- **Speech-to-Text (ASR)**: `/v1/audio/transcriptions` - Transcribe audio to text\n- **Image Generation**: `/v1/images/generations` - Generate images from text prompts\n\nText generation should use the **Responses API** (`/v1/responses`) by default. Chat Completions remains\navailable for compatibility.\n\n## Key Features\n\n- **Unified API Endpoint**: Access multiple LLM runtime instances through a single URL\n- **Automatic Load Balancing**: Latency-based request distribution across available endpoints\n- **Endpoint Management**: Centralized management of Ollama, vLLM, xLLM and other OpenAI-compatible servers\n- **Model Sync**: Automatic model discovery via `GET /v1/models` from registered endpoints\n- **Automatic Failure Detection**: Detect offline endpoints and exclude them from routing\n- **Real-time Monitoring**: Comprehensive visualization of endpoint states and performance metrics via web dashboard\n- **Request History Tracking**: Complete request/response logging with 7-day retention\n- **WebUI Management**: Manage endpoints, monitoring, and control through browser-based dashboard\n- **Cross-Platform Support**: Works on Windows 10+, macOS 12+, and Linux\n- **Self Update (User-Approved)**: Detect new GitHub Releases, notify via dashboard/tray, drain in-flight inference, then restart into the new version\n- **GPU-Aware Routing**: Intelligent request routing based on GPU capabilities and availability\n- **Cloud Model Prefixes**: Add `openai:` `google:` or `anthropic:` in the model name to proxy to the corresponding cloud provider while keeping the same OpenAI-compatible endpoint.\n\n## Assistant CLI for LLM Assistants\n\nAs of February 17, 2026, the former MCP server package (`@llmlb/mcp-server`) and npm\ndistribution have been removed. Assistant workflows now use built-in CLI commands.\n\n### Why `llmlb assistant`?\n\n| Feature | `llmlb assistant` | Manual Bash + curl |\n|---------|-------------------|--------------------|\n| Authentication | Auto-injected from env | Manual header management |\n| Security | Host whitelist + injection prevention | No built-in protection |\n| Sensitive data | Masked in command echo/output | Exposed in shell history |\n| API docs | `assistant openapi` / `assistant guide` | External reference needed |\n| Timeout handling | Built-in per request | Manual implementation |\n\n### Core Commands\n\n```bash\n# execute_curl equivalent\nllmlb assistant curl --command \"curl http://localhost:32768/v1/models\"\n\n# print OpenAPI JSON\nllmlb assistant openapi\n\n# print API guide text\nllmlb assistant guide --category overview\n```\n\n### Skills / Plugins\n\n- Claude Code plugin metadata: `.claude-plugin/marketplace.json`\n- Claude plugin entry: `.claude-plugin/plugins/llmlb-cli/plugin.json`\n- Claude skill mirror: `.claude/skills/llmlb-cli-usage/SKILL.md`\n- Codex skill: `.codex/skills/llmlb-cli-usage/SKILL.md`\n- Codex packaged output directory: `codex-skills/dist/`\n\n```bash\npython3 .codex/skills/.system/skill-creator/scripts/package_skill.py \\\n  .codex/skills/llmlb-cli-usage \\\n  codex-skills/dist\n```\n\n## Quick Start\n\n### LLM Load Balancer (llmlb)\n\n```bash\n# Build\ncargo build --release -p llmlb\n\n# Run\n./target/release/llmlb\n# Default: http://0.0.0.0:32768\n\n# Access dashboard\n# Open http://localhost:32768/dashboard in browser\n# (No internal API token required)\n```\n\n**Environment Variables:**\n\n| Variable | Default | Description |\n|----------|---------|-------------|\n| `LLMLB_HOST` | `0.0.0.0` | Bind address |\n| `LLMLB_PORT` | `32768` | Listen port |\n| `LLMLB_DATABASE_URL` | `sqlite:~/.llmlb/load balancer.db` | Database URL |\n| `LLMLB_LOG_LEVEL` | `info` | Log level |\n| `LLMLB_JWT_SECRET` | (auto-generated) | JWT signing secret |\n| `LLMLB_ADMIN_USERNAME` | `admin` | Initial admin username |\n| `LLMLB_ADMIN_PASSWORD` | (required) | Initial admin password |\n\n**Backward compatibility:** Legacy env var names are supported but deprecated (see full list below).\n\n**System Tray (Windows/macOS only):**\n\nOn Windows 10+ and macOS 12+, the load balancer displays a system tray icon.\nDouble-click to open the dashboard. Docker/Linux runs as a headless CLI process.\nUse `llmlb serve --no-tray` to force headless mode on supported platforms.\n\n**Self Update (notification + restart):**\n\nllmlb checks GitHub Releases in the background (best-effort, cached up to 24h). When an update is\navailable, it notifies via the dashboard and (Windows/macOS) the tray menu.\n\nWhen you approve the update (\"Restart to update\"), llmlb rejects new inference requests (`/v1/*`)\nwith 503 + `Retry-After`, waits for in-flight inference requests (including streaming) to finish,\nthen applies the update and restarts.\n\nAuto-apply method depends on the platform/install:\n\n- Portable install: replace the executable in-place when writable\n- macOS `.pkg` / Windows `.msi`: run the installer (may require elevation)\n- Linux non-writable installs: auto-apply is not supported; reinstall manually from GitHub Releases\n\n### CLI Reference\n\nThe CLI provides a small set of management subcommands:\n\n```bash\n# Start server (tray on Windows/macOS, headless on Linux)\nllmlb serve\n\n# Force headless mode on supported platforms\nllmlb serve --no-tray\n\n# Show running server status (lockfile-based)\nllmlb status\nllmlb status --port 32768\n\n# Stop a running server\nllmlb stop --port 32768\n```\n\nDay-to-day management is still done via the Dashboard UI (`/dashboard`) or the HTTP APIs.\n\n### xLLM (C++)\n\nThe xLLM runtime has moved to a separate repository:\n\n- <https://github.com/akiojin/xLLM>\n\nBuild/run instructions and environment variables are documented there.\n\n\n## Load Balancing\n\nLLM Load Balancer supports multiple load balancing strategies to optimize request distribution across runtimes.\n\n### Strategies\n\n#### 1. Metrics-Based Load Balancing (Recommended)\n\nSelects runtimes based on real-time metrics (CPU usage, memory usage, active requests). This intelligent mode provides optimal performance by dynamically routing requests to the least loaded runtime, ensuring efficient resource utilization.\n\n**Configuration:**\n```bash\n# Enable metrics-based load balancing\nLLMLB_LOAD_BALANCER_MODE=metrics cargo run -p llmlb\n```\n\n**Load Score Calculation:**\n```\nscore = cpu_usage + memory_usage + (active_requests × 10)\n```\n\nThe runtime with the **lowest score** is selected. If all runtimes have CPU usage > 80%, the system automatically falls back to round-robin.\n\n**Example:**\n- Runtime A: CPU 20%, Memory 30%, Active 1 → Score = 60 ✓ Selected\n- Runtime B: CPU 70%, Memory 50%, Active 5 → Score = 170\n\n#### 2. Advanced Load Balancing (Default)\n\nCombines multiple factors including response time, active requests, and CPU usage to provide sophisticated runtime selection with adaptive performance optimization.\n\n**Configuration:**\n```bash\n# Use default advanced load balancing (or omit LOAD_BALANCER_MODE)\nLLMLB_LOAD_BALANCER_MODE=auto cargo run -p llmlb\n```\n\n### Health / Metrics\n\nllmlb performs **pull-based health checks** against registered endpoints. Endpoints do not push\nheartbeats to the load balancer (there is no `POST /api/health`).\n\n- Endpoint status is surfaced in the dashboard and `GET /api/endpoints`.\n- Prometheus metrics are exported via `GET /api/metrics/cloud` (JWT admin or API key with\n  `metrics.read`).\n\n## Architecture\n\nLLM Load Balancer coordinates local llama.cpp runtimes and optionally proxies to cloud LLM providers via model prefixes.\n\n### Components\n- **LLM Load Balancer (Rust)**: Receives OpenAI-compatible traffic, chooses a path, and proxies requests. Exposes dashboard, metrics, and admin APIs.\n- **Local Runtimes (C++ / llama.cpp)**: Serve GGUF models; register and send heartbeats to the load balancer.\n- **Cloud Proxy**: When a model name starts with `openai:` `google:` or `anthropic:` the load balancer forwards to the corresponding cloud API.\n- **Storage**: SQLite for load balancer metadata; model files live on each runtime.\n- **Observability**: Prometheus metrics, structured logs, dashboard stats.\n\n### System Overview\n\n![System Overview](docs/diagrams/architecture.readme.en.svg)\n\nDraw.io source: `docs/diagrams/architecture.drawio` (Page: System Overview (README.md))\n\n### Request Flow\n```\nClient\n  │ POST /v1/chat/completions\n  ▼\nLLM Load Balancer (OpenAI-compatible)\n  ├─ Prefix? → Cloud API (OpenAI / Google / Anthropic)\n  └─ No prefix → Scheduler → Local Runtime\n                       └─ llama.cpp inference → Response\n```\n\n### Communication Flow (Proxy Pattern)\n\nLLM Load Balancer uses a **Proxy Pattern** - clients only need to know the load balancer URL.\n\n#### Traditional Method (Without LLM Load Balancer)\n```bash\n# Direct access to each runtime API (default: runtime_port=32769)\ncurl http://machine1:32769/v1/responses -d '...'\ncurl http://machine2:32769/v1/responses -d '...'\ncurl http://machine3:32769/v1/responses -d '...'\n```\n\n#### With LLM Load Balancer (Proxy)\n```bash\n# Unified access to LLM Load Balancer - automatic routing to the optimal runtime\ncurl http://lb:32768/v1/responses -d '...'\ncurl http://lb:32768/v1/responses -d '...'\ncurl http://lb:32768/v1/responses -d '...'\n```\n\n**Detailed Request Flow:**\n\n1. **Client → LLM Load Balancer**\n   ```\n   POST http://lb:32768/v1/responses\n   Content-Type: application/json\n\n   {\"model\": \"llama2\", \"input\": \"Hello!\"}\n   ```\n\n2. **LLM Load Balancer Internal Processing**\n   - Select optimal runtime (Load Balancing)\n   - Forward request to selected runtime via HTTP client\n\n3. **LLM Load Balancer → Runtime (Internal Communication)**\n   ```\n   POST http://runtime1:32769/v1/responses\n   Content-Type: application/json\n\n   {\"model\": \"llama2\", \"input\": \"Hello!\"}\n   ```\n\n4. **Runtime Local Processing**\n   - Runtime loads model on-demand (from local cache or load-balancer-provided source)\n   - Runtime runs llama.cpp inference and returns an OpenAI-compatible response\n\n5. **LLM Load Balancer → Client (Return Response)**\n   ```json\n  {\n    \"id\": \"resp_123\",\n    \"object\": \"response\",\n    \"output\": [\n      {\n        \"type\": \"message\",\n        \"role\": \"assistant\",\n        \"content\": [\n          { \"type\": \"output_text\", \"text\": \"Hello!\" }\n        ]\n      }\n    ]\n  }\n   ```\n\n> **Note**: LLM Load Balancer supports OpenAI-compatible APIs and **recommends** the\n> Responses API (`/v1/responses`). Chat Completions remains available for\n> compatibility.\n\n**From Client's Perspective**:\n- LLM Load Balancer appears as the only OpenAI-compatible API server\n- No need to be aware of multiple internal runtimes\n- Complete with a single HTTP request\n\n### Model Sync (No Push Distribution)\n\n- the load balancer never pushes models to runtimes.\n- Runtimes resolve models on-demand in this order:\n  - local cache (`LLM_RUNTIME_MODELS_DIR`)\n  - allowlisted origin download (Hugging Face, etc.; configure via `LLM_RUNTIME_ORIGIN_ALLOWLIST`)\n  - manifest-based selection from the load balancer (`GET /api/models/registry/:model_name/manifest.json`)\n\n### Scheduling & Health\n- Endpoints are registered via `/api/endpoints` (dashboard UI or API). CPU-only endpoints are also supported.\n- Health is pull-based: llmlb periodically probes endpoints and uses status/latency for load balancing.\n- Dashboard surfaces `*_key_present` flags so operators see which cloud keys are configured.\n\n### Benefits of Proxy Pattern\n\n1. **Unified Endpoint**\n   - Clients only need to know the load balancer URL\n   - No need to know each runtime location\n\n2. **Transparent Load Balancing**\n  - LLM Load Balancer automatically selects the optimal runtime\n   - Clients benefit from load distribution without awareness\n\n3. **Automatic Retry on Failure**\n  - If Runtime1 fails → LLM Load Balancer automatically tries Runtime2\n   - No re-request needed from client\n\n4. **Security**\n   - Runtime IP addresses not exposed to clients\n  - Only LLM Load Balancer needs to be publicly accessible\n\n5. **Scalability**\n   - Adding runtimes automatically increases processing capacity\n   - No changes needed on client side\n\n## Project Structure\n\n```\nllmlb/\n├── llmlb/              # Rust load balancer (HTTP APIs, dashboard, proxy, common types)\n├── xllm (external)     # <https://github.com/akiojin/xLLM>\n├── .claude-plugin/      # Claude Code plugin metadata and bundled skills\n├── .codex/skills/       # Codex skills (source)\n├── codex-skills/dist/   # Packaged Codex .skill artifacts\n└── specs/               # Specifications (Spec-Driven Development)\n```\n\n## Dashboard\n\nThe dashboard is served by the load balancer at `/dashboard`.\nUse it to monitor endpoints, view request history, inspect logs, and manage models.\n\n### Quick usage\n\n1. Start the load balancer:\n   ```bash\n   cargo run -p llmlb\n   ```\n1. Open:\n   ```text\n   http://localhost:32768/dashboard\n   ```\n\n### Playground routes\n\n- Endpoint Playground: `/dashboard/#playground/:endpointId`\n  - For direct endpoint verification via `POST /api/endpoints/:id/chat/completions` (JWT session)\n- LB Playground: `/dashboard/#lb-playground`\n  - For load-balancer routing tests via `GET /v1/models` and `POST /v1/chat/completions` (API key)\n\n## Endpoint Management\n\nthe load balancer centrally manages external inference servers (Ollama, vLLM, xLLM, etc.) as \"endpoints\".\n\n### Supported Endpoints\n\n| Type | Description | Health Check |\n|------|-------------|--------------|\n| **xLLM** | In-house inference server (llama.cpp/whisper.cpp) | `GET /v1/models` |\n| **Ollama** | Ollama server | `GET /v1/models` |\n| **vLLM** | vLLM inference server | `GET /v1/models` |\n| **OpenAI-compatible** | Other OpenAI-compatible APIs | `GET /v1/models` |\n\n### Registration via Dashboard\n\n1. Dashboard → Sidebar \"Endpoints\"\n2. Click \"New Endpoint\"\n3. Enter name and base URL (e.g., `http://192.168.1.100:11434`)\n4. \"Connection Test\" → \"Save\"\n\n### Registration via REST API\n\n```bash\n# Register endpoint\ncurl -X POST http://localhost:32768/api/endpoints \\\n  -H \"Authorization: Bearer sk_your_api_key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"Ollama Server A\", \"base_url\": \"http://192.168.1.100:11434\"}'\n\n# List endpoints\ncurl http://localhost:32768/api/endpoints \\\n  -H \"Authorization: Bearer sk_your_api_key\"\n\n# Sync models\ncurl -X POST http://localhost:32768/api/endpoints/{id}/sync \\\n  -H \"Authorization: Bearer sk_your_api_key\"\n```\n\n### Status Transitions\n\n- **pending**: Just registered (awaiting health check)\n- **online**: Health check successful\n- **offline**: Health check failed\n- **error**: Connection error\n\nFor details, see [specs/SPEC-e8e9326e/quickstart.md](./specs/SPEC-e8e9326e/quickstart.md).\n\n## Hugging Face registration (safetensors / GGUF)\n\n- Optional env vars: set `HF_TOKEN` to raise Hugging Face rate limits; set `HF_BASE_URL` when using a mirror/cache.\n- Web (recommended):\n  - Dashboard → **Models** → **Register**\n  - Choose `format`: `safetensors` (native engines) or `gguf` (llama.cpp fallback).\n    - If the repo contains both `safetensors` and `.gguf`, `format` is required.\n    - Safetensors text generation is available only when the safetensors.cpp engine is enabled\n      (Metal/CUDA). Use `gguf` for GGUF-only models.\n  - Enter a Hugging Face repo or file URL (e.g. `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16`).\n  - For `format=gguf`:\n    - Either specify an exact `.gguf` `filename`, or choose `gguf_policy` (`quality` / `memory` / `speed`)\n      to auto-pick from GGUF siblings.\n  - For `format=safetensors`:\n    - The HF snapshot must include `config.json` and `tokenizer.json`.\n    - Sharded weights must include an `.index.json`.\n    - If official GPU artifacts are provided (for example `model.metal.bin`), they may be used as\n      execution cache when supported. Otherwise, safetensors are used directly.\n    - Windows requires CUDA builds (`BUILD_WITH_CUDA=ON`). DirectML is not supported.\n  - LLM Load Balancer stores **metadata + manifest only** (no binary download).\n  - Model IDs are the Hugging Face repo ID (e.g. `org/model`).\n  - `/v1/models` lists models including queued/caching/error with `lifecycle_status` + `download_progress`.\n  - Runtimes pull models on-demand via the model registry endpoints:\n    - `GET /api/models/registry/:model_name/manifest.json`\n    - `GET /api/models/registry/:model_name/files/:file_name`\n    - (Legacy) `GET /api/models/blob/:model_name` for single-file GGUF.\n- API:\n  - `POST /api/models/register` with `repo` and optional `filename`.\n- `/v1/models` lists registered models; `ready` reflects runtime sync status.\n\n## Installation\n\n### Prerequisites\n\n- Linux/macOS/Windows x64 (GPU recommended)\n- Rust toolchain (stable) and cargo\n- Docker (optional)\n- CUDA Driver (for NVIDIA GPU) - see [CUDA Setup](#cuda-setup-nvidia-gpu)\n\n### Pre-built Binaries\n\nDownload platform-specific binaries from [GitHub Releases](https://github.com/akiojin/llmlb/releases).\n\n| Platform | Files |\n|----------|-------|\n| Linux x86_64 | `llmlb-linux-x86_64.tar.gz` |\n| macOS ARM64 (Apple Silicon) | `llmlb-macos-arm64.tar.gz`, `llmlb-macos-arm64.pkg` |\n| macOS x86_64 (Intel) | `llmlb-macos-x86_64.tar.gz`, `llmlb-macos-x86_64.pkg` |\n| Windows x86_64 | `llmlb-windows-x86_64.zip`, `llmlb-windows-x86_64.msi` |\n\n#### macOS Notes\n\nThe macOS `.pkg` installers are not code-signed, so you'll see a security warning on first run.\n\n**To install:**\n\n1. Right-click the `.pkg` file in Finder → Select \"Open\"\n2. Click \"Open\" to proceed\n\n**Or remove the quarantine attribute via Terminal:**\n\n```bash\nsudo xattr -d com.apple.quarantine llmlb-macos-*.pkg\n```\n\n### CUDA Setup (NVIDIA GPU)\n\nFor NVIDIA GPU acceleration, you need:\n\n| Component | Build Environment | Runtime Environment |\n|-----------|-------------------|---------------------|\n| **CUDA Driver** | Required | Required |\n| **CUDA Toolkit** | Required (for `nvcc`) | Not required |\n\n#### Installing CUDA Driver\n\nThe CUDA Driver is typically installed with NVIDIA graphics drivers.\n\n```bash\n# Verify driver installation\nnvidia-smi\n```\n\nIf `nvidia-smi` shows your GPU, the driver is installed.\n\n#### Installing CUDA Toolkit (Build Environment Only)\n\nRequired only for building the runtime with CUDA support (`BUILD_WITH_CUDA=ON`).\n\n**Windows:**\n\n1. Download from [CUDA Toolkit Downloads](https://developer.nvidia.com/cuda-downloads)\n2. Select: Windows → x86_64 → 11 → exe (local)\n3. Run the installer (Express installation recommended)\n4. Verify: Open new terminal and run `nvcc --version`\n\n**Linux (Ubuntu/Debian):**\n\n```bash\n# Add NVIDIA package repository\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt update\n\n# Install CUDA Toolkit\nsudo apt install cuda-toolkit-12-4\n\n# Add to PATH (add to ~/.bashrc)\nexport PATH=/usr/local/cuda/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n# Verify\nnvcc --version\n```\n\n**Note:** Runtime environments (runtimes running pre-built binaries) only need the CUDA\nDriver, not the full Toolkit.\n\n### 1) Build from Rust source (Recommended)\n```bash\ngit clone https://github.com/akiojin/llmlb.git\ncd llmlb\nmake quality-checks   # fmt/clippy/test/markdownlint\ncargo build -p llmlb --release\n```\nArtifact: `target/release/llmlb`\n\n### 2) Run with Docker\n```bash\ndocker build -t llmlb:latest .\ndocker run --rm -p 32768:32768 --gpus all \\\n  -e OPENAI_API_KEY=... \\\n  llmlb:latest\n```\nIf not using GPU, remove `--gpus all` or set `CUDA_VISIBLE_DEVICES=\"\"`.\n\n### 3) C++ Runtime Build\nSee <https://github.com/akiojin/xLLM> for runtime build/run details.\n\n### Requirements\n\n- **LLM Load Balancer**: Rust toolchain (stable)\n- **Runtime**: CMake + a C++ toolchain, and a supported GPU (NVIDIA / AMD / Apple Silicon)\n\n## Usage\n\n### Basic Usage\n\n1. **Start LLM Load Balancer**\n   ```bash\n   ./target/release/llmlb\n   # Default: http://0.0.0.0:32768\n   ```\n\n2. **Start Runtimes on Multiple Machines**\n   ```bash\n   # Machine 1\n   LLMLB_URL=http://lb:32768 \\\n   # Replace with your actual API key (scope: runtime)\n   LLM_RUNTIME_API_KEY=sk_your_runtime_register_key \\\n\n   # Machine 2\n   LLMLB_URL=http://lb:32768 \\\n   # Replace with your actual API key (scope: runtime)\n   LLM_RUNTIME_API_KEY=sk_your_runtime_register_key \\\n   ```\n\n3. **Send Inference Requests to LLM Load Balancer (OpenAI-compatible, Responses API recommended)**\n   ```bash\n   curl http://lb:32768/v1/responses \\\n     -H \"Content-Type: application/json\" \\\n     -H \"Authorization: Bearer sk_your_api_key\" \\\n     -d '{\n       \"model\": \"gpt-oss-20b\",\n       \"input\": \"Hello!\"\n     }'\n   ```\n\n   **Image generation example**\n   ```bash\n   curl http://lb:32768/v1/images/generations \\\n     -H \"Content-Type: application/json\" \\\n     -H \"Authorization: Bearer sk_your_api_key\" \\\n     -d '{\n       \"model\": \"stable-diffusion/v1-5-pruned-emaonly.safetensors\",\n       \"prompt\": \"A white cat sitting on a windowsill\",\n       \"size\": \"512x512\",\n       \"n\": 1,\n       \"response_format\": \"b64_json\"\n     }'\n   ```\n\n   **Image understanding example**\n   ```bash\n   curl http://lb:32768/v1/chat/completions \\\n     -H \"Content-Type: application/json\" \\\n     -H \"Authorization: Bearer sk_your_api_key\" \\\n     -d '{\n       \"model\": \"llava-v1.5-7b\",\n       \"messages\": [\n         {\n           \"role\": \"user\",\n           \"content\": [\n             {\"type\": \"text\", \"text\": \"What is in this image?\"},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg==\"}}\n           ]\n         }\n       ],\n       \"max_tokens\": 300\n     }'\n   ```\n4. **List Registered Endpoints**\n   ```bash\n   curl http://lb:32768/api/endpoints \\\n     # JWT (admin/viewer):\n     -H \"Authorization: Bearer <jwt>\"\n     # or API key (permissions: endpoints.read):\n     # -H \"X-API-Key: sk_your_endpoints_read_key\"\n   ```\n\n### Environment Variables\n\n#### LLM Load Balancer (llmlb)\n\n| Variable | Default | Description | Legacy / Notes |\n|----------|---------|-------------|----------------|\n| `LLMLB_HOST` | `0.0.0.0` | Bind address | - |\n| `LLMLB_PORT` | `32768` | Listen port | - |\n| `LLMLB_DATABASE_URL` | `sqlite:~/.llmlb/load balancer.db` | Database URL | `DATABASE_URL` |\n| `LLMLB_DATA_DIR` | `~/.llmlb` | Base directory for logs, request history, and self-update cache/payload | - |\n| `LLMLB_JWT_SECRET` | (auto-generated) | JWT signing secret | `JWT_SECRET` |\n| `LLMLB_ADMIN_USERNAME` | `admin` | Initial admin username | `ADMIN_USERNAME` |\n| `LLMLB_ADMIN_PASSWORD` | (required, first run) | Initial admin password | `ADMIN_PASSWORD` |\n| `LLMLB_LOG_LEVEL` | `info` | Log level (`EnvFilter`) | `LLM_LOG_LEVEL`, `RUST_LOG` |\n| `LLMLB_LOG_DIR` | `~/.llmlb/logs` | Log directory | `LLM_LOG_DIR` (deprecated) |\n| `LLMLB_LOG_RETENTION_DAYS` | `7` | Log retention days | `LLM_LOG_RETENTION_DAYS` |\n| `LLMLB_HEALTH_CHECK_INTERVAL` | `30` | Endpoint health check interval (seconds) | `HEALTH_CHECK_INTERVAL` |\n| `LLMLB_LOAD_BALANCER_MODE` | `auto` | Load balancer mode (`auto` / `metrics`) | `LOAD_BALANCER_MODE` |\n| `LLMLB_QUEUE_MAX` | `100` | Admission queue limit | `QUEUE_MAX` |\n| `LLMLB_QUEUE_TIMEOUT_SECS` | `60` | Admission queue timeout (seconds) | `QUEUE_TIMEOUT_SECS` |\n| `LLMLB_REQUEST_HISTORY_RETENTION_DAYS` | `7` | Request history retention days | `REQUEST_HISTORY_RETENTION_DAYS` |\n| `LLMLB_REQUEST_HISTORY_CLEANUP_INTERVAL_SECS` | `3600` | Request history cleanup interval (seconds) | `REQUEST_HISTORY_CLEANUP_INTERVAL_SECS` |\n| `LLMLB_DEFAULT_EMBEDDING_MODEL` | `nomic-embed-text-v1.5` | Default embedding model | `LLM_DEFAULT_EMBEDDING_MODEL` |\n| `LLMLB_AUTH_DISABLED` | `false` | Disable auth checks (dev/test only) | `AUTH_DISABLED` |\n| `LLM_DEFAULT_EMBEDDING_MODEL` | `nomic-embed-text-v1.5` | Default embedding model | deprecated (use `LLMLB_DEFAULT_EMBEDDING_MODEL`) |\n| `AUTH_DISABLED` | `false` | Disable auth checks (dev/test only) | deprecated (use `LLMLB_AUTH_DISABLED`) |\n| `REQUEST_HISTORY_RETENTION_DAYS` | `7` | Request history retention days | deprecated (use `LLMLB_REQUEST_HISTORY_RETENTION_DAYS`) |\n| `REQUEST_HISTORY_CLEANUP_INTERVAL_SECS` | `3600` | Request history cleanup interval (seconds) | deprecated (use `LLMLB_REQUEST_HISTORY_CLEANUP_INTERVAL_SECS`) |\n\nCloud / external services:\n\n| Variable | Default | Description | Notes |\n|----------|---------|-------------|-------|\n| `OPENAI_API_KEY` | - | API key for `openai:` models | required |\n| `OPENAI_BASE_URL` | `https://api.openai.com` | Override OpenAI base URL | optional |\n| `GOOGLE_API_KEY` | - | API key for `google:` models | required |\n| `GOOGLE_API_BASE_URL` | `https://generativelanguage.googleapis.com/v1beta` | Override Google base URL | optional |\n| `ANTHROPIC_API_KEY` | - | API key for `anthropic:` models | required |\n| `ANTHROPIC_API_BASE_URL` | `https://api.anthropic.com` | Override Anthropic base URL | optional |\n| `HF_TOKEN` | - | Hugging Face token for model pulls | optional |\n| `LLMLB_API_KEY` | - | API key used by e2e tests/clients | client/test use |\n\n#### Runtime (llm-runtime)\n\n| Variable | Default | Description | Legacy / Notes |\n|----------|---------|-------------|----------------|\n| `LLMLB_URL` | `http://127.0.0.1:32768` | Load balancer URL to register with | - |\n| `LLM_RUNTIME_API_KEY` | - | API key for runtime registration / model registry download | scope: `runtime` |\n| `LLM_RUNTIME_PORT` | `32769` | Runtime listen port | - |\n| `LLM_RUNTIME_MODELS_DIR` | `~/.llmlb/models` | Model storage directory | `LLM_MODELS_DIR` |\n| `LLM_RUNTIME_ORIGIN_ALLOWLIST` | `huggingface.co/*,cdn-lfs.huggingface.co/*` | Allowlist for direct origin downloads (comma-separated) | `LLM_ORIGIN_ALLOWLIST` |\n| `LLM_RUNTIME_BIND_ADDRESS` | `0.0.0.0` | Bind address | `LLM_BIND_ADDRESS` |\n| `LLM_RUNTIME_IP` | auto-detected | Runtime IP reported to load balancer | - |\n| `LLM_RUNTIME_HEARTBEAT_SECS` | `10` | Heartbeat interval (seconds) | `LLM_HEARTBEAT_SECS` |\n| `LLM_RUNTIME_LOG_LEVEL` | `info` | Log level | `LLM_LOG_LEVEL`, `LOG_LEVEL` |\n| `LLM_RUNTIME_LOG_DIR` | `~/.llmlb/logs` | Log directory | `LLM_LOG_DIR` |\n| `LLM_RUNTIME_LOG_RETENTION_DAYS` | `7` | Log retention days | `LLM_LOG_RETENTION_DAYS` |\n| `LLM_RUNTIME_CONFIG` | `~/.llmlb/config.json` | Path to runtime config file | - |\n| `LLM_MODEL_IDLE_TIMEOUT` | unset | Seconds before unloading idle models | enabled when set |\n| `LLM_MAX_LOADED_MODELS` | unset | Cap on simultaneously loaded models | enabled when set |\n| `LLM_MAX_MEMORY_BYTES` | unset | Max memory for loaded models | enabled when set |\n\n**Backward compatibility**: Legacy names are read for fallback but are deprecated—prefer the new names above.\n\nNote: Engine plugins were removed in favor of built-in managers. See <<https://github.com/akiojin/xLLM>/blob/main/docs/migrations/plugin-to-manager.md>.\n\n## Troubleshooting\n\n### GPU not found at startup\n- Check: `nvidia-smi` or `CUDA_VISIBLE_DEVICES`\n- Disable via env var: Runtime side `LLM_ALLOW_NO_GPU=true` (disabled by default)\n- If it still fails, check for NVML library presence\n\n### Cloud models return 401/400\n- Check if `OPENAI_API_KEY` / `GOOGLE_API_KEY` / `ANTHROPIC_API_KEY` are set on the load balancer side\n- If `*_key_present` is false in Dashboard `/api/dashboard/stats`, it's not set\n- Models without prefixes are routed locally, so do not add a prefix if you don't have cloud keys\n\n### Port conflict\n- LLM Load Balancer: Change `LLMLB_PORT` (e.g., `LLMLB_PORT=18080`)\n- Runtime: Change `LLM_RUNTIME_PORT` or use `--port`\n\n### SQLite file creation failed\n- Check write permissions for the directory in `LLMLB_DATABASE_URL` path\n- On Windows, check if the path contains spaces\n\n### Dashboard does not appear\n- Clear browser cache\n- Try `cargo clean` -> `cargo run` to check if bundled static files are broken\n- Check static delivery settings for `/dashboard/*` if using a reverse proxy\n\n### OpenAI compatible API returns 503 / Model not registered\n- Returns 503 if there are no online endpoints. Wait for endpoint startup/model load or check status at `/api/dashboard/endpoints` (JWT) or `/api/endpoints` (JWT/API key).\n- If specified model does not exist locally, wait for runtime to auto-pull\n\n### Too many / too few logs\n- Control via `LLMLB_LOG_LEVEL` or `RUST_LOG` env var (e.g., `LLMLB_LOG_LEVEL=info` or `RUST_LOG=llmlb=debug`)\n- Runtime logs use `spdlog`. Structured logs can be configured via `tracing_subscriber`\n\n## Development\n\nFor detailed development guidelines, testing procedures, and contribution workflow, see\n[CLAUDE.md](./CLAUDE.md).\n\n```bash\n# Full quality gate\nmake quality-checks\n```\n\n### PoCs\n\n- gpt-oss (auto): `make poc-gptoss`\n- gpt-oss (macOS / Metal): `make poc-gptoss-metal`\n- gpt-oss (Linux / CUDA via GGUF, experimental): `make poc-gptoss-cuda`\n  - Logs/workdir are created under `tmp/poc-gptoss-cuda/` (lb/runtime logs, request JSON, etc.)\n\nNotes:\n- gpt-oss-20b uses safetensors (index + shards + config/tokenizer) as the source of truth.\n- GPU is required. Supported backends: macOS (Metal) and Windows (CUDA). Linux/CUDA is experimental.\n\n### Spec-Driven Development\n\nThis project follows Spec-Driven Development:\n\n1. `/speckit.specify` - Create feature specification\n2. `/speckit.plan` - Create implementation plan\n3. `/speckit.tasks` - Break down into tasks\n4. Execute tasks (strict TDD cycle)\n\nSee [CLAUDE.md](./CLAUDE.md) for details.\n\n### Claude Code Worktree Hooks\n\nThis project uses Claude Code PreToolUse Hooks to enforce Worktree environment\nboundaries and prevent accidental operations that could disrupt the development workflow.\n\n**Features:**\n\n- **Git Branch Protection**: Blocks `git checkout`, `git switch`, `git worktree`\ncommands to prevent branch switching\n- **Directory Navigation Control**: Blocks `cd` commands that would move outside\nthe Worktree boundary\n- **Smart Allow Lists**: Permits read-only operations like `git branch --list`\n- **Fast Execution**: Average response time < 50ms (target: < 100ms)\n\n**Installation & Configuration:**\n\nFor detailed setup instructions, manual testing examples, and troubleshooting, see:\n\n- [Quickstart Guide](./specs/SPEC-dc648675/quickstart.md) - Step-by-step setup\nand verification\n- [Feature Specification](./specs/SPEC-dc648675/spec.md) - Requirements and\nacceptance criteria\n- [Implementation Plan](./specs/SPEC-dc648675/plan.md) - Technical design and\narchitecture\n- [Performance Report](./specs/SPEC-dc648675/performance.md) - Benchmark results\n\n**Running Hook Tests:**\n\n```bash\n# Run all Hook contract tests (13 test cases)\nmake test-hooks\n\n# Or run manually with Bats\nnpx bats tests/hooks/test-block-git-branch-ops.bats tests/hooks/test-block-cd-command.bats\n\n# Run performance benchmark\ntests/hooks/benchmark-hooks.sh\n```\n\n**Automated Testing:**\n\nHook tests are automatically executed in CI/CD:\n\n- GitHub Actions: `.github/workflows/test-hooks.yml` (standalone)\n- Quality Checks: `.github/workflows/quality-checks.yml` (integrated)\n- Makefile: `make quality-checks` includes `test-hooks` target\n\n## Request History\n\nLLM Load Balancer automatically logs all requests and responses for debugging,\nauditing, and analysis purposes.\n\n### Features\n\n- **Complete Request/Response Logging**: Captures full request bodies,\nresponse bodies, and metadata\n- **Automatic Retention**: Keeps history for 7 days with automatic cleanup\n- **Web Dashboard**: View, filter, and search request history through the\nweb interface\n- **Export Capabilities**: Export history as CSV\n- **Filtering Options**: Filter by model, runtime, status, and time range\n\n### Accessing Request History\n\n#### Via Web Dashboard\n\n1. Open the load balancer dashboard: `http://localhost:32768/dashboard`\n2. Navigate to the \"Request History\" section\n3. Use filters to narrow down specific requests\n4. Click on any request to view full details including request/response bodies\n\n#### Via API\n\n**List Request History:**\n```bash\nGET /api/dashboard/request-responses?page=1&per_page=50\n```\n\n**Get Request Details:**\n```bash\nGET /api/dashboard/request-responses/{id}\n```\n\n**Export History:**\n```bash\nGET /api/dashboard/request-responses/export\n```\n\n### Storage\n\nRequest history is stored in SQLite at:\n- Linux/macOS: `~/.llmlb/lb.db`\n- Windows: `%USERPROFILE%\\.llmlb\\lb.db`\n\nLegacy `request_history.json` files (if present) are automatically imported on startup and renamed\nto `.migrated`.\n\n## API Specification\n\n### LLM Load Balancer API\n\n#### Authentication Endpoints\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| POST | `/api/auth/login` | User authentication, JWT issuance (sets HttpOnly cookie) | None |\n| POST | `/api/auth/logout` | Logout | JWT (HttpOnly cookie or Authorization header) |\n| GET | `/api/auth/me` | Get authenticated user info | JWT (HttpOnly cookie or Authorization header) |\n\nNote: When using the JWT cookie for mutating dashboard requests, include the CSRF token via\n`X-CSRF-Token` header (token is provided in `llmlb_csrf` cookie). Origin/Referer must match the\ndashboard origin.\n\n#### Roles & API Key Permissions\n\n**User roles (JWT):**\n\n| Role | Capabilities |\n|------|--------------|\n| `admin` | Full access to `/api` management APIs and all dashboard features |\n| `viewer` | Read-only access to dashboard and endpoint read APIs; cannot access admin management APIs |\n\n**API key permissions:**\n\n| Permission | Grants |\n|---|---|\n| `openai.inference` | OpenAI-compatible inference endpoints (`POST /v1/*` except `GET /v1/models*`) |\n| `openai.models.read` | Model discovery (`GET /v1/models*`) |\n| `endpoints.read` | Read-only endpoint APIs (`GET /api/endpoints*`) |\n| `endpoints.manage` | Endpoint mutations (`POST/PUT/DELETE /api/endpoints*`, `POST /api/endpoints/:id/test`, `POST /api/endpoints/:id/sync`, `POST /api/endpoints/:id/download`) |\n| `api_keys.manage` | API key management (`/api/api-keys*`) |\n| `users.manage` | User management (`/api/users*`) |\n| `invitations.manage` | Invitation management (`/api/invitations*`) |\n| `models.manage` | Model register/delete (`POST /api/models/register`, `DELETE /api/models/*`) |\n| `registry.read` | Model registry and lists (`GET /api/models/registry/*`, `GET /api/models`, `GET /api/models/hub`) |\n| `logs.read` | Node log proxy (`GET /api/nodes/:node_id/logs`) |\n| `metrics.read` | Metrics export (`GET /api/metrics/cloud`) |\n\nDebug builds accept `sk_debug`, `sk_debug_runtime`, `sk_debug_api`, `sk_debug_admin` (see `docs/authentication.md`).\n\nNote: `/api/dashboard/*` is JWT-only (API keys are rejected).\n\n#### User Management Endpoints\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| GET | `/api/users` | List users | JWT+Admin or API key (`users.manage`) |\n| POST | `/api/users` | Create user | JWT+Admin or API key (`users.manage`) |\n| PUT | `/api/users/:id` | Update user | JWT+Admin or API key (`users.manage`) |\n| DELETE | `/api/users/:id` | Delete user | JWT+Admin or API key (`users.manage`) |\n\n#### API Key Management Endpoints\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| GET | `/api/api-keys` | List API keys | JWT+Admin or API key (`api_keys.manage`) |\n| POST | `/api/api-keys` | Create API key | JWT+Admin or API key (`api_keys.manage`) |\n| PUT | `/api/api-keys/:id` | Update API key | JWT+Admin or API key (`api_keys.manage`) |\n| DELETE | `/api/api-keys/:id` | Delete API key | JWT+Admin or API key (`api_keys.manage`) |\n\n#### Invitation Management Endpoints\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| GET | `/api/invitations` | List invitations | JWT+Admin or API key (`invitations.manage`) |\n| POST | `/api/invitations` | Create invitation | JWT+Admin or API key (`invitations.manage`) |\n| DELETE | `/api/invitations/:id` | Revoke invitation | JWT+Admin or API key (`invitations.manage`) |\n\n#### Endpoint Management Endpoints\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| GET | `/api/endpoints` | List endpoints | JWT (admin/viewer) or API key (`endpoints.read`) |\n| GET | `/api/endpoints/:id` | Get endpoint details | JWT (admin/viewer) or API key (`endpoints.read`) |\n| GET | `/api/endpoints/:id/models` | List endpoint models | JWT (admin/viewer) or API key (`endpoints.read`) |\n| GET | `/api/endpoints/:id/models/:model/info` | Get endpoint model info | JWT (admin/viewer) or API key (`endpoints.read`) |\n| GET | `/api/endpoints/:id/download/progress` | Download progress | JWT (admin/viewer) or API key (`endpoints.read`) |\n| POST | `/api/endpoints` | Register endpoint | JWT+Admin or API key (`endpoints.manage`) |\n| PUT | `/api/endpoints/:id` | Update endpoint | JWT+Admin or API key (`endpoints.manage`) |\n| DELETE | `/api/endpoints/:id` | Delete endpoint | JWT+Admin or API key (`endpoints.manage`) |\n| POST | `/api/endpoints/:id/test` | Connection test | JWT+Admin or API key (`endpoints.manage`) |\n| POST | `/api/endpoints/:id/sync` | Sync models | JWT+Admin or API key (`endpoints.manage`) |\n| POST | `/api/endpoints/:id/download` | Download model | JWT+Admin or API key (`endpoints.manage`) |\n\n#### OpenAI-Compatible Endpoints\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| POST | `/v1/chat/completions` | Chat completions API | API key (`openai.inference`) |\n| POST | `/v1/completions` | Text completions API | API key (`openai.inference`) |\n| POST | `/v1/embeddings` | Embeddings API | API key (`openai.inference`) |\n| POST | `/v1/responses` | Responses API | API key (`openai.inference`) |\n| POST | `/v1/audio/transcriptions` | Audio transcriptions API | API key (`openai.inference`) |\n| POST | `/v1/audio/speech` | Audio speech API | API key (`openai.inference`) |\n| POST | `/v1/images/generations` | Image generations API | API key (`openai.inference`) |\n| POST | `/v1/images/edits` | Image edits API | API key (`openai.inference`) |\n| POST | `/v1/images/variations` | Image variations API | API key (`openai.inference`) |\n| GET | `/v1/models` | List models (Azure-style capabilities) | API key (`openai.models.read`) |\n| GET | `/v1/models/:model_id` | Get specific model info | API key (`openai.models.read`) |\n\n#### Model Management Endpoints\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| GET | `/api/models` | List registered models | JWT+Admin or API key (`registry.read`) |\n| GET | `/api/models/hub` | List supported models + status | JWT+Admin or API key (`registry.read`) |\n| POST | `/api/models/register` | Register model (HF) | JWT+Admin or API key (`models.manage`) |\n| DELETE | `/api/models/*model_name` | Delete model | JWT+Admin or API key (`models.manage`) |\n| GET | `/api/models/registry/:model_name/manifest.json` | Get model manifest (file list) | API key (`registry.read`) |\n\n#### Dashboard Endpoints\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| GET | `/api/dashboard/endpoints` | Endpoint info list | JWT only |\n| GET | `/api/dashboard/models` | Dashboard models list | JWT only |\n| GET | `/api/dashboard/stats` | System statistics | JWT only |\n| GET | `/api/dashboard/request-history` | Request history (legacy) | JWT only |\n| GET | `/api/dashboard/overview` | Dashboard overview | JWT only |\n| GET | `/api/dashboard/metrics/:node_id` | Node metrics history | JWT only |\n| GET | `/api/dashboard/request-responses` | Request/response list | JWT only |\n| GET | `/api/dashboard/request-responses/:id` | Request/response details | JWT only |\n| GET | `/api/dashboard/request-responses/export` | Export request/responses | JWT only |\n| GET | `/api/dashboard/stats/tokens` | Token stats | JWT only |\n| GET | `/api/dashboard/stats/tokens/daily` | Daily token stats | JWT only |\n| GET | `/api/dashboard/stats/tokens/monthly` | Monthly token stats | JWT only |\n| GET | `/api/dashboard/logs/lb` | Load balancer logs | JWT only |\n\n#### Log & Metrics Endpoints\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| GET | `/api/nodes/:node_id/logs` | Node logs proxy | JWT+Admin or API key (`logs.read`) |\n| GET | `/api/metrics/cloud` | Prometheus metrics export | JWT+Admin or API key (`metrics.read`) |\n\n#### Playground Proxy\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| POST | `/api/endpoints/:id/chat/completions` | Proxy to endpoint for dashboard playground | JWT only |\n\nNotes:\n- The proxy above is for **endpoint-specific** playground (`#playground/:endpointId`).\n- LB Playground (`#lb-playground`) uses the standard OpenAI-compatible routes (`/v1/models`, `/v1/chat/completions`) with API key auth.\n\n#### Static Files & Metrics\n\n| Method | Path | Description | Auth |\n|--------|------|-------------|------|\n| GET | `/dashboard` | Dashboard UI | None |\n| GET | `/dashboard/*path` | Dashboard static files | None |\n| GET | `/api/metrics/cloud` | Prometheus metrics export | JWT+Admin or API key (`metrics.read`) |\n\n### Runtime API (C++)\n\n#### OpenAI-Compatible Endpoints\n\n| Method | Path | Description |\n|--------|------|-------------|\n| GET | `/v1/models` | List available models |\n| POST | `/v1/chat/completions` | Chat completions (streaming supported) |\n| POST | `/v1/completions` | Text completions |\n| POST | `/v1/embeddings` | Embeddings generation |\n\n#### Runtime Management Endpoints\n\n| Method | Path | Description |\n|--------|------|-------------|\n| GET | `/health` | Health check |\n| GET | `/startup` | Startup status check |\n| GET | `/metrics` | Metrics (JSON format) |\n| GET | `/metrics/prom` | Prometheus metrics |\n| GET | `/api/logs?tail=200` | Tail runtime logs (JSON) |\n| GET | `/log/level` | Get current log level |\n| POST | `/log/level` | Change log level |\n| GET | `/internal-error` | Intentional error (debug) |\n\n### Request/Response Examples\n\n#### POST /api/endpoints\n\nRegister an endpoint.\n\n**Headers:**\n\n- JWT (admin): `Authorization: Bearer <jwt>`\n- or API key (permissions: `endpoints.manage`): `X-API-Key: <api_key>`\n\n**Request:**\n\n```json\n{\n  \"name\": \"my-vllm\",\n  \"base_url\": \"http://127.0.0.1:8000\",\n  \"api_key\": \"sk-optional\"\n}\n```\n\n**Response:**\n\n```json\n{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"name\": \"my-vllm\",\n  \"base_url\": \"http://127.0.0.1:8000\",\n  \"status\": \"pending\",\n  \"created_at\": \"2026-02-09T00:00:00Z\"\n}\n```\n\n#### GET /v1/models\n\nList available models with Azure OpenAI-style capabilities.\n\n**Response:**\n\n```json\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"meta-llama/llama-3.1-8b\",\n      \"object\": \"model\",\n      \"created\": 0,\n      \"owned_by\": \"lb\",\n      \"capabilities\": {\n        \"chat_completion\": true,\n        \"completion\": true,\n        \"embeddings\": false,\n        \"fine_tune\": false,\n        \"inference\": true,\n        \"text_to_speech\": false,\n        \"speech_to_text\": false,\n        \"image_generation\": false\n      },\n      \"ready\": true\n    }\n  ]\n}\n```\n\n> **Note**: `capabilities` uses Azure OpenAI-style boolean object format.\n> `ready` is a load balancer extension derived from runtime sync state.\n\n#### POST /v1/responses\n\nResponses API (recommended).\n\n**Request:**\n\n```json\n{\n  \"model\": \"gpt-oss-20b\",\n  \"input\": \"Hello!\"\n}\n```\n\n**Response:**\n\n```json\n{\n  \"id\": \"resp_123\",\n  \"object\": \"response\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        { \"type\": \"output_text\", \"text\": \"Hello! How can I help you?\" }\n      ]\n    }\n  ]\n}\n```\n\n> **Compatibility**: `/v1/chat/completions` remains available for legacy clients.\n> **Important**: LLM Load Balancer only supports OpenAI-compatible response format.\n\n## License\n\nMIT License\n\n## Contributing\n\nIssues and Pull Requests are welcome.\n\nFor detailed development guidelines, see [CLAUDE.md](./CLAUDE.md).\n### Cloud model prefixes (OpenAI-compatible API)\n\n- Supported prefixes: `openai:`, `google:`, `anthropic:` (alias `ahtnorpic:`)\n- Usage: set `model` to e.g. `openai:gpt-4o`, `google:gemini-1.5-pro`, `anthropic:claude-3-opus`\n- Environment variables:\n  - `OPENAI_API_KEY` (required), `OPENAI_BASE_URL` (optional, default `https://api.openai.com`)\n  - `GOOGLE_API_KEY` (required), `GOOGLE_API_BASE_URL` (optional, default `https://generativelanguage.googleapis.com/v1beta`)\n  - `ANTHROPIC_API_KEY` (required), `ANTHROPIC_API_BASE_URL` (optional, default `https://api.anthropic.com`)\n- Behavior: prefix is stripped before forwarding; responses remain OpenAI-compatible. Streaming is passthrough as SSE.\n- Metrics: `/api/metrics/cloud` exports Prometheus text with per-provider counters (`cloud_requests_total{provider,status}`) and latency histogram (`cloud_request_latency_seconds{provider}`).\n"
      },
      "plugins": [
        {
          "name": "llmlb-cli",
          "description": "CLI-first assistant workflows for llmlb (curl/openapi/guide).",
          "version": "0.1.0",
          "source": "./.claude-plugin/plugins/llmlb-cli",
          "category": "development",
          "keywords": [
            "llmlb",
            "cli",
            "assistant",
            "openapi",
            "claude-code"
          ],
          "categories": [
            "assistant",
            "claude-code",
            "cli",
            "development",
            "llmlb",
            "openapi"
          ],
          "install_commands": [
            "/plugin marketplace add akiojin/llmlb",
            "/plugin install llmlb-cli@llmlb-skills"
          ]
        }
      ]
    },
    {
      "name": "unity-cli",
      "version": "1.0.0",
      "description": "Claude Code skills for unity-cli command workflows and Unity CLI Bridge operations.",
      "repo_full_name": "akiojin/unity-cli",
      "repo_url": "https://github.com/akiojin/unity-cli",
      "repo_description": "Rust CLI for Unity Editor automation over Unity TCP protocol",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-02-18T09:51:17Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"unity-cli\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Claude Code skills for unity-cli command workflows and Unity CLI Bridge operations.\",\n  \"owner\": {\n    \"name\": \"akiojin\",\n    \"url\": \"https://github.com/akiojin\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"unity-cli\",\n      \"description\": \"Claude Code plugin for Rust-based unity-cli workflows.\",\n      \"version\": \"1.0.0\",\n      \"source\": \"./.claude-plugin/plugins/unity-cli\",\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        "README.md": "# unity-cli\n\nRust-based CLI for Unity Editor automation over Unity TCP.\n\n## Install\n\nFrom crates.io:\n\n```bash\ncargo install unity-cli\n```\n\nFrom GitHub:\n\n```bash\ncargo install --git https://github.com/akiojin/unity-cli.git unity-cli\n```\n\n## Quick Start\n\n```bash\nunity-cli system ping\nunity-cli scene create MainScene\nunity-cli raw create_gameobject --json '{\"name\":\"Player\"}'\n```\n\n## Command Groups\n\n- `system`\n- `scene`\n- `instances`\n- `tool`\n- `raw`\n\nUse `raw` for full command coverage when no typed subcommand exists.\n\n## Local Tools (Rust-side)\n\nThese tools run locally:\n\n- `read`\n- `search`\n- `list_packages`\n- `get_symbols`\n- `build_index`\n- `update_index`\n- `find_symbol`\n- `find_refs`\n\n## Unity Package (UPM)\n\nUnity-side bridge package:\n\n- `UnityCliBridge/Packages/unity-cli-bridge`\n\nInstall URL:\n\n```text\nhttps://github.com/akiojin/unity-cli.git?path=UnityCliBridge/Packages/unity-cli-bridge\n```\n\n## LSP\n\nBundled C# LSP source:\n\n- `lsp/Program.cs`\n- `lsp/Server.csproj`\n\nTest command:\n\n```bash\ndotnet test lsp/Server.Tests.csproj\n```\n\n## Environment Variables\n\n- `UNITY_PROJECT_ROOT`\n- `UNITY_CLI_HOST`\n- `UNITY_CLI_PORT`\n- `UNITY_CLI_TIMEOUT_MS`\n- `UNITY_CLI_LSP_MODE` (`off` | `auto` | `required`)\n- `UNITY_CLI_LSP_COMMAND`\n- `UNITY_CLI_LSP_BIN`\n\nBackward-compatible `UNITY_MCP_*` aliases are also accepted.\n\n## Development\n\n- Contributing: `CONTRIBUTING.md`\n- Development guide: `docs/development.md`\n- Release guide: `RELEASE.md`\n\n## License\n\nMIT (`LICENSE`)\n\nIf you build an app using `unity-cli`, please include attribution (credits/about/README).\nRecommended:\n\n`This product uses unity-cli (https://github.com/akiojin/unity-cli), licensed under MIT.`\n"
      },
      "plugins": [
        {
          "name": "unity-cli",
          "description": "Claude Code plugin for Rust-based unity-cli workflows.",
          "version": "1.0.0",
          "source": "./.claude-plugin/plugins/unity-cli",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add akiojin/unity-cli",
            "/plugin install unity-cli@unity-cli"
          ]
        }
      ]
    }
  ]
}