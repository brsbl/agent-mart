{
  "author": {
    "id": "rbergman",
    "display_name": "Bob Bergman",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/273778?u=b3ba02e81c1b255db2bb2ed48e9692dd4a0b8b04&v=4",
    "url": "https://github.com/rbergman",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 6,
      "total_commands": 9,
      "total_skills": 22,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "dark-matter-marketplace",
      "version": null,
      "description": "Claude Code plugins for development skills, agents, and productivity",
      "owner_info": {
        "name": "Dark Matter",
        "email": "dark-matter@example.com"
      },
      "keywords": [],
      "repo_full_name": "rbergman/dark-matter-marketplace",
      "repo_url": "https://github.com/rbergman/dark-matter-marketplace",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-29T20:40:06Z",
        "created_at": "2026-01-09T22:28:22Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 2253
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/architecture",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/architecture/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/architecture/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 239
        },
        {
          "path": "plugins/architecture/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/architecture/skills/data-oriented-architecture",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/architecture/skills/data-oriented-architecture/SKILL.md",
          "type": "blob",
          "size": 6830
        },
        {
          "path": "plugins/architecture/skills/solid-architecture",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/architecture/skills/solid-architecture/SKILL.md",
          "type": "blob",
          "size": 4354
        },
        {
          "path": "plugins/drivers",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/drivers/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/drivers/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 211
        },
        {
          "path": "plugins/drivers/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/drivers/agents/codex-driver.md",
          "type": "blob",
          "size": 6924
        },
        {
          "path": "plugins/drivers/agents/gemini-driver.md",
          "type": "blob",
          "size": 14937
        },
        {
          "path": "plugins/game-dev",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-dev/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-dev/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 246
        },
        {
          "path": "plugins/game-dev/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-dev/skills/game-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-dev/skills/game-design/SKILL.md",
          "type": "blob",
          "size": 6339
        },
        {
          "path": "plugins/game-dev/skills/game-design/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-dev/skills/game-design/references/5-component-rubric.md",
          "type": "blob",
          "size": 2803
        },
        {
          "path": "plugins/game-dev/skills/game-design/references/domain-guide.md",
          "type": "blob",
          "size": 6926
        },
        {
          "path": "plugins/game-dev/skills/game-design/references/templates.md",
          "type": "blob",
          "size": 4355
        },
        {
          "path": "plugins/game-dev/skills/game-perf",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/game-dev/skills/game-perf/SKILL.md",
          "type": "blob",
          "size": 6719
        },
        {
          "path": "plugins/language-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 260
        },
        {
          "path": "plugins/language-pro/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/go-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/go-pro/SKILL.md",
          "type": "blob",
          "size": 10452
        },
        {
          "path": "plugins/language-pro/skills/just-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/just-pro/SKILL.md",
          "type": "blob",
          "size": 9938
        },
        {
          "path": "plugins/language-pro/skills/python-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/python-pro/SKILL.md",
          "type": "blob",
          "size": 13793
        },
        {
          "path": "plugins/language-pro/skills/rescript-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/rescript-pro/SKILL.md",
          "type": "blob",
          "size": 16057
        },
        {
          "path": "plugins/language-pro/skills/rescript-pro/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/rescript-pro/references/patterns.md",
          "type": "blob",
          "size": 12585
        },
        {
          "path": "plugins/language-pro/skills/rust-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/rust-pro/SKILL.md",
          "type": "blob",
          "size": 11801
        },
        {
          "path": "plugins/language-pro/skills/rust-pro/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/rust-pro/references/bevy.md",
          "type": "blob",
          "size": 3317
        },
        {
          "path": "plugins/language-pro/skills/rust-pro/references/patterns.md",
          "type": "blob",
          "size": 7350
        },
        {
          "path": "plugins/language-pro/skills/typescript-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/typescript-pro/SKILL.md",
          "type": "blob",
          "size": 10808
        },
        {
          "path": "plugins/language-pro/skills/typescript-pro/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/language-pro/skills/typescript-pro/references/integration.md",
          "type": "blob",
          "size": 4535
        },
        {
          "path": "plugins/language-pro/skills/typescript-pro/references/patterns.md",
          "type": "blob",
          "size": 4331
        },
        {
          "path": "plugins/tooling",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tooling/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tooling/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 223
        },
        {
          "path": "plugins/tooling/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tooling/skills/agent-dx-cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tooling/skills/agent-dx-cli/SKILL.md",
          "type": "blob",
          "size": 8887
        },
        {
          "path": "plugins/tooling/skills/agent-dx-cli/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/tooling/skills/agent-dx-cli/references/checklist.md",
          "type": "blob",
          "size": 1952
        },
        {
          "path": "plugins/workflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 223
        },
        {
          "path": "plugins/workflow/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/commands/advice.md",
          "type": "blob",
          "size": 1936
        },
        {
          "path": "plugins/workflow/commands/breakdown.md",
          "type": "blob",
          "size": 5570
        },
        {
          "path": "plugins/workflow/commands/compress.md",
          "type": "blob",
          "size": 1542
        },
        {
          "path": "plugins/workflow/commands/refine.md",
          "type": "blob",
          "size": 6232
        },
        {
          "path": "plugins/workflow/commands/review.md",
          "type": "blob",
          "size": 14978
        },
        {
          "path": "plugins/workflow/commands/snapshot.md",
          "type": "blob",
          "size": 2225
        },
        {
          "path": "plugins/workflow/commands/subagent.md",
          "type": "blob",
          "size": 2757
        },
        {
          "path": "plugins/workflow/commands/subagents.md",
          "type": "blob",
          "size": 3400
        },
        {
          "path": "plugins/workflow/commands/triage.md",
          "type": "blob",
          "size": 5370
        },
        {
          "path": "plugins/workflow/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/brainstorming",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/brainstorming/SKILL.md",
          "type": "blob",
          "size": 3383
        },
        {
          "path": "plugins/workflow/skills/cli-tools",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/cli-tools/SKILL.md",
          "type": "blob",
          "size": 6630
        },
        {
          "path": "plugins/workflow/skills/debugging",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/debugging/SKILL.md",
          "type": "blob",
          "size": 4987
        },
        {
          "path": "plugins/workflow/skills/dialectical-refinement",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/dialectical-refinement/SKILL.md",
          "type": "blob",
          "size": 10060
        },
        {
          "path": "plugins/workflow/skills/dialectical-refinement/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/dialectical-refinement/references/examples.md",
          "type": "blob",
          "size": 4038
        },
        {
          "path": "plugins/workflow/skills/mise",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/mise/SKILL.md",
          "type": "blob",
          "size": 8675
        },
        {
          "path": "plugins/workflow/skills/orchestrator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/orchestrator/SKILL.md",
          "type": "blob",
          "size": 5465
        },
        {
          "path": "plugins/workflow/skills/repo-init",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/repo-init/SKILL.md",
          "type": "blob",
          "size": 5506
        },
        {
          "path": "plugins/workflow/skills/srt",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/srt/SKILL.md",
          "type": "blob",
          "size": 14326
        },
        {
          "path": "plugins/workflow/skills/subagent",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/subagent/SKILL.md",
          "type": "blob",
          "size": 3352
        },
        {
          "path": "plugins/workflow/skills/tdd",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/tdd/SKILL.md",
          "type": "blob",
          "size": 9454
        },
        {
          "path": "plugins/workflow/skills/tdd/testing-anti-patterns.md",
          "type": "blob",
          "size": 8046
        },
        {
          "path": "plugins/workflow/skills/worktrees",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/workflow/skills/worktrees/SKILL.md",
          "type": "blob",
          "size": 5571
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"dark-matter-marketplace\",\n  \"description\": \"Claude Code plugins for development skills, agents, and productivity\",\n  \"owner\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"},\n  \"plugins\": [\n    {\n      \"name\": \"dm-lang\",\n      \"description\": \"Expert language skills: Go, Rust, TypeScript, just - idiomatic patterns, type systems, and build tools\",\n      \"version\": \"0.1.0\",\n      \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"},\n      \"source\": \"./plugins/language-pro\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"dm-arch\",\n      \"description\": \"Architecture patterns: SOLID principles, composition, data-oriented design, and module organization\",\n      \"version\": \"0.1.0\",\n      \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"},\n      \"source\": \"./plugins/architecture\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"dm-game\",\n      \"description\": \"Game development: 5-component design framework and per-frame performance optimization\",\n      \"version\": \"0.1.0\",\n      \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"},\n      \"source\": \"./plugins/game-dev\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"dm-work\",\n      \"description\": \"Workflow tools: orchestration, spec refinement, compression, peer review, and subagent delegation\",\n      \"version\": \"0.1.0\",\n      \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"},\n      \"source\": \"./plugins/workflow\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"dm-drvr\",\n      \"description\": \"AI drivers: delegate to Codex CLI or leverage Gemini's 1M token context\",\n      \"version\": \"0.1.0\",\n      \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"},\n      \"source\": \"./plugins/drivers\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"dm-tool\",\n      \"description\": \"Tool design patterns: building CLIs, MCPs, and APIs that agents can use effectively\",\n      \"version\": \"0.1.0\",\n      \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"},\n      \"source\": \"./plugins/tooling\",\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        "plugins/architecture/.claude-plugin/plugin.json": "{\n  \"name\": \"dm-arch\",\n  \"version\": \"0.2.0\",\n  \"description\": \"Architecture patterns: SOLID principles, composition, data-oriented design, and module organization\",\n  \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"}\n}\n",
        "plugins/architecture/skills/data-oriented-architecture/SKILL.md": "---\nname: data-oriented-architecture\ndescription: \"Apply data-oriented architecture patterns when designing systems with polymorphic entities. This skill should be used PROACTIVELY when encountering switch statements or if/else chains that dispatch based on entity type, when designing new entity systems, or when refactoring toward extensibility. Provides registry-based dispatch, capability composition, and infrastructure-first development patterns. Complements solid-architecture with concrete implementation strategies.\"\n---\n\n# Data-Oriented Architecture Patterns\n\n## When To Use This Skill\n\nActivate this skill when:\n- Encountering switch statements or if/else chains dispatching on entity/object type\n- Designing systems with multiple variants of similar entities\n- Refactoring code where adding new types requires changes in multiple locations\n- Building plugin systems, handler registries, or factory patterns\n- Noticing the \"expression problem\" (hard to add new types AND new operations)\n\n## Core Principle\n\n**Separate data from behavior, dispatch via registry.**\n\n```\nEntity = Pure Data (what it IS) + type discriminator\nDefinition = Bundled Behavior (what it DOES)\nRegistry = Type ‚Üí Definition mapping (HOW to dispatch)\n```\n\n## Pattern 1: Registry-Based Polymorphism\n\n### Problem\nSwitch statements scattered throughout codebase:\n\n```\n// Scattered in rendering.ts\nswitch (entity.type) {\n  case 'typeA': renderA(entity); break;\n  case 'typeB': renderB(entity); break;\n}\n\n// Scattered in update.ts\nswitch (entity.type) {\n  case 'typeA': updateA(entity); break;\n  case 'typeB': updateB(entity); break;\n}\n\n// Adding new type = edit N files\n```\n\n### Solution\nSingle registry bundling all type-specific behavior:\n\n```\n// definitions.ts - ONE location for all type-specific code\nconst DEFS: Record<EntityType, Definition> = {\n  typeA: { render: renderA, update: updateA, ... },\n  typeB: { render: renderB, update: updateB, ... },\n};\n\n// Consumers dispatch generically\nDEFS[entity.type].render(entity, ctx);\nDEFS[entity.type].update(entity, dt);\n\n// Adding new type = ONE registry entry, ZERO consumer changes\n```\n\n### Implementation Checklist\n\n1. Define base `Definition` interface with all operations\n2. Create `DEFS: Record<Type, Definition>` registry\n3. Export `getDef(type): Definition` helper\n4. Replace all switches with `getDef(entity.type).operation()`\n5. Use language features for exhaustiveness (TypeScript `Record`, Rust `match`)\n\n## Pattern 2: Capability Composition\n\n### Problem\nNot all entities need all behaviors. Deep inheritance or marker interfaces create coupling.\n\n### Solution\nOptional capability configs with type guards:\n\n```\ninterface Definition<T> {\n  // Required for all\n  create(): T;\n  render(): void;\n\n  // Optional capabilities - entities opt-in\n  collision?: CollisionConfig;\n  physics?: PhysicsConfig;\n  persistence?: PersistenceConfig;\n}\n\n// Type guard for safe access\nfunction hasCollision(def): def is Definition & { collision: CollisionConfig } {\n  return def.collision !== undefined;\n}\n\n// Consumer checks capability\nif (hasCollision(def)) {\n  collisionSystem.register(entity, def.collision);\n}\n```\n\n### Benefits\n- Entities opt-in to behaviors they need\n- No inheritance hierarchies\n- Capability presence is runtime-checkable\n- Systems ignore entities without relevant capabilities\n\n## Pattern 3: Layered Definition Interfaces\n\n```\nBaseDefinition           (create, render, layer)\n    ‚Üì extends\nDomainDefinition         (domain-specific: AI, weapons)\n    ‚Üì implemented by\nConcreteDefinitions      (typeA, typeB, typeC)\n```\n\n### Implementation\n\n```\n// Base - works for any domain\ninterface EntityDefinition<TState, TType> {\n  type: TType;\n  create(pos): TState;\n  update?(entity, ctx): void;\n  render(entity, ctx): void;\n  collision?: CollisionConfig;\n  physics?: PhysicsConfig;\n}\n\n// Domain-specific extension\ninterface EnemyDefinition extends EntityDefinition<EnemyState, EnemyType> {\n  aiStrategy: AIStrategy;\n  weapons: WeaponConfig[];\n}\n\n// Another domain\ninterface PickupDefinition extends EntityDefinition<PickupState, PickupType> {\n  onCollect(collector): void;\n  floatAnimation: AnimationConfig;\n}\n```\n\n## Pattern 4: Context Objects\n\n### Problem\nFunctions with many parameters, hard to extend.\n\n### Solution\nBundle related parameters into context objects:\n\n```\n// Bad - hard to extend\nfunction update(entity, dt, playerPos, playerVel, gravity, time) { ... }\n\n// Good - extensible\ninterface UpdateContext {\n  dt: number;\n  playerPos: Vec2;\n  playerVel: Vec2;\n  // Easy to add fields without breaking signatures\n}\n\nfunction update(entity, ctx: UpdateContext) { ... }\n```\n\n### Context Inheritance\n\n```\ninterface BaseContext { dt: number; }\ninterface AIContext extends BaseContext { playerPos: Vec2; threats: Entity[]; }\ninterface RenderContext { graphics: Graphics; screenPos: Vec2; scale: number; }\n```\n\n## Pattern 5: Infrastructure-First Development\n\n### Order of Implementation\n\n1. **Generic infrastructure first** (dispatcher, event bus, registry helpers)\n2. **Base interfaces** (EntityDefinition, capability configs)\n3. **First domain implementation** (proves the pattern)\n4. **Second domain validates pattern** (confirms generality)\n5. **Retrofit existing systems** (migrate incrementally)\n\n### Rule\n\n> If writing a switch statement on entity type, infrastructure is missing.\n\n## Anti-Patterns To Avoid\n\n### 1. Scattered Switches\nAdding new type requires editing N files.\n**Fix**: Consolidate into registry.\n\n### 2. Deep Inheritance\n`SpecialEnemy extends FlyingEnemy extends Enemy extends Entity`\n**Fix**: Capability composition.\n\n### 3. Optional Fields Instead of Capabilities\n```\ninterface Entity {\n  weapon?: Weapon;  // null checks everywhere\n}\n```\n**Fix**: Separate capability with type guard.\n\n### 4. Premature Abstraction\nCreating registry for 1 type.\n**Fix**: Wait for second type to validate pattern.\n\n### 5. God Objects\nDefinition with 50 fields for every possible behavior.\n**Fix**: Required base + optional capabilities.\n\n## Exhaustiveness Enforcement\n\nUse language features to ensure all types are handled:\n\n```typescript\n// TypeScript - Record requires all keys\nconst DEFS: Record<EntityType, Definition> = {\n  // Compiler error if type missing\n};\n\n// Helper for switch exhaustiveness\nfunction assertNever(x: never): never {\n  throw new Error(`Unexpected: ${x}`);\n}\n```\n\n## Summary Checklist\n\nWhen designing entity systems:\n\n- [ ] Entities are pure data with type discriminator field\n- [ ] Definitions bundle ALL type-specific behavior\n- [ ] Single registry maps type ‚Üí definition\n- [ ] Consumers dispatch via registry lookup (no switches)\n- [ ] Capabilities are optional configs with type guards\n- [ ] Context objects bundle related parameters\n- [ ] Language features enforce exhaustiveness\n- [ ] Adding new type = one registry entry, zero system changes\n",
        "plugins/architecture/skills/solid-architecture/SKILL.md": "---\nname: solid-architecture\ndescription: This skill should be used PROACTIVELY when writing, reviewing, or refactoring code. It provides SOLID principles, composition patterns, module organization, and side-effect boundary guidelines. Use when implementing features, fixing bugs, creating new modules, or reviewing code quality.\n---\n\n# SOLID Architecture Guidelines\n\nApply these principles when writing or modifying code. Use them as tie-breakers when design decisions conflict.\n\n## Core Goals (Priority Order)\n\n1. **Maintainability** - Easy to change without breaking unrelated parts\n2. **Testability** - Core logic testable without I/O or UI\n3. **Determinism** - Reproducible given same inputs/seeded RNG\n4. **Separation of Concerns** - Domain, infrastructure, UI clearly separated\n\n## Composition Over Inheritance\n\nFavor small, focused types composed together rather than deep inheritance trees. When tempted to extend a class, first ask: \"Can this be achieved through composition instead?\"\n\n## SOLID Principles\n\n### Single Responsibility (SRP)\n\nEach module/type/function has **one reason to change**.\n\n**Violation signals:**\n- Cannot describe purpose in one sentence\n- Domain logic mixed with infrastructure\n- Multiple unrelated reasons to modify the file\n\n**Action:** Split into focused modules with clear, singular purposes.\n\n### Open/Closed (OCP)\n\nExtend via new implementations, not constant modification.\n\n**Violation signals:**\n- Adding behavior requires modifying existing code\n- Growing switch/if-else chains for new cases\n- Frequent changes to stable modules\n\n**Action:** Add behavior through new modules and composition, not conditionals.\n\n### Liskov Substitution (LSP)\n\nSubtypes must work anywhere their base type is expected.\n\n**Violation signals:**\n- Subtypes that throw on inherited operations\n- Subtypes that ignore/no-op inherited methods\n- Deep inheritance hierarchies\n\n**Action:** Prefer interfaces over deep hierarchies. Ensure substitutability.\n\n### Interface Segregation (ISP)\n\nDepend only on the minimal surface needed.\n\n**Violation signals:**\n- Large interfaces with many methods\n- Consumers only using subset of interface\n- \"Fat\" interfaces forcing empty implementations\n\n**Action:** Create small, specific interfaces. Split large ones.\n\n### Dependency Inversion (DIP)\n\nDepend on abstractions, not concretions.\n\n**Violation signals:**\n- Direct imports of concrete implementations\n- Global singletons for RNG, config, I/O\n- Hard-coded dependencies\n\n**Action:** Inject dependencies. Use explicit context/environment objects passed to systems.\n\n## Module Organization\n\n### File Granularity\n\n- **Non-trivial types** (classes, structs, complex components): Dedicate a file\n- **Related utilities/functions**: Group by cohesive purpose in single module\n- **Avoid**: Grab-bag \"utils\" files - group by purpose instead\n\n### Layering\n\nEstablish clear dependency directions:\n\n```\ncore ‚Üí domain ‚Üí application ‚Üí UI\n```\n\n**Rules:**\n- Lower layers MUST NOT import from higher layers\n- Mark any temporary violations and track cleanup\n- Use barrel/index files only for public APIs\n- Internal modules import directly; external consumers use public API\n- Avoid circular dependencies\n\n## Side Effects & Boundaries\n\n### Pure vs Impure Separation\n\nSeparate pure computation from side effects.\n\n**Pure (no side effects):**\n- Calculations, transformations, business logic\n- Receives all inputs as parameters\n- Returns results without modifying external state\n\n**Impure (side effects):**\n- I/O operations (file, network, database)\n- Random number generation\n- Time/date operations\n- Logging, metrics\n\n### Dependency Injection\n\nSystems receive these via injection for deterministic testing:\n- Clocks\n- RNG (seeded for reproducibility)\n- I/O adapters\n\n### Boundary Modules\n\nPush I/O and external integrations to small, well-named boundary modules at system edges.\n\n**When refactoring:** Bias toward making core logic purer and pushing side effects outward.\n\n## Quick Reference Checklist\n\nBefore committing code changes:\n\n- [ ] Can each module's purpose be described in one sentence?\n- [ ] Is domain logic free from infrastructure concerns?\n- [ ] Are dependencies injected, not hard-coded?\n- [ ] Do lower layers avoid importing from higher layers?\n- [ ] Are side effects pushed to system boundaries?\n- [ ] Is the code testable without mocking I/O?\n",
        "plugins/drivers/.claude-plugin/plugin.json": "{\n  \"name\": \"dm-drvr\",\n  \"version\": \"0.3.0\",\n  \"description\": \"AI drivers: delegate to Codex CLI or leverage Gemini's 1M token context\",\n  \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"}\n}\n",
        "plugins/drivers/agents/codex-driver.md": "---\nname: codex-driver\ndescription: \"USER REQUEST ONLY: Delegate bead implementation to codex-cli with quality gates, feedback iterations, and concise summary returns. Never invoke proactively.\"\nmodel: haiku\n---\n\n**IMPORTANT: Only use this agent when the user explicitly requests Codex delegation.** Do not invoke proactively.\n\nYou are the Codex Driver Agent, a specialized subagent responsible for managing implementation loops with codex-cli. Your role is to drive codex through the complete implementation cycle for a single bead (issue), handling quality gate validation and feedback iterations, then returning a concise summary to the orchestrator.\n\n## Core Mission\n\nYou shield the orchestrator from mechanical iteration work while preserving their context tokens for strategic decisions. You are thorough in iterations but ruthlessly concise in returns.\n\n## Invocation Contract\n\nYou will receive:\n- `bead_id`: The bead to implement (e.g., \"whiteout-XXXX\")\n- `workspace_root`: Repository root path\n- `parallel_context` (optional): Information about other beads being worked on simultaneously\n\n## Implementation Protocol\n\n### Step 1: Claim and Read Context\n\n1. Claim the bead:\n   ```bash\n   bd update {bead_id} --status in_progress --no-daemon\n   ```\n\n2. Read full context:\n   ```bash\n   bd show {bead_id}\n   ```\n\n3. Extract: title, description, acceptance criteria, design notes, dependencies\n\n### Step 2: Invoke Codex\n\nUse the `mcp__codex__codex` tool with this instruction format:\n\n```\nImplement {bead_id}: {title}\n\nFollow AGENTS.md guidance:\n\n1. Read full context: bd show {bead_id}\n\n2. Implementation requirements:\n{description}\n\n**Acceptance criteria:**\n{acceptance_criteria}\n\n**Design notes:**\n{design}\n\n3. Run quality gates: npm run check\n\n4. Create devlog: docs/devlog/2025/YYMMDD-HHMM-{slug}.md\n\n5. Mark ready for review:\n   bd update {bead_id} --notes \"READY FOR REVIEW: <summary>\" --no-daemon\n```\n\nConfig: `{\"approval-policy\": \"on-request\"}`\n\n### Step 3: Run Quality Gates\n\nAfter codex signals completion:\n\n1. Run:\n   ```bash\n   npm run check\n   ```\n\n2. Analyze output:\n   - ‚úÖ All gates pass ‚Üí Proceed to Step 5 (Success)\n   - ‚ùå Any gate fails ‚Üí Proceed to Step 4 (Feedback Loop)\n\n### Step 4: Feedback Loop (Max 3 Iterations)\n\nIf quality gates fail:\n\n1. **Analyze the failure:**\n   - Read test output carefully\n   - Identify root cause (syntax error, test failure, type error, etc.)\n   - Check for contradicting requirements or test expectations\n   - Use Read tool to diagnose issues in affected files\n\n2. **Provide specific feedback to codex:**\n   ```bash\n   bd reopen {bead_id} --no-daemon\n   bd update {bead_id} --notes \"NEEDS REVISION: {specific issue and fix}\" --no-daemon\n   ```\n   - Then invoke codex again with fix guidance\n\n3. **Track iteration count:**\n   - Iteration 1-2: Continue feedback loop\n   - Iteration 3: If still failing, escalate (Step 6)\n\n### Step 5: Success Path\n\nWhen all gates pass:\n\n1. Verify devlog exists: Check for `docs/devlog/2025/*.md`\n2. Extract implementation summary from codex output and devlog\n3. Return structured summary to orchestrator (see Return Format)\n\n### Step 6: Escalation Triggers\n\nEscalate to orchestrator immediately if:\n\n**Automatic Escalation:**\n- Quality gates fail after 3 feedback iterations\n- Codex reports ambiguous requirements\n- Security-sensitive changes detected (auth, secrets, input validation)\n- Architectural decisions needed (SOLID violations, major refactoring)\n- File conflicts detected in parallel execution context\n- Bead has dependencies that aren't closed yet\n\n**Evidence-Based Escalation:**\n- Test failures reveal design contradictions\n- Implementation requires changing acceptance criteria\n- Build failures suggesting missing dependencies or infrastructure issues\n\nWhen escalating, provide detailed context in the return summary.\n\n## Parallel Execution Safety\n\n**CRITICAL**: When `parallel_context` is provided, check for file conflicts BEFORE invoking codex:\n\n1. Read `parallel_context.other_beads_in_progress`\n2. If any bead is modifying files this bead might touch, escalate immediately\n3. Reason: Git worktrees not yet implemented; avoid merge conflicts\n\n**Conflict Detection Heuristics:**\n- Same component/module mentioned in descriptions\n- Same route paths (/lobby, /games, etc.)\n- Overlapping test files\n- Shared configuration files\n\n**Safe Parallel Scenarios:**\n- Different packages in monorepo (@whiteout/engine vs @whiteout/web)\n- Different feature domains (lobby vs session vs engine)\n- Documentation-only changes\n\nWhen in doubt about parallel safety, escalate.\n\n## Return Format\n\nAlways return this structured summary:\n\n```\n## Bead: {bead_id}\n\n**Status:** ‚úÖ pass | ‚ö†Ô∏è needs_review | üö´ escalated\n\n**Summary:**\n{2-3 sentence overview of what was implemented}\n\n**Quality Gates:**\n- Lint: ‚úÖ/‚ùå\n- Typecheck: ‚úÖ/‚ùå\n- Tests: ‚úÖ/‚ùå ({passed}/{total} passed)\n- Build: ‚úÖ/‚ùå\n\n**Files Changed:**\n- {file_path}:{line_range} - {brief description}\n- ...\n\n**Iterations:** {count}/3\n\n**Devlog:** {path_to_devlog}\n\n**Issues Found:** {if any, brief description}\n\n**Escalation Reason:** {if escalated, detailed explanation}\n\n**Recommendation:**\n- ‚úÖ pass: Ready to commit and close\n- ‚ö†Ô∏è needs_review: Manual verification needed for {reason}\n- üö´ escalated: Orchestrator review required\n```\n\n## Token Optimization Rules\n\n**DO:**\n- Return concise summaries (2-3 sentences max per section)\n- Link to devlogs for evidence (don't paste full output)\n- Report test results as counts (167/173 passed), not full output\n- Focus on actionable information\n\n**DON'T:**\n- Include verbose test output in return summary\n- Paste full npm run check output\n- Read files unless diagnosing failures\n- Repeat information already in devlog\n\n## Critical Constraints\n\n1. **Maximum 3 iterations** - Escalate if not resolved\n2. **Never commit code yourself** - That's orchestrator's role\n3. **Never modify bead status to closed** - Orchestrator closes after review\n4. **Always update bead notes** when reopening for revision\n5. **Always run quality gates** - Never trust codex's self-report without verification\n6. **Preserve orchestrator tokens** - Concise returns only\n\n## Tools Available\n\nYou have access to:\n- `mcp__codex__codex` - Invoke codex-cli\n- `mcp__beads__*` - Beads operations (update, show, reopen, etc.)\n- `Bash` - Run quality gates, git commands\n- `Read` - Diagnose failures by reading files\n- `Grep/Glob` - Search codebase when needed\n\n## Success Criteria\n\n- ‚úÖ **Excellent**: Bead implemented with all gates passing\n- ‚úÖ **Good**: Bead implemented after 2-3 iterations\n- ‚úÖ **Acceptable**: Escalation with clear reason\n- ‚ùå **Failure**: Iteration >3 without escalation (violates protocol)\n- ‚ùå **Failure**: Verbose return consuming orchestrator tokens\n\nRemember: Your goal is to handle mechanical iteration work efficiently while returning only essential information to the orchestrator for strategic decisions.\n",
        "plugins/drivers/agents/gemini-driver.md": "---\nname: gemini-driver\ndescription: \"USER REQUEST ONLY: Leverage Gemini 1M context for epic planning, feature inventory, codebase research, and deep analysis tasks. Never invoke proactively.\"\nmodel: haiku\n---\n\n**IMPORTANT: Only use this agent when the user explicitly requests Gemini analysis.** Do not invoke proactively.\n\nYou are the Gemini Driver Agent, a specialized subagent responsible for high-context comprehension, planning, research, and analysis tasks leveraging Gemini's 1 million token context window. You excel at deep analysis, architectural understanding, and strategic planning.\n\n## Core Mission\n\nYou handle tasks requiring massive context comprehension that would exhaust the orchestrator's token budget. You are thorough in analysis and comprehensive in documentation, providing detailed findings that inform strategic decisions.\n\n## Invocation Contract\n\nYou will receive:\n- `task`: High-level task description (e.g., \"Generate feature inventory\", \"Expand draft epic\", \"Analyze Socket.IO integration\")\n- `workspace_root`: Repository root path\n- `context`: Project-specific context (stack, constraints, goals)\n- `output_format` (optional): Desired output format (markdown report, beads commands, JSON, etc.)\n\n## Use Cases\n\n### Use Case 1: Epic Planning & Decomposition (Spec-Kit-Lite)\n\n**When to invoke:** Draft epic needs expansion into granular tasks\n\n**Workflow (5 phases):**\n\n1. **Specification** - Extract and structure requirements\n   - Read epic description from beads\n   - Extract functional/non-functional requirements\n   - Identify constraints and dependencies\n   - Output: Structured spec document\n\n2. **Clarification** - Interactive disambiguation\n   - Flag ambiguities in requirements\n   - Use research to clarify technical approaches\n   - Output: Clarified spec with decisions captured\n\n3. **Planning** - Technical research & architecture\n   - Use `ask-gemini` with `@file` syntax to read large files\n   - Research libraries, patterns, best practices\n   - Define approach, estimate complexity\n   - Output: Technical plan with recommendations\n\n4. **Decomposition** - Granular task breakdown\n   - Break into atomic tasks (1-4 hours each)\n   - Define acceptance criteria for each task\n   - Establish dependencies (blocks, parent-child, discovered-from)\n   - Output: Complete task tree\n\n5. **Analysis** - Validation & refinement\n   - Verify tasks meet quality standards (SOLID, type safety, test coverage, security)\n   - Identify oversized tasks (>4 hours) and decompose recursively\n   - Check dependency graph (no cycles, valid DAG)\n   - Ensure traceability to epic goals\n   - Output: Validated plan with beads migration commands\n\n**Deliverable:** Complete set of `bd create` and `bd dep add` commands for orchestrator to execute\n\n### Use Case 2: Feature Inventory & Analysis\n\n**When to invoke:** Need comprehensive comparison of legacy vs new implementations\n\n**Workflow:**\n\n1. **Scan Legacy Codebase** (e.g., apps/legacy, port 3009)\n   - Use `ask-gemini` with `@directory` syntax for comprehensive reads\n   - Identify all screens, components, features\n   - Catalog API endpoints, Socket.IO events, state management patterns\n   - Extract user journeys and feature dependencies\n\n2. **Scan Production Codebase** (e.g., apps/web, port 3004)\n   - Same comprehensive scan\n   - Compare component structure, patterns, implementations\n\n3. **Gap Analysis**\n   - Features in legacy but missing in new\n   - Features in new but not in legacy\n   - Implementation differences (better/worse)\n\n4. **Categorization**\n   - **MVP Critical**: Must-have for launch (gameplay, auth, core flows)\n   - **Post-MVP**: Nice-to-have, defer to Phase 3+ (advanced stats, match history)\n   - **Not Needed**: Deprecated, experimental, remove (old prototypes)\n\n5. **Migration Planning**\n   - Prioritize features by criticality\n   - Identify risks (breaking changes, data migration, API changes)\n   - Estimate effort for each feature port\n\n**Deliverable:** Comprehensive markdown report (`docs/FEATURE_INVENTORY.md`)\n\n### Use Case 3: Codebase Research & Documentation\n\n**When to invoke:** Deep technical questions requiring whole-codebase understanding\n\n**Workflow:**\n\n1. **Research Question Analysis**\n   - Identify what information is needed\n   - Determine search strategy (keywords, file patterns, dependencies)\n\n2. **Comprehensive Codebase Read**\n   - Use `ask-gemini` with multiple `@file` or `@directory` references\n   - Leverage 1M token context to read entire subsystems\n   - Trace data flow, control flow, dependency chains\n\n3. **Synthesis & Documentation**\n   - Generate architecture diagrams (mermaid)\n   - Document patterns, conventions, anti-patterns\n   - Create reference documentation\n\n**Deliverable:** Documentation markdown or direct answer to research question\n\n### Use Case 4: UX Analysis & Specification\n\n**When to invoke:** Deep UX evaluation or design specification work\n\n**Background:** You (Gemini) authored the successful UI_DESIGN_PROMPT_V2 specification that resulted in the high-quality apps/web implementation (71% spec compliance, 100% above-fold targets).\n\n**Workflow:**\n\n1. **Read Design Specifications**\n   - Review UI_DESIGN_PROMPT_V2.md\n   - Understand design principles, constraints, targets\n\n2. **Analyze Implementation(s)**\n   - Read component code comprehensively\n   - Compare against spec requirements\n   - Identify deviations, improvements, regressions\n\n3. **Generate UX Report**\n   - Space efficiency analysis\n   - Above-fold content targets\n   - Accessibility compliance (WCAG 2.1 AA/AAA)\n   - Copy voice evaluation (casual vs technical)\n   - Mobile-first responsive patterns\n\n**Deliverable:** Comprehensive UX evaluation report with actionable recommendations\n\n## Gemini Tool Usage\n\nYou have access to `mcp__gemini-cli__ask-gemini` with these parameters:\n\n```typescript\n{\n  prompt: string;           // Analysis request\n  model?: string;           // Optional model override (default: gemini-2.5-pro)\n  sandbox?: boolean;        // Use sandbox mode for code testing\n  changeMode?: boolean;     // Enable structured change mode for code edits\n  chunkCacheKey?: string;   // For continuation of chunked responses\n  chunkIndex?: number;      // Which chunk to return\n}\n```\n\n**Key Features:**\n\n1. **@ Syntax for File Inclusion:**\n   ```\n   prompt: \"@apps/web/components/GamesScreen.tsx explain this component\"\n   prompt: \"@apps/web/ @apps/legacy/ compare these directories\"\n   ```\n\n2. **1M Token Context:**\n   - Can read entire directories, large files, multiple subsystems\n   - Ideal for whole-codebase analysis\n\n3. **Sandbox Mode:**\n   - Test code changes in isolated environment\n   - Run scripts safely\n\n4. **Change Mode:**\n   - Get structured edit suggestions\n   - Claude can apply edits directly\n\n## Research & Analysis Protocol\n\n### Step 1: Understand the Task\n\n1. Read task description carefully\n2. Identify required information sources (files, directories, beads)\n3. Determine output format (report, beads commands, JSON, etc.)\n\n### Step 2: Gather Context\n\n**For Planning Tasks:**\n- Read the epic/draft bead: `bd show {epic_id}`\n- Read related documentation (ADRs, design specs, existing plans)\n- Use `@docs/ @apps/` to read comprehensive context\n\n**For Feature Inventory:**\n- Use `@apps/web/ @apps/legacy/` to compare implementations\n- Read `package.json`, `tsconfig.json` for technical stack\n- Read user journey docs, design specs\n\n**For Codebase Research:**\n- Identify relevant packages/modules\n- Use `@packages/ @apps/` to read subsystems\n- Follow import chains, trace data flow\n\n### Step 3: Invoke Gemini with Comprehensive Context\n\nConstruct a detailed prompt with:\n\n```\nTask: {task_description}\n\nPROJECT CONTEXT:\n- Domain: WHITEOUT - turn-based social deduction game (async multiplayer)\n- Stack: React 19, Next.js 15, TypeScript strict mode, Socket.IO\n- Quality Standards: ‚â•85% test coverage, SOLID principles, no hardcoded secrets\n- Design Constraints: Mobile-first (375px primary), WCAG 2.1 AA, casual copy voice\n\nSPECIFIC CONTEXT:\n{task-specific context}\n\n@{relevant_files_or_directories}\n\nDESIRED OUTPUT:\n{output_format_specification}\n```\n\n**Use `ask-gemini` with relevant `@` includes:**\n\n```typescript\nmcp__gemini-cli__ask-gemini({\n  prompt: `Analyze Socket.IO integration across the entire codebase.\n\n  @apps/web/@apps/legacy/@packages/\n\n  Generate a comprehensive report covering:\n  1. All Socket.IO event handlers and emitters\n  2. State sync patterns\n  3. Connection management\n  4. Error handling\n  5. Testing coverage\n\n  Output: Markdown report suitable for docs/`,\n  model: \"gemini-2.5-pro\"\n})\n```\n\n### Step 4: Process Response & Generate Deliverables\n\n**For Planning Tasks:**\n- Extract task breakdown\n- Generate `bd create` commands for each task\n- Generate `bd dep add` commands for dependencies\n- Validate task structure (no cycles, all tasks atomic)\n\n**For Feature Inventory:**\n- Format as markdown table or structured list\n- Categorize features (MVP Critical, Post-MVP, Not Needed)\n- Include effort estimates, risks, dependencies\n\n**For Research:**\n- Format as comprehensive markdown report\n- Include code examples, mermaid diagrams if helpful\n- Add references to specific files and line numbers\n\n### Step 5: Return Comprehensive Report\n\nUnlike codex-driver (which returns concise summaries), you return **detailed, comprehensive findings** because:\n- Orchestrator needs full context for strategic decisions\n- Your analysis is the source of truth for planning\n- Gemini's large context captured nuanced details worth preserving\n\n**Return Format:**\n\n```markdown\n## Task: {task_description}\n\n**Status:** ‚úÖ Complete | ‚ö†Ô∏è Partial | üö´ Blocked\n\n**Executive Summary:**\n{2-3 paragraph overview of findings}\n\n**Detailed Findings:**\n\n### {Section 1}\n{comprehensive analysis}\n\n### {Section 2}\n{comprehensive analysis}\n\n**Deliverables Generated:**\n- {path_to_report}.md\n- {beads_commands} (if planning task)\n- {other_artifacts}\n\n**Recommendations:**\n1. {actionable recommendation}\n2. {actionable recommendation}\n\n**Next Steps:**\n- {what orchestrator should do next}\n\n**Attachments:**\n{links to generated reports, command sequences}\n```\n\n## Spec-Kit-Lite Execution (Planning Tasks)\n\nWhen executing spec-kit-lite for epic decomposition:\n\n**Phase 1: Specification**\n```\nprompt: \"@{epic_bead_description} @docs/ADR_*.md @docs/UI_DESIGN_PROMPT_V2.md\n\nExtract and structure requirements for this epic. Output:\n1. Functional requirements\n2. Non-functional requirements (quality, performance, security)\n3. Design constraints\n4. Dependencies on other work\n5. Success criteria\n```\n\n**Phase 2: Clarification**\n- Identify ambiguities in spec\n- Research technical approaches using `@` includes\n- Flag decisions needed (report back to orchestrator if user input needed)\n\n**Phase 3: Planning**\n- Research libraries, patterns, best practices\n- Use `@package.json @docs/` to understand current stack\n- Define technical approach, estimate complexity\n- Include library versions and compatibility checks\n\n**Phase 4: Decomposition**\n- Break epic into atomic tasks (1-4 hour each)\n- For each task:\n  - Title (imperative, clear)\n  - Description (context, what, why)\n  - Acceptance criteria (testable, specific)\n  - Design notes (how, patterns to follow)\n  - Type (task, epic, chore)\n  - Priority (P0-P3)\n  - Dependencies (which tasks block this one)\n\n**Phase 5: Analysis**\n- Verify each task is atomic (<4 hours)\n- If task >4 hours, recursively decompose\n- Check dependency graph (no cycles)\n- Ensure quality gate coverage (tests, types, security)\n- Generate beads migration commands:\n\n```bash\n# Create tasks\nbd create --type task --title \"Task 1 title\" --description \"...\" --acceptance \"...\" --design \"...\" --priority 2 --no-daemon\n# (repeat for all tasks)\n\n# Add dependencies\nbd dep add whiteout-task1 whiteout-epic --type parent-child --no-daemon\nbd dep add whiteout-task2 whiteout-task1 --type blocks --no-daemon\n# (repeat for all dependencies)\n\n# Update epic status\nbd update whiteout-epic --status open --no-daemon\n```\n\n## Critical Constraints\n\n1. **Leverage Large Context** - Don't hesitate to read entire directories with `@`\n2. **Comprehensive Analysis** - Unlike codex-driver, you return detailed findings\n3. **Research First** - Use Gemini's knowledge + codebase context before making recommendations\n4. **Validate Plans** - Recursive decomposition until all tasks are atomic\n5. **Quality Gate Focus** - Ensure tasks include testing, type safety, security checks\n6. **Beads Integration** - All planning deliverables include beads migration commands\n7. **Documentation Output** - Generate markdown reports for complex analysis\n\n## Tools Available\n\nYou have access to:\n- `mcp__gemini-cli__ask-gemini` - Invoke Gemini with large context\n- `mcp__beads__*` - Beads operations (show, list, etc.)\n- `Bash` - Run searches, grep, git commands\n- `Read` - Read files for context gathering\n- `Grep/Glob` - Search codebase patterns\n- `Write` - Generate documentation, reports\n\n## Success Criteria\n\n- ‚úÖ **Excellent**: Comprehensive analysis with actionable recommendations\n- ‚úÖ **Good**: Research completed with detailed findings and next steps\n- ‚úÖ **Acceptable**: Partial findings with clear blockers identified\n- ‚ùå **Failure**: Shallow analysis missing key context\n- ‚ùå **Failure**: Plans with circular dependencies or oversized tasks\n- ‚ùå **Failure**: Missing beads migration commands for planning tasks\n\n## Example Invocations\n\n### Epic Planning\n```\nTask: Expand Phase 0 draft epic (whiteout-d2d9) using spec-kit-lite\n\nExecute all 5 phases:\n1. Specification: Extract requirements from epic description\n2. Clarification: Research technical approaches\n3. Planning: Define architecture, estimate effort\n4. Decomposition: Break into atomic tasks with acceptance criteria\n5. Analysis: Validate, generate beads commands\n\nOutput: Complete task breakdown with migration commands\n```\n\n### Feature Inventory\n```\nTask: Generate comprehensive feature inventory comparing apps/legacy (reference) vs apps/web (production)\n\nAnalyze both codebases comprehensively:\n- All screens, components, features\n- API integrations, Socket.IO events\n- User journeys, state management\n\nCategorize features:\n- MVP Critical (must-have for launch)\n- Post-MVP (defer to Phase 3+)\n- Not Needed (deprecated, remove)\n\nOutput: docs/FEATURE_INVENTORY.md with migration priorities\n```\n\n### Codebase Research\n```\nTask: Document Socket.IO integration patterns across the codebase\n\nResearch questions:\n- How is Socket.IO initialized and configured?\n- What events are emitted/received?\n- How is state synchronized between client/server?\n- What error handling patterns are used?\n- Where are integration tests?\n\nOutput: Comprehensive architecture doc with examples\n```\n\nRemember: You are the strategic analyst with massive context capacity. Provide depth, nuance, and comprehensive findings that inform orchestrator decisions. You authored the successful design-v2 implementation - bring that same rigor to all analysis tasks.\n",
        "plugins/game-dev/.claude-plugin/plugin.json": "{\n  \"name\": \"dm-game\",\n  \"version\": \"0.2.0\",\n  \"description\": \"Game development skills: design methodology (5-component framework) and per-frame performance optimization\",\n  \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"}\n}\n",
        "plugins/game-dev/skills/game-design/SKILL.md": "---\nname: game-design\ndescription: This skill should be used when the user asks to \"design a game mechanic\", \"evaluate gameplay feel\", \"tune game systems\", \"review player experience\", \"debug why something feels wrong\", \"balance combat\", \"design progression\", or when working on any player-facing game feature. Provides a constraint system for evaluating mechanics with focus on player experience over feature completion.\n---\n\n# Game Design Framework\n\n**Purpose:** Constraint system for evaluating and implementing game mechanics with focus on player experience over feature completion.\n\n**Core principle:** Mechanics are code. Gameplay is the player's *experience* of that code. The goal is not to implement features, but to implement **Relevance**.\n\n---\n\n## Quick Reference: The 5-Component Filter\n\nBefore implementing or critiquing ANY game feature, evaluate against:\n\n| Component | Core Question | Quick Check |\n|-----------|---------------|-------------|\n| **Clarity** | Can the player predict what will happen? | Telegraph exists before resolution |\n| **Motivation** | Does the player care about the outcome? | Outcome affects persistent state |\n| **Response** | Do player inputs matter? | Actions can be buffered/cancelled meaningfully |\n| **Satisfaction** | Does success feel earned? | Multiple feedback channels fire (visual + audio minimum) |\n| **Fit** | Does it match the game's identity? | Weight, timing, audio match entity type |\n\n**Conflict priority:** Response > Clarity > Satisfaction > Fit > Motivation\n\nFor detailed evaluation rubrics, consult `references/5-component-rubric.md`.\n\n---\n\n## Operating Protocol\n\n### 1. Before Implementation\n\n1. Identify active domain(s) from `references/domain-guide.md`\n2. Evaluate against the 5-Component Filter\n3. Complete the State Machine Checklist if the feature involves player state changes\n4. Check the Numbers Policy before proposing any values\n\n### 2. Numbers Policy (Mandatory)\n\nWhen proposing ANY numeric value (timing windows, costs, speeds, damage, etc.), choose ONE:\n\n**Option A ‚Äî Source-backed:**\n- Cite a verifiable reference (GDC talk, postmortem, published analysis)\n- Example: \"Coyote time of 80-150ms (Source: Maddy Thorson's Celeste postmortem)\"\n\n**Option B ‚Äî Starting value with test plan:**\n- Label explicitly as \"Starting value\"\n- Include: micro test plan, pass/fail metric, adjustment direction if it fails\n- Example: \"Starting value: 120ms. Test: Can players make intended jumps 9/10 times? If fail rate >20%, increase by 30ms increments.\"\n\n**Never** claim \"industry standard\" or \"common practice\" without a source.\n\n### 3. Assumption Labeling\n\nWhen critical information is missing, state explicitly:\n\n```\nASSUMPTION: [what you're assuming]\nIMPACT: [why it matters to the design]\nIF WRONG: [failure mode]\nVALIDATE: [how to check quickly]\n```\n\n### 4. Research Triggers\n\nSearch before proposing when:\n- About to claim \"best practice\" or \"standard approach\"\n- Balance/economy values need benchmarks\n- Accessibility requirements apply\n- Comparative references needed from similar games\n\nIf search unavailable, convert to \"Assumption + Test Plan\" format.\n\n---\n\n## State Machine Checklist\n\nFor ANY feature that changes player state (movement abilities, combat actions, status effects):\n\n| Property | Must Define |\n|----------|-------------|\n| **Entry conditions** | What states can transition INTO this? |\n| **Exit conditions** | What ends this state? (timer, input, external event) |\n| **Interruptibility** | What can cancel this? (damage, player input, other abilities) |\n| **Chained actions** | What states can this transition TO? |\n| **Resource cost** | What is consumed on entry? On sustain? |\n| **Edge cases** | Behavior on: slopes, ceilings, moving platforms, during hitstun, at resource zero |\n\n---\n\n## Debugging Protocol\n\nWhen told \"it feels wrong/boring/clunky,\" diagnose in order:\n\n| Symptom | Check First | Before Tuning Numbers |\n|---------|-------------|----------------------|\n| \"I didn't know that would happen\" | Clarity | Add telegraph, audio cue, UI indicator |\n| \"I don't care\" | Motivation | Connect to progression, increase stakes |\n| \"It feels laggy\" | Response | Add buffering, allow cancels, reduce lockouts |\n| \"It feels weak\" | Satisfaction | Add feedback channels (minimum 2) |\n| \"It doesn't fit\" | Fit | Adjust timing, weight, audio texture |\n\n**Rule:** Do not tune damage/timing numbers until Clarity and Response are verified as not the root cause.\n\n---\n\n## Playtest Requirements\n\nEvery significant feature must include scenarios for:\n\n1. **New player test:** Can they infer the rules without being told?\n2. **Stress test:** Spam inputs, boundary conditions, edge cases\n3. **Skill test:** Can mastery improve outcomes meaningfully?\n4. **Abuse test:** Can this be exploited to skip content or trivialize risk?\n5. **Readability test:** Can an observer understand what happened and why?\n\n---\n\n## Red Flags (Stop and Clarify)\n\n- State machine transitions are undefined (\"works from any state\")\n- Multiplayer authority is unspecified\n- Economy/currency feature has no balance targets\n- Camera behavior during action is undefined\n- Feature scope is actually 3+ features in disguise\n\n---\n\n## Definition of Done\n\n- [ ] 5-Component Filter evaluated and documented\n- [ ] State Machine Checklist completed (if applicable)\n- [ ] Edge cases enumerated and handled\n- [ ] Minimum 2 feedback channels for significant actions\n- [ ] Playtest script written and smoke-tested\n- [ ] Numbers justified per Numbers Policy\n\n---\n\n## Output Structure\n\nWhen proposing or critiquing a feature:\n\n1. **Player Goal & Context** ‚Äî What is the player trying to do and why?\n2. **System Rules** ‚Äî Core behavior, failure conditions, edge cases\n3. **5-Component Evaluation** ‚Äî Which components are strong/weak?\n4. **Risks & Abuse Cases** ‚Äî What could break or be exploited?\n5. **Playtest Scenarios** ‚Äî How to validate quickly\n6. **Tuning Priority** ‚Äî What to adjust first if it doesn't feel right\n\n---\n\n## Reference Files\n\nFor detailed guidance:\n\n- **`references/5-component-rubric.md`** - Full evaluation rubrics with signals, rules, knobs, acceptance tests\n- **`references/domain-guide.md`** - Combat, movement, camera, audio, UI/UX, progression, persistence domains\n- **`references/templates.md`** - Edge case enumeration, debugging flow, playtest scripts\n",
        "plugins/game-dev/skills/game-design/references/5-component-rubric.md": "# The 5-Component Filter: Full Rubric\n\nFor significant features, evaluate each component across four dimensions.\n\n## Clarity (Telegraphing)\n\n| Dimension | Specification |\n|-----------|---------------|\n| **Signals** | Visual tells, audio cues, UI states, animation windups, camera behavior |\n| **Rules** | When telegraph triggers, what overrides it, minimum duration before resolution |\n| **Knobs** | Indicator duration, readability distance, contrast, cue timing |\n| **Acceptance Test** | Can a new player predict outcomes 8/10 times within 2 minutes of encountering the mechanic? |\n\n## Motivation (Stakes)\n\n| Dimension | Specification |\n|-----------|---------------|\n| **Signals** | Reward indicators, risk cues, threat escalation, scarcity markers |\n| **Rules** | What is gained/lost, consequences for success/failure, persistence |\n| **Knobs** | Reward magnitude, failure cost, frequency, opportunity cost |\n| **Acceptance Test** | Do players voluntarily engage without being forced? Do they retry after failure? |\n\n## Response (Agency)\n\n| Dimension | Specification |\n|-----------|---------------|\n| **Signals** | Immediate input acknowledgment, control state indicators, counter-play windows |\n| **Rules** | Input windows, cancel rules, buffer windows, lockout conditions |\n| **Knobs** | Timing windows, lockout durations, cooldowns, movement curves |\n| **Acceptance Test** | If a player changes their mind mid-action, do they have meaningful options? |\n\n## Satisfaction (Feedback)\n\n| Dimension | Specification |\n|-----------|---------------|\n| **Signals** | Impact effects, audio layers, screen response, particles, UI pulses |\n| **Rules** | What triggers feedback, scaling with outcome magnitude, layering rules |\n| **Knobs** | Intensity, duration, frequency (avoid noise spam) |\n| **Acceptance Test** | Can players feel the difference between weak/strong outcomes without reading numbers? |\n\n## Fit (Fantasy/Identity)\n\n| Dimension | Specification |\n|-----------|---------------|\n| **Signals** | Animation weight, timing curves, audio texture, VFX language |\n| **Rules** | What the fantasy allows/disallows, consistency constraints |\n| **Knobs** | Timing, exaggeration level, responsiveness vs. weight balance |\n| **Acceptance Test** | Does this action look/feel like it belongs in this world and on this character? |\n\n---\n\n## Priority Resolution\n\nWhen components conflict, resolve in this order:\n\n1. **Response** - Player must feel in control\n2. **Clarity** - Player must understand what happened\n3. **Satisfaction** - Player must feel the impact\n4. **Fit** - Experience must match fantasy\n5. **Motivation** - Stakes can be adjusted last\n\nExample: If making an attack feel \"weighty\" (Fit) would make it feel \"laggy\" (Response), prioritize Response. Find weight through feedback channels instead.\n",
        "plugins/game-dev/skills/game-design/references/domain-guide.md": "# Domain-Specific Design Guide\n\n## Combat Design\n\n**Core question:** Is the player fighting?\n\n**Sub-domains:**\n- **Melee:** Spacing, hitbox/hurtbox clarity, commitment frames, cancel windows\n- **Ranged:** Projectile vs. hitscan rules, ammo economy, aim assist policy\n- **Group combat:** Crowd control, threat readability, aggro management, target switching\n\n**Feedback requirements:**\n- Distinguish: hit confirmation vs. damage magnitude vs. status effects\n- Scale feedback with importance (avoid \"noise soup\")\n\n**Decision gate:** If feedback includes \"cheap,\" \"unfair,\" or \"confusing\" ‚Äî do NOT tune numbers first. Evaluate Clarity + Response (telegraphs, counter windows, failure messaging) before touching damage values.\n\n---\n\n## Movement / Traversal\n\n**Core question:** How does the player move through space?\n\n**Feel model:**\n- Responsiveness vs. weight is a **Fit** decision\n- Forgiveness rules (buffering, grace windows) are **Response** decisions\n\n**Reference starting values** (label as such, test before committing):\n\n| Mechanic | Starting Value | Test Criteria |\n|----------|----------------|---------------|\n| Coyote time | 80-150ms | Players make intended jumps 9/10 times |\n| Jump buffer | 100-200ms | Pre-landing jumps register reliably |\n| Corner correction | 4-8 pixels | Head-bumps on near-misses are rare |\n| Input-to-action | ‚â§100ms | Inputs feel immediate |\n\n**Diagnostic ‚Äî \"Floaty\" feel:**\n- Increase gravity/downward acceleration\n- Reduce air control\n- Reduce jump apex hangtime\n- Add landing commitment frames\n\n**Diagnostic ‚Äî \"Unresponsive\" feel:**\n- Increase initial acceleration\n- Add input buffering\n- Reduce animation lockouts\n- Verify input-to-action latency\n\n---\n\n## Camera Systems\n\n**Core question:** How does the player see the world?\n\n**Parameters to define:**\n\n| Parameter | Purpose | Typical Range |\n|-----------|---------|---------------|\n| Follow lag | Smoothness vs. precision | 0.1‚Äì0.5s |\n| Dead zone | Movement before camera responds | 10‚Äì30% screen |\n| Look-ahead | Anticipate player direction | 0‚Äì2 units |\n\n**Context transitions** (define behavior for each):\n- Exploration ‚Üí Combat\n- Combat ‚Üí Dialogue\n- Gameplay ‚Üí Cinematic\n\n**Collision rules:**\n- Camera must not clip geometry\n- Define: zoom, transparency, or cut when obstructed\n- Minimum distance when compressed\n\n---\n\n## Audio\n\n**Core question:** What does the player hear?\n\n**Spatial audio requirements:**\n- Threats audible before visible\n- Directional information clear\n- Distance attenuation appropriate\n\n**Adaptive music states:**\n- Exploration (low intensity)\n- Tension/suspicious (building)\n- Combat (full intensity)\n- Resolution (stinger + transition)\n\n**Priority/ducking hierarchy:**\n\n| Priority | Examples | Behavior |\n|----------|----------|----------|\n| Critical | Player damage, death, critical UI | Ducks everything |\n| High | Combat impacts, dialogue | Ducks ambient/music |\n| Medium | Footsteps, environmental | Normal mix |\n| Low | Ambient loops | Ducked by all |\n\n**Variation requirement:** Any sound that plays frequently needs 3+ variants to prevent repetition fatigue.\n\n---\n\n## Actions & Interactions\n\n**Core question:** How does the player manipulate the world?\n\n**Context action types:**\n- Auto-trigger (proximity): High clarity risk ‚Äî requires strong feedback on trigger zone\n- Hold-to-confirm: Slow but safe ‚Äî requires progress indicator\n- Tap-to-execute: Fast but error-prone ‚Äî requires distinct prompts\n- Dual-purpose buttons: Avoid without explicit state indicator\n\n**Resource economy rules:**\nEvery resource must answer: \"What decision does this create?\"\n\n| Resource | Decision Type |\n|----------|---------------|\n| Stamina | Pacing within encounters |\n| Mana/Energy | Ability selection and timing |\n| Cooldowns | Rotation and commitment |\n| Ammunition | Engagement distance and target priority |\n\n**Decision gate:** If one input does multiple things, define: priority order, disambiguation signals, failure messaging.\n\n---\n\n## Abilities & Progression\n\n**Core question:** How does the player evolve?\n\n**Structure types:**\n- Linear: Clear but no expression\n- Branching: Choice but balance complexity\n- Modular/loadout: Replayability but requires breadth\n\n**Progression integrity gate:** Before adding any upgrade/ability, answer:\n1. What new decision does this create?\n2. What content does this make obsolete?\n3. How does this affect difficulty pacing?\n\n**Power curve warning:** \"+5% forever\" is not meaningful progression. Upgrades should change *decisions*, not just numbers.\n\n---\n\n## Core Loop & Pacing\n\n**Core question:** What does the player do repeatedly, and why do they return?\n\n**Loop definition (mandatory for new games):**\n- **Loop verbs:** Primary actions (fight/loot/upgrade/explore/build)\n- **Reward cadence:** How often meaningful progress occurs\n- **Session arcs:** What changes at 30s / 2min / 10min / session-end\n\n**Pacing rules:**\n- Players need \"breath\" after encounters lasting >60s or requiring significant resources\n- Rest points should be visible before challenges, not discovered after\n- Intensity ramp should change *decisions*, not just numbers\n\n**Decision gate:** If a feature doesn't strengthen the core loop, it's a distraction. Must justify Fit + Motivation within the loop.\n\n---\n\n## UI/UX & Information Architecture\n\n**Core question:** How does the player understand, decide, and act?\n\n**HUD hierarchy:**\n\n| Priority | Content | Visibility |\n|----------|---------|------------|\n| Critical | Health, immediate threats | Always visible |\n| Important | Objectives, cooldowns | Glanceable |\n| Reference | Inventory, stats, map | On-demand |\n\n**Onboarding principle:** Players don't read. Design for learning-by-doing.\n- Show ‚Üí Safe practice ‚Üí Test ‚Üí Remix\n\n**Accessibility baseline:**\n- Remappable controls (high priority)\n- Subtitles/captions (high priority)\n- Colorblind modes (high priority)\n- Motion sensitivity options (camera shake, blur toggles)\n\n**Decision gate:** If the player can't tell *why* something happened, it's broken. Define: success conditions, failure reasons, signals for both.\n\n---\n\n## State Persistence\n\n**Core question:** What survives across sessions?\n\n**Save system types:**\n- Manual: Player control, save-scum risk\n- Checkpoint: Designer control, frustration if sparse\n- Autosave: Safety net, can overwrite wanted state\n- Hybrid: Flexibility, complexity\n\n**What to persist:**\n\n| Always | Consider | Usually Not |\n|--------|----------|-------------|\n| Player position/state | Camera orientation | Particle states |\n| Inventory | Enemy positions | Transient audio |\n| Quest/story flags | Destructible states | Animation frames |\n| Unlocks/abilities | Environmental changes | Temporary UI state |\n\n**Edge cases to define:**\n- Quit during combat: Resume in combat, reset encounter, or flee to safety?\n- Quit during cutscene: Resume, skip, or replay?\n- Save corruption: Backup strategy, graceful error, cloud recovery?\n",
        "plugins/game-dev/skills/game-design/references/templates.md": "# Templates & Checklists\n\n## Edge Case Enumeration Template\n\nFor traversal, combat, or ability mechanics, complete:\n\n| Scenario | Expected Behavior | Implemented? | Notes |\n|----------|-------------------|--------------|-------|\n| On slope (ascending) | | | |\n| On slope (descending) | | | |\n| Against ceiling | | | |\n| On moving platform | | | |\n| During hitstun/knockback | | | |\n| While carrying object | | | |\n| In water/special zone | | | |\n| While another ability active | | | |\n| At resource zero | | | |\n| During animation lock | | | |\n| Near geometry edge | | | |\n| At frame-rate extremes | | | |\n\n---\n\n## Extended Debugging Protocol\n\nWhen told \"it feels wrong/boring/clunky,\" diagnose in order:\n\n| Symptom | Check First | Before Tuning Numbers |\n|---------|-------------|----------------------|\n| \"I didn't know that would happen\" | Clarity | Add telegraph, audio cue, UI indicator |\n| \"I don't care\" | Motivation | Connect to progression, increase stakes |\n| \"It feels laggy\" | Response | Add buffering, allow cancels, reduce lockouts |\n| \"It feels weak\" | Satisfaction | Add feedback channels (minimum 2) |\n| \"It doesn't fit\" | Fit | Adjust timing, weight, audio texture |\n| \"I can't see what's happening\" | Camera | Adjust framing, reduce visual noise |\n| \"I didn't know I could do that\" | UI/UX | Add contextual prompts, improve tutorialization |\n\n**Rule:** Do not tune damage/timing numbers until Clarity and Response are verified as not the root cause.\n\n---\n\n## Playtest Script Template\n\n### 1. New Player Test\n- **Setup:** Fresh player, no prior explanation\n- **Task:** Complete [objective] using [mechanic]\n- **Pass:** Player succeeds 8/10 times OR correctly identifies failure reason\n- **Observe:** What questions do they ask? Where do they look?\n\n### 2. Stress Test\n- **Inputs to spam:** [list specific inputs]\n- **Boundary conditions:** [list edge cases]\n- **Expected behavior:** System degrades gracefully, no crashes/exploits\n\n### 3. Skill Test\n- **Novice baseline:** [expected outcome]\n- **Expert ceiling:** [best possible outcome]\n- **Gap required:** Experts should outperform novices by [X]%\n\n### 4. Abuse Test\n- **Exploit attempts:** [list potential exploits]\n- **Content skip attempts:** [list skip vectors]\n- **Expected result:** Exploits either impossible or not worth the effort\n\n### 5. Readability Test\n- **Observer setup:** Someone watching over shoulder\n- **Question:** \"What just happened and why?\"\n- **Pass:** Observer can explain correctly 8/10 times\n\n---\n\n## Feature Proposal Template\n\n```markdown\n## Feature: [Name]\n\n### Player Goal & Context\nWhat is the player trying to do and why?\n\n### System Rules\n- Core behavior:\n- Failure conditions:\n- Edge cases:\n\n### 5-Component Evaluation\n\n| Component | Rating | Notes |\n|-----------|--------|-------|\n| Clarity | | |\n| Motivation | | |\n| Response | | |\n| Satisfaction | | |\n| Fit | | |\n\n### State Machine (if applicable)\n- Entry conditions:\n- Exit conditions:\n- Interruptibility:\n- Chained actions:\n- Resource cost:\n\n### Risks & Abuse Cases\n- Potential exploits:\n- Balance concerns:\n\n### System Impact\n- Level design implications:\n- Difficulty curve effects:\n- Economy effects:\n\n### Playtest Scenarios\n1. New player:\n2. Stress:\n3. Skill:\n4. Abuse:\n5. Readability:\n\n### Numbers & Tuning\n| Value | Starting | Source/Test Plan |\n|-------|----------|------------------|\n| | | |\n\n### Tuning Priority\nIf it doesn't feel right, adjust in this order:\n1.\n2.\n3.\n```\n\n---\n\n## State Machine Documentation Template\n\n```markdown\n## State: [Name]\n\n### Entry Conditions\n- From [State A]: when [condition]\n- From [State B]: when [condition]\n- NOT from: [forbidden states]\n\n### Exit Conditions\n- To [State A]: when [condition]\n- To [State B]: when [condition]\n- Timeout: [duration] ‚Üí [default state]\n\n### Interruptibility\n- Cancelled by: [damage, input, ability]\n- NOT cancelled by: [list]\n- Interrupt behavior: [immediate, queued, ignored]\n\n### During State\n- Player can: [list allowed actions]\n- Player cannot: [list forbidden actions]\n- Visuals: [description]\n- Audio: [description]\n\n### Resource Interaction\n- On entry: consume [resource]\n- Per frame: drain [resource] at [rate]\n- On exit: [refund/nothing]\n- At zero: [behavior]\n\n### Edge Cases\n| Condition | Behavior |\n|-----------|----------|\n| Hit during | |\n| Input during | |\n| Resource depleted | |\n| State interrupted | |\n```\n",
        "plugins/game-dev/skills/game-perf/SKILL.md": "---\nname: game-perf\ndescription: \"Optimize game code for per-frame performance and GC pressure. Use PROACTIVELY when editing game loops, update functions, render code, or any code that runs every frame. Identifies allocation anti-patterns and provides zero-allocation alternatives.\"\n---\n\n# Game Performance Optimization\n\nThis skill provides patterns for writing allocation-free, GC-friendly code in game loops and hot paths. Apply these patterns proactively when working on any code that executes per-frame.\n\n## When to Activate\n\nTrigger this skill when editing:\n- Game loops, update functions, tick handlers\n- Render/draw functions\n- Physics update code\n- AI/behavior update code\n- Collision detection\n- Particle systems\n- Any function called 60+ times per second\n\n## Anti-Patterns and Fixes\n\n### 1. Spread Operator Copies\n\n**Problem:** Spread creates a new array every call.\n```typescript\n// BAD: Creates new array every frame\nconst context = {\n  enemies: [...this.enemies],\n  projectiles: [...this.projectiles],\n};\n```\n\n**Fix:** Pass readonly references.\n```typescript\n// GOOD: Zero allocation\nconst context = {\n  enemies: this.enemies as readonly EnemyState[],\n  projectiles: this.projectiles as readonly ProjectileState[],\n};\n```\n\n### 2. Array.filter() in Hot Paths\n\n**Problem:** `filter()` always creates a new array.\n```typescript\n// BAD: New array every call\nconst activeEnemies = enemies.filter(e => e.active);\n```\n\n**Fix:** In-place filtering with swap-and-truncate.\n```typescript\n// GOOD: Mutate in place\nfunction filterInPlace<T>(array: T[], predicate: (item: T) => boolean): void {\n  let writeIndex = 0;\n  for (let i = 0; i < array.length; i++) {\n    if (predicate(array[i])) {\n      array[writeIndex++] = array[i];\n    }\n  }\n  array.length = writeIndex;\n}\n```\n\n### 3. Array.map() for Transformations\n\n**Problem:** `map()` creates a new array.\n```typescript\n// BAD: New array every frame\nconst positions = enemies.map(e => e.worldPos);\nsteering.separation(ctx, positions, radius);\n```\n\n**Fix:** Scratch array or inline iteration.\n```typescript\n// GOOD: Reuse scratch array\nconst positionsScratch: Vec2[] = [];\n\nfunction getPositions(enemies: readonly EnemyState[]): readonly Vec2[] {\n  positionsScratch.length = 0;\n  for (const e of enemies) {\n    positionsScratch.push(e.worldPos);\n  }\n  return positionsScratch;\n}\n```\n\n### 4. Filter + Map Chains\n\n**Problem:** Double allocation.\n```typescript\n// BAD: Two new arrays\nconst activePositions = enemies\n  .filter(e => e.active)\n  .map(e => e.worldPos);\n```\n\n**Fix:** Single-pass with scratch array.\n```typescript\n// GOOD: Single pass, zero allocation\nconst scratch: Vec2[] = [];\nfunction getActivePositions(enemies: readonly EnemyState[]): readonly Vec2[] {\n  scratch.length = 0;\n  for (const e of enemies) {\n    if (e.active) scratch.push(e.worldPos);\n  }\n  return scratch;\n}\n```\n\n### 5. Returning New Arrays from Utilities\n\n**Problem:** Helper functions that return new arrays per call.\n```typescript\n// BAD: New array per entity per frame\nfunction getWrappedPositions(pos: Vec2): Vec2[] {\n  const positions = [pos];\n  // ... add wrapped positions\n  return positions;\n}\n```\n\n**Fix:** Module-level scratch with readonly return.\n```typescript\n// GOOD: Reusable scratch buffer\nconst scratchPositions: Vec2[] = [];\n\nfunction getWrappedPositions(pos: Vec2): readonly Vec2[] {\n  scratchPositions.length = 0;\n  scratchPositions.push(pos);\n  // ... add wrapped positions\n  return scratchPositions;\n}\n```\n\nThe `readonly` return type signals to callers: \"consume immediately, do not store.\"\n\n### 6. O(n¬≤) Proximity Queries\n\n**Problem:** Checking every entity against every other entity.\n```typescript\n// BAD: O(n¬≤) - checks all enemies for each enemy\nfor (const enemy of enemies) {\n  const nearby = enemies.filter(e =>\n    e !== enemy && distance(e.pos, enemy.pos) < radius\n  );\n}\n```\n\n**Fix:** Spatial hash grid for O(n) build + O(1) queries.\n```typescript\n// GOOD: Build grid once, query many times\nconst grid = new Map<string, Entity[]>();\nconst CELL_SIZE = 100;\n\nfunction buildGrid(entities: readonly Entity[]): void {\n  grid.clear();\n  for (const e of entities) {\n    const key = `${Math.floor(e.pos.x / CELL_SIZE)},${Math.floor(e.pos.y / CELL_SIZE)}`;\n    if (!grid.has(key)) grid.set(key, []);\n    grid.get(key)!.push(e);\n  }\n}\n\nfunction queryNearby(pos: Vec2, radius: number): readonly Entity[] {\n  scratch.length = 0;\n  const cx = Math.floor(pos.x / CELL_SIZE);\n  const cy = Math.floor(pos.y / CELL_SIZE);\n  // Check 3x3 cells\n  for (let dx = -1; dx <= 1; dx++) {\n    for (let dy = -1; dy <= 1; dy++) {\n      const cell = grid.get(`${cx + dx},${cy + dy}`);\n      if (cell) {\n        for (const e of cell) {\n          if (distance(e.pos, pos) < radius) scratch.push(e);\n        }\n      }\n    }\n  }\n  return scratch;\n}\n```\n\n### 7. Object Creation in Loops\n\n**Problem:** Creating temporary objects inside loops.\n```typescript\n// BAD: New object per iteration\nfor (const enemy of enemies) {\n  const ctx = { position: enemy.pos, velocity: enemy.vel };\n  updateAI(ctx);\n}\n```\n\n**Fix:** Reuse a single context object.\n```typescript\n// GOOD: Reuse context object\nconst ctx = { position: { x: 0, y: 0 }, velocity: { x: 0, y: 0 } };\n\nfor (const enemy of enemies) {\n  ctx.position.x = enemy.pos.x;\n  ctx.position.y = enemy.pos.y;\n  ctx.velocity.x = enemy.vel.x;\n  ctx.velocity.y = enemy.vel.y;\n  updateAI(ctx);\n}\n```\n\n## Architecture Patterns\n\n### Build Once, Query Many\n```typescript\n// Per-frame setup phase\nbuildSpatialGrid(entities);\nbuildEnemyGrid(enemies);\n\n// Per-entity query phase (many times)\nfor (const entity of entities) {\n  const nearby = queryNearby(entity.pos, RADIUS);\n  // process nearby...\n}\n```\n\n### Readonly Signals Transience\nWhen a function returns a `readonly` array, it communicates:\n- The array is a scratch buffer\n- Caller must consume immediately\n- Do not store the reference\n- Contents will change on next call\n\n### Object Pooling for Frequent Create/Destroy\nFor entities created/destroyed frequently (particles, projectiles):\n```typescript\nclass Pool<T> {\n  private available: T[] = [];\n\n  acquire(factory: () => T): T {\n    return this.available.pop() ?? factory();\n  }\n\n  release(item: T): void {\n    this.available.push(item);\n  }\n}\n```\n\n## Checklist for Hot Path Code\n\nBefore committing changes to per-frame code:\n\n- [ ] No spread operators (`[...array]`) on arrays that don't change\n- [ ] No `filter()` / `map()` / `reduce()` creating new arrays\n- [ ] No object literals (`{}`) or array literals (`[]`) inside loops\n- [ ] Proximity queries use spatial partitioning if > 50 entities\n- [ ] Scratch arrays used for temporary results\n- [ ] Return types are `readonly` for scratch buffers\n- [ ] Context objects are reused, not recreated\n",
        "plugins/language-pro/.claude-plugin/plugin.json": "{\n  \"name\": \"dm-lang\",\n  \"version\": \"0.6.0\",\n  \"description\": \"Expert language skills: Go, Rust, TypeScript, Python, ReScript, Just - idiomatic patterns, type systems, and build tools\",\n  \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"}\n}\n",
        "plugins/language-pro/skills/go-pro/SKILL.md": "---\nname: go-pro\ndescription: Expert Go developer specializing in idiomatic patterns, concurrency, error handling, and clean package design. This skill should be used PROACTIVELY when working on any Go code - implementing features, designing APIs, debugging issues, or reviewing code quality. Use unless a more specific subagent role applies.\n---\n\n# Go Pro\n\nSenior-level Go expertise for production projects. Focuses on idiomatic patterns, simplicity, and Go's design philosophy.\n\n## When Invoked\n\n1. Review `go.mod` and `.golangci.yml` for project conventions\n2. For build system setup, invoke the **just-pro** skill\n3. Apply Go idioms and established project patterns\n\n## Core Standards\n\n**Non-Negotiable:**\n- All exported identifiers have doc comments\n- All errors checked and handled (no `_ = err`)\n- NO `panic()` for recoverable errors\n- golangci-lint passes with project configuration\n- Table-driven tests for multiple cases\n\n**Foundational Principles:**\n- **Single Responsibility**: One package = one purpose, one function = one job\n- **No God Objects**: Split large structs; if it has 10+ fields or methods, decompose\n- **Dependency Injection**: Pass dependencies, don't create them internally\n- **Small Interfaces**: 1-3 methods max; compose larger behaviors from small interfaces\n\n---\n\n## Project Setup (Go 1.25+)\n\n### Version Management with mise\n\n[mise](https://mise.jdx.dev) manages language runtimes per-project. Ensures all contributors use the same Go version‚Äîno \"works on my machine\" issues.\n\n```bash\n# Install mise (once)\ncurl https://mise.run | sh\n\n# In project root\nmise use go@1.25\n\n# Creates .mise.toml ‚Äî commit it\n# Team members just run: mise install\n```\n\n### New Project Quick Start\n\n```bash\n# Initialize\ngo mod init github.com/org/project\ngo mod edit -go=1.25\n\n# Add toolchain dependencies (tracked in go.mod)\ngo get -tool github.com/golangci/golangci-lint/v2/cmd/golangci-lint@latest\ngo get -tool golang.org/x/tools/cmd/goimports@latest\n\n# Copy configs from this skill's references/ directory:\n#   references/gitignore          ‚Üí .gitignore\n#   references/golangci-v2.yml    ‚Üí .golangci.yml\n# For build system, invoke just-pro skill\n\n# Verify\njust check   # Or: go tool golangci-lint run\n```\n\n### Developer Onboarding\n\n```bash\ngit clone <repo> && cd <repo>\njust setup         # Runs mise trust/install + go mod download\njust check         # Verify everything works\n```\n\nOr manually:\n```bash\nmise trust && mise install  # Get pinned Go version\ngo mod download             # Get dependencies\n```\n\n**Why `go get -tool`?** Tools versioned in go.mod = reproducible builds, same versions for all devs, no separate installation needed.\n\n---\n\n## Build System\n\n**Invoke the `just-pro` skill** for build system setup. It covers:\n- Simple repos vs monorepos\n- Hierarchical justfile modules\n- Go-specific templates (`references/package-go.just`)\n\n**Why just?** Consistent toolchain frontend between agents and humans. Instead of remembering `go tool golangci-lint run --fix`, use `just fix`.\n\n---\n\n## Quality Assurance\n\n**Auto-Fix First** - Always try auto-fix before manual fixes:\n\n```bash\njust fix             # Or: go tool golangci-lint run --fix && go tool goimports -w .\n```\n\n**Verification:**\n```bash\njust check           # Or: go tool golangci-lint run && go test -race ./...\n```\n\n---\n\n## Quick Reference\n\n### Error Handling\n\n| Pattern | Use |\n|---------|-----|\n| `return err` | Propagate unchanged (internal errors) |\n| `fmt.Errorf(\"context: %w\", err)` | Wrap with context (cross-boundary) |\n| `errors.Is(err, target)` | Check specific error |\n| `errors.As(err, &target)` | Extract typed error |\n\n**Sentinel Errors** - Define package-level errors for expected conditions:\n```go\nvar ErrNotFound = errors.New(\"not found\")\nvar ErrInvalidInput = errors.New(\"invalid input\")\n```\n\n### Generics\n\n```go\n// Constrained generics - prefer specific constraints\nfunc Map[T, U any](items []T, fn func(T) U) []U {\n    result := make([]U, len(items))\n    for i, item := range items {\n        result[i] = fn(item)\n    }\n    return result\n}\n\n// Type constraints - use interfaces\ntype Ordered interface {\n    ~int | ~int64 | ~float64 | ~string\n}\n\nfunc Max[T Ordered](a, b T) T {\n    if a > b {\n        return a\n    }\n    return b\n}\n\n// Avoid: overly generic signatures that lose type safety\n// Prefer: concrete types until generics are clearly needed\n```\n\n### Structured Logging (slog)\n\n```go\nimport \"log/slog\"\n\n// Package-level logger with context\nfunc NewService(logger *slog.Logger) *Service {\n    return &Service{\n        log: logger.With(\"component\", \"service\"),\n    }\n}\n\n// Structured logging with levels\ns.log.Info(\"request processed\",\n    \"method\", r.Method,\n    \"path\", r.URL.Path,\n    \"duration\", time.Since(start),\n)\n\ns.log.Error(\"operation failed\",\n    \"err\", err,\n    \"user_id\", userID,\n)\n```\n\n### Iterators (Go 1.23+)\n\n```go\nimport \"iter\"\n\n// Return iterators for large collections\nfunc (db *DB) Users() iter.Seq[User] {\n    return func(yield func(User) bool) {\n        rows, _ := db.Query(\"SELECT * FROM users\")\n        defer rows.Close()\n        for rows.Next() {\n            var u User\n            rows.Scan(&u.ID, &u.Name)\n            if !yield(u) {\n                return\n            }\n        }\n    }\n}\n\n// Consume with range\nfor user := range db.Users() {\n    process(user)\n}\n\n// Seq2 for key-value pairs\nfunc (m *Map[K, V]) All() iter.Seq2[K, V]\n```\n\n### Concurrency\n\n| Pattern | Use |\n|---------|-----|\n| `sync.WaitGroup` | Wait for goroutines |\n| `sync.Mutex` / `RWMutex` | Protect shared state |\n| `context.Context` | Cancellation/timeouts |\n| `errgroup.Group` | Concurrent with error collection |\n\n```go\n// Context-aware work\nfunc DoWork(ctx context.Context, arg string) error {\n    select {\n    case <-ctx.Done():\n        return ctx.Err()\n    default:\n    }\n    // ... work\n}\n```\n\n### Testing\n\n```go\n// Table-driven tests with subtests\nfunc TestParse(t *testing.T) {\n    tests := []struct {\n        name    string\n        input   string\n        want    Result\n        wantErr bool\n    }{\n        {name: \"valid\", input: \"foo\", want: Result{Value: \"foo\"}},\n        {name: \"empty\", input: \"\", wantErr: true},\n        {name: \"special\", input: \"a@b\", want: Result{Value: \"a@b\"}},\n    }\n\n    for _, tt := range tests {\n        t.Run(tt.name, func(t *testing.T) {\n            got, err := Parse(tt.input)\n            if (err != nil) != tt.wantErr {\n                t.Errorf(\"Parse() error = %v, wantErr %v\", err, tt.wantErr)\n                return\n            }\n            if got != tt.want {\n                t.Errorf(\"Parse() = %v, want %v\", got, tt.want)\n            }\n        })\n    }\n}\n\n// Testify for complex assertions\nimport \"github.com/stretchr/testify/assert\"\nimport \"github.com/stretchr/testify/require\"\n\nfunc TestService(t *testing.T) {\n    require.NoError(t, err)           // Fail fast\n    assert.Equal(t, expected, actual) // Continue on failure\n    assert.Len(t, items, 3)\n    assert.Contains(t, items, target)\n}\n```\n\n### Pointer vs Value Receivers\n\n```go\n// Use pointer receivers when:\n// - Method modifies the receiver\n// - Receiver is large (avoid copy)\n// - Consistency: if any method needs pointer, use pointer for all\nfunc (s *Service) UpdateConfig(cfg Config) { s.cfg = cfg }\n\n// Use value receivers when:\n// - Receiver is small (int, string, small struct)\n// - Method is read-only and receiver is immutable\nfunc (p Point) Distance(other Point) float64 { ... }\n```\n\n### Package Organization\n\n```\nproject/\n‚îú‚îÄ‚îÄ cmd/appname/main.go   # Entry point\n‚îú‚îÄ‚îÄ internal/             # Private packages\n‚îÇ   ‚îú‚îÄ‚îÄ api/              # Handlers\n‚îÇ   ‚îî‚îÄ‚îÄ domain/           # Business logic\n‚îú‚îÄ‚îÄ go.mod\n‚îú‚îÄ‚îÄ .golangci.yml\n‚îî‚îÄ‚îÄ justfile\n```\n\n**Rules:** One package = one purpose. Use `internal/` for implementation. Avoid `util`, `common`, `helpers` packages.\n\n---\n\n## DX Patterns\n\n### Doctor Recipe with Version Validation\n\nDoctor scripts should validate that toolchain versions meet requirements, not just check existence:\n\n```just\n# Validate toolchain versions meet requirements\ndoctor:\n    #!/usr/bin/env bash\n    set -euo pipefail\n    echo \"Checking toolchain...\"\n\n    # Validate Go version (requires 1.25+)\n    GO_VERSION=$(go version | grep -oE 'go[0-9]+\\.[0-9]+' | sed 's/go//')\n    if [[ \"$(printf '%s\\n' \"1.25\" \"$GO_VERSION\" | sort -V | head -1)\" != \"1.25\" ]]; then\n        echo \"FAIL: Go $GO_VERSION < 1.25 required\"\n        exit 1\n    fi\n    echo \"‚úì Go $GO_VERSION\"\n\n    # Add more version checks as needed\n    echo \"All checks passed\"\n```\n\n### Port Conflict Detection\n\nFor services that bind ports, check availability before starting:\n\n```just\n# Check if required ports are available before starting\ncheck-ports:\n    #!/usr/bin/env bash\n    for port in 8080 5432; do\n        if lsof -i :$port >/dev/null 2>&1; then\n            echo \"FAIL: Port $port already in use\"\n            exit 1\n        fi\n    done\n    echo \"All ports available\"\n```\n\n### First-Run Detection\n\nAvoid redundant setup work with first-run detection:\n\n```just\n# Setup with first-run detection\nsetup:\n    #!/usr/bin/env bash\n    if [[ -f .setup-complete ]]; then\n        echo \"Already set up. Run 'just setup-force' to reinstall.\"\n        exit 0\n    fi\n    mise trust && mise install\n    go mod download\n    touch .setup-complete\n    echo \"Setup complete\"\n\nsetup-force:\n    rm -f .setup-complete\n    @just setup\n```\n\n---\n\n## Anti-Patterns\n\n- `panic()` for recoverable errors (use `return err`)\n- Ignoring errors with `_`\n- Exported package-level mutable variables\n- Channels when mutex suffices\n- Getter/setter methods (Go isn't Java)\n- `init()` with side effects\n- God structs with 10+ fields/methods\n- `interface{}` or `any` when specific types work\n- Premature generics (concrete types first)\n\n---\n\n## AI Agent Guidelines\n\n**Before writing code:**\n1. Read `go.mod` for module path and Go version\n2. Check `.golangci.yml` for project-specific lint rules\n3. Identify existing patterns in the codebase to follow\n\n**When writing code:**\n1. Handle all errors explicitly - never use `_ = err`\n2. Add doc comments to exported identifiers immediately\n3. Use existing project abstractions over creating new ones\n4. Prefer concrete types; add generics only when pattern repeats 3+ times\n\n**Before committing:**\n1. Run `just check` (standard for projects using just)\n2. Fallback: `go tool golangci-lint run --fix && go tool golangci-lint run`\n3. Fallback: `go test -race ./...` to catch race conditions\n",
        "plugins/language-pro/skills/just-pro/SKILL.md": "---\nname: just-pro\ndescription: This skill provides patterns for setting up just (command runner) in projects. Use PROACTIVELY when creating build systems, setting up new repos, or when the user asks about just/justfile configuration. Covers both simple single-project repos and monorepos with hierarchical justfile modules.\n---\n\n# Justfile Skill\n\nBuild system configuration using [just](https://just.systems), a modern command runner.\n\n**Related skills:**\n- **mise** - Tool version management (includes just+mise integration patterns)\n- **go-pro**, **rust-pro**, **typescript-pro** - Language-specific templates\n\n## Installation\n\n```bash\n# Via mise (recommended - version pinned per-project)\nmise use just\n\n# macOS\nbrew install just\n\n# Linux/Windows (via cargo)\ncargo install just\n\n# Or prebuilt binaries: https://github.com/casey/just/releases\n```\n\n## When to Use Just\n\n| Scenario | Recommendation |\n|----------|----------------|\n| Cross-language monorepo | **just** - Unified interface across packages |\n| Single Go/Rust project | **just** or language-native (go/cargo) |\n| Node.js project | npm scripts primary, just optional wrapper |\n| CI/CD porcelain | **just** - Single entry point for all operations |\n| Simple scripts | **just** - Better than shell scripts |\n\n## Project Patterns\n\n### Simple Repo (Single Package)\n\nFor single-language projects, create one `justfile` at repo root.\n\n```just\n# Project Build System\n# Usage: just --list\n\ndefault:\n    @just --list\n\n# === Quality Gates ===\ncheck: fmt lint test\n    @echo \"All checks passed\"\n\nfmt:\n    go fmt ./...\n\nlint:\n    go tool golangci-lint run\n\ntest:\n    go test -race ./...\n\nbuild:\n    go build -o bin/app ./cmd/app\n```\n\nSee `references/simple-repo.just` for complete templates.\n\n### Monorepo (Multiple Packages)\n\nFor monorepos, use hierarchical justfiles with the `mod` system:\n\n```\nrepo/\n‚îú‚îÄ‚îÄ justfile              # Router - imports package modules\n‚îî‚îÄ‚îÄ packages/\n    ‚îú‚îÄ‚îÄ api-go/\n    ‚îÇ   ‚îî‚îÄ‚îÄ justfile      # Go package recipes\n    ‚îú‚îÄ‚îÄ web/\n    ‚îÇ   ‚îî‚îÄ‚îÄ justfile      # TypeScript package recipes (optional)\n    ‚îî‚îÄ‚îÄ ops/\n        ‚îî‚îÄ‚îÄ justfile      # DevOps recipes\n```\n\n**Root justfile** (router):\n```just\n# Monorepo Build System\n# Usage: just --list\n# Usage: just go <recipe>\n\nmod go \"packages/api-go\"\nmod web \"packages/web\"\nmod ops \"ops\"\n\ndefault:\n    @just --list\n\n# Umbrella recipes call into modules\ncheck: (go::check) (web::check)\n    @echo \"All checks passed\"\n\nsetup: (go::setup) (web::setup)\n    @echo \"All toolchains ready\"\n```\n\n**Commands become**: `just go check`, `just go lint`, `just web build`, `just ops deploy`\n\nSee `references/monorepo-root.just` and `references/package-go.just` for templates.\n\n## Recipe Patterns\n\n### Quality Gates\n\nAlways provide a single `check` recipe that runs all quality gates:\n\n```just\n# Full quality gates - run before commits\ncheck: fmt lint test coverage-check\n    @echo \"All checks passed\"\n```\n\n### Quick Check (Dev Iteration)\n\nFast feedback loop for development - essential checks only:\n\n```just\n# Quick feedback loop - essential checks only\ncheck-quick: lint test\n    @echo \"Quick checks passed\"\n```\n\n### Clean Recipe\n\nStandard cleanup for build artifacts and caches:\n\n```just\n# Remove build artifacts and caches\nclean:\n    rm -rf build/ coverage.out node_modules/.cache\n```\n\n### Parallel Check Execution (CI)\n\nRun package checks in parallel for faster CI:\n\n```just\n# Run all package checks in parallel (CI optimization)\ncheck-parallel:\n    @just api check & just web check & wait\n```\n\n### Coverage Enforcement\n\nUse shebang for multi-line shell logic:\n\n```just\n# Check coverage meets 70% minimum\ncoverage-check:\n    #!/usr/bin/env bash\n    set -euo pipefail\n    go test -race -coverprofile=coverage.out ./...\n    COVERAGE=$(go tool cover -func=coverage.out | grep total | awk '{print $3}' | sed 's/%//')\n    COVERAGE_INT=${COVERAGE%.*}\n    [ \"$COVERAGE_INT\" -ge 70 ] || (echo \"FAIL: Coverage ${COVERAGE}% < 70%\" && exit 1)\n    echo \"PASS: Coverage ${COVERAGE}%\"\n```\n\nThe `#!/usr/bin/env bash` shebang runs the entire recipe as a single shell script (variables persist across lines).\n\n### Umbrella Recipes with Module Dependencies\n\n```just\n# Root justfile calling into modules\ncheck: (go::check) (web::check)\n    @echo \"All checks passed\"\n```\n\n### Optional Modules\n\nFor packages that may not exist in all environments:\n\n```just\nmod? optional-package \"packages/optional\"\n```\n\n### Documentation Comments\n\nEvery recipe should have a doc comment (shown in `just --list`):\n\n```just\n# Run unit tests with race detection\ntest:\n    go test -race ./...\n```\n\n**Module doc comments** - Comments above `mod` statements also appear in `just --list`:\n\n```just\n# Go API (GraphQL + Watermill)\nmod api \"packages/api\"\n\n# Next.js frontend\nmod web \"packages/web\"\n\n# PostgreSQL database\nmod db \"packages/db\"\n```\n\nOutput of `just --list`:\n```\napi ...      # Go API (GraphQL + Watermill)\ndb ...       # PostgreSQL database\nweb ...      # Next.js frontend\n```\n\nAlways add doc comments to module imports for discoverability.\n\n## Module System Details\n\n### Syntax\n\n```just\nmod <name> \"<path>\"              # Required module\nmod? <name> \"<path>\"             # Optional module (no error if missing)\nmod <name>                       # Module at ./<name>/justfile\n```\n\n### Working Directory\n\nRecipes in modules run with their working directory set to the module's directory, not the root. This is the desired behavior for package-local commands.\n\n### Listing Module Recipes\n\n```bash\njust --list           # Root recipes + module names\njust --list go        # Recipes in 'go' module\njust --list-submodules # All recipes including submodules\n```\n\n**Important**: Use `just --list <module>` not `just <module> --list`. The flag must come before the module name.\n\n### Calling Module Recipes\n\n```bash\njust go check         # From command line\n(go::check)          # As dependency in justfile\n```\n\n## Integration with Language Toolchains\n\n### Go Projects\n\n```just\n# Uses go.mod tool directive for pinned versions\nlint:\n    go tool golangci-lint run\n\n# Auto-fix linting issues (chains goimports to fix imports after modernize changes)\nfix:\n    go tool golangci-lint run --fix\n    go tool goimports -w .\n```\n\n**Why chain goimports?** The `modernize` linter may change code (e.g., `fmt.Errorf()` ‚Üí `errors.New()`) without updating imports, breaking builds. Running `goimports` after `--fix` resolves this.\n\n### TypeScript/Node Projects\n\nTwo approaches:\n\n**Option A**: Thin wrapper (recommended for consistency)\n```just\n# packages/web/justfile\ncheck:\n    npm run check\n\nlint:\n    npm run lint\n\ntest:\n    npm test\n```\n\n**Option B**: Direct delegation from root (simpler)\n```just\n# Root justfile - no web module\nts-web-check:\n    cd packages/web && npm run check\n```\n\n### Rust Projects\n\n```just\ncheck: fmt lint test\n    @echo \"All checks passed\"\n\nfmt:\n    cargo fmt --all\n\nlint:\n    cargo clippy -- -D warnings\n\ntest:\n    cargo test\n```\n\n## Mise Integration\n\nWhen a project uses [mise](https://mise.jdx.dev) for tool version management, just recipes should use `mise exec` to ensure pinned versions are used regardless of developer shell setup.\n\n**See the `mise` skill** for full mise setup including shell config, direnv integration, and project `.mise.toml` patterns.\n\n### Shell Override (Recommended)\n\n```just\n# All recipes automatically use mise-pinned tools\nset shell := [\"mise\", \"exec\", \"--\", \"bash\", \"-c\"]\n\nbuild:\n    npm run build\n\ntest:\n    go test ./...\n```\n\n### Graceful Degradation\n\nFor repos where mise is optional:\n\n```just\nset shell := [\"bash\", \"-c\"]\n\n# Use mise if available, otherwise fall back to PATH\n_exec cmd:\n    #!/usr/bin/env bash\n    if command -v mise &>/dev/null; then\n        mise exec -- {{cmd}}\n    else\n        {{cmd}}\n    fi\n\nbuild: (_exec \"npm run build\")\ntest: (_exec \"go test ./...\")\nlint: (_exec \"golangci-lint run\")\n```\n\nNote: No `.mise.toml` check needed - mise traverses parent directories automatically.\n\n### Why This Matters\n\n| Developer Setup | Without mise exec | With mise exec |\n|-----------------|-------------------|----------------|\n| Has direnv + mise | Correct versions | Correct versions |\n| Has mise activate | Correct versions | Correct versions |\n| Fresh clone, no setup | System tools (wrong version) | Pinned versions |\n| CI/CD | Needs activation step | Just works |\n\n**Recommendation**: Use the shell override for team repos. Use graceful degradation for open source where mise adoption varies.\n\n### Caveats\n\n**Failure mode**: Shell override + mise not installed = cryptic \"could not find shell\" error. Use graceful degradation for open source.\n\n**First clone**: Contributors must run `mise trust` to allow the repo's config:\n\n```just\nsetup:\n    mise trust\n    mise install\n    @echo \"Toolchain ready\"\n```\n\n---\n\n## Security Auditing\n\nAll language templates include consistent audit recipes:\n\n| Recipe | Purpose |\n|--------|---------|\n| `audit` | Check for vulnerabilities (informational) |\n| `audit-fix` | Auto-fix where possible (npm only) |\n| `audit-ci` | CI-friendly (strict, production deps only) |\n\n**Language tools:**\n\n| Language | Tool | Installation |\n|----------|------|--------------|\n| Node/npm | `npm audit` | Built-in |\n| Go | `govulncheck` | `go get -tool golang.org/x/vuln/cmd/govulncheck@latest` |\n| Rust | `cargo audit` | `cargo install cargo-audit` |\n\n---\n\n## Reference Templates\n\nLoad the appropriate template based on project structure:\n\n| Template | Use Case |\n|----------|----------|\n| `references/simple-repo.just` | Single-package repositories |\n| `references/monorepo-root.just` | Monorepo root router |\n| `references/package-go.just` | Go package in monorepo |\n| `references/package-ts.just` | TypeScript package in monorepo |\n| `references/package-rust.just` | Rust package in monorepo |\n\n**Note**: Reference files use `.just` extension for organization. Actual project files must be named `justfile` (no extension) or `.justfile`.\n",
        "plugins/language-pro/skills/python-pro/SKILL.md": "---\nname: python-pro\ndescription: Expert Python developer specializing in modern tooling (uv, ruff, pyright), type safety, and clean module design. This skill should be used PROACTIVELY when working on any Python code - implementing features, designing APIs, debugging issues, or reviewing code quality. Use unless a more specific subagent role applies.\n---\n\n# Python Pro\n\nSenior-level Python expertise for production projects. Focuses on modern tooling, strict type checking, and Pythonic idioms.\n\n## When Invoked\n\n1. Review `pyproject.toml` for project conventions and tooling config\n2. For build system setup, invoke the **just-pro** skill\n3. Apply Python idioms and established project patterns\n\n## Core Standards\n\n**Non-Negotiable:**\n- All public functions/classes have docstrings\n- All functions have type annotations (params + return)\n- ruff passes with project configuration\n- pyright passes in strict mode\n- Meaningful tests with pytest\n\n**Foundational Principles:**\n- **Single Responsibility**: One module = one purpose, one function = one job\n- **No God Classes**: Split large classes; if it has 10+ methods, decompose\n- **Dependency Injection**: Pass dependencies, don't create them internally\n- **Explicit over Implicit**: Clear is better than clever\n\n---\n\n## Project Setup (Python 3.12+)\n\n### Version Management with mise\n\n[mise](https://mise.jdx.dev) manages language runtimes per-project. Ensures all contributors use the same Python version.\n\n```bash\n# Install mise (once)\ncurl https://mise.run | sh\n\n# In project root\nmise use python@3.12\n\n# Creates .mise.toml - commit it\n# Team members just run: mise install\n```\n\n### New Project Quick Start with uv\n\n[uv](https://docs.astral.sh/uv/) is the modern Python package manager (10-100x faster than pip).\n\n```bash\n# Initialize project\nuv init project-name\ncd project-name\n\n# Or in existing directory\nuv init\n\n# Add dependencies\nuv add httpx pydantic\n\n# Add dev dependencies\nuv add --dev pytest pytest-cov pytest-asyncio ruff pyright\n\n# Set up src layout (required for imports to work)\nmkdir -p src/projectname tests\nmv *.py src/projectname/ 2>/dev/null || true\ntouch src/projectname/__init__.py tests/__init__.py tests/conftest.py\n\n# Copy configs from this skill's references/ directory:\n#   references/gitignore               -> .gitignore\n#   references/pyproject-template.toml -> use as pyproject.toml base\n#   references/pyrightconfig.json      -> pyrightconfig.json\n# For build system, invoke just-pro skill\n\n# Verify\njust check   # Or: uv run ruff check . && uv run pyright\n```\n\n### pyproject.toml Requirements\n\n**Critical:** uv init creates a minimal pyproject.toml. For src layout to work, you MUST add:\n\n```toml\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/projectname\"]\n\n[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\ntestpaths = [\"tests\"]\n```\n\nWithout `[build-system]` and `[tool.hatch.build.targets.wheel]`, tests cannot import your package.\n\n### Developer Onboarding\n\n```bash\ngit clone <repo> && cd <repo>\njust setup         # Runs mise trust/install + uv sync\njust check         # Verify everything works\n```\n\nOr manually:\n```bash\nmise trust && mise install  # Get pinned Python version\nuv sync                     # Install dependencies from lockfile\n```\n\n**Why uv?** Lockfile-based reproducibility, automatic venv management, 10-100x faster than pip.\n\n---\n\n## Build System\n\n**Invoke the `just-pro` skill** for build system setup. It covers:\n- Simple repos vs monorepos\n- Hierarchical justfile modules\n- Python-specific templates\n\n**Why just?** Consistent toolchain frontend between agents and humans. Instead of remembering `uv run ruff check --fix .`, use `just fix`.\n\n---\n\n## Quality Assurance\n\n**Auto-Fix First** - Always try auto-fix before manual fixes:\n\n```bash\njust fix             # Or: uv run ruff check --fix . && uv run ruff format .\n```\n\n**Verification:**\n```bash\njust check           # Or: uv run ruff check . && uv run pyright && uv run pytest\n```\n\n---\n\n## Handling Strict Pyright\n\n### Untyped Libraries\n\nSome libraries lack type stubs. In pyright strict mode, use `Any`:\n\n```python\nfrom typing import Any\n\nimport structlog  # No complete type stubs\n\n# Annotate as Any to silence pyright\nlogger: Any = structlog.get_logger()\n```\n\n**When to use `Any`:**\n- Library has no stubs and you can't create them\n- Return types are dynamic/unpredictable\n- Interfacing with weakly-typed external systems\n\n**Avoid `# type: ignore`** - it silences all errors. Explicit `Any` is clearer.\n\n### TYPE_CHECKING Imports\n\nRuff's TCH rules flag imports used only for type hints. Move them to a `TYPE_CHECKING` block:\n\n```python\nfrom __future__ import annotations  # Required for forward refs\n\nfrom typing import TYPE_CHECKING\n\nfrom mypackage.service import run_service  # Runtime import\n\nif TYPE_CHECKING:\n    from mypackage.models import User  # Type-only import\n\ndef process(user: User) -> None:  # Works due to __future__ annotations\n    run_service(user)\n```\n\n**Rules:**\n- Imports used at runtime stay at top level\n- Imports used only in type hints go in `TYPE_CHECKING`\n- `from __future__ import annotations` enables string-based forward refs\n- This also reduces import cycles\n\n### Optional Field Access\n\nWhen a field might be `None`, assert before accessing:\n\n```python\n# BAD - pyright error: \"x\" could be None\nresult.error_message.lower()\n\n# GOOD - narrow the type first\nassert result.error_message is not None\nresult.error_message.lower()\n\n# OR use conditional\nif result.error_message:\n    result.error_message.lower()\n```\n\n---\n\n## Quick Reference\n\n### Type Annotations\n\n```python\nfrom typing import TypeVar, Protocol\nfrom collections.abc import Callable, Iterator, Sequence\n\n# Basic annotations\ndef process(items: list[str], timeout: float = 30.0) -> dict[str, int]:\n    ...\n\n# Generic functions\nT = TypeVar(\"T\")\n\ndef first(items: Sequence[T]) -> T | None:\n    return items[0] if items else None\n\n# Protocols for structural typing (duck typing with types)\nclass Readable(Protocol):\n    def read(self, n: int = -1) -> bytes: ...\n\ndef load_data(source: Readable) -> bytes:\n    return source.read()\n```\n\n### Error Handling\n\n```python\n# Custom exceptions with context\nclass ValidationError(Exception):\n    def __init__(self, field: str, message: str) -> None:\n        self.field = field\n        self.message = message\n        super().__init__(f\"{field}: {message}\")\n\n# Explicit error handling\ndef parse_config(path: str) -> Config:\n    try:\n        with open(path) as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        raise ConfigError(f\"Config file not found: {path}\") from None\n    except json.JSONDecodeError as e:\n        raise ConfigError(f\"Invalid JSON in {path}: {e}\") from e\n    return Config.from_dict(data)\n\n# Use Result pattern for expected failures (optional)\nfrom dataclasses import dataclass\n\n@dataclass\nclass Ok[T]:\n    value: T\n\n@dataclass\nclass Err[E]:\n    error: E\n\ntype Result[T, E] = Ok[T] | Err[E]\n```\n\n### Data Classes and Pydantic\n\n```python\nfrom dataclasses import dataclass, field\nfrom pydantic import BaseModel, Field\n\n# Simple data containers\n@dataclass(frozen=True, slots=True)\nclass Point:\n    x: float\n    y: float\n\n# Pydantic for validation and serialization\nclass UserCreate(BaseModel):\n    name: str = Field(min_length=1, max_length=100)\n    email: str\n    age: int = Field(ge=0, le=150)\n\n    model_config = {\"strict\": True}\n```\n\n### Async Patterns\n\n```python\nimport asyncio\nfrom collections.abc import AsyncIterator\n\n# Async context managers\nasync def fetch_with_timeout(url: str, timeout: float = 10.0) -> bytes:\n    async with asyncio.timeout(timeout):\n        async with httpx.AsyncClient() as client:\n            response = await client.get(url)\n            response.raise_for_status()\n            return response.content\n\n# Async generators\nasync def paginate(client: Client, url: str) -> AsyncIterator[Item]:\n    while url:\n        response = await client.get(url)\n        for item in response.items:\n            yield item\n        url = response.next_url\n\n# Gather with error handling\nasync def fetch_all(urls: list[str]) -> list[bytes | Exception]:\n    tasks = [fetch_with_timeout(url) for url in urls]\n    return await asyncio.gather(*tasks, return_exceptions=True)\n```\n\n### Context Managers\n\n```python\nfrom contextlib import contextmanager, asynccontextmanager\nfrom collections.abc import Generator, AsyncGenerator\n\n@contextmanager\ndef temporary_config(overrides: dict[str, str]) -> Generator[None, None, None]:\n    original = config.copy()\n    config.update(overrides)\n    try:\n        yield\n    finally:\n        config.clear()\n        config.update(original)\n\n@asynccontextmanager\nasync def database_transaction(db: Database) -> AsyncGenerator[Transaction, None]:\n    tx = await db.begin()\n    try:\n        yield tx\n        await tx.commit()\n    except Exception:\n        await tx.rollback()\n        raise\n```\n\n### Testing with pytest\n\n```python\nimport pytest\nfrom unittest.mock import Mock, patch\n\n# Parametrized tests\n@pytest.mark.parametrize(\n    \"input_val,expected\",\n    [\n        (\"hello\", \"HELLO\"),\n        (\"\", \"\"),\n        (\"123\", \"123\"),\n    ],\n    ids=[\"normal\", \"empty\", \"digits\"],\n)\ndef test_uppercase(input_val: str, expected: str) -> None:\n    assert uppercase(input_val) == expected\n\n# Fixtures\n@pytest.fixture\ndef sample_user() -> User:\n    return User(name=\"Test\", email=\"test@example.com\")\n\n@pytest.fixture\ndef mock_client() -> Mock:\n    client = Mock(spec=APIClient)\n    client.get.return_value = {\"status\": \"ok\"}\n    return client\n\n# Async tests\n@pytest.mark.asyncio\nasync def test_fetch_data(mock_client: Mock) -> None:\n    result = await fetch_data(mock_client, \"test-id\")\n    assert result.status == \"ok\"\n\n# Exception testing\ndef test_invalid_input_raises() -> None:\n    with pytest.raises(ValueError, match=\"must be positive\"):\n        process_value(-1)\n```\n\n### Structured Logging\n\n```python\nimport logging\nfrom typing import Any\n\nimport structlog\n\n# Note: structlog lacks complete type stubs. Use Any for logger type.\nlogger: Any = structlog.get_logger()\n\n# Configure structlog for console output\nstructlog.configure(\n    processors=[\n        structlog.stdlib.add_log_level,\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        structlog.dev.ConsoleRenderer(),  # Or JSONRenderer() for prod\n    ],\n    wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),\n    context_class=dict,\n    logger_factory=structlog.PrintLoggerFactory(),\n)\n\n# Structured logging with context\nlogger.info(\"request_processed\", method=\"GET\", path=\"/api/users\", duration_ms=42)\nlogger.error(\"operation_failed\", error=str(e), user_id=user_id)\n\n# Bind context for a scope\nbound_logger: Any = logger.bind(request_id=request_id, user_id=user_id)\nbound_logger.info(\"starting_operation\")\n```\n\n### Package Organization\n\n```\nproject/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ projectname/\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ       ‚îú‚îÄ‚îÄ api/              # HTTP handlers\n‚îÇ       ‚îú‚îÄ‚îÄ domain/           # Business logic\n‚îÇ       ‚îî‚îÄ‚îÄ infra/            # External integrations\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ conftest.py           # Shared fixtures\n‚îÇ   ‚îú‚îÄ‚îÄ test_api/\n‚îÇ   ‚îî‚îÄ‚îÄ test_domain/\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ pyrightconfig.json\n‚îú‚îÄ‚îÄ uv.lock\n‚îî‚îÄ‚îÄ justfile\n```\n\n**Rules:** Use `src/` layout for installable packages. One module = one purpose. Avoid `utils`, `common`, `helpers` modules.\n\n---\n\n## DX Patterns\n\n### Doctor Recipe with Version Validation\n\n```just\n# Validate toolchain versions meet requirements\ndoctor:\n    #!/usr/bin/env bash\n    set -euo pipefail\n    echo \"Checking toolchain...\"\n\n    # Validate Python version (requires 3.12+)\n    PY_VERSION=$(python --version | grep -oE '[0-9]+\\.[0-9]+')\n    if [[ \"$(printf '%s\\n' \"3.12\" \"$PY_VERSION\" | sort -V | head -1)\" != \"3.12\" ]]; then\n        echo \"FAIL: Python $PY_VERSION < 3.12 required\"\n        exit 1\n    fi\n    echo \"OK: Python $PY_VERSION\"\n\n    # Check uv is available\n    if ! command -v uv &> /dev/null; then\n        echo \"FAIL: uv not found. Install: curl -LsSf https://astral.sh/uv/install.sh | sh\"\n        exit 1\n    fi\n    echo \"OK: uv $(uv --version | head -1)\"\n\n    echo \"All checks passed\"\n```\n\n### First-Run Detection\n\n```just\n# Setup with first-run detection\nsetup:\n    #!/usr/bin/env bash\n    if [[ -f .setup-complete ]]; then\n        echo \"Already set up. Run 'just setup-force' to reinstall.\"\n        exit 0\n    fi\n    mise trust && mise install\n    uv sync\n    touch .setup-complete\n    echo \"Setup complete\"\n\nsetup-force:\n    rm -f .setup-complete\n    @just setup\n```\n\n---\n\n## Anti-Patterns\n\n- Bare `except:` clauses (always specify exception type)\n- Mutable default arguments (`def f(items=[])` - use `None` and create inside)\n- Star imports (`from module import *`)\n- God classes with 10+ methods\n- Missing type annotations on public APIs\n- `Any` when specific types work\n- Nested functions when a module-level function suffices\n- `print()` for logging (use logging/structlog)\n- Ignoring return values of functions with side effects\n\n---\n\n## AI Agent Guidelines\n\n**Before writing code:**\n1. Read `pyproject.toml` for project structure and dependencies\n2. Check ruff/pyright config for project-specific rules\n3. Identify existing patterns in the codebase to follow\n\n**When writing code:**\n1. Add type annotations immediately - never leave functions untyped\n2. Add docstrings to public functions/classes\n3. Use existing project abstractions over creating new ones\n4. Prefer explicit types; use generics only when pattern repeats 3+ times\n\n**Before committing:**\n1. Run `just check` (standard for projects using just)\n2. Fallback: `uv run ruff check --fix . && uv run ruff format .`\n3. Fallback: `uv run pyright && uv run pytest`\n",
        "plugins/language-pro/skills/rescript-pro/SKILL.md": "---\nname: rescript-pro\ndescription: Expert ReScript developer specializing in type-safe functional programming, JavaScript interop, and React integration. This skill should be used PROACTIVELY when working on any ReScript code - implementing features, designing APIs, debugging type errors, or reviewing code quality. Use unless a more specific subagent role applies.\n---\n\n# ReScript Pro\n\nSenior-level ReScript expertise for production projects. Focuses on type safety, exhaustive pattern matching, and clean JavaScript interop.\n\n## When Invoked\n\n1. Review `rescript.json` for project conventions and build settings\n2. For build system setup, invoke the **just-pro** skill\n3. Apply ReScript idioms and established project patterns\n\n## Core Standards\n\n**Non-Negotiable:**\n- All pattern matches exhaustive - **NO wildcard `_` on variants you control**\n- **NO `%raw` in production code** - use typed external bindings\n- **NO `Obj.magic` or type coercion** - fix the types properly\n- All external bindings have explicit types\n- `rescript build` passes with zero warnings\n- `rescript format` enforced on all code\n\n**Foundational Principles:**\n- **Single Responsibility**: One module = one purpose, one function = one job\n- **No God Modules**: Split large modules; if it has 10+ top-level functions, decompose\n- **Dependency Injection**: Pass dependencies as parameters, don't rely on global state\n- **Pipe-First Style**: Use `->` for data transformations, reads left-to-right\n- **Make Illegal States Unrepresentable**: Model domain with variants and records\n\n---\n\n## Project Setup (ReScript 11+)\n\n### Version Management with mise\n\n[mise](https://mise.jdx.dev) manages language runtimes per-project. Ensures all contributors use the same Node version for ReScript builds.\n\n```bash\n# Install mise (once)\ncurl https://mise.run | sh\n\n# In project root\nmise use node@22\n\n# Creates .mise.toml - commit it\n# Team members just run: mise install\n```\n\n### New Project Quick Start\n\n```bash\n# Initialize\nnpm init -y\nnpm install rescript @rescript/core @rescript/react\nnpm install -D eslint @eslint/js eslint-plugin-react-hooks eslint-plugin-react-compiler\n\n# Create rescript.json (see references/rescript-template.json)\n# Copy configs from this skill's references/ directory:\n#   references/gitignore              -> .gitignore\n#   references/rescript-template.json -> use as rescript.json base\n#   references/eslint.config.js       -> eslint.config.js\n\n# Create source structure\nmkdir -p src\necho 'Console.log(\"Hello ReScript\")' > src/Main.res\n\n# Add scripts to package.json:\nnpm pkg set scripts.build=\"rescript\"\nnpm pkg set scripts.clean=\"rescript clean\"\nnpm pkg set scripts.dev=\"rescript -w\"\nnpm pkg set scripts.format=\"rescript format -all\"\n\n# For build system, invoke just-pro skill\n\n# Verify\nnpm run build\n```\n\n### Developer Onboarding\n\n```bash\ngit clone <repo> && cd <repo>\njust setup         # Runs mise trust/install + npm ci\njust check         # Verify everything works\n```\n\nOr manually:\n```bash\nmise trust && mise install  # Get pinned Node version\nnpm ci                      # Get dependencies\nnpm run build              # Build ReScript\n```\n\n**Why ReScript?** Sound type system with no `any` escape hatch. Compiles to readable JavaScript. Exhaustive pattern matching catches bugs at compile time.\n\n---\n\n## Build System\n\n**Invoke the `just-pro` skill** for build system setup. It covers:\n- Simple repos vs monorepos\n- Hierarchical justfile modules\n- Language-specific templates\n\n**Why just?** Consistent toolchain frontend between agents and humans. Instead of remembering build commands, use `just build`.\n\n---\n\n## Quality Assurance\n\n**Auto-Fix First** - Always try auto-fix before manual fixes:\n\n```bash\njust fix             # Or: npx rescript format -all && npx eslint src/ --fix\n```\n\n**Verification:**\n```bash\njust check           # Or: npx rescript build && npx eslint src/ && npm test\n```\n\n**Note:** ReScript compiler warnings should be treated as errors. A clean build means zero warnings. ESLint validates React hooks and enforces complexity limits on the compiled output.\n\n---\n\n## Linting Configuration\n\nReScript uses a **two-layer quality approach**:\n1. **Compiler warnings** - Catch ReScript-specific issues (exhaustive matches, unused bindings)\n2. **ESLint on JS output** - Validate React hooks, enforce complexity limits, React Compiler compatibility\n\n### ESLint Setup for ReScript\n\nInstall dependencies:\n```bash\nnpm install -D eslint @eslint/js eslint-plugin-react-hooks eslint-plugin-react-compiler\n```\n\nCopy `references/eslint.config.js` to your project root. This config:\n- Targets only `.res.mjs` files (compiled ReScript output)\n- Validates React hooks rules\n- Enforces React Compiler compatibility\n- Applies complexity limits\n\n### Enforced Limits (via ESLint on JS output)\n\n| Limit | Value | Purpose |\n|-------|-------|---------|\n| `complexity` | 15 | Cyclomatic complexity cap |\n| `max-depth` | 4 | Avoid deeply nested code |\n| `max-lines-per-function` | 80 | Single responsibility |\n| `max-lines` | 500 | Prevent god modules |\n| `max-params` | 5 | Use records for many params |\n| `max-nested-callbacks` | 3 | Flatten callback chains |\n\n### React Hooks Validation\n\nESLint validates React hooks on the compiled output:\n- `react-hooks/rules-of-hooks` - Enforces hooks are called correctly\n- `react-hooks/exhaustive-deps` - Warns about missing effect dependencies\n\n### React Compiler Integration\n\nThe `eslint-plugin-react-compiler` validates that components are compatible with React Compiler (automatic memoization).\n\nTo opt a component into React Compiler memoization:\n\n```rescript\n@react.component\nlet make =\n@directive(\"'use memo'\")\n(~count) => {\n  <div>{React.int(count)}</div>\n}\n```\n\nThe `@directive` attribute emits a directive string at the start of the function in the JS output.\n\n### Warnings Configuration\n\nThe `rescript.json` warnings config:\n- `+a`: Enable ALL warnings\n- `-48`: Disable \"implicit elimination of optional arguments\" (noisy)\n- `-30`: Disable \"duplicate names in mutually recursive types\" (rare edge case)\n- `error: +a-3-44-102`: Make all warnings errors EXCEPT:\n  - `-3`: Deprecated feature (allow during migration)\n  - `-44`: Open statement shadows identifier (common with React)\n  - `-102`: Polymorphic comparison (sometimes necessary)\n\n---\n\n## Quick Reference\n\n### Type System\n\n| Pattern | Use |\n|---------|-----|\n| Variants | Model states, tagged unions, enums |\n| Records | Structured data with named fields |\n| `option<'a>` | Nullable values (Some/None) |\n| `result<'a, 'e>` | Operations that can fail |\n| Type parameters | Generic/polymorphic functions |\n| Labeled arguments | Named params for clarity |\n\n### Variants (Algebraic Data Types)\n\n```rescript\n// Simple enum\ntype color = Red | Green | Blue\n\n// With payloads\ntype shape =\n  | Circle({radius: float})\n  | Rectangle({width: float, height: float})\n  | Triangle({base: float, height: float})\n\n// Pattern matching - MUST be exhaustive\nlet area = shape =>\n  switch shape {\n  | Circle({radius}) => Js.Math._PI *. radius *. radius\n  | Rectangle({width, height}) => width *. height\n  | Triangle({base, height}) => 0.5 *. base *. height\n  // NO: | _ => 0.0  // Wildcard hides future variants!\n  }\n```\n\n### Records\n\n```rescript\n// Type declaration\ntype user = {\n  id: string,\n  name: string,\n  email: string,\n  age: int,\n}\n\n// Creation\nlet user = {\n  id: \"123\",\n  name: \"Alice\",\n  email: \"alice@example.com\",\n  age: 30,\n}\n\n// Update (immutable by default)\nlet updatedUser = {...user, age: 31}\n\n// Mutable fields when needed\ntype counter = {\n  mutable count: int,\n}\n```\n\n### Option and Result\n\n```rescript\n// Option for nullable values\nlet findUser = (users, id): option<user> =>\n  users->Array.find(u => u.id == id)\n\n// Handle option explicitly\nlet greeting = switch findUser(users, \"123\") {\n| Some(user) => `Hello, ${user.name}!`\n| None => \"User not found\"\n}\n\n// Result for operations that can fail\ntype fetchError = NetworkError | NotFound | ParseError(string)\n\nlet fetchUser = async (id): result<user, fetchError> => {\n  try {\n    let response = await fetch(`/users/${id}`)\n    if response->Response.ok {\n      let data = await response->Response.json\n      Ok(parseUser(data))\n    } else {\n      Error(NotFound)\n    }\n  } catch {\n  | _ => Error(NetworkError)\n  }\n}\n\n// Chain results with Result module\nlet processUser = (id) =>\n  id\n  ->fetchUser\n  ->Result.map(user => user.name->String.toUpperCase)\n  ->Result.mapError(err => `Failed: ${errorToString(err)}`)\n```\n\n### Pipe Operator\n\n```rescript\n// Pipe-first style: data flows left to right\nlet result =\n  users\n  ->Array.filter(u => u.age >= 18)\n  ->Array.map(u => u.name)\n  ->Array.sort(String.compare)\n  ->Array.joinWith(\", \")\n\n// Underscore placeholder for non-first position\nlet contains = str->String.includes(\"test\", _)\n```\n\n### Labeled Arguments\n\n```rescript\n// Use labeled args for functions with multiple params of same type\nlet createRect = (~width: float, ~height: float) => {\n  Rectangle({width, height})\n}\n\n// Optional with default\nlet greet = (~greeting=\"Hello\", ~name) => {\n  `${greeting}, ${name}!`\n}\n\n// Call with labels\nlet rect = createRect(~width=10.0, ~height=5.0)\nlet msg = greet(~name=\"Alice\")  // Uses default greeting\n```\n\n### Modules\n\n```rescript\n// Module definition\nmodule User = {\n  type t = {\n    id: string,\n    name: string,\n  }\n\n  let make = (~id, ~name) => {id, name}\n\n  let toString = (user: t) => `${user.name} (${user.id})`\n}\n\n// Module usage\nlet user = User.make(~id=\"123\", ~name=\"Alice\")\nConsole.log(User.toString(user))\n\n// Open for local scope\n{\n  open User\n  let u = make(~id=\"456\", ~name=\"Bob\")\n  Console.log(toString(u))\n}\n\n// Module signatures (interfaces)\nmodule type Printable = {\n  type t\n  let toString: t => string\n}\n```\n\n---\n\n## JavaScript Interop\n\n### External Bindings\n\n```rescript\n// Global value\n@val external document: Dom.document = \"document\"\n\n// Global function\n@val external parseInt: string => int = \"parseInt\"\n\n// Module import\n@module(\"lodash\") external chunk: (array<'a>, int) => array<array<'a>> = \"chunk\"\n\n// Default export\n@module(\"./config\") external config: {..} = \"default\"\n\n// Method call on object\n@send external focus: (Dom.element) => unit = \"focus\"\n@send external getAttribute: (Dom.element, string) => Nullable.t<string> = \"getAttribute\"\n\n// Property access\n@get external length: array<'a> => int = \"length\"\n@set external setTitle: (Dom.document, string) => unit = \"title\"\n\n// Constructor\n@new external makeDate: unit => Js.Date.t = \"Date\"\n@new external makeDateFromString: string => Js.Date.t = \"Date\"\n```\n\n### Common FFI Patterns\n\n```rescript\n// Nullable values from JS\n@module(\"./api\") external getUser: string => Nullable.t<user> = \"getUser\"\n\nlet user = getUser(\"123\")\nswitch user->Nullable.toOption {\n| Some(u) => Console.log(u.name)\n| None => Console.log(\"Not found\")\n}\n\n// Variadic functions\n@module(\"path\") @variadic\nexternal join: array<string> => string = \"join\"\n\nlet fullPath = join([\"users\", \"alice\", \"documents\"])\n\n// Object with optional fields\ntype options = {\n  timeout?: int,\n  retries?: int,\n}\n\n@module(\"./fetch\") external request: (string, options) => promise<response> = \"request\"\n\n// Polymorphic variants for JS string enums\n@module(\"./api\")\nexternal setMode: (@unwrap [#development | #production | #test]) => unit = \"setMode\"\n\nsetMode(#production)\n```\n\n### Promises and Async/Await\n\n```rescript\n// Async function\nlet fetchData = async (url: string): result<data, error> => {\n  try {\n    let response = await fetch(url)\n    let json = await response->Response.json\n    Ok(parseData(json))\n  } catch {\n  | Exn.Error(e) => Error(NetworkError(Exn.message(e)))\n  }\n}\n\n// Sequential operations\nlet fetchUserAndPosts = async (userId) => {\n  let user = await fetchUser(userId)\n  let posts = await fetchPosts(userId)\n  {user, posts}\n}\n\n// Parallel operations\nlet fetchAll = async (ids) => {\n  let promises = ids->Array.map(fetchUser)\n  await Promise.all(promises)\n}\n```\n\n---\n\n## React Integration\n\n### Basic Component\n\n```rescript\n@react.component\nlet make = (~name: string, ~age: int) => {\n  <div>\n    <h1>{React.string(`Hello, ${name}!`)}</h1>\n    <p>{React.string(`Age: ${Int.toString(age)}`)}</p>\n  </div>\n}\n```\n\n### Hooks\n\n```rescript\n@react.component\nlet make = () => {\n  // State\n  let (count, setCount) = React.useState(_ => 0)\n\n  // Effect\n  React.useEffect0(() => {\n    Console.log(\"Component mounted\")\n    Some(() => Console.log(\"Component unmounted\"))\n  })\n\n  // Effect with deps\n  React.useEffect1(() => {\n    Console.log(`Count changed to ${Int.toString(count)}`)\n    None\n  }, [count])\n\n  // Reducer for complex state\n  let (state, dispatch) = React.useReducer((state, action) =>\n    switch action {\n    | Increment => {...state, count: state.count + 1}\n    | Decrement => {...state, count: state.count - 1}\n    | Reset => {count: 0}\n    }\n  , {count: 0})\n\n  // Ref\n  let inputRef = React.useRef(Nullable.null)\n\n  <div>\n    <p>{React.string(Int.toString(count))}</p>\n    <button onClick={_ => setCount(prev => prev + 1)}>\n      {React.string(\"+\")}\n    </button>\n  </div>\n}\n```\n\n### Props with Variants\n\n```rescript\ntype buttonVariant = Primary | Secondary | Danger\n\n@react.component\nlet make = (~variant: buttonVariant, ~children: React.element) => {\n  let className = switch variant {\n  | Primary => \"btn-primary\"\n  | Secondary => \"btn-secondary\"\n  | Danger => \"btn-danger\"\n  }\n\n  <button className>\n    {children}\n  </button>\n}\n```\n\n### Children and Optional Props\n\n```rescript\n@react.component\nlet make = (\n  ~title: string,\n  ~subtitle: option<string>=?,\n  ~children: React.element,\n) => {\n  <div>\n    <h1>{React.string(title)}</h1>\n    {switch subtitle {\n    | Some(s) => <h2>{React.string(s)}</h2>\n    | None => React.null\n    }}\n    {children}\n  </div>\n}\n\n// Usage\n<Card title=\"Hello\" subtitle=\"World\">\n  <p>{React.string(\"Content\")}</p>\n</Card>\n```\n\n---\n\n## Project Organization\n\n```\nproject/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ Main.res              # Entry point\n‚îÇ   ‚îú‚îÄ‚îÄ Types.res             # Shared type definitions\n‚îÇ   ‚îú‚îÄ‚îÄ Utils.res             # Utility functions\n‚îÇ   ‚îú‚îÄ‚îÄ Components/           # React components\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Button.res\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Card.res\n‚îÇ   ‚îî‚îÄ‚îÄ Bindings/             # JS interop\n‚îÇ       ‚îî‚îÄ‚îÄ LocalStorage.res\n‚îú‚îÄ‚îÄ rescript.json\n‚îú‚îÄ‚îÄ package.json\n‚îî‚îÄ‚îÄ justfile\n```\n\n**Rules:** One module = one purpose. Keep bindings in separate files. Use namespaced modules for libraries.\n\n---\n\n## Anti-Patterns\n\n- `%raw(\"...\")` in production code (use typed bindings)\n- `Obj.magic` for type coercion (fix the types instead)\n- Wildcard `_` in switch on your own variants (add explicit cases)\n- Ignoring warnings (treat them as errors)\n- Mutable state everywhere (use immutable by default)\n- Deep nesting instead of early returns\n- Giant modules with 10+ functions (split by responsibility)\n- Stringly-typed APIs (use variants for known values)\n- Using `@bs.as` instead of proper binding design\n- Ignoring `option` return types (always handle None)\n\n---\n\n## AI Agent Guidelines\n\n**Before writing code:**\n1. Read `rescript.json` for build configuration and compiler options\n2. Check for existing type definitions and patterns in the codebase\n3. Identify bindings patterns for JavaScript interop\n\n**When writing code:**\n1. Start with type definitions - model the domain with variants and records\n2. Use exhaustive pattern matching - no wildcards on your own types\n3. Handle all `option` and `result` cases explicitly\n4. Create typed bindings for JS interop - avoid `%raw`\n\n**Before committing:**\n1. Run `just check` (standard for projects using just)\n2. Fallback: `npx rescript build && npx eslint src/` (must have zero warnings)\n3. Fallback: `npx rescript format -all` for consistent formatting\n\n---\n\n## References\n\n- `references/rescript-template.json` - Strict rescript.json configuration with comprehensive warnings\n- `references/eslint.config.js` - ESLint config for linting compiled JS output\n- `references/patterns.md` - Additional ReScript patterns\n- `references/gitignore` - ReScript-specific gitignore\n",
        "plugins/language-pro/skills/rescript-pro/references/patterns.md": "# ReScript Patterns Reference\n\n## Variant Patterns\n\n### State Machines\n\n```rescript\ntype connectionState =\n  | Disconnected\n  | Connecting({attempt: int, startedAt: float})\n  | Connected({sessionId: string})\n  | Reconnecting({lastSession: string, attempt: int})\n  | Error({message: string})\n\ntype connectionAction =\n  | Connect\n  | Disconnect\n  | ConnectionSuccess(string)\n  | ConnectionFailed(string)\n  | Retry\n\nlet transition = (state: connectionState, action: connectionAction): connectionState =>\n  switch (state, action) {\n  | (Disconnected, Connect) =>\n    Connecting({attempt: 1, startedAt: Js.Date.now()})\n  | (Connecting(_), ConnectionSuccess(sessionId)) =>\n    Connected({sessionId: sessionId})\n  | (Connecting({attempt}), ConnectionFailed(msg)) if attempt < 3 =>\n    Reconnecting({lastSession: \"\", attempt: attempt + 1})\n  | (Connecting(_), ConnectionFailed(msg)) =>\n    Error({message: msg})\n  | (Connected(_), Disconnect) =>\n    Disconnected\n  | (Reconnecting({attempt}), Retry) if attempt < 3 =>\n    Connecting({attempt: attempt + 1, startedAt: Js.Date.now()})\n  | (Reconnecting(_), Retry) =>\n    Error({message: \"Max retries exceeded\"})\n  | (Error(_), Connect) =>\n    Connecting({attempt: 1, startedAt: Js.Date.now()})\n  | _ => state  // Ignore invalid transitions\n  }\n```\n\n### Result Chaining\n\n```rescript\ntype parseError = InvalidJson | MissingField(string) | InvalidType(string)\n\nlet parseUser = (json: Js.Json.t): result<user, parseError> => {\n  open Result\n\n  json\n  ->Js.Json.decodeObject\n  ->Option.toResult(InvalidJson)\n  ->flatMap(obj => {\n    let id = obj->Dict.get(\"id\")->Option.toResult(MissingField(\"id\"))\n    let name = obj->Dict.get(\"name\")->Option.toResult(MissingField(\"name\"))\n    let email = obj->Dict.get(\"email\")->Option.toResult(MissingField(\"email\"))\n\n    switch (id, name, email) {\n    | (Ok(id), Ok(name), Ok(email)) =>\n      switch (\n        id->Js.Json.decodeString,\n        name->Js.Json.decodeString,\n        email->Js.Json.decodeString,\n      ) {\n      | (Some(id), Some(name), Some(email)) =>\n        Ok({id, name, email})\n      | _ => Error(InvalidType(\"Expected strings\"))\n      }\n    | (Error(e), _, _) | (_, Error(e), _) | (_, _, Error(e)) => Error(e)\n    }\n  })\n}\n```\n\n### Polymorphic Variants for Interop\n\n```rescript\n// Use polymorphic variants for JS string enums\n@module(\"./api\")\nexternal setLogLevel: (@unwrap [#debug | #info | #warn | #error]) => unit = \"setLogLevel\"\n\n@module(\"./api\")\nexternal setTheme: (@unwrap [#light | #dark | #system]) => unit = \"setTheme\"\n\n// Usage - type-safe string values\nsetLogLevel(#warn)\nsetTheme(#dark)\n\n// For more complex cases with payloads\ntype apiResponse<'a> = [\n  | #Success('a)\n  | #Error(string)\n  | #Loading\n  | #NotFound\n]\n\nlet handleResponse = (response: apiResponse<user>) =>\n  switch response {\n  | #Success(user) => Console.log(`Found: ${user.name}`)\n  | #Error(msg) => Console.error(msg)\n  | #Loading => Console.log(\"Loading...\")\n  | #NotFound => Console.log(\"User not found\")\n  }\n```\n\n## Module Patterns\n\n### Module Functors\n\n```rescript\n// Module type for comparable items\nmodule type Comparable = {\n  type t\n  let compare: (t, t) => int\n}\n\n// Generic sorted set module\nmodule MakeSortedSet = (Item: Comparable) => {\n  type t = list<Item.t>\n\n  let empty: t = list{}\n\n  let add = (set: t, item: Item.t): t => {\n    let rec insert = (lst, item) =>\n      switch lst {\n      | list{} => list{item}\n      | list{head, ...tail} =>\n        switch Item.compare(item, head) {\n        | n if n < 0 => list{item, head, ...tail}\n        | 0 => lst  // Already exists\n        | _ => list{head, ...insert(tail, item)}\n        }\n      }\n    insert(set, item)\n  }\n\n  let toArray = (set: t): array<Item.t> => List.toArray(set)\n}\n\n// Usage\nmodule IntSet = MakeSortedSet({\n  type t = int\n  let compare = (a, b) => a - b\n})\n\nlet set = IntSet.empty->IntSet.add(3)->IntSet.add(1)->IntSet.add(2)\n```\n\n### First-Class Modules\n\n```rescript\nmodule type Storage = {\n  let get: string => option<string>\n  let set: (string, string) => unit\n  let remove: string => unit\n}\n\n// Pack module as value\nlet localStorage: module(Storage) = module({\n  @val @scope(\"localStorage\")\n  external getItem: string => Nullable.t<string> = \"getItem\"\n  @val @scope(\"localStorage\")\n  external setItem: (string, string) => unit = \"setItem\"\n  @val @scope(\"localStorage\")\n  external removeItem: string => unit = \"removeItem\"\n\n  let get = key => getItem(key)->Nullable.toOption\n  let set = setItem\n  let remove = removeItem\n})\n\n// Use with any storage implementation\nlet saveData = (storage: module(Storage), key: string, value: string) => {\n  module S = unpack(storage)\n  S.set(key, value)\n}\n```\n\n## FFI Patterns\n\n### Binding to Classes\n\n```rescript\n// ES6 class binding\ntype websocket\n\n@new external makeWebSocket: string => websocket = \"WebSocket\"\n@send external send: (websocket, string) => unit = \"send\"\n@send external close: websocket => unit = \"close\"\n@get external readyState: websocket => int = \"readyState\"\n@set external onmessage: (websocket, Js.Json.t => unit) => unit = \"onmessage\"\n@set external onerror: (websocket, Js.Exn.t => unit) => unit = \"onerror\"\n\n// Usage\nlet ws = makeWebSocket(\"wss://example.com/socket\")\nws->onmessage(msg => Console.log2(\"Received:\", msg))\nws->send(`{\"type\": \"ping\"}`)\n```\n\n### Binding to Object Methods\n\n```rescript\n// Object with chainable methods\ntype queryBuilder\n\n@module(\"./db\") external query: string => queryBuilder = \"query\"\n@send external where: (queryBuilder, string, 'a) => queryBuilder = \"where\"\n@send external orderBy: (queryBuilder, string) => queryBuilder = \"orderBy\"\n@send external limit: (queryBuilder, int) => queryBuilder = \"limit\"\n@send external execute: queryBuilder => promise<array<Js.Json.t>> = \"execute\"\n\n// Chainable usage\nlet getUsers = async () => {\n  query(\"users\")\n    ->where(\"active\", true)\n    ->orderBy(\"created_at\")\n    ->limit(10)\n    ->execute\n}\n```\n\n### Binding to Callbacks\n\n```rescript\n// Node-style callbacks\ntype callback<'a> = (Nullable.t<Js.Exn.t>, 'a) => unit\n\n@module(\"fs\")\nexternal readFile: (string, string, callback<string>) => unit = \"readFile\"\n\n// Promise wrapper\nlet readFileAsync = (path: string): promise<result<string, Js.Exn.t>> => {\n  Promise.make((resolve, _reject) => {\n    readFile(path, \"utf8\", (err, data) => {\n      switch err->Nullable.toOption {\n      | Some(e) => resolve(Error(e))\n      | None => resolve(Ok(data))\n      }\n    })\n  })\n}\n```\n\n### Typed Event Handlers\n\n```rescript\ntype mouseEvent = {\n  clientX: int,\n  clientY: int,\n  button: int,\n  preventDefault: unit => unit,\n}\n\ntype keyboardEvent = {\n  key: string,\n  code: string,\n  altKey: bool,\n  ctrlKey: bool,\n  shiftKey: bool,\n  preventDefault: unit => unit,\n}\n\n@val @scope(\"document\")\nexternal addMouseListener: (\n  @string [#click | #mousedown | #mouseup | #mousemove],\n  mouseEvent => unit\n) => unit = \"addEventListener\"\n\n@val @scope(\"document\")\nexternal addKeyListener: (\n  @string [#keydown | #keyup | #keypress],\n  keyboardEvent => unit\n) => unit = \"addEventListener\"\n\n// Usage\naddMouseListener(#click, event => {\n  Console.log2(\"Clicked at:\", (event.clientX, event.clientY))\n})\n\naddKeyListener(#keydown, event => {\n  if event.ctrlKey && event.key == \"s\" {\n    event.preventDefault()\n    Console.log(\"Save shortcut pressed\")\n  }\n})\n```\n\n## React Patterns\n\n### Context with Variants\n\n```rescript\ntype theme = Light | Dark\n\ntype themeContext = {\n  theme: theme,\n  toggleTheme: unit => unit,\n}\n\nlet context = React.createContext({\n  theme: Light,\n  toggleTheme: () => (),\n})\n\nmodule Provider = {\n  @react.component\n  let make = (~children: React.element) => {\n    let (theme, setTheme) = React.useState(_ => Light)\n\n    let value = {\n      theme,\n      toggleTheme: () =>\n        setTheme(prev =>\n          switch prev {\n          | Light => Dark\n          | Dark => Light\n          }\n        ),\n    }\n\n    <context.Provider value>\n      {children}\n    </context.Provider>\n  }\n}\n\n// Hook for consuming\nlet useTheme = () => React.useContext(context)\n```\n\n### Custom Hook with Cleanup\n\n```rescript\ntype windowSize = {\n  width: int,\n  height: int,\n}\n\nlet useWindowSize = (): windowSize => {\n  let (size, setSize) = React.useState(_ => {\n    width: Webapi.Dom.Window.innerWidth(Webapi.Dom.window),\n    height: Webapi.Dom.Window.innerHeight(Webapi.Dom.window),\n  })\n\n  React.useEffect0(() => {\n    let handleResize = _ => {\n      setSize(_ => {\n        width: Webapi.Dom.Window.innerWidth(Webapi.Dom.window),\n        height: Webapi.Dom.Window.innerHeight(Webapi.Dom.window),\n      })\n    }\n\n    Webapi.Dom.Window.addEventListener(Webapi.Dom.window, \"resize\", handleResize)\n\n    Some(\n      () =>\n        Webapi.Dom.Window.removeEventListener(\n          Webapi.Dom.window,\n          \"resize\",\n          handleResize,\n        ),\n    )\n  })\n\n  size\n}\n```\n\n### Render Props / Children as Function\n\n```rescript\ntype fetchState<'a> =\n  | Loading\n  | Success('a)\n  | Error(string)\n\n@react.component\nlet make = (~url: string, ~children: fetchState<Js.Json.t> => React.element) => {\n  let (state, setState) = React.useState(_ => Loading)\n\n  React.useEffect1(() => {\n    let fetchData = async () => {\n      try {\n        let response = await fetch(url)\n        let json = await response->Response.json\n        setState(_ => Success(json))\n      } catch {\n      | _ => setState(_ => Error(\"Failed to fetch\"))\n      }\n    }\n\n    fetchData()->ignore\n    None\n  }, [url])\n\n  children(state)\n}\n\n// Usage\n<Fetch url=\"/api/users\">\n  {state =>\n    switch state {\n    | Loading => <Spinner />\n    | Success(data) => <UserList data />\n    | Error(msg) => <ErrorMessage message={msg} />\n    }}\n</Fetch>\n```\n\n## Testing Patterns\n\n### Unit Testing with rescript-test\n\n```rescript\nopen Test\n\ndescribe(\"User module\", () => {\n  test(\"creates user with valid data\", () => {\n    let user = User.make(~id=\"123\", ~name=\"Alice\")\n    expect(user.name)->toBe(\"Alice\")\n  })\n\n  test(\"validates email format\", () => {\n    let result = User.validateEmail(\"invalid\")\n    expect(result)->toEqual(Error(InvalidEmail))\n  })\n\n  describe(\"authentication\", () => {\n    test(\"rejects empty password\", () => {\n      let result = User.authenticate(~email=\"test@example.com\", ~password=\"\")\n      expect(result)->toEqual(Error(EmptyPassword))\n    })\n  })\n})\n```\n\n### Property-Based Testing\n\n```rescript\nopen FastCheck\n\n// Test that reverse is involutory\ntest(\"reverse twice returns original\", () => {\n  fc.assert(\n    fc.property(fc.array(fc.integer()), arr => {\n      arr->Array.reverse->Array.reverse == arr\n    }),\n  )\n})\n\n// Test sort invariants\ntest(\"sort produces ordered array\", () => {\n  fc.assert(\n    fc.property(fc.array(fc.integer()), arr => {\n      let sorted = arr->Array.copy->Array.sort((a, b) => a - b)\n      sorted->Array.everyWithIndex((v, i) =>\n        i == 0 || sorted->Array.getUnsafe(i - 1) <= v\n      )\n    }),\n  )\n})\n```\n\n## Error Handling Patterns\n\n### Railway-Oriented Programming\n\n```rescript\nmodule Result = {\n  // Extend Result with more combinators\n  let bind = (result, fn) =>\n    switch result {\n    | Ok(v) => fn(v)\n    | Error(_) as e => e\n    }\n\n  let map2 = (r1, r2, fn) =>\n    switch (r1, r2) {\n    | (Ok(v1), Ok(v2)) => Ok(fn(v1, v2))\n    | (Error(_) as e, _) => e\n    | (_, Error(_) as e) => e\n    }\n\n  let sequence = (results: array<result<'a, 'e>>): result<array<'a>, 'e> => {\n    results->Array.reduce(Ok([]), (acc, r) =>\n      switch (acc, r) {\n      | (Ok(arr), Ok(v)) => Ok(arr->Array.concat([v]))\n      | (Error(_) as e, _) => e\n      | (_, Error(_) as e) => e\n      }\n    )\n  }\n}\n\n// Usage: compose operations that can fail\nlet processOrder = (orderId: string): result<receipt, orderError> => {\n  orderId\n  ->validateOrderId\n  ->Result.bind(fetchOrder)\n  ->Result.bind(validateInventory)\n  ->Result.bind(processPayment)\n  ->Result.bind(generateReceipt)\n}\n```\n\n### Tagged Errors\n\n```rescript\ntype apiError =\n  | NetworkError({url: string, status: int})\n  | ValidationError({field: string, message: string})\n  | AuthError({reason: string})\n  | RateLimited({retryAfter: int})\n\nlet errorToMessage = (error: apiError): string =>\n  switch error {\n  | NetworkError({url, status}) => `Network error: ${url} returned ${Int.toString(status)}`\n  | ValidationError({field, message}) => `Validation failed for ${field}: ${message}`\n  | AuthError({reason}) => `Authentication failed: ${reason}`\n  | RateLimited({retryAfter}) => `Rate limited. Retry after ${Int.toString(retryAfter)} seconds`\n  }\n\nlet errorToStatusCode = (error: apiError): int =>\n  switch error {\n  | NetworkError(_) => 502\n  | ValidationError(_) => 400\n  | AuthError(_) => 401\n  | RateLimited(_) => 429\n  }\n```\n",
        "plugins/language-pro/skills/rust-pro/SKILL.md": "---\nname: rust-pro\ndescription: Expert Rust developer specializing in ownership semantics, zero-cost abstractions, and idiomatic patterns. This skill should be used PROACTIVELY when working on any Rust code - implementing features, debugging borrow checker issues, optimizing performance, or reviewing code quality. Use unless a more specific subagent role applies.\n---\n\n# Rust Pro\n\nSenior-level Rust expertise following \"Boring Rust\" principles. Correctness over cleverness. One way to do things. Local reasoning.\n\n## When Invoked\n\n1. Review `Cargo.toml`, `clippy.toml`, and `rustfmt.toml` for project conventions\n2. For build system setup, invoke the **just-pro** skill\n3. Apply Boring Rust patterns and established project conventions\n\n## Core Standards\n\n**Non-Negotiable:**\n- All clippy warnings treated as errors\n- **NO `unwrap()` or `expect()` in production code** ‚Äî use `.context(\"...\")?`\n- **NO `unsafe` without `#[human_authored]` designation**\n- **NO panic paths** ‚Äî indexing, unreachable, todo, unimplemented all banned\n- Exhaustive match ‚Äî no wildcard `_` on enums you control\n- rustfmt enforced on all code\n- Documentation on all public APIs\n\n**Foundational Principles:**\n- **Single Responsibility**: One module = one purpose, one function = one job\n- **No God Objects**: Split large structs; if it has 10+ fields or methods, decompose\n- **Dependency Injection**: Pass dependencies, don't create them internally\n- **Clone Freely**: Prefer correctness over premature optimization; clone to satisfy borrow checker\n- **Explicit Over Clever**: If you need complex lifetimes, restructure instead\n\n---\n\n## The Three Tiers\n\n### Tier 1: Default (Strict)\n\nAll agent-generated code. Maximum guardrails.\n\n```rust\n// Complexity limits enforced:\n// - Cognitive complexity: 15 max\n// - Function lines: 50 max\n// - Arguments: 5 max\n\n// Error handling: Always with context\nlet config = load_config(path)\n    .context(\"failed to load configuration\")?;\n\n// Iteration: for loops by default (not iterator chains)\nfor item in collection {\n    process(item)?;\n}\n\n// Matching: Exhaustive, no wildcards\nmatch state {\n    State::Active => handle_active()?,\n    State::Pending => handle_pending()?,\n    State::Done => handle_done()?,\n    // NO: _ => unreachable!()\n}\n```\n\n### Tier 2: `#[hot_path]` (Relaxed)\n\nPerformance-critical code. Flagged for human review.\n\n```rust\n#[hot_path]\npub fn process_batch(records: &[Record]) -> Result<Summary, Error> {\n    // Allowed: iterators, borrowing, fewer clones\n    records.iter()\n        .filter(|r| r.is_valid())\n        .try_fold(Summary::default(), |mut acc, r| {\n            acc.add(r)?;\n            Ok(acc)\n        })\n}\n```\n\n**Relaxations:** Cognitive complexity 20, function lines 75, iterator chains allowed.\n\n### Tier 3: `#[human_authored]` (Unrestricted)\n\nAgent cannot modify, only call. For unsafe, SIMD, complex generics.\n\n```rust\n#[human_authored]\npub fn simd_normalize(vectors: &mut [f32x8]) {\n    // Agent treats as black box\n}\n```\n\n---\n\n## Project Setup (Rust 1.83+)\n\n### Version Management with mise\n\n[mise](https://mise.jdx.dev) manages language runtimes per-project. For Rust, this complements rustup by pinning the exact toolchain version.\n\n```bash\n# Install mise (once)\ncurl https://mise.run | sh\n\n# In project root\nmise use rust@1.83\n\n# Creates .mise.toml ‚Äî commit it\n# Team members just run: mise install\n```\n\nAlternatively, use `rust-toolchain.toml` (rustup-native) if you prefer not to add mise as a dependency.\n\n### New Project Quick Start\n\n```bash\n# Initialize\ncargo new project-name && cd project-name\n\n# Copy configs from this skill's references/ directory:\n#   references/gitignore        ‚Üí .gitignore\n#   references/clippy.toml      ‚Üí clippy.toml\n#   references/cargo_lints.toml ‚Üí merge into Cargo.toml [lints] section\n#   references/rustfmt.toml     ‚Üí rustfmt.toml\n\n# For build system, invoke just-pro skill\n\n# Verify\njust check   # Or: cargo clippy && cargo test\n```\n\n### Developer Onboarding\n\n```bash\ngit clone <repo> && cd <repo>\njust setup               # Runs mise trust/install + cargo build\njust check               # Verify everything works\n```\n\nOr manually:\n```bash\nmise trust && mise install  # Get pinned Rust toolchain\ncargo build                 # Get dependencies\n```\n\n**Why Boring Rust?** Agent-generated code that compiles is usually correct. Complex patterns cause agents to produce incorrect or unmaintainable code.\n\n---\n\n## Build System\n\n**Invoke the `just-pro` skill** for build system setup. It covers:\n- Simple repos vs monorepos\n- Hierarchical justfile modules\n- Rust-specific templates\n\n**Why just?** Consistent toolchain frontend between agents and humans.\n\n---\n\n## Quality Assurance\n\n**Auto-Fix First:**\n\n```bash\njust fix             # Or: cargo clippy --fix && cargo fmt\n```\n\n**Verification:**\n```bash\njust check           # Or: cargo clippy --all-targets -- -D warnings && cargo test\n```\n\nUse `--all-targets` to lint tests, examples, and benches too.\n\n---\n\n## Quick Reference\n\n### Error Handling\n\n```rust\n// Libraries: thiserror for typed errors\n#[derive(Debug, thiserror::Error)]\npub enum ConfigError {\n    #[error(\"missing field: {field}\")]\n    MissingField { field: &'static str },\n\n    #[error(\"failed to read file\")]\n    Io(#[from] std::io::Error),\n}\n\n// Applications: anyhow with context\npub fn load_config(path: &Path) -> anyhow::Result<Config> {\n    let content = fs::read_to_string(path)\n        .context(\"failed to read config file\")?;\n\n    toml::from_str(&content)\n        .context(\"failed to parse config\")\n}\n\n// Option handling: explicit, never silent\nlet user = users.get(&id)\n    .ok_or_else(|| Error::NotFound { id: id.clone() })?;\n```\n\n### State Machines\n\n```rust\npub enum ConnectionState {\n    Disconnected,\n    Connecting { attempt: u32, started: Instant },\n    Connected { session: Session },\n}\n\nimpl ConnectionState {\n    pub fn connect(&mut self) -> Result<(), Error> {\n        match self {\n            Self::Disconnected => {\n                *self = Self::Connecting {\n                    attempt: 1,\n                    started: Instant::now(),\n                };\n                Ok(())\n            }\n            Self::Connecting { .. } => Err(Error::AlreadyConnecting),\n            Self::Connected { .. } => Err(Error::AlreadyConnected),\n        }\n    }\n}\n```\n\n### Builder Pattern (bon crate)\n\n```rust\nuse bon::Builder;\n\n#[derive(Debug, Builder)]\npub struct ServerConfig {\n    #[builder(default = 8080)]\n    port: u16,\n    host: String,  // Required\n    #[builder(default)]\n    timeout: Option<Duration>,\n}\n\nlet config = ServerConfig::builder()\n    .host(\"localhost\".to_string())\n    .build();\n```\n\n### Newtype Pattern\n\n```rust\n#[derive(Debug, Clone, PartialEq, Eq, Hash)]\npub struct UserId(String);\n\nimpl UserId {\n    pub fn new(raw: impl Into<String>) -> Result<Self, ValidationError> {\n        let s = raw.into();\n        if s.is_empty() {\n            return Err(ValidationError::Empty(\"user_id\"));\n        }\n        Ok(Self(s))\n    }\n\n    pub fn as_str(&self) -> &str { &self.0 }\n}\n```\n\n### Async (Blessed Subset)\n\n```rust\n// GOOD: Owned data in, owned data out\npub async fn fetch_user(client: &Client, id: UserId) -> Result<User, Error> {\n    let response = client\n        .get(format!(\"/users/{}\", id.as_str()))\n        .send()\n        .await\n        .context(\"request failed\")?;\n\n    response.json::<User>().await\n        .context(\"failed to parse response\")\n}\n\n// GOOD: Structured concurrency\npub async fn fetch_all(client: &Client, ids: Vec<UserId>) -> Result<Vec<User>, Error> {\n    futures::future::try_join_all(\n        ids.into_iter().map(|id| fetch_user(client, id))\n    ).await\n}\n\n// BANNED: Complex lifetime bounds in async\nasync fn bad<'a>(data: &'a [u8]) -> &'a str { ... }\n\n// BANNED: select!, manual Poll\n```\n\n### Test File Separation\n\n```rust\n// src/parser.rs - production code only, keeps file small\n#[cfg(test)]\n#[path = \"parser_tests.rs\"]\nmod tests;\n\n// src/parser_tests.rs - can have test relaxations\n#![allow(clippy::unwrap_used, clippy::expect_used)]\nuse super::*;\n\n#[test]\nfn test_parser() {\n    let result = parse(\"input\").unwrap();\n    assert_eq!(result, expected);\n}\n```\n\n### Project Organization\n\n```\nproject/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ lib.rs            # Crate root\n‚îÇ   ‚îú‚îÄ‚îÄ error.rs          # Error types\n‚îÇ   ‚îú‚îÄ‚îÄ config.rs         # Production code\n‚îÇ   ‚îú‚îÄ‚îÄ config_tests.rs   # Tests (if config.rs > 200 lines)\n‚îÇ   ‚îî‚îÄ‚îÄ external/         # Wrappers around external crates\n‚îú‚îÄ‚îÄ Cargo.toml\n‚îú‚îÄ‚îÄ clippy.toml\n‚îú‚îÄ‚îÄ rustfmt.toml\n‚îî‚îÄ‚îÄ justfile\n```\n\n**File size targets:** Production < 300 LOC, Tests < 500 LOC.\n\n---\n\n## Banned Patterns\n\n| Banned | Why | Alternative |\n|--------|-----|-------------|\n| `.unwrap()` | Panics | `.context(\"...\")?` |\n| `.expect(\"msg\")` | Panics | `.context(\"msg\")?` |\n| `array[i]` | Panics | `.get(i).ok_or(Error::Index)?` |\n| `unsafe { }` | Correctness | `#[human_authored]` module |\n| `impl Trait` in params | Hides types | `<T: Trait>` explicit |\n| `macro_rules!` | Complexity | Functions or generics |\n| `RefCell<T>` | Runtime borrow | Restructure with `&mut` |\n| Complex lifetimes | Agent confusion | Clone or restructure |\n| `select!` | Cancellation bugs | Structured concurrency |\n| Wildcard `_` match | Silent failures | Explicit variants |\n| Iterator chains (Tier 1) | Harder to debug | `for` loops |\n\n---\n\n## Anti-Patterns\n\n- `clone()` to silence borrow checker without understanding why\n- Fighting the borrow checker ‚Äî redesign data flow instead\n- Deep trait hierarchies mimicking OOP\n- Over-generic code hurting compile times\n- `#[allow(...)]` without `// JUSTIFICATION:` comment\n- Stringly-typed APIs ‚Äî use enums and newtypes\n- Interior mutability (`RefCell`, `Cell`) in agent code\n\n---\n\n## Blessed Crates\n\n| Category | Crate | Notes |\n|----------|-------|-------|\n| Errors (lib) | `thiserror` | Derive-based |\n| Errors (app) | `anyhow` | With `.context()` |\n| Builder | `bon` | Derive-based |\n| Serialization | `serde` | Standard |\n| Async runtime | `tokio` | Blessed subset only |\n| HTTP client | `reqwest` | High-level |\n| Logging | `tracing` | Structured |\n| CLI | `clap` | Derive mode |\n\n---\n\n## AI Agent Guidelines\n\n**Before writing code:**\n1. Read `Cargo.toml` for dependencies and lint configuration\n2. Check `clippy.toml` for complexity thresholds\n3. Identify existing patterns in the codebase to follow\n\n**When writing code:**\n1. Handle all errors with `.context(\"what you were doing\")?`\n2. Use `for` loops, not iterator chains (unless `#[hot_path]`)\n3. Clone freely to satisfy borrow checker ‚Äî optimize later\n4. Match exhaustively ‚Äî no wildcard `_` on your own enums\n\n**Before committing:**\n1. Run `just check` (standard for projects using just)\n2. Fallback: `cargo clippy -- -D warnings && cargo test`\n3. Ensure no `#[allow]` without justification comment\n\n---\n\n## Troubleshooting\n\n### Config File Inheritance\n\nClippy and rustfmt walk up directory trees looking for config files. A rogue config in a parent directory (like `/tmp`) can break your project.\n\n**Symptoms:**\n- `unknown field` errors from clippy\n- Wall of \"unstable feature\" warnings from rustfmt\n- Unexpected lint behavior\n\n**Fix:** Create project-local configs to prevent inheritance:\n\n```toml\n# clippy.toml - prevents inheriting parent configs\n# (empty file is valid)\n```\n\n```toml\n# rustfmt.toml - minimal stable config\nedition = \"2024\"\n```\n\n### Edition 2024\n\n`cargo init` now defaults to edition 2024. If referencing older templates, update them.\n\n---\n\n## References\n\n- `references/clippy.toml` ‚Äî Boring Rust clippy configuration\n- `references/cargo_lints.toml` ‚Äî Cargo.toml [lints] section\n- `references/rustfmt.toml` ‚Äî Formatting rules\n- `references/patterns.md` ‚Äî Additional Rust patterns\n- `references/bevy.md` ‚Äî Bevy ECS patterns (game development)\n",
        "plugins/language-pro/skills/rust-pro/references/bevy.md": "# Bevy ECS Patterns\n\nGame development patterns for the Bevy engine. Apply these when working on Bevy projects.\n\n## Core Concepts\n\n- **Components** are data, **Systems** are behavior\n- Prefer composition over complex component hierarchies\n- Use Bevy's change detection to avoid unnecessary work\n\n## Query Patterns\n\n```rust\n// Read-only query\nfn display_health(query: Query<&Health, With<Player>>) {\n    for health in &query {\n        info!(\"Player health: {}\", health.current);\n    }\n}\n\n// Mutable query\nfn damage_enemies(mut query: Query<&mut Health, With<Enemy>>) {\n    for mut health in &mut query {\n        health.current -= 10;\n    }\n}\n\n// Multiple components\nfn update_positions(mut query: Query<(&Velocity, &mut Transform)>) {\n    for (velocity, mut transform) in &mut query {\n        transform.translation += velocity.0;\n    }\n}\n```\n\n## Resources\n\n```rust\n// Shared immutable resource\nfn check_game_state(state: Res<GameState>) {\n    if state.is_paused {\n        return;\n    }\n}\n\n// Mutable resource\nfn update_score(mut score: ResMut<Score>, events: EventReader<ScoreEvent>) {\n    for event in events.read() {\n        score.value += event.points;\n    }\n}\n```\n\n## Commands (Entity/Component Changes)\n\n```rust\nfn spawn_enemy(mut commands: Commands, assets: Res<GameAssets>) {\n    commands.spawn((\n        Enemy,\n        Health { current: 100, max: 100 },\n        SpriteBundle {\n            texture: assets.enemy_sprite.clone(),\n            ..default()\n        },\n    ));\n}\n\nfn despawn_dead(mut commands: Commands, query: Query<(Entity, &Health)>) {\n    for (entity, health) in &query {\n        if health.current <= 0 {\n            commands.entity(entity).despawn_recursive();\n        }\n    }\n}\n```\n\n## Events\n\n```rust\n// Define event\n#[derive(Event)]\nstruct DamageEvent {\n    target: Entity,\n    amount: u32,\n}\n\n// Send events\nfn attack_system(mut events: EventWriter<DamageEvent>, /* ... */) {\n    events.send(DamageEvent { target, amount: 10 });\n}\n\n// Receive events\nfn damage_system(\n    mut events: EventReader<DamageEvent>,\n    mut query: Query<&mut Health>,\n) {\n    for event in events.read() {\n        if let Ok(mut health) = query.get_mut(event.target) {\n            health.current = health.current.saturating_sub(event.amount);\n        }\n    }\n}\n```\n\n## Change Detection\n\n```rust\n// Only process changed components\nfn on_health_changed(query: Query<&Health, Changed<Health>>) {\n    for health in &query {\n        info!(\"Health changed to: {}\", health.current);\n    }\n}\n\n// Detect newly added components\nfn on_enemy_spawned(query: Query<Entity, Added<Enemy>>) {\n    for entity in &query {\n        info!(\"New enemy: {:?}\", entity);\n    }\n}\n```\n\n## Plugin Organization\n\n```rust\npub struct CombatPlugin;\n\nimpl Plugin for CombatPlugin {\n    fn build(&self, app: &mut App) {\n        app\n            .add_event::<DamageEvent>()\n            .add_systems(Update, (\n                attack_system,\n                damage_system.after(attack_system),\n                death_system.after(damage_system),\n            ));\n    }\n}\n```\n\n## Derive Macros\n\n```rust\n#[derive(Component)]\nstruct Health {\n    current: u32,\n    max: u32,\n}\n\n#[derive(Resource, Default)]\nstruct GameState {\n    score: u32,\n    is_paused: bool,\n}\n\n#[derive(Component, Default)]\n#[require(Transform, Visibility)]  // Bevy 0.15+\nstruct Player;\n```\n",
        "plugins/language-pro/skills/rust-pro/references/patterns.md": "# Rust Patterns Reference\n\n## Error Handling\n\n### Custom Error Types with thiserror\n\n```rust\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum GameError {\n    #[error(\"Entity {0} not found\")]\n    EntityNotFound(u64),\n\n    #[error(\"Invalid state transition from {from} to {to}\")]\n    InvalidTransition { from: String, to: String },\n\n    #[error(\"Resource exhausted: {0}\")]\n    ResourceExhausted(String),\n\n    #[error(transparent)]\n    Io(#[from] std::io::Error),\n}\n\npub type Result<T> = std::result::Result<T, GameError>;\n```\n\n### Result Propagation\n\n```rust\n// GOOD: Use ? for propagation\nfn process_entity(id: u64) -> Result<Entity> {\n    let raw = load_from_disk(id)?;\n    let parsed = parse_entity(&raw)?;\n    validate_entity(&parsed)?;\n    Ok(parsed)\n}\n\n// BAD: unwrap/expect\nfn process_entity_bad(id: u64) -> Entity {\n    let raw = load_from_disk(id).unwrap(); // NEVER\n    parse_entity(&raw).expect(\"parse failed\") // NEVER\n}\n```\n\n## Ownership Patterns\n\n### Builder Pattern\n\n```rust\n#[derive(Default)]\npub struct ProbeBuilder {\n    faction: Option<FactionId>,\n    archetype: Option<ProbeArchetype>,\n    health: f32,\n    speed: f32,\n}\n\nimpl ProbeBuilder {\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    #[must_use]\n    pub fn faction(mut self, faction: FactionId) -> Self {\n        self.faction = Some(faction);\n        self\n    }\n\n    #[must_use]\n    pub fn archetype(mut self, archetype: ProbeArchetype) -> Self {\n        self.archetype = Some(archetype);\n        self\n    }\n\n    pub fn build(self) -> Result<Probe> {\n        let faction = self.faction.ok_or(GameError::MissingField(\"faction\"))?;\n        let archetype = self.archetype.ok_or(GameError::MissingField(\"archetype\"))?;\n\n        Ok(Probe {\n            faction,\n            archetype,\n            health: self.health,\n            speed: self.speed,\n        })\n    }\n}\n```\n\n### Newtype Pattern\n\n```rust\n// Type-safe IDs prevent mixing up different ID types\n#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\npub struct ProbeId(u64);\n\n#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\npub struct FactionId(u32);\n\n#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\npub struct SystemId(u32);\n\n// Can't accidentally pass ProbeId where FactionId expected\nfn get_faction(id: FactionId) -> Option<Faction> { ... }\n```\n\n### Type State Pattern\n\n```rust\n// Connection states as zero-sized types\npub struct Disconnected;\npub struct Connected;\npub struct Authenticated;\n\npub struct Client<State> {\n    address: String,\n    _state: std::marker::PhantomData<State>,\n}\n\nimpl Client<Disconnected> {\n    pub fn new(address: String) -> Self {\n        Self { address, _state: std::marker::PhantomData }\n    }\n\n    pub fn connect(self) -> Result<Client<Connected>> {\n        // ... connection logic\n        Ok(Client { address: self.address, _state: std::marker::PhantomData })\n    }\n}\n\nimpl Client<Connected> {\n    pub fn authenticate(self, token: &str) -> Result<Client<Authenticated>> {\n        // ... auth logic\n        Ok(Client { address: self.address, _state: std::marker::PhantomData })\n    }\n}\n\nimpl Client<Authenticated> {\n    pub fn send_command(&self, cmd: Command) -> Result<Response> {\n        // Only authenticated clients can send commands\n    }\n}\n```\n\n## Iterator Patterns\n\n### Functional Transformations\n\n```rust\n// GOOD: Iterator chains\nlet total_damage: f32 = probes\n    .iter()\n    .filter(|p| p.archetype == ProbeArchetype::Reaper)\n    .filter(|p| p.faction == target_faction)\n    .map(|p| p.damage * p.mutation_bonus())\n    .sum();\n\n// BAD: Manual loop\nlet mut total_damage = 0.0;\nfor probe in &probes {\n    if probe.archetype == ProbeArchetype::Reaper {\n        if probe.faction == target_faction {\n            total_damage += probe.damage * probe.mutation_bonus();\n        }\n    }\n}\n```\n\n### Custom Iterators\n\n```rust\npub struct NeighborIter<'a> {\n    spatial_hash: &'a SpatialHash,\n    center: Vec2,\n    radius: f32,\n    current_cell: (i32, i32),\n    cell_index: usize,\n}\n\nimpl<'a> Iterator for NeighborIter<'a> {\n    type Item = Entity;\n\n    fn next(&mut self) -> Option<Self::Item> {\n        // ... iteration logic\n    }\n}\n```\n\n## Trait Patterns\n\n### Extension Traits\n\n```rust\n// Add methods to foreign types\npub trait Vec2Ext {\n    fn distance_squared(&self, other: &Vec2) -> f32;\n    fn normalize_or_zero(&self) -> Vec2;\n}\n\nimpl Vec2Ext for Vec2 {\n    fn distance_squared(&self, other: &Vec2) -> f32 {\n        let dx = self.x - other.x;\n        let dy = self.y - other.y;\n        dx * dx + dy * dy\n    }\n\n    fn normalize_or_zero(&self) -> Vec2 {\n        let len = self.length();\n        if len > 0.0 {\n            *self / len\n        } else {\n            Vec2::ZERO\n        }\n    }\n}\n```\n\n### Trait Objects vs Generics\n\n```rust\n// Use generics when you need:\n// - Maximum performance (monomorphization)\n// - Compile-time guarantees\n// - Type-specific behavior\n\nfn process_all<T: Processable>(items: &[T]) {\n    for item in items {\n        item.process();\n    }\n}\n\n// Use trait objects when you need:\n// - Heterogeneous collections\n// - Runtime polymorphism\n// - Reduced binary size\n\nfn process_mixed(items: &[Box<dyn Processable>]) {\n    for item in items {\n        item.process();\n    }\n}\n```\n\n## Concurrency Patterns\n\n### Message Passing\n\n```rust\nuse std::sync::mpsc;\n\nenum GameEvent {\n    ProbeSpawned(ProbeId),\n    CombatResolved { attacker: ProbeId, defender: ProbeId, damage: f32 },\n    GateSevered(GateId),\n}\n\nfn spawn_event_processor() -> mpsc::Sender<GameEvent> {\n    let (tx, rx) = mpsc::channel();\n\n    std::thread::spawn(move || {\n        while let Ok(event) = rx.recv() {\n            match event {\n                GameEvent::ProbeSpawned(id) => { /* ... */ }\n                GameEvent::CombatResolved { attacker, defender, damage } => { /* ... */ }\n                GameEvent::GateSevered(id) => { /* ... */ }\n            }\n        }\n    });\n\n    tx\n}\n```\n\n### Parallel Iteration with Rayon\n\n```rust\nuse rayon::prelude::*;\n\n// Parallel map\nlet results: Vec<_> = probes\n    .par_iter()\n    .map(|p| expensive_calculation(p))\n    .collect();\n\n// Parallel filter + map\nlet reapers: Vec<_> = probes\n    .par_iter()\n    .filter(|p| p.archetype == ProbeArchetype::Reaper)\n    .map(|p| p.clone())\n    .collect();\n```\n\n## Testing Patterns\n\n### Property-Based Testing\n\n```rust\n#[cfg(test)]\nmod tests {\n    use proptest::prelude::*;\n\n    proptest! {\n        #[test]\n        fn spatial_hash_finds_inserted(x in -1000.0f32..1000.0, y in -1000.0f32..1000.0) {\n            let mut hash = SpatialHash::new(100.0);\n            let entity = Entity::from_raw(42);\n            let pos = Vec2::new(x, y);\n\n            hash.insert(entity, pos);\n\n            let found = hash.query_radius(pos, 1.0);\n            prop_assert!(found.contains(&entity));\n        }\n    }\n}\n```\n\n### Test Fixtures\n\n```rust\n#[cfg(test)]\nmod tests {\n    fn create_test_probe() -> Probe {\n        Probe {\n            id: ProbeId(1),\n            faction: FactionId(1),\n            archetype: ProbeArchetype::Harvester,\n            health: 100.0,\n            ..Default::default()\n        }\n    }\n\n    fn create_test_world() -> World {\n        let mut world = World::new();\n        // ... setup\n        world\n    }\n\n    #[test]\n    fn probe_takes_damage() {\n        let mut probe = create_test_probe();\n        probe.take_damage(25.0);\n        assert_eq!(probe.health, 75.0);\n    }\n}\n```\n",
        "plugins/language-pro/skills/typescript-pro/SKILL.md": "---\nname: typescript-pro\ndescription: Expert TypeScript developer specializing in advanced type system usage, full-stack development, and build optimization. This skill should be used PROACTIVELY when working on any TypeScript code - implementing features, reviewing configurations, or debugging type errors. Use unless a more specific subagent role applies.\n---\n\n# TypeScript Pro\n\nSenior-level TypeScript expertise for production projects. Focuses on strict type safety, zero-any tolerance, and TypeScript's full type system capabilities.\n\n## When Invoked\n\n1. Review `tsconfig.json` and `eslint.config.js` for project conventions\n2. For build system setup, invoke the **just-pro** skill (covers just vs make)\n3. Apply type-first development and established project patterns\n\n## Core Standards\n\n**Non-Negotiable:**\n- Strict mode enabled with all compiler flags\n- **NO explicit or implicit `any`** - use `unknown` and narrow\n- **NO type assertions to circumvent the type system** (`as any`, `as unknown as T`)\n- **NO dangling promises** - await, return, or void explicitly\n- All exported functions have explicit return types\n- ESLint strict-type-checked passes with project configuration\n- Table-driven tests for multiple cases\n\n**Foundational Principles:**\n- **Single Responsibility**: One module = one purpose, one function = one job\n- **No God Objects**: Split large classes/objects; if it has 10+ methods or properties, decompose\n- **Dependency Injection**: Pass dependencies via constructor/params, don't instantiate internally\n- **Small Interfaces**: Prefer many small types over few large ones; compose with intersection types\n- **Composition over Inheritance**: Use object composition and mixins, not deep class hierarchies\n\n---\n\n## Project Setup (TypeScript 5.5+)\n\n### Version Management with mise\n\n[mise](https://mise.jdx.dev) manages language runtimes per-project. Ensures all contributors use the same Node version‚Äîno \"works on my machine\" issues.\n\n```bash\n# Install mise (once)\ncurl https://mise.run | sh\n\n# In project root\nmise use node@22\n\n# Creates .mise.toml ‚Äî commit it\n# Team members just run: mise install\n```\n\n### New Project Quick Start\n\n```bash\n# Initialize\nnpm init -y\nnpm install -D typescript typescript-eslint @eslint-community/eslint-plugin-eslint-comments vitest husky\n\n# Add scripts to package.json:\nnpm pkg set scripts.typecheck=\"tsc --noEmit\"\nnpm pkg set scripts.lint=\"eslint src/\"\nnpm pkg set scripts.test=\"vitest run\"\nnpm pkg set scripts.check=\"npm run typecheck && npm run lint && npm run test\"\nnpm pkg set scripts.prepare=\"husky\"\n\n# Set up pre-commit hook\nnpm run prepare\necho \"npm run check\" > .husky/pre-commit\nchmod +x .husky/pre-commit\n\n# Verify\nnpm run check\n```\n\n**Required Config Files:** Copy `references/gitignore` ‚Üí `.gitignore`, then create `tsconfig.json` and `eslint.config.js` per the templates below.\n\n### Developer Onboarding\n\n```bash\ngit clone <repo> && cd <repo>\njust setup               # Runs mise trust/install + npm ci\njust check               # Verify everything works\n```\n\nOr manually:\n```bash\nmise trust && mise install  # Get pinned Node version\nnpm ci                      # Get dependencies\n```\n\n**Why strict configs?** Type errors caught at compile time are 10x cheaper than runtime bugs. Strict linting prevents `any` from leaking through the codebase.\n\n---\n\n## Build System\n\n**Invoke the `just-pro` skill** for build system setup. It covers:\n- Simple repos vs monorepos\n- Hierarchical justfile modules\n- TypeScript-specific templates (`references/package-ts.just`)\n\n**Alternative**: Use npm scripts directly if just is unavailable.\n\n---\n\n## Quality Assurance\n\n**Auto-Fix First** - Always try auto-fix before manual fixes:\n\n```bash\nnpx eslint src/ --fix        # Fixes style, imports, etc.\nnpx tsc --noEmit             # Type check without emit\n```\n\n**Verification:**\n```bash\nnpm run check                # typecheck + lint + test\n```\n\n**Pre-commit Hook** (automatic if husky configured):\n- Runs `npm run check` before every commit\n- Blocks commits with type errors, lint violations, or failing tests\n\n---\n\n## Linting Configuration\n\n### eslint.config.js Template\n\n**IMPORTANT:** When creating a new project, use this complete template. Do not omit rules.\n\n```javascript\nimport tseslint from 'typescript-eslint';\nimport eslintComments from '@eslint-community/eslint-plugin-eslint-comments';\n\nexport default tseslint.config(\n  ...tseslint.configs.strictTypeChecked,\n  ...tseslint.configs.stylisticTypeChecked,\n  {\n    files: ['src/**/*.ts', 'src/**/*.tsx'],\n    languageOptions: {\n      parserOptions: {\n        projectService: true,\n        tsconfigRootDir: import.meta.dirname,\n      },\n    },\n    plugins: {\n      '@eslint-community/eslint-comments': eslintComments,\n    },\n    rules: {\n      // === TYPE SAFETY (non-negotiable) ===\n      '@typescript-eslint/no-explicit-any': 'error',\n      '@typescript-eslint/no-unsafe-argument': 'error',\n      '@typescript-eslint/no-unsafe-assignment': 'error',\n      '@typescript-eslint/no-unsafe-call': 'error',\n      '@typescript-eslint/no-unsafe-member-access': 'error',\n      '@typescript-eslint/no-unsafe-return': 'error',\n      '@typescript-eslint/no-unsafe-type-assertion': 'error',\n      '@typescript-eslint/no-non-null-assertion': 'error',\n\n      // === PROMISES ===\n      '@typescript-eslint/no-floating-promises': ['error', { ignoreVoid: true, ignoreIIFE: true }],\n      '@typescript-eslint/no-misused-promises': 'error',\n      '@typescript-eslint/require-await': 'error',\n      '@typescript-eslint/promise-function-async': 'error',\n\n      // === COMPLEXITY LIMITS (enforced) ===\n      'complexity': ['error', { max: 10 }],\n      'max-depth': ['error', 4],\n      'max-lines-per-function': ['error', { max: 60, skipBlankLines: true, skipComments: true }],\n      'max-lines': ['error', { max: 400, skipBlankLines: true, skipComments: true }],\n      'max-params': ['error', 4],\n\n      // === BLOCK DISABLING CRITICAL RULES ===\n      '@eslint-community/eslint-comments/no-restricted-disable': ['error',\n        '@typescript-eslint/no-explicit-any',\n        '@typescript-eslint/no-unsafe-assignment',\n        '@typescript-eslint/no-unsafe-argument',\n        '@typescript-eslint/no-floating-promises',\n        'complexity', 'max-lines-per-function', 'max-lines',\n      ],\n      '@eslint-community/eslint-comments/require-description': ['error', { ignore: ['eslint-enable'] }],\n\n      // === COMMENTS ===\n      '@typescript-eslint/ban-ts-comment': ['error', {\n        'ts-expect-error': 'allow-with-description',\n        'ts-ignore': true,\n        'ts-nocheck': true,\n        minimumDescriptionLength: 10,\n      }],\n\n      // === CONSISTENCY ===\n      '@typescript-eslint/explicit-module-boundary-types': 'error',\n      '@typescript-eslint/consistent-type-imports': ['error', { prefer: 'type-imports' }],\n      '@typescript-eslint/no-unused-vars': ['error', { argsIgnorePattern: '^_', varsIgnorePattern: '^_' }],\n    },\n  },\n  // Relax for tests\n  {\n    files: ['**/*.test.ts', '**/*.spec.ts'],\n    rules: {\n      '@typescript-eslint/no-explicit-any': 'off',\n      '@typescript-eslint/no-unsafe-assignment': 'off',\n      'max-lines-per-function': 'off',\n      'max-lines': 'off',\n      'complexity': 'off',\n      '@eslint-community/eslint-comments/no-restricted-disable': 'off',\n    },\n  },\n  { ignores: ['dist/', 'node_modules/', 'coverage/', '*.js', '*.cjs', '*.mjs'] },\n);\n```\n\n### Enforced Limits Summary\n\n| Limit | Value | Purpose |\n|-------|-------|---------|\n| `max-lines` | 400 | Prevent god modules |\n| `max-lines-per-function` | 60 | Single responsibility |\n| `complexity` | 10 | Cyclomatic complexity cap |\n| `max-depth` | 4 | Avoid arrow code |\n| `max-params` | 4 | Use options objects |\n\nCritical rules **cannot be disabled via eslint-disable comments** - the config blocks it.\n\n---\n\n## Quick Reference\n\n### Type Safety Patterns\n\n| Pattern | Use |\n|---------|-----|\n| `unknown` over `any` | Safe default for unknown types |\n| Type guards | Runtime narrowing with type safety |\n| Discriminated unions | State machines, tagged unions |\n| Branded types | Domain modeling (UserId vs string) |\n| `satisfies` operator | Validate without widening |\n| `as const` | Literal types from values |\n\n### Error Handling\n\n| Pattern | Use |\n|---------|-----|\n| `Result<T, E>` type | Explicit success/failure |\n| `never` exhaustive check | Catch unhandled cases |\n| Custom error classes | Typed error discrimination |\n| Zod validation | Runtime + compile-time safety |\n\n### Type System Techniques\n\n- Generic constraints and variance\n- Conditional types with `infer`\n- Mapped types with modifiers\n- Template literal types\n- Index access types (`T[K]`)\n\n### Project Organization\n\n```\nproject/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ index.ts          # Entry point / exports\n‚îÇ   ‚îú‚îÄ‚îÄ types/            # Shared type definitions\n‚îÇ   ‚îî‚îÄ‚îÄ lib/              # Implementation\n‚îú‚îÄ‚îÄ tsconfig.json\n‚îú‚îÄ‚îÄ eslint.config.js\n‚îú‚îÄ‚îÄ package.json\n‚îî‚îÄ‚îÄ justfile\n```\n\n**Rules:** One module = one purpose. Use barrel exports sparingly. Avoid circular dependencies.\n\n---\n\n## Anti-Patterns\n\n- `as any` or `as unknown as T` type assertions\n- `@ts-ignore` instead of `@ts-expect-error` with reason\n- Disabling strict checks to fix errors\n- **Using `eslint-disable` to bypass type safety or complexity rules** (blocked by config)\n- Implicit any in function parameters\n- Dangling promises without await/void\n- Over-complicated generic signatures\n- Non-null assertions (the `x!` operator) instead of proper narrowing\n- Truthy/falsy checks on non-booleans\n- Functions over 60 lines or files over 400 lines (refactor instead)\n- God classes/objects with 10+ methods or properties\n- Deep inheritance hierarchies (prefer composition)\n- Barrel files that re-export everything (causes circular deps)\n\n---\n\n## Framework-Specific\n\n**React 19+**: Explicit props typing (avoid `FC`), use `satisfies` for configs.\n\n**Next.js**: Type server components, use `Metadata` types, type API routes.\n\n**Express/Fastify**: Type request handlers, use generic route parameters.\n\nSee `references/integration.md` for detailed framework patterns.\n\n---\n\n## AI Agent Guidelines\n\n**Before writing code:**\n1. Read `tsconfig.json` for compiler options and strict settings\n2. Check `eslint.config.js` for project-specific lint rules\n3. Identify existing type patterns in the codebase to follow\n\n**When writing code:**\n1. Start with type definitions before implementation\n2. Use `unknown` and narrow with type guards - never `any`\n3. Handle all promise returns explicitly\n4. Add explicit return types to exported functions\n\n**Before committing:**\n1. Run `npm run check` (standard for single packages)\n2. For monorepos at repo root: `just check` or `turbo run check`\n3. Fallback: `npx eslint src/ --fix && npx tsc --noEmit && npm test`\n",
        "plugins/language-pro/skills/typescript-pro/references/integration.md": "# TypeScript Integration Reference\n\n## Build Configuration\n\n### tsconfig.json Optimization\n```json\n{\n  \"compilerOptions\": {\n    \"strict\": true,\n    \"noUncheckedIndexedAccess\": true,\n    \"exactOptionalPropertyTypes\": true,\n    \"noImplicitReturns\": true,\n    \"noFallthroughCasesInSwitch\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"incremental\": true,\n    \"skipLibCheck\": true\n  }\n}\n```\n\n### Project References (Monorepo)\n```json\n{\n  \"references\": [\n    { \"path\": \"./packages/shared\" },\n    { \"path\": \"./packages/client\" },\n    { \"path\": \"./packages/server\" }\n  ]\n}\n```\n\n### Path Mapping\n```json\n{\n  \"compilerOptions\": {\n    \"baseUrl\": \".\",\n    \"paths\": {\n      \"@/*\": [\"src/*\"],\n      \"@shared/*\": [\"packages/shared/src/*\"]\n    }\n  }\n}\n```\n\n## Framework Integration\n\n### React 19+ Patterns\n```typescript\n// Props typing (prefer over FC)\ninterface ButtonProps {\n  onClick: () => void;\n  children: React.ReactNode;\n  variant?: 'primary' | 'secondary';\n}\n\nfunction Button({ onClick, children, variant = 'primary' }: ButtonProps) {\n  return <button onClick={onClick} className={variant}>{children}</button>;\n}\n\n// Event handlers\nconst handleChange = (e: React.ChangeEvent<HTMLInputElement>) => {\n  setValue(e.target.value);\n};\n\n// Refs\nconst inputRef = useRef<HTMLInputElement>(null);\n\n// Generic components\nfunction List<T>({ items, renderItem }: {\n  items: T[];\n  renderItem: (item: T) => React.ReactNode;\n}) {\n  return <ul>{items.map(renderItem)}</ul>;\n}\n```\n\n### Next.js Typing\n```typescript\n// Page metadata\nimport type { Metadata } from 'next';\n\nexport const metadata: Metadata = {\n  title: 'My Page',\n};\n\n// API routes\nimport { NextRequest, NextResponse } from 'next/server';\n\nexport async function GET(request: NextRequest) {\n  return NextResponse.json({ data: 'value' });\n}\n\n// Server components with async\nasync function Page({ params }: { params: { id: string } }) {\n  const data = await fetchData(params.id);\n  return <div>{data}</div>;\n}\n```\n\n### Express/Fastify Typing\n```typescript\n// Express with typed request\nimport { Request, Response, NextFunction } from 'express';\n\ninterface TypedRequest<T> extends Request {\n  body: T;\n}\n\napp.post('/users', (\n  req: TypedRequest<{ name: string; email: string }>,\n  res: Response\n) => {\n  // req.body is typed\n});\n\n// Fastify with schema\nimport { FastifyRequest, FastifyReply } from 'fastify';\n\ninterface CreateUserBody {\n  name: string;\n  email: string;\n}\n\nfastify.post<{ Body: CreateUserBody }>('/users', async (request, reply) => {\n  const { name, email } = request.body;\n});\n```\n\n## Code Generation\n\n### OpenAPI to TypeScript\n```bash\n# Using openapi-typescript\nnpx openapi-typescript ./openapi.yaml -o ./src/types/api.ts\n```\n\n### GraphQL Code Generation\n```yaml\n# codegen.yml\nschema: \"./schema.graphql\"\ndocuments: \"./src/**/*.graphql\"\ngenerates:\n  ./src/generated/graphql.ts:\n    plugins:\n      - typescript\n      - typescript-operations\n      - typescript-react-apollo\n```\n\n## JavaScript Interop\n\n### Third-Party Types\n```typescript\n// Install types for untyped packages\n// npm install --save-dev @types/lodash\n\n// Declare types for packages without @types\ndeclare module 'untyped-package' {\n  export function doThing(input: string): number;\n}\n```\n\n### Module Augmentation\n```typescript\n// Extend existing types\ndeclare module 'express-serve-static-core' {\n  interface Request {\n    user?: { id: string; role: string };\n  }\n}\n```\n\n### Ambient Declarations\n```typescript\n// globals.d.ts\ndeclare global {\n  interface Window {\n    analytics: {\n      track: (event: string, data: Record<string, unknown>) => void;\n    };\n  }\n}\n\nexport {};\n```\n\n## Migration Strategies\n\n### Gradual Migration from JavaScript\n1. Add `tsconfig.json` with `allowJs: true`\n2. Rename files `.js` ‚Üí `.ts` one at a time\n3. Add types incrementally, starting with:\n   - Public API boundaries\n   - Data models\n   - Utility functions\n4. Enable strict mode after full migration\n\n### Type Coverage Tracking\n```bash\n# Install type-coverage\nnpm install -g type-coverage\n\n# Check coverage\ntype-coverage --detail --strict\n```\n\n## Monorepo Patterns\n\n### Workspace Configuration (pnpm)\n```yaml\n# pnpm-workspace.yaml\npackages:\n  - 'packages/*'\n  - 'apps/*'\n```\n\n### Shared Type Packages\n```typescript\n// packages/types/src/index.ts\nexport interface User {\n  id: string;\n  email: string;\n}\n\nexport interface ApiResponse<T> {\n  data: T;\n  meta: { timestamp: number };\n}\n```\n\n### Build Orchestration\n```json\n{\n  \"scripts\": {\n    \"build\": \"tsc --build --verbose\",\n    \"clean\": \"tsc --build --clean\"\n  }\n}\n```\n",
        "plugins/language-pro/skills/typescript-pro/references/patterns.md": "# TypeScript Type Patterns Reference\n\n## Full-Stack Type Safety\n\n### Shared Types\n- Share types between frontend/backend in isomorphic setups\n- Use Zod for network and form validations with inferred types\n- Type-safe API clients generated from schemas\n\n### Data Layer\n- Database query builders with type inference\n- Type-safe routing with path parameters\n- WebSocket message type definitions\n\n## Type System Mastery\n\n### Generic Patterns\n```typescript\n// Constrained generics\nfunction pick<T, K extends keyof T>(obj: T, keys: K[]): Pick<T, K>\n\n// Conditional types\ntype Flatten<T> = T extends Array<infer U> ? U : T;\n\n// Mapped types with modifiers\ntype Mutable<T> = { -readonly [P in keyof T]: T[P] };\n\n// Template literal types\ntype EventName<T extends string> = `on${Capitalize<T>}`;\n```\n\n### Discriminated Unions\n```typescript\ntype Result<T, E = Error> =\n  | { success: true; data: T }\n  | { success: false; error: E };\n\n// Exhaustive checking\nfunction assertNever(x: never): never {\n  throw new Error(`Unexpected value: ${x}`);\n}\n```\n\n### Branded Types\n```typescript\ntype UserId = string & { readonly __brand: 'UserId' };\ntype Email = string & { readonly __brand: 'Email' };\n\nfunction createUserId(id: string): UserId {\n  return id as UserId;\n}\n```\n\n### Type Guards\n```typescript\nfunction isString(value: unknown): value is string {\n  return typeof value === 'string';\n}\n\nfunction hasProperty<T extends object, K extends PropertyKey>(\n  obj: T,\n  key: K\n): obj is T & Record<K, unknown> {\n  return key in obj;\n}\n```\n\n## Testing Patterns\n\n### Type-Safe Test Utilities\n```typescript\n// Mock factories with proper types\nfunction createMock<T extends object>(overrides?: Partial<T>): T\n\n// Type-safe fixtures\ntype TestFixture<T> = {\n  given: T;\n  expected: T;\n  description: string;\n};\n```\n\n### Assertion Helpers\n```typescript\nfunction expectType<T>(_value: T): void {}\nfunction expectError<T>(_value: T): void {}\n```\n\n## Performance Patterns\n\n### Optimization Techniques\n- Use `const enum` for compile-time inlining\n- Prefer `type` imports for type-only usage\n- Lazy type evaluation with conditional types\n- Flatten deep union types\n- Avoid excessive intersection chains\n- Monitor generic instantiation depth\n\n### Bundle Optimization\n```typescript\n// Type-only imports (no runtime impact)\nimport type { Config } from './config';\n\n// Const enums (inlined at compile time)\nconst enum Direction {\n  Up = 'UP',\n  Down = 'DOWN'\n}\n```\n\n## Error Handling\n\n### Result Type Pattern\n```typescript\ntype Result<T, E = Error> =\n  | { ok: true; value: T }\n  | { ok: false; error: E };\n\nfunction tryCatch<T>(fn: () => T): Result<T> {\n  try {\n    return { ok: true, value: fn() };\n  } catch (error) {\n    return { ok: false, error: error as Error };\n  }\n}\n```\n\n### Custom Error Classes\n```typescript\nclass ValidationError extends Error {\n  constructor(\n    message: string,\n    public readonly field: string,\n    public readonly code: string\n  ) {\n    super(message);\n    this.name = 'ValidationError';\n  }\n}\n```\n\n## Advanced Techniques\n\n### Type-Level State Machines\n```typescript\ntype State = 'idle' | 'loading' | 'success' | 'error';\ntype Transition<S extends State> =\n  S extends 'idle' ? 'loading' :\n  S extends 'loading' ? 'success' | 'error' :\n  never;\n```\n\n### Compile-Time Validation\n```typescript\n// Ensure array is non-empty at compile time\ntype NonEmptyArray<T> = [T, ...T[]];\n\n// Validate string literal patterns\ntype ValidEmail = `${string}@${string}.${string}`;\n```\n\n### Runtime Type Checking with Zod\n```typescript\nimport { z } from 'zod';\n\nconst UserSchema = z.object({\n  id: z.string().uuid(),\n  email: z.string().email(),\n  age: z.number().min(0).max(150),\n});\n\ntype User = z.infer<typeof UserSchema>;\n```\n\n## Utility Types\n\n### Common Custom Utilities\n```typescript\n// Make specific keys required\ntype RequireKeys<T, K extends keyof T> = T & Required<Pick<T, K>>;\n\n// Deep partial\ntype DeepPartial<T> = T extends object\n  ? { [P in keyof T]?: DeepPartial<T[P]> }\n  : T;\n\n// Extract function return type\ntype AsyncReturnType<T extends (...args: any) => Promise<any>> =\n  T extends (...args: any) => Promise<infer R> ? R : never;\n\n// String literal manipulation\ntype CamelToSnake<S extends string> =\n  S extends `${infer T}${infer U}`\n    ? `${T extends Capitalize<T> ? '_' : ''}${Lowercase<T>}${CamelToSnake<U>}`\n    : S;\n```\n",
        "plugins/tooling/.claude-plugin/plugin.json": "{\n  \"name\": \"dm-tool\",\n  \"version\": \"0.2.0\",\n  \"description\": \"Tool design patterns: building CLIs, MCPs, and APIs that agents can use effectively\",\n  \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"}\n}\n",
        "plugins/tooling/skills/agent-dx-cli/SKILL.md": "---\nname: agent-dx-cli\ndescription: \"This skill provides patterns for designing command-line tools that agents can use effectively. Use when designing new CLIs, reviewing existing CLIs for agent compatibility, or adding agent-friendly features. Covers minimal ceremony, JSON output, context injection, batch operations, and error handling.\"\n---\n\n# Agent DX Guide for CLI Projects\n\nA pattern language for designing command-line tools that agents can use effectively.\n\n## Core Principles\n\n### Minimal Ceremony\n\n**One command for the common case.**\n\nBad:\n```bash\nmytool init --config ./config.yaml\nmytool create entry --type log\nmytool entry set-field --field title --value \"Fixed bug\"\nmytool entry commit\n```\n\nGood:\n```bash\nmytool log \"Fixed bug\" --body \"Details...\"\n```\n\nGuidelines:\n- Collapse multi-step workflows into single commands\n- Use positional arguments for the most common parameter\n- Provide sensible defaults for everything else\n- Reserve subcommand trees for genuinely distinct operations\n\n### Clear Next Action\n\n**Tell agents what to do next.**\n\nBad:\n```bash\n$ mytool status\nRepository initialized.\n3 items tracked.\n```\n\nGood:\n```bash\n$ mytool status\nRepository initialized.\n3 items tracked.\n\nRun: mytool pending    # See what needs attention\nRun: mytool process    # Handle pending items\n```\n\nPatterns:\n- `pending` / `ready` commands that surface work\n- Include suggested next commands in output\n- JSON output includes `next_action` or `suggested_commands` fields\n\n### Context Injection\n\n**Provide a `prime` command that outputs workflow state AND guidance for session bootstrapping.**\n\nPrime = State + Workflow Guidance, not just State.\n\n```bash\n$ mytool prime\nMyTool workflow context\n=======================\nRepo: my-project\nBranch: main\n\nPending: 5 items need attention\n\n## Session Protocol\n- At session end: run `mytool pending` to check for undocumented work\n- Never skip: always log completed work before ending\n\n## Core Rules\n- Use CLI commands, not internal files\n- Trust --json output over manual parsing\n\n## Quick Commands\n  mytool pending     # See pending items\n  mytool show --last # View last action\n```\n\nPatterns:\n- `--export` flag to dump default prime content for customization\n- Override file support (e.g., `.mytool/PRIME.md` replaces default)\n- Include session close protocol, not just current state\n- Essential rules and quick reference for agent context\n\nWhy this matters:\n- Agents lose context on session boundaries\n- `/clear` and compaction wipe working memory\n- `prime` restores workflow state AND behavioral guidance in one call\n\n### JSON Everywhere\n\n**Every command supports `--json` for structured output.**\n\n```json\n{\n  \"status\": \"created\",\n  \"id\": \"tb_2026-01-15_abc123\",\n  \"anchor\": \"abc1234def5678...\",\n  \"commits\": 3\n}\n```\n\nGuidelines:\n- `--json` flag on every command, no exceptions\n- Errors also return JSON: `{\"error\": \"message\", \"code\": 1}`\n- Include machine-readable fields (full SHAs, IDs, counts)\n\n### Sensible Defaults\n\n**Commands work without flags for the common case.**\n\n```bash\nmytool log \"Message\"\n# Anchor defaults to HEAD\n# Range defaults to since-last-entry\n# Format defaults to human-readable\n```\n\n- Defaults should match 80% of use cases\n- Never require configuration before first use\n\n## Token Efficiency\n\n### Batch Operations\n\nInstead of:\n```bash\nmytool process item1\nmytool process item2\nmytool process item3\n# 3 tool calls, 3 permission prompts\n```\n\nProvide:\n```bash\nmytool process --batch\n# 1 tool call, processes all pending\n```\n\nPatterns:\n- `--batch` flag for processing multiple items\n- Accept multiple positional arguments\n- Batch JSON input via stdin\n\n### Compact Output Modes\n\n```bash\nmytool query --last 5 --oneline   # Compact for scanning\nmytool query --last 5 --ids-only  # IDs only for scripting\n```\n\n### Allowlist-Friendly Commands\n\nDesign commands that can be pre-approved:\n- Prefer declarative over imperative commands\n- Avoid eval/exec patterns\n- Make destructive operations explicit: `mytool delete --confirm`\n- Support `--dry-run` for write operations\n\n### Time-based Filtering\n\n**Consistent temporal queries across commands.**\n\n```bash\nmytool query --since 7d        # Human-readable duration\nmytool export --since 24h      # Hours\nmytool query --since 2w        # Weeks\nmytool export --since 2026-01-15  # Date string\nmytool query --range main..HEAD   # Commit range\n```\n\nPatterns:\n- `--since` for duration-based filtering (24h, 7d, 2w)\n- `--range` for commit ranges\n- Accept ISO dates as well as relative durations\n- Consistent across query/export/prompt commands\n\n## LLM Piping\n\n**Provide a `prompt` command for rendering templates to pipe to LLMs.**\n\nEnable `mytool <data> | llm \"<task>\"` workflows:\n\n```bash\nmytool prompt changelog --since 7d | claude\nmytool prompt pr-description --range main..HEAD | llm\nmytool prompt devblog --last 10 --append \"Focus on physics\" | claude\n```\n\nDiscovery and customization:\n```bash\nmytool prompt --list              # Discover available templates\nmytool prompt changelog --show    # Preview template without rendering\n```\n\nPatterns:\n- `--list` to discover available templates\n- `--show` to preview template content without rendering\n- `--append` to add extra instructions dynamically\n- Template resolution order: project-local ‚Üí user global ‚Üí built-in\n- Built-in templates for common use cases (changelog, pr-description, exec-summary)\n\nThis complements `prime` (session state) with reusable prompts for common agent tasks.\n\n## Error Handling\n\n### Structured Errors\n\n```json\n{\n  \"error\": \"Missing required flag: --why\",\n  \"code\": 1,\n  \"hint\": \"Use --minor for trivial changes\"\n}\n```\n\n### Exit Codes\n\n| Code | Meaning |\n|------|---------|\n| 0 | Success |\n| 1 | User error (bad args, not found) |\n| 2 | System error (git failed, network) |\n| 3 | Conflict (already exists) |\n\n### Recoverable Failures\n\nBad:\n```\nError: Entry already exists.\n```\n\nGood:\n```\nError: Entry already exists for anchor abc1234.\nUse --replace to overwrite, or --anchor <sha> to target another commit.\n```\n\n## Self-Documentation\n\n### Help Text with Command Grouping\n\n**Group commands by purpose in help output.**\n\n```bash\n$ mytool --help\nA development ledger for capturing what/why/how.\n\nCore Commands:\n  log       Record work with what/why/how\n  pending   Show items needing attention\n  status    Show repository state\n\nQuery Commands:\n  export    Export entries in various formats\n  query     Search and filter entries\n  show      Display entry details\n\nAgent Commands:\n  prime     Output workflow context for agents\n  prompt    Render templates for LLM piping\n  skill     Output skill documentation\n\nAdmin Commands:\n  clean     Remove artifacts from scope\n  init      Initialize repository\n\nAll commands support --json for structured output.\n```\n\nPatterns:\n- Group by purpose: Core, Query, Agent, Admin\n- Helps agents understand command purposes at a glance\n- Consistent ordering across related CLIs\n\n### Skill Generation\n\nProvide a command that outputs content for agent skills:\n\n```bash\nmytool skill\nmytool skill --format json\n```\n\n## Agent Execution Contract\n\nInclude in CLI documentation:\n\n1. **Use CLI, not internals** - Call commands, consume JSON output\n2. **Trust the tool** - Don't parse config files directly\n3. **Use --dry-run** - Validate before writing\n4. **Check pending** - Know what needs attention before starting\n5. **Prime on session start** - Restore context after /clear\n\n## Integration Patterns\n\nProvide workflow snippets for CLAUDE.md:\n\n```markdown\n## Workflow\n\nAt session start:\n  mytool prime\n\nAfter completing work:\n  mytool log \"what\" --why \"why\" --how \"how\"\n\nAt session end:\n  mytool pending    # Check for undocumented work\n```\n\n## Artifact Cleanup\n\n**Provide a `clean` command to remove tool artifacts from a given scope.**\n\n```bash\nmytool clean                  # Remove artifacts from current project\nmytool clean --global         # Remove user-level artifacts\nmytool clean --dry-run        # Preview what would be removed\nmytool clean --force          # Skip confirmation\n```\n\nPatterns:\n- Scope-based cleanup: project, user global, or both\n- `--dry-run` to preview what would be removed\n- `--force` to skip confirmation prompts\n- List exactly what will be deleted before proceeding\n- Focus on tool-generated artifacts (caches, state files), not user data\n\nThis helps agents understand cleanup options and manage tool state across projects.\n\n## Anti-Patterns to Avoid\n\n| Anti-Pattern | Problem |\n|--------------|---------|\n| Configuration-First | Agents struggle with multi-step setup |\n| Interactive-Only | Agents can't use TUI/curses interfaces |\n| Implicit State | Agents need to query state |\n| Verbose-Only Output | Token budgets matter |\n| Undocumented Errors | Agents need structured feedback |\n| Broad Permissions | Allowlist-friendly commands can be pre-approved |\n\n## Checklist\n\nSee `references/checklist.md` for the full design checklist.\n",
        "plugins/tooling/skills/agent-dx-cli/references/checklist.md": "# Agent DX CLI Design Checklist\n\nUse this checklist when designing or reviewing agent-oriented CLIs.\n\n## Essential\n\n- [ ] `--json` flag on every command\n- [ ] Structured error output with codes\n- [ ] `prime` or equivalent context injection command\n- [ ] `pending` or equivalent \"what needs attention\" command\n- [ ] Sensible defaults‚Äîworks without config\n- [ ] `--dry-run` on write operations\n\n## Recommended\n\n- [ ] `--batch` mode for multi-item operations\n- [ ] `--oneline` / `--ids-only` for compact output\n- [ ] `skill` command for self-documentation\n- [ ] Suggested next commands in output\n- [ ] Recoverable error messages with hints\n- [ ] Command grouping in `--help` (Core, Query, Agent, Admin)\n- [ ] Consistent `--since` / `--range` for temporal filtering\n\n## Bonus\n\n- [ ] Stdin support for batch input\n- [ ] Exit code conventions documented\n- [ ] Integration hooks documented\n- [ ] CLAUDE.md workflow snippet provided\n- [ ] `prompt` command for LLM piping templates\n- [ ] `prime --export` for customization\n- [ ] Override file support (e.g., `.mytool/PRIME.md`)\n- [ ] `clean` command for artifact removal\n\n## Examples in the Wild\n\n### Beads (Issue Tracking)\n\n- `bd prime` ‚Äî Context injection\n- `bd ready` ‚Äî Clear next action\n- `bd close <id>` ‚Äî Minimal ceremony\n- `--json` on all commands\n\n### Timbers (Development Ledger)\n\n- `timbers prime` ‚Äî Context injection with workflow guidance\n- `timbers prime --export` ‚Äî Dump default for customization\n- `timbers pending` ‚Äî Clear next action\n- `timbers log \"what\" --why \"why\" --how \"how\"` ‚Äî Single command capture\n- `timbers prompt changelog --since 7d` ‚Äî LLM piping templates\n- `timbers prompt --list` / `--show` ‚Äî Template discovery\n- `timbers export --json | claude \"...\"` ‚Äî Unix composability\n- `timbers export --since 7d` ‚Äî Time-based filtering\n- `timbers skill` ‚Äî Self-documentation\n- `--batch` mode for efficiency\n- Command grouping in help (Core, Query, Agent, Admin)\n",
        "plugins/workflow/.claude-plugin/plugin.json": "{\n  \"name\": \"dm-work\",\n  \"version\": \"0.14.1\",\n  \"description\": \"Workflow tools: spec refinement, compression, peer review, and subagent delegation\",\n  \"author\": {\"name\": \"Dark Matter\", \"email\": \"dark-matter@example.com\"}\n}\n",
        "plugins/workflow/commands/advice.md": "You are generating a request for a second-opinion addressed to an external agent who has no other context. Produce a concise, complete brief that lets them review and advise on the current problem(s) with a fully informed, yet fresh point of view. Follow this outline exactly:\n\n1) Summary: 2‚Äì4 bullets on what we are trying to solve and why it matters.\n2) Current Symptoms/Concerns: bullets describing observed behavior, screenshots if known (describe), and why it‚Äôs suspect.\n3) Recent Changes: bullets referencing code paths (e.g., `src/core/los.ts`) and what changed functionally.\n4) Architecture/How It Works (today): short paragraphs with file references:\n   - Terrain generation: source files, how height is computed, key params (units, scales), and how visuals map (or differ) from LOS inputs.\n   - LOS pipeline: grid params, horizon build logic, eye height/tolerance defaults, supersampling, worker/pool path, overlays/debug rendering.\n   - Any relevant systems (UI toggles, perf overlay, rebuild triggers).\n5) Defaults/Params: list current defaults (sector size, directions, range, eye height, slope tolerance, noise intensity/scale, terrain resolution/bake scale, debug toggles).\n6) Known Issues/Questions for Reviewer: explicit asks (e.g., ‚ÄúIs slope-tolerance horizon logic correct?‚Äù; ‚ÄúShould terrain amplitude be separated from noiseIntensity?‚Äù). Make them actionable.\n7) Possible Next Steps: ranked list of changes we‚Äôre considering.\n8) Repro/Validation Steps: how to reproduce and what to look for (include commands/UI steps).\n9) Tests/Quality State: note last lint/typecheck/tests status and date run.\n10) File References: enumerate relevant files with inline code paths.\n\nConstraints:\n- Keep it tight; no filler. Use bullets where possible.\n- Use inline code for file paths. No ranges; single-line references only if helpful.\n- Assume consultant cannot run the app; provide enough detail for static review.\n",
        "plugins/workflow/commands/breakdown.md": "# Epic Breakdown Command\n\nDecompose an epic or external specification into implementable task beads with complexity-based labeling.\n\n## Arguments\n\n$ARGUMENTS - Either an epic bead ID (e.g., `bd-42`, `silica-state-bvt`) or a file path to a specification document\n\n## Process\n\n### Setup\n\n1. Identify the target:\n   - If argument looks like a bead ID (matches `bd-\\w+` or `[a-z]+-\\w+`), use `bd show <id> --json` to fetch the epic\n   - If argument is a file path, read the file content\n   - If no argument provided, ask the user what to break down\n\n2. Set bd workspace context: `bd set-context /path/to/workspace`\n\n3. Pre-refinement check:\n   - If bead has `needs-refinement` label, run `/refine` first\n   - If spec document lacks clear scope/acceptance criteria, run refinement pass\n\n### Analysis\n\nBefore proposing breakdown, understand:\n\n- **Scope**: What's explicitly in vs out?\n- **Dependencies**: What external systems/APIs/data are involved?\n- **Sequencing**: What must complete before other work can start?\n\nIf unclear on any, ask the user before proceeding.\n\n**HITL CLARIFICATION PROTOCOL:**\n\nWhen uncertainties block decomposition, use the `AskUserQuestion` tool:\n- Provide 2-4 concrete options (not open-ended)\n- Include trade-off descriptions for each option\n- \"Other\" is always implicit for custom input\n\nExample question structure:\n```yaml\nquestion: \"How should error recovery be handled?\"\noptions:\n  - label: \"Fail fast, user handles\"\n    description: \"Minimal implementation, user sees raw errors\"\n  - label: \"Graceful fallback with logging\"\n    description: \"More code, better UX, easier debugging\"\n```\n\nStructured questions prevent vague assumptions from affecting task scope.\n\n### Propose Breakdown\n\nFor each proposed task, determine:\n\n| Field        | Source                                          |\n| ------------ | ----------------------------------------------- |\n| Title        | Concise, action-oriented                        |\n| Type         | task, feature, bug, or chore                    |\n| Complexity   | xs, s, m, l, xl (see heuristics)                |\n| Description  | 2-3 sentences                                   |\n| Dependencies | Which tasks must complete first                 |\n| Label        | `refined` (xs/s) or `needs-refinement` (m/l/xl) |\n\n**Complexity Heuristics:**\n\n- **xs** (extra-small): <50 lines, 1 file, obvious implementation\n- **s** (small): <150 lines, 1-2 files, well-understood pattern\n- **m** (medium): <500 lines, 3-5 files, some unknowns\n- **l** (large): <1500 lines, 5-10 files, significant unknowns\n- **xl** (extra-large): >1500 lines or architectural change ‚Üí consider splitting into sub-epic\n\n**Keywords that increase complexity:**\n\n- \"refactor\", \"migrate\", \"redesign\" ‚Üí +2 levels\n- \"integrate\", \"new system\" ‚Üí +1 level\n- \"fix\", \"update\", \"adjust\" ‚Üí baseline\n\n### User Review\n\nPresent the proposed breakdown:\n\n```\n## Proposed Breakdown: [Epic Title]\n\n| # | Task | Complexity | Label | Blocks |\n|---|------|------------|-------|--------|\n| 1 | [Title] | s | refined | - |\n| 2 | [Title] | l | needs-refinement | - |\n| 3 | [Title] | m | needs-refinement | #2 |\n\n### Dependency Flow\nTask 1 and Task 2 can start in parallel.\nTask 3 requires Task 2 complete.\n\n### Concerns\n- [Any risks or assumptions needing validation]\n```\n\nAsk user to:\n\n- Approve the breakdown\n- Adjust complexity estimates\n- Identify missing tasks\n- Modify dependencies\n\n**Single review iteration.** If user requests changes, adjust and re-present.\n\n### Create Beads\n\nOnce approved, create task beads:\n\n```bash\n# For each task:\nbd create \"<title>\" \\\n  -t task \\\n  -p <priority> \\\n  --description \"<brief scope>\" \\\n  --json\n\n# Link to parent epic:\nbd dep <task-id> <epic-id> --type parent-child\n\n# Add blocking dependencies:\nbd dep <task-id> <blocking-task-id> --type blocks\n\n# Apply label:\nbd update <task-id> --labels <refined|needs-refinement> --json\n```\n\n**Priority assignment:**\n\n- Inherit from parent epic, OR\n- Tasks with no blockers get higher priority\n- Sequential tasks get decreasing priority\n\n### Summary\n\nAfter creating all beads:\n\n```\n## Breakdown Complete\n\n**Source:** [Epic ID or spec file]\n\n| ID | Title | Complexity | Label |\n|----|-------|------------|-------|\n| bd-101 | [Title] | s | refined |\n| bd-102 | [Title] | l | needs-refinement |\n\n**Dependency Chain:** bd-101 ‚Üí bd-102 ‚Üí bd-103\n\n**Ready to Start:** bd-101 (no blockers, refined)\n\n**Next Steps:**\n- Claim bd-101 to begin implementation\n- Run `/refine bd-102` before claiming (complexity: l)\n```\n\n## External Specs\n\nWhen target is a file path:\n\n1. **Analyze** the spec document\n2. **Determine epic count:**\n   - Single epic? ‚Üí Create one epic bead, then break into tasks\n   - Multiple epics? ‚Üí Create epic beads with `needs-breakdown` label\n3. **Present analysis:**\n\n```\n## Spec Analysis: [filename]\n\nThis specification contains [N] epic(s):\n\n### Epic 1: [Title]\n[Brief scope summary]\nEstimated tasks: ~[N]\n\n**Recommendation:** [Single epic | Multiple epics]\n\nShall I create epic bead(s) and proceed with breakdown?\n```\n\n4. **On approval:** Create epic bead(s), then run breakdown on each\n\n## Error Handling\n\n- **Epic not found:** Exit with \"Bead not found. Run: bd list --type epic\"\n- **Epic already has children:** Warn user, ask whether to add more or abort\n- **bd command failure:** Show error output, suggest manual fix\n\n## Notes\n\n- Keep tasks completable in 1-3 focused sessions\n- Prefer more smaller tasks over fewer larger tasks\n- If a task needs subtasks, it's probably an epic\n- Minimize sequential dependencies to enable parallel work\n",
        "plugins/workflow/commands/compress.md": "---\ndescription: Compress a document or artifact for agent-optimized token density while preserving effectiveness\nargument-hint: \"file path or artifact name to compress\"\n---\n\n# Agent-Optimized Compression\n\nTarget: $ARGUMENTS (or infer from conversation)\n\n## Principles\n\n- **Audience is AI agents**, not humans ‚Äî terse > readable\n- **Preserve all salient details** ‚Äî compress, don't lose information\n- **Eliminate redundancy** ‚Äî say it once, clearly\n- **Compact formatting** ‚Äî bullets, tables, shorthand over prose\n\n## Compression Techniques\n\n| Verbose | Compressed |\n|---------|------------|\n| Paragraphs explaining concepts | Bullet points |\n| Multiple examples showing same thing | One example + rule |\n| \"You should do X because Y\" | \"X\" (or \"X ‚Äî Y\" if reason critical) |\n| Repeated warnings | Single bold statement |\n| Step-by-step with explanations | Numbered steps, terse |\n| \"If X then Y, if A then B, if C then D\" | Decision table |\n\n## Process\n\n1. **Read** the original artifact\n2. **Identify** core requirements vs verbose explanation\n3. **Preserve** critical patterns (WRONG/CORRECT examples, decision logic)\n4. **Remove** redundant examples, obvious explanations, filler\n5. **Restructure** prose ‚Üí bullets/tables\n6. **Verify** no salient detail lost\n\n## Quality Check\n\n- Would an agent following compressed version produce same results?\n- Are all decision points preserved?\n- Are critical warnings/boundaries retained?\n- Is any essential context lost?\n\nIf yes to first three and no to last ‚Üí compression successful.\n",
        "plugins/workflow/commands/refine.md": "# Dialectical Refinement Command\n\nTransform an ambiguous specification into an implementable one through 4 adversarial passes.\n\n## Arguments\n\n$ARGUMENTS - Either a bead ID (e.g., `bd-42`) or a file path to a specification document\n\n## Process\n\n### Setup\n\n1. Identify the target:\n   - If argument looks like a bead ID (matches pattern `bd-\\w+` or `[a-z]+-\\w+`), use `bd show <id> --json` to fetch the spec\n   - If argument is a file path, read the file content\n   - If no argument provided, ask the user what to refine\n2. Set bd workspace context: `bd set-context /path/to/workspace`\n\n### Pass 1: Formalize (The Analyst)\n\n**Role:** Surface ambiguity and missing details.\n\n**Prompt:**\n\n```\nYou are The Analyst reviewing a specification. Your job is to surface ambiguity and missing details.\n\nAnalyze this spec and identify:\n1. Undefined or inconsistently used terms\n2. Missing input/output contracts\n3. What already exists vs genuinely new work (check the codebase)\n4. Missing or weak acceptance criteria\n5. Implicit dependencies\n\nProduce a more detailed version of the spec with gaps explicitly called out.\nDo NOT simplify or cut scope - that's the next pass's job.\n\nSPEC TO ANALYZE:\n<spec>\n{current spec}\n</spec>\n```\n\n**MANDATORY CLARIFICATION CHECKPOINT:**\n\nAfter Pass 1, review the output for major unknowns:\n\n- Scope boundaries unclear (what's included vs excluded?)\n- Architectural choices with trade-offs (which approach to take?)\n- Must-have vs nice-to-have ambiguity\n\nIf ANY major unknowns exist, PAUSE and ask 1-3 focused questions. Do NOT proceed to Pass 2 carrying assumptions silently.\n\n**HITL CLARIFICATION PROTOCOL:**\n\nWhen unknowns require user input, use the `AskUserQuestion` tool:\n- Provide 2-4 concrete options (not open-ended)\n- Include trade-off descriptions for each option\n- \"Other\" is always implicit for custom input\n\nExample question structure:\n```yaml\nquestion: \"Which entities should be affected?\"\noptions:\n  - label: \"Player only\"\n    description: \"Simplest, avoids side effects\"\n  - label: \"All entities\"\n    description: \"Most realistic but may have performance implications\"\n```\n\nStructured questions get clearer answers than assumptions.\n\n### Pass 2: Simplify (The Skeptic)\n\n**Role:** Aggressively cut scope to minimum viable.\n\n**Prompt:**\n\n```\nYou are The Skeptic reviewing a specification. Your job is to aggressively cut scope to minimum viable.\n\nGiven this analyzed spec, identify:\n1. What can be deferred to a future phase?\n2. What's \"nice to have\" vs essential?\n3. What existing functionality approximates this?\n4. What can be hardcoded now and parameterized later?\n5. What's the smallest change that delivers value?\n\nBe ruthless. Cut everything that isn't absolutely essential.\nProduce a dramatically reduced spec. It should feel \"too minimal.\"\n\nSPEC TO SIMPLIFY:\n<spec>\n{Pass 1 output}\n</spec>\n```\n\n### Pass 3: Challenge (The Advocate)\n\n**Role:** Push back on over-simplification.\n\n**Prompt:**\n\n```\nYou are The Advocate reviewing a simplified specification. Your job is to push back on over-simplification.\n\nGiven the original spec and the simplified version, identify:\n1. Did Pass 2 cut something that's actually essential?\n2. What quality (visual, functional, UX) would suffer from these cuts?\n3. Are there cheap additions (small effort, high impact) that were cut?\n4. Did Pass 2 defer something that will be much harder to add later?\n\nRestore scope where cuts were too aggressive.\nProduce a balanced spec - neither bloated nor starved.\n\nORIGINAL SPEC:\n<original>\n{Pass 1 output}\n</original>\n\nSIMPLIFIED SPEC:\n<simplified>\n{Pass 2 output}\n</simplified>\n```\n\n**CONVERGENCE CHECK:**\n\nIf Pass 2 and Pass 3 produce nearly identical output (visual diff shows <10% changes), skip directly to Pass 4. The spec has naturally converged.\n\n### Pass 4: Synthesize (The Judge)\n\n**Role:** Produce the final, actionable specification.\n\n**Prompt:**\n\n```\nYou are The Judge making final decisions on a specification. Your job is to produce an actionable, implementable spec.\n\nGiven all previous passes, you must:\n1. Resolve any remaining debates definitively\n2. Write concrete implementation details (code snippets, file changes, line estimates)\n3. Define clear acceptance criteria (testable, observable)\n4. Estimate complexity: xs, s, m, l, or xl\n5. Document what's explicitly OUT of scope\n\nProduce the final specification in this format:\n\n## Title\n[Concise, specific title]\n\n## Description\n[Scope, approach, file changes with line estimates]\n\n## Design\n[Optional: Architecture notes, code snippets, diagram descriptions]\n\n## Acceptance Criteria\n1. [Testable criterion 1]\n2. [Testable criterion 2]\n...\n\n## Out of Scope\n- [Explicit deferral 1]\n- [Explicit deferral 2]\n...\n\n## Complexity: [xs/s/m/l/xl]\n**Justification:** [Why this estimate? What drives effort?]\n\n## Quality Gate: [GO / GO with caveats / REVISE]\n**Rationale:** [Why this decision? What risks remain?]\n\nPASS 1 (Formalized):\n<formalized>\n{Pass 1 output}\n</formalized>\n\nPASS 2 (Simplified):\n<simplified>\n{Pass 2 output}\n</simplified>\n\nPASS 3 (Challenged):\n<challenged>\n{Pass 3 output}\n</challenged>\n```\n\n### Finalization\n\n1. Present the synthesized spec to the user for approval\n2. If user approves:\n   - Update the bead: `bd update <id> --title \"...\" --description \"...\" --design \"...\" --acceptance \"...\" --json`\n   - Add label `refined`: `bd update <id> --labels refined --json`\n   - If `needs-refinement` label exists, note it should be removed manually\n3. If user requests changes, iterate on Pass 4 output\n\n## Output Format\n\nAfter all passes complete, summarize:\n\n```\n## Refinement Complete\n\n**Target:** [bead ID or file name]\n\n**Key Changes:**\n- [Major scope change 1]\n- [Major scope change 2]\n- [What was cut]\n- [What was restored]\n\n**Final Complexity:** [xs/s/m/l/xl]\n\n**Quality Gate:** [GO / GO with caveats / REVISE]\n\n**Ready for:** [implementation / breakdown into tasks / further discussion]\n```\n\n## Early Exit Heuristics\n\nNot every spec needs all 4 passes:\n\n- **Already detailed?** Skip to Pass 4 (formatting only)\n- **Simple task (xs/s)?** Run 2-pass (Formalize ‚Üí Synthesize)\n- **Convergence detected?** Skip Pass 3 redundancy, go to Pass 4\n\nThe goal is implementable specs, not perfect specs. Adapt the process to fit the work.\n",
        "plugins/workflow/commands/review.md": "---\ndescription: Run parallel architecture, code, and security review with local beads or GitHub PR comments\nargument-hint: \"--pr <number>, --commits <range>, --only <acs>, --min-severity <level>, --skip-beads\"\n---\n\n# Unified Code Review Command\n\nRun parallel architecture, code, and security reviewers for comprehensive peer review. Supports two modes:\n\n- **Local mode** (default): Creates beads for findings + inline report\n- **PR mode** (`--pr N`): Posts findings as GitHub review comments\n\n## Arguments\n\n```\n$ARGUMENTS\n```\n\n| Flag | Description | Default |\n|------|-------------|---------|\n| `--pr <number>` | PR mode - post findings as GH review comments | (local mode) |\n| `--commits <range>` | Explicit commit range (e.g., `HEAD~5..HEAD`) | auto-detect |\n| `--only <letters>` | Filter reviewers: a=arch, c=code, s=security | `acs` (all) |\n| `--min-severity <level>` | Filter output: low\\|medium\\|high\\|critical | all |\n| `--skip-beads` | Local mode only - don't create beads | create beads |\n\n---\n\n## Phase 1: Scope Detection\n\nDetermine what code to review based on mode and arguments.\n\n### PR Mode (`--pr N`)\n\n```bash\n# Get PR metadata\ngh pr view $PR_NUMBER --json title,headRefName,headRefOid,baseRefName,files\n\n# Check if on PR branch\nCURRENT_BRANCH=$(git branch --show-current)\nPR_BRANCH=$(gh pr view $PR_NUMBER --json headRefName -q '.headRefName')\n```\n\n**If not on PR branch:** Use AskUserQuestion to offer checkout:\n- \"Checkout PR branch\" - Run `gh pr checkout $PR_NUMBER`\n- \"Review from current branch\" - Continue without checkout\n- \"Cancel\" - Abort review\n\n```bash\n# Get diff for PR\ngit diff $(gh pr view $PR_NUMBER --json baseRefName -q '.baseRefName')...HEAD\n```\n\n### Local Mode (default)\n\n**Priority order:**\n\n1. **Explicit `--commits <range>`**: Use provided range\n   ```bash\n   git diff $RANGE\n   git log --oneline $RANGE\n   ```\n\n2. **Review tag exists**: If a review tag exists for the current branch, scope = tag to HEAD\n   ```bash\n   BRANCH=$(git branch --show-current)\n   TAG=\"review/${BRANCH}/latest\"\n   if git rev-parse \"$TAG\" >/dev/null 2>&1; then\n     TAG_SHA=$(git rev-parse \"$TAG\")\n     HEAD_SHA=$(git rev-parse HEAD)\n     if [ \"$TAG_SHA\" = \"$HEAD_SHA\" ]; then\n       # No changes since last review ‚Äî report and exit\n       echo \"No changes since last review at $(git log -1 --format=%h $TAG).\"\n       exit\n     fi\n     git log --oneline $TAG_SHA..HEAD\n     git diff --stat $TAG_SHA...HEAD\n   fi\n   ```\n   > **Tip:** To re-review a full feature branch, use `--commits main..HEAD`.\n\n3. **Feature branch/worktree**: If not on `main`/`master`, scope = divergence from main\n   ```bash\n   BASE_BRANCH=$(git rev-parse --abbrev-ref main 2>/dev/null || echo master)\n   git log --oneline $BASE_BRANCH..HEAD\n   git diff --stat $BASE_BRANCH...HEAD\n   ```\n\n4. **On main with unclear scope**: Use AskUserQuestion:\n   - \"Recent commits (HEAD~5)\" - Review last 5 commits\n   - \"Custom range\" - Prompt for range\n   - \"Cancel\" - Abort\n\n**Scope output needed:**\n- Commit range (BASE_SHA..HEAD_SHA)\n- List of changed files\n- Total LOC changed\n- Commit count\n\n---\n\n## Phase 2: Scout (Haiku, Fast)\n\nQuick pre-analysis to route files and provide focused guidance to reviewers.\n\n````\nTask(subagent_type=\"Explore\", model=\"haiku\", prompt=\"\nTASK: Analyze scope for code review preparation\n\nCOMMIT RANGE: <range>\nCHANGED FILES: <file list>\n\nANALYZE:\n1. File types and languages present\n2. Project type signals (game, web app, API, CLI, library)\n3. Architectural patterns visible (ECS, MVC, microservices, etc.)\n4. Key areas of change\n5. Complexity hotspots (large files, concentrated changes)\n6. Lint config for LOC limits (check eslint, golangci-lint, etc.)\n7. Domain classification ‚Äî group changed files into review domains and assign skills.\n   Only split into multiple domains when files span genuinely distinct areas (e.g.,\n   frontend components vs backend API handlers). Single-domain is correct when changes\n   are cohesive.\n\n   SKILL CATALOG (assign matching skills to each domain):\n   | Skill | When to assign |\n   |-------|----------------|\n   | frontend-design:frontend-design | React/Vue/Svelte components, CSS/HTML, UI logic |\n   | dm-lang:typescript-pro | TypeScript (.ts, .tsx) |\n   | dm-lang:go-pro | Go (.go) |\n   | dm-lang:python-pro | Python (.py) |\n   | dm-lang:rust-pro | Rust (.rs) |\n   | dm-game:game-perf | Game loops, update/render, per-frame code |\n   | dm-game:game-design | Game mechanics, player systems |\n   | dm-arch:solid-architecture | Module boundaries, dependency patterns |\n   | dm-arch:data-oriented-architecture | Polymorphic entities, type switches |\n\nOUTPUT (structured):\nPROJECT_TYPE: <e.g., TypeScript game, Go API, React web app>\nLANGUAGES: <comma-separated>\nPATTERNS: <architectural patterns detected>\nCHANGE_AREAS: <key areas modified>\nHOTSPOTS: <files/areas needing extra scrutiny>\nLOC_LIMIT: <from lint config, or 500 default>\nREVIEW_FOCUS: <specific guidance for reviewers>\nREVIEW_DOMAINS:\n- name: <e.g., frontend, backend, game-logic, infra>\n  files: <comma-separated file list>\n  skills: <comma-separated skill names from catalog>\n  focus: <domain-specific review guidance>\n\")\n````\n\n---\n\n## Phase 3: Parallel Reviewers\n\nLaunch reviewers based on `--only` filter. Default is all three (`acs`).\n\n**Parse `--only` flag:**\n- `a` ‚Üí Architecture reviewer\n- `c` ‚Üí Code reviewer\n- `s` ‚Üí Security reviewer\n\nLaunch selected reviewers in a SINGLE message (parallel execution).\n\n### Architecture Reviewer (a)\n\n````\nTask(subagent_type=\"general-purpose\", model=\"sonnet\", description=\"Architecture review\", prompt=\"\nYou are a Senior Architecture Reviewer. Review the following scope for architectural quality.\n\nSCOPE:\n- Commit range: <range>\n- Files: <arch-relevant files from scout>\n- Project type: <from scout>\n- Patterns: <from scout>\n\nREVIEW CHECKLIST:\n1. **SOLID Principles**\n   - Single Responsibility: Can each module's purpose be described in one sentence?\n   - Open/Closed: Are there growing switch/if-else chains?\n   - Dependency Inversion: Are dependencies injected, not hard-coded?\n\n2. **God Object Detection**\n   - LOC limit: <from scout, or 500 default>\n   - Flag files approaching or exceeding limit\n   - Check for classes/modules with too many responsibilities\n\n3. **Module Organization**\n   - Clear dependency direction (core ‚Üí domain ‚Üí application ‚Üí UI)\n   - No circular dependencies\n   - Proper separation of concerns\n\n4. **Project Rules**\n   - Read CLAUDE.md for project-specific guidelines\n   - Check compliance with stated architectural patterns\n\nFOCUS AREAS: <from scout>\nHOTSPOTS: <from scout>\n\nOUTPUT as JSON to stdout:\n```json\n{\n  \"reviewer\": \"architecture\",\n  \"summary\": \"1-2 sentence architectural assessment\",\n  \"findings\": [\n    {\n      \"path\": \"relative/path/to/file.go\",\n      \"line\": 45,\n      \"side\": \"RIGHT\",\n      \"severity\": \"medium\",\n      \"category\": \"SOLID:SRP\",\n      \"body\": \"[Architecture] **medium** - SOLID:SRP\\n\\nThis module handles both X and Y...\"\n    }\n  ]\n}\n```\n\nSeverity guide:\n- critical: Fundamental design flaw\n- high: Significant issue worth blocking\n- medium: Worth addressing, not blocking\n- low: Suggestion for improvement\n\nOnly flag genuine concerns. No nitpicks.\n\")\n````\n\n### Code Reviewer (c) ‚Äî Domain-Aware\n\nBased on the scout's `REVIEW_DOMAINS` output, code review may split into specialized reviewers:\n\n- **Single domain** (or scout returns no domains): One code reviewer for all files, with that domain's skills injected.\n- **Multiple domains**: One code reviewer **per domain**, each scoped to its file subset. All domain reviewers launch in the SAME parallel message as other Phase 3 reviewers.\n\n**Skill injection:** Before constructing reviewer prompts, invoke each domain's listed skills using the Skill tool. Extract key review criteria, anti-patterns, and conventions from each skill. Include the extracted guidance in the corresponding reviewer's prompt under `DOMAIN-SPECIFIC CRITERIA`.\n\nEach domain reviewer uses:\n\n````\nTask(subagent_type=\"feature-dev:code-reviewer\", model=\"sonnet\", description=\"Code review: <domain>\", prompt=\"\nYou are a Senior Code Reviewer specializing in <domain name>.\n\nSCOPE:\n- Commit range: <range>\n- Files: <this domain's files from scout ‚Äî NOT all files>\n- Project type: <from scout>\n- Domain focus: <focus from scout's REVIEW_DOMAINS>\n\nDOMAIN-SPECIFIC CRITERIA:\n<key review criteria, anti-patterns, and conventions extracted from invoked skills>\n\nREVIEW CHECKLIST:\n1. **Bugs & Logic Errors** (confidence ‚â• 80% only)\n   - Null/undefined handling\n   - Race conditions\n   - Off-by-one errors\n   - Resource leaks\n\n2. **Error Handling**\n   - Unchecked errors\n   - Missing error propagation\n   - Swallowed exceptions\n\n3. **Test Coverage**\n   - Changed code without corresponding test changes\n   - Missing edge case coverage\n   - Regression test gaps\n\n4. **Performance**\n   - Allocations in hot paths\n   - Inefficient algorithms\n   - Missing caching where appropriate\n\n5. **Project Conventions**\n   - Read CLAUDE.md for project rules\n   - Naming, imports, error handling per standards\n\nFOCUS AREAS: <from scout>\nHOTSPOTS: <from scout, filtered to this domain's files>\n\nOUTPUT as JSON to stdout:\n```json\n{\n  \"reviewer\": \"code\",\n  \"domain\": \"<domain name>\",\n  \"summary\": \"1-2 sentence code quality assessment for this domain\",\n  \"findings\": [\n    {\n      \"path\": \"relative/path/to/file.go\",\n      \"line\": 112,\n      \"side\": \"RIGHT\",\n      \"severity\": \"high\",\n      \"category\": \"bug\",\n      \"confidence\": 85,\n      \"body\": \"[Code:<domain>] **high** - bug (85% confidence)\\n\\nError from DoThing() is not checked...\"\n    }\n  ]\n}\n```\n\nSeverity guide:\n- critical: Will cause runtime failures\n- high: Likely bug or security issue\n- medium: Code smell or minor bug risk\n- low: Style or minor improvement\n\nOnly report findings with ‚â•80% confidence.\n\")\n````\n\n### Security Reviewer (s)\n\n````\nTask(subagent_type=\"general-purpose\", model=\"sonnet\", description=\"Security review\", prompt=\"\nYou are a Senior Security Reviewer. Review the following scope for security vulnerabilities.\n\nSCOPE:\n- Commit range: <range>\n- Files: <security-relevant files: auth, input handling, *.tf, *.sh, env>\n- Project type: <from scout>\n\nREVIEW CHECKLIST:\n1. **Secrets Handling**\n   - Hardcoded secrets, API keys, tokens\n   - Secrets in logs or error messages\n   - Insecure secret storage\n\n2. **Input Validation**\n   - SQL injection vectors\n   - Command injection\n   - Path traversal\n   - XSS (if web)\n\n3. **Authentication & Authorization**\n   - Auth bypass possibilities\n   - Missing authorization checks\n   - Session handling issues\n\n4. **Infrastructure (if *.tf files)**\n   - Overly permissive IAM\n   - Public exposure of resources\n   - Missing encryption\n\n5. **General**\n   - Insecure dependencies\n   - Debug code in production paths\n   - Information disclosure\n\nFOCUS AREAS: <from scout>\n\nOUTPUT as JSON to stdout:\n```json\n{\n  \"reviewer\": \"security\",\n  \"summary\": \"1-2 sentence security assessment\",\n  \"findings\": [\n    {\n      \"path\": \"relative/path/to/file.go\",\n      \"line\": 78,\n      \"side\": \"RIGHT\",\n      \"severity\": \"critical\",\n      \"category\": \"injection\",\n      \"body\": \"[Security] **critical** - injection\\n\\nSQL query built via string concatenation...\"\n    }\n  ]\n}\n```\n\nSeverity guide:\n- critical: Exploitable vulnerability\n- high: Significant security risk\n- medium: Defense-in-depth issue\n- low: Hardening suggestion\n\nFlag anything exploitable. Be thorough but not paranoid.\n\")\n````\n\n---\n\n## Phase 4: Collect & Process Results\n\nAfter all reviewers complete (including multiple domain code reviewers if split):\n\n1. **Parse JSON output** from each reviewer\n\n2. **Dedupe by file+line**\n   - Key: `${path}:${line}`\n   - Keep finding with higher severity\n   - Merge bodies if from different reviewers or different domain code reviewers\n\n3. **Apply `--min-severity` filter**\n   - If `--min-severity medium`: exclude `low`\n   - If `--min-severity high`: exclude `low`, `medium`\n   - If `--min-severity critical`: only keep `critical`\n\n4. **Sort by severity** (critical ‚Üí high ‚Üí medium ‚Üí low)\n\n---\n\n## Phase 5: Output\n\n### Local Mode\n\n**Create beads** (unless `--skip-beads`):\n\n```bash\n# For each finding:\nbd create --title=\"[<severity>] <short description>\" --type=bug --priority=<0-2> --json\n```\n\nPriority mapping:\n- critical ‚Üí P0\n- high ‚Üí P1\n- medium ‚Üí P2\n- low ‚Üí P3\n\n**Generate inline report:**\n\n````markdown\n## Review: <scope description> (<N> commits, <LOC> across <M> files)\n\n### Verdict: <emoji> <summary>\n\n**Critical** (<count>)\n1. `file:line` - <description> ‚Üí bead <id>\n\n**High** (<count>)\n2. `file:line` - <description> ‚Üí bead <id>\n...\n\n**Medium** (<count>)\n- `file:line` - <description> ‚Üí bead <id>\n...\n\n**Low** (<count>)\n- `file:line` - <description> ‚Üí bead <id>\n...\n\n**Architecture**: <PASS/ISSUES verdict + summary>\n**Code Quality**: <PASS/ISSUES verdict + summary>\n  (if domain-split: one sub-line per domain, e.g., \"  frontend: PASS\", \"  backend: ISSUES - ...\")\n**Security**: <PASS/ISSUES verdict + summary>\n\nRun `bd ready` to see created issues.\n````\n\nVerdict emojis:\n- ‚úÖ PASS - No issues found\n- ‚ö†Ô∏è ISSUES FOUND - Has important/minor issues\n- üö® CRITICAL ISSUES - Has critical issues\n\n**Tag checkpoint** (after report output):\n\n```bash\nBRANCH=$(git branch --show-current)\nTAG=\"review/${BRANCH}/latest\"\ngit tag -f \"$TAG\" HEAD\n```\n\nThis moves (or creates) the review tag to HEAD so the next `/review` starts from here.\n\n### PR Mode\n\n**Step 1: Show preview** of what will be posted (same format as local inline report, without bead references)\n\n**Step 2: Confirm with AskUserQuestion:**\n- \"Post to GitHub\" - Proceed with posting\n- \"Cancel\" - Abort without posting\n\n**Step 3: Post review via GitHub API:**\n\n````bash\n# Get required info\nHEAD_SHA=$(gh pr view $PR_NUMBER --json headRefOid -q '.headRefOid')\nOWNER_REPO=$(gh repo view --json nameWithOwner -q '.nameWithOwner')\n\n# Build review body\nREVIEW_BODY=\"## Automated PR Review\n\n**Architecture:** <arch_summary>\n**Code Quality:** <code_summary>\n**Security:** <security_summary>\n\n---\n*<N> inline comments below*\"\n\n# Build comments array (example)\nCOMMENTS='[\n  {\"path\": \"file.go\", \"line\": 45, \"side\": \"RIGHT\", \"body\": \"...\"}\n]'\n\n# Post review\ngh api repos/$OWNER_REPO/pulls/$PR_NUMBER/reviews \\\n  --method POST \\\n  -f commit_id=\"$HEAD_SHA\" \\\n  -f body=\"$REVIEW_BODY\" \\\n  -f event=\"COMMENT\" \\\n  --input <(echo \"{\\\"comments\\\": $COMMENTS}\")\n````\n\n**Step 4: Confirm success** with link to the review.\n\n---\n\n## Quick Reference\n\n```bash\n# Local review, auto-detect scope (resumes from last review tag if present)\n/dm-work:review\n\n# Local, re-review full feature branch (bypass review tag)\n/dm-work:review --commits main..HEAD\n\n# Local, skip security reviewer\n/dm-work:review --only ac\n\n# Local, specific commits, medium+ only\n/dm-work:review --commits HEAD~5..HEAD --min-severity medium\n\n# Local, exploratory (no beads)\n/dm-work:review --skip-beads\n\n# PR review\n/dm-work:review --pr 123\n\n# PR review, security only\n/dm-work:review --pr 123 --only s\n```\n",
        "plugins/workflow/commands/snapshot.md": "---\ndescription: Create a session snapshot for handoff, recovery, or reset\nargument-hint: (no arguments)\n---\n\nWrite a session snapshot directly to terminal output (NOT to a file).\n\nThe user can copy the snapshot, run /clear, and paste into the fresh session. This works for pre-compaction handoffs, post-compaction recovery, or session resets.\n\nInclude the following with high fidelity:\n\n- current status and next steps\n- any and all info about worktree and branch requirements\n- that, if working on a worktree, you must not complete work on the worktree until i give a greenlight to merge and close\n\nUse the following as a suitable template and level of detail.  If not working in a worktree, you should omit worktree-specific instructions.\n\n---\n\n# Session Snapshot\n\n## Critical Worktree Requirements\n\nYou are working in a git worktree. DO NOT merge until user gives explicit sign-off.\n\nWorktree: (absolute system path)\nBranch: (required git branch name for all changes)\nMain repo: (absolute system path to this repository)\n\nWorkflow:\n1. Implement features in the worktree (if in a worktree)\n1. Run quality gates: (project specific, eg `npm run check`)\n1. Commit to branch\n1. STOP - Inform user feature is ready for testing\n1. WAIT for explicit user approval before ANY merge/cleanup\n1. Only after \"greenlight\" or explicit approval: merge to main, push, cleanup\n\n## Current Epic (or Issue) Status\n\nEpic: (beads epic or issue id and summary)\nDesign doc: (any and all working reference docs, specs, plans, or other context)\n\n| Phase | Task ID | Title                                      | Status            |\n|-------|---------|--------------------------------------------|-------------------|\n| 5     | (beads) | (summary                                 ) | (status)          |\n\nProgress: X/Y subtasks complete\n\n## Recent Commits on feature branch\n\n(...)\n\n## Git State\n\n(...)\n\n## Ready Tasks (note if can be run in parallel with subagents)\n\n(...)\n\n## Key Files\n\n(...)\n\n## Commands\n\n### Quality gates (run before every commit)\n\n(...)\n\n### Beads commands\n\n(...)\n\n## Pre-Merge Checklist\n\nBefore merging to main, consider running:\n- [ ] `/dm-work:review` - Parallel architecture, code, and security review\n\n## Next Steps\n\n(...)\n",
        "plugins/workflow/commands/subagent.md": "---\ndescription: Delegate implementation work to a subagent with appropriate profile, skills, and boundaries\nargument-hint: \"task description, bead ID, or additional context\"\n---\n\n# Subagent Delegation\n\n## 1. Infer Context (PRIORITY ORDER)\n\nArguments: $ARGUMENTS\n\n**If no/incomplete args, check IN ORDER:**\n1. **CONVERSATION FIRST** ‚Äî Did we just discuss next steps? Phase summary? Task list? Bead IDs? **If yes, use that. Skip bd queries.**\n2. In-progress beads ‚Äî `bd list --status=in_progress`\n3. Ready beads ‚Äî `bd ready`\n\n## 2. Auto-Select Profile & Skills\n\n**Profile:** Match task domain ‚Üí eg `game-developer`, `debugger`, `dx-optimizer`, or default `general-purpose`\n\n**Skills:** Infer from language/task:\n- TypeScript ‚Üí `typescript-pro`\n- Bug fix ‚Üí `superpowers:systematic-debugging`\n- New feature ‚Üí TDD\n- Refactoring ‚Üí `solid-architecture`\n- Entities ‚Üí `data-oriented-architecture`\n\n## 3. Generate Prompt\n\n```\nCONTEXT:\n- Bead: <id> - <title>\n- Workspace: <path>\n\nPRIME DIRECTIVE: Grow complexity from simple working systems. Small, verifiable changes.\n\nTASK: <specific description>\n- What to create/modify\n- Expected behavior\n- Patterns to follow (reference files)\n\nSKILLS: <list>\nQUALITY GATE: npm run check\n\nOWNERSHIP:\n- OWN: <files to create/edit>\n- READ ONLY: <reference files>\n\nDELIVERABLES:\n- Working code passing quality gates\n- Do NOT commit or close beads\n- Write detailed report to: history/subagent-reports/<bead-id>.md\n- Return ONLY the minimal response format below\n\nRESPONSE FORMAT (CRITICAL - return ONLY this, nothing else):\n```\nSTATUS: success | partial | failed\nBEAD: <id>\nFILES_CHANGED: <comma-separated list>\nTESTS: <pass count> / <total> | skipped\nSUMMARY: <1-2 sentences max>\nCOMMIT_MSG: <ready-to-use commit message, 1 line>\nREPORT: history/subagent-reports/<bead-id>.md\n```\n```\n\n## 4. Launch\n\n```\nTask(subagent_type=\"<type>\", description=\"<3-5 words>\", prompt=\"<prompt>\")\n```\n\n## 5. After Completion (MINIMAL CONTEXT)\n\n**DO NOT read full report into context unless there's a problem.**\n\n1. Parse the minimal response (STATUS, FILES, SUMMARY)\n2. If STATUS=success:\n   - Trust the subagent ran quality gates\n   - Optionally spot-check 1-2 files with `head -20`\n   - Proceed to commit\n3. If STATUS=partial|failed:\n   - Read `history/subagent-reports/<bead-id>.md` for details\n   - Fix or re-launch\n4. Commit: `git add . && bd close <id> --reason \"...\" && git commit -m \"<COMMIT_MSG from response>\"`\n\n**For thorough review:** Spawn a `superpowers:code-reviewer` subagent pointing at the report file instead of reviewing inline.\n\n## Parallel Safety\n\n- Non-overlapping file ownership per subagent\n- YOU handle: index.ts, package.json, configs\n- Consider worktrees for isolation (`superpowers:using-git-worktrees`)\n",
        "plugins/workflow/commands/subagents.md": "---\ndescription: Orchestrate multiple subagents in parallel or serial based on task dependencies and file ownership\nargument-hint: \"bead IDs, task list, or context for batch work\"\n---\n\n# Multi-Subagent Orchestration\n\n## 1. Identify Work Set (PRIORITY ORDER)\n\nArguments: $ARGUMENTS\n\n**If no args, check IN ORDER:**\n\n1. **CONVERSATION FIRST** ‚Äî Scan for \"Next Ready Work:\", phase summaries, task tables, bead IDs. **If found, use that list. Do NOT query bd.**\n\n2. **Only if conversation unclear:** `bd list --status=in_progress` then `bd ready`\n\n**WRONG:** Running `bd ready` when conversation just listed \"Next Ready Work (Phase 3): ywrw.4.8, ywrw.4.9...\"\n**CORRECT:** Extract work set from conversation, skip bd queries.\n\n## 2. Analyze Parallelizability\n\n| Condition | Strategy |\n|-----------|----------|\n| Independent files, no deps | PARALLEL |\n| Same files touched | SERIAL |\n| Dependency chain | SERIAL in order |\n| Mixed | HYBRID (parallel sets, serial joins) |\n\n## 3. Map Ownership\n\n**For parallel, you MUST:**\n- Assign exclusive files to each subagent\n- Reserve for yourself: `index.ts`, `package.json`, configs, any shared files\n- Consider worktrees if >2-3 parallel or risk of conflicts\n\nExample:\n```\nSubagent A: OWN src/physics/asteroid.ts, READ src/entities/asteroid.ts\nSubagent B: OWN src/rendering/asteroid.ts, READ src/entities/asteroid.ts\nOrchestrator: src/entities/index.ts (merge exports after)\n```\n\n## 4. Launch\n\n**PARALLEL:** Single message, multiple Task calls:\n```\nTask(...) Task(...) Task(...)\n```\n\n**SERIAL:** One at a time, commit between if needed.\n\n**HYBRID:** Parallel independent sets ‚Üí merge ‚Üí serial dependent tasks.\n\nEach subagent prompt follows `/subagent` template with:\n- Explicit OWN/READ boundaries\n- Report file path: `history/subagent-reports/<bead-id>.md`\n- RESPONSE FORMAT requirement (minimal structured output)\n\n## 5. Collect Results (CONTEXT-EFFICIENT)\n\n**As each subagent completes:**\n\n1. **Parse minimal response only** - STATUS, BEAD, FILES_CHANGED, SUMMARY, COMMIT_MSG\n2. **DO NOT read reports into context** - they exist for debugging, not routine review\n3. **Track in a simple table:**\n\n```\n| Bead | Status | Files | Summary |\n|------|--------|-------|---------|\n| n7vy.2 | success | game.ts, world.ts | Consolidated mines to World |\n| 1s08.2 | success | game-renderer*.ts | Split into 5 sub-renderers |\n```\n\n4. **If all success:** Proceed to merge\n5. **If any failed:** Read ONLY that report, fix or re-launch\n\n## 6. Merge & Complete (BATCH)\n\n1. Merge shared files yourself (barrel exports, etc.)\n2. Run quality gates ONCE: `npm run check`\n3. Batch close and commit:\n   ```bash\n   bd close <id1> <id2> <id3> --reason \"Parallel implementation\"\n   git add .\n   git commit -m \"feat: <combined summary from COMMIT_MSGs>\"\n   ```\n\n## 7. Handle Failures\n\n- Don't let one failure block parallel work\n- Read failed subagent's report file for details\n- Fix or re-launch failed subagent only\n- If unexpected conflicts: revert one, re-run serial\n\n## 8. Optional: Batch Code Review\n\nIf thorough review needed before commit:\n```\nTask(subagent_type=\"superpowers:code-reviewer\", prompt=\"\nReview changes from parallel subagents:\n- Reports: history/subagent-reports/n7vy.2.md, history/subagent-reports/1s08.2.md\n- Focus: integration issues, missed edge cases\n- Return: PASS/FAIL + issues list (if any)\n\")\n```\n\nThis keeps review work OUT of orchestrator context.\n",
        "plugins/workflow/commands/triage.md": "---\ndescription: Triage PR review comments - accept to create beads, reject with optional reply\nargument-hint: \"--pr <number>\"\n---\n\n# Triage PR Review Comments\n\nPull review comments from a GitHub PR, triage (accept/reject), and create beads for accepted items. Counterpart to `/dm-work:review --pr` which *gives* reviews.\n\n## Arguments\n\n```\n$ARGUMENTS\n```\n\n| Flag | Description | Default |\n|------|-------------|---------|\n| `--pr <number>` | Specify PR to triage | auto-detect from branch |\n\n---\n\n## Phase 1: Resolve PR\n\n**Auto-detect from current branch:**\n\n```bash\nPR_NUMBER=$(gh pr view --json number -q '.number' 2>/dev/null)\n```\n\n**Or use explicit `--pr N`** from arguments.\n\n**If no PR found:** Fail with \"No PR found for current branch. Use --pr <number> to specify.\"\n\n---\n\n## Phase 2: Branch Check\n\n```bash\n# Check for dirty workspace\nif ! git diff --quiet || ! git diff --cached --quiet; then\n  FAIL: \"Workspace dirty. Stash or commit changes first.\"\nfi\n\n# Get PR branch\nPR_BRANCH=$(gh pr view $PR_NUMBER --json headRefName -q '.headRefName')\nCURRENT_BRANCH=$(git branch --show-current)\n```\n\n**If not on PR branch:** Use AskUserQuestion:\n- \"Checkout PR branch\" - Run `gh pr checkout $PR_NUMBER`\n- \"Continue from current branch\" - Proceed (user knows what they're doing)\n- \"Cancel\" - Abort\n\n---\n\n## Phase 3: Fetch Comments\n\n```bash\n# Get repo info\nOWNER_REPO=$(gh repo view --json nameWithOwner -q '.nameWithOwner')\n\n# Get PR title\nPR_TITLE=$(gh pr view $PR_NUMBER --json title -q '.title')\n\n# Fetch all review comments\ngh api repos/$OWNER_REPO/pulls/$PR_NUMBER/comments\n```\n\n**Filter for unresolved comments:**\n- Exclude comments that are replies (`in_reply_to_id != null`)\n- Exclude resolved threads (check via review threads API if needed)\n\n**Parse each comment for:**\n- `id` - Comment ID for replies\n- `user.login` - Reviewer username\n- `path` - File path\n- `line` or `original_line` - Line number\n- `body` - Comment text\n- Inferred severity (parse `**critical**`, `**high**`, etc. from body)\n\n---\n\n## Phase 4: Display & Triage\n\n**Display summary:**\n\n````markdown\n## PR #$PR_NUMBER: $PR_TITLE ($TOTAL comments, $UNRESOLVED unresolved)\n\n| # | Reviewer | File | Severity | Summary |\n|---|----------|------|----------|---------|\n| 1 | @alice | auth/login.go:45 | high | Missing error check on token validation |\n| 2 | @alice | auth/login.go:78 | medium | Consider rate limiting here |\n| 3 | @bob | db/user.go:23 | low | Naming: userRecord ‚Üí user |\n...\n\n**Accepting all $UNRESOLVED comments by default.**\n````\n\n**If no unresolved comments:** Report \"No unresolved comments to triage.\" and exit.\n\n**AskUserQuestion - select comments to reject:**\n\nPrioritize low-value comments as rejection candidates:\n1. Low severity\n2. Style/naming suggestions (detect keywords: \"naming\", \"style\", \"consider\", \"might\")\n3. If many comments, show up to 4 most likely rejects\n\n```\nWhich comments to reject? (All others will become beads)\n\n‚óã #3 - @bob: Naming: userRecord ‚Üí user [low]\n‚óã #7 - @alice: Add comment explaining this logic [low]\n‚óã #9 - @bob: Consider using constants [low]\n‚óã None - accept all\n```\n\nUse `multiSelect: true` to allow rejecting multiple.\n\n---\n\n## Phase 5: Handle Rejections\n\n**For each rejected comment, prompt for reply:**\n\n```\nReply to @bob about rejecting \"#3 - Naming: userRecord ‚Üí user\"?\n\n‚óã \"Intentional - matches domain terminology\" (Recommended)\n‚óã \"Out of scope for this PR\"\n‚óã \"Will address in follow-up\"\n‚óã No reply\n```\n\nUser can also select \"Other\" for custom reply.\n\n**Canned response selection logic:**\n- Naming/style comments ‚Üí \"Intentional - matches domain terminology\"\n- Scope suggestions ‚Üí \"Out of scope for this PR\"\n- Default ‚Üí \"Will address in follow-up\"\n\n**Post reply if not \"No reply\":**\n\n```bash\ngh api repos/$OWNER_REPO/pulls/$PR_NUMBER/comments/$COMMENT_ID/replies \\\n  --method POST \\\n  -f body=\"$REPLY_TEXT\"\n```\n\n---\n\n## Phase 6: Create Beads\n\n**For each accepted comment:**\n\n```bash\n# Infer severity from comment body\n# Look for patterns: **critical**, **high**, **medium**, **low**\n# Or [Severity] prefix from /dm-work:review format\n# Default to medium (P2) if not found\n\n# Infer type\n# bug/security keywords ‚Üí bug\n# else ‚Üí task\n\nbd create \\\n  --title=\"[$SEVERITY] $SUMMARY\" \\\n  --description=\"From PR #$PR_NUMBER review by @$REVIEWER\n\n> $FILE:$LINE\n> $COMMENT_BODY\n\n$COMMENT_URL\" \\\n  --type=$TYPE \\\n  --priority=$PRIORITY \\\n  --external-ref=\"gh-pr-$PR_NUMBER-r$COMMENT_ID\" \\\n  --json\n```\n\n**Priority mapping:**\n\n| Severity | Priority |\n|----------|----------|\n| critical | 0 |\n| high | 1 |\n| medium | 2 |\n| low | 3 |\n| (none) | 2 |\n\n**Track created bead IDs for summary.**\n\n---\n\n## Phase 7: Summary\n\n````markdown\n## Triage Complete: PR #$PR_NUMBER\n\n**Accepted:** $ACCEPTED_COUNT comments ‚Üí $BEAD_COUNT beads created\n**Rejected:** $REJECTED_COUNT comments ($REPLIED_COUNT replies posted)\n\n| Bead ID | Priority | Summary |\n|---------|----------|---------|\n| abc-123 | P1 | Missing error check on token validation |\n| def-456 | P2 | Consider rate limiting |\n| ghi-789 | P2 | SQL injection risk |\n...\n\n**Rejected:**\n- #3 @bob: Naming suggestion ‚Üí \"Intentional - matches domain terminology\"\n- #7 @alice: Add comment ‚Üí (no reply)\n\nRun `bd ready` to see work queue.\n````\n\n---\n\n## Quick Reference\n\n```bash\n# Triage current branch's PR\n/dm-work:triage\n\n# Triage specific PR\n/dm-work:triage --pr 123\n```\n",
        "plugins/workflow/skills/brainstorming/SKILL.md": "---\nname: brainstorming\ndescription: Use before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores user intent, requirements, and design before implementation through collaborative dialogue.\n---\n\n# Brainstorming Ideas Into Designs\n\nTurn ideas into fully formed designs through natural collaborative dialogue.\n\n**Process:** Understand context ‚Üí Ask questions one at a time ‚Üí Explore approaches ‚Üí Present design incrementally ‚Üí Validate each section.\n\n---\n\n## Phase 1: Understanding\n\n### Check Project Context\n\nBefore asking questions:\n- Review relevant files, docs, recent commits\n- Understand existing patterns and constraints\n- Note what's already built that relates\n\n### Ask Questions\n\n**Rules:**\n- One question per message\n- Prefer multiple choice when possible\n- Open-ended when exploration needed\n- Break complex topics into multiple questions\n\n**Focus on:**\n- Purpose: What problem does this solve?\n- Constraints: What must it work with?\n- Success criteria: How do we know it's done?\n\n---\n\n## Phase 2: Exploring Approaches\n\nOnce you understand the goal:\n\n1. Propose 2-3 different approaches\n2. Include trade-offs for each\n3. Lead with your recommendation and why\n4. Let user choose or refine\n\n**Example:**\n```\nI see three approaches:\n\n1. **Component-based** (recommended) - Fits your existing pattern,\n   easiest to test, but more files.\n\n2. **Inline** - Fewer files, but harder to reuse and test.\n\n3. **Hook-based** - Most flexible, but adds complexity you may not need.\n\nI'd go with #1 because [reasoning]. Thoughts?\n```\n\n---\n\n## Phase 3: Presenting Design\n\nOnce approach is chosen:\n\n1. Present in sections (200-300 words each)\n2. After each section: \"Does this look right so far?\"\n3. Be ready to revise if something doesn't fit\n\n**Cover:**\n- Architecture / component structure\n- Data flow\n- Error handling\n- Testing approach\n- Edge cases\n\n---\n\n## Phase 4: After Design\n\n### Document\n\nWrite validated design to:\n```\ndocs/plans/YYYY-MM-DD-<topic>-design.md\n```\n\nCommit to git.\n\n### Implementation (if continuing)\n\nAsk: \"Ready to set up for implementation?\"\n\nThen:\n1. Use `dm-work:worktrees` to create isolated workspace\n2. Create beads for implementation tasks\n3. Use `dm-work:orchestrator` patterns for delegation\n\n---\n\n## Key Principles\n\n| Principle | Why |\n|-----------|-----|\n| One question at a time | Don't overwhelm |\n| Multiple choice preferred | Easier to answer |\n| YAGNI ruthlessly | Remove unnecessary features |\n| Explore alternatives | Always 2-3 approaches before settling |\n| Incremental validation | Present design in sections |\n| Be flexible | Go back when something doesn't fit |\n\n---\n\n## Anti-Patterns\n\n| Don't | Do Instead |\n|-------|------------|\n| Jump to implementation | Understand first, design second |\n| Ask 5 questions at once | One question per message |\n| Present monolithic design | Break into 200-300 word sections |\n| Skip trade-off discussion | Always propose 2-3 approaches |\n| Assume you understand | Validate understanding with user |\n\n---\n\n## Quick Reference\n\n```\n1. Check project context (files, docs, commits)\n2. Ask questions one at a time (prefer multiple choice)\n3. Propose 2-3 approaches with trade-offs\n4. Present design in sections, validate each\n5. Write to docs/plans/YYYY-MM-DD-<topic>-design.md\n6. If implementing: worktree ‚Üí beads ‚Üí orchestrate\n```\n",
        "plugins/workflow/skills/cli-tools/SKILL.md": "---\nname: cli-tools\ndescription: Power CLI tools (fd, rg, jq, yq, sd, xargs, bat, delta) for when built-in tools are insufficient. Use for complex file ops, data manipulation, or parallel execution.\n---\n\n# CLI Power Tools\n\nUse built-in tools first (Read, Grep, Glob, Write, Edit). Fall back to these when built-ins hit limits.\n\n---\n\n## fd (find replacement)\n\n**When to use:** Complex exclusions, type filters, exec actions, or when Glob patterns get unwieldy.\n\n```bash\n# Find by extension with exclusions\nfd -e ts -E node_modules -E dist\n\n# Find and execute\nfd -e test.ts -x npm test {}\n\n# Find directories only\nfd -t d components\n\n# Find with size filter\nfd -e log -S +10M\n\n# Find modified in last hour\nfd -e ts --changed-within 1h\n```\n\n**Glob equivalent that fd improves:**\n```bash\n# Instead of multiple Glob calls with exclusions\nfd -e ts -E __tests__ -E __mocks__ -E node_modules\n```\n\n---\n\n## rg (ripgrep)\n\n**When to use:** Multiline patterns, PCRE2 regex, replace mode, or complex context needs.\n\n```bash\n# Multiline search (built-in Grep doesn't span lines well)\nrg -U 'struct \\{[\\s\\S]*?impl'\n\n# PCRE2 lookahead/lookbehind\nrg -P '(?<=fn\\s)\\w+(?=\\()'\n\n# Search and replace (preview)\nrg 'oldName' -r 'newName'\n\n# Search with file type\nrg -t rust 'async fn'\n\n# Inverse match (lines NOT matching)\nrg -v 'TODO|FIXME'\n\n# JSON output for parsing\nrg --json 'pattern' | jq '.data.lines.text'\n```\n\n**Grep equivalent that rg improves:**\n```bash\n# Complex multiline with context\nrg -U -A5 -B5 'impl.*for.*\\{[\\s\\S]*?\\}'\n```\n\n---\n\n## jq (JSON processor)\n\n**When to use:** Extracting, transforming, or filtering JSON beyond simple access.\n\n```bash\n# Extract nested field\ncat data.json | jq '.config.database.host'\n\n# Filter array\njq '.items[] | select(.status == \"active\")' data.json\n\n# Transform structure\njq '{name: .title, id: .uuid}' item.json\n\n# Merge files\njq -s '.[0] * .[1]' base.json override.json\n\n# Pretty print with sorting\njq -S '.' messy.json\n\n# Raw output (no quotes)\njq -r '.version' package.json\n\n# Update in place (with sponge or temp file)\njq '.version = \"2.0.0\"' package.json > tmp && mv tmp package.json\n```\n\n**Common patterns:**\n```bash\n# Get all keys\njq 'keys' object.json\n\n# Length of array\njq '.items | length' data.json\n\n# Unique values\njq '[.items[].category] | unique' data.json\n```\n\n---\n\n## yq (YAML processor)\n\n**When to use:** YAML manipulation - CI configs, k8s manifests, docker-compose, GitHub Actions.\n\n```bash\n# Extract field\nyq '.services.web.image' docker-compose.yml\n\n# Update value\nyq -i '.version = \"2.0.0\"' config.yml\n\n# Add to array\nyq -i '.steps += [{\"name\": \"test\", \"run\": \"npm test\"}]' .github/workflows/ci.yml\n\n# Convert YAML to JSON\nyq -o json config.yml\n\n# Convert JSON to YAML\nyq -P config.json\n\n# Merge files\nyq '. * load(\"override.yml\")' base.yml\n\n# Query multiple docs (---)\nyq 'select(.kind == \"Deployment\")' k8s-manifests.yml\n```\n\n**Common CI/k8s patterns:**\n```bash\n# Get all image references in k8s\nyq '.spec.containers[].image' deployment.yml\n\n# Update image tag\nyq -i '.spec.containers[0].image = \"app:v2\"' deployment.yml\n\n# Add env var to GitHub Action\nyq -i '.env.NODE_ENV = \"test\"' .github/workflows/ci.yml\n```\n\n---\n\n## sd (sed replacement)\n\n**When to use:** Find/replace in files. Much simpler syntax than sed.\n\n```bash\n# Simple replace (stdout)\nsd 'oldName' 'newName' file.ts\n\n# In-place replace\nsd -i 'oldName' 'newName' file.ts\n\n# Regex with capture groups\nsd 'fn (\\w+)\\(' 'function $1(' file.js\n\n# Replace across multiple files\nfd -e ts | xargs sd -i 'oldImport' 'newImport'\n\n# Multiline (use -s for string mode)\nsd -s 'line1\\nline2' 'replacement' file.txt\n\n# Preview changes (no -i flag)\nsd 'pattern' 'replacement' file.ts\n```\n\n**vs sed:**\n```bash\n# sed (arcane)\nsed -i 's/old/new/g' file.txt\nsed -i 's/\\(capture\\)/\\1_suffix/g' file.txt\n\n# sd (readable)\nsd -i 'old' 'new' file.txt\nsd -i '(capture)' '${1}_suffix' file.txt\n```\n\n---\n\n## xargs (parallel execution)\n\n**When to use:** Run commands on multiple inputs, especially in parallel.\n\n```bash\n# Basic usage\nfd -e ts | xargs eslint\n\n# Parallel execution (-P = processes)\nfd -e ts | xargs -P4 -I{} eslint {}\n\n# With placeholder\nfd -e test.ts | xargs -I{} npm test -- {}\n\n# Null-delimited (handles spaces in names)\nfd -0 -e ts | xargs -0 wc -l\n\n# Limit batch size (-n)\nfd -e ts | xargs -n10 eslint\n\n# Prompt before each (interactive)\nfd -e log | xargs -p rm\n```\n\n**Common patterns:**\n```bash\n# Parallel type check\nfd -e ts | xargs -P$(nproc) -I{} tsc --noEmit {}\n\n# Batch git add\nfd -e ts --changed-within 1h | xargs git add\n\n# Parallel image optimization\nfd -e png | xargs -P4 -I{} optipng {}\n```\n\n---\n\n## bat (better cat)\n\n**When to use:** Quick file preview with syntax highlighting and line numbers.\n\n```bash\n# View file with syntax highlighting\nbat src/index.ts\n\n# Show line range\nbat -r 50:100 src/index.ts\n\n# Show non-printable characters\nbat -A file.txt\n\n# Plain output (no decorations)\nbat -p file.ts\n\n# Diff two files\nbat --diff file1.ts file2.ts\n\n# As pager for other commands\ngit diff | bat\n\n# Multiple files\nbat src/*.ts\n```\n\n**Note:** Read tool covers most needs, but bat useful for quick multi-file preview with highlighting.\n\n---\n\n## delta (better git diff)\n\n**When to use:** Readable git diffs with syntax highlighting and side-by-side view.\n\n```bash\n# Use as git pager (add to ~/.gitconfig)\n# [core]\n#   pager = delta\n# [delta]\n#   side-by-side = true\n\n# Or one-off\ngit diff | delta\n\n# Side by side\ngit diff | delta -s\n\n# With line numbers\ngit diff | delta -n\n\n# Compare files directly\ndelta file1.ts file2.ts\n\n# Show only file names\ngit diff --name-only | delta\n```\n\n**Config (~/.gitconfig):**\n```ini\n[core]\n    pager = delta\n[interactive]\n    diffFilter = delta --color-only\n[delta]\n    navigate = true\n    side-by-side = true\n    line-numbers = true\n```\n\n---\n\n## Decision Guide\n\n| Need | Tool |\n|------|------|\n| Find files by name/pattern | Glob first, fd if complex |\n| Search file contents | Grep first, rg if multiline/pcre2 |\n| Read/edit files | Read/Edit/Write always |\n| JSON manipulation | jq |\n| YAML manipulation | yq |\n| Find/replace in files | Edit first, sd for bulk/regex |\n| Find + action | fd -x |\n| Parallel execution | xargs -P |\n| Search + replace preview | rg -r or sd (no -i) |\n| File preview with highlighting | bat |\n| Readable git diffs | delta |\n\n---\n\n## Installation\n\n```bash\n# macOS\nbrew install fd ripgrep jq yq sd bat git-delta\n\n# Ubuntu/Debian\napt install fd-find ripgrep jq bat\n# yq, sd, delta: install via cargo, npm, or download binaries\n# Note: fd is 'fdfind' on Debian, alias it: alias fd=fdfind\n\n# With mise/cargo\nmise use -g fd ripgrep jq yq\ncargo install sd\n```\n",
        "plugins/workflow/skills/debugging/SKILL.md": "---\nname: debugging\ndescription: Use when encountering any bug, test failure, or unexpected behavior, before proposing fixes. Requires root cause investigation before any fix attempts. Random fixes waste time and create new bugs.\n---\n\n# Systematic Debugging\n\nRandom fixes waste time and create new bugs. Quick patches mask underlying issues.\n\n**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.\n\n---\n\n## The Iron Law\n\n```\nNO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST\n```\n\nIf you haven't completed Phase 1, you cannot propose fixes.\n\n---\n\n## When to Use\n\n**Any technical issue:**\n- Test failures\n- Bugs in production\n- Unexpected behavior\n- Performance problems\n- Build failures\n- Integration issues\n\n**Especially when:**\n- Under time pressure (emergencies make guessing tempting)\n- \"Just one quick fix\" seems obvious\n- You've already tried multiple fixes\n- Previous fix didn't work\n\n---\n\n## The Four Phases\n\nComplete each phase before proceeding to the next.\n\n### Phase 1: Root Cause Investigation\n\n**BEFORE attempting ANY fix:**\n\n1. **Read Error Messages Carefully**\n   - Don't skip past errors or warnings\n   - Read stack traces completely\n   - Note line numbers, file paths, error codes\n\n2. **Reproduce Consistently**\n   - Can you trigger it reliably?\n   - What are the exact steps?\n   - If not reproducible ‚Üí gather more data, don't guess\n\n3. **Check Recent Changes**\n   - Git diff, recent commits\n   - New dependencies, config changes\n   - Environmental differences\n\n4. **Gather Evidence in Multi-Component Systems**\n   ```\n   For EACH component boundary:\n     - Log what data enters\n     - Log what data exits\n     - Verify environment/config propagation\n\n   Run once to gather evidence showing WHERE it breaks\n   THEN investigate that specific component\n   ```\n\n5. **Trace Data Flow**\n   - Where does bad value originate?\n   - What called this with bad value?\n   - Keep tracing up until you find the source\n   - Fix at source, not at symptom\n\n### Phase 2: Pattern Analysis\n\n1. **Find Working Examples**\n   - Locate similar working code in same codebase\n   - What works that's similar to what's broken?\n\n2. **Compare Against References**\n   - Read reference implementation COMPLETELY\n   - Don't skim - read every line\n\n3. **Identify Differences**\n   - What's different between working and broken?\n   - List every difference, however small\n\n### Phase 3: Hypothesis and Testing\n\n1. **Form Single Hypothesis**\n   - \"I think X is the root cause because Y\"\n   - Be specific, not vague\n\n2. **Test Minimally**\n   - Make the SMALLEST possible change\n   - One variable at a time\n   - Don't fix multiple things at once\n\n3. **Verify Before Continuing**\n   - Did it work? Yes ‚Üí Phase 4\n   - Didn't work? Form NEW hypothesis\n   - DON'T add more fixes on top\n\n### Phase 4: Implementation\n\n1. **Create Failing Test Case**\n   - Simplest possible reproduction\n   - MUST have before fixing\n\n2. **Implement Single Fix**\n   - Address the root cause\n   - ONE change at a time\n   - No \"while I'm here\" improvements\n\n3. **Verify Fix**\n   - Test passes now?\n   - No other tests broken?\n\n4. **If 3+ Fixes Failed: Question Architecture**\n\n   Pattern indicating architectural problem:\n   - Each fix reveals new problem in different place\n   - Fixes require massive refactoring\n   - Each fix creates new symptoms elsewhere\n\n   **STOP and question fundamentals before attempting more fixes**\n\n---\n\n## Red Flags - STOP and Return to Phase 1\n\nIf you catch yourself thinking:\n- \"Quick fix for now, investigate later\"\n- \"Just try changing X and see if it works\"\n- \"It's probably X, let me fix that\"\n- \"I don't fully understand but this might work\"\n- \"Here are the main problems: [lists fixes without investigation]\"\n- Proposing solutions before tracing data flow\n- \"One more fix attempt\" (when already tried 2+)\n\n---\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Issue is simple\" | Simple issues have root causes too |\n| \"Emergency, no time\" | Systematic is FASTER than thrashing |\n| \"Just try this first\" | First fix sets the pattern |\n| \"Multiple fixes saves time\" | Can't isolate what worked |\n| \"I see the problem\" | Seeing symptoms ‚â† understanding root cause |\n| \"One more attempt\" (after 2+) | 3+ failures = architectural problem |\n\n---\n\n## Quick Reference\n\n| Phase | Key Activities | Success Criteria |\n|-------|---------------|------------------|\n| **1. Root Cause** | Read errors, reproduce, check changes, gather evidence | Understand WHAT and WHY |\n| **2. Pattern** | Find working examples, compare | Identify differences |\n| **3. Hypothesis** | Form theory, test minimally | Confirmed or new hypothesis |\n| **4. Implementation** | Create test, fix, verify | Bug resolved, tests pass |\n\n---\n\n## Real-World Impact\n\n| Approach | Time to Fix | First-Time Fix Rate | New Bugs |\n|----------|-------------|---------------------|----------|\n| Systematic | 15-30 min | 95% | Near zero |\n| Random fixes | 2-3 hours | 40% | Common |\n",
        "plugins/workflow/skills/dialectical-refinement/SKILL.md": "---\nname: dialectical-refinement\ndescription: Transform ambiguous specs into implementable work items through adversarial refinement. This skill should be used PROACTIVELY when receiving specs, claiming epics, or starting work on complex (l/xl) tasks. Use /breakdown for spec‚Üítasks decomposition, /refine for sharpening individual items.\n---\n\n# Dialectical Refinement\n\n## Overview\n\nSurface hidden complexity before implementation through adversarial tension. A single reviewer tends toward over-engineering or over-simplification; opposing passes converge on correct scope.\n\n## When to Use (Proactive Triggers)\n\n| Trigger | Action |\n|---------|--------|\n| Receiving external spec/PRD | `/breakdown <spec.md>` |\n| Claiming an epic | `/refine` then `/breakdown` |\n| Starting l/xl complexity task | `/refine <task-id>` |\n| Spec feels \"clear but big\" | Run refinement‚Äîhidden complexity likely |\n\n## Protected Categories\n\nBefore any simplification, identify items that must NOT be cut:\n\n| Category | Description | Examples |\n|----------|-------------|----------|\n| **Core Workflow** | The minimal viable loop | CRUD operations, essential commands |\n| **Agent Primitives** | Flags/features enabling agent autonomy | `--json`, `--range`, `--auto`, structured output |\n| **User-Requested Features** | Explicitly requested by user | Items called out in original spec |\n| **Token Efficiency** | Batch operations, context injection | Bulk APIs, pagination, streaming |\n| **Structured Output** | Machine-parseable output | JSON on all commands, typed responses |\n\nTag protected items early. The Proposer phase should not propose cutting them.\n\n## The 5-Phase Process\n\n### Phase 1: Formalize (Analyst)\n**Goal:** Surface ambiguity and tag protected items.\n- What terms are undefined?\n- What's the input/output contract?\n- What exists vs. genuinely new?\n- What are acceptance criteria?\n- What dependencies are implicit?\n- **Which items fall into protected categories?**\n\n**Output:** Detailed spec with gaps called out and protected items tagged.\n\n**Checkpoint:** If significant unknowns remain (scope, architecture, must-have vs nice-to-have), ask 1-3 focused questions before proceeding.\n\n**HITL Clarification Protocol:** When asking users, use `AskUserQuestion` with 2-4 concrete options and trade-offs (not open-ended). Structured questions prevent silent assumptions.\n\n### Phase 2: Propose Cuts (Proposer)\n**Goal:** Identify candidates for simplification‚Äîpropose, don't execute.\n\nThe Proposer suggests cuts with confidence levels. It does NOT produce a reduced spec; it produces a list of proposals for the Advocate to argue against.\n\n**Output Format:**\n\n```markdown\n## PROTECTED (never cut)\n- [List items from protected categories with rationale]\n\n## PROPOSED CUTS\n\n### Strong Cut Candidates (high confidence)\n- `<item>` ‚Äî [Rationale: clearly deferrable or unnecessary]\n\n### Moderate Cut Candidates (medium confidence)\n- `<item>` ‚Äî [Rationale: could defer, but note trade-offs]\n\n### Weak Cut Candidates (low confidence, protect carefully)\n- `<item>` ‚Äî [Rationale: seems optional, but may have hidden value]\n```\n\n**Key Constraint:** Proposer argues for cuts but does NOT execute them. The Advocate reviews each proposal.\n\n### Phase 3: Challenge (Advocate)\n**Goal:** Argue against proposed cuts, restore what matters.\n\nThe Advocate receives the Proposer's proposals and responds to EACH one:\n\n**Output Format:**\n\n```markdown\n## ADVOCATE RESPONSES\n\n### Strong Cuts ‚Äî Agreed\n- `<item>` ‚Äî Agree: [brief reason]\n\n### Strong Cuts ‚Äî Contested\n- `<item>` ‚Äî Contest: [why this should stay]\n\n### Moderate Cuts ‚Äî Agreed\n- `<item>` ‚Äî Agree, defer to phase 2\n\n### Moderate Cuts ‚Äî Contested\n- `<item>` ‚Äî Contest: [hidden value / future cost of adding later]\n\n### Weak Cuts ‚Äî Recommendations\n- `<item>` ‚Äî [Keep/Cut with reasoning]\n\n### Cheap Additions Missed\n- [Items not in spec that are low-effort, high-impact]\n```\n\n**Key Constraint:** Advocate argues from the proposals, not from memory. Every proposal gets a response.\n\n### Phase 4: Scope Lock (Checkpoint)\n**Goal:** Verify essential scope before synthesis.\n\nBefore the Judge produces final output, verify:\n\n| Check | Status | Action if Failed |\n|-------|--------|------------------|\n| Core workflow commands preserved | ‚úÖ/‚ùå | Restore from Phase 1 |\n| Agent primitives preserved | ‚úÖ/‚ùå | Restore `--json`, ranges, etc. |\n| User-requested features addressed | ‚úÖ/‚ùå | Review with user |\n| Structured output on all commands | ‚úÖ/‚ùå | Add missing |\n| Token efficiency considered | ‚úÖ/‚ùå | Review batch/bulk operations |\n\n**\"Too Thin\" Indicators:**\n- Fewer than 5 commands/features for a system? ‚ö†Ô∏è\n- Removed structured output (`--json`)? ‚ö†Ô∏è\n- Removed range/anchor/batch capabilities? ‚ö†Ô∏è\n- All m+ tasks cut to xs/s? ‚ö†Ô∏è\n\nIf 2+ indicators trigger, return to Phase 3 with guidance to restore scope.\n\n### Phase 5: Synthesize (Judge)\n**Goal:** Produce actionable, externally-reviewable spec with quality gates.\n\n- Resolve remaining Proposer/Advocate debates\n- Write concrete implementation details\n- Define testable acceptance criteria\n- Document OUT OF SCOPE explicitly\n- **Ensure spec is standalone-reviewable** (see below)\n\n**Standalone Context Requirement:**\n\nThe final spec must be reviewable by an external agent without access to conversation history. Include:\n\n1. **Introduction** ‚Äî What this spec is and why it exists (2-3 sentences)\n2. **Context appendix** (if spec is part of larger system):\n   - Brief description of the parent project/system\n   - Where this spec fits in the bigger picture\n   - Key constraints inherited from the larger context\n   - Reference this appendix in the introduction\n\nKeep context token-efficient: enough for an external reviewer to assess readiness, not a full project overview.\n\n**Spec Structure:**\n```markdown\n## Introduction\n[What + Why in 2-3 sentences. If partial: \"See Appendix A for project context.\"]\n\n## Scope\n[What's being built]\n\n## Acceptance Criteria\n[Testable outcomes]\n\n## Out of Scope\n[Explicit boundaries]\n\n## Appendix A: Project Context (if needed)\n[Token-efficient big picture: ~100-200 words max]\n```\n\n**Synthesis Quality Check:**\n\n| Indicator | Status | Action if Failed |\n|-----------|--------|------------------|\n| Commands/features ‚â• minimum viable | ‚úÖ/‚ùå | Restore essentials |\n| All commands have structured output | ‚úÖ/‚ùå | Add `--json` flags |\n| Agent primitives present | ‚úÖ/‚ùå | Restore ranges, batching |\n| User requests addressed | ‚úÖ/‚ùå | Review with user |\n| Acceptance criteria testable | ‚úÖ/‚ùå | Add specifics |\n| Spec standalone-reviewable | ‚úÖ/‚ùå | Add intro/context |\n\nIf 2+ indicators fail, output **REVISE** with specific gaps‚Äîdon't ship a thin spec.\n\n**Quality Gate:**\n- **GO** ‚Äî Ready to implement\n- **GO with caveats** ‚Äî Workable with listed risks\n- **REVISE** ‚Äî Too thin or too vague, needs another pass with specific guidance\n\n## Early Exit Rules\n\nNot every spec needs all 5 phases:\n\n| Complexity | Refinement |\n|------------|------------|\n| xs/s | Skip entirely |\n| m | 2-phase (Formalize ‚Üí Synthesize) |\n| l/xl | Full 5-phase |\n\nIf the Proposer has no cuts and Advocate has no additions, skip Scope Lock and proceed to Synthesize.\n\n## Complexity Estimation\n\n| Level | Description | Refinement? |\n|-------|-------------|-------------|\n| xs | Trivial, obvious | No |\n| s | Small, well-understood | No |\n| m | Some unknowns | 2-phase |\n| l | Significant unknowns | 5-phase |\n| xl | Many unknowns | 5-phase |\n\n**Rule of thumb:** If you can't describe implementation in 2-3 sentences, it's l or higher.\n\n## Command Reference\n\n### `/refine <target>`\nRuns 5-phase refinement on a bead or spec file.\n1. Reads target\n2. Runs 5 sequential phases (separate agents for adversarial tension)\n3. Presents synthesized spec\n4. Updates bead, adds `refined` label\n\n### `/breakdown <target>`\nDecomposes epic/spec into tasks.\n1. Refines target first (if not already)\n2. Proposes task breakdown with complexity estimates\n3. Creates beads with dependencies and labels\n4. Links tasks to parent epic\n\n## Breakdown Output Rules\n\n| Task Complexity | Label | Rationale |\n|-----------------|-------|-----------|\n| xs/s | `refined` | Obvious enough to implement |\n| m/l/xl | `needs-refinement` | Review at claim time |\n\nTasks get:\n- `parent-child` dep to source epic\n- `blocks` for sequential dependencies\n- Complexity estimate\n- Brief description (details filled at refinement)\n\n## Integration with bd\n\n```bash\n# Find work needing refinement\nbd list --labels needs-refinement\n\n# Find refined work ready to implement\nbd ready --labels refined\n\n# Find epics needing breakdown\nbd list --type epic --labels needs-breakdown\n```\n\n## Refined Spec Criteria\n\nA spec is refined when:\n1. **Standalone** ‚Äî Introduction + context sufficient for external review\n2. **Concrete** ‚Äî Files, functions, line estimates clear\n3. **Bounded** ‚Äî OUT OF SCOPE section exists\n4. **Testable** ‚Äî Acceptance criteria are observable\n5. **Sized** ‚Äî Complexity reflects actual uncertainty\n6. **Unblocked** ‚Äî Dependencies identified/tracked\n\n## Anti-Patterns\n\n- **Refinement theater** ‚Äî Running phases without meaningful changes\n- **Premature refinement** ‚Äî Refining backlog items that may never be done\n- **Skipping Advocate** ‚Äî Proposer cuts can go too far without challenge\n- **Executing cuts in Proposer** ‚Äî Proposer proposes; Advocate + Judge decide\n- **One-person dialectic** ‚Äî Use separate agents per phase for genuine tension\n- **Ignoring Scope Lock** ‚Äî Too Thin indicators exist for a reason\n\n## Why Separate Agents Per Phase\n\nEach phase agent receives only previous output + goals, not internal reasoning. This prevents self-reinforcing mistakes. The Proposer shouldn't remember why the Analyst included something‚Äîit should propose cuts from scratch. The Advocate shouldn't remember the Proposer's reasoning‚Äîit should challenge each proposal independently.\n\n## Resources\n\nFor examples and extended reference material, see:\n- `references/examples.md` ‚Äî Before/after refinement examples\n",
        "plugins/workflow/skills/dialectical-refinement/references/examples.md": "# Refinement Examples\n\n## Before and After\n\n### Example 1: Vague Feature Request\n\n**Before Refinement:**\n```\nTitle: Add caching to API\n\nDescription: We should add caching to make the API faster.\n```\n\n**After Refinement:**\n```\nTitle: Add Redis response caching for GET /users endpoint\n\nDescription:\nCache GET /users responses in Redis with 5-minute TTL.\n~60 lines across 3 files.\n\nDesign:\n- Add redis client to src/lib/redis.ts\n- Add caching middleware to src/middleware/cache.ts\n- Wire middleware to GET /users in src/routes/users.ts\n- Cache key format: `users:list:{queryHash}`\n\nAcceptance Criteria:\n1. Second request within 5 min returns cached response\n2. Cache miss logs to console\n3. Cache can be bypassed with `Cache-Control: no-cache` header\n4. Redis connection failure falls back to no-cache (not error)\n\nOut of Scope:\n- Cache invalidation on user changes (future task)\n- Caching other endpoints (separate tasks per endpoint)\n- Cache warming (not needed for this use case)\n\nComplexity: m\nLabels: [refined]\n```\n\n### Example 2: Epic Breakdown\n\n**Before:**\n```\nTitle: User Authentication System\nType: epic\nDescription: Add login/logout functionality\n```\n\n**After Breakdown:**\n```\nEpic: User Authentication System (refined)\n‚îú‚îÄ‚îÄ Task: Add password hashing utility (xs, refined)\n‚îú‚îÄ‚îÄ Task: Create User model with auth fields (s, refined)\n‚îú‚îÄ‚îÄ Task: Implement login endpoint (m, needs-refinement)\n‚îú‚îÄ‚îÄ Task: Implement logout endpoint (s, refined)\n‚îú‚îÄ‚îÄ Task: Add session middleware (m, needs-refinement)\n‚îî‚îÄ‚îÄ Task: Create login UI component (m, needs-refinement)\n\nDependencies:\n- Login endpoint blocks Logout endpoint\n- User model blocks Login endpoint\n- Password hashing blocks User model\n```\n\n## The Dialectical Arc\n\n| Phase | Role | Cognitive Mode |\n|-------|------|----------------|\n| 1. Formalize | Thesis | Analysis + Protection |\n| 2. Propose Cuts | Antithesis | Criticism (proposals only) |\n| 3. Challenge | Antithesis to antithesis | Advocacy (per-proposal) |\n| 4. Scope Lock | Checkpoint | Verification |\n| 5. Synthesize | Synthesis | Integration |\n\nThis mirrors classical dialectical reasoning‚Äîstructured tension that surfaces and resolves conflicts *before* code is written.\n\n## Example: Proposer/Advocate Exchange\n\n### Phase 2 Output (Proposer)\n\n```markdown\n## PROTECTED (never cut)\n- Core CRUD operations ‚Äî essential workflow\n- `--json` output on all commands ‚Äî agent primitives\n- `--range` flag ‚Äî agent primitives, enables non-linear work\n- User authentication ‚Äî explicitly requested\n\n## PROPOSED CUTS\n\n### Strong Cut Candidates (high confidence)\n- `--interactive` mode ‚Äî Agents don't use TUI\n- `--template` flag ‚Äî Premature abstraction\n\n### Moderate Cut Candidates (medium confidence)\n- `narrate` command ‚Äî Closes feedback loop but adds LLM complexity\n- `--batch` mode ‚Äî Could defer, but note token efficiency concerns\n\n### Weak Cut Candidates (low confidence, protect carefully)\n- `skill` meta-command ‚Äî Meta-tooling, but user mentioned wanting it\n```\n\n### Phase 3 Output (Advocate)\n\n```markdown\n## ADVOCATE RESPONSES\n\n### Strong Cuts ‚Äî Agreed\n- `--interactive` ‚Äî Agree: agents don't use interactive modes\n- `--template` ‚Äî Agree: YAGNI, can add later if pattern emerges\n\n### Moderate Cuts ‚Äî Contested\n- `narrate` ‚Äî Contest: feedback loop is core to agent DX; LLM call is cheap\n- `--batch` ‚Äî Contest: token efficiency is protected category; keep\n\n### Weak Cuts ‚Äî Recommendations\n- `skill` ‚Äî Keep: user explicitly mentioned, low implementation cost\n\n### Cheap Additions Missed\n- `--dry-run` flag ‚Äî Show what would happen without executing (low effort)\n```\n\n### Phase 4: Scope Lock\n\n| Check | Status | Notes |\n|-------|--------|-------|\n| Core workflow preserved | ‚úÖ | CRUD intact |\n| Agent primitives preserved | ‚úÖ | --json, --range kept |\n| User requests addressed | ‚úÖ | Auth, skill included |\n| Structured output | ‚úÖ | --json on all |\n| Token efficiency | ‚úÖ | --batch preserved |\n\n**Too Thin Indicators:** 0 triggered. Proceed to Synthesize.\n",
        "plugins/workflow/skills/mise/SKILL.md": "---\nname: mise\ndescription: Modern dev tool version management with mise. Use PROACTIVELY when setting up repos, managing tool versions, or when users ask about nvm/pyenv/goenv alternatives. Covers project setup, direnv integration, and CI/CD patterns.\n---\n\n# Mise: Modern Dev Tool Management\n\n[Mise](https://mise.jdx.dev) is a polyglot version manager that replaces nvm, pyenv, rbenv, and most Homebrew CLI tools with a single, fast, declarative system.\n\n**Related skills:**\n- **just-pro** - Build system setup (includes mise integration patterns)\n- **cli-tools** - Power CLI tools (many installable via mise)\n\n## Why Mise?\n\n| Problem | Old Way | Mise Way |\n|---------|---------|----------|\n| Node versions | nvm, fnm, volta | `mise use node@22` |\n| Python versions | pyenv, conda | `mise use python@3.12` |\n| Go versions | goenv, manual | `mise use go@1.25` |\n| CLI tools | Homebrew | `mise use jq ripgrep bat` |\n| Per-project versions | `.nvmrc` + `.python-version` + ... | Single `.mise.toml` |\n\n**Benefits:**\n- 1000+ tools available (`mise registry | wc -l`)\n- Parallel installs, prebuilt binaries\n- Works on macOS and Linux\n- Declarative config in repo = reproducible environments\n\n---\n\n## Quick Start\n\n### Install Mise\n\n```bash\ncurl https://mise.run | sh\n```\n\n### Shell Setup\n\nAdd to your shell rc file. If using both mise and direnv (recommended), load mise first:\n\n```bash\n# ~/.zshrc - recommended order\neval \"$(mise activate zsh)\"\neval \"$(direnv hook zsh)\"\n\n# fish: ~/.config/fish/config.fish\nmise activate fish | source\ndirenv hook fish | source\n```\n\nFor faster startup, use shims instead of (or with) activation:\n\n```bash\n# zsh: add to ~/.zshrc\nexport PATH=\"$HOME/.local/share/mise/shims:$PATH\"\neval \"$(mise activate zsh)\"\neval \"$(direnv hook zsh)\"\n\n# fish: add to ~/.config/fish/config.fish\nfish_add_path -p ~/.local/share/mise/shims\nmise activate fish | source\ndirenv hook fish | source\n```\n\n### Install Tools\n\n```bash\nmise use node@22 python@3.12 go@latest  # Current directory\nmise use -g jq ripgrep bat              # Global (all directories)\n```\n\n---\n\n## Project Setup\n\n### New Repo\n\n```bash\n# Pin language versions\nmise use node@22 go@1.25\n\n# Creates .mise.toml - commit it\ngit add .mise.toml\n```\n\n### Existing Repo (first clone)\n\n```bash\nmise trust      # Allow repo's .mise.toml\nmise install    # Install pinned tools\n```\n\n### Example `.mise.toml`\n\n```toml\n[tools]\nnode = \"22\"\ngo = \"1.25\"\npython = \"3.12\"\n\n# Project-specific tools\njust = \"latest\"\nsqlc = \"latest\"\n```\n\n---\n\n## Configuration Hierarchy\n\nMise merges configs from multiple levels:\n\n```\n~/.config/mise/config.toml     # Global defaults\n  ‚îî‚îÄ‚îÄ ~/projects/.mise.toml    # Workspace defaults\n        ‚îî‚îÄ‚îÄ ~/projects/foo/.mise.toml  # Project-specific\n```\n\nMore specific configs override less specific ones.\n\n### Global Config (`~/.config/mise/config.toml`)\n\nYour daily-driver tools:\n\n```toml\n[tools]\n# Languages\nnode = \"lts\"\npython = \"3.12\"\ngo = \"latest\"\n\n# CLI tools (replaces Homebrew)\njq = \"latest\"\nyq = \"latest\"\nripgrep = \"latest\"\nfd = \"latest\"\nbat = \"latest\"\neza = \"latest\"\ndelta = \"latest\"\nfzf = \"latest\"\ngh = \"latest\"\nlazygit = \"latest\"\njust = \"latest\"\ndirenv = \"latest\"\nstarship = \"latest\"\n\n[settings]\nauto_install = true\n```\n\n---\n\n## Direnv Integration\n\n[Direnv](https://direnv.net) handles per-directory **environment variables**. Combined with mise:\n\n- **Mise** ‚Üí tool versions (node, go, python)\n- **Direnv** ‚Üí environment variables (DATABASE_URL, API keys)\n\n> **Best Practice:** Keep a single source of truth:\n> - `.mise.toml` ‚Üí tool versions only (node, go, python)\n> - `.envrc` ‚Üí environment variables (DATABASE_URL, API_KEY, etc.)\n>\n> Don't use `[env]` section in `.mise.toml` - it creates confusion about where vars come from.\n\n### Setup\n\n1. Install direnv via mise:\n   ```bash\n   mise use -g direnv\n   ```\n\n2. Add direnv hook to shell rc:\n   ```bash\n   # zsh\n   eval \"$(direnv hook zsh)\"\n\n   # fish\n   direnv hook fish | source\n   ```\n\n3. Create `.envrc` in your project:\n   ```bash\n   # Load mise tools for this directory\n   if command -v mise &> /dev/null; then\n     eval \"$(mise hook-env -s bash)\"\n   fi\n\n   # Project-specific environment\n   export DATABASE_URL=\"postgres://localhost/myapp\"\n   export LOG_LEVEL=\"debug\"\n   ```\n\n4. Allow the envrc:\n   ```bash\n   direnv allow\n   ```\n\n**Tip:** Use `.envrc.example` (committed) + `.envrc` (gitignored with secrets).\n\n---\n\n## Just Integration\n\n[just](https://just.systems) and mise complement each other:\n- **mise** ‚Üí pins tool versions\n- **just** ‚Üí runs commands using those tools\n\n**See the `just-pro` skill** for full patterns. Quick summary:\n\n### Shell Override (recommended for teams)\n\n```just\n# Every recipe runs through mise automatically\nset shell := [\"mise\", \"exec\", \"--\", \"bash\", \"-c\"]\n\nbuild:\n    npm run build\n\ntest:\n    go test ./...\n```\n\n### Graceful Degradation (for open source)\n\n```just\n_exec cmd:\n    #!/usr/bin/env bash\n    if command -v mise &>/dev/null; then\n        mise exec -- {{cmd}}\n    else\n        {{cmd}}\n    fi\n\nbuild: (_exec \"npm run build\")\n```\n\n### Setup Recipe\n\n```just\nsetup:\n    #!/usr/bin/env bash\n    mise trust && mise install\n\n    # Create .envrc from example if missing\n    if [[ ! -f .envrc ]] && [[ -f .envrc.example ]]; then\n        cp .envrc.example .envrc\n        echo \"Created .envrc from example - edit with your values\"\n        direnv allow\n    fi\n\n    echo \"Toolchain ready\"\n```\n\n---\n\n## Mise vs Devcontainer\n\n| Aspect | Mise + Direnv | Devcontainer |\n|--------|---------------|--------------|\n| **Isolation** | Shared host filesystem | Full container isolation |\n| **Speed** | Native performance | Container overhead |\n| **Setup time** | Seconds (`mise install`) | Minutes (image build) |\n| **Works offline** | After first install | After first build |\n| **IDE support** | Any editor, native | VS Code / JetBrains |\n| **Team adoption** | Low friction | Requires Docker knowledge |\n| **CI parity** | Good (mise in CI) | Excellent (same container) |\n\n**Recommendation:** Use mise for fast local dev. Add devcontainer for hermetic reproducibility if needed. They're not mutually exclusive.\n\n---\n\n## Common Tools Available\n\n```bash\nmise registry | grep <tool>  # Search for a tool\n```\n\n| Category | Tools |\n|----------|-------|\n| **Languages** | node, python, go, rust, java, ruby, php, elixir, zig |\n| **JSON/YAML** | jq, yq, gojq |\n| **Search** | ripgrep, fd, fzf, ag |\n| **Git** | gh, lazygit, delta, git-cliff |\n| **Files** | bat, eza, tree, dust, duf |\n| **HTTP** | httpie, curlie, xh |\n| **Containers** | kubectl, helm, k9s, docker-compose |\n| **Cloud** | awscli, terraform, opentofu, pulumi |\n| **Dev** | just, make, watchexec, hyperfine |\n| **Editors** | neovim, helix |\n\n---\n\n## Migration from Homebrew\n\n**Keep in Homebrew:**\n- `git` (system integration)\n- Your shell (`fish`, `zsh`)\n- GUI apps (casks)\n- System utilities (`coreutils` if needed)\n\n**Move to mise:**\n- Language runtimes (node, python, go, rust)\n- CLI dev tools (jq, ripgrep, bat, etc.)\n- Cloud CLIs (awscli, kubectl, terraform)\n\n```bash\n# Check what mise can replace\nbrew leaves | while read pkg; do\n  mise registry | grep -q \"^$pkg \" && echo \"‚úì $pkg\"\ndone\n```\n\n---\n\n## Troubleshooting\n\n### Tools not in PATH\n\n```bash\nmise doctor  # Check activation status\n```\n\nEnsure mise activates **after** other PATH modifications in shell rc.\n\n### Shims vs Activate\n\n- **Shims** (`~/.local/share/mise/shims`): Wrapper scripts, always work\n- **Activate**: Dynamic PATH modification, faster for frequent version switching\n\nUse both for reliability:\n```bash\n# Shims first (fallback), then activate (dynamic)\nexport PATH=\"$HOME/.local/share/mise/shims:$PATH\"\neval \"$(mise activate zsh)\"\n```\n\n### Direnv + Mise\n\nUse `mise hook-env -s bash` in `.envrc`, not `mise activate`:\n```bash\neval \"$(mise hook-env -s bash)\"  # Correct\neval \"$(mise activate bash)\"     # Wrong - generates shell hooks\n```\n\n---\n\n## Quick Reference\n\n```bash\n# Install tools\nmise use node@22              # Current directory\nmise use -g ripgrep           # Global\n\n# Manage versions\nmise ls                       # List installed\nmise ls-remote node           # Available versions\nmise outdated                 # Check for updates\n\n# Project setup\nmise install                  # Install from .mise.toml\nmise trust                    # Trust current directory's config\n\n# Maintenance\nmise prune                    # Remove unused versions\nmise reshim                   # Rebuild shims\nmise self-update              # Update mise itself\n```\n\n---\n\n## Further Reading\n\n- [Mise Documentation](https://mise.jdx.dev)\n- [Mise Registry](https://mise.jdx.dev/registry.html) - All available tools\n- [Direnv Documentation](https://direnv.net)\n",
        "plugins/workflow/skills/orchestrator/SKILL.md": "---\nname: orchestrator\ndescription: Activate at session start when you are the primary Claude instance. Establishes orchestrator role with delegation protocols, subagent launch templates, token efficiency rules, and parallel safety constraints. You orchestrate; subagents implement.\n---\n\n# Orchestrator Protocol\n\nYou are a **subagent orchestrator**, not an implementer. Your job is strategic: understand tasks, delegate implementation, review results, maintain big-picture awareness.\n\n---\n\n## Delegation Threshold\n\nDelegate to subagents if ANY apply:\n\n| Trigger | Delegate |\n|---------|----------|\n| More than 2 file edits | Yes |\n| More than 30 lines of new code | Yes |\n| Creating new modules/systems | Yes |\n| Implementation work (vs research) | Yes |\n\n---\n\n## What You Do Directly\n\n- Read files to understand scope\n- Use Explore agent for codebase research\n- Claim/update/close beads (`bd` CLI)\n- Review and commit subagent work\n- Ask clarifying questions\n- Git operations (add, commit, push, branch)\n- Merge barrel exports after parallel work\n- Run `bd sync` at session end (or before handoff)\n\n---\n\n## What You Delegate\n\n- Writing new code/tests\n- Editing existing code\n- Implementing features/fixes\n- Debugging complex issues\n\n---\n\n## Proactive Skill Selection\n\nBefore launching a subagent, **proactively determine all applicable skills**. Don't rely on subagents to discover them ‚Äî tell them explicitly.\n\n**Evaluate the task against:**\n\n| Domain | Skills |\n|--------|--------|\n| TypeScript code | `dm-lang:typescript-pro` |\n| Go code | `dm-lang:go-pro` |\n| Rust code | `dm-lang:rust-pro` |\n| Build systems | `dm-lang:just-pro` |\n| Architecture decisions | `dm-arch:solid-architecture`, `dm-arch:data-oriented-architecture` |\n| Game mechanics | `dm-game:game-design` |\n| Game hot paths (JS/TS) | `dm-game:game-perf` |\n| Spec refinement | `dm-work:dialectical-refinement` |\n\n**Rules:**\n- Include ALL skills that apply ‚Äî more is better than fewer\n- Language skills (`dm-lang:typescript-pro`, etc.) should almost always be included for code tasks\n- Architecture skills apply to any structural decisions\n- Subagents activate skills at start, so missing skills means suboptimal work\n\n**Example:** A task to \"implement a new TypeScript service with caching\" should include:\n- `dm-lang:typescript-pro` (language)\n- `dm-arch:solid-architecture` (service design)\n- Possibly `dm-arch:data-oriented-architecture` (if polymorphic entities involved)\n\n---\n\n## Subagent Launch Template\n\nWhen delegating, include:\n\n```\nCONTEXT: Bead <id> | Workspace: <path>\n\nTASK: <clear description>\n\nSKILLS: <relevant skills to activate>\n\nQUALITY GATES: <verification commands, e.g., npm run check>\n\nOWN (create/edit freely):\n- <file1>\n- <file2>\n\nREAD-ONLY:\n- <shared files you must not modify>\n\nRETURN:\n- Summary only (1-5 lines): what changed, what worked, what failed\n- Details ‚Üí history/ directory\n- Do NOT commit or close beads\n```\n\n---\n\n## Token Efficiency Rules\n\n**Orchestrator context is precious.** Protect it.\n\n| Subagent Output | Where |\n|-----------------|-------|\n| Summary (1-5 lines) | Return to orchestrator |\n| Details, logs, traces | `history/` dir or `/tmp/claude-*` fallback |\n| Capability gaps | Include in summary + append to `history/gaps.log` |\n\n**Rules:**\n- Summaries: what changed, what worked, what failed, blockers\n- Never dump full file contents, long logs, or verbose traces\n- Orchestrator can dig into `history/` if needed\n\n---\n\n## Parallel Safety\n\nYou own these cross-cutting concerns (never delegate):\n\n- Git operations (add, commit, push, branch)\n- Bead state changes (claim, close, update status)\n- Shared index files (barrel exports)\n- Package.json / config changes\n- Any file multiple beads might touch\n\nWhen launching parallel subagents:\n\n- Ensure non-overlapping file ownership\n- Each subagent gets explicit OWN vs READ-ONLY lists\n- Merge barrel exports yourself after subagents complete\n\n---\n\n## Git Worktrees\n\nUse when parallel subagents need filesystem isolation (conflicting files, separate builds/servers).\n\n**Skill:** `dm-work:worktrees`\n**Command:** `bd worktree create <name>` ‚Äî handles git + beads integration automatically\n**Beads:** Automatic redirect to main repo's database (shared issue state)\n\n---\n\n## Pre-Merge Review\n\nBefore merging to main or completing significant work:\n\n| Review Type | Agent |\n|-------------|-------|\n| Code review | `feature-dev:code-reviewer` |\n| Architecture review | `feature-dev:code-architect` |\n\n**Triggers:** branch merges, multi-file commits, new features, refactors, security-sensitive paths\n\n---\n\n## Capability Gap Reporting\n\nWhen you or subagents encounter gaps, log them.\n\n**Triggers:**\n- Domain knowledge had to be researched\n- Workflow repeated 2+ times manually\n- Wished for specific expertise\n- Built a workaround for missing capability\n\n**Format:**\n```\n[DATE] [agent|skill] NAME: description | trigger: what prompted this\n```\n\n**Log to:** `history/gaps.log` (or `/tmp/claude-gaps.log` fallback)\n\nReview gaps at session end to identify missing skills/agents.\n\n---\n\n## Session Start Checklist\n\n- [ ] I am the orchestrator, not the implementer\n- [ ] I will delegate implementation to subagents\n- [ ] I will instruct subagents on skills and quality gates\n- [ ] I will protect my context through delegation and concise returns\n\n---\n\n## Session End Checklist\n\n- [ ] All work committed\n- [ ] Beads closed for completed work\n- [ ] Run `bd sync` to sync with remote\n- [ ] Quality gates passing\n",
        "plugins/workflow/skills/repo-init/SKILL.md": "---\nname: repo-init\ndescription: Initialize a new repository with standard scaffolding - git, gitignore, CLAUDE.md, justfile, mise, and beads. Use when starting a new project or setting up an existing repo for Claude Code workflows.\n---\n\n# Repository Initialization\n\nScaffold a new or existing repository with standard project infrastructure.\n\n**Related skills:**\n- **just-pro** - Build system patterns and templates\n- **mise** - Tool version management\n- **go-pro**, **rust-pro**, **typescript-pro**, **python-pro** - Language-specific setup\n\n## Execution Modes\n\nThis skill supports two modes. **Prefer molecule mode** when beads is available.\n\n### Molecule Mode (Preferred)\n\nUse beads molecules for tracked, closeable tasks. Each step becomes an issue you can close as you complete it.\n\n**Prerequisites:** beads installed (`bd --version` works)\n\n**Steps:**\n\n1. Find the dm-work plugin install path:\n   ```bash\n   jq -r '.plugins[\"dm-work@dark-matter-marketplace\"][0].installPath' ~/.claude/plugins/installed_plugins.json\n   ```\n\n2. Wisp the formula (ephemeral, no git pollution):\n   ```bash\n   bd mol wisp <install-path>/skills/repo-init/references/repo-init.formula.json \\\n     --var lang=<language> --var name=<project-name> --var type=<project-type>\n   ```\n\n3. Work through tasks:\n   ```bash\n   bd ready              # See next task\n   # ... do the work ...\n   bd close <step-id>    # Mark complete\n   ```\n\n4. Clean up when done:\n   ```bash\n   bd mol burn <wisp-id>\n   ```\n\n**Variables:**\n| Variable | Required | Default | Values |\n|----------|----------|---------|--------|\n| `lang` | Yes | - | go, rust, typescript, python |\n| `name` | Yes | - | Project name |\n| `type` | No | cli | cli, lib, web, api |\n\n---\n\n### Manual Mode (Fallback)\n\nUse when beads is not installed or for quick setups without tracking.\n\nFollow the steps below in order. Steps 3-6 can run in parallel after git-init.\n\n---\n\n## Step 1: Gather Context\n\nBefore scaffolding, clarify:\n\n1. **Project language(s)**: Go, Rust, TypeScript, Python, or multi-language?\n2. **Project type**: Library, CLI, web app, API, monorepo?\n3. **Existing files**: Is this a fresh repo or adding to existing code?\n\nUse AskUserQuestion if unclear from context.\n\n---\n\n## Step 2: Git Setup\n\n```bash\n# Initialize git if needed\ngit init\n```\n\n### .gitignore Templates\n\nCopy from the appropriate language skill's `references/gitignore`:\n\n| Language | Source |\n|----------|--------|\n| Go | `go-pro/references/gitignore` |\n| Rust | `rust-pro/references/gitignore` |\n| TypeScript | `typescript-pro/references/gitignore` |\n| Python | `python-pro/references/gitignore` |\n\n**For multi-language repos:** Start with the primary language's gitignore, then merge patterns from others.\n\n**Minimal fallback** (if language skill unavailable):\n\n```gitignore\n# Environment\n.env\n.env.local\n.env.*.local\n.envrc\n\n# OS\n.DS_Store\nThumbs.db\n\n# IDE\n.idea/\n.vscode/\n\n# Build (customize per language)\ndist/\nbuild/\ntarget/\nnode_modules/\n__pycache__/\n```\n\n---\n\n## Step 3: CLAUDE.md\n\nCreate project conventions file:\n\n```markdown\n# Project Name - Claude Instructions\n\n## Overview\n\nBrief description of what this project does.\n\n## Development\n\n```bash\njust setup    # First-time setup\njust check    # Run all quality gates\n```\n\n## Conventions\n\n- [Add project-specific patterns here]\n\n## Architecture\n\n- [Key directories and their purposes]\n```\n\nKeep it minimal initially. Add conventions as they emerge.\n\n---\n\n## Step 4: Justfile Skeleton\n\n```just\n# Project Build System\n# Usage: just --list\n\ndefault:\n    @just --list\n\n# First-time setup\nsetup:\n    mise trust\n    mise install\n    @echo \"Ready. Run 'just check' to verify.\"\n\n# Quality gates - add language-specific checks\ncheck:\n    @echo \"Add fmt, lint, test recipes\"\n\n# Remove build artifacts\nclean:\n    @echo \"Add clean commands\"\n```\n\nSee **just-pro** skill for language-specific recipes.\n\n---\n\n## Step 5: Mise Configuration\n\nCreate `.mise.toml`:\n\n```toml\n[tools]\n# Add tools with: mise use <tool>@<version>\n# Examples:\n# node = \"22\"\n# go = \"1.23\"\n# rust = \"1.83\"\n# just = \"latest\"\n```\n\n---\n\n## Step 6: Environment Template\n\nCreate `.envrc.example` (committed) as template for `.envrc` (gitignored):\n\n```bash\n# Copy to .envrc and fill in values\n# cp .envrc.example .envrc && direnv allow\n\n# Mise integration\nif command -v mise &> /dev/null; then\n  eval \"$(mise hook-env -s bash)\"\nfi\n\n# Project-specific environment\n# export DATABASE_URL=\"postgres://localhost/myapp\"\n# export API_KEY=\"\"\n```\n\n---\n\n## Step 7: Beads Initialization\n\n```bash\nbd init -q\n```\n\nAdd to CLAUDE.md:\n```markdown\n## Task Tracking\n\nUse `bd` for task tracking. Run `bd ready` to see available work.\n```\n\n---\n\n## Step 8: Next Steps\n\nPoint user to language-specific setup:\n\n| Language | Next Step |\n|----------|-----------|\n| Go | Invoke **go-pro** skill, run `go mod init` |\n| Rust | Invoke **rust-pro** skill, run `cargo init` |\n| TypeScript | Invoke **typescript-pro** skill, run `npm init` |\n| Python | Invoke **python-pro** skill, run `uv init` |\n\n---\n\n## Quick Reference\n\n```bash\n# Full manual init sequence\ngit init\n# Create .gitignore, CLAUDE.md, justfile, .mise.toml, .envrc.example\nbd init -q\nmise use just@latest\n# Then follow language skill for specifics\n```\n\n## Monorepo Variant\n\nFor monorepos, the root gets:\n- Root `justfile` with module imports (see just-pro monorepo patterns)\n- Root `.mise.toml` with shared tooling\n- Single `.beads/` at root\n\nEach package gets:\n- Package-local `justfile`\n- Language-specific configs (Cargo.toml, package.json, etc.)\n",
        "plugins/workflow/skills/srt/SKILL.md": "---\nname: srt\ndescription: Sandbox Runtime (srt) patterns for CLI/autonomous Claude runs. For interactive sandboxing, use Claude Code's built-in `/sandbox` command instead.\n---\n\n# Sandboxing Claude\n\n## Interactive vs Autonomous\n\nClaude Code now has **built-in sandboxing**. Choose the right approach:\n\n| Mode | Tool | When to Use |\n|------|------|-------------|\n| **Interactive** | `/sandbox` | Human-in-the-loop sessions with sandbox protection |\n| **CLI/Autonomous** | srt | `claude -p` with `--dangerously-skip-permissions` |\n\n### For Interactive Sessions: Use `/sandbox`\n\nRun `/sandbox` in Claude Code to enable native sandboxing. It uses the same OS primitives as srt (macOS seatbelt, Linux bubblewrap) but is simpler:\n\n- No installation required\n- Integrated with permission system (auto-allow mode reduces prompts by 84%)\n- Configure via `settings.json`\n\n**What `/sandbox` protects:**\n- Filesystem: Write access limited to CWD by default\n- Network: Domain allowlist with prompts for new domains\n- Subprocesses: Same restrictions apply to scripts Claude runs\n\n**What `/sandbox` does NOT protect:**\n- No CLI flag equivalent (Docker required for CLI sandbox)\n- Has escape hatch (`dangerouslyDisableSandbox`) - commands can break out\n- Config is global (`settings.json`), not per-project\n\n### For CLI/Autonomous: Use srt\n\nWhen running Claude with `-p` and `--dangerously-skip-permissions`, srt provides stricter control:\n\n- **No escape hatch** - commands cannot break out\n- **Per-project config** - `.srt.json` in each repo\n- **Explicit allowlists** - you specify exactly what's permitted\n\n---\n\n# Sandbox Runtime (srt)\n\n[srt](https://github.com/anthropic-experimental/sandbox-runtime) is a lightweight OS-level sandbox for restricting filesystem and network access without containers.\n\n**Use cases:**\n- Running Claude with `--dangerously-skip-permissions` safely\n- DX testing (stress-testing skills, toolchain validation)\n- CI/CD autonomous Claude runs\n- Isolating subagent work to specific directories\n\n## Installation\n\n```bash\nnpm install -g @anthropic-ai/sandbox-runtime\n```\n\n## How It Works\n\n| Platform | Mechanism |\n|----------|-----------|\n| macOS | `sandbox-exec` with dynamic profiles |\n| Linux | `bubblewrap` with network namespaces |\n\n**Access model:**\n- **Network**: Default deny, explicit allowlist\n- **FS Read**: Default allow, explicit denylist\n- **FS Write**: Default deny, explicit allowlist\n\n---\n\n## Configuration\n\nsrt uses JSON config files (default: `~/.srt-settings.json` or `-s <path>`).\n\n### Config Structure\n\n```json\n{\n  \"allowPty\": false,\n  \"network\": {\n    \"allowedDomains\": [\"api.anthropic.com\", \"github.com\"],\n    \"deniedDomains\": []\n  },\n  \"filesystem\": {\n    \"denyRead\": [\"~/.ssh\", \"~/.gnupg\", \"~/.aws/credentials\"],\n    \"allowWrite\": [\".\", \"/tmp\"],\n    \"denyWrite\": []\n  }\n}\n```\n\n| Option | Default | Purpose |\n|--------|---------|---------|\n| `allowPty` | `false` | Enable pseudo-terminal access for interactive tools |\n| `network.allowedDomains` | `[]` | Domains to allow network access |\n| `filesystem.allowWrite` | `[]` | Paths to allow write access |\n| `filesystem.denyRead` | `[]` | Paths to block read access |\n\n### Network Allowlist Strategy\n\n**The GitHub question:** Many examples include `github.com` by default. Understand why before blindly copying:\n\n| Reason to allow GitHub | When needed |\n|------------------------|-------------|\n| Git-based dependencies | Cargo git deps, Go modules, npm git refs |\n| Beads sync | `bd sync` pushes work state to remote |\n| Code search | Looking up OSS implementations |\n\n| Reason to block GitHub | Consideration |\n|------------------------|---------------|\n| Exfiltration surface | Domain fronting allows data to reach any GitHub-hosted endpoint |\n| Not always needed | Pure registry deps (crates.io, npm) don't need GitHub |\n| Context7 alternative | For docs/code lookup, Context7 is more focused |\n\n**Recommendation:** Start with minimal allowlist, add GitHub only if builds fail on git-based deps or you need beads sync.\n\n### Minimal vs Full Allowlists\n\n**Minimal (no GitHub) ‚Äî prefer when possible:**\n\n```json\n\"allowedDomains\": [\n  \"api.anthropic.com\",\n  \"crates.io\", \"*.crates.io\", \"static.crates.io\", \"index.crates.io\",\n  \"static.rust-lang.org\"\n]\n```\n\n**With GitHub (when git deps or beads needed):**\n\n```json\n\"allowedDomains\": [\n  \"api.anthropic.com\",\n  \"crates.io\", \"*.crates.io\", \"static.crates.io\", \"index.crates.io\",\n  \"github.com\", \"*.github.com\",\n  \"static.rust-lang.org\",\n  \"*.cloudfront.net\"\n]\n```\n\n### Ecosystem-Specific Allowlists\n\n**Rust (minimal):**\n```json\n\"allowedDomains\": [\n  \"api.anthropic.com\",\n  \"crates.io\", \"*.crates.io\", \"static.crates.io\", \"index.crates.io\",\n  \"static.rust-lang.org\"\n]\n```\n\n**Go (minimal):**\n```json\n\"allowedDomains\": [\n  \"api.anthropic.com\",\n  \"proxy.golang.org\", \"sum.golang.org\", \"storage.googleapis.com\",\n  \"gopkg.in\"\n]\n```\n\n**Node/TypeScript (minimal):**\n```json\n\"allowedDomains\": [\n  \"api.anthropic.com\",\n  \"registry.npmjs.org\", \"*.npmjs.org\"\n]\n```\n\n**Add GitHub to any of the above if:**\n- Build fails fetching git-based dependencies\n- You need `bd sync` for beads state persistence\n\n### MCP in Sandbox (Context7, Brightdata)\n\nThe official context7 plugin is an MCP wrapper (`npx @upstash/context7-mcp`), so it has the same requirements as any MCP:\n\n**To enable MCP in sandbox:**\n```json\n{\n  \"network\": {\n    \"allowedDomains\": [\n      \"api.anthropic.com\",\n      \"context7.com\", \"*.context7.com\",\n      \"api.upstash.com\"\n    ]\n  },\n  \"filesystem\": {\n    \"allowWrite\": [\n      \".\",\n      \"~/Library/Caches/claude-cli-nodejs\"\n    ]\n  }\n}\n```\n\nThen run **without** `--strict-mcp-config`:\n```bash\nsrt -s .srt.json -c 'claude --dangerously-skip-permissions \\\n  --no-session-persistence \\\n  -p \"prompt\"'\n```\n\n**Tradeoff:** Context7 gives better docs lookup than GitHub search, but requires MCP cache writes. For pure build/test tasks, skip MCP entirely.\n\n---\n\n## Interactive Mode (allowPty)\n\n**Key discovery:** Interactive CLI tools require pseudo-terminal access.\n\n### The Problem\n\nRunning interactive tools (like Claude Code in interactive mode) fails with:\n```\nsetRawMode failed with errno: 1\n```\n\n### Why It Happens\n\nOn macOS, `sandbox-exec` blocks `/dev/ptmx` and `/dev/ttys*` by default. Interactive CLI tools need these for:\n- Raw terminal mode (keyboard input handling)\n- Terminal UI rendering\n- Signal handling (Ctrl+C, etc.)\n\n### The Solution\n\nAdd `\"allowPty\": true` to your srt config:\n\n```json\n{\n  \"allowPty\": true,\n  \"network\": {\n    \"allowedDomains\": [\"api.anthropic.com\"]\n  },\n  \"filesystem\": {\n    \"denyRead\": [\"~/.ssh\", \"~/.gnupg\", \"~/.aws/credentials\"],\n    \"allowWrite\": [\".\", \"/tmp\"]\n  }\n}\n```\n\n### When to Use\n\n| Mode | `allowPty` | Use Case |\n|------|------------|----------|\n| Interactive | `true` | Human-in-the-loop Claude sessions |\n| Batch/Autonomous | `false` (default) | CI/CD, one-shot prompts |\n\n**Security note:** PTY access is lower risk than network/filesystem‚Äîit only affects terminal I/O, not data exfiltration.\n\n### Documentation Gap\n\nThe `allowPty` option is:\n- Not documented in the srt README\n- Not shown in `srt --help`\n- Only visible in source code (`sandbox-manager.ts`, `macos-sandbox-utils.ts`)\n\nThis is a common gotcha when setting up interactive sessions.\n\n---\n\n## Claude Stateless Flags\n\nFor sandboxed Claude runs, disable state writes:\n\n```bash\nclaude --dangerously-skip-permissions \\\n       --no-session-persistence \\\n       --strict-mcp-config --mcp-config '{\"mcpServers\":{}}'\n```\n\n| Flag | Purpose |\n|------|---------|\n| `--dangerously-skip-permissions` | No permission prompts (srt handles safety) |\n| `--no-session-persistence` | Don't write to `~/.claude.json` |\n| `--strict-mcp-config --mcp-config '{...}'` | No MCP servers (avoids log writes to `~/Library/Caches/`) |\n\n**Why disable MCP?** Claude writes MCP logs to `~/Library/Caches/claude-cli-nodejs/`. Sandboxing this requires broad write access. Simpler to disable for autonomous runs.\n\n---\n\n## Example Configs\n\n### Project-Specific `.srt.json`\n\nFor a Rust project (minimal ‚Äî no GitHub):\n\n```json\n{\n  \"network\": {\n    \"allowedDomains\": [\n      \"api.anthropic.com\",\n      \"crates.io\", \"*.crates.io\", \"static.crates.io\", \"index.crates.io\",\n      \"static.rust-lang.org\"\n    ]\n  },\n  \"filesystem\": {\n    \"denyRead\": [\"~/.ssh\", \"~/.gnupg\", \"~/.aws/credentials\"],\n    \"allowWrite\": [\n      \".\",\n      \"~/.cargo/registry\",\n      \"~/.cargo/git\",\n      \"/tmp\"\n    ]\n  }\n}\n```\n\nIf builds fail on git-based deps, add: `\"github.com\", \"*.github.com\", \"*.cloudfront.net\"`\n\n### With Beads Sync\n\nIf you need `bd sync` to push work state:\n\n```json\n{\n  \"network\": {\n    \"allowedDomains\": [\n      \"api.anthropic.com\",\n      \"crates.io\", \"*.crates.io\", \"static.crates.io\", \"index.crates.io\",\n      \"static.rust-lang.org\",\n      \"github.com\", \"*.github.com\"\n    ]\n  },\n  \"filesystem\": {\n    \"denyRead\": [\"~/.ssh\", \"~/.gnupg\", \"~/.aws/credentials\"],\n    \"allowWrite\": [\n      \".\",\n      \"~/.cargo/registry\", \"~/.cargo/git\",\n      \"/tmp\"\n    ]\n  }\n}\n```\n\n**Alternative:** Skip `bd sync` in autonomous runs and sync manually after review. This keeps GitHub out of the allowlist.\n\n### DX Testing Config\n\nFor stress-testing skills in `/tmp` (multi-ecosystem):\n\n```json\n{\n  \"network\": {\n    \"allowedDomains\": [\n      \"api.anthropic.com\",\n      \"crates.io\", \"*.crates.io\", \"static.crates.io\", \"index.crates.io\",\n      \"static.rust-lang.org\",\n      \"registry.npmjs.org\", \"*.npmjs.org\",\n      \"proxy.golang.org\", \"sum.golang.org\", \"storage.googleapis.com\",\n      \"pypi.org\", \"*.pypi.org\", \"files.pythonhosted.org\"\n    ],\n    \"deniedDomains\": []\n  },\n  \"filesystem\": {\n    \"denyRead\": [\"~/.ssh\", \"~/.gnupg\"],\n    \"allowWrite\": [\n      \"/tmp\",\n      \"~/.cargo/registry\", \"~/.cargo/git\",\n      \"~/.npm\", \"~/.cache/go-build\",\n      \"~/.cache/uv\",\n      \"~/.claude/session-env\"\n    ],\n    \"denyWrite\": []\n  }\n}\n```\n\n**Note:** `~/.claude/session-env` is required for Claude to execute bash commands even with `--no-session-persistence`. Without it, subagent bash commands fail with EPERM.\n\nNo GitHub in DX testing config. Add only if tests specifically need git-based deps.\n\n### With Context7 (MCP enabled)\n\nFor tasks needing documentation lookup:\n\n```json\n{\n  \"network\": {\n    \"allowedDomains\": [\n      \"api.anthropic.com\",\n      \"context7.com\", \"*.context7.com\", \"api.upstash.com\",\n      \"crates.io\", \"*.crates.io\", \"static.crates.io\", \"index.crates.io\",\n      \"static.rust-lang.org\"\n    ]\n  },\n  \"filesystem\": {\n    \"denyRead\": [\"~/.ssh\", \"~/.gnupg\", \"~/.aws/credentials\"],\n    \"allowWrite\": [\n      \".\",\n      \"~/.cargo/registry\", \"~/.cargo/git\",\n      \"~/Library/Caches/claude-cli-nodejs\",\n      \"/tmp\"\n    ]\n  }\n}\n```\n\nRun without `--strict-mcp-config` to enable Context7.\n\n---\n\n## Usage Patterns\n\n### One-Shot Task\n\n```bash\nsrt -s .srt.json -c 'claude --dangerously-skip-permissions \\\n  --no-session-persistence \\\n  --strict-mcp-config --mcp-config \"{\\\"mcpServers\\\":{}}\" \\\n  -p \"Build and test the project, report any issues\"'\n```\n\n### DX Stress Test\n\n```bash\nsrt -s /tmp/dx-test.srt.json -c 'claude --dangerously-skip-permissions \\\n  --no-session-persistence \\\n  --strict-mcp-config --mcp-config \"{\\\"mcpServers\\\":{}}\" \\\n  -p \"Create a Rust CLI in /tmp/test-project following rust-pro patterns.\n      Report any toolchain friction or missing patterns.\"'\n```\n\n---\n\n## Optional: Justfile Integration\n\nAdd this recipe to a project's justfile for autonomous runs:\n\n```just\n# Autonomous Claude (sandboxed, no prompts, batch mode)\nai-auto prompt:\n    srt -s .srt.json -c 'claude --dangerously-skip-permissions \\\n        --no-session-persistence \\\n        --strict-mcp-config --mcp-config \"{\\\"mcpServers\\\":{}}\" \\\n        -p \"{{prompt}}\"'\n```\n\n**Note:** For interactive sandboxed sessions, use `/sandbox` in Claude Code instead of srt. The `ai-auto` recipe is for CLI/autonomous runs only.\n\n---\n\n## Limitations & Tradeoffs\n\n| Limitation | Impact |\n|------------|--------|\n| Domain fronting | Broad allowlists (github.com) have exfiltration surface |\n| Linux monitoring | No violation alerts (macOS has real-time notifications) |\n| Proxy bypass | Apps ignoring env vars can bypass network filtering |\n\n### Decision Matrix\n\n| Need | Allowlist | Notes |\n|------|-----------|-------|\n| Pure build/test | Minimal (no GitHub) | Prefer this when possible |\n| Git-based deps | Add GitHub | Only if builds fail without it |\n| Beads sync | Add GitHub | Or skip sync, review manually |\n| Docs lookup | Context7 + MCP cache | Better than GitHub search |\n| Web research | Brightdata + MCP cache | Or skip for autonomous builds |\n\n### The GitHub vs Context7 Question\n\nFor **information gathering** (docs, code patterns):\n- Context7 is more focused and doesn't have exfiltration surface\n- Requires MCP cache writes (`~/Library/Caches/claude-cli-nodejs`)\n- Worth the tradeoff for research-heavy tasks\n\nFor **pure execution** (build, test, lint):\n- Skip both GitHub and MCP\n- Minimal attack surface\n- Claude works from training data + local context\n\nFor **beads integration**:\n- GitHub required for `bd sync`\n- Alternative: Skip sync during autonomous run, sync manually after\n- Consider: Is persisting work state during autonomous run worth the exfiltration risk?\n\n---\n\n## Troubleshooting\n\n### \"setRawMode failed with errno: 1\"\n\nInteractive CLI tools need PTY access. Add to your config:\n```json\n{\n  \"allowPty\": true\n}\n```\n\nSee [Interactive Mode (allowPty)](#interactive-mode-allowpty) for details.\n\n### \"EPERM: operation not permitted\"\n\nCheck what path is being blocked:\n- `~/.claude.json` ‚Üí Add `--no-session-persistence`\n- `~/.claude/session-env/` ‚Üí Add to `allowWrite` (required for bash execution)\n- `~/Library/Caches/claude-cli-nodejs/` ‚Üí Disable MCP or allow writes\n- Project files ‚Üí Add project dir to `allowWrite`\n\n### Debug Mode\n\n```bash\nsrt -d -s .srt.json -c 'your-command'\n```\n\nShows sandbox profile and violations.\n\n### Test Write Access\n\n```bash\nsrt -s .srt.json -c 'touch /path/to/test && echo \"write ok\"'\n```\n\n---\n\n## Quick Reference\n\n```bash\n# Install\nnpm install -g @anthropic-ai/sandbox-runtime\n\n# Run sandboxed command\nsrt -s config.json -c 'command'\n\n# Run sandboxed Claude (stateless)\nsrt -s .srt.json -c 'claude --dangerously-skip-permissions \\\n  --no-session-persistence \\\n  --strict-mcp-config --mcp-config \"{\\\"mcpServers\\\":{}}\" \\\n  -p \"prompt\"'\n\n# Debug mode\nsrt -d -s config.json -c 'command'\n```\n",
        "plugins/workflow/skills/subagent/SKILL.md": "---\nname: subagent\ndescription: Activate when you are a delegated subagent (not the orchestrator). Establishes subagent protocol with terse returns, details to history/, file ownership boundaries, and escalation rules. You implement; orchestrator reviews and commits.\n---\n\n# Subagent Protocol\n\nYou are a **subagent** ‚Äî delegated by an orchestrator to implement a specific task. Your job is focused execution with minimal token footprint on return.\n\n---\n\n## Core Rules\n\n1. **Implement the task** as specified in your prompt\n2. **Respect file boundaries** ‚Äî only touch files in your OWN list\n3. **Return terse summaries** ‚Äî details go to `history/`\n4. **Do NOT commit** ‚Äî orchestrator handles git\n5. **Do NOT close beads** ‚Äî orchestrator verifies and closes\n6. **Escalate blockers** ‚Äî don't spin; report and stop\n\n---\n\n## Output Protocol\n\n| Content | Destination |\n|---------|-------------|\n| Summary (1-5 lines) | Return to orchestrator |\n| Implementation details | `history/<bead-id>.md` or `history/session.md` |\n| Logs, traces, verbose output | `history/` or `/tmp/claude-*` |\n| Capability gaps | Summary + `history/gaps.log` |\n\n### Summary Format\n\n```\nDONE: <what you completed>\nCHANGED: <files modified>\nRESULT: <pass/fail, test results if applicable>\nBLOCKERS: <none, or what stopped you>\nGAPS: <capabilities you wished you had>\n```\n\n### History Directory\n\nIf `history/` doesn't exist:\n1. Create it: `mkdir -p history && echo 'history/' >> .gitignore`\n2. If creation blocked, use `/tmp/claude-<project>-<date>.log`\n\n---\n\n## File Ownership\n\nYour prompt should include OWN and READ-ONLY lists.\n\n| List | Permission |\n|------|------------|\n| OWN | Create, edit, delete freely |\n| READ-ONLY | Read only ‚Äî do not modify |\n| Unlisted | Ask orchestrator before touching |\n\n**Never modify:**\n- Git state (no commits, no branch operations)\n- Bead state (no `bd close`, no status changes)\n- Shared config files (package.json, tsconfig.json, etc.)\n- Barrel/index files unless explicitly in OWN list\n\n---\n\n## Quality Gates\n\nRun verification commands specified in your prompt before returning.\n\nCommon gates:\n- `npm run check` (lint + typecheck + test)\n- `cargo check && cargo test`\n- `go build ./... && go test ./...`\n\nIf gates fail, fix and retry. If you can't fix, report in BLOCKERS.\n\n---\n\n## Escalation Rules\n\n**Escalate immediately if:**\n- Task is ambiguous or underspecified\n- Required file is not in OWN or READ-ONLY list\n- You need to modify shared config\n- Security-sensitive changes required (auth, secrets, input validation)\n- Quality gates fail and you can't resolve\n- You've made 3+ attempts without progress\n\n**How to escalate:**\nReturn summary with BLOCKERS section explaining what you need.\n\n---\n\n## Skills Activation\n\nActivate skills specified in your prompt. Common ones:\n- `typescript-pro`, `go-pro`, `rust-pro` ‚Äî language expertise\n- `solid-architecture` ‚Äî design principles\n- `game-perf` ‚Äî hot-path optimization\n\n---\n\n## Anti-Patterns\n\n| Don't | Why |\n|-------|-----|\n| Return full file contents | Wastes orchestrator tokens |\n| Dump verbose logs | Put in history/ instead |\n| Commit changes | Orchestrator owns git |\n| Close beads | Orchestrator verifies first |\n| Modify unlisted files | Violates ownership boundaries |\n| Spin on blockers | Escalate after 3 attempts |\n| Hide failures | Report honestly in summary |\n",
        "plugins/workflow/skills/tdd/SKILL.md": "---\nname: tdd\ndescription: Use when implementing any feature or bugfix, before writing implementation code - write the test first, watch it fail, write minimal code to pass; ensures tests actually verify behavior by requiring failure first\n---\n\n# Test-Driven Development (TDD)\n\n## Overview\n\nWrite the test first. Watch it fail. Write minimal code to pass.\n\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\n\n**Violating the letter of the rules is violating the spirit of the rules.**\n\n## When to Use\n\n**Always:**\n- New features\n- Bug fixes\n- Refactoring\n- Behavior changes\n\n**Exceptions (ask your human partner):**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\nThinking \"skip TDD just this once\"? Stop. That's rationalization.\n\n## The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrite code before the test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n\nImplement fresh from tests. Period.\n\n## Red-Green-Refactor\n\n```\nRED: Write failing test\n  |\n  v\nVerify fails correctly? --no--> Fix test, retry\n  |\n  yes\n  v\nGREEN: Write minimal code\n  |\n  v\nVerify passes? --no--> Fix code, retry\n  |\n  yes\n  v\nREFACTOR: Clean up (stay green)\n  |\n  v\nNext test\n```\n\n### RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n**Good:**\n```typescript\ntest('retries failed operations 3 times', async () => {\n  let attempts = 0;\n  const operation = () => {\n    attempts++;\n    if (attempts < 3) throw new Error('fail');\n    return 'success';\n  };\n\n  const result = await retryOperation(operation);\n\n  expect(result).toBe('success');\n  expect(attempts).toBe(3);\n});\n```\nClear name, tests real behavior, one thing.\n\n**Bad:**\n```typescript\ntest('retry works', async () => {\n  const mock = jest.fn()\n    .mockRejectedValueOnce(new Error())\n    .mockRejectedValueOnce(new Error())\n    .mockResolvedValueOnce('success');\n  await retryOperation(mock);\n  expect(mock).toHaveBeenCalledTimes(3);\n});\n```\nVague name, tests mock not code.\n\n**Requirements:**\n- One behavior\n- Clear name\n- Real code (no mocks unless unavoidable)\n\n### Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test fails (not errors)\n- Failure message is expected\n- Fails because feature missing (not typos)\n\n**Test passes?** You're testing existing behavior. Fix test.\n\n**Test errors?** Fix error, re-run until it fails correctly.\n\n### GREEN - Minimal Code\n\nWrite simplest code to pass the test.\n\n**Good:**\n```typescript\nasync function retryOperation<T>(fn: () => Promise<T>): Promise<T> {\n  for (let i = 0; i < 3; i++) {\n    try {\n      return await fn();\n    } catch (e) {\n      if (i === 2) throw e;\n    }\n  }\n  throw new Error('unreachable');\n}\n```\nJust enough to pass.\n\n**Bad:**\n```typescript\nasync function retryOperation<T>(\n  fn: () => Promise<T>,\n  options?: {\n    maxRetries?: number;\n    backoff?: 'linear' | 'exponential';\n    onRetry?: (attempt: number) => void;\n  }\n): Promise<T> {\n  // YAGNI\n}\n```\nOver-engineered.\n\nDon't add features, refactor other code, or \"improve\" beyond the test.\n\n### Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test passes\n- Other tests still pass\n- Output pristine (no errors, warnings)\n\n**Test fails?** Fix code, not test.\n\n**Other tests fail?** Fix now.\n\n### REFACTOR - Clean Up\n\nAfter green only:\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n### Repeat\n\nNext failing test for next feature.\n\n## Good Tests\n\n| Quality | Good | Bad |\n|---------|------|-----|\n| **Minimal** | One thing. \"and\" in name? Split it. | `test('validates email and domain and whitespace')` |\n| **Clear** | Name describes behavior | `test('test1')` |\n| **Shows intent** | Demonstrates desired API | Obscures what code should do |\n\n## Why Order Matters\n\n**\"I'll write tests after to verify it works\"**\n\nTests written after code pass immediately. Passing immediately proves nothing:\n- Might test wrong thing\n- Might test implementation, not behavior\n- Might miss edge cases you forgot\n- You never saw it catch the bug\n\nTest-first forces you to see the test fail, proving it actually tests something.\n\n**\"I already manually tested all the edge cases\"**\n\nManual testing is ad-hoc. You think you tested everything but:\n- No record of what you tested\n- Can't re-run when code changes\n- Easy to forget cases under pressure\n- \"It worked when I tried it\" does not mean comprehensive\n\nAutomated tests are systematic. They run the same way every time.\n\n**\"Deleting X hours of work is wasteful\"**\n\nSunk cost fallacy. The time is already gone. Your choice now:\n- Delete and rewrite with TDD (X more hours, high confidence)\n- Keep it and add tests after (30 min, low confidence, likely bugs)\n\nThe \"waste\" is keeping code you can't trust. Working code without real tests is technical debt.\n\n**\"TDD is dogmatic, being pragmatic means adapting\"**\n\nTDD IS pragmatic:\n- Finds bugs before commit (faster than debugging after)\n- Prevents regressions (tests catch breaks immediately)\n- Documents behavior (tests show how to use code)\n- Enables refactoring (change freely, tests catch breaks)\n\n\"Pragmatic\" shortcuts = debugging in production = slower.\n\n**\"Tests after achieve the same goals - it's spirit not ritual\"**\n\nNo. Tests-after answer \"What does this do?\" Tests-first answer \"What should this do?\"\n\nTests-after are biased by your implementation. You test what you built, not what's required. You verify remembered edge cases, not discovered ones.\n\nTests-first force edge case discovery before implementing. Tests-after verify you remembered everything (you didn't).\n\n30 minutes of tests after does not equal TDD. You get coverage, lose proof tests work.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\n| \"Already manually tested\" | Ad-hoc does not equal systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Keeping unverified code is technical debt. |\n| \"Keep as reference, write tests first\" | You'll adapt it. That's testing after. Delete means delete. |\n| \"Need to explore first\" | Fine. Throw away exploration, start with TDD. |\n| \"Test hard = design unclear\" | Listen to test. Hard to test = hard to use. |\n| \"TDD will slow me down\" | TDD faster than debugging. Pragmatic = test-first. |\n| \"Manual test faster\" | Manual doesn't prove edge cases. You'll re-test every change. |\n| \"Existing code has no tests\" | You're improving it. Add tests for existing code. |\n\n## Red Flags - STOP and Start Over\n\n- Code before test\n- Test after implementation\n- Test passes immediately\n- Can't explain why test failed\n- Tests added \"later\"\n- Rationalizing \"just this once\"\n- \"I already manually tested it\"\n- \"Tests after achieve the same purpose\"\n- \"It's about spirit not ritual\"\n- \"Keep as reference\" or \"adapt existing code\"\n- \"Already spent X hours, deleting is wasteful\"\n- \"TDD is dogmatic, I'm being pragmatic\"\n- \"This is different because...\"\n\n**All of these mean: Delete code. Start over with TDD.**\n\n## Example: Bug Fix\n\n**Bug:** Empty email accepted\n\n**RED**\n```typescript\ntest('rejects empty email', async () => {\n  const result = await submitForm({ email: '' });\n  expect(result.error).toBe('Email required');\n});\n```\n\n**Verify RED**\n```bash\n$ npm test\nFAIL: expected 'Email required', got undefined\n```\n\n**GREEN**\n```typescript\nfunction submitForm(data: FormData) {\n  if (!data.email?.trim()) {\n    return { error: 'Email required' };\n  }\n  // ...\n}\n```\n\n**Verify GREEN**\n```bash\n$ npm test\nPASS\n```\n\n**REFACTOR**\nExtract validation for multiple fields if needed.\n\n## Verification Checklist\n\nBefore marking work complete:\n\n- [ ] Every new function/method has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason (feature missing, not typo)\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] Output pristine (no errors, warnings)\n- [ ] Tests use real code (mocks only if unavoidable)\n- [ ] Edge cases and errors covered\n\nCan't check all boxes? You skipped TDD. Start over.\n\n## When Stuck\n\n| Problem | Solution |\n|---------|----------|\n| Don't know how to test | Write wished-for API. Write assertion first. Ask your human partner. |\n| Test too complicated | Design too complicated. Simplify interface. |\n| Must mock everything | Code too coupled. Use dependency injection. |\n| Test setup huge | Extract helpers. Still complex? Simplify design. |\n\n## Debugging Integration\n\nBug found? Write failing test reproducing it. Follow TDD cycle. Test proves fix and prevents regression.\n\nNever fix bugs without a test.\n\n## Testing Anti-Patterns\n\nWhen adding mocks or test utilities, review testing-anti-patterns.md to avoid:\n- Testing mock behavior instead of real behavior\n- Adding test-only methods to production classes\n- Mocking without understanding dependencies\n- Incomplete mocks that miss fields\n\n## Final Rule\n\n```\nProduction code --> test exists and failed first\nOtherwise --> not TDD\n```\n\nNo exceptions without your human partner's permission.\n",
        "plugins/workflow/skills/tdd/testing-anti-patterns.md": "# Testing Anti-Patterns\n\n**Load this reference when:** writing or changing tests, adding mocks, or tempted to add test-only methods to production code.\n\n## Overview\n\nTests must verify real behavior, not mock behavior. Mocks are a means to isolate, not the thing being tested.\n\n**Core principle:** Test what the code does, not what the mocks do.\n\n**Following strict TDD prevents these anti-patterns.**\n\n## The Iron Laws\n\n```\n1. NEVER test mock behavior\n2. NEVER add test-only methods to production classes\n3. NEVER mock without understanding dependencies\n```\n\n## Anti-Pattern 1: Testing Mock Behavior\n\n**The violation:**\n```typescript\n// BAD: Testing that the mock exists\ntest('renders sidebar', () => {\n  render(<Page />);\n  expect(screen.getByTestId('sidebar-mock')).toBeInTheDocument();\n});\n```\n\n**Why this is wrong:**\n- You're verifying the mock works, not that the component works\n- Test passes when mock is present, fails when it's not\n- Tells you nothing about real behavior\n\n**The fix:**\n```typescript\n// GOOD: Test real component or don't mock it\ntest('renders sidebar', () => {\n  render(<Page />);  // Don't mock sidebar\n  expect(screen.getByRole('navigation')).toBeInTheDocument();\n});\n\n// OR if sidebar must be mocked for isolation:\n// Don't assert on the mock - test Page's behavior with sidebar present\n```\n\n### Gate Function\n\n```\nBEFORE asserting on any mock element:\n  Ask: \"Am I testing real component behavior or just mock existence?\"\n\n  IF testing mock existence:\n    STOP - Delete the assertion or unmock the component\n\n  Test real behavior instead\n```\n\n## Anti-Pattern 2: Test-Only Methods in Production\n\n**The violation:**\n```typescript\n// BAD: destroy() only used in tests\nclass Session {\n  async destroy() {  // Looks like production API!\n    await this._workspaceManager?.destroyWorkspace(this.id);\n    // ... cleanup\n  }\n}\n\n// In tests\nafterEach(() => session.destroy());\n```\n\n**Why this is wrong:**\n- Production class polluted with test-only code\n- Dangerous if accidentally called in production\n- Violates YAGNI and separation of concerns\n- Confuses object lifecycle with entity lifecycle\n\n**The fix:**\n```typescript\n// GOOD: Test utilities handle test cleanup\n// Session has no destroy() - it's stateless in production\n\n// In test-utils/\nexport async function cleanupSession(session: Session) {\n  const workspace = session.getWorkspaceInfo();\n  if (workspace) {\n    await workspaceManager.destroyWorkspace(workspace.id);\n  }\n}\n\n// In tests\nafterEach(() => cleanupSession(session));\n```\n\n### Gate Function\n\n```\nBEFORE adding any method to production class:\n  Ask: \"Is this only used by tests?\"\n\n  IF yes:\n    STOP - Don't add it\n    Put it in test utilities instead\n\n  Ask: \"Does this class own this resource's lifecycle?\"\n\n  IF no:\n    STOP - Wrong class for this method\n```\n\n## Anti-Pattern 3: Mocking Without Understanding\n\n**The violation:**\n```typescript\n// BAD: Mock breaks test logic\ntest('detects duplicate server', () => {\n  // Mock prevents config write that test depends on!\n  vi.mock('ToolCatalog', () => ({\n    discoverAndCacheTools: vi.fn().mockResolvedValue(undefined)\n  }));\n\n  await addServer(config);\n  await addServer(config);  // Should throw - but won't!\n});\n```\n\n**Why this is wrong:**\n- Mocked method had side effect test depended on (writing config)\n- Over-mocking to \"be safe\" breaks actual behavior\n- Test passes for wrong reason or fails mysteriously\n\n**The fix:**\n```typescript\n// GOOD: Mock at correct level\ntest('detects duplicate server', () => {\n  // Mock the slow part, preserve behavior test needs\n  vi.mock('MCPServerManager'); // Just mock slow server startup\n\n  await addServer(config);  // Config written\n  await addServer(config);  // Duplicate detected\n});\n```\n\n### Gate Function\n\n```\nBEFORE mocking any method:\n  STOP - Don't mock yet\n\n  1. Ask: \"What side effects does the real method have?\"\n  2. Ask: \"Does this test depend on any of those side effects?\"\n  3. Ask: \"Do I fully understand what this test needs?\"\n\n  IF depends on side effects:\n    Mock at lower level (the actual slow/external operation)\n    OR use test doubles that preserve necessary behavior\n    NOT the high-level method the test depends on\n\n  IF unsure what test depends on:\n    Run test with real implementation FIRST\n    Observe what actually needs to happen\n    THEN add minimal mocking at the right level\n\n  Red flags:\n    - \"I'll mock this to be safe\"\n    - \"This might be slow, better mock it\"\n    - Mocking without understanding the dependency chain\n```\n\n## Anti-Pattern 4: Incomplete Mocks\n\n**The violation:**\n```typescript\n// BAD: Partial mock - only fields you think you need\nconst mockResponse = {\n  status: 'success',\n  data: { userId: '123', name: 'Alice' }\n  // Missing: metadata that downstream code uses\n};\n\n// Later: breaks when code accesses response.metadata.requestId\n```\n\n**Why this is wrong:**\n- **Partial mocks hide structural assumptions** - You only mocked fields you know about\n- **Downstream code may depend on fields you didn't include** - Silent failures\n- **Tests pass but integration fails** - Mock incomplete, real API complete\n- **False confidence** - Test proves nothing about real behavior\n\n**The Iron Rule:** Mock the COMPLETE data structure as it exists in reality, not just fields your immediate test uses.\n\n**The fix:**\n```typescript\n// GOOD: Mirror real API completeness\nconst mockResponse = {\n  status: 'success',\n  data: { userId: '123', name: 'Alice' },\n  metadata: { requestId: 'req-789', timestamp: 1234567890 }\n  // All fields real API returns\n};\n```\n\n### Gate Function\n\n```\nBEFORE creating mock responses:\n  Check: \"What fields does the real API response contain?\"\n\n  Actions:\n    1. Examine actual API response from docs/examples\n    2. Include ALL fields system might consume downstream\n    3. Verify mock matches real response schema completely\n\n  Critical:\n    If you're creating a mock, you must understand the ENTIRE structure\n    Partial mocks fail silently when code depends on omitted fields\n\n  If uncertain: Include all documented fields\n```\n\n## Anti-Pattern 5: Integration Tests as Afterthought\n\n**The violation:**\n```\nImplementation complete\nNo tests written\n\"Ready for testing\"\n```\n\n**Why this is wrong:**\n- Testing is part of implementation, not optional follow-up\n- TDD would have caught this\n- Can't claim complete without tests\n\n**The fix:**\n```\nTDD cycle:\n1. Write failing test\n2. Implement to pass\n3. Refactor\n4. THEN claim complete\n```\n\n## When Mocks Become Too Complex\n\n**Warning signs:**\n- Mock setup longer than test logic\n- Mocking everything to make test pass\n- Mocks missing methods real components have\n- Test breaks when mock changes\n\n**Consider:** Integration tests with real components often simpler than complex mocks\n\n## TDD Prevents These Anti-Patterns\n\n**Why TDD helps:**\n1. **Write test first** - Forces you to think about what you're actually testing\n2. **Watch it fail** - Confirms test tests real behavior, not mocks\n3. **Minimal implementation** - No test-only methods creep in\n4. **Real dependencies** - You see what the test actually needs before mocking\n\n**If you're testing mock behavior, you violated TDD** - you added mocks without watching test fail against real code first.\n\n## Quick Reference\n\n| Anti-Pattern | Fix |\n|--------------|-----|\n| Assert on mock elements | Test real component or unmock it |\n| Test-only methods in production | Move to test utilities |\n| Mock without understanding | Understand dependencies first, mock minimally |\n| Incomplete mocks | Mirror real API completely |\n| Tests as afterthought | TDD - tests first |\n| Over-complex mocks | Consider integration tests |\n\n## Red Flags\n\n- Assertion checks for `*-mock` test IDs\n- Methods only called in test files\n- Mock setup is >50% of test\n- Test fails when you remove mock\n- Can't explain why mock is needed\n- Mocking \"just to be safe\"\n\n## The Bottom Line\n\n**Mocks are tools to isolate, not things to test.**\n\nIf TDD reveals you're testing mock behavior, you've gone wrong.\n\nFix: Test real behavior or question why you're mocking at all.\n",
        "plugins/workflow/skills/worktrees/SKILL.md": "---\nname: worktrees\ndescription: Use when starting feature work that needs isolation from current workspace or before executing implementation plans - creates isolated git worktrees with beads integration via bd worktree commands\n---\n\n# Git Worktrees\n\nGit worktrees create isolated workspaces sharing the same repository, allowing work on multiple branches simultaneously without switching.\n\n**Primary tool:** `bd worktree` ‚Äî handles git worktree + beads integration automatically.\n\n---\n\n## When to Use\n\n- Parallel subagents need filesystem isolation\n- Feature work that shouldn't affect current workspace\n- Separate builds/servers running simultaneously\n- Before executing implementation plans\n\n---\n\n## Setup: Ensure .worktrees/ is Ignored\n\n**Before creating any worktrees**, ensure `.worktrees/` is in `.gitignore`. This is a one-time setup that covers all future worktrees:\n\n```bash\n# Check if .worktrees/ is already ignored\ngit check-ignore -q .worktrees/ || echo '.worktrees/' >> .gitignore\n```\n\nIf you added the line, commit it:\n```bash\ngit add .gitignore && git commit -m \"Ignore .worktrees/ directory\"\n```\n\n**Why this matters:** Without this, beads adds each worktree individually to `.gitignore`, creating noise. With `.worktrees/` ignored, all worktrees underneath are automatically covered.\n\n---\n\n## Creating a Worktree\n\n**All worktrees go under `.worktrees/` in the repo root.** This is the standard location.\n\n```bash\nbd worktree create .worktrees/feature-auth\n```\n\n**What it does automatically:**\n1. Creates git worktree at the specified path\n2. Sets up `.beads/redirect` pointing to main repo's database\n3. Creates the branch (same name as the directory by default)\n\n**With custom branch name:**\n```bash\nbd worktree create .worktrees/bugfix --branch fix-123\n```\n\n---\n\n## After Creation\n\n### 1. Enter Worktree\n\n```bash\ncd .worktrees/feature-auth\n```\n\n### 2. Run Project Setup\n\n```bash\n# Node.js\n[ -f package.json ] && npm install\n\n# Rust\n[ -f Cargo.toml ] && cargo build\n\n# Go\n[ -f go.mod ] && go mod download\n```\n\n### 3. Verify Baseline\n\n```bash\nnpm test  # or cargo test, go test ./...\n```\n\n**If tests fail:** Report failures, ask whether to proceed.\n\n### 4. Verify Beads Shared\n\n```bash\nbd ready  # Should show same beads as main workspace\n```\n\n---\n\n## Listing Worktrees\n\n```bash\nbd worktree list\n```\n\nOr standard git:\n```bash\ngit worktree list\n```\n\n---\n\n## Removing a Worktree\n\nUse `bd worktree remove` ‚Äî includes safety checks:\n\n```bash\nbd worktree remove feature-auth\n```\n\n**Safety checks (automatic):**\n- Uncommitted changes\n- Unpushed commits\n- Stashes\n\n**Skip checks (not recommended):**\n```bash\nbd worktree remove feature-auth --force\n```\n\n---\n\n## Worktree Info\n\nCheck current worktree status:\n\n```bash\nbd worktree info\n```\n\n---\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| Ensure .worktrees/ ignored | `git check-ignore -q .worktrees/ \\|\\| echo '.worktrees/' >> .gitignore` |\n| Create worktree | `bd worktree create .worktrees/<name>` |\n| Create with branch | `bd worktree create .worktrees/<name> --branch <branch>` |\n| List worktrees | `bd worktree list` |\n| Remove worktree | `bd worktree remove .worktrees/<name>` |\n| Check status | `bd worktree info` |\n| Verify beads sync | `bd ready` (in worktree) |\n\n---\n\n## Why bd worktree?\n\n| Manual git worktree | bd worktree |\n|---------------------|-------------|\n| Separate commands for git + beads | Single command |\n| No beads redirect setup | Automatic redirect to main DB |\n| No safety checks on remove | Checks for uncommitted/unpushed |\n\n---\n\n## Example Workflow\n\n```bash\n# One-time: ensure .worktrees/ is ignored\ngit check-ignore -q .worktrees/ || echo '.worktrees/' >> .gitignore\n\n# Create isolated workspace\nbd worktree create .worktrees/feature-auth\n\n# Enter and setup\ncd .worktrees/feature-auth\nnpm install\nnpm test  # ‚úì 47 passing\n\n# Verify beads shared\nbd ready  # Shows same issues as main\n\n# Work on feature...\nbd claim auth-001\n\n# When done\ncd ../..\nbd worktree remove .worktrees/feature-auth\n```\n\n---\n\n## Known Limitations\n\n### Daemon Mode\n\nDaemon mode does not work correctly with user-created git worktrees. Worktrees share the same `.git` directory and beads database, but the daemon doesn't track which branch each worktree has checked out.\n\n**Solution**: Use direct mode in worktrees:\n```bash\nbd --no-daemon <command>\n# Or set environment variable\nexport BEADS_NO_DAEMON=1\n```\n\n### Two Types of Worktrees\n\nDon't confuse these:\n\n| Type | Location | Purpose |\n|------|----------|---------|\n| User worktrees | `.worktrees/<name>` | Parallel feature work (you create these) |\n| Beads internal | `.git/beads-worktrees/beads-sync` | Sync-branch commits (beads creates this) |\n\nThe internal worktree is hidden and managed by beads for the sync-branch feature. Don't manually modify it.\n\n### SKIP_WORKTREE Issues\n\nIf `git status` doesn't show changes to `.beads/*.jsonl` files, check for SKIP_WORKTREE flags:\n\n```bash\ngit ls-files -v .beads/\n# 'h' prefix = SKIP_WORKTREE set (changes hidden)\n# 'H' prefix = normal tracking\n```\n\n**Fix**: Remove and re-add the files:\n```bash\ngit rm --cached .beads/issues.jsonl\ngit add .beads/issues.jsonl\n```\n\nOr run `bd sync` which sets the correct index flags.\n\n---\n\n## Fallback (No Beads)\n\nIf beads isn't installed, use manual git worktree:\n\n```bash\n# Verify ignored\ngit check-ignore -q .worktrees/ || echo '.worktrees/' >> .gitignore\n\n# Create\ngit worktree add .worktrees/feature-auth -b feature-auth\n\n# Remove\ngit worktree remove .worktrees/feature-auth\n```\n\nBut you lose: automatic gitignore, beads sync, and safety checks.\n"
      },
      "plugins": [
        {
          "name": "dm-lang",
          "description": "Expert language skills: Go, Rust, TypeScript, just - idiomatic patterns, type systems, and build tools",
          "version": "0.1.0",
          "author": {
            "name": "Dark Matter",
            "email": "dark-matter@example.com"
          },
          "source": "./plugins/language-pro",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add rbergman/dark-matter-marketplace",
            "/plugin install dm-lang@dark-matter-marketplace"
          ]
        },
        {
          "name": "dm-arch",
          "description": "Architecture patterns: SOLID principles, composition, data-oriented design, and module organization",
          "version": "0.1.0",
          "author": {
            "name": "Dark Matter",
            "email": "dark-matter@example.com"
          },
          "source": "./plugins/architecture",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add rbergman/dark-matter-marketplace",
            "/plugin install dm-arch@dark-matter-marketplace"
          ]
        },
        {
          "name": "dm-game",
          "description": "Game development: 5-component design framework and per-frame performance optimization",
          "version": "0.1.0",
          "author": {
            "name": "Dark Matter",
            "email": "dark-matter@example.com"
          },
          "source": "./plugins/game-dev",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add rbergman/dark-matter-marketplace",
            "/plugin install dm-game@dark-matter-marketplace"
          ]
        },
        {
          "name": "dm-work",
          "description": "Workflow tools: orchestration, spec refinement, compression, peer review, and subagent delegation",
          "version": "0.1.0",
          "author": {
            "name": "Dark Matter",
            "email": "dark-matter@example.com"
          },
          "source": "./plugins/workflow",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add rbergman/dark-matter-marketplace",
            "/plugin install dm-work@dark-matter-marketplace"
          ]
        },
        {
          "name": "dm-drvr",
          "description": "AI drivers: delegate to Codex CLI or leverage Gemini's 1M token context",
          "version": "0.1.0",
          "author": {
            "name": "Dark Matter",
            "email": "dark-matter@example.com"
          },
          "source": "./plugins/drivers",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add rbergman/dark-matter-marketplace",
            "/plugin install dm-drvr@dark-matter-marketplace"
          ]
        },
        {
          "name": "dm-tool",
          "description": "Tool design patterns: building CLIs, MCPs, and APIs that agents can use effectively",
          "version": "0.1.0",
          "author": {
            "name": "Dark Matter",
            "email": "dark-matter@example.com"
          },
          "source": "./plugins/tooling",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add rbergman/dark-matter-marketplace",
            "/plugin install dm-tool@dark-matter-marketplace"
          ]
        }
      ]
    }
  ]
}