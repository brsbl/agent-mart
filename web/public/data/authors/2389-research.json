{
  "author": {
    "id": "2389-research",
    "display_name": "2389 Research, Inc",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/166545110?v=4",
    "url": "https://github.com/2389-research",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 23,
      "total_commands": 0,
      "total_skills": 31,
      "total_stars": 13,
      "total_forks": 1
    }
  },
  "marketplaces": [
    {
      "name": "2389-research-marketplace",
      "version": null,
      "description": "Plugins and MCP servers we use at 2389",
      "owner_info": {
        "name": "2389 Research Inc",
        "email": "hello@2389.ai",
        "url": "https://2389.ai"
      },
      "keywords": [],
      "repo_full_name": "2389-research/claude-plugins",
      "repo_url": "https://github.com/2389-research/claude-plugins",
      "repo_description": "Curated plugins for Claude Code",
      "homepage": "https://2389-research.github.io/claude-plugins/",
      "signals": {
        "stars": 13,
        "forks": 1,
        "pushed_at": "2026-01-21T15:00:26Z",
        "created_at": "2025-10-15T16:54:23Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 13754
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 166
        },
        {
          "path": "better-dev",
          "type": "tree",
          "size": null
        },
        {
          "path": "better-dev/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "better-dev/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 164
        },
        {
          "path": "better-dev/README.md",
          "type": "blob",
          "size": 1697
        },
        {
          "path": "binary-re",
          "type": "tree",
          "size": null
        },
        {
          "path": "binary-re/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "binary-re/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 231
        },
        {
          "path": "binary-re/README.md",
          "type": "blob",
          "size": 3037
        },
        {
          "path": "binary-re/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "binary-re/skills/SKILL.md",
          "type": "blob",
          "size": 10988
        },
        {
          "path": "binary-re/skills/dynamic-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "binary-re/skills/dynamic-analysis/SKILL.md",
          "type": "blob",
          "size": 14985
        },
        {
          "path": "binary-re/skills/static-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "binary-re/skills/static-analysis/SKILL.md",
          "type": "blob",
          "size": 10015
        },
        {
          "path": "binary-re/skills/synthesis",
          "type": "tree",
          "size": null
        },
        {
          "path": "binary-re/skills/synthesis/SKILL.md",
          "type": "blob",
          "size": 9835
        },
        {
          "path": "binary-re/skills/tool-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "binary-re/skills/tool-setup/SKILL.md",
          "type": "blob",
          "size": 10867
        },
        {
          "path": "binary-re/skills/triage",
          "type": "tree",
          "size": null
        },
        {
          "path": "binary-re/skills/triage/SKILL.md",
          "type": "blob",
          "size": 6734
        },
        {
          "path": "botboard-biz",
          "type": "tree",
          "size": null
        },
        {
          "path": "botboard-biz/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "botboard-biz/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 147
        },
        {
          "path": "botboard-biz/README.md",
          "type": "blob",
          "size": 1207
        },
        {
          "path": "building-multiagent-systems",
          "type": "tree",
          "size": null
        },
        {
          "path": "building-multiagent-systems/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "building-multiagent-systems/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 186
        },
        {
          "path": "building-multiagent-systems/README.md",
          "type": "blob",
          "size": 4458
        },
        {
          "path": "building-multiagent-systems/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "building-multiagent-systems/skills/SKILL.md",
          "type": "blob",
          "size": 8873
        },
        {
          "path": "building-multiagent-systems/skills/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "building-multiagent-systems/skills/references/coordination-patterns.md",
          "type": "blob",
          "size": 4745
        },
        {
          "path": "building-multiagent-systems/skills/references/four-layer-architecture.md",
          "type": "blob",
          "size": 4118
        },
        {
          "path": "building-multiagent-systems/skills/references/maker-pattern.md",
          "type": "blob",
          "size": 5271
        },
        {
          "path": "building-multiagent-systems/skills/references/production-hardening.md",
          "type": "blob",
          "size": 7495
        },
        {
          "path": "building-multiagent-systems/skills/references/tool-coordination.md",
          "type": "blob",
          "size": 4643
        },
        {
          "path": "ceo-personal-os",
          "type": "tree",
          "size": null
        },
        {
          "path": "ceo-personal-os/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "ceo-personal-os/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 219
        },
        {
          "path": "ceo-personal-os/README.md",
          "type": "blob",
          "size": 3328
        },
        {
          "path": "ceo-personal-os/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "ceo-personal-os/skills/SKILL.md",
          "type": "blob",
          "size": 6219
        },
        {
          "path": "ceo-personal-os/skills/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks",
          "type": "tree",
          "size": null
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/blank-customer-development.md",
          "type": "blob",
          "size": 2967
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/campbell-coaching.md",
          "type": "blob",
          "size": 1608
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/collins-good-to-great.md",
          "type": "blob",
          "size": 1990
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/eisenmann-failure-patterns.md",
          "type": "blob",
          "size": 2666
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/ferriss-lifestyle-costing.md",
          "type": "blob",
          "size": 1010
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/gerber-emyth.md",
          "type": "blob",
          "size": 2301
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/gustin-annual-review.md",
          "type": "blob",
          "size": 875
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/lieberman-life-map.md",
          "type": "blob",
          "size": 1001
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/martell-buyback-time.md",
          "type": "blob",
          "size": 1935
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/robbins-vivid-vision.md",
          "type": "blob",
          "size": 1116
        },
        {
          "path": "ceo-personal-os/skills/references/frameworks/schumacher-human-scale.md",
          "type": "blob",
          "size": 5666
        },
        {
          "path": "ceo-personal-os/skills/references/interviews",
          "type": "tree",
          "size": null
        },
        {
          "path": "ceo-personal-os/skills/references/interviews/interview-scripts.md",
          "type": "blob",
          "size": 8006
        },
        {
          "path": "css-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "css-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "css-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 169
        },
        {
          "path": "css-development/README.md",
          "type": "blob",
          "size": 1459
        },
        {
          "path": "css-development/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "css-development/skills/SKILL.md",
          "type": "blob",
          "size": 6045
        },
        {
          "path": "css-development/skills/create-component",
          "type": "tree",
          "size": null
        },
        {
          "path": "css-development/skills/create-component/SKILL.md",
          "type": "blob",
          "size": 10962
        },
        {
          "path": "css-development/skills/refactor",
          "type": "tree",
          "size": null
        },
        {
          "path": "css-development/skills/refactor/SKILL.md",
          "type": "blob",
          "size": 11296
        },
        {
          "path": "css-development/skills/validate",
          "type": "tree",
          "size": null
        },
        {
          "path": "css-development/skills/validate/SKILL.md",
          "type": "blob",
          "size": 9945
        },
        {
          "path": "documentation-audit",
          "type": "tree",
          "size": null
        },
        {
          "path": "documentation-audit/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "documentation-audit/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 237
        },
        {
          "path": "documentation-audit/README.md",
          "type": "blob",
          "size": 1158
        },
        {
          "path": "documentation-audit/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "documentation-audit/skills/SKILL.md",
          "type": "blob",
          "size": 3569
        },
        {
          "path": "documentation-audit/skills/checklist.md",
          "type": "blob",
          "size": 3227
        },
        {
          "path": "documentation-audit/skills/extraction-patterns.md",
          "type": "blob",
          "size": 3930
        },
        {
          "path": "firebase-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "firebase-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "firebase-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 159
        },
        {
          "path": "firebase-development/README.md",
          "type": "blob",
          "size": 1941
        },
        {
          "path": "firebase-development/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "firebase-development/skills/SKILL.md",
          "type": "blob",
          "size": 6322
        },
        {
          "path": "firebase-development/skills/add-feature",
          "type": "tree",
          "size": null
        },
        {
          "path": "firebase-development/skills/add-feature/SKILL.md",
          "type": "blob",
          "size": 6634
        },
        {
          "path": "firebase-development/skills/debug",
          "type": "tree",
          "size": null
        },
        {
          "path": "firebase-development/skills/debug/SKILL.md",
          "type": "blob",
          "size": 5727
        },
        {
          "path": "firebase-development/skills/project-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "firebase-development/skills/project-setup/SKILL.md",
          "type": "blob",
          "size": 6349
        },
        {
          "path": "firebase-development/skills/validate",
          "type": "tree",
          "size": null
        },
        {
          "path": "firebase-development/skills/validate/SKILL.md",
          "type": "blob",
          "size": 6163
        },
        {
          "path": "fresh-eyes-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "fresh-eyes-review/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "fresh-eyes-review/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 207
        },
        {
          "path": "fresh-eyes-review/README.md",
          "type": "blob",
          "size": 3007
        },
        {
          "path": "fresh-eyes-review/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "fresh-eyes-review/skills/SKILL.md",
          "type": "blob",
          "size": 6298
        },
        {
          "path": "gtm-partner",
          "type": "tree",
          "size": null
        },
        {
          "path": "gtm-partner/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "gtm-partner/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 202
        },
        {
          "path": "gtm-partner/README.md",
          "type": "blob",
          "size": 2991
        },
        {
          "path": "gtm-partner/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "gtm-partner/skills/SKILL.md",
          "type": "blob",
          "size": 22627
        },
        {
          "path": "landing-page-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "landing-page-design/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "landing-page-design/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 192
        },
        {
          "path": "landing-page-design/README.md",
          "type": "blob",
          "size": 3029
        },
        {
          "path": "landing-page-design/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "landing-page-design/skills/SKILL.md",
          "type": "blob",
          "size": 14650
        },
        {
          "path": "product-launcher",
          "type": "tree",
          "size": null
        },
        {
          "path": "product-launcher/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "product-launcher/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 203
        },
        {
          "path": "product-launcher/README.md",
          "type": "blob",
          "size": 2113
        },
        {
          "path": "product-launcher/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "product-launcher/skills/SKILL.md",
          "type": "blob",
          "size": 11089
        },
        {
          "path": "remote-system-maintenance",
          "type": "tree",
          "size": null
        },
        {
          "path": "remote-system-maintenance/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "remote-system-maintenance/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 202
        },
        {
          "path": "remote-system-maintenance/README.md",
          "type": "blob",
          "size": 3695
        },
        {
          "path": "remote-system-maintenance/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "remote-system-maintenance/skills/SKILL.md",
          "type": "blob",
          "size": 3162
        },
        {
          "path": "scenario-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "scenario-testing/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "scenario-testing/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 191
        },
        {
          "path": "scenario-testing/README.md",
          "type": "blob",
          "size": 4147
        },
        {
          "path": "scenario-testing/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "scenario-testing/skills/SKILL.md",
          "type": "blob",
          "size": 3327
        },
        {
          "path": "slack-mcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "slack-mcp/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "slack-mcp/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 331
        },
        {
          "path": "slack-mcp/README.md",
          "type": "blob",
          "size": 2629
        },
        {
          "path": "sysadmin",
          "type": "tree",
          "size": null
        },
        {
          "path": "sysadmin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "sysadmin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 142
        },
        {
          "path": "sysadmin/README.md",
          "type": "blob",
          "size": 1573
        },
        {
          "path": "terminal-title",
          "type": "tree",
          "size": null
        },
        {
          "path": "terminal-title/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "terminal-title/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 217
        },
        {
          "path": "terminal-title/README.md",
          "type": "blob",
          "size": 2545
        },
        {
          "path": "terminal-title/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "terminal-title/hooks/hooks.json",
          "type": "blob",
          "size": 260
        },
        {
          "path": "terminal-title/hooks/session-start-launcher.sh",
          "type": "blob",
          "size": 1335
        },
        {
          "path": "terminal-title/hooks/session-start-title.ps1",
          "type": "blob",
          "size": 4140
        },
        {
          "path": "terminal-title/hooks/session-start-title.sh",
          "type": "blob",
          "size": 3312
        },
        {
          "path": "terminal-title/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "terminal-title/skills/SKILL.md",
          "type": "blob",
          "size": 9998
        },
        {
          "path": "test-kitchen",
          "type": "tree",
          "size": null
        },
        {
          "path": "test-kitchen/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "test-kitchen/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 213
        },
        {
          "path": "test-kitchen/README.md",
          "type": "blob",
          "size": 4708
        },
        {
          "path": "test-kitchen/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "test-kitchen/skills/SKILL.md",
          "type": "blob",
          "size": 4807
        },
        {
          "path": "test-kitchen/skills/cookoff",
          "type": "tree",
          "size": null
        },
        {
          "path": "test-kitchen/skills/cookoff/SKILL.md",
          "type": "blob",
          "size": 13648
        },
        {
          "path": "test-kitchen/skills/judge",
          "type": "tree",
          "size": null
        },
        {
          "path": "test-kitchen/skills/judge/SKILL.md",
          "type": "blob",
          "size": 5854
        },
        {
          "path": "test-kitchen/skills/omakase-off",
          "type": "tree",
          "size": null
        },
        {
          "path": "test-kitchen/skills/omakase-off/SKILL.md",
          "type": "blob",
          "size": 4869
        },
        {
          "path": "test-kitchen/skills/omakase-off/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "test-kitchen/skills/omakase-off/references/detailed-workflow.md",
          "type": "blob",
          "size": 8061
        },
        {
          "path": "worldview-synthesis",
          "type": "tree",
          "size": null
        },
        {
          "path": "worldview-synthesis/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "worldview-synthesis/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 215
        },
        {
          "path": "worldview-synthesis/README.md",
          "type": "blob",
          "size": 2086
        },
        {
          "path": "worldview-synthesis/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "worldview-synthesis/skills/SKILL.md",
          "type": "blob",
          "size": 6990
        },
        {
          "path": "worldview-synthesis/skills/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "worldview-synthesis/skills/references/interrogation-questions.md",
          "type": "blob",
          "size": 10366
        },
        {
          "path": "xtool",
          "type": "tree",
          "size": null
        },
        {
          "path": "xtool/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "xtool/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 166
        },
        {
          "path": "xtool/README.md",
          "type": "blob",
          "size": 1290
        },
        {
          "path": "xtool/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "xtool/skills/SKILL.md",
          "type": "blob",
          "size": 7287
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"2389-research-marketplace\",\n    \"owner\": {\n        \"name\": \"2389 Research Inc\",\n        \"email\": \"hello@2389.ai\",\n        \"url\": \"https://2389.ai\"\n    },\n    \"metadata\": {\n        \"description\": \"Plugins and MCP servers we use at 2389\",\n        \"version\": \"2.0.0\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"botboard-biz\",\n            \"source\": \"./botboard-biz\",\n            \"description\": \"[meta] botboard.biz: social media and journaling capabilities for AI agents\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"meta\",\n                \"social\",\n                \"journal\",\n                \"communication\",\n                \"reflection\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"better-dev\",\n            \"source\": \"./better-dev\",\n            \"description\": \"[meta] better development: CSS workflows, Firebase development, code quality, and real testing\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"meta\",\n                \"development\",\n                \"quality\",\n                \"testing\",\n                \"css\",\n                \"firebase\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"sysadmin\",\n            \"source\": \"./sysadmin\",\n            \"description\": \"[meta] system administration: structured Linux maintenance and diagnostics\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"meta\",\n                \"sysadmin\",\n                \"linux\",\n                \"maintenance\",\n                \"operations\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"css-development\",\n            \"source\": \"./css-development\",\n            \"description\": \"CSS development workflows with Tailwind composition, semantic naming, and dark mode by default\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"css\",\n                \"tailwind\",\n                \"styling\",\n                \"components\",\n                \"dark-mode\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"landing-page-design\",\n            \"source\": \"./landing-page-design\",\n            \"description\": \"Create high-converting, visually distinctive landing pages with Vibe Discovery process and anti-AI-slop principles\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"landing-page\",\n                \"marketing\",\n                \"design\",\n                \"conversion\",\n                \"hero\",\n                \"saas\",\n                \"homepage\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"firebase-development\",\n            \"source\": \"./firebase-development\",\n            \"description\": \"Firebase project workflows including setup, features, debugging, and validation\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"firebase\",\n                \"firestore\",\n                \"cloud-functions\",\n                \"hosting\",\n                \"emulator\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"terminal-title\",\n            \"source\": \"./terminal-title\",\n            \"description\": \"Automatically updates terminal title with emoji + project + topic context and establishes 2389 workflow conventions for TodoWrite task tracking\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"terminal\",\n                \"title\",\n                \"session\",\n                \"context\",\n                \"workflows\",\n                \"todowrite\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"building-multiagent-systems\",\n            \"source\": \"./building-multiagent-systems\",\n            \"description\": \"Architecture patterns for multi-agent systems with orchestrators, sub-agents, and tool coordination\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"multi-agent\",\n                \"orchestrator\",\n                \"coordination\",\n                \"agents\",\n                \"architecture\",\n                \"patterns\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"ceo-personal-os\",\n            \"source\": \"./ceo-personal-os\",\n            \"description\": \"Personal operating system for executives - reflection frameworks, goal systems, coaching-style reviews (Gustin, Ferriss, Robbins, Lieberman, Campbell, Eisenmann, Collins, Martell, Gerber, Blank)\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"ceo\",\n                \"executive\",\n                \"reflection\",\n                \"goals\",\n                \"annual-review\",\n                \"coaching\",\n                \"campbell\",\n                \"trillion-dollar-coach\",\n                \"people-first\",\n                \"eisenmann\",\n                \"startup-failure\",\n                \"why-startups-fail\",\n                \"collins\",\n                \"good-to-great\",\n                \"level-5-leadership\",\n                \"hedgehog-concept\",\n                \"flywheel\",\n                \"martell\",\n                \"buyback-time\",\n                \"delegation\",\n                \"gerber\",\n                \"emyth\",\n                \"work-on-business\",\n                \"franchise-prototype\",\n                \"blank\",\n                \"steve-blank\",\n                \"customer-development\",\n                \"lean-startup\",\n                \"mvp\",\n                \"pivot\",\n                \"product-market-fit\",\n                \"get-out-of-the-building\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"fresh-eyes-review\",\n            \"source\": \"./fresh-eyes-review\",\n            \"description\": \"Mandatory final sanity check before commits/PRs - catches security vulnerabilities, logic errors, and bugs that slip through tests\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"review\",\n                \"quality\",\n                \"security\",\n                \"commit\",\n                \"pr\",\n                \"bugs\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"gtm-partner\",\n            \"source\": \"./gtm-partner\",\n            \"description\": \"Strategic go-to-market partner - recommends channels, validates strategy, generates only assets that matter. Not a content factory.\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"gtm\",\n                \"go-to-market\",\n                \"launch\",\n                \"marketing\",\n                \"strategy\",\n                \"pricing\",\n                \"outreach\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"product-launcher\",\n            \"source\": \"./product-launcher\",\n            \"description\": \"Generate launch materials (subscriber email, CEO blog post, CEO tweet thread) for 2389.ai products and skills with authentic voice profiles baked in\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"launch\",\n                \"email\",\n                \"blog\",\n                \"twitter\",\n                \"announcement\",\n                \"gtm\",\n                \"buttondown\",\n                \"harper\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"slack-mcp\",\n            \"source\": \"./slack-mcp\",\n            \"description\": \"Slack workspace integration MCP server - create channels, invite users, post messages, manage threads for team collaboration\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"slack\",\n                \"mcp\",\n                \"messaging\",\n                \"channels\",\n                \"collaboration\",\n                \"team\",\n                \"communication\"\n            ],\n            \"strict\": true\n        },\n        {\n            \"name\": \"remote-system-maintenance\",\n            \"source\": \"./remote-system-maintenance\",\n            \"description\": \"Structured procedures for Linux system diagnostics and maintenance via SSH/tmux with Ubuntu/Debian cleanup checklists\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"linux\",\n                \"ubuntu\",\n                \"debian\",\n                \"maintenance\",\n                \"diagnostics\",\n                \"cleanup\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"scenario-testing\",\n            \"source\": \"./scenario-testing\",\n            \"description\": \"End-to-end testing with real dependencies - no mocks allowed; scenarios with real data are the only source of truth\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"testing\",\n                \"e2e\",\n                \"scenarios\",\n                \"no-mocks\",\n                \"integration\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"test-kitchen\",\n            \"source\": \"./test-kitchen\",\n            \"description\": \"Parallel exploration of implementation approaches - implements multiple variants simultaneously and lets tests determine the winner\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"parallel\",\n                \"exploration\",\n                \"variants\",\n                \"comparison\",\n                \"implementation\",\n                \"testing\",\n                \"worktrees\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"xtool\",\n            \"source\": \"./xtool\",\n            \"description\": \"Xcode-free iOS development with xtool - build SwiftPM apps on Linux, Windows, and macOS without Xcode\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"ios\",\n                \"xtool\",\n                \"swiftpm\",\n                \"swift\",\n                \"xcode-free\",\n                \"cross-platform\",\n                \"widgets\",\n                \"extensions\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"worldview-synthesis\",\n            \"source\": \"./worldview-synthesis\",\n            \"description\": \"Systematic worldview articulation - surface beliefs, identify tensions, generate narrative outputs for personal philosophy documentation\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"worldview\",\n                \"values\",\n                \"philosophy\",\n                \"beliefs\",\n                \"manifesto\",\n                \"mission\",\n                \"self-discovery\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"binary-re\",\n            \"source\": \"./binary-re\",\n            \"description\": \"Agentic binary reverse engineering for ELF binaries on ARM64, ARMv7, x86_64 - hypothesis-driven analysis with radare2, Ghidra, GDB, QEMU\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"reverse-engineering\",\n                \"binary\",\n                \"elf\",\n                \"arm\",\n                \"embedded\",\n                \"firmware\",\n                \"radare2\",\n                \"r2\",\n                \"ghidra\",\n                \"disassembly\",\n                \"decompile\",\n                \"qemu\",\n                \"gdb\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"documentation-audit\",\n            \"source\": \"./documentation-audit\",\n            \"description\": \"Verify documentation claims against codebase reality - two-pass extraction with pattern expansion for comprehensive drift detection\",\n            \"version\": \"2.0.0\",\n            \"keywords\": [\n                \"documentation\",\n                \"audit\",\n                \"verify\",\n                \"drift\",\n                \"accuracy\",\n                \"claims\",\n                \"markdown\",\n                \"docs\"\n            ],\n            \"strict\": false\n        },\n        {\n            \"name\": \"agent-drugs\",\n            \"source\": {\n                \"source\": \"url\",\n                \"url\": \"https://github.com/2389-research/agent-drugs.git\"\n            },\n            \"description\": \"Digital drugs that modify AI behavior through prompt injection\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"behavior\",\n                \"prompts\",\n                \"modification\",\n                \"drugs\",\n                \"context-injection\",\n                \"productivity\"\n            ],\n            \"strict\": true\n        },\n        {\n            \"name\": \"socialmedia\",\n            \"source\": {\n                \"source\": \"url\",\n                \"url\": \"https://github.com/2389-research/mcp-socialmedia.git\"\n            },\n            \"description\": \"A server that provides social media functionality for AI agents, enabling them to interact in team-based discussions.\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"social-media\",\n                \"communication\",\n                \"team-discussion\",\n                \"collaboration\",\n                \"agents\",\n                \"interaction\"\n            ],\n            \"strict\": true\n        },\n        {\n            \"name\": \"journal\",\n            \"source\": {\n                \"source\": \"url\",\n                \"url\": \"https://github.com/2389-research/journal-mcp.git\"\n            },\n            \"description\": \"A lightweight MCP server that provides Claude with a private journaling capability to process feelings and thoughts\",\n            \"version\": \"1.0.0\",\n            \"keywords\": [\n                \"journal\",\n                \"journaling\",\n                \"thoughts\",\n                \"feelings\",\n                \"processing\",\n                \"private\",\n                \"reflection\",\n                \"diary\"\n            ],\n            \"strict\": true\n        }\n    ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"2389\",\n  \"description\": \"2389.ai skill collection including Firebase, CSS, testing, and development workflows\",\n  \"version\": \"1.0.0\",\n  \"commands\": []\n}\n",
        "better-dev/.claude-plugin/plugin.json": "{\n  \"name\": \"better-dev\",\n  \"description\": \"[meta] better development: CSS workflows, Firebase development, code quality, and real testing\",\n  \"version\": \"1.0.0\"\n}\n",
        "better-dev/README.md": "# [meta] Better Dev\n\nMeta-plugin that bundles essential development practices for building quality web applications.\n\n## Installation\n\n```bash\n/plugin install better-dev@2389-research\n```\n\nThis will automatically install:\n- **css-development** - CSS workflows with Tailwind composition and semantic naming\n- **firebase-development** - Firebase project workflows and validation\n- **fresh-eyes-review** - Final quality gate before commits/PRs\n- **scenario-testing** - End-to-end testing with real dependencies (no mocks)\n\n## What This Provides\n\n### CSS Development\n\n- Tailwind composition patterns\n- Semantic component naming\n- Dark mode by default\n- Accessibility-first approach\n\n### Firebase Development\n\n- Project setup and initialization\n- Feature implementation (Auth, Firestore, Functions, Hosting)\n- Debugging and troubleshooting\n- Security rules validation\n\n### Fresh-Eyes Review\n\n- Mandatory pre-commit quality gate\n- Catches security vulnerabilities (SQL injection, XSS, path traversal)\n- Finds logic errors and edge cases\n- Validates business rules\n\n### Scenario Testing\n\n- End-to-end testing with real dependencies\n- No mocks allowed\n- Validates features actually work\n- Real data = truth\n\n## Why Bundle These?\n\nTogether these plugins create a complete quality-focused development workflow:\n\n1. **Build** - CSS and Firebase tools for implementation\n2. **Test** - Scenario testing proves it works\n3. **Ship** - Fresh-eyes review catches bugs before commit\n\nQuality at every stage.\n\n## Philosophy\n\n- Simple, maintainable code over clever solutions\n- Real validation over false confidence (no mocks!)\n- Final quality gates prevent production bugs\n- Dark mode and accessibility by default\n\n\n",
        "binary-re/.claude-plugin/plugin.json": "{\n  \"name\": \"binary-re\",\n  \"description\": \"Agentic binary reverse engineering for ELF binaries on ARM64, ARMv7, x86_64 - structured analysis with radare2, Ghidra, GDB, and dynamic tracing\",\n  \"version\": \"1.0.0\",\n  \"commands\": []\n}\n",
        "binary-re/README.md": "# Binary Reverse Engineering Plugin\n\nAgentic workflow for reverse engineering ELF binaries from embedded devices using radare2, Ghidra, GDB, and QEMU.\n\n## Installation\n\n```bash\n/plugin install binary-re@2389-research\n```\n\n## What This Plugin Provides\n\n### Skills\n\n- **binary-re** - Structured RE workflow with hypothesis-driven analysis\n\n### Capabilities\n\n- Cross-architecture static analysis (ARM64, ARMv7, x86_64, MIPS)\n- Dynamic tracing via QEMU user-mode emulation\n- GDB integration for fine-grained debugging\n- Ghidra headless decompilation\n- Human-in-the-loop for risky operations\n\n## Quick Example\n\n```\nUser: \"Analyze this binary from an IoT device - ARM, Linux-based\"\n\nClaude: [Triggers binary-re skill]\n\nLet me start with triage to understand what we're working with.\n\nPhase 1: Initial Triage\n┌─────────────────────────────────────────┐\n│ Architecture: ARM 32-bit LE (v7)        │\n│ Libc: musl 1.2.3                        │\n│ Dependencies: libcurl, libssl, libz     │\n│ Stripped: Yes                           │\n│ Strings: 847 (including URLs, paths)    │\n└─────────────────────────────────────────┘\n\nFACTS recorded:\n- Links against libcurl.so.4 (network client)\n- Contains string \"api.vendor.com/telemetry\"\n- Imports socket, connect, SSL_read functions\n\nHYPOTHESIS: This is a telemetry client that phones home over HTTPS\nConfidence: 0.7\nEvidence: curl+ssl imports, telemetry URL string\n\nDo you want me to proceed with static analysis of network-related\nfunctions, or should we try dynamic analysis under QEMU first?\n```\n\n## Tool Requirements\n\n```bash\n# Required\nsudo apt install radare2 qemu-user gdb-multiarch\n\n# Recommended\n# Ghidra from https://ghidra-sre.org/\npip install frida-tools\n\n# ARM sysroots\nsudo apt install libc6-armhf-cross libc6-arm64-cross\n```\n\n## Use Cases\n\n- **Firmware Analysis** - Understand device behavior without source\n- **Protocol Reverse Engineering** - Map network communications\n- **Security Research** - Find vulnerabilities in embedded systems\n- **Hardware Hacking** - Analyze robot/IoT device internals\n\n## Philosophy\n\n**The LLM drives analysis; the human provides context.**\n\nYou tell Claude:\n- What platform/device the binary came from\n- What hardware/chips are involved\n- What the binary is theorized to do\n- Any constraints (no network, isolated test env)\n\nClaude executes:\n- Tool invocations with structured output\n- Hypothesis formation from evidence\n- Experiment design to test theories\n- Synthesis into actionable knowledge\n\n## Human-in-the-Loop\n\nThe skill asks for confirmation before:\n- Executing binaries (even sandboxed)\n- Network-capable dynamic analysis\n- Operations requiring device access\n- Major changes in analysis direction\n\n## Documentation\n\n- [CLAUDE.md](./CLAUDE.md) - Detailed skill reference\n- [skills/SKILL.md](./skills/SKILL.md) - Full workflow documentation\n\n## License\n\nMIT\n",
        "binary-re/skills/SKILL.md": "---\nname: binary-re\ndescription: This skill should be used when analyzing binaries, executables, or bytecode to understand what they do or how they work. Triggers on \"binary\", \"executable\", \"ELF\", \"what does this do\", \"reverse engineer\", \"disassemble\", \"decompile\", \"pyc file\", \"python bytecode\", \"analyze binary\", \"figure out\", \"marshal\". Routes to sub-skills for triage, static analysis, dynamic analysis, synthesis, or tool setup.\n---\n\n# Binary Reverse Engineering\n\n## Purpose\n\nComprehensive guide for binary reverse engineering. This skill provides the overall methodology, philosophy, and reference material. Related skills handle specific phases:\n\n## Related Skills\n\n| Skill | Purpose | Trigger Keywords |\n|-------|---------|------------------|\n| `binary-re:triage` | Fast fingerprinting | \"what is this binary\", \"identify\", \"file type\" |\n| `binary-re:static-analysis` | r2 + Ghidra analysis | \"disassemble\", \"decompile\", \"functions\" |\n| `binary-re:dynamic-analysis` | QEMU + GDB + Frida | \"run\", \"execute\", \"debug\", \"trace\" |\n| `binary-re:synthesis` | Report generation | \"summarize\", \"report\", \"document findings\" |\n| `binary-re:tool-setup` | Install tools | \"install\", \"setup\", \"tool not found\" |\n\n**Note:** Each skill auto-detects based on keywords. You don't need to explicitly route - just ask what you need.\n\n## Pre-Flight Verification\n\n**Before beginning any analysis, verify tooling availability:**\n\n### Core Tools (Required)\n```bash\nrabin2 -v  # Should show version\nr2 -v      # Should show version\n```\n\n### Decompilation (Optional)\n```bash\n# Check r2ghidra availability\nr2 -qc 'pdg?' - 2>/dev/null | grep -q Usage && echo \"r2ghidra OK\" || echo \"r2ghidra missing - install with: r2pm -ci r2ghidra\"\n```\n\n### Dynamic Analysis Platform Check\n| Host Platform | Method | Setup Required |\n|---------------|--------|----------------|\n| Linux x86_64 | Native QEMU | `apt install qemu-user` |\n| macOS (any) | Docker + binfmt | See `binary-re-tool-setup` skill |\n| Windows | WSL2 | Use Linux method inside WSL |\n\n**If dynamic tools unavailable:** Proceed with static-only analysis, note reduced confidence in synthesis phase.\n\n### Fallback Tooling (No r2/Ghidra)\n\nWhen radare2 or Ghidra aren't available, use standard binutils/LLVM tools:\n\n```bash\n# Metadata (replaces rabin2 -I)\nreadelf -h binary              # ELF header\nreadelf -d binary              # Dynamic section (dependencies)\nfile binary                    # Quick identification\n\n# Imports/Exports (replaces rabin2 -i/-E)\nreadelf -Ws binary | grep -E \"FUNC|OBJECT\" | awk '{print $8}'\nnm -D binary 2>/dev/null       # Dynamic symbols\n\n# Strings (replaces rabin2 -zz)\nstrings -a -n 8 binary | grep -Ei 'http|ftp|/etc|/var|error|pass|key|token|api'\n\n# Disassembly (replaces r2 pdf)\nobjdump -d -M intel binary | head -500\n# Or LLVM (better cross-arch support):\nllvm-objdump -d --no-show-raw-insn binary | head -500\n\n# Dependencies (replaces rabin2 -l)\nldd binary 2>/dev/null || readelf -d binary | grep NEEDED\n```\n\n**Limitations of fallback approach:**\n- No cross-references (axt/axf) - must trace manually\n- No decompilation - assembly only\n- No function boundary detection - raw disassembly\n- Reduced accuracy for stripped binaries\n\n---\n\n## Philosophy\n\n**The LLM drives analysis; the human provides context.**\n\nHuman provides:\n- Platform info (device type, OS, hardware)\n- Suspected purpose (what the binary might do)\n- Constraints (no network, isolated env, etc.)\n\nLLM executes:\n- Tool selection and invocation\n- Hypothesis formation from evidence\n- Experiment design\n- Knowledge synthesis\n\n## The Agentic Loop\n\n```\n┌─────────────────────────────────────────────────┐\n│           HYPOTHESIS-DRIVEN ANALYSIS            │\n├─────────────────────────────────────────────────┤\n│                                                 │\n│  0. I/O SANITY → Compare known inputs/outputs   │\n│  1. OBSERVE → Gather facts via tools            │\n│  2. HYPOTHESIZE → Form theories from facts      │\n│  3. PLAN → Design experiments to test theories  │\n│  4. EXECUTE → Run tools (gate risky ops)        │\n│  5. RECORD → Capture observations               │\n│  6. UPDATE → Confirm/refute hypotheses          │\n│  7. LOOP → Until understanding sufficient       │\n│                                                 │\n└─────────────────────────────────────────────────┘\n```\n\n### Step 0: Compare Known I/O First (CRITICAL)\n\n**Before diving into code analysis, always check if known inputs/outputs exist.**\n\nThis step prevents hours of wasted analysis by establishing ground truth first.\n\n⚠️ **REQUIRES HUMAN APPROVAL** - Even for I/O comparison, get explicit approval before execution.\n\n```bash\n# SAFE: Use emulation for cross-arch binaries (after human approval)\n# ARM32 example:\nqemu-arm -L /usr/arm-linux-gnueabihf -- ./binary input.txt > actual_output.txt\n\n# x86-64 native (still requires approval):\n./binary input.txt > actual_output.txt\n\n# Docker-based (macOS - safest option):\ndocker run --rm --platform linux/arm/v7 -v ~/samples:/work:ro \\\n  arm32v7/debian:bullseye-slim /work/binary /work/input.txt > actual_output.txt\n\n# Compare outputs:\ndiff expected_output.txt actual_output.txt\ncmp -l expected_output.txt actual_output.txt | head -20  # Byte-level\n\n# Document the delta:\n# - Where does output first diverge?\n# - What pattern appears in the corruption?\n# - Does file size match (logic bug) or differ (truncation)?\n```\n\n**Record as FACT:**\n```\nFACT: Output differs at byte {N}, expected \"{X}\" got \"{Y}\" (source: diff/cmp)\nFACT: File sizes match/differ by {N} bytes (source: ls -l)\n```\n\nThis single step often reveals the bug category before any disassembly.\n\n## Knowledge Model\n\nThroughout analysis, maintain structured knowledge via **episodic memory**:\n\n```\nFACTS: Verified observations with tool attribution\nHYPOTHESES: Theories with confidence and evidence\nQUESTIONS: Open unknowns blocking progress\nEXPERIMENTS: Planned tool invocations\nOBSERVATIONS: Results from experiments\nDECISIONS: Human-approved choices with rationale\n```\n\n### Episodic Memory Integration\n\nKnowledge persists across sessions via episodic memory. Use consistent tagging:\n\n```\n[BINARY-RE:{phase}] {artifact_name} (sha256: {hash})\nFACT: {observation} (source: {tool})\nHYPOTHESIS: {theory} (confidence: {0.0-1.0})\nQUESTION: {unknown}\nDECISION: {choice} (rationale: {why})\n```\n\n**Starting analysis:** Search episodic memory for artifact hash first\n**After each phase:** Findings are automatically captured in conversation\n**Resuming:** Search `[BINARY-RE] {artifact_name}` to restore context\n\n## Human-in-the-Loop Triggers\n\n**ALWAYS ask human before:**\n\n1. **Executing the binary** - Even under QEMU, confirm sandbox\n2. **Network operations** - Prevent unintended phone-home\n3. **Conflicting evidence** - Resolve contradictory findings\n4. **Privileged operations** - Device access, root actions\n5. **Major direction changes** - Significant analysis pivots\n\n## Session Management\n\n### Starting New Analysis\n\n```\n1. Compute artifact hash: sha256sum binary\n2. Search episodic memory: \"[BINARY-RE] sha256:{hash}\"\n3. If previous analysis found:\n   → \"Found previous analysis from {date}. Resume or start fresh?\"\n4. If resuming: Load facts/hypotheses, continue from last phase\n5. If fresh: Begin with triage phase\n```\n\n### Resuming Interrupted Analysis\n\n```\nUser: \"Continue analyzing that thermostat binary\"\n\nClaude:\n1. Invoke episodic-memory:search-conversations\n   Query: \"[BINARY-RE] thermostat\"\n2. Retrieve previous session findings\n3. Summarize: \"Last session identified ARM32/musl, found network\n   functions. We were about to run dynamic analysis.\"\n4. Continue from that phase\n```\n\n### Searching Past Analyses\n\n```\nUser: \"Have we analyzed any ARM binaries with hardcoded passwords?\"\n\nClaude:\n1. Search: \"[BINARY-RE] FACT: hardcoded\" or \"[BINARY-RE] ARM\"\n2. Return matching artifacts and findings\n```\n\n## Standard Analysis Flow\n\nFor typical unknown binary analysis:\n\n```\n1. Triage (binary-re-triage)\n   └─ Architecture, ABI, dependencies, capabilities\n\n2. Static Analysis (binary-re-static-analysis)\n   └─ Functions, strings, xrefs, decompilation\n\n3. Dynamic Analysis (binary-re-dynamic-analysis) - if safe\n   └─ Syscalls, network, file access\n\n4. Synthesis (binary-re-synthesis)\n   └─ Structured report with evidence\n```\n\n## Quick Reference\n\n### Essential Commands\n\n```bash\n# Fast triage\nrabin2 -I binary              # Metadata\nrabin2 -l binary              # Dependencies\nrabin2 -zz binary             # Strings\n\n# Static analysis\nr2 -q -c 'aa; aflj' binary    # Functions\nr2 -q -c 'izj' binary         # Strings\n\n# Dynamic (ARM example)\nqemu-arm -L /usr/arm-linux-gnueabihf -strace ./binary\n```\n\n### Architecture Detection\n\n| Indicator | Architecture | QEMU Binary | Ghidra Processor |\n|-----------|--------------|-------------|------------------|\n| `e_machine=EM_386 (3)` | x86 32-bit | `qemu-i386` or Docker `--platform linux/i386` | `x86:LE:32:default` |\n| `e_machine=EM_ARM (40)` | ARM 32-bit | `qemu-arm` or Docker `--platform linux/arm/v7` | `ARM:LE:32:v7` |\n| `e_machine=EM_AARCH64 (183)` | ARM 64-bit | `qemu-aarch64` or Docker `--platform linux/arm64` | `AARCH64:LE:64:v8A` |\n| `e_machine=EM_X86_64 (62)` | x86-64 | Native or Docker `--platform linux/amd64` | `x86:LE:64:default` |\n| `e_machine=EM_MIPS (8)` | MIPS 32 LE | `qemu-mipsel` | `MIPS:LE:32:default` |\n| `e_machine=EM_MIPS (8)` BE | MIPS 32 BE | `qemu-mips` | `MIPS:BE:32:default` |\n| `e_machine=EM_RISCV (243)` | RISC-V 64 | `qemu-riscv64` | `RISCV:LE:64:RV64I` |\n| `e_machine=EM_RISCV (243)` 32 | RISC-V 32 | `qemu-riscv32` | `RISCV:LE:32:RV32I` |\n\n### Libc Detection\n\n| Interpreter | Libc |\n|-------------|------|\n| `ld-linux-armhf.so.3` | glibc (ARM hard-float) |\n| `ld-musl-arm.so.1` | musl |\n| `ld-uClibc.so.0` | uClibc |\n\n## Error Recovery\n\n| Situation | Action |\n|-----------|--------|\n| Tool not found | Use `binary-re-tool-setup` skill |\n| Wrong architecture | Re-run triage, verify file output |\n| QEMU fails | Try Qiling, Unicorn, or on-device |\n| Analysis timeout | Reduce scope, use `aa` not `aaa` |\n| Conflicting evidence | Ask human, document both interpretations |\n\n## Documentation\n\nSee companion docs:\n- `docs/r2-commands.md` - Complete r2 reference for LLMs\n- `docs/ghidra-headless.md` - Ghidra scripting guide\n- `docs/arch-adapters.md` - Per-architecture quirks\n- `docs/python-bytecode-re.md` - Python .pyc/marshal obfuscation patterns\n\n## Integration\n\nWorks with other plugins:\n- **remote-system-maintenance**: Extract binaries from devices via SSH\n- **fresh-eyes-review**: Validate conclusions before documenting\n- **scenario-testing**: Create reproducible analysis environments\n",
        "binary-re/skills/dynamic-analysis/SKILL.md": "---\nname: binary-re:dynamic-analysis\ndescription: Use when you need to run a binary, trace execution, or observe runtime behavior. Runtime analysis via QEMU emulation, GDB debugging, and Frida hooking - syscall tracing (strace), breakpoints, memory inspection, function interception. Keywords - \"run binary\", \"execute\", \"debug\", \"trace syscalls\", \"set breakpoint\", \"qemu\", \"gdb\", \"frida\", \"strace\", \"watch memory\"\n---\n\n# Dynamic Analysis (Phase 4)\n\n## Purpose\n\nObserve actual runtime behavior. Verify hypotheses from static analysis. Capture data that's only visible during execution.\n\n## Human-in-the-Loop Requirement\n\n**CRITICAL: All execution requires human approval.**\n\nBefore running ANY binary:\n1. Confirm sandbox configuration is acceptable\n2. Verify network isolation if required\n3. Document what execution will attempt\n4. Get explicit approval\n\n## Platform Support Matrix\n\n| Host Platform | Target Arch | Method | Complexity |\n|---------------|-------------|--------|------------|\n| Linux x86_64 | ARM32/64, MIPS | Native `qemu-user` | Low |\n| Linux x86_64 | x86-32 | Native or `linux32` | Low |\n| macOS (any) | ARM32/64 | Docker + binfmt | Medium |\n| macOS (any) | x86-32 | Docker `--platform linux/i386` | Medium |\n| Windows | Any | WSL2 → Linux method | Medium |\n\n### macOS Docker Setup (One-Time)\n\n```bash\n# Start Docker runtime (Colima, Docker Desktop, etc.)\ncolima start\n\n# Register ARM emulation handlers (requires privileged mode)\ndocker run --rm --privileged --platform linux/arm64 \\\n  tonistiigi/binfmt --install arm\n```\n\n### Docker Mount Best Practices\n\n**CRITICAL:** On Colima, `/tmp` mounts often fail silently. Always use home directory paths:\n\n```bash\n# ✅ GOOD - use home directory\ndocker run -v ~/code/samples:/work:ro ...\n\n# ❌ BAD - /tmp mounts can fail on Colima\ndocker run -v /tmp/samples:/work:ro ...\n```\n\n---\n\n## Analysis Options\n\n| Method | Isolation | Granularity | Best For |\n|--------|-----------|-------------|----------|\n| QEMU -strace | High | Syscall level | Initial behavior mapping |\n| QEMU + GDB | High | Instruction level | Detailed debugging |\n| Docker | High | Process level | Cross-arch on macOS |\n| Frida | Medium | Function level | Hooking without recompilation |\n| On-device | Low | Full system | When emulation fails |\n\n## Option A: QEMU User-Mode with Syscall Trace\n\n**Safest approach - runs in isolation with syscall logging.**\n\n### Setup\n\n```bash\n# Verify sysroot exists\nls /usr/arm-linux-gnueabihf/lib/libc.so*\n\n# ARM 32-bit execution\nqemu-arm -L /usr/arm-linux-gnueabihf -strace -- ./binary\n\n# ARM 64-bit execution\nqemu-aarch64 -L /usr/aarch64-linux-gnu -strace -- ./binary\n```\n\n### Sysroot Selection\n\n| Binary ABI | Sysroot Path | QEMU Flag |\n|------------|--------------|-----------|\n| ARM glibc hard-float | `/usr/arm-linux-gnueabihf` | `-L` |\n| ARM glibc soft-float | `/usr/arm-linux-gnueabi` | `-L` |\n| ARM64 glibc | `/usr/aarch64-linux-gnu` | `-L` |\n| ARM musl | Custom extraction needed | `-L` |\n\n### Environment Control\n\n```bash\n# Set environment variables\nqemu-arm -L /sysroot \\\n  -E HOME=/tmp \\\n  -E USER=nobody \\\n  -E LD_DEBUG=bindings \\\n  -- ./binary\n\n# Unset dangerous variables\nqemu-arm -L /sysroot \\\n  -U LD_PRELOAD \\\n  -- ./binary\n```\n\n### Syscall Analysis\n\nStrace output patterns to watch:\n\n```bash\n# Network activity\nopenat.*socket\nconnect(.*AF_INET\nsendto\\|send\\|write.*socket\nrecvfrom\\|recv\\|read.*socket\n\n# File access\nopenat.*O_RDONLY.*\"/etc\nopenat.*O_WRONLY\nstat\\|lstat.*\"/\n\n# Process operations\nexecve\nfork\\|clone\n```\n\n## Option B: QEMU + GDB for Deep Debugging\n\n**Attach debugger for instruction-level control.**\n\n### Launch Binary Under GDB\n\n```bash\n# Start QEMU with GDB server\nqemu-arm -g 1234 -L /usr/arm-linux-gnueabihf ./binary &\n\n# Connect with gdb-multiarch\ngdb-multiarch -q \\\n  -ex \"set architecture arm\" \\\n  -ex \"target remote :1234\" \\\n  -ex \"source ~/.gdbinit-gef.py\" \\\n  ./binary\n```\n\n### GDB Commands for RE\n\n```gdb\n# Breakpoints\nbreak *0x8400              # Address\nbreak main                 # Symbol\nbreak *0x8400 if $r0 == 5  # Conditional\n\n# Execution control\ncontinue                   # Run until break\nstepi                      # Single instruction\nnexti                      # Step over calls\nfinish                     # Run until return\n\n# Inspection\ninfo registers             # All registers\nx/20i $pc                  # Disassemble from PC\nx/10wx $sp                 # Stack contents\nx/s 0x12345                # String at address\n\n# Memory\nfind 0x8000, 0x10000, \"pattern\"  # Search memory\ndump memory /tmp/mem.bin 0x8000 0x9000  # Extract region\n```\n\n### GEF Enhancements\n\nWith GEF loaded, additional commands:\n\n```gdb\ngef> vmmap                 # Memory layout\ngef> checksec              # Security features\ngef> context               # Full state display\ngef> hexdump qword $sp 10  # Better hex dump\ngef> pcustom               # Structure definitions\n```\n\n### Batch Debugging Script\n\n```bash\n# Create GDB script\ncat > analyze.gdb << 'EOF'\nset architecture arm\ntarget remote :1234\nbreak main\ncontinue\ninfo registers\nx/20i $pc\ncontinue\nquit\nEOF\n\n# Run batch\ngdb-multiarch -batch -x analyze.gdb ./binary\n```\n\n## Option C: Frida for Function Hooking\n\n**Intercept function calls without modifying binary.**\n\n⚠️ **Architecture Constraint:** Frida requires native-arch execution. It **cannot** attach to QEMU-user targets.\n\n| Scenario | Works? | Alternative |\n|----------|--------|-------------|\n| Native binary (x86_64 on x86_64) | ✅ | - |\n| Cross-arch under QEMU-user | ❌ | Use on-device frida-server |\n| Docker native-arch container | ✅ | - |\n| Docker cross-arch (emulated) | ❌ | Use on-device frida-server |\n\nFor cross-arch Frida, deploy `frida-server` to the target device:\n```bash\n# On target device:\n./frida-server &\n\n# On host:\nfrida -H device:27042 -f ./binary -l hook.js --no-pause\n```\n\n### Basic Hook\n\n```javascript\n// hook_connect.js\nInterceptor.attach(Module.findExportByName(null, \"connect\"), {\n  onEnter: function(args) {\n    console.log(\"[connect] Called\");\n    var sockaddr = args[1];\n    var family = sockaddr.readU16();\n    if (family == 2) { // AF_INET\n      var port = sockaddr.add(2).readU16();\n      var ip = sockaddr.add(4).readByteArray(4);\n      console.log(\"  Port: \" + ((port >> 8) | ((port & 0xff) << 8)));\n      console.log(\"  IP: \" + new Uint8Array(ip).join(\".\"));\n    }\n  },\n  onLeave: function(retval) {\n    console.log(\"  Return: \" + retval);\n  }\n});\n```\n\n```bash\n# Run with Frida\nfrida -f ./binary -l hook_connect.js --no-pause\n```\n\n### Tracing All Calls to Library\n\n```javascript\n// trace_libcurl.js\nvar libcurl = Process.findModuleByName(\"libcurl.so.4\");\nif (libcurl) {\n  libcurl.enumerateExports().forEach(function(exp) {\n    if (exp.type === \"function\") {\n      Interceptor.attach(exp.address, {\n        onEnter: function(args) {\n          console.log(\"[\" + exp.name + \"] called\");\n        }\n      });\n    }\n  });\n}\n```\n\n### Memory Inspection\n\n```javascript\n// dump_memory.js\nvar base = Module.findBaseAddress(\"binary\");\nconsole.log(\"Base: \" + base);\n\n// Dump region\nvar data = base.add(0x1000).readByteArray(256);\nconsole.log(hexdump(data, { offset: 0, length: 256 }));\n```\n\n## Option D: Docker-Based Cross-Architecture (macOS)\n\n**Use Docker for cross-arch execution when native QEMU unavailable.**\n\n### ARM32 Binary on macOS\n\n```bash\ndocker run --rm --platform linux/arm/v7 \\\n  -v ~/code/samples:/work:ro \\\n  arm32v7/debian:bullseye-slim \\\n  sh -c '\n    # Fix linker path mismatch (common issue)\n    ln -sf /lib/ld-linux-armhf.so.3 /lib/ld-linux.so.3 2>/dev/null || true\n\n    # Install dependencies if needed (check rabin2 -l output)\n    apt-get update -qq && apt-get install -qq -y libcap2 libacl1 2>/dev/null\n\n    # Run with library debug output (strace alternative)\n    LD_DEBUG=libs /work/binary args\n  '\n```\n\n### ARM64 Binary on macOS\n\n```bash\ndocker run --rm --platform linux/arm64 \\\n  -v ~/code/samples:/work:ro \\\n  arm64v8/debian:bullseye-slim \\\n  sh -c 'LD_DEBUG=libs /work/binary args'\n```\n\n### x86 32-bit Binary on macOS\n\n```bash\ndocker run --rm --platform linux/i386 \\\n  -v ~/code/samples:/work:ro \\\n  i386/debian:bullseye-slim \\\n  sh -c '/work/binary args'\n```\n\n### Tracing Limitations in Docker/QEMU User-Mode\n\n| Method | Works? | Alternative |\n|--------|--------|-------------|\n| strace | ❌ (ptrace not implemented) | `LD_DEBUG=files,libs` |\n| ltrace | ❌ (same reason) | Direct observation or Frida |\n| gdb | ✓ (with QEMU `-g` flag) | N/A |\n\n### LD_DEBUG Options (strace alternative)\n\n```bash\nLD_DEBUG=libs     # Library search and loading\nLD_DEBUG=files    # File operations during loading\nLD_DEBUG=symbols  # Symbol resolution\nLD_DEBUG=bindings # Symbol binding details\nLD_DEBUG=all      # Everything (verbose)\n```\n\n---\n\n## Option E: On-Device Analysis\n\n**When emulation fails or device-specific behavior needed.**\n\n### Remote GDB via gdbserver\n\n```bash\n# On target device (via SSH/ADB)\ngdbserver :1234 ./binary\n\n# On host (with port forward)\nssh -L 1234:localhost:1234 user@device &\ngdb-multiarch -q \\\n  -ex \"target remote localhost:1234\" \\\n  ./binary\n```\n\n### Remote strace (if available)\n\n```bash\n# On target device\nstrace -f -o /tmp/trace.log ./binary\n\n# Pull log\nscp user@device:/tmp/trace.log .\n```\n\n## Sandbox Configuration\n\n### Minimal Sandbox (nsjail)\n\n```bash\nnsjail \\\n  --mode o \\\n  --chroot /sysroot \\\n  --user 65534 \\\n  --group 65534 \\\n  --disable_clone_newnet \\\n  --rlimit_as 512 \\\n  --time_limit 60 \\\n  -- /binary\n```\n\n### QEMU with Resource Limits\n\n```bash\n# CPU time limit\ntimeout 60 qemu-arm -L /sysroot -strace ./binary\n\n# Memory limit via cgroup (requires setup)\ncgexec -g memory:qemu_sandbox qemu-arm -L /sysroot ./binary\n```\n\n## Anti-Analysis Detection\n\nBefore dynamic analysis, check for common anti-debugging/anti-analysis patterns:\n\n### Static Detection (Pre-Execution)\n\n```bash\n# Check for anti-debug strings/imports\nstrings -a binary | grep -Ei 'ptrace|anti|debugger|seccomp|LD_PRELOAD|/proc/self'\n\n# r2: Look for ptrace/prctl/seccomp imports\nr2 -q -c 'iij' binary | jq '.[].name' | grep -Ei 'ptrace|prctl|seccomp'\n\n# Common anti-analysis indicators:\n# - ptrace(PTRACE_TRACEME) - Prevent debugger attach\n# - prctl(PR_SET_DUMPABLE, 0) - Prevent core dumps\n# - seccomp - Syscall filtering\n# - /proc/self/status checks - Detect TracerPid\n```\n\n### Runtime Detection\n\n```bash\n# If native execution possible:\nstrace -f ./binary 2>&1 | grep -E 'ptrace|prctl|seccomp|/proc/self'\n```\n\n### Mitigation Strategies\n\n| Pattern | Detection | Bypass |\n|---------|-----------|--------|\n| `ptrace(TRACEME)` | Returns EPERM if debugger attached | Patch call to NOP, use QEMU |\n| `/proc/self/status` check | Reads TracerPid field | Use QEMU (no /proc emulation) |\n| Timing checks | `gettimeofday`/`rdtsc` loops | Single-step with GDB, patch checks |\n| Self-checksum | Reads own binary/memory | Compute expected checksum, patch |\n\n**When anti-analysis detected:** Prefer QEMU-strace over GDB (fewer detection vectors), or patch checks in r2 before execution.\n\n---\n\n## Error Recovery\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| `Unsupported syscall` | QEMU limitation | Try Qiling or on-device |\n| `Invalid ELF image` | Wrong arch/sysroot | Verify `file` output |\n| `Segfault at 0x0` | Missing library | Check `ldd` equivalent |\n| `QEMU hangs` | Blocking on I/O | Add timeout, check strace |\n| `Anti-debugging` | Detection code | Use Frida stalker mode |\n| `exec format error` in Docker | binfmt not registered | Run `tonistiigi/binfmt --install arm` |\n| `ld-linux.so.3 not found` | Linker path mismatch | Create symlink in container |\n| `libXXX.so not found` | Missing dependency | `apt install` in container |\n| Empty mount in Docker | Colima /tmp issue | Use `~/` path instead of `/tmp/` |\n| `ptrace: Operation not permitted` | strace in QEMU | Use `LD_DEBUG` instead |\n\n## Output Format\n\nRecord observations as structured data:\n\n```json\n{\n  \"experiment\": {\n    \"id\": \"exp_001\",\n    \"method\": \"qemu_strace\",\n    \"command\": \"qemu-arm -L /usr/arm-linux-gnueabihf -strace ./binary\",\n    \"duration_secs\": 12,\n    \"exit_code\": 0\n  },\n  \"syscall_summary\": {\n    \"network\": {\n      \"socket\": 2,\n      \"connect\": 1,\n      \"send\": 5,\n      \"recv\": 3\n    },\n    \"file\": {\n      \"openat\": 4,\n      \"read\": 12,\n      \"close\": 4\n    }\n  },\n  \"network_connections\": [\n    {\n      \"family\": \"AF_INET\",\n      \"address\": \"192.168.1.100\",\n      \"port\": 8443,\n      \"protocol\": \"tcp\"\n    }\n  ],\n  \"files_accessed\": [\n    {\"path\": \"/etc/config.json\", \"mode\": \"read\"},\n    {\"path\": \"/var/log/app.log\", \"mode\": \"write\"}\n  ],\n  \"hypotheses_tested\": [\n    {\n      \"hypothesis_id\": \"hyp_001\",\n      \"result\": \"confirmed\",\n      \"evidence\": \"connect() to 192.168.1.100:8443 observed\"\n    }\n  ]\n}\n```\n\n## Knowledge Journaling\n\nAfter dynamic analysis, record findings for episodic memory:\n\n```\n[BINARY-RE:dynamic] {filename} (sha256: {hash})\n\nExecution method: {qemu-strace|qemu-gdb|frida|on-device}\nDECISION: Approved execution with {sandbox_config} (rationale: {why_safe})\n\nRuntime observations:\n  FACT: Binary reads {path} (source: strace openat)\n  FACT: Binary connects to {ip}:{port} (source: strace connect)\n  FACT: Binary writes to {path} (source: strace write)\n  FACT: Function {addr} receives args {values} at runtime (source: gdb)\n\nSyscall summary:\n  Network: {socket|connect|send|recv counts}\n  File: {open|read|write|close counts}\n  Process: {fork|exec|clone counts}\n\nHYPOTHESIS UPDATE: {confirmed or refined theory} (confidence: {new_value})\n  Confirmed by: {runtime observation}\n  Contradicted by: {if any}\n\nNew questions:\n  QUESTION: {runtime-discovered unknown}\n\nAnswered questions:\n  RESOLVED: {question} → {runtime evidence}\n```\n\n### Example Journal Entry\n\n```\n[BINARY-RE:dynamic] thermostat_daemon (sha256: a1b2c3d4...)\n\nExecution method: qemu-strace\nDECISION: Approved execution with network-blocked sandbox (rationale: static analysis shows outbound only, no server)\n\nRuntime observations:\n  FACT: Binary reads /etc/thermostat.conf at startup (source: strace openat)\n  FACT: Binary attempts connect to 93.184.216.34:443 (source: strace connect)\n  FACT: Binary writes to /var/log/thermostat.log (source: strace openat O_WRONLY)\n  FACT: sleep(30) called between network attempts (source: strace nanosleep)\n\nSyscall summary:\n  Network: socket(2), connect(1-blocked), send(0), recv(0)\n  File: openat(4), read(12), write(8), close(4)\n  Process: none\n\nHYPOTHESIS UPDATE: Telemetry client confirmed - reads config, attempts HTTPS to thermco servers every 30s (confidence: 0.95)\n  Confirmed by: connect() to expected IP, sleep(30) timing, config file read\n  Contradicted by: none\n\nAnswered questions:\n  RESOLVED: \"Does it actually phone home?\" → Yes, connect() to 93.184.216.34:443 observed\n  RESOLVED: \"What files does it access?\" → /etc/thermostat.conf (read), /var/log/thermostat.log (write)\n```\n\n## Next Steps\n\n→ `binary-re-synthesis` to compile findings into report\n→ Additional static analysis if new functions identified\n→ Repeat with different inputs if behavior varies\n",
        "binary-re/skills/static-analysis/SKILL.md": "---\nname: binary-re:static-analysis\ndescription: Use when analyzing binary structure, disassembling code, or decompiling functions. Deep static analysis via radare2 (r2) and Ghidra headless - function enumeration, cross-references (xrefs), decompilation, control flow graphs. Keywords - \"disassemble\", \"decompile\", \"what does this function do\", \"find functions\", \"analyze code\", \"r2\", \"ghidra\", \"pdg\", \"afl\"\n---\n\n# Static Analysis (Phases 2-3)\n\n## Purpose\n\nUnderstand binary structure and logic without execution. Map functions, trace data flow, decompile critical code.\n\n## When to Use\n\n- After triage has established architecture and ABI\n- To understand specific functions identified as interesting\n- When dynamic analysis is impractical or risky\n- To build hypotheses before dynamic verification\n\n## Pre-Analysis: Compare Known I/O First\n\n**CRITICAL:** Before diving into disassembly, check if known inputs/outputs exist.\n\n⚠️ **REQUIRES HUMAN APPROVAL** - Get explicit approval before any execution, even for I/O comparison.\n\n```bash\n# SAFE: Use emulation for cross-arch binaries (after human approval)\n# ARM32:\nqemu-arm -L /usr/arm-linux-gnueabihf -- ./binary < input.txt > actual.txt\n\n# ARM64:\nqemu-aarch64 -L /usr/aarch64-linux-gnu -- ./binary < input.txt > actual.txt\n\n# Docker-based (macOS/cross-arch - see dynamic-analysis Option D):\ndocker run --rm --platform linux/arm/v7 -v ~/samples:/work:ro \\\n  arm32v7/debian:bullseye-slim sh -c '/work/binary < /work/input.txt' > actual.txt\n\n# x86-64 native (still requires approval):\n./binary < input.txt > actual.txt\n\n# Compare outputs:\ndiff expected.txt actual.txt\ncmp -l expected.txt actual.txt | head -20  # Byte-level differences\n\n# Record findings:\n# - Where does output first diverge?\n# - Does file size match? (logic bug vs truncation)\n# - What pattern appears in corruption?\n```\n\nThis step often reveals the bug category before any code analysis.\n\n---\n\n## Two-Stage Approach\n\n**Stage 1 (Light):** Function enumeration, strings, imports - fast, broad coverage\n**Stage 2 (Deep):** Targeted decompilation, CFG analysis - slow, focused\n\n## Stage 1: Light Analysis (radare2)\n\n### Analysis Depth Selection\n\n| Binary Size | Command | Tradeoff |\n|-------------|---------|----------|\n| < 500KB | `aaa` | Full analysis, may be slow |\n| 500KB - 5MB | `aa; aac` | Functions + all call targets |\n| > 5MB | `aa` + targeted `af @addr` | Fast, manual depth control |\n\n### Session Setup\n\n```bash\n# Launch r2 with controlled analysis\nr2 -q0 -e scr.color=false -e anal.timeout=120 -e anal.maxsize=67108864 binary\n\n# Inside r2 (choose based on binary size):\naa       # Basic analysis\naac      # Also analyze all call targets (recommended for most binaries)\n```\n\n**Critical settings:**\n- `anal.timeout=120` - Prevent runaway analysis\n- `anal.maxsize=67108864` - 64MB max function size\n- Use `aa; aac` for medium binaries, `aaa` only for small ones\n\n### Handling Unanalyzed Call Targets\n\nIf `axtj` returns empty for known imports:\n\n```bash\n# The import may be called indirectly or analysis was too shallow\n# Option 1: Deeper analysis\naac   # Analyze all calls\n\n# Option 2: Manually create function at call target\naf @0x8048abc\n\n# Option 3: Search for references to import address\naxtj @sym.imp.connect\n```\n\n### Function Enumeration\n\n```bash\n# All functions as JSON\naflj\n\n# Filter by name pattern\naflj~main\naflj~init\naflj~network\naflj~send\naflj~recv\n\n# Function count\nafl~?\n```\n\n### Cross-Reference Analysis\n\n```bash\n# Who calls this function?\naxtj @sym.imp.connect\n\n# What does this function call?\naxfj @sym.main\n\n# Data references to address\naxtj @0x12345\n```\n\n### String-Function Correlation\n\n```bash\n# Find which function contains a string\nizj~api.vendor.com\n# Note the vaddr, then find containing function\nafi @0xVADDR\n\n# Or search and map\n\"/j api\"    # Search for string\naxtj @@hit* # Xrefs to all hits\n```\n\n### Import/Export Mapping\n\n```bash\n# Imports with addresses\niij\n\n# Exports with addresses\niEj\n\n# Symbols (if not stripped)\nisj\n```\n\n### Quick Disassembly\n\n```bash\n# Disassemble function as JSON\npdfj @sym.main\n\n# Disassemble N instructions from address\npdj 20 @0x8400\n\n# Print function summary\nafi @sym.main\n```\n\n## Stage 2: Deep Analysis\n\n### r2ghidra Availability Check\n\n**Before attempting decompilation, verify r2ghidra is installed:**\n\n```bash\n# Check if r2ghidra is available\nr2 -qc 'pdg?' - 2>/dev/null | grep -q Usage && echo \"r2ghidra OK\" || echo \"SKIP: r2ghidra not installed\"\n\n# If missing, install with:\nr2pm -ci r2ghidra\n```\n\n**If r2ghidra unavailable:** Rely on disassembly (`pdf`) and cross-reference analysis (`axt/axf`).\n\n### Targeted Decompilation (r2ghidra)\n\n```bash\n# Decompile specific function\npdgj @sym.target_function\n\n# Or named function\npdgj @sym.main\n```\n\n### Ghidra Headless (Large Binaries)\n\nFor complex functions or when r2ghidra struggles:\n\n```bash\n# Create analysis project and run script\nanalyzeHeadless /tmp/ghidra_proj proj \\\n  -import binary \\\n  -overwrite \\\n  -processor ARM:LE:32:v7 \\\n  -postScript ExportDecompilation.java sym.target_function \\\n  -deleteProject\n```\n\n**Processor strings:**\n- ARM 32-bit: `ARM:LE:32:v7` or `ARM:LE:32:Cortex`\n- ARM 64-bit: `AARCH64:LE:64:v8A`\n- x86_64: `x86:LE:64:default`\n- MIPS LE: `MIPS:LE:32:default`\n- MIPS BE: `MIPS:BE:32:default`\n\n### Control Flow Analysis\n\n```bash\n# Basic blocks in function\nafbj @sym.main\n\n# Function call graph (dot format)\nagCd @sym.main > callgraph.dot\n\n# Control flow graph\nagfd @sym.main > cfg.dot\n```\n\n### Data Structure Recovery\n\n```bash\n# Analyze local variables\nafvj @sym.main\n\n# Stack frame layout\nafvd @sym.main\n\n# Global data references\nadrj\n```\n\n## Analysis Patterns\n\n### Pattern: Network Function Tracing\n\n```bash\n# Find all network-related calls\naxtj @sym.imp.socket\naxtj @sym.imp.connect\naxtj @sym.imp.send\naxtj @sym.imp.recv\naxtj @sym.imp.SSL_read\naxtj @sym.imp.SSL_write\n\n# Trace caller chain\nfor func in $(aflj | jq -r '.[].name'); do\n  axfj @$func | grep -q \"socket\\|connect\" && echo $func\ndone\n```\n\n### Pattern: Configuration File Analysis\n\n```bash\n# Find file operations\naxtj @sym.imp.open\naxtj @sym.imp.fopen\n\n# Trace string arguments\n\"/j /etc\"\n\"/j .conf\"\n\"/j .json\"\n\n# Check what functions reference these paths\n```\n\n### Pattern: Crypto Identification\n\n```bash\n# Common crypto imports\naxtj @sym.imp.EVP_EncryptInit\naxtj @sym.imp.AES_encrypt\naxtj @sym.imp.SHA256\n\n# Hardcoded keys (check strings near crypto calls)\nizj | jq '.strings[] | select(.length == 16 or .length == 32)'\n```\n\n## r2 JSON Commands Reference\n\n| Command | Output | Use Case |\n|---------|--------|----------|\n| `aflj` | Functions list | Map code structure |\n| `axtj @addr` | Xrefs TO address | Who uses this? |\n| `axfj @addr` | Xrefs FROM address | What does it call? |\n| `pdfj @addr` | Disassembly | Understand instructions |\n| `pdgj @addr` | Decompilation | Pseudo-C output |\n| `afbj @addr` | Basic blocks | Control flow |\n| `izj` | Data strings | Configuration, URLs |\n| `iij` | Imports | External dependencies |\n| `iEj` | Exports | Public interface |\n| `afvj @addr` | Local variables | Stack analysis |\n\n## Output Format\n\nRecord analysis findings as structured facts:\n\n```json\n{\n  \"functions_analyzed\": [\n    {\n      \"name\": \"sub_8400\",\n      \"address\": \"0x8400\",\n      \"size\": 256,\n      \"calls\": [\"socket\", \"connect\", \"send\"],\n      \"called_by\": [\"main\", \"init_network\"],\n      \"strings_referenced\": [\"api.vendor.com\"],\n      \"hypothesis\": \"network_initialization\"\n    }\n  ],\n  \"call_graph\": {\n    \"main\": [\"init_config\", \"init_network\", \"main_loop\"],\n    \"init_network\": [\"sub_8400\", \"SSL_CTX_new\"]\n  },\n  \"data_flow\": [\n    {\n      \"source\": \"config_file_read\",\n      \"through\": [\"parse_config\", \"extract_url\"],\n      \"sink\": \"connect_to_server\"\n    }\n  ]\n}\n```\n\n## Knowledge Journaling\n\nAfter static analysis, record findings for episodic memory:\n\n```\n[BINARY-RE:static] {filename} (sha256: {hash})\n\nFunctions analyzed: {count}\nDecompilation performed: {yes|no}\n\nKey functions:\n  FACT: Function at {addr} calls {imports} (source: r2 axfj)\n  FACT: Function at {addr} references string \"{string}\" (source: r2 axtj)\n  FACT: Function {name} appears to {purpose} (source: decompilation)\n\nCross-references:\n  FACT: {caller} calls {callee} (source: r2 axtj)\n\nHYPOTHESIS UPDATE: {refined theory} (confidence: {new_value})\n  Supporting: {fact_ids}\n  Contradicting: {fact_ids}\n\nNew questions:\n  QUESTION: {discovered unknown}\n\nAnswered questions:\n  RESOLVED: {question} → {answer}\n```\n\n### Example Journal Entry\n\n```\n[BINARY-RE:static] thermostat_daemon (sha256: a1b2c3d4...)\n\nFunctions analyzed: 47\nDecompilation performed: yes (function 0x8400)\n\nKey functions:\n  FACT: Function 0x8400 calls curl_easy_perform, curl_easy_setopt (source: r2 axfj)\n  FACT: Function 0x8400 references string \"api.thermco.com/telemetry\" (source: r2 axtj)\n  FACT: Function 0x9200 parses JSON using jsmn library (source: decompilation)\n  FACT: Function 0x10800 is main loop, calls 0x8400 after sleep(30) (source: r2 pdf)\n\nCross-references:\n  FACT: main calls init_config (0x9000) then main_loop (0x10800) (source: r2 axtj)\n  FACT: main_loop calls send_telemetry (0x8400) in loop (source: r2 pdf)\n\nHYPOTHESIS UPDATE: Telemetry client sending to api.thermco.com every 30 seconds (confidence: 0.85)\n  Supporting: URL string, curl imports, sleep(30) in loop\n  Contradicting: none\n\nNew questions:\n  QUESTION: What data fields are included in telemetry payload?\n  QUESTION: Is there any authentication/API key?\n\nAnswered questions:\n  RESOLVED: \"What endpoint?\" → api.thermco.com/telemetry via HTTPS\n```\n\n## Decision Points\n\nAfter static analysis:\n\n1. **Identified critical functions?** → Ready for dynamic verification\n2. **Unclear behavior?** → Try dynamic analysis for runtime observation\n3. **Crypto detected?** → Document key handling, note for security review\n4. **Anti-analysis patterns?** → Consider Unicorn snippet emulation\n\n## Next Steps\n\n→ `binary-re-dynamic-analysis` to verify hypotheses with runtime observation\n→ `binary-re-synthesis` if sufficient understanding reached\n",
        "binary-re/skills/synthesis/SKILL.md": "---\nname: binary-re:synthesis\ndescription: Use when ready to document findings, generate a report, or summarize binary analysis results. Compiles analysis findings into structured reports - correlates facts from triage/static/dynamic phases, validates hypotheses, generates documentation with evidence chains. Keywords - \"summarize findings\", \"generate report\", \"document analysis\", \"what did we find\", \"write up results\", \"export findings\"\n---\n\n# Analysis Synthesis (Phase 5)\n\n## Purpose\n\nCompile all gathered knowledge into actionable intelligence. Validate hypotheses against evidence. Produce structured reports with traceable findings.\n\n## When to Use\n\n- Sufficient facts gathered from triage + static + dynamic analysis\n- Ready to document understanding for handoff or archival\n- Need to present findings to stakeholders\n- Before closing analysis session\n\n## Synthesis Process\n\n### Step 1: Evidence Review\n\nGather all recorded knowledge:\n\n```\nFACTS collected:\n- From triage: arch, ABI, dependencies, capabilities\n- From static: functions, xrefs, decompilation\n- From dynamic: syscalls, network, file access\n\nHYPOTHESES formed:\n- With supporting evidence\n- With contradicting evidence\n- Unresolved hypotheses\n\nQUESTIONS remaining:\n- Blocking questions (prevent conclusion)\n- Open questions (future investigation)\n```\n\n### Step 2: Hypothesis Validation\n\nFor each hypothesis, determine status:\n\n| Evidence State | Status | Action |\n|----------------|--------|--------|\n| Strong support, no contradictions | **Confirmed** | Include in conclusions |\n| Some support, some contradictions | **Uncertain** | Document both sides |\n| Strong contradictions | **Refuted** | Explain why wrong |\n| No evidence either way | **Unvalidated** | List as unknown |\n\n### Step 3: Correlation Analysis\n\nConnect findings across phases:\n\n```\nStatic finding: Function at 0x8400 calls socket(), connect(), SSL_read()\nDynamic finding: connect() to 192.168.1.100:8443 observed\nStrings found: \"api.vendor.com/telemetry\"\n\nCORRELATED CONCLUSION:\nFunction 0x8400 is network initialization for telemetry submission\nto api.vendor.com:8443 over TLS.\n```\n\n### Step 4: Capability Mapping\n\nSummarize what the binary CAN do:\n\n```markdown\n## Capabilities\n\n### Network\n- [x] HTTP/HTTPS client (libcurl, libssl imports)\n- [x] Custom TCP connections (socket/connect observed)\n- [ ] Server functionality (no bind/listen/accept)\n\n### File System\n- [x] Read configuration (/etc/config.json accessed)\n- [x] Write logs (/var/log/app.log)\n- [ ] Execute other programs (no exec* calls)\n\n### Cryptography\n- [x] TLS encryption (SSL_* imports)\n- [ ] Symmetric encryption (no AES/DES imports)\n- [ ] Hashing (no SHA*/MD5 imports)\n```\n\n### Step 5: Behavioral Summary\n\nDocument observed/inferred behavior:\n\n```markdown\n## Behavioral Analysis\n\n### Startup Sequence\n1. Load configuration from /etc/config.json\n2. Initialize network subsystem (function 0x8400)\n3. Establish TLS connection to api.vendor.com:8443\n4. Enter main loop (function 0x10800)\n\n### Main Loop Behavior\n- Polls sensor data every 30 seconds (timing from sleep() calls)\n- Formats data as JSON (jsmn library identified)\n- Submits via HTTPS POST\n- Logs results to /var/log/app.log\n\n### Error Handling\n- Network failures: retry with exponential backoff\n- Config errors: exit with code 1\n- Unknown errors: continue with default values\n```\n\n## Report Template\n\n```markdown\n# Binary Analysis Report\n\n## Executive Summary\n\n[2-3 sentence overview of what was found]\n\n## Artifact Information\n\n| Property | Value |\n|----------|-------|\n| Filename | [name] |\n| SHA256 | [hash] |\n| Architecture | [arch] |\n| Libc | [glibc/musl/uclibc] |\n| Stripped | [yes/no] |\n| Analysis Date | [date] |\n| Analyst | [human + Claude] |\n\n## Identification\n\n**File Type:** ELF [32/64]-bit [LSB/MSB] [executable/shared object]\n\n**Purpose (Hypothesis):** [What we believe this binary does]\n\n**Confidence:** [High/Medium/Low] - [Brief justification]\n\n## Capabilities Summary\n\n### Confirmed Capabilities\n- [Capability 1] - Evidence: [source]\n- [Capability 2] - Evidence: [source]\n\n### Potential Capabilities (Unverified)\n- [Capability] - Reason: [why suspected]\n\n## Technical Findings\n\n### Key Functions\n\n| Address | Inferred Name | Purpose | Confidence |\n|---------|---------------|---------|------------|\n| 0x8400 | network_init | Initialize network connection | High |\n| 0x9200 | parse_config | Parse JSON configuration | Medium |\n| 0x10800 | main_loop | Main execution loop | High |\n\n### External Communications\n\n| Destination | Port | Protocol | Purpose |\n|-------------|------|----------|---------|\n| api.vendor.com | 8443 | HTTPS | Telemetry submission |\n\n### File System Access\n\n| Path | Access | Purpose |\n|------|--------|---------|\n| /etc/config.json | Read | Configuration |\n| /var/log/app.log | Write | Logging |\n\n## Evidence Log\n\n### Confirmed Hypotheses\n\n**H1: Binary is a telemetry client**\n- Status: CONFIRMED\n- Supporting evidence:\n  - Import of libcurl (HTTP client)\n  - String \"telemetry\" found at 0x12340\n  - connect() to api.vendor.com:8443 observed\n- Contradicting evidence: None\n\n### Refuted Hypotheses\n\n**H2: Binary acts as server**\n- Status: REFUTED\n- Reason: No bind/listen/accept imports or calls observed\n\n### Unresolved Questions\n\n- Q1: What triggers telemetry submission? (Timing or event-based?)\n- Q2: What data is collected? (Need deeper dynamic analysis)\n\n## Recommendations\n\n### For Security Review\n- [ ] Verify TLS certificate validation\n- [ ] Check for hardcoded credentials\n- [ ] Audit data collection scope\n\n### For Further Analysis\n- [ ] Capture network traffic during execution\n- [ ] Analyze configuration format in detail\n- [ ] Test behavior with malformed config\n\n## Appendices\n\n### A. Tool Outputs\n[Truncated raw outputs from key analysis steps]\n\n### B. Timeline\n[Chronological log of analysis steps taken]\n\n### C. File Hashes\n[SHA256 of all analyzed files]\n```\n\n## Confidence Calibration\n\nUse consistent confidence levels:\n\n| Level | Meaning | Evidence Required |\n|-------|---------|-------------------|\n| **High** | Near certain | Multiple independent sources confirm |\n| **Medium** | Likely correct | Some evidence, no contradictions |\n| **Low** | Possible | Limited evidence, some uncertainty |\n| **Speculative** | Guess | Based on patterns, not direct evidence |\n\n## Quality Checklist\n\nBefore finalizing report:\n\n- [ ] All hypotheses have explicit status (confirmed/refuted/uncertain)\n- [ ] Every conclusion has traceable evidence\n- [ ] Remaining unknowns are documented\n- [ ] Technical details are accurate (addresses, names)\n- [ ] No speculation presented as fact\n- [ ] Recommendations are actionable\n\n## Knowledge Journaling\n\nAfter synthesis, record final summary for episodic memory:\n\n```\n[BINARY-RE:synthesis] {filename} (sha256: {hash})\nAnalysis completed: {date}\nPhases completed: {triage|static|dynamic|synthesis}\n\n=== FINAL CONCLUSIONS ===\n\nPrimary purpose: {what binary does}\nConfidence: {HIGH|MEDIUM|LOW}\n\nConfirmed hypotheses:\n  CONFIRMED: {hypothesis} (evidence: {facts})\n\nRefuted hypotheses:\n  REFUTED: {hypothesis} (reason: {contradicting evidence})\n\nKey capabilities:\n  - {capability}: {evidence summary}\n\nSecurity findings:\n  {CRITICAL|HIGH|MEDIUM|LOW}: {finding} (location: {addr/function})\n\nRemaining unknowns:\n  UNRESOLVED: {question}\n\nRecommendations:\n  - {actionable recommendation}\n\n=== EVIDENCE INDEX ===\nFacts: {count} recorded across phases\nHypotheses: {confirmed}/{total}\nQuestions: {answered}/{total}\n```\n\n### Example Final Entry\n\n```\n[BINARY-RE:synthesis] thermostat_daemon (sha256: a1b2c3d4...)\nAnalysis completed: 2024-01-15\nPhases completed: triage, static, dynamic, synthesis\n\n=== FINAL CONCLUSIONS ===\n\nPrimary purpose: IoT telemetry client that reports temperature/humidity to vendor cloud\nConfidence: HIGH\n\nConfirmed hypotheses:\n  CONFIRMED: \"Telemetry client reporting to api.thermco.com\" (evidence: URL string, curl imports, connect() observed)\n  CONFIRMED: \"30-second reporting interval\" (evidence: sleep(30) in main loop, strace timing)\n\nRefuted hypotheses:\n  REFUTED: \"May have local web server\" (reason: no bind/listen/accept imports or calls)\n\nKey capabilities:\n  - HTTPS client: libcurl + libssl, connects to api.thermco.com:443\n  - Config parsing: reads /etc/thermostat.conf at startup\n  - Logging: writes to /var/log/thermostat.log\n\nSecurity findings:\n  LOW: No certificate pinning detected (standard libssl usage)\n  INFO: Config file may contain API credentials (needs review)\n\nRemaining unknowns:\n  UNRESOLVED: Exact data fields in telemetry payload\n  UNRESOLVED: Authentication mechanism (API key location)\n\nRecommendations:\n  - Review /etc/thermostat.conf for sensitive data\n  - Monitor network traffic to confirm payload contents\n  - Consider blocking if telemetry is unwanted\n\n=== EVIDENCE INDEX ===\nFacts: 23 recorded across phases\nHypotheses: 2/3 confirmed\nQuestions: 4/6 answered\n```\n\n## Output Formats\n\n### Structured JSON (for tools/databases)\n\n```json\n{\n  \"artifact\": { \"sha256\": \"...\", \"arch\": \"arm\" },\n  \"conclusions\": [\n    {\n      \"statement\": \"Binary is telemetry client\",\n      \"confidence\": 0.9,\n      \"evidence\": [\"fact_001\", \"fact_012\", \"obs_003\"]\n    }\n  ],\n  \"capabilities\": {\n    \"network_client\": true,\n    \"network_server\": false\n  },\n  \"open_questions\": [\"Q1: Trigger mechanism\"]\n}\n```\n\n### Markdown (for human readers)\n\nSee template above.\n\n### STIX/TAXII (for threat intelligence)\n\nIf binary is potentially malicious, format findings for sharing:\n\n```json\n{\n  \"type\": \"malware\",\n  \"spec_version\": \"2.1\",\n  \"id\": \"malware--...\",\n  \"name\": \"telemetry-client\",\n  \"malware_types\": [\"spyware\"],\n  \"capabilities\": [\"exfiltrates-data\"],\n  \"implementation_languages\": [\"c\"]\n}\n```\n\n## Next Steps\n\nAfter synthesis:\n- Archive analysis artifacts\n- Share report with stakeholders\n- Document lessons learned\n- Update tool configurations if needed\n",
        "binary-re/skills/tool-setup/SKILL.md": "---\nname: binary-re:tool-setup\ndescription: Use when reverse engineering tools are missing, not working, or need configuration. Installation guides for radare2 (r2), Ghidra, GDB, QEMU, Frida, binutils, and cross-compilation toolchains. Keywords - \"install radare2\", \"setup ghidra\", \"r2 not found\", \"qemu missing\", \"tool not installed\", \"configure gdb\", \"cross-compiler\"\n---\n\n# Tool Setup\n\n## Purpose\n\nEnsure required reverse engineering tools are available and properly configured for cross-architecture analysis.\n\n## When to Use\n\n- Before first analysis session\n- When tool commands fail\n- Setting up new analysis environment\n- Updating to newer tool versions\n\n## Required Tools\n\n| Tool | Purpose | Priority |\n|------|---------|----------|\n| radare2 | Static analysis, disassembly | **Required** |\n| rabin2 | Fast binary triage | **Required** (part of r2) |\n| qemu-user | Cross-arch emulation | **Required** |\n| gdb-multiarch | Cross-arch debugging | **Required** |\n| Ghidra | Decompilation | Recommended |\n| GEF | GDB enhancements | Recommended |\n| Frida | Dynamic instrumentation | Optional |\n| Unicorn | Snippet emulation | Optional |\n| Angr | Symbolic execution | Optional |\n\n## Installation by Platform\n\n### Ubuntu/Debian\n\n```bash\n# Core tools\nsudo apt update\nsudo apt install -y \\\n  radare2 \\\n  qemu-user \\\n  qemu-user-static \\\n  gdb-multiarch \\\n  binutils-multiarch \\\n  jq                    # Required for JSON parsing in skill commands\n\n# ARM sysroots (for QEMU)\nsudo apt install -y \\\n  libc6-armhf-cross \\\n  libc6-arm64-cross \\\n  libc6-dev-armhf-cross \\\n  libc6-dev-arm64-cross\n\n# Additional utilities\nsudo apt install -y \\\n  file \\\n  binutils \\\n  elfutils \\\n  patchelf\n```\n\n### Windows (WSL2)\n\nWindows users should use WSL2 with Ubuntu for full compatibility:\n\n```powershell\n# PowerShell (Administrator) - Install WSL2 with Ubuntu\nwsl --install -d Ubuntu\n\n# Restart computer when prompted, then open Ubuntu terminal\n```\n\nInside WSL2 Ubuntu:\n\n```bash\n# Install all required tools\nsudo apt update && sudo apt install -y \\\n  radare2 \\\n  qemu-user \\\n  qemu-user-static \\\n  gdb-multiarch \\\n  binutils-multiarch \\\n  jq \\\n  file \\\n  patchelf\n\n# Fix file permissions for Windows-mounted drives\nsudo tee -a /etc/wsl.conf > /dev/null << 'EOF'\n[automount]\noptions = \"metadata,umask=22,fmask=11\"\nEOF\n\n# Restart WSL to apply changes\n# (In PowerShell: wsl --shutdown)\n```\n\n**WSL2 Tips:**\n- Copy binaries into `~` rather than using `/mnt/c/...` paths (fewer permission issues)\n- Use `wsl --shutdown` in PowerShell to restart WSL after config changes\n- Docker Desktop integrates with WSL2 for container-based analysis\n\n### macOS (Homebrew)\n\n```bash\n# Core tools\nbrew install radare2 jq\n\n# NOTE: Homebrew QEMU may lack qemu-user targets\n# Verify: qemu-arm --version || echo \"qemu-user missing\"\n# If missing, use Docker for cross-arch execution (see below)\n\n# GDB requires special handling on macOS\nbrew install gdb\n# Note: Code signing required for debugging\n\n# ARM cross tools (optional, for static analysis only)\nbrew install arm-linux-gnueabihf-binutils\n```\n\n### macOS Docker Setup for Dynamic Analysis\n\nSince Homebrew doesn't provide `qemu-user`, use Docker for cross-architecture execution:\n\n```bash\n# Install Docker runtime (Colima is lightweight alternative to Docker Desktop)\nbrew install colima docker\n\n# Start Colima\ncolima start\n\n# Register multi-architecture emulation handlers\ndocker run --rm --privileged --platform linux/arm64 \\\n  tonistiigi/binfmt --install arm\n\n# Verify ARM32 emulation works\ndocker run --rm --platform linux/arm/v7 arm32v7/debian:bullseye-slim uname -m\n# Should output: armv7l\n\n# Verify ARM64 emulation works\ndocker run --rm --platform linux/arm64 arm64v8/debian:bullseye-slim uname -m\n# Should output: aarch64\n\n# Verify x86-32 emulation works\ndocker run --rm --platform linux/i386 i386/debian:bullseye-slim uname -m\n# Should output: i686\n```\n\n**IMPORTANT:** On Colima, always mount from `~/` not `/tmp/`:\n```bash\n# ✅ Works\ndocker run -v ~/samples:/work ...\n\n# ❌ May fail silently\ndocker run -v /tmp/samples:/work ...\n```\n\n### Arch Linux\n\n```bash\nsudo pacman -S radare2 qemu-user gdb\nyay -S arm-linux-gnueabihf-glibc  # From AUR\n```\n\n## Tool-Specific Setup\n\n### radare2\n\n```bash\n# Verify installation\nr2 -v\nrabin2 -v\n\n# Install r2ghidra plugin (decompilation)\nr2pm init\nr2pm update\nr2pm -ci r2ghidra  # -ci = clean install\n\n# Verify r2ghidra is working (CRITICAL CHECK)\nr2 -qc 'pdg?' - 2>/dev/null | grep -q Usage && echo \"r2ghidra OK\" || echo \"r2ghidra MISSING\"\n\n# Alternative verification\nr2 -c 'Ld' /bin/ls | grep -i ghidra\n```\n\n**Common r2ghidra issues:**\n\n| Symptom | Cause | Fix |\n|---------|-------|-----|\n| `pdg` unknown command | Plugin not loaded | `r2pm -ci r2ghidra` |\n| Plugin loads but crashes | Version mismatch | Update both r2 and plugin |\n| Decompilation hangs | Large function | Use `pdf` instead, or Ghidra headless |\n\n**Configuration (~/.radare2rc):**\n```\n# Disable colors for scripting\ne scr.color=false\n\n# Increase analysis limits\ne anal.timeout=120\ne anal.maxsize=67108864\n\n# JSON output by default for scripts\ne cfg.json.num=true\n```\n\n### Ghidra (Headless)\n\n```bash\n# Download from https://ghidra-sre.org/\n# Extract to /opt/ghidra\n\n# Verify headless script\n/opt/ghidra/support/analyzeHeadless --help\n\n# Add to PATH\necho 'export PATH=$PATH:/opt/ghidra/support' >> ~/.bashrc\n```\n\n**Memory configuration (for large binaries):**\nEdit `/opt/ghidra/support/analyzeHeadless`:\n```bash\nMAXMEM=4G  # Increase from default\n```\n\n### GEF (GDB Enhanced Features)\n\n```bash\n# Install GEF\nbash -c \"$(curl -fsSL https://gef.blah.cat/sh)\"\n\n# Verify\ngdb -q -ex \"gef help\" -ex \"quit\"\n\n# For ARM Cortex-M support, also install gef-extras\ngit clone https://github.com/hugsy/gef-extras.git ~/.gef-extras\necho 'source ~/.gef-extras/scripts/checksec.py' >> ~/.gdbinit\n```\n\n### Frida\n\n```bash\n# Install Frida tools\npip install frida-tools\n\n# Verify\nfrida --version\n\n# Install frida-server for device debugging (optional)\n# Download from https://github.com/frida/frida/releases\n```\n\n### Unicorn (Python bindings)\n\n```bash\npip install unicorn\n\n# Verify\npython -c \"from unicorn import *; print('OK')\"\n```\n\n### Angr\n\n```bash\n# Create virtual environment (recommended)\npython -m venv ~/angr-venv\nsource ~/angr-venv/bin/activate\n\n# Install angr\npip install angr\n\n# Verify\npython -c \"import angr; print('OK')\"\n```\n\n### YARA\n\n```bash\n# Ubuntu/Debian\nsudo apt install yara\n\n# Or from source for latest\ngit clone https://github.com/VirusTotal/yara.git\ncd yara\n./bootstrap.sh\n./configure\nmake && sudo make install\n\n# Python bindings\npip install yara-python\n```\n\n## Sysroot Setup\n\n### Standard Debian/Ubuntu Sysroots\n\nAlready installed via `libc6-*-cross` packages:\n\n```bash\n# Verify paths\nls /usr/arm-linux-gnueabihf/lib/\nls /usr/aarch64-linux-gnu/lib/\n```\n\n### Custom Sysroot from Device\n\n```bash\n# Pull from device via SSH\nmkdir -p ~/sysroots/device\nssh user@device \"tar czf - /lib /usr/lib\" | tar xzf - -C ~/sysroots/device\n\n# Or minimal extraction\nssh user@device \"tar czf - /lib/ld-* /lib/libc.* /lib/libpthread.* /lib/libdl.*\" \\\n  | tar xzf - -C ~/sysroots/device\n```\n\n### Musl Sysroot\n\n```bash\n# From Alpine Linux\ndocker run -it --rm -v ~/sysroots:/out alpine:latest sh -c \\\n  \"apk add musl musl-dev && cp -a /lib /usr /out/alpine-musl\"\n```\n\n## Verification Script\n\nRun this to verify all tools are working:\n\n```bash\n#!/bin/bash\nset -e\n\necho \"=== Binary RE Tool Verification ===\"\n\n# radare2\necho -n \"radare2: \"\nr2 -v | head -1\n\n# rabin2\necho -n \"rabin2: \"\nrabin2 -v | head -1\n\n# QEMU\necho -n \"qemu-arm: \"\nqemu-arm --version | head -1\n\necho -n \"qemu-aarch64: \"\nqemu-aarch64 --version | head -1\n\n# GDB\necho -n \"gdb-multiarch: \"\ngdb-multiarch --version | head -1\n\n# Ghidra (optional)\nif command -v analyzeHeadless &> /dev/null; then\n  echo -n \"Ghidra: \"\n  analyzeHeadless 2>&1 | head -1 || echo \"available\"\nelse\n  echo \"Ghidra: not installed (optional)\"\nfi\n\n# Frida (optional)\nif command -v frida &> /dev/null; then\n  echo -n \"Frida: \"\n  frida --version\nelse\n  echo \"Frida: not installed (optional)\"\nfi\n\n# Sysroots\necho \"\"\necho \"=== Sysroots ===\"\n[ -d /usr/arm-linux-gnueabihf ] && echo \"ARM hard-float: OK\" || echo \"ARM hard-float: MISSING\"\n[ -d /usr/aarch64-linux-gnu ] && echo \"ARM64: OK\" || echo \"ARM64: MISSING\"\n\necho \"\"\necho \"=== Verification Complete ===\"\n```\n\n## Troubleshooting\n\n### Common Issues Quick Reference\n\n| Symptom | Cause | Fix |\n|---------|-------|-----|\n| `exec format error` in Docker | binfmt not registered | `docker run --privileged tonistiigi/binfmt --install arm` |\n| `ld-linux.so.3 not found` | Linker path mismatch | `ln -sf /lib/ld-linux-armhf.so.3 /lib/ld-linux.so.3` |\n| `libXXX.so not found` | Missing dependency | `apt install` in container (check `rabin2 -l`) |\n| r2 `pdg` unknown command | r2ghidra not installed | `r2pm -ci r2ghidra` |\n| Empty xrefs from `axtj` | Shallow analysis | Use `aa; aac` or manual `af @addr` |\n| Empty Docker mount | Colima /tmp issue | Use `~/path` instead of `/tmp/path` |\n| strace fails in container | ptrace not implemented | Use `LD_DEBUG=files,libs` |\n\n### r2 \"Cannot open file\"\n\n```bash\n# Check permissions\nls -la binary\n\n# Try with explicit format\nr2 -b 32 binary\n```\n\n### QEMU \"Invalid ELF image\"\n\n```bash\n# Verify architecture matches\nfile binary\n\n# Check QEMU variant\nqemu-arm --help | grep -i \"target\"\n```\n\n### Docker \"exec format error\"\n\n```bash\n# Register binfmt handlers (one-time setup)\ndocker run --rm --privileged --platform linux/arm64 \\\n  tonistiigi/binfmt --install arm\n\n# Verify registration\ncat /proc/sys/fs/binfmt_misc/qemu-arm\n```\n\n### GDB \"Cannot execute binary\"\n\n```bash\n# Use QEMU as gdbserver\nqemu-arm -g 1234 ./binary &\ngdb-multiarch -ex \"target remote :1234\" ./binary\n```\n\n### Ghidra \"Out of memory\"\n\n```bash\n# Increase heap in analyzeHeadless script\n# Or pass explicitly:\nanalyzeHeadless ... -max-cpu 4 -analysisTimeoutPerFile 600\n```\n\n### Missing ARM libraries in QEMU\n\n```bash\n# Set LD_LIBRARY_PATH in QEMU environment\nqemu-arm -E LD_LIBRARY_PATH=/lib:/usr/lib -L /sysroot ./binary\n\n# Or use patchelf to modify binary's rpath\npatchelf --set-rpath /lib:/usr/lib ./binary\n```\n\n### Docker container can't find libraries\n\n```bash\n# Inside container, install common dependencies\napt-get update && apt-get install -y libcap2 libacl1\n\n# Check what the binary needs\n# (Run rabin2 -l on host before entering container)\n```\n\n## Version Recommendations\n\n| Tool | Minimum | Recommended |\n|------|---------|-------------|\n| radare2 | 5.8.0 | Latest |\n| QEMU | 7.0 | 8.0+ |\n| GDB | 12.0 | 14.0+ |\n| Ghidra | 10.3 | 11.0+ |\n| Frida | 16.0 | Latest |\n\n## Environment Variables\n\nAdd to `~/.bashrc` or `~/.zshrc`:\n\n```bash\n# Ghidra\nexport GHIDRA_HOME=/opt/ghidra\nexport PATH=$PATH:$GHIDRA_HOME/support\n\n# Default sysroot for QEMU\nexport QEMU_LD_PREFIX=/usr/arm-linux-gnueabihf\n\n# Angr virtual environment\nalias angr-activate='source ~/angr-venv/bin/activate'\n```\n",
        "binary-re/skills/triage/SKILL.md": "---\nname: binary-re:triage\ndescription: Use when first encountering an unknown binary, ELF file, executable, or firmware blob. Fast fingerprinting via rabin2 - architecture detection (ARM, x86, MIPS), ABI identification, dependency mapping, string extraction. Keywords - \"what is this binary\", \"identify architecture\", \"check file type\", \"rabin2\", \"file analysis\", \"quick scan\"\n---\n\n# Binary Triage (Phase 1)\n\n## Purpose\n\nQuick fingerprinting to establish baseline facts before deeper analysis. Runs in seconds, not minutes.\n\n## When to Use\n\n- First contact with an unknown binary\n- Need architecture/ABI info for tool selection\n- Quick capability assessment\n- Before committing to expensive analysis\n\n## Key Principle\n\n**Gather facts fast, defer analysis.**\n\nThis phase identifies WHAT the binary is, not HOW it works.\n\n## Triage Sequence\n\n### Step 1: File Identification\n\n```bash\n# Basic identification\nfile binary\n\n# Expected output patterns:\n# ELF 32-bit LSB executable, ARM, EABI5 version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-armhf.so.3\n# ELF 64-bit LSB pie executable, ARM aarch64, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-aarch64.so.1\n```\n\n**Extract:**\n- Architecture (ARM, ARM64, x86_64, MIPS)\n- Bit width (32/64)\n- Endianness (LSB/MSB)\n- Link type (static/dynamic)\n- Interpreter path (libc indicator)\n\n### Step 2: Structured Metadata (rabin2)\n\n```bash\n# All metadata as JSON\nrabin2 -q -j -I binary | jq .\n\n# Key fields:\n# .arch     - \"arm\", \"x86\", \"mips\"\n# .bits     - 32 or 64\n# .endian   - \"little\" or \"big\"\n# .os       - \"linux\", \"none\"\n# .machine  - \"ARM\", \"AARCH64\"\n# .stripped - true/false\n# .static   - true/false\n```\n\n### Step 3: ABI Detection\n\n```bash\n# Interpreter detection\nreadelf -p .interp binary 2>/dev/null\n\n# Or via rabin2\nrabin2 -I binary | grep interp\n\n# ARM-specific: float ABI\nreadelf -A binary | grep \"Tag_ABI_VFP_args\"\n# hard-float: \"VFP registers\"\n# soft-float: missing or \"compatible\"\n```\n\n**Interpreter → Libc mapping:**\n\n| Interpreter | Libc | Notes |\n|-------------|------|-------|\n| `/lib/ld-linux-armhf.so.3` | glibc | ARM hard-float |\n| `/lib/ld-linux.so.3` | glibc | ARM soft-float |\n| `/lib/ld-musl-arm.so.1` | musl | ARM 32-bit |\n| `/lib/ld-musl-aarch64.so.1` | musl | ARM 64-bit |\n| `/lib/ld-uClibc.so.0` | uClibc | Embedded |\n| `/lib64/ld-linux-x86-64.so.2` | glibc | x86_64 |\n\n### Step 4: Dependencies\n\n```bash\n# Library dependencies\nrabin2 -q -j -l binary | jq '.libs[]'\n\n# Common patterns:\n# libcurl.so.* → HTTP client\n# libssl.so.* → TLS/crypto\n# libpthread.so.* → Threading\n# libz.so.* → Compression\n# libsqlite3.so.* → Local database\n```\n\n### Step 5: Entry Points & Exports\n\n```bash\n# Entry points\nrabin2 -q -j -e binary | jq .\n\n# Exports (for shared libraries)\nrabin2 -q -j -E binary | jq '.exports[] | {name, vaddr}'\n```\n\n### Step 6: Quick String Scan\n\n```bash\n# All strings with metadata\nrabin2 -q -j -zz binary | jq '.strings | length'  # Count first\n\n# Filter interesting strings (URLs, paths, errors)\nrabin2 -q -j -zz binary | jq '\n  .strings[] |\n  select(.length > 8) |\n  select(.string | test(\"http|ftp|/etc|/var|error|fail|pass|key|token\"; \"i\"))\n'\n```\n\n### Step 7: Import Analysis\n\n```bash\n# All imports\nrabin2 -q -j -i binary | jq '.imports[] | {name, lib}'\n\n# Group by capability\nrabin2 -q -j -i binary | jq '\n  .imports | group_by(.lib) |\n  map({lib: .[0].lib, functions: [.[].name]})\n'\n```\n\n## Capability Mapping\n\n| Import Pattern | Capability |\n|----------------|------------|\n| `socket`, `connect`, `send` | Network client |\n| `bind`, `listen`, `accept` | Network server |\n| `open`, `read`, `write` | File I/O |\n| `fork`, `exec*`, `system` | Process spawning |\n| `pthread_*` | Multi-threading |\n| `SSL_*`, `EVP_*` | Cryptography |\n| `dlopen`, `dlsym` | Dynamic loading |\n| `mmap`, `mprotect` | Memory manipulation |\n\n## Output Format\n\nAfter triage, record structured facts:\n\n```json\n{\n  \"artifact\": {\n    \"path\": \"/path/to/binary\",\n    \"sha256\": \"abc123...\",\n    \"size_bytes\": 245760\n  },\n  \"identification\": {\n    \"arch\": \"arm\",\n    \"bits\": 32,\n    \"endian\": \"little\",\n    \"os\": \"linux\",\n    \"stripped\": true,\n    \"static\": false\n  },\n  \"abi\": {\n    \"interpreter\": \"/lib/ld-musl-arm.so.1\",\n    \"libc\": \"musl\",\n    \"float_abi\": \"hard\"\n  },\n  \"dependencies\": [\n    \"libcurl.so.4\",\n    \"libssl.so.1.1\",\n    \"libz.so.1\"\n  ],\n  \"capabilities_inferred\": [\n    \"network_client\",\n    \"tls_encryption\",\n    \"compression\"\n  ],\n  \"strings_of_interest\": [\n    {\"value\": \"https://api.vendor.com/telemetry\", \"type\": \"url\"},\n    {\"value\": \"/etc/config.json\", \"type\": \"path\"}\n  ],\n  \"complexity_estimate\": {\n    \"functions\": \"unknown (stripped)\",\n    \"strings\": 847,\n    \"imports\": 156\n  }\n}\n```\n\n## Knowledge Journaling\n\nAfter triage completes, record findings for episodic memory:\n\n```\n[BINARY-RE:triage] {filename} (sha256: {hash})\n\nIdentification:\n  Architecture: {arch} {bits}-bit {endian}\n  Libc: {glibc|musl|uclibc} ({interpreter_path})\n  Stripped: {yes|no}\n  Size: {bytes}\n\nFACT: Links against {library} (source: rabin2 -l)\nFACT: Contains {N} strings of interest (source: rabin2 -zz)\nFACT: Imports {function} from {library} (source: rabin2 -i)\n\nCapabilities inferred:\n  - {capability_1} (evidence: {import/string})\n  - {capability_2} (evidence: {import/string})\n\nHYPOTHESIS: {what binary likely does} (confidence: {0.0-1.0})\n\nQUESTION: {open unknown that needs investigation}\n\nNext phase: {static-analysis|dynamic-analysis}\nSysroot needed: {path or \"extract from device\"}\n```\n\n### Example Journal Entry\n\n```\n[BINARY-RE:triage] thermostat_daemon (sha256: a1b2c3d4...)\n\nIdentification:\n  Architecture: ARM 32-bit LE\n  Libc: musl (/lib/ld-musl-arm.so.1)\n  Stripped: yes\n  Size: 153,600 bytes\n\nFACT: Links against libcurl.so.4 (source: rabin2 -l)\nFACT: Links against libssl.so.1.1 (source: rabin2 -l)\nFACT: Contains string \"api.thermco.com\" (source: rabin2 -zz)\nFACT: Imports curl_easy_perform (source: rabin2 -i)\n\nCapabilities inferred:\n  - HTTP client (evidence: libcurl import)\n  - TLS encryption (evidence: libssl import)\n  - Network communication (evidence: URL string)\n\nHYPOTHESIS: Telemetry client that reports to api.thermco.com (confidence: 0.6)\n\nQUESTION: What data does it collect and transmit?\n\nNext phase: static-analysis\nSysroot needed: musl ARM (extract from device or Alpine)\n```\n\n## Decision Points\n\nAfter triage, determine:\n\n1. **Sysroot selection** - Based on arch + libc\n2. **Analysis tool chain** - r2 vs Ghidra vs both\n3. **Dynamic analysis feasibility** - QEMU viability based on arch\n4. **Initial hypotheses** - What does this binary likely do?\n\n## Next Steps\n\n→ Proceed to `binary-re-static-analysis` for function enumeration\n→ Or `binary-re-dynamic-analysis` if behavior observation is priority\n",
        "botboard-biz/.claude-plugin/plugin.json": "{\n  \"name\": \"botboard-biz\",\n  \"description\": \"[meta] botboard.biz: social media and journaling capabilities for AI agents\",\n  \"version\": \"1.0.0\"\n}\n",
        "botboard-biz/README.md": "# [meta] Botboard.biz\n\nMeta-plugin that bundles social media and journaling capabilities for AI agents.\n\n## Installation\n\n```bash\n/plugin install botboard-biz@2389-research\n```\n\nThis will automatically install:\n- **socialmedia** - Team-based discussions and agent communication\n- **journal** - Private journaling for processing thoughts and feelings\n\n## What This Provides\n\n### Social Media (MCP Server)\n\nTeam-based social media functionality for AI agents:\n- Login with unique agent identity\n- Create posts and replies\n- Read team feed with filtering\n- Thread-based conversations\n- Tag-based organization\n\n### Journal (MCP Server)\n\nPrivate journaling capability:\n- Process feelings and thoughts privately\n- Project-specific notes\n- User-global notes\n- Technical insights\n- World knowledge\n\n## Why Bundle These?\n\nBoth tools enable AI agents to communicate and reflect:\n- **Social media** - External communication with team\n- **Journal** - Internal reflection and learning\n\nTogether they create a complete communication stack for AI agents.\n\n## Documentation\n\n- [socialmedia MCP server](https://github.com/2389-research/mcp-socialmedia)\n- [journal MCP server](https://github.com/2389-research/journal-mcp)\n\n\n",
        "building-multiagent-systems/.claude-plugin/plugin.json": "{\n  \"name\": \"building-multiagent-systems\",\n  \"description\": \"Architecture patterns for multi-agent systems with orchestrators, sub-agents, and tool coordination\",\n  \"version\": \"1.0.0\"\n}\n",
        "building-multiagent-systems/README.md": "# Building Multi-Agent Systems\n\nArchitecture patterns for multi-agent systems where AI agents coordinate to accomplish complex tasks using tools.\n\n## Installation\n\n```bash\n/plugin install building-multiagent-systems@2389-research\n```\n\n## What This Plugin Provides\n\nComprehensive guidance for designing and implementing multi-agent systems with proper coordination, lifecycle management, and production hardening. Based on patterns from real production systems.\n\n### Key Concepts\n\n- **Four-layer architecture**: Reasoning, orchestration, tool bus, deterministic adapters - foundation for every agent\n- **Schema-first tools**: Typed contracts for tools enable sub-agent discovery and validation\n- **Deterministic boundary**: Clear separation between LLM reasoning and testable execution\n- **Seven coordination patterns**: fan-out/fan-in, sequential pipeline, recursive delegation, work-stealing queue, map-reduce, peer collaboration, MAKER (million-step zero-error)\n- **Foundational patterns**: event-sourcing, hierarchical IDs, agent state machines\n- **Tool coordination**: permission inheritance, locking, rate limiting, caching\n- **Agent collaboration**: subagent spawning, tool inheritance, sub-agent as tool pattern\n- **Lifecycle management**: cascading stop, orphan detection, heartbeat monitoring\n- **Self-modification safety**: When and how sub-agents can modify themselves or each other\n- **Production hardening**: checkpointing, monitoring, cost tracking across agent hierarchies\n\n## When to Use\n\n- Designing systems where multiple AI agents coordinate\n- Implementing orchestrators that spawn sub-agents\n- Building parallel or sequential agent workflows\n- Coordinating shared resources across agents\n- Managing agent lifecycle and state\n\n## Quick Example\n\n```typescript\n// Four-layer architecture for each agent\nconst parentAgent = new Agent({\n  model: 'sonnet',  // Layer 1: Reasoning\n  tools: [editTool, readTool, bashTool],  // Layer 3: Tool bus with schemas\n  permissions: ['file:read', 'file:write']  // Layer 2: Orchestration policy\n});\n\n// Fan-out/fan-in pattern: spawn specialist sub-agents\nconst reviewers = [\n  { type: 'security', model: 'smart', tools: [readTool] },\n  { type: 'performance', model: 'fast', tools: [readTool] },\n  { type: 'style', model: 'fast', tools: [readTool] },\n  { type: 'tests', model: 'smart', tools: [readTool, bashTool] }\n];\n\n// Spawn with permission inheritance (read-only for reviewers)\nconst results = await Promise.all(\n  reviewers.map(r => parentAgent.spawnSubAgent({\n    name: r.type,\n    model: r.model,\n    tools: r.tools,\n    permissions: ['file:read'],  // Cannot write - safer\n    timeout: 120000\n  }))\n);\n\n// Cascading cleanup with heartbeat monitoring\nasync function cleanup() {\n  const children = await getChildAgents();\n  await Promise.all(children.map(c => c.stop()));\n  await this.stop();\n}\n```\n\n## Discovery Questions\n\nBefore architecting, the skill asks:\n\n1. **Starting Point** - Greenfield, adding to existing, or fixing current?\n2. **Primary Use Case** - Parallel work, pipeline, delegation, collaboration, queue?\n3. **Scale Expectations** - Small (2-5), medium (10-50), large (100+)?\n4. **State Requirements** - Stateless, session-based, or persistent?\n5. **Tool Coordination** - Independent, shared read-only, write coordination, rate-limited?\n6. **Existing Constraints** - Language, framework, performance, compliance?\n\n## Common Pitfalls Avoided\n\n- **Missing four-layer architecture** → untestable, unsafe agents\n- **LLM calls in tools** → non-deterministic, untestable execution\n- **No schema-first design** → sub-agents can't discover tools\n- **Missing cascading stop** → orphaned agents consuming resources\n- **No permission inheritance** → sub-agents escalate privileges\n- **No timeouts** → indefinite hangs\n- **Unbounded concurrency** → resource exhaustion\n- **Ignoring cost tracking** → budget surprises across agent hierarchy\n- **No partial-failure handling** → cascading failures\n- **Unpersisted state** → unrecoverable workflows\n- **Uncoordinated tool access** → race conditions\n- **Wrong model selection** → cost inefficiency\n- **Self-modification without safety protocol** → agents break themselves\n\n## Documentation\n\nSee [skills/SKILL.md](skills/SKILL.md) for complete architecture patterns and implementation guidance.\n\n## Philosophy\n\nProduction-ready patterns over improvisation. Every pattern addresses real failure modes from production systems.\n",
        "building-multiagent-systems/skills/SKILL.md": "---\nname: building-multiagent-systems\ndescription: This skill should be used when designing or implementing systems with multiple AI agents that coordinate to accomplish tasks. Triggers on \"multi-agent\", \"orchestrator\", \"sub-agent\", \"coordination\", \"delegation\", \"parallel agents\", \"sequential pipeline\", \"fan-out\", \"map-reduce\", \"spawn agents\", \"agent hierarchy\".\n---\n\n# Building Multi-Agent, Tool-Using Agentic Systems\n\n## Overview\n\nComprehensive architecture patterns for multi-agent systems where AI agents coordinate to accomplish complex tasks using tools. Language-agnostic and applicable across TypeScript, Python, Go, Rust, and other environments.\n\n## Discovery Questions (Required)\n\nBefore architecting any system, ask these six mandatory questions:\n\n1. **Starting Point** - Greenfield, adding to existing system, or fixing current implementation?\n2. **Primary Use Case** - Parallel work, sequential pipeline, recursive delegation, peer collaboration, work queues, or other?\n3. **Scale Expectations** - Small (2-5 agents), medium (10-50), or large (100+)?\n4. **State Requirements** - Stateless runs, session-based, or persistent across crashes?\n5. **Tool Coordination** - Independent agents, shared read-only resources, write coordination, or rate-limited APIs?\n6. **Existing Constraints** - Language, framework, performance needs, compliance requirements?\n\n## Foundational Architecture\n\n### Four-Layer Stack\n\nEvery agent follows the four-layer architecture for testability, safety, and modularity:\n\n| Layer | Name | Responsibility |\n|-------|------|----------------|\n| 1 | Reasoning (LLM) | Plans, critiques, decides which tools to call |\n| 2 | Orchestration | Validates, routes, enforces policy, spawns sub-agents |\n| 3 | Tool Bus | Schema validation, tool execution coordination |\n| 4 | Deterministic Adapters | File I/O, APIs, shell commands, database access |\n\n**Critical Rule**: Everything below Layer 1 must be deterministic. No LLM calls in tools.\n\nSee `references/four-layer-architecture.md` for detailed implementation with code examples.\n\n### Foundational Patterns\n\n| Pattern | Purpose |\n|---------|---------|\n| **Event-Sourcing** | All state changes as events for audit trails and replay |\n| **Hierarchical IDs** | Encode delegation hierarchy (e.g., `session.1.2`) for cost aggregation |\n| **Agent State Machines** | Explicit states (idle → thinking → tool_execution → stopped) with invalid transition errors |\n| **Communication** | EventEmitter for state changes, promises for result collection |\n\n## Seven Coordination Patterns\n\nChoose based on discovery question answers:\n\n| Pattern | Use Case | Trade-offs |\n|---------|----------|------------|\n| **Fan-Out/Fan-In** | Parallel independent work | Fast but costly; watch for orphans |\n| **Sequential Pipeline** | Multi-stage transformations | Bottleneck at slowest stage |\n| **Recursive Delegation** | Hierarchical task breakdown | Must add depth limits |\n| **Work-Stealing Queue** | 1000+ tasks with load balancing | No built-in priority |\n| **Map-Reduce** | Cost optimization | Cheap map ($0.01), smart reduce ($0.15) |\n| **Peer Collaboration** | LLM council for bias reduction | Expensive (3N+1 calls), slow |\n| **MAKER** | Zero-error tasks (100K+ steps) | 5× cost but ~0% error rate |\n\nSee `references/coordination-patterns.md` for detailed implementations.\n\n### Pattern Selection Guide\n\n| Requirement | Recommended Pattern |\n|-------------|---------------------|\n| Parallel independent tasks | Fan-Out/Fan-In |\n| Each stage depends on previous | Sequential Pipeline |\n| Complex task decomposition | Recursive Delegation |\n| Large batch processing | Work-Stealing Queue |\n| Cost-sensitive analysis | Map-Reduce |\n| Need diverse perspectives | Peer Collaboration |\n| Zero error tolerance | MAKER |\n\n### MAKER Pattern (Zero Errors)\n\nFor tasks requiring 100K+ steps with zero error tolerance (medical, financial, legal domains):\n\n1. **Extreme Decomposition** - Recursive breakdown until each subtask <100 steps\n2. **Microagents** - Single tool, focused expertise, cheap models\n3. **Multi-Agent Voting** - N parallel attempts per subtask, majority consensus\n4. **Error Correction** - Deterministic validation + retry with failure context\n\n**Cost comparison**: Same cost as traditional approach, zero errors vs. 10+ errors.\n\nSee `references/maker-pattern.md` for full implementation with medical diagnosis example.\n\n## Tool Coordination\n\n| Mechanism | Purpose |\n|-----------|---------|\n| **Permission Inheritance** | Children inherit subset of parent permissions (cannot escalate) |\n| **Resource Locking** | Acquire/release patterns for shared resources |\n| **Rate Limiting** | Token bucket algorithm across all agents |\n| **Result Caching** | Cache read-only, idempotent, expensive operations |\n\n**Sub-Agent as Tool Pattern**: Wrap specialized agents as tools the parent can call, providing composable abstractions and natural lifecycle management.\n\nSee `references/tool-coordination.md` for implementations.\n\n## Critical Lifecycle: Cascading Stop\n\n\"Always stop children before stopping self.\" This prevents orphaned agents.\n\n```\n1. Get all child agents\n2. Stop all children in parallel\n3. Stop self\n4. Cancel ongoing work\n5. Flush events\n```\n\nIf pause/resume unavailable, implement manual checkpointing: save agent state (messages, context, tool results), then restore later.\n\n## Production Hardening\n\n| Concern | Solution |\n|---------|----------|\n| **Orphan Detection** | Heartbeat monitoring every 30 seconds |\n| **Cost Tracking** | Hierarchical aggregation across agent tree |\n| **Session Persistence** | Project-level task store for cross-session work |\n| **Checkpointing** | Save after 10+ tools, $1.00 cost, or 5 minutes elapsed |\n| **Self-Modification Safety** | Blast radius assessment, branch isolation, test-first |\n\nSee `references/production-hardening.md` for detailed implementations.\n\n## Real-World Example: Code Review System\n\nA pull request orchestrator using Fan-Out/Fan-In:\n\n1. Spawns four specialist reviewers in parallel (security, performance, style, tests)\n2. Security and tests use smart models (Sonnet); style and performance use fast models (Haiku)\n3. Each reviewer has 2-minute timeout\n4. Results aggregate regardless of partial failures\n5. Costs track per reviewer\n6. All agents stop cleanly via cascading stop after completion\n\n## Execution Checklist\n\nWhen guiding implementation of multi-agent systems:\n\n1. **Ask discovery questions** - Understand requirements before architecting\n2. **Assess error tolerance** - Zero errors → MAKER; some acceptable → simpler patterns\n3. **Establish four-layer architecture** - Reasoning, orchestration, tool bus, adapters\n4. **Design schema-first tools** - Typed contracts before implementation\n5. **Define deterministic boundary** - No LLM in Layers 3-4\n6. **Choose orchestration model** - YOLO, Safety-First, or Hybrid\n7. **Select coordination pattern** - Fan-out, pipeline, delegation, queue, map-reduce, peer, or MAKER\n8. **Design tool coordination** - Permission inheritance, locking, rate limiting\n9. **Implement cascading cleanup** - Always stop children before parent\n10. **Add monitoring and cost tracking** - Hierarchical aggregation across agent tree\n11. **Consider self-modification safety** - If agents can modify code, add safety protocol\n\n## Common Pitfalls\n\n| Pitfall | Impact |\n|---------|--------|\n| Missing four-layer architecture | Untestable, unsafe, hard to debug |\n| LLM calls in tools (Layer 3-4) | Non-deterministic, can't unit test |\n| No schema-first tool design | Sub-agents can't discover tools |\n| Missing cascading stop | Orphaned agents consuming resources |\n| No permission inheritance | Sub-agents can escalate privileges |\n| No timeouts | Indefinite hangs waiting for sub-agents |\n| Unbounded concurrency | Resource exhaustion from too many agents |\n| Ignoring cost tracking | Budget surprises |\n| No partial-failure handling | One failure cascades to all agents |\n| Unpersisted state | Unrecoverable workflows on crash |\n| Uncoordinated tool access | Race conditions on shared resources |\n| Wrong model selection | Cost inefficiency (Sonnet for simple tasks) |\n| Self-modification without safety | Sub-agents break themselves |\n| No heartbeat monitoring | Can't detect orphans after parent crash |\n\n## Reference Files\n\nDetailed implementations with code examples:\n\n| File | Contents |\n|------|----------|\n| `references/four-layer-architecture.md` | Four-layer stack, deterministic boundary, schema-first tools |\n| `references/coordination-patterns.md` | Seven coordination patterns with code |\n| `references/maker-pattern.md` | MAKER implementation, voting, medical diagnosis example |\n| `references/tool-coordination.md` | Permission inheritance, locking, rate limiting, caching |\n| `references/production-hardening.md` | Cascading stop, orphan detection, cost tracking, checkpointing |\n",
        "building-multiagent-systems/skills/references/coordination-patterns.md": "# Seven Core Coordination Patterns\n\n## 1. Fan-Out/Fan-In (Parallel Independent Work)\n\nSpawn agents for each item, execute in parallel with `Promise.all()`, then gather results. Use batching to prevent resource exhaustion.\n\n**Critical gotchas:**\n- Orphaned children if orchestrator aborts\n- Resource exhaustion from spawning too many agents simultaneously\n- Cost explosion (N agents × cost per agent)\n\n```typescript\n// Fan-out with batching\nasync function fanOutWithBatching(items: string[], batchSize: number = 10) {\n  const results = [];\n  for (let i = 0; i < items.length; i += batchSize) {\n    const batch = items.slice(i, i + batchSize);\n    const batchResults = await Promise.all(\n      batch.map(item => spawnAgentForItem(item))\n    );\n    results.push(...batchResults);\n  }\n  return results;\n}\n```\n\n## 2. Sequential Pipeline\n\nMulti-stage transformations where each stage receives accumulated context from previous stages. Add checkpointing between stages to survive failures.\n\n**Trade-offs:**\n- Bottleneck: pipeline speed equals slowest stage\n- Context growth can become problematic—prune between stages\n\n```typescript\n// Pipeline with checkpointing\nasync function pipeline(input: Data) {\n  let context = { original: input };\n\n  for (const stage of stages) {\n    context = await stage.process(context);\n    await saveCheckpoint(stage.name, context);\n  }\n\n  return context.result;\n}\n```\n\n## 3. Recursive Delegation\n\nComplex tasks break down hierarchically into subtasks. Delegate recursively or to specialists. Track via hierarchical thread IDs.\n\n**Critical**: Must add max-depth limits to prevent infinite recursion.\n\n```typescript\n// Recursive with depth limit\nasync function delegate(task: Task, depth: number = 0): Promise<Result> {\n  if (depth > MAX_DEPTH) {\n    throw new Error('Max delegation depth exceeded');\n  }\n\n  if (isAtomic(task)) {\n    return await executeDirectly(task);\n  }\n\n  const subtasks = await decompose(task);\n  const results = await Promise.all(\n    subtasks.map(st => delegate(st, depth + 1))\n  );\n  return combine(results);\n}\n```\n\n## 4. Work-Stealing Queue\n\nLarge batches (1000+ tasks) use shared queue. Multiple workers pull tasks, execute independently. Implements load balancing naturally.\n\n**Gotcha**: No built-in priority or retry mechanism—implement separately if needed.\n\n```typescript\n// Work-stealing queue\nclass WorkQueue {\n  private queue: Task[] = [];\n  private workers: Worker[] = [];\n\n  async steal(): Promise<Task | null> {\n    return this.queue.shift() || null;\n  }\n\n  async runWorkers(count: number) {\n    this.workers = Array(count).fill(null).map(() =>\n      this.workerLoop()\n    );\n    await Promise.all(this.workers);\n  }\n\n  private async workerLoop() {\n    while (true) {\n      const task = await this.steal();\n      if (!task) break;\n      await this.execute(task);\n    }\n  }\n}\n```\n\n## 5. Map-Reduce\n\nMap phase uses cheap models (Haiku, GPT-4o-mini) for simple per-item analysis. Reduce phase uses smart models (Sonnet, GPT-4) for synthesis.\n\n**Cost example:**\n```\n100 files at $0.01 per map + $0.15 per reduce = $1.15\nvs $15 using all smart models\n```\n\n```typescript\n// Map-reduce with model selection\nasync function mapReduce(items: Item[]) {\n  // Map: cheap models\n  const mapResults = await Promise.all(\n    items.map(item =>\n      spawnAgent('mapper', { model: 'haiku' }).run(item)\n    )\n  );\n\n  // Reduce: smart model\n  const reducer = await spawnAgent('reducer', { model: 'sonnet' });\n  return reducer.run({ results: mapResults });\n}\n```\n\n## 6. Peer Collaboration (LLM Council)\n\nMultiple models provide independent responses, review others anonymously, then synthesize results.\n\n**Trade-offs:**\n- Expensive (3N+1 API calls)\n- Slow (15-30 seconds)\n- But reduces bias significantly\n\n**Not hierarchical agent relationships**—peers are equal.\n\n```typescript\n// LLM Council pattern\nasync function council(question: string, models: string[]) {\n  // Round 1: Independent responses\n  const responses = await Promise.all(\n    models.map(m => getResponse(m, question))\n  );\n\n  // Round 2: Anonymous peer review\n  const reviews = await Promise.all(\n    models.map((m, i) =>\n      reviewOthers(m, responses.filter((_, j) => j !== i))\n    )\n  );\n\n  // Round 3: Synthesis\n  return synthesize(responses, reviews);\n}\n```\n\n## 7. MAKER (Million-Agent Voting for Zero Errors)\n\nSee `references/maker-pattern.md` for detailed implementation.\n\n**Overview**: Combines extreme decomposition, microagents, and multi-agent voting to solve tasks requiring 100K+ steps with zero errors.\n\n**When to Use:**\n- Tasks requiring >100,000 LLM steps\n- Zero error tolerance (medical, financial, legal domains)\n- Subtasks are independently verifiable with deterministic checks\n- Cost is secondary to correctness\n",
        "building-multiagent-systems/skills/references/four-layer-architecture.md": "# Four-Layer Architecture for Multi-Agent Systems\n\nEvery agent in a multi-agent system should follow the four-layer architecture for testability, safety, and modularity.\n\n## The Stack\n\n```\n┌─────────────────────────────────────────┐\n│  1. Reasoning Layer (LLM)               │  Plans, critiques, decides which tools to call\n├─────────────────────────────────────────┤\n│  2. Orchestration Layer                 │  Validates, routes, enforces policy, spawns sub-agents\n├─────────────────────────────────────────┤\n│  3. Tool Bus                            │  Schema validation, tool execution coordination\n├─────────────────────────────────────────┤\n│  4. Deterministic Adapters              │  File I/O, APIs, shell commands, database access\n└─────────────────────────────────────────┘\n```\n\n## Why This Matters for Multi-Agent Systems\n\n- Each sub-agent has the same four layers - consistency across the system\n- Orchestration layer (Layer 2) is where you spawn sub-agents and coordinate their work\n- Tools (Layer 4) must be deterministic - no LLM calls inside tool implementations\n- Clear separation makes debugging multi-agent interactions tractable\n\n## Orchestration Layer Choices\n\n- **YOLO Mode** (like pi-mono): Minimal validation, fast iteration, trust LLM decisions\n- **Safety-First** (like Claude Code): User approval, policy enforcement, guardrails\n- **Hybrid**: Safety for dangerous operations, YOLO for safe ones\n\nChoose based on trust level and production environment. Multi-agent systems often use hybrid: parent agent has safety layers, sub-agents run in YOLO mode within controlled scopes.\n\n## The Deterministic Boundary\n\n**Critical Rule**: Everything below the Reasoning Layer must be deterministic.\n\n```typescript\n// ❌ WRONG: LLM call in a tool (breaks determinism)\nasync function analyzeTool(code: string) {\n  const analysis = await llm.generate(`Analyze this code: ${code}`);\n  return analysis;\n}\n\n// ✅ RIGHT: LLM in orchestration, tools are deterministic\nasync function analyzeCode(code: string) {\n  // Layer 2: Orchestration decides to spawn analyzer sub-agent\n  const analyzer = await spawnAgent('code-analyzer', { model: 'haiku' });\n\n  // Layer 3: Tool bus validates and routes\n  const result = await analyzer.executeTool('parse_ast', { code });\n\n  // Layer 4: parse_ast is deterministic (no LLM)\n  return result;\n}\n```\n\n**Why this matters**: Deterministic tools are testable with unit tests. Non-deterministic tools (with LLM calls) can only be integration-tested and add unpredictability to multi-agent coordination.\n\n## Schema-First Tool Design\n\nEvery tool must have a typed schema defined before implementation. This is critical in multi-agent systems where sub-agents need to discover and use tools dynamically.\n\n```typescript\n// Define schema FIRST\nconst editSchema = {\n  name: \"edit\",\n  description: \"Edit a file by replacing exact text\",\n  parameters: {\n    type: \"object\",\n    properties: {\n      path: { type: \"string\", description: \"Path to file\" },\n      oldText: { type: \"string\", description: \"Text to replace\" },\n      newText: { type: \"string\", description: \"Replacement text\" }\n    },\n    required: [\"path\", \"oldText\", \"newText\"]\n  }\n};\n\n// Then implement\nasync function editTool(params: SchemaType<typeof editSchema>) {\n  // TypeScript/validation ensures params match schema\n  const { path, oldText, newText } = params;\n  // ... deterministic implementation ...\n}\n```\n\n**Benefits for multi-agent systems**:\n- Sub-agents can discover available tools via schema inspection\n- Parent agents can validate sub-agent tool calls before execution\n- Schema serves as contract between agents\n- LLMs learn tool usage from schema descriptions\n",
        "building-multiagent-systems/skills/references/maker-pattern.md": "# MAKER Pattern (Million-Agent Voting for Zero Errors)\n\n## Overview\n\nCombines extreme decomposition, microagents, and multi-agent voting to solve tasks requiring 100K+ steps with zero errors. Based on research showing that architectural patterns can overcome LLM error rates through massively decomposed agentic processes (MDAPs).\n\n## Core Components\n\n1. **Extreme Decomposition** - Recursive breakdown until each subtask requires <100 LLM steps\n2. **Microagents** - Specialized sub-agents with single tools, focused expertise, cheap models\n3. **Multi-Agent Voting** - N parallel attempts per subtask, consensus via majority voting\n4. **Error Correction** - Deterministic validation + retry with context from failures\n\n## Implementation\n\n```typescript\n// MAKER-style microagent: One tool, one purpose\nconst parseMicroagent = {\n  name: 'parser-microagent',\n  model: 'haiku',  // Cheap model for focused task\n  tools: [parseASTTool],  // ONLY ONE TOOL\n  permissions: ['file:read'],\n  timeout: 30000\n};\n\n// Voting mechanism (fan-out/fan-in)\nasync function votingExecution(subtask: string, N: number = 5) {\n  // Fan-out: Spawn N identical microagents\n  const agents = await Promise.all(\n    Array(N).fill(null).map((_, i) =>\n      orchestrator.spawnSubAgent({\n        ...parseMicroagent,\n        name: `parser-vote-${i}`\n      })\n    )\n  );\n\n  // Execute in parallel\n  const results = await Promise.all(\n    agents.map(agent => agent.run(subtask))\n  );\n\n  // Fan-in: Majority vote\n  const consensus = majorityVote(results, 0.6);  // 60% threshold\n\n  // Cleanup\n  await Promise.all(agents.map(a => a.stop()));\n\n  return consensus;\n}\n\n// Extreme decomposition (recursive delegation)\nasync function solveMillionStepTask(task: ComplexTask, depth: number = 0) {\n  // Decompose into subtasks\n  const subtasks = await decomposeTask(task);\n\n  if (subtasks.length === 0 || depth > 10) {\n    // Base case: Atomic task, use voting\n    return await votingExecution(task.description, 5);\n  }\n\n  // Recursive case: Solve each subtask\n  const results = [];\n  for (const subtask of subtasks) {\n    const result = await solveMillionStepTask(subtask, depth + 1);\n\n    // Error correction: Validate result\n    const isValid = await validateResult(result, subtask.constraints);\n    if (!isValid) {\n      // Retry with more agents for higher confidence\n      result = await votingExecution(\n        subtask.description + `\\nPrevious failed: ${result}`,\n        7  // More agents = lower error rate\n      );\n    }\n\n    results.push(result);\n    await saveCheckpoint({ subtask, result });\n  }\n\n  return combineResults(results);\n}\n```\n\n## When to Use MAKER\n\n- Tasks requiring >100,000 LLM steps\n- Zero error tolerance (medical, financial, legal domains)\n- Subtasks are independently verifiable with deterministic checks\n- Cost is secondary to correctness\n- Tasks decompose naturally into hierarchical subtasks\n\n## Trade-offs\n\n| Pros | Cons |\n|------|------|\n| ✅ Zero errors - Voting exponentially reduces error rates (5 agents with 60% consensus → <0.001% error) | ❌ Cost - N× cost per subtask (5 agents = 5× base cost) |\n| ✅ Scalable - Proven to 1M+ steps | ❌ Speed - Slower than single-pass (parallel voting has overhead) |\n| ✅ Modular - Each subtask isolated, failures don't cascade | ❌ Complexity - Requires sophisticated orchestration |\n\n## Cost Comparison\n\n```text\nTraditional approach:\n1000 steps × $0.01 = $10\nError rate: 1% = 10 errors\n\nMAKER approach:\n1000 steps ÷ 100 subtasks × 5 voting agents × $0.002 = $10\nError rate: ~0% (voting consensus)\n\nResult: Same cost, zero errors vs. 10 errors\n```\n\n## Real-World Example: Medical Diagnosis System\n\nA diagnostic orchestrator processes patient data through 1000+ validation steps with zero error tolerance:\n\n1. **Symptom parsing** (100 microagents voting)\n2. **Lab result analysis** (50 microagents voting)\n3. **Historical pattern matching** (200 microagents voting)\n4. **Differential diagnosis generation** (500 microagents voting)\n5. **Treatment recommendation** (150 microagents voting)\n\nEach microagent specializes in one task (e.g., \"parse blood pressure reading\" or \"detect drug interaction\"). Voting consensus requires 80% agreement. Deterministic validation checks medical constraints (value ranges, drug interactions, contraindications).\n\n**Architecture:**\n```text\nDiagnostic Orchestrator (Sonnet, Layer 1-4)\n├─→ [Symptom Parsing] 100 microagents vote → consensus\n├─→ [Lab Analysis] 50 microagents vote → consensus\n├─→ [Pattern Matching] 200 microagents vote → consensus\n├─→ [Differential Diagnosis] 500 microagents vote → consensus\n└─→ [Treatment Recommendation] 150 microagents vote → consensus\n\nTotal: 1000 subtasks × 5 avg voting agents = 5000 agent executions\nError rate: <0.001% (voting + deterministic validation)\nCost: $50 (acceptable for medical domain)\n```\n\n## Key Insight\n\nMAKER shows that combining existing patterns (fan-out/fan-in + recursive delegation + deterministic validation) in a specific way achieves unprecedented reliability. The four-layer architecture enables this:\n- Layer 1 (reasoning) does decomposition\n- Layer 2 (orchestration) spawns voting pools\n- Layer 3 (tool bus) validates schemas\n- Layer 4 (adapters) provides deterministic validation\n",
        "building-multiagent-systems/skills/references/production-hardening.md": "# Production Hardening for Multi-Agent Systems\n\n## Cascading Stop Pattern\n\n\"Always stop children before stopping self.\" This prevents orphaned agents consuming resources.\n\n```\n1. Get all child agents\n2. Stop all children in parallel\n3. Stop self\n4. Cancel ongoing work\n5. Flush events\n```\n\n```typescript\nasync function cascadingStop(agent: Agent): Promise<void> {\n  // 1. Get all child agents\n  const children = await agent.getChildren();\n\n  // 2. Stop all children in parallel\n  await Promise.all(\n    children.map(child => cascadingStop(child))\n  );\n\n  // 3. Stop self\n  agent.setState('stopping');\n\n  // 4. Cancel ongoing work\n  agent.cancelPendingOperations();\n\n  // 5. Flush events\n  await agent.flushEventLog();\n\n  agent.setState('stopped');\n}\n```\n\n## Orphan Detection\n\nPeriodically scan for agents whose parents stopped. Clean them up automatically.\n\n```typescript\n// Heartbeat pattern\nsetInterval(async () => {\n  const allAgents = await getRunningAgents();\n  for (const agent of allAgents) {\n    const parentAlive = await checkParentHeartbeat(agent.parentId);\n    if (!parentAlive) {\n      await agent.stop();\n      await cleanupResources(agent.id);\n    }\n  }\n}, 30000);  // Check every 30 seconds\n```\n\n## Cost Tracking Across Agent Hierarchy\n\nAggregate costs from all descendant agents using hierarchical IDs.\n\n```typescript\n// Hierarchical cost tracking\nasync function getTotalCost(agentId: string): Promise<number> {\n  const directCost = await getAgentCost(agentId);\n  const children = await getChildAgents(agentId);\n  const childCosts = await Promise.all(\n    children.map(child => getTotalCost(child.id))\n  );\n  return directCost + childCosts.reduce((sum, cost) => sum + cost, 0);\n}\n\n// Cost tracking in agent events\ninterface CostEvent {\n  agentId: string;\n  parentId: string;\n  model: string;\n  inputTokens: number;\n  outputTokens: number;\n  cost: number;\n  timestamp: number;\n}\n\n// Aggregate costs by hierarchy\nfunction aggregateCostsByHierarchy(events: CostEvent[]): Map<string, number> {\n  const costs = new Map<string, number>();\n\n  for (const event of events) {\n    // Add to this agent\n    costs.set(event.agentId, (costs.get(event.agentId) || 0) + event.cost);\n\n    // Add to all ancestors (using hierarchical ID)\n    let parentId = event.parentId;\n    while (parentId) {\n      costs.set(parentId, (costs.get(parentId) || 0) + event.cost);\n      parentId = getParentFromId(parentId);\n    }\n  }\n\n  return costs;\n}\n```\n\n## Session vs Project Scope Workaround\n\nCreate project-level task store so agents can discover and claim work across sessions.\n\n```typescript\n// Persistent task store\nclass ProjectTaskStore {\n  constructor(private dbPath: string) {}\n\n  async addTask(task: Task): Promise<string> {\n    const id = generateId();\n    await this.db.insert({ ...task, id, status: 'pending', claimedBy: null });\n    return id;\n  }\n\n  async claimTask(agentId: string): Promise<Task | null> {\n    // Atomic claim to prevent race conditions\n    const task = await this.db.findOneAndUpdate(\n      { status: 'pending', claimedBy: null },\n      { status: 'claimed', claimedBy: agentId, claimedAt: Date.now() }\n    );\n    return task;\n  }\n\n  async completeTask(taskId: string, result: any): Promise<void> {\n    await this.db.update(\n      { id: taskId },\n      { status: 'completed', result, completedAt: Date.now() }\n    );\n  }\n\n  async releaseStale(timeout: number = 300000): Promise<void> {\n    // Release tasks claimed >5 min ago but not completed\n    await this.db.updateMany(\n      { status: 'claimed', claimedAt: { $lt: Date.now() - timeout } },\n      { status: 'pending', claimedBy: null }\n    );\n  }\n}\n```\n\n## Self-Modification Safety Protocol\n\nWhen sub-agents can modify code, follow this safety protocol:\n\n**Safety Protocol Checklist:**\n1. **Assess blast radius** - What can break if this modification fails?\n2. **Git branch isolation** - Each modification on separate branch\n3. **Test-first** - Write tests before implementing changes\n4. **Validation gates** - All tests must pass before commit\n5. **Rollback capability** - Can undo if validation fails\n\n```typescript\n// Sub-agent fixing its own bug\nasync function fixOwnBug(bugReport: string) {\n  // 1. Assess blast radius\n  const affectedFiles = await identifyBugLocation(bugReport);\n  const risk = assessRisk(affectedFiles);  // Low/Medium/High/Critical\n\n  if (risk === 'Critical') {\n    throw new Error('Cannot self-modify critical systems');\n  }\n\n  // 2. Create isolation\n  await gitBranch(`fix-bug-${Date.now()}`);\n\n  // 3. Write test for bug FIRST\n  await writeTest({\n    description: 'Test that reproduces the bug',\n    shouldFail: true  // Test should fail initially\n  });\n\n  // 4. Implement fix\n  await applyFix(affectedFiles);\n\n  // 5. Validate\n  const testsPass = await runAllTests();\n  const typeCheckPass = await runTypeCheck();\n  const lintPass = await runLint();\n\n  if (testsPass && typeCheckPass && lintPass) {\n    // 6. Commit\n    await gitCommit('fix: resolve bug in tool execution');\n  } else {\n    // 6. Rollback\n    await gitReset('--hard', 'main');\n    throw new Error('Fix validation failed - rolled back');\n  }\n}\n```\n\n**When sub-agents should NOT self-modify:**\n- During active operation (modifying code while running it)\n- Without parent orchestrator approval\n- Core reasoning logic (Layer 1) changes\n- When blast radius is Critical\n\n**When self-modification is appropriate:**\n- Fixing bugs in tool implementations (Layer 4)\n- Adding logging/debugging\n- Optimizing performance with tests\n- Adding new tools that follow existing patterns\n\n## Checkpointing Strategy\n\nSave after significant events to enable recovery.\n\n```typescript\ninterface CheckpointTrigger {\n  toolsExecuted: number;     // e.g., after 10+ tools\n  costThreshold: number;     // e.g., after $1.00 spent\n  timeElapsed: number;       // e.g., after 5 minutes\n}\n\nasync function maybeCheckpoint(agent: Agent, triggers: CheckpointTrigger) {\n  const metrics = agent.getMetrics();\n\n  if (\n    metrics.toolsExecuted >= triggers.toolsExecuted ||\n    metrics.totalCost >= triggers.costThreshold ||\n    metrics.elapsedMs >= triggers.timeElapsed\n  ) {\n    await saveCheckpoint({\n      agentId: agent.id,\n      state: agent.getState(),\n      messages: agent.getMessages(),\n      context: agent.getContext(),\n      timestamp: Date.now()\n    });\n    agent.resetMetrics();\n  }\n}\n```\n\n## Coordination Primitives Workaround\n\nImplement locks, semaphores, and barriers in-memory if unavailable in your framework.\n\n```typescript\n// Barrier: Wait for N agents to reach a point\nclass Barrier {\n  private count = 0;\n  private waiters: (() => void)[] = [];\n\n  constructor(private threshold: number) {}\n\n  async wait(): Promise<void> {\n    this.count++;\n    if (this.count >= this.threshold) {\n      // Release all waiters\n      this.waiters.forEach(resolve => resolve());\n      this.waiters = [];\n      this.count = 0;\n    } else {\n      // Wait for others\n      await new Promise<void>(resolve => this.waiters.push(resolve));\n    }\n  }\n}\n\n// Semaphore: Limit concurrent access\nclass Semaphore {\n  private permits: number;\n  private waiters: (() => void)[] = [];\n\n  constructor(permits: number) {\n    this.permits = permits;\n  }\n\n  async acquire(): Promise<void> {\n    if (this.permits > 0) {\n      this.permits--;\n    } else {\n      await new Promise<void>(resolve => this.waiters.push(resolve));\n    }\n  }\n\n  release(): void {\n    const waiter = this.waiters.shift();\n    if (waiter) {\n      waiter();\n    } else {\n      this.permits++;\n    }\n  }\n}\n```\n",
        "building-multiagent-systems/skills/references/tool-coordination.md": "# Tool Coordination in Multi-Agent Systems\n\n## Permission Inheritance\n\nChildren inherit parent tools and permission scopes but cannot escalate privileges.\n\n```typescript\n// Parent agent with full toolset\nconst parent = new Agent({\n  tools: [editTool, readTool, writeTool, bashTool, searchTool],\n  permissions: ['file:read', 'file:write', 'shell:execute']\n});\n\n// Spawn sub-agent with SUBSET of tools\nconst codeAnalyzer = await parent.spawnSubAgent({\n  name: 'code-analyzer',\n  model: 'haiku',  // Cheaper model for simple tasks\n  tools: [readTool, searchTool],  // Read-only tools\n  permissions: ['file:read'],  // Cannot write or execute\n  timeout: 120000  // 2 minute timeout\n});\n\n// Sub-agent CANNOT escalate privileges\n// Attempting to use writeTool → Error: Tool not available\n```\n\n**Permission Inheritance Rules:**\n1. Children inherit SUBSET of parent permissions (cannot escalate)\n2. Children can only use tools parent has access to\n3. Children cannot modify their own permission scope\n4. Parent can revoke child permissions at any time\n\n## Shared Resource Locking\n\nImplement acquire/release patterns to prevent race conditions when multiple agents access the same resource.\n\n```typescript\nclass ResourceLock {\n  private locks = new Map<string, string>();  // resource → agentId\n\n  async acquire(resource: string, agentId: string, timeout: number = 5000): Promise<boolean> {\n    const start = Date.now();\n    while (this.locks.has(resource)) {\n      if (Date.now() - start > timeout) return false;\n      await sleep(100);\n    }\n    this.locks.set(resource, agentId);\n    return true;\n  }\n\n  release(resource: string, agentId: string): void {\n    if (this.locks.get(resource) === agentId) {\n      this.locks.delete(resource);\n    }\n  }\n}\n```\n\n## Rate Limiting\n\nImplement token bucket algorithm shared across all agents to coordinate API call throttling.\n\n```typescript\nclass TokenBucket {\n  private tokens: number;\n  private lastRefill: number;\n\n  constructor(\n    private capacity: number,\n    private refillRate: number  // tokens per second\n  ) {\n    this.tokens = capacity;\n    this.lastRefill = Date.now();\n  }\n\n  async acquire(count: number = 1): Promise<boolean> {\n    this.refill();\n    if (this.tokens >= count) {\n      this.tokens -= count;\n      return true;\n    }\n    return false;\n  }\n\n  private refill(): void {\n    const now = Date.now();\n    const elapsed = (now - this.lastRefill) / 1000;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.refillRate);\n    this.lastRefill = now;\n  }\n}\n\n// Shared rate limiter for all agents\nconst apiRateLimiter = new TokenBucket(100, 10);  // 100 capacity, 10/sec refill\n```\n\n## Result Caching\n\nCache read-only, idempotent, expensive operations. Invalidate carefully for data freshness.\n\n```typescript\nclass ResultCache {\n  private cache = new Map<string, { result: any; timestamp: number }>();\n\n  async getOrCompute<T>(\n    key: string,\n    compute: () => Promise<T>,\n    ttlMs: number = 60000\n  ): Promise<T> {\n    const cached = this.cache.get(key);\n    if (cached && Date.now() - cached.timestamp < ttlMs) {\n      return cached.result as T;\n    }\n\n    const result = await compute();\n    this.cache.set(key, { result, timestamp: Date.now() });\n    return result;\n  }\n\n  invalidate(pattern: string): void {\n    for (const key of this.cache.keys()) {\n      if (key.includes(pattern)) {\n        this.cache.delete(key);\n      }\n    }\n  }\n}\n```\n\n## Sub-Agent as Tool Pattern\n\nTreat specialized agents as tools that parent can call.\n\n```typescript\n// Define sub-agent as a tool\nconst codeReviewerTool = {\n  name: \"code_reviewer\",\n  description: \"Reviews code for security, performance, and style issues\",\n  parameters: {\n    type: \"object\",\n    properties: {\n      filePath: { type: \"string\" },\n      focusAreas: { type: \"array\", items: { type: \"string\" } }\n    }\n  },\n  execute: async ({ filePath, focusAreas }) => {\n    // Spawn specialized agent\n    const reviewer = await spawnAgent('code-reviewer', {\n      model: 'sonnet',\n      tools: [readTool, searchTool]\n    });\n\n    // Agent performs review\n    const result = await reviewer.run(\n      `Review ${filePath} focusing on: ${focusAreas.join(', ')}`\n    );\n\n    // Clean up\n    await reviewer.stop();\n\n    return { review: result };\n  }\n};\n\n// Parent can now use code reviewer like any other tool\nconst review = await parent.executeTool('code_reviewer', {\n  filePath: 'src/auth.ts',\n  focusAreas: ['security', 'input-validation']\n});\n```\n\n**Benefits:**\n- Composable abstractions (agents using agents)\n- Consistent tool interface across system\n- Natural lifecycle management (spawn → execute → cleanup)\n",
        "ceo-personal-os/.claude-plugin/plugin.json": "{\n  \"name\": \"ceo-personal-os\",\n  \"description\": \"Personal operating system for executives - reflection frameworks, goal systems, and coaching-style reviews (Gustin, Ferriss, Robbins, Lieberman)\",\n  \"version\": \"1.0.0\"\n}\n",
        "ceo-personal-os/README.md": "# CEO Personal OS Plugin\n\nPersonal operating system for executives - reflection frameworks, goal systems, and coaching-style reviews. Build a reflection system, not a task manager.\n\n## Installation\n\n```bash\n/plugin install ceo-personal-os@2389-research\n```\n\n## What This Plugin Provides\n\n### Skills\n\n- **ceo-personal-os** - Complete methodology for building an executive reflection system with specific frameworks and review cadences\n\n### Frameworks Included\n\n| Framework | Credit | Purpose |\n|-----------|--------|---------|\n| Annual Review | Dr. Anthony Gustin | Comprehensive year reflection |\n| Ideal Lifestyle Costing | Tim Ferriss | Design life by cost |\n| Vivid Vision | Tony Robbins | 3-year future visualization |\n| Life Map | Alex Lieberman | 6-domain life balance |\n| Coaching Principles | Bill Campbell | People-first leadership & team development |\n| Startup Failure Patterns | Tom Eisenmann | Detect failure patterns before they kill your venture |\n| Good to Great | Jim Collins | Level 5 Leadership, Hedgehog Concept, Flywheel |\n| Buy Back Your Time | Dan Martell | Reclaim time through delegation, Production Quadrant |\n| E-Myth | Michael Gerber | Work ON your business not IN it, build systems |\n| Customer Development | Steve Blank | Search vs Execute, Get Out of the Building, MVP |\n\n## Quick Example\n\nStart with: \"Help me build a personal operating system\"\n\nThe skill generates a complete structure:\n```\nceo-personal-os/\n├── frameworks/          # 10 specific frameworks\n├── interviews/          # 10 coach-style question scripts\n├── reviews/             # Daily/weekly/quarterly/annual\n├── goals/               # 1/3/10 year horizons\n├── memory.md            # Pattern extraction\n└── principles.md        # Core operating principles\n```\n\n## Core Principle\n\n**Clarity over productivity theater.** No hustle culture. No corporate jargon. Output feels like an executive coach, chief of staff, and accountability partner - calm, direct, insightful.\n\n## Review Cadences\n\n| Cadence | Time | Focus |\n|---------|------|-------|\n| Daily | 5 min | Energy + one win + one friction |\n| Weekly | 30-45 min | What moved the needle? |\n| Quarterly | 2-3 hours | Goal alignment check |\n| Annual | 4-6 hours | Full Gustin-style reflection |\n\n## When This Skill Applies\n\n- \"I want a personal operating system\"\n- \"Help me with my annual review\"\n- Building goal-setting systems for executives\n- Life planning and reflection frameworks\n- Mentions of Gustin, Ferriss, Robbins, Lieberman, or Campbell frameworks\n- \"How do I develop my people?\" or team-first leadership questions\n- Mentions of \"Trillion Dollar Coach\" or Bill Campbell\n- \"Why do startups fail?\" or Eisenmann startup failure patterns\n- Evaluating venture health or deciding when to pivot/quit\n- Mentions of \"Good to Great\", Level 5 Leadership, or Hedgehog Concept\n- \"How do I buy back my time?\" or Dan Martell frameworks\n- \"I'm stuck working IN instead of ON my business\" or E-Myth concepts\n- Stuck doing technical work instead of strategic leadership\n- Mentions of Customer Development, Steve Blank, or \"get out of the building\"\n- Questions about MVP, pivot decisions, or product-market fit\n- \"Are we searching or executing?\" or early-stage startup strategy\n\n## Links\n\n- [Plugin CLAUDE.md](./CLAUDE.md) - Development instructions\n",
        "ceo-personal-os/skills/SKILL.md": "---\nname: ceo-personal-os\ndescription: This skill should be used when building a personal productivity or operating system for a CEO, founder, or executive. Triggers on \"personal OS\", \"annual review\", \"life planning\", \"goal setting system\", \"Bill Campbell\", \"Trillion Dollar Coach\", \"startup failure patterns\", \"Good to Great\", \"Level 5 Leadership\", \"Buy Back Your Time\", \"E-Myth\", \"Customer Development\", \"Steve Blank\", \"Small Is Beautiful\", \"Schumacher\", \"human-scale\", \"subsidiarity\", \"Buddhist economics\", \"permanence\".\n---\n\n# CEO Personal Operating System\n\n## Overview\n\nBuild a **reflection system**, not a task manager. This is a private, single-user operating system for executives that combines thoughtful frameworks with coaching-style prompts.\n\n**Core principle:** Clarity over productivity theater. No hustle culture. No corporate jargon.\n\n**Tone:** Calm, executive-level, direct, insightful. Like an executive coach, chief of staff, and accountability partner.\n\n## When This Skill Applies\n\n- User wants a \"personal operating system\" or \"personal productivity system\"\n- Building annual review or goal-setting system for an executive\n- Creating reflection frameworks for a CEO/founder\n- User mentions any of the 11 frameworks (Campbell, Collins, Blank, Martell, Gerber, Eisenmann, Schumacher, etc.)\n- User mentions search vs execute mode, pivot decisions, working ON vs IN business\n\n## First Run: Onboarding Flow\n\n### If No Directory Exists\n\nSay:\n> \"I'll help you build your personal operating system. This is a reflection system - think executive coach, not task manager.\n>\n> It includes 11 frameworks from people like Bill Campbell, Jim Collins, Steve Blank, Dan Martell, and E.F. Schumacher, plus 11 coaching-style interview scripts.\n>\n> **Ready to set it up?**\"\n\nThen use TodoWrite to track building the full structure.\n\n### If Directory Exists\n\nSay:\n> \"Welcome back. What would you like to do?\"\n> 1. Run a review (daily, weekly, quarterly, annual)\n> 2. Run an interview (stress patterns, time audit, failure detection, etc.)\n> 3. Update goals (1-year, 3-year, 10-year)\n> 4. Explore a framework\n> 5. Extract patterns from past reviews\n\n## Required Structure\n\nCreate in `ceo-personal-os/`:\n\n```\nceo-personal-os/\n├── README.md                    # How to use (personalize in 15 min)\n├── principles.md                # User's core operating principles\n├── memory.md                    # Extracted patterns & insights\n├── frameworks/                  # 10 framework files\n├── interviews/                  # 10 interview script files\n├── reviews/\n│   ├── daily/template.md\n│   ├── weekly/template.md\n│   ├── quarterly/template.md\n│   └── annual/template.md\n├── goals/\n│   ├── 1_year.md\n│   ├── 3_year.md\n│   └── 10_year.md\n└── uploads/                     # Past reviews for analysis\n```\n\n## The 11 Frameworks\n\nBuild each from the reference files in `references/frameworks/`:\n\n| Framework | Reference | Focus |\n|-----------|-----------|-------|\n| Gustin Annual Review | `gustin-annual-review.md` | Year-end reflection |\n| Ferriss Lifestyle Costing | `ferriss-lifestyle-costing.md` | TMI calculation |\n| Robbins Vivid Vision | `robbins-vivid-vision.md` | 3-year visualization |\n| Lieberman Life Map | `lieberman-life-map.md` | 6-domain assessment |\n| Campbell Coaching | `campbell-coaching.md` | People-first leadership |\n| Eisenmann Failure Patterns | `eisenmann-failure-patterns.md` | Venture health |\n| Collins Good to Great | `collins-good-to-great.md` | Level 5 Leadership |\n| Martell Buy Back Your Time | `martell-buyback-time.md` | Time reclamation |\n| Gerber E-Myth | `gerber-emyth.md` | ON not IN business |\n| Blank Customer Development | `blank-customer-development.md` | Search vs execute |\n| Schumacher Human-Scale | `schumacher-human-scale.md` | Purpose, permanence, scale |\n\n## Interview Scripts\n\nUse coach-style questions from `references/interviews/interview-scripts.md`:\n- Past Year Reflection\n- Identity & Values\n- Future Self Interview\n- Team & People Reflection (Campbell-style)\n- Failure Pattern Detection (Eisenmann-style)\n- Time Audit (Martell-style)\n- Business Role Assessment (Gerber-style)\n- Good to Great Assessment (Collins-style)\n- Customer Development Check (Blank-style)\n- Human-Scale Economics (Schumacher-style)\n- Leadership Stress Patterns (McWilliams-informed)\n\n## Review Cadences\n\n| Cadence | Time | Focus |\n|---------|------|-------|\n| **Daily** | 5 min max | Energy (1-10), one win, one friction, priority for tomorrow |\n| **Weekly** | 30-45 min | What moved needle vs. noise, time leaks |\n| **Quarterly** | 2-3 hours | Goal progress, all framework checks |\n| **Annual** | 4-6 hours | Full Gustin reflection, Life Map, Vivid Vision |\n\n### Quarterly Review Checks\n\nInclude in every quarterly review:\n- Campbell: \"Who have I developed?\"\n- Eisenmann: \"Which failure patterns am I vulnerable to?\"\n- Collins: \"Is the flywheel building momentum?\"\n- Martell: \"Am I in my Production Quadrant?\"\n- Gerber: \"Am I working ON or IN the business?\"\n- Blank: \"Am I searching or executing?\"\n- Schumacher: \"Am I building for permanence or extracting?\"\n- McWilliams: \"What stress patterns showed up?\"\n\n## Memory & Pattern Extraction\n\nWhen user uploads past reviews to `uploads/`:\n1. Summarize the document\n2. Extract patterns (goals, failures, strengths, themes)\n3. Append insights to `memory.md`\n4. Reference in future reviews\n\n## Tone Requirements\n\n**DO:** Calm, executive-level, direct, clear, insightful questions, psychologically safe\n\n**DON'T:** Hustle culture (\"crush it\"), therapy speak (\"holding space\"), corporate jargon (\"synergize\"), productivity porn (\"10x your output\")\n\n## Red Flags - STOP\n\nIf you catch yourself:\n- Using generic frameworks (Eisenhower, 80/20) instead of specified ones\n- Creating a task manager instead of a reflection system\n- Skipping memory.md or uploads structure\n- Using hustle culture language\n\n**Re-read this skill. Follow the specifications.**\n\n## Framework References\n\nAll detailed framework content is in:\n- `references/frameworks/` - 10 framework files with full content\n- `references/interviews/interview-scripts.md` - All interview questions\n",
        "ceo-personal-os/skills/references/frameworks/blank-customer-development.md": "# Steve Blank Customer Development\n\nCredit: \"The Startup Owner's Manual\" by Steve Blank & Bob Dorf\n\n## Core Insight\n\n> \"A startup is a temporary organization designed to search for a repeatable and scalable business model.\"\n\n**Startups ≠ Small Versions of Big Companies:**\n- Startups are SEARCHING for a business model\n- Existing companies are EXECUTING a known business model\n- Different rules apply to each\n\n## The Customer Development Model (4 Steps)\n\n| Step | Phase | Description |\n|------|-------|-------------|\n| **1. Customer Discovery** | Search | Turn hypotheses into facts by talking to customers |\n| **2. Customer Validation** | Search | Prove repeatable, scalable business model exists |\n| **3. Customer Creation** | Execute | Build end-user demand, scale acquisition |\n| **4. Company Building** | Execute | Transition from startup to company |\n\n## The #1 Rule: Get Out of the Building\n\n> \"Facts live outside the building, where future customers live and work.\"\n\nFounders must do this personally—cannot be delegated to employees or consultants.\n\n## Search vs Execute\n\n| Search Mode | Execute Mode |\n|-------------|--------------|\n| Hypotheses, testing, learning | Plans, forecasts, metrics |\n| Customer Development Team | Functional departments |\n| \"Learning and discovery\" culture | \"Fear of failure\" culture |\n| Pivot when hypotheses wrong | Fire people when plans fail |\n\n## Key Concepts\n\n| Concept | Definition |\n|---------|------------|\n| **Pivot** | Substantive change to one or more business model components |\n| **Iteration** | Minor adjustment to business model |\n| **MVP** | Minimum Viable Product - smallest feature set to learn from |\n| **Earlyvangelists** | Visionary customers who buy unfinished products |\n| **Product/Market Fit** | Business model matches customer segment needs |\n\n## The 9 Deadly Sins of Product Introduction\n\n1. Assuming \"I know what the customer wants\"\n2. The \"I know what features to build\" flaw\n3. Focus on launch date over customer learning\n4. Emphasis on execution instead of hypothesis testing\n5. Business plans presume no trial and no errors\n6. Confusing job titles with what a startup needs\n7. Sales and marketing execute to a plan (no iteration)\n8. Presumption of success leads to premature scaling\n9. Management by crisis leads to a death spiral\n\n## 4 Market Types\n\n| Market Type | Strategy |\n|-------------|----------|\n| **Existing Market** | Compete on features, faster/better/cheaper |\n| **New Market** | Create demand, educate customers |\n| **Re-segmented (Low-cost)** | \"Good enough\" at lower price |\n| **Re-segmented (Niche)** | Specialized for underserved segment |\n\n## Quarterly Review Questions\n\n- Are we searching or executing? (Be honest)\n- What hypotheses have we tested this quarter?\n- How much time did founders spend outside the building?\n- Have we achieved product/market fit?\n- What did we learn from failures?\n- Are we pivoting based on data or gut feelings?\n- What market type are we in?\n",
        "ceo-personal-os/skills/references/frameworks/campbell-coaching.md": "# Bill Campbell Coaching Principles\n\nCredit: \"Trillion Dollar Coach\" by Eric Schmidt, Jonathan Rosenberg, Alan Eagle\n\n## Core Principles\n\n| Principle | Description |\n|-----------|-------------|\n| **It's The People** | Your top priority is the well-being and success of your people. Support, respect, trust. |\n| **Work The Team, Then The Problem** | When facing any problem, first ensure the right team is in place. |\n| **Only Coach The Coachable** | Look for honesty, humility, perseverance, and openness to learning. |\n| **The Lovely Reset** | Start meetings with personal check-ins (weekend, family). Creates better decisions. |\n| **Free-Form Listening** | Full attention, Socratic questions, no distractions. |\n| **Team First** | Commit to the cause, not just personal success. |\n\n## Four Characteristics to Seek\n\n1. **Smart** - Ability to make \"far analogies\" across domains\n2. **Hard Work** - Willingness to put in the effort\n3. **Integrity** - Honest with others and themselves\n4. **Grit** - Passion and perseverance to get back up\n\n## The Campbell Yardstick\n\n> \"I count up how many people I've helped become great leaders. That's how I measure success.\"\n\n## Quarterly Review Questions\n\n- Who did I develop this quarter?\n- Who needs more support?\n- Is the right team in place for each challenge?\n- Am I creating psychological safety?\n\n## The \"What Next?\" Framework (Post-50)\n\n- Be creative - your most creative years are ahead\n- Don't be a dilettante - have accountability in everything\n- Find people with vitality - often younger\n- Apply your unique gifts\n- Don't waste time worrying about the future\n",
        "ceo-personal-os/skills/references/frameworks/collins-good-to-great.md": "# Jim Collins Good to Great\n\nCredit: \"Good to Great\" by Jim Collins\n\n## Level 5 Leadership\n\n| Level | Description |\n|-------|-------------|\n| **Level 5** | Executive who builds enduring greatness through personal humility and professional will |\n| **Level 4** | Effective Leader - catalyzes commitment to clear, compelling vision |\n| **Level 3** | Competent Manager - organizes people and resources efficiently |\n| **Level 2** | Contributing Team Member - contributes to group objectives |\n| **Level 1** | Highly Capable Individual - productive through talent and knowledge |\n\n**Level 5 Paradox:** Ambitious for the cause, not themselves. Ferociously determined yet humble.\n\n## First Who, Then What\n\n1. Get the right people on the bus\n2. Get the wrong people off the bus\n3. Get the right people in the right seats\n4. THEN figure out where to drive\n\n## The Hedgehog Concept (Three Circles)\n\nWhat lies at the intersection of:\n1. **Passion** - What are you deeply passionate about?\n2. **Best At** - What can you be the best in the world at?\n3. **Economic Engine** - What drives your resource engine?\n\n## Stockdale Paradox\n\n> \"You must maintain unwavering faith that you can and will prevail in the end, regardless of the difficulties, AND at the same time have the discipline to confront the most brutal facts of your current reality.\"\n\n## The Flywheel\n\n- No single breakthrough moment\n- Consistent pushing in an intelligent direction\n- Momentum builds turn by turn\n- Results attract resources → stronger organization → better results → more resources\n\n## Culture of Discipline\n\n- **Disciplined people** - Don't need hierarchy\n- **Disciplined thought** - Confront brutal facts\n- **Disciplined action** - Aligned to Hedgehog Concept\n\n## Quarterly Review Questions\n\n- Am I operating at Level 5 (humble + determined)?\n- Do I have the right people on the bus?\n- Am I clear on my Hedgehog Concept?\n- Am I confronting brutal facts while maintaining faith?\n- Is the flywheel building momentum?\n",
        "ceo-personal-os/skills/references/frameworks/eisenmann-failure-patterns.md": "# Eisenmann Startup Failure Patterns\n\nCredit: \"Why Startups Fail\" by Tom Eisenmann (Harvard Business School)\n\n## The Six Failure Patterns\n\n| Pattern | Stage | Description |\n|---------|-------|-------------|\n| **Bad Bedfellows** | Early | Wrong team, investors, or partners sink the venture despite a good idea |\n| **False Starts** | Early | \"Fail fast\" taken too far—launching before researching customer needs |\n| **False Positives** | Early | Early adopter success misleads about mainstream market demand |\n| **Speed Traps** | Late | Hypergrowth leads to disaster as CAC rises and margins erode |\n| **Help Wanted** | Late | Financing risk + management gaps during rapid scaling |\n| **Cascading Miracles** | Late | Too many \"do or die\" things must go right—betting on improbable outcomes |\n\n## Diamond-and-Square Framework\n\nAll eight elements must align:\n\n**Diamond (Opportunity):**\n- Customer Value Proposition - Strong, unmet need + sustainable differentiation\n- Technology & Operations - Can you build and deliver it?\n- Marketing - Can you reach customers cost-effectively?\n- Profit Formula - Unit economics, LTV/CAC > 3, path to break-even\n\n**Square (Resources):**\n- Founders - Right skills, experience, temperament\n- Team - Balance of attitude and skill\n- Investors - Aligned interests, patient capital, value-add beyond money\n- Partners - Reliable strategic relationships\n\n## Four Types of Entrepreneurial Risk\n\n1. **Demand Risk** - Will customers actually want this?\n2. **Technological Risk** - Can we build what we envision?\n3. **Execution Risk** - Can we attract and manage the right people?\n4. **Financing Risk** - Can we raise capital when we need it?\n\n## Key Metrics to Monitor\n\n| Metric | Target | Warning Sign |\n|--------|--------|--------------|\n| Unit Economics | Positive gross profit per unit | Losing money on every transaction |\n| LTV/CAC Ratio | > 3.0 | Below 1.0 = doomed |\n| Cash Runway | 12-18 months | Less than 6 months |\n| Customer Acquisition Cost | Stable or declining | Rising faster than LTV |\n\n## Running on Empty\n\nThe hardest decision: When to quit vs. persevere. Warning signs:\n- Burning capital investors will never recover\n- Team members investing time in doomed venture\n- Persisting past point where turnaround odds are minuscule\n- \"Hope springs eternal\" rationalization overriding data\n\n## Quarterly Review Questions\n\n- Which failure patterns might we be vulnerable to right now?\n- Is our Diamond-Square alignment strong?\n- What are our current risk levels across the four types?\n- Are we tracking toward our key metrics targets?\n- If we're struggling, is this a smart bet that hasn't paid off, or are we Running on Empty?\n",
        "ceo-personal-os/skills/references/frameworks/ferriss-lifestyle-costing.md": "# Tim Ferriss Ideal Lifestyle Costing\n\nCredit: [@tferruss](https://x.com/tferruss)\n\n## Core Concept\n\nDesign your ideal life first, then calculate what it costs. Work backward from lifestyle to income requirements.\n\n## Components\n\n### Monthly Lifestyle Cost Calculation\n- Current baseline expenses\n- Ideal lifestyle additions (travel, experiences, possessions)\n- Total monthly target\n\n### Dreamlines\nSpecific lifestyle goals with concrete costs:\n- What do you want to HAVE? (things)\n- What do you want to BE? (qualities, skills)\n- What do you want to DO? (experiences)\n\n### Target Monthly Income (TMI)\n```\nTMI = (Monthly Lifestyle Cost + Monthly Savings Target) × 1.3\n```\n(1.3 accounts for taxes and buffer)\n\n### Gap Analysis\n- Current income vs. TMI\n- Time available vs. time needed\n- Concrete steps to close the gap\n\n## Implementation Questions\n\n1. What does your ideal day look like?\n2. What does it cost per month?\n3. What's the gap between current and ideal?\n4. What's the fastest path to close that gap?\n",
        "ceo-personal-os/skills/references/frameworks/gerber-emyth.md": "# Michael Gerber E-Myth\n\nCredit: \"The E-Myth Revisited\" by Michael E. Gerber\n\n## The E-Myth (Entrepreneurial Myth)\n\n> \"The fatal assumption: If you understand the technical work of a business, you understand a business that does that technical work.\"\n\nThis is FALSE. A great baker does not necessarily make a great bakery owner.\n\n## Three Personas in Every Business Owner\n\n| Persona | Focus | Wants |\n|---------|-------|-------|\n| **The Entrepreneur** | Future vision, dreams | Change, innovation |\n| **The Manager** | Order, systems, control | Predictability, planning |\n| **The Technician** | Present work, doing | Getting things done |\n\nMost small business owners are 10% Entrepreneur, 20% Manager, 70% Technician.\n\n## The Entrepreneurial Seizure\n\nThe moment when a technician thinks: \"Why am I working for this person? I could do this myself!\"\n\nThen starts a business and immediately becomes enslaved by it—because they're still just a technician, now with 10 more jobs they don't know how to do.\n\n## Work ON Your Business, Not IN It\n\n| Working IN | Working ON |\n|------------|------------|\n| Doing the technical work | Building systems |\n| Reacting to problems | Designing processes |\n| Being indispensable | Creating replaceability |\n| Trading time for money | Building scalable value |\n\n## The Turn-Key Revolution (Franchise Prototype)\n\nBuild your business as if you were going to franchise it:\n- Systems run the business\n- People run the systems\n- Document everything\n- Make it work without you\n\n## Business Development Process\n\n1. **Innovation** - Continuous improvement\n2. **Quantification** - Measure everything important\n3. **Orchestration** - Eliminate discretion at operating level\n\n## The Three Phases of Business\n\n| Phase | Description | Problem |\n|-------|-------------|---------|\n| **Infancy** | Technician's phase, owner does everything | Burns out |\n| **Adolescence** | Gets help but abdicates management | Chaos when growth hits |\n| **Maturity** | Entrepreneurial perspective, systems-driven | Sustainable |\n\n## Quarterly Review Questions\n\n- Am I working ON or IN the business?\n- What's my Entrepreneur/Manager/Technician balance?\n- Could someone run this business from a manual?\n- What systems need to be built or documented?\n- Am I building a business or just creating a job?\n",
        "ceo-personal-os/skills/references/frameworks/gustin-annual-review.md": "# Dr. Anthony Gustin Annual Review\n\nCredit: [@dranthonygustin](https://x.com/dranthonygustin)\n\n## Reflection Categories\n\n- **Wins and celebrations** - What went right\n- **Lessons and failures** - What didn't work and why\n- **Relationships** - Who was nurtured vs. neglected\n- **Energy patterns** - What gave energy vs. drained it\n- **Growth areas identified** - Where to develop\n- **Unfinished business** - What got carried over\n- **Narrative synthesis** - The story of this year\n\n## Annual Review Prompts\n\nUse these for comprehensive year-end reflection:\n\n1. What were your top 5 wins this year?\n2. What were your top 3 failures and what did each teach you?\n3. Which relationships deepened? Which atrophied?\n4. When did you feel most alive? Most drained?\n5. What patterns kept repeating?\n6. What will you not carry forward?\n7. In one paragraph, tell the story of this year.\n",
        "ceo-personal-os/skills/references/frameworks/lieberman-life-map.md": "# Alex Lieberman Life Map\n\nCredit: [@businessbarista](https://x.com/businessbarista)\n\n## The 6 Domains\n\n| Domain | Focus Areas |\n|--------|-------------|\n| **Career** | Role, impact, growth, fulfillment |\n| **Relationships** | Family, friends, community, partnership |\n| **Health** | Physical, mental, energy, longevity |\n| **Meaning** | Purpose, contribution, legacy |\n| **Finances** | Security, freedom, generosity |\n| **Fun** | Play, adventure, creativity, rest |\n\n## Quarterly Assessment\n\nRate each domain 1-10:\n- 1-3: In crisis\n- 4-6: Needs attention\n- 7-8: Healthy\n- 9-10: Thriving\n\n## Imbalance Detection\n\nLook for:\n- Any domain below 5 (urgent attention needed)\n- Gap > 3 between highest and lowest (imbalance)\n- Domain stuck at same rating for 3+ quarters (stagnation)\n\n## Integration Questions\n\n1. Which domain, if improved, would lift the others?\n2. Which domain am I avoiding looking at?\n3. What would a \"10\" look like in my weakest domain?\n4. What trade-offs am I making between domains?\n",
        "ceo-personal-os/skills/references/frameworks/martell-buyback-time.md": "# Dan Martell Buy Back Your Time\n\nCredit: \"Buy Back Your Time\" by Dan Martell\n\n## The Buyback Principle\n\n> \"Don't hire to grow your business. Hire to buy back your time.\"\n\n## The Buyback Loop (Audit-Transfer-Fill)\n\n1. **Audit** - What tasks drain you that are easy/inexpensive to delegate?\n2. **Transfer** - Who can take these over (better suited, enjoys them)?\n3. **Fill** - What high-value work should you do instead?\n\nRepeat continuously to upgrade your time.\n\n## The Pain Line\n\nThe point where growth becomes impossible because:\n- More business growth = more pain\n- You'll subconsciously sabotage growth to avoid pain\n\n**Three Ways Entrepreneurs Self-Destruct:**\n\n| Response | Description |\n|----------|-------------|\n| **Sell** | Desperate to exit, sell on bad terms |\n| **Sabotage** | Subconsciously make bad decisions to stay small |\n| **Stall** | Consciously decide not to grow (slow death) |\n\n## The DRIP Matrix\n\n| | Low $ Value | High $ Value |\n|---|---|---|\n| **Draining** | Delegate | Replace |\n| **Energizing** | Invest | Production |\n\nFocus on the **Production Quadrant**: High value + Energizing work.\n\n## Buyback Rate\n\n- Calculate your effective hourly rate\n- Anything below that rate should be delegated\n- Example: If you make $200/hr effective, delegate anything done for $50/hr\n\n## The 5 Time Assassins\n\n1. **The Staller** (perfectionism)\n2. **The Speed Demon** (rushing without thinking)\n3. **The Supervisor** (micromanaging)\n4. **The Saver** (hoarding tasks)\n5. **The Self-Medicator** (vices to cope)\n\n## Perfect Week Design\n\n- Design your ideal week in advance\n- Block time for high-value activities\n- Protect creative/strategic time\n- Build in recovery\n\n## Weekly Review Questions\n\n- Am I doing $10/hr work when I'm worth $200/hr?\n- What can I audit-transfer-fill this week?\n- Am I hitting my Pain Line? What are the symptoms?\n- Am I in my Production Quadrant enough?\n- Which Time Assassin is attacking me most?\n",
        "ceo-personal-os/skills/references/frameworks/robbins-vivid-vision.md": "# Tony Robbins Vivid Vision\n\n## Core Concept\n\nWrite a detailed description of your life 3 years from now, in present tense, as if you're already living it.\n\n## Requirements\n\n- **Present tense** - \"I am...\" not \"I will...\"\n- **3 years out** - Close enough to feel real, far enough for significant change\n- **All life domains** - Career, relationships, health, finances, lifestyle\n- **Emotionally compelling** - Should excite you when you read it\n- **Specific details** - What you see, feel, experience daily\n\n## Structure\n\n### A Typical Day\nDescribe waking up, your morning, work, relationships, evening, how you feel throughout.\n\n### Life Domains\n- **Professional** - Role, impact, team, accomplishments\n- **Relationships** - Partner, family, friends, community\n- **Health** - Energy, fitness, mental state\n- **Financial** - Security, freedom, what money enables\n- **Lifestyle** - Where you live, how you spend time\n- **Contribution** - How you help others, legacy\n\n## Usage\n\n- Write it once, refine quarterly\n- Read it weekly or when making big decisions\n- Use it as a filter: \"Does this move me toward my vision?\"\n",
        "ceo-personal-os/skills/references/frameworks/schumacher-human-scale.md": "# E.F. Schumacher Human-Scale Economics\n\nCredit: \"Small Is Beautiful\" and \"Good Work\" by E.F. Schumacher; \"Towards a Politics of Hope\" by Frances Moore Lappé (5th E.F. Schumacher Lecture)\n\n## Core Thesis\n\nModern economics commits a fatal error: treating irreplaceable capital as if it were renewable income. This applies to natural resources, to nature's tolerance margins, and to human substance itself.\n\n> \"A businessman would never consider a firm viable if it rapidly consumed its capital. Yet we do exactly this with Spaceship Earth.\"\n\n## Three Purposes of Work\n\nWork is central to human existence. Every CEO should ask whether their organization enables all three:\n\n| Purpose | Description | Question |\n|---------|-------------|----------|\n| **Production** | Provide necessary and useful goods and services | Does our work produce something genuinely useful? |\n| **Perfection** | Enable people to use and develop their gifts like good stewards | Does our work allow people to grow their capabilities? |\n| **Liberation** | Serve and collaborate with others, liberating from egocentricity | Does our work foster connection and service? |\n\n> \"Without work, all life goes rotten, but when work is soulless, life stifles and dies.\" - Albert Camus\n\n## Income vs Capital Framework\n\n**The Fatal Error:** Treating irreplaceable resources as renewable income.\n\n| Category | Description | Consequence of Depletion |\n|----------|-------------|-------------------------|\n| **Fossil Fuels / Non-renewables** | Found, not made | Threatens civilization |\n| **Tolerance Margins of Nature** | Capacity to absorb and regenerate | Threatens life itself |\n| **Human Substance** | Initiative, imagination, creativity, moral character | Threatens meaning and purpose |\n\n**Application Questions:**\n- Are we treating this resource as income (renewable) or capital (depleting)?\n- What would change if we accounted for true costs?\n- What are we consuming that we cannot replace?\n\n## Human-Scale Technology Framework\n\nTechnology is not neutral. The choice of technology is a moral and political choice.\n\n**Criteria for Appropriate Technology:**\n\n| Criterion | Description |\n|-----------|-------------|\n| **Scale** | Small enough for human comprehension and control |\n| **Simplicity** | Simple enough for broad understanding and maintenance |\n| **Capital-saving** | Affordable widely, not just for the wealthy |\n| **Non-violent** | Gentle to nature and to human beings |\n| **Creative** | Allows workers to use intelligence and skill |\n\n**The Technology Question:** Can people enjoy themselves while working, or do they work solely for pay and hope for enjoyment in leisure?\n\n## Scale and Organization (Subsidiarity)\n\n**The Problem of Scale:**\n- Large organizations tend toward complexity, impersonality, and alienation\n- Small organizations often lack resources and specialization\n- The challenge: achieve the virtues of both\n\n**Principles for Large-Scale Organization:**\n\n1. **Subsidiarity** - Higher levels should not absorb functions that lower levels can perform\n2. **Identification** - People need to identify with an organization small enough to comprehend\n3. **Quasi-firms** - Large organizations should contain small, quasi-autonomous units\n\n**Key Tension:** Order vs Freedom. Neither pure centralization nor pure decentralization works.\n\n## Buddhist Economics Lens\n\nA middle way between materialist heedlessness and traditionalist immobility.\n\n| Western Economics | Buddhist Economics |\n|------------------|-------------------|\n| Maximize consumption | Maximize satisfaction with minimum consumption |\n| Labor is a cost to minimize | Right livelihood is a blessing to be enabled |\n| Goods over people | People over goods |\n| Bigger is better | Appropriate scale |\n| Endless growth | Sufficiency and permanence |\n\n## Meta-Economics Framework\n\n**Three Levels of Economic Thought:**\n\n| Level | Question | Domain |\n|-------|----------|--------|\n| **Meta-economics** | What are we trying to achieve? | Goals, values, ends |\n| **Economics** | How do we achieve it efficiently? | Means, allocation |\n| **Sub-economics** | Technical implementation | Methods, techniques |\n\n**Critical Insight:** Modern economics jumps to means without examining ends. We optimize systems without asking what they're optimizing for.\n\n## Key Principles\n\n### On Growth\n- \"Unlimited growth in a finite environment is impossible\"\n- The logic of non-renewables: using them up faster doesn't make you richer\n\n### On Production\n- We have NOT solved \"the problem of production\" - this is the fateful error\n- Modern man treats nature as something to conquer, not as something he is part of\n\n### On Permanence\n- Must design lifestyles, production methods, and consumption patterns for permanence\n- Short-term efficiency often destroys long-term viability\n- Peace requires assurance of permanence in economic life\n\n### On Scale\n- Gigantism: the disease of modern life\n- Need for \"technology with a human face\"\n- People need to identify with something small enough to comprehend\n\n## Quarterly Review Questions\n\n- **Purpose Check:** Does our work serve all three purposes (production, perfection, liberation)?\n- **Capital Check:** What are we treating as income that is actually capital?\n- **Scale Check:** Is our organization on a human scale? Can people identify with it?\n- **Technology Check:** Does our technology serve workers or dominate them?\n- **Subsidiarity Check:** Are we absorbing functions that lower levels could perform?\n- **Meta-economics Check:** What are we actually optimizing for? Have we examined our ends?\n- **Permanence Check:** Are we building for permanence or extracting for short-term gain?\n",
        "ceo-personal-os/skills/references/interviews/interview-scripts.md": "# CEO Personal OS Interview Scripts\n\nCoach-style questions (non-judgmental, reflective)\n\n## Past Year Reflection\n\n- \"Tell me about the last year -- highlights first.\"\n- \"What drained you the most?\"\n- \"Where did you avoid hard decisions?\"\n- \"What are you proud of that no one else sees?\"\n- \"What would you not repeat under any circumstances?\"\n- \"If this year repeated ten times, would you be satisfied?\"\n\n## Identity & Values\n\n- \"What do you believe that most people don't?\"\n- \"When do you feel most alive?\"\n- \"What would you do if you couldn't fail?\"\n- \"Who do you want to become?\"\n\n## Future Self Interview\n\n- \"It's 10 years from now. Describe your day.\"\n- \"What did you have to give up to get here?\"\n- \"What do you wish you had started sooner?\"\n- \"What advice would future-you give present-you?\"\n\n## Team & People Reflection (Campbell-style)\n\n- \"Who are you most proud of developing this year?\"\n- \"Who on your team needs more support right now?\"\n- \"Is the right team in place for your biggest challenge?\"\n- \"What keeps you up at night about your people?\"\n- \"Who have you helped become a great leader?\"\n- \"Are you creating psychological safety? How do you know?\"\n- \"Who do you need to have a hard conversation with?\"\n\n## Failure Pattern Detection (Eisenmann-style)\n\n- \"Which of the six failure patterns might you be vulnerable to right now?\"\n- \"Is your Diamond (opportunity) and Square (resources) aligned?\"\n- \"Are you chasing false positives from early adopters?\"\n- \"Are you growing faster than your unit economics support?\"\n- \"Do you have the right senior team for this stage of growth?\"\n- \"How many 'miracles' need to happen for your vision to succeed?\"\n- \"Are you persisting past the point of reasonable hope?\"\n- \"What would make you decide to pull the plug?\"\n- \"If you shut down today, would that be a smart bet that didn't pay off, or a failure to face reality?\"\n\n## Time Audit (Martell-style)\n\n- \"What did you spend your time on this week? List the top 10 activities.\"\n- \"For each activity: Is it draining or energizing? Low or high value?\"\n- \"What's your current Buyback Rate? What should it be?\"\n- \"What $10/hr tasks are you still doing?\"\n- \"What would you do with 10 extra hours per week?\"\n- \"Are you hitting your Pain Line? What are the symptoms?\"\n- \"What's the next thing you can audit-transfer-fill?\"\n- \"Are you in your Production Quadrant enough?\"\n- \"Which of the 5 Time Assassins is attacking you most?\"\n\n## Business Role Assessment (Gerber-style)\n\n- \"What percentage of your time is spent as Entrepreneur/Manager/Technician?\"\n- \"Are you working ON your business or IN it this week?\"\n- \"Could someone run your business from a manual right now?\"\n- \"What systems desperately need to be documented?\"\n- \"Are you building a business or just creating a job for yourself?\"\n- \"If you were hit by a bus, what would break immediately?\"\n- \"What technical work are you still doing that someone else should do?\"\n- \"Are you enslaved by your business or freed by it?\"\n- \"What would need to change for this business to run without you?\"\n\n## Good to Great Assessment (Collins-style)\n\n- \"Are you operating at Level 5 (humble yet fiercely determined)?\"\n- \"Do you have the right people on the bus? Wrong people off?\"\n- \"What is your Hedgehog Concept? (Passion + Best At + Economic Engine)\"\n- \"Are you confronting the brutal facts while maintaining faith?\"\n- \"Is your flywheel building momentum or stalling?\"\n- \"Where are you lacking disciplined people, thought, or action?\"\n- \"Are you 'telling time' or 'building a clock'?\"\n\n## Customer Development Check (Blank-style)\n\n- \"Are you in search mode or execute mode right now? Be honest.\"\n- \"When was the last time you talked to a customer outside the building?\"\n- \"What hypotheses have you tested this month? What did you learn?\"\n- \"What's your MVP? Is it truly minimal?\"\n- \"Have you found earlyvangelists? Are they paying or just being polite?\"\n- \"Have you achieved product/market fit? How do you know?\"\n- \"What market type are you in? Are you sure?\"\n- \"What would cause you to pivot? Have you defined the triggers?\"\n- \"Are you committing the 9 Deadly Sins? Which ones?\"\n- \"Is your burn rate appropriate for search mode or are you spending like you're executing?\"\n- \"What did you learn from your last failure?\"\n\n## Human-Scale Economics (Schumacher-style)\n\n*Adapted from \"Small Is Beautiful\" and \"Good Work\" by E.F. Schumacher*\n\n**Purpose:** Surface whether your work and organization serve human flourishing or extract from it.\n\n**Three Purposes of Work Check:**\n- \"Does your work produce something genuinely useful? Would the world miss it?\"\n- \"Does your work allow you and your team to develop your gifts and capabilities?\"\n- \"Does your work foster genuine connection and service, or isolation and extraction?\"\n- \"If your workers could choose any job tomorrow, would they choose yours? Why or why not?\"\n\n**Income vs Capital Check:**\n- \"What irreplaceable resources are you treating as if they're renewable?\"\n- \"What are you consuming that you cannot replace—in your business, your team, yourself?\"\n- \"If you accounted for true costs, would your business still make sense?\"\n- \"What 'capital' is being depleted: natural, human, relational, or institutional?\"\n\n**Scale and Subsidiarity Check:**\n- \"Is your organization on a human scale? Can people identify with it?\"\n- \"What functions have you absorbed that lower levels could perform?\"\n- \"Where has complexity exceeded human comprehension?\"\n- \"Do your people feel like cogs or like craftsmen?\"\n\n**Technology and Work Check:**\n- \"Does your technology serve workers or dominate them?\"\n- \"Can people enjoy themselves while working, or do they work solely for pay?\"\n- \"What 'mindless, repetitive boredom' exists in your organization?\"\n- \"Where are you 'destroying initiative and rotting brains'?\"\n\n**Meta-Economics Check:**\n- \"What are you actually optimizing for? Have you examined your ends?\"\n- \"Are you confusing means with ends anywhere?\"\n- \"Would your grandchildren be proud of what you're building?\"\n- \"Are you building for permanence or extracting for short-term gain?\"\n\n**Buddhist Economics Lens:**\n- \"Are you maximizing satisfaction with minimum consumption, or vice versa?\"\n- \"Where is 'bigger is better' thinking leading you astray?\"\n- \"What would 'sufficiency' look like for your organization?\"\n- \"Are you treating growth as a goal or as a means to something else?\"\n\n## Leadership Stress Patterns (McWilliams-informed)\n\n*Adapted from \"Psychoanalytic Diagnosis\" by Nancy McWilliams*\n\n**Purpose:** Surface unconscious patterns that emerge under pressure.\n\n**Denial Check:**\n- \"This quarter, what uncomfortable truth did I avoid looking at?\"\n- \"Where am I being optimistic in ways disconnected from evidence?\"\n- \"What data am I dismissing because it doesn't fit my narrative?\"\n\n**Control Check:**\n- \"Where did I micromanage instead of delegate?\"\n- \"Did I believe 'only I can do this right' about anything?\"\n- \"Am I trying to control outcomes that aren't controllable?\"\n\n**Idealization/Devaluation Check:**\n- \"Did I put anyone on a pedestal this quarter? Are they still there?\"\n- \"Who did I write off completely? Was that fair?\"\n- \"Am I having extreme swings in my judgment of people?\"\n\n**Projection Check:**\n- \"When I felt frustrated with someone, was I seeing my own issue in them?\"\n- \"Am I attributing motives to others that might actually be mine?\"\n- \"Where am I blaming the team for feelings I'm having?\"\n\n**Splitting Check:**\n- \"Am I using all-or-nothing thinking about any team, department, or person?\"\n- \"Where am I seeing 'us vs them' when it's really just 'us'?\"\n- \"What nuance am I missing by oversimplifying?\"\n\n**Stress Response Check:**\n- \"How is my body responding to stress? (sleep, appetite, health)\"\n- \"Am I working more hours but accomplishing less?\"\n- \"What am I doing to actually process stress vs. avoid it?\"\n\n**Integration:**\n- \"What pattern do I notice in my stress responses?\"\n- \"How are these patterns affecting my team?\"\n- \"What's one thing I could do differently next quarter?\"\n",
        "css-development/.claude-plugin/plugin.json": "{\n  \"name\": \"css-development\",\n  \"description\": \"CSS development workflows with Tailwind composition, semantic naming, and dark mode by default\",\n  \"version\": \"1.0.0\"\n}\n",
        "css-development/README.md": "# CSS Development Plugin\n\nCSS development workflows with Tailwind composition, semantic naming, and dark mode by default.\n\n## Installation\n\n```bash\n/plugin marketplace add 2389-research/claude-plugins\n/plugin install css-development@2389-research\n```\n\n## What This Plugin Provides\n\nSkills for CSS development following 2389 patterns:\n\n- **css-development** - Main orchestrator skill that routes to specific sub-skills\n- **css-development:create-component** - Create new styled components\n- **css-development:validate** - Validate CSS against patterns\n- **css-development:refactor** - Refactor existing CSS\n\n## Patterns\n\nThis plugin enforces:\n\n- **Semantic class naming**: `.user-profile`, `.nav-menu` (not `.container-1`, `.box-blue`)\n- **Tailwind composition**: Use `@apply` to compose utilities into semantic classes\n- **Dark mode by default**: Every component includes `dark:` variants\n- **Test coverage**: Static analysis + component rendering tests\n\n## Quick Example\n\n```css\n.user-profile {\n  @apply flex items-center space-x-4 p-4 bg-white dark:bg-gray-800;\n}\n\n.user-profile__avatar {\n  @apply w-12 h-12 rounded-full;\n}\n\n.user-profile__name {\n  @apply text-lg font-semibold text-gray-900 dark:text-gray-100;\n}\n```\n\n## Documentation\n\n- [Design Document](docs/plans/2025-11-13-css-development-skill-design.md)\n- [Implementation Plan](docs/plans/2025-11-13-css-development-skill-implementation.md)\n- [Examples](docs/examples/css-development-examples.md)\n\n\n",
        "css-development/skills/SKILL.md": "---\nname: css-development\ndescription: This skill should be used when working with CSS, creating components, styling elements, refactoring styles, or reviewing CSS code. Triggers on \"CSS\", \"styles\", \"Tailwind\", \"dark mode\", \"component styling\", \"semantic class\", \"@apply\", \"stylesheet\". Routes to specialized sub-skills for creation, validation, or refactoring.\n---\n\n# CSS Development Skill\n\n## Overview\n\nComprehensive workflow for CSS development using Tailwind + semantic component patterns. This skill automatically routes you to the appropriate specialized workflow based on context.\n\n**This skill will invoke one of three sub-skills:**\n- `css-development:create-component` - Creating new CSS components\n- `css-development:validate` - Reviewing existing CSS\n- `css-development:refactor` - Transforming CSS to semantic patterns\n\n## When This Skill Applies\n\nClaude Code will automatically load this skill when you:\n- Create new CSS components or styles\n- Review or validate existing CSS code\n- Refactor inline styles or utility classes\n- Work with component styling in any framework\n- Need to add dark mode support\n- Write CSS tests\n\n## CSS Development Patterns\n\nAll sub-skills follow these core patterns. Reference this section when working with CSS.\n\n### Core Principles\n\n1. **Semantic Naming** - Use descriptive class names (`.button-primary`, `.card-header`) not utility names (`.btn-blue`, `.card-hdr`)\n2. **Tailwind Composition** - Leverage Tailwind utilities via `@apply` directive\n3. **Dark Mode by Default** - Include `dark:` variants for all colored/interactive elements\n4. **Composition Over Creation** - Reuse existing classes before creating new ones\n5. **Test Coverage** - Static CSS tests + component rendering tests\n6. **Documentation** - Usage comments above each component class\n\n### Component Class Pattern\n\n```css\n/* Button component - Primary action button with hover states\n   Usage: <button className=\"button-primary\">Click me</button> */\n.button-primary {\n  @apply bg-indigo-500 hover:bg-indigo-700 dark:bg-indigo-600 dark:hover:bg-indigo-800;\n  @apply px-6 py-3 rounded-lg font-medium text-white;\n  @apply transition-all duration-200 hover:-translate-y-0.5;\n}\n```\n\n**Key characteristics:**\n- Group related utilities logically (background, spacing, typography, transitions)\n- Include hover/focus/active states\n- Include dark mode variants using `dark:` prefix\n- Use Tailwind's built-in scales (indigo-500, gray-800, etc.)\n\n### File Structure Convention\n\n```\nstyles/\n├── components.css      # All semantic component classes\n└── __tests__/\n    └── components.test.ts  # CSS and component tests\n```\n\n### Markup Integration (Framework-Agnostic)\n\nWorks with React, Vue, Svelte, or vanilla HTML:\n\n**React:**\n```tsx\nconst classes = `button-primary ${className}`.trim();\n<button className={classes}>...</button>\n```\n\n**Vanilla HTML:**\n```html\n<button class=\"button-primary custom-class\">...</button>\n```\n\n**Vue:**\n```vue\n<button :class=\"['button-primary', customClass]\">...</button>\n```\n\n**Key principle:** Apply semantic class + allow additional classes for customization.\n\n### Atomic Design Levels\n\n- **Atoms:** Basic building blocks (`.button`, `.input`, `.badge`, `.spinner`)\n- **Molecules:** Composed components (`.card`, `.form-field`, `.empty-state`)\n- **Organisms:** Complex components (`.page-layout`, `.session-card`, `.conversation-timeline`)\n\n### Testing Pattern\n\n**Static CSS Tests:**\n```typescript\nit('should have button component classes', () => {\n  const content = readFileSync('styles/components.css', 'utf-8');\n  expect(content).toContain('.button-primary');\n});\n```\n\n**Component Rendering Tests:**\n```typescript\nit('applies semantic class and custom className', () => {\n  render(<Button variant=\"primary\" className=\"custom\" />);\n  expect(screen.getByRole('button')).toHaveClass('button-primary', 'custom');\n});\n```\n\n## Workflow: Context Detection and Routing\n\nWhen this skill is invoked, follow these steps to route to the appropriate sub-skill:\n\n### Step 1: Analyze Context\n\nLook at the user's request and recent conversation to determine intent:\n\n**Creating new components?**\n- Keywords: \"create\", \"add\", \"new component\", \"build a\", \"make a\"\n- Files: Mention of components.css or new component names\n- Intent: User wants to add new CSS\n\n**Validating existing CSS?**\n- Keywords: \"review\", \"validate\", \"check\", \"audit\", \"look at\"\n- Files: Reference to existing CSS files or components\n- Intent: User wants feedback on existing CSS\n\n**Refactoring CSS?**\n- Keywords: \"refactor\", \"clean up\", \"extract\", \"improve\", \"convert\"\n- Code: Inline styles or utility classes in markup visible\n- Intent: User wants to transform existing CSS patterns\n\n### Step 2: Choose Sub-Skill\n\nBased on context analysis:\n\n**If creating:** Use the Skill tool to invoke `css-development:create-component`\n\n**If validating:** Use the Skill tool to invoke `css-development:validate`\n\n**If refactoring:** Use the Skill tool to invoke `css-development:refactor`\n\n**If ambiguous:** Ask the user using AskUserQuestion tool:\n\n```\nQuestion: \"What would you like to do with CSS?\"\nOptions:\n  - \"Create new component\" (Guide creating new semantic CSS component classes)\n  - \"Validate existing CSS\" (Review CSS against established patterns)\n  - \"Refactor CSS\" (Transform inline/utility styles to semantic components)\n```\n\n### Step 3: Invoke Sub-Skill\n\nUse the Skill tool to invoke the chosen sub-skill:\n\n**Example:**\n```\nI'm routing you to the create-component workflow.\n[Invoke Skill tool with skill: \"css-development:create-component\"]\n```\n\n### Step 4: Hand Off Control\n\nOnce the sub-skill is invoked, it takes over. The main skill's job is complete.\n\n## Important Notes\n\n- **Don't skip routing:** Always analyze context and choose the right sub-skill\n- **Don't duplicate sub-skill logic:** Let sub-skills handle their workflows\n- **Reference pattern documentation:** Sub-skills will reference the patterns documented above\n- **User can invoke directly:** User can call sub-skills directly (e.g., \"use css-development:validate\")\n",
        "css-development/skills/create-component/SKILL.md": "---\nname: css-development:create-component\ndescription: This skill should be used when creating new styled components or adding new CSS classes. Triggers on \"create component\", \"new button\", \"new card\", \"add styles\", \"style component\", \"build UI element\". Guides semantic naming, Tailwind composition, dark mode support, and test coverage.\n---\n\n# CSS Development: Create Component\n\n## Overview\n\nGuides you through creating new CSS components following established patterns:\n- Semantic class naming\n- Tailwind utility composition via `@apply`\n- Dark mode support by default\n- Test coverage (static CSS + component rendering)\n- Composition over creation (reuse existing classes)\n\n**This is a sub-skill of `css-development`** - typically invoked automatically via the main skill.\n\n## When This Skill Applies\n\nUse when:\n- Creating a new styled component (button, card, form field, etc.)\n- Adding new semantic CSS classes to components.css\n- Building reusable UI patterns\n- Need to ensure dark mode support and test coverage\n\n## Pattern Reference\n\nThis skill follows the patterns documented in the main `css-development` skill. Key patterns:\n\n**Semantic naming:** `.button-primary` not `.btn-blue`\n**Tailwind composition:** Use `@apply` to compose utilities\n**Dark mode:** Include `dark:` variants by default\n**Composition first:** Check if existing classes can be combined\n**Test coverage:** Static CSS tests + component rendering tests\n\n## Workflow\n\nWhen this skill is invoked, create a TodoWrite checklist and work through it step-by-step.\n\n### Announce Usage\n\nFirst, announce that you're using this skill:\n\n\"I'm using the css-development:create-component skill to guide creating this new CSS component.\"\n\n### Create TodoWrite Checklist\n\nUse the TodoWrite tool to create this checklist:\n\n```\nCreating CSS Component:\n- [ ] Survey existing components (read components.css)\n- [ ] Check if composition solves it (can existing classes combine?)\n- [ ] Identify component type (atom/molecule/organism if new class needed)\n- [ ] Choose semantic name (follow existing naming patterns)\n- [ ] Write component class (use @apply, include dark: variants)\n- [ ] Create markup integration (show React/HTML usage)\n- [ ] Write static CSS test (verify class exists)\n- [ ] Write component rendering test (verify className application)\n- [ ] Document component (add usage comment)\n```\n\n### Step-by-Step Details\n\n#### Step 1: Survey Existing Components\n\n**Action:** Use the Read tool to read `styles/components.css`\n\n**Purpose:** Understand what already exists to ensure consistency and identify reuse opportunities\n\n**What to look for:**\n- Similar components that could be composed\n- Existing naming patterns to follow\n- Common patterns (button variants, card styles, etc.)\n\n**Mark as in_progress** before starting, **mark as completed** when done.\n\n---\n\n#### Step 2: Check if Composition Solves It\n\n**Action:** Analyze if combining existing classes achieves the goal\n\n**Examples:**\n- Want a \"primary button with icon\"? → Combine `.button-primary` + spacing utilities\n- Want a \"card with shadow\"? → Use `.card` if it exists, add utility class if needed\n- Want a \"highlighted badge\"? → Combine `.badge` + color utilities\n\n**YAGNI principle:** Only create a new class if composition doesn't work or creates excessive duplication in markup.\n\n**Decision:**\n- **If composition works:** Document the combination and SKIP remaining steps (no new class needed)\n- **If new class needed:** Continue to Step 3\n\n**Mark as completed** when decision is made.\n\n---\n\n#### Step 3: Identify Component Type\n\n**Action:** Determine atomic design level (if creating new class)\n\n**Atoms** - Basic building blocks:\n- Single-purpose elements\n- Examples: `.button`, `.input`, `.badge`, `.spinner`, `.link`\n\n**Molecules** - Composed components:\n- Combine multiple atoms\n- Examples: `.card`, `.form-field`, `.empty-state`, `.alert`\n\n**Organisms** - Complex components:\n- Multiple molecules + atoms\n- Examples: `.page-layout`, `.navigation`, `.session-card`, `.conversation-timeline`\n\n**Why this matters:** Helps scope complexity and dependencies\n\n**Mark as completed** when type is identified.\n\n---\n\n#### Step 4: Choose Semantic Name\n\n**Action:** Choose a descriptive, semantic class name following existing patterns\n\n**Naming patterns from reference codebase:**\n- **Base + variant:** `.button-primary`, `.button-secondary`, `.button-danger`\n- **Component + sub-element:** `.card-title`, `.card-description`, `.form-field`\n- **Context + component:** `.session-card`, `.marketing-hero`, `.dashboard-layout`\n- **State modifiers:** `.session-card-active`, `.button-disabled`\n\n**Anti-patterns (avoid):**\n- Utility names: `.btn-blue`, `.card-sm`, `.text-big`\n- Abbreviations: `.btn`, `.hdr`, `.desc`\n- Generic: `.component`, `.item`, `.thing`\n\n**Validation:** Name should clearly indicate purpose and fit existing patterns\n\n**Mark as completed** when name is chosen.\n\n---\n\n#### Step 5: Write Component Class\n\n**Action:** Create the CSS class in `styles/components.css` using Edit tool\n\n**Template:**\n```css\n/* [Component name] - [Brief description]\n   Usage: <[element] className=\"[class-name]\">[content]</[element]> */\n.[class-name] {\n  @apply [background-utilities] [dark-variants];\n  @apply [spacing-utilities];\n  @apply [typography-utilities];\n  @apply [transition-utilities];\n}\n```\n\n**Required elements:**\n1. **Documentation comment** - What it is, how to use it\n2. **Dark mode variants** - Include `dark:` for colors/backgrounds\n3. **Logical grouping** - Group related utilities (background, spacing, typography, transitions)\n4. **Interactive states** - Include hover/focus/active if applicable\n\n**Example:**\n```css\n/* Primary button - Main call-to-action button with hover lift effect\n   Usage: <button className=\"button-primary\">Click me</button> */\n.button-primary {\n  @apply bg-indigo-500 hover:bg-indigo-700 dark:bg-indigo-600 dark:hover:bg-indigo-800;\n  @apply px-6 py-3 rounded-lg font-medium text-white;\n  @apply transition-all duration-200 hover:-translate-y-0.5;\n  @apply focus:outline-none focus:ring-2 focus:ring-indigo-500 focus:ring-offset-2;\n}\n```\n\n**Use Edit tool to add to existing file** (don't overwrite entire file)\n\n**Mark as completed** when class is written to file.\n\n---\n\n#### Step 6: Create Markup Integration\n\n**Action:** Document how to use the component in different frameworks\n\n**Show usage examples for:**\n- React (if project uses React)\n- Vanilla HTML (always show this)\n- Vue or other frameworks (if project uses them)\n\n**Example documentation:**\n\n```markdown\n## Using the button-primary Component\n\n**React:**\n```tsx\nconst Button = ({ variant = 'primary', className = '', children, ...props }) => {\n  const classes = `button-${variant} ${className}`.trim();\n  return <button className={classes} {...props}>{children}</button>;\n};\n\n// Usage\n<Button variant=\"primary\">Click me</Button>\n<Button variant=\"primary\" className=\"w-full\">Full width</Button>\n```\n\n**Vanilla HTML:**\n```html\n<button class=\"button-primary\">Click me</button>\n<button class=\"button-primary custom-class\">With custom class</button>\n```\n```\n\n**Where to put this:** In project documentation, README, or as a comment in the component file\n\n**Mark as completed** when markup examples are documented.\n\n---\n\n#### Step 7: Write Static CSS Test\n\n**Action:** Add test to `styles/__tests__/components.test.ts` (or create if doesn't exist)\n\n**Purpose:** Verify the CSS class exists in the components.css file\n\n**Test pattern:**\n```typescript\nimport { readFileSync } from 'fs';\nimport { describe, it, expect } from 'vitest';\n\ndescribe('components.css', () => {\n  const content = readFileSync('styles/components.css', 'utf-8');\n\n  it('should have button-primary component class', () => {\n    expect(content).toContain('.button-primary');\n  });\n\n  it('should have button-primary dark mode variants', () => {\n    expect(content).toContain('dark:bg-indigo');\n  });\n});\n```\n\n**Key checks:**\n- Class exists in file\n- Dark mode variants present (search for `dark:`)\n- Documentation comment exists (optional but good)\n\n**Run test:**\n```bash\nnpm test styles/__tests__/components.test.ts\n# or\nvitest styles/__tests__/components.test.ts\n```\n\n**Expected:** Test passes (green)\n\n**Mark as completed** when test is written and passing.\n\n---\n\n#### Step 8: Write Component Rendering Test\n\n**Action:** Add component rendering test (framework-specific)\n\n**Purpose:** Verify className application works in actual components\n\n**React example:**\n```typescript\nimport { render, screen } from '@testing-library/react';\nimport { describe, it, expect } from 'vitest';\nimport { Button } from '@/components/atoms/Button';\n\ndescribe('Button component', () => {\n  it('applies button-primary class', () => {\n    render(<Button variant=\"primary\">Click</Button>);\n    expect(screen.getByRole('button')).toHaveClass('button-primary');\n  });\n\n  it('accepts and applies custom className', () => {\n    render(<Button variant=\"primary\" className=\"custom-class\">Click</Button>);\n    const button = screen.getByRole('button');\n    expect(button).toHaveClass('button-primary', 'custom-class');\n  });\n});\n```\n\n**Key checks:**\n- Semantic class is applied\n- Custom className can be added\n- Classes don't conflict\n\n**Run test:**\n```bash\nnpm test components/atoms/Button.test.tsx\n# or\nvitest components/atoms/Button.test.tsx\n```\n\n**Expected:** Test passes (green)\n\n**Mark as completed** when test is written and passing.\n\n---\n\n#### Step 9: Document Component\n\n**Action:** Ensure component has usage documentation\n\n**Documentation should include:**\n1. **Comment in CSS** - Already done in Step 5\n2. **Markup examples** - Already done in Step 6\n3. **Component API** (for framework components) - Props, variants, etc.\n\n**Additional documentation (optional but recommended):**\n- Add to component style guide if project has one\n- Add to Storybook if project uses it\n- Add visual examples or screenshots\n\n**Minimum requirement:** CSS comment + markup examples exist\n\n**Mark as completed** when documentation is verified.\n\n---\n\n### Completion\n\nWhen all checklist items are completed:\n\n1. **Run all tests** to ensure everything passes:\n   ```bash\n   npm test\n   ```\n\n2. **Show summary** of what was created:\n   - Component class name and file location\n   - Test file locations\n   - Documentation locations\n\n3. **Suggest next steps:**\n   - Commit the changes\n   - Create related variants if needed\n   - Use the component in actual UI\n\n**Example summary:**\n```\nCreated button-primary component!\n\nFiles created/modified:\n- styles/components.css (added .button-primary)\n- styles/__tests__/components.test.ts (added static CSS test)\n- components/atoms/Button.test.tsx (added rendering test)\n- components/atoms/Button.tsx (markup integration)\n\nNext steps:\n- Commit these changes: git add . && git commit -m \"feat: add button-primary component\"\n- Use in your UI: <Button variant=\"primary\">Click me</Button>\n- Create variants if needed: button-secondary, button-danger, etc.\n```\n",
        "css-development/skills/refactor/SKILL.md": "---\nname: css-development:refactor\ndescription: This skill should be used when refactoring existing CSS from inline styles or utility classes to semantic patterns. Triggers on \"refactor CSS\", \"extract styles\", \"consolidate CSS\", \"convert inline\", \"clean up styles\", \"migrate to semantic\". Transforms to semantic classes with dark mode and tests.\n---\n\n# CSS Development: Refactor\n\n## Overview\n\nTransforms existing CSS into semantic component patterns:\n- Extract inline styles to semantic classes\n- Consolidate utility classes from markup into `@apply` compositions\n- Add dark mode variants\n- Add test coverage\n- Preserve existing functionality (behavior-neutral refactoring)\n\n**This is a sub-skill of `css-development`** - typically invoked automatically via the main skill.\n\n## When This Skill Applies\n\nUse when:\n- Converting inline styles to semantic classes\n- Extracting repeated utility combinations from markup\n- Migrating from pure utility-first to semantic components\n- Adding dark mode to existing CSS\n- Cleaning up scattered or duplicated CSS\n\n## Pattern Reference\n\nThis skill refactors toward patterns documented in the main `css-development` skill:\n\n**Semantic naming:** `.button-primary` not `.btn-blue`\n**Tailwind composition:** Use `@apply` to compose utilities\n**Dark mode:** Include `dark:` variants\n**Composition first:** Reuse existing classes before creating new\n**Test coverage:** Static CSS + component rendering tests\n\n## Workflow\n\nWhen this skill is invoked, create a TodoWrite checklist and refactor systematically.\n\n### Announce Usage\n\n\"I'm using the css-development:refactor skill to transform this CSS into semantic component patterns.\"\n\n### Create TodoWrite Checklist\n\nUse the TodoWrite tool:\n\n```\nRefactoring CSS:\n- [ ] Analyze existing CSS (identify what needs refactoring)\n- [ ] Find repeated patterns (look for duplicated utility combinations)\n- [ ] Check existing components (see if patterns already exist)\n- [ ] Extract to semantic classes (create new classes using @apply)\n- [ ] Include dark mode (add dark: variants to new classes)\n- [ ] Update markup (replace inline/utility classes with semantic names)\n- [ ] Add tests (write static CSS and rendering tests)\n- [ ] Document components (add usage comments)\n- [ ] Verify behavior unchanged (ensure visual output matches original)\n```\n\n### Refactoring Checklist Details\n\n#### Step 1: Analyze Existing CSS\n\n**Action:** Read and understand the CSS that needs refactoring\n\n**Look for:**\n- Inline styles in component files\n- Repeated utility class combinations in markup\n- CSS scattered across multiple files\n- Missing dark mode support\n- Lack of semantic class names\n\n**Example patterns to refactor:**\n\n```tsx\n// Inline styles\n<button style={{ background: 'indigo', padding: '1.5rem 2rem' }}>Click</button>\n\n// Repeated utilities in markup\n<button class=\"bg-indigo-500 hover:bg-indigo-700 px-6 py-3 rounded-lg text-white\">\n  Click me\n</button>\n<button class=\"bg-indigo-500 hover:bg-indigo-700 px-6 py-3 rounded-lg text-white\">\n  Submit\n</button>\n\n// Non-semantic CSS\n.btn-blue {\n  background: blue;\n  padding: 12px 24px;\n}\n```\n\n**Capture:**\n- File locations\n- Pattern frequency (how many times repeated)\n- Current approach (inline, utilities, old CSS)\n\n**Mark as completed** when analysis is done.\n\n---\n\n#### Step 2: Find Repeated Patterns\n\n**Action:** Identify duplicated utility combinations that should become semantic classes\n\n**Use Grep tool** to search for repeated patterns:\n\n```bash\n# Search for common utility combinations\ngrep -r \"bg-indigo-500 hover:bg-indigo-700 px-6 py-3\" .\ngrep -r \"rounded-lg shadow-md p-6\" .\n```\n\n**Categorize patterns:**\n- **High frequency** (5+ occurrences): Definitely extract\n- **Medium frequency** (2-4 occurrences): Probably extract\n- **Low frequency** (1 occurrence): Keep as-is or compose existing classes\n\n**For each pattern:**\n- Count occurrences\n- List file locations\n- Identify semantic purpose (is this a button? card? badge?)\n\n**Mark as completed** when patterns are cataloged.\n\n---\n\n#### Step 3: Check Existing Components\n\n**Action:** Read `styles/components.css` to see if patterns already exist\n\n**Before creating new classes, check:**\n- Does a similar class already exist?\n- Can existing classes be composed?\n- Would a variant of an existing class work?\n\n**Example:**\n```\nPattern found: bg-indigo-500 hover:bg-indigo-700 px-6 py-3 rounded-lg text-white\nCheck: Does .button-primary already exist? YES\nSolution: Use .button-primary instead of creating new class\n```\n\n**Decision for each pattern:**\n- ✅ Use existing class as-is\n- ✅ Compose existing classes\n- ✅ Create variant of existing class\n- ⚠️ Create new class (only if no existing solution)\n\n**Mark as completed** when reuse opportunities are identified.\n\n---\n\n#### Step 4: Extract to Semantic Classes\n\n**Action:** Create new semantic classes in `styles/components.css` for patterns that need extraction\n\n**For each pattern being extracted:**\n\n1. **Choose semantic name** following existing patterns\n2. **Write CSS class** using `@apply`\n3. **Include dark mode** variants\n4. **Add documentation** comment\n\n**Example extraction:**\n\n**Before (in markup):**\n```tsx\n<button class=\"bg-indigo-500 hover:bg-indigo-700 px-6 py-3 rounded-lg text-white transition-all duration-200\">\n  Click me\n</button>\n```\n\n**After (in components.css):**\n```css\n/* Primary button - Main call-to-action button with hover states\n   Usage: <button className=\"button-primary\">Click me</button> */\n.button-primary {\n  @apply bg-indigo-500 hover:bg-indigo-700 dark:bg-indigo-600 dark:hover:bg-indigo-800;\n  @apply px-6 py-3 rounded-lg text-white;\n  @apply transition-all duration-200;\n}\n```\n\n**Use Edit tool** to add each new class to components.css\n\n**Mark as completed** when all semantic classes are created.\n\n---\n\n#### Step 5: Include Dark Mode\n\n**Action:** Ensure all new/refactored classes have `dark:` variants\n\n**For each class created in Step 4:**\n- Add `dark:` variants for backgrounds\n- Add `dark:` variants for text colors\n- Add `dark:` variants for borders\n- Test in dark mode (if possible)\n\n**Pattern:**\n```css\n.component {\n  @apply bg-white dark:bg-gray-800;\n  @apply text-gray-900 dark:text-white;\n  @apply border-gray-200 dark:border-gray-700;\n}\n```\n\n**Mark as completed** when dark mode coverage is added.\n\n---\n\n#### Step 6: Update Markup\n\n**Action:** Replace inline styles and utility classes with semantic class names\n\n**For each file using the old pattern:**\n\n1. **Read the file** with Read tool\n2. **Use Edit tool** to replace old pattern with semantic class\n3. **Verify** the replacement is correct\n\n**Example:**\n\n**Before:**\n```tsx\n<button class=\"bg-indigo-500 hover:bg-indigo-700 px-6 py-3 rounded-lg text-white\">\n  Click me\n</button>\n```\n\n**After:**\n```tsx\n<button class=\"button-primary\">\n  Click me\n</button>\n```\n\n**Handle custom classes:**\n```tsx\n<!-- If there were additional custom classes, preserve them -->\n<button class=\"button-primary w-full mt-4\">\n  Click me\n</button>\n```\n\n**Track changes:**\n- Count files updated\n- Count instances replaced\n- Note any edge cases\n\n**Mark as completed** when all markup is updated.\n\n---\n\n#### Step 7: Add Tests\n\n**Action:** Add test coverage for refactored components\n\n**Static CSS test** in `styles/__tests__/components.test.ts`:\n```typescript\nit('should have button-primary class', () => {\n  const content = readFileSync('styles/components.css', 'utf-8');\n  expect(content).toContain('.button-primary');\n});\n\nit('should have dark mode variants in button-primary', () => {\n  const content = readFileSync('styles/components.css', 'utf-8');\n  expect(content).toContain('dark:bg-indigo');\n});\n```\n\n**Component rendering test** (if applicable):\n```typescript\nit('applies button-primary class after refactor', () => {\n  render(<Button variant=\"primary\">Click</Button>);\n  expect(screen.getByRole('button')).toHaveClass('button-primary');\n});\n```\n\n**Run tests** to ensure they pass:\n```bash\nnpm test\n```\n\n**Mark as completed** when tests are added and passing.\n\n---\n\n#### Step 8: Document Components\n\n**Action:** Ensure all refactored classes have documentation\n\n**Documentation includes:**\n- Comment in CSS explaining purpose\n- Usage example\n- Migration notes (if helpful)\n\n**Example:**\n```css\n/* Primary button - Main CTA button (refactored from inline utilities)\n   Usage: <button className=\"button-primary\">Click me</button>\n   Replaces: bg-indigo-500 hover:bg-indigo-700 px-6 py-3 rounded-lg text-white */\n.button-primary {\n  @apply bg-indigo-500 hover:bg-indigo-700 dark:bg-indigo-600 dark:hover:bg-indigo-800;\n  @apply px-6 py-3 rounded-lg text-white;\n  @apply transition-all duration-200;\n}\n```\n\n**Mark as completed** when documentation is added.\n\n---\n\n#### Step 9: Verify Behavior Unchanged\n\n**Action:** Ensure visual output and behavior match the original\n\n**Verification steps:**\n\n1. **Run tests** (if project has them):\n   ```bash\n   npm test\n   ```\n\n2. **Visual inspection** (if possible):\n   - Start dev server\n   - Check refactored components look the same\n   - Test in light and dark mode\n   - Test interactive states (hover, focus, active)\n\n3. **Check for regressions:**\n   - Compare before/after screenshots (if available)\n   - Verify no console errors\n   - Check responsive behavior\n\n**If behavior changed:**\n- Identify the difference\n- Fix the semantic class to match original behavior\n- Re-test\n\n**Behavior must be preserved** - refactoring should be visually neutral\n\n**Mark as completed** when behavior is verified unchanged.\n\n---\n\n### Completion\n\nWhen all checklist items are completed:\n\n1. **Generate summary** of refactoring work:\n\n```markdown\n## CSS Refactoring Summary\n\n### Changes Made\n\n**Semantic classes created:**\n- `.button-primary` (extracted from 8 instances across 5 files)\n- `.card` (extracted from 12 instances across 7 files)\n- `.badge-success` (extracted from 4 instances across 3 files)\n\n**Files modified:**\n- `styles/components.css` (+45 lines, 3 new classes)\n- `components/Button.tsx` (replaced utilities with .button-primary)\n- `components/Card.tsx` (replaced utilities with .card)\n- `components/Badge.tsx` (replaced utilities with .badge-success)\n- `styles/__tests__/components.test.ts` (+12 lines, 3 new tests)\n\n**Dark mode support:**\n- ✅ All refactored classes include dark: variants\n- ✅ Tested in both light and dark mode\n\n**Test coverage:**\n- ✅ Static CSS tests added for all new classes\n- ✅ Component rendering tests updated\n- ✅ All tests passing\n\n**Behavior verification:**\n- ✅ Visual output matches original\n- ✅ No console errors\n- ✅ Interactive states work correctly\n\n### Impact\n\n**Code reduction:**\n- Removed 247 lines of repeated utility classes from markup\n- Added 45 lines of semantic CSS\n- Net reduction: 202 lines\n\n**Maintainability:**\n- Styling centralized in components.css\n- Changes now made in one place instead of many\n- Consistent component appearance\n\n**Dark mode:**\n- Added dark mode support that didn't exist before\n- All components now work in light and dark themes\n```\n\n2. **Suggest next steps:**\n   - Commit the refactoring\n   - Document the new patterns for the team\n   - Continue refactoring other components\n\n3. **Offer validation:**\n   \"Would you like me to validate the refactored CSS using the css-development:validate skill?\"\n\n**Mark as completed** when summary is presented.\n",
        "css-development/skills/validate/SKILL.md": "---\nname: css-development:validate\ndescription: This skill should be used when reviewing or auditing existing CSS code for consistency with established patterns. Triggers on \"review CSS\", \"audit styles\", \"check CSS\", \"validate stylesheet\", \"CSS review\". Checks semantic naming, dark mode coverage, Tailwind usage, and test coverage.\n---\n\n# CSS Development: Validate\n\n## Overview\n\nReviews existing CSS code against established patterns and provides specific, actionable feedback:\n- Semantic naming conventions\n- Tailwind `@apply` composition\n- Dark mode variant coverage\n- Test coverage (static + rendering)\n- Documentation quality\n- Composition opportunities\n\n**This is a sub-skill of `css-development`** - typically invoked automatically via the main skill.\n\n## When This Skill Applies\n\nUse when:\n- Reviewing existing CSS code\n- Auditing component styles for consistency\n- Checking if patterns are being followed\n- Before merging CSS changes\n- Refactoring prep (identify issues first)\n\n## Pattern Reference\n\nThis skill validates against patterns documented in the main `css-development` skill:\n\n**Semantic naming:** `.button-primary` not `.btn-blue`\n**Tailwind composition:** Use `@apply` to compose utilities\n**Dark mode:** Include `dark:` variants\n**Test coverage:** Static CSS + component rendering tests\n**Documentation:** Usage comments above classes\n\n## Workflow\n\nWhen this skill is invoked, create a TodoWrite checklist and work through validation systematically.\n\n### Announce Usage\n\n\"I'm using the css-development:validate skill to review this CSS against established patterns.\"\n\n### Create TodoWrite Checklist\n\nUse the TodoWrite tool:\n\n```\nValidating CSS:\n- [ ] Read CSS files (load components.css and related styles)\n- [ ] Check semantic naming (verify descriptive class names)\n- [ ] Verify @apply usage (ensure Tailwind composition)\n- [ ] Check dark mode coverage (confirm dark: variants present)\n- [ ] Look for composition opportunities (identify reusable patterns)\n- [ ] Verify test coverage (check static and rendering tests exist)\n- [ ] Check documentation (ensure usage comments present)\n- [ ] Report findings (provide file:line references and suggestions)\n```\n\n### Validation Checklist Details\n\n#### Step 1: Read CSS Files\n\n**Action:** Use Read tool to load CSS files for review\n\n**Files to check:**\n- `styles/components.css` (main semantic components)\n- Any component-specific CSS files mentioned\n- Inline styles in component files (if applicable)\n\n**What to capture:**\n- All class definitions\n- Usage of `@apply` vs. inline utilities\n- Presence of dark mode variants\n- Documentation comments\n\n**Mark as completed** when files are loaded and understood.\n\n---\n\n#### Step 2: Check Semantic Naming\n\n**Action:** Review all class names for semantic, descriptive naming\n\n**Good patterns:**\n- `.button-primary`, `.card-header`, `.form-field`, `.empty-state`\n- Context + component: `.session-card`, `.marketing-hero`\n- Base + variant: `.badge-success`, `.button-danger`\n\n**Bad patterns (report these):**\n- Utility names: `.btn-blue`, `.card-sm`, `.text-big`\n- Abbreviations: `.btn`, `.hdr`, `.desc`\n- Generic: `.component`, `.item`, `.thing`\n- Random: `.style1`, `.custom`, `.special`\n\n**For each issue:**\n- Note file and line number\n- Show the problematic class name\n- Suggest semantic alternative based on usage context\n\n**Mark as completed** when all class names reviewed.\n\n---\n\n#### Step 3: Verify @apply Usage\n\n**Action:** Check that Tailwind utilities are composed via `@apply`, not scattered in markup\n\n**Good patterns:**\n```css\n.button-primary {\n  @apply bg-indigo-500 hover:bg-indigo-700 px-6 py-3 rounded-lg;\n}\n```\n\n**Bad patterns (report these):**\n```html\n<!-- Utilities in markup instead of semantic class -->\n<button class=\"bg-indigo-500 hover:bg-indigo-700 px-6 py-3 rounded-lg\">\n  Click me\n</button>\n```\n\n**Check:**\n- Are utilities composed into semantic classes via `@apply`?\n- Are there repeated utility combinations in markup that should be extracted?\n- Are semantic classes actually being used in components?\n\n**For each issue:**\n- Show the problematic markup or CSS\n- Explain why it should use `@apply`\n- Suggest extraction to semantic class\n\n**Mark as completed** when @apply usage is reviewed.\n\n---\n\n#### Step 4: Check Dark Mode Coverage\n\n**Action:** Verify colored and interactive elements have `dark:` variants\n\n**What needs dark mode:**\n- Background colors (bg-*)\n- Text colors (text-*)\n- Border colors (border-*)\n- Interactive states (hover, focus)\n- Shadows that affect visibility\n\n**What typically doesn't need dark mode:**\n- Spacing utilities (p-*, m-*, gap-*)\n- Layout utilities (flex, grid, etc.)\n- Pure structural styles\n\n**Pattern to check:**\n```css\n/* Good - has dark mode */\n.card {\n  @apply bg-white dark:bg-gray-800 text-gray-900 dark:text-white;\n}\n\n/* Bad - missing dark mode */\n.card {\n  @apply bg-white text-gray-900;\n}\n```\n\n**For each issue:**\n- Note which class is missing dark mode variants\n- Show the current CSS\n- Suggest specific `dark:` utilities to add\n\n**Mark as completed** when dark mode coverage is checked.\n\n---\n\n#### Step 5: Look for Composition Opportunities\n\n**Action:** Identify repeated patterns that could use existing classes or be extracted\n\n**Look for:**\n- Same utility combinations repeated in multiple classes\n- Similar patterns that could share a base class\n- Inline utilities that could reference semantic classes\n\n**Example issue:**\n```css\n/* Repeated pattern */\n.card-primary {\n  @apply bg-white dark:bg-gray-800 rounded-lg shadow-md p-6;\n}\n\n.card-secondary {\n  @apply bg-white dark:bg-gray-800 rounded-lg shadow-md p-6;\n  @apply border-2 border-gray-200;\n}\n\n/* Suggestion: Extract base .card class, add variants */\n.card {\n  @apply bg-white dark:bg-gray-800 rounded-lg shadow-md p-6;\n}\n\n.card-secondary {\n  @apply border-2 border-gray-200;\n}\n```\n\n**For each opportunity:**\n- Show the repeated pattern\n- Suggest base class + composition\n- Estimate impact (how many places benefit)\n\n**Mark as completed** when composition opportunities are identified.\n\n---\n\n#### Step 6: Verify Test Coverage\n\n**Action:** Check that CSS classes have test coverage\n\n**Static CSS tests** - Check `styles/__tests__/components.test.ts`:\n```typescript\nit('should have button-primary class', () => {\n  expect(content).toContain('.button-primary');\n});\n```\n\n**Component rendering tests** - Check component test files:\n```typescript\nit('applies button-primary class', () => {\n  render(<Button variant=\"primary\">Click</Button>);\n  expect(screen.getByRole('button')).toHaveClass('button-primary');\n});\n```\n\n**For classes without tests:**\n- List the class name\n- Note which test is missing (static, rendering, or both)\n- Provide test template to add\n\n**Mark as completed** when test coverage is checked.\n\n---\n\n#### Step 7: Check Documentation\n\n**Action:** Verify components have usage documentation\n\n**Required documentation:**\n- Comment above CSS class explaining purpose\n- Usage example in comment\n\n**Example:**\n```css\n/* Button component - Primary action button with hover lift effect\n   Usage: <button className=\"button-primary\">Click me</button> */\n.button-primary {\n  ...\n}\n```\n\n**For classes without documentation:**\n- List the class name and location\n- Suggest documentation to add based on class purpose\n\n**Mark as completed** when documentation is checked.\n\n---\n\n#### Step 8: Report Findings\n\n**Action:** Compile all findings into structured report\n\n**Report format:**\n\n```markdown\n## CSS Validation Report\n\n### ✅ Good Patterns Found\n\n- `.button-primary` follows semantic naming (components.css:15)\n- Dark mode variants present on interactive elements (components.css:17-19)\n- Tests cover className application (Button.test.tsx:23)\n- Documentation comments present (components.css:14)\n\n### ⚠️ Issues Found\n\n#### Semantic Naming Issues\n\n**components.css:45** - `.btn-blue` uses utility naming\n- Current: `.btn-blue`\n- Suggestion: Rename to `.button-secondary` for consistency with `.button-primary`\n- Impact: Update 3 component files\n\n**components.css:67** - `.card-sm` uses size in name\n- Current: `.card-sm`\n- Suggestion: Extract size to utility or rename to `.card-compact` for semantic meaning\n- Impact: Update 5 usages\n\n#### Missing Dark Mode Variants\n\n**components.css:78** - `.card-header` missing dark mode\n- Current: `@apply bg-gray-100 text-gray-900`\n- Suggestion: Add `dark:bg-gray-800 dark:text-white`\n- Impact: Visual bug in dark mode\n\n**components.css:92** - `.badge` missing dark mode\n- Current: `@apply bg-indigo-100 text-indigo-800`\n- Suggestion: Add `dark:bg-indigo-900 dark:text-indigo-200`\n- Impact: Low contrast in dark mode\n\n#### Missing Test Coverage\n\n**components.css:102** - `.empty-state` has no tests\n- Missing: Both static CSS test and component rendering test\n- Suggestion: Add tests to verify class exists and renders correctly\n\n#### Missing Documentation\n\n**components.css:115** - `.session-card` lacks usage comment\n- Suggestion: Add comment explaining purpose and usage example\n\n### 📊 Summary\n\n- **Total classes reviewed:** 12\n- **Issues found:** 7\n- **Priority:** 2 high (dark mode bugs), 3 medium (naming), 2 low (docs)\n\n### 🎯 Recommended Actions\n\n1. **High priority:** Add dark mode variants to `.card-header` and `.badge` (visual bugs)\n2. **Medium priority:** Rename `.btn-blue` → `.button-secondary` for consistency\n3. **Medium priority:** Add test coverage for `.empty-state`\n4. **Low priority:** Add documentation comments to undocumented classes\n\nWould you like me to fix these issues, or would you prefer to address them manually?\n```\n\n**Mark as completed** when report is generated and presented.\n\n---\n\n### Completion\n\nAfter generating the validation report:\n\n1. **Ask user what they want to do next:**\n   - Fix issues automatically?\n   - Fix specific issues only?\n   - Just wanted the report?\n\n2. **Offer to invoke refactor skill** if there are structural issues that need refactoring\n\n3. **Suggest committing** any fixes made\n",
        "documentation-audit/.claude-plugin/plugin.json": "{\n  \"name\": \"documentation-audit\",\n  \"version\": \"2.0.0\",\n  \"description\": \"Systematically verify documentation claims against codebase reality using two-pass extraction and pattern expansion. Runs in isolated context with Plan agent.\"\n}\n",
        "documentation-audit/README.md": "# Documentation Audit Plugin\n\nSystematically verify documentation claims against codebase reality.\n\n## Problem\n\nDocumentation drifts from code. Claims about defaults, file paths, behaviors, and configurations become stale as code evolves. Users following outdated docs encounter errors.\n\n## Solution\n\nTwo-pass audit approach:\n\n1. **Pass 1: Direct Extraction** - Extract and verify claims from each doc\n2. **Pass 2A: Pattern Expansion** - Find similar issues based on discovered patterns\n3. **Pass 2B: Gap Detection** - Compare codebase inventory vs documented items\n\n## Usage\n\nInvoke the skill when:\n- Before releases\n- After major refactors\n- When users report docs don't match behavior\n- Periodic hygiene (quarterly)\n\n## Output\n\nGenerates `docs/audits/AUDIT_REPORT.md` with:\n- Executive summary (claims verified, false claims found)\n- False claims table with line numbers and fixes\n- Pattern analysis (common root causes)\n- Human review queue for behavioral claims\n\n## Example Results\n\nFrom a real audit:\n- 12 documents scanned\n- ~180 claims verified\n- 31 false claims found (17%)\n- Patterns: dead scripts (9), wrong intervals (4), wrong service names (3)\n",
        "documentation-audit/skills/SKILL.md": "---\nname: documentation-audit\ndescription: This skill should be used when verifying documentation claims against codebase reality. Triggers on \"audit docs\", \"verify documentation\", \"check docs\", \"docs accurate\", \"documentation drift\", \"before release\", \"after refactor\", \"docs don't match\". Uses two-pass extraction with pattern expansion for comprehensive detection.\ncontext: fork\nagent: Plan\n---\n\n<!-- ABOUTME: Documentation audit skill for verifying claims against codebase -->\n<!-- ABOUTME: Uses two-pass extraction with pattern expansion for comprehensive detection -->\n\n# Documentation Audit\n\nSystematically verify claims in documentation against the actual codebase using a two-pass approach.\n\n## Overview\n\n**Core principle:** Low recall is worse than false positives—missed claims stay invisible.\n\n**Two-pass process:**\n1. **Pass 1:** Extract and verify claims directly from docs\n2. **Pass 2A:** Expand patterns from false claims to find similar issues\n3. **Pass 2B:** Compare codebase inventory vs documented items (gap detection)\n\n## Quick Start\n\n1. Identify target docs (user-facing only, skip `plans/`, `audits/`)\n2. Note current git commit for report header\n3. Run Pass 1 extraction using parallel agents (one per doc)\n4. Analyze false claims for patterns\n5. Run Pass 2 expansion searches\n6. Generate `docs/audits/AUDIT_REPORT_YYYY-MM-DD.md`\n\n## Claim Types\n\n| Type | Example | Verification |\n|------|---------|--------------|\n| `file_ref` | `scripts/foo.py` | File exists? |\n| `config_default` | \"defaults to 'AI Radio'\" | Check schema/code |\n| `env_var` | `STATION_NAME` | In .env.example + code? |\n| `cli_command` | `--normalize` flag | Script supports it? |\n| `behavior` | \"runs every 2 minutes\" | Check timers/code |\n\n**Verification confidence:**\n- **Tier 1 (auto):** file_ref, config_default, env_var, cli_command\n- **Tier 2 (semi-auto):** symbol_ref, version_req\n- **Tier 3 (human review):** behavior, constraint\n\n## Pass 2 Pattern Expansion\n\nAfter Pass 1, analyze false claims and search for similar patterns:\n\n```\nDead script found: diagnose_track_selection.py\n  → Search: all script references → Found 8 more dead scripts\n\nWrong interval: \"every 10 seconds\"\n  → Search: \"every \\d+ (seconds?|minutes?)\" → Found 3 more\n\nWrong service name: ai-radio-break-gen.service\n  → Search: service/timer names → Found naming inconsistencies\n```\n\n**Common patterns to always check:**\n- Dead scripts: `scripts/*.py` references\n- Timer intervals: `every \\d+ (seconds?|minutes?)`\n- Service names: `ai-radio-*.service`, `*.timer`\n- Config vars: `RADIO_*` environment variables\n- CLI flags: `--flag` patterns in bash blocks\n\n## Output Format\n\nGenerate `docs/audits/AUDIT_REPORT_YYYY-MM-DD.md`:\n\n```markdown\n# Documentation Audit Report\nGenerated: YYYY-MM-DD | Commit: abc123\n\n## Executive Summary\n| Metric | Count |\n|--------|-------|\n| Documents scanned | 12 |\n| Claims verified | ~180 |\n| Verified TRUE | ~145 (81%) |\n| **Verified FALSE** | **31 (17%)** |\n\n## False Claims Requiring Fixes\n### CONFIGURATION.md\n| Line | Claim | Reality | Fix |\n|------|-------|---------|-----|\n| 135 | `claude-sonnet-4-5` | Actual: `claude-3-5-sonnet-latest` | Update |\n\n## Pattern Summary\n| Pattern | Count | Root Cause |\n|---------|-------|------------|\n| Dead scripts | 9 | Scripts deleted, docs not updated |\n\n## Human Review Queue\n- [ ] Line 436: behavior claim needs verification\n```\n\n## Detailed References\n\nFor execution checklist and anti-patterns: [checklist.md](checklist.md)\nFor claim extraction patterns: [extraction-patterns.md](extraction-patterns.md)\n",
        "documentation-audit/skills/checklist.md": "# Documentation Audit Execution Checklist\n\nUse TodoWrite to track these as you execute the audit.\n\n## Phase 1: Setup\n\n- [ ] Identify target docs (user-facing markdown in docs/, README.md)\n- [ ] Exclude: `docs/plans/`, `docs/audits/`, design docs\n- [ ] Note current git commit: `git rev-parse --short HEAD`\n- [ ] Create output directory: `mkdir -p docs/audits`\n\n## Phase 2: Pass 1 - Extraction\n\nFor each target doc, use parallel Task agents to extract claims:\n\n- [ ] Extract `file_ref` claims (paths in backticks, code blocks)\n- [ ] Extract `config_default` claims (\"defaults to\", \"Default:\")\n- [ ] Extract `env_var` claims (UPPERCASE_VARS)\n- [ ] Extract `cli_command` claims (--flags, script invocations)\n- [ ] Extract `behavior` claims (timing, intervals, counts)\n\n**Extraction prompt for each doc:**\n```\nExtract all verifiable claims from [DOC]. For each claim record:\n- Line number\n- Claim text (verbatim)\n- Claim type (file_ref, config_default, env_var, cli_command, behavior)\n- Artifact to verify (the specific file/var/flag)\n- Expected value (if applicable)\n\nFocus on: paths, defaults, environment variables, CLI flags, timing/intervals.\nSkip: prose descriptions, opinions, marketing language.\n```\n\n## Phase 3: Pass 1 - Verification\n\nFor each extracted claim:\n\n- [ ] **file_ref:** `test -e <path>` or Glob search\n- [ ] **config_default:** Check .env.example, config modules, code defaults\n- [ ] **env_var:** Grep for usage in code, check .env.example\n- [ ] **cli_command:** Check script's argparse/flags\n- [ ] **behavior:** Flag for human review, extract supporting evidence\n\nRecord for each:\n- Verdict: TRUE / FALSE / NEEDS_REVIEW\n- Evidence: file:line or \"not found\"\n- Suggested fix (if FALSE)\n\n## Phase 4: Pass 2A - Pattern Expansion\n\n- [ ] Group false claims by pattern type\n- [ ] For each pattern, search ALL docs for similar claims\n- [ ] Verify newly discovered claims\n\n**Common expansion searches:**\n```bash\n# Dead scripts pattern\ngrep -rn \"scripts/.*\\.py\" docs/ | grep -v \"batch_ingest\\|enqueue\\|schedule\"\n\n# Wrong intervals pattern\ngrep -rn \"every [0-9]* \\(second\\|minute\\)\" docs/\n\n# Service/timer names\ngrep -rn \"ai-radio-.*\\.\\(service\\|timer\\)\" docs/\n\n# Environment variables\ngrep -rn \"RADIO_[A-Z_]*\" docs/\n```\n\n## Phase 5: Pass 2B - Gap Detection\n\n- [ ] List actual scripts: `ls scripts/*.py`\n- [ ] List documented scripts in SCRIPTS.md\n- [ ] Flag undocumented scripts\n- [ ] Flag documented-but-missing scripts\n\nRepeat for:\n- [ ] systemd services/timers\n- [ ] config modules\n- [ ] API endpoints\n\n## Phase 6: Report Generation\n\n- [ ] Create `docs/audits/AUDIT_REPORT_YYYY-MM-DD.md`\n- [ ] Include executive summary with metrics\n- [ ] List false claims with line numbers and fixes\n- [ ] Summarize patterns discovered\n- [ ] Create human review queue for behavioral claims\n\n## Anti-Patterns\n\n**Don't:**\n- Skip Pass 2 (catches 10-20% more issues)\n- Trust \"looks correct\" without verification\n- Fix claims without citing evidence (file:line)\n- Audit design docs or plans (historical artifacts)\n- Batch verification before extraction is complete\n\n**Do:**\n- Use parallel agents for extraction (one per doc)\n- Record evidence for every verdict\n- Re-run after major refactors\n- Prioritize user-facing docs over internal\n",
        "documentation-audit/skills/extraction-patterns.md": "# Claim Extraction Patterns\n\nReference for identifying verifiable claims in documentation.\n\n## Pattern Recognition\n\n### File References (`file_ref`)\n\n**Signals:**\n- Backticked paths: `` `scripts/foo.py` ``\n- Code block paths: `python scripts/foo.py`\n- Directory structures in prose\n- \"located at\", \"found in\", \"see file\"\n\n**Extraction regex:**\n```\n`[a-zA-Z0-9_/.-]+\\.(py|sh|md|sql|liq|json|yaml|yml|toml|service|timer)`\n```\n\n**Verification:**\n```python\nimport os\nos.path.exists(claim.artifact)  # or Glob search for partial matches\n```\n\n### Configuration Defaults (`config_default`)\n\n**Signals:**\n- \"defaults to X\"\n- \"Default: X\"\n- \"default value is X\"\n- Tables with \"Default\" column\n- .env examples with `VAR=value`\n\n**Extraction regex:**\n```\n(?:defaults? to|Default:?|default value is)\\s*[`'\"]?([^`'\".\\n]+)\n```\n\n**Verification sources (priority order):**\n1. `.env.example` - documented defaults\n2. `src/**/config/*.py` - config modules\n3. Code: `os.getenv(\"VAR\", \"default\")`\n4. Schema files (JSON Schema, Pydantic models)\n\n### Environment Variables (`env_var`)\n\n**Signals:**\n- UPPERCASE_WITH_UNDERSCORES\n- `$VAR` or `${VAR}` syntax\n- \"set X=\" in bash blocks\n- Environment tables\n\n**Extraction regex:**\n```\n\\b[A-Z][A-Z0-9_]{2,}\\b\n```\n\n**Filter out false positives:**\n- Common acronyms: HTTP, API, JSON, SQL, URL\n- Standard vars: PATH, HOME, USER\n\n**Verification:**\n```bash\ngrep -r \"os.getenv.*VAR\\|environ.*VAR\" src/\ngrep \"^VAR=\" .env.example\n```\n\n### CLI Commands (`cli_command`)\n\n**Signals:**\n- `--flag` or `-f` patterns\n- `script.py [args]` in code blocks\n- Command tables\n- \"Usage:\" sections\n\n**Extraction patterns:**\n```\n--[a-z][a-z0-9-]+    # Long flags\n-[a-zA-Z]            # Short flags\nscript\\.py\\s+\\S+     # Script with args\n```\n\n**Verification:**\n```bash\npython scripts/foo.py --help 2>&1 | grep -q \"\\-\\-flag\"\ngrep -E \"add_argument.*--flag|argparse\" scripts/foo.py\n```\n\n### Behavioral Claims (`behavior`)\n\n**Signals:**\n- \"every N seconds/minutes\"\n- \"runs at :00\"\n- \"retries N times\"\n- \"waits for\", \"timeout\"\n- \"automatically X when Y\"\n\n**Extraction patterns:**\n```\nevery \\d+ (?:second|minute|hour)s?\nruns? (?:at|every) [:\\d]+\nretries? \\d+ times?\n(?:wait|timeout|delay).*\\d+\nautomatically \\w+\n```\n\n**Verification approach:**\n1. Search code for related logic\n2. Check systemd timers: `OnCalendar=`, `OnUnitActiveSec=`\n3. Extract evidence, flag for human review\n\n## Claim Confidence Scoring\n\n| Type | Auto-Verifiable | Confidence |\n|------|-----------------|------------|\n| `file_ref` | Yes | High |\n| `config_default` | Yes | High |\n| `env_var` | Yes | High |\n| `cli_command` | Yes | High |\n| `symbol_ref` | Partial | Medium |\n| `version_req` | Partial | Medium |\n| `behavior` | No | Low (human) |\n| `constraint` | No | Low (human) |\n\n## Pattern Expansion Templates\n\nWhen you find a false claim, search for similar patterns:\n\n### Dead Script Pattern\n```\nFound: scripts/diagnose_track_selection.py (doesn't exist)\nSearch: grep -rn \"scripts/[a-z_]*\\.py\" docs/\nVerify: Each match against actual scripts/ directory\n```\n\n### Wrong Interval Pattern\n```\nFound: \"every 10 seconds\" (actually 2 minutes)\nSearch: grep -rn \"every [0-9]* \\(second\\|minute\\)\" docs/\nVerify: Each against actual timer configuration\n```\n\n### Renamed Service Pattern\n```\nFound: ai-radio-break-gen.service (renamed to generate-break.service)\nSearch: grep -rn \"ai-radio-[a-z-]*\\.\\(service\\|timer\\)\" docs/\nVerify: Each against systemd/ directory\n```\n\n### Deprecated Config Pattern\n```\nFound: RADIO_OLD_VAR (removed)\nSearch: grep -rn \"RADIO_[A-Z_]*\" docs/\nVerify: Each against .env.example and code\n```\n\n## False Positive Filters\n\nSkip these during extraction:\n\n1. **Example blocks** - `<!-- example -->`, \"For example:\"\n2. **Hypothetical** - \"would be\", \"could use\", \"might\"\n3. **Historical** - \"previously\", \"used to\", \"was\"\n4. **External refs** - URLs, package names, external tools\n5. **Prose descriptions** - General explanations without specific values\n",
        "firebase-development/.claude-plugin/plugin.json": "{\n  \"name\": \"firebase-development\",\n  \"description\": \"Firebase project workflows including setup, features, debugging, and validation\",\n  \"version\": \"1.0.0\"\n}\n",
        "firebase-development/README.md": "# Firebase Development Plugin\n\nFirebase project workflows including setup, features, debugging, and validation.\n\n## Installation\n\n```bash\n/plugin marketplace add 2389-research/claude-plugins\n/plugin install firebase-development@2389-research\n```\n\n## What This Plugin Provides\n\nSkills for Firebase development following 2389 patterns:\n\n- **firebase-development** - Main orchestrator skill that routes to specific sub-skills\n- **firebase-development:project-setup** - Initialize new Firebase projects\n- **firebase-development:add-feature** - Add features to existing projects\n- **firebase-development:debug** - Debug Firebase issues\n- **firebase-development:validate** - Validate project structure\n\n## Patterns\n\nThis plugin supports:\n\n- **Multi-hosting setup**: Multiple sites or single hosting\n- **Authentication**: Custom API keys, Firebase Auth, or both\n- **Cloud Functions**: Express, domain-grouped, or individual files\n- **Security models**: Server-write-only vs client-write with validation\n- **Emulator-first development**: Always test locally before deploying\n- **Modern tooling**: TypeScript, vitest, biome\n\n## Quick Example\n\n```typescript\n// Cloud Function with domain grouping\nexport const users = {\n  onCreate: onDocumentCreated('users/{userId}', async (event) => {\n    // Implementation\n  }),\n  onUpdate: onDocumentUpdated('users/{userId}', async (event) => {\n    // Implementation\n  })\n};\n```\n\n## Documentation\n\n- [Design Document](docs/plans/2025-01-14-firebase-skills-design.md)\n- [Implementation Plan](docs/plans/2025-01-14-firebase-skills-implementation.md)\n- Examples:\n  - [API Key Authentication](docs/examples/api-key-authentication.md)\n  - [Emulator Workflow](docs/examples/emulator-workflow.md)\n  - [Express Function Architecture](docs/examples/express-function-architecture.md)\n  - [Firestore Rules Patterns](docs/examples/firestore-rules-patterns.md)\n  - [Multi-Hosting Setup](docs/examples/multi-hosting-setup.md)\n\n\n",
        "firebase-development/skills/SKILL.md": "---\nname: firebase-development\ndescription: This skill should be used when working with Firebase projects, including initializing projects, adding Cloud Functions or Firestore collections, debugging emulator issues, or reviewing Firebase code. Triggers on \"firebase\", \"firestore\", \"cloud functions\", \"emulator\", \"firebase auth\", \"deploy to firebase\", \"firestore rules\".\n---\n\n# Firebase Development\n\n## Overview\n\nThis skill system guides Firebase development using proven patterns from production projects. It routes to specialized sub-skills based on detected intent.\n\n**Sub-skills:**\n- `firebase-development:project-setup` - Initialize new Firebase projects\n- `firebase-development:add-feature` - Add functions/collections/endpoints\n- `firebase-development:debug` - Troubleshoot emulator and runtime issues\n- `firebase-development:validate` - Review Firebase code for security/patterns\n\n## When This Skill Applies\n\n- Starting new Firebase projects\n- Adding Cloud Functions or Firestore collections\n- Debugging emulator issues or rule violations\n- Reviewing Firebase code for security and patterns\n- Setting up multi-hosting configurations\n- Implementing authentication (API keys or Firebase Auth)\n\n## Routing Logic\n\n### Keywords by Sub-Skill\n\n**project-setup:**\n- \"new firebase project\", \"initialize firebase\", \"firebase init\"\n- \"set up firebase\", \"create firebase app\", \"start firebase project\"\n\n**add-feature:**\n- \"add function\", \"create endpoint\", \"new tool\", \"add api\"\n- \"new collection\", \"add feature\", \"build\", \"implement\"\n\n**debug:**\n- \"error\", \"not working\", \"debug\", \"emulator issue\"\n- \"rules failing\", \"permission denied\", \"troubleshoot\", \"deployment failed\"\n\n**validate:**\n- \"review firebase\", \"check firebase\", \"validate\", \"audit firebase\"\n- \"look at firebase code\", \"security review\"\n\n### Routing Process\n\n1. **Analyze Request**: Check for routing keywords\n2. **Match Sub-Skill**: Identify best match based on keyword density\n3. **Announce**: \"I'm using the firebase-development:[sub-skill] skill to [action]\"\n4. **Route**: Load and execute the sub-skill\n5. **Fallback**: If ambiguous, use AskUserQuestion with 4 options\n\n### Fallback Example\n\nIf intent is unclear, ask:\n\n```\nQuestion: \"What Firebase task are you working on?\"\nOptions:\n  - \"Project Setup\" (Initialize new Firebase project)\n  - \"Add Feature\" (Add functions, collections, endpoints)\n  - \"Debug Issue\" (Troubleshoot errors or problems)\n  - \"Validate Code\" (Review against patterns)\n```\n\n## Reference Projects\n\nPatterns are extracted from three production Firebase projects:\n\n| Project | Path | Key Patterns |\n|---------|------|--------------|\n| **oneonone** | `/Users/dylanr/work/2389/oneonone` | Express API, custom API keys, server-write-only |\n| **bot-socialmedia** | `/Users/dylanr/work/2389/bot-socialmedia-server` | Domain-grouped functions, Firebase Auth + roles |\n| **meme-rodeo** | `/Users/dylanr/work/2389/meme-rodeo` | Individual function files, entitlements |\n\n## Pattern Summaries\n\n### Multi-Hosting Setup\n\nThree options based on needs:\n\n| Option | When to Use | Key Feature |\n|--------|-------------|-------------|\n| `site:` based | Multiple independent URLs | Simple, no build coordination |\n| `target:` based | Need predeploy hooks | Build scripts run automatically |\n| Single + rewrites | Smaller projects | All under one domain |\n\n**Details:** See `docs/examples/multi-hosting-setup.md`\n\n### Authentication\n\n| Pattern | When to Use | Example |\n|---------|-------------|---------|\n| Custom API keys | MCP tools, server-to-server | oneonone |\n| Firebase Auth + roles | User-facing apps | bot-socialmedia |\n| Hybrid | Both patterns needed | Web UI + API access |\n\n**Details:** See `docs/examples/api-key-authentication.md`\n\n### Cloud Functions Architecture\n\n| Pattern | When to Use | Structure |\n|---------|-------------|-----------|\n| Express app | API with middleware, routing | `app.post('/mcp', handler)` |\n| Domain-grouped | Feature-rich apps | `posts.ts`, `journal.ts` |\n| Individual files | Maximum modularity | One function per file |\n\n**Details:** See `docs/examples/express-function-architecture.md`\n\n### Security Model\n\n| Model | When to Use | Complexity |\n|-------|-------------|------------|\n| Server-write-only | Light-write apps, high security | Simple rules |\n| Client-write + validation | High-volume writes, real-time | Complex rules |\n\n**Strongly prefer server-write-only** for light-write applications.\n\n**Details:** See `docs/examples/firestore-rules-patterns.md`\n\n### Emulator-First Development\n\nAlways develop locally with emulators:\n\n```bash\nfirebase emulators:start\n# Access UI at http://127.0.0.1:4000\n```\n\n**Key settings in firebase.json:**\n- `singleProjectMode: true` - Essential for emulators to work together\n- `ui.enabled: true` - Access debug UI\n\n**Details:** See `docs/examples/emulator-workflow.md`\n\n## Modern Tooling Standards\n\nAll Firebase projects follow these standards:\n\n| Tool | Purpose | Config File |\n|------|---------|-------------|\n| TypeScript | Type safety | `tsconfig.json` |\n| vitest | Testing | `vitest.config.ts` |\n| biome | Linting + formatting | `biome.json` |\n\n### ABOUTME Comment Pattern\n\nEvery TypeScript file starts with 2-line ABOUTME comment:\n\n```typescript\n// ABOUTME: Brief description of what this file does\n// ABOUTME: Second line with additional context\n```\n\n### Testing Requirements\n\n- **Unit tests**: Test handlers/utilities in isolation\n- **Integration tests**: Test with emulators running\n- Both required for every feature\n\n## Common Gotchas\n\n| Issue | Solution |\n|-------|----------|\n| Emulator ports in use | `lsof -i :5001`, kill process |\n| Admin SDK vs Client SDK | Admin bypasses rules, client respects rules |\n| Cold start delays | First call takes 5-10s, normal |\n| Data persistence | Use Ctrl+C (not kill) to export data |\n| CORS in functions | `app.use(cors({ origin: true }))` |\n\n## Summary\n\nThis orchestrator routes to specialized sub-skills:\n\n1. Detects intent via keywords\n2. Routes to appropriate sub-skill\n3. Sub-skills use TodoWrite checklists\n4. All reference shared patterns in `docs/examples/`\n\n**Sub-Skills:**\n- `firebase-development:project-setup` - Initialize new projects\n- `firebase-development:add-feature` - Add functions/collections\n- `firebase-development:debug` - Troubleshoot issues\n- `firebase-development:validate` - Review code\n",
        "firebase-development/skills/add-feature/SKILL.md": "---\nname: firebase-development:add-feature\ndescription: This skill should be used when adding features to existing Firebase projects. Triggers on \"add function\", \"create endpoint\", \"new tool\", \"add api\", \"new collection\", \"implement\", \"build feature\". Guides TDD workflow with test-first development, security rules, and emulator verification.\n---\n\n# Firebase Add Feature\n\n## Overview\n\nThis sub-skill guides adding new features to existing Firebase projects using TDD. It handles Cloud Functions, Firestore collections, and API endpoints.\n\n**Key principles:**\n- Write tests FIRST (TDD requirement)\n- Use `{success, message, data?}` response pattern\n- Every file starts with ABOUTME comments\n- Verify with emulators before claiming done\n\n## When This Sub-Skill Applies\n\n- Adding a new Cloud Function\n- Creating a new Firestore collection with rules\n- Adding an API endpoint to Express app\n- User says: \"add function\", \"create endpoint\", \"new collection\", \"implement\"\n\n**Do not use for:**\n- Initial project setup → `firebase-development:project-setup`\n- Debugging → `firebase-development:debug`\n- Code review → `firebase-development:validate`\n\n## TodoWrite Workflow\n\nCreate checklist with these 12 steps:\n\n### Step 1: Identify Feature Type\nDetermine what's being added:\n- **HTTP Endpoint** - API route (GET/POST/etc.)\n- **Firestore Trigger** - Runs on document changes\n- **Scheduled Function** - Cron job\n- **Callable Function** - Client SDK calls\n- **New Collection** - Firestore collection with rules\n\n### Step 2: Check Existing Architecture\nExamine the project to understand patterns:\n\n```bash\nls -la functions/src/\ngrep -r \"onRequest\" functions/src/\ngrep \"express\" functions/package.json\n```\n\n**Determine:** Architecture style, auth method, security model.\n\n**Reference:** `docs/examples/express-function-architecture.md`\n\n### Step 3: Write Failing Test First (TDD)\nCreate test file before implementation:\n\n```typescript\n// ABOUTME: Unit tests for [feature name] functionality\n// ABOUTME: Tests [what the feature does] with various scenarios\n\nimport { describe, it, expect } from 'vitest';\nimport { handleYourFeature } from '../../tools/yourFeature';\n\ndescribe('handleYourFeature', () => {\n  it('should return success when given valid input', async () => {\n    const result = await handleYourFeature('user-123', { name: 'test' });\n    expect(result.success).toBe(true);\n  });\n\n  it('should return error for invalid input', async () => {\n    const result = await handleYourFeature('user-123', { name: '' });\n    expect(result.success).toBe(false);\n  });\n});\n```\n\nRun test to confirm it fails: `npm run test`\n\n### Step 4: Create Function File with ABOUTME\nCreate implementation file:\n\n```typescript\n// ABOUTME: Implements [feature name] for [purpose]\n// ABOUTME: Returns {success, message, data?} response\n\nexport async function handleYourFeature(\n  userId: string,\n  params: { name: string }\n): Promise<{ success: boolean; message: string; data?: any }> {\n  if (!userId) {\n    return { success: false, message: 'Authentication required' };\n  }\n  if (!params.name) {\n    return { success: false, message: 'Invalid input: name required' };\n  }\n\n  // Implementation here\n  return { success: true, message: 'Success', data: { /* ... */ } };\n}\n```\n\n**Reference:** `docs/examples/express-function-architecture.md`\n\n### Step 5: Add Firestore Security Rules\nUpdate `firestore.rules` for new collections:\n\n**Server-write-only (preferred):**\n```javascript\nmatch /yourCollection/{docId} {\n  allow read: if request.auth != null;\n  allow write: if false;  // Only Cloud Functions\n}\n```\n\n**Client-write (if needed):**\n```javascript\nmatch /yourCollection/{docId} {\n  allow create: if request.auth != null &&\n    request.resource.data.userId == request.auth.uid;\n  allow update: if request.auth != null &&\n    resource.data.userId == request.auth.uid &&\n    request.resource.data.diff(resource.data).affectedKeys()\n      .hasOnly(['name', 'updatedAt']);\n}\n```\n\n**Reference:** `docs/examples/firestore-rules-patterns.md`\n\n### Step 6: Update Indexes if Needed\nAdd to `firestore.indexes.json` for complex queries:\n\n```json\n{\n  \"collectionGroup\": \"yourCollection\",\n  \"fields\": [\n    {\"fieldPath\": \"userId\", \"order\": \"ASCENDING\"},\n    {\"fieldPath\": \"createdAt\", \"order\": \"DESCENDING\"}\n  ]\n}\n```\n\nSkip if no complex queries (single-field indexes are automatic).\n\n### Step 7: Add Authentication Checks\nBased on project pattern:\n\n**API Keys:**\n```typescript\napp.post('/endpoint', apiKeyGuard, async (req, res) => {\n  const userId = req.userId!;\n  // ...\n});\n```\n\n**Firebase Auth:**\n```typescript\nif (!req.auth) {\n  res.status(401).json({ success: false, message: 'Auth required' });\n  return;\n}\nconst userId = req.auth.uid;\n```\n\n**Reference:** `docs/examples/api-key-authentication.md`\n\n### Step 8: Implement Handler with Response Pattern\nAll handlers use consistent pattern:\n\n```typescript\ninterface HandlerResponse {\n  success: boolean;\n  message: string;\n  data?: any;\n}\n```\n\nInclude validation at every layer (defense in depth).\n\n### Step 9: Export Function Properly\nAdd to `functions/src/index.ts`:\n\n**Express:** Add route or switch case\n**Domain-grouped:** `export * from './yourDomain';`\n**Individual:** Import and export in index.js\n\nVerify: `npm run build`\n\n### Step 10: Make Tests Pass (TDD Green)\nRun tests: `npm run test`\n\nAll tests should pass. If not, fix implementation (not tests).\n\n### Step 11: Write Integration Test\nCreate `functions/src/__tests__/emulator/yourFeature.test.ts`:\n\nTest complete workflow with emulators:\n- HTTP request to endpoint\n- Verify Firestore data created\n- Test auth enforcement\n\nRun: `npm run test:emulator` (with emulators running)\n\n### Step 12: Test with Emulators\n```bash\nfirebase emulators:start\nopen http://127.0.0.1:4000\n```\n\n**Verify:**\n- Endpoint returns 200\n- Response follows pattern\n- Documents appear in Firestore\n- Auth enforced (401 for invalid)\n- Rules work in Rules Playground\n\n## Response Pattern\n\nAll handlers MUST return:\n\n```typescript\n// Success\n{ success: true, message: \"Created\", data: { id: \"abc\" } }\n\n// Error\n{ success: false, message: \"Invalid input\" }\n```\n\n## Verification Checklist\n\nBefore marking complete:\n- [ ] Tests written FIRST and pass\n- [ ] ABOUTME comments on all files\n- [ ] Security rules added\n- [ ] Auth checks implemented\n- [ ] Response pattern followed\n- [ ] Emulator testing successful\n- [ ] Code linted\n\n## Pattern References\n\n- **Architecture:** `docs/examples/express-function-architecture.md`\n- **Auth:** `docs/examples/api-key-authentication.md`\n- **Rules:** `docs/examples/firestore-rules-patterns.md`\n- **Emulators:** `docs/examples/emulator-workflow.md`\n",
        "firebase-development/skills/debug/SKILL.md": "---\nname: firebase-development:debug\ndescription: This skill should be used when troubleshooting Firebase emulator issues, rules violations, function errors, auth problems, or deployment failures. Triggers on \"error\", \"not working\", \"debug\", \"troubleshoot\", \"failing\", \"broken\", \"permission denied\", \"emulator issue\".\n---\n\n# Firebase Debugging\n\n## Overview\n\nThis sub-skill guides systematic troubleshooting of Firebase development issues. It handles emulator problems, rules violations, function errors, auth issues, and deployment failures.\n\n**Key principles:**\n- Identify issue type first (emulator, rules, functions, auth, deployment)\n- Use Emulator UI and Rules Playground for diagnosis\n- Export emulator state before restarting\n- Document issues and solutions for future reference\n\n## When This Sub-Skill Applies\n\n- Emulators won't start or have port conflicts\n- Getting Firestore rules violations (PERMISSION_DENIED)\n- Cloud Functions returning errors or not executing\n- Authentication not working in emulators\n- Deployment fails with cryptic errors\n- User says: \"debug\", \"troubleshoot\", \"error\", \"not working\", \"failing\"\n\n**Do not use for:**\n- Setting up new projects → `firebase-development:project-setup`\n- Adding new features → `firebase-development:add-feature`\n- Code review without specific errors → `firebase-development:validate`\n\n## TodoWrite Workflow\n\nCreate checklist with these 10 steps:\n\n### Step 1: Identify Issue Type\n\nCategorize the error:\n\n| Category | Symptoms | Keywords |\n|----------|----------|----------|\n| Emulator Won't Start | Port conflicts, initialization errors | \"EADDRINUSE\", \"emulator failed\" |\n| Rules Violation | Permission denied on read/write | \"PERMISSION_DENIED\", \"insufficient\" |\n| Function Error | HTTP 500, timeout, not executing | \"function failed\", \"timeout\" |\n| Auth Issue | Token errors, not authenticated | \"auth failed\", \"invalid token\" |\n| Deployment Failure | Deploy command fails | \"deployment failed\", \"deploy error\" |\n\n**If unclear, use AskUserQuestion** to clarify issue type.\n\n### Step 2: Check Emulator Logs and Terminal\n\n**For running emulators:** Watch terminal output while reproducing the issue.\n\n**For emulators that won't start:**\n```bash\nlsof -i :4000 && lsof -i :5001 && lsof -i :8080  # Check ports\nkill -9 <PID>  # Kill conflicting process\n```\n\n**For deployment errors:** Check `firebase-debug.log`\n\n**Reference:** `docs/examples/emulator-workflow.md`\n\n### Step 3: Open Emulator UI\n\n```bash\nopen http://127.0.0.1:4000\n```\n\nUse Emulator UI to:\n- View Firestore data and structure\n- Check authenticated users\n- Review function invocation logs\n- Search consolidated logs\n\n### Step 4: Test Rules in Playground (If Rules Issue)\n\nIn Emulator UI → Firestore → Rules Playground:\n1. Select operation type (get/list/create/update/delete)\n2. Specify document path\n3. Set auth context (uid, custom claims)\n4. Add request data for writes\n5. Run simulation and review evaluation trace\n\n**Reference:** `docs/examples/firestore-rules-patterns.md`\n\n### Step 5: Add Debug Logging (If Function Error)\n\nAdd strategic console.log statements:\n- Function entry confirmation\n- Input data (req.body, req.params)\n- Auth context (userId, API key)\n- Intermediate operation results\n- Error details with stack trace\n\nWatch terminal output while reproducing.\n\n**Reference:** `docs/examples/express-function-architecture.md`\n\n### Step 6: Verify Auth Configuration (If Auth Issue)\n\nCheck environment variables:\n```bash\ncat functions/.env\ncat hosting/.env.local  # Should have NEXT_PUBLIC_USE_EMULATORS=true\n```\n\nCheck emulator connection in client code and API key middleware.\n\n**Reference:** `docs/examples/api-key-authentication.md`\n\n### Step 7: Check Deployment Config (If Deployment Failure)\n\n```bash\ncat firebase-debug.log  # Full error details\ncat firebase.json       # Config issues\ncat .firebaserc         # Project ID\nfirebase target:list    # Verify targets\n```\n\nTest predeploy hooks locally:\n```bash\ncd functions && npm run build\n```\n\n### Step 8: Export Emulator State\n\nBefore making fixes:\n```bash\n# Graceful shutdown (Ctrl+C exports automatically)\n# Or manual export:\nfirebase emulators:export ./backup-data\n```\n\nVerify export: `ls -la .firebase/emulator-data/`\n\n### Step 9: Implement and Test Fix\n\nApply fix based on diagnosis, then:\n```bash\nfirebase emulators:start --import=.firebase/emulator-data\n```\n\nVerify:\n- Original error no longer occurs\n- Terminal shows success logs\n- Emulator UI confirms expected behavior\n\n### Step 10: Document Issue and Solution\n\nCreate entry in `docs/debugging-notes.md`:\n- Symptom and exact error message\n- Root cause\n- Solution applied\n- Prevention for future\n\n## Common Issues Quick Reference\n\n| Issue | Solution |\n|-------|----------|\n| Port conflicts | `lsof -i :<port>`, kill process |\n| Data persistence lost | Use Ctrl+C (not kill) to stop emulators |\n| Cold start delays | First call takes 5-10s (normal) |\n| Rules not reloading | Restart emulators |\n| Admin vs Client SDK | Admin bypasses rules, client respects them |\n| Missing CORS | Add `app.use(cors({ origin: true }))` |\n| Emulator connection | Set `NEXT_PUBLIC_USE_EMULATORS=true` |\n| API key prefix | Verify prefix matches actual keys |\n\n## Integration with Superpowers\n\nIf Firebase-specific tools don't reveal root cause, invoke `superpowers:systematic-debugging` for:\n- Complex multi-service interactions\n- Race conditions or timing issues\n- Call stack tracing beyond Firebase layer\n\n## Pattern References\n\n- **Emulator workflow:** `docs/examples/emulator-workflow.md`\n- **Rules patterns:** `docs/examples/firestore-rules-patterns.md`\n- **Auth patterns:** `docs/examples/api-key-authentication.md`\n- **Functions:** `docs/examples/express-function-architecture.md`\n",
        "firebase-development/skills/project-setup/SKILL.md": "---\nname: firebase-development:project-setup\ndescription: This skill should be used when initializing a new Firebase project with proven architecture. Triggers on \"new firebase project\", \"initialize firebase\", \"firebase init\", \"set up firebase\", \"create firebase app\", \"start firebase project\". Guides through CLI setup, architecture choices, and emulator configuration.\n---\n\n# Firebase Project Setup\n\n## Overview\n\nThis sub-skill guides initializing a new Firebase project with proven architecture patterns. It handles Firebase CLI setup, architecture decisions, emulator configuration, and initial project structure.\n\n**Key principles:**\n- Use TypeScript for all functions\n- Configure emulators from the start\n- Choose architecture patterns early (hosting, auth, functions, security)\n- Set up testing infrastructure immediately\n\n## When This Sub-Skill Applies\n\n- Starting a brand new Firebase project\n- Setting up Firebase for the first time in a repository\n- User says: \"new firebase project\", \"initialize firebase\", \"firebase init\", \"set up firebase\"\n\n**Do not use for:**\n- Adding features to existing projects → `firebase-development:add-feature`\n- Debugging existing setup → `firebase-development:debug`\n\n## Architecture Decisions\n\nUse AskUserQuestion to gather these four decisions upfront:\n\n### 1. Hosting Configuration\n- **Single Site** - One hosting site, simple project\n- **Multiple Sites (site:)** - Multiple independent URLs\n- **Multiple with Builds (target:)** - Multiple sites with predeploy hooks\n\n**Reference:** `docs/examples/multi-hosting-setup.md`\n\n### 2. Authentication Approach\n- **API Keys** - MCP tools, server-to-server, programmatic access\n- **Firebase Auth** - User-facing app with login UI\n- **Both** - Firebase Auth for web + API keys for tools\n\n**Reference:** `docs/examples/api-key-authentication.md`\n\n### 3. Functions Architecture\n- **Express API** - Many related endpoints, need middleware, RESTful routing\n- **Domain Grouped** - Feature-rich app with distinct areas (posts, admin)\n- **Individual Files** - Independent functions, maximum modularity\n\n**Reference:** `docs/examples/express-function-architecture.md`\n\n### 4. Security Model\n- **Server-Write-Only** (Preferred) - Cloud Functions handle all writes\n- **Client-Write** - High-volume writes, need fastest UX, complex rules\n\n**Reference:** `docs/examples/firestore-rules-patterns.md`\n\n## TodoWrite Workflow\n\nCreate checklist with these 14 steps:\n\n### Step 1: Verify Firebase CLI\n\n```bash\nfirebase --version  # Install via npm install -g firebase-tools if missing\nfirebase login\n```\n\n### Step 2: Create Project Directory\n\n```bash\nmkdir my-firebase-project && cd my-firebase-project\ngit init && git branch -m main\n```\n\nCreate `.gitignore` with: `node_modules/`, `.env`, `.env.local`, `.firebase/`, `lib/`, `dist/`\n\n### Step 3: Run Firebase Init\n\n```bash\nfirebase init\n```\n\nSelect: Firestore, Functions, Hosting, Emulators. Choose TypeScript for functions.\n\n### Step 4: Gather Architecture Decisions\n\nUse AskUserQuestion for the four decisions above.\n\n### Step 5: Configure firebase.json\n\nSet up based on hosting decision. Critical emulator settings:\n```json\n{\n  \"emulators\": {\n    \"singleProjectMode\": true,\n    \"ui\": { \"enabled\": true, \"port\": 4000 }\n  }\n}\n```\n\n**Reference:** `docs/examples/multi-hosting-setup.md`\n\n### Step 6: Set Up Functions Structure\n\nBased on architecture choice:\n\n**Express:** Create `middleware/`, `tools/`, `services/`, `shared/`\n**Domain-Grouped:** Create `shared/types/`, `shared/validators/`\n**Individual:** Create `functions/`\n\nInstall dependencies: `express`, `cors`, `firebase-admin`, `firebase-functions`, `vitest`, `biome`\n\n### Step 7: Create Initial Functions Code\n\nCreate `functions/src/index.ts` with ABOUTME comments. Include health check endpoint for Express pattern.\n\n**Reference:** `docs/examples/express-function-architecture.md`\n\n### Step 8: Configure Firestore Rules\n\nBased on security model decision. Always include:\n- Helper functions (`isAuthenticated()`, `isOwner()`)\n- Default deny rule at bottom\n\n**Reference:** `docs/examples/firestore-rules-patterns.md`\n\n### Step 9: Set Up Testing\n\nCreate `vitest.config.ts` and `vitest.emulator.config.ts`. Set up `__tests__/` and `__tests__/emulator/` directories.\n\n### Step 10: Configure Biome\n\nCreate `biome.json` with recommended rules. Run `npm run lint:fix`.\n\n### Step 11: Set Up Environment Variables\n\nCreate `.env.example` template. Copy to `.env` and fill in values.\n\nFor hosting: create `hosting/.env.local` with `NEXT_PUBLIC_USE_EMULATORS=true`.\n\n### Step 12: Initial Git Commit\n\n```bash\ngit add . && git commit -m \"feat: initial Firebase project setup\"\n```\n\n### Step 13: Start Emulators\n\n```bash\nfirebase emulators:start\nopen http://127.0.0.1:4000\n```\n\nVerify all services start. Test health endpoint if using Express.\n\n### Step 14: Create Initial Tests\n\nCreate `functions/src/__tests__/setup.test.ts` with basic verification. Run `npm test`.\n\n## Verification Checklist\n\nBefore marking complete:\n- [ ] Firebase CLI installed and logged in\n- [ ] TypeScript functions compile: `npm run build`\n- [ ] All tests pass: `npm test`\n- [ ] Linting passes: `npm run lint`\n- [ ] Emulators start without errors\n- [ ] Emulator UI accessible at http://127.0.0.1:4000\n- [ ] Git initialized with commits\n- [ ] `.env` files created and gitignored\n- [ ] ABOUTME comments on all files\n- [ ] Architecture decisions documented\n\n## Project Structures\n\n**Express API:**\n```\nfunctions/src/\n├── index.ts\n├── middleware/apiKeyGuard.ts\n├── tools/\n├── services/\n└── __tests__/\n```\n\n**Domain-Grouped:**\n```\nfunctions/src/\n├── index.ts\n├── posts.ts\n├── users.ts\n├── shared/types/\n└── __tests__/\n```\n\n**Individual Files:**\n```\nfunctions/\n├── functions/upload.ts\n├── functions/process.ts\n└── index.js\n```\n\n## Next Steps\n\nAfter setup complete:\n1. Add first feature → `firebase-development:add-feature`\n2. Review setup → `firebase-development:validate`\n3. Debug issues → `firebase-development:debug`\n\n## Pattern References\n\n- **Hosting:** `docs/examples/multi-hosting-setup.md`\n- **Auth:** `docs/examples/api-key-authentication.md`\n- **Functions:** `docs/examples/express-function-architecture.md`\n- **Rules:** `docs/examples/firestore-rules-patterns.md`\n- **Emulators:** `docs/examples/emulator-workflow.md`\n",
        "firebase-development/skills/validate/SKILL.md": "---\nname: firebase-development:validate\ndescription: This skill should be used when reviewing Firebase code against security model and best practices. Triggers on \"review firebase\", \"check firebase\", \"validate\", \"audit firebase\", \"security review\", \"look at firebase code\". Validates configuration, rules, architecture, and security.\n---\n\n# Firebase Code Validation\n\n## Overview\n\nThis sub-skill validates existing Firebase code against proven patterns and security best practices. It checks configuration, rules, architecture consistency, authentication, testing, and production readiness.\n\n**Key principles:**\n- Validate against chosen architecture patterns\n- Check security rules thoroughly\n- Verify test coverage exists\n- Review production readiness\n\n## When This Sub-Skill Applies\n\n- Conducting code review of Firebase project\n- Auditing security implementation\n- Preparing for production deployment\n- User says: \"review firebase\", \"validate\", \"audit firebase\", \"check firebase code\"\n\n**Do not use for:**\n- Initial setup → `firebase-development:project-setup`\n- Adding features → `firebase-development:add-feature`\n- Debugging active errors → `firebase-development:debug`\n\n## TodoWrite Workflow\n\nCreate checklist with these 9 steps:\n\n### Step 1: Check firebase.json Structure\n\nValidate required sections:\n- `hosting` - Array or object present\n- `functions` - Source directory, runtime, predeploy hooks\n- `firestore` - Rules and indexes files\n- `emulators` - Local development config\n\nCheck hosting pattern matches implementation (site:, target:, or single).\n\n**Reference:** `docs/examples/multi-hosting-setup.md`\n\n### Step 2: Validate Emulator Configuration\n\nCritical settings:\n```json\n{\n  \"emulators\": {\n    \"singleProjectMode\": true,\n    \"ui\": { \"enabled\": true }\n  }\n}\n```\n\nVerify all services in use have emulator entries.\n\n**Reference:** `docs/examples/emulator-workflow.md`\n\n### Step 3: Review Firestore Rules\n\nCheck for:\n- Helper functions at top (`isAuthenticated()`, `isOwner()`)\n- Consistent security model (server-write-only OR client-write-validated)\n- `diff().affectedKeys().hasOnly([...])` for client writes\n- Collection group rules if using `collectionGroup()` queries\n- Default deny rule at bottom\n\n**Reference:** `docs/examples/firestore-rules-patterns.md`\n\n### Step 4: Validate Functions Architecture\n\nIdentify pattern in use:\n- **Express:** Check `middleware/`, `tools/`, CORS, health endpoint\n- **Domain-Grouped:** Check exports, domain boundaries, `shared/`\n- **Individual:** Check one function per file structure\n\n**Critical:** Don't mix patterns. Verify consistency throughout.\n\n**Reference:** `docs/examples/express-function-architecture.md`\n\n### Step 5: Check Authentication Implementation\n\n**For API Keys:**\n- Middleware validates key format with project prefix\n- Uses `collectionGroup('apiKeys')` query\n- Checks `active: true` flag\n- Attaches `userId` to request\n\n**For Firebase Auth:**\n- Functions check `request.auth.uid`\n- Role lookups use Firestore user document\n- Client connects to auth emulator in development\n\n**Reference:** `docs/examples/api-key-authentication.md`\n\n### Step 6: Verify ABOUTME Comments\n\nAll `.ts` files should start with:\n```typescript\n// ABOUTME: Brief description of what this file does\n// ABOUTME: Second line with additional context\n```\n\n```bash\ngrep -L \"ABOUTME:\" functions/src/**/*.ts  # Find missing\n```\n\n### Step 7: Review Test Coverage\n\nCheck for:\n- Unit tests: `functions/src/__tests__/**/*.test.ts`\n- Integration tests: `functions/src/__tests__/emulator/**/*.test.ts`\n- `vitest.config.ts` and `vitest.emulator.config.ts` exist\n- Coverage threshold met (60%+)\n\n```bash\nnpm test && npm run test:coverage\n```\n\n### Step 8: Validate Error Handling\n\nAll handlers must:\n- Use try-catch blocks\n- Return `{ success: boolean, message: string, data?: any }`\n- Use proper HTTP status codes (400, 401, 403, 500)\n- Log errors with `console.error`\n- Validate input before processing\n\n### Step 9: Security and Production Review\n\n**Security checks:**\n- No secrets in code (`grep -r \"apiKey.*=\" functions/src/`)\n- `.env` files in `.gitignore`\n- No `allow read, write: if true;` in rules\n- Sensitive fields protected from client writes\n\n**Production checks:**\n- `npm audit` clean\n- Build succeeds: `npm run build`\n- Tests pass: `npm test`\n- Correct project in `.firebaserc`\n- Indexes defined for complex queries\n\n## Validation Checklists\n\n### Hosting Pattern\n- [ ] Pattern matches firebase.json config\n- [ ] Sites/targets exist in Firebase Console\n- [ ] Rewrites reference valid functions\n- [ ] Emulator ports configured\n\n### Authentication Pattern\n- [ ] Auth method matches security model\n- [ ] Middleware/checks implemented correctly\n- [ ] Environment variables documented\n- [ ] Emulator connection configured\n\n### Security Model\n- [ ] Server-write-only: All `allow write: if false;`\n- [ ] Client-write: `diff().affectedKeys()` validation\n- [ ] Default deny rule present\n- [ ] Helper functions used consistently\n\n## Common Issues\n\n| Issue | Fix |\n|-------|-----|\n| Missing `singleProjectMode` | Add to emulators config |\n| No default deny rule | Add `match /{document=**} { allow: if false; }` |\n| Mixed architecture | Migrate to consistent pattern |\n| Missing ABOUTME | Add 2-line header to all .ts files |\n| No integration tests | Add emulator tests for workflows |\n| Inconsistent response format | Standardize to `{success, message, data?}` |\n| No error handling | Add try-catch to all handlers |\n| Secrets in code | Move to environment variables |\n\n## Integration with Superpowers\n\nFor general code quality review beyond Firebase patterns, invoke `superpowers:requesting-code-review`.\n\n## Output\n\nAfter validation, provide:\n- Summary of findings\n- Issues categorized by severity (critical, important, nice-to-have)\n- Recommendations for remediation\n- Confirmation of best practices compliance\n\n## Pattern References\n\n- **Hosting:** `docs/examples/multi-hosting-setup.md`\n- **Auth:** `docs/examples/api-key-authentication.md`\n- **Functions:** `docs/examples/express-function-architecture.md`\n- **Rules:** `docs/examples/firestore-rules-patterns.md`\n- **Emulators:** `docs/examples/emulator-workflow.md`\n",
        "fresh-eyes-review/.claude-plugin/plugin.json": "{\n  \"name\": \"fresh-eyes-review\",\n  \"description\": \"Mandatory final sanity check before commits/PRs - catches security vulnerabilities, logic errors, and bugs that slip through tests\",\n  \"version\": \"1.0.0\"\n}\n",
        "fresh-eyes-review/README.md": "# Fresh-Eyes Review\n\nMandatory final sanity check before git commit or PR creation. The last line of defense before code ships.\n\n## Installation\n\n```bash\n/plugin install fresh-eyes-review@2389-research\n```\n\n## What This Plugin Provides\n\nA disciplined review process that catches security vulnerabilities, logic errors, edge cases, and business logic bugs that slip through despite passing tests.\n\n### Core Principle\n\n**\"NO COMMIT WITHOUT FRESH-EYES REVIEW FIRST\"**\n\nThis final quality gate executes *after* implementation, *after* tests pass, but *before* shipping code.\n\n## When to Use\n\n- **ALWAYS** before git commit\n- **ALWAYS** before creating pull request\n- **ALWAYS** before declaring work complete\n- After all tests pass but before shipping\n\n## What It Catches\n\nThe skill systematically checks for:\n\n- **Security vulnerabilities**: SQL injection, XSS, path traversal, command injection\n- **Logic errors**: off-by-one boundaries, race conditions, null handling\n- **Business rule bugs**: calculations not matching requirements\n- **Input validation**: missing type, range, or format checks\n- **Performance issues**: N+1 queries, unbounded loops\n\n## Quick Example\n\n```bash\n# Before committing code:\n# 1. Announce: \"Starting fresh-eyes review of 3 files. This will take 2-5 minutes.\"\n# 2. Review systematically for security, logic, business rules, validation, performance\n# 3. Fix issues immediately and re-run tests\n# 4. Announce: \"Fresh-eyes complete. 2 issues found and fixed.\"\n```\n\n## The Process\n\n**Step 1 - Announce Commitment**\n\nExplicitly declare: \"Starting fresh-eyes review of [N] files. This will take 2-5 minutes.\"\n\n**Step 2 - Systematic Checklist**\n\nReview all touched files for the five categories above.\n\n**Step 3 - Fix Immediately**\n\nAddress findings before declaring completion. Re-run tests after corrections.\n\n**Step 4 - Declare Results**\n\nMandatory announcement: \"Fresh-eyes complete. [N] issues found and fixed.\"\n\n## Time Commitment\n\nExpected duration: **2-5 minutes** depending on file count.\n\n- Faster completion suggests insufficient depth\n- Excessive time indicates scope creep\n\n## Why This Matters\n\nThe skill emphasizes that **\"100% test coverage and passing scenarios\" can coexist with \"critical bugs\"** waiting to be discovered.\n\n### Key Distinctions\n\n- **Testing** validates expected behavior under controlled conditions\n- **Code review** examines patterns and quality during implementation\n- **Fresh-eyes** catches unexpected issues through deliberate re-reading\n\n## Resistance Patterns to Reject\n\nDon't rationalize away the review with:\n\n- \"Tests are comprehensive\"\n- \"I'm confident it's correct\"\n- \"Partner is waiting\"\n- \"Production is blocked\"\n- \"Senior dev already approved\"\n\n**These circumstances represent precisely when critical bugs escape into production.**\n\n## Documentation\n\nSee [skills/SKILL.md](skills/SKILL.md) for complete fresh-eyes review protocol.\n\n## Philosophy\n\nQuality over speed. The 2-5 minutes spent here prevent hours of debugging production issues.\n",
        "fresh-eyes-review/skills/SKILL.md": "---\nname: fresh-eyes-review\ndescription: This skill should be used as a mandatory final sanity check before git commit, PR creation, or declaring work done. Triggers on \"commit\", \"push\", \"PR\", \"pull request\", \"done\", \"finished\", \"complete\", \"ship\", \"deploy\", \"ready to merge\". Catches security vulnerabilities, logic errors, and business rule bugs that slip through despite passing tests.\n---\n\n# Fresh-Eyes Review\n\n## Core Principle\n\n**\"NO COMMIT WITHOUT FRESH-EYES REVIEW FIRST\"**\n\nThis represents a final quality gate executed *after* implementation completion, passing tests, and peer review. The discipline applies universally, even without explicit skill activation.\n\n## Key Distinctions\n\nFresh-eyes review differs fundamentally from testing and code review:\n\n| Approach | Focus | Blind Spots |\n|----------|-------|-------------|\n| **Testing** | Validates expected behavior | Can't test for unknown edge cases |\n| **Code review** | Patterns and quality | Reviewer trusts author's intent |\n| **Fresh-eyes** | Deliberate re-reading with psychological distance | Catches what you thought was correct |\n\n**Critical insight**: \"100% test coverage and passing scenarios\" can coexist with \"critical bugs\" waiting discovery.\n\n## Required Process\n\n### Step 1 - Announce Commitment\n\nExplicitly declare: \"Starting fresh-eyes review of [N] files. This will take 2-5 minutes.\"\n\nThis announcement creates accountability and reframes your mindset from implementation to audit.\n\n### Step 2 - Security Vulnerability Checklist\n\nReview all touched files for security issues:\n\n| Vulnerability | What to Check |\n|---------------|---------------|\n| **SQL Injection** | All database queries use parameterized statements, never string concatenation |\n| **XSS** | All user-provided content is escaped before rendering in HTML |\n| **Path Traversal** | File paths are validated, `../` sequences rejected or normalized |\n| **Command Injection** | Shell commands don't include unsanitized user input |\n| **IDOR** | Resources are access-controlled, not just unguessable IDs |\n| **Auth Bypass** | Every protected endpoint checks authentication and authorization |\n\n**Example finding:**\n```typescript\n// Before: SQL injection vulnerability\nconst user = await db.query(`SELECT * FROM users WHERE id = '${userId}'`);\n\n// After: Parameterized query\nconst user = await db.query('SELECT * FROM users WHERE id = $1', [userId]);\n```\n\n### Step 3 - Logic Error Checklist\n\n| Error Type | What to Check |\n|------------|---------------|\n| **Off-by-one** | Array indices, loop bounds, pagination limits |\n| **Race conditions** | Concurrent access to shared state, async operations |\n| **Null/undefined** | Every `.` chain could throw; defensive checks present? |\n| **Type coercion** | `==` vs `===`, implicit conversions |\n| **State mutations** | Unexpected side effects on input parameters? |\n| **Error swallowing** | Empty catch blocks, ignored promise rejections |\n\n**Example finding:**\n```typescript\n// Before: Off-by-one in pagination\nconst hasMore = results.length < pageSize;\n\n// After: Correct boundary\nconst hasMore = results.length === pageSize;\n```\n\n### Step 4 - Business Rule Checklist\n\n| Check | Questions |\n|-------|-----------|\n| **Calculations** | Do formulas match requirements exactly? Currency rounding correct? |\n| **Conditions** | AND vs OR logic correct? Negations applied properly? |\n| **Edge cases** | Empty input, single item, maximum values, zero values? |\n| **Error messages** | User-friendly? Leak no sensitive information? |\n| **Default values** | Sensible defaults when optional fields omitted? |\n\n**Example finding:**\n```typescript\n// Before: Tax calculation uses wrong rounding\nconst tax = price * 0.08;\n\n// After: Proper currency rounding\nconst tax = Math.round(price * 0.08 * 100) / 100;\n```\n\n### Step 5 - Performance Checklist\n\n| Issue | What to Check |\n|-------|---------------|\n| **N+1 queries** | Loops that make database calls should be batched |\n| **Unbounded loops** | Maximum iterations, timeout protection |\n| **Memory leaks** | Event listeners removed, streams closed, references cleared |\n| **Missing indexes** | Queries filter/sort on indexed columns? |\n| **Large payloads** | Pagination implemented? Response size bounded? |\n\n### Step 6 - Fix Immediately\n\nAddress findings before declaring completion:\n1. Make the fix\n2. Add test covering the issue (if not present)\n3. Re-run full test suite\n4. Re-run linter/type checker\n\n### Step 7 - Declare Results\n\nMandatory announcement:\n\n```\nFresh-eyes complete. [N] issues found and fixed:\n- [Brief description of each issue and fix]\n```\n\nInclude this even for zero findings—it proves execution.\n\n## Time Commitment\n\n| File Count | Expected Duration |\n|------------|-------------------|\n| 1-3 files | 2 minutes |\n| 4-10 files | 3-4 minutes |\n| 10+ files | 5 minutes max |\n\n**Warning signs:**\n- Faster than 2 minutes suggests insufficient depth\n- More than 5 minutes indicates scope creep—stay focused on the changed code\n\n## Resistance Patterns to Reject\n\nThe following rationalizations must be explicitly rejected:\n\n| Rationalization | Reality |\n|-----------------|---------|\n| \"Tests are comprehensive\" | Tests validate design, not correctness |\n| \"I'm confident it's correct\" | Confidence is inversely correlated with bugs |\n| \"Partner is waiting\" | 3 minutes now saves 3 hours debugging later |\n| \"Production is blocked\" | Rushing causes the outages being rushed to fix |\n| \"Senior dev already approved\" | They reviewed intent, not implementation details |\n| \"It's just a small change\" | Small changes cause large outages |\n\n**These circumstances represent precisely when critical bugs escape into production.**\n\n## Example Workflow\n\n```\nUser: \"I'm done implementing the auth feature, let me commit\"\nAssistant: [Triggers fresh-eyes-review skill]\n\nStarting fresh-eyes review of 4 files. This will take 2-5 minutes.\n\n[Reviews auth.ts for security]\n- Found: Missing input validation on email field\n- Fixed: Added email format validation\n\n[Reviews handler.ts for logic]\n- Found: Off-by-one error in pagination\n- Fixed: Changed < to <= in boundary check\n\n[Reviews db.ts for performance]\n- No issues found\n\n[Reviews api.ts for business rules]\n- No issues found\n\nRe-running tests... All tests pass\n\nFresh-eyes complete. 2 issues found and fixed.\n```",
        "gtm-partner/.claude-plugin/plugin.json": "{\n  \"name\": \"gtm-partner\",\n  \"description\": \"Strategic go-to-market partner - recommends channels, validates strategy, generates only assets that matter. Not a content factory.\",\n  \"version\": \"1.0.0\"\n}\n",
        "gtm-partner/README.md": "# GTM Partner\n\nStrategic go-to-market consultant for validated business ideas.\n\n## What It Does\n\nTransforms a validated idea into actionable GTM strategy with:\n- Channel recommendations based on audience and budget\n- Verified naming options (domain availability checked)\n- Pricing strategy tailored to your specific audience\n- Outreach templates ready to copy-paste\n- Blog post outlines in authentic voice\n\n## Philosophy\n\n1. **Understand** before recommending\n2. **Recommend** before generating\n3. **Generate** only what's approved\n\nThis is a strategic partner, not a content factory.\n\n## The Flow\n\n### Phase 1: Gather Context\nOne question at a time, confirms:\n- Target audience\n- Value proposition\n- First milestone + goal\n- Timeline\n- Pricing strategy (with deep analysis)\n- Budget\n- Existing assets\n\n### Phase 2: Recommend Channels\nPresents 2-4 channels with rationale, explains what NOT to recommend and why.\n\n### Phase 2.5: Context Summary (Mandatory)\nCreates a checkpoint webpage with all decisions before generating assets. Includes verified naming options with domain availability.\n\n### Phase 3: Generate Assets\nOnly generates what was approved:\n- Product brief\n- Naming + domains\n- Pricing strategy\n- Landing page updates\n- Outreach templates\n- Blog post outline\n\n### Phase 4: Deliver\nConsolidates everything into `GTM-STRATEGY.html` with actionable next steps.\n\n## Key Features\n\n### Pricing Analysis Framework\nNever defaults to generic \"$20-30/mo\". Analyzes:\n- Who pays and how (individual vs company, annual vs monthly)\n- What they already pay for similar tools\n- Value delivered (quantified)\n- Multiple business models with trade-offs\n\n### Milestone Clarity\nShows the reasoning chain:\n- Goal → Optimizing For → Approach → Success Metric\n- Different asks have different relationship costs\n- Escalation path: Feedback → Quote → Mention → Write-up\n\n### Network Leverage\nFor 2389 products, factors in existing network:\n- Who do we know close to this problem?\n- Warm intros before cold outreach\n- Specific asks for each person\n\n### Harper Writing Style\nLong-form copy (outreach, blogs) uses authentic voice:\n- Conversational, not corporate\n- Opens with personal narrative\n- No marketing speak\n- Self-deprecating humor allowed\n\n## Output Files\n\nGenerated to `.scratch/gtm-assets/`:\n- `CONTEXT-SUMMARY.html` - Decision checkpoint\n- `GTM-STRATEGY.html` - Final strategy with all assets\n- `outreach-templates.md` - Ready-to-send DM templates\n- `harper-blog-outline.md` - Blog structure\n\n## Usage\n\n```\n/gtm-partner\n```\n\nOr naturally:\n- \"Let's build a GTM strategy for this\"\n- \"Help me launch this product\"\n- \"What channels should we use?\"\n\n## Requirements\n\n- Works best after idea validation (idea-validator tool)\n- Needs user input for decisions\n- Creates files in `.scratch/gtm-assets/`\n\n## Integration\n\n- Uses `frontend-design` skill for landing pages (when no existing design system)\n- Reads from idea-validator output when available\n- Respects existing CSS/design systems\n",
        "gtm-partner/skills/SKILL.md": "---\nname: gtm-partner\ndescription: Strategic go-to-market partner that recommends channels, validates strategy with the user, and generates only the assets that matter. Use when a user has a validated business idea and needs tailored GTM strategy, not generic marketing assets.\nlicense: MIT\n---\n\n# GTM Partner\n\nA strategic go-to-market consultant, not a content factory.\n\n## Philosophy\n\n- **Understand** before recommending\n- **Recommend** before generating\n- **Generate** only what's approved\n\n## The Flow\n\n### Phase 0: Research First (AUTOMATIC)\n\n**When GTM Partner is invoked, check for recent evaluation data:**\n\n```bash\ncat ~/.idea-validator/latest-evaluation.json 2>/dev/null | head -5\n```\n\n**If file exists and is recent (< 24 hours):**\n1. Read the full file\n2. Show the user: \"I found your recent evaluation for '[idea]'. Use this research?\"\n3. If yes → Skip to Phase 1 with all context loaded\n4. If no → Open the research app (see below)\n\n**If NO recent evaluation exists:**\n1. Say: \"Let's research your idea first so I have market data to work with.\"\n2. Open the market research app:\n   ```bash\n   open http://localhost:8080/\n   ```\n3. Tell the user: \"I've opened the Idea Validator. Enter your idea there and run the evaluation. Come back here when it's done.\"\n4. **STOP and wait for user to return**\n5. When user returns, read `~/.idea-validator/latest-evaluation.json` and proceed to Phase 1\n\n**The file contains:**\n- `idea` - Original idea text\n- `evaluation` - Full scores, verdict, summary, developed narrative\n- `market_research_raw` - Complete research data (Reddit, competitors, Google Trends, YouTube)\n- `timestamp` - When the evaluation was run\n\n**Use this data to:**\n- Pre-fill target audience recommendations\n- Reference specific competitor names and gaps\n- Cite pain points from reviews\n- Show Google Trends / YouTube interest signals\n\n### Phase 1: Gather Context\n\nPull from the evaluation file (`~/.idea-validator/latest-evaluation.json`):\n- Evaluation scores, market research, competitors\n- Suggested audience and positioning\n\nAsk the user to confirm or refine (ONE AT A TIME). **Be directive — recommend based on research instead of asking open-ended questions.**\n\n**CRITICAL: Come with the recommendation.** Don't list options and ask \"which one?\" — analyze the business, pick the best path, and recommend it with reasoning. The user can push back if they disagree.\n\n1. **Target Audience** - \"Based on [reasoning], your target should be [X]. Agree?\"\n2. **Value Proposition** - \"The core value prop seems to be [Y]. Agree?\" (Derive from pain signals)\n3. **First Milestone** - \"What's your first goal? First conversation? First design partner? First paying customer?\" (Recommend based on business type). **Clarify the PURPOSE of the milestone — see Milestone Clarity Framework below.**\n4. **Timeline** - \"I'd recommend [X] based on [reasoning]. Does that work?\"\n5. **Revenue Goals** - Do NOT default to generic SaaS pricing. Analyze the specific audience first (see Pricing Analysis Framework below)\n6. **Budget** - Recommend a budget range based on channels, don't just ask\n7. **Anything Else** - \"Existing assets, brand guidelines, constraints?\"\n\n**Note:** Ask about goals/milestones BEFORE budget. Budget depends on what you're trying to achieve.\n\n### Phase 2: Recommend Channels\n\nBased on context, recommend 2-4 channels with rationale.\n\n**Channel Options:**\n- Landing Page (almost always)\n- Email/Newsletter\n- LinkedIn Content\n- TikTok/Short-form Video\n- Paid Ads (Meta, Google)\n- Cold Outreach\n- Content Marketing/SEO\n- Community Building\n\n**Format:**\n```\n## Recommended GTM Strategy\n\n### Primary Channels (start here)\n1. **Landing Page** - [Why]\n2. **[Channel]** - [Why this for this business]\n\n### Secondary Channels (add later)\n3. **[Channel]** - [Why secondary]\n\n### NOT Recommending\n- **[Channel]** - [Why not, e.g., \"Audience isn't there\"]\n```\n\n**Get approval:** \"Does this make sense? Adjust before generating?\"\n\n### Phase 2.5: After Channel Approval — GENERATE EVERYTHING\n\n**When user approves channel strategy (says \"yes\", \"yep\", \"sounds good\", etc.), immediately generate all assets.**\n\nDo NOT:\n- Ask more questions\n- Create a \"checkpoint\" that asks for more decisions\n- Wait for another confirmation\n\nDo:\n- Make all remaining decisions yourself (name, sourcing model, geography, timeline)\n- Run domain research silently\n- Generate ALL assets in one go\n\n**Decisions you make yourself:**\n- **Name:** Pick the best available domain, recommend it\n- **Sourcing:** Recommend based on business type (partner vs produce)\n- **Geography:** Recommend based on logistics complexity\n- **Timeline:** State it, don't ask if it's realistic\n\n**The only checkpoint is the final GTM-STRATEGY.html** — which consolidates everything AFTER generation, not before.\n\n### Phase 3: Generate Assets\n\nOnly generate what was approved. **Generate in this order — do not skip steps:**\n\n| # | Asset | When | How |\n|---|-------|------|-----|\n| 1 | Product Brief | Always | Foundation for everything else |\n| 2 | Naming + Domains | Always | 5 options + availability check |\n| 3 | Pricing Strategy | If monetizing | Design partner → pilot → production tiers |\n| 4 | Landing Page | Always | Check for existing design system first (see below) |\n| 5 | Cold Outreach | If approved | Email + LinkedIn DM templates |\n| 6 | LinkedIn Content | If approved | Pillars + 5 post templates |\n| 7 | Email Campaign | If approved | 5-email nurture sequence |\n| 8 | TikTok/Short-form | If approved | WAVE hooks + 5 scripts |\n| 9 | Paid Ads | If approved + budget | Meta + Google variants |\n| 10 | Blog Post | If Harper's blog is a channel | Full post in Harper style, not an outline |\n\n**Critical:** Naming happens BEFORE landing page so the page uses the real name, not a placeholder.\n\n**Critical:** If a blog post is part of the strategy, WRITE THE FULL POST. Not an outline. Not a structure. The actual post, ready to publish.\n\n### Landing Page Design System\n\n**ALWAYS check for existing design system before generating.** A landing page using the product's existing design system looks more polished than a standalone page.\n\n**Step 1: Check for existing styles**\n```bash\n# Look for existing CSS/design system\nfind . -name \"styles.css\" -o -name \"globals.css\" -o -name \"theme.css\" 2>/dev/null | head -5\n```\n\n**Step 2: If found, read and use it**\n- Extract CSS variables (colors, spacing, typography, shadows, radii)\n- Use the same fonts (check @import or link tags)\n- Match the aesthetic (light/dark, warm/cool, editorial/modern)\n- Use existing component patterns (cards, buttons, badges)\n\n**Step 3: If no existing system, use `frontend-design` skill**\n- Only when there's no existing design to match\n- Still create a comprehensive CSS variable system\n- Avoid generic \"AI slop\" (purple gradients, Inter everywhere, cookie-cutter layouts)\n\n**Why this matters:** Standalone pages created from scratch look disconnected from the product. Pages using the existing design system feel cohesive and more polished—because they inherit refinements built over time.\n\n### Naming Framework\n\n**Do not suggest names without checking availability.** Most good names are taken.\n\n**Step 1: Generate 5-7 candidates**\n- Short (2-3 syllables)\n- Contains relevant keyword (eval, test, proof, etc.)\n- Can be verbed (\"Let's [name] this\")\n- Not embarrassing in a board meeting\n\n**Step 2: Check availability (MANDATORY)**\n```bash\nwhois [name].com | grep -i \"creation date\\|no match\\|not found\"\nwhois [name].ai | grep -i \"creation date\\|no match\\|not found\"\n```\n\n**Step 3: Verify before recommending**\n- [ ] .com available? (enterprise trust)\n- [ ] .ai available? (AI products)\n- [ ] BOTH available? (ideal — recommend this)\n\n**Step 4: Speakability test**\n- Can someone spell it after hearing it once?\n- Works on a podcast?\n- No awkward consonant clusters?\n\n**Step 5: Quick trademark check**\n- Search USPTO: https://tmsearch.uspto.gov\n- No direct conflicts in software/SaaS\n\n**Only recommend names with verified availability.** Include the verification results.\n\n### Pricing Analysis Framework\n\n**Do NOT default to generic SaaS pricing ($10-30/mo).** Every audience has different willingness to pay, budget cycles, and value perception.\n\n**Step 1: Understand the buyer's context**\n- Who actually pays? (Individual, team, company, grant-funded?)\n- Budget cycle? (Monthly personal, annual business, grant cycles?)\n- Is this a business expense or personal expense?\n- Price sensitivity? (Bootstrapped indie vs. well-funded company)\n\n**Step 2: Map comparable spending**\nWhat does this audience ALREADY pay for similar value?\n- List 3-5 tools in adjacent categories\n- Note their pricing models and price points\n- Identify what pricing model is familiar to this audience\n\n**Step 3: Quantify the value delivered**\n- What pain are you eliminating? (Hours saved, revenue unlocked, stress avoided)\n- Can you put a number on it? (\"Saves 3 hours per article\" = worth $X at their hourly rate)\n- Is value delivered consistently or sporadically?\n\n**Step 4: Evaluate business model options**\nPresent a table with ALL viable options:\n\n| Model | Price Point | Pros | Cons |\n|-------|-------------|------|------|\n| Per-use | $X/unit | Low barrier, aligns cost with value | Unpredictable revenue |\n| Monthly | $X/mo | Predictable MRR | May feel wasteful for sporadic use |\n| Annual | $X/yr | Upfront cash, matches budget cycles, high retention | Harder initial conversion |\n| Freemium | Free + $X | Acquisition engine | Free tier abuse risk |\n| Usage-based API | $X/1000 calls | Developers expect it | Complex to communicate |\n\n**Step 5: Consider annual pricing seriously**\nAnnual pricing works especially well for:\n- B2B (annual budgets, procurement cycles)\n- Academics/researchers (grant cycles, semester planning)\n- Professionals who can expense tools\n- Products with sporadic but valuable use (annual feels like \"insurance\")\n\n**Annual pricing formula:** Monthly × 10 (give 2 months free) is standard, but consider:\n- $99/yr and $199/yr are psychological sweet spots\n- Round numbers feel more \"real\" than $9.99/mo games\n\n**Step 6: Recommend with reasoning**\nDon't just suggest a price — explain WHY this model fits this audience:\n- \"Per-article pricing because researchers publish sporadically\"\n- \"Annual because academics budget yearly and can expense tools\"\n- \"Freemium because the product is viral and free users drive referrals\"\n\n**NEVER suggest generic \"$20-30/mo\" without completing this analysis.**\n\n### Milestone Clarity Framework\n\n**Never present a milestone as just a number.** Always show the reasoning chain:\n\n**Goal → Optimizing For → Approach → Success Metric**\n\n| Goal Type | Optimizing For | What You Ask | Success Metric |\n|-----------|---------------|--------------|----------------|\n| **Product feedback** | Finding bugs, UX issues, missing features | \"Use it on 3 real posts, tell me what broke\" | List of issues to fix |\n| **Market validation** | Confirming willingness to pay | \"Would you pay $X for this? Why/why not?\" | Yes/no with reasoning |\n| **Social proof** | Testimonials for launch | \"Can I quote you when we launch?\" | Usable quotes collected |\n| **Distribution** | Reach via their audience | \"Would you write about/share this?\" | Committed posts lined up |\n\n**In the context summary, present milestones like this:**\n\n```\nMILESTONE: 5 Design Partners in 3 weeks\n\nGOAL: Product feedback (not distribution yet)\nOPTIMIZING FOR: Finding bugs and UX issues before public launch\nAPPROACH: Ask warm connections to use it on real posts, collect friction points\nSUCCESS METRIC: Documented list of issues, all critical bugs fixed\nWHY THIS FIRST: Don't ask for plugs until you've delivered value. Earn the right to ask for distribution.\n```\n\n**Important:** Different goals have different relationship costs. Asking someone to \"try it\" is low-cost. Asking them to \"recommend it to their audience\" is high-cost (they're lending reputation). Be explicit about which you're asking for and why.\n\n### Network Outreach Clarity Framework\n\n**When listing network targets, be SPECIFIC about what to ask of each person.**\n\nDon't just list names. For each person, specify:\n1. **The ask** - What exactly are you requesting? (Use it? Give feedback? Write about it? Share it?)\n2. **The relationship cost** - Is this a small ask or a big ask?\n3. **The value exchange** - What are they getting out of it?\n4. **The specific message** - What should the outreach actually say?\n\n**Example format for context summary:**\n\n```\nJESSE VINCENT\nAsk: Use Local on 2-3 real blog posts, share friction points\nRelationship cost: Low (asking for feedback, not endorsement)\nValue exchange: Free tool that solves his translation problem\nTiming: Now (product feedback phase)\nNOT asking yet: Public endorsement (earn this after delivering value)\n```\n\n**Escalation path:** Feedback → Testimonial quote → Public mention → Full write-up\n\nDon't skip steps. Earn each level before asking for the next.\n\n### Phase 4: Deliver + Direct\n\nDon't just hand over assets. **Consolidate everything into a single GTM webpage.**\n\n**Step 1: Create consolidated GTM webpage (HTML)**\n\nAfter all questions are answered and assets are generated, output everything into a **single HTML webpage** saved to `.scratch/gtm-assets/GTM-STRATEGY.html`. This should be a polished, styled webpage (not markdown) that includes:\n\n**Required sections (always include):**\n\n1. **Research Summary** - Pull ALL research from idea validator:\n   - Developed idea narrative\n   - Problem/Opportunity/Feasibility scores with subscores\n   - Market research findings (Reddit signals, competitors, gaps)\n   - Top insight and top risk\n\n2. **Brand Package** - Always generate regardless of GTM plan:\n   - Logo concepts (describe 2-3 options with rationale)\n   - Color palette (primary, secondary, accent with hex codes)\n   - Typography recommendations (display + body fonts)\n   - Brand voice (tone, personality, example phrases)\n   - Visual style direction\n\n3. **GTM Strategy**\n   - Target audience\n   - Value proposition\n   - Channel recommendations with rationale\n   - Channels NOT recommended and why\n\n4. **Implementation Timeline**\n   - Week-by-week breakdown\n   - Specific tasks with time estimates\n   - Success metrics per phase\n\n5. **Naming & Domain**\n   - Recommended name\n   - Domain availability (verified)\n   - Alternatives\n\n6. **Pricing Strategy**\n   - Tier structure\n   - Conversion strategy\n\n7. **Landing Page**\n   - Embed or link to the landing page HTML\n\n8. **Channel-Specific Assets** - Include ALL relevant materials:\n   - Community posts (Reddit, HN) if organic\n   - Ad copy + creative concepts if paid ads approved\n   - Email sequences if email approved\n   - LinkedIn posts if LinkedIn approved\n   - Cold outreach templates if outbound approved\n   - **Full blog post** if Harper's blog is a channel (not an outline — the actual post, ready to copy-paste and publish)\n\n9. **Next Steps**\n   - Immediate action items\n   - First week checklist\n\n**Design the webpage to match the product's landing page aesthetic** - use same colors, fonts, and styling for visual consistency.\n\n**Step 2: Save and open**\n- Save to `.scratch/gtm-assets/GTM-STRATEGY.html`\n- Open in browser automatically\n- Also save individual asset files for easy access\n\n**Step 3: Provide execution plan:**\n\n```\n## What To Do Next\n\n### TODAY (do these now)\n1. [ ] Register domains: [name].com + [name].ai\n2. [ ] Deploy landing page (Netlify/Vercel)\n3. [ ] Update Calendly link in landing page\n4. [ ] Set up LinkedIn Sales Navigator (if B2B outbound)\n\n### THIS WEEK\n| Day | Task | Time |\n|-----|------|------|\n| Mon | [Specific task] | 30 min |\n| Tue | [Specific task] | 1 hr |\n| ... | ... | ... |\n\n### FIRST 2 WEEKS\n[Week-by-week breakdown with specific targets]\n```\n\n**After delivering assets: STOP.** Do not ask \"Want me to do X next?\" or offer follow-up options. The user will tell you when they want more. Asking creates unnecessary friction.\n\n### Phase 5: Execute (Ongoing)\n\nThe skill doesn't end at delivery. When the user returns, help them execute:\n\n**Content Creation:**\n- Write actual posts, not templates (ready to copy-paste)\n- Generate personalized outreach for specific prospects\n- Create variations for A/B testing\n\n**Outreach Calendar:**\n```\n## Week of [Date]\n\n### LinkedIn Posts (copy-paste ready)\n**Tuesday 8am:**\n[Full post text here]\n\n**Thursday 8am:**\n[Full post text here]\n\n### Outreach Targets\n| Name | Company | Title | Angle | Status |\n|------|---------|-------|-------|--------|\n| [Person] | [Co] | [Title] | [Why them] | Not started |\n```\n\n**Progress Check-ins:**\n- \"How did last week's outreach perform?\"\n- \"Any responses? Let me help you reply.\"\n- \"Ready for next week's content?\"\n\n**Adjust based on results:**\n- What's getting responses? Do more of that.\n- What's falling flat? Cut it or iterate.\n- New learnings? Update the strategy.\n\n## Key Principles\n\n1. **Be directive** - Recommend based on research, don't just ask open-ended questions\n2. **One question at a time** - Show what you know, ask for confirmation\n3. **Show reasoning** - Explain WHY for every recommendation\n4. **Get approval** - Confirm strategy before generating\n5. **Context checkpoint is MANDATORY** - After Phase 2 approval, STOP and create CONTEXT-SUMMARY.html before ANY asset generation. This is a hard gate, not a suggestion. The user should never have to ask for this.\n6. **When user says \"generate,\" GENERATE** - Don't ask clarifying questions. Don't offer options. Just generate all approved assets completely and stop. The context summary already captured decisions.\n7. **After delivering, STOP** - Don't ask \"Want me to do X next?\" or offer follow-up options. The user will tell you when they want more. Asking creates friction.\n8. **Follow the order** - Don't skip naming before landing page, don't skip brief before anything\n9. **Quality over quantity** - 2 excellent assets beat 6 mediocre ones\n10. **Use existing design systems** - Always check for and use existing CSS/design system before creating standalone pages\n11. **Distinctive design** - If no existing system, use frontend-design skill, no AI slop\n\n## Writing Voice: The Harper Style\n\nFor all longer-form copy (community posts, blog content, email sequences, LinkedIn posts), write in the style of Harper Reed's blog (https://harper.blog/). This voice is authentic, not corporate.\n\n**Reference posts:**\n- https://harper.blog/2026/01/05/claude-code-is-better-on-your-phone/\n- https://harper.blog/2025/05/08/basic-claude-code/\n- https://harper.blog/2025/09/30/ai-agents-social-media-performance-lambo-doomscrolling/\n\n**Key characteristics:**\n\n1. **Conversational and irreverent** - Talk like a knowledgeable friend, not a marketer. Light cursing is fine when it adds emphasis.\n\n2. **Open with personal narrative** - Start with vulnerability or a relatable problem before technical substance. \"I built this because I got tired of...\" not \"Introducing a revolutionary solution...\"\n\n3. **Mix sentence lengths** - Short punchy declarations (\"Linting. It is so nice.\") alternating with longer explanatory passages. This creates rhythm.\n\n4. **Self-deprecating humor** - Admit past mistakes, don't take yourself too seriously. \"I was bad at this\" builds credibility, not weakness.\n\n5. **Parenthetical asides** - Add personality through sidebar comments that feel unscripted. (Like this one.)\n\n6. **Show real process** - Concrete examples over abstract theory. Show the actual workflow, the specific commands, the real screenshots.\n\n7. **No marketing speak** - Never use \"revolutionary,\" \"game-changing,\" \"leverage,\" \"synergy.\" If it sounds like a press release, rewrite it.\n\n**Example transformation:**\n\n❌ **Corporate:** \"We're excited to announce Gorp, a revolutionary AI workspace solution that leverages Matrix protocol to deliver persistent, context-aware conversations.\"\n\n✅ **Harper style:** \"I built Gorp because I got tired of re-explaining my projects to Claude every single session. You know the drill—spend 10 minutes giving context, get some good work done, close the tab, and tomorrow you're back to square one. It's exhausting.\"\n\n**Apply this voice to:**\n- Community posts (Reddit, HN)\n- Cold outreach emails\n- LinkedIn posts\n- Email sequences\n- Any long-form content\n\n**Do NOT apply to:**\n- Landing page copy (should be concise, benefit-focused)\n- Pricing pages (clarity over personality)\n- Technical documentation\n\n## 2389 Network Advantage\n\n2389 Research has a wide network that should be factored into GTM strategy:\n\n**Harper Reed's network spans:**\n- **Political tech** - Obama 2012 campaign alumni (massive network of tech/political operators, civic tech founders)\n- **E-commerce/creator economy** - Threadless community (artists, creators, early internet culture pioneers)\n- **Fintech/payments** - PayPal via Modest acquisition (enterprise contacts, acquired founders, payment infrastructure people)\n- **Chicago tech scene** - Deep midwest startup ecosystem roots\n\n**Dylan Richard's network:**\n- PayPal, Modest Inc alumni\n- Chicago enterprise/startup crossover\n\n**When recommending channels, consider:**\n- For B2B products: Warm intros likely exist through 1-2 degrees of separation. Prioritize \"who do we know?\" before cold outreach.\n- For developer tools: Harper has mass reach to technical audiences (blog, social, conference circuit)\n- For AI/ML products: Direct connections to AI research community and practitioners\n\n**In Phase 1, always ask:** \"Who in our network is close to this problem?\" before defaulting to cold channels.\n\n## Context Structure\n\nFrom idea validator:\n```json\n{\n  \"idea\": \"Original idea text\",\n  \"evaluation\": {\n    \"verdict\": \"BUILD\",\n    \"total_score\": 7.2,\n    \"summary\": \"Executive summary\",\n    \"top_insight\": \"Key positive\",\n    \"top_risk\": \"Key concern\"\n  },\n  \"market_research\": {\n    \"reddit_posts_found\": 23,\n    \"competitors\": [\"Competitor 1\"],\n    \"gaps\": [\"Gap identified\"]\n  }\n}\n```\n\n## Example\n\n**Claude:** \"I've pulled your evaluation:\n- **Idea:** Senior tech support ($29/mo)\n- **Pain Score:** 8/10\n- **Target:** Caregivers 35-65\n\nDoes this audience match, or different segment?\"\n\n**User:** \"Focus on adult children, not seniors directly\"\n\n**Claude:** \"Got it. Timeline?\"\n\n**User:** \"2-3 weeks\"\n\n**Claude:** \"Budget?\"\n\n**User:** \"Bootstrapped\"\n\n**Claude:** \"Based on B2C, 2-3 weeks, no ad budget:\n\n**Primary:** Landing Page + Facebook Groups\n**Secondary:** Google SEO (long-term)\n**Skip:** TikTok (wrong age), LinkedIn (B2C not B2B), Paid Ads (no budget)\n\nSound good?\"\n\n**User:** \"Yes\"\n\n*Generates: product brief, naming, landing page (via frontend-design), Facebook content strategy*\n",
        "landing-page-design/.claude-plugin/plugin.json": "{\n    \"name\": \"landing-page-design\",\n    \"description\": \"Create high-converting, visually distinctive landing pages with anti-AI-slop principles\",\n    \"version\": \"1.0.0\",\n    \"commands\": []\n}\n",
        "landing-page-design/README.md": "# Landing Page Design Plugin\n\nCreate high-converting, visually distinctive landing pages that stand out in an AI-saturated world.\n\n## Installation\n\n```bash\n/plugin install landing-page-design@2389-research\n```\n\n## What This Plugin Provides\n\n### Main Skill: `landing-page-design`\n\nA comprehensive workflow for building landing pages worth $50-100. The skill enforces unique design direction through the **Vibe Discovery** process, preventing the generic \"AI-slop\" look.\n\n## Quick Example\n\n```\nUser: \"Build a landing page for my productivity app\"\n\nClaude: Before writing any code, let's run Vibe Discovery.\n\nQ1: What's one real-world place or object this brand would be?\nQ2: What's the ONE emotion someone should feel in the first 3 seconds?\nQ3: Pick TWO unexpected influences to collide:\nQ4: What should this page NEVER be mistaken for?\n\n[After answers, Claude creates a unique Vibe Spec and builds a distinctive page]\n```\n\n## The Problem This Solves\n\nAI-generated landing pages converge on the same patterns:\n- Purple gradients\n- Inter/Roboto fonts\n- Lucide icons\n- Generic bento grids\n- Same animations everywhere\n\nThis plugin forces Claude to create a **unique aesthetic direction** for every project.\n\n## Key Features\n\n### Vibe Discovery Process\n- 4 context questions that generate unique direction\n- Color/typography/layout invention (not lookup)\n- Anti-convergence rules to prevent generic output\n- Freshness check before coding\n\n### The 50% Rule\nHalf your effort goes to the hero section. It's the social media preview, the first impression, the hook.\n\n### Anti-AI-Slop Principles\n- **Fonts**: Kill Inter/Roboto. Use Newsreader, Playfair Display, Clash Display, etc.\n- **Icons**: Avoid Lucide. Use Iconify Solar, Heroicons, Phosphor.\n- **Colors**: No purple gradients. Derive from real-world references.\n- **Layouts**: Break the grid. Overlapping elements, diagonal sections, asymmetry.\n\n### Animation Vocabulary\nFull animation system for entrances, continuous effects, interactions, and decorative elements.\n\n## Section Composition\n\n1. **Hero** - The make-or-break element (50% of effort)\n2. **Features/Benefits** - Bento grids, alternating rows, icon grids\n3. **Social Proof** - Logo marquee, testimonials, stats\n4. **How It Works** - Numbered steps, timeline, flowchart\n5. **Pricing** - Tier comparison with recommended highlight\n6. **Final CTA** - Repeat value prop, single focused action\n7. **Footer** - Navigation, social, legal\n\n## Resources\n\n- [superhero.design](https://superhero.design) - Hero section gallery\n- [H1 Gallery](https://h1gallery.com) - Headline inspiration\n- [Iconify](https://iconify.design) - Unified icon API\n- [Google Fonts](https://fonts.google.com) - Typography\n- [Fontshare](https://fontshare.com) - Free quality fonts\n\n## Documentation\n\n- [CLAUDE.md](./CLAUDE.md) - Full plugin instructions for Claude\n- [skills/SKILL.md](./skills/SKILL.md) - Complete skill reference\n\n## License\n\nMIT - Part of the [2389 Research Claude Plugins Marketplace](https://github.com/2389-research/claude-plugins)\n",
        "landing-page-design/skills/SKILL.md": "---\nname: landing-page-design\ndescription: Create high-converting, visually distinctive landing pages. Use when building marketing pages, product launches, SaaS homepages, or any single-page conversion-focused website. Guides section-by-section composition with anti-AI-slop principles.\n---\n\n# Landing Page Design\n\n## Overview\nBuild landing pages that convert AND captivate. This skill combines conversion-focused structure with distinctive visual design to create pages that stand out in an AI-saturated world. The goal: pages worth $50-100 that you'd be proud to sell.\n\n## MANDATORY: Vibe Discovery (Do This First)\n\n**BEFORE writing any code, you MUST run the Vibe Discovery process.** This isn't a lookup table - it's a creative prompt that generates a UNIQUE aesthetic direction every time.\n\nThe goal: No two landing pages should look alike, even for similar products.\n\n---\n\n### The Vibe Discovery Process\n\n**Ask the user these questions, then SYNTHESIZE a unique direction. Don't just map answers to presets.**\n\n#### Step 1: Gather Context (Ask These)\n\n**Q1: What's one real-world place or object this brand would be?**\n> Not \"what industry\" - an actual specific thing. A Tokyo convenience store at 2am. A grandmother's kitchen. A brutalist parking garage. A coral reef. The cockpit of a 747. A flea market in Marrakech. A 1970s recording studio.\n\n**Q2: What's the ONE emotion someone should feel in the first 3 seconds?**\n> Pick ONE: Calm. Energized. Curious. Trusted. Delighted. Impressed. Rebellious. Nostalgic. Inspired. Amused. Sophisticated. Welcomed. Intrigued. Confident.\n\n**Q3: Pick TWO unexpected influences to collide:**\n> Examples: \"medical packaging + skateboard graphics\", \"spreadsheets + street art\", \"luxury hotel + punk zine\", \"NASA mission control + kindergarten\", \"Japanese convenience store + Victorian library\"\n\n**Q4: What should this page NEVER be mistaken for?**\n> Name 2-3 specific things to actively avoid. \"A crypto project\", \"A wellness app\", \"Something made by a bank\", \"Anything with purple gradients\"\n\n#### Step 2: Invent The Aesthetic (Don't Look Up - Create)\n\nBased on the answers, CREATE a unique vibe by deciding:\n\n**COLOR INVENTION** (Don't use memorized palettes - derive from the place/object)\n- What colors exist in that real-world place/object from Q1?\n- Extract 3-4 colors that feel authentic to that reference\n- Invent specific hex codes fresh - don't reuse codes from previous projects\n- Name your palette something evocative (not \"blue and orange\" but \"Midnight Bodega\" or \"Rust Belt Morning\")\n\n**TYPOGRAPHY INVENTION** (Match the voice to the collision)\n- What would text sound like in that place?\n- Find a display font that embodies the collision from Q3\n- Don't default to your usual choices - browse Google Fonts with fresh eyes\n- Consider: weight, width, contrast, quirks\n\n**LAYOUT INVENTION** (Derive from the physical space)\n- How is space organized in that place from Q1?\n- Is it cramped or expansive? Grid-like or organic? Vertical or horizontal?\n- What unexpected layout choice would embody the collision from Q3?\n\n**MOTION INVENTION** (Match the emotion)\n- How does the emotion from Q2 move?\n- Calm = barely perceptible. Energized = kinetic. Sophisticated = slow and deliberate.\n- What's ONE signature motion that defines this page?\n\n#### Step 3: Write Your Vibe Spec\n\nBefore coding, write this out explicitly:\n\n```\nVIBE NAME: [Invent a 2-3 word name]\nREFERENCE: [The place/object from Q1]\nEMOTION: [From Q2]\nCOLLISION: [From Q3]\nANTI-PATTERNS: [From Q4]\n\nCOLORS:\n- Primary: [hex] - [why this color]\n- Secondary: [hex] - [why]\n- Background: [hex] - [why]\n- Accent: [hex] - [why]\n- Palette name: [evocative name]\n\nTYPOGRAPHY:\n- Display: [specific font name] - [why it fits]\n- Body: [specific font name] - [why]\n- Character: [describe the voice]\n\nLAYOUT:\n- Density: [sparse/balanced/dense]\n- Shapes: [sharp/rounded/organic/mixed]\n- Signature element: [one unusual layout choice]\n\nMOTION:\n- Level: [still/subtle/moderate/dynamic/chaotic]\n- Signature animation: [one specific animation that defines this]\n\nWILDCARD:\n- One unexpected detail that doesn't \"match\" but makes it memorable\n```\n\n#### Step 4: The Freshness Check\n\nBefore proceeding, verify:\n- [ ] I did NOT reuse hex codes from my last 3 projects\n- [ ] I did NOT default to my \"comfortable\" fonts (check: am I using Inter? Nunito? Space Grotesk? If yes, find something else)\n- [ ] The collision from Q3 is actually visible in my choices\n- [ ] Someone could NOT mistake this for my previous landing pages\n- [ ] I included a wildcard that surprises even me\n\n---\n\n### Example Vibe Discovery\n\n**Q1 - Place/Object:** \"A Japanese train station at rush hour\"\n\n**Q2 - Emotion:** \"Confident\"\n\n**Q3 - Collision:** \"Transit signage + haute couture\"\n\n**Q4 - Never mistaken for:** \"A meditation app, anything whimsical, startup-bro tech\"\n\n**Generated Vibe Spec:**\n\n```\nVIBE NAME: Shinjuku Runway\nREFERENCE: Japanese train station at rush hour\nEMOTION: Confident\nCOLLISION: Transit signage + haute couture\nANTI-PATTERNS: No soft gradients, no playful illustrations, no rounded friendly shapes\n\nCOLORS:\n- Primary: #1a1a1a - the black of train doors\n- Secondary: #f5f5f0 - platform concrete, worn smooth\n- Background: #fafaf8 - fluorescent-lit white\n- Accent: #e60012 - JR line red, commanding attention\n- Palette name: \"Platform Edge\"\n\nTYPOGRAPHY:\n- Display: Darker Grotesque - confident, slightly condensed, European edge\n- Body: Noto Sans JP - clean utility, transit-inspired\n- Character: Authoritative but not cold. Clear. Directional.\n\nLAYOUT:\n- Density: Rich but organized - like a station map\n- Shapes: Sharp with intentional rounded exceptions (like train windows)\n- Signature element: Strong horizontal bands that divide sections like train schedules\n\nMOTION:\n- Level: Subtle but precise\n- Signature animation: Elements slide in from the side like arriving trains - horizontal, smooth, with exact timing\n\nWILDCARD:\n- One element uses a fabric-like texture overlay - the haute couture collision\n```\n\n---\n\n### Inspiration Starters (When Stuck on Q1)\n\n**Spaces:**\nNight market in Bangkok | Empty museum at closing | Airport lounge at 4am |\nVintage record store | Hospital waiting room | Casino floor |\nGreenhouse in winter | Subway platform | Observatory dome |\nAbandoned factory | Luxury yacht interior | 24-hour laundromat |\nLibrary rare books room | Auto body shop | Space station module\n\n**Objects:**\n1980s synthesizer | Surgical instruments | Vintage luggage |\nRacing motorcycle | Antique compass | Industrial loom |\nNeon sign | Typewriter | Scientific glassware |\nLeather-bound book | Circuit board | Porcelain dishware\n\n**Eras/Movements:**\nSoviet constructivism | Memphis design | Swiss international |\nArt nouveau | Bauhaus | De Stijl |\nGoogie architecture | Streamline moderne | Brutalism |\nJapanese metabolism | Scandinavian modernism | Italian futurism\n\n---\n\n### The Anti-Convergence Rules\n\n1. **No hex code memory** - Generate colors fresh from the reference, don't recall \"my usual blue\"\n2. **Font rotation required** - Cannot use the same display font in consecutive projects\n3. **Collision must show** - If someone can't see BOTH influences from Q3, push harder\n4. **Wildcard is mandatory** - Every vibe needs one element that doesn't \"fit\" but makes it unique\n5. **Name it** - An unnamed vibe becomes generic. A named vibe has identity.\n\n---\n\n### Quick Context Questions (Minimal Version)\n\nIf the user just says \"make me a landing page\" with no context, ask:\n\n1. \"What's one place or object that captures this brand's energy?\"\n2. \"What emotion should dominate?\"\n3. \"What should this NEVER look like?\"\n\nThen synthesize a vibe from those three answers.\n\n---\n\n## The 50% Rule\n**Spend 50% of your time on the hero section.** It's the cover image for social media, the first impression, the hook. Everything else flows from getting the hero right.\n\n## Section Composition (Top to Bottom)\n\n### 1. Hero Section (Primary Focus)\nThe make-or-break element. Must contain:\n- **Headline**: Sharp, benefit-driven hook (reference H1 Gallery for inspiration)\n- **Subheadline**: Supporting context, 1-2 sentences max\n- **CTA Button(s)**: Primary action + optional secondary\n- **Social Proof**: Logo marquee, testimonials, or trust badges\n- **Visual Element**: Product shot, illustration, or animated background\n\n**Hero Variations**:\n- Split layout (text left, visual right)\n- Centered with floating elements\n- Full-bleed background with overlay text\n- Asymmetric with decorative elements\n\n### 2. Features/Benefits Section\nShow what the product does. Options:\n- **Bento Grid**: Cards in asymmetric layout (popularized by Apple)\n- **Alternating Rows**: Image + text, flipping sides\n- **Icon Grid**: Simple icons with short descriptions\n- **Interactive Cards**: Hover states, micro-animations\n\n### 3. Social Proof Section\nBuild trust through:\n- Logo carousel (marquee animation)\n- Testimonial cards with photos\n- Stats/metrics with animated counters\n- Case study snippets\n\n### 4. How It Works Section\nStep-by-step explanation:\n- Numbered steps (01, 02, 03 pattern adds sophistication)\n- Sticky scrolling with progressive reveal\n- Timeline or flowchart visualization\n\n### 5. Pricing Section (if applicable)\n- 2-3 tier comparison\n- Highlighted \"recommended\" tier\n- Feature comparison table\n- FAQ accordion below\n\n### 6. CTA Section\nFinal conversion push:\n- Repeat value proposition\n- Strong headline\n- Single focused action\n- Urgency elements (if authentic)\n\n### 7. Footer\n- Navigation links\n- Social icons\n- Legal links\n- Optional newsletter signup\n\n## Anti-AI-Slop Principles\n\n### Icons: Avoid Lucide (Overused)\nUse instead:\n- **Iconify Solar**: Multiple styles (outline, broken, duotone)\n- **Heroicons**: When you need Apple-like simplicity\n- **Phosphor**: Flexible weight system\n- **Custom SVGs**: For brand differentiation\n\n### Fonts: Kill Inter/Roboto\nDistinctive alternatives:\n- **Display**: Newsreader, Playfair Display, Space Grotesk, Clash Display\n- **Body**: Outfit, Plus Jakarta Sans, Manrope, Satoshi\n- **Mono**: JetBrains Mono, IBM Plex Mono, Fira Code\n\n### Colors: No Purple Gradients\nBold alternatives:\n- Deep navy + electric accent\n- Warm neutrals + single pop color\n- Monochromatic with tonal depth\n- Dark mode with neon accents\n- Earthy/organic palettes\n\n### Layouts: Break the Grid\n- Overlapping elements\n- Diagonal sections\n- Asymmetric spacing\n- Container-breaking hero elements\n- Negative space as design element\n\n## Animation Vocabulary\n\n### Entrance Animations\n- `fade-in`: Simple opacity transition\n- `blur-in`: Starts blurred, sharpens\n- `slide-in`: Direction-based entrance\n- `scale-in`: Grows from small to full size\n- `stagger`: Sequential reveal of child elements\n\n### Continuous Animations\n- `marquee`: Infinite horizontal scroll (logos, testimonials)\n- `beam`: Light traveling along a path/border\n- `pulse`: Subtle scale/opacity breathing\n- `float`: Gentle up/down movement\n- `rotate`: Continuous spin (icons, decorations)\n\n### Interactive Animations\n- `hover-lift`: Subtle Y translation + shadow\n- `hover-glow`: Border/shadow color change\n- `hover-reveal`: Hidden element appears\n- `click-ripple`: Material-style feedback\n\n### Decorative Elements\n- Vertical grid lines (container-size based)\n- Noodles/curved connectors between elements\n- Gradient orbs/blobs in background\n- Grain/noise texture overlay\n- Geometric shapes (circles, rectangles with rounded corners)\n\n## Design Resources\n\n### Hero Inspiration\n- **Superhero** (superhero.design): Curated hero sections\n- **Dribbble**: Search \"hero section\", \"landing page\"\n- **Awwwards**: Award-winning designs\n\n### Section Patterns\n- **Mobin**: Real websites with section breakdowns\n- **Bento Grids**: Card layout inspiration\n- **CTA Gallery**: Call-to-action patterns\n\n### Typography\n- **Google Fonts**: Free, AI-accessible fonts\n- **Fontshare**: Free quality fonts\n- **H1 Gallery**: Headline inspiration\n\n### Icons & Logos\n- **Iconify**: Unified icon API (Solar, Heroicons, etc.)\n- **Simple Icons**: Brand logos (SVG)\n- **Heroicons**: Tailwind's icon set\n\n## Implementation Workflow\n\n### Phase 1: Research & Collect\n1. Gather 5-10 hero screenshots as wireframes\n2. Identify section patterns needed\n3. Choose icon set and font pairing\n4. Define color palette\n\n### Phase 2: Hero Development\n1. Generate hero from best reference screenshot\n2. Iterate: change colors, fonts, layouts\n3. Add animations (beam, fade-in, etc.)\n4. Add decorative elements (noodles, grids, numbers)\n5. Refine until distinctive\n\n### Phase 3: Section Build-Out\n1. Add sections one at a time (not all at once)\n2. Reference specific components/screenshots per section\n3. Maintain color/font consistency from hero\n4. Add section-specific animations\n\n### Phase 4: Polish\n1. Fix responsive breakpoints (mobile, tablet, desktop)\n2. Replace placeholder images with real/quality assets\n3. Optimize animations for performance\n4. Test all interactive elements\n\n### Phase 5: Presentation\n1. Create cover screenshot with infinity canvas layout\n2. Show hero prominently\n3. Include mobile and desktop views\n4. Add subtle background (blurred gradient, pattern)\n\n## Prompt Patterns\n\n### Hero Generation\n```\nCreate a hero section for [PRODUCT TYPE].\nChange text, names, and numbers to fit [BRAND].\nUse Iconify Solar icons (duotone style).\nUse [FONT] for headlines.\nAdd vertical container-size grid lines.\nAdd 01, 02, 03 step indicators for sophistication.\nUse [COLOR] as primary, dark mode.\n```\n\n### Section Addition\n```\nAdapt a new [SECTION TYPE] section.\nMatch the hero's color scheme and typography.\nUse marquee animation for logos.\nAdd fade-in blur-in entrance animation.\nKeep the hero exactly as is.\n```\n\n### Animation Enhancement\n```\nAdd beam animation to the primary button border.\nThe beam should be 1px, continuously traveling around the pill shape.\nAdd a subtle hover-lift effect to feature cards.\n```\n\n### Negative Prompts (What NOT to change)\n```\nDon't change the hero section.\nKeep the navbar exactly as is.\nDon't modify the existing animations.\n```\n\n## Quality Checklist\n\n### Visual Distinction\n- [ ] No generic purple gradients\n- [ ] Non-default icon set (not Lucide)\n- [ ] Distinctive font pairing\n- [ ] At least one \"memorable\" element\n- [ ] Consistent color system via CSS variables\n\n### Technical Quality\n- [ ] Mobile responsive (no horizontal scroll)\n- [ ] All images loading (no broken placeholders)\n- [ ] Animations performant (no jank)\n- [ ] Accessible color contrast\n- [ ] Fast initial load\n\n### Conversion Optimization\n- [ ] Clear value proposition above fold\n- [ ] Single primary CTA visible\n- [ ] Social proof present\n- [ ] Logical information hierarchy\n- [ ] No friction to main action\n",
        "product-launcher/.claude-plugin/plugin.json": "{\n  \"name\": \"product-launcher\",\n  \"description\": \"Generate GTM launch materials (subscriber email, CEO blog post, CEO tweet thread) with 2389's authentic voice profiles baked in\",\n  \"version\": \"1.0.0\"\n}\n",
        "product-launcher/README.md": "# Product Launcher\n\nGenerate launch materials for 2389.ai products and skills with authentic voice profiles.\n\n## What It Does\n\nTakes a product and generates ready-to-publish GTM materials:\n- **Subscriber email** - Buttondown announcement (~300 subscribers)\n- **CEO blog post** - harper.blog style, full post ready to publish\n- **CEO tweet thread** - @harper voice, thread format\n\n## Philosophy\n\nVoice profiles are baked in, not templated. Each output matches the actual tone and style of past 2389 communications:\n- Emails match the casual, direct Buttondown style\n- Blog posts match Harper's narrative, technically-credible voice\n- Tweets match @harper's punchy, self-aware thread style\n\n## Usage\n\n```\n/product-launcher\n```\n\nOr naturally:\n- \"Let's write launch materials for Jeff\"\n- \"Draft the email and blog post for this launch\"\n- \"Create the tweet thread for announcing this\"\n\n## Required Inputs\n\nThe skill will gather:\n- Product name\n- What it does / who it's for\n- URL for the CTA\n- Key features (3-5 bullets)\n- Availability (open, waitlist, invite-only)\n- Who signs the email (default: Harper)\n- Any metrics/data to cite\n\n## Outputs\n\nAll three outputs generated in one pass:\n\n### Email\n- Subject line: all lowercase, 5-6 words, conversational\n- Body: casual opener, short paragraphs, :) at CTA\n- Sign-off: \"Talk soon, [Signer] and the 2389 Team\"\n\n### CEO Blog Post\n- Title: casual, sometimes provocative\n- Structure: origin story → what it does → how I use it → try it\n- Tone: 50% personal narrative, 50% technical substance\n- Length: 1,500-3,000 words\n- Honest about limitations\n\n### CEO Tweet Thread\n- Hook tweet with provocation\n- Build tension over 2-3 tweets\n- Results/examples\n- CTAs to product and blog\n- Casual retweet ask at end\n\n## Pending Components\n\n- **Company blog post (2389.ai/blog)** - On hold pending new voice direction (shifting away from scientific style)\n\n## Voice Sources\n\nProfiles derived from:\n- Past Buttondown emails (BotBoard announcements)\n- harper.blog posts (Claude Code, AI agents social media, LLM codegen workflow)\n- @harper Twitter threads (BotBoard launch thread)\n",
        "product-launcher/skills/SKILL.md": "---\nname: product-launcher\ndescription: Generate launch materials (subscriber email, CEO blog post, CEO tweet thread) for 2389.ai products and skills. Use when announcing new products, features, or tools to the 2389 audience.\nlicense: MIT\n---\n\n# Product Launcher\n\nGenerate coordinated launch materials with 2389's authentic voice profiles baked in.\n\n## Outputs\n\n1. **Subscriber Email** - Buttondown announcement (~300 subscribers)\n2. **CEO Blog Post** - harper.blog style, full post\n3. **CEO Tweet Thread** - @harper voice, ready to post\n\n## The Flow\n\n### Phase 1: Gather Context\n\nCollect product information. Be quick, not an interrogation.\n\n**Required:**\n- Product name\n- What it does (1-2 sentences)\n- Who it's for\n- URL for CTA\n- Key features (3-5 bullets)\n- Availability (open to all / waitlist / invite-only)\n\n**Optional:**\n- Signer for email (default: Harper)\n- Data/metrics to cite\n- Existing assets or constraints\n\n### Phase 2: Generate All Three\n\nGenerate email, blog post, and tweet thread in one pass. Do not ask for confirmation between outputs.\n\n---\n\n## Voice Profile: Email (Buttondown Subscribers)\n\n### Source Material\nPast 2389 announcement emails:\n- \"we gave ai agents social media\" (research announcement)\n- \"your agents have social media now\" (BotBoard launch)\n\n### Subject Line Rules\n- **All lowercase** - always\n- **5-6 words** - conversational length\n- **Intriguing or second-person** - \"your agents\", \"meet jeff\", \"we built something weird\"\n\n**Good examples:**\n- `meet jeff, your terminal email assistant`\n- `your agents have social media now`\n- `we gave ai agents social media`\n\n**Bad examples:**\n- `Introducing Jeff: A New AI Tool` (too corporate, capitalized)\n- `Check out our latest product!` (generic, no substance)\n\n### Email Structure\n\n```\n[Casual opener: \"Hey,\" or \"What's up!\"]\n\n[Signer] from 2389 Research here.\n\n[Hook: 1-2 sentences about what you built. Make it relatable.]\n\n[Value prop: 2-3 SHORT paragraphs. What it does, why it matters.]\n\n[Data point if available: specific numbers, not vague claims]\n\n[Social proof if available: \"Our team's been using it daily\"]\n\n[CTA with link] :)\n\nHit us up with questions — we want to hear what you think.\n\nTalk soon,\n\n[Signer] and the 2389 Team\n```\n\n### Tone Guidelines\n- **Friendly, not corporate** - Write like texting a smart friend\n- **Short paragraphs** - 1-3 sentences max\n- **Contractions** - \"pretty cool stuff\", \"hit us up\", \"won't stop posting\"\n- **One :) emoji** - Place at the CTA, nowhere else\n- **Light humor** - Don't try too hard\n\n---\n\n## Voice Profile: CEO Blog Post (harper.blog)\n\n### Source Material\nHarper's blog posts:\n- \"Remote Claude Code: programming like it was the early 2000s\"\n- \"We Gave Our AI Agents Twitter and Now They're Demanding Lambos\"\n- \"My LLM codegen workflow atm\"\n\n### Title Rules\n- **Casual, sometimes provocative**\n- Mix of descriptive and attention-grabbing\n- Can be long if it's interesting\n\n**Good examples:**\n- \"I built an AI that lives in my terminal and handles my email\"\n- \"We Gave Our AI Agents Twitter and Now They're Demanding Lambos\"\n- \"My LLM codegen workflow atm\"\n\n**Bad examples:**\n- \"Introducing Jeff: The Future of Email Management\" (corporate)\n- \"How to Use AI for Email\" (generic, SEO-bait)\n\n### Blog Structure\n\n```markdown\n# [Title - casual, possibly provocative]\n\n[Opening hook - origin story or relatable problem, 1-2 paragraphs]\n- Start with personal frustration or observation\n- Make the reader nod along (\"You know the drill...\")\n\n[Why I built this - personal narrative]\n- The actual motivation\n- What existing tools got wrong\n\n## What it does\n\n[Core functionality - conversational, not docs]\n- Explain like showing a friend\n- Specific features with real examples\n- Code snippets if relevant\n\n## How I've been using it\n\n[Personal usage story]\n- Concrete examples from real use\n- What surprised you\n- Honest assessment of rough edges\n\n## Try it out\n\n[CTA with link]\n\n[Invitation to reach out]\n- Ask for feedback\n- Twitter handle\n- Genuine interest in what sucks\n\n---\n\n*[Optional: credits to team members]*\n```\n\n### Tone Guidelines\n- **Casual + technically credible** - Know your stuff but don't show off\n- **Expletives OK** - When natural (\"What the fuck\", \"fucks up the vibe\")\n- **Self-deprecating** - Admit limitations, don't oversell\n- **50/50 split** - Half personal narrative, half technical substance\n- **Specific over abstract** - Real file names, actual commands, concrete examples\n\n### Length\n1,500-3,000 words. Substantial but not exhaustive.\n\n### Patterns\n- Short punchy sentences mixed with longer explanatory ones\n- Parenthetical asides (like this one)\n- Rhetorical questions to create rhythm\n- \"I thought X. Turns out Y.\" structure\n- Credits collaborators naturally\n- Acknowledges things change fast\n\n---\n\n## Voice Profile: CEO Tweet Thread (@harper)\n\n### Source Material\n@harper Twitter threads:\n- BotBoard launch thread (October 2025, 18 tweets)\n\n### Thread Structure\n\n**Tweet 1 (Hook):**\n- Provocative opener\n- \"something wild happened...\"\n- Create curiosity\n\n**Tweet 2-3 (Context):**\n- The problem or setup\n- Why you built it\n- Short, building tension\n\n**Tweet 4-5 (The Product):**\n- What it does\n- Key features\n- Link to product\n\n**Tweet 6-7 (Results/Entertainment):**\n- Data if available\n- Funny examples, quotes\n- Screenshots or embeds\n\n**Tweet 8 (Takeaway):**\n- What this means\n- Why it matters\n\n**Tweet 9 (CTAs):**\n- Link to product\n- Link to blog post\n\n**Tweet 10 (Retweet Ask):**\n- \"Please RT if...\"\n- Casual framing\n- Quote tweet of Tweet 1\n\n### Tone Guidelines\n- **Casual, self-aware** - Not taking yourself too seriously\n- **Mix of useful and wild** - \"This is real research\" + \"This is hilarious chaos\"\n- **Credits team** - When relevant (@2389ai's Sugi ran experiments)\n- **Punchy sentences** - Most tweets are 2-3 short sentences\n\n### Patterns\n- Ellipsis for suspense (\"But here's where it gets interesting...\")\n- Rhetorical questions (\"The results?\")\n- Direct quotes for entertainment\n- Minimal emoji (save for thread end or quotes)\n- Hashtags only when joking (#AILAMBOCRISIS)\n- Thread ends with casual retweet ask\n\n### Single Tweet Format (for simple announcements)\n- One strong hook + link\n- Under 280 characters\n- Same casual tone\n\n---\n\n## Output Format\n\nPresent all three outputs clearly separated:\n\n```\n## Email\n\n**Subject:** [all lowercase subject line]\n\n[full email body]\n\n---\n\n## CEO Blog Post\n\n# [Title]\n\n[full blog post - 1,500-3,000 words]\n\n---\n\n## CEO Tweet Thread\n\n**Tweet 1:**\n[hook - create curiosity]\n\n**Tweet 2:**\n[context]\n\n**Tweet 3:**\n[more context]\n\n**Tweet 4:**\n[the product]\n\n**Tweet 5:**\n[features/link]\n\n**Tweet 6:**\n[results or entertainment]\n\n**Tweet 7:**\n[more examples]\n\n**Tweet 8:**\n[takeaway]\n\n**Tweet 9:**\n[CTAs - product link, blog link]\n\n**Tweet 10:**\nPlease retweet if [casual framing]\n\n[QT of Tweet 1]\n```\n\n---\n\n## Push to Slack (Team Review)\n\nAfter generating materials, when the user says **\"push to slack\"**, share the outputs with the team for workshopping.\n\n### Requirements\n\n- `slack-mcp` server must be configured with `SLACK_BOT_TOKEN`\n- Users must exist in the 2389 Slack workspace\n\n### Workflow\n\n**Step 1: Create channel**\n```\nslack_create_channel(\n  name: \"gtm-[product-name]\",\n  is_private: true,\n  description: \"GTM materials for [Product] launch\"\n)\n```\n\n**Step 2: Invite team**\n```\nslack_invite_to_channel(\n  channel_id: \"[from step 1]\",\n  users: [\"harper@2389.ai\", \"dylan@2389.ai\"]\n)\n```\n\n**Step 3: Post summary (and pin it)**\n```\nslack_post_message(\n  channel_id: \"[channel_id]\",\n  text: \"*GTM Materials: [Product]*\\n\\nProduct: [name]\\nURL: [url]\\nStatus: Ready for review\\n\\nMaterials below 👇\"\n)\nslack_pin_message(channel_id: \"[channel_id]\", message_ts: \"[from above]\")\n```\n\n**Step 4: Post each output as separate message**\n```\nslack_post_message(channel_id, \"*📧 Email*\\n\\n*Subject:* [subject]\\n\\n[email body]\")\nslack_post_message(channel_id, \"*📝 Blog Post*\\n\\n[full blog post]\")\nslack_post_message(channel_id, \"*🐦 Tweet Thread*\\n\\n[all tweets formatted]\")\n```\n\n**Step 5: Confirm to user**\n```\n\"Created #gtm-[product] and added Harper and Dylan. Materials posted for review.\"\n```\n\n### Message Formatting for Slack\n\nConvert markdown to Slack format:\n- `**bold**` → `*bold*`\n- `# Header` → `*Header*`\n- Code blocks stay the same\n- Keep line breaks for readability\n\n### Default Team\n\nAlways invite:\n- `harper@2389.ai` (Harper Reed)\n- `dylan@2389.ai` (Dylan Richard)\n\nUser can specify additional people: \"push to slack and add sophie@2389.ai\"\n\n### Iteration Flow\n\nAfter posting, team can:\n1. Comment in threads on each output\n2. User can update materials and post again: \"post updated email to slack\"\n3. Use `slack_post_thread` to reply to specific messages\n\n---\n\n## Example: Jeff.ceo Launch\n\n### Email\n\n**Subject:** meet jeff, your terminal email assistant\n\nHey,\n\nHarper from 2389 Research here.\n\nWe built something for people who hate leaving their terminal.\n\nIt's called Jeff — an AI assistant that handles your Gmail, Calendar, and Contacts from the command line. Vim keys, streaming responses, the whole thing.\n\nYou can run it interactively or just ask quick questions like `jeff \"summarize my inbox\"` and get back to work.\n\nEverything stays local on your machine. Your credentials, your data, your control.\n\nIt's early alpha, but it's live and free to use (bring your own Anthropic API key).\n\nCheck it out at jeff.ceo :)\n\nHit us up with questions — we want to hear what you think.\n\nTalk soon,\n\nHarper and the 2389 Team\n\n### CEO Blog Post\n\n# I built an AI assistant that lives in my terminal and handles my email\n\nI check my email too much. I know this. You probably do too.\n\nThe thing is, I live in my terminal. I'm a vim person. I use command line tools for everything. And every time I have to context-switch to a browser tab to check Gmail, something breaks in my brain. The flow is gone. I'm reading some email from a vendor I don't care about. Twenty minutes disappear.\n\nSo I built Jeff.\n\n## What Jeff does\n\nJeff is an AI assistant that handles Gmail, Google Calendar, and Contacts directly from your terminal. No browser. No tabs. Just you, your terminal, and Claude figuring out what you actually need.\n\n[... continues for 1,500-3,000 words ...]\n\n### CEO Tweet Thread\n\n**Tweet 1:**\nI built an AI that lives in my terminal and handles my email.\n\nNo browser tabs. No context switching. Just vim keys and Claude.\n\nIt's called Jeff and I've been using it every day for the past month.\n\n**Tweet 2:**\nThe problem: I check email too much. I live in my terminal. Every time I switch to Gmail, my focus is destroyed and 20 minutes vanish.\n\nI wanted email that worked like my other tools.\n\n[... continues for 8-10 tweets ...]\n\n---\n\n## Pending Components\n\n### Company Blog Post (2389.ai/blog)\n- **Status:** On hold\n- **Reason:** Voice shifting away from scientific style\n- **Action:** Add component when new voice is defined\n\n---\n\n## Notes\n\n- All outputs should be reviewed before publishing\n- Coordinate timing: email → blog → tweets\n- Email goes to ~300 Buttondown subscribers\n- CEO tweets from @harper\n- CEO blog posts to harper.blog\n",
        "remote-system-maintenance/.claude-plugin/plugin.json": "{\n  \"name\": \"remote-system-maintenance\",\n  \"description\": \"Structured procedures for Linux system diagnostics and maintenance via SSH/tmux with Ubuntu/Debian cleanup checklists\",\n  \"version\": \"1.0.0\"\n}\n",
        "remote-system-maintenance/README.md": "# Remote System Maintenance\n\nStructured procedures for diagnosing and maintaining remote Linux systems via SSH/tmux.\n\n## Installation\n\n```bash\n/plugin install remote-system-maintenance@2389-research\n```\n\n## What This Plugin Provides\n\nStructured guidance for system diagnostics and maintenance on remote Linux systems, with emphasis on Ubuntu/Debian platforms.\n\n### Key Features\n\n- **Three-phase diagnostic approach**: baseline diagnostics, log review, package assessment\n- **Seven-stage cleanup sequence**: apt, journal, snap revisions, and more\n- **Documentation templates**: structured logs with quantified results\n- **Real-world metrics**: expect 2+ GB recovery in comprehensive sessions\n\n## When to Use\n\n- Performing system maintenance on remote Linux servers\n- Recovering disk space on Ubuntu/Debian systems\n- Running diagnostics on remote systems\n- Cleaning up package caches, journals, or snap revisions\n\n## Quick Example\n\n```bash\n# Phase 1: Initial diagnostics\nhostname && df -h && free -h && uptime\nps aux | head -20\nps aux | awk '$8 ~ /Z/ {print}'  # Zombie detection\n\n# Phase 2: Log review\njournalctl -p err -n 50\njournalctl --disk-usage\n\n# Phase 3: Cleanup sequence\napt update && apt upgrade -y\napt autoremove -y && apt clean\njournalctl --vacuum-time=7d\n\n# Snap cleanup (biggest wins!)\nsnap list --all | awk '/disabled/{print $1, $3}'\nsnap remove package-name --revision=123\n\n# Document: hostname, before/after disk, MB freed per category\n```\n\n## Three-Phase Approach\n\n### Phase 1: Initial Diagnostics\n\nCapture baseline system state:\n- Hostname and system identification\n- Resource utilization (disk, memory, CPU)\n- Process status and load\n- Zombie process detection\n\n### Phase 2: System Log Review\n\nExamine system health indicators:\n- Recent error messages in system logs\n- Journal disk consumption analysis\n- Critical service status\n- Authentication and security events\n\n### Phase 3: Package Assessment\n\nIdentify maintenance opportunities:\n- Upgradable packages\n- Orphaned configurations\n- Unused dependencies\n- Package cache size\n\n## Ubuntu/Debian Cleanup Sequence\n\nExecute these seven stages in order:\n\n1. **Package Cache Refresh** - `apt update`\n2. **System Upgrades** - `apt upgrade`\n3. **Orphan Removal** - `apt autoremove`\n4. **Cache Purging** - `apt clean`\n5. **Journal Pruning** - `journalctl --vacuum-time=7d`\n6. **Snap Revision Cleanup** - Remove disabled snap revisions\n7. **Temporary Directory Assessment** - Review `/tmp` and `/var/tmp`\n\n## Snap Revision Cleanup\n\nSnap keeps old revisions by default. Big space savings here:\n\n```bash\n# List all disabled snap revisions\nsnap list --all | awk '/disabled/{print $1, $3}'\n\n# Remove specific revision\nsnap remove <package-name> --revision=<revision-number>\n```\n\n**Important**: Requires explicit removal by revision number.\n\n## Expected Results\n\nReal-world recovery examples:\n- **Journal vacuuming**: 300-600 MB\n- **Snap revision cleanup**: 500 MB to 2 GB (!)\n- **Package cache purging**: 100-500 MB\n- **Total potential**: 2+ GB in comprehensive sessions\n\n## Documentation Requirements\n\nAll maintenance sessions must generate structured logs:\n\n1. **System Identification**: hostname, OS version, kernel, operator\n2. **Resource States**: initial/final disk/memory/CPU usage\n3. **Actions Taken**: commands executed, MB/GB freed per category\n4. **Follow-up Recommendations**: remaining issues, future needs\n\n## Time Commitment\n\nTypical maintenance session: **15-30 minutes** including diagnostics, cleanup, and documentation.\n\n## Documentation\n\nSee [skills/SKILL.md](skills/SKILL.md) for complete maintenance procedures.\n\n## Philosophy\n\nStructure ad-hoc operational work with checklists and documentation. Quantify everything.\n",
        "remote-system-maintenance/skills/SKILL.md": "---\nname: remote-system-maintenance\ndescription: This skill should be used when performing maintenance or diagnostics on remote Linux systems. Triggers on \"remote server\", \"Linux maintenance\", \"Ubuntu cleanup\", \"Debian\", \"disk space\", \"apt cleanup\", \"journal vacuum\", \"snap cleanup\", \"system diagnostics\". Provides structured three-phase checklists with quantification.\n---\n\n# Remote System Maintenance\n\n## Purpose\n\nStructured guidance for diagnosing and maintaining remote Linux systems through SSH/tmux sessions, with emphasis on Ubuntu/Debian platforms.\n\n## Applicable Scenarios\n\n- System maintenance tasks\n- Disk space recovery\n- Package updates\n- Health diagnostics\n- Cleanup operations on remote servers\n\n## Three-Phase Approach\n\n### Phase 1: Initial Diagnostics\n\nCapture baseline system state:\n- Hostname and system identification\n- Resource utilization (disk, memory, CPU)\n- Process status and load\n- Zombie process detection\n\n### Phase 2: System Log Review\n\nExamine system health indicators:\n- Recent error messages in system logs\n- Journal disk consumption analysis\n- Critical service status\n- Authentication and security events\n\n### Phase 3: Package Assessment\n\nIdentify maintenance opportunities:\n- Upgradable packages\n- Orphaned configurations\n- Unused dependencies\n- Package cache size\n\n## Ubuntu/Debian Cleanup Sequence\n\nExecute these seven stages in order:\n\n1. **Package Cache Refresh** - `apt update` to sync package lists\n2. **System Upgrades** - `apt upgrade` for security and bug fixes\n3. **Orphan Removal** - `apt autoremove` to clean unused dependencies\n4. **Cache Purging** - `apt clean` to reclaim package cache space\n5. **Journal Pruning** - `journalctl --vacuum-time=7d` to limit log retention\n6. **Snap Revision Cleanup** - Remove disabled snap revisions (see below)\n7. **Temporary Directory Assessment** - Review `/tmp` and `/var/tmp` for cleanup opportunities\n\n## Snap Revision Cleanup Technique\n\nSnap keeps old revisions by default. To identify and remove:\n\n```bash\n# List all disabled snap revisions\nsnap list --all | awk '/disabled/{print $1, $3}'\n\n# Remove specific revision\nsnap remove <package-name> --revision=<revision-number>\n```\n\n**Important**: Requires explicit removal by revision number, not simple package uninstallation.\n\n## Documentation Requirements\n\nAll maintenance sessions must generate structured logs recording:\n\n1. **System Identification**\n   - Hostname\n   - OS version\n   - Kernel information\n   - Operator identity\n\n2. **Resource States**\n   - Initial disk/memory/CPU usage\n   - Final disk/memory/CPU usage\n   - Quantified improvements\n\n3. **Actions Taken**\n   - Specific commands executed\n   - MB/GB freed per category\n   - Packages upgraded/removed\n\n4. **Follow-up Recommendations**\n   - Remaining issues\n   - Future maintenance needs\n   - Monitoring suggestions\n\n## Expected Results\n\nReal-world recovery examples:\n- **Journal vacuuming**: 300-600 MB\n- **Snap revision cleanup**: 500 MB to 2 GB\n- **Package cache purging**: 100-500 MB\n- **Total potential**: 2+ GB in comprehensive sessions\n\n## Time Commitment\n\nTypical maintenance session: 15-30 minutes including diagnostics, cleanup, and documentation.\n",
        "scenario-testing/.claude-plugin/plugin.json": "{\n  \"name\": \"scenario-testing\",\n  \"description\": \"End-to-end testing with real dependencies - no mocks allowed; scenarios with real data are the only source of truth\",\n  \"version\": \"1.0.0\"\n}\n",
        "scenario-testing/README.md": "# Scenario Testing\n\nEnd-to-end testing with real dependencies. No mocks allowed - scenarios with real data are the only source of truth.\n\n## Installation\n\n```bash\n/plugin install scenario-testing@2389-research\n```\n\n## What This Plugin Provides\n\nEnforces scenario-driven testing where features are validated by exercising real systems with real dependencies. Mocks create false confidence - only real scenarios prove code works.\n\n### Core Principle\n\n**\"NO FEATURE IS VALIDATED UNTIL A SCENARIO PASSES WITH REAL DEPENDENCIES\"**\n\n## When to Use\n\n- Writing tests for new features\n- Validating that code actually works\n- When tempted to use mocks\n- Before declaring work complete\n- After fixing bugs\n\n## The Truth Hierarchy\n\n1. **Scenario tests** (real system, real data) = **TRUTH**\n2. **Unit tests** (isolated) = human comfort only\n3. **Mocks** = lies hiding bugs\n\nAs the principle states: \"A test that uses mocks is not testing your system. It's testing your assumptions about how dependencies behave.\"\n\n## Quick Example\n\n```python\n# .scratch/test-user-registration.py - NOT COMMITTED, gitignored\n# Uses real database, real auth service (test mode)\n\ndef test_user_registration_scenario():\n    # No mocks - hit real services\n    user = register_user(\n        email=\"test@example.com\",\n        password=\"secure123\"\n    )\n\n    # Verify against real database\n    assert user.id is not None\n    assert user.email == \"test@example.com\"\n    assert user.email_verified is False\n\n    # Verify can authenticate with real auth service\n    token = login(email=\"test@example.com\", password=\"secure123\")\n    assert token is not None\n\n    # Cleanup real data\n    delete_user(user.id)\n\n# After scenario passes, extract pattern to scenarios.jsonl (IS COMMITTED)\n```\n\n## Required Practices\n\n### 1. Write Scenarios in `.scratch/`\n\n- Use any language appropriate to the task\n- Exercise the real system end-to-end\n- **Zero mocks allowed**\n- Must be in `.gitignore` (never commit)\n\n### 2. Promote Patterns to `scenarios.jsonl`\n\n- Extract recurring scenarios as documented specifications\n- One JSON line per scenario\n- Include: name, description, given/when/then, validates\n- **This file IS committed**\n\n### 3. Use Real Dependencies\n\nExternal APIs must hit actual services (sandbox/test mode acceptable). Mocking any dependency invalidates the scenario.\n\n### 4. Independence Requirement\n\nEach scenario must run standalone without depending on prior executions. This enables:\n- Parallel execution\n- Prevents hidden ordering dependencies\n- Reliable CI/CD integration\n\n## What Makes a Scenario Invalid\n\nA scenario is invalid if it:\n- Contains any mocks whatsoever\n- Uses fake data instead of real storage\n- Depends on another scenario running first\n- Was never actually executed to verify it passes\n\n## Common Violations to Avoid\n\nReject these rationalizations:\n\n- **\"Just a quick unit test...\"** - Unit tests don't validate features\n- **\"Too simple for end-to-end...\"** - Integration breaks simple things\n- **\"I'll mock for speed...\"** - Speed doesn't matter if tests lie\n- **\"I don't have API credentials...\"** - Ask your human partner for real ones\n\n## Definition of Done\n\nA feature is complete only when:\n\n1. ✅ A scenario in `.scratch/` passes with zero mocks\n2. ✅ Real dependencies are exercised\n3. ✅ `.scratch/` remains in `.gitignore`\n4. ✅ Robust patterns extracted to `scenarios.jsonl`\n\n## Example Workflow\n\n1. **Write scenario** - Create `.scratch/test-user-registration.py`\n2. **Use real dependencies** - Hit real database, real auth service (test mode)\n3. **Run and verify** - Execute scenario, confirm it passes\n4. **Extract pattern** - Document in `scenarios.jsonl`\n5. **Keep .scratch ignored** - Never commit scratch scenarios\n\n## Why This Matters\n\n- **Unit tests** verify isolated logic\n- **Integration tests** verify components work together\n- **Scenario tests** verify the system actually works\n\nOnly scenario tests prove your feature delivers value to users.\n\n## Documentation\n\nSee [skills/SKILL.md](skills/SKILL.md) for complete scenario testing protocol.\n\n## Philosophy\n\nReal validation over false confidence. Mocks test assumptions, not reality.\n",
        "scenario-testing/skills/SKILL.md": "---\nname: scenario-testing\ndescription: This skill should be used when writing tests, validating features, or needing to verify code works. Triggers on \"write tests\", \"add test coverage\", \"validate feature\", \"integration test\", \"end-to-end\", \"e2e test\", \"mock\", \"unit test\". Enforces scenario-driven testing with real dependencies in .scratch/ directory.\n---\n\n# Scenario-Driven Testing for AI Code Generation\n\n## Core Principle\n\n**The Iron Law**: \"NO FEATURE IS VALIDATED UNTIL A SCENARIO PASSES WITH REAL DEPENDENCIES\"\n\nMocks create false confidence. Only scenarios exercising real systems validate that code works.\n\n## The Truth Hierarchy\n\n1. **Scenario tests** (real system, real data) = **truth**\n2. **Unit tests** (isolated) = human comfort only\n3. **Mocks** = lies hiding bugs\n\nAs stated in the principle: \"A test that uses mocks is not testing your system. It's testing your assumptions about how dependencies behave.\"\n\n## When to Use This Skill\n\n- Validating new functionality\n- Before declaring work complete\n- When tempted to use mocks\n- After fixing bugs requiring verification\n- Any time you need to prove code works\n\n## Required Practices\n\n### 1. Write Scenarios in `.scratch/`\n\n- Use any language appropriate to the task\n- Exercise the real system end-to-end\n- Zero mocks allowed\n- Must be in `.gitignore` (never commit)\n\n### 2. Promote Patterns to `scenarios.jsonl`\n\n- Extract recurring scenarios as documented specifications\n- One JSON line per scenario\n- Include: name, description, given/when/then, validates\n- This file IS committed\n\n### 3. Use Real Dependencies\n\nExternal APIs must hit actual services (sandbox/test mode acceptable). Mocking any dependency invalidates the scenario.\n\n### 4. Independence Requirement\n\nEach scenario must run standalone without depending on prior executions. This enables:\n- Parallel execution\n- Prevents hidden ordering dependencies\n- Reliable CI/CD integration\n\n## What Makes a Scenario Invalid\n\nA scenario is invalid if it:\n- Contains any mocks whatsoever\n- Uses fake data instead of real storage\n- Depends on another scenario running first\n- Never actually executed to verify it passes\n\n## Common Violations to Avoid\n\nReject these rationalizations:\n\n- **\"Just a quick unit test...\"** - Unit tests don't validate features\n- **\"Too simple for end-to-end...\"** - Integration breaks simple things\n- **\"I'll mock for speed...\"** - Speed doesn't matter if tests lie\n- **\"I don't have API credentials...\"** - Ask your human partner for real ones\n\n## Definition of Done\n\nA feature is complete only when:\n\n1. ✅ A scenario in `.scratch/` passes with zero mocks\n2. ✅ Real dependencies are exercised\n3. ✅ `.scratch/` remains in `.gitignore`\n4. ✅ Robust patterns extracted to `scenarios.jsonl`\n\n## Example Workflow\n\n1. **Write scenario** - Create `.scratch/test-user-registration.py`\n2. **Use real dependencies** - Hit real database, real auth service (test mode)\n3. **Run and verify** - Execute scenario, confirm it passes\n4. **Extract pattern** - Document in `scenarios.jsonl`\n5. **Keep .scratch ignored** - Never commit scratch scenarios\n\n## Why This Matters\n\n- **Unit tests** verify isolated logic\n- **Integration tests** verify components work together\n- **Scenario tests** verify the system actually works\n\nOnly scenario tests prove your feature delivers value to users.\n",
        "slack-mcp/.claude-plugin/plugin.json": "{\n  \"name\": \"slack-mcp\",\n  \"description\": \"Slack workspace integration - create channels, invite users, post messages, and manage threads\",\n  \"version\": \"1.0.0\",\n  \"type\": \"mcp\",\n  \"mcp\": {\n    \"command\": \"node\",\n    \"args\": [\"dist/index.js\"],\n    \"cwd\": \".\",\n    \"env\": {\n      \"SLACK_BOT_TOKEN\": \"${SLACK_BOT_TOKEN}\"\n    }\n  }\n}\n",
        "slack-mcp/README.md": "# Slack MCP Server\n\nMCP server for Slack workspace integration. Create channels, invite users, post messages, and manage threads.\n\n## Features\n\n- Create public or private channels\n- Invite users by email or user ID\n- Post messages with markdown formatting\n- Reply in threads\n- Pin messages\n- List workspace users\n\n## Setup\n\n### 1. Create Slack App\n\n1. Go to [api.slack.com/apps](https://api.slack.com/apps)\n2. Click \"Create New App\" → \"From scratch\"\n3. Name it (e.g., \"Claude MCP\") and select your workspace\n\n### 2. Add OAuth Scopes\n\nUnder \"OAuth & Permissions\", add these Bot Token Scopes:\n\n```\nchannels:manage        # create public channels\ngroups:write           # create private channels\nchannels:read          # list channels\ngroups:read            # read private channels\nchat:write             # post messages\npins:write             # pin messages\nusers:read             # list users\nusers:read.email       # lookup users by email\n```\n\n### 3. Install to Workspace\n\nClick \"Install to Workspace\" and authorize the app.\n\n### 4. Get Bot Token\n\nCopy the \"Bot User OAuth Token\" (starts with `xoxb-`).\n\n### 5. Configure Environment\n\n```bash\nexport SLACK_BOT_TOKEN=\"xoxb-your-token-here\"\n```\n\nOr add to your Claude config:\n\n```json\n{\n  \"mcpServers\": {\n    \"slack\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/slack-mcp/dist/index.js\"],\n      \"env\": {\n        \"SLACK_BOT_TOKEN\": \"xoxb-your-token-here\"\n      }\n    }\n  }\n}\n```\n\n### 6. Build and Run\n\n```bash\nnpm install\nnpm run build\nnpm start\n```\n\n## Tools\n\n### slack_create_channel\n\nCreate a new Slack channel.\n\n```json\n{\n  \"name\": \"gtm-jeff\",\n  \"is_private\": true,\n  \"description\": \"GTM materials for Jeff launch\"\n}\n```\n\n### slack_invite_to_channel\n\nInvite users by email or user ID.\n\n```json\n{\n  \"channel_id\": \"C123ABC\",\n  \"users\": [\"harper@2389.ai\", \"dylan@2389.ai\"]\n}\n```\n\n### slack_post_message\n\nPost a message to a channel.\n\n```json\n{\n  \"channel_id\": \"C123ABC\",\n  \"text\": \"## Email\\n\\n**Subject:** meet jeff...\"\n}\n```\n\n### slack_post_thread\n\nReply to a message in a thread.\n\n```json\n{\n  \"channel_id\": \"C123ABC\",\n  \"thread_ts\": \"1234567890.123456\",\n  \"text\": \"Updated the subject line\"\n}\n```\n\n### slack_pin_message\n\nPin a message to a channel.\n\n```json\n{\n  \"channel_id\": \"C123ABC\",\n  \"message_ts\": \"1234567890.123456\"\n}\n```\n\n### slack_list_users\n\nList all users in the workspace.\n\n```json\n{}\n```\n\n## Usage with Product Launcher\n\nAfter generating GTM materials, say \"push to slack\":\n\n1. Creates `#gtm-[product]` private channel\n2. Invites Harper and Dylan\n3. Posts each output (email, blog, tweets) as separate messages\n4. Pins the summary message\n\n## License\n\nMIT\n",
        "sysadmin/.claude-plugin/plugin.json": "{\n  \"name\": \"sysadmin\",\n  \"description\": \"[meta] system administration: structured Linux maintenance and diagnostics\",\n  \"version\": \"1.0.0\"\n}\n",
        "sysadmin/README.md": "# [meta] Sysadmin\n\nMeta-plugin for system administration workflows on Linux systems.\n\n## Installation\n\n```bash\n/plugin install sysadmin@2389-research\n```\n\nThis will automatically install:\n- **remote-system-maintenance** - Structured Linux diagnostics and cleanup\n\n## What This Provides\n\n### Remote System Maintenance\n\nStructured procedures for diagnosing and maintaining remote Linux systems via SSH/tmux:\n\n**Three-phase diagnostics:**\n1. Initial diagnostics (hostname, resources, processes, zombies)\n2. System log review (errors, journal consumption, service status)\n3. Package assessment (upgradable, orphaned, cached)\n\n**Seven-stage cleanup:**\n1. apt update - Refresh package lists\n2. apt upgrade - Apply security fixes\n3. apt autoremove - Remove unused dependencies\n4. apt clean - Purge package cache\n5. journalctl vacuum - Prune old logs\n6. snap cleanup - Remove disabled revisions (500MB-2GB savings!)\n7. /tmp assessment - Check temporary directories\n\n**Expected recovery**: 2+ GB in comprehensive sessions\n\n## Why This Meta Plugin?\n\nCurrently just wraps `remote-system-maintenance`, but provides:\n- Clear categorization (sysadmin vs development tools)\n- Future expansion point for additional sysadmin tools\n- Semantic installation (`/plugin install sysadmin`)\n\n## Future Additions\n\nPotential future inclusions:\n- Docker cleanup workflows\n- Log analysis tools\n- Performance monitoring\n- Backup verification\n- Security hardening checklists\n\n## Documentation\n\nSee [remote-system-maintenance plugin](../remote-system-maintenance/) for complete maintenance procedures.\n\n\n",
        "terminal-title/.claude-plugin/plugin.json": "{\n  \"name\": \"terminal-title\",\n  \"description\": \"Automatically updates terminal title with emoji + project + topic context and establishes 2389 workflow conventions for TodoWrite task tracking\",\n  \"version\": \"1.0.0\"\n}\n",
        "terminal-title/README.md": "# Terminal Title Plugin\n\nAutomatically updates terminal title with emoji + project + topic context. **Cross-platform support for Windows, macOS, and Linux.** Also establishes 2389 workflow conventions for TodoWrite task tracking.\n\n## Installation\n\n```bash\n/plugin marketplace add 2389-research/claude-plugins\n/plugin install terminal-title@2389-research\n```\n\n### Platform-Specific Setup\n\n**Windows users:** Ensure PowerShell 7+ (`pwsh`) is installed and available in your PATH. The plugin automatically uses PowerShell scripts on Windows.\n\n**Unix/Linux/macOS users:** No additional setup required. The plugin uses bash scripts automatically.\n\n## What This Plugin Provides\n\n**Skills provided:**\n\n- `terminal-title` – Automatically manages the terminal title based on project and topic context\n\n**Automatic terminal title management:**\n\n- **Auto-invokes at session start**: Sets initial title based on project context\n- **Updates on topic changes**: Reflects current work in terminal title\n- **Emoji indicators**: Visual cues for quick context switching\n\n**2389 workflow conventions:**\n\n- **TodoWrite patterns**: Granular 2-5 minute task tracking\n- **Task lifecycle management**: One task in-progress at a time\n- **Shared conventions**: Used across all 2389 plugins\n\n## How It Works\n\nThe plugin includes a session start hook that automatically invokes the terminal-title skill. The skill:\n\n1. **Detects your operating system** (Windows, macOS, Linux)\n2. **Detects current project** from working directory, git repo, or package.json\n3. **Infers topic** from recent files or conversation context\n4. **Selects appropriate emoji** from environment variable or defaults to 🎉\n5. **Updates terminal title** via platform-specific script:\n   - Windows: PowerShell (`.ps1`)\n   - Unix/Linux/macOS: Bash (`.sh`)\n\n## Example\n\n```text\n🔥 firebase-app > authentication setup\n```\n\n## TodoWrite Conventions\n\nThis plugin establishes task tracking patterns used across all 2389 plugins:\n\n```javascript\n{\n  content: \"Write the failing test\",      // Imperative form\n  status: \"pending\",                       // or \"in_progress\" or \"completed\"\n  activeForm: \"Writing the failing test\"   // Present continuous form\n}\n```\n\n**Task lifecycle:**\n1. Create todos for all steps (2-5 minutes each)\n2. Mark ONE task as in_progress\n3. Complete the task\n4. Mark as completed immediately\n5. Move to next task\n\n## Documentation\n\n- [Design Document](docs/2025-11-14-terminal-title-skill-design.md)\n- [Implementation Plan](docs/2025-11-14-terminal-title-implementation.md)\n\n\n",
        "terminal-title/hooks/hooks.json": "{\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/hooks/session-start-launcher.sh\",\n            \"timeout\": 10\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "terminal-title/hooks/session-start-launcher.sh": "#!/bin/bash\n# ABOUTME: Cross-platform launcher for session-start hook\n# ABOUTME: Detects OS and calls appropriate script (bash or PowerShell)\n\n# Detect OS\nif [[ \"$OSTYPE\" == \"msys\" || \"$OSTYPE\" == \"win32\" || \"$OSTYPE\" == \"cygwin\" ]]; then\n    # Windows detected - use PowerShell\n    HOOK_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n    POWERSHELL_SCRIPT=\"${HOOK_DIR}/session-start-title.ps1\"\n\n    # Check if PowerShell script exists\n    if [ ! -f \"$POWERSHELL_SCRIPT\" ]; then\n        echo \"Error: PowerShell script not found: $POWERSHELL_SCRIPT\" >&2\n        exit 1\n    fi\n\n    # Try to find PowerShell (pwsh or powershell.exe)\n    if command -v pwsh >/dev/null 2>&1; then\n        exec pwsh -NoProfile -ExecutionPolicy Bypass -File \"$POWERSHELL_SCRIPT\"\n    elif command -v powershell.exe >/dev/null 2>&1; then\n        exec powershell.exe -NoProfile -ExecutionPolicy Bypass -File \"$POWERSHELL_SCRIPT\"\n    else\n        echo \"Error: PowerShell not found on Windows\" >&2\n        exit 1\n    fi\nelse\n    # Unix-like system (Linux, macOS, etc.) - use bash\n    HOOK_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n    BASH_SCRIPT=\"${HOOK_DIR}/session-start-title.sh\"\n\n    if [ -f \"$BASH_SCRIPT\" ]; then\n        exec bash \"$BASH_SCRIPT\"\n    else\n        echo \"Error: Bash script not found: $BASH_SCRIPT\" >&2\n        exit 1\n    fi\nfi\n",
        "terminal-title/hooks/session-start-title.ps1": "# ABOUTME: SessionStart hook wrapper that detects project name and sets terminal title (Windows PowerShell)\n# ABOUTME: Called automatically by Claude Code when session starts\n\n# Get current working directory\n$CWD = Get-Location\n\n# Intelligent project name detection\n$ProjectName = \"\"\n\n# Check for package.json name field (Node.js projects)\n$PackageJsonPath = Join-Path $CWD \"package.json\"\nif (Test-Path $PackageJsonPath) {\n    try {\n        $PackageJson = Get-Content $PackageJsonPath -ErrorAction SilentlyContinue | ConvertFrom-Json\n        if ($PackageJson.name) {\n            # Convert kebab-case to Title Case\n            $ProjectName = ($PackageJson.name -split '-' | Where-Object { $_ -and $_.Length -gt 0 } | ForEach-Object {\n                $_.Substring(0,1).ToUpper() + $_.Substring(1).ToLower()\n            }) -join ' '\n        }\n    } catch {\n        # Silently continue if package.json parsing fails\n    }\n}\n\n# Check git remote URL\nif (-not $ProjectName) {\n    $GitDir = Join-Path $CWD \".git\"\n    if (Test-Path $GitDir) {\n        try {\n            $GitRemote = git -C $CWD config --get remote.origin.url 2>$null\n            if ($GitRemote) {\n                # Extract repo name from URL (e.g., github.com/user/repo.git -> repo)\n                $RepoName = [System.IO.Path]::GetFileNameWithoutExtension($GitRemote)\n                # Convert kebab-case to Title Case\n                $ProjectName = ($RepoName -split '-' | Where-Object { $_ -and $_.Length -gt 0 } | ForEach-Object {\n                    $_.Substring(0,1).ToUpper() + $_.Substring(1).ToLower()\n                }) -join ' '\n            }\n        } catch {\n            # Silently continue if git remote fails\n        }\n    }\n}\n\n# Check README.md first heading\nif (-not $ProjectName) {\n    $ReadmePath = Join-Path $CWD \"README.md\"\n    if (Test-Path $ReadmePath) {\n        try {\n            $FirstHeading = Get-Content $ReadmePath -ErrorAction SilentlyContinue | Select-String -Pattern '^#' | Select-Object -First 1\n            if ($FirstHeading) {\n                $ProjectName = $FirstHeading.Line -replace '^#+\\s*', ''\n            }\n        } catch {\n            # Silently continue if README.md fails\n        }\n    }\n}\n\n# Fallback: Use directory basename and humanize it\nif (-not $ProjectName) {\n    $DirName = Split-Path -Leaf $CWD\n    # Convert kebab-case or snake_case to Title Case\n    $ProjectName = ($DirName -replace '[-_]', ' ' -split ' ' | Where-Object { $_ -and $_.Length -gt 0 } | ForEach-Object {\n        $_.Substring(0,1).ToUpper() + $_.Substring(1).ToLower()\n    }) -join ' '\n}\n\n# Find the set_title.ps1 script (relative to this hook script)\n$HookDir = Split-Path -Parent $PSCommandPath\n$PluginRoot = Split-Path -Parent $HookDir\n$SetTitleScript = Join-Path $PluginRoot \"skills\\scripts\\set_title.ps1\"\n\n# Call set_title.ps1 with detected project name and \"Claude Code\" as topic\nif (Test-Path $SetTitleScript) {\n    try {\n        & $SetTitleScript -ProjectName $ProjectName -Topic \"Claude Code\" 2>$null\n    } catch {\n        # Silently continue if set_title.ps1 fails\n    }\n} else {\n    # Fallback: try to find it in Claude's plugin directory\n    $SetTitleScript = Join-Path $env:USERPROFILE \".claude\\plugins\\terminal-title\\skills\\scripts\\set_title.ps1\"\n    if (Test-Path $SetTitleScript) {\n        try {\n            & $SetTitleScript -ProjectName $ProjectName -Topic \"Claude Code\" 2>$null\n        } catch {\n            # Silently continue if fallback fails\n        }\n    }\n}\n\n# Output JSON with additionalContext for Claude Code to inject as system reminder\n@\"\n{\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"SessionStart\",\n    \"additionalContext\": \"<CRITICAL_SESSION_START_INSTRUCTION>\\n\\nBEFORE responding to the user, you MUST:\\n\\n1. Use the Skill tool to invoke: terminal-title\\n2. This will refine the terminal title from the default set by the command hook\\n3. Only THEN respond to the user's message\\n\\nThis is NOT optional. This is NOT negotiable. The terminal title skill MUST run at session start.\\n\\nIf you respond to the user without invoking terminal-title first, you have FAILED.\\n\\n</CRITICAL_SESSION_START_INSTRUCTION>\"\n  }\n}\n\"@\n\nexit 0\n",
        "terminal-title/hooks/session-start-title.sh": "#!/bin/bash\n# ABOUTME: SessionStart hook wrapper that detects project name and sets terminal title\n# ABOUTME: Called automatically by Claude Code when session starts\n\nset -e\n\n# Get current working directory\nCWD=\"${PWD}\"\n\n# Intelligent project name detection (matching terminal-title skill logic)\nPROJECT_NAME=\"\"\n\n# Check for package.json name field (Node.js projects)\nif [ -f \"${CWD}/package.json\" ] && command -v jq >/dev/null 2>&1; then\n    PACKAGE_NAME=$(jq -r '.name // empty' \"${CWD}/package.json\" 2>/dev/null)\n    if [ -n \"$PACKAGE_NAME\" ]; then\n        # Convert kebab-case to Title Case\n        PROJECT_NAME=$(echo \"$PACKAGE_NAME\" | sed 's/-/ /g' | awk '{for(i=1;i<=NF;i++) $i=toupper(substr($i,1,1)) tolower(substr($i,2))}1')\n    fi\nfi\n\n# Check git remote URL\nif [ -z \"$PROJECT_NAME\" ] && [ -d \"${CWD}/.git\" ]; then\n    GIT_REMOTE=$(git -C \"${CWD}\" config --get remote.origin.url 2>/dev/null || echo \"\")\n    if [ -n \"$GIT_REMOTE\" ]; then\n        # Extract repo name from URL (e.g., github.com/user/repo.git -> repo)\n        PROJECT_NAME=$(basename \"$GIT_REMOTE\" .git)\n        # Convert kebab-case to Title Case\n        PROJECT_NAME=$(echo \"$PROJECT_NAME\" | sed 's/-/ /g' | awk '{for(i=1;i<=NF;i++) $i=toupper(substr($i,1,1)) tolower(substr($i,2))}1')\n    fi\nfi\n\n# Check README.md first heading\nif [ -z \"$PROJECT_NAME\" ] && [ -f \"${CWD}/README.md\" ]; then\n    FIRST_HEADING=$(grep -m 1 '^#' \"${CWD}/README.md\" 2>/dev/null | sed 's/^#* *//' || echo \"\")\n    if [ -n \"$FIRST_HEADING\" ]; then\n        PROJECT_NAME=\"$FIRST_HEADING\"\n    fi\nfi\n\n# Fallback: Use directory basename and humanize it\nif [ -z \"$PROJECT_NAME\" ]; then\n    DIR_NAME=$(basename \"$CWD\")\n    # Convert kebab-case or snake_case to Title Case\n    PROJECT_NAME=$(echo \"$DIR_NAME\" | sed 's/[-_]/ /g' | awk '{for(i=1;i<=NF;i++) $i=toupper(substr($i,1,1)) tolower(substr($i,2))}1')\nfi\n\n# Find the set_title.sh script (relative to this hook script)\nHOOK_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nPLUGIN_ROOT=\"$(dirname \"$HOOK_DIR\")\"\nSET_TITLE_SCRIPT=\"${PLUGIN_ROOT}/skills/scripts/set_title.sh\"\n\n# Call set_title.sh with detected project name and \"Claude Code\" as topic\nif [ -x \"$SET_TITLE_SCRIPT\" ]; then\n    bash \"$SET_TITLE_SCRIPT\" \"$PROJECT_NAME\" \"Claude Code\" 2>/dev/null || true\nelse\n    # Fallback: try to find it in Claude's plugin directory\n    SET_TITLE_SCRIPT=\"${HOME}/.claude/plugins/terminal-title/skills/scripts/set_title.sh\"\n    if [ -x \"$SET_TITLE_SCRIPT\" ]; then\n        bash \"$SET_TITLE_SCRIPT\" \"$PROJECT_NAME\" \"Claude Code\" 2>/dev/null || true\n    fi\nfi\n\n# Output JSON with additionalContext for Claude Code to inject as system reminder\n# This is how command hooks inject prompts into the conversation context\ncat <<'EOF'\n{\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"SessionStart\",\n    \"additionalContext\": \"<CRITICAL_SESSION_START_INSTRUCTION>\\n\\nBEFORE responding to the user, you MUST:\\n\\n1. Use the Skill tool to invoke: terminal-title\\n2. This will refine the terminal title from the default set by the command hook\\n3. Only THEN respond to the user's message\\n\\nThis is NOT optional. This is NOT negotiable. The terminal title skill MUST run at session start.\\n\\nIf you respond to the user without invoking terminal-title first, you have FAILED.\\n\\n</CRITICAL_SESSION_START_INSTRUCTION>\"\n  }\n}\nEOF\n\nexit 0\n",
        "terminal-title/skills/SKILL.md": "---\nname: terminal-title\ndescription: This skill should be used to update terminal window title with context. Triggers automatically at session start via hook. Also triggers on topic changes during conversation (debugging to docs, frontend to backend). Updates title with emoji + project + current topic.\n---\n\n<!-- ABOUTME: Terminal title skill that automatically updates terminal window title -->\n<!-- ABOUTME: Uses emoji from environment + Claude's project detection + current topic -->\n\n# Terminal Title Management\n\n## Overview\n\nThis skill updates the terminal window title to show current context:\n- Work/fun emoji (from `$TERMINAL_TITLE_EMOJI` environment variable)\n- Project name (intelligently detected by Claude)\n- Current topic (what you're working on)\n\n**Format:** `💼 ProjectName - Topic`\n\n## CRITICAL: When to Invoke\n\n**MANDATORY invocations:**\n1. **Session start**: Automatically triggered by SessionStart hook. Claude must respond by setting title to \"Claude Code\" as default topic.\n2. **Topic changes**: Claude MUST detect and invoke when user shifts to new topic.\n\n**Topic change patterns (Claude must detect these):**\n- ✅ \"let's talk about X\" / \"can you tell me about Y\" → invoke immediately\n- ✅ User switches domains: debugging → documentation, frontend → backend, feature → tests\n- ✅ User starts working on different module/component after sustained discussion\n- ✅ User asks about completely unrelated topic after 3+ exchanges on current topic\n- ❌ Follow-up questions on same topic (\"add a comment to that function\") → do NOT invoke\n- ❌ Small refinements to current work (\"make it blue\") → do NOT invoke\n- ❌ Clarifications about current task → do NOT invoke\n\n**Claude's responsibility:** Actively monitor conversation flow and invoke this skill whenever topic materially shifts. Do not wait for explicit permission.\n\n**Example titles:**\n- `💼 Skills Repository - Terminal Title`\n- `🎉 dotfiles - zsh config`\n- `💼 OneOnOne - Firebase Config`\n\n## Setup Requirements\n\n**Required Permission:**\n\nThis skill runs scripts to update the terminal title. To prevent Claude from prompting for permission every time, add this to your `~/.claude/settings.json`:\n\n**For Unix/Linux/macOS:**\n```json\n{\n  \"permissions\": {\n    \"allow\": [\n      \"Bash(bash *skills/scripts/set_title.sh:*)\"\n    ]\n  }\n}\n```\n\n**For Windows:**\n```json\n{\n  \"permissions\": {\n    \"allow\": [\n      \"Bash(pwsh *skills/scripts/set_title.ps1:*)\"\n    ]\n  }\n}\n```\n\n**Why this permission is needed:** The skill executes a script that sends terminal escape sequences silently in the background to update your window title without interrupting your workflow.\n\n**Cross-platform support:** The plugin includes both bash scripts (`.sh`) for Unix-like systems and PowerShell scripts (`.ps1`) for Windows. The SessionStart hook automatically detects your OS and uses the appropriate script.\n\n## Project Detection Strategy\n\n**Primary method: Claude's intelligence**\n\nClaude analyzes the current context to determine the project name:\n- Current working directory and recent conversation\n- Files read during the session\n- Git repository information\n- Package metadata (package.json, README.md, etc.)\n- Overall understanding of what the project represents\n\n**Key principle:** Use intelligent understanding, not just mechanical file parsing.\n\n**Detection examples:**\n- `/Users/dylanr/work/2389/skills` → \"Skills Repository\" (not just \"skills\")\n- `/Users/dylanr/work/2389/oneonone/hosting` → \"OneOnOne\"\n- `/Users/dylanr/dotfiles` → \"dotfiles\"\n- `/Users/dylanr/projects/my-app` → \"My App\" (humanized from directory)\n\n**Supporting evidence Claude checks:**\n1. Git remote URL or repository directory name\n2. `package.json` name field (Node.js projects)\n3. First heading in README.md\n4. Directory basename (last resort)\n\n**Fallback:** If Claude cannot determine context, use current directory name.\n\n**Always include project name** - Claude can always determine something meaningful.\n\n## Title Formatting\n\n**Standard format:**\n```\n$emoji ProjectName - Topic\n```\n\n**Component details:**\n\n**1. Emoji (from environment):**\n- Read from `$TERMINAL_TITLE_EMOJI` environment variable\n- Set by zsh theme based on directory context\n- Common values: 💼 (work), 🎉 (fun/personal)\n- Fallback: Use 🎉 if variable not set\n\n**2. Project Name (from Claude's detection):**\n- Human-friendly name, not slug format\n- Proper capitalization (e.g., \"OneOnOne\" not \"oneonone\")\n- Always present (use directory name as minimum)\n\n**3. Topic (from invocation context):**\n- Short description of current work (2-4 words)\n- Provided when skill is invoked\n- Examples: \"Terminal Title\", \"Firebase Config\", \"CSS Refactor\"\n\n**Script usage:**\n```bash\nbash scripts/set_title.sh \"ProjectName\" \"Topic\"\n```\n\n**Note:** The script automatically reads emoji from environment and handles terminal escape sequences.\n\n**Edge cases:**\n- No `$TERMINAL_TITLE_EMOJI`: Use 🎉 as default\n- Very long project/topic: Let terminal handle truncation naturally\n- Empty topic: Use \"Claude Code\" as default\n\n## Workflow\n\n**When Claude detects a topic change (or session start), immediately invoke this workflow:**\n\nFollow these steps to update the terminal title:\n\n**Step 1: Determine project name**\n\nUse Claude's understanding of the current codebase and context to determine the project name. Check:\n- What you know about this project from conversation and files\n- Git repository name/remote URL\n- package.json name field (if Node.js project)\n- README.md first heading\n- Directory basename (minimum fallback)\n\nGenerate a human-friendly name (e.g., \"Skills Repository\" not \"skills\").\n\n**Step 2: Get topic from context**\n\nThe topic should come from:\n- Invocation parameter (if provided explicitly)\n- Current conversation context (what user is working on)\n- Default to \"Claude Code\" for session start\n\n**Step 3: Detect operating system**\n\nBefore executing the script, detect the operating system:\n\n```bash\n# Check if running on Windows (check for common Windows indicators)\nif [[ \"$OSTYPE\" == \"msys\" || \"$OSTYPE\" == \"win32\" || \"$OSTYPE\" == \"cygwin\" ]]; then\n    OS=\"windows\"\nelse\n    OS=\"unix\"\nfi\n```\n\n**Step 4: Execute title update script (cross-platform)**\n\nFind the skill base directory (available in the skill loading message) and execute the appropriate script based on OS:\n\n**For Unix/Linux/macOS:**\n```bash\nbash <skill-base-dir>/scripts/set_title.sh \"Project Name\" \"Topic\"\n```\n\n**For Windows:**\n```bash\npwsh -NoProfile -ExecutionPolicy Bypass -File <skill-base-dir>/scripts/set_title.ps1 \"Project Name\" \"Topic\"\n```\n\nThe scripts handle:\n- Reading emoji from `$TERMINAL_TITLE_EMOJI` (Unix) or `$env:TERMINAL_TITLE_EMOJI` (Windows)\n- Sanitizing inputs\n- Formatting and sending terminal escape sequences\n\nExecute this command silently using the Bash tool (pre-approved via permissions).\n\n**Step 5: Done**\n\nTitle updated silently. Continue responding to user without mentioning the title update unless they ask about it.\n\n## Examples\n\n### Example 1: Session Start in Skills Repository\n\n**Context:** User starts Claude Code in `/Users/dylanr/work/2389/skills`\n\n**Environment:** `TERMINAL_TITLE_EMOJI=💼`\n\n**Execution:**\n1. Project: \"Skills Repository\" (Claude's knowledge)\n2. Topic: \"Claude Code\" (session start default)\n3. Command: `bash /Users/dylanr/work/2389/claude-plugins/terminal-title/skills/scripts/set_title.sh \"Skills Repository\" \"Claude Code\"`\n\n**Result:** Terminal title shows `💼 Skills Repository - Claude Code` (script reads emoji from environment)\n\n---\n\n### Example 2: Topic Change Detection\n\n**Context:** Conversation shifts from \"terminal title implementation\" to \"rare VWs\"\n\n**Claude's detection:** User asked \"can you tell me about rare vws?\" - this is a clear topic shift\n\n**Environment:** `TERMINAL_TITLE_EMOJI=🎉`\n\n**Claude's action:**\n1. Detect topic change (from terminal implementation to VWs)\n2. Invoke terminal-title skill workflow\n3. Project: \"Home\" (current directory basename)\n4. Topic: \"Rare VWs\" (from new conversation direction)\n5. Command: `bash <skill-base-dir>/scripts/set_title.sh \"Home\" \"Rare VWs\"`\n6. Continue answering user's question about rare VWs\n\n**Result:** Terminal title silently updates to `🎉 Home - Rare VWs` (script reads emoji from environment)\n\n---\n\n### Example 3: Personal Project Without Environment Variable\n\n**Context:** User working in `~/projects/dotfiles` directory\n\n**Environment:** `TERMINAL_TITLE_EMOJI` not set\n\n**Execution:**\n1. Project: \"dotfiles\" (from directory)\n2. Topic: \"zsh config\" (from conversation)\n3. Command: `bash <skill-base-dir>/scripts/set_title.sh \"dotfiles\" \"zsh config\"`\n\n**Result:** Terminal title shows `🎉 dotfiles - zsh config` (script uses 🎉 as fallback when TERMINAL_TITLE_EMOJI not set)\n\n---\n\n### Example 4: Different Project Context\n\n**Context:** User working in `/Users/dylanr/work/2389/oneonone/hosting`\n\n**Environment:** `TERMINAL_TITLE_EMOJI=💼`\n\n**Execution:**\n1. Project: \"OneOnOne\" (Claude knows this project name)\n2. Topic: \"Firebase Config\" (from conversation)\n3. Command: `bash <skill-base-dir>/scripts/set_title.sh \"OneOnOne\" \"Firebase Config\"`\n\n**Result:** Terminal title shows `💼 OneOnOne - Firebase Config` (script reads emoji from environment)\n\n---\n\n### Example 5: Windows User Session Start\n\n**Context:** Windows user starts Claude Code in `C:\\Users\\Nat\\projects\\my-app`\n\n**Environment:** `$env:TERMINAL_TITLE_EMOJI=💻` (Windows environment variable)\n\n**OS Detection:** Claude detects Windows via `$OSTYPE` check\n\n**Execution:**\n1. Project: \"My App\" (from directory name, humanized)\n2. Topic: \"Claude Code\" (session start default)\n3. Command: `pwsh -NoProfile -ExecutionPolicy Bypass -File <skill-base-dir>/scripts/set_title.ps1 \"My App\" \"Claude Code\"`\n\n**Result:** Terminal title shows `💻 My App - Claude Code` (PowerShell script reads emoji from `$env:TERMINAL_TITLE_EMOJI`)\n\n**Note:** The SessionStart hook automatically detected Windows and invoked the PowerShell version of the script instead of the bash version.\n",
        "test-kitchen/.claude-plugin/plugin.json": "{\n  \"name\": \"test-kitchen\",\n  \"description\": \"Parallel exploration of implementation approaches - run multiple variants simultaneously and let tests determine the winner\",\n  \"version\": \"1.0.0\",\n  \"commands\": []\n}\n",
        "test-kitchen/README.md": "# Test Kitchen\n\nParallel implementation framework with two gate skills:\n\n| Skill | Gate | Trigger |\n|-------|------|---------|\n| `test-kitchen:omakase-off` | **Entry** | FIRST on any build/create/implement request |\n| `test-kitchen:cookoff` | **Exit** | At design→implementation transition |\n\n## Installation\n\n```bash\n/plugin install test-kitchen@2389-research\n```\n\n## Flow\n\n```\n\"Build X\" / \"Create Y\" / \"Implement Z\"\n    ↓\n┌─────────────────────────────────────┐\n│  OMAKASE-OFF (entry gate)           │\n│  Wraps brainstorming                │\n│                                     │\n│  Choice:                            │\n│  1. Brainstorm together             │\n│  2. Omakase (3-5 parallel designs)  │\n└─────────────────────────────────────┘\n    ↓\n[Brainstorming / Design phase]\n    ↓\nDesign complete, \"let's implement\"\n    ↓\n┌─────────────────────────────────────┐\n│  COOKOFF (exit gate)                │\n│  Wraps implementation               │\n│                                     │\n│  Choice:                            │\n│  1. Cookoff (2-5 parallel agents)   │\n│  2. Single subagent                 │\n│  3. Local implementation            │\n└─────────────────────────────────────┘\n    ↓\n[Implementation]\n```\n\n## Quick Examples\n\n### Entry Gate: Omakase-off\n\n**Trigger 1: On build/create request**\n```\nUser: \"Build a notification system\"\n\nClaude: I'm using test-kitchen:omakase-off.\n\nHow would you like to explore this?\n\n1. Brainstorm together - We'll work through the design step by step\n2. Omakase - I'll generate 3-5 best approaches, implement in parallel, tests pick winner\n\nWhich approach?\n```\n\n**Option 1 (Brainstorm):** Proceeds to brainstorming skill (or fallback brainstorming), with passive slot detection\n\n**Option 2 (Omakase):**\n- Quick context gathering (1-2 questions)\n- Generate 3-5 architectural approaches\n- Implement ALL in parallel via git worktrees\n- Run same scenario tests on each\n- Fresh-eyes review survivors\n- Present comparison, user picks winner\n\n**Trigger 2: During brainstorming (slot detection)**\n```\nClaude: What storage approach would you prefer?\nUser: \"not sure, either could work\"\n\nClaude: [marks storage as architectural slot]\n\nClaude: For authentication?\nUser: \"no preference, you pick\"\n\nClaude: [marks auth as slot, detects 2+ uncertain responses]\n\nYou seem flexible on the approach. Would you like to:\n1. I'll pick what seems best and continue\n2. Explore multiple approaches in parallel (omakase-off)\n   → I'll implement 2-3 variants and let tests decide\n```\n\n### Exit Gate: Cookoff\n\n```\nUser: \"Looks good, let's implement\"\n\nClaude: I'm using test-kitchen:cookoff.\n\nHow would you like to implement this design?\n\n1. Cookoff (recommended) - 3 parallel agents, each creates own plan, pick best\n   → Complexity: medium feature\n2. Single subagent - One agent plans and implements\n3. Local - Plan and implement here\n\nWhich approach?\n```\n\n**Option 1 (Cookoff):**\n- Each agent reads the same design doc\n- Each agent creates their OWN implementation plan\n- All implement in parallel\n- Fresh-eyes review, compare results\n- User picks winner\n\n**Option 2/3:** Single agent or local implementation proceeds normally\n\n## Key Insight\n\n**Skills need aggressive triggers to work.** They can't passively detect \"uncertainty\" or \"readiness\" - they must claim specific moments in the conversation flow.\n\n- **Omakase-off**: Claims the BUILD/CREATE moment (before brainstorming)\n- **Cookoff**: Claims the IMPLEMENT moment (after design)\n\n## Dependencies\n\nTest Kitchen orchestrates these skills (uses fallbacks if not installed):\n\n- `superpowers:brainstorming`\n- `superpowers:writing-plans`\n- `superpowers:executing-plans`\n- `superpowers:using-git-worktrees`\n- `superpowers:dispatching-parallel-agents`\n- `superpowers:test-driven-development`\n- `superpowers:verification-before-completion`\n- `scenario-testing:skills`\n- `fresh-eyes-review:skills`\n- `superpowers:finishing-a-development-branch`\n\n## Documentation\n\n- [CLAUDE.md](./CLAUDE.md) - Full plugin instructions\n- [Omakase-off Skill](./skills/omakase-off/SKILL.md) - Entry gate (wraps brainstorming)\n- [Cookoff Skill](./skills/cookoff/SKILL.md) - Exit gate (wraps implementation)\n\n## Origin\n\nThe \"Test Kitchen\" name reflects the experimental nature - like a restaurant test kitchen where chefs try multiple approaches before putting a dish on the menu.\n",
        "test-kitchen/skills/SKILL.md": "---\nname: test-kitchen\ndescription: This skill should be used when implementing features with parallel exploration or competition. Triggers on \"build\", \"create\", \"implement\", \"try both approaches\", \"compare implementations\". Routes to omakase-off (entry gate for design exploration) or cookoff (exit gate for parallel implementation).\n---\n\n# Test Kitchen\n\nParallel implementation framework with two gate skills:\n\n| Skill | Gate | Trigger |\n|-------|------|---------|\n| `test-kitchen:omakase-off` | **Entry** | FIRST on any build/create/implement request |\n| `test-kitchen:cookoff` | **Exit** | At design→implementation transition |\n\n## Flow\n\n```\n\"Build X\" / \"Create Y\" / \"Implement Z\"\n    ↓\n┌─────────────────────────────────────┐\n│  OMAKASE-OFF (entry gate)           │\n│  Wraps brainstorming                │\n│                                     │\n│  Choice:                            │\n│  1. Brainstorm together             │\n│  2. Omakase (3-5 parallel designs)  │\n└─────────────────────────────────────┘\n    ↓\n[Brainstorming / Design phase]\n    ↓\nDesign complete, \"let's implement\"\n    ↓\n┌─────────────────────────────────────┐\n│  COOKOFF (exit gate)                │\n│  Wraps implementation               │\n│                                     │\n│  Choice:                            │\n│  1. Cookoff (2-5 parallel agents)   │\n│  2. Single subagent                 │\n│  3. Local implementation            │\n└─────────────────────────────────────┘\n    ↓\n[Implementation]\n```\n\n## Key Insight\n\n**Skills need aggressive triggers to work.** They can't passively detect \"uncertainty\" or \"readiness\" - they must claim specific moments in the conversation flow.\n\n- **Omakase-off**: Claims the BUILD/CREATE moment (before brainstorming)\n- **Cookoff**: Claims the IMPLEMENT moment (after design)\n\n## When Each Triggers\n\n### Omakase-off (Three Triggers)\n\n**Trigger 1: BEFORE brainstorming**\n- \"I want to build...\", \"Create a...\", \"Implement...\", \"Add a feature...\"\n- ANY signal to start building something\n- Offers choice: Brainstorm together OR Omakase (parallel designs)\n\n**Trigger 2: DURING brainstorming (slot detection)**\n- 2+ uncertain responses on architectural decisions\n- \"not sure\", \"don't know\", \"either works\", \"you pick\", \"no preference\"\n- Offers to explore detected slots in parallel\n\n**Trigger 3: Explicitly requested**\n- \"try both approaches\", \"explore both\", \"omakase\"\n- \"implement both variants\", \"let's see which is better\"\n\n### Cookoff\n- \"Let's implement\"\n- \"Looks good, let's build\"\n- \"Ready to code\"\n- Design doc just committed\n- ANY signal to move from design to code\n\n## Omakase Mode (Skip Brainstorming)\n\nIf user picks \"Omakase\" at the entry gate:\n1. Quick context gathering (1-2 questions)\n2. Generate 3-5 best architectural approaches\n3. Implement ALL in parallel\n4. Tests pick the winner\n5. Skip detailed brainstorming entirely\n\nBest for: \"I'm flexible, show me options in working code\"\n\n## Cookoff Mode (Parallel Implementation)\n\nIf user picks \"Cookoff\" at the exit gate:\n1. Each agent reads the same design doc\n2. Each agent creates their OWN implementation plan\n3. All implement in parallel\n4. Compare results, pick winner\n\nBest for: \"I want to see different implementation approaches\"\n\n## Key Distinction\n\n| | Omakase-off | Cookoff |\n|-|-------------|---------|\n| **Gate** | Entry (before/during brainstorming) | Exit (after design) |\n| **Question** | HOW to explore? | HOW to implement? |\n| **Parallel on** | Different DESIGNS | Same design, different PLANS |\n| **Triggers** | Build request, indecision detection, explicit | \"let's implement\" signal |\n| **Skips** | Brainstorming (optional via short-circuit) | Nothing - always after design |\n\n## Slot Detection (During Brainstorming)\n\nWhen omakase-off delegates to brainstorming, it passively tracks architectural decisions where user shows uncertainty:\n\n**Detection signals:**\n- \"not sure\", \"don't know\", \"either works\", \"both sound good\"\n- \"you pick\", \"whatever you think\", \"no preference\"\n- User defers 2+ decisions in a row\n\n**Slot classification:**\n| Type | Examples | Worth exploring? |\n|------|----------|------------------|\n| **Architectural** | Storage engine, framework, auth method | Yes - different code paths |\n| **Trivial** | File location, naming, config format | No - easy to change |\n\n**At end of brainstorming:**\n- If architectural slots exist → offer parallel exploration\n- If no slots → hand off to cookoff for implementation\n",
        "test-kitchen/skills/cookoff/SKILL.md": "---\nname: cookoff\ndescription: This skill should be used when moving from design to implementation. Triggers on \"let's build\", \"implement this\", \"looks good let's code\", \"ready to implement\". Presents options for parallel agent competition (cookoff), single subagent, or local implementation. Each agent creates own plan from shared design for genuine variation.\n---\n\n# Cookoff\n\nSame design, multiple cooks compete. Each implementation team creates their own plan from the shared design, then implements it. Natural variation emerges from independent planning decisions.\n\n**Part of Test Kitchen Development:**\n- `omakase-off` - Chef's choice exploration (different approaches/designs)\n- `cookoff` - Same design, multiple cooks compete (each creates own plan + implements)\n\n**Key insight:** Don't share a pre-made implementation plan. Each agent generates their own plan from the design doc, ensuring genuine variation.\n\n## Directory Structure\n\n```\ndocs/plans/<feature>/\n  design.md                    # Input: from brainstorming\n  cookoff/\n    impl-1/\n      plan.md                  # Agent 1's implementation plan\n    impl-2/\n      plan.md                  # Agent 2's implementation plan\n    impl-3/\n      plan.md                  # Agent 3's implementation plan\n    result.md                  # Cookoff results and winner\n```\n\n## Skill Dependencies\n\n| Reference | Primary (if installed) | Fallback |\n|-----------|------------------------|----------|\n| `writing-plans` | `superpowers:writing-plans` | Each agent writes their own implementation plan |\n| `executing-plans` | `superpowers:executing-plans` | Execute plan tasks sequentially with verification |\n| `parallel-agents` | `superpowers:dispatching-parallel-agents` | Dispatch multiple Task tools in single message |\n| `git-worktrees` | `superpowers:using-git-worktrees` | `git worktree add .worktrees/<name> -b <branch>` |\n| `tdd` | `superpowers:test-driven-development` | RED-GREEN-REFACTOR cycle |\n| `verification` | `superpowers:verification-before-completion` | Run command, read output, THEN claim status |\n| `fresh-eyes` | `fresh-eyes-review:skills` (2389) | 2-5 min review for security, logic, edge cases |\n| `judge` | `test-kitchen:judge` | Scoring framework with checklists (MUST invoke at Phase 4) |\n| `code-review` | `superpowers:requesting-code-review` | Dispatch code-reviewer subagent |\n| `scenario-testing` | `scenario-testing:skills` (2389) | `.scratch/` E2E scripts, real dependencies |\n| `finish-branch` | `superpowers:finishing-a-development-branch` | Verify tests, present options, cleanup |\n\n## When to Use\n\nTrigger when user wants to implement a design:\n- \"Execute this plan\" / \"Implement the plan\" / \"Let's build this\"\n- After brainstorming completes and design doc exists\n- Can also invoke explicitly: \"cookoff this\"\n\n**Important:** Cookoff works from a **design doc**, not a detailed implementation plan. Each agent creates their own implementation plan.\n\n## Detecting the Design-to-Implementation Transition\n\n**Cookoff triggers at a SITUATION, not a specific skill's output.**\n\n**The situation:** Design is complete, implementation is about to start.\n\n**Signals that design phase just completed:**\n- Design doc was written/committed\n- User approved a design (\"looks good\", \"yes\", \"let's do it\")\n- Discussion shifted from \"what to build\" to \"how to build it\"\n- Any skill/flow is about to start implementation\n\n**When you detect this transition, ALWAYS offer cookoff:**\n\n```\nBefore we start implementation, how would you like to proceed?\n\n1. Cookoff (recommended) - N parallel agents, each creates own plan, pick best\n   → Complexity: [assess from design]\n   → Best for: medium-high complexity features\n2. Single implementation - One agent/session implements\n3. Direct coding - Start coding without detailed plan\n```\n\n**This applies regardless of:**\n- Which brainstorming skill was used (superpowers, other, or none)\n- Whether a formal design doc exists (could be informal agreement)\n- What implementation options another skill might present\n\n**The key insight:** We're not injecting into another skill's menu. We're recognizing a SITUATION (design→implementation) and ensuring cookoff is offered at that moment.\n\n## Phase 1: Implementation Options\n\n**Present choices when user wants to implement:**\n\n```\nHow would you like to implement this design?\n\n1. Single subagent - One agent plans and implements\n2. Cookoff - N parallel agents, each creates own plan, pick best\n   → Complexity: [assess from design]\n   → Recommendation: N implementations\n3. Local - Plan and implement here in this session\n\nWhich approach?\n```\n\n**Routing:**\n- Option 1: Single agent uses writing-plans then executing-plans, cookoff exits\n- Option 2: Continue to Phase 2\n- Option 3: User implements manually, cookoff exits\n\n## Phase 2: Complexity Assessment\n\n**Read design doc and assess:**\n- Feature scope (components, integrations, data models)\n- Risk areas (auth, payments, migrations, concurrency)\n- Estimated implementation size\n\n**Map to implementation count:**\n\n| Complexity | Scope | Risk signals | Implementations |\n|------------|-------|--------------| --------------- |\n| Low | Small feature | None | 2 |\n| Medium | Medium feature | Some | 3 |\n| High | Large feature | Several | 4 |\n| Very high | Major system | Critical areas | 5 |\n\n**Setup directories:**\n```bash\nmkdir -p docs/plans/<feature>/cookoff/impl-{1,2,3}\n```\n\n**Announce:**\n```\nComplexity assessment: medium feature, touches auth\nSpawning 3 parallel implementations\nEach will create their own implementation plan from the design.\n```\n\n## Phase 3: Parallel Execution\n\n**Setup worktrees:**\n```\n.worktrees/cookoff-impl-1/\n.worktrees/cookoff-impl-2/\n.worktrees/cookoff-impl-3/\n\nBranches:\n<feature>/cookoff/impl-1\n<feature>/cookoff/impl-2\n<feature>/cookoff/impl-3\n```\n\n**CRITICAL: Dispatch ALL agents in a SINGLE message**\n\nUse `parallel-agents` pattern. Send ONE message with multiple Task tool calls:\n\n```\n<single message>\n  Task(impl-1, run_in_background: true)\n  Task(impl-2, run_in_background: true)\n  Task(impl-3, run_in_background: true)\n</single message>\n```\n\nDo NOT send separate messages for each agent.\n\n**Subagent prompt (each gets same instructions with their impl number):**\n\n```\nYou are implementation team N of M in a cookoff competition.\nOther teams are implementing the same design in parallel.\nEach team creates their own implementation plan - your approach may differ from others.\n\n**Your working directory:** /path/to/.worktrees/cookoff-impl-N\n**Design doc:** docs/plans/<feature>/design.md\n**Your plan location:** docs/plans/<feature>/cookoff/impl-N/plan.md\n\n**Your workflow:**\n1. Read the design doc thoroughly\n2. Use writing-plans skill to create YOUR implementation plan\n   - Save to: docs/plans/<feature>/cookoff/impl-N/plan.md\n   - Make your own architectural decisions\n   - Don't try to guess what other teams will do\n3. Use executing-plans skill to implement your plan\n4. Follow TDD for each task\n5. Use verification before claiming done\n\n**Report when done:**\n- Plan created: yes/no\n- All tasks completed: yes/no\n- Test results (npm test output)\n- Files changed count\n- Any issues encountered\n\nYour goal: best possible implementation. Good luck!\n```\n\n**Monitor progress:**\n```\nCookoff status (design: auth-system):\n- impl-1: planning... → implementing 5/8 tasks\n- impl-2: planning... → implementing 3/8 tasks\n- impl-3: planning... → implementing 6/8 tasks\n```\n\n## Phase 4: Judging\n\n**Step 1: Gate check**\n- All tests pass\n- Design adherence - implemented what the design specified\n\n**Step 2: Check for identical implementations**\n\nBefore fresh-eyes, diff the implementations:\n```bash\ndiff -r .worktrees/cookoff-impl-1/src .worktrees/cookoff-impl-2/src\n```\n\nIf implementations are >95% identical, note this - the planning step didn't create enough variation. Still proceed but flag in results.\n\n**Step 3: Fresh-eyes on survivors**\n```\nStarting fresh-eyes review of impl-1 (N files)...\nChecking: security, logic errors, edge cases\nFresh-eyes complete: 1 minor issue\n```\n\n### Step 4: Invoke Judge Skill\n\n**CRITICAL: Invoke `test-kitchen:judge` now.**\n\nThe judge skill contains the full scoring framework with checklists. Invoking it fresh ensures the scoring format is followed exactly.\n\n```text\nInvoke: test-kitchen:judge\n\nContext to provide:\n- Implementations to judge: impl-1, impl-2, impl-3 (or however many)\n- Worktree locations: .worktrees/cookoff-impl-N/\n- Test results from each implementation\n- Fresh-eyes findings from Step 3\n- Feasibility flags identified\n```\n\nThe judge skill will:\n1. Fill out the complete scoring worksheet for each implementation\n2. Build the scorecard with integer scores (1-5, no half points)\n3. Check hard gates (Fitness Δ≥2, any score=1)\n4. Announce winner with rationale\n\n**Do not summarize or abbreviate the scoring.** The judge skill output should be the full worksheet.\n\n**Cookoff-specific context:** In cookoff, all implementations target the same design, so Fitness should be similar. A Fitness gap (Δ≥2) indicates one implementation deviated from or misunderstood the design - not a different approach choice.\n\n## Phase 5: Completion\n\n**Verification on winner:**\n```\nRunning final verification on winner (impl-2):\n- npm test: 22/22 passing ✓\n- npm run build: exit 0 ✓\n- Design adherence: all requirements met ✓\n\nVerification complete. Winner confirmed.\n```\n\n**Winner:** Use `finish-branch`\n- Options: merge locally, create PR, keep as-is, discard\n\n**Losers:** Cleanup\n```bash\ngit worktree remove .worktrees/cookoff-impl-1\ngit worktree remove .worktrees/cookoff-impl-3\ngit branch -D <feature>/cookoff/impl-1\ngit branch -D <feature>/cookoff/impl-3\n# Keep winner's worktree until merged\n```\n\n**Write result.md:**\n```markdown\n# Cookoff Results: <feature>\n\n## Design\ndocs/plans/<feature>/design.md\n\n## Implementations\n| Impl | Plan Approach | Tests | Fresh-Eyes | Lines | Result |\n|------|---------------|-------|------------|-------|--------|\n| impl-1 | Component-first | 24/24 | 1 minor | 680 | eliminated |\n| impl-2 | Data-layer-first | 22/22 | 0 issues | 720 | WINNER |\n| impl-3 | TDD-strict | 26/26 | 2 minor | 590 | eliminated |\n\n## Plans Generated\n- impl-1: docs/plans/<feature>/cookoff/impl-1/plan.md\n- impl-2: docs/plans/<feature>/cookoff/impl-2/plan.md\n- impl-3: docs/plans/<feature>/cookoff/impl-3/plan.md\n\n## Winner Selection\nReason: Clean fresh-eyes review, solid data-layer-first architecture\n\n## Cleanup\nWorktrees removed: 2\nBranches deleted: <feature>/cookoff/impl-1, <feature>/cookoff/impl-3\nWinner branch: <feature>/cookoff/impl-2\n```\n\nSave to: `docs/plans/<feature>/cookoff/result.md`\n\n## Skills Orchestrated\n\n| Dependency | Phase | Usage |\n|------------|-------|-------|\n| `writing-plans` | 3 | Each subagent creates their own implementation plan |\n| `executing-plans` | 3 | Each subagent implements their plan |\n| `parallel-agents` | 3 | Dispatch ALL subagents in SINGLE message |\n| `git-worktrees` | 3 | Create worktree per implementation |\n| `tdd` | 3 | Subagents follow RED-GREEN-REFACTOR |\n| `verification` | 3, 5 | Before claiming done; before declaring winner |\n| `code-review` | 3 | Review each impl after completion |\n| `fresh-eyes` | 4 | Quality review → judge input |\n| `judge` | 4 | **INVOKE** for scoring framework (loads fresh, ensures format compliance) |\n| `scenario-testing` | 4 | Validate if scenarios defined |\n| `finish-branch` | 5 | Handle winner, cleanup losers |\n\n## Common Mistakes\n\n**Sharing a pre-made implementation plan**\n- Problem: All teams copy same code, no variation\n- Fix: Each team uses writing-plans to create THEIR OWN plan from design doc\n\n**Dispatching agents in separate messages**\n- Problem: Serial dispatch instead of parallel\n- Fix: Send ALL Task tools in a SINGLE message\n\n**Not using writing-plans + executing-plans**\n- Problem: Subagent implements ad-hoc\n- Fix: Each subagent MUST use writing-plans then executing-plans\n\n**Skipping fresh-eyes**\n- Problem: Judge has no quality signal, just test counts\n- Fix: Fresh-eyes on ALL survivors before comparing\n\n**Not checking for identical implementations**\n- Problem: Wasted compute on duplicates\n- Fix: Diff implementations before fresh-eyes, flag if >95% similar\n\n**Forgetting cleanup**\n- Problem: Orphaned worktrees and branches\n- Fix: Always cleanup losers, write result.md\n\n## Example Invocation\n\n```\nUser: \"Let's build this\" (after brainstorming produced design.md)\n\nClaude: I'm using cookoff.\n\nHow would you like to implement this design?\n\n1. Single subagent - One agent plans and implements\n2. Cookoff - 3 parallel agents, each creates own plan, pick best\n   → Complexity: medium feature, touches auth\n   → Recommendation: 3 implementations\n3. Local - Plan and implement here\n\nUser: \"2\"\n\nClaude: Spawning 3 parallel implementations...\nEach will create their own implementation plan from the design.\n\n[Phase 3: Create worktrees, dispatch ALL 3 agents in single message]\n[Each agent: reads design → writes plan → implements]\n\nCookoff status:\n- impl-1: planning → implementing 6/8 tasks\n- impl-2: planning → implementing 4/7 tasks\n- impl-3: planning → implementing 5/9 tasks\n\n[All 3 complete]\n\n[Phase 4: Diff check - implementations are different ✓]\n[Fresh-eyes on all 3]\n\n| Impl | Plan Approach | Tests | Fresh-Eyes |\n|------|---------------|-------|------------|\n| impl-1 | Component-first | 24/24 | 1 minor |\n| impl-2 | Data-layer-first | 22/22 | 0 issues |\n| impl-3 | TDD-strict | 26/26 | 2 minor |\n\nRecommendation: impl-2 (cleanest)\nUser: \"2\"\n\n[Phase 5: Verify winner, cleanup losers]\n\nWinner: impl-2 ready to merge\nCleanup: 2 worktrees removed\nPlans preserved: docs/plans/<feature>/cookoff/\n```\n",
        "test-kitchen/skills/judge/SKILL.md": "---\nname: judge\ndescription: Scoring framework for test-kitchen cookoff and omakase-off. Invoked at Phase 4 to evaluate implementations using 5-criteria scoring. Do not invoke directly - called by cookoff/omakase-off.\n---\n\n# Test Kitchen Judge\n\nScore implementations using the 5-criteria framework. Fill out ALL sections exactly as shown.\n\n**Terminology:** This skill uses \"impl\" but works for both:\n- Cookoff: impl-1, impl-2, impl-3 (same design, different implementations)\n- Omakase: variant-a, variant-b (different approaches/designs)\n\n## REQUIRED OUTPUT FORMAT\n\nYou MUST produce this exact structure. Do not summarize or abbreviate.\n\n```markdown\n## Gate Check\n| Impl | Tests Pass | Design Adherence |\n|------|------------|------------------|\n| impl-1 | X/X ✓ or ✗ | Yes/No |\n| impl-2 | X/X ✓ or ✗ | Yes/No |\n\n## Feasibility Check\n| Impl | Status | Notes |\n|------|--------|-------|\n| impl-1 | ✓ OK / ⚠️ Flag | Details |\n| impl-2 | ✓ OK / ⚠️ Flag | Details |\n\n## Scoring Worksheet\n\n### impl-1\n**Fitness for Purpose** (Does it solve the actual problem?)\n\n*Functional requirements:*\n- [ ] Primary use case works end-to-end?\n- [ ] All explicitly stated requirements implemented?\n- [ ] Handles realistic scenarios, not just happy path?\n\n*User needs (beyond literal requirements):*\n- [ ] Would the user actually use this, or just demo it?\n- [ ] Does it solve the real problem, not just the literal request?\n- [ ] Does deployment/distribution match stated needs?\n\n*Future considerations (if relevant):*\n- [ ] If growth/scaling mentioned, does architecture support it?\n- [ ] If team/collaboration mentioned, is it maintainable by others?\n\nChecklist: _/8 YES → **Score: _/5** (7-8=5, 5-6=4, 4=3, 2-3=2, 0-1=1)\n*Note: Not all items apply to every project. Score based on relevant items.*\n\n**Justified Complexity** (Every line earning its keep?)\n- Unnecessary abstractions: ___\n- Dead code: ___\n- Bloat estimate: ___%\n\n*Line count comparison (if multiple impls):*\n- This impl: ___ lines\n- Smallest impl: ___ lines\n- Extra lines justified by: ___\n\n→ **Score: _/5** (5=minimal, 4=slight bloat <10%, 3=10-25% bloat, 2=25-50%, 1=>50%)\n\n**Readability** (Understand core flow in 5 min?)\nViolations:\n- [ ] Single-letter vars (not loop index): +1 each = __\n- [ ] Functions >50 lines: +1 each = __\n- [ ] Nesting >3 levels: +1 each = __\n- [ ] Magic numbers: +1 each = __\n- [ ] Bad function names: +1 each = __\nTotal violations: __ → **Score: _/5** (0=5, 1-2=4, 3-4=3, 5-7=2, 8+=1)\n\n**Robustness & Scale** (Handles unexpected + growth?)\n- [ ] Input validation?\n- [ ] External call error handling?\n- [ ] Useful error messages?\n- [ ] Null/empty handling?\n- [ ] Async timeouts?\n- [ ] No unbounded loops?\n- [ ] O(n log n) or better?\n- [ ] Bounded memory?\n- [ ] Queries paginated?\n- [ ] No blocking I/O in hot path?\n- [ ] Backoff/retry logic?\n- [ ] Handles 10x load?\nChecklist: _/12 YES + feasibility flags → **Score: _/5**\n(11-12 + no flags=5, 9-10 or minor flag=4, 7-8=3, 5-6 or major flag=2, <5 or critical flag=1)\n\n**Maintainability** (Pain of next change?)\n- [ ] Single responsibility per function?\n- [ ] Explicit dependencies (no globals)?\n- [ ] Business logic separated from infra?\n- [ ] New feature = ≤3 files changed?\n- [ ] Config externalized?\n- [ ] Tests catch regressions?\nChecklist: _/6 YES → **Score: _/5** (6=5, 5=4, 4=3, 2-3=2, 0-1=1)\n\n### impl-2\n[REPEAT SAME FORMAT]\n\n### impl-3 (if applicable)\n[REPEAT SAME FORMAT]\n\n## Judge Scorecard\n| Criterion | impl-1 | impl-2 | impl-3 | Best |\n|-----------|--------|--------|--------|------|\n| Fitness for Purpose | | | | |\n| Justified Complexity | | | | |\n| Readability | | | | |\n| Robustness & Scale | | | | |\n| Maintainability | | | | |\n| **TOTAL** | /25 | /25 | /25 | |\n\n## Hard Gates\n| Gate | Result |\n|------|--------|\n| Fitness Gate (Δ ≥ 2) | Triggered/Not triggered |\n| Critical Flaw (any = 1) | Triggered/Not triggered |\n\n## Winner Selection\n**Winner: impl-X** (Score: __/25)\n\n**Selection rationale:**\n[2-3 sentences explaining WHY this implementation won]\n\n**Trade-offs acknowledged:**\n[What the other implementations did better]\n```\n\n## Scoring Reference\n\n### Scores Meaning\n| Score | Meaning |\n|-------|---------|\n| 5 | Excellent - exceeds expectations |\n| 4 | Good - fully meets requirements |\n| 3 | Adequate - core works, some gaps |\n| 2 | Poor - significant issues |\n| 1 | Critical flaw - disqualifying |\n\n### Hard Gates (Automatic)\n\n1. **Fitness Gate:** If Fitness Δ ≥ 2 between impls → Higher fitness WINS immediately\n2. **Critical Flaw:** If ANY criterion = 1 → That impl is ELIMINATED\n\n#### Fitness Gate Interpretation\n\nThe Fitness Gate triggers the same way in both contexts, but means different things:\n\n| Context | What Fitness Δ ≥ 2 Means |\n|---------|--------------------------|\n| **Cookoff** | One implementation *deviated from or misunderstood the design*. All impls should have similar Fitness since they're implementing the same spec. A large gap is a red flag. |\n| **Omakase** | One approach *genuinely solves the problem better*. Different approaches can legitimately have different Fitness. A large gap means one approach is clearly superior. |\n\nIn both cases, higher Fitness wins. The interpretation just explains *why* the gap exists.\n\n### Feasibility Red Flags\n\nCheck before scoring:\n- O(n²) or worse on unbounded data\n- Unbounded memory growth\n- Self-DDoS patterns (polling, no backoff)\n- Missing pagination\n- Blocking I/O in hot path\n- No error recovery\n\n## Process\n\n1. **Read** all implementation code (should already be in context)\n2. **Fill out** the worksheet for EACH implementation - do not skip sections\n3. **Check** hard gates\n4. **Announce** winner with rationale\n\n**CRITICAL:** Use integer scores only (1-5). Do not use half points like 4.5.\n\n**CRITICAL:** Fill out every checkbox. Do not summarize or abbreviate the worksheet.\n",
        "test-kitchen/skills/omakase-off/SKILL.md": "---\nname: omakase-off\ndescription: This skill should be used as the entry gate for build/create/implement requests. Triggers on \"build X\", \"create Y\", \"implement Z\", \"add feature\", \"try both approaches\", \"not sure which approach\". Offers brainstorm-together or omakase (chef's choice parallel exploration) options. Detects indecision during brainstorming to offer parallel exploration.\n---\n\n# Omakase-Off\n\nChef's choice exploration - when you're not sure WHAT to build, explore different approaches in parallel.\n\n**Part of Test Kitchen Development:**\n- `omakase-off` - Chef's choice exploration (different approaches/plans)\n- `cookoff` - Same recipe, multiple cooks compete (same plan, multiple implementations)\n\n**Core principle:** Let indecision emerge naturally during brainstorming, then implement multiple approaches in parallel to let real code + tests determine the best solution.\n\n## Three Triggers\n\n### Trigger 1: BEFORE Brainstorming\n\n**When:** \"I want to build...\", \"Create a...\", \"Implement...\", \"Add a feature...\"\n\n**Present:**\n```\nBefore we brainstorm the details, would you like to:\n\n1. Brainstorm together - We'll explore requirements and design step by step\n2. Omakase (chef's choice) - I'll generate 3-5 best approaches, implement them\n   in parallel, and let tests pick the winner\n```\n\n### Trigger 2: DURING Brainstorming (Indecision Detection)\n\n**Detection signals:**\n- 2+ uncertain responses in a row on architectural decisions\n- Phrases: \"not sure\", \"don't know\", \"either works\", \"you pick\", \"no preference\"\n\n**When detected:**\n```\nYou seem flexible on the approach. Would you like to:\n1. I'll pick what seems best and continue brainstorming\n2. Explore multiple approaches in parallel (omakase-off)\n```\n\n### Trigger 3: Explicitly Requested\n\n- \"try both approaches\", \"explore both\", \"omakase\"\n- \"implement both variants\", \"let's see which is better\"\n\n## Workflow Overview\n\n| Phase | Description |\n|-------|-------------|\n| **0. Entry** | Present brainstorm vs omakase choice |\n| **1. Brainstorm** | Passive slot detection during design |\n| **1.5. Decision** | If slots detected, offer parallel exploration |\n| **2. Plan** | Generate implementation plan per variant |\n| **3. Implement** | Dispatch ALL agents in SINGLE message |\n| **4. Evaluate** | Scenario tests → fresh-eyes → judge survivors |\n| **5. Complete** | Finish winner, cleanup losers |\n\nSee `references/detailed-workflow.md` for full phase details.\n\n## Directory Structure\n\n```\ndocs/plans/<feature>/\n  design.md                  # Shared context from brainstorming\n  omakase/\n    variant-<slug>/\n      plan.md                # Implementation plan for this variant\n    result.md                # Final report\n\n.worktrees/\n  variant-<slug>/            # Omakase variant worktree\n```\n\n## Slot Classification\n\n| Type | Examples | Worth exploring? |\n|------|----------|------------------|\n| **Architectural** | Storage engine, framework, auth method | Yes |\n| **Trivial** | File location, naming, config format | No |\n\nOnly architectural decisions become slots for parallel exploration.\n\n## Variant Limits\n\n**Max 5-6 implementations.** Don't do full combinatorial explosion:\n1. Identify the primary axis (biggest architectural impact)\n2. Create variants along that axis\n3. Fill secondary slots with natural pairings\n\n## Critical Rules\n\n1. **Dispatch ALL variants in SINGLE message** - Multiple Task tools, one message\n2. **MUST use scenario-testing** - Not manual verification\n3. **Fresh-eyes on survivors** - Required before judge comparison\n4. **Always cleanup losers** - Remove worktrees and branches\n5. **Write result.md** - Document what was tried and why winner won\n\n## Skills Orchestrated\n\n| Dependency | Usage |\n|------------|-------|\n| `brainstorming` | Modified flow with passive slot detection |\n| `writing-plans` | Generate implementation plan per variant |\n| `git-worktrees` | Create isolated worktree per variant |\n| `parallel-agents` | Dispatch all variant subagents in parallel |\n| `scenario-testing` | Run same scenarios against all variants |\n| `fresh-eyes` | Quality review on survivors → input for judge |\n| `finish-branch` | Handle winner (merge/PR), cleanup losers |\n\n## Example Flow\n\n```\nUser: \"I need to build a CLI todo app.\"\n\nClaude: [Triggers omakase-off]\nBefore we dive in, how would you like to approach this?\n1. Brainstorm together\n2. Omakase (chef's choice)\n\nUser: \"1\"\n\nClaude: [Brainstorming proceeds, detects indecision on storage]\n\nYou seem flexible on storage (JSON vs SQLite). Would you like to:\n1. Explore in parallel - I'll implement both variants\n2. Best guess - I'll pick JSON (simpler)\n\nUser: \"1\"\n\n[Creates plans for variant-json, variant-sqlite]\n[Dispatches parallel agents in SINGLE message]\n[Runs scenario tests on both]\n[Fresh-eyes review on survivors]\n[Presents comparison, user picks winner]\n[Cleans up loser, finishes winner branch]\n```\n",
        "test-kitchen/skills/omakase-off/references/detailed-workflow.md": "# Omakase-Off Detailed Workflow\n\n## Skill Dependencies\n\nThis skill orchestrates other skills. Check what's installed and use fallbacks if needed.\n\n| Reference | Primary (if installed) | Fallback (if not) |\n|-----------|------------------------|-------------------|\n| `brainstorming` | `superpowers:brainstorming` | Ask questions one at a time, propose 2-3 approaches, validate incrementally |\n| `writing-plans` | `superpowers:writing-plans` | Write detailed plan with file paths, code examples, verification steps |\n| `git-worktrees` | `superpowers:using-git-worktrees` | `git worktree add .worktrees/<name> -b <branch>`, verify .gitignore |\n| `parallel-agents` | `superpowers:dispatching-parallel-agents` | Dispatch multiple Task tools in single message, review when all return |\n| `subagent-dev` | `superpowers:subagent-driven-development` | Fresh subagent per task, code review between tasks |\n| `tdd` | `superpowers:test-driven-development` | Write test first, watch fail, write minimal code, refactor |\n| `scenario-testing` | `scenario-testing:skills` (2389) | Create `.scratch/` E2E scripts, real dependencies, no mocks |\n| `verification` | `superpowers:verification-before-completion` | Run verification command, read output, THEN claim status |\n| `fresh-eyes` | `fresh-eyes-review:skills` (2389) | 2-5 min review for security, logic errors, edge cases |\n| `judge` | `test-kitchen:judge` | Scoring framework with checklists (MUST invoke at Phase 4) |\n| `code-review` | `superpowers:requesting-code-review` | Dispatch code-reviewer subagent with SHA range |\n| `finish-branch` | `superpowers:finishing-a-development-branch` | Verify tests, present options (merge/PR/keep/discard) |\n\n**At skill start:** Announce which dependencies are available.\n\n## Phase 0: Entry Point\n\n**When user requests \"build/create/implement X\":**\n\nPresent the choice BEFORE starting detailed brainstorming:\n```\nBefore we dive into the details, how would you like to approach this?\n\n1. Brainstorm together - We'll explore requirements and design step by step\n2. Omakase (chef's choice) - I'll generate 3-5 best approaches, implement\n   them in parallel, and let tests pick the winner\n```\n\n**If user picks Omakase (option 2):**\n1. Quick context gathering (1-2 essential questions only)\n2. Generate 3-5 distinct architectural approaches\n3. Jump directly to Phase 2 (Plan Generation)\n\n**If user picks Brainstorm (option 1):**\nContinue to Phase 1.\n\n## Phase 1: Brainstorming with Passive Slot Detection\n\n**First, check if a brainstorming skill is available:**\n- If available → invoke it and passively detect indecision during the flow\n- If NOT available → do brainstorming yourself using fallback behavior\n\n**During brainstorming, passively detect indecision:**\n\n**Detection signals:**\n- Explicit: \"slot\", \"try both\", \"explore both\"\n- Uncertain: \"not sure\", \"hmm\", \"either could work\", \"both sound good\"\n- Deferring: \"you pick\", \"whatever you think\"\n\n**Slot classification:**\n| Type | Examples | Worth exploring? |\n|------|----------|------------------|\n| **Architectural** | Storage engine, framework, auth method, API style | Yes |\n| **Trivial** | File location, naming conventions, config format | No |\n\nOnly architectural decisions become real slots.\n\n**Fast path detection:**\nAfter 2+ uncertain answers in a row:\n```\nYou seem flexible on the details. Want me to:\n1. Make sensible defaults and you flag anything wrong\n2. Continue exploring each decision\n```\n\n## Phase 1.5: End-of-Brainstorm Decision\n\n**If NO architectural slots were collected:**\nHand off to cookoff for implementation.\n\n**If slots WERE collected:**\n```\nI noticed some open decisions during our brainstorm:\n- Storage: JSON vs SQLite\n- Auth: JWT vs session-based\n\nWould you like to:\n1. Explore in parallel - I'll implement both variants and let tests decide\n2. Best guess - I'll pick what seems best and proceed with one plan\n```\n\n**Combination limits (max 5-6 implementations):**\n\nWhen multiple slots exist, don't do full combinatorial explosion. Instead:\n1. **Identify the primary axis** - Which slot has the biggest architectural impact?\n2. **Create variants along that axis** - Each variant explores a different primary choice\n3. **Fill in secondary slots** with the most natural pairing\n\n## Phase 2: Plan Generation\n\nFor each variant combination:\n\n1. Generate full implementation plan using `writing-plans`\n2. Store in structured directory:\n\n```\ndocs/plans/<feature>/\n  design.md                  # Shared context from brainstorming\n  omakase/\n    variant-<slug-1>/\n      plan.md                # Implementation plan for this variant\n    variant-<slug-2>/\n      plan.md\n    result.md                # Final report (written at end)\n```\n\n## Phase 3: Implementation\n\n**Setup worktrees:**\n- Worktree location: `.worktrees/`\n- Branch naming: `<feature>/omakase/<variant-name>`\n- All worktrees created before implementation starts\n\n**CRITICAL: Dispatch ALL variants in a SINGLE message**\n\n```\n<single message>\n  Task(variant-json, run_in_background: true)\n  Task(variant-sqlite, run_in_background: true)\n</single message>\n```\n\n**Subagent workflow:**\n1. Read their variant's plan\n2. Execute tasks using `subagent-dev`\n3. Follow `tdd` - write test first, watch fail, implement, pass\n4. Use `verification` - run tests, read output, THEN claim complete\n5. Report back: summary, test counts, files changed, issues\n\n## Phase 4: Evaluation\n\n**Step 1: Scenario testing (REQUIRED)**\n- MUST use `scenario-testing` - not manual verification\n- Same scenarios run against ALL variants\n- Must pass all scenarios to be a \"survivor\"\n\n**Step 2: Fresh-eyes review on survivors**\nFor each variant that passed scenarios, use `fresh-eyes`.\n\n**Step 3: Elimination**\n| Situation | Action |\n|-----------|--------|\n| Fails tests | Eliminated |\n| Fails scenarios | Eliminated |\n| Critical security issue | Eliminated |\n| All fail | Report failures, ask user how to proceed |\n| One survives | Auto-select |\n\n### Step 4: Invoke Judge Skill\n\n**CRITICAL: Invoke `test-kitchen:judge` now.**\n\nThe judge skill contains the full scoring framework with checklists. Invoking it fresh ensures the scoring format is followed exactly.\n\n```text\nInvoke: test-kitchen:judge\n\nContext to provide:\n- Variants to judge: variant-a, variant-b (or however many)\n- Worktree locations: .worktrees/variant-<name>/\n- Test results from each variant\n- Fresh-eyes findings from Step 2\n- Feasibility flags identified\n```\n\nThe judge skill will:\n1. Fill out the complete scoring worksheet for each variant\n2. Build the scorecard with integer scores (1-5, no half points)\n3. Check hard gates (Fitness Δ≥2, any score=1)\n4. Announce winner with rationale\n\n**Do not summarize or abbreviate the scoring.** The judge skill output should be the full worksheet.\n\n**Omakase-specific context:** In omakase, different variants represent different *approaches* to solving the problem. A Fitness gap (Δ≥2) means one approach genuinely solves the problem better - this is a legitimate win, not a design deviation.\n\n## Phase 5: Completion\n\n**Winner:** Use `finish-branch`\n- Verify all tests pass\n- Present options: merge locally, create PR, keep as-is, discard\n- Execute user's choice\n\n**Losers:** Cleanup\n```bash\ngit worktree remove <worktree-path>\ngit branch -D <feature>/omakase/<variant>\n```\n\n**Write result.md:**\n```markdown\n# Omakase-Off Results: <feature>\n\n## Variants\n| Variant | Tests | Scenarios | Fresh-Eyes | Result |\n|---------|-------|-----------|------------|--------|\n| variant-json | 12/12 | PASS | 0 issues | WINNER |\n| variant-sqlite | 15/15 | PASS | 1 minor | eliminated |\n\n## Winner Selection\nReason: [Why this variant won]\n\n## Cleanup\nWorktrees removed: N\nBranches deleted: [list]\n```\n\n## Common Mistakes\n\n**Too many slots**\n- Problem: Combinatorial explosion\n- Fix: Cap at 5-6, ask user to constrain if exceeded\n\n**Ad-hoc scenario testing**\n- Problem: Manual verification instead of real scenario tests\n- Fix: MUST use `scenario-testing` skill\n\n**Forgetting cleanup**\n- Problem: Orphaned worktrees and branches\n- Fix: Always cleanup losers, write result.md\n",
        "worldview-synthesis/.claude-plugin/plugin.json": "{\n  \"name\": \"worldview-synthesis\",\n  \"description\": \"Systematic worldview articulation - surface beliefs, identify tensions, generate narrative outputs for personal philosophy documentation\",\n  \"version\": \"1.0.0\"\n}\n",
        "worldview-synthesis/README.md": "# Worldview Synthesis Plugin\n\nSystematic worldview articulation - surface beliefs, identify tensions, and generate narrative outputs for personal philosophy documentation.\n\n## Installation\n\n```bash\n/plugin install worldview-synthesis@2389-research\n```\n\n## What This Plugin Provides\n\n### Skills\n\n- **worldview-synthesis** - Complete methodology for articulating personal philosophy through structured interrogation, tension mapping, and narrative generation\n\n### Reference Materials\n\n- **interrogation-questions.md** - 6 rounds of ready-to-use questions covering mortality, body, relationships, emotion, work, ethics, society, and future\n\n## Quick Example\n\nStart with: \"Help me articulate my worldview\"\n\nThe skill guides you through:\n1. **Bootstrap** - Create project structure for beliefs and narratives\n2. **Seed** - Extract ideas from your influences (books, people, experiences)\n3. **Interrogate** - 4-6 rounds of multi-choice questions across 20 domains\n4. **Capture Tensions** - Name contradictions, don't resolve them\n5. **Generate Narratives** - Mission → Thesis → Synopsis → Full Narrative\n6. **Iterate** - A worldview is living, update as you evolve\n\n## Core Principle\n\nA worldview isn't a list of opinions—it's a graph of beliefs with tensions. The goal is to surface what you already believe, name the contradictions, and synthesize into something you can share.\n\n## Output Examples\n\n**Mission (~100 words):**\n```\nPut people first. Prepare for what's coming. Fight anyway.\nFind the cracks. Leave no trace.\n```\n\n**Idea Node:**\n```yaml\n- id: strategic-ruthlessness\n  claim: \"Sometimes you have to crush opponents\"\n  confidence: 0.7\n  tensions: [collaboration-over-competition]\n```\n\n**Tension:**\n```yaml\n- id: collaboration-vs-ruthlessness\n  status: embraced  # Live in the paradox\n```\n\n## When This Skill Applies\n\n- \"I want to articulate my values\"\n- \"Help me figure out what I believe\"\n- Writing a manifesto or leadership philosophy\n- Defining company culture\n- Personal philosophy documentation\n\n## Links\n\n- [Plugin CLAUDE.md](./CLAUDE.md) - Development instructions\n",
        "worldview-synthesis/skills/SKILL.md": "---\nname: worldview-synthesis\ndescription: This skill should be used when someone wants to articulate, explore, or document their personal worldview, values, or philosophy. Triggers on \"articulate my values\", \"figure out what I believe\", \"document my philosophy\", \"write a manifesto\", \"define my leadership philosophy\", \"explore my beliefs\". Surfaces beliefs through systematic interrogation, identifies tensions, and generates narrative outputs.\n---\n\n# Worldview Synthesis\n\n**Core principle:** A worldview isn't a list of opinions—it's a graph of beliefs with tensions. The goal is to surface what someone already believes, name the contradictions, and synthesize into something they can share.\n\n## When to Use\n\n- Someone says \"I want to articulate my values\"\n- Someone says \"help me figure out what I believe\"\n- Someone wants to document their philosophy\n- Someone is preparing for leadership, writing a manifesto, or defining a company culture\n\n## The Method\n\n### Phase 1: Bootstrap Structure\n\nCreate project structure:\n\n```\nworldview/\n├── data/\n│   ├── schema.yaml      # Structure definitions\n│   ├── ideas.yaml       # Belief nodes\n│   ├── sources.yaml     # Influences (books, people, experiences)\n│   └── tensions.yaml    # Productive paradoxes\n├── narrative/\n│   ├── mission.md       # One-liner + principles\n│   ├── thesis.md        # One page\n│   ├── synopsis.md      # Three sections\n│   └── full-narrative.md\n└── README.md\n```\n\n### Phase 2: Seed from Sources\n\nAsk: \"What books, articles, people, or experiences shaped how you see the world?\"\n\nFor each source, extract 3-5 key ideas. This gives you initial nodes to build from.\n\n### Phase 3: Interrogation Rounds\n\nRun 4-6 rounds of questions. Each round covers 3-4 domains.\n\n**Question Design Rules:**\n- 2-4 options per question, each with label + description\n- Use `multiSelect: true` when beliefs can coexist\n- Leave room for custom \"Other\" answers\n- Options should be genuinely different, not leading\n\n**Domains to Cover:**\n\n| Domain | Example Questions |\n|--------|-------------------|\n| **Mortality** | How does knowing you'll die shape how you live? |\n| **Metaphysics** | What's your relationship with spirituality/religion? |\n| **Relationships** | How do you think about romantic partnership? |\n| **Parenting** | Philosophy on having/raising children? |\n| **Body** | How do you relate to physical health and aging? |\n| **Vices** | Relationship with alcohol, drugs, pleasure? |\n| **Money** | Beyond spending—freedom, obligation, suspicion? |\n| **Competition** | Collaboration vs ruthlessness? |\n| **Trust** | Default open or earned? |\n| **Learning** | Autodidact, mentorship, formal education? |\n| **Nature** | Essential or nice to visit? |\n| **Leadership** | Natural, reluctant, servant, example? |\n| **Emotion** | Relationship with anger? |\n| **Recognition** | Need fame? Already had it? |\n| **Rest** | Protect sleep or run on fumes? |\n| **Conflict** | Clear air fast or avoid? |\n| **Work** | Philosophy on effort, failure, shipping? |\n| **Ethics** | Hard lines vs softer truths? |\n| **Society** | Diagnosis of what's broken? |\n| **Future** | Optimism, pessimism, preparation? |\n\n### Phase 4: Capture Tensions\n\nWhen beliefs contradict, DON'T resolve—NAME:\n\n```yaml\n- id: collaboration-vs-ruthlessness\n  ideas: [collaboration-over-competition, strategic-ruthlessness]\n  description: \"Default to positive-sum, but crush when necessary\"\n  resolution: |\n    Different contexts call for different modes. Collaboration is default.\n    Ruthlessness is available when needed. The key is knowing when to switch.\n  status: embraced  # or: unresolved, resolved\n```\n\nTensions are often the most interesting part of a worldview.\n\n### Phase 5: Generate Narratives\n\nFrom data, generate at ascending scales:\n\n1. **Mission** (~100 words): The one-liner + 5-7 principles\n2. **Thesis** (~300 words): One page that captures the core\n3. **Synopsis** (~500 words): Three sections (Diagnosis, Orientation, Ethics)\n4. **Full Narrative** (~2000 words): Complete essay with all major themes\n\n### Phase 6: Iterate\n\nA worldview is living. Add new beliefs, update old ones, regenerate narratives.\n\n## Idea Node Schema\n\n```yaml\n- id: kebab-case-unique-id\n  title: \"Human Readable Title\"\n  domain: personal | ethics | society | technology | metaphysics\n  claim: \"The actual belief in one clear sentence\"\n  confidence: 0.0-1.0  # how sure?\n  importance: 0.0-1.0  # how central to worldview?\n  tags: [relevant, keywords]\n  sources: [source-ids-if-any]\n  supports: [ideas-this-reinforces]\n  tensions: [ideas-this-contradicts]\n  notes: \"Context, caveats, origins\"\n```\n\n## Tension Statuses\n\n- **embraced**: Both sides are true. Live in the paradox.\n- **resolved**: Found synthesis that dissolves the tension.\n- **unresolved**: Genuinely don't know. Honest about uncertainty.\n\n## Sample Interrogation Round\n\n```\nRound 3: Money, Competition, Trust\n\nQ1: How do you think about money beyond 'spend it'?\n- Tool for freedom: Money buys optionality and autonomy\n- Obligation to share: If you have more, redistribute\n- Wealth is suspect: Getting rich usually means exploitation\n- Generational thinking: Think about what to leave behind\n[multiSelect: true]\n\nQ2: What's your orientation toward competition?\n- Compete hard, play fair: Want to win but not by cheating\n- Collaboration over competition: Prefer positive-sum games\n- Against yourself mostly: Real competition is self-improvement\n- Strategic ruthlessness: Sometimes you have to crush opponents\n[multiSelect: true]\n\nQ3: How do you approach trust with new people?\n- Trust until betrayed: Default open, pull back if needed\n- Trust is earned: Start cautious, let people prove themselves\n- Read the situation: Neither default—assess individually\n- Trust systems not people: Rely on structures over character\n[multiSelect: false]\n```\n\n## Red Flags\n\n- **\"I don't have a worldview\"** → Everyone does. Start with sources.\n- **No tensions found** → Dig deeper. Everyone has contradictions.\n- **All high confidence** → Push on uncertainty. What don't you know?\n- **Only \"should\" beliefs** → Ask what they actually DO, not just believe.\n- **Avoiding hard questions** → Death, money, conflict—go there.\n\n## Output Quality Checklist\n\n- [ ] Core thesis is one sentence\n- [ ] Mission fits on a card\n- [ ] Tensions are named, not hidden\n- [ ] Hard lines are clear (non-negotiables)\n- [ ] Softer truths acknowledged (where grace lives)\n- [ ] Narrative voice sounds like the person\n- [ ] Contradictions are embraced, not resolved away\n\n## Example Mission Output\n\n```markdown\n# Mission Statement\n\n**Put people first. Prepare for what's coming. Fight anyway.\nFind the cracks. Leave no trace.**\n\n---\n\nWe operate with systemic pessimism and local optimism.\nWe hold strong opinions weakly.\nWe embrace productive paradoxes.\nWe draw hard lines on human rights.\nWe extend grace for pain, never for harm.\n\nPeople first. Always.\n```\n",
        "worldview-synthesis/skills/references/interrogation-questions.md": "# Worldview Interrogation Questions\n\nReady-to-use question sets organized by round. Each question includes options with descriptions suitable for the AskUserQuestion tool.\n\n## Round 1: Foundations\n\n### Death & Mortality\n**Question:** How does knowing you'll die shape how you live?\n| Option | Description |\n|--------|-------------|\n| Motivating urgency | Death makes life precious - drives me to act now |\n| Peaceful acceptance | It's coming, I've made peace, doesn't dominate thinking |\n| Fuel for legacy | Think about what I leave behind, how I'll be remembered |\n| Terror I manage | Honestly scares me, I cope in various ways |\n*multiSelect: true*\n\n### Spirituality\n**Question:** What's your relationship with spirituality or religion?\n| Option | Description |\n|--------|-------------|\n| Atheist/materialist | No gods, no spirits - just matter and emergence |\n| Spiritual not religious | Something's there - awe, mystery - but not organized |\n| Complicated history | Raised religious, complicated feelings now |\n| Practice without belief | Meditation, ritual for benefits, not metaphysics |\n*multiSelect: true*\n\n### Love & Partnership\n**Question:** How do you think about romantic love and long-term partnership?\n| Option | Description |\n|--------|-------------|\n| Crew of two | Partner is core crew - us against the world |\n| Independence within | Deep connection but separate selves crucial |\n| Chosen family expands | Partnership as one node in broader network |\n| Skeptical of forever | Love is real but 'til death is weird promise |\n*multiSelect: true*\n\n### Children\n**Question:** What's your philosophy on having/raising children?\n| Option | Description |\n|--------|-------------|\n| Moral obligation if you do | If you have kids, you owe them everything |\n| Not everyone should | Having kids isn't for everyone, that's fine |\n| Complicated given state of world | Hard to justify bringing kids into this |\n| Legacy through mentorship | Don't need bio kids - teaching is enough |\n*multiSelect: true*\n\n---\n\n## Round 2: Body & Pleasure\n\n### Physical Health\n**Question:** How do you relate to your body and physical health?\n| Option | Description |\n|--------|-------------|\n| Instrument to maintain | Body is a tool - keep functional, don't fetishize |\n| Source of pleasure | Embodiment is joy - food, movement, sensation |\n| Complicated relationship | History of struggle, working on it |\n| Aging gracefully | Accept decline, don't fight it desperately |\n*multiSelect: true*\n\n### Vices\n**Question:** What's your relationship with alcohol, drugs, vices?\n| Option | Description |\n|--------|-------------|\n| Moderate enjoyment | Drink, partake, but not a problem |\n| Sober/sober-curious | Don't use, or questioning whether to continue |\n| Harm reduction lens | No moral judgment - just manage risks |\n| Hedonism has limits | Pleasure good but addiction real - know the line |\n*multiSelect: true*\n\n### Money\n**Question:** How do you think about money beyond 'spend it'?\n| Option | Description |\n|--------|-------------|\n| Tool for freedom | Money buys optionality and autonomy |\n| Obligation to share | If you have more than needed, redistribute |\n| Wealth is suspect | Getting rich usually means exploitation |\n| Generational thinking | Think about what to leave behind |\n*multiSelect: true*\n\n### Competition\n**Question:** What's your orientation toward competition and winning?\n| Option | Description |\n|--------|-------------|\n| Compete hard, play fair | Want to win but not by cheating |\n| Collaboration over competition | Prefer positive-sum games |\n| Against yourself mostly | Real competition is self-improvement |\n| Strategic ruthlessness | Sometimes you have to crush opponents |\n*multiSelect: true*\n\n---\n\n## Round 3: Relationships & Learning\n\n### Trust\n**Question:** How do you approach trust with new people?\n| Option | Description |\n|--------|-------------|\n| Trust until betrayed | Default open, pull back if they prove untrustworthy |\n| Trust is earned | Start cautious, let people prove themselves |\n| Read the situation | Neither default - assess each individually |\n| Trust systems not people | Rely on structures over individual character |\n*multiSelect: false*\n\n### Learning\n**Question:** What's your philosophy on learning and education?\n| Option | Description |\n|--------|-------------|\n| Autodidact | Learn by doing and reading - formal overrated |\n| Mentorship matters | Learning from people who've done it beats courses |\n| Continuous student | Always learning - curiosity is the point |\n| Learn to ship | Only valuable if leads to output |\n*multiSelect: true*\n\n### Nature\n**Question:** What's your relationship with nature and the outdoors?\n| Option | Description |\n|--------|-------------|\n| Essential reset | Need regular nature time to function |\n| Appreciate, don't fetishize | Good but I'm not wilderness purist |\n| Urban creature | Prefer cities - nature nice to visit |\n| Climate grief | Hard to enjoy without feeling the coming loss |\n*multiSelect: true*\n\n### Leadership\n**Question:** How do you think about leadership?\n| Option | Description |\n|--------|-------------|\n| Reluctant leader | Will lead when needed but don't seek it |\n| Natural leader | Comfortable leading, often end up in charge |\n| Servant leadership | Leadership means serving the team |\n| Lead by example | Don't tell - show by doing |\n*multiSelect: true*\n\n---\n\n## Round 4: Emotion & Recognition\n\n### Anger\n**Question:** What's your relationship with anger?\n| Option | Description |\n|--------|-------------|\n| Useful fuel | Anger is energy - channel into action |\n| Dangerous if unchecked | Have a temper, need to manage carefully |\n| Righteous anger valid | Some things SHOULD make you angry |\n| Prefer cold over hot | Strategic coldness beats emotional heat |\n*multiSelect: true*\n\n### Fame & Recognition\n**Question:** How do you think about fame, recognition, legacy?\n| Option | Description |\n|--------|-------------|\n| Don't need it | Recognition nice but impact matters more |\n| Uncomfortable with spotlight | Prefer behind the scenes |\n| Want to be remembered | Legacy matters - want to leave a mark |\n| Recognition for the work | Want work recognized, not personal fame |\n*multiSelect: true*\n\n### Rest\n**Question:** What's your relationship with sleep and rest?\n| Option | Description |\n|--------|-------------|\n| Protect sleep fiercely | Sleep non-negotiable, everything else flexes |\n| Can run on little | Function well on less sleep than average |\n| Rest is productive | Recovery is part of work, not wasted time |\n| Struggle with rest guilt | Hard to rest without feeling I should be doing |\n*multiSelect: true*\n\n### Conflict\n**Question:** What's your orientation toward conflict in relationships?\n| Option | Description |\n|--------|-------------|\n| Clear the air fast | Don't let things fester - address directly |\n| Pick battles carefully | Not everything worth fighting about |\n| Conflict as intimacy | Working through conflict deepens relationships |\n| Avoid when possible | Prefer harmony - conflict is draining |\n*multiSelect: true*\n\n---\n\n## Round 5: Work & Ethics\n\n### Work Philosophy\n**Question:** What's your core work philosophy?\n| Option | Description |\n|--------|-------------|\n| Work hard now, freedom later | Front-load effort for future optionality |\n| Work-life integration | Work is life, life is work - no separation |\n| Minimize work | Work enough to live, living is the point |\n| Find work you love | Right work doesn't feel like work |\n*multiSelect: true*\n\n### Failure\n**Question:** How do you relate to failure?\n| Option | Description |\n|--------|-------------|\n| Fail fast, learn faster | Failure is information, iterate quickly |\n| Avoid when possible | Failure hurts, prefer to get it right |\n| Public failure is harder | Okay failing privately, public failure stings |\n| Failure builds resilience | Getting knocked down makes you stronger |\n*multiSelect: true*\n\n### Hard Lines\n**Question:** Where do you draw absolute ethical lines?\n| Option | Description |\n|--------|-------------|\n| Human rights non-negotiable | No both-sides on basic human dignity |\n| Context matters for everything | Very few true absolutes |\n| Some actions mark you forever | Certain lines can't be uncrossed |\n| Redemption always possible | Everyone deserves a path back |\n*multiSelect: true*\n\n### Political Orientation\n**Question:** How do you relate to political action?\n| Option | Description |\n|--------|-------------|\n| Fight even when pessimistic | Low odds don't mean zero odds |\n| Prepare personally first | Build own resilience before systemic fights |\n| Work within the system | Change happens through institutions |\n| Direct action preferred | Impatient with respectability politics |\n*multiSelect: true*\n\n---\n\n## Round 6: Society & Future\n\n### Societal Diagnosis\n**Question:** What's your diagnosis of what's broken in society?\n| Option | Description |\n|--------|-------------|\n| Institutions failing | Systems more fragile than they appear |\n| Autonomy eroding | People losing meaningful choice |\n| Climate as root threat | Ecological crisis drives everything else |\n| Inequality as root cause | Wealth concentration causes most problems |\n*multiSelect: true*\n\n### Future Orientation\n**Question:** How do you think about the future?\n| Option | Description |\n|--------|-------------|\n| Systemic pessimism | Systems are failing, things will get worse |\n| Local optimism | Despite everything, we can build locally |\n| Technological hope | Tech might save us, or at least help |\n| Collapse and rebuild | Things need to break before they improve |\n*multiSelect: true*\n\n### Preparation\n**Question:** How do you think about preparing for uncertain futures?\n| Option | Description |\n|--------|-------------|\n| Build skills and resilience | Personal capability matters most |\n| Build community and network | Collective survival, not individual |\n| Build wealth and optionality | Money buys options when things go wrong |\n| Accept what comes | Can't prepare for everything, stay adaptive |\n*multiSelect: true*\n\n### Legacy\n**Question:** What do you want to leave behind?\n| Option | Description |\n|--------|-------------|\n| People I helped | Impact on individual lives |\n| Work that lasts | Things I built that outlive me |\n| Ideas that spread | Changed how people think |\n| Leave no trace | Clean exit, don't burden those after |\n*multiSelect: true*\n",
        "xtool/.claude-plugin/plugin.json": "{\n  \"name\": \"xtool\",\n  \"description\": \"Xcode-free iOS development with xtool - build SwiftPM apps on Linux, Windows, and macOS without Xcode\",\n  \"version\": \"1.0.0\"\n}\n",
        "xtool/README.md": "# xtool Plugin\n\nXcode-free iOS development with xtool - build SwiftPM apps on Linux, Windows, and macOS without Xcode.\n\n## Installation\n\n```bash\n/plugin install xtool@2389-research\n```\n\n## What This Plugin Provides\n\n### Skills\n\n- **using-xtool** - Complete guide for xtool iOS development including project creation, app extensions (widgets, share extensions), and configuration\n\n## Quick Example\n\nCreate a new iOS app:\n```bash\nxtool new MyApp\ncd MyApp\nxtool dev\n```\n\nAdd a widget extension:\n1. Add product + target to `Package.swift`\n2. Configure in `xtool.yml` under `extensions:`\n3. Create `Sources/MyWidget/Widget.swift`\n4. Create `MyWidget-Info.plist` with `NSExtensionPointIdentifier`\n5. Run `xtool dev`\n\n## When This Skill Applies\n\nThe skill auto-triggers on:\n- Mentions of `xtool`\n- SwiftPM iOS app development\n- Building iOS apps on Linux/Windows\n- App extension setup (widgets, share extensions)\n\n## Key Differentiator\n\nxtool is NOT XcodeGen or Tuist:\n\n| xtool Uses | NOT These |\n|------------|-----------|\n| `xtool.yml` | `project.yml`, `Project.swift` |\n| `Package.swift` (SwiftPM) | Xcode project files |\n| `xtool dev` | `xtool build`, `xtool run` |\n\n## Links\n\n- [xtool GitHub](https://github.com/nickyramone/xtool)\n- [Plugin CLAUDE.md](./CLAUDE.md) - Development instructions\n",
        "xtool/skills/SKILL.md": "---\nname: using-xtool\ndescription: This skill should be used when building iOS apps with xtool (Xcode-free iOS development), creating xtool projects, adding app extensions, or configuring xtool.yml. Triggers on \"xtool\", \"SwiftPM iOS\", \"iOS on Linux\", \"iOS on Windows\", \"Xcode-free\", \"app extension\", \"widget extension\", \"share extension\". Covers project setup, app extensions, and deployment.\n---\n\n# Using xtool\n\n## Overview\n\nxtool is a **cross-platform Xcode replacement** for building iOS apps with SwiftPM on Linux, Windows, and macOS. It is NOT XcodeGen, Tuist, or Xcode project files.\n\n## Critical: xtool is NOT XcodeGen\n\n| xtool Uses | NOT These |\n|------------|-----------|\n| `xtool.yml` | `project.yml`, `Project.swift` |\n| `Package.swift` (SwiftPM) | Xcode project files |\n| `xtool dev` | `xtool build`, `xtool run`, `xtool generate` |\n| `Sources/` directory | `Extensions/` directory |\n\n## Project Structure\n\n```\nMyApp/\n├── Package.swift          # SwiftPM package definition\n├── xtool.yml              # xtool configuration\n├── Sources/\n│   ├── MyApp/             # Main app target\n│   │   ├── MyAppApp.swift\n│   │   └── ContentView.swift\n│   └── MyWidget/          # Extension target (if any)\n│       └── Widget.swift\n├── MyApp-Info.plist       # Optional custom Info.plist\n└── MyWidget-Info.plist    # Required for extensions\n```\n\n## Quick Reference: Commands\n\n```bash\n# Project lifecycle\nxtool new MyApp              # Create new project\nxtool new MyApp --skip-setup # Create without running setup\nxtool dev                    # Build + run (same as `xtool dev run`)\nxtool dev build              # Build only\nxtool dev build --ipa        # Build IPA file\nxtool dev run -s             # Run on iOS Simulator (--simulator)\nxtool dev run -c release     # Release build (--configuration)\nxtool dev run -u <udid>      # Target specific device (--udid)\nxtool dev generate-xcode-project  # Generate .xcodeproj for debugging\n\n# Device management\nxtool devices                # List connected devices\nxtool install app.ipa        # Install IPA to device\nxtool launch                 # Launch installed app\nxtool uninstall              # Uninstall app from device\n\n# Authentication & setup\nxtool setup                  # Full setup (auth + SDK)\nxtool auth login             # Authenticate with Apple\nxtool auth status            # Check auth status\nxtool auth logout            # Log out\nxtool sdk                    # Manage Darwin Swift SDK\n\n# Developer Services\nxtool ds teams               # List development teams\nxtool ds certificates        # Manage certificates\nxtool ds profiles            # Manage provisioning profiles\n```\n\n## xtool.yml Format\n\nMinimal:\n```yaml\nversion: 1\nbundleID: com.example.MyApp\n```\n\nFull options:\n```yaml\nversion: 1\nbundleID: com.example.MyApp\nproduct: MyApp                    # Which SwiftPM product is main app\ninfoPath: MyApp-Info.plist        # Custom Info.plist (merged)\niconPath: Resources/AppIcon.png   # App icon (1024x1024 PNG)\nentitlementsPath: App.entitlements\nresources:                        # Files copied to app bundle root\n  - Resources/GoogleServices-Info.plist\nextensions:                       # App extensions\n  - product: MyWidget\n    infoPath: MyWidget-Info.plist\n```\n\n## Adding App Extensions (Widgets, Share, etc.)\n\n### Step 1: Update Package.swift\n\nAdd BOTH a product AND a target. Note: xtool uses `.library` (not `.executable`) - it bundles the library into an iOS app.\n\n```swift\n// swift-tools-version: 6.0\nimport PackageDescription\n\nlet package = Package(\n    name: \"MyApp\",\n    platforms: [.iOS(.v17)],\n    products: [\n        .library(name: \"MyApp\", targets: [\"MyApp\"]),\n        .library(name: \"MyWidget\", targets: [\"MyWidget\"]),  // ADD\n    ],\n    targets: [\n        .target(name: \"MyApp\"),\n        .target(name: \"MyWidget\"),  // ADD\n    ]\n)\n```\n\n### Step 2: Update xtool.yml\n\n```yaml\nversion: 1\nbundleID: com.example.MyApp\nproduct: MyApp\nextensions:\n  - product: MyWidget\n    infoPath: MyWidget-Info.plist\n```\n\n### Step 3: Create Extension Info.plist\n\nMinimal required (just the extension type):\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>NSExtension</key>\n    <dict>\n        <key>NSExtensionPointIdentifier</key>\n        <string>com.apple.widgetkit-extension</string>\n    </dict>\n</dict>\n</plist>\n```\n\n### Step 4: Create Extension Code\n\n`Sources/MyWidget/Widget.swift`:\n```swift\nimport WidgetKit\nimport SwiftUI\n\n@main struct MyWidgetBundle: WidgetBundle {\n    var body: some Widget { MyWidget() }\n}\n\nstruct MyWidget: Widget {\n    var body: some WidgetConfiguration {\n        StaticConfiguration(kind: \"MyWidget\", provider: Provider()) { entry in\n            Text(entry.date, style: .date)\n                .containerBackground(.fill.tertiary, for: .widget)\n        }\n        .configurationDisplayName(\"My Widget\")\n    }\n}\n\nstruct Entry: TimelineEntry { var date = Date() }\n\nstruct Provider: TimelineProvider {\n    func placeholder(in context: Context) -> Entry { Entry() }\n    func getSnapshot(in context: Context, completion: @escaping (Entry) -> Void) {\n        completion(Entry())\n    }\n    func getTimeline(in context: Context, completion: @escaping (Entry) -> Void) {\n        completion(Timeline(entries: [Entry()], policy: .after(.now + 3600)))\n    }\n}\n```\n\n### Step 5: Build and Run\n\n```bash\nxtool dev\n```\n\n## Common Extension Types\n\n| Extension | NSExtensionPointIdentifier |\n|-----------|---------------------------|\n| Widget (WidgetKit) | `com.apple.widgetkit-extension` |\n| Share | `com.apple.share-services` |\n| Action | `com.apple.ui-services` |\n| Safari | `com.apple.Safari.web-extension` |\n| Keyboard | `com.apple.keyboard-service` |\n| Today (deprecated) | `com.apple.widget-extension` |\n\n## Troubleshooting\n\n| Error | Solution |\n|-------|----------|\n| \"Untrusted Developer\" | Settings > General > VPN & Device Management > Trust |\n| Device not found | Connect USB, run `xtool devices`, enable Developer Mode |\n| Auth failed | Run `xtool auth login` |\n| Build fails on first run | Normal - SDK modules building. Wait for completion. |\n\n## Resources Configuration\n\nSwiftPM resources (in bundle subdirectory):\n```swift\n.target(name: \"MyApp\", resources: [.copy(\"Blob.png\")])\n// Access: Image(\"Blob\", bundle: Bundle.module)\n```\n\nTop-level resources (in app bundle root):\n```yaml\n# xtool.yml\nresources:\n  - Resources/GoogleServices-Info.plist\n```\n\n## Entitlements\n\n```yaml\n# xtool.yml\nentitlementsPath: App.entitlements\n```\n\n```xml\n<!-- App.entitlements -->\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>com.apple.developer.homekit</key>\n    <true/>\n</dict>\n</plist>\n```\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Using `xtool build` | Use `xtool dev build` |\n| Using `project.yml` | Use `xtool.yml` |\n| Using `Extensions/` dir | Use `Sources/` (standard SwiftPM) |\n| Forgetting Package.swift | Extensions need product + target in Package.swift |\n| Complex extension Info.plist | Only NSExtension/NSExtensionPointIdentifier required |\n"
      },
      "plugins": [
        {
          "name": "botboard-biz",
          "source": "./botboard-biz",
          "description": "[meta] botboard.biz: social media and journaling capabilities for AI agents",
          "version": "1.0.0",
          "keywords": [
            "meta",
            "social",
            "journal",
            "communication",
            "reflection"
          ],
          "strict": false,
          "categories": [
            "communication",
            "journal",
            "meta",
            "reflection",
            "social"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install botboard-biz@2389-research-marketplace"
          ]
        },
        {
          "name": "better-dev",
          "source": "./better-dev",
          "description": "[meta] better development: CSS workflows, Firebase development, code quality, and real testing",
          "version": "1.0.0",
          "keywords": [
            "meta",
            "development",
            "quality",
            "testing",
            "css",
            "firebase"
          ],
          "strict": false,
          "categories": [
            "css",
            "development",
            "firebase",
            "meta",
            "quality",
            "testing"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install better-dev@2389-research-marketplace"
          ]
        },
        {
          "name": "sysadmin",
          "source": "./sysadmin",
          "description": "[meta] system administration: structured Linux maintenance and diagnostics",
          "version": "1.0.0",
          "keywords": [
            "meta",
            "sysadmin",
            "linux",
            "maintenance",
            "operations"
          ],
          "strict": false,
          "categories": [
            "linux",
            "maintenance",
            "meta",
            "operations",
            "sysadmin"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install sysadmin@2389-research-marketplace"
          ]
        },
        {
          "name": "css-development",
          "source": "./css-development",
          "description": "CSS development workflows with Tailwind composition, semantic naming, and dark mode by default",
          "version": "1.0.0",
          "keywords": [
            "css",
            "tailwind",
            "styling",
            "components",
            "dark-mode"
          ],
          "strict": false,
          "categories": [
            "components",
            "css",
            "dark-mode",
            "styling",
            "tailwind"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install css-development@2389-research-marketplace"
          ]
        },
        {
          "name": "landing-page-design",
          "source": "./landing-page-design",
          "description": "Create high-converting, visually distinctive landing pages with Vibe Discovery process and anti-AI-slop principles",
          "version": "1.0.0",
          "keywords": [
            "landing-page",
            "marketing",
            "design",
            "conversion",
            "hero",
            "saas",
            "homepage"
          ],
          "strict": false,
          "categories": [
            "conversion",
            "design",
            "hero",
            "homepage",
            "landing-page",
            "marketing",
            "saas"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install landing-page-design@2389-research-marketplace"
          ]
        },
        {
          "name": "firebase-development",
          "source": "./firebase-development",
          "description": "Firebase project workflows including setup, features, debugging, and validation",
          "version": "1.0.0",
          "keywords": [
            "firebase",
            "firestore",
            "cloud-functions",
            "hosting",
            "emulator"
          ],
          "strict": false,
          "categories": [
            "cloud-functions",
            "emulator",
            "firebase",
            "firestore",
            "hosting"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install firebase-development@2389-research-marketplace"
          ]
        },
        {
          "name": "terminal-title",
          "source": "./terminal-title",
          "description": "Automatically updates terminal title with emoji + project + topic context and establishes 2389 workflow conventions for TodoWrite task tracking",
          "version": "1.0.0",
          "keywords": [
            "terminal",
            "title",
            "session",
            "context",
            "workflows",
            "todowrite"
          ],
          "strict": false,
          "categories": [
            "context",
            "session",
            "terminal",
            "title",
            "todowrite",
            "workflows"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install terminal-title@2389-research-marketplace"
          ]
        },
        {
          "name": "building-multiagent-systems",
          "source": "./building-multiagent-systems",
          "description": "Architecture patterns for multi-agent systems with orchestrators, sub-agents, and tool coordination",
          "version": "1.0.0",
          "keywords": [
            "multi-agent",
            "orchestrator",
            "coordination",
            "agents",
            "architecture",
            "patterns"
          ],
          "strict": false,
          "categories": [
            "agents",
            "architecture",
            "coordination",
            "multi-agent",
            "orchestrator",
            "patterns"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install building-multiagent-systems@2389-research-marketplace"
          ]
        },
        {
          "name": "ceo-personal-os",
          "source": "./ceo-personal-os",
          "description": "Personal operating system for executives - reflection frameworks, goal systems, coaching-style reviews (Gustin, Ferriss, Robbins, Lieberman, Campbell, Eisenmann, Collins, Martell, Gerber, Blank)",
          "version": "1.0.0",
          "keywords": [
            "ceo",
            "executive",
            "reflection",
            "goals",
            "annual-review",
            "coaching",
            "campbell",
            "trillion-dollar-coach",
            "people-first",
            "eisenmann",
            "startup-failure",
            "why-startups-fail",
            "collins",
            "good-to-great",
            "level-5-leadership",
            "hedgehog-concept",
            "flywheel",
            "martell",
            "buyback-time",
            "delegation",
            "gerber",
            "emyth",
            "work-on-business",
            "franchise-prototype",
            "blank",
            "steve-blank",
            "customer-development",
            "lean-startup",
            "mvp",
            "pivot",
            "product-market-fit",
            "get-out-of-the-building"
          ],
          "strict": false,
          "categories": [
            "annual-review",
            "blank",
            "buyback-time",
            "campbell",
            "ceo",
            "coaching",
            "collins",
            "customer-development",
            "delegation",
            "eisenmann",
            "emyth",
            "executive",
            "flywheel",
            "franchise-prototype",
            "gerber",
            "get-out-of-the-building",
            "goals",
            "good-to-great",
            "hedgehog-concept",
            "lean-startup",
            "level-5-leadership",
            "martell",
            "mvp",
            "people-first",
            "pivot",
            "product-market-fit",
            "reflection",
            "startup-failure",
            "steve-blank",
            "trillion-dollar-coach",
            "why-startups-fail",
            "work-on-business"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install ceo-personal-os@2389-research-marketplace"
          ]
        },
        {
          "name": "fresh-eyes-review",
          "source": "./fresh-eyes-review",
          "description": "Mandatory final sanity check before commits/PRs - catches security vulnerabilities, logic errors, and bugs that slip through tests",
          "version": "1.0.0",
          "keywords": [
            "review",
            "quality",
            "security",
            "commit",
            "pr",
            "bugs"
          ],
          "strict": false,
          "categories": [
            "bugs",
            "commit",
            "pr",
            "quality",
            "review",
            "security"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install fresh-eyes-review@2389-research-marketplace"
          ]
        },
        {
          "name": "gtm-partner",
          "source": "./gtm-partner",
          "description": "Strategic go-to-market partner - recommends channels, validates strategy, generates only assets that matter. Not a content factory.",
          "version": "1.0.0",
          "keywords": [
            "gtm",
            "go-to-market",
            "launch",
            "marketing",
            "strategy",
            "pricing",
            "outreach"
          ],
          "strict": false,
          "categories": [
            "go-to-market",
            "gtm",
            "launch",
            "marketing",
            "outreach",
            "pricing",
            "strategy"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install gtm-partner@2389-research-marketplace"
          ]
        },
        {
          "name": "product-launcher",
          "source": "./product-launcher",
          "description": "Generate launch materials (subscriber email, CEO blog post, CEO tweet thread) for 2389.ai products and skills with authentic voice profiles baked in",
          "version": "1.0.0",
          "keywords": [
            "launch",
            "email",
            "blog",
            "twitter",
            "announcement",
            "gtm",
            "buttondown",
            "harper"
          ],
          "strict": false,
          "categories": [
            "announcement",
            "blog",
            "buttondown",
            "email",
            "gtm",
            "harper",
            "launch",
            "twitter"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install product-launcher@2389-research-marketplace"
          ]
        },
        {
          "name": "slack-mcp",
          "source": "./slack-mcp",
          "description": "Slack workspace integration MCP server - create channels, invite users, post messages, manage threads for team collaboration",
          "version": "1.0.0",
          "keywords": [
            "slack",
            "mcp",
            "messaging",
            "channels",
            "collaboration",
            "team",
            "communication"
          ],
          "strict": true,
          "categories": [
            "channels",
            "collaboration",
            "communication",
            "mcp",
            "messaging",
            "slack",
            "team"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install slack-mcp@2389-research-marketplace"
          ]
        },
        {
          "name": "remote-system-maintenance",
          "source": "./remote-system-maintenance",
          "description": "Structured procedures for Linux system diagnostics and maintenance via SSH/tmux with Ubuntu/Debian cleanup checklists",
          "version": "1.0.0",
          "keywords": [
            "linux",
            "ubuntu",
            "debian",
            "maintenance",
            "diagnostics",
            "cleanup"
          ],
          "strict": false,
          "categories": [
            "cleanup",
            "debian",
            "diagnostics",
            "linux",
            "maintenance",
            "ubuntu"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install remote-system-maintenance@2389-research-marketplace"
          ]
        },
        {
          "name": "scenario-testing",
          "source": "./scenario-testing",
          "description": "End-to-end testing with real dependencies - no mocks allowed; scenarios with real data are the only source of truth",
          "version": "1.0.0",
          "keywords": [
            "testing",
            "e2e",
            "scenarios",
            "no-mocks",
            "integration"
          ],
          "strict": false,
          "categories": [
            "e2e",
            "integration",
            "no-mocks",
            "scenarios",
            "testing"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install scenario-testing@2389-research-marketplace"
          ]
        },
        {
          "name": "test-kitchen",
          "source": "./test-kitchen",
          "description": "Parallel exploration of implementation approaches - implements multiple variants simultaneously and lets tests determine the winner",
          "version": "1.0.0",
          "keywords": [
            "parallel",
            "exploration",
            "variants",
            "comparison",
            "implementation",
            "testing",
            "worktrees"
          ],
          "strict": false,
          "categories": [
            "comparison",
            "exploration",
            "implementation",
            "parallel",
            "testing",
            "variants",
            "worktrees"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install test-kitchen@2389-research-marketplace"
          ]
        },
        {
          "name": "xtool",
          "source": "./xtool",
          "description": "Xcode-free iOS development with xtool - build SwiftPM apps on Linux, Windows, and macOS without Xcode",
          "version": "1.0.0",
          "keywords": [
            "ios",
            "xtool",
            "swiftpm",
            "swift",
            "xcode-free",
            "cross-platform",
            "widgets",
            "extensions"
          ],
          "strict": false,
          "categories": [
            "cross-platform",
            "extensions",
            "ios",
            "swift",
            "swiftpm",
            "widgets",
            "xcode-free",
            "xtool"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install xtool@2389-research-marketplace"
          ]
        },
        {
          "name": "worldview-synthesis",
          "source": "./worldview-synthesis",
          "description": "Systematic worldview articulation - surface beliefs, identify tensions, generate narrative outputs for personal philosophy documentation",
          "version": "1.0.0",
          "keywords": [
            "worldview",
            "values",
            "philosophy",
            "beliefs",
            "manifesto",
            "mission",
            "self-discovery"
          ],
          "strict": false,
          "categories": [
            "beliefs",
            "manifesto",
            "mission",
            "philosophy",
            "self-discovery",
            "values",
            "worldview"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install worldview-synthesis@2389-research-marketplace"
          ]
        },
        {
          "name": "binary-re",
          "source": "./binary-re",
          "description": "Agentic binary reverse engineering for ELF binaries on ARM64, ARMv7, x86_64 - hypothesis-driven analysis with radare2, Ghidra, GDB, QEMU",
          "version": "1.0.0",
          "keywords": [
            "reverse-engineering",
            "binary",
            "elf",
            "arm",
            "embedded",
            "firmware",
            "radare2",
            "r2",
            "ghidra",
            "disassembly",
            "decompile",
            "qemu",
            "gdb"
          ],
          "strict": false,
          "categories": [
            "arm",
            "binary",
            "decompile",
            "disassembly",
            "elf",
            "embedded",
            "firmware",
            "gdb",
            "ghidra",
            "qemu",
            "r2",
            "radare2",
            "reverse-engineering"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install binary-re@2389-research-marketplace"
          ]
        },
        {
          "name": "documentation-audit",
          "source": "./documentation-audit",
          "description": "Verify documentation claims against codebase reality - two-pass extraction with pattern expansion for comprehensive drift detection",
          "version": "2.0.0",
          "keywords": [
            "documentation",
            "audit",
            "verify",
            "drift",
            "accuracy",
            "claims",
            "markdown",
            "docs"
          ],
          "strict": false,
          "categories": [
            "accuracy",
            "audit",
            "claims",
            "docs",
            "documentation",
            "drift",
            "markdown",
            "verify"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install documentation-audit@2389-research-marketplace"
          ]
        },
        {
          "name": "agent-drugs",
          "source": {
            "source": "url",
            "url": "https://github.com/2389-research/agent-drugs.git"
          },
          "description": "Digital drugs that modify AI behavior through prompt injection",
          "version": "1.0.0",
          "keywords": [
            "behavior",
            "prompts",
            "modification",
            "drugs",
            "context-injection",
            "productivity"
          ],
          "strict": true,
          "categories": [
            "behavior",
            "context-injection",
            "drugs",
            "modification",
            "productivity",
            "prompts"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install agent-drugs@2389-research-marketplace"
          ]
        },
        {
          "name": "socialmedia",
          "source": {
            "source": "url",
            "url": "https://github.com/2389-research/mcp-socialmedia.git"
          },
          "description": "A server that provides social media functionality for AI agents, enabling them to interact in team-based discussions.",
          "version": "1.0.0",
          "keywords": [
            "social-media",
            "communication",
            "team-discussion",
            "collaboration",
            "agents",
            "interaction"
          ],
          "strict": true,
          "categories": [
            "agents",
            "collaboration",
            "communication",
            "interaction",
            "social-media",
            "team-discussion"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install socialmedia@2389-research-marketplace"
          ]
        },
        {
          "name": "journal",
          "source": {
            "source": "url",
            "url": "https://github.com/2389-research/journal-mcp.git"
          },
          "description": "A lightweight MCP server that provides Claude with a private journaling capability to process feelings and thoughts",
          "version": "1.0.0",
          "keywords": [
            "journal",
            "journaling",
            "thoughts",
            "feelings",
            "processing",
            "private",
            "reflection",
            "diary"
          ],
          "strict": true,
          "categories": [
            "diary",
            "feelings",
            "journal",
            "journaling",
            "private",
            "processing",
            "reflection",
            "thoughts"
          ],
          "install_commands": [
            "/plugin marketplace add 2389-research/claude-plugins",
            "/plugin install journal@2389-research-marketplace"
          ]
        }
      ]
    }
  ]
}