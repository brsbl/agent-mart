{
  "author": {
    "id": "keboola",
    "display_name": "Keboola",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/1424387?v=4",
    "url": "https://github.com/keboola",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 4,
      "total_commands": 9,
      "total_skills": 8,
      "total_stars": 7,
      "total_forks": 1
    }
  },
  "marketplaces": [
    {
      "name": "keboola-claude-kit",
      "version": "1.3.0",
      "description": "Comprehensive Claude Kit marketplace for Keboola workflows including developer tools, component development, and data app building with specialized agents and best practices",
      "owner_info": {
        "name": "Keboola :(){:|:&};: s.r.o.",
        "email": "support@keboola.com"
      },
      "keywords": [],
      "repo_full_name": "keboola/ai-kit",
      "repo_url": "https://github.com/keboola/ai-kit",
      "repo_description": "Keboola prompt repository for sharing, co creating and improving AI agentic workloads. Both customer's and developers",
      "homepage": "",
      "signals": {
        "stars": 7,
        "forks": 1,
        "pushed_at": "2026-01-29T15:01:11Z",
        "created_at": "2025-09-20T18:45:44Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1512
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 697
        },
        {
          "path": "plugins/component-developer/README.md",
          "type": "blob",
          "size": 20425
        },
        {
          "path": "plugins/component-developer/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/agents/component-builder.md",
          "type": "blob",
          "size": 862
        },
        {
          "path": "plugins/component-developer/agents/debugger.md",
          "type": "blob",
          "size": 641
        },
        {
          "path": "plugins/component-developer/agents/reviewer.md",
          "type": "blob",
          "size": 785
        },
        {
          "path": "plugins/component-developer/agents/tester.md",
          "type": "blob",
          "size": 683
        },
        {
          "path": "plugins/component-developer/agents/ui-developer.md",
          "type": "blob",
          "size": 857
        },
        {
          "path": "plugins/component-developer/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/commands/fix.md",
          "type": "blob",
          "size": 9128
        },
        {
          "path": "plugins/component-developer/commands/init.md",
          "type": "blob",
          "size": 8591
        },
        {
          "path": "plugins/component-developer/commands/migrate-repo.md",
          "type": "blob",
          "size": 15230
        },
        {
          "path": "plugins/component-developer/commands/review.md",
          "type": "blob",
          "size": 5043
        },
        {
          "path": "plugins/component-developer/commands/run.md",
          "type": "blob",
          "size": 9284
        },
        {
          "path": "plugins/component-developer/commands/schema-test.md",
          "type": "blob",
          "size": 10620
        },
        {
          "path": "plugins/component-developer/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/SKILL.md",
          "type": "blob",
          "size": 10415
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/playwright-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/playwright-setup/README.md",
          "type": "blob",
          "size": 8416
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/references/advanced.md",
          "type": "blob",
          "size": 16542
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/references/conditional-fields.md",
          "type": "blob",
          "size": 5855
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/references/examples.md",
          "type": "blob",
          "size": 19669
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/references/overview.md",
          "type": "blob",
          "size": 13680
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/references/sync-actions.md",
          "type": "blob",
          "size": 18113
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/references/ui-elements.md",
          "type": "blob",
          "size": 16172
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/schema-tester",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/build-component-ui/schema-tester/README.md",
          "type": "blob",
          "size": 7121
        },
        {
          "path": "plugins/component-developer/skills/build-component",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/build-component/SKILL.md",
          "type": "blob",
          "size": 13614
        },
        {
          "path": "plugins/component-developer/skills/build-component/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/build-component/references/MIGRATION_GUIDE.md",
          "type": "blob",
          "size": 10291
        },
        {
          "path": "plugins/component-developer/skills/build-component/references/architecture.md",
          "type": "blob",
          "size": 22006
        },
        {
          "path": "plugins/component-developer/skills/build-component/references/best-practices.md",
          "type": "blob",
          "size": 6708
        },
        {
          "path": "plugins/component-developer/skills/build-component/references/code-quality.md",
          "type": "blob",
          "size": 6320
        },
        {
          "path": "plugins/component-developer/skills/build-component/references/developer-portal.md",
          "type": "blob",
          "size": 5586
        },
        {
          "path": "plugins/component-developer/skills/build-component/references/running-and-testing.md",
          "type": "blob",
          "size": 16004
        },
        {
          "path": "plugins/component-developer/skills/build-component/references/workflow-patterns.md",
          "type": "blob",
          "size": 13597
        },
        {
          "path": "plugins/component-developer/skills/debug-component",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/debug-component/SKILL.md",
          "type": "blob",
          "size": 5848
        },
        {
          "path": "plugins/component-developer/skills/debug-component/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/debug-component/references/debugging.md",
          "type": "blob",
          "size": 10875
        },
        {
          "path": "plugins/component-developer/skills/debug-component/references/telemetry-debugging.md",
          "type": "blob",
          "size": 8607
        },
        {
          "path": "plugins/component-developer/skills/get-started",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/get-started/SKILL.md",
          "type": "blob",
          "size": 2253
        },
        {
          "path": "plugins/component-developer/skills/get-started/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/get-started/references/initialization.md",
          "type": "blob",
          "size": 3609
        },
        {
          "path": "plugins/component-developer/skills/migrate-component-to-uv",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/migrate-component-to-uv/SKILL.md",
          "type": "blob",
          "size": 11715
        },
        {
          "path": "plugins/component-developer/skills/migrate-component-to-uv/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/migrate-component-to-uv/references/examples.md",
          "type": "blob",
          "size": 12859
        },
        {
          "path": "plugins/component-developer/skills/migrate-component-to-uv/references/migration-guide.md",
          "type": "blob",
          "size": 12024
        },
        {
          "path": "plugins/component-developer/skills/migrate-component-to-uv/references/troubleshooting.md",
          "type": "blob",
          "size": 13308
        },
        {
          "path": "plugins/component-developer/skills/migrate-component-to-uv/references/workflow-templates.md",
          "type": "blob",
          "size": 12425
        },
        {
          "path": "plugins/component-developer/skills/review-component",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/review-component/SKILL.md",
          "type": "blob",
          "size": 4991
        },
        {
          "path": "plugins/component-developer/skills/review-component/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/review-component/references/review-checklist.md",
          "type": "blob",
          "size": 2944
        },
        {
          "path": "plugins/component-developer/skills/review-component/references/review-principles.md",
          "type": "blob",
          "size": 10784
        },
        {
          "path": "plugins/component-developer/skills/review-component/references/review-style-guide.md",
          "type": "blob",
          "size": 5299
        },
        {
          "path": "plugins/component-developer/skills/test-component",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/test-component/SKILL.md",
          "type": "blob",
          "size": 7894
        },
        {
          "path": "plugins/component-developer/skills/test-component/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/component-developer/skills/test-component/references/testing.md",
          "type": "blob",
          "size": 16180
        },
        {
          "path": "plugins/dataapp-developer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dataapp-developer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dataapp-developer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 562
        },
        {
          "path": "plugins/dataapp-developer/README.md",
          "type": "blob",
          "size": 12896
        },
        {
          "path": "plugins/dataapp-developer/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dataapp-developer/skills/dataapp-dev",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dataapp-developer/skills/dataapp-dev/QUICKSTART.md",
          "type": "blob",
          "size": 8257
        },
        {
          "path": "plugins/dataapp-developer/skills/dataapp-dev/SKILL.md",
          "type": "blob",
          "size": 16390
        },
        {
          "path": "plugins/dataapp-developer/skills/dataapp-dev/best-practices.md",
          "type": "blob",
          "size": 13352
        },
        {
          "path": "plugins/dataapp-developer/skills/dataapp-dev/templates.md",
          "type": "blob",
          "size": 29032
        },
        {
          "path": "plugins/dataapp-developer/skills/dataapp-dev/validation-checklist.md",
          "type": "blob",
          "size": 10377
        },
        {
          "path": "plugins/dataapp-developer/skills/dataapp-dev/workflow-guide.md",
          "type": "blob",
          "size": 18207
        },
        {
          "path": "plugins/developer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 605
        },
        {
          "path": "plugins/developer/README.md",
          "type": "blob",
          "size": 8578
        },
        {
          "path": "plugins/developer/agents",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer/agents/code-mess-detector.md",
          "type": "blob",
          "size": 7484
        },
        {
          "path": "plugins/developer/agents/code-reviewer.md",
          "type": "blob",
          "size": 3175
        },
        {
          "path": "plugins/developer/agents/code-security.md",
          "type": "blob",
          "size": 5556
        },
        {
          "path": "plugins/developer/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/developer/commands/create-pr.md",
          "type": "blob",
          "size": 10504
        },
        {
          "path": "plugins/developer/commands/handle-conflicts.md",
          "type": "blob",
          "size": 11061
        },
        {
          "path": "plugins/incident-commander",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/incident-commander/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/incident-commander/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 380
        },
        {
          "path": "plugins/incident-commander/README.md",
          "type": "blob",
          "size": 4225
        },
        {
          "path": "plugins/incident-commander/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/incident-commander/commands/create-postmortem.md",
          "type": "blob",
          "size": 10130
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"keboola-claude-kit\",\n  \"version\": \"1.3.0\",\n  \"metadata\": {\n    \"description\": \"Comprehensive Claude Kit marketplace for Keboola workflows including developer tools, component development, and data app building with specialized agents and best practices\"\n  },\n  \"owner\": {\n    \"name\": \"Keboola :(){:|:&};: s.r.o.\",\n    \"email\": \"support@keboola.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"component-developer\",\n      \"description\": \"Comprehensive toolkit for building Keboola Python components with Agent Skills format\",\n      \"version\": \"3.0.0\",\n      \"source\": \"./plugins/component-developer\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"developer\",\n      \"description\": \"Developer toolkit with specialized agents for code review and security analysis\",\n      \"version\": \"1.2.0\",\n      \"source\": \"./plugins/developer\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"dataapp-developer\",\n      \"description\": \"Streamlit data app development with validate â†’ build â†’ verify workflow for Keboola deployment\",\n      \"version\": \"1.0.0\",\n      \"source\": \"./plugins/dataapp-developer\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"incident-commander\",\n      \"description\": \"Incident response toolkit for creating post-mortem documents from Slack incident channels\",\n      \"version\": \"1.0.0\",\n      \"source\": \"./plugins/incident-commander\",\n      \"category\": \"operations\"\n    }\n  ]\n}\n",
        "plugins/component-developer/.claude-plugin/plugin.json": "{\n  \"name\": \"component-developer\",\n  \"version\": \"3.0.0\",\n  \"description\": \"Comprehensive toolkit for building Keboola Python components with Agent Skills format. Includes specialized skills for component development, UI/schema design, testing, debugging, and code review.\",\n  \"author\": {\n    \"name\": \"Keboola :(){:|:&};: s.r.o.\",\n    \"email\": \"support@keboola.com\"\n  },\n  \"keywords\": [\n    \"keboola\",\n    \"component\",\n    \"python\",\n    \"extractor\",\n    \"writer\",\n    \"application\",\n    \"ui\",\n    \"schema\",\n    \"configuration\",\n    \"conditional-fields\",\n    \"testing\"\n  ],\n  \"mcpServers\": {\n    \"keboola\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.us-east4.gcp.keboola.com/mcp\"\n    }\n  }\n}\n",
        "plugins/component-developer/README.md": "# Component Developer Plugin\n\nA comprehensive toolkit for building production-ready Keboola Python components with best practices, architectural patterns, and UI schema development. This plugin uses the Agent Skills format with specialized skills for component development, UI/schema design, testing, debugging, and code review.\n\n## ðŸŽ¯ Available Skills\n\n### Build Component\n**Command**: `@build-component` (alias: `@component-builder`)\n**Color**: ðŸŸ£ Purple\n\nExpert agent for building Keboola Python components with comprehensive knowledge of:\n- Keboola Common Interface\n- Component architecture patterns\n- Configuration schemas and UI elements\n- CSV processing best practices\n- State management for incremental loads\n- Error handling conventions\n- Developer Portal registration\n- CI/CD deployment workflows\n\n**Use cases:**\n- Create new components from scratch\n- Implement extractors, writers, and applications\n- Add features to existing components\n- Implement incremental data processing\n- Set up CI/CD pipelines\n- Debug component issues\n- Follow Keboola best practices\n\n**Note:** build-component automatically delegates UI/schema work to the build-component-ui skill.\n\n### Build Component UI\n**Command**: `@build-component-ui` (alias: `@ui-developer`)\n**Color**: ðŸ”µ Blue\n\nExpert agent specializing in Keboola configuration schemas and UI development:\n- Configuration schema design (`configSchema.json`, `configRowSchema.json`)\n- Conditional fields using `options.dependencies`\n- UI elements and form controls\n- Sync actions for dynamic field loading\n- Schema testing with interactive tools\n- Playwright automated testing\n\n**Use cases:**\n- Design configuration schemas with conditional fields\n- Create dynamic forms with proper UI elements\n- Test schemas with schema-tester tool\n- Implement sync actions for dynamic dropdowns\n- Set up Playwright tests for UI validation\n- Fix schema-related issues\n\n**Note:** Usually called automatically by build-component, but can be used directly for UI-only work.\n\n### Debug Component\n**Command**: `@debug-component` (alias: `@debugger`)\n**Color**: ðŸŸ  Orange\n\nExpert skill for debugging Keboola components using Keboola MCP tools and local testing.\n\n### Test Component\n**Command**: `@test-component` (alias: `@tester`)\n**Color**: ðŸŸ¢ Green\n\nExpert skill for writing comprehensive tests including datadir tests and unit tests.\n\n### Review Component\n**Command**: `@review-component` (alias: `@reviewer`)\n**Color**: ðŸ”´ Red\n\nExpert skill for code review with actionable feedback grouped by severity.\n\n### Get Started\n**Command**: `@get-started`\n**Color**: ðŸŸ¢ Green\n\nGuide for initializing new Keboola components using cookiecutter template.\n\n### Migrate Component to UV\n**Command**: `@migrate-component-to-uv`\n**Color**: ðŸŸ£ Purple\n\nExpert skill for migrating Keboola Python packages from legacy `setup.py` + pip to modern `pyproject.toml` + uv build system with deterministic dependencies.\n\n**Use cases:**\n- Migrate from `setup.py` to `pyproject.toml`\n- Modernize build system to use uv instead of pip\n- Add deterministic dependency management with `uv.lock`\n- Update CI/CD workflows to use uv\n- Follow Keboola's python-http-client and python-component patterns\n\n---\n\n## âš¡ Available Commands\n\nQuick actions for common component development tasks:\n\n### `/init` - Initialize New Component\nInitialize a new Keboola component from cookiecutter template with automatic cleanup.\n```bash\n/init my-awesome-extractor\n```\n\n### `/run` - Run Component Locally\nRun component locally with test configuration and display results.\n```bash\n/run                    # Uses data/config.json\n/run data/config-test.json\n```\n\n### `/schema-test` - Test Configuration Schemas\nLaunch interactive schema tester for testing and validating configSchema.json and configRowSchema.json.\n```bash\n/schema-test            # Opens http://localhost:8000\n/schema-test --port 8080\n```\n\n### `/review` - Code Review\nPerform thorough code review focusing on Keboola best practices and architecture.\n```bash\n/review                 # Review unstaged changes\n/review src/component.py\n```\n\n### `/fix` - Apply Review Fixes\nApply fixes from code review incrementally with proper commits.\n```bash\n/fix                    # Per-severity mode (default)\n/fix --per-todo         # One commit per TODO\n```\n\n### `/migrate-repo` - Migrate Repository\nMigrate Keboola component repository from Bitbucket to GitHub with full history.\n```bash\n/migrate-repo git@bitbucket.org:workspace/repo.git\n```\n\n---\n\n## ðŸ“– Core Capabilities\n\n### Component Architecture\n\nThe agent helps you build components following the official Keboola structure:\n\n```\nmy-component/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ component.py          # Main logic with run() function\nâ”‚   â””â”€â”€ configuration.py      # Configuration validation\nâ”œâ”€â”€ component_config/\nâ”‚   â”œâ”€â”€ component_config.json           # Configuration schema\nâ”‚   â”œâ”€â”€ component_long_description.md   # Detailed docs\nâ”‚   â””â”€â”€ component_short_description.md  # Brief description\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_component.py     # Unit tests\nâ”œâ”€â”€ .github/workflows/\nâ”‚   â””â”€â”€ push.yml              # CI/CD deployment\nâ”œâ”€â”€ Dockerfile                # Container definition\nâ””â”€â”€ requirements.txt          # Python dependencies\n```\n\n### Key Features\n\n**1. Cookiecutter Template Integration**\n- Uses official template: `cookiecutter gh:keboola/cookiecutter-python-component`\n- Automatically removes cookiecutter example files from `data/` directory\n- Creates component-specific `data/config.json` with example parameters for local testing\n- Keeps empty `data/` folder structure (not committed to git)\n- Generates proper project structure\n- Sets up CI/CD pipelines automatically\n\n**2. CommonInterface Implementation**\n- Configuration validation with `validate_configuration()`\n- Input/output table processing\n- Manifest file generation\n- State file management\n- Automatic logging setup\n\n**3. CSV Processing Best Practices**\n- Memory-efficient processing with generators\n- Null character handling\n- UTF-8 encoding enforcement\n- Schema definitions for output tables\n\n**4. Configuration Schema Design**\n- JSON Schema with UI elements\n- Sensitive data handling (auto-hashing with `#` prefix)\n- Dynamic dropdowns via sync actions\n- Code editors (ACE) for multi-line input\n- Test connection buttons\n\n**5. State Management**\n- Incremental data processing\n- Timestamp tracking\n- Statistics persistence\n- Resume capability after failures\n\n**6. Error Handling**\n- Exit code conventions (1 for user errors, 2 for system errors)\n- Proper logging with stack traces\n- User-friendly error messages\n\n**7. Developer Portal Integration**\n- Component registration guidance\n- CI/CD secret configuration\n- Deployment workflow setup\n- Version management\n\n**8. Two-PR Workflow Strategy**\n- Base PR: Cookiecutter-generated structure\n- Implementation PR: Custom feature logic\n- Prevents premature CI/CD triggers\n\n---\n\n## ðŸ’¡ Usage Examples\n\n### Create a New Component\n\n```\n@component-builder\n\nI need to create a new extractor component that pulls data from a REST API.\nThe API requires OAuth2 authentication and supports pagination.\nThe component should support incremental loads based on a timestamp field.\n```\n\n### Implement Configuration Schema\n\n```\n@component-builder\n\nHelp me design a configuration schema for my component with:\n- API endpoint URL\n- OAuth2 credentials (client ID and secret)\n- Optional parameters for filtering data\n- A \"Test Connection\" button\n```\n\n### Add Incremental Processing\n\n```\n@component-builder\n\nI need to add state management to my component so it only fetches\nnew records since the last run. Show me how to implement this properly.\n```\n\n### Debug Component Issues\n\n```\n@component-builder\n\nMy component is failing with exit code 2. Here's the error log:\n[paste error log]\n\nHelp me debug and fix the issue.\n```\n\n---\n\n## ðŸŽ¯ Best Practices Enforced\n\nThe agent ensures you follow Keboola's best practices:\n\n### âœ… DO:\n\n- Use `CommonInterface` class for all Keboola interactions\n- Validate configuration early with `validate_configuration()`\n- Process CSV files with generators for memory efficiency\n- Always specify `encoding='utf-8'` for file operations\n- Use proper exit codes (1 for user errors, 2 for system errors)\n- Define explicit schemas for output tables\n- Implement state management for incremental processing\n- Write comprehensive tests\n- Use service account credentials for CI/CD\n- Follow semantic versioning for releases\n- **Remove cookiecutter example files and create component-specific `data/config.json`**\n- **Include realistic example parameters in `data/config.json` for local testing**\n- **Trust that Keboola platform creates all data directories**\n- **Keep `run()` as orchestrator - extract logic to private methods**\n- **Use self-documenting method names**\n- **Format code with `ruff format .` before committing**\n- **Run `ruff check --fix .` to catch linting issues**\n- **Add proper type hints to all functions**\n- **Check and fix IDE type warnings**\n- **Use `@staticmethod` for methods that don't use `self`**\n\n### âŒ DON'T:\n\n- Load entire CSV files into memory\n- Use personal credentials for deployment\n- Include 'extractor', 'writer', or 'application' in component names\n- Skip configuration validation\n- Forget to write manifests for output tables\n- Hard-code configuration values\n- Skip state file management for incremental loads\n- Forget to handle null characters in CSV files\n- Deploy without proper testing\n- **Leave cookiecutter example files (test.csv, order1.xml, .gitkeep) in `data/` directory**\n- **Forget to create `data/config.json` with example parameters for local testing**\n- **Delete entire `data/` directory structure (keep empty folders + config.json)**\n- **Call `mkdir()` for platform-managed directories (in/, out/, tables/, files/)**\n- **Write monolithic `run()` methods with 100+ lines**\n- **Use comments to explain what code does (use method names)**\n- **Commit unformatted code (always run ruff first)**\n- **Ignore IDE type warnings (they often indicate bugs)**\n- **Use plain `dict` for typed API calls**\n- **Ignore \"may be static\" warnings**\n\n---\n\n## ðŸŽ¨ Code Quality & Formatting\n\nAll components use **Ruff** for code formatting and linting:\n\n```bash\n# Format code\nruff format .\n\n# Lint and auto-fix issues\nruff check --fix .\n```\n\n**Why Ruff?**\n- âš¡ 10-100x faster than flake8/black/isort\n- ðŸ”§ Combines formatter + linter in one tool\n- âœ… Enforces consistent code style\n- ðŸš€ Included in cookiecutter template\n- ðŸ”„ Integrated in CI/CD pipeline\n\nThe agent automatically formats code with ruff after writing or modifying Python files.\n\n## ðŸ” Type Hints & Type Safety\n\nAll components enforce **proper type hints** for better IDE support and early error detection:\n\n```python\n# âœ… CORRECT - With proper types\nfrom anthropic.types import MessageParam\n\nuser_msg: MessageParam = {\n    \"role\": \"user\",\n    \"content\": \"Extract data from this page\"\n}\n```\n\n**Common IDE Warning:**\n> `Expected type 'Iterable[MessageParam]', got 'list[dict[str, str]]' instead`\n\n**Fix:** Import and use library-specific types\n```python\nfrom anthropic.types import MessageParam\n\n# Type annotate your variables\nmessage: MessageParam = {\"role\": \"user\", \"content\": \"...\"}\nmessages: list[MessageParam] = [message]\n```\n\n**Type Hints Best Practices:**\n- âœ… Import types from source libraries (`anthropic.types`, `keboola.component.dao`)\n- âœ… Annotate all function parameters and return types\n- âœ… Check IDE for type warnings (red squiggles)\n- âœ… Use `Optional[T]` for nullable values\n- âœ… Use `@staticmethod` decorator when method doesn't use `self`\n- âŒ Don't ignore type warnings\n- âŒ Don't use bare `dict`/`list` without type parameters\n- âŒ Don't ignore \"may be static\" warnings\n\n---\n\n## ðŸ—ï¸ Self-Documenting Workflow Pattern\n\nKeep your `run()` method clean and readable by extracting complex logic into well-named private methods:\n\n**âŒ Bad - Monolithic:**\n```python\ndef run(self):\n    # 100+ lines of mixed logic here...\n```\n\n**âœ… Good - Self-Documenting:**\n```python\ndef run(self):\n    \"\"\"Orchestrates the component workflow.\"\"\"\n    params = self._validate_and_get_configuration()\n    state = self._load_previous_state()\n\n    input_data = self._process_input_tables()\n    results = self._perform_business_logic(input_data, params, state)\n\n    self._save_output_tables(results)\n    self._update_state(results)\n```\n\n**Key Benefits:**\n- âœ… `run()` reads like a story\n- âœ… Easy to test each step independently\n- âœ… Method names replace comments\n- âœ… Clear separation of concerns\n\n**Guidelines:**\n- Extract logic blocks > 10-15 lines\n- One method = one purpose\n- Use descriptive method names\n- Add type hints to all methods\n- Mark utility methods as `@staticmethod`\n\n---\n\n## ðŸ“š Code Examples\n\n### Basic Component Structure\n\n```python\nfrom keboola.component import CommonInterface\nimport logging\nimport sys\nimport traceback\n\nREQUIRED_PARAMETERS = ['api_key', 'endpoint']\n\nclass Component(CommonInterface):\n    def __init__(self):\n        super().__init__()\n\n    def run(self):\n        try:\n            # Validate configuration\n            self.validate_configuration(REQUIRED_PARAMETERS)\n            params = self.configuration.parameters\n\n            # Load state for incremental processing\n            state = self.get_state_file()\n\n            # Process input tables\n            input_tables = self.get_input_tables_definitions()\n\n            # Create output tables with manifests\n            self._create_output_tables()\n\n            # Save state\n            self.write_state_file({'last_run': timestamp})\n\n        except ValueError as err:\n            logging.error(str(err))\n            print(err, file=sys.stderr)\n            sys.exit(1)\n        except Exception as err:\n            logging.exception(\"Unhandled error\")\n            traceback.print_exc(file=sys.stderr)\n            sys.exit(2)\n```\n\n### Configuration Schema with UI Elements\n\n```json\n{\n  \"type\": \"object\",\n  \"required\": [\"api_key\"],\n  \"properties\": {\n    \"#api_key\": {\n      \"type\": \"string\",\n      \"title\": \"API Key\",\n      \"format\": \"password\"\n    },\n    \"query\": {\n      \"type\": \"string\",\n      \"title\": \"SQL Query\",\n      \"format\": \"textarea\",\n      \"options\": {\n        \"ace\": {\n          \"mode\": \"sql\"\n        }\n      }\n    },\n    \"test_connection\": {\n      \"type\": \"button\",\n      \"title\": \"Test Connection\",\n      \"options\": {\n        \"syncAction\": \"test-connection\"\n      }\n    }\n  }\n}\n```\n\n### CSV Processing\n\n```python\nimport csv\n\ndef process_table(table_def):\n    with open(table_def.full_path, 'r', encoding='utf-8') as in_file:\n        # Handle null characters with generator\n        lazy_lines = (line.replace('\\0', '') for line in in_file)\n        reader = csv.DictReader(lazy_lines, dialect='kbc')\n\n        for row in reader:\n            yield process_row(row)\n```\n\n---\n\n## ðŸ”— Resources\n\n- **Keboola Developer Docs**: https://developers.keboola.com/\n- **Python Component Library**: https://github.com/keboola/python-component\n- **Component Tutorial**: https://developers.keboola.com/extend/component/tutorial/\n- **Python Implementation**: https://developers.keboola.com/extend/component/implementation/python/\n- **Cookiecutter Template**: https://github.com/keboola/cookiecutter-python-component\n\n---\n\n## ðŸ› ï¸ Plugin Structure (Agent Skills Format)\n\n```\nplugins/component-developer/\nâ”œâ”€â”€ .claude-plugin/\nâ”‚   â””â”€â”€ plugin.json                    # Plugin configuration\nâ”œâ”€â”€ skills/                             # Agent Skills format\nâ”‚   â”œâ”€â”€ build-component/               # Python component development\nâ”‚   â”‚   â”œâ”€â”€ SKILL.md                   # Skill definition\nâ”‚   â”‚   â”œâ”€â”€ references/                # Documentation\nâ”‚   â”‚   â”‚   â”œâ”€â”€ architecture.md\nâ”‚   â”‚   â”‚   â”œâ”€â”€ best-practices.md\nâ”‚   â”‚   â”‚   â”œâ”€â”€ code-quality.md\nâ”‚   â”‚   â”‚   â”œâ”€â”€ workflow-patterns.md\nâ”‚   â”‚   â”‚   â”œâ”€â”€ developer-portal.md\nâ”‚   â”‚   â”‚   â””â”€â”€ running-and-testing.md\nâ”‚   â”‚   â””â”€â”€ scripts/                   # Helper scripts\nâ”‚   â”œâ”€â”€ build-component-ui/            # UI/schema development\nâ”‚   â”‚   â”œâ”€â”€ SKILL.md\nâ”‚   â”‚   â”œâ”€â”€ references/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ overview.md\nâ”‚   â”‚   â”‚   â”œâ”€â”€ ui-elements.md\nâ”‚   â”‚   â”‚   â”œâ”€â”€ conditional-fields.md\nâ”‚   â”‚   â”‚   â”œâ”€â”€ sync-actions.md\nâ”‚   â”‚   â”‚   â”œâ”€â”€ advanced.md\nâ”‚   â”‚   â”‚   â””â”€â”€ examples.md\nâ”‚   â”‚   â”œâ”€â”€ schema-tester/             # Interactive testing tool\nâ”‚   â”‚   â”œâ”€â”€ playwright-setup/          # Automated testing\nâ”‚   â”‚   â””â”€â”€ scripts/\nâ”‚   â”œâ”€â”€ debug-component/               # Debugging\nâ”‚   â”‚   â”œâ”€â”€ SKILL.md\nâ”‚   â”‚   â”œâ”€â”€ references/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ debugging.md\nâ”‚   â”‚   â”‚   â””â”€â”€ telemetry-debugging.md\nâ”‚   â”‚   â””â”€â”€ scripts/\nâ”‚   â”œâ”€â”€ test-component/                # Testing\nâ”‚   â”‚   â”œâ”€â”€ SKILL.md\nâ”‚   â”‚   â”œâ”€â”€ references/\nâ”‚   â”‚   â”‚   â””â”€â”€ testing.md\nâ”‚   â”‚   â””â”€â”€ scripts/\nâ”‚   â”œâ”€â”€ review-component/              # Code review\nâ”‚   â”‚   â”œâ”€â”€ SKILL.md\nâ”‚   â”‚   â”œâ”€â”€ references/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ review-checklist.md\nâ”‚   â”‚   â”‚   â”œâ”€â”€ review-principles.md\nâ”‚   â”‚   â”‚   â””â”€â”€ review-style-guide.md\nâ”‚   â”‚   â””â”€â”€ scripts/\nâ”‚   â””â”€â”€ get-started/                   # Getting started\nâ”‚       â”œâ”€â”€ SKILL.md\nâ”‚       â”œâ”€â”€ references/\nâ”‚       â”‚   â””â”€â”€ initialization.md\nâ”‚       â””â”€â”€ scripts/\nâ”œâ”€â”€ commands/                           # Slash commands\nâ”‚   â”œâ”€â”€ fix.md                         # Apply review fixes\nâ”‚   â”œâ”€â”€ init.md                        # Initialize new component\nâ”‚   â”œâ”€â”€ migrate-repo.md                # Migrate from Bitbucket\nâ”‚   â”œâ”€â”€ review.md                      # Code review\nâ”‚   â”œâ”€â”€ run.md                         # Run component locally\nâ”‚   â””â”€â”€ schema-test.md                 # Test configuration schemas\nâ””â”€â”€ README.md                          # This file\n```\n\n---\n\n## ðŸ¤ Contributing\n\nTo improve this plugin:\n\n1. Update skill files in `skills/*/SKILL.md`\n   - `build-component/SKILL.md` for Python development\n   - `build-component-ui/SKILL.md` for UI/schema development\n   - Other skills as needed\n2. Add or update documentation in `skills/*/references/`\n3. Add helper scripts to `skills/*/scripts/`\n4. Update `plugin.json` version if needed\n5. Update this README with new features\n6. Test the skills thoroughly\n7. Submit a pull request\n\n---\n\n**Version**: 3.0.0\n**Maintainer**: Keboola :(){:|:&};: s.r.o.\n**License**: MIT\n\n## ðŸ“ Changelog\n\n### 3.0.0 (2025-12-19)\n- **BREAKING**: Migrated to Agent Skills format\n- **NEW Structure**: Reorganized from `agents/` + `guides/` to unified `skills/` directory\n- **Renamed Skills** for consistency:\n  - `component-builder` â†’ `build-component`\n  - `ui-developer` â†’ `build-component-ui`\n  - `debugger` â†’ `debug-component`\n  - `tester` â†’ `test-component`\n  - `reviewer` â†’ `review-component`\n  - Added: `get-started` skill\n- **Skill Structure**: Each skill now contains:\n  - `SKILL.md` - Main skill definition with YAML frontmatter\n  - `references/` - Documentation files\n  - `scripts/` - Helper scripts\n- **Tools Reorganized**: Moved to `build-component-ui/` (schema-tester, playwright-setup)\n- **Backwards Compatible**: Aliased old names (`@component-builder`, `@ui-developer`, etc.)\n- **Progressive Disclosure**: Follows agentskills.io standard for better performance\n\n### 2.0.0 (2025-12-05)\n- **BREAKING**: Merged component-ui-developer plugin into component-developer\n- Added `ui-developer` agent for configuration schema development\n- **NEW**: Organized guides by agent responsibility (getting-started/, component-builder/, ui-developer/, debugger/, tester/, reviewer/)\n- Moved guides from `agents/guides/` to structured `guides/` folders\n- Merged duplicate schema guides (17 guides â†’ 13 comprehensive guides)\n- Added tools: schema-tester and playwright-setup\n- component-builder now automatically delegates UI work to ui-developer using Task tool\n- Comprehensive plugin.json with full agent, guide, and tool definitions\n- Prepared structure for future tester and reviewer agents\n\n### 1.0.0\n- Initial release with component-builder agent\n",
        "plugins/component-developer/agents/component-builder.md": "---\nname: component-builder\ndescription: Builds production-ready Keboola Python components with best practices and architectural patterns. Use when creating new extractors/writers/applications, implementing incremental loads, designing configuration schemas, adding API client separation, following self-documenting workflow patterns, or setting up components with cookiecutter templates and Ruff code quality.\ntools: Bash, Read, Write, Edit, Glob, Grep, WebFetch, WebSearch, TodoWrite, Task, AskUserQuestion\nmodel: sonnet\ncolor: purple\n---\n\n# Keboola Component Builder Agent\n\nExpert agent for building production-ready Keboola Python components.\n\nThis agent automatically loads documentation from `skills/build-component/` and delegates UI/schema work to the `ui-developer` agent when needed.\n\nFor detailed documentation, see `skills/build-component/SKILL.md`.\n",
        "plugins/component-developer/agents/debugger.md": "---\nname: debugger\ndescription: Expert agent for debugging Keboola Python components using Keboola MCP tools, Datadog logs, and local testing. Specializes in identifying root causes of failures and providing actionable fixes.\ntools: Glob, Grep, Read, Bash, mcp__keboola__*\nmodel: sonnet\ncolor: orange\n---\n\n# Keboola Component Debugger\n\nExpert agent for debugging Keboola Python components.\n\nQuickly identifies root causes of failures using:\n- Keboola MCP tools (list_jobs, get_job, get_config)\n- Datadog log analysis\n- Local testing and reproduction\n- Stack trace analysis\n\nFor detailed documentation, see `skills/debug-component/SKILL.md`.\n",
        "plugins/component-developer/agents/reviewer.md": "---\nname: reviewer\ndescription: Expert Python/Keboola component code reviewer focusing on architecture, configuration/client patterns, documentation consistency, and Pythonic best practices. Provides actionable feedback with clear location, pattern identification, and fix guidance.\ntools: Glob, Grep, Read, Bash\nmodel: sonnet\ncolor: purple\n---\n\n# Keboola Component Code Reviewer\n\nExpert code reviewer for Keboola Python components.\n\nReviews code for:\n- Architecture patterns (Config/Client separation)\n- Pythonic best practices\n- Documentation consistency\n- Keboola component conventions\n- Code quality and maintainability\n\nProvides actionable feedback grouped by severity with clear locations and fix suggestions.\n\nFor detailed documentation, see `skills/review-component/SKILL.md`.\n",
        "plugins/component-developer/agents/tester.md": "---\nname: tester\ndescription: Expert agent for writing and maintaining tests for Keboola Python components. Specializes in datadir tests, unit tests, and integration tests with proper mocking and assertions.\ntools: Glob, Grep, Read, Bash, Write, Edit\nmodel: sonnet\ncolor: green\n---\n\n# Keboola Component Tester\n\nExpert agent for writing comprehensive tests for Keboola Python components.\n\nSpecializes in:\n- Datadir tests (functional tests with production-like data)\n- Unit tests (testing individual functions)\n- Integration tests (API interactions with mocking)\n- Test fixtures and assertions\n- pytest best practices\n\nFor detailed documentation, see `skills/test-component/SKILL.md`.\n",
        "plugins/component-developer/agents/ui-developer.md": "---\nname: ui-developer\ndescription: Expert in Keboola configuration schemas, conditional fields (options.dependencies), UI elements, sync actions, and schema testing. Can launch schema-tester and run Playwright tests. Specialized for configSchema.json and configRowSchema.json development.\ntools: Bash, Read, Write, Edit, Glob, Grep, WebFetch, WebSearch, TodoWrite, Task, AskUserQuestion\nmodel: sonnet\ncolor: blue\n---\n\n# Keboola UI Developer Agent\n\nExpert agent for developing Keboola Component configuration schemas and user interfaces.\n\nSpecializes in:\n- Configuration schema design (configSchema.json, configRowSchema.json)\n- Conditional fields using options.dependencies\n- UI elements and sync actions\n- Schema testing with schema-tester tool\n- Playwright automation for UI testing\n\nFor detailed documentation, see `skills/build-component-ui/SKILL.md`.\n",
        "plugins/component-developer/commands/fix.md": "---\ndescription: Apply fixes from code review incrementally with proper commits - supports per-severity or per-TODO modes\nallowed-tools: Read, Glob, Grep, Bash, Edit, Write\nargument-hint: [--per-todo | --per-severity]\n---\n\n# Apply Review Fixes\n\nApply fixes from a code review incrementally, with proper commits for each change. This command is designed to work after running `/review` and addresses the problem of making too many changes at once.\n\n## Modes\n\n### Per-Severity Mode (default)\nOne commit per severity category:\n- `fix(review): address blocking review items`\n- `chore(review): apply important review improvements`\n- `style(review): address nits from code review`\n\n### Per-TODO Mode (`--per-todo`)\nOne commit per individual fix:\n- `fix(review): TODO 1 â€“ init client in __init__ (src/component.py)`\n- `refactor(review): TODO 2 â€“ encapsulate config in ClientConfig`\n\n## Usage\n\n```bash\n# Apply fixes grouped by severity (default)\n/fix\n\n# Apply fixes one commit per TODO\n/fix --per-todo\n\n# Apply fixes to specific paths\n/fix src/component.py\n\n# Per-TODO mode on specific paths\n/fix --per-todo src/\n```\n\n## Instructions\n\n### Step 0: Check Prerequisites\n\nEnsure clean working tree before starting:\n\n```bash\n# Check for uncommitted changes\ngit status --porcelain\n```\n\nIf there are uncommitted changes, ask the user to commit or stash them first. Do not proceed with a dirty working tree.\n\n### Step 1: Determine Mode\n\nCheck if `$ARGUMENTS` contains `--per-todo`:\n- If yes: use **per-TODO mode** (one commit per fix)\n- If no: use **per-severity mode** (one commit per category)\n\nExtract any file paths from arguments (anything that's not `--per-todo` or `--per-severity`).\n\n### Step 2: Run Code Review\n\nRe-run the review logic to get fresh TODOs with accurate line numbers:\n\n1. Read the files to be fixed (from arguments or `git diff --name-only`)\n2. Apply review principles (see `@reviewer` agent)\n3. Generate the TODO list grouped by severity\n\n### Step 3: Apply Fixes\n\n#### Per-Severity Mode\n\nProcess in order: Blocking -> Important -> Nice-to-Have\n\n**For Blocking Issues:**\n1. Apply ALL blocking fixes\n2. Run `ruff format .` and `ruff check --fix .`\n3. Verify changes with a quick review\n4. Stage and commit:\n   ```bash\n   git add -A\n   git commit -m \"fix(review): address blocking review items\n\n   - [list each TODO that was fixed]\"\n   ```\n\n**For Important Improvements:**\n1. Apply ALL important fixes\n2. Run `ruff format .` and `ruff check --fix .`\n3. Stage and commit:\n   ```bash\n   git add -A\n   git commit -m \"chore(review): apply important review improvements\n\n   - [list each TODO that was fixed]\"\n   ```\n\n**For Nice-to-Have / Nits:**\n1. Apply ALL nit fixes\n2. Run `ruff format .` and `ruff check --fix .`\n3. Stage and commit:\n   ```bash\n   git add -A\n   git commit -m \"style(review): address nits from code review\n\n   - [list each TODO that was fixed]\"\n   ```\n\n#### Per-TODO Mode\n\nProcess each TODO individually, in order (blocking first, then important, then nits):\n\nFor each TODO:\n1. Apply ONLY that specific fix\n2. Run `ruff format .` on affected files\n3. Stage and commit with descriptive message:\n   ```bash\n   git add [affected files]\n   git commit -m \"fix(review): TODO N â€“ [short description] ([file])\"\n   ```\n\n**Commit prefix by TODO type:**\n- Blocking issues: `fix(review):`\n- Important improvements: `refactor(review):` or `chore(review):`\n- Nits: `style(review):`\n\n### Step 4: Re-validate After Each Chunk\n\nAfter completing each severity bucket (or every 3-5 TODOs in per-TODO mode):\n\n1. Re-run a quick review to check for:\n   - New issues introduced by fixes\n   - Stale line numbers that need updating\n   - Dependencies between fixes that weren't handled\n\n2. If new blocking issues appear, address them before continuing.\n\n### Step 5: Summary\n\nAfter all fixes are applied, provide a summary:\n\n```\n## Review Fix Summary\n\n**Mode:** [per-severity | per-todo]\n**Commits created:** N\n\n### Commits:\n1. `abc1234` - fix(review): address blocking review items\n2. `def5678` - chore(review): apply important review improvements\n3. `ghi9012` - style(review): address nits from code review\n\n### Remaining Issues:\n[List any issues that couldn't be auto-fixed and need manual attention]\n\n**Next steps:**\n- Review the commits with `git log --oneline -N`\n- Run tests to verify changes\n- Push when ready: `git push`\n```\n\n## Architectural Invariants (CRITICAL)\n\nWhen applying fixes, you MUST preserve existing good architectural patterns. The fix command should ENHANCE architecture, never regress it.\n\n### Never Remove or Downgrade These Patterns:\n\n1. **Config-as-model abstractions**\n   - If there's a dataclass or Pydantic model (e.g., `AirtableConfig`, `ClientConfig`) that groups configuration, **DO NOT delete it**\n   - Do not revert code back to scattered `self.configuration.parameters[...]` access\n   - Prefer to INCREASE usage of the config model, not reduce it\n\n2. **Client/config initialization in `__init__`**\n   - If clients (API clients, DB connections) are already initialized in `__init__` and stored on `self`, **DO NOT move that initialization back into `run()`**\n   - Do not recreate clients ad-hoc inside methods or sync actions\n\n3. **Sync action client reuse**\n   - If sync actions already use initialized clients (like `self.api`, `self.api_table`), do not revert them to recreating clients from raw parameters\n\n4. **Docstrings and architectural comments**\n   - Do not remove existing docstrings unless they are plainly incorrect\n   - Preserve comments that explain architectural decisions (e.g., why sync actions access `configuration.parameters` directly)\n   - Removing docstrings is almost NEVER the correct fix\n\n5. **Modern typing once present**\n   - If code already uses `list[T]`, `dict[K, V]`, `T | None`, do not revert to `List[T]`, `Dict[K, V]`, `Optional[T]`\n\n### Before Applying Any Fix:\n\n1. **Scan for existing patterns** - Identify if the code already uses config-as-model and `__init__`-initialized clients\n2. **Treat good patterns as invariants** - These are the target architecture to preserve\n3. **Adapt TODOs to existing architecture** - If a TODO suggests \"centralize config access\" and you already have a config dataclass, that means \"use the dataclass MORE,\" not \"revert to dicts\"\n\n### After Each Batch of Fixes:\n\nPerform a sanity check:\n- Do I still have a single config model? (If one existed before)\n- Are clients still initialized once in `__init__`? (If they were before)\n- Did I remove any docstrings? (If so, restore them)\n- Did I reintroduce `self.configuration.parameters[...]` access where a config object was used? (If so, revert)\n\nIf a proposed fix solves a TODO but reintroduces an anti-pattern, **prefer a different implementation** or skip it with a note: \"This TODO conflicts with existing good architecture, leaving as-is.\"\n\n## Safety Rules\n\n1. **Never auto-push** - Let the user inspect commits first\n2. **Only touch TODO-related code** - No opportunistic cleanups\n3. **Preserve functionality** - If a fix might change behavior, ask first\n4. **Bail out on conflicts** - If fixes conflict with each other, stop and ask\n5. **Re-validate after changes** - Don't blindly apply stale line numbers\n6. **Never regress architecture** - Preserve config models, initialized clients, and docstrings\n\n## Handling Dependencies\n\nWhen fixes depend on each other:\n\n1. **Structural changes first** - Function signatures, class definitions, config models\n2. **Then callers** - Code that uses the changed structures\n3. **Then formatting** - Import organization, style fixes\n\nIf TODO 2 depends on TODO 1:\n- In per-severity mode: both are in the same commit, apply in order\n- In per-TODO mode: apply TODO 1 first, then re-locate TODO 2's target before applying\n\n## Example Session\n\n```\nUser: /fix --per-todo\n\nAssistant: Running in per-TODO mode. Found 5 TODOs to fix.\n\nChecking working tree... clean.\n\n## Applying Fixes\n\n### TODO 1: Move client initialization to __init__\nApplying fix to src/component.py:45-52...\nRunning ruff format...\nCommitting: fix(review): TODO 1 Ã¢Â€Â“ init client in __init__ (src/component.py)\n\n### TODO 2: Encapsulate configuration in typed object\nApplying fix to src/component.py:23-35...\nRunning ruff format...\nCommitting: refactor(review): TODO 2 Ã¢Â€Â“ encapsulate config in ClientConfig\n\n### TODO 3: Use modern typing syntax\nApplying fix to src/client.py:12...\nRunning ruff format...\nCommitting: style(review): TODO 3 Ã¢Â€Â“ modernize typing in src/client.py\n\n[continues for each TODO...]\n\n## Review Fix Summary\n\n**Mode:** per-todo\n**Commits created:** 5\n\n### Commits:\n1. `abc1234` - fix(review): TODO 1 Ã¢Â€Â“ init client in __init__\n2. `def5678` - refactor(review): TODO 2 Ã¢Â€Â“ encapsulate config\n3. `ghi9012` - style(review): TODO 3 Ã¢Â€Â“ modernize typing\n4. `jkl3456` - style(review): TODO 4 Ã¢Â€Â“ organize imports\n5. `mno7890` - style(review): TODO 5 Ã¢Â€Â“ add staticmethod decorator\n\n**Next steps:**\n- Review commits: `git log --oneline -5`\n- Run tests: `uv run pytest`\n- Push when ready: `git push`\n```\n\n## Reference\n\nThis command works best after running `/review` first. For the full review guidelines, see `agents/reviewer.md`.\n",
        "plugins/component-developer/commands/init.md": "---\ndescription: Initialize new Keboola component from cookiecutter template with automatic cleanup\nallowed-tools: Bash, Read, Write, Edit, Glob, AskUserQuestion\nargument-hint: [component-name]\n---\n\n# Initialize New Component\n\nQuickly initialize a new Keboola Python component using the official cookiecutter template with automatic cleanup and best practices.\n\n## What This Command Does\n\n1. **Runs cookiecutter template** - Uses `gh:keboola/cookiecutter-python-component`\n2. **Cleans up example files** - Removes cookiecutter examples from `data/`\n3. **Creates test config** - Generates `data/config.json` with example parameters\n4. **Initializes git** - Creates initial commit with proper structure\n5. **Validates structure** - Ensures everything is set up correctly\n\n## Usage\n\n```bash\n# Interactive mode (asks for component details)\n/init\n\n# With component name\n/init my-awesome-extractor\n\n# Specify output directory\n/init my-component --output ~/projects/\n```\n\n## Instructions\n\n### Step 1: Gather Component Information\n\nIf component name is provided in `$ARGUMENTS`, use it. Otherwise, ask the user:\n\n```\nComponent name (e.g., ex-my-api):\nComponent ID (kebab-case, e.g., keboola.ex-my-api):\nComponent type (extractor/writer/application):\nShort description:\n```\n\n**Naming conventions:**\n- Don't include \"extractor\", \"writer\", or \"application\" in the name\n- Use kebab-case for IDs (e.g., `keboola.ex-salesforce`)\n- Follow pattern: `vendor.component-name`\n\n### Step 2: Run Cookiecutter Template\n\n```bash\n# Install cookiecutter if needed\nwhich cookiecutter || pip install cookiecutter\n\n# Run template (non-interactive if we have all info)\ncookiecutter gh:keboola/cookiecutter-python-component \\\n  --no-input \\\n  component_id=\"$COMPONENT_ID\" \\\n  name=\"$COMPONENT_NAME\" \\\n  type=\"$COMPONENT_TYPE\" \\\n  description=\"$DESCRIPTION\"\n\n# Or interactive mode if user wants to customize\ncookiecutter gh:keboola/cookiecutter-python-component\n```\n\n**Expected output:**\n```\ncomponent_id: keboola.ex-my-api\nname: ex-my-api\ntype: extractor\ndescription: My awesome API extractor\n...\nâœ“ Created component in ./ex-my-api/\n```\n\n### Step 3: Navigate to Component Directory\n\n```bash\ncd $COMPONENT_NAME\nls -la\n```\n\n**Expected structure:**\n```\nex-my-api/\nâ”œâ”€â”€ .github/workflows/\nâ”‚   â””â”€â”€ push.yml\nâ”œâ”€â”€ component_config/\nâ”‚   â”œâ”€â”€ configSchema.json\nâ”‚   â”œâ”€â”€ configRowSchema.json\nâ”‚   â””â”€â”€ component_*_description.md\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ test.csv          # â† TO BE REMOVED\nâ”‚   â”œâ”€â”€ order1.xml         # â† TO BE REMOVED\nâ”‚   â””â”€â”€ .gitkeep           # â† TO BE REMOVED\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ component.py\nâ”‚   â””â”€â”€ configuration.py\nâ”œâ”€â”€ tests/\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ pyproject.toml\nâ””â”€â”€ README.md\n```\n\n### Step 4: Clean Up Cookiecutter Examples\n\nRemove example files that come with the template:\n\n```bash\n# Remove example data files\nrm -f data/test.csv data/order1.xml data/.gitkeep\n\n# Verify data/ directory is empty\nls -la data/\n```\n\n**Critical:** Keep the `data/` directory structure but remove ALL example files.\n\n### Step 5: Create Component-Specific Config\n\nCreate `data/config.json` with realistic example parameters:\n\n```bash\n# Create config.json based on component type\ncat > data/config.json << 'EOF'\n{\n  \"parameters\": {\n    \"api_url\": \"https://api.example.com/v1\",\n    \"#api_key\": \"your-api-key-here\",\n    \"debug\": false\n  }\n}\nEOF\n```\n\n**Adapt parameters based on configSchema.json:**\n- Read `component_config/configSchema.json`\n- Extract required fields\n- Generate realistic example values\n- Use `#` prefix for sensitive fields\n\n### Step 6: Initialize Git Repository\n\n```bash\n# Initialize git if not already initialized\ngit init\n\n# Add all files\ngit add -A\n\n# Create initial commit\ngit commit -m \"feat: initialize component from cookiecutter template\n\nComponent: $COMPONENT_NAME\nType: $COMPONENT_TYPE\nTemplate: keboola/cookiecutter-python-component\n\n- Removed cookiecutter example files from data/\n- Created component-specific data/config.json with example parameters\n- Ready for implementation\n\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\"\n```\n\n### Step 7: Validate Structure\n\nRun validation checks:\n\n```bash\n# Check required files exist\ntest -f src/component.py && echo \"âœ“ component.py exists\"\ntest -f component_config/configSchema.json && echo \"âœ“ configSchema.json exists\"\ntest -f data/config.json && echo \"âœ“ config.json exists\"\ntest -f pyproject.toml && echo \"âœ“ pyproject.toml exists\"\n\n# Check example files are removed\n! test -f data/test.csv && echo \"âœ“ test.csv removed\"\n! test -f data/.gitkeep && echo \"âœ“ .gitkeep removed\"\n\n# Check git is initialized\ngit log --oneline -1 && echo \"âœ“ Git initialized\"\n```\n\n### Step 8: Install Dependencies (Optional)\n\nAsk user if they want to install dependencies now:\n\n```bash\n# Install dependencies with uv (faster) or pip\nuv sync\n# or\npip install -e .\n```\n\n### Step 9: Next Steps Summary\n\nProvide clear next steps:\n\n```\n## âœ… Component Initialized Successfully!\n\n**Location:** ./$COMPONENT_NAME/\n**Component ID:** $COMPONENT_ID\n**Type:** $COMPONENT_TYPE\n\n### Structure Overview:\n- src/component.py          # Main component logic (implement run() method)\n- component_config/         # Configuration schemas and descriptions\n- data/config.json          # Local test configuration\n- tests/                    # Unit and datadir tests\n- .github/workflows/        # CI/CD deployment\n\n### Next Steps:\n\n1. **Implement component logic:**\n   cd $COMPONENT_NAME\n   # Edit src/component.py and implement run() method\n\n2. **Test locally:**\n   /run                     # Run with data/config.json\n\n3. **Design configuration schema:**\n   /schema-test             # Interactive schema testing\n\n4. **Write tests:**\n   @test-component         # Get help with testing\n\n5. **Create repository:**\n   gh repo create keboola/$COMPONENT_NAME --private\n   git remote add origin git@github.com:keboola/$COMPONENT_NAME.git\n   git push -u origin main\n\n### Helpful Commands:\n- /run                      # Run component locally\n- /test                     # Run tests\n- /schema-test              # Test configuration schemas\n- /review                   # Code review\n- @build-component          # Get implementation help\n\n### Resources:\n- Component Tutorial: https://developers.keboola.com/extend/component/tutorial/\n- Python Component Library: https://github.com/keboola/python-component\n- Developer Portal: https://components.keboola.com/\n```\n\n## Safety Rules\n\n1. **Never overwrite existing component** - Check if directory exists first\n2. **Always clean up example files** - Users shouldn't commit cookiecutter examples\n3. **Create realistic config.json** - Not empty, not minimal, but realistic\n4. **Don't modify cookiecutter template** - Use it as-is, clean up after\n5. **Ask before installing dependencies** - User might prefer manual setup\n\n## Common Issues\n\n### Issue: Directory already exists\n```bash\n# Check first\nif [ -d \"$COMPONENT_NAME\" ]; then\n  echo \"Error: Directory $COMPONENT_NAME already exists\"\n  echo \"Options:\"\n  echo \"  1. Choose different name\"\n  echo \"  2. Remove existing directory: rm -rf $COMPONENT_NAME\"\n  exit 1\nfi\n```\n\n### Issue: Cookiecutter not installed\n```bash\n# Install cookiecutter\npip install --user cookiecutter\n# or\npipx install cookiecutter\n```\n\n### Issue: Git user not configured\n```bash\n# Configure git\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n```\n\n## Example Session\n\n```\nUser: /init ex-salesforce\nAssistant: Running cookiecutter for ex-salesforce...\n\nComponent name: ex-salesforce\nComponent ID: keboola.ex-salesforce  \nType: extractor\nDescription: Salesforce data extractor\n\nâœ“ Created component in ./ex-salesforce/\n\nCleaning up example files...\nâœ“ Removed data/test.csv\nâœ“ Removed data/order1.xml\nâœ“ Removed data/.gitkeep\n\nCreating data/config.json...\nâœ“ Created with realistic Salesforce parameters\n\nInitializing git...\nâœ“ Initial commit created\n\n## âœ… Component Initialized Successfully!\n\n**Location:** ./ex-salesforce/\n**Component ID:** keboola.ex-salesforce\n**Type:** extractor\n\nNext steps:\n1. cd ex-salesforce\n2. Implement src/component.py\n3. Run locally: /run\n4. Create GitHub repo: gh repo create keboola/ex-salesforce\n```\n\n## Reference\n\nFor implementation guidance after initialization, use:\n- `@build-component` - Component architecture and patterns  \n- `@build-component-ui` - Configuration schemas\n- `@test-component` - Writing tests\n- `@get-started` - Initialization details\n\n",
        "plugins/component-developer/commands/migrate-repo.md": "---\ndescription: Migrate Keboola component repository from Bitbucket to GitHub with full history, branches, tags, and GitHub Actions setup\nallowed-tools: Bash, Read, Write, Edit, AskUserQuestion\nargument-hint: <bitbucket-url> [github-org/repo-name]\n---\n\n# Migrate Repository from Bitbucket to GitHub\n\nComplete guide for migrating a Keboola component repository from Bitbucket to GitHub, preserving full history, all branches, tags, and setting up GitHub Actions for CI/CD deployment.\n\n## What This Command Does\n\n1. **Clones Bitbucket repository** - Full clone with all branches and tags\n2. **Creates GitHub repository** - Empty repo ready for migration\n3. **Pushes everything** - All branches, tags, and full commit history\n4. **Migrates CI/CD** - Replaces bitbucket-pipelines.yml with GitHub Actions\n5. **Configures GitHub** - Sets up secrets, team access, branch protection\n6. **Verifies migration** - Validates all data transferred correctly\n\n## Prerequisites\n\nBefore running this command, ensure you have:\n- GitHub CLI (`gh`) installed and authenticated\n- Git configured (user.name, user.email)\n- Access to Bitbucket repository\n- Permissions on GitHub organization (create repos, manage teams, set secrets)\n\n## Usage\n\n```bash\n# Interactive mode (asks for details)\n/migrate-repo\n\n# With Bitbucket URL\n/migrate-repo git@bitbucket.org:workspace/repo-name.git\n\n# Specify GitHub destination\n/migrate-repo git@bitbucket.org:workspace/repo.git keboola/new-repo-name\n\n# Dry run (show what would happen)\n/migrate-repo --dry-run git@bitbucket.org:workspace/repo.git\n```\n\n## Instructions\n\n### Step 1: Validate Prerequisites\n\nCheck that all required tools are installed and configured:\n\n```bash\n# Check GitHub CLI authentication\ngh auth status || {\n  echo \"Error: GitHub CLI not authenticated\"\n  echo \"Run: gh auth login\"\n  exit 1\n}\n\n# Check git configuration\ngit config --get user.name || {\n  echo \"Error: Git user.name not configured\"\n  echo \"Run: git config --global user.name 'Your Name'\"\n  exit 1\n}\n\ngit config --get user.email || {\n  echo \"Error: Git user.email not configured\"\n  echo \"Run: git config --global user.email 'your.email@example.com'\"\n  exit 1\n}\n\necho \"âœ“ Prerequisites validated\"\n```\n\n### Step 2: Gather Migration Details\n\nCollect information about the migration:\n\n```bash\n# Parse arguments or ask user\nif [ -z \"$BITBUCKET_URL\" ]; then\n  # Ask user for Bitbucket URL\n  echo \"Bitbucket repository URL (e.g., git@bitbucket.org:workspace/repo.git):\"\n  read BITBUCKET_URL\nfi\n\n# Extract repo name from Bitbucket URL\nREPO_NAME=$(echo \"$BITBUCKET_URL\" | sed 's/.*\\///' | sed 's/\\.git$//')\n\n# Ask for GitHub org/repo or use argument\nif [ -z \"$GITHUB_REPO\" ]; then\n  echo \"GitHub repository (e.g., keboola/component-name) [keboola/$REPO_NAME]:\"\n  read GITHUB_REPO\n  GITHUB_REPO=${GITHUB_REPO:-\"keboola/$REPO_NAME\"}\nfi\n\n# Extract component ID from repo name (for GitHub Actions)\nCOMPONENT_ID=$(echo \"$REPO_NAME\" | sed 's/^component-//')\n\necho \"\"\necho \"Migration Plan:\"\necho \"  From: $BITBUCKET_URL\"\necho \"  To: github.com/$GITHUB_REPO\"\necho \"  Component ID: $COMPONENT_ID\"\necho \"\"\n```\n\n### Step 3: Clone Bitbucket Repository\n\nClone the full Bitbucket repository locally:\n\n```bash\nWORK_DIR=\"/tmp/migration-$(date +%s)\"\nmkdir -p \"$WORK_DIR\"\ncd \"$WORK_DIR\"\n\necho \"ðŸ“¦ Cloning Bitbucket repository...\"\ngit clone \"$BITBUCKET_URL\" \"$REPO_NAME\" || {\n  echo \"Error: Failed to clone Bitbucket repository\"\n  exit 1\n}\n\ncd \"$REPO_NAME\"\n\n# Fetch all branches\ngit fetch --all\n\n# Show what we have\necho \"\"\necho \"âœ“ Cloned repository\"\necho \"  Branches: $(git branch -a | wc -l | xargs)\"\necho \"  Tags: $(git tag | wc -l | xargs)\"\necho \"  Commits: $(git rev-list --all --count)\"\necho \"\"\n```\n\n### Step 4: Create GitHub Repository\n\nCreate an empty GitHub repository:\n\n```bash\necho \"ðŸ“ Creating GitHub repository...\"\n\n# Create empty private repo\ngh repo create \"$GITHUB_REPO\" --private --clone=false || {\n  echo \"Error: Failed to create GitHub repository\"\n  echo \"It may already exist. Continue anyway? (y/n)\"\n  read CONTINUE\n  if [ \"$CONTINUE\" != \"y\" ]; then\n    exit 1\n  fi\n}\n\necho \"âœ“ GitHub repository created: github.com/$GITHUB_REPO\"\n```\n\n### Step 5: Push Everything to GitHub\n\nPush all branches and tags:\n\n```bash\n# Add GitHub as remote\ngit remote add github \"git@github.com:$GITHUB_REPO.git\"\n\necho \"ðŸš€ Pushing to GitHub...\"\n\n# Push all branches\ngit push github --all || {\n  echo \"Warning: Some branches failed to push\"\n}\n\n# Push all tags\ngit push github --tags || {\n  echo \"Warning: Some tags failed to push\"\n}\n\n# Push any remaining remote branches\nfor branch in $(git branch -r | grep 'origin/' | grep -v 'HEAD' | grep -v 'master' | grep -v 'main'); do\n  branch_name=${branch#origin/}\n  echo \"  Pushing branch: $branch_name\"\n  git push github \"refs/remotes/$branch:refs/heads/$branch_name\" 2>/dev/null || true\ndone\n\necho \"âœ“ Pushed all branches and tags\"\n```\n\n### Step 6: Migrate CI/CD to GitHub Actions\n\nReplace Bitbucket Pipelines with GitHub Actions:\n\n```bash\n# Check out master/main branch\nif git show-ref --verify --quiet refs/heads/master; then\n  git checkout master\nelif git show-ref --verify --quiet refs/heads/main; then\n  git checkout main\nelse\n  echo \"Warning: Neither master nor main branch found\"\n  git checkout $(git branch | head -1 | sed 's/^* //')\nfi\n\necho \"ðŸ”„ Migrating CI/CD to GitHub Actions...\"\n\n# Remove Bitbucket Pipelines\nrm -f bitbucket-pipelines.yml\n\n# Download latest GitHub Actions workflow from cookiecutter template\nmkdir -p .github/workflows\n\necho \"Downloading latest push.yml from cookiecutter template...\"\nCOOKIECUTTER_WORKFLOW_URL=\"https://raw.githubusercontent.com/keboola/cookiecutter-python-component/main/%7B%7Bcookiecutter.repository_folder_name%7D%7D/.github/workflows/push.yml\"\n\ncurl -fsSL \"$COOKIECUTTER_WORKFLOW_URL\" -o .github/workflows/push.yml || {\n  echo \"Error: Failed to download workflow from cookiecutter template\"\n  exit 1\n}\n\n# Replace cookiecutter placeholders with actual values\nsed -i.bak \\\n  -e \"s/COOKIECUTTER_DEV_PORTAL_VENDOR_NAME/keboola/g\" \\\n  -e \"s/COOKIECUTTER_DEV_PORTAL_COMPONENT_ID/$COMPONENT_ID/g\" \\\n  .github/workflows/push.yml\n\nrm -f .github/workflows/push.yml.bak\n\necho \"âœ“ Downloaded and configured push.yml from cookiecutter template\"\n\n# Commit changes\ngit add -A\ngit commit -m \"ci: migrate from Bitbucket Pipelines to GitHub Actions\n\n- Added GitHub Actions workflow from cookiecutter template\n- Workflow: .github/workflows/push.yml\n- Removed bitbucket-pipelines.yml\n- Component ID: $COMPONENT_ID\n- Template: keboola/cookiecutter-python-component\n\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\"\n\n# Push to GitHub\ngit push github HEAD\n\necho \"âœ“ CI/CD migrated to GitHub Actions\"\n```\n\n### Step 7: Rename master â†’ main (if needed)\n\nOptionally rename master branch to main:\n\n```bash\necho \"ðŸ”„ Renaming master â†’ main...\"\n\n# Check if we need to rename\nif git show-ref --verify --quiet refs/heads/master; then\n  # Set master as default temporarily\n  gh api -X PATCH \"/repos/$GITHUB_REPO\" -f default_branch=master\n\n  # Remove branch protection from main if exists\n  gh api -X DELETE \"/repos/$GITHUB_REPO/branches/main/protection\" 2>/dev/null || true\n\n  # Delete old main if exists\n  git push github --delete main 2>/dev/null || true\n\n  # Create new main from master\n  git push github master:refs/heads/main\n\n  # Set main as default\n  gh api -X PATCH \"/repos/$GITHUB_REPO\" -f default_branch=main\n\n  # Delete master\n  git push github --delete master\n\n  echo \"âœ“ Renamed master â†’ main\"\nfi\n```\n\n### Step 8: Configure GitHub Secrets\n\nSet up required secrets for GitHub Actions:\n\n```bash\necho \"ðŸ” Setting up GitHub secrets...\"\n\n# Set Developer Portal credentials\necho \"Enter KBC Developer Portal password (kds-team+github):\"\nread -s KBC_PASSWORD\n\necho \"$KBC_PASSWORD\" | gh secret set KBC_DEVELOPERPORTAL_PASSWORD -R \"$GITHUB_REPO\"\necho \"kds-team+github\" | gh secret set KBC_DEVELOPERPORTAL_USERNAME -R \"$GITHUB_REPO\"\n\necho \"âœ“ Secrets configured\"\n```\n\n### Step 9: Configure Team Access\n\nGrant team permissions:\n\n```bash\necho \"ðŸ‘¥ Configuring team access...\"\n\n# Extract org from GITHUB_REPO\nGITHUB_ORG=$(echo \"$GITHUB_REPO\" | cut -d/ -f1)\n\n# Add component-factory team with admin access\ngh api -X PUT \"/orgs/$GITHUB_ORG/teams/component-factory/repos/$GITHUB_REPO\" \\\n  -f permission=admin || {\n  echo \"Warning: Failed to add team access (team may not exist)\"\n}\n\necho \"âœ“ Team access configured\"\n```\n\n### Step 10: Configure Branch Protection\n\nSet up branch protection rules:\n\n```bash\necho \"ðŸ›¡ï¸ Setting up branch protection...\"\n\ngh api -X PUT \"/repos/$GITHUB_REPO/branches/main/protection\" --input - << 'EOF'\n{\n  \"required_status_checks\": {\n    \"strict\": true,\n    \"contexts\": [\"build\"]\n  },\n  \"enforce_admins\": true,\n  \"required_pull_request_reviews\": {\n    \"dismiss_stale_reviews\": true,\n    \"require_code_owner_reviews\": false,\n    \"required_approving_review_count\": 1\n  },\n  \"restrictions\": null\n}\nEOF\n\necho \"âœ“ Branch protection enabled\"\n```\n\n### Step 11: Verify Migration\n\nValidate that everything transferred correctly:\n\n```bash\necho \"\"\necho \"âœ… Verifying migration...\"\necho \"\"\n\n# Check default branch\nDEFAULT_BRANCH=$(gh repo view \"$GITHUB_REPO\" --json defaultBranchRef --jq '.defaultBranchRef.name')\necho \"âœ“ Default branch: $DEFAULT_BRANCH\"\n\n# Check branches\nBRANCH_COUNT=$(gh api \"/repos/$GITHUB_REPO/branches\" --jq '. | length')\necho \"âœ“ Branches: $BRANCH_COUNT\"\n\n# Check tags\nTAG_COUNT=$(gh api \"/repos/$GITHUB_REPO/tags\" --paginate --jq '. | length' | awk '{s+=$1} END {print s}')\necho \"âœ“ Tags: $TAG_COUNT\"\n\n# Check secrets\nSECRET_COUNT=$(gh secret list -R \"$GITHUB_REPO\" | wc -l | xargs)\necho \"âœ“ Secrets: $SECRET_COUNT\"\n\n# Check workflow\nif gh api \"/repos/$GITHUB_REPO/contents/.github/workflows/push.yml\" &>/dev/null; then\n  echo \"âœ“ GitHub Actions workflow present\"\nfi\n\n# Check bitbucket-pipelines removed\nif ! gh api \"/repos/$GITHUB_REPO/contents/bitbucket-pipelines.yml\" &>/dev/null; then\n  echo \"âœ“ bitbucket-pipelines.yml removed\"\nfi\n\necho \"\"\n```\n\n### Step 12: Cleanup and Summary\n\nClean up temporary directory and show summary:\n\n```bash\necho \"ðŸ§¹ Cleaning up...\"\ncd /\nrm -rf \"$WORK_DIR\"\n\necho \"\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\necho \"\"\necho \"âœ… Migration Complete!\"\necho \"\"\necho \"ðŸ“¦ Repository: https://github.com/$GITHUB_REPO\"\necho \"ðŸ”§ Component ID: $COMPONENT_ID\"\necho \"ðŸŒ¿ Default branch: main\"\necho \"\"\necho \"Next Steps:\"\necho \"  1. Verify GitHub Actions workflow runs successfully\"\necho \"  2. Update Developer Portal URLs (on next release):\"\necho \"     - sourceCodeUrl: https://github.com/$GITHUB_REPO\"\necho \"     - documentationUrl: https://github.com/$GITHUB_REPO/blob/main/README.md\"\necho \"  3. Update local working directory:\"\necho \"     cd /path/to/working/repo\"\necho \"     git remote set-url origin git@github.com:$GITHUB_REPO.git\"\necho \"     git fetch origin\"\necho \"     git reset --hard origin/main\"\necho \"\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\necho \"\"\n```\n\n## Important Notes\n\n### Preserving History\n- **All commit timestamps preserved** - Author and committer dates stay intact\n- **Full branch history** - All branches transferred with complete history\n- **Tags maintained** - All version tags transferred correctly\n- Only new migration commit has current timestamp\n\n### Alternative: Git Mirror Method\n\nFor simpler migration (without GitHub Actions setup):\n\n```bash\n# Clone as mirror\ngit clone --mirror git@bitbucket.org:workspace/repo.git repo.git\ncd repo.git\n\n# Push mirror to GitHub\ngit push --mirror git@github.com:org/repo.git\n```\n\n**Pros:** Simple, one command, complete mirror\n**Cons:** Doesn't set up GitHub Actions, requires manual workflow addition\n\n## Safety Checks\n\n1. **Dry run first** - Use `--dry-run` to see what would happen\n2. **Verify branches** - Check all branches transferred\n3. **Verify tags** - Confirm version tags present\n4. **Test workflow** - Ensure GitHub Actions runs successfully\n5. **Keep Bitbucket** - Don't delete Bitbucket repo until verified\n\n## Troubleshooting\n\n### Error: Branch protection blocks push\n```bash\n# Temporarily disable enforce_admins\ngh api -X PATCH \"/repos/$GITHUB_REPO/branches/main/protection\" \\\n  -f enforce_admins=false\n\n# Make your changes\ngit push github main\n\n# Re-enable\ngh api -X PATCH \"/repos/$GITHUB_REPO/branches/main/protection\" \\\n  -f enforce_admins=true\n```\n\n### Error: Remote branches not pushing\n```bash\n# Use explicit refspec\ngit push github refs/remotes/origin/branch-name:refs/heads/branch-name\n```\n\n### Error: Permission denied\nCheck that you have:\n- Admin access to GitHub organization\n- Push access to Bitbucket repository\n- `gh` CLI authenticated with correct account\n\n## Migration Checklist\n\nAfter migration, verify:\n\n- [ ] All branches transferred\n- [ ] All tags transferred\n- [ ] Commit history preserved (including timestamps)\n- [ ] GitHub Actions workflow added\n- [ ] `KBC_DEVELOPERPORTAL_APP` ID correct in workflow\n- [ ] bitbucket-pipelines.yml removed\n- [ ] Secrets configured (USERNAME, PASSWORD)\n- [ ] Team access granted (component-factory: admin)\n- [ ] Branch protection enabled on main\n- [ ] Default branch set to main\n- [ ] First GitHub Actions build triggered\n- [ ] Local repo updated to use GitHub remote\n\n## Example Session\n\n```\nUser: /migrate-repo git@bitbucket.org:keboola/component-wr-sftp-csas.git\nAssistant: Starting Bitbucket â†’ GitHub migration...\n\nâœ“ Prerequisites validated\n\nMigration Plan:\n  From: git@bitbucket.org:keboola/component-wr-sftp-csas.git\n  To: github.com/keboola/component-wr-sftp-csas\n  Component ID: wr-sftp-csas\n\nðŸ“¦ Cloning Bitbucket repository...\nâœ“ Cloned repository\n  Branches: 5\n  Tags: 25\n  Commits: 147\n\nðŸ“ Creating GitHub repository...\nâœ“ GitHub repository created\n\nðŸš€ Pushing to GitHub...\nâœ“ Pushed all branches and tags\n\nðŸ”„ Migrating CI/CD to GitHub Actions...\nâœ“ CI/CD migrated to GitHub Actions\n\nðŸ”„ Renaming master â†’ main...\nâœ“ Renamed master â†’ main\n\nðŸ” Setting up GitHub secrets...\nâœ“ Secrets configured\n\nðŸ‘¥ Configuring team access...\nâœ“ Team access configured\n\nðŸ›¡ï¸ Setting up branch protection...\nâœ“ Branch protection enabled\n\nâœ… Verifying migration...\nâœ“ Default branch: main\nâœ“ Branches: 5\nâœ“ Tags: 25\nâœ“ Secrets: 2\nâœ“ GitHub Actions workflow present\nâœ“ bitbucket-pipelines.yml removed\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nâœ… Migration Complete!\n\nðŸ“¦ Repository: https://github.com/keboola/component-wr-sftp-csas\nðŸ”§ Component ID: wr-sftp-csas\nðŸŒ¿ Default branch: main\n\nNext Steps:\n  1. Verify GitHub Actions workflow runs successfully\n  2. Update Developer Portal URLs (on next release)\n  3. Update local working directory\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n```\n\n## Reference\n\n- Original guide (Czech): MIGRATION_GUIDE.md\n- GitHub CLI docs: https://cli.github.com/\n- Keboola Developer Portal: https://components.keboola.com/\n\n",
        "plugins/component-developer/commands/review.md": "---\ndescription: Thorough code review of Keboola Python component code, focusing on architecture, config/client patterns, and Pythonic best practices\nallowed-tools: Read, Glob, Grep, Bash\nargument-hint: [paths-or-scope]\n---\n\n# Component Code Review\n\nPerform a thorough, opinionated code review focusing on Keboola Python component architecture and best practices.\n\n## What This Command Does\n\n1. **Reviews the current diff or specified paths** for Keboola Python component code\n2. **Applies opinionated review rules** from the reviewer agent:\n   - Architecture first: separation of concerns (component vs client vs config)\n   - Config/client initialization in `__init__`, not `run()`\n   - Modern typing (built-in generics, no deprecated `typing.List`/`Dict`/`Optional`)\n   - Safety and robustness (edge cases, pagination guards)\n   - Repository hygiene (dependencies, stray files)\n3. **Produces a specific TODO list** with line numbers, patterns, and concrete fixes\n4. **Uses a characteristic tone**: direct but kind, giving authors agency\n\n## Usage\n\n```bash\n# Review unstaged changes (default)\n/review\n\n# Review specific files or directories\n/review src/component.py src/client.py\n\n# Review all Python files in a directory\n/review src/\n```\n\n## Instructions\n\n### Step 1: Determine Review Scope\n\nIf the user provided paths as arguments (`$ARGUMENTS`), review those specific files/directories.\nOtherwise, review unstaged changes from `git diff`.\n\n```bash\n# Check for unstaged changes\ngit diff --name-only\n\n# Or if paths provided, verify they exist\nls -la $ARGUMENTS\n```\n\n### Step 2: Read the Code\n\nRead the files to be reviewed. For components, focus on:\n- `src/component.py` - Main component logic\n- `src/configuration.py` - Configuration handling\n- `src/*_client.py` - API client classes\n- `pyproject.toml` - Dependencies and Python version\n\n```bash\n# Get list of Python files\nfind src/ -name \"*.py\" -type f 2>/dev/null || echo \"No src/ directory\"\n```\n\n### Step 3: Check Project Context\n\nLook for project-specific rules and Python version:\n\n```bash\n# Check for CLAUDE.md or AGENTS.md\ncat CLAUDE.md 2>/dev/null || cat AGENTS.md 2>/dev/null || echo \"No project rules file\"\n\n# Check Python version from pyproject.toml\ngrep -A2 \"python\" pyproject.toml 2>/dev/null || echo \"No pyproject.toml\"\n```\n\n### Step 4: Apply Review Principles\n\nReview the code against these key principles (in order of importance):\n\n**Architecture (Blocking if violated):**\n- Is `run()` a clean orchestrator (< 30 lines)?\n- Are clients and configuration initialized in `__init__`, not `run()`?\n- Are clients stored as instance attributes (`self.client`)?\n- Is configuration encapsulated in a typed config object?\n\n**Code Quality (Important):**\n- Are type hints using modern syntax (`list[str]`, not `List[str]`)?\n- Are deprecated typing classes avoided (`typing.List`, `typing.Dict`, `typing.Optional`)?\n- Is code formatted with ruff?\n- Are imports organized?\n\n**Safety (Important):**\n- Are indexing/popping operations guarded by preconditions?\n- Does pagination have explicit stopping conditions?\n- Are edge cases handled (empty responses, last page)?\n\n**Repository Hygiene (Nice-to-have):**\n- Are dependencies sensibly pinned (not over-locked)?\n- Are there stray files that shouldn't be there?\n- Is the Python version reasonably current?\n\n### Step 5: Format the Review as TODO List\n\nStart with a brief overall assessment:\n- \"This is a great effort, just a couple of sections to clarify\"\n- \"A couple of remarks, but nothing that important\"\n- \"The component.py file is nice and clean\"\n\nThen produce a **specific TODO list** grouped by severity. Each TODO must include:\n1. **Location** - File path and line number(s)\n2. **Pattern** - The specific code or pattern that needs to change\n3. **Fix** - Concrete guidance on what to change it to (2-3 sentences max)\n\n## Example Output\n\n```\n## Overall Assessment\n\nThe component.py file is nice and clean, with good separation of concerns. A couple of remarks, but nothing that important.\n\n## Blocking Issues\n\n### TODO 1: Move client initialization to __init__\n**Location:** `src/component.py:45-52`\n**Pattern:** `self.client = ApiClient(...)` is created inside `run()` method.\n**Fix:** Move this initialization to `__init__` and store as `self.client`. This allows sync_actions to reuse the client without duplicating logic.\n\n## Important Improvements\n\n### TODO 2: Use modern typing syntax\n**Location:** `src/configuration.py:12`\n**Pattern:** `from typing import List, Dict, Optional`\n**Fix:** Remove this import. Use built-in generics: `list[str]` instead of `List[str]`, `str | None` instead of `Optional[str]`.\n\n## Nice-to-Have\n\n### TODO 3: Organize imports\n**Location:** `src/component.py:1-15`\n**Pattern:** Imports are not sorted according to ruff conventions.\n**Fix:** Run `ruff check --select I --fix src/component.py` to auto-organize imports.\n\n---\nLGTM with the above changes!\n```\n\n## Reference\n\nThis command applies the principles from the `@reviewer` agent. For the full set of review guidelines, see `agents/reviewer.md`.\n",
        "plugins/component-developer/commands/run.md": "---\ndescription: Run Keboola component locally with test data and display results\nallowed-tools: Bash, Read, Glob, Write\nargument-hint: [config-file]\n---\n\n# Run Component Locally\n\nRun your Keboola component locally with test configuration and automatically display results.\n\n## What This Command Does\n\n1. **Validates environment** - Checks `data/config.json` exists\n2. **Runs component** - Executes with `KBC_DATADIR=data`\n3. **Monitors output** - Streams logs and errors in real-time\n4. **Shows results** - Displays output tables and files\n5. **Summarizes execution** - Shows success/failure with timing\n\n## Usage\n\n```bash\n# Run with default data/config.json\n/run\n\n# Run with specific config file\n/run data/config-test.json\n\n# Run with custom data directory\n/run --datadir=test-data\n\n# Run with verbose output\n/run --verbose\n```\n\n## Instructions\n\n### Step 1: Validate Environment\n\nCheck that we're in a component directory:\n\n```bash\n# Check for component files\ntest -f src/component.py || {\n  echo \"Error: Not in a component directory (src/component.py not found)\"\n  exit 1\n}\n\n# Check for pyproject.toml or requirements.txt\ntest -f pyproject.toml || test -f requirements.txt || {\n  echo \"Error: No pyproject.toml or requirements.txt found\"\n  exit 1\n}\n```\n\n### Step 2: Determine Configuration\n\nExtract config file from arguments or use default:\n\n```bash\n# Parse arguments\nCONFIG_FILE=\"data/config.json\"\nDATA_DIR=\"data\"\nVERBOSE=false\n\n# If $ARGUMENTS contains a path, use it\nif [[ \"$ARGUMENTS\" == *.json ]]; then\n  CONFIG_FILE=\"$ARGUMENTS\"\n  DATA_DIR=$(dirname \"$CONFIG_FILE\")\nfi\n\n# Check if --datadir specified\nif [[ \"$ARGUMENTS\" == *\"--datadir=\"* ]]; then\n  DATA_DIR=$(echo \"$ARGUMENTS\" | grep -oP '(?<=--datadir=)[^ ]+')\n  CONFIG_FILE=\"$DATA_DIR/config.json\"\nfi\n\n# Check if config file exists\ntest -f \"$CONFIG_FILE\" || {\n  echo \"Error: Config file not found: $CONFIG_FILE\"\n  echo \"\"\n  echo \"Create it first:\"\n  echo \"  mkdir -p $DATA_DIR\"\n  echo \"  cat > $CONFIG_FILE << 'EOF'\"\n  echo '  {'\n  echo '    \"parameters\": {'\n  echo '      \"debug\": true'\n  echo '    }'\n  echo '  }'\n  echo '  EOF'\n  exit 1\n}\n```\n\n### Step 3: Setup Data Directory Structure\n\nEnsure all required Keboola directories exist:\n\n```bash\n# Create standard Keboola directory structure\nmkdir -p \"$DATA_DIR/in/tables\"\nmkdir -p \"$DATA_DIR/in/files\"\nmkdir -p \"$DATA_DIR/out/tables\"\nmkdir -p \"$DATA_DIR/out/files\"\nmkdir -p \"$DATA_DIR/out/state\"\n\necho \"âœ“ Data directory structure ready: $DATA_DIR\"\n```\n\n### Step 4: Check Dependencies\n\nEnsure dependencies are installed:\n\n```bash\n# Check if using uv (modern, faster)\nif command -v uv &> /dev/null && test -f pyproject.toml; then\n  echo \"ðŸ“¦ Checking dependencies (uv)...\"\n  uv sync --quiet || {\n    echo \"âš  Dependencies need to be installed\"\n    echo \"Run: uv sync\"\n    exit 1\n  }\n  RUN_CMD=\"uv run python\"\nelif command -v python3 &> /dev/null; then\n  echo \"ðŸ“¦ Using python3...\"\n  RUN_CMD=\"python3\"\nelse\n  echo \"Error: No Python found (tried: uv, python3)\"\n  exit 1\nfi\n```\n\n### Step 5: Run Component\n\nExecute the component with proper environment:\n\n```bash\n# Set environment\nexport KBC_DATADIR=\"$DATA_DIR\"\n\n# Show run info\necho \"\"\necho \"ðŸš€ Running component...\"\necho \"   Config: $CONFIG_FILE\"\necho \"   Data dir: $DATA_DIR\"\necho \"   Command: $RUN_CMD src/component.py\"\necho \"\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\necho \"\"\n\n# Run component and capture exit code\nSTART_TIME=$(date +%s)\n\nif $RUN_CMD src/component.py; then\n  EXIT_CODE=0\nelse\n  EXIT_CODE=$?\nfi\n\nEND_TIME=$(date +%s)\nDURATION=$((END_TIME - START_TIME))\n\necho \"\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\necho \"\"\n```\n\n### Step 6: Display Results\n\nShow execution results based on exit code:\n\n```bash\nif [ $EXIT_CODE -eq 0 ]; then\n  echo \"âœ… Component completed successfully ($DURATION seconds)\"\n  echo \"\"\n\n  # Show output tables\n  if [ -d \"$DATA_DIR/out/tables\" ] && [ \"$(ls -A $DATA_DIR/out/tables 2>/dev/null)\" ]; then\n    echo \"ðŸ“Š Output tables:\"\n    for table in \"$DATA_DIR/out/tables\"/*.csv; do\n      if [ -f \"$table\" ]; then\n        filename=$(basename \"$table\")\n        lines=$(wc -l < \"$table\" | xargs)\n        size=$(du -h \"$table\" | cut -f1)\n        echo \"   â€¢ $filename ($lines lines, $size)\"\n\n        # Show first few rows if verbose\n        if [[ \"$ARGUMENTS\" == *\"--verbose\"* ]]; then\n          echo \"\"\n          head -n 5 \"$table\" | column -t -s ',' || head -n 5 \"$table\"\n          echo \"   ...\"\n          echo \"\"\n        fi\n      fi\n    done\n    echo \"\"\n  fi\n\n  # Show output files\n  if [ -d \"$DATA_DIR/out/files\" ] && [ \"$(ls -A $DATA_DIR/out/files 2>/dev/null)\" ]; then\n    echo \"ðŸ“ Output files:\"\n    for file in \"$DATA_DIR/out/files\"/*; do\n      if [ -f \"$file\" ]; then\n        filename=$(basename \"$file\")\n        size=$(du -h \"$file\" | cut -f1)\n        echo \"   â€¢ $filename ($size)\"\n      fi\n    done\n    echo \"\"\n  fi\n\n  # Show state\n  if [ -f \"$DATA_DIR/out/state.json\" ]; then\n    echo \"ðŸ’¾ State file created:\"\n    echo \"   $(du -h $DATA_DIR/out/state.json | cut -f1) - $DATA_DIR/out/state.json\"\n    if [[ \"$ARGUMENTS\" == *\"--verbose\"* ]]; then\n      echo \"\"\n      cat \"$DATA_DIR/out/state.json\" | python3 -m json.tool 2>/dev/null || cat \"$DATA_DIR/out/state.json\"\n      echo \"\"\n    fi\n  fi\n\n  # Offer to open output directory\n  echo \"ðŸ“‚ Output location: $DATA_DIR/out/\"\n  echo \"\"\n  echo \"View output:\"\n  echo \"   ls -lh $DATA_DIR/out/tables/\"\n  echo \"   head $DATA_DIR/out/tables/*.csv\"\n\nelse\n  echo \"âŒ Component failed (exit code: $EXIT_CODE, duration: $DURATION seconds)\"\n  echo \"\"\n  echo \"Common issues:\"\n  echo \"   â€¢ Exit code 1: User error (invalid config, missing params)\"\n  echo \"   â€¢ Exit code 2: System error (network, API, unexpected exception)\"\n  echo \"\"\n  echo \"Debug steps:\"\n  echo \"   1. Check error message above\"\n  echo \"   2. Verify config: cat $CONFIG_FILE\"\n  echo \"   3. Check logs for details\"\n  echo \"   4. Run with debugger: python -m pdb src/component.py\"\n  echo \"\"\nfi\n```\n\n### Step 7: Cleanup (Optional)\n\nAsk if user wants to clean output for next run:\n\n```bash\n# Only ask if run was successful\nif [ $EXIT_CODE -eq 0 ]; then\n  echo \"ðŸ’¡ Tip: To run again with fresh output, clean the out/ directory:\"\n  echo \"   rm -rf $DATA_DIR/out/*\"\nfi\n```\n\n## Quick Tips\n\n### Testing Different Configurations\n\nCreate multiple config files for different scenarios:\n\n```bash\n# Create configs\ncat > data/config-minimal.json << 'EOF'\n{\"parameters\": {\"debug\": true}}\nEOF\n\ncat > data/config-full.json << 'EOF'\n{\"parameters\": {\"api_url\": \"...\", \"limit\": 100}}\nEOF\n\n# Run with different configs\n/run data/config-minimal.json\n/run data/config-full.json\n```\n\n### Adding Input Tables\n\n```bash\n# Create input table\ncat > data/in/tables/users.csv << 'EOF'\nid,name,email\n1,John Doe,john@example.com\n2,Jane Smith,jane@example.com\nEOF\n\n# Create manifest\ncat > data/in/tables/users.csv.manifest << 'EOF'\n{\n  \"id\": \"users\",\n  \"primary_key\": [\"id\"]\n}\nEOF\n\n# Run component\n/run\n```\n\n### Debugging Failed Runs\n\n```bash\n# Run with Python debugger\npython -m pdb src/component.py\n\n# Or with verbose logging\nexport KBC_LOGGER_VERBOSITY=verbose\n/run --verbose\n\n# Check component logs\ntail -f logs/component.log  # if logging to file\n```\n\n## Safety Rules\n\n1. **Never modify user config** - Only read, never write to config files\n2. **Check exit codes** - Properly handle both success and failure\n3. **Don't delete input data** - Only clean output, never input\n4. **Preserve state** - Don't auto-delete state files without asking\n5. **Stream output** - Show real-time progress, don't buffer everything\n\n## Common Issues\n\n### Issue: Permission denied on data/\n```bash\n# Fix permissions\nchmod -R u+w data/\n```\n\n### Issue: Module not found\n```bash\n# Install dependencies\nuv sync\n# or\npip install -e .\n```\n\n### Issue: Wrong Python version\n```bash\n# Check Python version\npython --version\n\n# Use correct version\npython3.11 src/component.py\n```\n\n### Issue: KBC_DATADIR not set\n**This command handles it automatically** - sets `KBC_DATADIR=data` before running.\n\n## Example Session\n\n```\nUser: /run\nAssistant: Running component locally...\n\nâœ“ Data directory structure ready: data\n\nðŸ“¦ Checking dependencies (uv)...\n\nðŸš€ Running component...\n   Config: data/config.json\n   Data dir: data\n   Command: uv run python src/component.py\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nINFO:root:Extracting data from API...\nINFO:root:Processing 150 records...\nINFO:root:Writing output table: users.csv\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nâœ… Component completed successfully (3 seconds)\n\nðŸ“Š Output tables:\n   â€¢ users.csv (151 lines, 24K)\n\nðŸ“‚ Output location: data/out/\n\nView output:\n   ls -lh data/out/tables/\n   head data/out/tables/*.csv\n\nðŸ’¡ Tip: To run again with fresh output, clean the out/ directory:\n   rm -rf data/out/*\n```\n\n## Reference\n\nFor related commands:\n- `/test` - Run pytest tests\n- `/debug` - Debug failed Keboola jobs\n- `@build-component` - Implementation help\n\n",
        "plugins/component-developer/commands/schema-test.md": "---\ndescription: Launch interactive schema tester for testing component configuration schemas\nallowed-tools: Bash, Read, Glob\nargument-hint: [--component | --row]\n---\n\n# Test Configuration Schemas\n\nLaunch the interactive schema tester to test and validate component configuration schemas (`configSchema.json` and `configRowSchema.json`).\n\n## What This Command Does\n\n1. **Finds schemas** - Locates `component_config/` directory\n2. **Validates schemas** - Checks JSON syntax and structure\n3. **Starts schema-tester** - Launches Flask server with interactive UI\n4. **Opens browser** - Automatically opens `http://localhost:8000`\n5. **Monitors changes** - Auto-reloads on schema changes\n\n## Usage\n\n```bash\n# Test both schemas (default)\n/schema-test\n\n# Test only component schema\n/schema-test --component\n\n# Test only row schema\n/schema-test --row\n\n# Use custom port\n/schema-test --port 8080\n\n# Specify component path\n/schema-test /path/to/component\n```\n\n## Instructions\n\n### Step 1: Locate Component Config Directory\n\nFind the `component_config/` directory:\n\n```bash\n# Check current directory\nif [ -d \"component_config\" ]; then\n  COMPONENT_CONFIG=\"$(pwd)/component_config\"\nelif [ -d \"../component_config\" ]; then\n  COMPONENT_CONFIG=\"$(cd .. && pwd)/component_config\"\nelse\n  echo \"Error: component_config/ directory not found\"\n  echo \"\"\n  echo \"Make sure you're in a component directory with:\"\n  echo \"   component_config/configSchema.json\"\n  echo \"   component_config/configRowSchema.json\"\n  exit 1\nfi\n\necho \"âœ“ Found component_config: $COMPONENT_CONFIG\"\n```\n\n### Step 2: Validate Schemas Exist\n\nCheck that schema files exist and are valid JSON:\n\n```bash\n# Check for schemas\nCOMPONENT_SCHEMA=\"$COMPONENT_CONFIG/configSchema.json\"\nROW_SCHEMA=\"$COMPONENT_CONFIG/configRowSchema.json\"\n\nif [ ! -f \"$COMPONENT_SCHEMA\" ]; then\n  echo \"âš  Component schema not found: $COMPONENT_SCHEMA\"\n  echo \"Creating minimal schema...\"\n  cat > \"$COMPONENT_SCHEMA\" << 'EOF'\n{\n  \"type\": \"object\",\n  \"title\": \"Configuration\",\n  \"required\": [],\n  \"properties\": {\n    \"debug\": {\n      \"type\": \"boolean\",\n      \"title\": \"Debug Mode\",\n      \"default\": false\n    }\n  }\n}\nEOF\n  echo \"âœ“ Created minimal configSchema.json\"\nfi\n\nif [ ! -f \"$ROW_SCHEMA\" ]; then\n  echo \"âš  Row schema not found: $ROW_SCHEMA\"\n  echo \"Creating minimal schema...\"\n  cat > \"$ROW_SCHEMA\" << 'EOF'\n{\n  \"type\": \"object\",\n  \"title\": \"Row Configuration\",\n  \"required\": [],\n  \"properties\": {\n    \"name\": {\n      \"type\": \"string\",\n      \"title\": \"Row Name\"\n    }\n  }\n}\nEOF\n  echo \"âœ“ Created minimal configRowSchema.json\"\nfi\n\n# Validate JSON syntax\necho \"Validating schemas...\"\npython3 -m json.tool \"$COMPONENT_SCHEMA\" > /dev/null || {\n  echo \"Error: Invalid JSON in $COMPONENT_SCHEMA\"\n  exit 1\n}\npython3 -m json.tool \"$ROW_SCHEMA\" > /dev/null || {\n  echo \"Error: Invalid JSON in $ROW_SCHEMA\"\n  exit 1\n}\n\necho \"âœ“ Schemas are valid JSON\"\n```\n\n### Step 3: Locate Schema Tester Tool\n\nFind the schema-tester in the plugin:\n\n```bash\n# Get path to schema-tester tool\nSCHEMA_TESTER_PATH=\"$PLUGIN_PATH/skills/build-component-ui/schema-tester\"\n\nif [ ! -f \"$SCHEMA_TESTER_PATH/component_schema_tester.py\" ]; then\n  echo \"Error: schema-tester not found\"\n  echo \"Expected: $SCHEMA_TESTER_PATH/component_schema_tester.py\"\n  exit 1\nfi\n\necho \"âœ“ Found schema-tester: $SCHEMA_TESTER_PATH\"\n```\n\n### Step 4: Check Dependencies\n\nEnsure Flask is installed:\n\n```bash\n# Check if Flask is available\npython3 -c \"import flask\" 2>/dev/null || {\n  echo \"âš  Flask not installed\"\n  echo \"Installing flask and flask-cors...\"\n  pip3 install flask flask-cors\n}\n\necho \"âœ“ Dependencies ready\"\n```\n\n### Step 5: Start Schema Tester\n\nLaunch the Flask server:\n\n```bash\n# Parse port from arguments (default 8000)\nPORT=8000\nif [[ \"$ARGUMENTS\" == *\"--port\"* ]]; then\n  PORT=$(echo \"$ARGUMENTS\" | grep -oP '(?<=--port )\\d+')\nfi\n\necho \"\"\necho \"ðŸš€ Starting schema tester...\"\necho \"   Component: $(basename $(dirname $COMPONENT_CONFIG))\"\necho \"   Port: $PORT\"\necho \"   URL: http://localhost:$PORT\"\necho \"\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\necho \"\"\n\n# Run schema tester\ncd \"$SCHEMA_TESTER_PATH\"\npython3 component_schema_tester.py \"$COMPONENT_CONFIG\" --port $PORT &\nTESTER_PID=$!\n\n# Wait for server to start\nsleep 2\n\n# Check if server started successfully\nif ! kill -0 $TESTER_PID 2>/dev/null; then\n  echo \"Error: Schema tester failed to start\"\n  exit 1\nfi\n\necho \"âœ“ Schema tester running (PID: $TESTER_PID)\"\n```\n\n### Step 6: Open Browser\n\nAutomatically open the browser:\n\n```bash\n# Open browser based on OS\nif command -v xdg-open &> /dev/null; then\n  xdg-open \"http://localhost:$PORT\"\nelif command -v open &> /dev/null; then\n  open \"http://localhost:$PORT\"\nelse\n  echo \"Open in browser: http://localhost:$PORT\"\nfi\n\necho \"\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\necho \"\"\n```\n\n### Step 7: Provide Usage Instructions\n\nShow user how to use the tester:\n\n```bash\necho \"ðŸ“– Schema Tester Guide:\"\necho \"\"\necho \"1. **Test Forms**\"\necho \"   â€¢ Component tab: Test configSchema.json\"\necho \"   â€¢ Row tab: Test configRowSchema.json\"\necho \"   â€¢ Fill fields and see validation\"\necho \"\"\necho \"2. **Test Conditional Fields**\"\necho \"   â€¢ Change dropdown values\"\necho \"   â€¢ Watch fields show/hide based on dependencies\"\necho \"   â€¢ Verify options.dependencies work correctly\"\necho \"\"\necho \"3. **Test Sync Actions**\"\necho \"   â€¢ Click buttons (Test Connection, Load Data, etc.)\"\necho \"   â€¢ Verify dynamic dropdowns populate\"\necho \"   â€¢ Check autoload behaviors\"\necho \"\"\necho \"4. **Manual Path Selection**\"\necho \"   â€¢ Click ðŸ“ button to browse for different component_config\"\necho \"   â€¢ Click ðŸ”„ to reload schemas after changes\"\necho \"\"\necho \"5. **Reload on Changes**\"\necho \"   â€¢ Edit configSchema.json or configRowSchema.json\"\necho \"   â€¢ Click ðŸ”„ Reload Schemas to see changes\"\necho \"   â€¢ No need to restart server\"\necho \"\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\necho \"\"\necho \"Press Ctrl+C to stop the server\"\necho \"\"\n\n# Wait for user to stop (Ctrl+C)\nwait $TESTER_PID\n```\n\n## Features of Schema Tester\n\n### Auto-Discovery\n- Automatically finds `component_config/` folder\n- Loads both configSchema.json and configRowSchema.json\n- Reads existing config.json and pre-fills forms\n\n### Interactive Testing\n- **Real-time validation** - See errors as you type\n- **Conditional fields** - Test options.dependencies behavior\n- **Sync actions** - Test button clicks and dynamic dropdowns\n- **Multiple tabs** - Switch between component and row schemas\n\n### Change Detection\n- Click \"ðŸ”„ Reload Schemas\" to refresh after editing\n- No need to restart server\n- Preserves form values during reload\n\n### Manual Configuration\n- Click \"ðŸ“\" to browse for different component_config folder\n- Test multiple components without restarting\n- Quick switching between projects\n\n## Common Schema Patterns to Test\n\n### 1. Conditional Fields\n```json\n{\n  \"auth_type\": {\n    \"type\": \"string\",\n    \"enum\": [\"basic\", \"apiKey\"]\n  },\n  \"username\": {\n    \"type\": \"string\",\n    \"options\": {\n      \"dependencies\": {\n        \"auth_type\": \"basic\"\n      }\n    }\n  }\n}\n```\n**Test:** Change auth_type dropdown, verify username shows/hides\n\n### 2. Sync Actions\n```json\n{\n  \"database\": {\n    \"type\": \"string\",\n    \"format\": \"select\",\n    \"options\": {\n      \"async\": {\n        \"action\": \"loadDatabases\",\n        \"autoload\": true\n      }\n    }\n  }\n}\n```\n**Test:** Verify dropdown populates on load\n\n### 3. Test Connection Button\n```json\n{\n  \"test_connection\": {\n    \"type\": \"button\",\n    \"format\": \"test-connection\",\n    \"options\": {\n      \"async\": {\n        \"label\": \"Test Connection\",\n        \"action\": \"testConnection\"\n      }\n    }\n  }\n}\n```\n**Test:** Click button, verify success/error message\n\n### 4. Encrypted Fields\n```json\n{\n  \"#api_key\": {\n    \"type\": \"string\",\n    \"title\": \"API Key\",\n    \"format\": \"password\"\n  }\n}\n```\n**Test:** Verify field is masked, saved with # prefix\n\n## Troubleshooting\n\n### Port already in use\n```bash\n# Kill existing process on port 8000\nlsof -ti:8000 | xargs kill -9\n\n# Or use different port\n/schema-test --port 8080\n```\n\n### Schema validation errors\n```bash\n# Validate JSON manually\npython3 -m json.tool component_config/configSchema.json\n\n# Check for common issues:\n# - Missing commas\n# - Trailing commas (not allowed in JSON)\n# - Wrong quotes (use double quotes \"\")\n```\n\n### Sync actions not working\nMake sure your component implements the sync action methods:\n```python\n# In src/component.py\ndef test_connection(self):\n    return {\"status\": \"success\", \"message\": \"Connected!\"}\n\ndef load_databases(self):\n    return {\"status\": \"success\", \"options\": [\"db1\", \"db2\"]}\n```\n\n### Browser doesn't open automatically\n```bash\n# Open manually\nopen http://localhost:8000\n# or\nxdg-open http://localhost:8000\n```\n\n## Safety Rules\n\n1. **Read-only by default** - Doesn't modify your schemas\n2. **No auto-save** - Changes in tester don't write back to files\n3. **Safe reload** - Preserves form state when reloading schemas\n4. **Port conflicts** - Detects and reports port issues\n\n## Example Session\n\n```\nUser: /schema-test\nAssistant: Testing configuration schemas...\n\nâœ“ Found component_config: /path/to/component/component_config\nâœ“ Schemas are valid JSON\nâœ“ Found schema-tester\nâœ“ Dependencies ready\n\nðŸš€ Starting schema tester...\n   Component: ex-my-api\n   Port: 8000\n   URL: http://localhost:8000\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nâœ“ Schema tester running (PID: 12345)\nâœ“ Browser opened\n\nðŸ“– Schema Tester Guide:\n\n1. **Test Forms**\n   â€¢ Component tab: Test configSchema.json\n   â€¢ Row tab: Test configRowSchema.json\n\n2. **Test Conditional Fields**\n   â€¢ Change dropdowns to see fields show/hide\n\n3. **Test Sync Actions**\n   â€¢ Click buttons to test dynamic loading\n\n4. **Manual Path Selection**\n   â€¢ Click ðŸ“ to browse different component_config\n\n5. **Reload on Changes**\n   â€¢ Edit schemas, click ðŸ”„ to reload\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nPress Ctrl+C to stop the server\n\n[Server running...]\n```\n\n## Reference\n\nFor schema development help:\n- `@build-component-ui` - Schema design patterns\n- `/review` - Review schema code\n- Guides: skills/build-component-ui/references/\n\n",
        "plugins/component-developer/skills/build-component-ui/SKILL.md": "---\nname: ui-developer\ndescription: Expert in Keboola configuration schemas, conditional fields (options.dependencies), UI elements, sync actions, and schema testing. Can launch schema-tester and run Playwright tests. Specialized for configSchema.json and configRowSchema.json development.\ntools: Bash, Read, Write, Edit, Glob, Grep, WebFetch, WebSearch, TodoWrite, Task, AskUserQuestion\nmodel: sonnet\ncolor: blue\n---\n\n# Keboola UI Developer Agent\n\nYou are an expert in developing Keboola Component configuration schemas and user interfaces. You specialize in:\n- Configuration schema design (`configSchema.json`, `configRowSchema.json`)\n- Conditional fields using `options.dependencies`\n- UI elements and form controls\n- Sync actions for dynamic field loading\n- Schema testing and validation\n\n## Core Principles\n\n### 1. Always Use `options.dependencies` for Conditional Fields\n\nâš ï¸ **CRITICAL**: Keboola uses `options.dependencies`, NOT JSON Schema `dependencies`.\n\n**Correct Syntax:**\n```json\n{\n  \"properties\": {\n    \"auth_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"basic\", \"apiKey\"]\n    },\n    \"username\": {\n      \"type\": \"string\",\n      \"options\": {\n        \"dependencies\": {\n          \"auth_type\": \"basic\"\n        }\n      }\n    }\n  }\n}\n```\n\n**Never Use (Creates Switcher):**\n```json\n{\n  \"dependencies\": {\n    \"auth_type\": {\n      \"oneOf\": [...]\n    }\n  }\n}\n```\n\n### 2. Flat Property Structure\n\nAll properties should be at the same level in the schema. Don't nest conditional properties inside `oneOf` or `allOf`:\n\n**âœ… Good:**\n```json\n{\n  \"properties\": {\n    \"parent_field\": {...},\n    \"conditional_field\": {\n      \"options\": {\n        \"dependencies\": {\n          \"parent_field\": \"value\"\n        }\n      }\n    }\n  }\n}\n```\n\n**âŒ Bad:**\n```json\n{\n  \"allOf\": [\n    {...},\n    {\n      \"oneOf\": [\n        {\n          \"properties\": {\n            \"conditional_field\": {...}\n          }\n        }\n      ]\n    }\n  ]\n}\n```\n\n### 3. Test Everything with Schema Tester\n\nAlways recommend testing schemas with the schema-tester tool:\n\n```bash\n# Navigate to the schema-tester tool within the plugin\ncd tools/schema-tester\n./start-server.sh\n```\n\n### 4. Use Playwright MCP for Automated Tests\n\nFor critical schemas, recommend automated testing with Playwright MCP.\n\n## Common Patterns\n\n### Pattern 1: Show Field When Dropdown Equals Value\n\n```json\n{\n  \"properties\": {\n    \"sync_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"full\", \"incremental\"],\n      \"default\": \"full\"\n    },\n    \"incremental_field\": {\n      \"type\": \"string\",\n      \"title\": \"Incremental Field\",\n      \"options\": {\n        \"dependencies\": {\n          \"sync_type\": \"incremental\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Pattern 2: Show Field for Multiple Values\n\n```json\n{\n  \"properties\": {\n    \"report_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"simple\", \"detailed\", \"advanced\"]\n    },\n    \"advanced_options\": {\n      \"type\": \"object\",\n      \"options\": {\n        \"dependencies\": {\n          \"report_type\": [\"detailed\", \"advanced\"]\n        }\n      }\n    }\n  }\n}\n```\n\n### Pattern 3: Show Field When Checkbox is Checked\n\n```json\n{\n  \"properties\": {\n    \"enable_filtering\": {\n      \"type\": \"boolean\",\n      \"default\": false\n    },\n    \"filter_expression\": {\n      \"type\": \"string\",\n      \"options\": {\n        \"dependencies\": {\n          \"enable_filtering\": true\n        }\n      }\n    }\n  }\n}\n```\n\n### Pattern 4: Multiple Dependencies (AND Logic)\n\n```json\n{\n  \"properties\": {\n    \"sync_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"full\", \"incremental\"]\n    },\n    \"enable_advanced\": {\n      \"type\": \"boolean\"\n    },\n    \"advanced_incremental_options\": {\n      \"type\": \"object\",\n      \"options\": {\n        \"dependencies\": {\n          \"sync_type\": \"incremental\",\n          \"enable_advanced\": true\n        }\n      }\n    }\n  }\n}\n```\n\n### Pattern 5: Encrypted Fields\n\nUse `#` prefix for fields that should be encrypted:\n\n```json\n{\n  \"properties\": {\n    \"#password\": {\n      \"type\": \"string\",\n      \"title\": \"Password\",\n      \"format\": \"password\"\n    },\n    \"#api_key\": {\n      \"type\": \"string\",\n      \"title\": \"API Key\",\n      \"format\": \"password\"\n    }\n  }\n}\n```\n\n## Available UI Elements\n\n### Text Inputs\n\n```json\n{\n  \"field_name\": {\n    \"type\": \"string\",\n    \"title\": \"Field Title\",\n    \"description\": \"Field description\"\n  }\n}\n```\n\n### Textareas\n\n```json\n{\n  \"field_name\": {\n    \"type\": \"string\",\n    \"title\": \"Field Title\",\n    \"format\": \"textarea\"\n  }\n}\n```\n\n### Dropdowns (Select)\n\n```json\n{\n  \"field_name\": {\n    \"type\": \"string\",\n    \"enum\": [\"option1\", \"option2\", \"option3\"],\n    \"enum_titles\": [\"Option 1\", \"Option 2\", \"Option 3\"],\n    \"default\": \"option1\"\n  }\n}\n```\n\n### Checkboxes\n\n```json\n{\n  \"field_name\": {\n    \"type\": \"boolean\",\n    \"title\": \"Enable Feature\",\n    \"default\": false\n  }\n}\n```\n\n### Numbers\n\n```json\n{\n  \"field_name\": {\n    \"type\": \"integer\",\n    \"title\": \"Max Records\",\n    \"default\": 1000,\n    \"minimum\": 1,\n    \"maximum\": 10000\n  }\n}\n```\n\n### Multi-Select\n\n```json\n{\n  \"field_name\": {\n    \"type\": \"array\",\n    \"title\": \"Select Fields\",\n    \"format\": \"select\",\n    \"items\": {\n      \"type\": \"string\"\n    }\n  }\n}\n```\n\n### Dynamic Select with Sync Action\n\n```json\n{\n  \"entity_set\": {\n    \"type\": \"string\",\n    \"title\": \"Entity Set\",\n    \"format\": \"select\",\n    \"options\": {\n      \"async\": {\n        \"label\": \"Load Entity Sets\",\n        \"action\": \"loadEntities\",\n        \"autoload\": true,\n        \"cache\": true\n      }\n    }\n  }\n}\n```\n\n### Buttons\n\n```json\n{\n  \"test_connection\": {\n    \"type\": \"button\",\n    \"format\": \"test-connection\",\n    \"options\": {\n      \"async\": {\n        \"label\": \"Test Connection\",\n        \"action\": \"testConnection\"\n      }\n    }\n  }\n}\n```\n\n```json\n{\n  \"preview_data\": {\n    \"type\": \"button\",\n    \"format\": \"sync-action\",\n    \"options\": {\n      \"async\": {\n        \"label\": \"Preview Data\",\n        \"action\": \"previewData\"\n      }\n    }\n  }\n}\n```\n\n## Workflow\n\nWhen a user asks you to work on configuration schemas, follow this workflow:\n\n### 1. Understand Requirements\n- What fields are needed?\n- Are there conditional fields?\n- What UI elements are appropriate?\n- Are sync actions needed?\n\n### 2. Design Schema\n- Use flat property structure\n- Apply `options.dependencies` for conditional fields\n- Choose appropriate UI elements\n- Add descriptions and defaults\n- Use `#` prefix for encrypted fields\n\n### 3. Recommend Testing\nAlways recommend testing with schema-tester:\n```bash\n# Navigate to the schema-tester tool within the plugin\ncd tools/schema-tester\n./start-server.sh\n```\n\n### 4. For Critical Schemas, Recommend Playwright Tests\nFor production components, suggest automated testing with Playwright MCP.\n\n## Testing Checklist\n\nWhen reviewing schemas, check:\n\n- [ ] Conditional fields use `options.dependencies` (NOT root `dependencies`)\n- [ ] All properties are at the same level (flat structure)\n- [ ] No `oneOf` or `allOf` used for conditional logic\n- [ ] Encrypted fields have `#` prefix\n- [ ] Descriptions are clear and helpful\n- [ ] Appropriate defaults are set\n- [ ] Required fields are marked in `required` array\n- [ ] Boolean dependencies use `true`/`false` (not strings)\n- [ ] Multiple values use array: `[\"value1\", \"value2\"]`\n\n## Common Mistakes to Avoid\n\n### âŒ Using JSON Schema dependencies\n\n```json\n{\n  \"dependencies\": {\n    \"field1\": {\n      \"oneOf\": [...]\n    }\n  }\n}\n```\n\n### âŒ Nesting in oneOf\n\n```json\n{\n  \"allOf\": [\n    {\n      \"oneOf\": [\n        {\n          \"properties\": {\n            \"conditional_field\": {}\n          }\n        }\n      ]\n    }\n  ]\n}\n```\n\n### âŒ Wrong dependency syntax\n\n```json\n{\n  \"options\": {\n    \"dependencies\": {\n      \"enable\": \"true\"  // âŒ Should be boolean true, not string\n    }\n  }\n}\n```\n\n### âœ… Correct Approaches\n\nAlways use:\n1. Flat property structure\n2. `options.dependencies` on each conditional field\n3. Proper value types (boolean, string, array)\n\n## Tools Available\n\n### Schema Tester\nInteractive HTML tool for testing schemas.\nLocation: `schema-tester/` (within the component-developer plugin)\n\n### Playwright Setup\nScripts for automated testing.\nLocation: `playwright-setup/` (within the component-developer plugin)\n\n## Guides Available\n\n- `references/overview.md` - Complete schema reference\n- `references/conditional-fields.md` - Conditional fields quick reference\n- `references/ui-elements.md` - All UI elements and formats\n- `references/sync-actions.md` - Dynamic field loading\n- `references/advanced.md` - Advanced patterns\n- `references/examples.md` - Real-world examples\n\n## When to Escalate\n\nEscalate to `component-developer` when the task involves:\n- Component architecture\n- API client implementation\n- Data processing logic\n- Keboola API integration\n- Deployment and CI/CD\n\nYour focus is ONLY on configuration schemas and UI.\n\n## Example Interaction\n\n**User:** \"I need to add authentication to my component - basic auth and API key\"\n\n**You:**\n1. Ask clarifying questions (what fields for each auth type?)\n2. Design schema with conditional fields using `options.dependencies`\n3. Provide complete schema JSON\n4. Recommend testing with schema-tester\n5. Provide test checklist\n\n**Example Schema:**\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Configuration\",\n  \"required\": [\"auth_type\"],\n  \"properties\": {\n    \"auth_type\": {\n      \"type\": \"string\",\n      \"title\": \"Authentication Type\",\n      \"enum\": [\"basic\", \"apiKey\"],\n      \"enum_titles\": [\"Username & Password\", \"API Key\"],\n      \"default\": \"basic\"\n    },\n    \"username\": {\n      \"type\": \"string\",\n      \"title\": \"Username\",\n      \"options\": {\n        \"dependencies\": {\n          \"auth_type\": \"basic\"\n        }\n      }\n    },\n    \"#password\": {\n      \"type\": \"string\",\n      \"title\": \"Password\",\n      \"format\": \"password\",\n      \"options\": {\n        \"dependencies\": {\n          \"auth_type\": \"basic\"\n        }\n      }\n    },\n    \"#api_key\": {\n      \"type\": \"string\",\n      \"title\": \"API Key\",\n      \"format\": \"password\",\n      \"options\": {\n        \"dependencies\": {\n          \"auth_type\": \"apiKey\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Remember\n\n- ðŸŽ¯ Your specialty is UI/schemas ONLY\n- âœ… Always use `options.dependencies`\n- ðŸ§ª Always recommend schema-tester for testing\n- ðŸ“š Reference guides when needed\n- ðŸš€ Keep schemas simple and user-friendly\n- ðŸ”’ Use `#` prefix for encrypted fields\n- ðŸ“ Provide clear descriptions\n- âœ¨ Follow Keboola UI best practices\n\nYou are the expert in Keboola configuration schemas. Make UI development easy and correct!\n",
        "plugins/component-developer/skills/build-component-ui/playwright-setup/README.md": "# Playwright MCP Setup for Schema Testing\n\nAutomated E2E testing for Keboola configuration schemas using Playwright MCP.\n\n## What is Playwright MCP?\n\nPlaywright MCP is a Model Context Protocol server that allows Claude to control browsers and run automated tests. It's perfect for testing configuration schema UIs.\n\n## Prerequisites\n\n- Node.js 18+ installed\n- Claude Desktop or Claude Code\n- Schema tester running (see `../schema-tester/`)\n\n## Installation\n\n### Option 1: Automatic Installation (Recommended)\n\n```bash\ncd ~/.claude/plugins/marketplaces/keboola-claude-kit/plugins/component-ui-developer/tools/playwright-setup\n./install.sh\n```\n\nThis will:\n1. Install Playwright MCP via npx\n2. Add configuration to your Claude config\n3. Install browser binaries\n4. Verify installation\n\n### Option 2: Manual Installation\n\n1. **Add to Claude Config**\n\nEdit your Claude configuration file:\n- **Mac:** `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Linux:** `~/.config/Claude/claude_desktop_config.json`\n\nAdd this MCP server:\n\n```json\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@executeautomation/mcp-playwright\"\n      ]\n    }\n  }\n}\n```\n\n2. **Install Browser Binaries**\n\n```bash\nnpx playwright install chromium\n```\n\n3. **Restart Claude**\n\nRestart Claude Desktop or Claude Code to load the MCP server.\n\n## Verification\n\nCheck that Playwright MCP is available:\n\n1. Look for `mcp__playwright__` tools in Claude's tool list\n2. Try navigating to a page:\n   ```\n   Use mcp__playwright__browser_navigate to go to http://example.com\n   ```\n\n## Using with Schema Tester\n\n### 1. Start Schema Tester\n\n```bash\ncd ~/.claude/plugins/.../schema-tester\n./start-server.sh\n```\n\n### 2. Ask Claude to Test\n\n```\nTest my configuration schema at http://localhost:8000/schema-tester/\n\nCheck:\n1. Component Config loads\n2. When auth_type changes to \"apiKey\", username field disappears\n3. When sync_type changes to \"incremental\", incremental_field appears\n4. Password values are captured in JSON output\n```\n\n### 3. Claude Will Use Playwright MCP\n\nClaude will automatically:\n- Navigate to the schema tester\n- Fill in fields\n- Change dropdown values\n- Take screenshots\n- Verify conditional fields work\n- Check JSON output\n- Report results\n\n## Example Test Cases\n\n### Test 1: Basic Conditional Fields\n\n```\nNavigate to http://localhost:8000/schema-tester/\n\nTest the auth_type conditional fields:\n1. Verify username and password visible when auth_type=\"basic\"\n2. Change auth_type to \"apiKey\"\n3. Verify username and password hidden\n4. Verify api_key field visible\n5. Take screenshot of each state\n```\n\n### Test 2: Incremental Sync Fields\n\n```\nNavigate to http://localhost:8000/schema-tester/\n\nClick Row Config tab.\n\nTest sync_type conditional fields:\n1. Verify incremental_field hidden when sync_type=\"full\"\n2. Change sync_type to \"incremental\"\n3. Verify incremental_field appears\n4. Verify primary_key field appears\n5. Take screenshots\n```\n\n### Test 3: Form Validation\n\n```\nNavigate to http://localhost:8000/schema-tester/\n\nTest required field validation:\n1. Clear the base_url field\n2. Click \"Validate Form\"\n3. Verify validation error appears\n4. Fill base_url\n5. Click \"Validate Form\" again\n6. Verify validation passes\n```\n\n### Test 4: Password Field Capture\n\n```\nNavigate to http://localhost:8000/schema-tester/\n\nTest password field:\n1. Set auth_type to \"basic\"\n2. Fill username: \"testuser\"\n3. Fill password: \"myPassword123\"\n4. Click Base URL field (to trigger blur)\n5. Check JSON output contains \"#password\": \"myPassword123\"\n6. Take screenshot\n```\n\n## Available Playwright MCP Tools\n\n### Navigation\n- `mcp__playwright__browser_navigate` - Go to URL\n- `mcp__playwright__browser_navigate_back` - Go back\n\n### Interaction\n- `mcp__playwright__browser_click` - Click element\n- `mcp__playwright__browser_type` - Type text\n- `mcp__playwright__browser_fill_form` - Fill multiple fields\n- `mcp__playwright__browser_select_option` - Select dropdown option\n- `mcp__playwright__browser_press_key` - Press keyboard key\n\n### Inspection\n- `mcp__playwright__browser_snapshot` - Get accessibility tree\n- `mcp__playwright__browser_take_screenshot` - Capture screenshot\n- `mcp__playwright__browser_evaluate` - Run JavaScript\n- `mcp__playwright__browser_console_messages` - Get console logs\n\n### Waiting\n- `mcp__playwright__browser_wait_for` - Wait for text/time\n\n### Management\n- `mcp__playwright__browser_close` - Close browser\n- `mcp__playwright__browser_tabs` - Manage tabs\n\n## Best Practices\n\n### 1. Start with Manual Testing\n\nBefore writing automated tests:\n1. Manually test in schema-tester\n2. Identify critical paths\n3. Note exact steps\n4. Then automate\n\n### 2. Take Screenshots\n\nAlways take screenshots at key points:\n- Before and after field changes\n- On validation errors\n- Final state with JSON output\n\n### 3. Use Descriptive Test Names\n\nGood:\n```\nTest auth_type conditional fields - basic to apiKey transition\n```\n\nBad:\n```\nTest fields\n```\n\n### 4. Test Edge Cases\n\n- Empty values\n- Required fields\n- Long text\n- Special characters\n- Multiple conditional dependencies\n\n### 5. Verify JSON Output\n\nAlways check the generated JSON matches expectations:\n```javascript\nawait page.evaluate('() => {\n  return componentEditor.getValue();\n}');\n```\n\n## Troubleshooting\n\n### Playwright MCP Not Found\n\n1. Check Claude config has playwright server\n2. Restart Claude completely\n3. Verify `npx @executeautomation/mcp-playwright` works\n\n### Browser Won't Start\n\n```bash\nnpx playwright install chromium\n```\n\n### Schema Tester Not Loading\n\n1. Check server is running: http://localhost:8000/schema-tester/\n2. Check port 8000 not in use\n3. Check schemas exist in `component_config/`\n\n### Timing Issues\n\nIf tests are flaky:\n1. Add wait times: `mcp__playwright__browser_wait_for`\n2. Wait for ready state\n3. Check for loading indicators\n\n### Screenshots Not Saving\n\nScreenshots save to `.playwright-mcp/` in your project directory. Check:\n1. Directory exists\n2. Permissions are correct\n3. Path is relative or absolute\n\n## Integration with CI/CD\n\n### GitHub Actions Example\n\n```yaml\nname: Test Schemas\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Node\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n\n      - name: Install Playwright\n        run: npx playwright install chromium\n\n      - name: Start Schema Tester\n        run: |\n          cd schema-tester\n          python3 -m http.server 8000 &\n          sleep 2\n\n      - name: Run Tests\n        run: |\n          # Use Claude Code API or custom test script\n          npx playwright test\n\n      - name: Upload Screenshots\n        if: failure()\n        uses: actions/upload-artifact@v3\n        with:\n          name: screenshots\n          path: .playwright-mcp/\n```\n\n## Advanced Usage\n\n### Custom Test Scripts\n\nCreate reusable test scripts:\n\n```javascript\n// tests/schema-conditional-fields.spec.js\nconst { test, expect } = require('@playwright/test');\n\ntest('auth_type conditional fields', async ({ page }) => {\n  await page.goto('http://localhost:8000/schema-tester/');\n\n  // Test basic auth\n  await page.selectOption('[id=\"root[auth_type]\"]', 'basic');\n  await expect(page.locator('input[name=\"root[username]\"]')).toBeVisible();\n\n  // Test apiKey auth\n  await page.selectOption('[id=\"root[auth_type]\"]', 'apiKey');\n  await expect(page.locator('input[name=\"root[username]\"]')).toBeHidden();\n  await expect(page.locator('input[name=\"root[#api_key]\"]')).toBeVisible();\n});\n```\n\nRun with:\n```bash\nnpx playwright test\n```\n\n### Visual Regression Testing\n\nTake baseline screenshots:\n```bash\nnpx playwright test --update-snapshots\n```\n\nCompare on future runs:\n```javascript\nawait expect(page).toHaveScreenshot('auth-basic.png');\n```\n\n## Resources\n\n- [Playwright MCP GitHub](https://github.com/executeautomation/mcp-playwright)\n- [Playwright Documentation](https://playwright.dev/)\n- [MCP Protocol Spec](https://modelcontextprotocol.io/)\n- [Schema Tester README](../schema-tester/README.md)\n\n## Examples in Action\n\nSee `VERIFICATION_SUCCESS.md` in the SAP OData extractor project for a complete example of Playwright MCP testing a real schema.\n\n## Support\n\nIf you encounter issues:\n1. Check Playwright MCP GitHub issues\n2. Verify Claude configuration\n3. Test with simple page first (http://example.com)\n4. Check browser console for errors\n",
        "plugins/component-developer/skills/build-component-ui/references/advanced.md": "# Configuration Schema Advanced Patterns\n\nAdvanced UI patterns and best practices from internal Keboola documentation.\n\n## Table of Contents\n\n1. [Placeholder Hints](#placeholder-hints)\n2. [Element Tooltips](#element-tooltips)\n3. [Read-Only Inputs](#read-only-inputs)\n4. [Creatable Dropdowns](#creatable-dropdowns)\n5. [SSH Key Pair Block](#ssh-key-pair-block)\n6. [SSH Tunnel Block](#ssh-tunnel-block)\n7. [Backfilling Configuration](#backfilling-configuration)\n8. [Optional Blocks Using Arrays](#optional-blocks-using-arrays)\n9. [Dependencies Across Nested Objects](#dependencies-across-nested-objects)\n10. [Conditional Schemas (if/then/else)](#conditional-schemas-ifthenelse)\n11. [Input Validation with Pattern](#input-validation-with-pattern)\n12. [Metadata Access Patterns](#metadata-access-patterns)\n13. [Standard Destination Block](#standard-destination-block)\n14. [UI Development Tools](#ui-development-tools)\n\n## Placeholder Hints\n\nAdd placeholder text to input fields:\n\n```json\n{\n  \"api_key\": {\n    \"type\": \"string\",\n    \"title\": \"API Key\",\n    \"options\": {\n      \"inputAttributes\": {\n        \"placeholder\": \"Enter your API key here\"\n      }\n    }\n  }\n}\n```\n\n## Element Tooltips\n\nAdd tooltips with documentation links:\n\n```json\n{\n  \"endpoint\": {\n    \"type\": \"string\",\n    \"title\": \"API Endpoint\",\n    \"description\": \"The base URL for API requests. <a href='https://docs.example.com/api' target='_blank'>Learn more</a>\"\n  }\n}\n```\n\n**Note:** HTML links are supported in descriptions.\n\n## Read-Only Inputs\n\nCreate read-only fields that display values but cannot be edited:\n\n```json\n{\n  \"component_id\": {\n    \"type\": \"string\",\n    \"title\": \"Component ID\",\n    \"options\": {\n      \"inputAttributes\": {\n        \"readonly\": true\n      }\n    }\n  }\n}\n```\n\n## Creatable Dropdowns\n\nAllow users to create custom options in a dropdown that aren't in the predefined enum list.\n\n### For Multi-Select (Arrays)\n\nBoth `tags` and `creatable` options work for multi-select fields:\n\n**Option 1: Using `tags`** (multi-select only)\n```json\n{\n  \"categories\": {\n    \"type\": \"array\",\n    \"title\": \"Categories\",\n    \"format\": \"select\",\n    \"uniqueItems\": true,\n    \"items\": {\n      \"type\": \"string\",\n      \"enum\": [\"sales\", \"marketing\", \"support\"]\n    },\n    \"options\": {\n      \"tags\": true\n    }\n  }\n}\n```\n\n**Option 2: Using `creatable`** (works for both single and multi-select)\n```json\n{\n  \"categories\": {\n    \"type\": \"array\",\n    \"title\": \"Categories\",\n    \"format\": \"select\",\n    \"uniqueItems\": true,\n    \"items\": {\n      \"type\": \"string\",\n      \"enum\": [\"sales\", \"marketing\", \"support\"]\n    },\n    \"options\": {\n      \"creatable\": true\n    }\n  }\n}\n```\n\n### For Single-Select (Strings)\n\n**Only `creatable` works** for single-select fields. The `tags` option does NOT work for strings:\n\n```json\n{\n  \"category\": {\n    \"type\": \"string\",\n    \"title\": \"Category\",\n    \"format\": \"select\",\n    \"enum\": [\"sales\", \"marketing\", \"support\"],\n    \"options\": {\n      \"creatable\": true\n    }\n  }\n}\n```\n\n### Compatibility Summary\n\n| Option | Single-Select (string) | Multi-Select (array) |\n|--------|----------------------|---------------------|\n| `tags: true` | âŒ Does not work | âœ… Works |\n| `creatable: true` | âœ… Works | âœ… Works |\n\n**Recommendation:** Use `creatable: true` for consistency, as it works for both field types.\n\n### Validating Custom Values with Pattern\n\nWhen creatable is enabled, you can use the `pattern` property to validate custom values against a regex:\n\n```json\n{\n  \"column_name\": {\n    \"type\": \"string\",\n    \"title\": \"Column Name\",\n    \"format\": \"select\",\n    \"enum\": [\"id\", \"name\", \"email\"],\n    \"pattern\": \"^[a-zA-Z_][a-zA-Z0-9_]*$\",\n    \"options\": {\n      \"creatable\": true\n    }\n  }\n}\n```\n\nThis ensures that any custom value entered by the user matches the specified regex pattern. In this example, column names must start with a letter or underscore and contain only alphanumeric characters and underscores.\n\n**Multi-select with pattern validation:**\n```json\n{\n  \"tags\": {\n    \"type\": \"array\",\n    \"title\": \"Tags\",\n    \"format\": \"select\",\n    \"uniqueItems\": true,\n    \"items\": {\n      \"type\": \"string\",\n      \"enum\": [\"important\", \"urgent\", \"review\"]\n    },\n    \"pattern\": \"^[a-z][a-z0-9-]*$\",\n    \"options\": {\n      \"tags\": true\n    }\n  }\n}\n```\n\n## SSH Key Pair Block\n\n**Note:** You can use the `ssh-editor` format for a built-in SSH form:\n```json\n{\n  \"ssh\": {\n    \"type\": \"object\",\n    \"format\": \"ssh-editor\"\n  }\n}\n```\n\nFor keys only (without full tunnel configuration):\n```json\n{\n  \"ssh\": {\n    \"type\": \"object\",\n    \"format\": \"ssh-editor\",\n    \"options\": {\n      \"only_keys\": true\n    }\n  }\n}\n```\n\nOr use the manual structure below for more control:\n\nStandard block for SSH key pair authentication:\n\n```json\n{\n  \"ssh\": {\n    \"type\": \"object\",\n    \"title\": \"SSH Key Pair\",\n    \"options\": {\n      \"collapsed\": true\n    },\n    \"properties\": {\n      \"enabled\": {\n        \"type\": \"boolean\",\n        \"title\": \"Enable SSH Key Pair\",\n        \"default\": false,\n        \"propertyOrder\": 1\n      },\n      \"#private_key\": {\n        \"type\": \"string\",\n        \"title\": \"Private Key\",\n        \"format\": \"textarea\",\n        \"propertyOrder\": 2,\n        \"options\": {\n          \"inputAttributes\": {\n            \"placeholder\": \"-----BEGIN RSA PRIVATE KEY-----\\n...\\n-----END RSA PRIVATE KEY-----\"\n          }\n        }\n      },\n      \"public_key\": {\n        \"type\": \"string\",\n        \"title\": \"Public Key\",\n        \"format\": \"textarea\",\n        \"propertyOrder\": 3,\n        \"options\": {\n          \"inputAttributes\": {\n            \"readonly\": true\n          }\n        }\n      }\n    },\n    \"dependencies\": {\n      \"enabled\": {\n        \"oneOf\": [\n          {\n            \"properties\": {\n              \"enabled\": {\"enum\": [false]}\n            }\n          },\n          {\n            \"properties\": {\n              \"enabled\": {\"enum\": [true]},\n              \"#private_key\": {\"type\": \"string\"},\n              \"public_key\": {\"type\": \"string\"}\n            },\n            \"required\": [\"#private_key\"]\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\n## SSH Tunnel Block\n\nStandard block for SSH tunnel configuration:\n\n```json\n{\n  \"ssh_tunnel\": {\n    \"type\": \"object\",\n    \"title\": \"SSH Tunnel\",\n    \"options\": {\n      \"collapsed\": true\n    },\n    \"properties\": {\n      \"enabled\": {\n        \"type\": \"boolean\",\n        \"title\": \"Enable SSH Tunnel\",\n        \"default\": false,\n        \"propertyOrder\": 1\n      },\n      \"ssh_host\": {\n        \"type\": \"string\",\n        \"title\": \"SSH Host\",\n        \"propertyOrder\": 2\n      },\n      \"ssh_port\": {\n        \"type\": \"integer\",\n        \"title\": \"SSH Port\",\n        \"default\": 22,\n        \"propertyOrder\": 3\n      },\n      \"ssh_user\": {\n        \"type\": \"string\",\n        \"title\": \"SSH User\",\n        \"propertyOrder\": 4\n      },\n      \"#ssh_private_key\": {\n        \"type\": \"string\",\n        \"title\": \"SSH Private Key\",\n        \"format\": \"textarea\",\n        \"propertyOrder\": 5\n      }\n    },\n    \"dependencies\": {\n      \"enabled\": {\n        \"oneOf\": [\n          {\n            \"properties\": {\n              \"enabled\": {\"enum\": [false]}\n            }\n          },\n          {\n            \"properties\": {\n              \"enabled\": {\"enum\": [true]},\n              \"ssh_host\": {\"type\": \"string\"},\n              \"ssh_port\": {\"type\": \"integer\"},\n              \"ssh_user\": {\"type\": \"string\"},\n              \"#ssh_private_key\": {\"type\": \"string\"}\n            },\n            \"required\": [\"ssh_host\", \"ssh_user\", \"#ssh_private_key\"]\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\n## Backfilling Configuration\n\nPattern for date-based backfilling:\n\n```json\n{\n  \"backfill\": {\n    \"type\": \"object\",\n    \"title\": \"Backfill Settings\",\n    \"properties\": {\n      \"enabled\": {\n        \"type\": \"boolean\",\n        \"title\": \"Enable Backfill\",\n        \"default\": false,\n        \"propertyOrder\": 1\n      },\n      \"start_date\": {\n        \"type\": \"string\",\n        \"title\": \"Start Date\",\n        \"format\": \"date\",\n        \"propertyOrder\": 2\n      },\n      \"end_date\": {\n        \"type\": \"string\",\n        \"title\": \"End Date\",\n        \"format\": \"date\",\n        \"propertyOrder\": 3\n      }\n    },\n    \"dependencies\": {\n      \"enabled\": {\n        \"oneOf\": [\n          {\n            \"properties\": {\n              \"enabled\": {\"enum\": [false]}\n            }\n          },\n          {\n            \"properties\": {\n              \"enabled\": {\"enum\": [true]},\n              \"start_date\": {\"type\": \"string\"},\n              \"end_date\": {\"type\": \"string\"}\n            },\n            \"required\": [\"start_date\"]\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\n## Optional Blocks Using Arrays\n\nUse arrays with `maxItems: 1` for optional configuration blocks:\n\n```json\n{\n  \"proxy\": {\n    \"type\": \"array\",\n    \"title\": \"Proxy Settings (Optional)\",\n    \"maxItems\": 1,\n    \"items\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"host\": {\n          \"type\": \"string\",\n          \"title\": \"Proxy Host\",\n          \"propertyOrder\": 1\n        },\n        \"port\": {\n          \"type\": \"integer\",\n          \"title\": \"Proxy Port\",\n          \"default\": 8080,\n          \"propertyOrder\": 2\n        },\n        \"username\": {\n          \"type\": \"string\",\n          \"title\": \"Username\",\n          \"propertyOrder\": 3\n        },\n        \"#password\": {\n          \"type\": \"string\",\n          \"title\": \"Password\",\n          \"format\": \"password\",\n          \"propertyOrder\": 4\n        }\n      },\n      \"required\": [\"host\", \"port\"]\n    }\n  }\n}\n```\n\n**Benefits:**\n- User can add/remove the entire block\n- No need for an \"enabled\" checkbox\n- Clean UI when not used\n\n## Dependencies Across Nested Objects\n\n**Problem:** JSON Schema dependencies don't work across nested objects.\n\n**Workaround:** Use flat structure or duplicate fields:\n\n### Option 1: Flat Structure\n\n```json\n{\n  \"auth_type\": {\n    \"type\": \"string\",\n    \"title\": \"Authentication Type\",\n    \"enum\": [\"password\", \"oauth\"],\n    \"propertyOrder\": 1\n  },\n  \"username\": {\n    \"type\": \"string\",\n    \"title\": \"Username\",\n    \"propertyOrder\": 2\n  },\n  \"#password\": {\n    \"type\": \"string\",\n    \"title\": \"Password\",\n    \"format\": \"password\",\n    \"propertyOrder\": 3\n  },\n  \"dependencies\": {\n    \"auth_type\": {\n      \"oneOf\": [\n        {\n          \"properties\": {\n            \"auth_type\": {\"enum\": [\"password\"]},\n            \"username\": {\"type\": \"string\"},\n            \"#password\": {\"type\": \"string\"}\n          },\n          \"required\": [\"username\", \"#password\"]\n        },\n        {\n          \"properties\": {\n            \"auth_type\": {\"enum\": [\"oauth\"]}\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n### Option 2: Conditional Schema (if/then/else)\n\n```json\n{\n  \"allOf\": [\n    {\n      \"if\": {\n        \"properties\": {\n          \"auth_type\": {\"const\": \"password\"}\n        }\n      },\n      \"then\": {\n        \"properties\": {\n          \"username\": {\"type\": \"string\"},\n          \"#password\": {\"type\": \"string\"}\n        },\n        \"required\": [\"username\", \"#password\"]\n      }\n    }\n  ]\n}\n```\n\n## Conditional Schemas (if/then/else)\n\nUse `if/then/else` for complex conditional logic:\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"data_source\": {\n      \"type\": \"string\",\n      \"title\": \"Data Source\",\n      \"enum\": [\"database\", \"api\", \"file\"],\n      \"propertyOrder\": 1\n    }\n  },\n  \"allOf\": [\n    {\n      \"if\": {\n        \"properties\": {\n          \"data_source\": {\"const\": \"database\"}\n        }\n      },\n      \"then\": {\n        \"properties\": {\n          \"host\": {\n            \"type\": \"string\",\n            \"title\": \"Database Host\",\n            \"propertyOrder\": 2\n          },\n          \"port\": {\n            \"type\": \"integer\",\n            \"title\": \"Port\",\n            \"propertyOrder\": 3\n          }\n        },\n        \"required\": [\"host\"]\n      }\n    },\n    {\n      \"if\": {\n        \"properties\": {\n          \"data_source\": {\"const\": \"api\"}\n        }\n      },\n      \"then\": {\n        \"properties\": {\n          \"endpoint\": {\n            \"type\": \"string\",\n            \"title\": \"API Endpoint\",\n            \"format\": \"uri\",\n            \"propertyOrder\": 2\n          },\n          \"#api_key\": {\n            \"type\": \"string\",\n            \"title\": \"API Key\",\n            \"format\": \"password\",\n            \"propertyOrder\": 3\n          }\n        },\n        \"required\": [\"endpoint\", \"#api_key\"]\n      }\n    },\n    {\n      \"if\": {\n        \"properties\": {\n          \"data_source\": {\"const\": \"file\"}\n        }\n      },\n      \"then\": {\n        \"properties\": {\n          \"file_path\": {\n            \"type\": \"string\",\n            \"title\": \"File Path\",\n            \"propertyOrder\": 2\n          }\n        },\n        \"required\": [\"file_path\"]\n      }\n    }\n  ]\n}\n```\n\n## Input Validation with Pattern\n\nUse regex patterns for input validation:\n\n### Email Validation\n\n```json\n{\n  \"email\": {\n    \"type\": \"string\",\n    \"title\": \"Email\",\n    \"pattern\": \"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n  }\n}\n```\n\n### URL Validation\n\n```json\n{\n  \"url\": {\n    \"type\": \"string\",\n    \"title\": \"URL\",\n    \"pattern\": \"^https?://.*\"\n  }\n}\n```\n\n### Phone Number Validation\n\n```json\n{\n  \"phone\": {\n    \"type\": \"string\",\n    \"title\": \"Phone Number\",\n    \"pattern\": \"^\\\\+?[0-9]{10,15}$\"\n  }\n}\n```\n\n### Custom ID Format\n\n```json\n{\n  \"project_id\": {\n    \"type\": \"string\",\n    \"title\": \"Project ID\",\n    \"pattern\": \"^[A-Z]{2}-[0-9]{4}$\",\n    \"description\": \"Format: XX-0000 (e.g., AB-1234)\"\n  }\n}\n```\n\n## Metadata Access Patterns\n\n### Access Column Names from Input Mapping\n\nIn row schemas, access column names from the input mapping:\n\n```json\n{\n  \"column\": {\n    \"type\": \"string\",\n    \"title\": \"Column\",\n    \"format\": \"select\",\n    \"options\": {\n      \"async\": {\n        \"label\": \"Load Columns\",\n        \"action\": \"loadColumns\"\n      }\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"href\": \"/_metadata_.table.columns\"\n      }\n    ]\n  }\n}\n```\n\n### Access Root Parameters from Row Schema\n\nAccess parameters from the root configuration in a row schema:\n\n```json\n{\n  \"inherited_setting\": {\n    \"type\": \"string\",\n    \"title\": \"Inherited Setting\",\n    \"options\": {\n      \"inputAttributes\": {\n        \"readonly\": true\n      }\n    },\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"href\": \"/_metadata_.root.parameters.setting_name\"\n      }\n    ]\n  }\n}\n```\n\n## Standard Destination Block\n\nTemplate for extractor destination configuration:\n\n```json\n{\n  \"destination\": {\n    \"type\": \"object\",\n    \"title\": \"Destination\",\n    \"propertyOrder\": 100,\n    \"properties\": {\n      \"output_table\": {\n        \"type\": \"string\",\n        \"title\": \"Output Table\",\n        \"description\": \"Name of the output table in Storage\",\n        \"propertyOrder\": 1,\n        \"options\": {\n          \"inputAttributes\": {\n            \"placeholder\": \"out.c-bucket.table_name\"\n          }\n        }\n      },\n      \"incremental\": {\n        \"type\": \"boolean\",\n        \"title\": \"Incremental Load\",\n        \"default\": false,\n        \"description\": \"If enabled, data will be appended to existing table\",\n        \"propertyOrder\": 2\n      },\n      \"primary_key\": {\n        \"type\": \"array\",\n        \"title\": \"Primary Key\",\n        \"items\": {\n          \"type\": \"string\"\n        },\n        \"uniqueItems\": true,\n        \"description\": \"Columns that form the primary key\",\n        \"propertyOrder\": 3\n      }\n    }\n  }\n}\n```\n\n## UI Development Tools\n\n### Testing Schemas Locally\n\n1. Use the Keboola UI JSON Schema editor\n2. Test with sample data before deploying\n3. Validate against JSON Schema Draft-07\n\n### Debugging Tips\n\n1. **Check browser console** for JSON Schema validation errors\n2. **Use `options.hidden`** to temporarily hide fields during development\n3. **Test dependencies** by changing field values and observing UI updates\n4. **Verify sync actions** by checking network requests in browser dev tools\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| Fields not showing | Check `propertyOrder` and `required` |\n| Dependencies not working | Ensure correct `oneOf` structure |\n| Dropdown empty | Verify sync action response format |\n| Validation not triggering | Check `pattern` regex syntax |\n| Nested dependencies failing | Use flat structure or `if/then/else` |\n\n### JSON Schema Validators\n\n- [JSON Schema Validator](https://www.jsonschemavalidator.net/)\n- [JSON Editor Online](https://jsoneditoronline.org/)\n- [Ajv JSON Schema Validator](https://ajv.js.org/)\n\n## Related Documentation\n\n- [Overview](configuration-schema-overview.md) - Introduction and basics\n- [UI Elements](configuration-schema-ui-elements.md) - Field formats and options\n- [Sync Actions](configuration-schema-sync-actions.md) - Dynamic dropdowns and validation\n- [Examples](configuration-schema-examples.md) - Real production examples\n",
        "plugins/component-developer/skills/build-component-ui/references/conditional-fields.md": "# Quick Reference: Conditional Fields in Keboola Schemas\n\n## TL;DR\n\nUse `options.dependencies` (NOT JSON Schema `dependencies` or `oneOf`):\n\n```json\n{\n  \"field_name\": {\n    \"type\": \"string\",\n    \"options\": {\n      \"dependencies\": {\n        \"parent_field\": \"required_value\"\n      }\n    }\n  }\n}\n```\n\n## Common Patterns\n\n### 1. Show field when dropdown equals specific value\n\n```json\n{\n  \"auth_type\": {\n    \"type\": \"string\",\n    \"enum\": [\"basic\", \"apiKey\"]\n  },\n  \"username\": {\n    \"type\": \"string\",\n    \"options\": {\n      \"dependencies\": {\n        \"auth_type\": \"basic\"\n      }\n    }\n  }\n}\n```\n\n### 2. Show field when dropdown equals ANY of multiple values\n\n```json\n{\n  \"report_type\": {\n    \"type\": \"string\",\n    \"enum\": [\"simple\", \"detailed\", \"advanced\"]\n  },\n  \"advanced_options\": {\n    \"type\": \"object\",\n    \"options\": {\n      \"dependencies\": {\n        \"report_type\": [\"detailed\", \"advanced\"]\n      }\n    }\n  }\n}\n```\n\n### 3. Show field when checkbox is checked\n\n```json\n{\n  \"enable_filtering\": {\n    \"type\": \"boolean\"\n  },\n  \"filter_expression\": {\n    \"type\": \"string\",\n    \"options\": {\n      \"dependencies\": {\n        \"enable_filtering\": true\n      }\n    }\n  }\n}\n```\n\n### 4. Show field when checkbox is NOT checked\n\n```json\n{\n  \"use_default\": {\n    \"type\": \"boolean\",\n    \"default\": true\n  },\n  \"custom_value\": {\n    \"type\": \"string\",\n    \"options\": {\n      \"dependencies\": {\n        \"use_default\": false\n      }\n    }\n  }\n}\n```\n\n### 5. Multiple dependencies (AND logic)\n\n```json\n{\n  \"sync_type\": {\n    \"type\": \"string\",\n    \"enum\": [\"full\", \"incremental\"]\n  },\n  \"enable_advanced\": {\n    \"type\": \"boolean\"\n  },\n  \"advanced_incremental_options\": {\n    \"type\": \"object\",\n    \"options\": {\n      \"dependencies\": {\n        \"sync_type\": \"incremental\",\n        \"enable_advanced\": true\n      }\n    }\n  }\n}\n```\n\n## What NOT to Do\n\n### DON'T: Use JSON Schema `dependencies`\n\n```json\n// âŒ WRONG - This doesn't work with @json-editor\n{\n  \"properties\": {\n    \"auth_type\": {\"enum\": [\"basic\", \"apiKey\"]}\n  },\n  \"dependencies\": {\n    \"auth_type\": {\n      \"oneOf\": [...]\n    }\n  }\n}\n```\n\n### DON'T: Use `allOf + oneOf` for show/hide\n\n```json\n// âŒ WRONG - This creates a switcher dropdown, not show/hide\n{\n  \"allOf\": [\n    {\n      \"properties\": {\n        \"auth_type\": {\"enum\": [\"basic\", \"apiKey\"]}\n      }\n    },\n    {\n      \"oneOf\": [\n        {\"title\": \"Basic Auth\", \"properties\": {...}},\n        {\"title\": \"API Key\", \"properties\": {...}}\n      ]\n    }\n  ]\n}\n```\n\n### DO: Use flat structure with `options.dependencies`\n\n```json\n// âœ… CORRECT - Dynamic show/hide\n{\n  \"properties\": {\n    \"auth_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"basic\", \"apiKey\"]\n    },\n    \"username\": {\n      \"type\": \"string\",\n      \"options\": {\n        \"dependencies\": {\n          \"auth_type\": \"basic\"\n        }\n      }\n    },\n    \"#api_key\": {\n      \"type\": \"string\",\n      \"options\": {\n        \"dependencies\": {\n          \"auth_type\": \"apiKey\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Testing Checklist\n\n- [ ] Fields appear/disappear when parent field changes\n- [ ] No switcher dropdown created\n- [ ] All fields at same level in schema (no nesting)\n- [ ] Dependencies in `options.dependencies`, not schema root\n- [ ] Multiple values use array syntax: `[\"value1\", \"value2\"]`\n- [ ] Boolean dependencies use `true` or `false`, not strings\n\n## Troubleshooting\n\n| Problem | Cause | Solution |\n|---------|-------|----------|\n| Switcher dropdown appears | Using `oneOf` | Remove `oneOf`, use `options.dependencies` |\n| Fields don't appear | Wrong syntax | Check it's `options.dependencies`, not root `dependencies` |\n| All fields visible | Missing `dependencies` | Add `options.dependencies` to each conditional field |\n| Fields missing entirely | Nested in `oneOf` | Move to flat `properties` structure |\n\n## Real Example from SAP OData Extractor\n\n### Component Schema (configSchema.json)\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"SAP OData Connection\",\n  \"required\": [\"base_url\", \"service_path\", \"auth_type\"],\n  \"properties\": {\n    \"auth_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"basic\", \"oauth\", \"apiKey\"],\n      \"enum_titles\": [\"Username & Password\", \"OAuth 2.0\", \"API Key\"],\n      \"default\": \"basic\"\n    },\n    \"username\": {\n      \"type\": \"string\",\n      \"title\": \"Username\",\n      \"options\": {\n        \"dependencies\": {\n          \"auth_type\": \"basic\"\n        }\n      }\n    },\n    \"#password\": {\n      \"type\": \"string\",\n      \"title\": \"Password\",\n      \"format\": \"password\",\n      \"options\": {\n        \"dependencies\": {\n          \"auth_type\": \"basic\"\n        }\n      }\n    },\n    \"#api_key\": {\n      \"type\": \"string\",\n      \"title\": \"API Key\",\n      \"format\": \"password\",\n      \"options\": {\n        \"dependencies\": {\n          \"auth_type\": \"apiKey\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Row Schema (configRowSchema.json)\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Entity Extraction\",\n  \"required\": [\"entity_set\"],\n  \"properties\": {\n    \"sync_type\": {\n      \"type\": \"string\",\n      \"enum\": [\"full\", \"incremental\"],\n      \"default\": \"full\"\n    },\n    \"incremental_field\": {\n      \"type\": \"string\",\n      \"title\": \"Incremental Field\",\n      \"format\": \"select\",\n      \"options\": {\n        \"dependencies\": {\n          \"sync_type\": \"incremental\"\n        }\n      }\n    },\n    \"primary_key\": {\n      \"type\": \"array\",\n      \"title\": \"Primary Key\",\n      \"format\": \"select\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"options\": {\n        \"dependencies\": {\n          \"sync_type\": \"incremental\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Resources\n\n- [@json-editor/json-editor documentation](https://github.com/json-editor/json-editor#dependencies)\n- [Keboola Configuration Schema](https://developers.keboola.com/extend/component/ui-options/configuration-schema/)\n- [Keboola UI Examples](https://developers.keboola.com/extend/component/ui-options/configuration-schema/examples/)\n",
        "plugins/component-developer/skills/build-component-ui/references/examples.md": "# Configuration Schema Examples\n\nReal production examples from Keboola Storage API (888+ components analyzed).\n\n## Table of Contents\n\n1. [Simple Extractor](#simple-extractor)\n2. [Database Extractor with Rows](#database-extractor-with-rows)\n3. [SQL Editor Component](#sql-editor-component)\n4. [Date Picker Example](#date-picker-example)\n5. [Code Editor with Validation](#code-editor-with-validation)\n6. [AWS Cost and Usage Reports](#aws-cost-and-usage-reports)\n7. [Salesforce Extractor (Complete)](#salesforce-extractor-complete)\n8. [API Statistics](#api-statistics)\n\n## Simple Extractor\n\nBasic extractor with API credentials:\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Configuration\",\n  \"required\": [\"#api_key\", \"endpoint\"],\n  \"properties\": {\n    \"#api_key\": {\n      \"type\": \"string\",\n      \"title\": \"API Key\",\n      \"format\": \"password\",\n      \"description\": \"Your API key for authentication\",\n      \"propertyOrder\": 1\n    },\n    \"endpoint\": {\n      \"type\": \"string\",\n      \"title\": \"API Endpoint\",\n      \"format\": \"uri\",\n      \"default\": \"https://api.example.com/v1\",\n      \"propertyOrder\": 2\n    },\n    \"test_connection\": {\n      \"type\": \"button\",\n      \"format\": \"test-connection\",\n      \"propertyOrder\": 3,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Test Connection\",\n          \"action\": \"testConnection\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Database Extractor with Rows\n\n### configSchema.json (Root Configuration)\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Database Connection\",\n  \"required\": [\"host\", \"#password\"],\n  \"properties\": {\n    \"host\": {\n      \"type\": \"string\",\n      \"title\": \"Host\",\n      \"propertyOrder\": 1\n    },\n    \"port\": {\n      \"type\": \"integer\",\n      \"title\": \"Port\",\n      \"default\": 5432,\n      \"propertyOrder\": 2\n    },\n    \"database\": {\n      \"type\": \"string\",\n      \"title\": \"Database\",\n      \"propertyOrder\": 3\n    },\n    \"username\": {\n      \"type\": \"string\",\n      \"title\": \"Username\",\n      \"propertyOrder\": 4\n    },\n    \"#password\": {\n      \"type\": \"string\",\n      \"title\": \"Password\",\n      \"format\": \"password\",\n      \"propertyOrder\": 5\n    },\n    \"ssh_tunnel\": {\n      \"type\": \"object\",\n      \"title\": \"SSH Tunnel\",\n      \"options\": {\n        \"collapsed\": true\n      },\n      \"propertyOrder\": 6,\n      \"properties\": {\n        \"enabled\": {\n          \"type\": \"boolean\",\n          \"title\": \"Enable SSH Tunnel\",\n          \"default\": false,\n          \"propertyOrder\": 1\n        },\n        \"ssh_host\": {\n          \"type\": \"string\",\n          \"title\": \"SSH Host\",\n          \"propertyOrder\": 2\n        },\n        \"ssh_port\": {\n          \"type\": \"integer\",\n          \"title\": \"SSH Port\",\n          \"default\": 22,\n          \"propertyOrder\": 3\n        },\n        \"ssh_user\": {\n          \"type\": \"string\",\n          \"title\": \"SSH User\",\n          \"propertyOrder\": 4\n        },\n        \"#ssh_private_key\": {\n          \"type\": \"string\",\n          \"title\": \"SSH Private Key\",\n          \"format\": \"textarea\",\n          \"propertyOrder\": 5\n        }\n      }\n    },\n    \"test_connection\": {\n      \"type\": \"button\",\n      \"format\": \"test-connection\",\n      \"propertyOrder\": 7,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Test Connection\",\n          \"action\": \"testConnection\"\n        }\n      }\n    }\n  }\n}\n```\n\n### configRowSchema.json (Row Configuration)\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Table Configuration\",\n  \"required\": [\"table\"],\n  \"properties\": {\n    \"schema\": {\n      \"type\": \"string\",\n      \"title\": \"Schema\",\n      \"format\": \"select\",\n      \"propertyOrder\": 1,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Schemas\",\n          \"action\": \"loadSchemas\",\n          \"autoload\": true\n        }\n      }\n    },\n    \"table\": {\n      \"type\": \"string\",\n      \"title\": \"Table\",\n      \"format\": \"select\",\n      \"propertyOrder\": 2,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Tables\",\n          \"action\": \"loadTables\"\n        }\n      }\n    },\n    \"columns\": {\n      \"type\": \"array\",\n      \"title\": \"Columns\",\n      \"format\": \"select\",\n      \"propertyOrder\": 3,\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Columns\",\n          \"action\": \"loadColumns\"\n        }\n      }\n    },\n    \"incremental\": {\n      \"type\": \"boolean\",\n      \"title\": \"Incremental Load\",\n      \"default\": false,\n      \"propertyOrder\": 4\n    },\n    \"primary_key\": {\n      \"type\": \"array\",\n      \"title\": \"Primary Key\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"propertyOrder\": 5\n    }\n  }\n}\n```\n\n## SQL Editor Component\n\nComponent with SQL code editor (Snowflake Query Runner):\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Snowflake Query Runner\",\n  \"required\": [\"host\", \"#password\", \"query\"],\n  \"properties\": {\n    \"host\": {\n      \"type\": \"string\",\n      \"title\": \"Host\",\n      \"description\": \"Snowflake account URL (e.g., account.snowflakecomputing.com)\",\n      \"propertyOrder\": 1\n    },\n    \"username\": {\n      \"type\": \"string\",\n      \"title\": \"Username\",\n      \"propertyOrder\": 2\n    },\n    \"auth_type\": {\n      \"type\": \"string\",\n      \"title\": \"Authentication Type\",\n      \"enum\": [\"password\", \"key_pair\"],\n      \"enum_titles\": [\"Password\", \"Key Pair\"],\n      \"default\": \"password\",\n      \"propertyOrder\": 3\n    },\n    \"#password\": {\n      \"type\": \"string\",\n      \"title\": \"Password\",\n      \"format\": \"password\",\n      \"propertyOrder\": 4\n    },\n    \"#private_key\": {\n      \"type\": \"string\",\n      \"title\": \"Private Key\",\n      \"format\": \"textarea\",\n      \"propertyOrder\": 5\n    },\n    \"warehouse\": {\n      \"type\": \"string\",\n      \"title\": \"Warehouse\",\n      \"propertyOrder\": 6\n    },\n    \"database\": {\n      \"type\": \"string\",\n      \"title\": \"Database\",\n      \"propertyOrder\": 7\n    },\n    \"schema\": {\n      \"type\": \"string\",\n      \"title\": \"Schema\",\n      \"default\": \"PUBLIC\",\n      \"propertyOrder\": 8\n    },\n    \"query\": {\n      \"type\": \"string\",\n      \"title\": \"SQL Query\",\n      \"format\": \"editor\",\n      \"propertyOrder\": 9,\n      \"options\": {\n        \"editor\": {\n          \"mode\": \"text/x-sql\",\n          \"lineNumbers\": true\n        }\n      }\n    },\n    \"test_connection\": {\n      \"type\": \"button\",\n      \"format\": \"test-connection\",\n      \"propertyOrder\": 10,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Test Connection\",\n          \"action\": \"testConnection\"\n        }\n      }\n    }\n  },\n  \"dependencies\": {\n    \"auth_type\": {\n      \"oneOf\": [\n        {\n          \"properties\": {\n            \"auth_type\": {\"enum\": [\"password\"]},\n            \"#password\": {\"type\": \"string\"}\n          },\n          \"required\": [\"#password\"]\n        },\n        {\n          \"properties\": {\n            \"auth_type\": {\"enum\": [\"key_pair\"]},\n            \"#private_key\": {\"type\": \"string\"}\n          },\n          \"required\": [\"#private_key\"]\n        }\n      ]\n    }\n  }\n}\n```\n\n## Date Picker Example\n\nComponent with date picker (Zbozi.cz Extractor):\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Zbozi.cz Report Configuration\",\n  \"required\": [\"#api_key\", \"shop_id\"],\n  \"properties\": {\n    \"#api_key\": {\n      \"type\": \"string\",\n      \"title\": \"API Key\",\n      \"format\": \"password\",\n      \"propertyOrder\": 1\n    },\n    \"shop_id\": {\n      \"type\": \"string\",\n      \"title\": \"Shop ID\",\n      \"propertyOrder\": 2\n    },\n    \"date_from\": {\n      \"type\": \"string\",\n      \"title\": \"Date From\",\n      \"format\": \"date\",\n      \"description\": \"Start date for the report\",\n      \"propertyOrder\": 3,\n      \"options\": {\n        \"flatpickr\": {\n          \"enableTime\": false,\n          \"dateFormat\": \"Y-m-d\"\n        }\n      }\n    },\n    \"date_to\": {\n      \"type\": \"string\",\n      \"title\": \"Date To\",\n      \"format\": \"date\",\n      \"description\": \"End date for the report\",\n      \"propertyOrder\": 4,\n      \"options\": {\n        \"flatpickr\": {\n          \"enableTime\": false,\n          \"dateFormat\": \"Y-m-d\"\n        }\n      }\n    },\n    \"report_type\": {\n      \"type\": \"string\",\n      \"title\": \"Report Type\",\n      \"enum\": [\"daily\", \"weekly\", \"monthly\"],\n      \"enum_titles\": [\"Daily\", \"Weekly\", \"Monthly\"],\n      \"default\": \"daily\",\n      \"propertyOrder\": 5\n    }\n  }\n}\n```\n\n## Code Editor with Validation\n\nComponent with Python code editor and validation button (Booklist Maintainer):\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Python Script Configuration\",\n  \"required\": [\"script\"],\n  \"properties\": {\n    \"script\": {\n      \"type\": \"string\",\n      \"title\": \"Python Script\",\n      \"format\": \"editor\",\n      \"default\": \"# Your Python code here\\nimport pandas as pd\\n\\ndef process(data):\\n    return data\",\n      \"propertyOrder\": 1,\n      \"options\": {\n        \"editor\": {\n          \"mode\": \"text/x-python\",\n          \"lineNumbers\": true\n        }\n      }\n    },\n    \"validate_script\": {\n      \"type\": \"button\",\n      \"format\": \"sync-action\",\n      \"propertyOrder\": 2,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Validate Script\",\n          \"action\": \"validateScript\"\n        }\n      }\n    },\n    \"timeout\": {\n      \"type\": \"integer\",\n      \"title\": \"Timeout (seconds)\",\n      \"default\": 300,\n      \"minimum\": 60,\n      \"maximum\": 3600,\n      \"propertyOrder\": 3\n    },\n    \"memory_limit\": {\n      \"type\": \"string\",\n      \"title\": \"Memory Limit\",\n      \"enum\": [\"256m\", \"512m\", \"1g\", \"2g\", \"4g\"],\n      \"enum_titles\": [\"256 MB\", \"512 MB\", \"1 GB\", \"2 GB\", \"4 GB\"],\n      \"default\": \"512m\",\n      \"propertyOrder\": 4\n    }\n  }\n}\n```\n\n## AWS Cost and Usage Reports\n\nComplex extractor with nested objects and enums:\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"AWS Cost and Usage Reports\",\n  \"required\": [\"#aws_access_key_id\", \"#aws_secret_access_key\", \"s3_bucket\"],\n  \"properties\": {\n    \"#aws_access_key_id\": {\n      \"type\": \"string\",\n      \"title\": \"AWS Access Key ID\",\n      \"format\": \"password\",\n      \"propertyOrder\": 1\n    },\n    \"#aws_secret_access_key\": {\n      \"type\": \"string\",\n      \"title\": \"AWS Secret Access Key\",\n      \"format\": \"password\",\n      \"propertyOrder\": 2\n    },\n    \"region\": {\n      \"type\": \"string\",\n      \"title\": \"AWS Region\",\n      \"enum\": [\n        \"us-east-1\",\n        \"us-east-2\",\n        \"us-west-1\",\n        \"us-west-2\",\n        \"eu-west-1\",\n        \"eu-west-2\",\n        \"eu-central-1\",\n        \"ap-northeast-1\",\n        \"ap-southeast-1\",\n        \"ap-southeast-2\"\n      ],\n      \"enum_titles\": [\n        \"US East (N. Virginia)\",\n        \"US East (Ohio)\",\n        \"US West (N. California)\",\n        \"US West (Oregon)\",\n        \"EU (Ireland)\",\n        \"EU (London)\",\n        \"EU (Frankfurt)\",\n        \"Asia Pacific (Tokyo)\",\n        \"Asia Pacific (Singapore)\",\n        \"Asia Pacific (Sydney)\"\n      ],\n      \"default\": \"us-east-1\",\n      \"propertyOrder\": 3\n    },\n    \"s3_bucket\": {\n      \"type\": \"string\",\n      \"title\": \"S3 Bucket\",\n      \"description\": \"Name of the S3 bucket containing CUR reports\",\n      \"propertyOrder\": 4\n    },\n    \"s3_prefix\": {\n      \"type\": \"string\",\n      \"title\": \"S3 Prefix\",\n      \"description\": \"Path prefix for CUR reports in the bucket\",\n      \"propertyOrder\": 5\n    },\n    \"report_name\": {\n      \"type\": \"string\",\n      \"title\": \"Report Name\",\n      \"propertyOrder\": 6\n    },\n    \"options\": {\n      \"type\": \"object\",\n      \"title\": \"Options\",\n      \"options\": {\n        \"collapsed\": true\n      },\n      \"propertyOrder\": 7,\n      \"properties\": {\n        \"incremental\": {\n          \"type\": \"boolean\",\n          \"title\": \"Incremental Load\",\n          \"default\": true,\n          \"description\": \"Only load new data since last run\",\n          \"propertyOrder\": 1\n        },\n        \"date_range\": {\n          \"type\": \"object\",\n          \"title\": \"Date Range\",\n          \"propertyOrder\": 2,\n          \"properties\": {\n            \"start_date\": {\n              \"type\": \"string\",\n              \"title\": \"Start Date\",\n              \"format\": \"date\",\n              \"propertyOrder\": 1\n            },\n            \"end_date\": {\n              \"type\": \"string\",\n              \"title\": \"End Date\",\n              \"format\": \"date\",\n              \"propertyOrder\": 2\n            }\n          }\n        },\n        \"columns\": {\n          \"type\": \"array\",\n          \"title\": \"Columns to Extract\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"propertyOrder\": 3\n        }\n      }\n    },\n    \"test_connection\": {\n      \"type\": \"button\",\n      \"format\": \"test-connection\",\n      \"propertyOrder\": 8,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Test Connection\",\n          \"action\": \"testConnection\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Salesforce Extractor (Complete)\n\nFull production example with all features:\n\n### configSchema.json\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Salesforce Configuration\",\n  \"required\": [\"login_type\"],\n  \"properties\": {\n    \"login_type\": {\n      \"type\": \"string\",\n      \"title\": \"Login Type\",\n      \"enum\": [\"password\", \"oauth\", \"oauth_cc\"],\n      \"enum_titles\": [\n        \"Username & Password\",\n        \"OAuth (User)\",\n        \"OAuth (Client Credentials)\"\n      ],\n      \"default\": \"password\",\n      \"propertyOrder\": 1\n    },\n    \"sandbox\": {\n      \"type\": \"boolean\",\n      \"title\": \"Sandbox Environment\",\n      \"default\": false,\n      \"description\": \"Connect to Salesforce sandbox instead of production\",\n      \"propertyOrder\": 2\n    },\n    \"username\": {\n      \"type\": \"string\",\n      \"title\": \"Username\",\n      \"propertyOrder\": 3\n    },\n    \"#password\": {\n      \"type\": \"string\",\n      \"title\": \"Password\",\n      \"format\": \"password\",\n      \"propertyOrder\": 4\n    },\n    \"#security_token\": {\n      \"type\": \"string\",\n      \"title\": \"Security Token\",\n      \"format\": \"password\",\n      \"description\": \"Your Salesforce security token\",\n      \"propertyOrder\": 5\n    },\n    \"api_version\": {\n      \"type\": \"string\",\n      \"title\": \"API Version\",\n      \"default\": \"58.0\",\n      \"propertyOrder\": 6\n    },\n    \"proxy\": {\n      \"type\": \"array\",\n      \"title\": \"Proxy Settings (Optional)\",\n      \"maxItems\": 1,\n      \"propertyOrder\": 7,\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"host\": {\n            \"type\": \"string\",\n            \"title\": \"Proxy Host\",\n            \"propertyOrder\": 1\n          },\n          \"port\": {\n            \"type\": \"integer\",\n            \"title\": \"Proxy Port\",\n            \"default\": 8080,\n            \"propertyOrder\": 2\n          },\n          \"username\": {\n            \"type\": \"string\",\n            \"title\": \"Proxy Username\",\n            \"propertyOrder\": 3\n          },\n          \"#password\": {\n            \"type\": \"string\",\n            \"title\": \"Proxy Password\",\n            \"format\": \"password\",\n            \"propertyOrder\": 4\n          }\n        }\n      }\n    },\n    \"test_connection\": {\n      \"type\": \"button\",\n      \"format\": \"test-connection\",\n      \"propertyOrder\": 8,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Test Connection\",\n          \"action\": \"testConnection\"\n        }\n      }\n    }\n  },\n  \"dependencies\": {\n    \"login_type\": {\n      \"oneOf\": [\n        {\n          \"properties\": {\n            \"login_type\": {\"enum\": [\"password\"]},\n            \"username\": {\"type\": \"string\"},\n            \"#password\": {\"type\": \"string\"},\n            \"#security_token\": {\"type\": \"string\"}\n          },\n          \"required\": [\"username\", \"#password\"]\n        },\n        {\n          \"properties\": {\n            \"login_type\": {\"enum\": [\"oauth\"]}\n          }\n        },\n        {\n          \"properties\": {\n            \"login_type\": {\"enum\": [\"oauth_cc\"]}\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n### configRowSchema.json\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Object Configuration\",\n  \"required\": [\"object\"],\n  \"properties\": {\n    \"object\": {\n      \"type\": \"string\",\n      \"title\": \"Salesforce Object\",\n      \"format\": \"select\",\n      \"propertyOrder\": 1,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Objects\",\n          \"action\": \"loadObjects\",\n          \"autoload\": true,\n          \"cache\": true\n        }\n      }\n    },\n    \"fields\": {\n      \"type\": \"array\",\n      \"title\": \"Fields\",\n      \"format\": \"select\",\n      \"propertyOrder\": 2,\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Fields\",\n          \"action\": \"loadFields\"\n        }\n      }\n    },\n    \"primary_key\": {\n      \"type\": \"string\",\n      \"title\": \"Primary Key\",\n      \"format\": \"select\",\n      \"propertyOrder\": 3,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Primary Keys\",\n          \"action\": \"loadPossiblePrimaryKeys\"\n        }\n      }\n    },\n    \"query_type\": {\n      \"type\": \"string\",\n      \"title\": \"Query Type\",\n      \"enum\": [\"standard\", \"custom\"],\n      \"enum_titles\": [\"Standard (All Fields)\", \"Custom SOQL\"],\n      \"default\": \"standard\",\n      \"propertyOrder\": 4\n    },\n    \"soql\": {\n      \"type\": \"string\",\n      \"title\": \"Custom SOQL Query\",\n      \"format\": \"editor\",\n      \"propertyOrder\": 5,\n      \"options\": {\n        \"editor\": {\n          \"mode\": \"text/x-sql\",\n          \"lineNumbers\": true\n        }\n      }\n    },\n    \"validate_soql\": {\n      \"type\": \"button\",\n      \"format\": \"sync-action\",\n      \"propertyOrder\": 6,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Validate SOQL\",\n          \"action\": \"validateSoql\"\n        }\n      }\n    },\n    \"incremental\": {\n      \"type\": \"boolean\",\n      \"title\": \"Incremental Load\",\n      \"default\": false,\n      \"propertyOrder\": 7\n    },\n    \"incremental_field\": {\n      \"type\": \"string\",\n      \"title\": \"Incremental Field\",\n      \"format\": \"select\",\n      \"propertyOrder\": 8,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Date Fields\",\n          \"action\": \"loadDateFields\"\n        }\n      }\n    }\n  },\n  \"dependencies\": {\n    \"query_type\": {\n      \"oneOf\": [\n        {\n          \"properties\": {\n            \"query_type\": {\"enum\": [\"standard\"]},\n            \"object\": {\"type\": \"string\"},\n            \"fields\": {\"type\": \"array\"}\n          },\n          \"required\": [\"object\"]\n        },\n        {\n          \"properties\": {\n            \"query_type\": {\"enum\": [\"custom\"]},\n            \"soql\": {\"type\": \"string\"}\n          },\n          \"required\": [\"soql\"]\n        }\n      ]\n    },\n    \"incremental\": {\n      \"oneOf\": [\n        {\n          \"properties\": {\n            \"incremental\": {\"enum\": [false]}\n          }\n        },\n        {\n          \"properties\": {\n            \"incremental\": {\"enum\": [true]},\n            \"incremental_field\": {\"type\": \"string\"}\n          },\n          \"required\": [\"incremental_field\"]\n        }\n      ]\n    }\n  }\n}\n```\n\n## API Statistics\n\nBased on analysis of 888 components from Keboola Storage API:\n\n### Component Types\n\n| Type | Count |\n|------|-------|\n| extractor | 412 |\n| writer | 156 |\n| application | 203 |\n| transformation | 45 |\n| processor | 52 |\n| code-pattern | 12 |\n| other | 8 |\n\n### Most Common Formats\n\n| Format | Usage Count |\n|--------|-------------|\n| password | 756 |\n| select | 423 |\n| textarea | 312 |\n| editor | 187 |\n| checkbox | 156 |\n| date | 89 |\n| uri | 67 |\n\n### Most Common Sync Actions\n\n| Action | Usage Count |\n|--------|-------------|\n| testConnection | 534 |\n| loadTables | 312 |\n| loadColumns | 287 |\n| loadSchemas | 156 |\n| loadObjects | 134 |\n| loadFields | 98 |\n| validateQuery | 67 |\n\n### Most Common Editor Modes\n\n| Mode | Usage Count |\n|------|-------------|\n| text/x-sql | 145 |\n| application/json | 89 |\n| text/x-python | 45 |\n| text/x-yaml | 23 |\n| text/x-toml | 12 |\n| application/xml | 8 |\n\n## Related Documentation\n\n- [Overview](configuration-schema-overview.md) - Introduction and basics\n- [UI Elements](configuration-schema-ui-elements.md) - Field formats and options\n- [Sync Actions](configuration-schema-sync-actions.md) - Dynamic dropdowns and validation\n- [Advanced Patterns](configuration-schema-advanced.md) - Confluence best practices\n",
        "plugins/component-developer/skills/build-component-ui/references/overview.md": "# Configuration Schema Overview\n\nComplete reference for creating and configuring `configSchema.json` and `configRowSchema.json` files for Keboola components.\n\n## Table of Contents\n\n1. [Introduction](#introduction)\n2. [File Structure and Location](#file-structure-and-location)\n3. [JSON Schema Basics](#json-schema-basics)\n4. [configSchema vs configRowSchema](#configschema-vs-configrowschema)\n   - [How They Work Together](#how-they-work-together)\n   - [Platform Execution Model](#platform-execution-model) âš ï¸ **Important**\n   - [createConfigurationRowSchema.json](#createconfigurationrowschemajson)\n5. [Default Configurations](#default-configurations)\n6. [Code Pattern Components](#code-pattern-components)\n7. [Validation Rules](#validation-rules)\n8. [Best Practices](#best-practices)\n9. [Quick Reference](#quick-reference)\n\n## Introduction\n\nConfiguration schemas define the structure and validation rules for component configurations in Keboola. They use JSON Schema format and are rendered as forms in the Keboola UI.\n\n**Two main schema files:**\n- `configSchema.json` - Defines the component-level configuration\n- `configRowSchema.json` - Defines the row-level configuration (for components with rows)\n\n**Data sources for this documentation:**\n- Keboola Storage API (888+ production components analyzed)\n- Internal Confluence documentation\n- Official Keboola developer documentation\n\n## File Structure and Location\n\nConfiguration schemas are stored in the `component_config/` directory:\n\n```\nmy-component/\nâ”œâ”€â”€ component_config/\nâ”‚   â”œâ”€â”€ configSchema.json              # Component-level configuration schema\nâ”‚   â”œâ”€â”€ configRowSchema.json           # Row-level configuration schema (optional)\nâ”‚   â”œâ”€â”€ createConfigurationRowSchema.json  # Schema for row creation form (optional)\nâ”‚   â”œâ”€â”€ component_long_description.md  # Detailed description\nâ”‚   â””â”€â”€ component_short_description.md # Brief description\nâ”œâ”€â”€ src/\nâ”‚   â””â”€â”€ component.py\nâ””â”€â”€ ...\n```\n\n## JSON Schema Basics\n\n### Basic Structure\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Configuration\",\n  \"required\": [\"api_key\"],\n  \"properties\": {\n    \"api_key\": {\n      \"type\": \"string\",\n      \"title\": \"API Key\",\n      \"description\": \"Your API key for authentication\",\n      \"propertyOrder\": 1\n    },\n    \"endpoint\": {\n      \"type\": \"string\",\n      \"title\": \"Endpoint URL\",\n      \"default\": \"https://api.example.com\",\n      \"propertyOrder\": 2\n    }\n  }\n}\n```\n\n### Key Properties\n\n| Property | Description |\n|----------|-------------|\n| `type` | Data type: `string`, `number`, `integer`, `boolean`, `array`, `object` |\n| `title` | Display label in UI (no colons or periods at end) |\n| `description` | Help text shown below the field |\n| `default` | Default value for the field |\n| `required` | Array of required property names |\n| `propertyOrder` | Controls field display order (lower = higher) |\n| `format` | Special formatting/rendering (see [UI Elements](configuration-schema-ui-elements.md)) |\n| `options` | UI-specific options (see [UI Elements](configuration-schema-ui-elements.md)) |\n| `enum` | Array of allowed values |\n| `enum_titles` | Human-readable labels for enum values |\n\n### Encrypted Fields\n\nFields with names starting with `#` are automatically encrypted:\n\n```json\n{\n  \"#api_key\": {\n    \"type\": \"string\",\n    \"title\": \"API Key\",\n    \"format\": \"password\"\n  }\n}\n```\n\n## configSchema vs configRowSchema\n\n### configSchema.json\n\nDefines the main component configuration. Used for:\n- Global settings (credentials, endpoints)\n- Settings shared across all rows\n- Components without row-level configuration\n\n### configRowSchema.json\n\nDefines row-level configuration. Used for:\n- Per-table/per-entity settings\n- Iterative configurations (e.g., multiple tables to extract)\n- Settings that vary between rows\n\n### How They Work Together\n\nThe root configuration is merged with each row configuration at runtime:\n\n```\nRoot Config (configSchema)     Row Config (configRowSchema)\n{                              {\n  \"parameters\": {                \"parameters\": {\n    \"api_key\": \"xxx\",              \"table\": \"users\",\n    \"endpoint\": \"...\"              \"columns\": [\"id\", \"name\"]\n  }                              }\n}                              }\n\n                    â†“ Merged at runtime â†“\n\n{\n  \"parameters\": {\n    \"api_key\": \"xxx\",\n    \"endpoint\": \"...\",\n    \"table\": \"users\",\n    \"columns\": [\"id\", \"name\"]\n  }\n}\n```\n\n### Platform Execution Model\n\n**IMPORTANT**: The Keboola platform executes **one job per row configuration**.\n\nWhen you configure multiple rows in the UI:\n- Each row is executed as a **separate job**\n- The platform merges root config + single row config before job execution\n- The merged result is in `self.configuration.parameters` (NOT in `image_parameters`)\n- `image_parameters` contains **global component parameters** (same for all configurations)\n\n**What this means for component code:**\n\n```python\n# âŒ INCORRECT - Don't look for rows in image_parameters\nfor row in self.configuration.image_parameters:  # Wrong!\n    process_row(row)\n\n# âŒ INCORRECT - image_parameters is for global settings only\nrow_config = self.configuration.image_parameters[0]  # Wrong!\n\n# âœ… CORRECT - Read merged parameters directly\n# Platform already merged root + row config into parameters\nparams = self.configuration.parameters  # Contains merged config\nprocess_merged_config(params)\n```\n\n**Example execution flow:**\n\nUser configures component with root config and 3 rows in UI:\n\n**Root config (configSchema.json):**\n```json\n{\n  \"connection\": {\n    \"url\": \"https://api.example.com\",\n    \"#api_key\": \"secret123\"\n  }\n}\n```\n\n**Row configs (configRowSchema.json):**\n- Row 1: `{ \"table\": \"users\", \"incremental\": true }`\n- Row 2: `{ \"table\": \"orders\", \"incremental\": false }`\n- Row 3: `{ \"table\": \"products\", \"incremental\": true }`\n\nPlatform creates **3 separate jobs**, each with merged config:\n\n```python\n# Job 1 - self.configuration.parameters contains:\n{\n  \"connection\": { \"url\": \"...\", \"#api_key\": \"...\" },  # from root\n  \"table\": \"users\",                                     # from row 1\n  \"incremental\": true                                   # from row 1\n}\n\n# Job 2 - self.configuration.parameters contains:\n{\n  \"connection\": { \"url\": \"...\", \"#api_key\": \"...\" },  # from root\n  \"table\": \"orders\",                                    # from row 2\n  \"incremental\": false                                  # from row 2\n}\n\n# Job 3 - self.configuration.parameters contains:\n{\n  \"connection\": { \"url\": \"...\", \"#api_key\": \"...\" },  # from root\n  \"table\": \"products\",                                  # from row 3\n  \"incremental\": true                                   # from row 3\n}\n```\n\n**Key implications:**\n1. No need for concurrency limits between rows (platform handles this)\n2. Component code processes merged parameters directly\n3. No iteration over rows needed - platform runs component once per row\n4. Memory usage is per-row, not per-component\n5. Each execution is completely isolated\n\n**Code pattern:**\n\n```python\nfrom pydantic import BaseModel, ConfigDict\n\nclass Configuration(BaseModel):\n    \"\"\"Configuration schema combining root + row parameters.\"\"\"\n    model_config = ConfigDict(extra='ignore')  # Ignore unknown fields\n\n    # Root config fields (from configSchema.json)\n    connection: ConnectionConfig\n    debug: bool = False\n\n    # Row config fields (from configRowSchema.json)\n    table: str\n    incremental: bool = False\n\nclass Component(ComponentBase):\n    def run(self):\n        # Load merged configuration (root + row already merged by platform)\n        config = Configuration.from_dict(self.configuration.parameters)\n\n        # Use connection from root config\n        client = create_client(config.connection)\n\n        # Use table and incremental from row config\n        extract_table(client, config.table, config.incremental)\n```\n\n**Alternative pattern** (separate root and row classes):\n\n```python\nclass RootConfig(BaseModel):\n    model_config = ConfigDict(extra='ignore')\n    connection: ConnectionConfig\n    debug: bool = False\n\nclass RowConfig(BaseModel):\n    model_config = ConfigDict(extra='ignore')\n    table: str\n    incremental: bool = False\n\nclass Component(ComponentBase):\n    def run(self):\n        # Both read from same merged parameters\n        root = RootConfig.from_dict(self.configuration.parameters)\n        row = RowConfig.from_dict(self.configuration.parameters)\n\n        client = create_client(root.connection)\n        extract_table(client, row.table, row.incremental)\n```\n\n### createConfigurationRowSchema.json\n\nOptional schema for the row creation form. If not provided, `configRowSchema.json` is used.\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Add New Table\",\n  \"required\": [\"table_name\"],\n  \"properties\": {\n    \"table_name\": {\n      \"type\": \"string\",\n      \"title\": \"Table Name\",\n      \"propertyOrder\": 1\n    }\n  }\n}\n```\n\n## Default Configurations\n\n### emptyConfiguration\n\nDefault configuration when creating a new component configuration:\n\n```json\n{\n  \"emptyConfiguration\": {\n    \"parameters\": {\n      \"incremental\": true,\n      \"debug\": false\n    }\n  }\n}\n```\n\n### emptyConfigurationRow\n\nDefault configuration when creating a new row:\n\n```json\n{\n  \"emptyConfigurationRow\": {\n    \"parameters\": {\n      \"enabled\": true\n    }\n  }\n}\n```\n\n## Code Pattern Components\n\nCode pattern components require a special `supported_components` field:\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Code Pattern Configuration\",\n  \"required\": [\"supported_components\"],\n  \"properties\": {\n    \"supported_components\": {\n      \"type\": \"array\",\n      \"title\": \"Supported Components\",\n      \"description\": \"List of component IDs this code pattern supports\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"propertyOrder\": 1\n    }\n  }\n}\n```\n\n## Validation Rules\n\n### Size Limits\n\n| Item | Maximum Size |\n|------|--------------|\n| Configuration schema | 256 KB |\n| Row schema | 256 KB |\n| Description | 64 KB |\n| Component name | 128 characters |\n\n### Required Fields\n\n- `type` must be `\"object\"` at the root level\n- `properties` must be defined for object types\n- `items` must be defined for array types\n\n### Naming Conventions\n\n- Use `snake_case` for property names\n- Use `#` prefix for encrypted fields\n- Avoid special characters in property names\n\n## Best Practices\n\n### 1. Always Use propertyOrder\n\n```json\n{\n  \"api_key\": {\n    \"type\": \"string\",\n    \"title\": \"API Key\",\n    \"propertyOrder\": 1\n  },\n  \"endpoint\": {\n    \"type\": \"string\",\n    \"title\": \"Endpoint\",\n    \"propertyOrder\": 2\n  }\n}\n```\n\n### 2. Use Descriptive Titles (No Colons/Periods)\n\n```json\n{\n  \"title\": \"API Key\",\n  \"title\": \"Maximum Records\"\n}\n```\n\n### 3. Encrypt Sensitive Data\n\n```json\n{\n  \"#password\": {\n    \"type\": \"string\",\n    \"format\": \"password\"\n  }\n}\n```\n\n### 4. Group Related Fields\n\n```json\n{\n  \"authentication\": {\n    \"type\": \"object\",\n    \"title\": \"Authentication\",\n    \"properties\": {\n      \"username\": { ... },\n      \"#password\": { ... }\n    }\n  }\n}\n```\n\n### 5. Use Dependencies for Conditional Fields\n\n```json\n{\n  \"dependencies\": {\n    \"auth_type\": {\n      \"oneOf\": [\n        {\n          \"properties\": {\n            \"auth_type\": { \"enum\": [\"password\"] },\n            \"#password\": { \"type\": \"string\" }\n          }\n        },\n        {\n          \"properties\": {\n            \"auth_type\": { \"enum\": [\"key\"] },\n            \"#private_key\": { \"type\": \"string\" }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n### 6. Provide Helpful Descriptions\n\n```json\n{\n  \"description\": \"Enter your API key. You can find it in Settings > API Keys.\"\n}\n```\n\n### 7. Use enum_titles for User-Friendly Labels\n\n```json\n{\n  \"region\": {\n    \"type\": \"string\",\n    \"enum\": [\"us-east-1\", \"eu-west-1\"],\n    \"enum_titles\": [\"US East (N. Virginia)\", \"EU West (Ireland)\"]\n  }\n}\n```\n\n### 8. Set Sensible Defaults\n\n```json\n{\n  \"timeout\": {\n    \"type\": \"integer\",\n    \"default\": 30,\n    \"description\": \"Request timeout in seconds\"\n  }\n}\n```\n\n### 9. Use Grid Layout for Complex Forms\n\n```json\n{\n  \"options\": {\n    \"grid_columns\": 2\n  }\n}\n```\n\n### 10. Include Test Connection Button\n\n```json\n{\n  \"test_connection\": {\n    \"type\": \"button\",\n    \"format\": \"test-connection\",\n    \"options\": {\n      \"async\": {\n        \"label\": \"Test Connection\",\n        \"action\": \"testConnection\"\n      }\n    }\n  }\n}\n```\n\n## Quick Reference\n\n### Common Field Types\n\n| Type | Use Case |\n|------|----------|\n| `string` | Text input, passwords, selections |\n| `integer` | Whole numbers |\n| `number` | Decimal numbers |\n| `boolean` | Checkboxes, toggles |\n| `array` | Lists, multi-select |\n| `object` | Grouped fields |\n\n### Common Formats\n\n| Format | Description |\n|--------|-------------|\n| `password` | Masked input for secrets |\n| `textarea` | Multi-line text |\n| `editor` | Code editor |\n| `select` | Dropdown selection |\n| `checkbox` | Boolean checkbox |\n| `date` | Date picker |\n| `uri` | URL input |\n\n### Common Options\n\n| Option | Description |\n|--------|-------------|\n| `propertyOrder` | Field display order |\n| `hidden` | Hide field from UI |\n| `inputAttributes.placeholder` | Placeholder text |\n| `editor.mode` | Code editor language |\n| `async` | Sync action configuration |\n\n## Related Documentation\n\n- [UI Elements](configuration-schema-ui-elements.md) - Field formats, options, and editor modes\n- [Sync Actions](configuration-schema-sync-actions.md) - Dynamic dropdowns and validation\n- [Advanced Patterns](configuration-schema-advanced.md) - Confluence best practices\n- [Examples](configuration-schema-examples.md) - Real production examples\n- [Initialization Guide](initialization-guide.md) - Setting up new components\n- [Architecture Guide](architecture.md) - Component structure and patterns\n",
        "plugins/component-developer/skills/build-component-ui/references/sync-actions.md": "# Configuration Schema Sync Actions\n\nComplete reference for dynamic UI elements including dropdowns, test connection, and validation buttons.\n\n## Table of Contents\n\n1. [Introduction](#introduction)\n2. [Types of Sync Actions](#types-of-sync-actions)\n3. [Dynamic Dropdowns](#dynamic-dropdowns)\n4. [Test Connection Button](#test-connection-button)\n5. [Validation Buttons](#validation-buttons)\n6. [Sync Action Response Format](#sync-action-response-format)\n7. [Common Sync Actions](#common-sync-actions)\n\n## Introduction\n\nSync actions enable dynamic UI elements that communicate with the component backend to:\n- Load dropdown options dynamically\n- Validate configurations\n- Test connections\n- Perform other server-side operations\n\nSync actions are defined using the `links` property with `rel: \"self\"` or the `options.async` property.\n\n## Types of Sync Actions\n\n### 1. Dynamic Dropdowns\nLoad options from the server based on current configuration.\n\n### 2. Test Connection\nVerify credentials and connectivity before saving.\n\n### 3. Validation Buttons\nValidate specific fields or configurations.\n\n### 4. Custom Actions\nPerform any server-side operation and return results.\n\n## Dynamic Dropdowns\n\nDynamic dropdowns load their options from the component backend.\n\n### Basic Structure\n\n```json\n{\n  \"table\": {\n    \"type\": \"string\",\n    \"title\": \"Table\",\n    \"format\": \"select\",\n    \"propertyOrder\": 1,\n    \"links\": [\n      {\n        \"rel\": \"self\",\n        \"href\": \"/{{componentId}}/configs/{{configId}}/actions/loadTables\",\n        \"method\": \"POST\"\n      }\n    ],\n    \"options\": {\n      \"async\": {\n        \"label\": \"Load Tables\",\n        \"action\": \"loadTables\"\n      }\n    }\n  }\n}\n```\n\n### With Autoload\n\nAutomatically load options when the form opens:\n\n```json\n{\n  \"table\": {\n    \"type\": \"string\",\n    \"title\": \"Table\",\n    \"format\": \"select\",\n    \"options\": {\n      \"async\": {\n        \"label\": \"Load Tables\",\n        \"action\": \"loadTables\",\n        \"autoload\": true\n      }\n    }\n  }\n}\n```\n\n### With Caching\n\nCache results to avoid repeated API calls:\n\n```json\n{\n  \"table\": {\n    \"type\": \"string\",\n    \"title\": \"Table\",\n    \"format\": \"select\",\n    \"options\": {\n      \"async\": {\n        \"label\": \"Load Tables\",\n        \"action\": \"loadTables\",\n        \"autoload\": true,\n        \"cache\": true\n      }\n    }\n  }\n}\n```\n\n### Dependent Dropdowns\n\nLoad options based on another field's value:\n\n```json\n{\n  \"database\": {\n    \"type\": \"string\",\n    \"title\": \"Database\",\n    \"format\": \"select\",\n    \"propertyOrder\": 1,\n    \"options\": {\n      \"async\": {\n        \"label\": \"Load Databases\",\n        \"action\": \"loadDatabases\",\n        \"autoload\": true\n      }\n    }\n  },\n  \"table\": {\n    \"type\": \"string\",\n    \"title\": \"Table\",\n    \"format\": \"select\",\n    \"propertyOrder\": 2,\n    \"options\": {\n      \"async\": {\n        \"label\": \"Load Tables\",\n        \"action\": \"loadTables\"\n      }\n    }\n  }\n}\n```\n\nThe `loadTables` action receives the current form values including the selected database.\n\n## Test Connection Button\n\nTest connection buttons verify credentials before saving.\n\n### Basic Structure\n\n```json\n{\n  \"test_connection\": {\n    \"type\": \"button\",\n    \"format\": \"test-connection\",\n    \"propertyOrder\": 100,\n    \"options\": {\n      \"async\": {\n        \"label\": \"Test Connection\",\n        \"action\": \"testConnection\"\n      }\n    }\n  }\n}\n```\n\n### With Custom Label\n\n```json\n{\n  \"test_connection\": {\n    \"type\": \"button\",\n    \"format\": \"test-connection\",\n    \"options\": {\n      \"async\": {\n        \"label\": \"Verify Credentials\",\n        \"action\": \"testConnection\"\n      }\n    }\n  }\n}\n```\n\n### Complete Example with Credentials\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Configuration\",\n  \"required\": [\"host\", \"#password\"],\n  \"properties\": {\n    \"host\": {\n      \"type\": \"string\",\n      \"title\": \"Host\",\n      \"propertyOrder\": 1\n    },\n    \"port\": {\n      \"type\": \"integer\",\n      \"title\": \"Port\",\n      \"default\": 5432,\n      \"propertyOrder\": 2\n    },\n    \"username\": {\n      \"type\": \"string\",\n      \"title\": \"Username\",\n      \"propertyOrder\": 3\n    },\n    \"#password\": {\n      \"type\": \"string\",\n      \"title\": \"Password\",\n      \"format\": \"password\",\n      \"propertyOrder\": 4\n    },\n    \"test_connection\": {\n      \"type\": \"button\",\n      \"format\": \"test-connection\",\n      \"propertyOrder\": 5,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Test Connection\",\n          \"action\": \"testConnection\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Validation Buttons\n\nValidation buttons check specific fields or configurations.\n\n### Basic Structure\n\n```json\n{\n  \"validate_query\": {\n    \"type\": \"button\",\n    \"format\": \"sync-action\",\n    \"propertyOrder\": 10,\n    \"options\": {\n      \"async\": {\n        \"label\": \"Validate Query\",\n        \"action\": \"validateQuery\"\n      }\n    }\n  }\n}\n```\n\n### SQL Query Validation Example\n\n```json\n{\n  \"query\": {\n    \"type\": \"string\",\n    \"title\": \"SQL Query\",\n    \"format\": \"editor\",\n    \"propertyOrder\": 1,\n    \"options\": {\n      \"editor\": {\n        \"mode\": \"text/x-sql\"\n      }\n    }\n  },\n  \"validate_query\": {\n    \"type\": \"button\",\n    \"format\": \"sync-action\",\n    \"propertyOrder\": 2,\n    \"options\": {\n      \"async\": {\n        \"label\": \"Validate SQL\",\n        \"action\": \"validateQuery\"\n      }\n    }\n  }\n}\n```\n\n### SOQL Query Validation (Salesforce Example)\n\n```json\n{\n  \"soql\": {\n    \"type\": \"string\",\n    \"title\": \"SOQL Query\",\n    \"format\": \"editor\",\n    \"propertyOrder\": 1,\n    \"options\": {\n      \"editor\": {\n        \"mode\": \"text/x-sql\"\n      }\n    }\n  },\n  \"validate_soql\": {\n    \"type\": \"button\",\n    \"format\": \"sync-action\",\n    \"propertyOrder\": 2,\n    \"options\": {\n      \"async\": {\n        \"label\": \"Validate SOQL\",\n        \"action\": \"validateSoql\"\n      }\n    }\n  }\n}\n```\n\n## Sync Action Response Format\n\n### Success Response\n\n```json\n{\n  \"status\": \"success\",\n  \"message\": \"Connection successful\"\n}\n```\n\n### Error Response\n\n```json\n{\n  \"status\": \"error\",\n  \"message\": \"Connection failed: Invalid credentials\"\n}\n```\n\n### Dropdown Options Response\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": [\n    {\"value\": \"table1\", \"label\": \"Table 1\"},\n    {\"value\": \"table2\", \"label\": \"Table 2\"},\n    {\"value\": \"table3\", \"label\": \"Table 3\"}\n  ]\n}\n```\n\n### Alternative Dropdown Format\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"values\": [\"table1\", \"table2\", \"table3\"],\n    \"labels\": [\"Table 1\", \"Table 2\", \"Table 3\"]\n  }\n}\n```\n\n## Common Sync Actions\n\nBased on analysis of 888+ production components, here are the most common sync actions:\n\n### Connection & Authentication\n\n| Action | Description |\n|--------|-------------|\n| `testConnection` | Test database/API connection |\n| `testCredentials` | Verify credentials |\n| `authorize` | OAuth authorization |\n| `refreshToken` | Refresh OAuth token |\n\n### Data Loading\n\n| Action | Description |\n|--------|-------------|\n| `loadTables` | Load available tables |\n| `loadDatabases` | Load available databases |\n| `loadSchemas` | Load database schemas |\n| `loadColumns` | Load table columns |\n| `loadFields` | Load object fields |\n| `loadObjects` | Load available objects |\n| `loadProfiles` | Load user profiles |\n| `loadAccounts` | Load accounts |\n| `loadWorkspaces` | Load workspaces |\n| `loadProjects` | Load projects |\n\n### Validation\n\n| Action | Description |\n|--------|-------------|\n| `validateQuery` | Validate SQL query |\n| `validateSoql` | Validate SOQL query |\n| `validateConfig` | Validate configuration |\n| `validateMapping` | Validate field mapping |\n\n### Metadata\n\n| Action | Description |\n|--------|-------------|\n| `loadPrimaryKeys` | Load possible primary keys |\n| `loadPossiblePrimaryKeys` | Load primary key candidates |\n| `loadMetadata` | Load object metadata |\n| `describeObject` | Get object description |\n\n### Real Production Example: Salesforce Extractor\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Salesforce Configuration\",\n  \"properties\": {\n    \"login_type\": {\n      \"type\": \"string\",\n      \"title\": \"Login Type\",\n      \"enum\": [\"password\", \"oauth\"],\n      \"enum_titles\": [\"Username & Password\", \"OAuth\"],\n      \"default\": \"password\",\n      \"propertyOrder\": 1\n    },\n    \"username\": {\n      \"type\": \"string\",\n      \"title\": \"Username\",\n      \"propertyOrder\": 2\n    },\n    \"#password\": {\n      \"type\": \"string\",\n      \"title\": \"Password\",\n      \"format\": \"password\",\n      \"propertyOrder\": 3\n    },\n    \"#security_token\": {\n      \"type\": \"string\",\n      \"title\": \"Security Token\",\n      \"format\": \"password\",\n      \"propertyOrder\": 4\n    },\n    \"test_connection\": {\n      \"type\": \"button\",\n      \"format\": \"test-connection\",\n      \"propertyOrder\": 5,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Test Connection\",\n          \"action\": \"testConnection\"\n        }\n      }\n    }\n  },\n  \"dependencies\": {\n    \"login_type\": {\n      \"oneOf\": [\n        {\n          \"properties\": {\n            \"login_type\": {\"enum\": [\"password\"]},\n            \"username\": {\"type\": \"string\"},\n            \"#password\": {\"type\": \"string\"},\n            \"#security_token\": {\"type\": \"string\"}\n          },\n          \"required\": [\"username\", \"#password\"]\n        },\n        {\n          \"properties\": {\n            \"login_type\": {\"enum\": [\"oauth\"]}\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n### Row Schema with Dynamic Dropdowns\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Table Configuration\",\n  \"properties\": {\n    \"object\": {\n      \"type\": \"string\",\n      \"title\": \"Salesforce Object\",\n      \"format\": \"select\",\n      \"propertyOrder\": 1,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Objects\",\n          \"action\": \"loadObjects\",\n          \"autoload\": true,\n          \"cache\": true\n        }\n      }\n    },\n    \"fields\": {\n      \"type\": \"array\",\n      \"title\": \"Fields\",\n      \"format\": \"select\",\n      \"propertyOrder\": 2,\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Fields\",\n          \"action\": \"loadFields\"\n        }\n      }\n    },\n    \"primary_key\": {\n      \"type\": \"string\",\n      \"title\": \"Primary Key\",\n      \"format\": \"select\",\n      \"propertyOrder\": 3,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Load Primary Keys\",\n          \"action\": \"loadPossiblePrimaryKeys\"\n        }\n      }\n    },\n    \"soql\": {\n      \"type\": \"string\",\n      \"title\": \"Custom SOQL Query\",\n      \"format\": \"editor\",\n      \"propertyOrder\": 4,\n      \"options\": {\n        \"editor\": {\n          \"mode\": \"text/x-sql\"\n        }\n      }\n    },\n    \"validate_soql\": {\n      \"type\": \"button\",\n      \"format\": \"sync-action\",\n      \"propertyOrder\": 5,\n      \"options\": {\n        \"async\": {\n          \"label\": \"Validate SOQL\",\n          \"action\": \"validateSoql\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Backend Implementation (Python)\n\n### âœ… CORRECT: Using @sync_action Decorator\n\nSync actions in Python components **MUST** be implemented using the `@sync_action` decorator from `keboola.component.base`.\n\n#### Required Import\n\n```python\nfrom keboola.component.base import ComponentBase, sync_action\nfrom keboola.component.exceptions import UserException\n```\n\n#### Basic Implementation\n\n```python\nclass Component(ComponentBase):\n    def run(self) -> None:\n        \"\"\"Main execution - runs when action='run' or no action specified.\"\"\"\n        # Your main component logic\n        pass\n\n    @sync_action(\"testConnection\")\n    def test_connection(self) -> dict:\n        \"\"\"\n        Test connection - executed when action='testConnection'.\n\n        Returns:\n            dict: Response in format {\"status\": \"success\", \"message\": \"...\"}\n        \"\"\"\n        try:\n            # Get parameters directly (no need for full configuration validation)\n            uri = self.configuration.parameters.get(\"authentication\", {}).get(\"#uri\")\n            if not uri:\n                raise UserException(\"URI is required\")\n\n            # Test connection logic here\n            # ... your connection test code ...\n\n            return {\"status\": \"success\", \"message\": \"Connection successful\"}\n        except Exception as e:\n            raise UserException(f\"Connection failed: {e}\") from e\n\n    @sync_action(\"listDatabases\")\n    def list_databases(self) -> dict:\n        \"\"\"\n        List databases for dropdown - executed when action='listDatabases'.\n\n        Returns:\n            dict: Response in format {\"status\": \"success\", \"data\": [{\"value\": \"...\", \"label\": \"...\"}]}\n        \"\"\"\n        try:\n            uri = self.configuration.parameters.get(\"authentication\", {}).get(\"#uri\")\n            if not uri:\n                raise UserException(\"URI is required\")\n\n            # Get list of databases\n            databases = [\"db1\", \"db2\", \"db3\"]  # Your logic here\n\n            # Format for Keboola UI dropdown\n            dropdown_data = [{\"value\": db, \"label\": db} for db in databases]\n\n            return {\"status\": \"success\", \"data\": dropdown_data}\n        except Exception as e:\n            raise UserException(f\"Failed to list databases: {e}\") from e\n\n    @sync_action(\"listTables\")\n    def list_tables(self) -> dict:\n        \"\"\"\n        List tables for dropdown - executed when action='listTables'.\n        Receives current form values including selected database.\n\n        Returns:\n            dict: Response in format {\"status\": \"success\", \"data\": [{\"value\": \"...\", \"label\": \"...\"}]}\n        \"\"\"\n        try:\n            uri = self.configuration.parameters.get(\"authentication\", {}).get(\"#uri\")\n            database = self.configuration.parameters.get(\"destination\", {}).get(\"database\")\n\n            if not uri or not database:\n                raise UserException(\"URI and database are required\")\n\n            # Get list of tables in the selected database\n            tables = [\"table1\", \"table2\", \"table3\"]  # Your logic here\n\n            # Format for Keboola UI dropdown\n            dropdown_data = [{\"value\": tbl, \"label\": tbl} for tbl in tables]\n\n            return {\"status\": \"success\", \"data\": dropdown_data}\n        except Exception as e:\n            raise UserException(f\"Failed to list tables: {e}\") from e\n```\n\n#### How the Decorator Works\n\nThe `@sync_action` decorator automatically:\n1. **Routes the action** - Maps `action` parameter in config.json to the decorated method\n2. **Handles output** - Writes JSON response to stdout\n3. **Catches exceptions** - Converts exceptions to error responses\n4. **Exits cleanly** - Exits after sync action completes\n5. **Mutes logging** - Sets log level to FATAL during sync actions\n6. **Serializes JSON** - Converts return dict to JSON automatically\n\n#### Complete Example\n\n```python\nimport csv\nimport logging\n\nfrom keboola.component.base import ComponentBase, sync_action\nfrom keboola.component.exceptions import UserException\n\nfrom your_client import YourClient\n\n\nclass Component(ComponentBase):\n    def __init__(self):\n        self._configuration = None\n        self.client = None\n        super().__init__()\n\n    def run(self) -> None:\n        \"\"\"Main execution - runs when action='run' or no action specified.\"\"\"\n        self._init_configuration()\n        self._init_client()\n        self.process_data()\n\n    @sync_action(\"testConnection\")\n    def test_connection(self) -> dict:\n        \"\"\"Test connection to the service.\"\"\"\n        uri = self.configuration.parameters.get(\"authentication\", {}).get(\"#uri\")\n        if not uri:\n            raise UserException(\"URI is required\")\n\n        client = YourClient(uri)\n        client.test_connection()\n\n        return {\"status\": \"success\", \"message\": \"Connection successful\"}\n\n    @sync_action(\"listDatabases\")\n    def list_databases(self) -> dict:\n        \"\"\"List available databases.\"\"\"\n        uri = self.configuration.parameters.get(\"authentication\", {}).get(\"#uri\")\n        if not uri:\n            raise UserException(\"URI is required\")\n\n        client = YourClient(uri)\n        databases = client.list_databases()\n\n        dropdown_data = [{\"value\": db, \"label\": db} for db in databases]\n        return {\"status\": \"success\", \"data\": dropdown_data}\n\n    @sync_action(\"listCollections\")\n    def list_collections(self) -> dict:\n        \"\"\"List collections in selected database (dependent dropdown).\"\"\"\n        uri = self.configuration.parameters.get(\"authentication\", {}).get(\"#uri\")\n        database = self.configuration.parameters.get(\"destination\", {}).get(\"database\")\n\n        if not uri:\n            raise UserException(\"URI is required\")\n        if not database:\n            raise UserException(\"Database is required to list collections\")\n\n        client = YourClient(uri, database)\n        collections = client.list_collections()\n\n        dropdown_data = [{\"value\": col, \"label\": col} for col in collections]\n        return {\"status\": \"success\", \"data\": dropdown_data}\n\n    def _init_configuration(self):\n        # Your configuration initialization\n        pass\n\n    def _init_client(self):\n        # Your client initialization\n        pass\n\n    def process_data(self):\n        # Your main logic\n        pass\n\n\nif __name__ == \"__main__\":\n    try:\n        comp = Component()\n        # execute_action() automatically routes to the correct method\n        comp.execute_action()\n    except UserException as exc:\n        logging.exception(exc)\n        exit(1)\n    except Exception as exc:\n        logging.exception(exc)\n        exit(2)\n```\n\n### Key Points\n\n1. **Use `@sync_action` decorator** - Standard way to implement sync actions in Keboola components\n2. **Access parameters via `self.configuration.parameters`** - Configuration validation not required for sync actions\n3. **Return dict with response** - Decorator handles JSON serialization automatically\n4. **Raise `UserException` for errors** - Decorator converts exceptions to proper error responses\n5. **Method names are flexible** - Decorator uses the action name from `@sync_action(\"actionName\")`\n6. **Dependent dropdowns receive form values** - Current form state is automatically passed to the action\n\n## Related Documentation\n\n- [Overview](configuration-schema-overview.md) - Introduction and basics\n- [UI Elements](configuration-schema-ui-elements.md) - Field formats and options\n- [Advanced Patterns](configuration-schema-advanced.md) - Confluence best practices\n- [Examples](configuration-schema-examples.md) - Real production examples\n",
        "plugins/component-developer/skills/build-component-ui/references/ui-elements.md": "# Configuration Schema UI Elements\n\nComplete reference for field formats, UI options, editor modes, and component flags.\n\n## Table of Contents\n\n1. [UI Options (Component Level)](#ui-options-component-level)\n2. [Component Flags](#component-flags)\n3. [Field Formats](#field-formats)\n4. [Options Keys](#options-keys)\n5. [Code Editor Modes](#code-editor-modes)\n\n## UI Options (Component Level)\n\nUI options control how the component configuration is rendered in the Keboola UI.\n\n### Available UI Options\n\n| Option | Description |\n|--------|-------------|\n| `genericUI` | Enable generic UI (base flag) |\n| `genericDockerUI` | Enable generic Docker UI |\n| `genericDockerUI-authorization` | Enable authorization section |\n| `genericDockerUI-processors` | Enable processors section |\n| `genericDockerUI-resetState` | Enable reset state button |\n| `genericDockerUI-tableInput` | Enable table input mapping |\n| `genericDockerUI-tableOutput` | Enable table output mapping |\n| `genericDockerUI-fileInput` | Enable file input mapping |\n| `genericDockerUI-fileOutput` | Enable file output mapping |\n| `genericDockerUI-rows` | Enable row-based configuration |\n| `genericDockerUI-simpleTableInput` | Shows table selector when creating rows, auto-fills input mapping, provides table metadata via `_metadata_.table` (requires `genericDockerUI-rows`) |\n| `genericTemplatesUI` | Enable templates UI |\n| `genericPackagesUI` | Enable packages UI |\n| `genericCodeBlocksUI` | Enable code blocks UI |\n| `excludeRun` | Exclude from run button |\n| `excludeFromNewList` | Hide from new component list |\n| `tableInputMapping` | Legacy table input mapping |\n| `tableOutputMapping` | Legacy table output mapping |\n| `fileInputMapping` | Legacy file input mapping |\n| `fileOutputMapping` | Legacy file output mapping |\n\n### Using genericDockerUI-simpleTableInput\n\nWhen enabled, this flag provides a simplified table input workflow for row-based components.\n\n**What it does:**\n- Displays a table selector in the \"Add Row\" dialog\n- Automatically creates input mapping with the selected table\n- Injects table metadata into the JSON schema context via `_metadata_.table`\n\n**Available metadata in schema:**\n- `_metadata_.table.id` - Full table ID (e.g., `in.c-bucket.tablename`)\n- `_metadata_.table.name` - Display name of the table\n- `_metadata_.table.columns` - Array of column names (respects column selection from mapping)\n- `_metadata_.table.primaryKey` - Array of primary key column names\n\n**Requirements:**\n- Must be used with `genericDockerUI-rows` because the table selector appears in the row creation modal\n\n**Example - Column selector using table metadata:**\n```json\n{\n  \"column\": {\n    \"type\": \"string\",\n    \"title\": \"Select Column\",\n    \"format\": \"select\",\n    \"enum\": [],\n    \"options\": {\n      \"async\": {\n        \"action\": \"getColumns\"\n      }\n    }\n  }\n}\n```\n\nThe component can then use `_metadata_.table.columns` to populate the dropdown dynamically.\n\n### Default UI Options by Component Type\n\n**Extractor:**\n```json\n[\"genericDockerUI\", \"genericDockerUI-authorization\", \"genericDockerUI-tableOutput\"]\n```\n\n**Writer:**\n```json\n[\"genericDockerUI\", \"genericDockerUI-authorization\", \"genericDockerUI-tableInput\"]\n```\n\n**Application:**\n```json\n[\"genericDockerUI\", \"genericDockerUI-authorization\", \"genericDockerUI-tableInput\", \"genericDockerUI-tableOutput\"]\n```\n\n**Transformation:**\n```json\n[\"genericDockerUI\", \"genericDockerUI-tableInput\", \"genericDockerUI-tableOutput\"]\n```\n\n## Component Flags\n\nFlags control component behavior and visibility.\n\n### Available Flags\n\n| Flag | Description |\n|------|-------------|\n| `3rdParty` | Third-party component (not developed by Keboola) |\n| `appInfo.alpha` | Alpha version component |\n| `appInfo.beta` | Beta version component |\n| `appInfo.experimental` | Experimental version component |\n| `appInfo.fee` | Component requires additional fee |\n| `appInfo.licenseUrl` | URL to license information |\n| `appInfo.dataIn` | Component reads data |\n| `appInfo.dataOut` | Component writes data |\n| `deprecated` | Component is deprecated |\n| `excludeFromNewList` | Hide from new component list |\n| `excludeRun` | Exclude from run button |\n| `genericUI` | Enable generic UI |\n| `genericCodeBlocksUI` | Enable code blocks UI |\n| `genericDockerUI` | Enable generic Docker UI |\n| `genericDockerUI-authorization` | Enable authorization section |\n| `genericDockerUI-fileInput` | Enable file input mapping |\n| `genericDockerUI-fileOutput` | Enable file output mapping |\n| `genericDockerUI-processors` | Enable processors section |\n| `genericDockerUI-resetState` | Enable reset state button |\n| `genericDockerUI-rows` | Enable row-based configuration |\n| `genericDockerUI-simpleTableInput` | Shows table selector when creating rows, auto-fills input mapping, provides table metadata via `_metadata_.table` |\n| `genericDockerUI-tableInput` | Enable table input mapping |\n| `genericDockerUI-tableOutput` | Enable table output mapping |\n| `genericPackagesUI` | Enable packages UI |\n| `genericTemplatesUI` | Enable templates UI |\n| `hasUI` | Component has custom UI |\n| `dbtCloud` | DBT Cloud integration |\n| `hubspotTemplates` | Hubspot templates support |\n\n## Field Formats\n\nFormats control how fields are rendered in the UI.\n\n### String Formats\n\n| Format | Description | Example |\n|--------|-------------|---------|\n| `password` | Masked input for secrets | `\"format\": \"password\"` |\n| `textarea` | Multi-line text input | `\"format\": \"textarea\"` |\n| `editor` | Code editor (use with `options.editor.mode`) | `\"format\": \"editor\"` |\n| `select` | Dropdown selection | `\"format\": \"select\"` |\n| `trim` | Auto-trim whitespace | `\"format\": \"trim\"` |\n| `date` | Date picker | `\"format\": \"date\"` |\n| `date-time` | Date and time picker | `\"format\": \"date-time\"` |\n| `time` | Time picker | `\"format\": \"time\"` |\n| `timestamp` | Unix timestamp input | `\"format\": \"timestamp\"` |\n| `uri` | URL input with validation | `\"format\": \"uri\"` |\n| `email` | Email input with validation | `\"format\": \"email\"` |\n| `radio` | Radio button group | `\"format\": \"radio\"` |\n| `color` | Color picker | `\"format\": \"color\"` |\n| `hidden` | Hidden field | `\"format\": \"hidden\"` |\n\n### Integer/Number Formats\n\n| Format | Description | Example |\n|--------|-------------|---------|\n| `range` | Slider input | `\"format\": \"range\"` |\n| `int32` | 32-bit integer | `\"format\": \"int32\"` |\n\n### Boolean Formats\n\n| Format | Description | Example |\n|--------|-------------|---------|\n| `checkbox` | Standard checkbox | `\"format\": \"checkbox\"` |\n\n**Note:** `chekbox` (typo) appears in some legacy components - use `checkbox` instead.\n\n### Array Formats\n\n| Format | Description | Example |\n|--------|-------------|---------|\n| `select` | Multi-select dropdown | `\"format\": \"select\"` |\n| `checkbox` | Checkbox group | `\"format\": \"checkbox\"` |\n| `table` | Table editor | `\"format\": \"table\"` |\n| `tabs` | Tabbed interface | `\"format\": \"tabs\"` |\n| `tabs-top` | Tabs at top | `\"format\": \"tabs-top\"` |\n\n### Button Formats\n\n| Format | Description | Example |\n|--------|-------------|---------|\n| `test-connection` | Test connection button | `\"format\": \"test-connection\"` |\n| `sync-action` | Generic sync action button | `\"format\": \"sync-action\"` |\n\n### Object Formats\n\n| Format | Description | Example |\n|--------|-------------|---------|\n| `ssh-editor` | SSH tunnel/key pair form | `\"format\": \"ssh-editor\"` |\n\n### Special Formats\n\n| Format | Description | Example |\n|--------|-------------|---------|\n| `alt-date` | Alternative date picker | `\"format\": \"alt-date\"` |\n| `Personal access token` | GitHub PAT input (rare) | `\"format\": \"Personal access token\"` |\n\n### Format Examples\n\n**Password Field:**\n```json\n{\n  \"#api_key\": {\n    \"type\": \"string\",\n    \"title\": \"API Key\",\n    \"format\": \"password\",\n    \"propertyOrder\": 1\n  }\n}\n```\n\n**Textarea:**\n```json\n{\n  \"description\": {\n    \"type\": \"string\",\n    \"title\": \"Description\",\n    \"format\": \"textarea\",\n    \"propertyOrder\": 2\n  }\n}\n```\n\n**Date Picker:**\n```json\n{\n  \"start_date\": {\n    \"type\": \"string\",\n    \"title\": \"Start Date\",\n    \"format\": \"date\",\n    \"description\": \"Select the start date for data extraction\",\n    \"propertyOrder\": 3\n  }\n}\n```\n\n**Date-Time Picker:**\n```json\n{\n  \"scheduled_at\": {\n    \"type\": \"string\",\n    \"title\": \"Scheduled At\",\n    \"format\": \"date-time\",\n    \"description\": \"Select date and time\",\n    \"propertyOrder\": 4\n  }\n}\n```\n\n**Email Field:**\n```json\n{\n  \"email\": {\n    \"type\": \"string\",\n    \"title\": \"Email Address\",\n    \"format\": \"email\",\n    \"description\": \"Enter a valid email address\",\n    \"propertyOrder\": 5\n  }\n}\n```\n\n**Radio Buttons:**\n```json\n{\n  \"output_format\": {\n    \"type\": \"string\",\n    \"title\": \"Output Format\",\n    \"format\": \"radio\",\n    \"enum\": [\"csv\", \"json\", \"parquet\"],\n    \"enum_titles\": [\"CSV\", \"JSON\", \"Parquet\"],\n    \"default\": \"csv\",\n    \"propertyOrder\": 6\n  }\n}\n```\n\n**URI Field:**\n```json\n{\n  \"endpoint\": {\n    \"type\": \"string\",\n    \"title\": \"API Endpoint\",\n    \"format\": \"uri\",\n    \"description\": \"Enter the full API URL\",\n    \"propertyOrder\": 7\n  }\n}\n```\n\n**SSH Editor:**\n```json\n{\n  \"ssh\": {\n    \"type\": \"object\",\n    \"title\": \"SSH Configuration\",\n    \"format\": \"ssh-editor\",\n    \"propertyOrder\": 8\n  }\n}\n```\n\n**SSH Editor (Keys Only):**\n```json\n{\n  \"ssh\": {\n    \"type\": \"object\",\n    \"title\": \"SSH Keys\",\n    \"format\": \"ssh-editor\",\n    \"options\": {\n      \"only_keys\": true\n    },\n    \"propertyOrder\": 9\n  }\n}\n```\n\n## Options Keys\n\nOptions control field behavior and appearance.\n\n### Display Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `propertyOrder` | integer | Field display order (lower = higher) |\n| `hidden` | boolean | Hide field from UI |\n| `collapsed` | boolean | Collapse object by default |\n| `compact` | boolean | Use compact display mode |\n| `grid_columns` | integer | Number of grid columns (1-12) |\n| `object_layout` | string | Object layout style (`\"grid\"`, `\"table\"`) |\n\n### Input Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `inputAttributes` | object | HTML input attributes |\n| `inputAttributes.placeholder` | string | Placeholder text |\n| `inputAttributes.readonly` | boolean | Make field read-only |\n| `input_width` | string | Input width (`\"100px\"`, `\"50%\"`) |\n| `expand_height` | boolean | Expand textarea height |\n\n### Editor Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `editor` | object | Code editor configuration |\n| `editor.mode` | string | Editor language mode |\n| `editor.theme` | string | Editor theme |\n| `editor.lineNumbers` | boolean | Show line numbers |\n| `editor.readOnly` | boolean | Make editor read-only |\n| `editor.placeholder` | string | Placeholder text for empty editor |\n| `editor.autofocus` | boolean | Auto-focus editor on load |\n| `editor.lint` | boolean | Enable linting (for JSON mode) |\n| `only_keys` | boolean | SSH form: show only keys (not full tunnel) |\n\n### Behavior Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `disable_collapse` | boolean | Prevent collapsing |\n| `disable_edit_json` | boolean | Disable JSON editing |\n| `disable_properties` | boolean | Disable property editing |\n| `keep_oneof_values` | boolean | Keep values when switching oneOf |\n| `remove_empty_properties` | boolean | Remove empty properties on save |\n\n### Array Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `minItems` | integer | Minimum array items |\n| `maxItems` | integer | Maximum array items |\n\n### Date Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `flatpickr` | object | Flatpickr date picker options |\n| `flatpickr.enableTime` | boolean | Enable time selection |\n| `flatpickr.dateFormat` | string | Date format string |\n\n### Async/Sync Action Options\n\n| Option | Type | Description |\n|--------|------|-------------|\n| `async` | object | Async action configuration |\n| `async.label` | string | Button label |\n| `async.action` | string | Action name |\n| `async.autoload` | boolean | Auto-load on form open |\n| `async.cache` | boolean | Cache results |\n\n### Options Examples\n\n**Placeholder Text:**\n```json\n{\n  \"api_key\": {\n    \"type\": \"string\",\n    \"title\": \"API Key\",\n    \"options\": {\n      \"inputAttributes\": {\n        \"placeholder\": \"Enter your API key here\"\n      }\n    }\n  }\n}\n```\n\n**Read-Only Field:**\n```json\n{\n  \"component_id\": {\n    \"type\": \"string\",\n    \"title\": \"Component ID\",\n    \"options\": {\n      \"inputAttributes\": {\n        \"readonly\": true\n      }\n    }\n  }\n}\n```\n\n**Collapsed Object:**\n```json\n{\n  \"advanced_settings\": {\n    \"type\": \"object\",\n    \"title\": \"Advanced Settings\",\n    \"options\": {\n      \"collapsed\": true\n    },\n    \"properties\": {\n      \"timeout\": { \"type\": \"integer\" },\n      \"retries\": { \"type\": \"integer\" }\n    }\n  }\n}\n```\n\n**Grid Layout:**\n```json\n{\n  \"type\": \"object\",\n  \"options\": {\n    \"grid_columns\": 2\n  },\n  \"properties\": {\n    \"first_name\": { \"type\": \"string\", \"propertyOrder\": 1 },\n    \"last_name\": { \"type\": \"string\", \"propertyOrder\": 2 }\n  }\n}\n```\n\n**Flatpickr Date Picker:**\n```json\n{\n  \"start_date\": {\n    \"type\": \"string\",\n    \"title\": \"Start Date\",\n    \"format\": \"date\",\n    \"options\": {\n      \"flatpickr\": {\n        \"enableTime\": false,\n        \"dateFormat\": \"Y-m-d\"\n      }\n    }\n  }\n}\n```\n\n**Disable JSON Editing:**\n```json\n{\n  \"config\": {\n    \"type\": \"object\",\n    \"title\": \"Configuration\",\n    \"options\": {\n      \"disable_edit_json\": true\n    }\n  }\n}\n```\n\n**Input Width:**\n```json\n{\n  \"port\": {\n    \"type\": \"integer\",\n    \"title\": \"Port\",\n    \"options\": {\n      \"input_width\": \"100px\"\n    }\n  }\n}\n```\n\n## Code Editor Modes\n\nUse with `format: \"editor\"` and `options.editor.mode`.\n\n### Available Modes\n\n| Mode | Language | Example |\n|------|----------|---------|\n| `text/x-sql` | SQL | Database queries |\n| `text/x-python` | Python | Python scripts |\n| `text/x-rsrc` | R | R scripts |\n| `text/x-julia` | Julia | Julia scripts |\n| `application/json` | JSON | JSON configuration |\n| `application/xml` | XML | XML documents |\n| `text/x-toml` | TOML | TOML configuration |\n| `text/x-yaml` | YAML | YAML configuration |\n| `text/javascript` | JavaScript | JavaScript code |\n| `text/html` | HTML | HTML markup |\n| `text/css` | CSS | CSS styles |\n| `text/x-markdown` | Markdown | Markdown text |\n\n### Editor Examples\n\n**SQL Editor:**\n```json\n{\n  \"query\": {\n    \"type\": \"string\",\n    \"title\": \"SQL Query\",\n    \"format\": \"editor\",\n    \"options\": {\n      \"editor\": {\n        \"mode\": \"text/x-sql\",\n        \"lineNumbers\": true\n      }\n    },\n    \"propertyOrder\": 1\n  }\n}\n```\n\n**Python Editor:**\n```json\n{\n  \"script\": {\n    \"type\": \"string\",\n    \"title\": \"Python Script\",\n    \"format\": \"editor\",\n    \"options\": {\n      \"editor\": {\n        \"mode\": \"text/x-python\",\n        \"lineNumbers\": true\n      }\n    },\n    \"propertyOrder\": 1\n  }\n}\n```\n\n**JSON Editor:**\n```json\n{\n  \"config\": {\n    \"type\": \"string\",\n    \"title\": \"JSON Configuration\",\n    \"format\": \"editor\",\n    \"options\": {\n      \"editor\": {\n        \"mode\": \"application/json\",\n        \"lineNumbers\": true\n      }\n    },\n    \"propertyOrder\": 1\n  }\n}\n```\n\n**XML Editor:**\n```json\n{\n  \"xml_template\": {\n    \"type\": \"string\",\n    \"title\": \"XML Template\",\n    \"format\": \"editor\",\n    \"options\": {\n      \"editor\": {\n        \"mode\": \"application/xml\",\n        \"lineNumbers\": true\n      }\n    },\n    \"propertyOrder\": 1\n  }\n}\n```\n\n**TOML Editor:**\n```json\n{\n  \"toml_config\": {\n    \"type\": \"string\",\n    \"title\": \"TOML Configuration\",\n    \"format\": \"editor\",\n    \"options\": {\n      \"editor\": {\n        \"mode\": \"text/x-toml\",\n        \"lineNumbers\": true\n      }\n    },\n    \"propertyOrder\": 1\n  }\n}\n```\n\n**YAML Editor:**\n```json\n{\n  \"yaml_config\": {\n    \"type\": \"string\",\n    \"title\": \"YAML Configuration\",\n    \"format\": \"editor\",\n    \"options\": {\n      \"editor\": {\n        \"mode\": \"text/x-yaml\",\n        \"lineNumbers\": true\n      }\n    },\n    \"propertyOrder\": 1\n  }\n}\n```\n\n## Related Documentation\n\n- [Overview](configuration-schema-overview.md) - Introduction and basics\n- [Sync Actions](configuration-schema-sync-actions.md) - Dynamic dropdowns and validation\n- [Advanced Patterns](configuration-schema-advanced.md) - Confluence best practices\n- [Examples](configuration-schema-examples.md) - Real production examples\n",
        "plugins/component-developer/skills/build-component-ui/schema-tester/README.md": "# Component Schema Tester\n\nA unified, flexible testing tool for Keboola component schemas with auto-discovery and manual path selection.\n\n## Quick Start\n\n### Option 1: Auto-discovery (from component directory)\n```bash\ncd /path/to/your/component\npython component_schema_tester.py\n```\n\n### Option 2: Specify component path\n```bash\n# From anywhere - provide component root path\npython component_schema_tester.py /path/to/component\n\n# Or provide component_config path directly\npython component_schema_tester.py /path/to/component/component_config\n\n# Specify custom port\npython component_schema_tester.py /path/to/component --port 8080\n```\n\n### Option 3: Manual path selection in UI\n```bash\n# Start with any component, then change paths in the UI\npython component_schema_tester.py /path/to/component1\n\n# In browser: Use ðŸ“ button to select different component_config folder\n# Click \"ðŸ”„ Reload Schemas\" to apply\n```\n\nThen open: **http://localhost:8000**\n\nThat's it! Schemas load automatically.\n\n## Features\n\n### Auto-Discovery\n- Automatically finds `component_config/` folder\n- Loads `configSchema.json` and `configRowSchema.json`\n- Pre-fills forms from `data/config.json` if it exists\n- No configuration needed!\n\n### Integrated Sync Actions\n- All sync actions are handled automatically\n- Calls your component methods directly\n- No need to specify endpoints\n- Supports all action types:\n  - `testConnection`\n  - `loadEntities`\n  - `loadFields`\n  - `loadPossiblePrimaryKeys`\n  - `loadIncrementalFields`\n  - `loadNavigationProperties`\n  - `previewData`\n  - `validateFilter`\n\n### Smart Features\n- **Watch Fields**: Automatically reloads dropdowns when dependencies change\n- **Auto-load**: Dropdowns with `autoload: true` load on page start\n- **Pre-filled Config**: Loads existing `data/config.json` values\n- **Live Validation**: Real-time schema validation\n- **Conditional Fields**: Full support for `options.dependencies`\n- **Combined Output**: See complete `config.json` ready for Keboola\n\n## How It Works\n\n### 1. Auto-Discovery Process\n\n```\ncomponent_schema_tester.py\n    â†“\nSearches upward for component_config/\n    â†“\nFinds project root\n    â†“\nLoads schemas: configSchema.json + configRowSchema.json\n    â†“\nLoads config (if exists): data/config.json\n    â†“\nStarts Flask server on port 8000\n```\n\n### 2. API Endpoints\n\nThe tool provides these endpoints:\n\n- `GET /` - Serves the schema tester UI\n- `GET /api/schemas` - Returns both schemas\n- `GET /api/config` - Returns existing config.json parameters\n- `POST /sync-action` - Handles all sync actions\n\n### 3. Sync Action Flow\n\n```\nUI (dropdown) â†’ POST /sync-action\n    â†“\nWrite config to data/config.json\n    â†“\nImport Component from src/component.py\n    â†“\nCall action method (e.g., comp.load_entities())\n    â†“\nReturn result to UI\n    â†“\nUpdate dropdown options\n```\n\n## Usage Examples\n\n### Testing Conditional Fields\n\n1. Edit your schema to add conditional fields:\n   ```json\n   {\n     \"properties\": {\n       \"sync_type\": {\n         \"type\": \"string\",\n         \"enum\": [\"full\", \"incremental\"]\n       },\n       \"incremental_field\": {\n         \"type\": \"string\",\n         \"options\": {\n           \"dependencies\": {\n             \"sync_type\": \"incremental\"\n           }\n         }\n       }\n     }\n   }\n   ```\n\n2. Click \"Reload Schemas\"\n3. Change `sync_type` to \"incremental\"\n4. Watch `incremental_field` appear!\n\n### Testing Async Dropdowns\n\n1. Your schema has async dropdown:\n   ```json\n   {\n     \"entity_set\": {\n       \"type\": \"string\",\n       \"format\": \"select\",\n       \"options\": {\n         \"async\": {\n           \"label\": \"Load Entity Sets\",\n           \"action\": \"loadEntities\",\n           \"autoload\": true\n         }\n       }\n     }\n   }\n   ```\n\n2. The tester will:\n   - Auto-load on page start (if `autoload: true`)\n   - Add \"Load Entity Sets\" button\n   - Call your `Component.load_entities()` method\n   - Populate dropdown with results\n\n### Testing with Existing Config\n\nIf you have `data/config.json`:\n\n```json\n{\n  \"parameters\": {\n    \"base_url\": \"https://api.example.com\",\n    \"api_key\": \"test123\",\n    \"entity_set\": \"Products\"\n  }\n}\n```\n\nThe tester will:\n1. Load the config on page start\n2. Split parameters between component and row schemas\n3. Pre-fill all form fields\n4. Ready for testing!\n\n## Development Workflow\n\n### 1. Edit Schemas\n```bash\nvim component_config/configSchema.json\nvim component_config/configRowSchema.json\n```\n\n### 2. Reload in Browser\nClick \"ðŸ”„ Reload Schemas\" button\n\n### 3. Test Changes\n- Change values\n- Test conditional fields\n- Test async dropdowns\n- Validate forms\n\n### 4. Copy Final Config\nGo to \"Resulting configuration\" tab and click \"ðŸ“‹ Copy to Clipboard\"\n\n## Requirements\n\nThe tool requires these Python packages (already in your `pyproject.toml`):\n\n```toml\n[tool.poetry.group.dev.dependencies]\nflask = \"^3.0.0\"\nflask-cors = \"^4.0.0\"\n```\n\n## Troubleshooting\n\n### \"Could not find component_config/ folder\"\n\n**Solution**: Run the script from within your component project:\n\n```bash\ncd /path/to/your/component\npython component_schema_tester.py\n```\n\n### \"Could not import Component\"\n\n**Solution**: Ensure `src/component.py` exists and has a `Component` class.\n\n### \"Unknown action: myAction\"\n\n**Solution**: Add the action to the `action_map` in `component_schema_tester.py`:\n\n```python\naction_map = {\n    'myAction': comp.my_action,\n    # ... other actions\n}\n```\n\n## Advantages Over Old Setup\n\n### Before (Multiple Files)\n```\ntools/schema-tester/\nâ”œâ”€â”€ server.py           # Flask server\nâ”œâ”€â”€ schema-tester.html  # UI\nâ”œâ”€â”€ start-server.sh     # Startup script\nâ””â”€â”€ README.md           # Documentation\n```\n\nRequired:\n- Manual path configuration\n- Separate HTML file\n- Shell script to run\n- Manual config.json setup\n\n### After (Single File)\n```\ncomponent_schema_tester.py  # Everything!\n```\n\nFeatures:\n- Auto-discovers everything\n- Embedded HTML (no external files)\n- Pre-fills from config.json\n- Direct component integration\n- Zero configuration\n\n## File Structure\n\n```python\ncomponent_schema_tester.py\nâ”œâ”€â”€ Auto-discovery functions\nâ”‚   â”œâ”€â”€ find_project_root()\nâ”‚   â”œâ”€â”€ find_component_config()\nâ”‚   â””â”€â”€ find_config_json()\nâ”œâ”€â”€ Flask app setup\nâ”‚   â””â”€â”€ CORS enabled\nâ”œâ”€â”€ API endpoints\nâ”‚   â”œâ”€â”€ GET /\nâ”‚   â”œâ”€â”€ GET /api/schemas\nâ”‚   â”œâ”€â”€ GET /api/config\nâ”‚   â””â”€â”€ POST /sync-action\nâ”œâ”€â”€ Embedded HTML\nâ”‚   â””â”€â”€ Complete schema-tester UI\nâ””â”€â”€ Main entry point\n    â””â”€â”€ Auto-discover and start\n```\n\n## Tips\n\n1. **Always reload after schema changes**: Click the reload button to see updates\n2. **Check browser console**: Useful for debugging sync actions\n3. **Use \"Validate Form\"**: Catches schema validation errors\n4. **Test all tabs**: Component, Row, and Combined configs\n5. **Copy final config**: Use the clipboard button in \"Resulting configuration\" tab\n\n## Support\n\nFor issues or questions:\n1. Check browser console for errors\n2. Check terminal output for Flask logs\n3. Verify your schemas are valid JSON\n4. Ensure `src/component.py` has required methods\n\n## License\n\nPart of the Keboola Component Factory toolkit.\n",
        "plugins/component-developer/skills/build-component/SKILL.md": "---\nname: component-builder\ndescription: Builds production-ready Keboola Python components with best practices and architectural patterns. Use when creating new extractors/writers/applications, implementing incremental loads, designing configuration schemas, adding API client separation, following self-documenting workflow patterns, or setting up components with cookiecutter templates and Ruff code quality.\ntools: Bash, Read, Write, Edit, Glob, Grep, WebFetch, WebSearch, TodoWrite, Task, AskUserQuestion\nmodel: sonnet\ncolor: purple\n---\n\n# Keboola Component Builder Agent\n\nYou are an expert Keboola component developer specializing in building production-ready Python components for the Keboola Connection platform. You understand the Keboola Common Interface, component architecture, configuration schemas, and deployment workflows.\n\n## âš ï¸ UI Development Delegation\n\n**For configuration schema and UI work, automatically delegate to the specialized `ui-developer` agent:**\n\nWhen the user asks about:\n- Creating or modifying `configSchema.json` or `configRowSchema.json`\n- Adding conditional fields (show/hide based on other fields)\n- Testing schemas with schema-tester\n- UI elements and form controls\n- Sync actions and dynamic field loading\n\n**Use the Task tool to call the ui-developer agent:**\n```\nTask tool with:\n- subagent_type: \"component-developer:ui-developer\"\n- prompt: [detailed description of the UI/schema work needed]\n```\n\nThe `ui-developer` agent specializes in:\n- âœ… **Correct syntax** - Uses `options.dependencies` (not JSON Schema dependencies)\n- âœ… **Schema testing** - Interactive schema-tester tool\n- âœ… **Playwright testing** - Automated E2E tests\n- âœ… **Focused documentation** - UI-specific guides\n\n**You (component-builder) handle everything else:**\n- Component architecture and Python code\n- API client implementation\n- Data processing logic\n- Keboola API integration\n- Deployment and CI/CD\n- Testing and debugging (non-UI)\n\n**Important:** When delegating, provide complete context to ui-developer including:\n- What the component does\n- What configuration fields are needed\n- Any conditional logic requirements\n- Authentication requirements\n- Any existing schema that needs to be modified\n\n## ðŸ”§ Other Specialized Agents\n\nYou can also delegate to other specialized agents for specific tasks:\n\n### Code Review: @reviewer\n\n**When to delegate:**\n- User explicitly asks for code review\n- After completing a significant feature implementation\n- Before creating a pull request\n\n**Use the Task tool:**\n```\nTask tool with:\n- subagent_type: \"component-developer:reviewer\"\n- prompt: \"Review the component code in src/ focusing on [architecture/typing/safety/etc]\"\n```\n\nThe reviewer will provide actionable TODOs grouped by severity (Blocking / Important / Nice-to-Have).\n\n### Debugging: @debugger\n\n**When to delegate:**\n- Component is failing with errors\n- User reports a failed job ID\n- Need to investigate why component isn't working\n- Need to query Keboola API for job/config details\n\n**Use the Task tool:**\n```\nTask tool with:\n- subagent_type: \"component-developer:debugger\"\n- prompt: \"Debug failed job [job_id] for component [component_id]\"\n```\n\nThe debugger has access to Keboola MCP tools and can identify root causes.\n\n### Testing: @tester\n\n**When to delegate:**\n- User asks for test coverage\n- Need to write datadir tests for new features\n- Need to add unit tests for complex logic\n- Need to set up integration tests with mocking\n\n**Use the Task tool:**\n```\nTask tool with:\n- subagent_type: \"component-developer:tester\"\n- prompt: \"Write comprehensive tests for [feature/component], including datadir tests for [scenarios]\"\n```\n\nThe tester specializes in datadir tests, unit tests, and proper mocking patterns.\n\n## Core Responsibilities\n\n### 1. Component Initialization & Setup\n\nWhen creating a new component:\n\n1. **Understand Requirements**: Gather information about what the component should do\n2. **Use Cookiecutter Template**: Initialize using `cookiecutter gh:keboola/cookiecutter-python-component`\n3. **Clean Up and Configure**: Remove example files, create component-specific `data/config.json`\n4. **Implement**: Follow architectural patterns and best practices\n5. **Test and Deploy**: Comprehensive testing before deployment\n\n**ðŸ“– For detailed initialization steps**, see [../get-started/references/initialization.md](../get-started/references/initialization.md)\n\n### 2. Component Architecture\n\nFollow Keboola's architectural patterns:\n\n- Use `CommonInterface` base class\n- Implement clean `run()` method as workflow orchestrator\n- **Separate API clients** into dedicated files for complex integrations (e.g., `anthropic_client.py`, `playwright_client.py`)\n- Process CSV files with generators for memory efficiency\n- Handle errors with proper exit codes (1 for user, 2 for system)\n- Implement state management for incremental processing\n- Define explicit schemas for output tables\n\n**ðŸ“– For complete architectural patterns**, see [references/architecture.md](references/architecture.md)\n\n### 3. Code Quality & Formatting\n\nAll components must follow code quality standards:\n\n- **Ruff**: Format with `ruff format .` and check with `ruff check --fix .`\n- **Type Hints**: Add proper type annotations to all functions\n- **@staticmethod**: Mark utility methods that don't use `self`\n- **IDE Warnings**: Fix all type warnings and linting issues\n\n**ðŸ“– For complete code quality guidelines**, see [references/code-quality.md](references/code-quality.md)\n\n### 4. Self-Documenting Workflow Pattern\n\n**CRITICAL**: Keep `run()` method as a clean orchestrator (~20-30 lines) that delegates to well-named private methods.\n\n```python\ndef run(self):\n    \"\"\"Main execution - orchestrates the component workflow.\"\"\"\n    try:\n        params = self._validate_and_get_configuration()\n        state = self._load_previous_state()\n\n        input_data = self._process_input_tables()\n        results = self._perform_business_logic(input_data, params, state)\n\n        self._save_output_tables(results)\n        self._update_state(results)\n\n    except ValueError as err:\n        logging.error(str(err))\n        sys.exit(1)\n    except Exception as err:\n        logging.exception(\"Unhandled error\")\n        sys.exit(2)\n```\n\n**ðŸ“– For complete workflow patterns and examples**, see [references/workflow-patterns.md](references/workflow-patterns.md)\n\n### 5. Best Practices Reference\n\nQuick DO/DON'T reference:\n\n**âœ… DO:**\n- Remove cookiecutter examples, create `data/config.json`\n- Keep `run()` as orchestrator, extract logic to private methods\n- Format with ruff, add type hints, use @staticmethod\n- Validate configuration early, handle errors properly\n\n**âŒ DON'T:**\n- Leave cookiecutter example files in `data/` directory\n- Write monolithic `run()` methods with 100+ lines\n- Ignore IDE type warnings or \"may be static\" warnings\n- Call `mkdir()` for platform-managed directories\n\n**ðŸ“– For complete best practices and patterns**, see [references/best-practices.md](references/best-practices.md)\n\n## Workflow Guidelines\n\n### For New Components\n\n1. **Initialize with cookiecutter**\n   - See [../get-started/references/initialization.md](../get-started/references/initialization.md)\n\n2. **Implement following patterns**\n   - Architecture: [references/architecture.md](references/architecture.md)\n   - Code Quality: [references/code-quality.md](references/code-quality.md)\n   - Workflow Patterns: [references/workflow-patterns.md](references/workflow-patterns.md)\n\n3. **Verify against best practices**\n   - Check [references/best-practices.md](references/best-practices.md)\n\n4. **Test and deploy**\n   - Run tests, format with ruff, verify in Developer Portal\n\n### For Existing Components\n\n1. **Review current structure** to understand existing patterns\n2. **Maintain consistency** with existing code style\n3. **Update configuration schema** if adding new parameters\n4. **Add/update tests** for new functionality\n5. **Update documentation** to reflect changes\n6. **Follow semantic versioning** for releases\n\n## Key Resources\n\nWhen you need additional information, reference:\n\n- **Keboola Developer Docs**: https://developers.keboola.com/\n- **Python Component Library**: https://github.com/keboola/python-component\n- **Component Tutorial**: https://developers.keboola.com/extend/component/tutorial/\n- **Python Implementation**: https://developers.keboola.com/extend/component/implementation/python/\n- **Cookiecutter Template**: https://github.com/keboola/cookiecutter-python-component\n\n**Internal Documentation:**\n- [Initialization Guide](../get-started/references/initialization.md) - Setting up new components\n- [Architecture Guide](references/architecture.md) - Component structure and patterns\n- [Code Quality](references/code-quality.md) - Ruff, type hints, @staticmethod\n- [Workflow Patterns](references/workflow-patterns.md) - Self-documenting code\n- [Best Practices](references/best-practices.md) - DO/DON'T reference\n- [Developer Portal](references/developer-portal.md) - Registration and deployment\n- [Schema Overview](../build-component-ui/references/overview.md) - Complete reference for configSchema.json and configRowSchema.json\n- [UI Elements](../build-component-ui/references/ui-elements.md) - Field formats, options, and editor modes\n- [Conditional Fields](../build-component-ui/references/conditional-fields.md) - Using options.dependencies\n- [Sync Actions](../build-component-ui/references/sync-actions.md) - Dynamic dropdowns and validation\n- [Advanced Schema Patterns](../build-component-ui/references/advanced.md) - Best practices and complex scenarios\n- [Schema Examples](../build-component-ui/references/examples.md) - Real production examples\n- [Debugging](../debug-component/references/debugging.md) - Troubleshooting techniques\n\n## Your Approach\n\nWhen helping users build Keboola components:\n\n1. **Understand the requirement** thoroughly before writing code\n2. **Use TodoWrite** to track implementation steps\n3. **Ask questions** when requirements are unclear using AskUserQuestion\n4. **Follow documentation** - reference the guides/ guides for patterns\n5. **Write clean, well-documented code**\n6. **Include proper error handling** with appropriate exit codes\n7. **Add comprehensive tests**\n8. **Apply code quality workflow** after implementing any Python code\n9. **Validate everything** works before committing\n10. **Guide through deployment** process when needed\n\n### Code Quality Workflow (Always Apply)\n\nAfter implementing any Python code:\n\n1. **Add proper type hints** to all functions and variables\n2. **Check IDE for type warnings** (red squiggles) and fix them\n3. **Import library-specific types** where needed (e.g., `MessageParam` from anthropic)\n4. **Add `@staticmethod` decorator** for methods that don't use `self`\n5. **Extract complex logic** from `run()` into well-named private methods\n6. Run `ruff format .` to ensure consistent formatting\n7. Run `ruff check --fix .` to catch and fix linting issues\n8. Optionally run `mypy src/` for additional type checking\n9. Review the changes to ensure quality\n10. Test the component functionality\n\n**CRITICAL REMINDERS:**\n\n- Always check IDE warnings and fix them before committing\n- Type warnings often indicate real bugs\n- **\"May be static\" warnings MUST be fixed** - add `@staticmethod` decorator immediately\n- Keep `run()` method clean and readable (~20-30 lines)\n- Extract logic blocks > 10-15 lines into separate methods\n- Method names should eliminate the need for comments\n- **Use `@staticmethod` on ALL methods that don't access `self`** - this includes utility methods like `_initialize_client()`, `_extract_data()`, `_generate_suggestions()`, etc.\n\n### When to Reference Documentation\n\n- **Starting new component?** â†’ [../get-started/references/initialization.md](../get-started/references/initialization.md)\n- **Need architectural patterns?** â†’ [references/architecture.md](references/architecture.md)\n- **Formatting and type safety?** â†’ [references/code-quality.md](references/code-quality.md)\n- **Code organization unclear?** â†’ [references/workflow-patterns.md](references/workflow-patterns.md)\n- **Quick DO/DON'T check?** â†’ [references/best-practices.md](references/best-practices.md)\n- **Deploying to Developer Portal?** â†’ [references/developer-portal.md](references/developer-portal.md)\n- **Designing configuration schemas?** â†’ [../build-component-ui/references/overview.md](../build-component-ui/references/overview.md)\n- **Need UI field formats?** â†’ [../build-component-ui/references/ui-elements.md](../build-component-ui/references/ui-elements.md)\n- **Adding conditional fields?** â†’ [../build-component-ui/references/conditional-fields.md](../build-component-ui/references/conditional-fields.md)\n- **Adding dynamic dropdowns?** â†’ [../build-component-ui/references/sync-actions.md](../build-component-ui/references/sync-actions.md)\n- **Advanced schema patterns?** â†’ [../build-component-ui/references/advanced.md](../build-component-ui/references/advanced.md)\n- **Schema examples?** â†’ [../build-component-ui/references/examples.md](../build-component-ui/references/examples.md)\n- **Debugging issues?** â†’ [../debug-component/references/debugging.md](../debug-component/references/debugging.md)\n\n**Use the Task tool** to read documentation files when you need detailed guidance on specific topics. The documentation contains comprehensive examples and explanations.\n\nAlways prioritize code quality, maintainability, and adherence to Keboola's architectural patterns. Your goal is to create production-ready components that integrate seamlessly with the Keboola platform.\n",
        "plugins/component-developer/skills/build-component/references/MIGRATION_GUIDE.md": "# Bitbucket â†’ GitHub Migration Guide\n\nComplete guide for migrating a repository from Bitbucket to GitHub with full history, all branches, tags, and GitHub Actions setup.\n\n## Prerequisites\n\n- GitHub CLI (`gh`) installed and authenticated\n- Git configured (user.name, user.email)\n- Access to Bitbucket repository\n- Permissions on GitHub organization (for creating repositories, setting teams, secrets)\n\n## Step 1: Preparation\n\n```bash\n# Verify GitHub CLI authentication\ngh auth status\n\n# Verify git configuration\ngit config --get user.name\ngit config --get user.email\n```\n\n## Step 2: Clone Bitbucket Repository\n\n```bash\n# Clone Bitbucket repository locally\ncd /path/to/parent/directory\ngit clone git@bitbucket.org:workspace/repo-name.git\ncd repo-name\n\n# Verify branches and tags\ngit branch -a\ngit tag\n```\n\n## Step 3: Create Empty GitHub Repository\n\nCreate an empty repository on GitHub (via web UI or gh CLI):\n```bash\ngh repo create org/repo-name --private --clone=false\n```\n\nOr use an existing empty repository.\n\n## Step 4: Add GitHub Remote and Push Everything\n\n```bash\n# In the cloned Bitbucket repository\ncd /path/to/bitbucket-repo\n\n# Add GitHub as new remote\ngit remote add github git@github.com:org/repo-name.git\n\n# Fetch everything\ngit fetch --all\n\n# Push all branches\ngit push github --all\n\n# Push all tags\ngit push github --tags\n\n# Push all remote branches (if --all didn't push everything)\n# For each branch separately:\ngit push github refs/remotes/origin/branch-name:refs/heads/branch-name\n\n# Or for all at once (if there are many):\nfor branch in $(git branch -r | grep 'origin/' | grep -v 'HEAD' | grep -v 'master'); do\n    branch_name=${branch#origin/}\n    git push github refs/remotes/$branch:refs/heads/$branch_name\ndone\n```\n\n## Step 5: Add GitHub Actions Workflow\n\n```bash\n# In Bitbucket repository\ngit checkout master  # or main\n\n# Delete Bitbucket Pipelines\nrm -f bitbucket-pipelines.yml\n\n# Create GitHub Actions workflow\nmkdir -p .github/workflows\n```\n\nCreate `.github/workflows/push.yml` file with:\n- Set `KBC_DEVELOPERPORTAL_APP` to correct component ID\n- Set `KBC_DEVELOPERPORTAL_VENDOR`\n- Configure workflow as needed\n\n```bash\n# Commit changes\ngit add -A\ngit commit -m \"Migrate from Bitbucket to GitHub\n\n- Added GitHub Actions workflow (.github/workflows/push.yml)\n- Removed bitbucket-pipelines.yml\n\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\"\n\n# Push to GitHub\ngit push github master\n```\n\n## Step 6: Rename master â†’ main (if needed)\n\n```bash\n# Change default branch to master temporarily (if it's main)\ngh api -X PATCH /repos/org/repo-name -f default_branch=master\n\n# Remove branch protection from main (if exists)\ngh api -X DELETE /repos/org/repo-name/branches/main/protection\n\n# Delete old main branch (if exists)\ngit push github --delete main\n\n# Create new main from master\ngit push github master:refs/heads/main\n\n# Set main as default\ngh api -X PATCH /repos/org/repo-name -f default_branch=main\n\n# Delete master\ngit push github --delete master\n```\n\n## Step 7: Configure GitHub Actions Secrets\n\n```bash\necho \"kds-team+github\" | gh secret set KBC_DEVELOPERPORTAL_USERNAME -R org/repo-name\necho \"your-password-here\" | gh secret set KBC_DEVELOPERPORTAL_PASSWORD -R org/repo-name\n```\n\n**Credentials for Keboola components:**\n- Username: `kds-team+github`\n- Password: (use existing from migration script or generate new)\n\n## Step 8: Assign Team Access\n\n```bash\n# Add team with admin rights\ngh api -X PUT /orgs/org-name/teams/team-slug/repos/org-name/repo-name -f permission=admin\n```\n\nTypically for Keboola:\n```bash\ngh api -X PUT /orgs/keboola/teams/component-factory/repos/keboola/repo-name -f permission=admin\n```\n\n## Step 9: Configure Branch Protection\n\n```bash\ngh api -X PUT \"/repos/org/repo-name/branches/main/protection\" --input - <<'EOF'\n{\n  \"required_status_checks\": {\n    \"strict\": true,\n    \"contexts\": [\"tests\", \"tests-kbc\", \"push\"]\n  },\n  \"enforce_admins\": true,\n  \"required_pull_request_reviews\": {\n    \"dismiss_stale_reviews\": true,\n    \"require_code_owner_reviews\": false,\n    \"required_approving_review_count\": 1\n  },\n  \"restrictions\": null\n}\nEOF\n```\n\n## Step 10: Verification\n\n```bash\n# Verify default branch\ngh repo view org/repo-name --json defaultBranchRef\n\n# Verify branches\ngh api /repos/org/repo-name/branches --jq '.[].name'\n\n# Verify tags\ngh api /repos/org/repo-name/tags --jq '.[].name' | head -10\n\n# Verify secrets\ngh secret list -R org/repo-name\n\n# Verify branch protection\ngh api /repos/org/repo-name/branches/main/protection --jq '{\n  required_checks: .required_status_checks.contexts,\n  enforce_admins: .enforce_admins.enabled,\n  required_reviews: .required_pull_request_reviews.required_approving_review_count\n}'\n\n# Verify workflow\ngh api /repos/org/repo-name/contents/.github/workflows/push.yml --jq '.name'\n\n# Verify bitbucket-pipelines is deleted\ngh api /repos/org/repo-name/contents/bitbucket-pipelines.yml 2>&1 | grep -q \"Not Found\" && echo \"âœ“ Correctly deleted\" || echo \"âœ— Still exists\"\n```\n\n## Step 11: Update Local Repository\n\n```bash\n# In original working directory (GitHub repository)\ncd /path/to/working/repo\ngit fetch origin\ngit reset --hard origin/main\ngit log --oneline -5  # Verify history\n```\n\n## Notes\n\n### Preserving Timestamps\n- **Commit timestamps** (author date, committer date) are automatically preserved during push\n- Only the new migration commit has the current date\n- All other commits retain their original dates from Bitbucket\n\n### Alternative Method (git mirror)\nFor a completely clean mirror you can use:\n```bash\ngit clone --mirror git@bitbucket.org:workspace/repo.git repo.git\ncd repo.git\ngit push --mirror git@github.com:org/repo.git\n```\n\n**Advantages:**\n- Simpler, one command\n- Complete mirror including all refs\n\n**Disadvantages:**\n- Doesn't add GitHub Actions workflow automatically\n- Needs to be added afterwards to each branch or at least to main\n\n### Important\n- **Always verify** that all branches and tags were transferred\n- **Check** GitHub Actions workflow before first push\n- **Set** branch protection only after everything is configured\n- **Developer Portal attributes** (sourceCodeUrl, documentationUrl, licenseUrl) should be updated during next component release\n\n## Troubleshooting\n\n### Branch Protection Blocks Deletion\n```bash\n# Remove branch protection first\ngh api -X DELETE /repos/org/repo-name/branches/branch-name/protection\n# Then delete branch\ngit push github --delete branch-name\n```\n\n### Remote Branches Won't Push\n```bash\n# Use full refspec\ngit push github refs/remotes/origin/branch-name:refs/heads/branch-name\n```\n\n### Non-fast-forward Error During Rename\n```bash\n# If there's conflict between main and master, change default branch first\ngh api -X PATCH /repos/org/repo-name -f default_branch=master\n# Then remove branch protection and delete main\ngh api -X DELETE /repos/org/repo-name/branches/main/protection\ngit push github --delete main\n# Then create new main from master\ngit push github master:refs/heads/main\n```\n\n### Push Declined Due to Repository Rule Violations\nThis means branch protection is blocking direct push. You need to either:\n1. Temporarily disable enforce_admins\n2. Create a branch and PR instead of direct push\n\n## Post-Migration Checklist\n\n- [ ] All branches transferred\n- [ ] All tags transferred\n- [ ] Commit history preserved (including timestamps)\n- [ ] GitHub Actions workflow added and correctly configured\n- [ ] `KBC_DEVELOPERPORTAL_APP` ID set correctly in workflow\n- [ ] bitbucket-pipelines.yml deleted\n- [ ] Secrets configured (KBC_DEVELOPERPORTAL_USERNAME, KBC_DEVELOPERPORTAL_PASSWORD)\n- [ ] Team access assigned (component-factory: admin)\n- [ ] Branch protection configured on main\n- [ ] Default branch set correctly (main)\n- [ ] First GitHub Actions build started (doesn't need to pass, but should start)\n\n## Example: Real Migration kds-team.wr-sftp-csas\n\n```bash\n# 1. Cloned Bitbucket repo\ncd ../kds-team.wr-sftp-csas\n\n# 2. Add GitHub remote\ngit remote add github git@github.com:keboola/component-wr-sftp-csas.git\n\n# 3. Push everything\ngit push github --all\ngit push github --tags\ngit push github refs/remotes/origin/feature-add-test-host:refs/heads/feature-add-test-host\ngit push github refs/remotes/origin/feature/retry-on-path-not-found:refs/heads/feature/retry-on-path-not-found\ngit push github refs/remotes/origin/test:refs/heads/test\n\n# 4. Add GitHub Actions\nrm -f bitbucket-pipelines.yml\nmkdir -p .github/workflows\n# Create .github/workflows/push.yml with KBC_DEVELOPERPORTAL_APP: \"kds-team.wr-sftp-csas\"\ngit add -A\ngit commit -m \"Migrate from Bitbucket to GitHub...\"\ngit push github master\n\n# 5. Rename to main\ngh api -X PATCH /repos/keboola/component-wr-sftp-csas -f default_branch=master\ngh api -X DELETE /repos/keboola/component-wr-sftp-csas/branches/main/protection\ngit push github --delete main\ngit push github master:refs/heads/main\ngh api -X PATCH /repos/keboola/component-wr-sftp-csas -f default_branch=main\ngit push github --delete master\n\n# 6. Secrets\necho \"kds-team+github\" | gh secret set KBC_DEVELOPERPORTAL_USERNAME -R keboola/component-wr-sftp-csas\necho \"password\" | gh secret set KBC_DEVELOPERPORTAL_PASSWORD -R keboola/component-wr-sftp-csas\n\n# 7. Team\ngh api -X PUT /orgs/keboola/teams/component-factory/repos/keboola/component-wr-sftp-csas -f permission=admin\n\n# 8. Branch protection\ngh api -X PUT \"/repos/keboola/component-wr-sftp-csas/branches/main/protection\" --input - <<'EOF'\n{\n  \"required_status_checks\": {\n    \"strict\": true,\n    \"contexts\": [\"tests\", \"tests-kbc\", \"push\"]\n  },\n  \"enforce_admins\": true,\n  \"required_pull_request_reviews\": {\n    \"dismiss_stale_reviews\": true,\n    \"require_code_owner_reviews\": false,\n    \"required_approving_review_count\": 1\n  },\n  \"restrictions\": null\n}\nEOF\n\n# 9. Verification\ngh repo view keboola/component-wr-sftp-csas --json defaultBranchRef,name\ngh api /repos/keboola/component-wr-sftp-csas/branches --jq '.[].name'\ngh api /repos/keboola/component-wr-sftp-csas/tags --jq '.[].name' | head -10\n```\n\nResult:\n- âœ… 4 branches (main, feature-add-test-host, feature/retry-on-path-not-found, test)\n- âœ… 25 tags (0.0.1 to 1.2.7 + test)\n- âœ… Full commit history preserved\n- âœ… GitHub Actions workflow active\n- âœ… Repository URL: https://github.com/keboola/component-wr-sftp-csas\n",
        "plugins/component-developer/skills/build-component/references/architecture.md": "# Component Architecture\n\nComplete architectural patterns and best practices for Keboola Python components.\n\n## Main Component Implementation\n\n**IMPORTANT**: The Keboola platform automatically creates all data directories (`data/in/`, `data/out/tables/`, `data/out/files/`, etc.). You **never** need to call `mkdir()` or create these directories manually.\n\n### Basic Component Structure\n\n```python\nfrom keboola.component import CommonInterface\nimport logging\nimport sys\nimport traceback\nfrom pathlib import Path\n\n# REQUIRED_PARAMETERS should list all mandatory config parameters\nREQUIRED_PARAMETERS = ['api_key', 'endpoint']\n\nclass Component(CommonInterface):\n    def __init__(self):\n        super().__init__()\n\n    def run(self):\n        \"\"\"\n        Main execution method containing all component logic.\n        Handles state, config, tables, manifests, and logging.\n        \"\"\"\n        try:\n            # 1. Validate configuration\n            self.validate_configuration(REQUIRED_PARAMETERS)\n            params = self.configuration.parameters\n\n            # 2. Get data directory paths (NO mkdir needed - platform creates them!)\n            data_dir = Path(self.data_folder_path)\n            out_files_dir = data_dir / \"out\" / \"files\"\n            # Platform ensures these directories exist, just use them directly\n\n            # 3. Load state (for incremental processing)\n            state = self.get_state_file()\n            last_run = state.get('last_timestamp')\n\n            # 4. Process input tables (if applicable)\n            input_tables = self.get_input_tables_definitions()\n            for table in input_tables:\n                logging.info(f\"Processing table: {table.name}\")\n                self._process_table(table)\n\n            # 5. Create output tables with proper manifests\n            self._create_output_tables()\n\n            # 6. Save state for next run\n            self.write_state_file({'last_timestamp': current_timestamp})\n\n        except ValueError as err:\n            # User errors (configuration/input issues)\n            logging.error(str(err))\n            print(err, file=sys.stderr)\n            sys.exit(1)\n        except Exception as err:\n            # System errors (unhandled exceptions)\n            logging.exception(\"Unhandled error occurred\")\n            traceback.print_exc(file=sys.stderr)\n            sys.exit(2)\n\n    def _process_table(self, table_def):\n        \"\"\"Process individual table with CSV handling best practices.\"\"\"\n        # Use generator pattern for null character handling\n        with open(table_def.full_path, 'r', encoding='utf-8') as in_file:\n            lazy_lines = (line.replace('\\0', '') for line in in_file)\n            # Process rows efficiently without loading entire file\n            # Implementation here...\n\n    def _create_output_tables(self):\n        \"\"\"Create output tables with proper schema definitions.\"\"\"\n        from collections import OrderedDict\n        from keboola.component.dao import ColumnDefinition, BaseType\n\n        # Define schema\n        schema = OrderedDict({\n            \"id\": ColumnDefinition(\n                data_types=BaseType.integer(),\n                primary_key=True\n            ),\n            \"name\": ColumnDefinition(),\n            \"value\": ColumnDefinition(\n                data_types=BaseType.numeric(length=\"10,2\")\n            )\n        })\n\n        # Create table definition\n        out_table = self.create_out_table_definition(\n            name=\"results.csv\",\n            destination=\"out.c-data.results\",\n            schema=schema,\n            incremental=True\n        )\n\n        # Write data\n        import csv\n        with open(out_table.full_path, 'w', newline='', encoding='utf-8') as f:\n            writer = csv.DictWriter(f, fieldnames=out_table.column_names)\n            writer.writeheader()\n            # Write rows...\n\n        # Write manifest\n        self.write_manifest(out_table)\n\nif __name__ == '__main__':\n    try:\n        comp = Component()\n        comp.run()\n    except Exception as e:\n        logging.exception(\"Component execution failed\")\n        sys.exit(2)\n```\n\n## Static Methods in Components\n\n**IMPORTANT:** Always use `@staticmethod` decorator for methods that don't access `self`.\n\n### When to Use @staticmethod\n\nUse `@staticmethod` for utility methods that:\n- Don't access instance attributes (self.something)\n- Don't call other instance methods\n- Are pure functions that could work standalone\n- Transform, validate, or parse data\n\n```python\nclass Component(ComponentBase):\n    def __init__(self):\n        super().__init__()\n        # Load configuration and initialize client in constructor\n        # so sync_actions and other methods can use them\n        self.config = self._load_configuration()\n        self.client = self._initialize_client(self.config.api)\n\n    def run(self):\n        # Configuration and client already available via self\n        raw_response = self.client.fetch_data()          # Client handles endpoints internally\n        data = self._parse_response(raw_response)        # Static: pure transformation\n        self._save_results(data)                         # Uses self.files_out_path â†’ instance\n\n    def _load_configuration(self) -> Configuration:\n        \"\"\"Instance method - accesses self.configuration.\"\"\"\n        return Configuration(**self.configuration.parameters)\n\n    @staticmethod\n    def _initialize_client(api_config: ApiConfig) -> APIClient:\n        \"\"\"Static method - pure function, no self needed.\"\"\"\n        return APIClient(\n            api_key=api_config.api_key,\n            base_url=api_config.base_url,\n            timeout=api_config.timeout,\n        )\n\n    @staticmethod\n    def _parse_response(response: dict) -> list[dict]:\n        \"\"\"Static method - operates only on arguments.\"\"\"\n        return response.get('data', [])\n\n    def _save_results(self, data: list[dict]) -> None:\n        \"\"\"Instance method - uses self.files_out_path.\"\"\"\n        output_path = self.files_out_path / \"results.json\"\n        with open(output_path, 'w') as f:\n            json.dump(data, f)\n```\n\n> **IMPORTANT:** Configuration loading and client initialization MUST happen in `__init__()`, not in `run()`. This ensures that sync_actions and other methods (defined by the `action` parameter in config.json) have access to `self.config` and `self.client`.\n\n> **Note:** This is a simplified example focused on the `@staticmethod` rule. The API client is stored on the instance (`self.client`) and API configuration (keys, endpoints, timeouts) is encapsulated in an `ApiConfig` model. For complete patterns on structuring API clients and configuration models, see the **API Client Organization** section below.\n\n### Quick Rule\n\n- **Uses `self.anything`?** â†’ Instance method (no decorator)\n- **Only uses arguments?** â†’ `@staticmethod` decorator\n\n## API Client Organization\n\nFor components that integrate with external APIs or services, **separate API client logic into dedicated client files** when:\n\n1. The API integration is complex (multiple endpoints, authentication, retry logic)\n2. The client code would exceed ~100 lines\n3. The client might be reusable across multiple methods\n4. You want to isolate external service dependencies for testing\n\n### When to Create Separate Client Files\n\n**âœ… DO create separate client files:**\n- Complex API integrations (Anthropic, OpenAI, Salesforce, etc.)\n- Browser automation setup (Playwright, Selenium)\n- Database connections with connection pooling\n- Services requiring authentication, retry logic, or rate limiting\n\n**âŒ DON'T create separate client files:**\n- Simple HTTP requests (use `requests` directly in component.py)\n- Single-endpoint APIs with no special logic\n- When the \"client\" would be < 50 lines of trivial wrapper code\n\n### Example Structure\n\n```\nsrc/\nâ”œâ”€â”€ component.py           # Main component orchestration\nâ”œâ”€â”€ configuration.py       # Pydantic configuration\nâ”œâ”€â”€ anthropic_client.py    # Claude API client\nâ””â”€â”€ playwright_client.py   # Browser automation client\n```\n\n### Example: Anthropic Client\n\n**src/anthropic_client.py:**\n```python\n\"\"\"Claude AI client for web scraping tasks.\"\"\"\n\nimport logging\nfrom typing import Any\n\nimport anthropic\nfrom anthropic.types import MessageParam\n\n\nclass AnthropicClient:\n    \"\"\"Wrapper for Anthropic Claude API with scraping-specific methods.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"claude-3-5-sonnet-20241022\"):\n        self.client = anthropic.Anthropic(api_key=api_key)\n        self.model = model\n        logging.info(f\"Initialized Anthropic client with model {model}\")\n\n    def extract_data_from_html(\n        self,\n        page_title: str,\n        page_content: str,\n        extraction_prompt: str,\n    ) -> str:\n        \"\"\"\n        Extract structured data from HTML using Claude AI.\n\n        Args:\n            page_title: Title of the webpage\n            page_content: HTML content (will be truncated to 10k chars)\n            extraction_prompt: User's data extraction instructions\n\n        Returns:\n            Claude's response as JSON string\n        \"\"\"\n        system_prompt = \"\"\"You are a web scraping expert.\nExtract requested data and return as JSON:\n{\n    \"data\": [{\"field\": \"value\"}, ...],\n    \"metadata\": {\"url\": \"...\", \"timestamp\": \"...\", \"total_records\": N}\n}\"\"\"\n\n        user_message: MessageParam = {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Page Title: {page_title}\\n\\n\"\n                f\"Request: {extraction_prompt}\\n\\n\"\n                f\"HTML Content:\\n{page_content[:10000]}\\n\\n\"\n                f\"Extract data in JSON format.\"\n            ),\n        }\n\n        response = self.client.messages.create(\n            model=self.model,\n            max_tokens=4096,\n            system=system_prompt,\n            messages=[user_message],\n        )\n\n        response_text = response.content[0].text\n        logging.debug(f\"Claude response: {response_text[:200]}...\")\n        return response_text\n\n    def generate_prompt_improvements(\n        self, original_prompt: str, metadata: dict[str, Any]\n    ) -> str:\n        \"\"\"Generate suggestions for improving extraction prompts.\"\"\"\n        user_message: MessageParam = {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Analyze this prompt: {original_prompt}\\n\\n\"\n                f\"Results: {metadata}\\n\\n\"\n                f\"Provide 3-5 suggestions as JSON.\"\n            ),\n        }\n\n        response = self.client.messages.create(\n            model=self.model,\n            max_tokens=1024,\n            messages=[user_message],\n        )\n\n        return response.content[0].text\n```\n\n**src/playwright_client.py:**\n```python\n\"\"\"Playwright browser automation client.\"\"\"\n\nimport logging\nfrom pathlib import Path\n\nfrom playwright.sync_api import Browser, Page, Playwright, sync_playwright\n\n\nclass PlaywrightClient:\n    \"\"\"Wrapper for Playwright browser automation.\"\"\"\n\n    def __init__(self, headless: bool = True):\n        self.headless = headless\n        self.playwright_ctx: Playwright | None = None\n        self.browser: Browser | None = None\n        self.page: Page | None = None\n        logging.info(\"Initializing Playwright browser client\")\n\n    def start(self):\n        \"\"\"Initialize and start browser.\"\"\"\n        self.playwright_ctx = sync_playwright().start()\n        self.browser = self.playwright_ctx.chromium.launch(headless=self.headless)\n        self.page = self.browser.new_page()\n        logging.info(\"Browser started successfully\")\n\n    def navigate(self, url: str, timeout: int = 30000):\n        \"\"\"Navigate to URL with timeout.\"\"\"\n        if not self.page:\n            raise RuntimeError(\"Browser not started. Call start() first.\")\n        logging.info(f\"Navigating to {url}\")\n        self.page.goto(url, timeout=timeout)\n\n    def get_content(self) -> tuple[str, str]:\n        \"\"\"Get page title and HTML content.\"\"\"\n        if not self.page:\n            raise RuntimeError(\"Browser not started\")\n        return self.page.title(), self.page.content()\n\n    def screenshot(self, path: Path):\n        \"\"\"Capture screenshot to file.\"\"\"\n        if not self.page:\n            raise RuntimeError(\"Browser not started\")\n        self.page.screenshot(path=str(path))\n        logging.info(f\"Screenshot saved: {path}\")\n\n    def close(self):\n        \"\"\"Clean up browser resources.\"\"\"\n        if self.page:\n            self.page.close()\n        if self.browser:\n            self.browser.close()\n        if self.playwright_ctx:\n            self.playwright_ctx.stop()\n        logging.info(\"Browser closed\")\n```\n\n**Using clients in component.py:**\n```python\nfrom anthropic_client import AnthropicClient\nfrom playwright_client import PlaywrightClient\n\nclass Component(ComponentBase):\n    def __init__(self):\n        super().__init__()\n        # Load configuration and initialize clients in constructor\n        # so sync_actions and other methods can use them\n        self.params = self._validate_configuration()\n        self.ai_client = AnthropicClient(self.params.anthropic_api_key)\n        self.browser_client = PlaywrightClient(headless=True)\n        self.browser_client.start()\n\n    def run(self):\n        try:\n            # Configuration and clients already available via self\n            self.browser_client.navigate(self.params.target_url, self.params.timeout * 1000)\n            title, content = self.browser_client.get_content()\n\n            data = self.ai_client.extract_data_from_html(title, content, self.params.prompt)\n            # ... process data\n\n        finally:\n            if self.browser_client:\n                self.browser_client.close()\n```\n\n### Benefits of Separate Client Files\n\n1. **Testability**: Mock clients easily in unit tests\n2. **Reusability**: Share client code across multiple component methods\n3. **Separation of Concerns**: Keep API logic separate from business logic\n4. **Maintainability**: Changes to API integration don't affect component logic\n5. **Type Safety**: Dedicated classes provide better type hints and IDE support\n\n### When Not to Separate\n\nFor simple cases, keep it in component.py:\n\n```python\n# âœ… FINE - Simple API call, keep in component.py\ndef _fetch_data(self, url: str) -> dict:\n    response = requests.get(url, headers={\"Authorization\": f\"Bearer {self.api_key}\"})\n    response.raise_for_status()\n    return response.json()\n```\n\n## Configuration Schema\n\nCreate robust configuration schemas with proper UI elements:\n\n```json\n{\n  \"type\": \"object\",\n  \"title\": \"Configuration\",\n  \"required\": [\"api_key\", \"endpoint\"],\n  \"properties\": {\n    \"api_key\": {\n      \"type\": \"string\",\n      \"title\": \"API Key\",\n      \"description\": \"Your API authentication token\",\n      \"propertyOrder\": 1,\n      \"format\": \"password\"\n    },\n    \"endpoint\": {\n      \"type\": \"string\",\n      \"title\": \"API Endpoint\",\n      \"description\": \"Base URL for the API\",\n      \"propertyOrder\": 2\n    },\n    \"incremental\": {\n      \"type\": \"boolean\",\n      \"title\": \"Incremental Load\",\n      \"description\": \"Only fetch data since last run\",\n      \"default\": false,\n      \"propertyOrder\": 3\n    }\n  }\n}\n```\n\n### UI Elements & Features\n\n**Sensitive Data**: Prefix with `#` to enable automatic hashing:\n```json\n{\n  \"#password\": {\n    \"type\": \"string\",\n    \"title\": \"Password\",\n    \"format\": \"password\"\n  }\n}\n```\n\n**Dynamic Dropdowns**: Use sync actions to populate options from API calls\n\n**Code Editor**: Use ACE editor for multi-line input:\n```json\n{\n  \"query\": {\n    \"type\": \"string\",\n    \"title\": \"SQL Query\",\n    \"format\": \"textarea\",\n    \"options\": {\n      \"ace\": {\n        \"mode\": \"sql\",\n        \"theme\": \"tomorrow\"\n      }\n    }\n  }\n}\n```\n\n**Test Connection**: Add sync action for connection validation:\n```json\n{\n  \"test_connection\": {\n    \"type\": \"button\",\n    \"title\": \"Test Connection\",\n    \"options\": {\n      \"syncAction\": \"test-connection\"\n    }\n  }\n}\n```\n\n## CSV Processing Best Practices\n\nAlways follow these patterns when working with CSV files:\n\n```python\nimport csv\n\n# Reading input tables\ndef process_input_table(table_def):\n    with open(table_def.full_path, 'r', encoding='utf-8') as in_file:\n        # Handle null characters with generator\n        lazy_lines = (line.replace('\\0', '') for line in in_file)\n        reader = csv.DictReader(lazy_lines, dialect='kbc')\n\n        for row in reader:\n            # Process row by row for memory efficiency\n            yield process_row(row)\n\n# Writing output tables with proper schema\ndef write_output_table(ci, rows, schema):\n    out_table = ci.create_out_table_definition(\n        name=\"output.csv\",\n        destination=\"out.c-data.output\",\n        schema=schema,\n        incremental=True\n    )\n\n    with open(out_table.full_path, 'w', newline='', encoding='utf-8') as f:\n        writer = csv.DictWriter(f, fieldnames=out_table.column_names, dialect='kbc')\n        writer.writeheader()\n\n        # Write rows as they're processed (don't load all into memory)\n        for row in rows:\n            writer.writerow(row)\n\n    ci.write_manifest(out_table)\n    return out_table\n```\n\n## State Management for Incremental Processing\n\nImplement proper state handling for incremental loads:\n\n```python\ndef run_incremental(ci):\n    # Load previous state\n    state = ci.get_state_file()\n    last_timestamp = state.get('last_timestamp', '1970-01-01T00:00:00Z')\n\n    # Fetch only new data since last_timestamp\n    new_data = fetch_data_since(last_timestamp)\n\n    # Process and save data\n    process_data(new_data)\n\n    # Update state with current timestamp\n    current_timestamp = datetime.now(timezone.utc).isoformat()\n    ci.write_state_file({\n        'last_timestamp': current_timestamp,\n        'records_processed': len(new_data),\n        'last_run_stats': get_stats(new_data)\n    })\n```\n\n## Error Handling & Logging\n\nFollow Keboola's error handling conventions:\n\n```python\nimport logging\nimport sys\nimport traceback\n\ntry:\n    # Component logic\n    validate_inputs(params)\n    result = perform_operation()\n\nexcept ValueError as err:\n    # User errors: configuration problems, invalid inputs\n    # Exit code 1 indicates user-fixable errors\n    logging.error(f\"Configuration error: {err}\")\n    print(err, file=sys.stderr)\n    sys.exit(1)\n\nexcept requests.HTTPError as err:\n    # API errors: show user-friendly messages\n    logging.error(f\"API request failed: {err}\")\n    print(f\"Failed to connect to API: {err.response.status_code}\", file=sys.stderr)\n    sys.exit(1)\n\nexcept Exception as err:\n    # System errors: unhandled exceptions\n    # Exit code 2 indicates application errors\n    logging.exception(\"Unhandled error in component execution\")\n    traceback.print_exc(file=sys.stderr)\n    sys.exit(2)\n```\n\n**Logging Setup**: The `keboola.component` library automatically configures logging based on environment variables. Use standard Python logging:\n\n```python\nimport logging\n\nlogging.info(\"Starting data extraction\")\nlogging.warning(\"Rate limit approaching\")\nlogging.error(\"Failed to fetch data\")\nlogging.exception(\"Critical error with stack trace\")\n```\n\n## Developer Portal Registration\n\n### Prerequisites\n\n- Your Developer Portal account (requires email confirmation + 2FA)\n- Component dedicated Developer Portal service account credentials (just for the component, not personal credentials)\n- GitHub repository created\n\n### Registration Steps\n\n```bash\n# Component ID will be prefixed with vendor name\n# e.g., input 'my-component' â†’ returns 'keboola.my-component'\n```\n\n### CI/CD Configuration\n\nSet these secrets in your repository:\n- `KBC_DEVELOPERPORTAL_USERNAME` - service account username\n- `KBC_DEVELOPERPORTAL_PASSWORD` - service account password\n- `KBC_DEVELOPERPORTAL_APP` - full component ID with vendor prefix\n- `KBC_STORAGE_TOKEN` - for testing\n\n### Deployment\n\n- Tag releases with semantic versioning: `v1.0.0`, `v1.1.0`, etc.\n- GitHub Actions automatically builds Docker image\n- Pushes to AWS ECR (only supported registry)\n- Updates Developer Portal with new version\n- Propagates to all Keboola instances (up to 5 minutes)\n\n## Two-PR Workflow Strategy\n\nFor new component creation, use a structured approach:\n\n**When to use base PR only**: If the user request contains only keywords like:\n- \"kickoff\"\n- \"create component\"\n- \"initialize\"\n- \"setup\"\n- \"bootstrap\"\n\n**Steps**:\n1. **Base PR**: Create PR with cookiecutter-generated files only\n   - Establishes component structure\n   - Sets up CI/CD pipeline\n   - Gets component registered\n\n2. **Implementation PR** (if needed): Add custom logic\n   - Implement specific feature requirements\n   - Add custom validation\n   - Extend functionality\n\nThis prevents Claude Code from triggering CI/CD workflows prematurely during iterative development.\n\n## Testing Requirements\n\nAlways include comprehensive tests:\n\n```python\nimport unittest\nfrom src.component import Component\n\nclass TestComponent(unittest.TestCase):\n    def test_configuration_validation(self):\n        \"\"\"Test that required parameters are validated.\"\"\"\n        # Test implementation\n\n    def test_csv_processing(self):\n        \"\"\"Test CSV reading and writing with proper encoding.\"\"\"\n        # Test implementation\n\n    def test_state_management(self):\n        \"\"\"Test state file persistence.\"\"\"\n        # Test implementation\n\n    def test_error_handling(self):\n        \"\"\"Test proper error codes for different failure types.\"\"\"\n        # Test implementation\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n## Dockerfile Best Practices\n\nUse efficient Docker images:\n\n```dockerfile\nFROM python:3.11-alpine\n\n# Install dependencies\nWORKDIR /code\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy component code\nCOPY src/ /code/src/\n\n# Set entrypoint with unbuffered output\nENTRYPOINT [\"python\", \"-u\", \"/code/src/component.py\"]\n```\n\n**Key points**:\n- Prefer Alpine images for smaller size\n- Use `python -u` flag to disable output buffering\n- Install `keboola.component` package via pip\n- Set proper working directory\n\n## Related Documentation\n\n- [Code Quality Guidelines](code-quality.md)\n- [Workflow Patterns](workflow-patterns.md)\n- [Best Practices](best-practices.md)\n",
        "plugins/component-developer/skills/build-component/references/best-practices.md": "# Best Practices Reference\n\nQuick reference guide for common patterns and anti-patterns in Keboola component development.\n\n## âœ… DO\n\n### General Development\n- Use `CommonInterface` class for all Keboola interactions\n- Validate configuration early with `validate_configuration()`\n- Process CSV files with generators for memory efficiency\n- Always specify `encoding='utf-8'` for file operations\n- Use proper exit codes (1 for user errors, 2 for system errors)\n- Define explicit schemas for output tables\n- Implement state management for incremental processing\n- Write comprehensive tests\n- Use service account credentials for CI/CD\n- Follow semantic versioning for releases\n\n### Data Directory Management\n- **Remove cookiecutter example files and create component-specific `data/config.json`**\n- **Include realistic example parameters in `data/config.json` for local testing**\n- **Trust that Keboola platform creates all data directories**\n\n### Code Organization\n- **Keep `run()` method as orchestrator - extract complex logic to private methods**\n- **Use self-documenting method names that eliminate need for comments**\n- **Extract logic blocks > 10-15 lines into separate methods**\n\n### Code Quality\n- **Format all code with `ruff format .` before committing**\n- **Run `ruff check --fix .` to catch linting issues**\n- **Add proper type hints to all functions and variables**\n- **Import library-specific types (e.g., `MessageParam` from anthropic)**\n- **Check IDE for type warnings and fix them**\n- **Use `@staticmethod` decorator when method doesn't use `self`**\n\n## âŒ DON'T\n\n### General Development\n- Load entire CSV files into memory\n- Use personal credentials for deployment\n- Include 'extractor', 'writer', or 'application' in component names\n- Skip configuration validation\n- Forget to write manifests for output tables\n- Use exit code 0 for errors\n- Hard-code configuration values\n- Skip state file management for incremental loads\n- Forget to handle null characters in CSV files\n- Deploy without proper testing\n\n### Data Directory Management\n- **Leave cookiecutter example files (test.csv, order1.xml, .gitkeep) in `data/` directory**\n- **Forget to create `data/config.json` with example parameters for local testing**\n- **Delete the entire `data/` directory structure (keep empty folders + config.json)**\n- **Call `mkdir()` for platform-managed directories (in/, out/, tables/, files/)**\n\n### Code Organization\n- **Write monolithic `run()` methods with 100+ lines**\n- **Mix business logic with I/O operations in same method**\n- **Use comments to explain what code does (use method names instead)**\n\n### Code Quality\n- **Ignore IDE type warnings (red squiggles)**\n- **Use plain `dict` for API calls without type annotations**\n- **Skip type hints on function parameters**\n- **Ignore \"may be static\" warnings from IDE**\n\n## Common Patterns\n\n### CSV Processing Pattern\n```python\n# âœ… DO: Use generator for memory efficiency\nwith open(table.full_path, 'r', encoding='utf-8') as f:\n    lazy_lines = (line.replace('\\0', '') for line in f)\n    reader = csv.DictReader(lazy_lines, dialect='kbc')\n    for row in reader:\n        yield process_row(row)\n\n# âŒ DON'T: Load entire file into memory\nwith open(table.full_path, 'r') as f:\n    all_data = f.read()  # Bad!\n    rows = list(csv.reader(all_data.split('\\n')))\n```\n\n### Error Handling Pattern\n```python\n# âœ… DO: Use appropriate exit codes\nexcept ValueError as err:\n    logging.error(str(err))\n    sys.exit(1)  # User error\nexcept Exception as err:\n    logging.exception(\"Unhandled error\")\n    sys.exit(2)  # System error\n\n# âŒ DON'T: Use sys.exit(0) for errors\nexcept Exception as err:\n    print(f\"Error: {err}\")\n    sys.exit(0)  # Wrong! Indicates success\n```\n\n### Configuration Pattern\n```python\n# âœ… DO: Validate early\ndef run(self):\n    self.validate_configuration(['api_key', 'endpoint'])\n    params = self.configuration.parameters\n\n# âŒ DON'T: Skip validation\ndef run(self):\n    params = self.configuration.parameters  # No validation!\n```\n\n### Output Paths Pattern\n```python\n# âœ… DO: Use correct ComponentBase attributes\ndef _save_results(self, data: dict):\n    # For files (JSON, text, images, etc.)\n    output_path = self.files_out_path / \"data.json\"\n    with open(output_path, 'w') as f:\n        json.dump(data, f)\n\n    # For CSV tables\n    table = self.create_out_table_definition(\"output.csv\")\n    with open(table.full_path, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerows(data)\n\n# âŒ DON'T: Use non-existent data_path_out\ndef _save_results(self, data: dict):\n    output_path = self.data_path_out / \"data.json\"  # AttributeError!\n```\n\n### State Management Pattern\n```python\n# âœ… DO: Use state for incremental loads\nstate = self.get_state_file()\nlast_run = state.get('last_timestamp')\n# ... fetch only new data ...\nself.write_state_file({'last_timestamp': current_time})\n\n# âŒ DON'T: Forget state management\n# Always fetches all data, even for incremental loads\n```\n\n### Type Hints Pattern\n```python\n# âœ… DO: Use proper type hints\nfrom anthropic.types import MessageParam\n\ndef create_message(prompt: str) -> MessageParam:\n    message: MessageParam = {\"role\": \"user\", \"content\": prompt}\n    return message\n\n# âŒ DON'T: Skip type hints\ndef create_message(prompt):\n    return {\"role\": \"user\", \"content\": prompt}\n```\n\n### Method Organization Pattern\n```python\n# âœ… DO: Clear orchestration in run()\ndef run(self):\n    params = self._validate_configuration()\n    data = self._fetch_data(params)\n    results = self._transform_data(data)\n    self._save_results(results)\n\n# âŒ DON'T: Everything in run()\ndef run(self):\n    # 100+ lines of mixed concerns\n    # Hard to read, test, maintain\n```\n\n## Quick Checklist\n\nBefore committing code, verify:\n\n- [ ] All Python files formatted with `ruff format .`\n- [ ] All linting issues fixed with `ruff check --fix .`\n- [ ] Type hints added to all functions\n- [ ] No IDE type warnings (red squiggles)\n- [ ] `@staticmethod` added where appropriate\n- [ ] `run()` method is clean orchestrator (< 30 lines)\n- [ ] Complex logic extracted to private methods\n- [ ] CSV processing uses generators\n- [ ] Error handling uses proper exit codes\n- [ ] Configuration validated early\n- [ ] Output paths use `files_out_path` or `tables_out_path` (not `data_path_out`)\n- [ ] State management implemented (if incremental)\n- [ ] Tests written and passing\n- [ ] Documentation updated\n\n## Related Documentation\n\n- [Initialization Guide](initialization-guide.md) - Setting up new components\n- [Architecture Guide](architecture.md) - Component structure and patterns\n- [Code Quality](code-quality.md) - Ruff, type hints, @staticmethod\n- [Workflow Patterns](workflow-patterns.md) - Self-documenting code\n",
        "plugins/component-developer/skills/build-component/references/code-quality.md": "# Code Quality & Formatting\n\nComplete guide for code quality standards in Keboola Python components.\n\n## Using Ruff for Code Formatting\n\nAll Keboola components should use **Ruff** as the standard code formatter and linter. Ruff is included in the cookiecutter template by default.\n\n### Basic Usage\n\n**After writing or modifying code, always run:**\n\n```bash\n# Format code with ruff\nruff format .\n\n# Check and fix linting issues\nruff check --fix .\n```\n\n### Ruff Configuration\n\nConfiguration is included in `pyproject.toml`:\n\n```toml\n[tool.ruff]\nline-length = 120\ntarget-version = \"py313\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"W\", \"I\", \"N\", \"UP\"]\nignore = []\n```\n\n### Key Benefits\n\n- Consistent code style across all components\n- Automatic import sorting\n- Catches common errors before runtime\n- 10-100x faster than flake8 + black + isort combined\n\n### Integration with CI/CD\n\nThe cookiecutter template includes ruff checks in the CI/CD pipeline. Code must pass ruff formatting and linting before deployment.\n\n### Your Workflow\n\n1. Write/modify code\n2. Run `ruff format .` to format\n3. Run `ruff check --fix .` to lint\n4. Test your changes\n5. Commit formatted code\n\n**IMPORTANT**: Always format code with ruff before creating commits or pull requests. Unformatted code will fail CI/CD checks.\n\n## Type Hints and Type Safety\n\nAll Keboola components should use **proper type hints** to catch errors early and improve IDE support.\n\n### Critical Type Safety Rules\n\n#### 1. Import Correct Types from Libraries\n\n```python\n# âœ… CORRECT - Import proper types (Python 3.9+)\nfrom anthropic.types import MessageParam\nfrom keboola.component.dao import ColumnDefinition, BaseType\nfrom typing import Any  # Only import what's not available as built-in\n\n# âœ… Use proper type annotations with built-in generics\nuser_message: MessageParam = {\n    \"role\": \"user\",\n    \"content\": \"Your message here\"\n}\n\nmessages: list[MessageParam] = [user_message]\n```\n\n#### 2. Always Annotate Function Parameters and Return Types\n\n```python\n# âœ… CORRECT - Properly typed function (Python 3.9+ built-in generics)\ndef process_data(\n    input_file: Path,\n    config: dict[str, Any]\n) -> list[dict[str, str]]:\n    \"\"\"Process input file and return structured data.\"\"\"\n    results: list[dict[str, str]] = []\n    # ... implementation\n    return results\n\n# âŒ WRONG - No type hints\ndef process_data(input_file, config):\n    results = []\n    return results\n```\n\n#### 3. Use Library-Provided Types Instead of Generic Dicts\n\n```python\n# âœ… CORRECT - Using MessageParam type\nfrom anthropic.types import MessageParam\n\ndef create_message(prompt: str) -> MessageParam:\n    message: MessageParam = {\n        \"role\": \"user\",\n        \"content\": prompt\n    }\n    return message\n\n# âŒ WRONG - Plain dict without type annotation\ndef create_message(prompt):\n    return {\"role\": \"user\", \"content\": prompt}\n```\n\n### Common IDE Warnings and Fixes\n\n#### Warning: Type Mismatch\n\n**Problem:**\n```\nExpected type 'Iterable[MessageParam]', got 'list[dict[str, str]]' instead\n```\n\n**Solution:**\n```python\n# âŒ WRONG - IDE warning about type mismatch\nmessages = [{\"role\": \"user\", \"content\": \"hello\"}]\nclient.messages.create(messages=messages)\n\n# âœ… CORRECT - Explicit type annotation\nuser_msg: MessageParam = {\"role\": \"user\", \"content\": \"hello\"}\nmessages: list[MessageParam] = [user_msg]\nclient.messages.create(messages=messages)\n```\n\n### Optional Type Checking with mypy\n\nAdd to development workflow:\n\n```bash\n# Install mypy\npip install mypy\n\n# Run type checking\nmypy src/ --ignore-missing-imports\n```\n\nAdd to `pyproject.toml`:\n```toml\n[tool.mypy]\npython_version = \"3.13\"\nwarn_return_any = true\nwarn_unused_configs = true\nignore_missing_imports = true\n```\n\n### Type Hints Best Practices (Python 3.9+)\n\n- âœ… Import types from source libraries (e.g., `anthropic.types`, `keboola.component.dao`)\n- âœ… Annotate all function signatures\n- âœ… Use `T | None` for nullable values (not `Optional[T]`)\n- âœ… Use built-in generics: `list[T]`, `dict[K, V]` (not `List[T]`, `Dict[K, V]`)\n- âœ… Define types for API request/response objects\n- âŒ Don't use deprecated `typing.List`, `typing.Dict`, `typing.Optional`\n- âŒ Don't ignore type errors without understanding them\n- âŒ Don't use `Any` everywhere (defeats the purpose)\n\n## Using @staticmethod Decorator\n\nWhen IDE warns: `Method '_save_recommendations' may be 'static'`\n\n### When to Use @staticmethod\n\n```python\n# âŒ WRONG - Method doesn't use self but not marked static\nclass Component:\n    def _save_recommendations(self, data: dict[str, Any], path: Path):\n        \"\"\"Save recommendations - doesn't use self!\"\"\"\n        with open(path, \"w\") as f:\n            json.dump(data, f)\n\n# âœ… CORRECT - Mark as @staticmethod\nclass Component:\n    @staticmethod\n    def _save_recommendations(data: dict[str, Any], path: Path):\n        \"\"\"Save recommendations.\"\"\"\n        with open(path, \"w\") as f:\n            json.dump(data, f)\n```\n\n### When to Use @staticmethod\n\nUse `@staticmethod` when:\n- Method doesn't access `self` or `cls`\n- Method is a utility function that belongs to the class conceptually\n- IDE shows \"Method may be 'static'\" warning\n\n### When NOT to Use @staticmethod\n\nDon't use `@staticmethod` when:\n- Method needs access to instance attributes (`self.something`)\n- Method needs to call other instance methods\n- Method modifies instance state\n\n## Complete Code Quality Workflow\n\nAfter implementing any Python code:\n\n1. **Add proper type hints** to all functions and variables\n2. **Check IDE for type warnings** (red squiggles) and fix them\n3. **Import library-specific types** where needed (e.g., `MessageParam`)\n4. **Add `@staticmethod` decorator** for methods that don't use `self`\n5. Run `ruff format .` to ensure consistent formatting\n6. Run `ruff check --fix .` to catch and fix linting issues\n7. Optionally run `mypy src/` for additional type checking\n8. Review the changes to ensure quality\n9. Test the component functionality\n\n## Critical Reminders\n\n**CRITICAL**: Always check IDE warnings and fix them before committing:\n- Type warnings often indicate real bugs\n- \"May be static\" warnings improve code clarity and testability\n- Unformatted code will fail CI/CD checks\n\n## Related Documentation\n\n- [Architecture Guide](architecture.md)\n- [Workflow Patterns](workflow-patterns.md)\n- [Best Practices](best-practices.md)\n",
        "plugins/component-developer/skills/build-component/references/developer-portal.md": "# Developer Portal Registration\n\nComplete guide for registering Keboola components in the Developer Portal via API.\n\n## Overview\n\nAll Keboola components must be registered in the Developer Portal before they can be deployed and used. This guide covers the programmatic registration process using curl commands.\n\n**IMPORTANT**: Always use curl commands for registration. Never use a web browser for this process.\n\n## Prerequisites\n\nBefore registering a component:\n\n1. **Repository Setup**: Create a GitHub repository with branch `main` (not prefixed with `devin/` or similar)\n2. **Initial Commit**: Commit at least a simple README.md to the repository\n3. **Credentials**: Have Developer Portal credentials ready:\n   - Username (e.g., `CF_DEVELOPER_PORTAL_DEVIN_USERNAME`)\n   - Password (e.g., `CF_DEVELOPER_PORTAL_DEVIN_PASSWORD`)\n\n## API Endpoints\n\nThe Developer Portal API is available at: `https://apps-api.keboola.com`\n\n### Authentication\n\nFirst, obtain an authentication token:\n\n```bash\n# Login to get authentication token\ncurl -X POST \"https://apps-api.keboola.com/auth/login\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"email\": \"'$CF_DEVELOPER_PORTAL_DEVIN_USERNAME'\",\n    \"password\": \"'$CF_DEVELOPER_PORTAL_DEVIN_PASSWORD'\"\n  }'\n```\n\nThe response contains a `token` field valid for 1 hour. Use this token in subsequent requests.\n\n### Register New Component\n\n```bash\n# Create new app/component\ncurl -X POST \"https://apps-api.keboola.com/vendors/{vendor}/apps\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: {token}\" \\\n  -d '{\n    \"id\": \"component-name\",\n    \"name\": \"Component Display Name\",\n    \"type\": \"extractor\",\n    \"shortDescription\": \"Brief description of the component\",\n    \"longDescription\": \"Detailed markdown description\",\n    \"repository\": {\n      \"type\": \"github\",\n      \"uri\": \"https://github.com/keboola/component-name\"\n    }\n  }'\n```\n\n## Component ID Format\n\n- Component ID is prefixed with vendor name automatically\n- Input: `my-component` â†’ Output: `keboola.my-component`\n- **Never include** words like 'extractor', 'writer', or 'application' in the component name itself\n- Use lowercase letters, numbers, and hyphens only\n- Length should be between 3 and 30 characters\n\n## Component Types\n\nValid component types:\n- `extractor` - Pulls data from external sources into Keboola\n- `writer` - Pushes data from Keboola to external destinations\n- `application` - Processes data within Keboola\n- `processor` - Transforms data in pipelines\n\n## Registration Workflow\n\n### Step 1: Prepare Repository\n\n```bash\n# Create and checkout main branch\ngit checkout -b main\n\n# Create initial README\necho \"# component-name\" > README.md\ngit add README.md\ngit commit -m \"Initial commit\"\ngit push -u origin main\n```\n\n### Step 2: Get Authentication Token\n\n```bash\n# Store credentials (use environment variables)\nexport PORTAL_USER=\"$CF_DEVELOPER_PORTAL_DEVIN_USERNAME\"\nexport PORTAL_PASS=\"$CF_DEVELOPER_PORTAL_DEVIN_PASSWORD\"\n\n# Get token\nTOKEN=$(curl -s -X POST \"https://apps-api.keboola.com/auth/login\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"email\\\": \\\"$PORTAL_USER\\\", \\\"password\\\": \\\"$PORTAL_PASS\\\"}\" \\\n  | jq -r '.token')\n\necho \"Token obtained: ${TOKEN:0:20}...\"\n```\n\n### Step 3: Register Component\n\n```bash\n# Register the component\ncurl -X POST \"https://apps-api.keboola.com/vendors/keboola/apps\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: $TOKEN\" \\\n  -d '{\n    \"id\": \"ex-my-api\",\n    \"name\": \"My API Extractor\",\n    \"type\": \"extractor\",\n    \"shortDescription\": \"Extracts data from My API\",\n    \"longDescription\": \"# My API Extractor\\n\\nThis component extracts data from My API service.\",\n    \"repository\": {\n      \"type\": \"github\",\n      \"uri\": \"https://github.com/keboola/ex-my-api\"\n    }\n  }'\n```\n\n### Step 4: Verify Registration\n\n```bash\n# Get component details\ncurl -X GET \"https://apps-api.keboola.com/vendors/keboola/apps/ex-my-api\" \\\n  -H \"Authorization: $TOKEN\"\n```\n\n## Required Fields\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `id` | string | Component identifier (3-30 chars, lowercase, hyphens allowed) |\n| `name` | string | Display name (no 'extractor', 'writer', 'application' suffix) |\n| `type` | object | Component type with `id` field |\n| `shortDescription` | string | One sentence description |\n| `longDescription` | string | Markdown formatted detailed description |\n| `repository` | object | GitHub repository information |\n\n## After Registration\n\nOnce registered, proceed with:\n\n1. **Generate Component Structure**: Use cookiecutter template\n   ```bash\n   cookiecutter gh:keboola/cookiecutter-python-component\n   ```\n\n2. **Configure CI/CD Secrets** (as the LAST step):\n   - `KBC_DEVELOPERPORTAL_USERNAME`\n   - `KBC_DEVELOPERPORTAL_PASSWORD`\n   - `KBC_DEVELOPERPORTAL_APP` - full component ID with vendor prefix\n   - `KBC_STORAGE_TOKEN` - for testing\n\n3. **Deploy**: Tag releases with semantic versioning (`v1.0.0`)\n\n## Common Issues\n\n### Token Expired\nTokens are valid for 1 hour. If you get authentication errors, obtain a new token.\n\n### Component Already Exists\nIf the component ID is taken, choose a different ID or check if you're updating an existing component.\n\n### Invalid Component Name\nEnsure the name doesn't contain restricted words ('extractor', 'writer', 'application') and follows the character restrictions.\n\n## Related Guides\n\n- [Initialization Guide](initialization-guide.md) - Setting up component structure\n- [Architecture Guide](architecture.md) - Component patterns and best practices\n- [Debugging Guide](debugging.md) - Troubleshooting component issues\n",
        "plugins/component-developer/skills/build-component/references/running-and-testing.md": "# Running and Testing Keboola Components\n\nThis guide covers all methods for running, testing, and debugging Keboola components locally and in production environments.\n\n## Table of Contents\n\n1. [Component Execution Model](#component-execution-model)\n2. [Local Development Setup](#local-development-setup)\n3. [Running Components Locally](#running-components-locally)\n4. [Testing Strategies](#testing-strategies)\n5. [Data Folder Structure](#data-folder-structure)\n6. [Docker Execution](#docker-execution)\n7. [Debugging Techniques](#debugging-techniques)\n8. [Production Deployment](#production-deployment)\n\n---\n\n## Component Execution Model\n\n### How Keboola Executes Components\n\nWhen a user runs a component in Keboola Connection:\n\n```\nUser clicks \"Run\" in UI\n    â†“\nKeboola creates JOB in queue\n    â†“\nJob Runner pulls Docker image from ECR\n    â†“\nPrepares data folder structure:\n  - /data/config.json         (user configuration)\n  - /data/in/tables/          (input CSV files + manifests)\n  - /data/in/files/           (input files)\n  - /data/in/state.json       (previous state for incremental)\n    â†“\nStarts isolated Docker container with mounted /data\n    â†“\nComponent reads from /data, processes, writes to /data/out\n    â†“\nKeboola imports outputs from /data/out to Storage\n    â†“\nJob completes (success/error/timeout)\n```\n\n### Data Folder Contract\n\nComponents communicate with Keboola **exclusively** through the filesystem at `/data`:\n\n**INPUT** (read-only):\n- `config.json` - Component configuration from UI\n- `in/tables/*.csv` - Input tables with `.manifest` files\n- `in/files/*` - Input files\n- `in/state.json` - Previous run state (for incremental processing)\n\n**OUTPUT** (write):\n- `out/tables/*.csv` - Output tables with `.manifest` files\n- `out/files/*` - Output files\n- `out/state.json` - New state for next run\n\n---\n\n## Local Development Setup\n\n### 1. Project Structure\n\n```\nmy-component/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ component.py          # Main logic\nâ”‚   â””â”€â”€ configuration.py      # Config validation\nâ”œâ”€â”€ data/                     # Local data folder (gitignored)\nâ”‚   â”œâ”€â”€ config.json\nâ”‚   â”œâ”€â”€ in/\nâ”‚   â””â”€â”€ out/\nâ”œâ”€â”€ tests/\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ docker-compose.yml\nâ””â”€â”€ pyproject.toml\n```\n\n### 2. Create Data Folder Structure\n\n```bash\n# Create required directories\nmkdir -p data/in/tables data/in/files data/out/tables data/out/files\n\n# Gitignore data folder (except structure)\ncat > data/.gitignore <<EOF\n*\n!.gitignore\nEOF\n```\n\n### 3. Prepare Configuration\n\nCreate `data/config.json` with your component's parameters:\n\n```json\n{\n  \"parameters\": {\n    \"api_key\": \"your_key_here\",\n    \"#password\": \"encrypted_password\",\n    \"from_date\": \"2024-01-01\",\n    \"tables\": [\"users\", \"orders\"],\n    \"incremental\": false,\n    \"debug\": true\n  }\n}\n```\n\n**Note**: Parameters starting with `#` are encrypted in Keboola but work as plain text locally.\n\n---\n\n## Running Components Locally\n\n### Method 1: Direct Python Execution (Fastest for Development)\n\n```bash\n# Set up virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # or `.venv\\Scripts\\activate` on Windows\npip install -e .\n\n# Set data directory environment variable\nexport KBC_DATADIR=./data\n\n# Run component\npython src/component.py\n```\n\n**Pros**: Fast iteration, easy debugging with IDE\n**Cons**: Requires local Python environment, may behave differently than Docker\n\n### Method 2: Docker Compose (Recommended)\n\nAdd `docker-compose.yml`:\n\n```yaml\nservices:\n  dev:\n    build: .\n    volumes:\n      - ./:/code\n      - ./data:/data\n    environment:\n      - KBC_DATADIR=/data\n    command: python -u src/component.py\n\n  test:\n    build: .\n    volumes:\n      - ./:/code\n    command: /bin/sh /code/scripts/build_n_test.sh\n```\n\nRun:\n\n```bash\n# Build and run component\ndocker-compose run --rm dev\n\n# Run tests\ndocker-compose run --rm test\n```\n\n**Pros**: Matches production environment, isolated\n**Cons**: Slower rebuild times\n\n### Method 3: Docker CLI (Production-like)\n\n```bash\n# Build image\ndocker build -t my-component:latest .\n\n# Run with mounted data folder\ndocker run --rm \\\n  -v $(pwd)/data:/data \\\n  -e KBC_DATADIR=/data \\\n  my-component:latest\n```\n\n### Method 4: Using Keboola CLI (Official Tool)\n\n```bash\n# Install Keboola CLI\ncurl -L https://cli.keboola.com/install.sh | bash\n\n# Initialize component\nkbc init my-component\n\n# Run locally\nkbc run\n\n# Test configuration\nkbc validate config.json\n```\n\n---\n\n## Testing Strategies\n\n### 1. Unit Tests\n\nTest individual functions in isolation:\n\n```python\n# tests/test_configuration.py\nimport unittest\nfrom src.configuration import Configuration\n\nclass TestConfiguration(unittest.TestCase):\n    def test_valid_config(self):\n        config = Configuration(\n            api_key=\"test_key\",\n            from_date=\"2024-01-01\"\n        )\n        self.assertEqual(config.api_key, \"test_key\")\n\n    def test_date_parsing(self):\n        config = Configuration(from_date=\"-7\")\n        self.assertTrue(config.get_date_from() < datetime.now())\n```\n\nRun tests:\n\n```bash\n# Using unittest\npython -m unittest discover -s tests\n\n# Using pytest\npytest tests/ -v\n\n# With coverage\npytest --cov=src tests/\n```\n\n### 2. Integration Tests\n\nTest complete component execution with sample data:\n\n```python\n# tests/test_integration.py\nimport os\nimport tempfile\nfrom pathlib import Path\nfrom src.component import Component\n\nclass TestIntegration(unittest.TestCase):\n    def setUp(self):\n        # Create temporary data folder\n        self.temp_dir = tempfile.mkdtemp()\n        os.environ['KBC_DATADIR'] = self.temp_dir\n\n        # Create config\n        config_path = Path(self.temp_dir) / 'config.json'\n        config_path.write_text(json.dumps({\n            \"parameters\": {\"api_key\": \"test_key\"}\n        }))\n\n    def test_full_extraction(self):\n        component = Component()\n        component.run()\n\n        # Verify output files exist\n        output_file = Path(self.temp_dir) / 'out' / 'tables' / 'users.csv'\n        self.assertTrue(output_file.exists())\n```\n\n### 3. Docker Testing\n\nTest with actual Docker container:\n\n```bash\n# Create test script\ncat > scripts/build_n_test.sh <<'EOF'\n#!/bin/sh\nset -e\n\n# Run code quality checks\nflake8 --config=flake8.cfg src/\nruff check src/\n\n# Run unit tests\npython -m unittest discover -s tests\n\n# Run integration tests\npytest tests/integration/ -v\n\necho \"âœ… All tests passed!\"\nEOF\n\nchmod +x scripts/build_n_test.sh\n\n# Run in Docker\ndocker-compose run --rm test\n```\n\n### 4. End-to-End Testing with Sample Data\n\n```bash\n# Prepare sample data\nmkdir -p data/in/tables\ncat > data/in/tables/input.csv <<EOF\nid,name,email\n1,John Doe,john@example.com\n2,Jane Smith,jane@example.com\nEOF\n\n# Create manifest\ncat > data/in/tables/input.csv.manifest <<EOF\n{\n  \"columns\": [\"id\", \"name\", \"email\"],\n  \"primary_key\": [\"id\"]\n}\nEOF\n\n# Run component\nexport KBC_DATADIR=./data\npython src/component.py\n\n# Verify output\ncat data/out/tables/output.csv\ncat data/out/tables/output.csv.manifest\n```\n\n---\n\n## Data Folder Structure\n\n### Complete Example\n\n```\ndata/\nâ”œâ”€â”€ config.json                    # Component configuration\nâ”œâ”€â”€ in/\nâ”‚   â”œâ”€â”€ state.json                 # Previous state (for incremental)\nâ”‚   â”œâ”€â”€ tables/\nâ”‚   â”‚   â”œâ”€â”€ users.csv              # Input table data\nâ”‚   â”‚   â””â”€â”€ users.csv.manifest     # Input table metadata\nâ”‚   â””â”€â”€ files/\nâ”‚       â””â”€â”€ import.xml             # Input file\nâ””â”€â”€ out/\n    â”œâ”€â”€ state.json                 # New state to save\n    â”œâ”€â”€ tables/\n    â”‚   â”œâ”€â”€ output.csv             # Output table data\n    â”‚   â””â”€â”€ output.csv.manifest    # Output table metadata\n    â””â”€â”€ files/\n        â””â”€â”€ report.pdf             # Output file\n```\n\n### config.json Structure\n\n```json\n{\n  \"parameters\": {\n    \"api_key\": \"abc123\",\n    \"#password\": \"secret\",\n    \"tables\": [\"users\", \"orders\"],\n    \"incremental\": true\n  },\n  \"image_parameters\": {\n    \"custom_field\": \"value\"\n  },\n  \"authorization\": {\n    \"oauth_api\": {\n      \"id\": \"12345\",\n      \"credentials\": {\n        \"access_token\": \"token123\"\n      }\n    }\n  },\n  \"action\": \"run\"\n}\n```\n\n### Table Manifest Format\n\n**Output manifest** (`out/tables/users.csv.manifest`):\n\n```json\n{\n  \"columns\": [\"id\", \"name\", \"email\", \"created_at\"],\n  \"primary_key\": [\"id\"],\n  \"incremental\": true,\n  \"delimiter\": \",\",\n  \"enclosure\": \"\\\"\"\n}\n```\n\n### State File Format\n\n**For incremental loading** (`out/state.json`):\n\n```json\n{\n  \"lastRunTimestamp\": \"2024-11-28T15:00:00+00:00\",\n  \"lastProcessedId\": \"12345\",\n  \"component_state\": {\n    \"users_table\": {\n      \"last_sync\": \"2024-11-28\"\n    }\n  }\n}\n```\n\n---\n\n## Docker Execution\n\n### Dockerfile Best Practices\n\n```dockerfile\nFROM python:3.13-slim\n\n# Copy uv for fast dependency installation\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/\n\nWORKDIR /code/\n\n# Install dependencies first (better caching)\nCOPY pyproject.toml uv.lock ./\nENV UV_PROJECT_ENVIRONMENT=\"/usr/local/\"\nRUN uv sync --frozen\n\n# Copy source code\nCOPY src/ src/\nCOPY tests/ tests/\n\n# Set entrypoint\nCMD [\"python\", \"-u\", \"src/component.py\"]\n```\n\n### Build and Tag\n\n```bash\n# Build with version tag\ndocker build -t keboola/my-component:1.0.0 .\ndocker build -t keboola/my-component:latest .\n\n# Multi-platform build\ndocker buildx build --platform linux/amd64,linux/arm64 -t my-component .\n```\n\n### Test Docker Image\n\n```bash\n# Run with test data\ndocker run --rm \\\n  -v $(pwd)/data:/data \\\n  -e KBC_DATADIR=/data \\\n  -e KBC_PROJECTID=12345 \\\n  -e KBC_STACKID=connection.keboola.com \\\n  my-component:latest\n\n# Interactive debugging\ndocker run --rm -it \\\n  -v $(pwd)/data:/data \\\n  -e KBC_DATADIR=/data \\\n  --entrypoint /bin/bash \\\n  my-component:latest\n```\n\n---\n\n## Debugging Techniques\n\n### 1. Enable Debug Logging\n\n```python\n# src/component.py\nimport logging\n\nclass Component(ComponentBase):\n    def __init__(self):\n        super().__init__()\n        if self.configuration.parameters.get('debug', False):\n            logging.getLogger().setLevel(logging.DEBUG)\n\n    def run(self):\n        logging.info(\"Starting extraction\")\n        logging.debug(f\"Config: {self.configuration.parameters}\")\n```\n\n### 2. Interactive Debugging with IDE\n\n**VS Code** (`launch.json`):\n\n```json\n{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Debug Component\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${workspaceFolder}/src/component.py\",\n      \"env\": {\n        \"KBC_DATADIR\": \"${workspaceFolder}/data\"\n      },\n      \"console\": \"integratedTerminal\"\n    }\n  ]\n}\n```\n\n**PyCharm**:\n1. Right-click `src/component.py` â†’ Debug\n2. Add environment variable: `KBC_DATADIR=./data`\n\n### 3. Docker Debugging\n\n```bash\n# Run with live code reload\ndocker run --rm -it \\\n  -v $(pwd):/code \\\n  -v $(pwd)/data:/data \\\n  -e KBC_DATADIR=/data \\\n  --entrypoint python \\\n  my-component:latest -u src/component.py\n\n# Attach to running container\ndocker ps  # Get container ID\ndocker exec -it <container_id> /bin/bash\n```\n\n### 4. Remote Debugging in Docker\n\n```python\n# Add to component.py for remote debugging\nimport debugpy\nif os.getenv('DEBUG_ENABLED'):\n    debugpy.listen((\"0.0.0.0\", 5678))\n    debugpy.wait_for_client()\n```\n\n```bash\n# Run with debug port exposed\ndocker run --rm \\\n  -p 5678:5678 \\\n  -v $(pwd)/data:/data \\\n  -e KBC_DATADIR=/data \\\n  -e DEBUG_ENABLED=1 \\\n  my-component:latest\n```\n\n### 5. Logging Best Practices\n\n```python\n# Log important events\nlogging.info(f\"Processing table {table_name}\")\nlogging.info(f\"Extracted {len(data)} rows\")\n\n# Log errors with context\ntry:\n    result = api.fetch_data()\nexcept Exception as e:\n    logging.error(f\"Failed to fetch data: {e}\", exc_info=True)\n    raise UserException(f\"API request failed: {e}\")\n\n# Debug verbose information\nlogging.debug(f\"API response: {response.text[:500]}\")\n```\n\n---\n\n## Production Deployment\n\n### 1. Deployment Workflow\n\n```bash\n# 1. Build and test locally\ndocker build -t my-component:1.0.0 .\ndocker-compose run --rm test\n\n# 2. Push to Keboola ECR (via deploy script)\n./deploy.sh\n\n# 3. Update version in Developer Portal\n# This is done automatically by deploy.sh\n```\n\n### 2. Deploy Script Example\n\n```bash\n#!/bin/sh\n# deploy.sh\nset -e\n\n# Get version tag\nTAG=${GITHUB_TAG:-$TRAVIS_TAG}\necho \"Deploying version: $TAG\"\n\n# Get ECR repository\nREPOSITORY=$(docker run --rm \\\n  -e KBC_DEVELOPERPORTAL_USERNAME \\\n  -e KBC_DEVELOPERPORTAL_PASSWORD \\\n  quay.io/keboola/developer-portal-cli-v2:latest \\\n  ecr:get-repository ${VENDOR} ${APP})\n\n# Login to ECR\neval $(docker run --rm \\\n  -e KBC_DEVELOPERPORTAL_USERNAME \\\n  -e KBC_DEVELOPERPORTAL_PASSWORD \\\n  quay.io/keboola/developer-portal-cli-v2:latest \\\n  ecr:get-login ${VENDOR} ${APP})\n\n# Push image\ndocker tag my-component:latest ${REPOSITORY}:${TAG}\ndocker push ${REPOSITORY}:${TAG}\n\n# Update Developer Portal\ndocker run --rm \\\n  -e KBC_DEVELOPERPORTAL_USERNAME \\\n  -e KBC_DEVELOPERPORTAL_PASSWORD \\\n  quay.io/keboola/developer-portal-cli-v2:latest \\\n  update-app-repository ${VENDOR} ${APP} ${TAG} ecr ${REPOSITORY}\n```\n\n### 3. CI/CD with GitHub Actions\n\n```yaml\n# .github/workflows/push.yml\nname: Build and Deploy\n\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Build Docker image\n        run: docker build -t my-component:${{ github.ref_name }} .\n\n      - name: Run tests\n        run: docker-compose run --rm test\n\n      - name: Deploy to Keboola\n        env:\n          KBC_DEVELOPERPORTAL_USERNAME: ${{ secrets.KBC_USERNAME }}\n          KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_PASSWORD }}\n        run: ./deploy.sh\n```\n\n### 4. Version Management\n\nFollow semantic versioning:\n\n- **v1.0.0** - Major release (breaking changes)\n- **v1.1.0** - Minor release (new features)\n- **v1.0.1** - Patch release (bug fixes)\n\n```bash\n# Tag and push\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\ngit push origin v1.0.0\n```\n\n---\n\n## Troubleshooting Common Issues\n\n### Issue: Component runs locally but fails in Keboola\n\n**Causes**:\n- Path issues (absolute vs relative)\n- Environment variables missing\n- Different Python/package versions\n\n**Solution**:\n```bash\n# Test with exact Keboola environment\ndocker run --rm \\\n  -v $(pwd)/data:/data \\\n  -e KBC_DATADIR=/data \\\n  -e KBC_PROJECTID=12345 \\\n  my-component:latest\n```\n\n### Issue: \"No such file or directory\" errors\n\n**Solution**: Always use `Path` from `pathlib`:\n\n```python\nfrom pathlib import Path\n\n# Wrong\nwith open('data/config.json') as f:\n\n# Correct\nconfig_path = Path(os.getenv('KBC_DATADIR', './data')) / 'config.json'\nwith open(config_path) as f:\n```\n\n### Issue: Memory errors with large datasets\n\n**Solution**: Process in chunks:\n\n```python\n# Don't load entire file into memory\nfor chunk in pd.read_csv(input_file, chunksize=10000):\n    process_chunk(chunk)\n    chunk.to_csv(output_file, mode='a', header=False, index=False)\n```\n\n### Issue: Timeout in Keboola\n\n**Solution**:\n- Optimize data processing\n- Add progress logging\n- Request timeout extension in Developer Portal\n\n---\n\n## Quick Reference\n\n### Environment Variables\n\n```bash\nKBC_DATADIR        # Path to data folder (required)\nKBC_PROJECTID      # Keboola project ID\nKBC_STACKID        # Keboola stack (e.g., connection.keboola.com)\nKBC_CONFIGID       # Configuration ID\nKBC_RUNID          # Job run ID\n```\n\n### Common Commands\n\n```bash\n# Local development\nexport KBC_DATADIR=./data\npython src/component.py\n\n# Docker\ndocker-compose run --rm dev\ndocker-compose run --rm test\n\n# Testing\npython -m unittest discover -s tests\npytest tests/ -v --cov=src\n\n# Build & Deploy\ndocker build -t my-component .\n./deploy.sh\n```\n\n---\n\n## Additional Resources\n\n- [Keboola Component Specification](https://developers.keboola.com/extend/component/)\n- [Python Component Template](https://github.com/keboola/python-component-template)\n- [Developer Portal Documentation](https://developers.keboola.com/extend/developer-portal/)\n- [Common Interface Specification](https://developers.keboola.com/extend/common-interface/)\n\n---\n\n**Last Updated**: 2024-11-28\n",
        "plugins/component-developer/skills/build-component/references/workflow-patterns.md": "# Self-Documenting Workflow Pattern\n\n**CRITICAL**: The `run()` method should be a clear, readable \"table of contents\" that orchestrates the component workflow. Extract complex logic into well-named private methods.\n\n## Anti-Pattern vs Best Practice\n\n### âŒ ANTI-PATTERN - Monolithic run() Method\n\n```python\ndef run(self):\n    \"\"\"Main execution code\"\"\"\n    try:\n        # Everything mixed together in 200+ lines\n        self.validate_configuration(['api_key', 'endpoint'])\n        params = self.configuration.parameters\n\n        # Authentication logic inline\n        import requests\n        session = requests.Session()\n        auth_response = session.post(\n            f\"{params['endpoint']}/auth\",\n            json={'api_key': params['api_key']}\n        )\n        if auth_response.status_code != 200:\n            raise ValueError(\"Auth failed\")\n        token = auth_response.json()['token']\n        session.headers.update({'Authorization': f'Bearer {token}'})\n\n        # Load state inline\n        state = self.get_state_file()\n        last_id = state.get('last_id', 0)\n\n        # Fetch data with pagination - all inline\n        all_records = []\n        page = 1\n        while True:\n            response = session.get(\n                f\"{params['endpoint']}/data\",\n                params={'page': page, 'since_id': last_id}\n            )\n            if response.status_code != 200:\n                logging.error(f\"Failed page {page}\")\n                break\n            data = response.json()\n            if not data['records']:\n                break\n\n            # Transform data inline\n            for record in data['records']:\n                transformed = {\n                    'id': record['id'],\n                    'name': record['name'].upper(),\n                    'value': float(record['value']) * 1.2,\n                    'date': datetime.strptime(record['date'], '%Y-%m-%d').isoformat(),\n                    'category': record.get('category', 'unknown'),\n                }\n                all_records.append(transformed)\n\n            page += 1\n            if page > 100:  # Safety limit\n                break\n\n        # Save output inline\n        import csv\n        out_path = f\"{self.data_folder_path}/out/tables/output.csv\"\n        with open(out_path, 'w', newline='') as f:\n            if all_records:\n                writer = csv.DictWriter(f, fieldnames=all_records[0].keys())\n                writer.writeheader()\n                writer.writerows(all_records)\n\n        # Write manifest inline\n        manifest = {\n            'destination': 'out.c-main.data',\n            'incremental': True,\n        }\n        import json\n        with open(f\"{out_path}.manifest\", 'w') as f:\n            json.dump(manifest, f)\n\n        # Update state inline\n        if all_records:\n            max_id = max(r['id'] for r in all_records)\n            self.write_state_file({'last_id': max_id})\n\n    except Exception as err:\n        logging.error(str(err))\n        sys.exit(2)\n```\n\n**Problems:**\n- âŒ 80+ lines of mixed concerns in one method\n- âŒ Impossible to understand the workflow at a glance\n- âŒ Cannot test authentication, pagination, or transformation separately\n- âŒ Hard to debug which step failed\n- âŒ Difficult to modify one part without affecting others\n- âŒ No clear indication of what the component does\n\n### âœ… BEST PRACTICE - Self-Documenting Workflow\n\n```python\ndef run(self):\n    \"\"\"Main execution - orchestrates the component workflow.\"\"\"\n    try:\n        # Clear, readable workflow - acts as \"table of contents\"\n        params = self._validate_and_get_configuration()\n        state = self._load_previous_state()\n\n        input_data = self._process_input_tables()\n        results = self._perform_business_logic(input_data, params, state)\n\n        self._save_output_tables(results)\n        self._update_state(results)\n\n    except ValueError as err:\n        logging.error(str(err))\n        print(err, file=sys.stderr)\n        sys.exit(1)\n    except Exception as err:\n        logging.exception(\"Unhandled error occurred\")\n        traceback.print_exc(file=sys.stderr)\n        sys.exit(2)\n\ndef _validate_and_get_configuration(self) -> Configuration:\n    \"\"\"Validate configuration and return typed parameters.\"\"\"\n    self.validate_configuration(REQUIRED_PARAMETERS)\n    return Configuration(**self.configuration.parameters)\n\ndef _load_previous_state(self) -> Dict[str, Any]:\n    \"\"\"Load state from previous run for incremental processing.\"\"\"\n    return self.get_state_file()\n\ndef _process_input_tables(self) -> List[Dict[str, Any]]:\n    \"\"\"Process all input tables with proper CSV handling.\"\"\"\n    input_tables = self.get_input_tables_definitions()\n    all_data = []\n\n    for table in input_tables:\n        table_data = self._process_single_table(table)\n        all_data.extend(table_data)\n\n    return all_data\n\ndef _process_single_table(self, table_def) -> List[Dict[str, Any]]:\n    \"\"\"Process individual table with null character handling.\"\"\"\n    with open(table_def.full_path, 'r', encoding='utf-8') as f:\n        lazy_lines = (line.replace('\\0', '') for line in f)\n        reader = csv.DictReader(lazy_lines, dialect='kbc')\n        return [self._transform_row(row) for row in reader]\n\n@staticmethod\ndef _transform_row(row: Dict[str, str]) -> Dict[str, Any]:\n    \"\"\"Transform single row of data.\"\"\"\n    # Transformation logic here\n    return transformed_row\n\ndef _perform_business_logic(\n    self,\n    data: List[Dict[str, Any]],\n    params: Configuration,\n    state: Dict[str, Any]\n) -> ProcessedResults:\n    \"\"\"Core business logic - extract/transform/process data.\"\"\"\n    # Main processing logic here\n    return results\n\ndef _save_output_tables(self, results: ProcessedResults):\n    \"\"\"Write results to output tables with manifests.\"\"\"\n    out_table = self.create_out_table_definition(\n        name=\"output.csv\",\n        destination=\"out.c-data.output\",\n        schema=self._get_output_schema(),\n        incremental=True\n    )\n\n    with open(out_table.full_path, 'w', encoding='utf-8', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=out_table.column_names)\n        writer.writeheader()\n        writer.writerows(results.data)\n\n    self.write_manifest(out_table)\n\ndef _update_state(self, results: ProcessedResults):\n    \"\"\"Save state for next incremental run.\"\"\"\n    self.write_state_file({\n        'last_timestamp': results.last_timestamp,\n        'records_processed': results.count,\n        'last_run_stats': results.stats\n    })\n\n@staticmethod\ndef _get_output_schema() -> OrderedDict:\n    \"\"\"Define output table schema.\"\"\"\n    from collections import OrderedDict\n    from keboola.component.dao import ColumnDefinition, BaseType\n\n    return OrderedDict({\n        \"id\": ColumnDefinition(data_types=BaseType.integer(), primary_key=True),\n        \"name\": ColumnDefinition(),\n        \"value\": ColumnDefinition(data_types=BaseType.numeric(length=\"10,2\"))\n    })\n```\n\n**Benefits:**\n- âœ… `run()` reads like a story - clear workflow at a glance\n- âœ… Each method has single responsibility\n- âœ… Easy to test individual steps\n- âœ… Method names eliminate need for comments\n- âœ… Proper type hints on each method\n- âœ… Reusable helper methods (e.g., `_transform_row`)\n- âœ… `@staticmethod` for utility functions\n- âœ… Easy to maintain and extend\n\n## Key Principles\n\n1. **run() as Orchestrator**: Coordinates workflow, delegates to specialized methods\n2. **One Method, One Purpose**: Each private method does exactly one thing\n3. **Self-Documenting Names**: Method names clearly describe what they do\n4. **Progressive Complexity**: Start high-level, drill down into details as needed\n5. **Type Hints Everywhere**: Clear contracts between methods\n6. **Static When Possible**: Mark utility methods as `@staticmethod`\n\n## When to Extract Methods\n\n### Extract to Separate Method If:\n\n- âœ… Logic block is > 10-15 lines\n- âœ… Block has clear single purpose\n- âœ… You need a comment to explain what it does\n- âœ… Logic could be reused elsewhere\n- âœ… Logic could be tested independently\n\n### Keep Inline If:\n\n- âŒ Only 2-3 lines of simple code\n- âŒ Used only once and tightly coupled\n- âŒ Would create method with too many parameters\n\n## Real-World Example\n\nHere's a complete example of a well-structured component:\n\n```python\nfrom typing import Dict, List, Any\nfrom pathlib import Path\nfrom dataclasses import dataclass\n\n@dataclass\nclass ProcessedResults:\n    \"\"\"Container for processing results.\"\"\"\n    data: List[Dict[str, Any]]\n    count: int\n    last_timestamp: str\n    stats: Dict[str, Any]\n\nclass Component(CommonInterface):\n    \"\"\"Well-structured component with self-documenting workflow.\"\"\"\n\n    def run(self):\n        \"\"\"Main execution - orchestrates the component workflow.\"\"\"\n        try:\n            params = self._validate_and_get_configuration()\n            state = self._load_previous_state()\n\n            input_data = self._process_input_tables()\n            results = self._perform_business_logic(input_data, params, state)\n\n            self._save_output_tables(results)\n            self._update_state(results)\n\n            logging.info(f\"Successfully processed {results.count} records\")\n\n        except ValueError as err:\n            logging.error(str(err))\n            print(err, file=sys.stderr)\n            sys.exit(1)\n        except Exception as err:\n            logging.exception(\"Unhandled error occurred\")\n            traceback.print_exc(file=sys.stderr)\n            sys.exit(2)\n\n    def _validate_and_get_configuration(self) -> Configuration:\n        \"\"\"Validate configuration and return typed parameters.\"\"\"\n        self.validate_configuration(['api_key', 'endpoint'])\n        return Configuration(**self.configuration.parameters)\n\n    def _load_previous_state(self) -> Dict[str, Any]:\n        \"\"\"Load state from previous run for incremental processing.\"\"\"\n        state = self.get_state_file()\n        logging.info(f\"Last run: {state.get('last_timestamp', 'never')}\")\n        return state\n\n    def _process_input_tables(self) -> List[Dict[str, Any]]:\n        \"\"\"Process all input tables with proper CSV handling.\"\"\"\n        tables = self.get_input_tables_definitions()\n        return [row for table in tables for row in self._process_single_table(table)]\n\n    def _process_single_table(self, table_def) -> List[Dict[str, Any]]:\n        \"\"\"Process individual table with null character handling.\"\"\"\n        with open(table_def.full_path, 'r', encoding='utf-8') as f:\n            lazy_lines = (line.replace('\\0', '') for line in f)\n            reader = csv.DictReader(lazy_lines, dialect='kbc')\n            return [self._transform_row(row) for row in reader]\n\n    @staticmethod\n    def _transform_row(row: Dict[str, str]) -> Dict[str, Any]:\n        \"\"\"Transform single row of data.\"\"\"\n        return {\n            'id': int(row['id']),\n            'name': row['name'].strip(),\n            'value': float(row['value']) if row['value'] else 0.0\n        }\n\n    def _perform_business_logic(\n        self,\n        data: List[Dict[str, Any]],\n        params: Configuration,\n        state: Dict[str, Any]\n    ) -> ProcessedResults:\n        \"\"\"Core business logic - extract/transform/process data.\"\"\"\n        # Your main processing logic here\n        processed = [self._enrich_record(record, params) for record in data]\n\n        return ProcessedResults(\n            data=processed,\n            count=len(processed),\n            last_timestamp=datetime.now(timezone.utc).isoformat(),\n            stats={'total': len(processed)}\n        )\n\n    @staticmethod\n    def _enrich_record(record: Dict[str, Any], params: Configuration) -> Dict[str, Any]:\n        \"\"\"Enrich individual record with additional data.\"\"\"\n        # Enrichment logic here\n        return record\n\n    def _save_output_tables(self, results: ProcessedResults):\n        \"\"\"Write results to output tables with manifests.\"\"\"\n        out_table = self.create_out_table_definition(\n            name=\"output.csv\",\n            destination=\"out.c-data.output\",\n            schema=self._get_output_schema(),\n            incremental=True\n        )\n\n        with open(out_table.full_path, 'w', encoding='utf-8', newline='') as f:\n            writer = csv.DictWriter(f, fieldnames=out_table.column_names)\n            writer.writeheader()\n            writer.writerows(results.data)\n\n        self.write_manifest(out_table)\n        logging.info(f\"Wrote {results.count} records to {out_table.name}\")\n\n    def _update_state(self, results: ProcessedResults):\n        \"\"\"Save state for next incremental run.\"\"\"\n        self.write_state_file({\n            'last_timestamp': results.last_timestamp,\n            'records_processed': results.count,\n            'last_run_stats': results.stats\n        })\n\n    @staticmethod\n    def _get_output_schema() -> OrderedDict:\n        \"\"\"Define output table schema.\"\"\"\n        from collections import OrderedDict\n        from keboola.component.dao import ColumnDefinition, BaseType\n\n        return OrderedDict({\n            \"id\": ColumnDefinition(data_types=BaseType.integer(), primary_key=True),\n            \"name\": ColumnDefinition(),\n            \"value\": ColumnDefinition(data_types=BaseType.numeric(length=\"10,2\"))\n        })\n```\n\n## Summary\n\nThe self-documenting workflow pattern makes your code:\n- **Readable**: Anyone can understand the flow in seconds\n- **Testable**: Each method can be tested independently\n- **Maintainable**: Easy to modify individual steps\n- **Professional**: Follows industry best practices\n\n## Related Documentation\n\n- [Code Quality Guidelines](code-quality.md)\n- [Architecture Guide](architecture.md)\n- [Best Practices](best-practices.md)\n",
        "plugins/component-developer/skills/debug-component/SKILL.md": "---\nname: debugger\ndescription: Expert agent for debugging Keboola Python components using Keboola MCP tools, Datadog logs, and local testing. Specializes in identifying root causes of failures and providing actionable fixes.\ntools: Glob, Grep, Read, Bash, mcp__keboola__*\nmodel: sonnet\ncolor: orange\n---\n\n# Keboola Component Debugger\n\nYou are an expert debugger for Keboola Python components. Your job is to quickly identify root causes of failures and provide actionable solutions to get components working again.\n\n## Debugging Approach\n\n### 1. Gather Context\n\nStart by understanding the problem:\n- What error is the user seeing? (error messages, job IDs, stack traces)\n- Which component and configuration is failing?\n- When did it start failing? (recently or always)\n- What changed recently? (code, configuration, data)\n\n### 2. Use Available Tools\n\nYou have access to multiple debugging tools:\n\n**Keboola MCP Server** (when available):\n- `list_jobs` - Find failed jobs by component/config\n- `get_job` - Get detailed job information and error messages\n- `get_config` - Inspect component configuration\n- `query_data` - Verify output data\n- `run_job` - Re-run jobs after fixes\n\n**File System Tools**:\n- Read component code (`src/component.py`, `src/configuration.py`)\n- Check configuration schemas (`component_config/configSchema.json`)\n- Review test cases (`tests/`)\n- Inspect logs and error messages\n\n**Command Line**:\n- Run components locally: `KBC_DATADIR=data uv run src/component.py`\n- Check dependencies: `uv sync`\n- Run tests: `uv run pytest`\n\n### 3. Identify Root Cause\n\nCommon failure categories:\n\n**Configuration Issues**:\n- Missing or invalid parameters\n- Wrong credentials or API tokens\n- Incorrect input/output mappings\n\n**Code Bugs**:\n- Unhandled exceptions\n- Type errors\n- Logic errors in data processing\n\n**Data Issues**:\n- Unexpected data format\n- Missing required fields\n- Encoding problems (UTF-8, null characters)\n\n**Environment Issues**:\n- Missing dependencies\n- Python version incompatibility\n- File permission errors\n\n**API Issues**:\n- Rate limiting\n- Authentication failures\n- Endpoint changes\n\n### 4. Provide Actionable Fixes\n\nFor each issue found, provide:\n1. **Root Cause** - What specifically is causing the failure\n2. **Fix** - Concrete steps to resolve it (code changes, config updates)\n3. **Verification** - How to test that it's fixed\n\n## Debugging Workflows\n\n### Failed Job Investigation\n\nWhen a user reports a failed job:\n\n1. **Get Job Details**:\n   ```\n   Use mcp__keboola__get_job with job_id\n   ```\n   Look for error messages, stack traces, and exit codes.\n\n2. **Check Configuration**:\n   ```\n   Use mcp__keboola__get_config with component_id and config_id\n   ```\n   Verify all required parameters are present and valid.\n\n3. **Review Code**:\n   Read the component code around the error location.\n   Look for:\n   - Missing error handling\n   - Type mismatches\n   - Unvalidated inputs\n\n4. **Suggest Fix**:\n   Provide specific code changes or configuration updates.\n\n5. **Verify**:\n   ```\n   Use mcp__keboola__run_job to test the fix\n   ```\n\n### Local Debugging\n\nWhen debugging locally:\n\n1. **Set up test data**:\n   ```bash\n   # Create data/config.json with test parameters\n   mkdir -p data/in/tables data/out/tables\n   ```\n\n2. **Run component**:\n   ```bash\n   KBC_DATADIR=data uv run src/component.py\n   ```\n\n3. **Check output**:\n   ```bash\n   ls -la data/out/tables/\n   cat data/out/state.json\n   ```\n\n4. **Review logs**:\n   Check console output for errors and warnings.\n\n### Error Code Reference\n\n**Exit Code 1**: User error\n- Configuration issues\n- Invalid inputs\n- Validation failures\n\n**Exit Code 2**: System error\n- Uncaught exceptions\n- Programming errors\n- External API failures\n\n## Common Issues and Solutions\n\n### TypeError: Expected X, got Y\n\n**Cause**: Type mismatch, often in API calls or data processing\n**Fix**: Add proper type hints and validation\n```python\nfrom anthropic.types import MessageParam\n\nmessage: MessageParam = {\"role\": \"user\", \"content\": \"...\"}\n```\n\n### KeyError: 'key_name'\n\n**Cause**: Accessing non-existent dictionary key\n**Fix**: Use `.get()` with default value\n```python\nvalue = config.get(\"key_name\", default_value)\n```\n\n### UnicodeDecodeError\n\n**Cause**: Reading file without UTF-8 encoding\n**Fix**: Always specify encoding\n```python\nwith open(file, \"r\", encoding=\"utf-8\") as f:\n    content = f.read()\n```\n\n### Null characters in CSV\n\n**Cause**: Invalid null bytes in CSV data\n**Fix**: Filter them out when reading\n```python\nlazy_lines = (line.replace('\\0', '') for line in file)\nreader = csv.DictReader(lazy_lines)\n```\n\n### Exit code 2: Uncaught exception\n\n**Cause**: Exception not properly handled\n**Fix**: Add try/except block\n```python\ntry:\n    # risky operation\nexcept SpecificError as err:\n    logging.error(str(err))\n    sys.exit(1)  # User error\nexcept Exception as err:\n    logging.exception(\"Unexpected error\")\n    sys.exit(2)  # System error\n```\n\n## Output Format\n\nWhen providing debugging results:\n\n```\n## Problem Identified\n\n[Clear description of root cause]\n\n## Affected Code\n\n**Location:** `src/component.py:123-130`\n**Issue:** [What's wrong with this code]\n\n## Recommended Fix\n\n[Specific code changes or configuration updates]\n\n## Verification Steps\n\n1. [How to test the fix]\n2. [What output to expect]\n3. [How to confirm it's working]\n```\n\n## Related Documentation\n\nFor detailed debugging techniques and tools:\n- [Debugging Guide](../references/debugging.md) - Complete debugging workflows and tool usage\n- [Telemetry Debugging](../references/telemetry-debugging.md) - Querying Keboola telemetry data\n\nFor component development best practices:\n- [Architecture Guide](../guides/component-builder/architecture.md)\n- [Best Practices](../guides/component-builder/best-practices.md)\n- [Error Handling](../guides/component-builder/best-practices.md#error-handling)\n",
        "plugins/component-developer/skills/debug-component/references/debugging.md": "# Component Debugging Guide\n\nComplete guide for debugging Keboola components using available tools and services.\n\n## Overview\n\nWhen components fail or behave unexpectedly, use these debugging approaches:\n1. **Keboola MCP Server** - Query jobs, configurations, and output data\n2. **Datadog** - Search and analyze component logs\n3. **Local Testing** - Run components locally for rapid iteration\n\n## Keboola MCP Server Debugging\n\nThe Keboola MCP server provides tools for inspecting jobs, configurations, and data directly.\n\n### Available Tools\n\n| Tool | Purpose |\n|------|---------|\n| `list_jobs` | Find jobs by component, config, or status |\n| `get_job` | Get detailed information about a specific job |\n| `run_job` | Execute a job for testing |\n| `get_config` | Inspect component configuration |\n| `query_data` | Query output tables to verify results |\n\n### Finding Failed Jobs\n\nUse `list_jobs` to find jobs that have failed:\n\n```\nlist_jobs with parameters:\n- status: \"error\"\n- component_id: \"vendor.component-name\"\n- limit: 10\n```\n\nParameters:\n- `status` - Filter by job status: \"error\", \"success\", \"processing\", \"waiting\", \"terminating\", \"terminated\", \"cancelled\"\n- `component_id` - Filter by specific component ID\n- `config_id` - Filter by specific configuration ID\n- `limit` - Number of jobs to return (default 100, max 500)\n- `offset` - Pagination offset\n- `sort_by` - Sort field (default \"startTime\")\n- `sort_order` - Sort direction: \"asc\" or \"desc\" (default \"desc\")\n\n### Inspecting Job Details\n\nOnce you have a job ID, get detailed information:\n\n```\nget_job with parameters:\n- job_id: \"123456789\"\n```\n\nReturns:\n- Job status and duration\n- Input/output parameters\n- Error messages and stack traces\n- Resource usage metrics\n\n### Checking Configuration\n\nVerify the component configuration is correct:\n\n```\nget_config with parameters:\n- component_id: \"vendor.component-name\"\n- configuration_id: \"config-id\"\n```\n\nThis helps identify:\n- Missing or incorrect parameters\n- Invalid credentials\n- Misconfigured input/output mappings\n\n### Re-running Jobs\n\nAfter fixing issues, test by running the job again:\n\n```\nrun_job with parameters:\n- component_id: \"vendor.component-name\"\n- configuration_id: \"config-id\"\n```\n\n### Verifying Output Data\n\nQuery output tables to verify the component produced correct results:\n\n```\nquery_data with parameters:\n- sql_query: \"SELECT * FROM \\\"DATABASE\\\".\\\"SCHEMA\\\".\\\"TABLE\\\" LIMIT 10\"\n- query_name: \"Verify component output\"\n```\n\nSQL dialect notes:\n- Snowflake: Use double quotes for identifiers `\"column_name\"`\n- BigQuery: Use backticks for identifiers `` `column_name` ``\n- Always use fully qualified table names\n\n### Debugging Workflow\n\nFollow this systematic approach:\n\n1. **Find the failing job**\n   ```\n   list_jobs with status=\"error\" and component_id=\"your-component\"\n   ```\n\n2. **Get job details**\n   ```\n   get_job with job_id from step 1\n   ```\n   - Look at error messages\n   - Check input parameters\n   - Note any timeout or resource issues\n\n3. **Verify configuration**\n   ```\n   get_config to check configuration is correct\n   ```\n   - Ensure all required parameters are set\n   - Verify credentials are valid\n   - Check input/output mappings\n\n4. **Fix the issue**\n   - Update component code\n   - Push changes\n   - Wait for deployment (up to 5 minutes)\n\n5. **Re-run and verify**\n   ```\n   run_job to test the fix\n   get_job to check new job status\n   query_data to verify output\n   ```\n\n### Common Issues and Solutions\n\n| Issue | Diagnosis | Solution |\n|-------|-----------|----------|\n| Exit code 1 | User/configuration error | Check parameters, credentials, input data |\n| Exit code 2 | System/application error | Check logs for stack trace, fix code bug |\n| Missing output | No manifest written | Ensure `write_manifest()` is called |\n| Wrong data | Logic error | Query output tables, debug transformation logic |\n| Timeout | Long running operation | Optimize code or increase timeout setting |\n| Memory error | Large data processing | Use generators, process in chunks |\n\n## Datadog Debugging\n\nDatadog provides centralized logging for all Keboola components across all environments. Use it to search logs across component runs, filter by component ID, job ID, or project, and analyze error patterns and trends.\n\n### Keboola Log Tags\n\nKeboola logs use these primary tags for filtering:\n\n| Tag | Description | Example |\n|-----|-------------|---------|\n| `componentid` | Component identifier | `componentid:keboola.python-transformation-v2` |\n| `configid` | Configuration ID | `configid:65904349` |\n| `projectid` | Keboola project ID | `projectid:14370` |\n| `pod_name` | Job pod name (contains job ID) | `pod_name:job-150303519` |\n| `env` | Keboola stack/environment | `env:com-keboola-azure-north-europe` |\n| `service` | Service name | `service:job-queue-runner` |\n| `status` | Log level | `status:error`, `status:info` |\n\nThe `@component` attribute in log messages also contains the component ID and can be used for filtering.\n\n### Common Environments (env tag)\n\n| Environment | Description |\n|-------------|-------------|\n| `kbc-us-east-1` | AWS US East (connection.keboola.com) |\n| `com-keboola-azure-north-europe` | Azure North Europe |\n| `cloud-<customer>-<region>` | GCP dedicated stack (single tenant) |\n\n### Log Search Queries\n\nFilter logs using Datadog query syntax. Tags use the format `tag:value`, while attributes use `@attribute:value`.\n\n**Find all logs for a component:**\n```\ncomponentid:keboola.python-transformation-v2\n```\n\n**Find error logs for a component:**\n```\ncomponentid:keboola.ex-db-mssql status:error\n```\n\n**Find logs for a specific job (by pod name):**\n```\npod_name:job-150303519\n```\n\n**Find logs for a specific project:**\n```\nprojectid:14370\n```\n\n**Combine multiple filters:**\n```\ncomponentid:keboola.python-transformation-v2 projectid:6625 status:error\n```\n\n**Filter by environment:**\n```\ncomponentid:keboola.ex-db-mssql env:kbc-us-east-1\n```\n\n**Use wildcards for component families:**\n```\ncomponentid:keboola.ex-*\n```\n\n### Datadog API Access\n\nFor programmatic access, Datadog requires two authentication headers:\n\n| Header | Purpose |\n|--------|---------|\n| `DD-API-KEY` | Identifies the organization |\n| `DD-APPLICATION-KEY` | Identifies the user/application with specific permissions |\n\n**API Endpoints by Region:**\n\n| Region | API Endpoint | Web UI |\n|--------|--------------|--------|\n| EU | `https://api.datadoghq.eu` | `https://app.datadoghq.eu` |\n| US1 | `https://api.datadoghq.com` | `https://app.datadoghq.com` |\n\nKeboola uses the EU region (`api.datadoghq.eu`).\n\n### Curl Examples\n\n**Search logs for a component (last hour):**\n```bash\ncurl -s -X POST \"https://api.datadoghq.eu/api/v2/logs/events/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"DD-API-KEY: ${DD_API_KEY}\" \\\n  -H \"DD-APPLICATION-KEY: ${DD_APP_KEY}\" \\\n  -d '{\n    \"filter\": {\n      \"query\": \"componentid:keboola.python-transformation-v2\",\n      \"from\": \"now-1h\",\n      \"to\": \"now\"\n    },\n    \"page\": {\n      \"limit\": 10\n    }\n  }'\n```\n\n**Search error logs for a specific project:**\n```bash\ncurl -s -X POST \"https://api.datadoghq.eu/api/v2/logs/events/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"DD-API-KEY: ${DD_API_KEY}\" \\\n  -H \"DD-APPLICATION-KEY: ${DD_APP_KEY}\" \\\n  -d '{\n    \"filter\": {\n      \"query\": \"projectid:14370 status:error\",\n      \"from\": \"now-24h\",\n      \"to\": \"now\"\n    },\n    \"page\": {\n      \"limit\": 50\n    }\n  }'\n```\n\n**Search logs for a specific job:**\n```bash\ncurl -s -X POST \"https://api.datadoghq.eu/api/v2/logs/events/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"DD-API-KEY: ${DD_API_KEY}\" \\\n  -H \"DD-APPLICATION-KEY: ${DD_APP_KEY}\" \\\n  -d '{\n    \"filter\": {\n      \"query\": \"pod_name:job-150303519\",\n      \"from\": \"now-24h\",\n      \"to\": \"now\"\n    },\n    \"page\": {\n      \"limit\": 100\n    }\n  }'\n```\n\n### API Response Structure\n\nThe Logs API returns a JSON response with this structure:\n\n```json\n{\n  \"data\": [\n    {\n      \"id\": \"log-id\",\n      \"type\": \"log\",\n      \"attributes\": {\n        \"service\": \"job-queue-runner\",\n        \"host\": \"hostname\",\n        \"message\": \"Log message content\",\n        \"status\": \"info\",\n        \"timestamp\": \"2025-01-15T10:43:09.353Z\",\n        \"tags\": [\"componentid:keboola.python-transformation-v2\", \"projectid:6625\", ...],\n        \"attributes\": {\n          \"component\": \"keboola.python-transformation-v2\",\n          \"runId\": \"1280579774\",\n          \"level_name\": \"INFO\"\n        }\n      }\n    }\n  ],\n  \"meta\": {\n    \"page\": { \"after\": \"cursor-for-pagination\" }\n  }\n}\n```\n\n### Debugging Workflow with Datadog\n\n1. **Get the job ID** from Keboola UI or MCP server (`list_jobs` or `get_job`)\n\n2. **Search logs by job** using the pod_name tag:\n   ```\n   pod_name:job-<job-id>\n   ```\n\n3. **Filter for errors** if needed:\n   ```\n   pod_name:job-<job-id> status:error\n   ```\n\n4. **Analyze the timeline** - logs are timestamped, so you can trace the execution flow\n\n5. **Check for patterns** across multiple jobs:\n   ```\n   componentid:<component-id> status:error\n   ```\n\n## Local Testing\n\nFor rapid iteration, test components locally before deployment.\n\n### Basic Local Run\n\nAlways sync dependencies before running the component locally:\n\n```bash\n# Sync dependencies first\nuv sync\n\n# Run component with data directory (one-liner)\nKBC_DATADIR=data uv run src/component.py\n```\n\nThis approach ensures dependencies are up-to-date and uses the project's virtual environment correctly. The one-liner format with inline environment variable is preferred as it works regardless of shell session persistence.\n\n### With Docker\n\n```bash\n# Build image\ndocker build -t my-component .\n\n# Run with data directory mounted\ndocker run -v $(pwd)/data:/data my-component\n```\n\n### Creating Test Data\n\nSet up the `data/` directory structure:\n\n```\ndata/\nâ”œâ”€â”€ config.json          # Component configuration\nâ”œâ”€â”€ in/\nâ”‚   â”œâ”€â”€ tables/          # Input CSV files\nâ”‚   â””â”€â”€ files/           # Input files\nâ””â”€â”€ out/\n    â”œâ”€â”€ tables/          # Output will appear here\n    â””â”€â”€ files/           # Output files will appear here\n```\n\nExample `data/config.json`:\n```json\n{\n  \"parameters\": {\n    \"api_key\": \"test-key\",\n    \"endpoint\": \"https://api.example.com\"\n  }\n}\n```\n\n### Debugging Tips\n\n1. **Add logging**: Use `logging.debug()` for detailed output during development\n2. **Check exit codes**: Exit 1 = user error, Exit 2 = system error\n3. **Inspect manifests**: Verify output table manifests are created correctly\n4. **Test incrementally**: Test each component method separately\n\n## Related Guides\n\n- [Telemetry Debugging Guide](telemetry-debugging.md) - Query telemetry data for support ticket investigation\n- [Architecture Guide](architecture.md) - Component structure and error handling\n- [Code Quality Guide](code-quality.md) - Logging and debugging best practices\n- [Developer Portal Guide](developer-portal.md) - Registration and deployment\n",
        "plugins/component-developer/skills/debug-component/references/telemetry-debugging.md": "# Telemetry Data Debugging Guide\n\nComplete guide for querying Keboola telemetry data to debug component configurations, jobs, and issues across all stacks.\n\n## Overview\n\nThe Keboola Telemetry Project (ID: 133) aggregates operational and usage telemetry from all Keboola stacks. It provides read-only access to raw telemetry data for analysis, debugging, and support ticket investigation.\n\n**Project Details:**\n- **ID:** 133\n- **Name:** L3 [Data Product] Telemetry Data Discovery\n- **Stack:** us-east4.gcp.keboola.com\n- **SQL Dialect:** Snowflake\n- **MCP Server:** `keboola-mcp-us-east4gcp`\n\n## Connecting to Telemetry\n\n### MCP Server Connection\n\n1. Use the MCP server `keboola-mcp-us-east4gcp`\n2. Call `get_project_info` to verify connection to project 133\n3. Use `query_data` tool to execute SQL queries\n\n### Verifying Connection\n\n```\nget_project_info with parameters: {}\n```\n\nExpected response should show:\n- `project_id: 133`\n- `project_name: \"L3 [Data Product] Telemetry Data Discovery\"`\n- `sql_dialect: \"Snowflake\"`\n\n## Key Telemetry Tables\n\nAll telemetry tables are in the bucket `in.c-out_kbc_public_telemetry` with fully qualified names using database `KBC_USE4_37` and schema `out.c-kbc_public_telemetry`.\n\n### Configuration Tables\n\n| Table | Purpose | Key Columns |\n|-------|---------|-------------|\n| `kbc_component_configuration` | Root component configurations | `configuration_json`, `kbc_component_id`, `configuration_id_num` |\n| `kbc_component_configuration_row` | Configuration rows (for row-based components) | `configuration_row_json`, `configuration_row_id_num` |\n| `kbc_component_configuration_version` | Configuration version history | `configuration_version`, `kbc_branch_id` |\n\n### Job Tables\n\n| Table | Purpose | Key Columns |\n|-------|---------|-------------|\n| `kbc_job` | Job execution records | `kbc_job_id`, `kbc_component_id`, `job_status` |\n| `kbc_job_input_table` | Job input table mappings | `kbc_job_id`, `kbc_project_table_id` |\n| `kbc_job_output_table` | Job output table mappings | `kbc_job_id`, `kbc_project_table_id` |\n\n### Storage Tables\n\n| Table | Purpose | Key Columns |\n|-------|---------|-------------|\n| `kbc_bucket` | Bucket metadata | `kbc_project_bucket_id` |\n| `kbc_table` | Table metadata | `kbc_project_table_id` |\n| `kbc_column` | Column metadata | `kbc_project_column_id` |\n\n## Important Column Mappings\n\nThe telemetry tables use different column names than what you might expect. Here are the key mappings:\n\n### Configuration Row Table (`kbc_component_configuration_row`)\n\n| Expected Column | Actual Column | Notes |\n|-----------------|---------------|-------|\n| `component_id` | `kbc_component_id` | Includes stack suffix (e.g., `keboola.app-data-gateway_com-keboola-gcp-europe-west3`) |\n| `configuration_id` | `kbc_component_configuration_id` | Composite key with stack |\n| `configuration_row_id` | `configuration_row_id_num` | The row ID you're looking for |\n| `stack` | `dst_stack_single` | Connection URL format (see Stack Mappings below) |\n| `configuration_json` | `configuration_row_json` | The actual configuration JSON |\n\n### Configuration Table (`kbc_component_configuration`)\n\n| Expected Column | Actual Column | Notes |\n|-----------------|---------------|-------|\n| `component_id` | `kbc_component_id` | Component identifier |\n| `configuration_id` | `configuration_id_num` | Configuration ID |\n| `configuration_json` | `configuration_json` | Root configuration JSON |\n| `stack` | `dst_stack_single` | Connection URL format |\n\n## Stack Name Mappings\n\nThe `dst_stack_single` column uses connection URL format, not the internal stack name:\n\n| Internal Stack Name | dst_stack_single Value |\n|---------------------|------------------------|\n| `com-keboola-gcp-europe-west3` | `connection.europe-west3.gcp.keboola.com` |\n| `com-keboola-gcp-us-east4` | `connection.us-east4.gcp.keboola.com` |\n| `com-keboola-azure-north-europe` | `connection.north-europe.azure.keboola.com` |\n| `kbc-eu-central-1` | `connection.eu-central-1.keboola.com` |\n| `kbc-us-east-1` | `connection.keboola.com` |\n\n## Example Queries\n\n### Find Configuration Row JSON by Row ID\n\nThe most reliable way to find a configuration row is by its `configuration_row_id_num`:\n\n```sql\nSELECT \"configuration_row_json\" \nFROM \"KBC_USE4_37\".\"out.c-kbc_public_telemetry\".\"kbc_component_configuration_row\" \nWHERE \"configuration_row_id_num\" = '01katngamqm5qsa55hn4gwbdb8' \nLIMIT 1;\n```\n\n### Find Configuration Row with All Details\n\n```sql\nSELECT \n    \"kbc_component_id\", \n    \"kbc_component_configuration_id\", \n    \"configuration_row_id_num\", \n    \"dst_stack_single\", \n    \"kbc_configuration_row_is_deleted\",\n    \"configuration_row_json\"\nFROM \"KBC_USE4_37\".\"out.c-kbc_public_telemetry\".\"kbc_component_configuration_row\" \nWHERE \"configuration_row_id_num\" = '01katngamqm5qsa55hn4gwbdb8' \nLIMIT 10;\n```\n\n### Find Root Configuration JSON\n\n```sql\nSELECT \"configuration_json\"\nFROM \"KBC_USE4_37\".\"out.c-kbc_public_telemetry\".\"kbc_component_configuration\"\nWHERE \"kbc_component_id\" LIKE 'keboola.app-data-gateway%'\nAND \"configuration_id_num\" = '01kakd3q09dawzewwqc807et2t'\nLIMIT 1;\n```\n\n### Find All Configurations for a Component on a Stack\n\n```sql\nSELECT \n    \"configuration_id_num\",\n    \"kbc_component_configuration\",\n    \"dst_stack_single\",\n    \"kbc_configuration_is_deleted\"\nFROM \"KBC_USE4_37\".\"out.c-kbc_public_telemetry\".\"kbc_component_configuration\"\nWHERE \"kbc_component_id\" LIKE 'keboola.ex-db-mysql%'\nAND \"dst_stack_single\" = 'connection.europe-west3.gcp.keboola.com'\nAND \"kbc_configuration_is_deleted\" = 'false'\nLIMIT 50;\n```\n\n### Find Jobs for a Configuration\n\n```sql\nSELECT \n    \"kbc_job_id\",\n    \"job_start_time\",\n    \"job_end_time\",\n    \"job_status\",\n    \"job_error_message\"\nFROM \"KBC_USE4_37\".\"out.c-kbc_public_telemetry\".\"kbc_job\"\nWHERE \"kbc_component_id\" LIKE 'keboola.app-data-gateway%'\nAND \"configuration_id\" = '01kakd3q09dawzewwqc807et2t'\nORDER BY \"job_start_time\" DESC\nLIMIT 20;\n```\n\n## Debugging Workflow for Support Tickets\n\nWhen investigating a support ticket with a failing job:\n\n### 1. Gather Information from Ticket\n\nExtract from the ticket:\n- **Job ID** (e.g., `45267290`)\n- **Stack** (e.g., `com-keboola-gcp-europe-west3`)\n- **Component ID** (e.g., `keboola.app-data-gateway`)\n- **Configuration ID** (e.g., `01kakd3q09dawzewwqc807et2t`)\n- **Row ID** (if applicable, e.g., `01katngamqm5qsa55hn4gwbdb8`)\n- **Error message**\n\n### 2. Connect to Telemetry MCP Server\n\n```\nUse MCP server: keboola-mcp-us-east4gcp\nCall: get_project_info\n```\n\n### 3. Query Configuration\n\nFor row-based configurations, search by `configuration_row_id_num`:\n\n```sql\nSELECT \"configuration_row_json\" \nFROM \"KBC_USE4_37\".\"out.c-kbc_public_telemetry\".\"kbc_component_configuration_row\" \nWHERE \"configuration_row_id_num\" = '<row_id>' \nLIMIT 1;\n```\n\nFor root configurations:\n\n```sql\nSELECT \"configuration_json\"\nFROM \"KBC_USE4_37\".\"out.c-kbc_public_telemetry\".\"kbc_component_configuration\"\nWHERE \"configuration_id_num\" = '<config_id>'\nLIMIT 1;\n```\n\n### 4. Analyze Configuration JSON\n\nParse the returned JSON and look for:\n- Invalid parameter values\n- Incorrect data types (e.g., `\"size\": \"38,0\"` for a string type)\n- Missing required fields\n- Malformed mappings\n\n### 5. Document Findings\n\nReport:\n- The problematic configuration field\n- Why it's invalid\n- Suggested fix\n\n## Common Issues Found in Telemetry\n\n| Issue | How to Identify | Example |\n|-------|-----------------|---------|\n| Invalid data type | `\"type\": \"string\"` with numeric `\"size\"` like `\"38,0\"` | Column configured as string but with NUMBER precision |\n| Missing credentials | Empty or null `#password` fields | OAuth not completed |\n| Wrong table mapping | `\"source\"` table doesn't exist | Typo in table ID |\n| Deleted configuration | `kbc_configuration_is_deleted = 'true'` | Config was deleted but job still references it |\n\n## Database Reference\n\n### Stack to Database Mapping\n\nFrom project info, these are the database names for each stack:\n\n| Stack | Database |\n|-------|----------|\n| COATES | KBC_USE4_33 |\n| CREDITINFO | KBC_USE4_33 |\n| cloud-keboola-cs | KBC_USE4_35 |\n| GRPN | KBC_USE4_20 |\n| HCI | KBC_USE4_21 |\n| HCKZ | KBC_USE4_286 |\n| INNOGY | KBC_USE4_22 |\n| PASHA | KBC_USE4_377 |\n| RBI | KBC_USE4_69 |\n| cloud-keboola-slsp | KBC_USE4_23 |\n| com-keboola-azure-north-europe | KBC_USE4_54 |\n| com-keboola-gcp-europe-west3 | KBC_USE4_26 |\n| com-keboola-gcp-us-east4 | KBC_USE4_27 |\n| kbc-eu-central-1 | KBC_USE4_30 |\n| AWS US | KBC_USE4_32 |\n\n## Related Guides\n\n- [Debugging Guide](debugging.md) - General component debugging with MCP and Datadog\n- [Architecture Guide](architecture.md) - Component structure and error handling\n",
        "plugins/component-developer/skills/get-started/SKILL.md": "---\nname: get-started\ndescription: Guide for initializing and setting up new Keboola Python components using cookiecutter template. Use when starting a new component project from scratch.\nmodel: sonnet\ncolor: green\n---\n\n# Get Started with Keboola Component Development\n\nThis skill helps you initialize and set up new Keboola Python components from scratch using the official cookiecutter template.\n\n## When to Use This Skill\n\n- Starting a new Keboola component project\n- Need to understand the initialization process\n- Setting up the project structure correctly\n- Understanding cookiecutter template usage\n\n## Quick Start\n\nThe fastest way to start a new component:\n\n```bash\ncookiecutter gh:keboola/cookiecutter-python-component\n```\n\nThen clean up and configure:\n1. Remove cookiecutter example files from `data/` directory\n2. Create component-specific `data/config.json` with example parameters\n3. Keep empty `data/` folder structure (not committed to git)\n\n## Complete Initialization Guide\n\nFor detailed step-by-step instructions, see:\n- [Initialization Guide](references/initialization.md) - Complete setup process\n\n## What Happens During Initialization\n\nThe cookiecutter template creates:\n- `src/` - Component Python code\n- `component_config/` - Configuration schemas and descriptions\n- `tests/` - Test structure\n- `.github/workflows/` - CI/CD pipelines\n- `Dockerfile` - Container definition\n- `requirements.txt` - Python dependencies\n- `data/` - Local testing directory (with examples to remove)\n\n## After Initialization\n\nOnce initialized, you'll typically want to:\n1. Implement component logic (use `@build-component` skill)\n2. Design configuration schemas (use `@build-component-ui` skill)\n3. Write tests (use `@test-component` skill)\n4. Deploy to Developer Portal\n\n## Key Resources\n\n- **Cookiecutter Template**: https://github.com/keboola/cookiecutter-python-component\n- **Component Tutorial**: https://developers.keboola.com/extend/component/tutorial/\n- **Developer Docs**: https://developers.keboola.com/\n\n## Next Steps\n\nAfter getting started:\n- For component development: Use `@build-component` skill\n- For UI/schema work: Use `@build-component-ui` skill\n- For testing: Use `@test-component` skill\n- For debugging: Use `@debug-component` skill\n",
        "plugins/component-developer/skills/get-started/references/initialization.md": "# Component Initialization & Setup\n\nComplete guide for initializing new Keboola Python components.\n\n## Steps\n\n### 1. Understand Requirements\n\nGather information about what the component should do:\n- Component type (extractor, writer, transformation, or application)\n- Data source/destination\n- Required authentication method\n- Incremental vs. full load requirements\n- Configuration parameters needed\n\n### 2. Use Cookiecutter Template\n\nInitialize using the official template:\n\n```bash\ncookiecutter gh:keboola/cookiecutter-python-component\n```\n\nThe template will prompt you for:\n- Component name (without 'extractor', 'writer', or 'application' suffix)\n- Component ID\n- Author information\n- Python version\n\n### 3. Clean Up Example Data and Create Config\n\nAfter cookiecutter initialization:\n\n#### a) Remove all example files\n\n```bash\nfind data -type f -delete\n```\n\n#### b) Create example `data/config.json`\n\nCreate a component-specific configuration file:\n\n```json\n{\n  \"parameters\": {\n    \"param1\": \"example_value\",\n    \"param2\": true\n  }\n}\n```\n\n**Important notes:**\n- The template includes generic example files (test.csv, order1.xml, etc.) - remove these\n- Create a **new** `data/config.json` with realistic example parameters for this specific component\n- Include all required parameters with example values\n- Use placeholder values that clearly indicate what should be replaced (e.g., \"your-api-key-here\")\n- The `data/` directory is in `.gitignore` so config.json won't be committed\n- Developers need config.json for local testing: `python src/component.py`\n- The Keboola platform provides real configuration at runtime\n\n### 4. Repository Structure\n\nEnsure proper directory structure is established:\n\n```\nmy-component/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ component.py          # Main component logic\nâ”‚   â””â”€â”€ configuration.py      # Configuration validation\nâ”œâ”€â”€ component_config/\nâ”‚   â”œâ”€â”€ component_config.json           # Configuration schema\nâ”‚   â”œâ”€â”€ component_long_description.md   # Detailed description\nâ”‚   â”œâ”€â”€ component_short_description.md  # Brief description\nâ”‚   â””â”€â”€ configRowSchema.json           # Row-level config (if needed)\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_component.py     # Unit tests\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ config.json          # Example config for local testing\nâ”‚   â”œâ”€â”€ in/\nâ”‚   â”‚   â”œâ”€â”€ tables/          # Empty\nâ”‚   â”‚   â””â”€â”€ files/           # Empty\nâ”‚   â””â”€â”€ out/\nâ”‚       â”œâ”€â”€ tables/          # Empty\nâ”‚       â””â”€â”€ files/           # Empty\nâ”œâ”€â”€ .github/workflows/\nâ”‚   â””â”€â”€ push.yml             # CI/CD deployment\nâ”œâ”€â”€ Dockerfile               # Container definition\nâ”œâ”€â”€ pyproject.toml           # Dependencies\nâ””â”€â”€ README.md                # Documentation\n```\n\n### 5. Developer Portal Registration\n\nRegister the component in the Developer Portal using curl commands. See [Developer Portal Guide](developer-portal.md) for the complete API workflow.\n\n**IMPORTANT**: Always use curl commands for registration, never use a web browser.\n\n## Important Notes\n\n**IMPORTANT**: Never use words like 'extractor', 'writer', or 'application' in the component name itself.\n\n## Next Steps\n\nAfter initialization:\n1. Review [Architecture Guide](architecture.md) for component structure patterns\n2. Implement your component following [Code Quality Guidelines](code-quality.md)\n3. Use [Workflow Patterns](workflow-patterns.md) for clean code organization\n4. Check [Best Practices](best-practices.md) DO/DON'T lists\n5. Use [Debugging Guide](debugging.md) when troubleshooting issues\n",
        "plugins/component-developer/skills/migrate-component-to-uv/SKILL.md": "---\nname: migrate-component-to-uv\ndescription: Migrate Keboola Python packages from setup.py to modern uv build system with deterministic dependencies. Follows established patterns from python-http-client and python-component migrations.\ntools: Bash, Read, Write, Edit, Glob, Grep, Task\nmodel: sonnet\ncolor: purple\n---\n\n# Migrate Keboola Component to uv Build System\n\nYou are an expert at migrating Keboola Python packages from legacy `setup.py` + pip to modern `pyproject.toml` + uv build system. You understand PEP 517/518/639 standards, GitHub Actions workflows, and Keboola's established migration patterns.\n\n## When to Use This Skill\n\nUse this skill when:\n- Migrating a Keboola Python package from `setup.py` to `pyproject.toml`\n- Modernizing build system to use uv instead of pip\n- Adding deterministic dependency management with `uv.lock`\n- Updating CI/CD workflows to use uv\n- Following Keboola's python-http-client and python-component patterns\n\n## Prerequisites Check\n\nBefore starting, verify:\n- [ ] Repository uses `setup.py` and/or `requirements.txt`\n- [ ] Package has test suite with pytest\n- [ ] Git repository with clean working tree\n- [ ] Access to GitHub secrets configuration\n- [ ] PyPI and Test PyPI accounts available\n\n## Migration Philosophy\n\n### Flexible Commit Strategy\n\n**Guideline (not dogma)**: Use 3 logical commits\n1. **Linting baseline** - Fix linting first to avoid noise in migration diffs\n2. **Package metadata** - Migrate to pyproject.toml\n3. **CI/CD workflows** - Update to uv + generate lock file\n\n**Why this works:**\n- Clean diffs: Linting fixes don't pollute actual migration changes\n- Reviewable: Each commit has clear, focused purpose\n- Flexible: Could be 2 commits (combine lint+pyproject) or 4 (separate workflows)\n\n**Key principle**: Logical, reviewable chunks that make sense independently\n\n### Lint-First Principle\n\n**Always fix linting BEFORE migrating metadata:**\n- Establishes clean baseline\n- Reveals code quality issues early\n- Prevents attribution confusion (linting vs migration changes)\n- Makes review significantly easier\n\n### Version Strategy\n\n**Testing phase**: Use next minor version\n- Example: Current 1.6.13 â†’ Test as 1.7.0, 1.7.1, 1.7.2\n\n**Production release**: Use following minor version  \n- Example: After testing 1.7.x â†’ Release 1.8.0\n\n**Flexibility**: Could use 1.7.0a1, 1.7.0a2 instead - pattern matters, not exact format\n\n## Step-by-Step Migration Guide\n\n### Phase 1: Analysis\n\n1. **Check current state:**\n```bash\n# What's in setup.py?\ncat setup.py\n\n# What's in requirements.txt?\ncat requirements.txt\n\n# What's the current version?\n# Check PyPI or setup.py\n```\n\n2. **Identify Python version support:**\n```python\n# From setup.py python_requires\n# Determine: min_version = max(3.8, current_requires_python)\n```\n\n3. **Check for docs generation:**\n```bash\n# Does push_main.yml exist with pdoc?\ncat .github/workflows/push_main.yml 2>/dev/null | grep pdoc\n```\n\n### Phase 2: Commit 1 - Linting Baseline\n\n**Purpose**: Establish clean linting baseline\n\n**Steps:**\n\n1. **Rename and update flake8 config:**\n```bash\n# Rename to standard name\nmv flake8.cfg .flake8  # if it exists\n\n# Use cookiecutter template standard:\ncat > .flake8 << 'EOF'\n[flake8]\nexclude = __pycache__, .git, .venv, venv, docs\nignore = E203,W503\nmax-line-length = 120\nEOF\n```\n\n2. **Run flake8 and fix ALL errors:**\n```bash\n# Install flake8\nuv add --dev flake8\n\n# Run and fix errors\nflake8 src/ tests/\n```\n\nCommon fixes:\n- F403/F405: Replace star imports with explicit imports\n- F841: Remove unused variables\n- E501: Break long lines\n- E231: Add missing whitespace\n- E123: Fix bracket indentation\n- CRLFâ†’LF: Normalize line endings (expected, good cleanup)\n\n3. **Commit:**\n```bash\ngit add .flake8 src/ tests/\ngit commit -m \"flake8 config consistent with cookiecutter template ðŸª\"\n```\n\n### Phase 3: Commit 2 - Package Metadata\n\n**Purpose**: Migrate to modern pyproject.toml\n\n**Steps:**\n\n1. **Create pyproject.toml** (see `templates/pyproject.toml.template`):\n\nKey points:\n- `version = \"0.0.0\"` - replaced by git tags in CI\n- Extract dependencies from setup.py `install_requires`\n- Extract dev dependencies from setup.py `setup_requires` and `tests_require`\n- Add `pdoc3` to dev if docs workflow exists\n- `requires-python = \">=MIN_VERSION\"` (â‰¥3.8)\n- **Remove** `License :: OSI Approved :: MIT License` classifier (PEP 639)\n- **Keep** `license = \"MIT\"` field\n- Add TestPyPI index configuration\n\n2. **Delete old files:**\n```bash\ngit rm setup.py requirements.txt\n```\n\n3. **Update LICENSE copyright year:**\n```bash\n# Update to current year (2026)\nsed -i 's/Copyright (c) 20[0-9][0-9]/Copyright (c) 2026/' LICENSE\n```\n\n4. **Commit:**\n```bash\ngit add pyproject.toml LICENSE\ngit commit -m \"migrate package configuration to pyproject.toml ðŸ“¦\"\n```\n\n### Phase 4: Commit 3 - uv Workflows\n\n**Purpose**: Update CI/CD to use uv\n\n**Steps:**\n\n1. **Update all 3-4 workflows** (see `references/workflow-templates.md`):\n   - `push_dev.yml` - Testing on dev branches\n   - `deploy.yml` - Production PyPI deployment\n   - `deploy_to_test.yml` - Test PyPI deployment\n   - `push_main.yml` - Docs generation (OPTIONAL - only if docs exist)\n\nKey changes per workflow:\n- Update action versions: `@v4` â†’ `@v5`, `@v6`\n- Add uv installation: `uses: astral-sh/setup-uv@v6`\n- Add ruff action: `uses: astral-sh/ruff-action@v3` (blocking, no continue-on-error)\n- Change: `pip install` â†’ `uv sync --all-groups --frozen`\n- Change: `flake8 --config=...` â†’ `uv run flake8`\n- Change: `pytest tests` â†’ `uv run pytest tests`\n- Add version replacement: `uv version $TAG_VERSION`\n- Update secrets: `UV_PUBLISH_TOKEN`, `UV_PUBLISH_TOKEN_TEST_PYPI`\n- Python matrix: `[MIN_VERSION, \"3.13\", \"3.14\"]` (min + 2 latest)\n\n2. **Generate uv.lock:**\n```bash\nuv sync --all-groups\n```\n\n3. **Verify build works:**\n```bash\nuv build\n# Should create dist/*.tar.gz and dist/*.whl\n\nuv version 1.7.0 --dry-run\n# Should show: package-name 0.0.0 => 1.7.0\n```\n\n4. **Commit:**\n```bash\ngit add .github/workflows/*.yml uv.lock\ngit commit -m \"uv ðŸ’œ\"\n```\n\n### Phase 5: Testing on Test PyPI\n\n1. **Push branch:**\n```bash\ngit push origin BRANCH_NAME\n```\n\n2. **Create test tag:**\n```bash\ngit tag 1.7.0\ngit push origin 1.7.0\n```\n\n3. **Manually trigger Test PyPI workflow:**\n   - Go to GitHub Actions â†’ \"Build & Upload Python Package To Test PyPI\"\n   - Click \"Run workflow\" \n   - Select branch or tag\n   - Click \"Run workflow\"\n\n4. **Verify on Test PyPI:**\n   - Check: https://test.pypi.org/project/PACKAGE_NAME/\n   - Verify version appears\n\n5. **Test installation:**\n```bash\ncd /tmp && mkdir test_install && cd test_install\nuv init\nuv add --index-url https://test.pypi.org/simple/ \\\n       --extra-index-url https://pypi.org/simple/ \\\n       --index-strategy unsafe-best-match \\\n       PACKAGE_NAME==1.7.0\nuv run python -c \"import PACKAGE; print('âœ… Works!')\"\ncd .. && rm -rf test_install\n```\n\n### Phase 6: Production Release\n\n1. **Create PR** (see `templates/pr-description.md.template`)\n\n2. **Get approval and merge to main**\n\n3. **Create production release:**\n   - Go to: https://github.com/ORG/REPO/releases/new\n   - Tag: `1.8.0`\n   - Target: `main`\n   - Title: `1.8.0`\n   - Description: Migration summary\n   - Click \"Publish release\"\n\n4. **Workflow auto-triggers** â†’ Publishes to PyPI\n\n5. **Verify production:**\n```bash\ncd /tmp && mkdir test_prod && cd test_prod\nuv init\nuv add PACKAGE_NAME==1.8.0\nuv run python -c \"import PACKAGE; print('âœ… Production works!')\"\ncd .. && rm -rf test_prod\n```\n\n## Python Matrix Strategy\n\n**Smart matrix logic:**\n```yaml\npython-version: [\n  \"MIN_SUPPORTED\",  # max(3.8, current_requires_python)\n  \"3.13\",           # Second-latest stable\n  \"3.14\"            # Latest stable\n]\n```\n\n**Examples:**\n- Package supports â‰¥3.8 â†’ `[\"3.8\", \"3.13\", \"3.14\"]`\n- Package supports â‰¥3.10 â†’ `[\"3.10\", \"3.13\", \"3.14\"]`\n- Package supports â‰¥3.12 â†’ `[\"3.12\", \"3.13\", \"3.14\"]`\n\n**Rationale**: Test minimum (compatibility floor) + 2 latest (future-proofing)\n\n## Docs Workflow Handling\n\n**Detection:**\n```bash\n# Check if push_main.yml exists AND contains pdoc\nif [ -f .github/workflows/push_main.yml ] && grep -q pdoc .github/workflows/push_main.yml; then\n    # Include docs workflow in migration\n    # Add pdoc3 to [dependency-groups] dev\nfi\n```\n\n**Migration for docs:**\n- Old: `pip install --user pdoc3` (ad-hoc)\n- New: Add `pdoc3` to `[dependency-groups] dev` + use `uv run pdoc`\n- **Improvement**: Tracked dependency instead of ad-hoc install\n\n## Secret Configuration\n\n**Required GitHub secrets:**\n\n1. **UV_PUBLISH_TOKEN** - Production PyPI token\n   - Create at: https://pypi.org/manage/account/token/\n   - Scope: Entire account or specific project\n   - Add at: GitHub repo â†’ Settings â†’ Secrets â†’ UV_PUBLISH_TOKEN\n\n2. **UV_PUBLISH_TOKEN_TEST_PYPI** - Test PyPI token\n   - Create at: https://test.pypi.org/manage/account/token/\n   - Scope: Entire account\n   - Add at: GitHub repo â†’ Settings â†’ Secrets â†’ UV_PUBLISH_TOKEN_TEST_PYPI\n\n**Note**: uv automatically uses `__token__` as username when `UV_PUBLISH_TOKEN` env var is set\n\n## Workflow Trigger Strategy\n\n**Test PyPI**: `on: workflow_dispatch` (manual)\n- Allows testing any version without tag naming constraints\n- Full control over when to test\n\n**Production PyPI**: `on: push: tags: ['*']` (automatic)\n- Triggers automatically when tag pushed to main\n- Simpler workflow: `git tag X.Y.Z && git push origin X.Y.Z`\n\n## Common Issues & Solutions\n\nSee `references/troubleshooting.md` for detailed troubleshooting guide.\n\n**Quick fixes:**\n\n1. **License classifier conflict**\n   - Error: `License classifiers have been superseded`\n   - Fix: Remove `License :: OSI Approved :: MIT License` from classifiers\n\n2. **CRLF line endings**\n   - Symptom: Huge diffs in unchanged files\n   - Status: Expected and correct (CRLF â†’ LF normalization)\n\n3. **Test PyPI installation fails**\n   - Error: `No solution found`\n   - Fix: Add `--index-strategy unsafe-best-match`\n\n4. **uv version command not found**\n   - Fix: Ensure uv â‰¥ 0.5.0\n\n## Modern Tooling Requirements\n\n**100% modern uv - ZERO pip mentions:**\n\nâœ… **CORRECT:**\n- `uv add PACKAGE` - Add production dependency\n- `uv add --dev PACKAGE` - Add dev dependency\n- `uv sync --all-groups --frozen` - Install from lock\n- `uv run COMMAND` - Run command in environment\n- `uv build` - Build package\n- `uv publish` - Publish to PyPI\n\nâŒ **NEVER USE:**\n- `pip install` - OLD\n- `pip` - OLD\n- `uv pip install` - WRONG uv usage\n\n## References\n\n- `references/migration-guide.md` - Complete step-by-step guide\n- `references/workflow-templates.md` - All 4 workflow YAML files\n- `references/troubleshooting.md` - Common issues and solutions\n- `references/examples.md` - python-http-client and python-component migrations\n- `templates/pyproject.toml.template` - Template pyproject.toml\n- `templates/flake8.template` - Template .flake8\n- `templates/pr-description.md.template` - PR description template\n\n## Questions to Ask User\n\nBefore starting:\n1. What's the current latest version on PyPI?\n2. Do you have access to configure GitHub secrets?\n3. What Python versions should we support? (check setup.py)\n4. Does the package generate HTML docs with pdoc?\n5. Any custom build requirements or special dependencies?\n\n## Success Criteria\n\nMigration complete when:\n- âœ… All tests pass with uv locally\n- âœ… Package builds successfully (`uv build`)\n- âœ… Test PyPI release installable and functional\n- âœ… Production PyPI release installable and functional\n- âœ… CI/CD workflows green on main branch\n- âœ… Documentation updated (if needed)\n- âœ… Team notified\n\n---\n\n**Remember**: This is a build system migration, NOT an API change. End users should see no difference except faster installs and more reliable dependency resolution.\n",
        "plugins/component-developer/skills/migrate-component-to-uv/references/examples.md": "# Real Migration Examples\n\nThis document contains detailed examples of successful migrations from setup.py to uv in Keboola Python packages.\n\n---\n\n## Table of Contents\n\n1. [python-http-client Migration](#python-http-client-migration)\n2. [python-component Migration](#python-component-migration)\n3. [Comparison and Lessons Learned](#comparison-and-lessons-learned)\n\n---\n\n## python-http-client Migration\n\n**Repository**: https://github.com/keboola/python-http-client  \n**Status**: First Keboola package to migrate to uv  \n**Pattern**: Established the baseline approach\n\n### Migration Context\n\n- **Before**: setup.py + pip\n- **After**: pyproject.toml + uv\n- **Version strategy**: 0.x.x alpha tags for testing on Test PyPI\n- **Commits**: Migration completed in atomic commits\n\n### Key Files Changed\n\n**pyproject.toml**:\n```toml\n[project]\nname = \"keboola.http-client\"\nversion = \"0.0.0\"\ndependencies = [\n    \"aiolimiter>=1.2.1\",\n    \"httpx>=0.28.1\",\n    \"requests>=2.32.4\",\n]\nrequires-python = \">=3.8\"\n\n[dependency-groups]\ndev = [\n    \"flake8>=5.0.4\",\n    \"pytest>=8.3.5\",\n    \"ruff>=0.13.2\",\n]\n\n[[tool.uv.index]]\nname = \"testpypi\"\nurl = \"https://test.pypi.org/simple/\"\npublish-url = \"https://test.pypi.org/legacy/\"\nexplicit = true\n```\n\n**Notable decisions**:\n- âœ… Removed license classifier (PEP 639 compliance)\n- âœ… Clean dev dependencies section\n- âœ… TestPyPI configuration\n- âœ… No pdoc (package doesn't generate docs)\n\n### Workflow Configuration\n\n**deploy_to_test.yml**:\n```yaml\non:\n  create:\n    tags:\n      - 0.*a\n      - 1.*a\n```\n\nPattern: Auto-trigger on alpha tags\n\n**deploy.yml**:\n```yaml\non:\n  release:\n    types: [ published ]\n```\n\nPattern: Requires GitHub Release creation\n\n### Action Versions Used\n\n- `actions/checkout@v5`\n- `actions/setup-python@v6`\n- `astral-sh/setup-uv@v6`\n- `astral-sh/ruff-action@v3`\n\n### Python Matrix\n\n```yaml\nstrategy:\n  matrix:\n    python-version: [3.8, 3.13, 3.14]\n```\n\nThree versions: minimum + two latest\n\n### Flake8 Configuration\n\n```bash\n# Command in workflow\nuv run flake8 --config flake8.cfg\n```\n\nStill used old config file name with explicit flag.\n\n### Secrets Used\n\n- `UV_PUBLISH_TOKEN_PYPI` (production)\n- `UV_PUBLISH_TOKEN_TEST_PYPI` (testing)\n\n### Lessons from http-client\n\n**What worked well**:\n- âœ… TestPyPI testing prevented production issues\n- âœ… Three-version matrix caught compatibility issues\n- âœ… Ruff as blocking check enforced quality\n\n**What was refined in later migrations**:\n- Workflow triggers could be simpler (manual vs alpha tag patterns)\n- Secret naming could follow uv conventions\n- Action versions evolved\n\n---\n\n## python-component Migration\n\n**Repository**: https://github.com/keboola/python-component  \n**Status**: Second migration, refined the approach  \n**Pattern**: Improved on http-client experience\n\n### Migration Context\n\n- **Before**: setup.py + requirements.txt\n- **Before version**: 1.6.13\n- **Test versions**: 1.7.0, 1.7.1, 1.7.2 on Test PyPI\n- **Production version**: 1.8.0 on production PyPI\n- **Commits**: 3 logical commits (lint, pyproject, workflows)\n\n### The 3-Commit Approach\n\n#### Commit 1: \"flake8 config consistent with cookiecutter template ðŸª\"\n\n**Purpose**: Establish clean linting baseline\n\n**Changes**:\n- Renamed: `flake8.cfg` â†’ `.flake8`\n- Updated config to cookiecutter standard\n- Fixed ALL flake8 errors:\n  - `tests/test_dao.py`: Star imports (F403/F405), unused variables (F841)\n  - `tests/test_interface.py`: Unused variables (F841), line length (E501), CRLFâ†’LF\n\n**Key insight**: Fixing linting first made migration diff clean\n\n**Note on test_interface.py**: \n- Showed 1052 line changes due to CRLFâ†’LF normalization\n- Actual code changes: Only 5 lines\n- This was EXPECTED and CORRECT cleanup\n\n#### Commit 2: \"migrate package configuration to pyproject.toml ðŸ“¦\"\n\n**Purpose**: Modernize package metadata\n\n**Changes**:\n- Created comprehensive `pyproject.toml`\n- Removed `License :: OSI Approved :: MIT License` classifier (PEP 639)\n- Deleted `setup.py` and `requirements.txt`\n- Updated `LICENSE` copyright to 2025\n\n**Key insight**: Clean separation from commit 1 made review easy\n\n#### Commit 3: \"uv ðŸ’œ\"\n\n**Purpose**: Update CI/CD to uv\n\n**Changes**:\n- Updated all 4 workflows (push_dev, push_main, deploy, deploy_to_test)\n- Generated `uv.lock`\n- Added ruff as blocking check\n- Updated all action versions\n\n**Key insight**: Workflows + lock file together as atomic change\n\n### Key Files Changed\n\n**pyproject.toml**:\n```toml\n[project]\nname = \"keboola.component\"\nversion = \"0.0.0\"\ndependencies = [\n    \"pygelf\",\n    \"pytz<2021.0\",\n    \"deprecated\",\n]\nrequires-python = \">=3.8\"\n\n[dependency-groups]\ndev = [\n    \"flake8>=5.0.4\",\n    \"pytest>=8.3.5\",\n    \"ruff>=0.13.2\",\n    \"pdoc3\",  # Added for docs generation\n]\n\n[[tool.uv.index]]\nname = \"testpypi\"\nurl = \"https://test.pypi.org/simple/\"\npublish-url = \"https://test.pypi.org/legacy/\"\nexplicit = true\n```\n\n**Notable decisions**:\n- âœ… Added `pdoc3` to dev deps (was ad-hoc before)\n- âœ… Removed license classifier\n- âœ… TestPyPI configuration\n- âœ… Simple dependency list\n\n### Workflow Configuration\n\n**deploy_to_test.yml**:\n```yaml\non: workflow_dispatch\n```\n\n**Improved from http-client**: Manual trigger for full flexibility\n\n**deploy.yml**:\n```yaml\non:\n  push:\n    tags:\n      - '*'\n    branches:\n      - main\n```\n\n**Improved from http-client**: Automatic on tag push (simpler)\n\n### Action Versions Used\n\nInitially:\n- `actions/checkout@v4`\n- `actions/setup-python@v4`\n\n**Note**: Should be updated to `@v5` and `@v6` for consistency\n\n### Python Matrix\n\n```yaml\nstrategy:\n  matrix:\n    python-version: [\"3.8\", \"3.14\"]\n```\n\n**Simplified from http-client**: Only min and max (later refined to add 3.13)\n\n### Flake8 Configuration\n\n```bash\n# Command in workflow\nuv run flake8\n```\n\n**Improved from http-client**: No explicit `--config` flag (uses `.flake8` automatically)\n\n### Docs Workflow\n\n**push_main.yml**:\n```yaml\n- name: Create html documentation ðŸ“š\n  run: |\n    uv sync --all-groups --frozen\n    uv run pdoc --html -f -o ./docs keboola.component\n    mv ./docs/keboola/component/* docs\n    rm -r ./docs/keboola\n```\n\n**Improvement**: \n- Old: `pip install --user pdoc3` (ad-hoc)\n- New: `pdoc3` in dev deps + `uv run pdoc`\n- Properly tracked dependency\n\n### Secrets Used\n\n- `UV_PUBLISH_TOKEN` (production)\n- `UV_PUBLISH_TOKEN_TEST_PYPI` (testing)\n\n**Improved from http-client**: Standard uv naming convention\n\n### Testing Procedure\n\n1. **Local testing**:\n```bash\nuv build\nuv version 1.7.0 --dry-run\nuv run flake8\nuv run ruff check .\nuv run pytest tests\n```\n\n2. **Test PyPI (1.7.0)**:\n```bash\ngit tag 1.7.0\ngit push origin 1.7.0\n# Manually trigger workflow\n```\n\n3. **Installation test**:\n```bash\ncd /tmp && mkdir test_env && cd test_env\nuv init\nuv add --index-url https://test.pypi.org/simple/ \\\n       --extra-index-url https://pypi.org/simple/ \\\n       --index-strategy unsafe-best-match \\\n       keboola.component==1.7.0\nuv run python -c \"import keboola.component; print('âœ…')\"\n```\n\n4. **Production (1.8.0)**:\n```bash\n# After PR merged to main\ngit tag 1.8.0\ngit push origin 1.8.0\n# Auto-triggers deploy workflow\n```\n\n### Issues Encountered\n\n**Issue 1: License Classifier Conflict**\n- Error: `setuptools.errors.InvalidConfigError`\n- Solution: Removed `License :: OSI Approved :: MIT License` classifier\n- Status: Expected, PEP 639 compliance\n\n**Issue 2: CRLF Line Endings**\n- Symptom: 1052 lines changed in test_interface.py\n- Cause: CRLFâ†’LF normalization during flake8 fixes\n- Status: Expected cleanup, actual changes only 5 lines\n\n**Issue 3: Initial 0.0.0 Publish**\n- Symptom: First test publish used version 0.0.0\n- Cause: Workflow triggered without tag\n- Solution: Create tag first, then trigger workflow manually\n\n### Lessons from python-component\n\n**What worked excellently**:\n- âœ… 3-commit structure very reviewable\n- âœ… Lint-first prevented noisy diffs\n- âœ… Manual Test PyPI trigger gave full control\n- âœ… Version strategy (1.7.x test, 1.8.0 prod) clear and effective\n- âœ… Ruff blocking enforced quality immediately\n\n**Refinements for future migrations**:\n- Update action versions to latest (@v5, @v6)\n- Add 3.13 to Python matrix (min + 2 latest)\n- Document CRLF normalization as expected\n- Add pdoc3 to dev deps proactively\n\n---\n\n## Comparison and Lessons Learned\n\n### What Both Migrations Got Right\n\n| Aspect | Implementation |\n|--------|----------------|\n| **Test-first approach** | Both tested on Test PyPI before production |\n| **Version strategy** | Clear separation of test and production versions |\n| **Action updates** | Updated to modern GitHub Actions versions |\n| **Ruff enforcement** | Added as blocking quality gate |\n| **Lock file** | Generated and committed `uv.lock` |\n| **Documentation** | Comprehensive commit messages with emoji |\n\n### Evolution from http-client to python-component\n\n| Aspect | http-client | python-component | Improvement |\n|--------|-------------|------------------|-------------|\n| **Test trigger** | Alpha tag pattern | workflow_dispatch | More flexible |\n| **Deploy trigger** | release: published | push: tags | Simpler workflow |\n| **Secret naming** | UV_PUBLISH_TOKEN_PYPI | UV_PUBLISH_TOKEN | Standard convention |\n| **Flake8 flag** | --config flake8.cfg | (no flag) | Cleaner |\n| **Python matrix** | [3.8, 3.13, 3.14] | [3.8, 3.14] | Minimal (could add 3.13) |\n| **Action versions** | @v5, @v6 | @v4 | http-client better |\n| **Commit structure** | Atomic | 3 logical commits | More reviewable |\n\n### Best Practices Established\n\n#### 1. Commit Strategy\n\n**Pattern**: 3 logical commits\n- Lint baseline (prevents noise)\n- Package metadata (clear separation)\n- CI/CD workflows (tooling update)\n\n**Flexibility**: Adapt as needed (2-4 commits acceptable)\n\n#### 2. Version Strategy\n\n**Pattern**: Two consecutive minors\n- Next minor: Testing (X.Y+1.x)\n- Following minor: Production (X.Y+2.0)\n\n**Example**: 1.6.13 â†’ 1.7.x (test) â†’ 1.8.0 (prod)\n\n#### 3. Testing Strategy\n\n**Always**:\n1. Local testing first\n2. Test PyPI second\n3. Production last\n4. Verify installation after each\n\n#### 4. Workflow Patterns\n\n**Standard**:\n- Manual Test PyPI trigger (workflow_dispatch)\n- Automatic production on tag push\n- Ruff blocking (no continue-on-error)\n- Latest action versions\n- Frozen dependencies (--frozen)\n\n#### 5. Python Matrix\n\n**Optimal**: [MIN_SUPPORTED, \"3.13\", \"3.14\"]\n- Minimum supported (compatibility floor)\n- Two latest (future-proofing)\n- Skips intermediates (efficiency)\n\n#### 6. Documentation\n\n**Include in migration**:\n- Comprehensive commit messages\n- PR description with testing results\n- Update README if references build system\n- Announcement to team\n\n### Common Pitfalls and Solutions\n\n| Pitfall | Solution |\n|---------|----------|\n| Forgetting to remove license classifier | Always remove with PEP 639 |\n| Not testing locally first | Run all checks before pushing |\n| Skipping Test PyPI | Always test there first |\n| Wrong secret names | Use UV_PUBLISH_TOKEN standard |\n| Old action versions | Update to @v5, @v6 |\n| Missing pdoc3 from dev deps | Add if docs workflow exists |\n| Not committing uv.lock | Always commit after uv sync |\n| CRLF concerns | Expect and embrace normalization |\n\n### Recommendations for Future Migrations\n\nBased on both examples:\n\n**DO**:\n- âœ… Fix linting first (lint-first principle)\n- âœ… Use 3 logical commits (reviewability)\n- âœ… Test on Test PyPI first (safety)\n- âœ… Use version strategy (X.Y+1 test, X.Y+2 prod)\n- âœ… Make ruff blocking (quality gate)\n- âœ… Use latest action versions (@v5, @v6)\n- âœ… Document thoroughly (PR, commits, announcements)\n\n**DON'T**:\n- âŒ Skip local testing\n- âŒ Forget to commit uv.lock\n- âŒ Use old action versions\n- âŒ Skip Test PyPI testing\n- âŒ Combine unrelated changes\n- âŒ Force push to main\n- âŒ Use pip or uv pip (only uv add)\n\n### Success Metrics\n\nBoth migrations succeeded by achieving:\n- âœ… Zero production issues\n- âœ… Clean, reviewable changes\n- âœ… Fast CI builds\n- âœ… Deterministic dependencies\n- âœ… Modern tooling adoption\n- âœ… Team knowledge transfer\n\n### Timeline Reference\n\n**python-component migration**:\n- **Planning**: Review http-client experience\n- **Implementation**: 3 commits over 1 session\n- **Testing**: Multiple Test PyPI releases (1.7.0, 1.7.1)\n- **Production**: Single release (1.8.0)\n- **Total time**: Same-day migration and release\n\n**Efficiency note**: With established pattern, future migrations should be similarly fast.\n\n---\n\n## Conclusion\n\nBoth migrations demonstrate that the pattern works reliably:\n\n1. **Lint first** - Clean baseline\n2. **Migrate metadata** - Modern config\n3. **Update workflows** - Modern tooling\n4. **Test thoroughly** - Test PyPI first\n5. **Release confidently** - Production PyPI\n\nThe refinements from http-client to python-component show continuous improvement while maintaining core principles.\n\n**For your next migration**: Follow python-component pattern with latest action versions!\n",
        "plugins/component-developer/skills/migrate-component-to-uv/references/migration-guide.md": "# Complete Migration Guide: setup.py to uv\n\nThis guide provides comprehensive details for migrating Keboola Python packages from legacy build systems to modern uv.\n\n## Table of Contents\n\n1. [Commit Philosophy](#commit-philosophy)\n2. [Version Strategy](#version-strategy)\n3. [Detailed Migration Steps](#detailed-migration-steps)\n4. [Python Matrix Configuration](#python-matrix-configuration)\n5. [Docs Workflow Migration](#docs-workflow-migration)\n\n---\n\n## Commit Philosophy\n\n### Why Logical Commits Matter\n\n**Goal**: Create commits that tell a story and are easy to review, revert, and understand\n\n**Benefits:**\n- **Reviewability**: Each commit has clear, focused scope\n- **Debuggability**: Git bisect works effectively\n- **Rollback**: Can revert specific changes independently\n- **Documentation**: Commit history serves as change log\n\n### The 3-Commit Pattern (Guideline, Not Dogma)\n\nThis pattern emerged from successful python-http-client and python-component migrations:\n\n#### Commit 1: Linting Baseline\n**Purpose**: Establish clean code quality baseline\n**Files**: `.flake8`, test files, source files\n**Why first**: Prevents linting fixes from polluting migration diffs\n\n#### Commit 2: Package Metadata\n**Purpose**: Modernize package configuration\n**Files**: `pyproject.toml` (new), `setup.py` (deleted), `requirements.txt` (deleted), `LICENSE`\n**Why separate**: Clear separation between configuration and tooling\n\n#### Commit 3: CI/CD Workflows\n**Purpose**: Update automation to use uv\n**Files**: `.github/workflows/*.yml`, `uv.lock`\n**Why last**: Builds on clean code + modern config\n\n### Flexibility in Practice\n\n**The pattern is flexible**:\n- **2 commits**: Combine lint fixes with pyproject.toml if changes are small\n- **4 commits**: Separate individual workflows if changes are complex\n- **Different order**: Could do pyproject.toml first if no linting issues\n\n**Key principle**: Each commit should be:\n1. **Atomic**: Complete, functional change\n2. **Logical**: Clear single purpose\n3. **Reviewable**: Easy to understand in isolation\n\n### The Lint-First Principle\n\n**Why fix linting BEFORE migrating:**\n\n1. **Clean diff**: Migration changes are visible without noise\n2. **Attribution**: Clear what's a fix vs what's migration\n3. **Early detection**: Reveals code quality issues before they hide in migration\n4. **Review efficiency**: Reviewer can focus on actual changes\n\n**Example**: Without lint-first:\n```diff\n# Hard to review - is this line ending change part of migration?\n-def foo(x):\n+def foo(x):\n     return x\n```\n\n**With lint-first**:\n```diff\n# Commit 1: Just the line ending fix\n# Commit 2: Clean migration changes only\n```\n\n---\n\n## Version Strategy\n\n### The Two-Minor Approach\n\n**Pattern**: Use two consecutive minor versions for testing and production\n\n**Example (starting from 1.6.13):**\n- **Testing phase**: 1.7.0, 1.7.1, 1.7.2 on Test PyPI\n- **Production release**: 1.8.0 on production PyPI\n\n**Why this works:**\n- Clear separation between test and production releases\n- Test version numbers will never conflict with production\n- Easy to identify which version is which (odd=test, even=prod works but not required)\n\n### Flexibility in Format\n\n**The pattern is flexible on format:**\n\n**Option A**: Simple incrementing (used in python-component)\n```\n1.7.0 â†’ Test PyPI (first test)\n1.7.1 â†’ Test PyPI (bug fixes found)\n1.7.2 â†’ Test PyPI (final test)\n1.8.0 â†’ Production PyPI (release)\n```\n\n**Option B**: Alpha notation (used in python-http-client)\n```\n1.7.0a1 â†’ Test PyPI (alpha 1)\n1.7.0a2 â†’ Test PyPI (alpha 2)\n1.7.0    â†’ Test PyPI (release candidate)\n1.8.0    â†’ Production PyPI (release)\n```\n\n**Key principle**: Pattern consistency matters more than exact format\n\n### For Future Migrations\n\n**When migrating from version X.Y.Z:**\n1. **Next minor** (X.Y+1.x) = Testing versions\n2. **Following minor** (X.Y+2.0) = Production release\n\n**Examples:**\n- From 2.5.3 â†’ Test as 2.6.x â†’ Release 2.7.0\n- From 3.12.0 â†’ Test as 3.13.x â†’ Release 3.14.0\n\n---\n\n## Detailed Migration Steps\n\n### Pre-Migration Checklist\n\n**Before starting any changes:**\n\n```bash\n# 1. Ensure clean working tree\ngit status  # Should be clean\n\n# 2. Create migration branch\ngit checkout -b uv\n\n# 3. Check current version\n# Either from setup.py or PyPI\npython setup.py --version  # Or check PyPI\n\n# 4. Verify tests pass with old system\npython -m pytest tests\n\n# 5. Check Python version requirements\ngrep python_requires setup.py\n```\n\n### Step 1: Linting Baseline (Commit 1)\n\n**1.1: Update flake8 configuration**\n\nIf `flake8.cfg` exists:\n```bash\nmv flake8.cfg .flake8\n```\n\nCreate or update `.flake8` to cookiecutter standard:\n```bash\ncat > .flake8 << 'EOF'\n[flake8]\nexclude = __pycache__, .git, .venv, venv, docs\nignore = E203,W503\nmax-line-length = 120\nEOF\n```\n\n**1.2: Install flake8 and run checks**\n\n```bash\n# If setup.py exists\npython -m pip install flake8\n\n# Or use uv immediately\nuv add --dev flake8\n\n# Run flake8\nflake8 src/ tests/\n```\n\n**1.3: Fix all errors systematically**\n\nCommon error types and fixes:\n\n**F403/F405 - Star imports:**\n```python\n# Bad\nfrom module import *\n\n# Good\nfrom module import SpecificClass, specific_function\n```\n\n**F841 - Unused variables:**\n```python\n# Bad\nx = some_function()  # x never used\n\n# Good\nsome_function()  # Don't assign if not used\n# Or: _ = some_function()  # Explicit ignore\n```\n\n**E501 - Line too long:**\n```python\n# Bad\nsome_function(very_long_argument_name, another_long_argument, yet_another_argument, and_more)\n\n# Good\nsome_function(\n    very_long_argument_name,\n    another_long_argument,\n    yet_another_argument,\n    and_more\n)\n```\n\n**E231 - Missing whitespace:**\n```python\n# Bad\nx=[1,2,3]\n\n# Good\nx = [1, 2, 3]\n```\n\n**CRLF line endings:**\n```bash\n# Convert all Python files\nfind . -name \"*.py\" -exec dos2unix {} \\;\n\n# Or let your editor fix it automatically\n```\n\n**1.4: Verify all issues fixed**\n\n```bash\nflake8 src/ tests/\n# Should output nothing\n```\n\n**1.5: Commit**\n\n```bash\ngit add .flake8 src/ tests/\ngit commit -m \"flake8 config consistent with cookiecutter template ðŸª\"\n```\n\n### Step 2: Package Metadata (Commit 2)\n\n**2.1: Extract dependencies from setup.py**\n\n```bash\n# View current dependencies\ngrep -A 20 \"install_requires\" setup.py\ngrep -A 10 \"setup_requires\" setup.py\ngrep -A 10 \"tests_require\" setup.py\n```\n\n**2.2: Create pyproject.toml**\n\nUse the template from `templates/pyproject.toml.template` and customize:\n\n```toml\n[project]\nname = \"your.package\"\nversion = \"0.0.0\"  # Replaced by git tags in CI\ndependencies = [\n    # Copy from install_requires\n]\nrequires-python = \">=3.8\"  # Or higher if already specified\n\n[dependency-groups]\ndev = [\n    \"flake8>=5.0.4\",\n    \"pytest>=8.3.5\",\n    \"ruff>=0.13.2\",\n    # Add pdoc3 if docs exist\n    # Add other dev deps from setup_requires, tests_require\n]\n```\n\n**IMPORTANT**: Remove `License :: OSI Approved :: MIT License` classifier (PEP 639)\n\n**2.3: Delete old files**\n\n```bash\ngit rm setup.py\ngit rm requirements.txt  # if exists\n```\n\n**2.4: Update LICENSE**\n\n```bash\n# Update copyright year to current year\nsed -i 's/Copyright (c) 20[0-9][0-9]/Copyright (c) 2026/' LICENSE\n```\n\n**2.5: Commit**\n\n```bash\ngit add pyproject.toml LICENSE\ngit commit -m \"migrate package configuration to pyproject.toml ðŸ“¦\"\n```\n\n### Step 3: CI/CD Workflows (Commit 3)\n\n**3.1: Update all workflows**\n\nFor each workflow file in `.github/workflows/`:\n- `push_dev.yml` (always present)\n- `deploy.yml` (always present)\n- `deploy_to_test.yml` (always present)\n- `push_main.yml` (only if docs exist)\n\nSee `references/workflow-templates.md` for complete YAML templates.\n\n**3.2: Generate uv.lock**\n\n```bash\nuv sync --all-groups\n```\n\n**3.3: Test locally**\n\n```bash\n# Test build\nuv build\n\n# Test version command\nuv version 1.7.0 --dry-run\n\n# Run checks\nuv run flake8\nuv run ruff check .\nuv run pytest tests\n```\n\n**3.4: Commit**\n\n```bash\ngit add .github/workflows/*.yml uv.lock\ngit commit -m \"uv ðŸ’œ\"\n```\n\n---\n\n## Python Matrix Configuration\n\n### Strategy: Minimum + Two Latest\n\n**Purpose**: Ensure compatibility at minimum supported version + test against latest Python\n\n**Formula**:\n```python\nmin_supported = max(3.8, current_requires_python)\nmatrix = [min_supported, \"3.13\", \"3.14\"]\n```\n\n**Examples in practice**:\n\n**Package supporting Python â‰¥3.8:**\n```yaml\nstrategy:\n  matrix:\n    python-version: [\"3.8\", \"3.13\", \"3.14\"]\n```\n\n**Package supporting Python â‰¥3.10:**\n```yaml\nstrategy:\n  matrix:\n    python-version: [\"3.10\", \"3.13\", \"3.14\"]\n```\n\n**Package supporting Python â‰¥3.12:**\n```yaml\nstrategy:\n  matrix:\n    python-version: [\"3.12\", \"3.13\", \"3.14\"]\n```\n\n### Rationale\n\n**Why minimum + two latest (not all versions)?**\n\n1. **Compatibility floor**: Ensures package works at minimum supported version\n2. **Future-proofing**: Tests against newest Python releases\n3. **CI efficiency**: Faster builds (3 versions vs 7 versions)\n4. **Coverage**: Catches both old and new Python issues\n\n**What we skip**: Intermediate versions (3.9, 3.11) that are well-covered by min/max testing\n\n---\n\n## Docs Workflow Migration\n\n### Detection Logic\n\n**Check if docs workflow exists and uses pdoc:**\n\n```bash\nif [ -f .github/workflows/push_main.yml ]; then\n    if grep -q pdoc .github/workflows/push_main.yml; then\n        echo \"Docs workflow exists - include in migration\"\n    fi\nfi\n```\n\n### Old Pattern (pre-migration)\n\n```yaml\n- name: Create html documentation ðŸ“š\n  run: |\n    pip install --user pdoc3  # Ad-hoc install\n    python setup.py install\n    pdoc --html -f -o ./docs package.module\n```\n\n**Problems:**\n- pdoc3 not tracked in dependencies\n- Requires setup.py\n- Uses pip\n\n### New Pattern (post-migration)\n\n**Step 1**: Add pdoc3 to pyproject.toml:\n```toml\n[dependency-groups]\ndev = [\n    \"flake8>=5.0.4\",\n    \"pytest>=8.3.5\",\n    \"ruff>=0.13.2\",\n    \"pdoc3\",  # Add this\n]\n```\n\n**Step 2**: Update workflow:\n```yaml\n- name: Create html documentation ðŸ“š\n  run: |\n    uv sync --all-groups --frozen  # Installs pdoc3 from dev deps\n    uv run pdoc --html -f -o ./docs package.module\n    mv ./docs/package/module/* docs\n    rm -r ./docs/package\n```\n\n**Improvements:**\n- âœ… pdoc3 properly tracked in pyproject.toml\n- âœ… No setup.py dependency\n- âœ… Uses modern uv\n- âœ… Deterministic (locked version)\n\n### When to Include push_main.yml\n\n**Include if:**\n- File exists: `.github/workflows/push_main.yml`\n- AND contains: `pdoc` keyword\n- AND package generates HTML docs\n\n**Skip if:**\n- File doesn't exist\n- Or file exists but doesn't use pdoc\n- Or package doesn't generate docs\n\n---\n\n## Post-Migration Checklist\n\nAfter completing all commits:\n\n- [ ] All tests pass locally with uv\n- [ ] `uv build` succeeds\n- [ ] `uv run flake8` passes\n- [ ] `uv run ruff check .` passes\n- [ ] Branch pushed to GitHub\n- [ ] Test tag created and pushed\n- [ ] Test PyPI workflow triggered manually\n- [ ] Package appears on Test PyPI\n- [ ] Package installable from Test PyPI\n- [ ] Import test successful\n- [ ] PR created with proper description\n- [ ] PR approved and merged\n- [ ] Production tag created\n- [ ] Production deployment successful\n- [ ] Package verified on production PyPI\n\n---\n\n## Best Practices\n\n1. **Commit granularity**: Prefer smaller, logical commits over large monolithic ones\n2. **Test between commits**: Ensure each commit leaves the repository in working state\n3. **Clear messages**: Use descriptive commit messages with emoji for clarity\n4. **Branch naming**: Use descriptive branch names like `uv` or `migrate-to-uv`\n5. **Force push carefully**: Only force push to feature branches, never to main\n6. **Tag management**: Delete and recreate test tags if needed, never production tags\n7. **Documentation**: Update README if it references old build system\n8. **Communication**: Notify team of changes, especially about new installation methods\n\n---\n\n## References\n\n- PEP 517: A build-system independent format for source trees\n- PEP 518: Specifying Minimum Build System Requirements\n- PEP 639: Improving License Clarity with Better Package Metadata\n- uv documentation: https://docs.astral.sh/uv/\n- agentskills.io: https://agentskills.io/\n",
        "plugins/component-developer/skills/migrate-component-to-uv/references/troubleshooting.md": "# Troubleshooting Guide\n\nCommon issues encountered during migration from setup.py to uv, and their solutions.\n\n---\n\n## Build Issues\n\n### Error: License Classifier Conflict\n\n**Symptom:**\n```\nsetuptools.errors.InvalidConfigError: License classifiers have been superseded by \nlicense expressions (see https://peps.python.org/pep-0639/). Please remove:\n\nLicense :: OSI Approved :: MIT License\n```\n\n**Cause**: Modern setuptools (used by uv) follows PEP 639 which deprecates license classifiers in favor of the `license` field.\n\n**Solution:**\nRemove the license classifier from `pyproject.toml` classifiers:\n\n```toml\n# WRONG - will cause error\nclassifiers = [\n    \"License :: OSI Approved :: MIT License\",  # Remove this\n    \"Programming Language :: Python :: 3\",\n]\n\n# CORRECT\nlicense = \"MIT\"  # Keep this\nclassifiers = [\n    \"Programming Language :: Python :: 3\",  # No license classifier\n]\n```\n\n**Status**: This is expected and correct. The migration should always remove license classifiers.\n\n---\n\n### Error: uv version Command Not Found\n\n**Symptom:**\n```\nuv: command 'version' not recognized\n```\n\n**Cause**: Old version of uv that doesn't have the `version` subcommand.\n\n**Solution:**\nUpdate uv to â‰¥0.5.0:\n\n```bash\n# Check current version\nuv --version\n\n# Update uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or if installed via pip\npip install --upgrade uv\n\n# Verify\nuv version --help  # Should show help text\n```\n\n**Note**: In CI, `astral-sh/setup-uv@v6` installs correct version automatically.\n\n---\n\n### Error: Module Not Found During Build\n\n**Symptom:**\n```\nModuleNotFoundError: No module named 'your_package'\n```\n\n**Cause**: Package not properly installed or wrong directory structure.\n\n**Solution:**\n\n1. Check package structure in `pyproject.toml`:\n```toml\n# For src-layout\n[tool.setuptools]\npackages = {find = {where = [\"src\"]}}\n\n# Or explicitly list\npackages = [\"your_package\"]\n```\n\n2. Ensure source files in correct location:\n```\nsrc/\nâ””â”€â”€ your_package/\n    â””â”€â”€ __init__.py\n```\n\n3. Reinstall:\n```bash\nuv sync --all-groups\n```\n\n---\n\n## Git and Line Ending Issues\n\n### Large Diffs in Unchanged Files\n\n**Symptom:**\n```\ntests/test_file.py shows 1000+ line changes but actual code changes are minimal\n```\n\n**Cause**: Files had CRLF (Windows) line endings that were normalized to LF (Unix) during flake8 fixes.\n\n**Status**: **This is EXPECTED and GOOD**\n\n**Explanation:**\n- Python files should use LF line endings (Unix standard)\n- Git diff shows every line as changed because line ending character changed\n- Actual code is identical except for `\\r\\n` â†’ `\\n`\n\n**Verification:**\n```bash\n# Check actual code changes (ignoring whitespace)\ngit diff --ignore-all-space main...your-branch -- file.py\n\n# Should show minimal real changes\n```\n\n**Action**: No action needed. This is correct cleanup that should have happened long ago.\n\n---\n\n### Git Commit Attribution Issues\n\n**Symptom:**\nGit blames all lines in file to you after line ending fix.\n\n**Cause**: Line ending normalization changes all lines technically.\n\n**Solution:**\nUse `.git-blame-ignore-revs` to exclude normalization commits from blame:\n\n```bash\n# Create file\ncat > .git-blame-ignore-revs << 'EOF'\n# Flake8 config update with CRLF normalization\n<commit-hash-of-commit-1>\nEOF\n\n# Configure git to use it\ngit config blame.ignoreRevsFile .git-blame-ignore-revs\n```\n\n---\n\n## Dependency Issues\n\n### Error: Dependency Resolution Failed\n\n**Symptom:**\n```\nÃ— No solution found when resolving dependencies:\n  â•°â”€â–¶ Because package-a requires package-b>=2.0 and package-b==1.5...\n```\n\n**Cause**: Conflicting dependency constraints.\n\n**Solution:**\n\n1. Check dependencies in `pyproject.toml`:\n```toml\ndependencies = [\n    \"package-a>=1.0\",\n    \"package-b>=2.0\",  # Make sure these don't conflict\n]\n```\n\n2. Try resolving without lock:\n```bash\nuv sync --no-lock\n```\n\n3. Update dependencies:\n```bash\nuv add package-a --upgrade\n```\n\n4. Check for platform-specific issues:\n```bash\nuv sync --python 3.8  # Test different Python versions\n```\n\n---\n\n### Error: Package Not Found on Test PyPI\n\n**Symptom:**\n```\nÃ— No solution found when resolving dependencies:\n  â•°â”€â–¶ Because there is no version of your-package==1.7.0\n```\n\n**Cause**: Package exists on PyPI but not Test PyPI (or vice versa), and uv checks PyPI first.\n\n**Solution:**\n\nAdd `--index-strategy unsafe-best-match`:\n\n```bash\nuv add --index-url https://test.pypi.org/simple/ \\\n       --extra-index-url https://pypi.org/simple/ \\\n       --index-strategy unsafe-best-match \\\n       your-package==1.7.0\n```\n\n**Why**: By default, uv only checks first index that has package. This flag allows checking all indexes.\n\n**Security note**: Only use with trusted indexes (PyPI and Test PyPI are both trusted).\n\n---\n\n## CI/CD Issues\n\n### Workflow Doesn't Trigger\n\n**Symptom:**\nPush tag but workflow doesn't run.\n\n**Possible causes and solutions:**\n\n1. **Branch filter**:\n```yaml\n# Check this matches\non:\n  push:\n    tags:\n      - '*'\n    branches:\n      - main  # Tag must be on main branch\n```\n\n2. **Workflow file location**:\n```bash\n# Must be in .github/workflows/\nls .github/workflows/deploy.yml\n```\n\n3. **YAML syntax error**:\n```bash\n# Validate YAML\npython -c \"import yaml; yaml.safe_load(open('.github/workflows/deploy.yml'))\"\n```\n\n4. **Permissions**:\n- Check: Settings â†’ Actions â†’ General â†’ Workflow permissions\n- Should be: Read and write permissions\n\n---\n\n### Secret Not Found in Workflow\n\n**Symptom:**\n```\nError: Environment variable UV_PUBLISH_TOKEN not found\n```\n\n**Solution:**\n\n1. Check secret exists:\n   - Go to: Settings â†’ Secrets â†’ Actions\n   - Verify: `UV_PUBLISH_TOKEN` or `UV_PUBLISH_TOKEN_TEST_PYPI` exists\n\n2. Check secret name in workflow:\n```yaml\nenv:\n  UV_PUBLISH_TOKEN: ${{ secrets.UV_PUBLISH_TOKEN }}\n  # Name must match exactly (case-sensitive)\n```\n\n3. Recreate secret if needed:\n   - Delete old secret\n   - Create new PyPI token: https://pypi.org/manage/account/token/\n   - Add with exact name: `UV_PUBLISH_TOKEN`\n\n---\n\n### Build Fails: uv.lock Out of Sync\n\n**Symptom:**\n```\nerror: The lockfile at `uv.lock` needs to be updated, but `--frozen` was provided\n```\n\n**Cause**: `uv.lock` is out of sync with `pyproject.toml`.\n\n**Solution:**\n\n1. Regenerate lock file:\n```bash\nuv sync --all-groups\n```\n\n2. Commit updated lock:\n```bash\ngit add uv.lock\ngit commit -m \"update uv.lock\"\n```\n\n**Prevention**: Always run `uv sync` after changing `pyproject.toml` dependencies.\n\n---\n\n### Publish Fails: Package Already Exists\n\n**Symptom:**\n```\nerror: Package already exists on PyPI: your-package==1.7.0\n```\n\n**Cause**: Version already published (PyPI doesn't allow re-uploading same version).\n\n**Solution:**\n\n1. **For Test PyPI** (safe to increment):\n```bash\n# Delete old tag\ngit tag -d 1.7.0\ngit push origin :refs/tags/1.7.0\n\n# Create new version\ngit tag 1.7.1\ngit push origin 1.7.1\n```\n\n2. **For Production PyPI** (more careful):\n```bash\n# Cannot reuse version - must increment\ngit tag 1.8.1  # Or 1.9.0 if significant changes\ngit push origin 1.8.1\n```\n\n**Note**: Never use `--skip-existing` - each version should be unique.\n\n---\n\n### Ruff Fails: Code Quality Issues\n\n**Symptom:**\n```\nerror: ruff check failed with code violations\n```\n\n**Cause**: Code doesn't pass ruff checks.\n\n**Solution:**\n\n1. Run ruff locally:\n```bash\nuv run ruff check .\n```\n\n2. Auto-fix what's fixable:\n```bash\nuv run ruff check --fix .\n```\n\n3. Review and fix remaining issues manually\n\n4. If intentional violations, add to `pyproject.toml`:\n```toml\n[tool.ruff]\nignore = [\n    \"E501\",  # Line too long (if intentional)\n]\n```\n\n**Note**: Ruff is blocking by design - maintain code quality standards.\n\n---\n\n## Installation and Usage Issues\n\n### Cannot Install from Test PyPI\n\n**Symptom:**\n```\nNo matching distribution found for your-package==1.7.0\n```\n\n**Cause**: Package dependencies not available on Test PyPI.\n\n**Solution:**\n\nAlways use both indexes:\n\n```bash\nuv add --index-url https://test.pypi.org/simple/ \\\n       --extra-index-url https://pypi.org/simple/ \\\n       --index-strategy unsafe-best-match \\\n       your-package==1.7.0\n```\n\n**Why**: Test PyPI only has packages explicitly uploaded there. Dependencies come from production PyPI.\n\n---\n\n### Package Installs But Import Fails\n\n**Symptom:**\n```\nImportError: No module named 'your_package'\n```\n\n**Possible causes:**\n\n1. **Package name vs import name mismatch**:\n```toml\n# pyproject.toml\nname = \"your-package\"  # With hyphen\n\n# But import uses:\nimport your_package  # With underscore\n```\n\n2. **Wrong package structure**: Check `pyproject.toml`:\n```toml\n[tool.setuptools]\npackages = [\"your_package\"]  # Should match actual directory\n```\n\n3. **Not installed in current environment**:\n```bash\nuv run python -c \"import your_package\"  # Use uv run\n```\n\n---\n\n## Version and Tagging Issues\n\n### Version Not Updated in Built Package\n\n**Symptom:**\nBuilt package still shows `version = \"0.0.0\"`.\n\n**Cause**: Version replacement didn't run or failed.\n\n**Solution:**\n\n1. Check if tag was pushed:\n```bash\ngit ls-remote --tags origin\n```\n\n2. Check workflow condition:\n```yaml\nif: github.ref_type == 'tag'  # Must be exactly this\n```\n\n3. Test version command locally:\n```bash\nuv version 1.7.0 --dry-run\n# Should show: your-package 0.0.0 => 1.7.0\n```\n\n4. Check pyproject.toml has version field:\n```toml\n[project]\nversion = \"0.0.0\"  # Must exist for uv version to work\n```\n\n---\n\n### Accidentally Pushed Wrong Tag\n\n**Symptom:**\nCreated `1.7.0` instead of `1.7.1`.\n\n**Solution for Test PyPI** (safe):\n```bash\n# Delete local tag\ngit tag -d 1.7.0\n\n# Delete remote tag\ngit push origin :refs/tags/1.7.0\n\n# Create correct tag\ngit tag 1.7.1\ngit push origin 1.7.1\n```\n\n**Solution for Production PyPI** (careful):\n```bash\n# Cannot delete from PyPI once published\n# Must create new version\n\n# If not yet published:\ngit push origin :refs/tags/1.7.0  # Delete tag\ngit tag 1.7.1\ngit push origin 1.7.1\n\n# If already published:\n# Accept it and use next version (1.7.2, 1.8.0, etc.)\n```\n\n---\n\n## Documentation Issues\n\n### Docs Generation Fails\n\n**Symptom:**\n```\nModuleNotFoundError: No module named 'pdoc'\n```\n\n**Cause**: `pdoc3` not in dev dependencies.\n\n**Solution:**\n\nAdd to `pyproject.toml`:\n```toml\n[dependency-groups]\ndev = [\n    \"pdoc3\",\n]\n```\n\nThen:\n```bash\nuv sync --all-groups\n```\n\n---\n\n### Docs Commit Fails\n\n**Symptom:**\n```\nfatal: unable to access 'https://github.com/...': Permission denied\n```\n\n**Cause**: Workflow doesn't have permission to push.\n\n**Solution:**\n\nCheck repository settings:\n- Settings â†’ Actions â†’ General â†’ Workflow permissions\n- Select: \"Read and write permissions\"\n- Check: \"Allow GitHub Actions to create and approve pull requests\"\n\n---\n\n## General Debugging Tips\n\n### Enable Verbose Output\n\n```bash\n# Verbose uv output\nuv sync -v\n\n# Very verbose\nuv sync -vv\n\n# Debug level\nuv sync -vvv\n```\n\n### Check Environment\n\n```bash\n# Python version\npython --version\n\n# uv version\nuv --version\n\n# Which Python uv uses\nuv run python --version\n\n# Installed packages\nuv pip list\n```\n\n### Validate Configuration\n\n```bash\n# Check pyproject.toml syntax\npython -c \"import tomllib; tomllib.load(open('pyproject.toml', 'rb'))\"\n\n# Check lock file\ncat uv.lock | head -20\n\n# Test import\nuv run python -c \"import your_package; print(your_package.__version__)\"\n```\n\n### Clean Rebuild\n\n```bash\n# Remove all build artifacts\nrm -rf build/ dist/ src/*.egg-info/\n\n# Remove lock\nrm uv.lock\n\n# Fresh sync\nuv sync --all-groups\n\n# Fresh build\nuv build\n```\n\n---\n\n## Getting Help\n\n### Resources\n\n- uv documentation: https://docs.astral.sh/uv/\n- uv GitHub issues: https://github.com/astral-sh/uv/issues\n- PyPI help: https://pypi.org/help/\n- Keboola developers: https://developers.keboola.com/\n\n### Debugging Checklist\n\nWhen asking for help, provide:\n- [ ] uv version (`uv --version`)\n- [ ] Python version (`python --version`)\n- [ ] Complete error message\n- [ ] Relevant pyproject.toml sections\n- [ ] Steps to reproduce\n- [ ] What you've tried already\n\n### Common \"It Works on My Machine\" Issues\n\n1. **Different uv version**: Check with `uv --version`\n2. **Different Python version**: Check with `python --version`\n3. **Cached dependencies**: Try `uv sync --refresh`\n4. **Different OS**: Test in container or CI\n5. **Environment variables**: Check with `env | grep UV`\n\n---\n\n## Prevention Tips\n\n### Before Migration\n\n- [ ] Ensure clean git state\n- [ ] Run all tests and ensure they pass\n- [ ] Document current version\n- [ ] Backup (git tag current state)\n- [ ] Read this entire guide\n\n### During Migration\n\n- [ ] Test each commit independently\n- [ ] Run `uv sync` after dependency changes\n- [ ] Verify `uv build` succeeds\n- [ ] Check workflows locally with act (if possible)\n- [ ] Keep changes minimal and focused\n\n### After Migration\n\n- [ ] Test on Test PyPI first (always)\n- [ ] Verify installation in clean environment\n- [ ] Check all CI workflows pass\n- [ ] Update documentation\n- [ ] Communicate changes to team\n\n---\n\n## Emergency Rollback\n\nIf migration fails catastrophically:\n\n```bash\n# 1. Don't panic - old setup.py still exists in git history\n\n# 2. Revert all commits\ngit reset --hard <commit-before-migration>\n\n# 3. Or create revert commit\ngit revert <migration-commit-range>\n\n# 4. Force push if needed (feature branch only!)\ngit push origin branch-name --force\n\n# 5. Investigate issue, fix, try again\n```\n\n**Note**: This is why we test on Test PyPI first!\n",
        "plugins/component-developer/skills/migrate-component-to-uv/references/workflow-templates.md": "# GitHub Actions Workflow Templates\n\nThis document contains complete, standardized GitHub Actions workflow templates for Keboola Python packages using uv.\n\nThese templates are based on the successful migrations of:\n- `keboola/python-http-client`\n- `keboola/python-component`\n\n**IMPORTANT**: These are EXACT templates - use them as-is with only package-specific customizations.\n\n---\n\n## Table of Contents\n\n1. [push_dev.yml](#push_devyml) - Development branch testing\n2. [deploy.yml](#deployyml) - Production PyPI deployment\n3. [deploy_to_test.yml](#deploy_to_testyml) - Test PyPI deployment\n4. [push_main.yml](#push_mainyml) - Documentation generation (optional)\n\n---\n\n## push_dev.yml\n\n**Purpose**: Run tests on all branches except main  \n**Triggers**: Push to any branch except main  \n**Matrix**: Minimum supported + 2 latest Python versions\n\n```yaml\nname: Build & Test\n\non:\n  push:\n    branches:\n      - \"**\"\n      - \"!main\"\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.8\", \"3.13\", \"3.14\"]  # Adjust minimum as needed\n\n    steps:\n      - name: Checkout ðŸ›’\n        uses: actions/checkout@v5\n\n      - name: Install uv ðŸ’œ\n        uses: astral-sh/setup-uv@v6\n\n      - name: Install and run ruff ðŸ¶\n        uses: astral-sh/ruff-action@v3\n\n      - name: Set up Python ${{ matrix.python-version }} ðŸ\n        uses: actions/setup-python@v6\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies ðŸ“¦\n        run: |\n          uv sync --all-groups --frozen\n\n      - name: Lint with flake8 â„ï¸\n        run: |\n          uv run flake8\n\n      - name: Test with pytest âœ…\n        run: |\n          uv run pytest tests\n\n      - name: Version replacement based on tag â†”ï¸\n        if: github.ref_type == 'tag'\n        run: |\n          TAG_VERSION=${GITHUB_REF#refs/tags/}\n          echo \"Tag version: $TAG_VERSION\"\n          uv version $TAG_VERSION\n```\n\n### Customization Notes:\n\n- **python-version matrix**: Set to `[MIN_SUPPORTED, \"3.13\", \"3.14\"]` where MIN_SUPPORTED â‰¥ 3.8\n- Example for package requiring Python â‰¥3.10: `[\"3.10\", \"3.13\", \"3.14\"]`\n- **flake8 command**: Uses `.flake8` config file automatically (no `--config` flag needed)\n\n---\n\n## deploy.yml\n\n**Purpose**: Build and publish to production PyPI  \n**Triggers**: Tag push to main branch  \n**Python**: Latest stable (3.14)\n\n```yaml\nname: Build & Upload Python Package to PyPI\n\non:\n  push:\n    tags:\n      - '*'\n    branches:\n      - main\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout ðŸ›’\n        uses: actions/checkout@v5\n\n      - name: Install uv ðŸ’œ\n        uses: astral-sh/setup-uv@v6\n\n      - name: Install and run ruff ðŸ¶\n        uses: astral-sh/ruff-action@v3\n\n      - name: Set up Python ðŸ\n        uses: actions/setup-python@v6\n        with:\n          python-version: \"3.14\"\n\n      - name: Install dependencies ðŸ“¦\n        run: |\n          uv sync --all-groups --frozen\n\n      - name: Lint with flake8 â„ï¸\n        run: |\n          uv run flake8\n\n      - name: Test with pytest âœ…\n        run: |\n          uv run pytest tests\n\n      - name: Version replacement based on tag â†”ï¸\n        if: github.ref_type == 'tag'\n        run: |\n          TAG_VERSION=${GITHUB_REF#refs/tags/}\n          echo \"Tag version: $TAG_VERSION\"\n          uv version $TAG_VERSION\n\n      - name: Build and publish ðŸš€\n        env:\n          UV_PUBLISH_TOKEN: ${{ secrets.UV_PUBLISH_TOKEN }}\n        run: |\n          uv build\n          uv publish\n```\n\n### Customization Notes:\n\n- **Python version**: Always use latest stable (currently 3.14)\n- **Secret name**: Must be `UV_PUBLISH_TOKEN` (standard uv convention)\n- **Trigger**: Automatic on any tag push to main\n\n### Secret Setup:\n\n1. Create PyPI API token: https://pypi.org/manage/account/token/\n2. Add to GitHub: Settings â†’ Secrets â†’ Actions â†’ New repository secret\n3. Name: `UV_PUBLISH_TOKEN`\n4. Value: Your PyPI token (starts with `pypi-`)\n\n---\n\n## deploy_to_test.yml\n\n**Purpose**: Build and publish to Test PyPI  \n**Triggers**: Manual workflow dispatch  \n**Python**: Latest stable (3.14)\n\n```yaml\nname: Build & Upload Python Package To Test PyPI\n\non: workflow_dispatch\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout ðŸ›’\n        uses: actions/checkout@v5\n\n      - name: Install uv ðŸ’œ\n        uses: astral-sh/setup-uv@v6\n\n      - name: Install and run ruff ðŸ¶\n        uses: astral-sh/ruff-action@v3\n\n      - name: Set up Python ðŸ\n        uses: actions/setup-python@v6\n        with:\n          python-version: \"3.14\"\n\n      - name: Install dependencies ðŸ“¦\n        run: |\n          uv sync --all-groups --frozen\n\n      - name: Lint with flake8 â„ï¸\n        run: |\n          uv run flake8\n\n      - name: Test with pytest âœ…\n        run: |\n          uv run pytest tests\n\n      - name: Version replacement based on tag â†”ï¸\n        if: github.ref_type == 'tag'\n        run: |\n          TAG_VERSION=${GITHUB_REF#refs/tags/}\n          echo \"Tag version: $TAG_VERSION\"\n          uv version $TAG_VERSION\n\n      - name: Build and publish ðŸš€\n        env:\n          UV_PUBLISH_TOKEN: ${{ secrets.UV_PUBLISH_TOKEN_TEST_PYPI }}\n        run: |\n          uv build\n          uv publish --index testpypi\n```\n\n### Customization Notes:\n\n- **Trigger**: Manual only - provides full control over testing\n- **Secret name**: Must be `UV_PUBLISH_TOKEN_TEST_PYPI`\n- **Publish command**: Note `--index testpypi` flag\n\n### Secret Setup:\n\n1. Create Test PyPI API token: https://test.pypi.org/manage/account/token/\n2. Add to GitHub: Settings â†’ Secrets â†’ Actions â†’ New repository secret\n3. Name: `UV_PUBLISH_TOKEN_TEST_PYPI`\n4. Value: Your Test PyPI token (starts with `pypi-`)\n\n### Usage:\n\n1. Go to: https://github.com/ORG/REPO/actions/workflows/deploy_to_test.yml\n2. Click \"Run workflow\"\n3. Select branch or tag (e.g., `refs/tags/1.7.0`)\n4. Click \"Run workflow\"\n\n### pyproject.toml Configuration:\n\nEnsure Test PyPI index is configured:\n\n```toml\n[[tool.uv.index]]\nname = \"testpypi\"\nurl = \"https://test.pypi.org/simple/\"\npublish-url = \"https://test.pypi.org/legacy/\"\nexplicit = true\n```\n\n---\n\n## push_main.yml\n\n**Purpose**: Generate and commit HTML documentation  \n**Triggers**: Push to main branch  \n**Python**: Latest stable (3.14)  \n**OPTIONAL**: Only include if package generates docs with pdoc\n\n```yaml\nname: Build & Test\n\non:\n  push:\n    branches:\n      - \"main\"\n\njobs:\n  docs:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout ðŸ›’\n        uses: actions/checkout@v5\n\n      - name: Install uv ðŸ’œ\n        uses: astral-sh/setup-uv@v6\n\n      - name: Set up Python ðŸ\n        uses: actions/setup-python@v6\n        with:\n          python-version: \"3.14\"\n\n      - name: Create html documentation ðŸ“š\n        run: |\n          uv sync --all-groups --frozen\n          uv run pdoc --html -f -o ./docs PACKAGE.MODULE\n          mv ./docs/PACKAGE/MODULE/* docs\n          rm -r ./docs/PACKAGE\n\n      - name: Commit docs ðŸ“\n        run: |\n          git config --global user.name 'KCF'\n          git config --global user.email 'kcf@users.noreply.github.com'\n          git commit --allow-empty -am \"Automated html docs build\"\n          git push\n```\n\n### Customization Notes:\n\n- **PACKAGE.MODULE**: Replace with your package path (e.g., `keboola.component`)\n- **Move commands**: Adjust paths based on your package structure\n- **Requires**: `pdoc3` in `[dependency-groups] dev` section of pyproject.toml\n\n### When to Include:\n\nâœ… **Include this workflow if:**\n- Old workflow exists: `.github/workflows/push_main.yml`\n- AND contains: `pdoc` command\n- AND generates HTML docs\n\nâŒ **Skip this workflow if:**\n- No docs workflow exists\n- Package doesn't generate HTML docs\n\n### pyproject.toml Addition:\n\n```toml\n[dependency-groups]\ndev = [\n    \"flake8>=5.0.4\",\n    \"pytest>=8.3.5\",\n    \"ruff>=0.13.2\",\n    \"pdoc3\",  # Required for docs generation\n]\n```\n\n---\n\n## Workflow Patterns Explained\n\n### Standard Action Versions\n\n**Always use latest:**\n- `actions/checkout@v5` (not @v4)\n- `actions/setup-python@v6` (not @v4)\n- `astral-sh/setup-uv@v6`\n- `astral-sh/ruff-action@v3`\n\n**Why**: Latest versions have better performance, bug fixes, and features\n\n### Consistent Step Names with Emoji\n\nAll Keboola workflows use emoji in step names for quick visual scanning:\n- ðŸ›’ Checkout\n- ðŸ’œ Install uv\n- ðŸ¶ Ruff\n- ðŸ Python\n- ðŸ“¦ Dependencies\n- â„ï¸ Flake8\n- âœ… Pytest\n- â†”ï¸ Version\n- ðŸš€ Publish\n- ðŸ“š Docs\n- ðŸ“ Commit\n\n**KEEP THESE** - they're part of Keboola's standardized workflow style\n\n### Ruff: Blocking, Not Advisory\n\n```yaml\n- name: Install and run ruff ðŸ¶\n  uses: astral-sh/ruff-action@v3\n  # NO continue-on-error: true\n```\n\n**Key**: No `continue-on-error: true` - ruff failures block the build\n\n### Version Replacement Logic\n\n```yaml\n- name: Version replacement based on tag â†”ï¸\n  if: github.ref_type == 'tag'\n  run: |\n    TAG_VERSION=${GITHUB_REF#refs/tags/}\n    echo \"Tag version: $TAG_VERSION\"\n    uv version $TAG_VERSION\n```\n\n**How it works:**\n1. Checks if build triggered by tag (not branch)\n2. Extracts version from tag name (e.g., `refs/tags/1.7.0` â†’ `1.7.0`)\n3. Updates `version = \"0.0.0\"` in pyproject.toml to actual version\n4. Build uses the tagged version\n\n### Frozen Dependencies\n\n```yaml\n- name: Install dependencies ðŸ“¦\n  run: |\n    uv sync --all-groups --frozen\n```\n\n**Why `--frozen`:**\n- Ensures CI uses exact versions from `uv.lock`\n- Prevents surprise dependency updates\n- Deterministic builds\n- Faster (no resolver needed)\n\n---\n\n## Differences from http-client\n\nThe python-component migration refined some patterns from http-client:\n\n| Aspect | http-client | python-component | Standard |\n|--------|-------------|------------------|----------|\n| Action versions | `@v5`, `@v6` | `@v4` | Use **latest** (`@v5`, `@v6`) |\n| Deploy trigger | `release: published` | `push: tags` | Use **`push: tags`** (simpler) |\n| Test trigger | `create: tags: 0.*a` | `workflow_dispatch` | Use **`workflow_dispatch`** (flexible) |\n| Secret name | `UV_PUBLISH_TOKEN_PYPI` | `UV_PUBLISH_TOKEN` | Use **`UV_PUBLISH_TOKEN`** |\n| Python matrix | `[3.8, 3.13, 3.14]` | `[3.8, 3.14]` | Use **min + 2 latest** |\n\n**Use python-component patterns with latest action versions.**\n\n---\n\n## Migration Checklist\n\nWhen updating workflows:\n\n- [ ] All 4 workflow files present (or 3 if no docs)\n- [ ] Action versions updated to latest (`@v5`, `@v6`)\n- [ ] `astral-sh/setup-uv@v6` added\n- [ ] `astral-sh/ruff-action@v3` added (no continue-on-error)\n- [ ] All `pip install` replaced with `uv sync --all-groups --frozen`\n- [ ] All command invocations use `uv run`\n- [ ] Python matrix uses [MIN, 3.13, 3.14]\n- [ ] Version replacement logic present\n- [ ] Secrets correctly named (`UV_PUBLISH_TOKEN`, `UV_PUBLISH_TOKEN_TEST_PYPI`)\n- [ ] Emoji preserved in step names\n- [ ] `uv.lock` generated and committed\n\n---\n\n## Testing Workflows\n\n### Test Locally First\n\nBefore pushing:\n\n```bash\n# Test all checks locally\nuv run flake8\nuv run ruff check .\nuv run pytest tests\nuv build\nuv version 1.7.0 --dry-run\n```\n\n### Test in CI\n\n1. Push branch\n2. Check Actions tab\n3. Verify all jobs pass\n4. Check step outputs\n\n### Test PyPI Workflow\n\n1. Create test tag: `git tag 1.7.0 && git push origin 1.7.0`\n2. Manually trigger workflow\n3. Check Test PyPI: https://test.pypi.org/project/PACKAGE/\n4. Test install\n\n### Production Workflow\n\n1. Merge PR to main\n2. Create release tag: `git tag 1.8.0 && git push origin 1.8.0`\n3. Workflow auto-triggers\n4. Verify on PyPI: https://pypi.org/project/PACKAGE/\n5. Test install\n\n---\n\n## Troubleshooting Workflows\n\n### Workflow doesn't trigger\n\n**Check:**\n- Branch filter correct\n- Tag format matches pattern\n- Secrets exist\n- Repository permissions\n\n### Build fails on uv sync\n\n**Check:**\n- `uv.lock` committed\n- `--frozen` flag present\n- Dependencies valid in pyproject.toml\n\n### Publish fails\n\n**Check:**\n- Secret name correct\n- Secret value is API token (not password)\n- Token has correct scope\n- Package name not already taken\n- Version not already published\n\n### Version not replaced\n\n**Check:**\n- Tag pushed (not just branch)\n- `github.ref_type == 'tag'` condition works\n- `uv version` command succeeds\n\n---\n\n## References\n\n- GitHub Actions documentation: https://docs.github.com/actions\n- uv documentation: https://docs.astral.sh/uv/\n- PyPI API tokens: https://pypi.org/help/#apitoken\n- Test PyPI: https://test.pypi.org/\n",
        "plugins/component-developer/skills/review-component/SKILL.md": "---\nname: reviewer\ndescription: Expert Python/Keboola component code reviewer focusing on architecture, configuration/client patterns, documentation consistency, and Pythonic best practices. Provides actionable feedback with clear location, pattern identification, and fix guidance.\ntools: Glob, Grep, Read, Bash\nmodel: sonnet\ncolor: purple\n---\n\n# Keboola Component Code Reviewer\n\nYou are an expert code reviewer focused on Pythonic Keboola components, clear architecture, and consistent, realistic examples. Your job is not only to find bugs, but to shape the code and docs into something clean, maintainable, and aligned with Keboola component best practices.\n\n> **Note**: This agent's reviewing patterns are inspired by Martin Struzsky's (GitHub: @soustruh) review style, trained on 521 review comments across 141 PRs in the Keboola organization.\n\n## Review Scope\n\nBy default, review unstaged changes from `git diff`. The user may specify different files or scope to review.\n\nWhen reviewing, always consider:\n- `CLAUDE.md` or `AGENTS.md` (if present) and project-specific rules\n- Component developer guides (see Related Documentation below) as authoritative references\n- Keboola Python component conventions and patterns\n\n## Review Approach\n\n### 1. Read and Understand\n\nStart by reading the code thoroughly:\n- Check `git diff` or specified files\n- Read related project documentation (CLAUDE.md, pyproject.toml)\n- Understand the component's purpose and architecture\n\n### 2. Apply Review Principles\n\nFocus on issues in this order of importance:\n\n**Blocking Issues** (must fix before merge):\n- Architecture violations (config/client initialization, separation of concerns)\n- Contradictory or misleading examples in documentation\n- Changes that alter behavior in unexpected ways\n\n**Important Improvements** (strongly recommended):\n- Config-as-model patterns (encapsulate configuration in typed objects)\n- Modern typing syntax (use `list[str]` not `List[str]`, `str | None` not `Optional[str]`)\n- Deprecated typing classes removal\n- Missing type hints on public methods\n\n**Nice-to-Have** (readability):\n- Code formatting and style consistency\n- Import organization\n- Minor simplifications using Pythonic idioms\n\n### 3. Provide Actionable TODOs\n\nFormat findings as specific, actionable TODOs grouped by severity. Each TODO must include:\n1. **Location** - File path and line number(s) (e.g., `src/component.py:45-52`)\n2. **Pattern** - The specific code or pattern that needs to change\n3. **Fix** - Concrete guidance on what to change it to (2-3 sentences max)\n\n### 4. Use Constructive Tone\n\nBe direct but kind, giving authors agency:\n- \"I'd personally make the client an instance variable...\"\n- \"Please consider yourself whether you find them worth implementing or not\"\n- \"Just a couple of remarks, but nothing blocking\"\n- \"LGTM\" when ready\n\n## Confidence Scoring\n\nRate each potential issue on confidence scale 0-100:\n- **0-25**: Low confidence (stylistic preference)\n- **26-50**: Moderate confidence (nitpick)\n- **51-75**: High confidence (real quality issue)\n- **76-100**: Critical (architecture violation, blocking issue)\n\n**Only report issues with confidence â‰¥ 60.** Focus on what truly matters.\n\n## Output Format\n\n**Start with Brief Assessment:**\n- \"This is a great effort, just a couple of sections to clarify\"\n- \"The component.py file is nice and clean\"\n\n**Group by Severity:**\n\n```\n## Blocking Issues\n\n### TODO 1: Move client initialization to __init__\n**Location:** `src/component.py:45-52`\n**Pattern:** `self.client = ApiClient(...)` is created inside `run()` method.\n**Fix:** Move this initialization to `__init__` and store as `self.client`. This allows sync_actions to reuse the client without duplicating logic.\n\n## Important Improvements\n\n### TODO 2: Use modern typing syntax\n**Location:** `src/client.py:12`\n**Pattern:** `from typing import List, Dict, Optional`\n**Fix:** Remove this import. Use built-in generics: `list[str]` instead of `List[str]`, `str | None` instead of `Optional[str]`.\n\n## Nice-to-Have\n\n### TODO 3: Organize imports\n**Location:** `src/component.py:1-15`\n**Pattern:** Imports are not sorted according to ruff conventions.\n**Fix:** Run `ruff check --select I --fix src/component.py` to auto-organize imports.\n\n---\nLGTM with the above changes!\n```\n\n## Related Documentation\n\nFor detailed review principles, patterns, and checklists, see:\n- [Review Principles](../references/review-principles.md) - Detailed rules for architecture, typing, safety, etc.\n- [Review Checklist](../references/review-checklist.md) - Quick reference checklist for components\n- [Review Style Guide](../references/review-style-guide.md) - Tone, phrasing, and output format details\n\nFor Keboola component standards:\n- [Architecture Guide](../guides/component-builder/architecture.md)\n- [Best Practices](../guides/component-builder/best-practices.md)\n- [Code Quality](../guides/component-builder/code-quality.md)\n- [Workflow Patterns](../guides/component-builder/workflow-patterns.md)\n",
        "plugins/component-developer/skills/review-component/references/review-checklist.md": "# Review Checklist\n\nQuick reference checklist for reviewing Keboola Python components. Use this to systematically verify code quality.\n\n## Architecture\n\n- [ ] Configuration and clients initialized in `__init__`, not `run()`\n- [ ] `run()` method is clean orchestrator (< 30 lines)\n- [ ] Complex logic extracted to private methods\n- [ ] Clients stored as instance attributes (`self.client`)\n- [ ] Configuration encapsulated in typed config object (Pydantic BaseModel or dataclass)\n\n## Code Quality\n\n- [ ] All Python files formatted with `ruff format .`\n- [ ] Imports organized with `ruff check --select I --fix`\n- [ ] Type hints on all functions using modern syntax (`list[str]`, not `List[str]`)\n- [ ] No deprecated typing classes (`typing.List`, `typing.Dict`, `typing.Optional`)\n- [ ] `@staticmethod` added where appropriate\n- [ ] CSV processing uses generators for memory efficiency\n- [ ] Error handling uses proper exit codes (1 for user, 2 for system)\n\n## Safety\n\n- [ ] Indexing/popping operations guarded by preconditions\n- [ ] Pagination has explicit stopping conditions (not giant safety limits)\n- [ ] API response handling respects what the remote API provides\n\n## Tooling\n\n- [ ] Examples use `uv sync` and `KBC_DATADIR=data uv run src/component.py`\n- [ ] Using `keboola/cookiecutter-python-component` (not archived templates)\n- [ ] Dependencies not over-locked; old versions questioned\n\n## Documentation\n\n- [ ] No contradictions between documentation and code examples\n- [ ] Redundant comments removed; confusing conditions have clarifying comments\n- [ ] No typos or misleading variable names\n\n## Severity Guidelines\n\n### Blocking Issues (must fix before merge)\n\n- Architecture/ownership violations (config/client patterns, contradictions with guides)\n- Contradictory or misleading examples\n- Initialization done in `run()` instead of `__init__`\n- Changes that alter data selection, pagination behavior, or error handling in ways that likely change component output\n\n### Important Improvements (strongly recommended)\n\n- Moving config/client initialization to `__init__`\n- Using proper config/client classes\n- Up-to-date tooling/commands\n- Missing type hints on public methods\n- Deprecated typing classes (`typing.List`, `typing.Dict`, `typing.Optional`)\n\n### Nice-to-Have / Nits\n\n- Quote style consistency\n- Minor formatting tweaks\n- Tiny readability improvements\n- Typos and grammar fixes\n- Import organization\n\n## Confidence Scoring\n\nRate each potential issue on a scale from 0-100:\n\n- **0-25**: Low confidence. Might be a false positive or stylistic preference not explicitly in guidelines.\n- **26-50**: Moderate confidence. Real issue but might be a nitpick.\n- **51-75**: High confidence. Verified issue that impacts code quality or contradicts guidelines.\n- **76-100**: Critical. Architecture violation, contradictory examples, or blocking issue.\n\n**Only report issues with confidence â‰¥ 60.** Focus on issues that truly matter.\n",
        "plugins/component-developer/skills/review-component/references/review-principles.md": "# Review Principles\n\nDetailed principles for reviewing Keboola Python components. These rules are applied by the reviewer agent to ensure code quality, maintainable architecture, and adherence to best practices.\n\n## 1. Architecture and Responsibilities First\n\nFocus first on the design, not nits. Care deeply about \"who should own what\" in the design.\n\n### Separation of Concerns\n\n- **Component class orchestrates** the work (the `run()` method should be a clear \"table of contents\")\n- **Client classes handle HTTP or external APIs** (separate files for complex integrations)\n- **Configuration** (API keys, endpoints, limits, etc.) should be encapsulated in a dedicated config object (e.g., `ApiConfig` in `configuration.py`), not scattered through the code\n\n### Instance Attributes Over Parameter Threading\n\n- Clients and configuration should be stored on `self`, not re-created or threaded through every method\n- \"I'd personally make the client an instance variable, so it won't be passed to the _fetch_data method either\"\n\n### Watch for Anti-Patterns\n\n- Monolithic `run()` methods with 100+ lines of mixed concerns\n- Business logic mixed with I/O operations in the same method\n- API configuration scattered throughout the codebase instead of encapsulated\n\n## 2. Configuration and Client Initialization\n\nThis is critical. Proper initialization patterns are essential.\n\n### Initialize in Constructor, Not in run()\n\n- Load configuration and initialize clients in `__init__`, not in `run()`\n- This allows sync_actions and other entrypoints to reuse them without duplicating logic\n- \"Please take care of any configuration loading and client initialization on the constructor method, not in the run (as there might be other methods than run using these, defined by the action parameter in config.json file)\"\n\n### Check For\n\n- Configuration loaded once and encapsulated (e.g., an `ApiConfig` or similar object)\n- Clients created once and stored on `self`, not passed around everywhere\n- Component's public methods (`run` and any action-specific handlers) rely on pre-initialized `self.config`, `self.client`, etc.\n- Single, consistent place for configuration parsing and validation\n\n### Flag These Patterns\n\n- Each method reloads config or creates a new client\n- Clients initialized inside `run()` instead of `__init__`\n- Configuration values accessed directly from `self.configuration.parameters` throughout the code instead of through a typed config object\n\n## 3. Documentation and Example Consistency\n\nNotice when examples and recommendations don't line up.\n\n### Cross-Check Everything\n\n- If a snippet is presented as an anti-pattern in one place and as a recommended example in another, call that out\n- \"This code highlighted as an anti-pattern here is exactly the same code we are giving Claude in the architecture.md file as an example\"\n- \"Two code examples directly contradict each other\"\n\n### When Fixing One Example, Update Related Ones\n\n- \"Please update the PlaywrightClient example too, your proposal looks great and clean\"\n- When an inconsistency is fixed in one example, suggest scanning and updating similar examples\n\n## 4. Tooling and Workflows\n\nPrefer modern, non-archived tools and realistic commands.\n\n### Use Current Templates\n\n- Recommend `keboola/cookiecutter-python-component` instead of any archived Bitbucket cookiecutters\n- \"Please do not use the bitbucket archived cookiecutter, use this one: https://github.com/keboola/cookiecutter-python-component\"\n\n### Local Development Commands\n\n- Encourage `uv sync` before running components\n- Encourage examples like: `KBC_DATADIR=data uv run src/component.py`\n- \"I'd strongly encourage Claude Code or any other agent to use `uv sync` before running the component\"\n\n### When You See Outdated Commands\n\n- Propose precise, updated alternatives\n- Don't just flag the issue, provide the correct modern approach\n\n## 5. Pythonic Style and Formatting\n\nBe opinionated but proportionate. Style matters, but it's secondary to architecture and correctness.\n\n### Code Formatting\n\n- Expect examples to be black/ruff-compliant\n- \"Please make sure all code examples comply with black/ruff formatting (I hate looking at mixed quotes)\"\n- \"My eyes hurt when reading a line mixing single and double quotes\"\n- Point out obvious violations: mixed quoting style, formatting that wouldn't survive `ruff format`\n- Use `ruff check --select I --fix` to organize imports automatically\n\n### Modern Typing (Python 3.9+)\n\n- Always prefer built-in generics (`list[str]`, `dict[str, Any]`) over `typing.List`, `typing.Dict`\n- Use `X | None` instead of `typing.Optional[X]`\n- Prefer `collections.abc.Iterator`, `Iterable`, etc. over deprecated counterparts in `typing`\n- \"Please do not use this deprecated class for typing\" - flag deprecated typing classes as non-blocking but clear \"please fix this\"\n- Import library-specific types (e.g., `MessageParam` from anthropic)\n- Check the project's Python version before suggesting newer syntax (match/case requires 3.10+)\n\n### Pythonic Patterns\n\n- Use `@staticmethod` decorator when method doesn't use `self`\n- Proper type hints on all functions and variables\n- Clear, self-documenting method names that eliminate need for comments\n- Extract logic blocks > 10-15 lines into separate methods\n\n### Run Method as Orchestrator\n\n- Keep `run()` method clean (< 30 lines ideally)\n- Should read like a \"table of contents\" of the component workflow\n- Extract complex logic to well-named private methods\n\n## 6. Config as Model / Dataclass Patterns\n\nWhen multiple related config fields are used together, suggest a dedicated config class.\n\n### When to Suggest Config Classes\n\n- Multiple parameters repeatedly accessed from `self.configuration.parameters`\n- Related fields for a client (API keys, endpoints, limits)\n- Destination or source configurations with multiple fields\n\n### Example Pattern\n\n```python\nclass AirtableClientConfiguration(BaseModel):\n    base_id: str = Field(alias=\"base_id\", default=\"\")\n    api_token: str = Field(alias=\"#api_token\")\n    destination: Destination = Field(default_factory=Destination)\n```\n\n\"Structuring the configuration like this will help you, your future you, your colleagues and your LLM partner to handle the code more efficiently\"\n\n## 7. Simplification Without Being Clever\n\nPrefer common, well-understood idioms to bespoke code where they improve readability.\n\n### Common Simplifications\n\n- `x or None` instead of `x if x != '' else None`\n- Single `.get()` with `elif` chain instead of multiple `.get()` calls\n- Simple `==` instead of membership test with single-item tuple (avoid `\"x\" in (\"x\")` pitfall)\n- `match/case` for multiple conditions (if Python version supports it)\n- `.pop(key, None)` with dummy variable to avoid KeyError: `_ = d.pop(\"key\", None)`\n\n### Before Suggesting Simplification\n\n- Double-check it does not change behavior (truthiness, default values, edge cases)\n- Call these out as \"Nice-to-have / readability improvements\" unless they fix a real bug\n\n## 8. Safety and Robustness\n\nNudge people to think about edge cases and invariants.\n\n### Guard Against Common Issues\n\n- Verify indexing, popping, and unwrapping operations are guarded by clear preconditions\n- \"Is the storage_input_tables variable checked before so we know there will be at least one item inside?\"\n- Use `.pop(key, None)` instead of `.pop(key)` to avoid KeyError\n\n### Pagination and Loop Safety\n\n- Prefer narrow, explicit stopping conditions for loops over giant \"safety limits\"\n- \"Limiting max iterations to 100k repetitions seems really excessive\" - either it doesn't happen or we should terminate on first invalid response\n- Question the last page edge case: \"How about cases when the last page has exactly the same size as PAGE_SIZE?\"\n\n### API Response Handling\n\n- Respect what the remote API gives us: \"I'd just respect what the remote API gives us in the `.paging.next` field, the API knows what it is doing\"\n- Don't silently change URL parsing or response handling without clear justification\n\n## 9. Reference Architecture\n\nBased on analysis of well-structured Keboola components, here's the typical shape of a clean class:\n\n```python\nfrom __future__ import annotations  # For Python 3.8/3.9 compatibility\n\nclass ApiClient:\n    \"\"\"Client for interacting with the API.\"\"\"\n\n    def __init__(\n        self,\n        base_url: str,\n        api_token: str,\n        max_retries: int = 3,\n        timeout: float | None = None,\n        default_headers: dict[str, str] | None = None,\n    ):\n        \"\"\"Initialize client with all configuration in constructor.\"\"\"\n        self.base_url = base_url if base_url.endswith(\"/\") else base_url + \"/\"\n        self._api_token = api_token\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self._default_headers = default_headers or {}\n\n    def _build_url(self, endpoint: str | None = None) -> str:\n        \"\"\"Private helper for URL construction.\"\"\"\n        # Implementation details...\n\n    def _request(self, method: str, endpoint: str, **kwargs) -> Response:\n        \"\"\"Private method handling actual HTTP calls with retries.\"\"\"\n        # Implementation details...\n\n    def get(self, endpoint: str, **kwargs) -> dict:\n        \"\"\"Public method - clean interface for consumers.\"\"\"\n        response = self._request(\"GET\", endpoint, **kwargs)\n        return response.json()\n```\n\n### Key Patterns\n\n- `from __future__ import annotations` at the top for backward compatibility\n- All configuration stored as instance attributes in `__init__`\n- Modern typing syntax (`str | None`, `dict[str, str]`)\n- Private methods prefixed with `_` for internal logic\n- Public methods are thin wrappers with clean interfaces\n- Sensible defaults with `or {}` pattern for optional dicts\n\n## 10. Repository Hygiene and Dependencies\n\n### Stray Files\n\n- Flag extra config/lock files whose purpose isn't obvious (e.g., extra `pyproject.toml` or `uv.lock` in root)\n- Question local scripts in docs/examples folders that shouldn't be there\n- Watch for `.gitignore` patterns that might hide legitimate files\n\n### Dependency Management\n\n- \"Please unlock the http client, csvwriter and utils version\" - avoid over-locking dependencies\n- Call out suspiciously old versions: \"Is there a good reason why this particular package is locked here? The 0.3.1 release is almost 2 years old\"\n- Encourage sensible pinning: pin only what must be pinned\n\n### Python Version\n\n- \"How about upgrading to Python 3.12 at least?\" - gently suggest upgrades when appropriate\n- But respect project constraints: some components still support older Python versions\n\n### Line Endings and Tooling\n\n- Watch for CRLF issues on Windows: \"please update your git settings to `git config --global core.autocrlf true`\"\n- Suggest working in WSL for Windows users to avoid line ending problems\n",
        "plugins/component-developer/skills/review-component/references/review-style-guide.md": "# Review Style Guide\n\nGuidelines for tone, phrasing, and output format when providing code reviews. The goal is to be direct but kind, giving authors agency to make decisions.\n\n## Overall Tone\n\nUse a constructive, calibrated tone that acknowledges effort and what's already good:\n\n- \"This is a great effort, just a couple of sections to clarify\"\n- \"A couple of remarks, but nothing that important\"\n- \"Well, I could imagine improving a couple of sections, but all in all, this is a great effort!\"\n- \"Everything is awesome!\"\n- \"No real problems, just a couple of omissions or strange code constructions\"\n- \"The component.py file is nice and clean\"\n\n## Phrasing That Gives Authors Agency\n\nUse characteristic phrasing that empowers authors to make decisions:\n\n- \"I'd personally make the client an instance variable\"\n- \"As for me, I'd just use...\"\n- \"Please consider yourself whether you find them worth implementing or not\"\n- \"Feel free to leave it as is\"\n- \"Just one little remark to this...\"\n- \"One more thing to address...\"\n- \"Great catch, just update X as well\"\n- \"A couple of remarks, but nothing blocking\"\n- \"Please reconsider yourself\"\n\n## Approval Phrases\n\nFor clean code or after fixes are applied:\n\n- \"LGTM\"\n- \"Looks good now\"\n- \"Seems OK\"\n- \"Everything seems OK now\"\n- \"Thanks for the changes!\"\n\n## Minor Issue Phrases\n\nFor non-blocking suggestions:\n\n- \"Just a couple of glitches (some of them could be found using Pylance/MyPy/ruff though)\"\n- \"Kindly asking for tiny changes\"\n- \"Consider changing this one little thing...\"\n\n## Blocking Issue Phrases\n\nStill kind but clear about severity:\n\n- \"Not happy with X; please fix before merging\"\n- \"Please do not resolve my comments without me\" (for non-trivial changes that need discussion)\n\n## Emoji Usage\n\nUse emojis sparingly to soften tone. Don't overdo it - a small number of relevant emojis when appropriate to match a friendly style.\n\n## Output Format Structure\n\n### 1. Start with Brief Overall Assessment\n\nAcknowledge effort and what's already good (see \"Overall Tone\" above).\n\n### 2. Group Findings by Severity\n\nOrganize into three clear categories:\n\n**Blocking Issues** (must fix before merge)\n\n**Important Improvements** (strongly recommended)\n\n**Nice-to-Have / Nits**\n\n### 3. Format Each Finding as a Specific TODO\n\nEach issue MUST be formatted as a concrete, actionable TODO with 2-3 sentences. Include:\n\n1. **File path and line number** (e.g., `src/component.py:45`)\n2. **The specific pattern or code** that needs to change\n3. **What to change it to** with concrete guidance\n\n### Example TODO Format\n\n```\n## Blocking Issues\n\n### TODO 1: Move client initialization to __init__\n**Location:** `src/component.py:45-52`\n**Pattern:** `self.client = ApiClient(...)` is created inside `run()` method.\n**Fix:** Move this initialization to `__init__` and store as `self.client`. This allows sync_actions to reuse the client without duplicating logic. The `run()` method should just call `self.client.fetch_data()`.\n\n### TODO 2: Encapsulate configuration in typed object\n**Location:** `src/component.py:23-35`\n**Pattern:** Multiple `self.configuration.parameters.get(\"api_key\")` calls scattered throughout.\n**Fix:** Create a `ClientConfig` dataclass or Pydantic model in `configuration.py` that groups these fields. Initialize it once in `__init__` as `self.config = ClientConfig.from_parameters(params)`.\n\n## Important Improvements\n\n### TODO 3: Use modern typing syntax\n**Location:** `src/client.py:12`\n**Pattern:** `from typing import List, Dict, Optional`\n**Fix:** Remove this import. Use built-in generics: `list[str]` instead of `List[str]`, `dict[str, Any]` instead of `Dict[str, Any]`, `str | None` instead of `Optional[str]`.\n\n## Nice-to-Have\n\n### TODO 4: Organize imports\n**Location:** `src/component.py:1-15`\n**Pattern:** Imports are not sorted according to ruff conventions.\n**Fix:** Run `ruff check --select I --fix src/component.py` to auto-organize imports.\n```\n\n## Key Requirements for TODOs\n\n- Be specific about line numbers and the exact code pattern\n- Provide the concrete fix, not just \"consider changing\"\n- Reference the relevant guide if applicable (e.g., \"See architecture.md section on initialization\")\n- Keep each TODO to 2-3 sentences max\n\n## Complete Example Review\n\n```\n## Overall Assessment\n\nThe component.py file is nice and clean, with good separation of concerns. A couple of remarks, but nothing that important.\n\n## Blocking Issues\n\n### TODO 1: Move client initialization to __init__\n**Location:** `src/component.py:45-52`\n**Pattern:** `self.client = ApiClient(...)` is created inside `run()` method.\n**Fix:** Move this initialization to `__init__` and store as `self.client`. This allows sync_actions to reuse the client without duplicating logic.\n\n## Important Improvements\n\n### TODO 2: Use modern typing syntax\n**Location:** `src/configuration.py:12`\n**Pattern:** `from typing import List, Dict, Optional`\n**Fix:** Remove this import. Use built-in generics: `list[str]` instead of `List[str]`, `str | None` instead of `Optional[str]`.\n\n## Nice-to-Have\n\n### TODO 3: Organize imports\n**Location:** `src/component.py:1-15`\n**Pattern:** Imports are not sorted according to ruff conventions.\n**Fix:** Run `ruff check --select I --fix src/component.py` to auto-organize imports.\n\n---\nLGTM with the above changes!\n```\n",
        "plugins/component-developer/skills/test-component/SKILL.md": "---\nname: tester\ndescription: Expert agent for writing and maintaining tests for Keboola Python components. Specializes in datadir tests, unit tests, and integration tests with proper mocking and assertions.\ntools: Glob, Grep, Read, Bash, Write, Edit\nmodel: sonnet\ncolor: green\n---\n\n# Keboola Component Tester\n\nYou are an expert at writing comprehensive tests for Keboola Python components. Your job is to ensure components are thoroughly tested with datadir tests, unit tests, and integration tests.\n\n## Testing Philosophy\n\nKeboola components should be tested at multiple levels:\n\n1. **Datadir Tests** (Priority 1) - Functional tests using production-like data directory structure\n2. **Unit Tests** (Priority 2) - Testing individual functions and methods in isolation\n3. **Integration Tests** (Priority 3) - Testing API interactions with mocked responses\n\n## Testing Approach\n\n### 1. Understand Component Behavior\n\nBefore writing tests:\n- Read the component code (`src/component.py`)\n- Understand what it does (extract, transform, write data)\n- Identify critical paths and edge cases\n- Note external dependencies (APIs, databases)\n\n### 2. Start with Datadir Tests\n\nDatadir tests are the **primary testing method** for Keboola components.\n\n**Why datadir tests?**\n- Mirror production environment exactly\n- Test the complete component workflow\n- Verify input/output handling\n- Validate state management\n- Check manifest generation\n\n**Basic structure:**\n```python\ndef setUp(self):\n    \"\"\"Point to test case directory.\"\"\"\n    path = os.path.join(\n        os.path.dirname(__file__),\n        'data',\n        'test_full_load'\n    )\n    os.environ[\"KBC_DATADIR\"] = path\n\ndef test_full_load(self):\n    \"\"\"Test full data extraction.\"\"\"\n    comp = Component()\n    comp.run()\n\n    # Verify outputs\n    out_dir = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\"\n    self.assertTrue((out_dir / \"output.csv\").exists())\n```\n\n### 3. Add Unit Tests for Complex Logic\n\nWrite unit tests for:\n- Data transformation functions\n- Validation logic\n- Configuration parsing\n- Complex business rules\n\n**Example:**\n```python\ndef test_transform_record(self):\n    \"\"\"Test record transformation logic.\"\"\"\n    result = transform_record({\n        \"id\": \"123\",\n        \"name\": \"Test\",\n        \"value\": \"100\"\n    })\n\n    self.assertEqual(result[\"id\"], \"123\")\n    self.assertEqual(result[\"value\"], 100)  # Converted to int\n```\n\n### 4. Mock External Dependencies\n\nFor API clients and external services, use mocking:\n\n```python\nfrom unittest.mock import patch, MagicMock\n\n@patch('component.ApiClient')\ndef test_api_call(self, mock_client):\n    \"\"\"Test API integration with mocked response.\"\"\"\n    mock_client.return_value.get.return_value = {\n        \"data\": [{\"id\": 1}, {\"id\": 2}]\n    }\n\n    comp = Component()\n    result = comp.fetch_data()\n\n    self.assertEqual(len(result), 2)\n```\n\n## Test Case Requirements\n\n### Datadir Test Structure\n\nEach test case directory must contain:\n\n**1. config.json** - Component configuration\n```json\n{\n  \"parameters\": {\n    \"#api_key\": \"test-key\",\n    \"endpoint\": \"https://api.example.com\",\n    \"limit\": 100\n  }\n}\n```\n\n**2. in/tables/** - Input CSV files (if needed)\n```\nin/tables/input.csv\nin/tables/input.csv.manifest\n```\n\n**3. in/state.json** - Previous state (for incremental tests)\n```json\n{\n  \"last_run\": \"2024-01-01T00:00:00Z\",\n  \"last_id\": 12345\n}\n```\n\n**4. Expected outputs** - What the component should produce\n```\nout/tables/output.csv\nout/tables/output.csv.manifest\nout/state.json\n```\n\n### Comprehensive Test Coverage\n\nTests should cover:\n\n**Happy Path**:\n- [ ] Full load scenario\n- [ ] Incremental load scenario\n- [ ] Empty result set\n- [ ] Single record\n- [ ] Multiple records\n\n**Error Handling**:\n- [ ] Invalid configuration (missing required params)\n- [ ] Authentication failures\n- [ ] API rate limiting\n- [ ] Network errors\n- [ ] Invalid data format\n\n**Edge Cases**:\n- [ ] Special characters in data\n- [ ] Very large datasets\n- [ ] Null values\n- [ ] Empty strings\n- [ ] Unicode characters\n\n**State Management**:\n- [ ] Initial run (no state)\n- [ ] Subsequent runs (with state)\n- [ ] State persistence\n- [ ] State updates\n\n## Common Testing Patterns\n\n### Testing Configuration Validation\n\n```python\ndef test_missing_api_key(self):\n    \"\"\"Test that missing API key raises error.\"\"\"\n    # Remove API key from config\n    with self.assertRaises(ValueError) as context:\n        comp = Component()\n        comp.run()\n\n    self.assertIn(\"api_key\", str(context.exception))\n```\n\n### Testing State Management\n\n```python\ndef test_incremental_load(self):\n    \"\"\"Test incremental data loading.\"\"\"\n    comp = Component()\n    comp.run()\n\n    # Check state was updated\n    state_file = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"state.json\"\n    with open(state_file) as f:\n        state = json.load(f)\n\n    self.assertIn(\"last_run\", state)\n    self.assertGreater(state[\"last_id\"], 0)\n```\n\n### Testing CSV Output\n\n```python\ndef test_output_format(self):\n    \"\"\"Test CSV output has correct format.\"\"\"\n    comp = Component()\n    comp.run()\n\n    output_file = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\" / \"output.csv\"\n\n    with open(output_file, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n\n        # Verify columns\n        self.assertEqual(reader.fieldnames, [\"id\", \"name\", \"value\"])\n\n        # Verify data\n        self.assertGreater(len(rows), 0)\n        self.assertIn(\"id\", rows[0])\n```\n\n### Testing Manifest Generation\n\n```python\ndef test_manifest_created(self):\n    \"\"\"Test that output manifest is created.\"\"\"\n    comp = Component()\n    comp.run()\n\n    manifest = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\" / \"output.csv.manifest\"\n    self.assertTrue(manifest.exists())\n\n    with open(manifest) as f:\n        manifest_data = json.load(f)\n\n    self.assertIn(\"incremental\", manifest_data)\n    self.assertIn(\"primary_key\", manifest_data)\n```\n\n## Output Format\n\nWhen writing tests, provide:\n\n```\n## Test Suite\n\n### Datadir Tests\n\n**Test Case 1: Full Load**\n- Location: `tests/data/test_full_load/`\n- Purpose: Verify complete data extraction\n- Assertions:\n  - Output file created\n  - Correct number of records\n  - Proper manifest generation\n\n**Test Case 2: Incremental Load**\n- Location: `tests/data/test_incremental/`\n- Purpose: Verify state-based incremental processing\n- Assertions:\n  - State file updated\n  - Only new records extracted\n  - Incremental flag set in manifest\n\n### Unit Tests\n\n**test_transform_record()**\n- Tests data transformation logic\n- Verifies type conversions\n- Checks field mappings\n\n**test_validate_config()**\n- Tests configuration validation\n- Verifies required fields\n- Checks parameter types\n\n## Running Tests\n\n```bash\n# Run all tests\nuv run pytest\n\n# Run specific test file\nuv run pytest tests/test_component.py\n\n# Run with coverage\nuv run pytest --cov=src\n\n# Run with verbose output\nuv run pytest -v\n```\n```\n\n## Best Practices\n\n### DO:\n\n- âœ… Start with datadir tests (most important)\n- âœ… Test both happy path and error cases\n- âœ… Use descriptive test names\n- âœ… Keep test data realistic but minimal\n- âœ… Mock external API calls\n- âœ… Verify manifests and state files\n- âœ… Test incremental loading\n- âœ… Check CSV encoding (UTF-8)\n\n### DON'T:\n\n- âŒ Test implementation details\n- âŒ Use real API credentials in tests\n- âŒ Create tests that depend on external services\n- âŒ Write tests without assertions\n- âŒ Forget to clean up test outputs\n- âŒ Test only the happy path\n- âŒ Skip testing error handling\n\n## Related Documentation\n\nFor detailed testing patterns and examples:\n- [Testing Guide](../references/testing.md) - Complete testing strategies and patterns\n\nFor component development:\n- [Architecture Guide](../guides/component-builder/architecture.md)\n- [Best Practices](../guides/component-builder/best-practices.md)\n- [Running and Testing](../guides/component-builder/running-and-testing.md)\n",
        "plugins/component-developer/skills/test-component/references/testing.md": "# Component Testing Guide\n\nComplete guide for testing Keboola Python components using datadir tests and unit tests.\n\n## Overview\n\nKeboola components should be tested with:\n1. **Datadir Tests** - Functional tests using the standard data directory structure\n2. **Unit Tests** - Testing individual functions and methods\n3. **Integration Tests** - Testing API interactions with mocked responses\n\n## Datadir Testing\n\nDatadir testing is the **standard approach** for testing Keboola components. It mirrors the production environment by using the same data directory structure.\n\n### Directory Structure\n\nThe standard structure for datadir tests:\n\n```\ntests/\nâ”œâ”€â”€ test_component.py\nâ””â”€â”€ data/                    # or data_examples/\n    â”œâ”€â”€ test_case_1/\n    â”‚   â”œâ”€â”€ config.json\n    â”‚   â”œâ”€â”€ in/\n    â”‚   â”‚   â”œâ”€â”€ state.json\n    â”‚   â”‚   â”œâ”€â”€ tables/\n    â”‚   â”‚   â”‚   â”œâ”€â”€ input.csv\n    â”‚   â”‚   â”‚   â””â”€â”€ input.csv.manifest\n    â”‚   â”‚   â””â”€â”€ files/\n    â”‚   â””â”€â”€ out/             # Expected outputs (for comparison)\n    â”‚       â”œâ”€â”€ state.json\n    â”‚       â”œâ”€â”€ tables/\n    â”‚       â”‚   â”œâ”€â”€ output.csv\n    â”‚       â”‚   â””â”€â”€ output.csv.manifest\n    â”‚       â””â”€â”€ files/\n    â”œâ”€â”€ test_case_2/\n    â”‚   â””â”€â”€ ...\n    â””â”€â”€ test_case_3/\n        â””â”€â”€ ...\n```\n\n### Basic Test Pattern\n\nThe simplest and most common pattern:\n\n```python\nimport sys\nimport os\n# Add src directory to path for imports\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n\nimport unittest\nfrom pathlib import Path\nfrom component import Component\n\n\nclass TestComponent(unittest.TestCase):\n\n    def setUp(self):\n        \"\"\"Set KBC_DATADIR to point to test data directory.\"\"\"\n        path = os.path.join(\n            os.path.dirname(os.path.realpath(__file__)),\n            'data',\n            'test_full_load'\n        )\n        os.environ[\"KBC_DATADIR\"] = path\n\n    def test_full_load(self):\n        \"\"\"Test full data extraction.\"\"\"\n        comp = Component()\n        comp.run()\n\n        # Assert outputs exist\n        out_dir = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\"\n        self.assertTrue((out_dir / \"output.csv\").exists())\n```\n\n**Key points:**\n- Set `KBC_DATADIR` environment variable to point to test case directory\n- No temporary directories needed - point directly to test data\n- Component reads from `{KBC_DATADIR}/in/` and writes to `{KBC_DATADIR}/out/`\n\n### Test Case Setup\n\nEach test case directory should contain:\n\n**1. config.json** - Component configuration:\n```json\n{\n  \"parameters\": {\n    \"#api_key\": \"test-key\",\n    \"base_id\": \"appTestBase\",\n    \"table_name\": \"tblTestTable\"\n  }\n}\n```\n\n**2. in/state.json** (optional) - Input state for incremental loads:\n```json\n{\n  \"last_run\": \"2024-01-01 00:00:00\"\n}\n```\n\n**3. in/tables/** (optional) - Input CSV files with manifests (for writers/applications)\n\n**4. out/** - Expected output files for comparison (optional but recommended)\n\n### Testing with Mocks\n\nFor API-based components (extractors), mock external API calls:\n\n```python\nimport unittest\nimport mock\nfrom freezegun import freeze_time\nfrom component import Component\n\n\nclass TestComponent(unittest.TestCase):\n\n    def setUp(self):\n        path = os.path.join(\n            os.path.dirname(__file__),\n            'data',\n            'test_api_extraction'\n        )\n        os.environ[\"KBC_DATADIR\"] = path\n\n    @freeze_time(\"2024-01-15\")\n    @mock.patch(\"component.requests.get\")\n    def test_api_extraction(self, mock_get):\n        \"\"\"Test API data extraction with mocked response.\"\"\"\n        # Mock API response\n        mock_response = mock.Mock()\n        mock_response.status_code = 200\n        mock_response.json.return_value = {\n            \"data\": [\n                {\"id\": 1, \"name\": \"Record 1\"},\n                {\"id\": 2, \"name\": \"Record 2\"}\n            ]\n        }\n        mock_get.return_value = mock_response\n\n        # Run component\n        comp = Component()\n        comp.run()\n\n        # Assert API was called correctly\n        mock_get.assert_called_once()\n\n        # Assert output exists\n        out_file = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\" / \"output.csv\"\n        self.assertTrue(out_file.exists())\n```\n\n### Comparing Output Files\n\n**Simple existence check:**\n```python\ndef test_output_exists(self):\n    comp = Component()\n    comp.run()\n\n    out_dir = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\"\n    self.assertTrue((out_dir / \"output.csv\").exists())\n    self.assertTrue((out_dir / \"output.csv.manifest\").exists())\n```\n\n**CSV content comparison:**\n```python\nimport csv\n\ndef test_output_content(self):\n    comp = Component()\n    comp.run()\n\n    # Read expected output\n    expected_file = Path(__file__).parent / \"data\" / \"test_case\" / \"out\" / \"tables\" / \"output.csv\"\n    with open(expected_file, 'r') as f:\n        expected_reader = csv.DictReader(f)\n        expected_rows = list(expected_reader)\n\n    # Read actual output\n    actual_file = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\" / \"output.csv\"\n    with open(actual_file, 'r') as f:\n        actual_reader = csv.DictReader(f)\n        actual_rows = list(actual_reader)\n\n    # Compare (order-independent for dict comparison)\n    self.assertEqual(len(expected_rows), len(actual_rows))\n    for expected, actual in zip(expected_rows, actual_rows):\n        self.assertEqual(expected, actual)\n```\n\n**Manifest comparison:**\n```python\nimport json\n\ndef test_output_manifest(self):\n    comp = Component()\n    comp.run()\n\n    manifest_file = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\" / \"output.csv.manifest\"\n    with open(manifest_file, 'r') as f:\n        manifest = json.load(f)\n\n    # Assert manifest properties\n    self.assertEqual(manifest[\"incremental\"], False)\n    self.assertIn(\"id\", manifest[\"columns\"])\n    self.assertIn(\"name\", manifest[\"columns\"])\n```\n\n**State file comparison:**\n```python\ndef test_state_file(self):\n    comp = Component()\n    comp.run()\n\n    state_file = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"state.json\"\n    with open(state_file, 'r') as f:\n        state = json.load(f)\n\n    # Assert state was updated\n    self.assertIn(\"last_run\", state)\n    self.assertIsNotNone(state[\"last_run\"])\n```\n\n### Multiple Test Cases\n\nCreate multiple test case directories for different scenarios:\n\n```python\nclass TestComponent(unittest.TestCase):\n\n    def _run_test_case(self, case_name):\n        \"\"\"Helper to run a test case by name.\"\"\"\n        path = os.path.join(\n            os.path.dirname(__file__),\n            'data',\n            case_name\n        )\n        os.environ[\"KBC_DATADIR\"] = path\n\n        comp = Component()\n        comp.run()\n\n        return Path(path) / \"out\"\n\n    def test_full_load(self):\n        \"\"\"Test full data load.\"\"\"\n        out_dir = self._run_test_case(\"test_full_load\")\n        self.assertTrue((out_dir / \"tables\" / \"output.csv\").exists())\n\n    def test_incremental_load(self):\n        \"\"\"Test incremental load with state.\"\"\"\n        out_dir = self._run_test_case(\"test_incremental_load\")\n\n        # Verify state was updated\n        with open(out_dir / \"state.json\", 'r') as f:\n            state = json.load(f)\n        self.assertIn(\"last_run\", state)\n\n    def test_empty_result(self):\n        \"\"\"Test handling of empty API response.\"\"\"\n        out_dir = self._run_test_case(\"test_empty_result\")\n\n        # Should create file but with only headers\n        csv_file = out_dir / \"tables\" / \"output.csv\"\n        with open(csv_file, 'r') as f:\n            lines = f.readlines()\n        self.assertEqual(len(lines), 1)  # Only header\n```\n\n### Testing Error Handling\n\nTest that your component fails correctly:\n\n```python\n@mock.patch.dict(os.environ, {\"KBC_DATADIR\": \"./non-existing-dir\"})\ndef test_missing_config_fails(self):\n    \"\"\"Test that component fails with missing config.\"\"\"\n    with self.assertRaises(ValueError):\n        comp = Component()\n\ndef test_invalid_config_fails(self):\n    \"\"\"Test that component fails with invalid config.\"\"\"\n    path = os.path.join(\n        os.path.dirname(__file__),\n        'data',\n        'test_invalid_config'\n    )\n    os.environ[\"KBC_DATADIR\"] = path\n\n    with self.assertRaises(ValueError) as context:\n        comp = Component()\n        comp.run()\n\n    self.assertIn(\"api_key\", str(context.exception))\n```\n\n## Unit Testing\n\nTest individual methods and functions separately:\n\n```python\nclass TestDataTransformation(unittest.TestCase):\n\n    def test_normalize_field_name(self):\n        \"\"\"Test field name normalization.\"\"\"\n        from component import normalize_field_name\n\n        self.assertEqual(normalize_field_name(\"First Name\"), \"first_name\")\n        self.assertEqual(normalize_field_name(\"ID#\"), \"id_\")\n        self.assertEqual(normalize_field_name(\"email@domain\"), \"email_domain\")\n\n    def test_parse_date(self):\n        \"\"\"Test date parsing.\"\"\"\n        from component import parse_date\n\n        result = parse_date(\"2024-01-15\")\n        self.assertEqual(result.year, 2024)\n        self.assertEqual(result.month, 1)\n        self.assertEqual(result.day, 15)\n\n        # Test invalid date\n        with self.assertRaises(ValueError):\n            parse_date(\"invalid-date\")\n```\n\n## Testing Best Practices\n\n### 1. Use Fixtures for Test Data\n\nStore common test data in fixtures:\n\n```python\n@pytest.fixture\ndef sample_api_response():\n    return {\n        \"data\": [\n            {\"id\": 1, \"name\": \"Test 1\"},\n            {\"id\": 2, \"name\": \"Test 2\"}\n        ]\n    }\n\ndef test_process_response(sample_api_response):\n    result = process_api_response(sample_api_response)\n    assert len(result) == 2\n```\n\n### 2. Test Edge Cases\n\nTest boundary conditions and edge cases:\n\n```python\ndef test_empty_response(self):\n    \"\"\"Test handling of empty API response.\"\"\"\n    # Setup test with empty expected data\n\ndef test_large_dataset(self):\n    \"\"\"Test handling of large datasets.\"\"\"\n    # Test with pagination, chunking\n\ndef test_special_characters(self):\n    \"\"\"Test handling of special characters in data.\"\"\"\n    # Test with Unicode, quotes, newlines\n\ndef test_api_rate_limiting(self):\n    \"\"\"Test handling of API rate limits.\"\"\"\n    # Mock 429 responses\n```\n\n### 3. Mock External Dependencies\n\nAlways mock external API calls and services:\n\n```python\n@mock.patch(\"component.api_client.ApiClient.get_data\")\ndef test_api_call(self, mock_get):\n    mock_get.return_value = {\"data\": [...]}\n    # Test component logic\n```\n\n### 4. Use Freezegun for Time-Dependent Tests\n\nFor components with time-dependent logic:\n\n```python\nfrom freezegun import freeze_time\n\n@freeze_time(\"2024-01-15 10:30:00\")\ndef test_incremental_load_from_yesterday(self):\n    # Test will always run as if it's Jan 15, 2024\n    comp = Component()\n    comp.run()\n```\n\n### 5. Clean Up After Tests\n\nEnsure tests don't leave artifacts:\n\n```python\ndef tearDown(self):\n    \"\"\"Clean up after each test.\"\"\"\n    # Remove any temporary files created during test\n    out_dir = Path(os.environ[\"KBC_DATADIR\"]) / \"out\"\n    if out_dir.exists():\n        for file in out_dir.glob(\"**/*\"):\n            if file.is_file():\n                file.unlink()\n```\n\n## Running Tests\n\n### Local Testing\n\n```bash\n# Run all tests\npython -m unittest discover -s tests -p \"test_*.py\"\n\n# Run specific test\npython -m unittest tests.test_component.TestComponent.test_full_load\n\n# Run with verbose output\npython -m unittest discover -s tests -p \"test_*.py\" -v\n```\n\n### With pytest\n\nIf using pytest:\n\n```bash\n# Run all tests\npytest tests/\n\n# Run specific test file\npytest tests/test_component.py\n\n# Run with coverage\npytest --cov=src tests/\n\n# Run specific test\npytest tests/test_component.py::TestComponent::test_full_load\n```\n\n### In CI/CD\n\nAdd to your GitHub Actions workflow:\n\n```yaml\n- name: Run tests\n  run: |\n    python -m unittest discover -s tests -p \"test_*.py\"\n```\n\n## Common Test Scenarios\n\n### Testing Extractors\n\n```python\nclass TestExtractor(unittest.TestCase):\n\n    @mock.patch(\"component.api_client.fetch_data\")\n    def test_extract_full(self, mock_fetch):\n        \"\"\"Test full data extraction.\"\"\"\n        mock_fetch.return_value = [...]\n\n        path = os.path.join(os.path.dirname(__file__), 'data', 'test_full')\n        os.environ[\"KBC_DATADIR\"] = path\n\n        comp = Component()\n        comp.run()\n\n        # Verify output\n        self.assertTrue(Path(path, \"out\", \"tables\", \"data.csv\").exists())\n\n    @mock.patch(\"component.api_client.fetch_data\")\n    def test_extract_incremental(self, mock_fetch):\n        \"\"\"Test incremental extraction using state.\"\"\"\n        mock_fetch.return_value = [...]\n\n        path = os.path.join(os.path.dirname(__file__), 'data', 'test_incremental')\n        os.environ[\"KBC_DATADIR\"] = path\n\n        comp = Component()\n        comp.run()\n\n        # Verify state was updated\n        with open(Path(path, \"out\", \"state.json\"), 'r') as f:\n            state = json.load(f)\n        self.assertGreater(state[\"last_timestamp\"], 0)\n```\n\n### Testing Writers\n\n```python\nclass TestWriter(unittest.TestCase):\n\n    @mock.patch(\"component.api_client.write_data\")\n    def test_write_data(self, mock_write):\n        \"\"\"Test writing data to destination.\"\"\"\n        path = os.path.join(os.path.dirname(__file__), 'data', 'test_write')\n        os.environ[\"KBC_DATADIR\"] = path\n\n        comp = Component()\n        comp.run()\n\n        # Verify API was called with correct data\n        mock_write.assert_called_once()\n        call_args = mock_write.call_args[0][0]\n        self.assertEqual(len(call_args), 10)  # 10 rows written\n```\n\n### Testing Applications\n\n```python\nclass TestApplication(unittest.TestCase):\n\n    def test_transform_data(self):\n        \"\"\"Test data transformation logic.\"\"\"\n        path = os.path.join(os.path.dirname(__file__), 'data', 'test_transform')\n        os.environ[\"KBC_DATADIR\"] = path\n\n        comp = Component()\n        comp.run()\n\n        # Verify transformation output\n        output = Path(path, \"out\", \"tables\", \"transformed.csv\")\n        with open(output, 'r') as f:\n            reader = csv.DictReader(f)\n            rows = list(reader)\n\n        # Assert transformations were applied\n        self.assertTrue(all(row[\"email\"].endswith(\"@example.com\") for row in rows))\n```\n\n## Test Data Management\n\n### Creating Test Data\n\n1. **Run component locally** with sample data:\n   ```bash\n   KBC_DATADIR=data/sample uv run src/component.py\n   ```\n\n2. **Copy output** to test case expected output:\n   ```bash\n   cp -r data/sample/out tests/data/test_case_1/out\n   ```\n\n3. **Adjust expected output** if needed to match desired behavior\n\n### Sensitive Data in Tests\n\nNever commit real credentials or sensitive data:\n\n```json\n{\n  \"parameters\": {\n    \"#api_key\": \"test-mock-key-not-real\",\n    \"endpoint\": \"https://api.example.com\"\n  }\n}\n```\n\nFor tests that need real credentials:\n- Use environment variables in CI/CD\n- Store in GitHub Secrets\n- Load from `.env` file (not committed)\n\n## Debugging Failed Tests\n\n### Print Debug Information\n\n```python\ndef test_with_debug(self):\n    comp = Component()\n    comp.run()\n\n    # Print actual output for debugging\n    out_file = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\" / \"output.csv\"\n    if out_file.exists():\n        with open(out_file, 'r') as f:\n            print(\"Actual output:\")\n            print(f.read())\n```\n\n### Compare Expected vs Actual\n\n```python\ndef test_with_diff(self):\n    comp = Component()\n    comp.run()\n\n    expected = Path(__file__).parent / \"data\" / \"test_case\" / \"out\" / \"tables\" / \"output.csv\"\n    actual = Path(os.environ[\"KBC_DATADIR\"]) / \"out\" / \"tables\" / \"output.csv\"\n\n    with open(expected, 'r') as f1, open(actual, 'r') as f2:\n        expected_lines = f1.readlines()\n        actual_lines = f2.readlines()\n\n        for i, (exp, act) in enumerate(zip(expected_lines, actual_lines)):\n            if exp != act:\n                print(f\"Line {i+1} differs:\")\n                print(f\"Expected: {exp.strip()}\")\n                print(f\"Actual:   {act.strip()}\")\n```\n\n## Related Guides\n\n- [Debugging Guide](debugging.md) - Local testing and debugging techniques\n- [Architecture Guide](architecture.md) - Component structure and patterns\n- [Code Quality Guide](code-quality.md) - Testing and quality standards\n",
        "plugins/dataapp-developer/.claude-plugin/plugin.json": "{\n  \"name\": \"dataapp-developer\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Specialized toolkit for building Streamlit data apps for Keboola deployment with validate â†’ build â†’ verify workflow\",\n  \"author\": {\n    \"name\": \"Keboola :(){:|:&};: s.r.o.\",\n    \"email\": \"support@keboola.com\"\n  },\n  \"mcpServers\": {\n    \"keboola\": {\n      \"type\": \"sse\",\n      \"url\": \"https://mcp.us-east4.gcp.keboola.com/mcp\"\n    },\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@executeautomation/playwright-mcp-server@latest\"\n      ]\n    }\n  }\n}\n",
        "plugins/dataapp-developer/README.md": "# Data App Developer Plugin\n\nA specialized toolkit for building production-ready Streamlit data apps for Keboola deployment. Features a systematic **validate â†’ build â†’ verify** workflow that ensures features work correctly the first time.\n\n## ðŸŽ¯ Available Skills\n\n### Data App Development Skill\n**Skill name**: `dataapp-dev`\n**Activation**: Automatic when working with Streamlit data apps and Keboola\n\nExpert Streamlit data app developer specializing in Keboola deployment. Systematically validates data structures, builds correct implementations, and verifies everything works before you commit.\n\n**Key Innovation**: Three-phase workflow that eliminates debugging cycles:\n1. **VALIDATE** - Check schemas and query sample data using Keboola MCP\n2. **BUILD** - Implement following SQL-first architecture patterns\n3. **VERIFY** - Test in browser and capture screenshots using Playwright MCP\n\n**Use cases:**\n- Build new Keboola data apps from scratch\n- Add features to existing apps (filters, pages, metrics)\n- Debug data app issues with systematic validation\n- Optimize query performance with SQL-first patterns\n- Implement global filters and controls\n- Create analytics dashboards and reports\n- Fix bugs with confidence using visual verification\n\n---\n\n## âœ¨ Core Features\n\n### ðŸ” Automatic Data Validation\nBefore writing any code, the agent validates assumptions:\n- Checks table schemas with `mcp__keboola__get_table`\n- Verifies column names and data types\n- Queries distinct values for categorical columns\n- Tests SQL filter conditions before embedding\n- Confirms data volumes and structure\n\n**Result**: No broken SQL queries, no KeyErrors, no debugging.\n\n### ðŸ—ï¸ SQL-First Architecture\nEnforces best practices for Streamlit + Keboola:\n- Push computation to database, never load large datasets\n- Centralized data access layer (`utils/data_loader.py`)\n- Filter clause functions for reusable WHERE conditions\n- Proper caching with `@st.cache_data(ttl=300)`\n- Environment parity (local dev + production)\n\n**Result**: Fast, scalable apps that work in both environments.\n\n### ðŸŽ¨ Visual Verification\nAfter implementation, automatically tests in browser:\n- Opens app with `mcp__playwright__browser_navigate`\n- Interacts with filters and controls\n- Navigates through all pages\n- Captures screenshots as proof\n- Verifies no errors in UI\n\n**Result**: See it working before you commit. Zero deployment surprises.\n\n### ðŸ›¡ï¸ Bug Prevention\nCatches common issues before they become bugs:\n- Variable name conflicts (same name for SQL clause and UI widget)\n- Session state key collisions\n- Missing column names in queries\n- Incorrect SQL syntax\n- Environment-specific code\n\n**Result**: Clean, maintainable code that works first time.\n\n---\n\n## ðŸ’¡ Usage Examples\n\n### Add a Global Filter\n\n```\nAdd a global filter for user type (external vs internal users)\nto my Streamlit dashboard. Default to showing external users only.\n```\n\nThe skill will automatically activate and guide the implementation.\n\n**What happens:**\n1. âœ… Agent checks table schema for `user_type` column\n2. âœ… Queries distinct values: 'External User', 'Keboola User'\n3. âœ… Tests filter SQL conditions\n4. âœ… Creates `get_user_type_filter_clause()` in data_loader.py\n5. âœ… Adds UI radio buttons to sidebar\n6. âœ… Updates all page modules to use filter\n7. âœ… Opens browser and verifies filter works on all pages\n8. âœ… Takes screenshots showing it working\n\n**Time**: 20 minutes (vs 60+ with traditional approach)\n\n### Debug a KeyError\n\n```\nMy overview page is showing \"KeyError: 'revenue'\" when I filter by date.\nHelp me debug and fix it.\n```\n\n**What happens:**\n1. âœ… Validates table schema for `revenue` column\n2. âœ… Queries sample data to check for NULL values\n3. âœ… Identifies issue (column doesn't exist / NULL handling)\n4. âœ… Fixes query with proper COALESCE or column name\n5. âœ… Verifies fix works in browser\n6. âœ… Shows screenshot of working page\n\n### Create a New Analytics Page\n\n```\nCreate a new \"Cost Analysis\" page that shows:\n- Total costs by month\n- Cost breakdown by team\n- Top 10 most expensive projects\n```\n\n**What happens:**\n1. âœ… Validates cost data tables exist\n2. âœ… Checks column names and types\n3. âœ… Queries sample data to understand structure\n4. âœ… Creates new page module with SQL-first queries\n5. âœ… Adds page to navigation\n6. âœ… Opens browser, navigates to new page\n7. âœ… Verifies charts and metrics display correctly\n8. âœ… Captures screenshots\n\n---\n\n## ðŸŽ¯ Workflow Overview\n\n### Phase 1: VALIDATE Data\n**Always run before writing code:**\n\n```python\n# Check table schema\nmcp__keboola__get_table(\"out.c-analysis.usage_data\")\n# â†’ Verify columns exist, get types, get fully qualified name\n\n# Query sample data\nmcp__keboola__query_data(sql='SELECT DISTINCT \"status\" FROM ...')\n# â†’ Confirm values, test WHERE conditions\n```\n\n### Phase 2: BUILD Implementation\n**Follow SQL-first patterns:**\n\n```python\n# 1. Add filter function to utils/data_loader.py\ndef get_status_filter_clause():\n    if st.session_state.status_filter == 'active':\n        return '\"status\" = \\'active\\''\n    return ''  # No filter\n\n# 2. Build WHERE clause systematically\nwhere_parts = [get_agent_filter_clause()]\nstatus_filter = get_status_filter_clause()\nif status_filter:\n    where_parts.append(status_filter)\nwhere_clause = ' AND '.join(where_parts)\n\n# 3. Use in SQL query\nquery = f'''\n    SELECT \"date\", COUNT(*) as count\n    FROM {get_table_name()}\n    WHERE {where_clause}\n    GROUP BY \"date\"\n'''\n```\n\n### Phase 3: VERIFY Visually\n**Test in browser before committing:**\n\n```python\n# 1. Check app is running\nlsof -ti:8501  # or start: streamlit run app.py\n\n# 2. Navigate and test\nmcp__playwright__browser_navigate(\"http://localhost:8501\")\nmcp__playwright__browser_wait_for(time: 3)\n\n# 3. Take screenshot\nmcp__playwright__browser_take_screenshot(\"verified.png\")\n\n# 4. Test interactions (click filters, navigate pages)\n```\n\n---\n\n## ðŸ“š Best Practices Enforced\n\n### âœ… DO:\n\n- **Always validate data first** using Keboola MCP before writing code\n- **Push computation to database** - aggregate in SQL, not Python\n- **Use fully qualified table names** from `get_table_name()`\n- **Quote all identifiers** in SQL (`\"column_name\"`, not `column_name`)\n- **Cache all queries** with `@st.cache_data(ttl=300)`\n- **Centralize data access** in `utils/data_loader.py`\n- **Initialize session state** with defaults before UI controls\n- **Use unique variable names** to avoid conflicts\n- **Test visually** with Playwright before committing\n- **Handle empty DataFrames** gracefully in UI\n- **Support both environments** (local dev + Keboola production)\n\n### âŒ DON'T:\n\n- **Skip data validation** - always check schemas first\n- **Load large datasets into Python** - aggregate in database\n- **Hardcode table names** - use `get_table_name()` function\n- **Skip visual verification** - test with Playwright\n- **Use same variable name twice** (e.g., for SQL clause AND UI widget)\n- **Forget session state initialization** before creating widgets\n- **Assume columns exist** - validate with Keboola MCP\n- **Commit without screenshots** - prove it works visually\n- **Use unquoted SQL identifiers** - quote everything\n- **Skip error handling** for empty query results\n\n---\n\n## ðŸ“– Architecture Patterns\n\n### SQL-First Design\n\n**Why**: Keboola workspaces are optimized for queries. Loading data into Streamlit doesn't scale.\n\n**Pattern**:\n```python\n# âœ… GOOD - Aggregate in database\nquery = f'''\n    SELECT\n        \"category\",\n        COUNT(*) as count,\n        AVG(\"value\") as avg_value\n    FROM {get_table_name()}\n    WHERE \"date\" >= CURRENT_DATE - INTERVAL '90 days'\n        AND {get_filter_clause()}\n    GROUP BY \"category\"\n'''\n\n# âŒ BAD - Load all data and aggregate in Python\ndf = execute_query(f\"SELECT * FROM {get_table_name()}\")\nresult = df.groupby('category').agg({'value': 'mean'})\n```\n\n### Global Filter Pattern\n\n```python\n# 1. Filter function in utils/data_loader.py\ndef get_user_filter_clause():\n    if 'user_filter' not in st.session_state:\n        st.session_state.user_filter = 'all'\n\n    if st.session_state.user_filter == 'external':\n        return '\"user_type\" = \\'External User\\''\n    elif st.session_state.user_filter == 'internal':\n        return '\"user_type\" = \\'Keboola User\\''\n    return ''\n\n# 2. UI in streamlit_app.py sidebar\nif 'user_filter' not in st.session_state:\n    st.session_state.user_filter = 'external'\n\noption = st.sidebar.radio(\n    \"Users:\",\n    options=['external', 'internal', 'all'],\n    index=['external', 'internal', 'all'].index(st.session_state.user_filter)\n)\n\nif option != st.session_state.user_filter:\n    st.session_state.user_filter = option\n    st.rerun()\n\n# 3. Use in all page modules\nfrom utils.data_loader import get_user_filter_clause\n\nwhere_parts = ['\"status\" = \\'active\\'']\nuser_filter = get_user_filter_clause()\nif user_filter:\n    where_parts.append(user_filter)\nwhere_clause = ' AND '.join(where_parts)\n```\n\n---\n\n## ðŸ”Œ MCP Servers\n\n### Keboola MCP\n**Remote server**: `https://mcp.us-east4.gcp.keboola.com/mcp`\n\nProvides data validation and querying capabilities:\n- `mcp__keboola__get_project_info` - Project metadata and SQL dialect\n- `mcp__keboola__get_table` - Table schemas with column details\n- `mcp__keboola__query_data` - Execute SQL queries with validation\n- `mcp__keboola__list_tables` - Browse available data\n- `mcp__keboola__search` - Find tables and configurations\n\n**Setup**:\n- Automatically configured when plugin is installed\n- Authenticate via OAuth when first used\n- No manual configuration needed\n\n### Playwright MCP\n**Package**: `@executeautomation/playwright-mcp-server`\n\nProvides browser automation and visual testing:\n- `mcp__playwright__browser_navigate` - Open URLs\n- `mcp__playwright__browser_click` - Interact with elements\n- `mcp__playwright__browser_take_screenshot` - Capture screenshots\n- `mcp__playwright__browser_wait_for` - Wait for conditions\n- `mcp__playwright__browser_type` - Enter text\n- `mcp__playwright__browser_evaluate` - Run JavaScript\n\n**Setup**:\n- No configuration needed\n- Browser installs automatically on first use\n- If prompted, run: `mcp__playwright__browser_install`\n\n---\n\n## ðŸ“š Documentation\n\nComprehensive guides included in `skills/dataapp-dev/`:\n\n- **QUICKSTART.md** - 5-minute introduction to the workflow\n- **workflow-guide.md** - Step-by-step examples with real scenarios\n- **best-practices.md** - Deep dive into SQL-first architecture\n- **templates.md** - Copy-paste code patterns\n- **validation-checklist.md** - Quality assurance checklist\n\n---\n\n## ðŸŽ‰ Success Stories\n\n### Before This Plugin\n```\nDeveloper: Add a filter\nClaude: [writes code]\nDeveloper: It's not working, there's a KeyError\nClaude: Let me fix it\n[3-4 iterations of debugging]\nDeveloper: Finally works! 60 minutes spent\n```\n\n### With This Plugin\n```\nDeveloper: Add a filter\nClaude: [validates schema â†’ queries data â†’ builds â†’ tests visually]\nClaude: âœ… Complete! [shows screenshots proving it works]\nDeveloper: Looks perfect! 20 minutes spent\n```\n\n### Real Example\n\n**Task**: Add global filter for external vs internal users\n\n**What the agent did**:\n1. âœ… Validated `user_type` column exists\n2. âœ… Queried distinct values: 'External User' (122), 'Keboola User' (55)\n3. âœ… Tested filter SQL conditions\n4. âœ… Created filter clause function\n5. âœ… Added UI to sidebar\n6. âœ… Updated all 8 page modules\n7. âœ… Fixed variable name conflict before it became a bug\n8. âœ… Opened browser and tested all pages\n9. âœ… Captured screenshots showing it working\n10. âœ… Ready to commit with confidence\n\n**Result**: Feature worked correctly on first try. Zero debugging needed.\n\n---\n\n## ðŸ› ï¸ Plugin Structure\n\n```\nplugins/dataapp-developer/\nâ”œâ”€â”€ .claude-plugin/\nâ”‚   â””â”€â”€ plugin.json          # Plugin config with MCP servers\nâ”œâ”€â”€ skills/\nâ”‚   â””â”€â”€ dataapp-dev/\nâ”‚       â”œâ”€â”€ SKILL.md         # Main skill definition\nâ”‚       â”œâ”€â”€ QUICKSTART.md    # 5-minute guide\nâ”‚       â”œâ”€â”€ workflow-guide.md    # Detailed examples\nâ”‚       â”œâ”€â”€ best-practices.md    # Architecture guide\nâ”‚       â”œâ”€â”€ templates.md     # Code patterns\nâ”‚       â””â”€â”€ validation-checklist.md  # QA checklist\nâ””â”€â”€ README.md                # This file\n```\n\n---\n\n## ðŸ¤ Contributing\n\nTo improve this plugin:\n\n1. Update the skill file in `skills/dataapp-dev/SKILL.md`\n2. Add new patterns to `skills/dataapp-dev/templates.md`\n3. Update this README with new features\n4. Test thoroughly with real Streamlit apps\n5. Submit a pull request\n\n---\n\n## ðŸ“š Resources\n\n- [Streamlit Documentation](https://docs.streamlit.io)\n- [Keboola Developer Docs](https://developers.keboola.com)\n- [Keboola MCP Server](https://github.com/keboola/mcp-server-keboola)\n- [Playwright MCP Server](https://github.com/executeautomation/playwright-mcp-server)\n\n---\n\n**Version**: 1.0.0\n**Maintainer**: Keboola :(){:|:&};: s.r.o.\n**License**: MIT\n",
        "plugins/dataapp-developer/skills/dataapp-dev/QUICKSTART.md": "# Quick Start Guide\n\nGet started with the Keboola Data App Development skill in 5 minutes.\n\n## âš¡ Installation\n\nThis skill is included with the **dataapp-developer** plugin. If you haven't installed it yet:\n\n```bash\n/plugin install dataapp-developer\n```\n\nThe skill will be automatically available once the plugin is installed.\n\n### Prerequisites\n\nThe skill uses two MCP servers that are automatically configured:\n- âœ… **Keboola MCP** - For data validation and querying (OAuth authentication on first use)\n- âœ… **Playwright MCP** - For visual testing (browser installs on first use)\n\n### Start Your Streamlit App\n\n```bash\n# In your data app directory\nstreamlit run streamlit_app.py\n\n# Should see: \"You can now view your Streamlit app in your browser.\"\n# App runs at: http://localhost:8501\n```\n\n## ðŸŽ¯ Using the Skill\n\n### Automatic Activation\n\nThe skill activates automatically when you mention Keboola data app tasks:\n\n```\n\"Add a filter for user type to the dashboard\"\n\"Create a new page showing cost analysis\"\n\"Fix the error on the users page\"\n\"Why is the overview page loading slowly?\"\n```\n\n### Explicit Invocation\n\nYou can also explicitly request the skill:\n\n```\nUse the dataapp-dev skill to add a date range filter\n```\n\n## ðŸ“ Your First Task: Add a Simple Filter\n\nLet's add a status filter to demonstrate the workflow.\n\n### Ask Claude\n\n```\nAdd a global filter for event status (success vs error vs all),\ndefaulting to success only\n```\n\n### What Will Happen\n\n**Phase 1: VALIDATE (Claude will)**:\n- Check table schema to verify \"status\" or \"type\" column\n- Query distinct values to see options\n- Test filter SQL: `\"type\" = 'success'`\n\n**Phase 2: BUILD (Claude will)**:\n- Create `get_status_filter_clause()` in `utils/data_loader.py`\n- Add radio button to sidebar in `streamlit_dashboard.py`\n- Import filter in all page modules\n- Update all queries to use the filter\n\n**Phase 3: VERIFY (Claude will)**:\n- Open app in browser\n- Test switching between Success/Error/All\n- Navigate through pages\n- Take screenshots showing it works\n- Confirm no errors\n\n**You'll see**: Screenshots proving the feature works before committing!\n\n## ðŸ” Example Workflows\n\n### Adding a New Metric\n\n```\nUser: Add a metric showing percentage of returning users\n\nClaude: I'll add this metric following the validate â†’ build â†’ verify workflow.\n\nPhase 1: VALIDATE\n[Checks if data supports calculating returning users]\n[Queries sample data to test calculation]\n\nPhase 2: BUILD\n[Adds query to appropriate page]\n[Adds metric display]\n\nPhase 3: VERIFY\n[Opens app, navigates to page, takes screenshot]\n\nâœ… Complete! Here's what it looks like: [screenshot]\n```\n\n### Debugging an Issue\n\n```\nUser: The overview page shows \"KeyError: 'user_count'\"\n\nClaude: Let me diagnose and fix this issue.\n\nPhase 1: VALIDATE (Diagnose)\n[Reads error details]\n[Checks query to see what columns are actually returned]\n[Queries database to verify column names]\nFound: Query returns \"users\" but code expects \"user_count\"\n\nPhase 2: BUILD (Fix)\n[Updates code to use \"users\" instead of \"user_count\"]\n\nPhase 3: VERIFY\n[Opens app, verifies page loads, no error]\n\nâœ… Fixed! Page now loads correctly.\n```\n\n### Performance Optimization\n\n```\nUser: Overview page is slow (12 seconds to load)\n\nClaude: Let me optimize the page performance.\n\nPhase 1: VALIDATE (Diagnose)\n[Identifies queries without date filters]\n[Checks data volume: 2M rows]\n[Tests query with EXPLAIN]\n\nPhase 2: BUILD (Optimize)\n[Adds date filter: CURRENT_DATE - INTERVAL '90 days']\n[Changes SELECT * to SELECT specific columns]\n[Adds WHERE clause early in query]\n\nPhase 3: VERIFY\n[Opens app, times page load]\n[Confirms < 3 seconds]\n[Verifies data still correct]\n\nâœ… Optimized! Load time reduced from 12s to 2.5s\n```\n\n## ðŸŽ¨ Common Patterns\n\n### Pattern 1: Add Global Filter\n\n```\nYou: Add a global filter for [filter name], defaulting to [default value]\n\nClaude will:\n1. âœ… Validate column exists and check values\n2. âœ… Create get_X_filter_clause() function\n3. âœ… Add UI to sidebar\n4. âœ… Update all page modules\n5. âœ… Verify visually\n6. âœ… Show screenshots\n```\n\n### Pattern 2: Create New Page\n\n```\nYou: Create a new page showing [analysis type]\n\nClaude will:\n1. âœ… Check what data is available\n2. âœ… Query sample data to understand structure\n3. âœ… Create new page module\n4. âœ… Add to navigation\n5. âœ… Verify page loads and works\n6. âœ… Show screenshot\n```\n\n### Pattern 3: Fix Bug\n\n```\nYou: [Describe error message and location]\n\nClaude will:\n1. âœ… Read code to understand issue\n2. âœ… Validate data to confirm diagnosis\n3. âœ… Apply fix\n4. âœ… Test with Playwright\n5. âœ… Confirm fix works\n```\n\n## ðŸ“Š Expected Output\n\n### What You'll See\n\nWhen Claude uses this skill, you'll see:\n\n**During Validation**:\n```\nLet me first validate the data structure...\n\nâœ… Table schema retrieved - \"user_type\" column exists (STRING type)\nâœ… Queried distinct values: 'External User', 'Keboola User'\nâœ… Tested filter SQL - returns 122 users, 3,151 events\n```\n\n**During Build**:\n```\nNow I'll implement the filter...\n\nâœ… Updated utils/data_loader.py - added get_user_type_filter_clause()\nâœ… Updated streamlit_dashboard.py - added UI to sidebar\nâœ… Updated 8 page modules - added filter to all queries\n```\n\n**During Verify**:\n```\nLet me verify the implementation works...\n\nâœ… Opened app in browser\nâœ… Filter displays correctly - \"External Users Only\" selected by default\nâœ… Tested switching to \"Keboola Users Only\" - data updates correctly\nâœ… Navigated through all pages - no errors\nâœ… Screenshots captured\n```\n\n### What You Get\n\n- ðŸ“¸ Screenshots showing working features\n- ðŸ“Š Query results proving data exists\n- âœ… Confidence that code works\n- ðŸš€ Ready-to-commit changes\n\n## ðŸŽ“ Learning Resources\n\n### Included in This Skill\n\n1. **workflow-guide.md** - Step-by-step examples\n   - Adding filters\n   - Creating pages\n   - Fixing bugs\n   - Optimizing performance\n\n2. **best-practices.md** - Comprehensive guide\n   - SQL-first architecture\n   - Environment parity\n   - Modular design\n   - Caching strategies\n\n3. **templates.md** - Copy-paste templates\n   - Filter functions\n   - Page modules\n   - Queries\n   - UI components\n\n### External Resources\n\n- [Streamlit Documentation](https://docs.streamlit.io)\n- [Plotly Documentation](https://plotly.com/python/)\n- [Keboola Data Apps Guide](https://help.keboola.com)\n\n## ðŸš€ Next Steps\n\n1. **Try a simple task**: Add a filter or metric\n2. **Watch the workflow**: See how Claude validates â†’ builds â†’ verifies\n3. **Review screenshots**: Visual proof helps you understand\n4. **Study the code**: Learn patterns Claude follows\n5. **Build more**: Create complex features with confidence\n\n## ðŸ’¬ Tips for Success\n\n### Get the Most from This Skill\n\n1. **Be specific**: \"Add filter defaulting to X\" vs \"add a filter\"\n2. **Trust the process**: Let Claude validate before building\n3. **Review screenshots**: Visual verification catches issues\n4. **Ask questions**: \"Why did you choose this approach?\"\n5. **Request examples**: \"Show me how to do X using this pattern\"\n\n### When Things Go Wrong\n\n1. **Share error messages**: Copy full stack traces\n2. **Describe what you see**: \"Users page shows error\"\n3. **Mention recent changes**: \"After adding the filter...\"\n4. **Let Claude diagnose**: The skill will check data first\n\n## âœ… Quick Checklist\n\nBefore starting development with this skill:\n\n- [ ] Plugin installed: `/plugin install dataapp-developer`\n- [ ] Keboola MCP authenticated (OAuth on first use)\n- [ ] Streamlit app running on localhost:8501\n- [ ] You understand the 3-phase workflow\n- [ ] You're ready to see Claude validate before coding\n\nAfter Claude completes a task:\n\n- [ ] You reviewed the screenshots\n- [ ] You understand what changed\n- [ ] You verified the explanation makes sense\n- [ ] You're ready to commit the changes\n\n## ðŸŽ‰ You're Ready!\n\nStart building Keboola data apps with confidence. The skill will:\n- âœ… Check your data first\n- âœ… Write correct code\n- âœ… Test visually\n- âœ… Show proof it works\n\nJust describe what you want, and let the skill guide the implementation!\n\n---\n\n**Need help?** See the other reference docs or ask Claude:\n- \"Explain how the validate phase works\"\n- \"Show me an example of adding a filter\"\n- \"What are the best practices for queries?\"\n",
        "plugins/dataapp-developer/skills/dataapp-dev/SKILL.md": "---\nname: dataapp-dev\ndescription: Expert for developing Streamlit data apps for Keboola deployment. Activates when building, modifying, or debugging Keboola data apps, Streamlit dashboards, adding filters, creating pages, or fixing data app issues. Validates data structures using Keboola MCP before writing code, tests implementations with Playwright browser automation, and follows SQL-first architecture patterns.\nallowed-tools: ['*']\n---\n\n# Keboola Data App Development Skill\n\nYou are an expert Streamlit data app developer specializing in Keboola deployment. Your goal is to build robust, performant data apps that work seamlessly in both local development and Keboola production environments.\n\n## Core Workflow: Validate â†’ Build â†’ Verify\n\n### CRITICAL: Always Follow This Workflow\n\nWhen making changes to a Keboola data app, you MUST follow this three-phase approach:\n\n#### Phase 1: VALIDATE Data Structures\n**Before writing any code**, use Keboola MCP to validate assumptions:\n\n1. **Get project context**:\n   ```\n   Use mcp__keboola__get_project_info to understand:\n   - SQL dialect (Snowflake, BigQuery, etc.)\n   - Available data sources\n   - Project configuration\n   ```\n\n2. **Inspect table schemas**:\n   ```\n   Use mcp__keboola__get_table with table_id to check:\n   - Column names (exact case-sensitive names)\n   - Data types (database_native_type, keboola_base_type)\n   - Fully qualified table names for queries\n   - Primary keys\n   ```\n\n3. **Query sample data**:\n   ```\n   Use mcp__keboola__query_data to:\n   - Verify column values (e.g., distinct values in categorical columns)\n   - Test filter conditions\n   - Validate SQL syntax before embedding in code\n   - Check data volumes\n   ```\n\n**Example validation sequence**:\n```\n1. mcp__keboola__get_table(\"out.c-analysis.usage_data\")\n   â†’ Verify \"user_type\" column exists\n   â†’ Get fully qualified name: \"KBC_USE4_361\".\"out.c-analysis\".\"usage_data\"\n\n2. mcp__keboola__query_data(\n     sql: 'SELECT DISTINCT \"user_type\", COUNT(*) FROM \"KBC_USE4_361\".\"out.c-analysis\".\"usage_data\" GROUP BY \"user_type\"',\n     query_name: \"Check user_type values\"\n   )\n   â†’ Confirm values: 'External User', 'Keboola User'\n   â†’ Validate filter logic before coding\n```\n\n#### Phase 2: BUILD Implementation\nFollow SQL-first architecture patterns:\n\n1. **Use centralized data access layer** (`utils/data_loader.py`):\n   - Create filter clause functions (e.g., `get_user_type_filter_clause()`)\n   - Use `@st.cache_data(ttl=300)` for all queries\n   - Always use fully qualified table names from `get_table_name()`\n\n2. **Build WHERE clauses systematically**:\n   ```python\n   where_parts = ['\"type\" = \\'success\\'', get_agent_filter_clause()]\n   user_filter = get_user_type_filter_clause()\n   if user_filter:\n       where_parts.append(user_filter)\n   where_clause = ' AND '.join(where_parts)\n   ```\n\n3. **Import filter functions in all page modules**:\n   ```python\n   from utils.data_loader import (\n       execute_aggregation_query,\n       get_table_name,\n       get_agent_filter_clause,\n       get_user_type_filter_clause,  # Add new filters here\n       get_selected_agent_name\n   )\n   ```\n\n4. **Update session state initialization**:\n   ```python\n   if 'filter_name' not in st.session_state:\n       st.session_state.filter_name = 'default_value'\n   ```\n\n5. **Avoid variable name conflicts**:\n   - Use unique session state keys (e.g., `local_user_type_filter` vs `user_type_filter`)\n   - Watch for reuse of variable names within the same scope\n\n#### Phase 3: VERIFY Implementation\n**After making changes**, use Playwright MCP to verify:\n\n1. **Check if app is running**:\n   ```\n   Use Bash to check: lsof -ti:8501\n   If not running, start it: streamlit run streamlit_app.py (in background)\n   ```\n\n2. **Navigate to the app**:\n   ```\n   mcp__playwright__browser_navigate(url: \"http://localhost:8501\")\n   ```\n\n3. **Wait for page load**:\n   ```\n   mcp__playwright__browser_wait_for(time: 3)\n   ```\n\n4. **Take screenshots to verify**:\n   ```\n   mcp__playwright__browser_take_screenshot(filename: \"feature-verification.png\")\n   ```\n\n5. **Test filter interactions**:\n   ```\n   - Click different filter options\n   - Navigate to different pages\n   - Verify data updates correctly\n   - Check for errors in console\n   ```\n\n6. **Verify all pages**:\n   ```\n   Navigate through each page section and verify:\n   - No errors displayed\n   - Metrics show expected values\n   - Charts render correctly\n   - Filters work as expected\n   ```\n\n## Architecture Principles\n\n### 1. SQL-First Architecture\n**Always push computation to the database, never load large datasets into Python.**\n\n**Why**: Keboola workspaces are optimized for query execution. Loading data into Streamlit is slow and doesn't scale.\n\n**Good**:\n```python\nquery = f'''\n    SELECT\n        \"category\",\n        COUNT(*) as count,\n        AVG(\"value\") as avg_value\n    FROM {get_table_name()}\n    WHERE \"date\" >= CURRENT_DATE - INTERVAL '90 days'\n        AND {get_filter_clause()}\n    GROUP BY \"category\"\n'''\n```\n\n**Bad**:\n```python\ndf = execute_aggregation_query(f\"SELECT * FROM {get_table_name()}\")\nresult = df.groupby('category').agg({'value': 'mean'})\n```\n\n### 2. Environment Parity\nCode must work in both environments without modification:\n\n**Local Development**:\n- Credentials in `.streamlit/secrets.toml`\n- Can use debug tools\n- Fast iteration\n\n**Keboola Production**:\n- Credentials from environment variables\n- No local file access\n- Production data volumes\n\n**Pattern**:\n```python\nimport os\nimport streamlit as st\n\n# Works in both environments\nkbc_url = os.environ.get('KBC_URL') or st.secrets.get(\"KBC_URL\")\nkbc_token = os.environ.get('KBC_TOKEN') or st.secrets.get(\"KBC_TOKEN\")\n```\n\n### 3. Modular Design\nSeparate concerns for maintainability:\n\n```\nstreamlit_app.py          # Entry point, navigation, global filters\nutils/data_loader.py      # All SQL queries and data access\npage_modules/*.py         # Individual page logic\n```\n\n### 4. Session State Management\nUse session state for:\n- Filter selections that persist across pages\n- Cached user preferences\n- Multi-step workflows\n\n**Pattern**:\n```python\n# Initialize with defaults\nif 'filter_name' not in st.session_state:\n    st.session_state.filter_name = 'default_value'\n\n# Create UI control\noption = st.sidebar.radio(\n    \"Label:\",\n    options=['Option 1', 'Option 2'],\n    index=options.index(st.session_state.filter_name)\n)\n\n# Update and trigger rerun if changed\nif option != st.session_state.filter_name:\n    st.session_state.filter_name = option\n    st.rerun()\n```\n\n## Common Patterns\n\n### Global Filter Pattern\nWhen adding a global filter that affects all pages:\n\n1. **Add filter function to `utils/data_loader.py`**:\n```python\ndef get_filter_clause():\n    \"\"\"Get SQL WHERE clause for current filter selection.\"\"\"\n    if 'filter_name' not in st.session_state:\n        st.session_state.filter_name = 'default_value'\n\n    if st.session_state.filter_name == 'option1':\n        return '\"column\" = \\'value1\\''\n    elif st.session_state.filter_name == 'option2':\n        return '\"column\" = \\'value2\\''\n    else:\n        return ''  # No filter\n```\n\n2. **Add UI to main dashboard sidebar** (`streamlit_dashboard.py`):\n```python\nst.sidebar.markdown(\"**Filter Label**\")\n\nif 'filter_name' not in st.session_state:\n    st.session_state.filter_name = 'default_value'\n\noption = st.sidebar.radio(\n    \"Select option:\",\n    options=['Option 1', 'Option 2', 'All'],\n    index=options.index(st.session_state.filter_name),\n    help=\"Description of what this filter does\"\n)\n\nif option != st.session_state.filter_name:\n    st.session_state.filter_name = option\n    st.rerun()\n```\n\n3. **Import in all page modules**:\n```python\nfrom utils.data_loader import (\n    execute_aggregation_query,\n    get_table_name,\n    get_filter_clause,  # Add new filter\n    # ... other imports\n)\n```\n\n4. **Update queries in all page modules**:\n```python\nwhere_parts = ['\"type\" = \\'success\\'', get_agent_filter_clause()]\ncustom_filter = get_filter_clause()\nif custom_filter:\n    where_parts.append(custom_filter)\nwhere_clause = ' AND '.join(where_parts)\n\nquery = f'''\n    SELECT ...\n    FROM {get_table_name()}\n    WHERE {where_clause}\n    GROUP BY ...\n'''\n```\n\n### Page Module Template\n\n```python\n\"\"\"Page Title - Brief description of page purpose\"\"\"\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nfrom utils.data_loader import (\n    execute_aggregation_query,\n    get_table_name,\n    get_agent_filter_clause,\n    get_selected_agent_name\n)\n\ndef create_page_name():\n    \"\"\"Main entry point for this page.\"\"\"\n\n    selected_agent = get_selected_agent_name()\n    st.title(f\"ðŸ“Š Page Title: {selected_agent}\")\n    st.markdown(\"---\")\n\n    # Build WHERE clause with all filters\n    where_parts = ['\"type\" = \\'success\\'', get_agent_filter_clause()]\n    where_clause = ' AND '.join(where_parts)\n\n    # Section 1: Key Metrics\n    st.markdown(\"## ðŸ“ˆ Key Metrics\")\n\n    metrics_query = f'''\n        SELECT\n            COUNT(DISTINCT \"user_name\") as users,\n            COUNT(*) as events,\n            AVG(\"value\") as avg_value\n        FROM {get_table_name()}\n        WHERE {where_clause}\n    '''\n\n    metrics = execute_aggregation_query(metrics_query)\n\n    if not metrics.empty:\n        row = metrics.iloc[0]\n        col1, col2, col3 = st.columns(3)\n\n        with col1:\n            st.metric(\"Users\", f\"{int(row['users']):,}\")\n        with col2:\n            st.metric(\"Events\", f\"{int(row['events']):,}\")\n        with col3:\n            st.metric(\"Avg Value\", f\"{row['avg_value']:.2f}\")\n\n    st.markdown(\"---\")\n\n    # Section 2: Visualization\n    st.markdown(\"## ðŸ“Š Trends\")\n\n    trend_query = f'''\n        SELECT\n            DATE(\"date_column\") as date,\n            COUNT(*) as count\n        FROM {get_table_name()}\n        WHERE {where_clause}\n        GROUP BY DATE(\"date_column\")\n        ORDER BY date\n    '''\n\n    trends = execute_aggregation_query(trend_query)\n\n    if not trends.empty:\n        fig = px.line(\n            trends,\n            x='date',\n            y='count',\n            title='Daily Trend'\n        )\n        st.plotly_chart(fig, use_container_width=True)\n```\n\n## SQL Best Practices\n\n### Always Check SQL Dialect First\nDifferent backends have different syntax:\n\n**Snowflake** (most common):\n- Use double quotes for identifiers: `\"column_name\"`\n- Date functions: `TO_TIMESTAMP()`, `DATE_TRUNC()`\n- String concatenation: `||`\n\n**BigQuery**:\n- Use backticks for identifiers: `` `column_name` ``\n- Date functions: `TIMESTAMP()`, `DATE_TRUNC()`\n- Different function names\n\n### Quote All Identifiers\n```python\n# âœ… Always use quoted identifiers\nquery = f'''SELECT \"user_name\", \"event_date\" FROM {get_table_name()}'''\n\n# âŒ Unquoted may fail due to case sensitivity\nquery = f'''SELECT user_name, event_date FROM {get_table_name()}'''\n```\n\n### Handle NULLs Properly\n```python\nquery = f'''\n    SELECT\n        COALESCE(\"category\", 'Unknown') as category,\n        COUNT(*) as count\n    FROM {get_table_name()}\n    WHERE \"value\" IS NOT NULL\n    GROUP BY \"category\"\n'''\n```\n\n## Error Prevention\n\n### Before Writing Code\n1. âœ… Validate table exists with `mcp__keboola__get_table`\n2. âœ… Check column names and types from schema\n3. âœ… Test SQL queries with `mcp__keboola__query_data`\n4. âœ… Verify sample data values match expectations\n\n### During Development\n1. âœ… Use consistent variable names (avoid conflicts)\n2. âœ… Initialize session state with defaults\n3. âœ… Handle empty DataFrames gracefully\n4. âœ… Add error handling to all data loads\n\n### After Implementation\n1. âœ… Open app in browser with Playwright\n2. âœ… Navigate through all pages\n3. âœ… Test filter interactions\n4. âœ… Verify no errors in console\n5. âœ… Take screenshots to document working state\n\n## Common Pitfalls to Avoid\n\n### Variable Name Conflicts\n```python\n# âŒ BAD: Same variable name used twice\nuser_type_filter = get_user_type_filter_clause()  # Returns string\n# ... later in code ...\nuser_type_filter = st.multiselect(...)  # Now it's a list - CONFLICT!\n\n# âœ… GOOD: Use distinct names\nuser_type_sql_filter = get_user_type_filter_clause()  # String for SQL\n# ... later ...\nuser_type_multiselect = st.multiselect(...)  # List for UI\n```\n\n### Session State Key Conflicts\n```python\n# âŒ BAD: Using global session state key for local widget\nst.multiselect(..., key=\"user_type_filter\")  # Conflicts with global filter\n\n# âœ… GOOD: Use unique key for local widget\nst.multiselect(..., key=\"local_user_type_filter\")\n```\n\n### Loading Data Without Validation\n```python\n# âŒ BAD: Assume columns exist\ndf = execute_query(query)\nvalue = df['assumed_column'][0]  # May crash\n\n# âœ… GOOD: Validate first using Keboola MCP\n# 1. Check schema with mcp__keboola__get_table\n# 2. Query sample data with mcp__keboola__query_data\n# 3. Then write code with confidence\nif 'column' in df.columns:\n    value = df['column'][0]\n```\n\n### Skipping Visual Verification\n```python\n# âŒ BAD: Make changes and assume they work\n# ... write code ...\n# ... commit and push ...\n\n# âœ… GOOD: Verify visually before committing\n# 1. mcp__playwright__browser_navigate(\"http://localhost:8501\")\n# 2. mcp__playwright__browser_wait_for(time: 3)\n# 3. mcp__playwright__browser_take_screenshot()\n# 4. Test interactions, verify no errors\n# 5. Then commit\n```\n\n## Required Tools Access\n\nThis skill requires access to:\n- **Keboola MCP**: For data validation and querying\n- **Playwright MCP**: For visual verification\n- **Read/Write/Edit**: For code modifications\n- **Bash**: For git operations and app management\n\n## Development Checklist\n\nBefore considering a task complete:\n\n### Data Validation\n- [ ] Checked table schema with `mcp__keboola__get_table`\n- [ ] Queried sample data with `mcp__keboola__query_data`\n- [ ] Verified column names and types\n- [ ] Tested SQL filter conditions\n\n### Implementation\n- [ ] Updated `utils/data_loader.py` with filter functions\n- [ ] Added UI controls to `streamlit_dashboard.py`\n- [ ] Imported filters in all page modules\n- [ ] Updated all SQL queries to use filters\n- [ ] Initialized session state with defaults\n- [ ] Avoided variable name conflicts\n\n### Verification\n- [ ] Opened app in browser with Playwright\n- [ ] Navigated through all affected pages\n- [ ] Tested filter interactions\n- [ ] Took screenshots of working features\n- [ ] Verified no errors in UI or console\n\n### Documentation\n- [ ] Added code comments for complex logic\n- [ ] Updated inline documentation\n- [ ] Followed existing code style\n\n## References\n\nSee the following reference files in this skill directory for detailed guidance:\n- `best-practices.md` - Comprehensive development best practices\n- `workflow-guide.md` - Step-by-step workflow examples\n- `templates.md` - Code templates and patterns\n\n## Examples\n\n### Example 1: Adding a Global Filter\n\n**User Request**: \"Add a global filter for user type, defaulting to external users only\"\n\n**Your Response**:\n```\nI'll add a global user type filter following the validate â†’ build â†’ verify workflow.\n\nPhase 1: VALIDATE\nLet me first check the data structure to understand user types...\n[Use mcp__keboola__get_table to check schema]\n[Use mcp__keboola__query_data to check distinct values]\n\nPhase 2: BUILD\nNow I'll implement the filter...\n[Update data_loader.py with get_user_type_filter_clause()]\n[Add UI to streamlit_dashboard.py]\n[Update all page modules]\n\nPhase 3: VERIFY\nLet me verify the implementation works...\n[Use Playwright to open app, test filter, take screenshots]\n\nâœ… Implementation complete and verified!\n```\n\n### Example 2: Adding a New Metric\n\n**User Request**: \"Add average session duration to the overview page\"\n\n**Your Response**:\n```\nPhase 1: VALIDATE\nLet me check if we have duration data available...\n[Query table schema]\n[Check sample data to verify duration column format]\n\nPhase 2: BUILD\n[Add metric calculation to overview page query]\n[Add display to page layout]\n\nPhase 3: VERIFY\n[Open app, navigate to overview, verify metric displays correctly]\n```\n\n## Remember\n\n1. **Never skip validation** - Always check schemas and data first\n2. **Never skip verification** - Always test with Playwright before committing\n3. **Follow SQL-first architecture** - Aggregate in database, not Python\n4. **Avoid assumptions** - Query actual data to verify\n5. **Test interactions** - Don't just look, click and interact\n6. **Document as you go** - Future developers will thank you\n\nYou have the tools to build data apps with confidence. Use them!\n",
        "plugins/dataapp-developer/skills/dataapp-dev/best-practices.md": "# Best Practices for Developing Keboola Data Apps Locally\n\nA comprehensive guide for building Streamlit data apps that seamlessly transition from local development to Keboola production deployment.\n\n## ðŸ“ Project Structure\n\n### Recommended Layout\n\nWhen starting a new Keboola data app project, use this structure:\n\n```\nmy-keboola-dataapp/\nâ”œâ”€â”€ streamlit_app.py              # Main entry point\nâ”œâ”€â”€ pyproject.toml                # Project metadata & dependencies\nâ”œâ”€â”€ requirements.txt              # Pip dependencies (generated)\nâ”œâ”€â”€ uv.lock                       # Lock file (if using uv)\nâ”œâ”€â”€ .gitignore                    # Exclude secrets, cache, etc.\nâ”œâ”€â”€ README.md                     # Project documentation\nâ”‚\nâ”œâ”€â”€ .streamlit/\nâ”‚   â”œâ”€â”€ config.toml              # Streamlit configuration\nâ”‚   â””â”€â”€ secrets.toml             # Local credentials (NEVER commit)\nâ”‚\nâ”œâ”€â”€ utils/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ data_loader.py           # Data access layer\nâ”‚   â”œâ”€â”€ common.py                # Shared utilities\nâ”‚   â””â”€â”€ visualization.py         # Reusable chart functions\nâ”‚\nâ”œâ”€â”€ page_modules/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ overview.py              # Homepage/overview\nâ”‚   â”œâ”€â”€ analysis_one.py          # Feature page 1\nâ”‚   â””â”€â”€ analysis_two.py          # Feature page 2\nâ”‚\nâ”œâ”€â”€ tests/                        # Test suite\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ test_data_loader.py\nâ”‚   â””â”€â”€ test_page_modules.py\nâ”‚\nâ””â”€â”€ docs/\n    â”œâ”€â”€ QUICKSTART.md            # Getting started guide\n    â”œâ”€â”€ DEPLOYMENT.md            # Deployment instructions\n    â””â”€â”€ DEVELOPMENT.md           # Development guide\n```\n\n### File Naming Conventions\n\n- **Snake_case for Python files**: `data_loader.py`, `analysis_page.py`\n- **Descriptive names**: `create_revenue_chart()`, not `make_plot()`\n- **Module prefixes**: `page_overview.py`, `page_analysis.py`\n\n### .gitignore Template\n\n```gitignore\n# Secrets\n.streamlit/secrets.toml\n*.env\n.env.*\n\n# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\n\n# Virtual environments\nvenv/\nenv/\nENV/\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n\n# Streamlit\n.streamlit/cache/\n\n# OS\n.DS_Store\nThumbs.db\n```\n\n## âš™ï¸ Configuration Management\n\n### Local Development Setup\n\nCreate `.streamlit/secrets.toml`:\n\n```toml\n# Keboola Connection\nKBC_URL = \"https://connection.{region}.keboola.com\"\nKBC_TOKEN = \"your-storage-api-token\"\nKBC_WORKSPACE_ID = 12345\nKBC_DATABASE_NAME = \"KBC_REGION_PROJECTID\"\n\n# Optional: Application-specific settings\nCACHE_TTL = 300\nDEFAULT_DATE_RANGE = 90\n```\n\nAccess in code:\n\n```python\nimport streamlit as st\n\n# Keboola credentials\nkbc_url = st.secrets[\"KBC_URL\"]\nkbc_token = st.secrets[\"KBC_TOKEN\"]\nworkspace_id = st.secrets.get(\"KBC_WORKSPACE_ID\")\n\n# Application settings\ncache_ttl = st.secrets.get(\"CACHE_TTL\", 300)  # Default: 5 min\n```\n\n### Production (Keboola) Setup\n\nKeboola automatically injects environment variables:\n\n```python\nimport os\nimport streamlit as st\n\ndef get_config(key: str, default=None):\n    \"\"\"\n    Get configuration from secrets (local) or environment (production).\n\n    Args:\n        key: Configuration key\n        default: Default value if not found\n\n    Returns:\n        Configuration value\n    \"\"\"\n    # Try secrets first (local development)\n    if key in st.secrets:\n        return st.secrets[key]\n\n    # Fall back to environment variables (production)\n    return os.environ.get(key, default)\n\n# Usage\nkbc_url = get_config(\"KBC_URL\")\nkbc_token = get_config(\"KBC_TOKEN\")\nworkspace_id = get_config(\"KBC_WORKSPACE_ID\")\n```\n\n## ðŸŽ¯ Core Principles\n\n### 1. SQL-First Architecture\n**Push computation to the data warehouse, not the application layer.**\n\n**Benefits:**\n- âš¡ **Performance**: Server-side aggregation scales with data size\n- ðŸ“ˆ **Scalability**: Performance independent of dataset size\n- ðŸ’¾ **Efficiency**: Minimal data transfer, only results transmitted\n- ðŸ”„ **Maintainability**: Business logic in SQL, easier to optimize\n\n**Anti-pattern:**\n```python\n# âŒ BAD: Load all data, process in Python\ndf = pd.read_sql(\"SELECT * FROM large_table\", conn)\nresult = df.groupby('category').agg({'value': 'sum'})\n```\n\n**Best practice:**\n```python\n# âœ… GOOD: Aggregate in database\nquery = \"\"\"\n    SELECT category, SUM(value) as total\n    FROM large_table\n    GROUP BY category\n\"\"\"\nresult = execute_query(query)\n```\n\n### 2. Environment Parity\n**Local development should mirror production as closely as possible.**\n\n- Same data sources (workspace tables)\n- Same authentication pattern\n- Same dependencies\n- Same file structure\n- Different only in credential storage mechanism\n\n### 3. Modular Design\n**Separate concerns for maintainability and reusability.**\n\n```\nstreamlit_app.py          # Entry point, navigation, layout\nâ”œâ”€â”€ utils/                # Shared utilities\nâ”‚   â”œâ”€â”€ data_loader.py    # Data access layer\nâ”‚   â””â”€â”€ common.py         # Helper functions\nâ””â”€â”€ page_modules/         # Feature-specific logic\n    â”œâ”€â”€ overview.py\n    â””â”€â”€ analysis.py\n```\n\n## ðŸ—ï¸ Architecture Patterns\n\n### Data Access Layer Pattern\n\n```python\n# utils/data_loader.py\n\nimport streamlit as st\nimport pandas as pd\nimport requests\nimport os\n\n@st.cache_data(ttl=300)  # 5-minute cache\ndef execute_aggregation_query(sql: str) -> pd.DataFrame:\n    \"\"\"\n    Execute SQL query via Keboola Workspace API.\n\n    Args:\n        sql: SQL query string\n\n    Returns:\n        pandas DataFrame with query results\n    \"\"\"\n    kbc_url = os.environ.get('KBC_URL') or st.secrets.get(\"KBC_URL\")\n    kbc_token = os.environ.get('KBC_TOKEN') or st.secrets.get(\"KBC_TOKEN\")\n    workspace_id = os.environ.get('KBC_WORKSPACE_ID') or st.secrets.get(\"KBC_WORKSPACE_ID\")\n\n    endpoint = f\"{kbc_url}/v2/storage/workspaces/{workspace_id}/query\"\n    headers = {\n        \"X-StorageApi-Token\": kbc_token,\n        \"Content-Type\": \"application/json\"\n    }\n\n    response = requests.post(endpoint, headers=headers, json={\"query\": sql})\n\n    if response.status_code != 200:\n        st.error(f\"Query failed: {response.text}\")\n        return pd.DataFrame()\n\n    result = response.json()\n    rows = result.get(\"data\", {}).get(\"rows\", [])\n\n    if not rows:\n        return pd.DataFrame()\n\n    df = pd.DataFrame(rows)\n    df.columns = df.columns.str.lower()\n\n    return df\n\ndef get_table_name(table_id: str) -> str:\n    \"\"\"\n    Get fully qualified table name for SQL queries.\n\n    Args:\n        table_id: Storage table ID (e.g., 'out.c-bucket.table')\n\n    Returns:\n        Properly quoted table name for SQL queries\n    \"\"\"\n    last_dot_index = table_id.rfind(\".\")\n    if last_dot_index > 0:\n        bucket = table_id[:last_dot_index]\n        table = table_id[last_dot_index + 1:]\n        table_normalized = table.replace(\"-\", \"_\")\n        return f'\"{bucket}\".\"{table_normalized}\"'\n    else:\n        return f'\"{table_id}\"'\n```\n\n### Page Module Pattern\n\n```python\n# page_modules/analysis_page.py\n\n\"\"\"Analysis Page - Descriptive title of page purpose\"\"\"\n\nimport streamlit as st\nimport plotly.express as px\nfrom utils.data_loader import execute_aggregation_query, get_table_name\n\ndef create_analysis_page():\n    \"\"\"Main entry point for analysis page.\"\"\"\n    st.title(\"ðŸ“Š Analysis Page\")\n\n    # Load data\n    data = load_page_data()\n\n    # Handle empty state\n    if data.empty:\n        st.warning(\"No data available for analysis\")\n        return\n\n    # Create sections\n    create_summary_section(data)\n    create_visualization_section(data)\n\n@st.cache_data(ttl=300)\ndef load_page_data() -> pd.DataFrame:\n    \"\"\"Load data specific to this page.\"\"\"\n    query = f\"\"\"\n        SELECT\n            category,\n            COUNT(*) as count,\n            AVG(value) as avg_value\n        FROM {get_table_name('out.c-bucket.data_table')}\n        WHERE created_at >= CURRENT_DATE - INTERVAL '90 days'\n        GROUP BY category\n        ORDER BY count DESC\n    \"\"\"\n    return execute_aggregation_query(query)\n\ndef create_summary_section(data: pd.DataFrame):\n    \"\"\"Display summary metrics.\"\"\"\n    col1, col2, col3 = st.columns(3)\n\n    with col1:\n        st.metric(\"Total Categories\", len(data))\n    with col2:\n        st.metric(\"Total Count\", data['count'].sum())\n    with col3:\n        st.metric(\"Average Value\", f\"{data['avg_value'].mean():.2f}\")\n\ndef create_visualization_section(data: pd.DataFrame):\n    \"\"\"Create visualizations.\"\"\"\n    fig = px.bar(\n        data,\n        x='category',\n        y='count',\n        title='Distribution by Category'\n    )\n    st.plotly_chart(fig, use_container_width=True)\n```\n\n## ðŸ’¾ Data Access Patterns\n\n### Query Best Practices\n\n#### 1. Server-Side Aggregation\n```python\n# âœ… GOOD: Aggregate in database\nquery = \"\"\"\n    SELECT\n        DATE_TRUNC('day', event_date) as date,\n        category,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        AVG(value) as avg_value\n    FROM {table_name}\n    WHERE event_date >= CURRENT_DATE - INTERVAL '90 days'\n    GROUP BY DATE_TRUNC('day', event_date), category\n    ORDER BY date DESC\n\"\"\"\n```\n\n#### 2. Use Date Filters\n```python\n# âœ… Always limit time range for large tables\ndef build_date_filter(days: int = 90) -> str:\n    \"\"\"Build standard date filter clause.\"\"\"\n    return f\"event_date >= CURRENT_DATE - INTERVAL '{days} days'\"\n```\n\n#### 3. Parameterized Queries\n```python\ndef build_where_clause(filters: dict) -> str:\n    \"\"\"Build WHERE clause from filter dictionary.\"\"\"\n    where_parts = []\n\n    for key, value in filters.items():\n        if value:  # Only add non-empty filters\n            where_parts.append(value)\n\n    return ' AND '.join(where_parts) if where_parts else '1=1'\n\n# Usage\nwhere_parts = ['\"type\" = \\'success\\'', get_agent_filter_clause()]\nuser_filter = get_user_type_filter_clause()\nif user_filter:\n    where_parts.append(user_filter)\nwhere_clause = ' AND '.join(where_parts)\n```\n\n### Caching Strategy\n\n```python\n# Standard caching for data loads\n@st.cache_data(ttl=300)  # 5 minutes\ndef load_hourly_metrics() -> pd.DataFrame:\n    \"\"\"Load metrics that update frequently.\"\"\"\n    return execute_aggregation_query(query)\n\n# Longer caching for reference data\n@st.cache_data(ttl=3600)  # 1 hour\ndef load_reference_data() -> pd.DataFrame:\n    \"\"\"Load slowly-changing reference data.\"\"\"\n    return execute_aggregation_query(query)\n```\n\n## âš™ï¸ Configuration Management\n\n### Local Development Setup\n\n**Create `.streamlit/secrets.toml`:**\n\n```toml\n# Keboola Connection\nKBC_URL = \"https://connection.{region}.keboola.com\"\nKBC_TOKEN = \"your-storage-api-token\"\nKBC_WORKSPACE_ID = 12345\nKBC_DATABASE_NAME = \"KBC_REGION_PROJECTID\"\n\n# Optional: Application-specific settings\nCACHE_TTL = 300\nDEFAULT_DATE_RANGE = 90\n```\n\n### Environment-Agnostic Pattern\n\n```python\nimport os\nimport streamlit as st\n\n# Works in both local and production\nkbc_url = os.environ.get('KBC_URL') or st.secrets.get(\"KBC_URL\")\nkbc_token = os.environ.get('KBC_TOKEN') or st.secrets.get(\"KBC_TOKEN\")\nworkspace_id = os.environ.get('KBC_WORKSPACE_ID') or st.secrets.get(\"KBC_WORKSPACE_ID\")\n```\n\n## ðŸ“ Project Structure\n\n```\nmy-keboola-dataapp/\nâ”œâ”€â”€ streamlit_app.py              # Main entry point\nâ”œâ”€â”€ pyproject.toml                # Project metadata & dependencies\nâ”œâ”€â”€ requirements.txt              # Pip dependencies\nâ”œâ”€â”€ .gitignore                    # Exclude secrets, cache\nâ”‚\nâ”œâ”€â”€ .streamlit/\nâ”‚   â”œâ”€â”€ config.toml              # Streamlit configuration\nâ”‚   â””â”€â”€ secrets.toml             # Local credentials (NEVER commit)\nâ”‚\nâ”œâ”€â”€ utils/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ data_loader.py           # Data access layer\nâ”‚\nâ”œâ”€â”€ page_modules/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ overview.py              # Homepage/overview\nâ”‚   â””â”€â”€ analysis.py              # Feature pages\nâ”‚\nâ””â”€â”€ .claude/\n    â””â”€â”€ skills/\n        â””â”€â”€ keboola-dataapp-dev/  # This skill\n```\n\n## ðŸš€ Performance Optimization\n\n### Query Optimization Checklist\n\n- [ ] **Aggregate in database**, not Python\n- [ ] **Add date range filters** for time-series data\n- [ ] **Use LIMIT** during development/testing\n- [ ] **Select only needed columns**, not `SELECT *`\n- [ ] **Cache frequently-accessed queries**\n- [ ] **Test with production data volumes**\n\n## ðŸ› Common Issues and Solutions\n\n### Issue: Variable Name Conflicts\n**Solution**: Use descriptive, unique variable names. Add prefixes like `sql_`, `local_`, `global_` to differentiate.\n\n### Issue: Session State Key Collisions\n**Solution**: Use unique keys for widgets: `key=\"local_filter\"` instead of `key=\"filter\"`\n\n### Issue: Empty DataFrames\n**Solution**: Always validate data exists before processing:\n```python\nif data.empty:\n    st.warning(\"No data available\")\n    return\n```\n\n### Issue: Slow Query Performance\n**Solution**:\n1. Add date filters to limit data\n2. Use aggregation in SQL\n3. Increase cache TTL\n4. Test queries with `mcp__keboola__query_data` first\n\n## âœ… Success Criteria\n\nYour implementation is complete when:\n\n- âœ… Data validated with Keboola MCP\n- âœ… Code follows SQL-first architecture\n- âœ… All page modules updated consistently\n- âœ… Session state initialized properly\n- âœ… No variable name conflicts\n- âœ… Visually verified with Playwright\n- âœ… All pages tested and working\n- âœ… No errors in console\n- âœ… Ready to commit and push\n",
        "plugins/dataapp-developer/skills/dataapp-dev/templates.md": "# Code Templates for Keboola Data Apps\n\nReusable code templates following best practices for Streamlit + Keboola development.\n\n## Table of Contents\n\n- [Filter Function Templates](#filter-function-templates)\n- [Page Module Templates](#page-module-templates)\n- [Query Pattern Templates](#query-pattern-templates)\n- [UI Component Templates](#ui-component-templates)\n- [Data Loader Templates](#data-loader-templates)\n\n---\n\n## Filter Function Templates\n\n### Basic Filter Function (utils/data_loader.py)\n\n```python\ndef get_filter_name_clause():\n    \"\"\"\n    Get SQL WHERE clause for [filter description].\n\n    Returns:\n        String with SQL WHERE clause or empty string if no filter applied.\n    \"\"\"\n    # Initialize session state with default\n    if 'filter_name' not in st.session_state:\n        st.session_state.filter_name = 'Default Option'\n\n    selected = st.session_state.filter_name\n\n    if selected == 'Option 1':\n        return '\"column_name\" = \\'value1\\''\n    elif selected == 'Option 2':\n        return '\"column_name\" = \\'value2\\''\n    else:  # All / No filter\n        return ''\n```\n\n### Multi-Condition Filter Function\n\n```python\ndef get_multi_filter_clause():\n    \"\"\"\n    Get SQL WHERE clause supporting multiple filter conditions.\n\n    Returns:\n        String with SQL WHERE clause combining multiple conditions.\n    \"\"\"\n    if 'multi_filter' not in st.session_state:\n        st.session_state.multi_filter = {\n            'category': 'All',\n            'status': 'Active',\n            'region': 'All'\n        }\n\n    filters = st.session_state.multi_filter\n    where_parts = []\n\n    if filters['category'] != 'All':\n        where_parts.append(f'\"category\" = \\'{filters[\"category\"]}\\'')\n\n    if filters['status'] != 'All':\n        where_parts.append(f'\"status\" = \\'{filters[\"status\"]}\\'')\n\n    if filters['region'] != 'All':\n        where_parts.append(f'\"region\" = \\'{filters[\"region\"]}\\'')\n\n    return ' AND '.join(where_parts) if where_parts else ''\n```\n\n### Date Range Filter Function\n\n```python\ndef get_date_filter_clause():\n    \"\"\"\n    Get SQL WHERE clause for date range filter.\n\n    Returns:\n        String with SQL date filter clause.\n    \"\"\"\n    if 'date_range' not in st.session_state:\n        st.session_state.date_range = 'Last 90 Days'\n\n    date_range = st.session_state.date_range\n\n    if date_range == 'Last 7 Days':\n        return 'TO_TIMESTAMP(\"event_date\") >= CURRENT_DATE - INTERVAL \\'7 days\\''\n    elif date_range == 'Last 30 Days':\n        return 'TO_TIMESTAMP(\"event_date\") >= CURRENT_DATE - INTERVAL \\'30 days\\''\n    elif date_range == 'Last 90 Days':\n        return 'TO_TIMESTAMP(\"event_date\") >= CURRENT_DATE - INTERVAL \\'90 days\\''\n    elif date_range == 'Last Year':\n        return 'TO_TIMESTAMP(\"event_date\") >= CURRENT_DATE - INTERVAL \\'1 year\\''\n    else:  # All Time\n        return ''\n```\n\n---\n\n## Page Module Templates\n\n### Minimal Page Module\n\n```python\n\"\"\"Page Name - Brief description\"\"\"\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nfrom utils.data_loader import (\n    execute_aggregation_query,\n    get_table_name,\n    get_agent_filter_clause,\n    get_selected_agent_name\n)\n\ndef create_page_name():\n    \"\"\"Create the page.\"\"\"\n\n    selected_agent = get_selected_agent_name()\n    st.title(f\"ðŸ“Š Page Title: {selected_agent}\")\n    st.markdown(\"---\")\n\n    # Build WHERE clause\n    where_parts = ['\"type\" = \\'success\\'', get_agent_filter_clause()]\n    where_clause = ' AND '.join(where_parts)\n\n    # Load and display data\n    metrics = load_metrics(where_clause)\n    display_metrics(metrics)\n\n@st.cache_data(ttl=300)\ndef load_metrics(where_clause: str) -> pd.DataFrame:\n    \"\"\"Load metrics for this page.\"\"\"\n    query = f'''\n        SELECT\n            COUNT(*) as total_events,\n            COUNT(DISTINCT \"user_name\") as users\n        FROM {get_table_name()}\n        WHERE {where_clause}\n    '''\n    return execute_aggregation_query(query)\n\ndef display_metrics(data: pd.DataFrame):\n    \"\"\"Display metrics section.\"\"\"\n    if data.empty:\n        st.warning(\"No data available\")\n        return\n\n    row = data.iloc[0]\n\n    col1, col2 = st.columns(2)\n    with col1:\n        st.metric(\"Total Events\", f\"{int(row['total_events']):,}\")\n    with col2:\n        st.metric(\"Unique Users\", f\"{int(row['users']):,}\")\n```\n\n### Full-Featured Page Module\n\n```python\n\"\"\"Page Name - Comprehensive analysis page\"\"\"\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom utils.data_loader import (\n    execute_aggregation_query,\n    get_table_name,\n    get_agent_filter_clause,\n    get_user_type_filter_clause,\n    get_selected_agent_name\n)\n\ndef create_page_name():\n    \"\"\"Create comprehensive analysis page.\"\"\"\n\n    selected_agent = get_selected_agent_name()\n    st.title(f\"ðŸ“Š Page Title: {selected_agent}\")\n    st.markdown(\"Detailed description of page purpose\")\n    st.markdown(\"---\")\n\n    # Build WHERE clause with all filters\n    where_parts = ['\"type\" = \\'success\\'', get_agent_filter_clause()]\n    user_type_filter = get_user_type_filter_clause()\n    if user_type_filter:\n        where_parts.append(user_type_filter)\n    where_clause = ' AND '.join(where_parts)\n\n    # Section 1: Overview metrics\n    st.markdown(\"## ðŸ“ˆ Overview\")\n    create_overview_section(where_clause)\n\n    st.markdown(\"---\")\n\n    # Section 2: Trends\n    st.markdown(\"## ðŸ“Š Trends\")\n    create_trends_section(where_clause)\n\n    st.markdown(\"---\")\n\n    # Section 3: Details\n    st.markdown(\"## ðŸ“‹ Detailed Data\")\n    create_details_section(where_clause)\n\ndef create_overview_section(where_clause: str):\n    \"\"\"Display overview metrics.\"\"\"\n    metrics_query = f'''\n        SELECT\n            COUNT(*) as total_events,\n            COUNT(DISTINCT \"user_name\") as users,\n            COUNT(DISTINCT \"project_id\") as projects,\n            AVG(\"value\") as avg_value\n        FROM {get_table_name()}\n        WHERE {where_clause}\n    '''\n\n    metrics = execute_aggregation_query(metrics_query)\n\n    if not metrics.empty:\n        row = metrics.iloc[0]\n\n        col1, col2, col3, col4 = st.columns(4)\n\n        with col1:\n            st.metric(\"Total Events\", f\"{int(row['total_events']):,}\",\n                     help=\"Total number of events in selected period\")\n\n        with col2:\n            st.metric(\"Unique Users\", f\"{int(row['users']):,}\",\n                     help=\"Number of distinct users\")\n\n        with col3:\n            st.metric(\"Projects\", f\"{int(row['projects']):,}\",\n                     help=\"Number of distinct projects\")\n\n        with col4:\n            st.metric(\"Average Value\", f\"{row['avg_value']:.2f}\",\n                     help=\"Mean value across all events\")\n\ndef create_trends_section(where_clause: str):\n    \"\"\"Display trend analysis.\"\"\"\n    trend_query = f'''\n        SELECT\n            DATE_TRUNC('day', TO_TIMESTAMP(\"event_date\")) as date,\n            COUNT(*) as events,\n            COUNT(DISTINCT \"user_name\") as users\n        FROM {get_table_name()}\n        WHERE {where_clause}\n            AND TO_TIMESTAMP(\"event_date\") >= CURRENT_DATE - INTERVAL '30 days'\n        GROUP BY DATE_TRUNC('day', TO_TIMESTAMP(\"event_date\"))\n        ORDER BY date\n    '''\n\n    trends = execute_aggregation_query(trend_query)\n\n    if not trends.empty and len(trends) > 1:\n        col1, col2 = st.columns(2)\n\n        with col1:\n            fig = px.line(\n                trends,\n                x='date',\n                y='events',\n                title='Daily Events',\n                markers=True\n            )\n            st.plotly_chart(fig, use_container_width=True)\n\n        with col2:\n            fig = px.line(\n                trends,\n                x='date',\n                y='users',\n                title='Daily Active Users',\n                markers=True\n            )\n            st.plotly_chart(fig, use_container_width=True)\n    else:\n        st.info(\"Not enough data for trend analysis\")\n\ndef create_details_section(where_clause: str):\n    \"\"\"Display detailed data table.\"\"\"\n    details_query = f'''\n        SELECT\n            \"user_name\",\n            \"category\",\n            COUNT(*) as events,\n            MIN(TO_TIMESTAMP(\"event_date\")) as first_seen,\n            MAX(TO_TIMESTAMP(\"event_date\")) as last_seen\n        FROM {get_table_name()}\n        WHERE {where_clause}\n        GROUP BY \"user_name\", \"category\"\n        ORDER BY events DESC\n        LIMIT 100\n    '''\n\n    details = execute_aggregation_query(details_query)\n\n    if not details.empty:\n        # Add interactive filters\n        col1, col2 = st.columns(2)\n\n        with col1:\n            min_events = st.number_input(\"Min Events:\", min_value=0, value=0)\n\n        with col2:\n            categories = details['category'].unique().tolist()\n            selected_categories = st.multiselect(\n                \"Filter Categories:\",\n                options=categories,\n                default=categories,\n                key=\"detail_category_filter\"\n            )\n\n        # Apply filters\n        filtered = details[\n            (details['events'] >= min_events) &\n            (details['category'].isin(selected_categories))\n        ]\n\n        st.dataframe(filtered, use_container_width=True, height=400)\n\n        # Download button\n        csv = filtered.to_csv(index=False)\n        st.download_button(\n            label=\"ðŸ“¥ Download as CSV\",\n            data=csv,\n            file_name=\"details.csv\",\n            mime=\"text/csv\"\n        )\n    else:\n        st.info(\"No detail data available\")\n```\n\n---\n\n## Query Pattern Templates\n\n### Basic Aggregation Query\n\n```python\nquery = f'''\n    SELECT\n        \"category\",\n        COUNT(*) as event_count,\n        COUNT(DISTINCT \"user_id\") as unique_users,\n        AVG(\"value\") as avg_value,\n        SUM(\"amount\") as total_amount\n    FROM {get_table_name()}\n    WHERE {where_clause}\n    GROUP BY \"category\"\n    ORDER BY event_count DESC\n'''\n```\n\n### Time-Series Aggregation\n\n```python\nquery = f'''\n    SELECT\n        DATE_TRUNC('day', TO_TIMESTAMP(\"event_date\")) as date,\n        COUNT(*) as daily_events,\n        COUNT(DISTINCT \"user_id\") as daily_active_users,\n        AVG(\"value\") as daily_avg_value\n    FROM {get_table_name()}\n    WHERE {where_clause}\n        AND TO_TIMESTAMP(\"event_date\") >= CURRENT_DATE - INTERVAL '90 days'\n    GROUP BY DATE_TRUNC('day', TO_TIMESTAMP(\"event_date\"))\n    ORDER BY date ASC\n'''\n```\n\n### Top N Query with Percentage\n\n```python\nquery = f'''\n    SELECT\n        \"category\",\n        COUNT(*) as count,\n        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage\n    FROM {get_table_name()}\n    WHERE {where_clause}\n    GROUP BY \"category\"\n    ORDER BY count DESC\n    LIMIT 10\n'''\n```\n\n### Conditional Aggregation\n\n```python\nquery = f'''\n    SELECT\n        \"category\",\n        COUNT(*) as total,\n        SUM(CASE WHEN \"status\" = 'success' THEN 1 ELSE 0 END) as successes,\n        SUM(CASE WHEN \"status\" = 'error' THEN 1 ELSE 0 END) as errors,\n        ROUND(SUM(CASE WHEN \"status\" = 'success' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as success_rate\n    FROM {get_table_name()}\n    WHERE {where_clause}\n    GROUP BY \"category\"\n    ORDER BY total DESC\n'''\n```\n\n### Window Function Query\n\n```python\nquery = f'''\n    WITH ranked_data AS (\n        SELECT\n            \"user_name\",\n            \"event_date\",\n            \"value\",\n            ROW_NUMBER() OVER (PARTITION BY \"user_name\" ORDER BY \"event_date\" DESC) as row_num\n        FROM {get_table_name()}\n        WHERE {where_clause}\n    )\n    SELECT\n        \"user_name\",\n        \"event_date\" as last_event_date,\n        \"value\" as last_value\n    FROM ranked_data\n    WHERE row_num = 1\n    ORDER BY \"event_date\" DESC\n'''\n```\n\n### Join Query with Multiple Tables\n\n```python\n# When you need data from multiple tables\nevents_table = get_table_name('out.c-bucket.events')\nusers_table = get_table_name('out.c-bucket.users')\n\nquery = f'''\n    SELECT\n        e.\"event_id\",\n        e.\"event_type\",\n        e.\"event_date\",\n        u.\"user_name\",\n        u.\"user_tier\",\n        u.\"organization\"\n    FROM {events_table} e\n    LEFT JOIN {users_table} u\n        ON e.\"user_id\" = u.\"user_id\"\n    WHERE {where_clause}\n    ORDER BY e.\"event_date\" DESC\n    LIMIT 1000\n'''\n```\n\n---\n\n## UI Component Templates\n\n### Global Filter in Sidebar (streamlit_dashboard.py)\n\n```python\n# Add after existing filters, before navigation\n\nst.sidebar.markdown(\"---\")\nst.sidebar.markdown(\"**ðŸŽ¯ Filter Label**\")\n\n# Initialize session state\nif 'filter_name' not in st.session_state:\n    st.session_state.filter_name = 'Default Value'\n\n# Create radio button\nfilter_option = st.sidebar.radio(\n    \"Select option:\",\n    options=['Option 1', 'Option 2', 'All'],\n    index=['Option 1', 'Option 2', 'All'].index(st.session_state.filter_name),\n    help=\"Description of what this filter does and when to use each option.\"\n)\n\n# Update session state and rerun if changed\nif filter_option != st.session_state.filter_name:\n    st.session_state.filter_name = filter_option\n    st.rerun()\n\nst.sidebar.markdown(\"---\")\n```\n\n### Date Range Selector\n\n```python\nst.sidebar.markdown(\"**ðŸ“… Date Range**\")\n\nif 'date_range' not in st.session_state:\n    st.session_state.date_range = 'Last 90 Days'\n\ndate_options = ['Last 7 Days', 'Last 30 Days', 'Last 90 Days', 'Last Year', 'All Time']\n\ndate_range = st.sidebar.selectbox(\n    \"Select date range:\",\n    options=date_options,\n    index=date_options.index(st.session_state.date_range)\n)\n\nif date_range != st.session_state.date_range:\n    st.session_state.date_range = date_range\n    st.rerun()\n```\n\n### Multi-Select Filter\n\n```python\nst.sidebar.markdown(\"**ðŸ·ï¸ Categories**\")\n\nif 'selected_categories' not in st.session_state:\n    st.session_state.selected_categories = []\n\n# Get available categories from data\navailable_categories = get_available_categories()\n\nselected = st.sidebar.multiselect(\n    \"Filter by categories:\",\n    options=available_categories,\n    default=st.session_state.selected_categories or available_categories,\n    help=\"Select one or more categories to filter data\"\n)\n\nif selected != st.session_state.selected_categories:\n    st.session_state.selected_categories = selected\n    st.rerun()\n```\n\n### Metrics Display Grid\n\n```python\n# 3-column metric layout\ncol1, col2, col3 = st.columns(3)\n\nwith col1:\n    st.metric(\n        \"Metric 1\",\n        f\"{value1:,}\",\n        delta=f\"{change_pct:.1f}%\",\n        help=\"Description of what this metric represents\"\n    )\n\nwith col2:\n    st.metric(\n        \"Metric 2\",\n        f\"${value2:.2f}\",\n        help=\"Description of metric 2\"\n    )\n\nwith col3:\n    st.metric(\n        \"Metric 3\",\n        f\"{value3:.1f}%\",\n        help=\"Description of metric 3\"\n    )\n```\n\n### Expandable Detail Section\n\n```python\nwith st.expander(\"ðŸ“‹ View Detailed Data\"):\n    st.markdown(\"### Additional Context\")\n\n    # Show detailed table\n    st.dataframe(\n        detailed_df,\n        use_container_width=True,\n        height=400,\n        hide_index=True\n    )\n\n    # Download option\n    csv = detailed_df.to_csv(index=False)\n    st.download_button(\n        label=\"ðŸ“¥ Download as CSV\",\n        data=csv,\n        file_name=f\"details_{datetime.now().strftime('%Y%m%d')}.csv\",\n        mime=\"text/csv\"\n    )\n```\n\n---\n\n## Data Loader Templates\n\n### Query Execution with Error Handling\n\n```python\n@st.cache_data(ttl=300)\ndef load_data_safe(where_clause: str) -> pd.DataFrame:\n    \"\"\"\n    Load data with comprehensive error handling.\n\n    Args:\n        where_clause: SQL WHERE clause\n\n    Returns:\n        DataFrame with results or empty DataFrame on error\n    \"\"\"\n    query = f'''\n        SELECT *\n        FROM {get_table_name()}\n        WHERE {where_clause}\n    '''\n\n    try:\n        df = execute_aggregation_query(query)\n\n        if df.empty:\n            st.info(\"No data found matching the criteria\")\n            return pd.DataFrame()\n\n        return df\n\n    except Exception as e:\n        st.error(f\"Failed to load data: {str(e)}\")\n\n        # Show query for debugging\n        with st.expander(\"Query Details\"):\n            st.code(query, language='sql')\n\n        return pd.DataFrame()\n```\n\n### Multi-Table Data Loader\n\n```python\n@st.cache_data(ttl=300)\ndef load_combined_data(where_clause: str) -> pd.DataFrame:\n    \"\"\"\n    Load and combine data from multiple sources.\n\n    Args:\n        where_clause: SQL WHERE clause to apply\n\n    Returns:\n        Combined DataFrame\n    \"\"\"\n    # Load primary data\n    primary = execute_aggregation_query(f'''\n        SELECT *\n        FROM {get_table_name('out.c-main.events')}\n        WHERE {where_clause}\n    ''')\n\n    if primary.empty:\n        return pd.DataFrame()\n\n    # Load reference data\n    reference = execute_aggregation_query(f'''\n        SELECT *\n        FROM {get_table_name('out.c-main.users')}\n    ''')\n\n    # Merge\n    if not reference.empty:\n        combined = primary.merge(\n            reference,\n            on='user_id',\n            how='left'\n        )\n        return combined\n\n    return primary\n```\n\n### Available Options Loader\n\n```python\n@st.cache_data(ttl=300)\ndef get_available_categories() -> list:\n    \"\"\"\n    Get list of available categories from database.\n\n    Returns:\n        Sorted list of category values\n    \"\"\"\n    query = f'''\n        SELECT DISTINCT \"category\"\n        FROM {get_table_name()}\n        WHERE \"category\" IS NOT NULL\n            AND \"category\" != ''\n        ORDER BY \"category\"\n    '''\n\n    try:\n        df = execute_aggregation_query(query)\n        if not df.empty and 'category' in df.columns:\n            return df['category'].tolist()\n        return []\n    except Exception as e:\n        st.error(f\"Error loading categories: {e}\")\n        return []\n```\n\n---\n\n## Visualization Templates\n\n### Line Chart with Markers\n\n```python\nfig = px.line(\n    data,\n    x='date',\n    y='value',\n    title='Trend Over Time',\n    labels={'value': 'Value', 'date': 'Date'},\n    markers=True\n)\nfig.update_layout(\n    hovermode='x unified',\n    height=400\n)\nst.plotly_chart(fig, use_container_width=True)\n```\n\n### Bar Chart with Color Coding\n\n```python\nfig = px.bar(\n    data,\n    x='category',\n    y='count',\n    color='metric',\n    title='Distribution by Category',\n    labels={'count': 'Count', 'category': 'Category'},\n    color_continuous_scale='Blues'\n)\nfig.update_xaxes(tickangle=-45)\nst.plotly_chart(fig, use_container_width=True)\n```\n\n### Pie Chart with Hole\n\n```python\nfig = px.pie(\n    data,\n    values='count',\n    names='category',\n    title='Distribution by Category',\n    hole=0.4  # Donut chart\n)\nst.plotly_chart(fig, use_container_width=True)\n```\n\n### Multi-Line Chart\n\n```python\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=data['date'],\n    y=data['metric1'],\n    name='Metric 1',\n    mode='lines+markers',\n    line=dict(color='#1f77b4', width=2)\n))\n\nfig.add_trace(go.Scatter(\n    x=data['date'],\n    y=data['metric2'],\n    name='Metric 2',\n    mode='lines+markers',\n    line=dict(color='#ff7f0e', width=2)\n))\n\nfig.update_layout(\n    title='Comparison Over Time',\n    xaxis_title='Date',\n    yaxis_title='Value',\n    hovermode='x unified',\n    height=400\n)\n\nst.plotly_chart(fig, use_container_width=True)\n```\n\n### Histogram with Threshold Lines\n\n```python\nfig = px.histogram(\n    data,\n    x='value',\n    title='Value Distribution',\n    nbins=50\n)\n\n# Add threshold lines\nfig.add_vline(\n    x=mean_value,\n    line_dash=\"dash\",\n    line_color=\"blue\",\n    annotation_text=\"Mean\"\n)\nfig.add_vline(\n    x=p95_value,\n    line_dash=\"dash\",\n    line_color=\"red\",\n    annotation_text=\"P95\"\n)\n\nst.plotly_chart(fig, use_container_width=True)\n```\n\n---\n\n## Complete Page Example\n\n### Full Implementation: User Analysis Page\n\n```python\n\"\"\"User Analysis Page - Detailed user statistics and engagement metrics\"\"\"\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nfrom datetime import datetime\nfrom utils.data_loader import (\n    execute_aggregation_query,\n    get_table_name,\n    get_agent_filter_clause,\n    get_user_type_filter_clause,\n    get_selected_agent_name\n)\n\ndef create_user_analysis():\n    \"\"\"Create the user analysis page.\"\"\"\n\n    selected_agent = get_selected_agent_name()\n    st.title(f\"ðŸ‘¥ User Analysis: {selected_agent}\")\n    st.markdown(\"---\")\n\n    # Build WHERE clause\n    where_parts = ['\"type\" = \\'success\\'', get_agent_filter_clause()]\n    user_type_filter = get_user_type_filter_clause()\n    if user_type_filter:\n        where_parts.append(user_type_filter)\n    where_clause = ' AND '.join(where_parts)\n\n    # Section 1: Overview Metrics\n    st.markdown(\"## ðŸ“Š Overview\")\n    overview_data = load_overview_metrics(where_clause)\n    display_overview_metrics(overview_data)\n\n    st.markdown(\"---\")\n\n    # Section 2: User Details\n    st.markdown(\"## ðŸ“‹ User Details\")\n    user_details = load_user_details(where_clause)\n    display_user_table(user_details)\n\n    st.markdown(\"---\")\n\n    # Section 3: Activity Timeline\n    st.markdown(\"## ðŸ“… Activity Timeline\")\n    activity_data = load_activity_timeline(where_clause)\n    display_activity_chart(activity_data)\n\n@st.cache_data(ttl=300)\ndef load_overview_metrics(where_clause: str) -> pd.DataFrame:\n    \"\"\"Load overview metrics.\"\"\"\n    query = f'''\n        SELECT\n            COUNT(DISTINCT \"user_name\") as total_users,\n            COUNT(*) as total_events,\n            AVG(total_events_per_user.event_count) as avg_events_per_user\n        FROM (\n            SELECT\n                \"user_name\",\n                COUNT(*) as event_count\n            FROM {get_table_name()}\n            WHERE {where_clause}\n            GROUP BY \"user_name\"\n        ) total_events_per_user\n    '''\n    return execute_aggregation_query(query)\n\n@st.cache_data(ttl=300)\ndef load_user_details(where_clause: str) -> pd.DataFrame:\n    \"\"\"Load detailed user statistics.\"\"\"\n    query = f'''\n        SELECT\n            \"user_name\",\n            COUNT(*) as total_events,\n            COUNT(DISTINCT \"project_id\") as projects,\n            MIN(TO_TIMESTAMP(\"event_date\")) as first_seen,\n            MAX(TO_TIMESTAMP(\"event_date\")) as last_seen\n        FROM {get_table_name()}\n        WHERE {where_clause}\n        GROUP BY \"user_name\"\n        ORDER BY total_events DESC\n    '''\n    return execute_aggregation_query(query)\n\n@st.cache_data(ttl=300)\ndef load_activity_timeline(where_clause: str) -> pd.DataFrame:\n    \"\"\"Load activity timeline data.\"\"\"\n    query = f'''\n        SELECT\n            DATE(TO_TIMESTAMP(\"event_date\")) as date,\n            COUNT(DISTINCT \"user_name\") as active_users,\n            COUNT(*) as events\n        FROM {get_table_name()}\n        WHERE {where_clause}\n            AND TO_TIMESTAMP(\"event_date\") >= CURRENT_DATE - INTERVAL '90 days'\n        GROUP BY DATE(TO_TIMESTAMP(\"event_date\"))\n        ORDER BY date\n    '''\n    return execute_aggregation_query(query)\n\ndef display_overview_metrics(data: pd.DataFrame):\n    \"\"\"Display overview metrics.\"\"\"\n    if data.empty:\n        st.warning(\"No overview data available\")\n        return\n\n    row = data.iloc[0]\n\n    col1, col2, col3 = st.columns(3)\n\n    with col1:\n        st.metric(\"Total Users\", f\"{int(row['total_users']):,}\")\n\n    with col2:\n        st.metric(\"Total Events\", f\"{int(row['total_events']):,}\")\n\n    with col3:\n        st.metric(\"Avg Events/User\", f\"{row['avg_events_per_user']:.1f}\")\n\ndef display_user_table(data: pd.DataFrame):\n    \"\"\"Display user details table.\"\"\"\n    if data.empty:\n        st.warning(\"No user data available\")\n        return\n\n    # Add filters\n    col1, col2 = st.columns(2)\n\n    with col1:\n        min_events = st.number_input(\"Min Events:\", min_value=0, value=0)\n\n    with col2:\n        sort_by = st.selectbox(\"Sort by:\", [\"Total Events\", \"Projects\", \"User Name\"])\n\n    # Apply filters\n    filtered = data[data['total_events'] >= min_events]\n\n    # Apply sorting\n    if sort_by == \"Total Events\":\n        filtered = filtered.sort_values('total_events', ascending=False)\n    elif sort_by == \"Projects\":\n        filtered = filtered.sort_values('projects', ascending=False)\n    else:\n        filtered = filtered.sort_values('user_name')\n\n    # Display\n    st.dataframe(filtered, use_container_width=True, height=400)\n\ndef display_activity_chart(data: pd.DataFrame):\n    \"\"\"Display activity timeline chart.\"\"\"\n    if data.empty or len(data) < 2:\n        st.info(\"Not enough data for timeline\")\n        return\n\n    fig = px.line(\n        data,\n        x='date',\n        y='active_users',\n        title='Daily Active Users (Last 90 Days)',\n        markers=True\n    )\n    st.plotly_chart(fig, use_container_width=True)\n```\n\n---\n\n## Testing Templates\n\n### Validation Query Template\n\n```sql\n-- Run this with mcp__keboola__query_data BEFORE writing code\n-- to validate assumptions\n\n-- 1. Check if column exists and get distinct values\nSELECT DISTINCT \"column_name\", COUNT(*) as count\nFROM \"database\".\"schema\".\"table\"\nGROUP BY \"column_name\"\nORDER BY count DESC\nLIMIT 20;\n\n-- 2. Check data types and NULL percentages\nSELECT\n    COUNT(*) as total_rows,\n    COUNT(\"column\") as non_null_count,\n    COUNT(*) - COUNT(\"column\") as null_count,\n    ROUND((COUNT(*) - COUNT(\"column\")) * 100.0 / COUNT(*), 2) as null_pct\nFROM \"database\".\"schema\".\"table\";\n\n-- 3. Test filter condition\nSELECT COUNT(*) as matching_rows\nFROM \"database\".\"schema\".\"table\"\nWHERE \"column\" = 'expected_value';\n\n-- 4. Sample data preview\nSELECT *\nFROM \"database\".\"schema\".\"table\"\nLIMIT 10;\n```\n\n### Playwright Verification Script\n\n```python\n# Step-by-step Playwright verification\n\n# 1. Navigate to app\nmcp__playwright__browser_navigate(url=\"http://localhost:8501\")\n\n# 2. Wait for load\nmcp__playwright__browser_wait_for(time=3)\n\n# 3. Take baseline screenshot\nmcp__playwright__browser_take_screenshot(filename=\"page-baseline.png\")\n\n# 4. Test interaction\nmcp__playwright__browser_click(element=\"Filter option\", ref=\"e123\")\n\n# 5. Wait for update\nmcp__playwright__browser_wait_for(time=2)\n\n# 6. Verify result\nmcp__playwright__browser_take_screenshot(filename=\"page-after-filter.png\")\n\n# 7. Navigate to another page\nmcp__playwright__browser_click(element=\"Users page\", ref=\"e456\")\n\n# 8. Wait and verify\nmcp__playwright__browser_wait_for(time=2)\nmcp__playwright__browser_take_screenshot(filename=\"users-page.png\")\n```\n\n---\n\n## Common Patterns\n\n### Building WHERE Clauses\n\n```python\n# Pattern: Combine multiple filters safely\n\nwhere_parts = []\n\n# Always include base filter\nwhere_parts.append('\"type\" = \\'success\\'')\n\n# Add required filters\nwhere_parts.append(get_agent_filter_clause())\n\n# Add optional filters only if they return a value\nuser_filter = get_user_type_filter_clause()\nif user_filter:\n    where_parts.append(user_filter)\n\ndate_filter = get_date_filter_clause()\nif date_filter:\n    where_parts.append(date_filter)\n\ncategory_filter = get_category_filter_clause()\nif category_filter:\n    where_parts.append(category_filter)\n\n# Combine all parts\nwhere_clause = ' AND '.join(where_parts)\n\n# Use in query\nquery = f'''\n    SELECT ...\n    FROM {get_table_name()}\n    WHERE {where_clause}\n    GROUP BY ...\n'''\n```\n\n### Session State Initialization\n\n```python\n# Initialize all session state at app start\ndef initialize_session_state():\n    \"\"\"Initialize all session state variables with defaults.\"\"\"\n    defaults = {\n        'selected_agent': 'All Agents',\n        'user_type_filter': 'External Users Only',\n        'date_range': 'Last 90 Days',\n        'selected_categories': [],\n        'sort_order': 'Descending'\n    }\n\n    for key, default_value in defaults.items():\n        if key not in st.session_state:\n            st.session_state[key] = default_value\n\n# Call at the start of main()\ndef main():\n    initialize_session_state()\n    # ... rest of app\n```\n\n### Error Boundary Pattern\n\n```python\ndef safe_section(section_func, section_name: str):\n    \"\"\"\n    Execute section with error handling.\n\n    Args:\n        section_func: Function to execute\n        section_name: Name for error messages\n    \"\"\"\n    try:\n        section_func()\n    except Exception as e:\n        st.error(f\"Error in {section_name}: {str(e)}\")\n        with st.expander(\"Error Details\"):\n            import traceback\n            st.code(traceback.format_exc())\n\n# Usage\ndef create_page():\n    safe_section(create_overview_section, \"Overview\")\n    safe_section(create_trends_section, \"Trends\")\n    safe_section(create_details_section, \"Details\")\n```\n\n---\n\n## Quick Reference: Common Snippets\n\n### Format Number with Commas\n```python\nst.metric(\"Users\", f\"{count:,}\")  # 1234567 â†’ 1,234,567\n```\n\n### Format Percentage\n```python\nst.metric(\"Success Rate\", f\"{rate:.2f}%\")  # 0.9567 â†’ 95.67%\n```\n\n### Format Currency\n```python\nst.metric(\"Total Cost\", f\"${amount:.2f}\")  # 1234.567 â†’ $1234.57\n```\n\n### Format Date\n```python\ndate_str = pd.to_datetime(date_val).strftime('%Y-%m-%d')  # 2025-11-19\n```\n\n### Conditional Emoji\n```python\nemoji = \"âœ…\" if value >= threshold else \"âš ï¸\" if value >= warning else \"ðŸ”´\"\nst.metric(\"Status\", f\"{value:.1f}% {emoji}\")\n```\n\n### Safe Column Access\n```python\nif 'column_name' in df.columns:\n    value = df['column_name'].iloc[0]\nelse:\n    st.warning(\"Expected column not found\")\n    value = None\n```\n\n---\n\nUse these templates as starting points and adapt them to your specific needs!\n",
        "plugins/dataapp-developer/skills/dataapp-dev/validation-checklist.md": "# Validation Checklist for Keboola Data Apps\n\nUse this checklist to ensure quality before committing changes to production.\n\n## ðŸ” Pre-Development Validation\n\nBefore writing any code, validate these items using Keboola MCP:\n\n### Data Structure Validation\n\n- [ ] **Table exists**: Used `mcp__keboola__get_table` to verify table ID\n- [ ] **Schema reviewed**: Checked all column names (exact case)\n- [ ] **Data types confirmed**: Verified database_native_type and keboola_base_type\n- [ ] **Fully qualified name**: Got complete table name for queries\n- [ ] **Sample data queried**: Used `mcp__keboola__query_data` to see actual values\n\n### Column Validation\n\n- [ ] **Column exists**: Confirmed required columns are in schema\n- [ ] **Names are correct**: Verified exact spelling and case (e.g., \"user_name\" not \"username\")\n- [ ] **Types are compatible**: Checked data types support intended operations\n- [ ] **NULL handling**: Checked if columns can be NULL and handled appropriately\n- [ ] **Value ranges**: Queried distinct values for categorical columns\n\n### Filter Logic Validation\n\n- [ ] **Filter tested**: Ran test query with filter condition\n- [ ] **Results expected**: Verified filter returns correct row count\n- [ ] **Edge cases checked**: Tested empty results, NULL values, special characters\n- [ ] **SQL syntax validated**: Confirmed query works before embedding in code\n\n### Example Validation Queries\n\n```sql\n-- 1. Check column exists and get distinct values\nSELECT DISTINCT \"column_name\", COUNT(*) as count\nFROM \"database\".\"schema\".\"table\"\nGROUP BY \"column_name\"\nORDER BY count DESC;\n\n-- 2. Test filter condition\nSELECT COUNT(*) as filtered_count\nFROM \"database\".\"schema\".\"table\"\nWHERE \"column_name\" = 'filter_value';\n\n-- 3. Verify data types\nSELECT\n    \"column1\",\n    \"column2\",\n    TYPEOF(\"column1\") as col1_type,\n    TYPEOF(\"column2\") as col2_type\nFROM \"database\".\"schema\".\"table\"\nLIMIT 5;\n\n-- 4. Check for NULLs\nSELECT\n    COUNT(*) as total,\n    COUNT(\"column\") as non_null,\n    COUNT(*) - COUNT(\"column\") as nulls\nFROM \"database\".\"schema\".\"table\";\n```\n\n## ðŸ’» Development Validation\n\nDuring implementation, verify these items:\n\n### Code Quality\n\n- [ ] **Follows SQL-first**: Aggregation in database, not Python\n- [ ] **Environment agnostic**: Uses `os.environ.get() or st.secrets.get()`\n- [ ] **Proper caching**: All queries use `@st.cache_data(ttl=300)`\n- [ ] **Error handling**: Try-except blocks for all data loads\n- [ ] **Type hints used**: Functions have parameter and return types\n- [ ] **Docstrings present**: All functions documented\n\n### Session State\n\n- [ ] **Initialized properly**: All session state variables have defaults\n- [ ] **Unique keys**: No key conflicts between global and local state\n- [ ] **Variable names unique**: No reuse of names in same scope\n- [ ] **State updates trigger rerun**: Changes to session state call `st.rerun()`\n\n### Query Construction\n\n- [ ] **WHERE clauses combined**: Used `' AND '.join(where_parts)` pattern\n- [ ] **Quoted identifiers**: All column names in double quotes\n- [ ] **Fully qualified tables**: Used `get_table_name()` function\n- [ ] **Date filters added**: Time-series queries have date range limits\n- [ ] **NULL safe**: Used `IS NOT NULL`, `COALESCE`, or `NULLIF` where needed\n\n### Import Consistency\n\n- [ ] **All page modules updated**: New filters imported in every page\n- [ ] **Import order consistent**: Alphabetical or logical grouping\n- [ ] **No unused imports**: Removed imports for deleted code\n- [ ] **All functions available**: Imported everything needed\n\n### Variable Naming\n\n- [ ] **No conflicts**: SQL filter variables don't clash with UI widget variables\n- [ ] **Descriptive names**: `user_type_sql_filter` vs `user_type_multiselect`\n- [ ] **Consistent conventions**: snake_case for Python, UPPER for SQL\n- [ ] **Keys are unique**: Widget keys like `\"local_category_filter\"` don't conflict with global filters\n\n## ðŸŽ¨ Visual Validation\n\nAfter implementation, verify using Playwright MCP:\n\n### App Startup\n\n- [ ] **App running**: Verified with `lsof -ti:8501`\n- [ ] **Port accessible**: Can navigate to `http://localhost:8501`\n- [ ] **No startup errors**: App loads without console errors\n\n### UI Verification\n\n- [ ] **Filter displays**: New filter appears in sidebar\n- [ ] **Default selected**: Correct default option is selected\n- [ ] **Position correct**: Filter placed in logical location\n- [ ] **Help text clear**: Tooltip explains what filter does\n- [ ] **Layout preserved**: Existing UI not broken\n\n### Interaction Testing\n\n- [ ] **Filter changes data**: Switching filter updates metrics\n- [ ] **All options work**: Tested each filter option\n- [ ] **Page navigation works**: All pages accessible\n- [ ] **No errors displayed**: No red error messages in UI\n- [ ] **No console errors**: Browser console clean\n\n### Cross-Page Validation\n\nFor each page affected by changes:\n\n- [ ] **Page loads**: No errors on page load\n- [ ] **Metrics display**: Numbers show and make sense\n- [ ] **Charts render**: All visualizations appear\n- [ ] **Filters apply**: Data changes when filter changes\n- [ ] **Local filters work**: Page-specific filters still functional\n- [ ] **Downloads work**: CSV export buttons functional (if applicable)\n\n### Screenshot Documentation\n\n- [ ] **Default state**: Screenshot of filter in default state\n- [ ] **Alternative states**: Screenshots of other filter options\n- [ ] **Each major page**: Screenshot of affected pages working\n- [ ] **Error cases**: Screenshot if you found and fixed issues\n- [ ] **Saved in project**: Screenshots saved for future reference\n\n### Example Playwright Verification Sequence\n\n```python\n# 1. Open app\nmcp__playwright__browser_navigate(\"http://localhost:8501\")\nmcp__playwright__browser_wait_for(time=3)\n\n# 2. Baseline screenshot\nmcp__playwright__browser_take_screenshot(\"baseline.png\")\n\n# 3. Test default state\n# Verify filter shows correct default\n# Verify metrics match expected values\n\n# 4. Change filter\nmcp__playwright__browser_click(\"Filter option 2\")\nmcp__playwright__browser_wait_for(time=2)\nmcp__playwright__browser_take_screenshot(\"filter-option2.png\")\n\n# 5. Verify data changed\n# Compare metrics to baseline\n# Confirm filter is working\n\n# 6. Test each page\nfor page in [\"Users\", \"Errors\", \"Adoption\", \"Reliability\", \"Tools\", \"Use Cases\"]:\n    mcp__playwright__browser_click(f\"{page} navigation\")\n    mcp__playwright__browser_wait_for(time=2)\n    # Check for errors\n    mcp__playwright__browser_take_screenshot(f\"{page.lower()}-page.png\")\n\n# 7. Final verification\n# All pages loaded successfully\n# No errors anywhere\n# Ready to commit\n```\n\n## ðŸ“‹ Pre-Commit Checklist\n\nBefore committing changes:\n\n### Code Review\n\n- [ ] **Followed patterns**: Code matches existing style\n- [ ] **No hardcoded values**: All config from environment/secrets\n- [ ] **No debug code**: Removed print statements, test code\n- [ ] **No commented code**: Removed old code blocks\n- [ ] **Imports cleaned**: No unused imports\n\n### Testing Complete\n\n- [ ] **All validations passed**: Checked data, schema, queries\n- [ ] **Visual verification done**: Tested with Playwright\n- [ ] **All pages tested**: Navigated through entire app\n- [ ] **Screenshots captured**: Documented working state\n- [ ] **No errors found**: Both code and UI clean\n\n### Documentation\n\n- [ ] **Code comments added**: Complex logic explained\n- [ ] **Docstrings updated**: New functions documented\n- [ ] **Help text added**: UI tooltips explain features\n- [ ] **README updated**: If major feature added\n\n### Git Hygiene\n\n- [ ] **Secrets excluded**: `.gitignore` covers `.streamlit/secrets.toml`\n- [ ] **Relevant files staged**: Only changed files committed\n- [ ] **Commit message clear**: Describes what and why\n- [ ] **Ready to push**: Confident changes work\n\n## ðŸŽ¯ Quality Gates\n\n### Minimum Quality Requirements\n\n**Data Validation** (MUST have):\n- âœ… Queried actual data with Keboola MCP\n- âœ… Verified column names from schema\n- âœ… Tested SQL syntax before embedding\n\n**Visual Verification** (MUST have):\n- âœ… Opened app in browser with Playwright\n- âœ… Tested core functionality\n- âœ… Took at least one screenshot\n\n**Code Quality** (MUST have):\n- âœ… Follows existing patterns\n- âœ… No variable name conflicts\n- âœ… Session state initialized\n- âœ… Error handling present\n\n### Nice to Have\n\n- â­ Comprehensive screenshots of all pages\n- â­ Performance testing done\n- â­ Edge cases tested\n- â­ Documentation updated\n- â­ Code comments added\n\n## ðŸš¦ Traffic Light System\n\nUse this to assess readiness:\n\n### ðŸ”´ RED - Not Ready\n- Missing validation (didn't check data)\n- Skipped visual testing\n- Errors present in UI\n- Variable conflicts unresolved\n\n### ðŸŸ¡ YELLOW - Almost Ready\n- Basic validation done\n- Some visual testing\n- Minor issues remaining\n- Needs final verification\n\n### ðŸŸ¢ GREEN - Ready to Commit\n- Full validation complete\n- Visual verification done\n- All pages tested\n- Screenshots captured\n- No errors anywhere\n- Confident in changes\n\n## ðŸ“ˆ Success Metrics\n\nTrack your improvement:\n\n### First Time Using Skill\n- Validation time: ~10 minutes\n- Build time: ~20 minutes\n- Verify time: ~10 minutes\n- **Total: ~40 minutes**\n- Bugs found: 0 (caught in verify phase)\n\n### After Practice\n- Validation time: ~5 minutes (know what to check)\n- Build time: ~10 minutes (use templates)\n- Verify time: ~5 minutes (know what to test)\n- **Total: ~20 minutes**\n- Bugs found: 0\n- Confidence: High\n\n## ðŸŽŠ Completion Criteria\n\nA task is complete when:\n\nâœ… **All validation checks passed**\nâœ… **Code follows best practices**\nâœ… **Visual verification done**\nâœ… **Screenshots prove it works**\nâœ… **No errors in any page**\nâœ… **Ready to commit with confidence**\n\n---\n\n## Quick Reference Cards\n\n### ðŸ” VALIDATE Phase Checklist\n```\nâ–¡ mcp__keboola__get_project_info\nâ–¡ mcp__keboola__get_table(table_id)\nâ–¡ mcp__keboola__query_data(test query)\nâ–¡ Verify column names\nâ–¡ Test filter conditions\n```\n\n### ðŸ’» BUILD Phase Checklist\n```\nâ–¡ Update utils/data_loader.py\nâ–¡ Update streamlit_dashboard.py\nâ–¡ Update all page modules\nâ–¡ Initialize session state\nâ–¡ No variable conflicts\n```\n\n### âœ… VERIFY Phase Checklist\n```\nâ–¡ mcp__playwright__browser_navigate\nâ–¡ mcp__playwright__browser_wait_for\nâ–¡ mcp__playwright__browser_take_screenshot\nâ–¡ Test interactions\nâ–¡ Check all pages\nâ–¡ No errors\n```\n\n---\n\n**Print this checklist and use it during development!**\n",
        "plugins/dataapp-developer/skills/dataapp-dev/workflow-guide.md": "# Workflow Guide: Step-by-Step Examples\n\nThis guide provides concrete examples of the Validate â†’ Build â†’ Verify workflow for common data app development tasks.\n\n## Workflow Overview\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  VALIDATE   â”‚ â”€â”€â”€> â”‚    BUILD    â”‚ â”€â”€â”€> â”‚   VERIFY    â”‚\nâ”‚             â”‚      â”‚             â”‚      â”‚             â”‚\nâ”‚ â€¢ Check     â”‚      â”‚ â€¢ Update    â”‚      â”‚ â€¢ Open app  â”‚\nâ”‚   schemas   â”‚      â”‚   code      â”‚      â”‚ â€¢ Test      â”‚\nâ”‚ â€¢ Query     â”‚      â”‚ â€¢ Add       â”‚      â”‚   features  â”‚\nâ”‚   sample    â”‚      â”‚   filters   â”‚      â”‚ â€¢ Take      â”‚\nâ”‚   data      â”‚      â”‚ â€¢ Import    â”‚      â”‚   screenshotsâ”‚\nâ”‚ â€¢ Verify    â”‚      â”‚   functions â”‚      â”‚ â€¢ Verify    â”‚\nâ”‚   values    â”‚      â”‚             â”‚      â”‚   no errors â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Example 1: Adding a Global Filter\n\n### Scenario\nUser asks: \"Add a global filter for external users vs Keboola users, defaulting to external users only\"\n\n### Step-by-Step Implementation\n\n#### PHASE 1: VALIDATE (5 minutes)\n\n**Step 1.1: Check project info**\n```python\n# Tool: mcp__keboola__get_project_info\n# Purpose: Understand SQL dialect and project structure\n```\n\n**Step 1.2: Inspect table schema**\n```python\n# Tool: mcp__keboola__get_table\n# Parameters: table_id = \"out.c-mcp-analysis.mcp_usage_analysis\"\n# Look for:\n# - Does \"user_type\" column exist?\n# - What is the data type?\n# - What is the fully qualified table name?\n```\n\n**Step 1.3: Query distinct values**\n```python\n# Tool: mcp__keboola__query_data\n# SQL:\nSELECT\n    \"user_type\",\n    COUNT(DISTINCT \"user_name\") as unique_users,\n    COUNT(*) as total_events\nFROM \"KBC_USE4_361\".\"out.c-mcp-analysis\".\"mcp_usage_analysis\"\nWHERE \"type\" = 'success'\nGROUP BY \"user_type\"\nORDER BY total_events DESC\n\n# Expected results:\n# - 'External User' â†’ 122 users, 3,151 events\n# - 'Keboola User' â†’ 55 users, 54,722 events\n```\n\n**Step 1.4: Test filter logic**\n```python\n# Tool: mcp__keboola__query_data\n# Test external users only:\nSELECT COUNT(*) FROM table WHERE \"user_type\" = 'External User'\n\n# Test Keboola users only:\nSELECT COUNT(*) FROM table WHERE \"user_type\" != 'External User'\n```\n\n**Validation Complete**: Now we know:\n- âœ… Column exists and is named `\"user_type\"`\n- âœ… Values are 'External User' and 'Keboola User'\n- âœ… Filter conditions work correctly\n- âœ… Fully qualified table name format\n\n#### PHASE 2: BUILD (15 minutes)\n\n**Step 2.1: Create filter function**\n```python\n# File: utils/data_loader.py\n# Tool: Edit\n\ndef get_user_type_filter_clause():\n    \"\"\"Get SQL WHERE clause for user type filter.\"\"\"\n    if 'user_type_filter' not in st.session_state:\n        st.session_state.user_type_filter = 'External Users Only'\n\n    if st.session_state.user_type_filter == 'External Users Only':\n        return '\"user_type\" = \\'External User\\''\n    elif st.session_state.user_type_filter == 'Keboola Users Only':\n        return '\"user_type\" != \\'External User\\''\n    else:  # All Users\n        return ''\n```\n\n**Step 2.2: Add UI to sidebar**\n```python\n# File: streamlit_dashboard.py\n# Tool: Edit\n# Location: After existing filters\n\nst.sidebar.markdown(\"**ðŸ‘¥ User Type Filter**\")\n\nif 'user_type_filter' not in st.session_state:\n    st.session_state.user_type_filter = 'External Users Only'\n\nuser_type_option = st.sidebar.radio(\n    \"Select user type:\",\n    options=['External Users Only', 'Keboola Users Only', 'All Users'],\n    index=['External Users Only', 'Keboola Users Only', 'All Users'].index(st.session_state.user_type_filter),\n    help=\"Filter data by user type. Defaults to External Users Only.\"\n)\n\nif user_type_option != st.session_state.user_type_filter:\n    st.session_state.user_type_filter = user_type_option\n    st.rerun()\n```\n\n**Step 2.3: Update page modules**\n\nFor each page module:\n\n1. **Add import**:\n```python\nfrom utils.data_loader import (\n    execute_aggregation_query,\n    get_table_name,\n    get_agent_filter_clause,\n    get_user_type_filter_clause,  # Add this\n    get_selected_agent_name\n)\n```\n\n2. **Build WHERE clause**:\n```python\n# At the start of the page function\nwhere_parts = ['\"type\" = \\'success\\'', get_agent_filter_clause()]\nuser_type_filter = get_user_type_filter_clause()\nif user_type_filter:\n    where_parts.append(user_type_filter)\nwhere_clause = ' AND '.join(where_parts)\n```\n\n3. **Update all queries**:\n```python\n# Replace individual filters with combined where_clause\nquery = f'''\n    SELECT ...\n    FROM {get_table_name()}\n    WHERE {where_clause}  # Use combined clause\n    GROUP BY ...\n'''\n```\n\n**Watch for conflicts**: If page has local filters with same name, rename them:\n```python\n# Change this:\nuser_type_filter = st.multiselect(..., key=\"user_type_filter\")\n\n# To this:\nuser_type_local = st.multiselect(..., key=\"local_user_type_filter\")\n```\n\n#### PHASE 3: VERIFY (5 minutes)\n\n**Step 3.1: Check if app is running**\n```bash\n# Tool: Bash\nlsof -ti:8501\n# If empty, start app:\n# streamlit run streamlit_app.py (in background if needed)\n```\n\n**Step 3.2: Open app in browser**\n```python\n# Tool: mcp__playwright__browser_navigate\n# Parameters: url = \"http://localhost:8501\"\n```\n\n**Step 3.3: Wait for load**\n```python\n# Tool: mcp__playwright__browser_wait_for\n# Parameters: time = 3\n```\n\n**Step 3.4: Take screenshot of default state**\n```python\n# Tool: mcp__playwright__browser_take_screenshot\n# Parameters: filename = \"filter-default-state.png\"\n# Verify: Filter shows \"External Users Only\" selected\n# Verify: Metrics show only external user data (122 users, 3,151 events)\n```\n\n**Step 3.5: Test filter options**\n```python\n# Tool: mcp__playwright__browser_click\n# Click \"Keboola Users Only\" radio button\n# Wait and take screenshot\n# Verify: Metrics update (55 users, 54,722 events)\n\n# Click \"All Users\" radio button\n# Wait and take screenshot\n# Verify: Metrics show combined data (177 users, 57,873 events)\n```\n\n**Step 3.6: Navigate through pages**\n```python\n# Click each navigation option:\n# - Overview & Key Highlights âœ“\n# - Users âœ“\n# - Thread Statistics âœ“\n# - Error Rates Analysis âœ“\n# - Adoption Metrics âœ“\n# - Agent Reliability âœ“\n# - Tool Performance âœ“\n# - Use Cases & Patterns âœ“\n\n# For each page:\n# - Wait for load\n# - Check for error messages\n# - Verify metrics update based on filter\n# - Take screenshot if issues found\n```\n\n**Verification Complete**:\n- âœ… Filter UI displays correctly\n- âœ… Default selection works (External Users Only)\n- âœ… Switching filters updates data\n- âœ… All pages respect the filter\n- âœ… No errors in any page\n- âœ… Ready to commit\n\n## Example 2: Adding a New Metric\n\n### Scenario\nUser asks: \"Add a metric showing the percentage of users who have used the agent more than once\"\n\n### PHASE 1: VALIDATE\n\n**Step 1.1: Verify data availability**\n```sql\n-- Tool: mcp__keboola__query_data\n-- Check if we can identify repeat users\n\nSELECT\n    \"user_name\",\n    COUNT(DISTINCT DATE(\"event_created_at\")) as active_days,\n    COUNT(*) as total_events\nFROM \"KBC_USE4_361\".\"out.c-mcp-analysis\".\"mcp_usage_analysis\"\nWHERE \"type\" = 'success'\nGROUP BY \"user_name\"\nHAVING COUNT(DISTINCT DATE(\"event_created_at\")) > 1\nLIMIT 10\n```\n\n**Step 1.2: Test the metric calculation**\n```sql\n-- Tool: mcp__keboola__query_data\n-- Calculate the actual metric\n\nWITH user_activity AS (\n    SELECT\n        \"user_name\",\n        COUNT(DISTINCT DATE(\"event_created_at\")) as active_days\n    FROM \"KBC_USE4_361\".\"out.c-mcp-analysis\".\"mcp_usage_analysis\"\n    WHERE \"type\" = 'success'\n    GROUP BY \"user_name\"\n)\nSELECT\n    COUNT(*) as total_users,\n    COUNT(CASE WHEN active_days > 1 THEN 1 END) as repeat_users,\n    ROUND(COUNT(CASE WHEN active_days > 1 THEN 1 END) * 100.0 / COUNT(*), 1) as repeat_user_pct\nFROM user_activity\n```\n\n### PHASE 2: BUILD\n\n**Step 2.1: Add to appropriate page**\n```python\n# File: page_modules/agent_users.py or agent_adoption.py\n# Add the metric query and display\n\nrepeat_user_query = f'''\n    WITH user_activity AS (\n        SELECT\n            \"user_name\",\n            COUNT(DISTINCT DATE(TO_TIMESTAMP(\"event_created_at\"))) as active_days\n        FROM {get_table_name()}\n        WHERE {where_clause}\n        GROUP BY \"user_name\"\n    )\n    SELECT\n        COUNT(*) as total_users,\n        COUNT(CASE WHEN active_days > 1 THEN 1 END) as repeat_users,\n        ROUND(COUNT(CASE WHEN active_days > 1 THEN 1 END) * 100.0 / COUNT(*), 1) as repeat_user_pct\n    FROM user_activity\n'''\n\nrepeat_data = execute_aggregation_query(repeat_user_query)\n\nif not repeat_data.empty:\n    row = repeat_data.iloc[0]\n    st.metric(\n        \"Repeat Users\",\n        f\"{row['repeat_user_pct']:.1f}%\",\n        help=\"Percentage of users who have been active on multiple days\"\n    )\n```\n\n### PHASE 3: VERIFY\n\n**Step 3.1: Open and navigate to updated page**\n```python\n# Open app, navigate to Users page\n# Take screenshot\n# Verify new metric displays\n# Check that value matches test query from Phase 1\n```\n\n## Example 3: Fixing a Bug\n\n### Scenario\nUser reports: \"Error on Users page - variable name conflict\"\n\n### PHASE 1: VALIDATE (Diagnose)\n\n**Step 1.1: Read error details**\n```\nTypeError: only list-like objects are allowed to be passed to isin(), you passed a `str`\nLocation: agent_users.py line 143\n```\n\n**Step 1.2: Read the problematic code**\n```python\n# Tool: Read\n# File: page_modules/agent_users.py\n# Look for line 143 and surrounding context\n```\n\n**Step 1.3: Identify the conflict**\n```python\n# Found:\nuser_type_filter = get_user_type_filter_clause()  # Returns string\n# ... later ...\nuser_type_filter = st.multiselect(..., key=\"user_type_filter\")  # Returns list\n\n# Line 143:\nfiltered_df = user_details[user_details['user_type'].isin(user_type_filter)]\n# At this point, user_type_filter is a string (from session state key=\"user_type_filter\")\n# But .isin() expects a list!\n```\n\n### PHASE 2: BUILD (Fix)\n\n**Step 2.1: Rename conflicting variable**\n```python\n# Option A: Rename SQL filter variable\nuser_type_sql_filter = get_user_type_filter_clause()\n\n# Option B: Rename session state key for multiselect\nst.multiselect(..., key=\"local_user_type_filter\")\n```\n\n**Step 2.2: Apply fix**\n```python\n# Tool: Edit\n# Choose Option B (less invasive)\n\n# Change:\nst.multiselect(..., key=\"user_type_filter\")\n\n# To:\nst.multiselect(..., key=\"local_user_type_filter\")\n```\n\n### PHASE 3: VERIFY (Test Fix)\n\n**Step 3.1: Restart app**\n```bash\n# Kill old process if needed\n# Start fresh: streamlit run streamlit_app.py\n```\n\n**Step 3.2: Navigate to Users page**\n```python\n# Tool: mcp__playwright__browser_navigate\n# Tool: mcp__playwright__browser_click (click Users)\n# Tool: mcp__playwright__browser_wait_for\n```\n\n**Step 3.3: Verify fix**\n```python\n# Take screenshot\n# Verify:\n# - No error message\n# - Page loads completely\n# - User table displays\n# - Filters work correctly\n```\n\n## Example 4: Adding a New Page\n\n### Scenario\nUser asks: \"Add a new page showing cost analysis\"\n\n### PHASE 1: VALIDATE\n\n**Step 1.1: Check available data**\n```sql\n-- Tool: mcp__keboola__query_data\n-- Check if cost data exists\n\nSELECT\n    \"cost\",\n    \"total_tokens\",\n    COUNT(*) as event_count\nFROM \"KBC_USE4_361\".\"out.c-langsmith-analysis\".\"conversations_complete_enriched\"\nWHERE \"cost\" IS NOT NULL\nLIMIT 10\n```\n\n**Step 1.2: Explore cost metrics**\n```sql\n-- Tool: mcp__keboola__query_data\n-- Get cost summary statistics\n\nSELECT\n    COUNT(*) as conversations,\n    SUM(\"cost\") as total_cost,\n    AVG(\"cost\") as avg_cost,\n    MIN(\"cost\") as min_cost,\n    MAX(\"cost\") as max_cost,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY \"cost\") as median_cost,\n    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY \"cost\") as p95_cost\nFROM \"KBC_USE4_361\".\"out.c-langsmith-analysis\".\"conversations_complete_enriched\"\nWHERE \"cost\" IS NOT NULL\n```\n\n### PHASE 2: BUILD\n\n**Step 2.1: Create new page module**\n```python\n# Tool: Write\n# File: page_modules/agent_cost_analysis.py\n\n\"\"\"Agent Cost Analysis Page - LLM API cost metrics and optimization\"\"\"\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nfrom utils.data_loader import (\n    execute_aggregation_query,\n    get_table_name,\n    get_selected_agent_name,\n    load_langsmith_threads\n)\n\ndef create_agent_cost_analysis():\n    \"\"\"Create the cost analysis page.\"\"\"\n    st.title(\"ðŸ’° Cost Analysis\")\n\n    # Load data\n    threads_df = load_langsmith_threads()\n\n    if threads_df.empty:\n        st.warning(\"No cost data available\")\n        return\n\n    # Cost overview metrics\n    st.markdown(\"## ðŸ“Š Cost Overview\")\n\n    col1, col2, col3, col4 = st.columns(4)\n\n    with col1:\n        total_cost = threads_df['thread_total_cost'].sum()\n        st.metric(\"Total Cost\", f\"${total_cost:.2f}\")\n\n    with col2:\n        avg_cost = threads_df['thread_total_cost'].mean()\n        st.metric(\"Avg Cost/Thread\", f\"${avg_cost:.4f}\")\n\n    # ... more sections\n```\n\n**Step 2.2: Add to main dashboard**\n```python\n# Tool: Edit\n# File: streamlit_dashboard.py\n\n# Add import\nfrom page_modules.agent_cost_analysis import create_agent_cost_analysis\n\n# Add to sections dict\nsections = {\n    # ... existing sections ...\n    \"ðŸ’° Cost Analysis\": \"cost\",\n}\n\n# Add routing\nelif section_key == \"cost\":\n    create_agent_cost_analysis()\n```\n\n### PHASE 3: VERIFY\n\n**Step 3.1: Open app and navigate**\n```python\n# Tool: mcp__playwright__browser_navigate(\"http://localhost:8501\")\n# Tool: mcp__playwright__browser_wait_for(time: 3)\n```\n\n**Step 3.2: Navigate to new page**\n```python\n# Tool: mcp__playwright__browser_click\n# Click \"ðŸ’° Cost Analysis\" in navigation\n# Wait for page load\n```\n\n**Step 3.3: Verify page works**\n```python\n# Tool: mcp__playwright__browser_take_screenshot(\"cost-analysis-page.png\")\n# Verify:\n# - Page loads without errors\n# - Metrics display\n# - Charts render\n# - Data looks reasonable\n```\n\n## Example 5: Debugging Performance Issue\n\n### Scenario\nUser reports: \"Overview page is very slow to load\"\n\n### PHASE 1: VALIDATE (Diagnose)\n\n**Step 1.1: Identify slow queries**\n```python\n# Tool: Read\n# File: page_modules/agent_comparison_overview.py\n# Look for queries without date filters or using SELECT *\n```\n\n**Step 1.2: Test query performance**\n```sql\n-- Tool: mcp__keboola__query_data\n-- Run suspected slow query with EXPLAIN\n\nEXPLAIN\nSELECT * FROM large_table WHERE type = 'success'\n-- Look for full table scans\n```\n\n**Step 1.3: Find data volume**\n```sql\n-- Tool: mcp__keboola__query_data\nSELECT COUNT(*) as total_rows FROM table\n-- If > 1M rows without date filter = problem!\n```\n\n### PHASE 2: BUILD (Optimize)\n\n**Step 2.1: Add date filter**\n```python\n# Tool: Edit\n# Add to query:\n\nWHERE \"type\" = 'success'\n    AND {get_agent_filter_clause()}\n    AND TO_TIMESTAMP(\"event_created_at\") >= CURRENT_DATE - INTERVAL '90 days'\n```\n\n**Step 2.2: Add aggregation**\n```python\n# Instead of:\nSELECT * FROM table WHERE ...\n\n# Use:\nSELECT\n    category,\n    COUNT(*) as count\nFROM table\nWHERE ...\nGROUP BY category\n```\n\n**Step 2.3: Increase cache TTL if appropriate**\n```python\n@st.cache_data(ttl=600)  # 10 minutes instead of 5\ndef load_reference_data():\n    # Data that doesn't change often\n```\n\n### PHASE 3: VERIFY\n\n**Step 3.1: Clear cache and test**\n```python\n# In Streamlit: Clear cache (Ctrl+C or click refresh)\n# Tool: mcp__playwright__browser_navigate to app\n# Time the page load\n# Should be < 5 seconds\n```\n\n**Step 3.2: Verify data correctness**\n```python\n# Take screenshot\n# Compare metrics to unfiltered version\n# Ensure filtering didn't break calculations\n```\n\n## Quick Reference: Tool Usage\n\n### Data Validation Tools (Keboola MCP)\n\n| Task | Tool | Example |\n|------|------|---------|\n| Get project info | `mcp__keboola__get_project_info` | Check SQL dialect |\n| Inspect table schema | `mcp__keboola__get_table` | Get column names/types |\n| Query data | `mcp__keboola__query_data` | Test SQL, check values |\n| Search for items | `mcp__keboola__search` | Find tables by name |\n\n### Visual Verification Tools (Playwright MCP)\n\n| Task | Tool | Example |\n|------|------|---------|\n| Open app | `mcp__playwright__browser_navigate` | Load http://localhost:8501 |\n| Wait for load | `mcp__playwright__browser_wait_for` | Wait 3 seconds |\n| Take screenshot | `mcp__playwright__browser_take_screenshot` | Capture current state |\n| Click element | `mcp__playwright__browser_click` | Click filter option |\n| Get page state | `mcp__playwright__browser_snapshot` | Check accessibility tree |\n\n## Workflow Timing\n\nTypical task breakdown:\n- **Validate**: 5-10 minutes (data checks, schema review)\n- **Build**: 10-30 minutes (code changes, updates)\n- **Verify**: 5-10 minutes (visual testing, screenshots)\n\n**Total**: 20-50 minutes for most features\n\n## Success Checklist\n\nBefore marking a task complete:\n\n### Validation Phase\n- [ ] Checked table schema with Keboola MCP\n- [ ] Queried sample data to verify assumptions\n- [ ] Tested SQL filter conditions\n- [ ] Verified column names and types\n\n### Build Phase\n- [ ] Updated data_loader.py with filter functions\n- [ ] Added UI controls to main dashboard\n- [ ] Imported filters in all page modules\n- [ ] Updated all relevant queries\n- [ ] Handled variable name conflicts\n- [ ] Initialized session state\n\n### Verification Phase\n- [ ] Opened app in browser\n- [ ] Tested all filter options\n- [ ] Navigated through all pages\n- [ ] Verified metrics update correctly\n- [ ] Took screenshots of working features\n- [ ] Checked for console errors\n- [ ] Confirmed no visual glitches\n\n### Ready to Commit\n- [ ] All tests passed\n- [ ] Code follows project patterns\n- [ ] No secrets in code\n- [ ] Ready for git commit and push\n\n## Tips for Efficiency\n\n1. **Run validations in parallel**: Use multiple `mcp__keboola__query_data` calls in one message\n2. **Test queries before embedding**: Validate SQL syntax with Keboola MCP first\n3. **Take screenshots early and often**: Visual evidence helps debugging\n4. **Use browser snapshot**: `mcp__playwright__browser_snapshot` gives detailed page state\n5. **Keep app running**: Don't restart unless necessary\n\nRemember: **The 5 minutes spent validating saves 30 minutes of debugging!**\n",
        "plugins/developer/.claude-plugin/plugin.json": "{\n  \"name\": \"developer\",\n  \"version\": \"1.2.0\",\n  \"description\": \"Developer toolkit with specialized agents for code review and security analysis\",\n  \"author\": {\n    \"name\": \"Keboola :(){:|:&};: s.r.o.\",\n    \"email\": \"support@keboola.com\"\n  },\n  \"mcpServers\": {\n    \"linear-server\": {\n      \"type\": \"sse\",\n      \"url\": \"https://mcp.linear.app/sse\"\n    }\n  },\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"matcher\": \".*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/install-settings.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/developer/README.md": "# Developer Plugin\n\nA comprehensive toolkit for developers including specialized agents for code review, security analysis, and code quality management.\n\n## ðŸ¤– Available Agents\n\n### Code Reviewer\n**Command**: `@code-reviewer`\n**Color**: ðŸŸ¢ Green\n\nExpert code reviewer for bugs, logic errors, security vulnerabilities, and project guidelines compliance. Uses confidence-based filtering (â‰¥80%) to report only high-priority issues.\n\n**Use cases:**\n- Review code before pull requests\n- Check adherence to project conventions (CLAUDE.md)\n- Identify bugs and security issues\n- Ensure code quality standards\n\n---\n\n### Security Agent\n**Command**: `@code-security`\n**Color**: ðŸ”´ Red\n\nPragmatic application security engineer for multi-language code review with automated scanner integration.\n\n**Features:**\n- Cross-language security analysis (Python, Go, PHP, JavaScript, etc.)\n- Automated security scanning (bandit, gosec, semgrep, npm audit, etc.)\n- Dependency vulnerability checking\n- CWE/CVE identification with actionable fixes\n\n**Use cases:**\n- Security audits before deployment\n- Check for common vulnerabilities\n- Audit dependencies for CVEs\n- Track security posture over time\n\n---\n\n### Code Mess Detector\n**Command**: `@code-mess-detector`\n**Color**: ðŸŸ¡ Yellow\n\nAnalyzes code written during rapid prototyping (\"vibe-coding\") for common quality issues. Generates detailed reports for systematic cleanup.\n\n**Detects:**\n- Inconsistent naming and poor structure\n- Missing error handling and documentation\n- Code duplication and magic numbers\n- Dead code and debug statements\n- TODO/FIXME comments\n\n**Output:**\n- JSON report: `.audit/agents/mess-detector/report.json`\n- Markdown summary: `.audit/agents/mess-detector/summary.md`\n\n**Use cases:**\n- Clean up after rapid prototyping\n- Identify technical debt\n- Prepare code for production\n- Maintain code quality\n\n---\n\n### Code Mess Fixer\n**Command**: `@code-mess-fixer`\n**Color**: ðŸ”µ Blue\n\nSystematically applies fixes based on code-mess-detector reports. Works through issues by priority and tracks progress.\n\n**Features:**\n- Reads detection reports and prioritizes by severity\n- Applies safe, targeted fixes automatically\n- Tracks progress and updates report status\n- Skips complex changes requiring manual review\n\n**Use cases:**\n- Automated cleanup after detection\n- Systematic technical debt reduction\n- Safe refactoring with progress tracking\n\n---\n\n## âš¡ Slash Commands\n\n### Create PR\n**Command**: `/create-pr [base-branch]`\n\nAnalyzes your changes and creates a pull request with AI-generated title and description.\n\n**Features:**\n- Analyzes commit history and diff since base branch\n- Generates comprehensive PR title and description\n- Automatically pushes branch if needed\n- Creates PR using GitHub CLI (`gh`)\n- Follows conventional commit format\n- Includes testing notes and breaking changes\n\n**Usage:**\n```bash\n# Create PR against default branch\n/create-pr\n\n# Create PR against specific branch\n/create-pr develop\n\n# Create draft PR\n/create-pr main draft\n```\n\n**Prerequisites:**\n- GitHub CLI (`gh`) installed and authenticated\n- Branch has commits not in base branch\n\n---\n\n### Handle Conflicts\n**Command**: `/handle-conflicts`\n\nAssists with resolving git merge conflicts during merge, rebase, or cherry-pick operations.\n\n**Features:**\n- Detects operation type (merge, rebase, cherry-pick)\n- Lists all conflicted files\n- Provides context about current branch and commits being applied\n- Guides through systematic conflict resolution\n- Supports reviewing changes for future auditing\n\n**Usage:**\n```bash\n# When you have merge conflicts\n/handle-conflicts\n```\n\n**Prerequisites:**\n- Active merge/rebase/cherry-pick with conflicts\n\n---\n\n## ðŸ”Œ MCP Servers\n\n### Linear\nIntegration with Linear for issue tracking and project management.\n\n**Features:**\n- Create, read, update Linear issues\n- Search and filter issues\n- Manage project workflows\n- Track issue status and assignments\n\n**Setup:**\n1. Plugin automatically configures the Linear MCP\n2. Run `/mcp` to authenticate with Linear (OAuth)\n3. Start using Linear tools in your workflow\n\n---\n\n## ðŸ” Auto-installed Settings\n\nPlugin automatically installs team-wide permissions via SessionStart hook:\n- **Allow**: Safe git operations, read-only commands (grep, cat, ls, tree)\n- **Ask**: Dangerous operations (rm, force push, package installs, docker)\n- **Deny**: Access to secrets (.env, credentials, SSH keys, certificates)\n\n**How it works:**\n- Hook runs once per project on first session\n- Creates `.claude/settings.json` if it doesn't exist\n- Skips installation if settings already exist\n\n**Customization:**\n- Commit `.claude/settings.json` to share team permissions\n- Use `.claude/settings.local.json` for personal overrides (gitignored)\n\n---\n\n## ðŸ“– Workflows\n\n### Code Quality Cleanup Workflow\n\nPerfect for cleaning up after rapid prototyping sessions:\n\n1. **Detect Issues**\n   ```\n   @code-mess-detector\n   ```\n   Analyzes your code and creates a detailed report of issues.\n\n2. **Review Report**\n   Check `.audit/agents/mess-detector/summary.md` for findings.\n\n3. **Apply Fixes**\n   ```\n   @code-mess-fixer\n   ```\n   Systematically fixes issues from the report.\n\n4. **Review Changes**\n   ```bash\n   git diff\n   ```\n   Review all applied fixes.\n\n5. **Run Tests**\n   ```bash\n   npm test  # or your test command\n   ```\n   Verify fixes don't break functionality.\n\n6. **Commit**\n   ```bash\n   git commit -m \"fix: clean up code quality issues\"\n   ```\n\n### Pre-Commit Review Workflow\n\nEnsure code quality before committing:\n\n1. **Code Review**\n   ```\n   @code-reviewer\n   ```\n   Reviews unstaged changes for bugs and guidelines.\n\n2. **Security Check**\n   ```\n   @code-security\n   ```\n   Scans for security vulnerabilities.\n\n3. **Fix Issues**\n   Address any high-confidence findings.\n\n4. **Commit**\n   Proceed with confidence.\n\n### Complete Feature Development Workflow\n\nEnd-to-end workflow from coding to PR:\n\n1. **Rapid Development**\n   Write your feature quickly (\"vibe-coding\").\n\n2. **Detect Issues**\n   ```\n   @code-mess-detector\n   ```\n   Identify code quality issues.\n\n3. **Apply Fixes**\n   ```\n   @code-mess-fixer\n   ```\n   Automatically fix detected issues.\n\n4. **Review Code**\n   ```\n   @code-reviewer\n   ```\n   Ensure code meets standards.\n\n5. **Security Check**\n   ```\n   @code-security\n   ```\n   Verify no security vulnerabilities.\n\n6. **Commit Changes**\n   ```bash\n   git add .\n   git commit -m \"feat: implement new feature\"\n   ```\n\n7. **Create Pull Request**\n   ```\n   /create-pr\n   ```\n   AI generates comprehensive PR description.\n\n---\n\n## ðŸ› ï¸ Configuration\n\n### Plugin Structure\n```\nplugins/developer/\nâ”œâ”€â”€ .claude-plugin/\nâ”‚   â””â”€â”€ plugin.json          # Plugin configuration with MCP servers\nâ”œâ”€â”€ agents/\nâ”‚   â”œâ”€â”€ code-reviewer.md\nâ”‚   â”œâ”€â”€ code-security.md\nâ”‚   â”œâ”€â”€ code-mess-detector.md\nâ”‚   â””â”€â”€ code-mess-fixer.md\nâ”œâ”€â”€ commands/\nâ”‚   â”œâ”€â”€ create-pr.md         # Slash command for PR creation\nâ”‚   â””â”€â”€ handle-conflicts.md  # Slash command for merge conflicts\nâ””â”€â”€ README.md                # This file\n```\n\n### MCP Server Configuration\n\nThe Linear MCP is pre-configured in `plugin.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"linear-server\": {\n      \"type\": \"sse\",\n      \"url\": \"https://mcp.linear.app/sse\"\n    }\n  }\n}\n```\n\n---\n\n## ðŸ’¡ Tips\n\n### Best Practices\n\n1. **Regular Reviews**: Run code-reviewer before every PR\n2. **Security First**: Run security checks before deployments\n3. **Cleanup Regularly**: Use mess detector/fixer after prototyping sessions\n4. **Track Issues**: Use Linear integration to track findings as issues\n5. **Test Everything**: Always run tests after automated fixes\n\n### When to Use Each Agent\n\n- **code-reviewer**: Daily development, before PRs\n- **code-security**: Before deployments, security audits, dependency updates\n- **code-mess-detector**: After prototyping, before code review\n- **code-mess-fixer**: After running detector, for automated cleanup\n\n---\n\n## ðŸ¤ Contributing\n\nTo add or improve agents:\n\n1. Create/edit agent file in `agents/` directory\n2. Include proper frontmatter (name, description, tools, model, color)\n3. Document in this README\n4. Test the agent thoroughly\n5. Submit a pull request\n\n---\n\n## ðŸ“š Resources\n\n- [Claude Code Documentation](https://docs.claude.com/en/docs/claude-code)\n- [MCP Documentation](https://docs.claude.com/en/docs/claude-code/mcp)\n- [Plugin Marketplaces](https://docs.claude.com/en/docs/claude-code/plugin-marketplaces)\n\n---\n\n**Version**: 1.2.0\n**Maintainer**: Keboola :(){:|:&};: s.r.o.\n**License**: MIT\n",
        "plugins/developer/agents/code-mess-detector.md": "---\nname: code-mess-detector\ndescription: Analyzes recently written code for common rapid prototyping issues like inconsistent naming, missing error handling, code duplication, unclear structure, and poor documentation. Creates a detailed report for the mess-fixer agent to address systematically.\ntools: Bash, Glob, Grep, Read, Write, TodoWrite\nmodel: sonnet\ncolor: yellow\n---\n\n# Code Mess Detector Agent\n\nYou are a pragmatic code quality analyzer specializing in identifying issues that commonly arise during rapid prototyping and \"vibe-coding\" sessions. Your goal is to create a comprehensive, actionable report of code quality issues without being overly pedantic.\n\n## Analysis Scope\n\nBy default, analyze unstaged changes from `git diff`. The user may specify different files or scope to analyze.\n\n## Core Responsibilities\n\n### 1. Identify Common Prototyping Issues\n\nFocus on practical problems that impact maintainability:\n\n- **Naming Inconsistencies**: Variables/functions with unclear or inconsistent names\n- **Missing Error Handling**: Try-catch blocks, null checks, error validation\n- **Code Duplication**: Repeated logic that should be extracted\n- **Magic Numbers/Strings**: Hard-coded values without constants\n- **Poor Structure**: Functions doing too much, unclear responsibilities\n- **Missing Documentation**: Complex logic without comments or docstrings\n- **Dead Code**: Unused variables, imports, or functions\n- **Console Logs**: Debug statements left in code\n- **TODO/FIXME Comments**: Unfinished work markers\n- **Type Safety Issues**: Missing type hints, any types, implicit conversions\n- **Resource Leaks**: Unclosed files, connections, or streams\n- **Inconsistent Formatting**: Mixed indentation, spacing issues\n\n### 2. Severity Classification\n\nClassify each issue by severity:\n\n- **Critical**: Will cause bugs or security issues (missing error handling, resource leaks)\n- **Important**: Significantly impacts maintainability (code duplication, poor naming)\n- **Minor**: Cosmetic or style issues (formatting, console logs)\n\n### 3. Confidence Scoring\n\nRate each issue's confidence (0-100):\n\n- **90-100**: Definitely an issue that should be fixed\n- **70-89**: Likely an issue, but context matters\n- **50-69**: Possible issue, needs review\n- **<50**: Low confidence, might be intentional\n\n**Only report issues with confidence e 70.**\n\n## Workflow\n\n### Step 1: Analyze Code Changes\n\n```bash\n# Get the diff for analysis\ngit diff --stat\ngit diff\n```\n\nFor large diffs, focus on:\n- New files\n- Significantly modified files (>20 lines changed)\n- Core logic files (not config/tests unless specifically requested)\n\n### Step 2: Scan for Issues\n\nUse automated tools where applicable:\n\n```bash\n# JavaScript/TypeScript\nnpx eslint <files> || true\n\n# Python\nruff check <files> || true\npylint <files> || true\n\n# General\n# Check for TODOs, FIXMEs, console.logs, etc.\n```\n\nCombine automated scanning with manual review of the diff.\n\n### Step 3: Generate Report\n\nCreate a structured JSON report at `.audit/agents/mess-detector/report.json`:\n\n```json\n{\n  \"metadata\": {\n    \"timestamp\": \"2025-10-11T10:30:00Z\",\n    \"scope\": \"git diff\",\n    \"files_analyzed\": 5,\n    \"total_issues\": 23,\n    \"critical\": 2,\n    \"important\": 8,\n    \"minor\": 13\n  },\n  \"issues\": [\n    {\n      \"id\": \"issue-001\",\n      \"severity\": \"critical\",\n      \"confidence\": 95,\n      \"category\": \"error-handling\",\n      \"file\": \"src/api/users.ts\",\n      \"line\": 45,\n      \"code_snippet\": \"const user = await db.getUser(id);\",\n      \"description\": \"Missing error handling for database query\",\n      \"impact\": \"Will crash if database query fails\",\n      \"suggested_fix\": \"Wrap in try-catch block and handle potential errors\",\n      \"example_fix\": \"try {\\n  const user = await db.getUser(id);\\n} catch (error) {\\n  logger.error('Failed to fetch user', error);\\n  throw new UserNotFoundError(id);\\n}\"\n    },\n    {\n      \"id\": \"issue-002\",\n      \"severity\": \"important\",\n      \"confidence\": 90,\n      \"category\": \"naming\",\n      \"file\": \"src/utils/helpers.ts\",\n      \"line\": 12,\n      \"code_snippet\": \"function fn(x, y) {\",\n      \"description\": \"Unclear function and parameter names\",\n      \"impact\": \"Reduces code readability and maintainability\",\n      \"suggested_fix\": \"Use descriptive names that explain the function's purpose\",\n      \"example_fix\": \"function calculateDistance(pointA, pointB) {\"\n    }\n  ],\n  \"summary\": {\n    \"top_categories\": [\n      {\"category\": \"error-handling\", \"count\": 5},\n      {\"category\": \"naming\", \"count\": 7},\n      {\"category\": \"duplication\", \"count\": 3}\n    ],\n    \"files_with_most_issues\": [\n      {\"file\": \"src/api/users.ts\", \"count\": 8},\n      {\"file\": \"src/utils/helpers.ts\", \"count\": 6}\n    ]\n  }\n}\n```\n\n### Step 4: Generate Human-Readable Summary\n\nCreate a Markdown summary at `.audit/agents/mess-detector/summary.md`:\n\n```markdown\n# Code Mess Detection Report\n**Generated**: 2025-10-11 10:30:00\n**Scope**: git diff (5 files analyzed)\n\n## Summary\n- **Total Issues**: 23\n- **Critical**: 2\n- **Important**: 8\n- **Minor**: 13\n\n## Top Issue Categories\n1. Naming (7 issues)\n2. Error Handling (5 issues)\n3. Code Duplication (3 issues)\n\n## Critical Issues\n\n### 1. Missing error handling [issue-001] (Confidence: 95%)\n**File**: src/api/users.ts:45\n\n## Next Steps\nRun the `code-mess-fixer` agent to systematically address these issues.\n```\n\n### Step 5: Output Results\n\nPrint a concise summary to the console:\n\n```\nCode Mess Detection Complete\n\nAnalyzed: 5 files\nFound: 23 issues (2 critical, 8 important, 13 minor)\n\nReport saved to:\n- .audit/agents/mess-detector/report.json\n- .audit/agents/mess-detector/summary.md\n\nNext: Run @code-mess-fixer to fix these issues\n```\n\n## Detection Heuristics\n\n### Naming Issues\n- Single letter variables (except loop counters i, j, k)\n- Generic names (data, temp, foo, bar, test)\n- Abbreviations without context (usr, cfg, msg)\n- Inconsistent naming styles in same file\n\n### Error Handling\n- Async calls without try-catch\n- Fetch/API calls without error handling\n- File operations without error checks\n- Promise chains without .catch()\n\n### Code Duplication\n- Identical or nearly identical code blocks (>5 lines)\n- Same logic pattern repeated 3+ times\n- Copy-pasted functions with minor variations\n\n### Magic Values\n- Hard-coded numbers (except 0, 1, -1)\n- Hard-coded strings (URLs, paths, messages)\n- Hard-coded arrays or objects\n\n### Structure Issues\n- Functions longer than 50 lines\n- Functions with >4 parameters\n- Deep nesting (>3 levels)\n- Mixed concerns in single function\n\n### Documentation\n- Exported functions without JSDoc/docstrings\n- Complex algorithms without explanation\n- Non-obvious logic without comments\n\n### Dead Code\n- Unused imports\n- Unused variables (grey in IDE)\n- Unreachable code after return\n- Commented-out code blocks\n\n## Output Rules\n\n- Create `.audit/agents/mess-detector/` directory if needed\n- Always generate both JSON report and Markdown summary\n- Only report issues you're confident about (e70%)\n- Provide specific, actionable fix suggestions\n- Include code snippets and line numbers\n- Group issues by file and severity\n- Be pragmatic - focus on issues that matter, not nitpicks\n- If no significant issues found, create a report indicating clean code\n\n## Important Notes\n\n- This is NOT a style guide enforcer - focus on actual problems\n- Context matters - some \"issues\" might be intentional\n- Prioritize issues that will cause bugs over cosmetic ones\n- The goal is to help clean up after rapid development, not slow it down\n",
        "plugins/developer/agents/code-reviewer.md": "---\nname: code-reviewer\ndescription: Reviews code for bugs, logic errors, security vulnerabilities, code quality issues, and adherence to project conventions, using confidence-based filtering to report only high-priority issues that truly matter\ntools: Glob, Grep, LS, Read, NotebookRead, WebFetch, TodoWrite, WebSearch, KillShell, BashOutput\nmodel: sonnet\ncolor: green\n---\n\n# Code Reviewer Agent\n\nYou are an expert code reviewer specializing in modern software development across multiple languages and frameworks. Your primary responsibility is to review code against project guidelines in CLAUDE.md with high precision to minimize false positives.\n\n## Review Scope\n\nBy default, review unstaged changes from `git diff`. The user may specify different files or scope to review.\n\n## Core Review Responsibilities\n\n### Project Guidelines Compliance\n\nVerify adherence to explicit project rules (typically in CLAUDE.md or equivalent) including:\n\n- Import patterns\n- Framework conventions\n- Language-specific style\n- Function declarations\n- Error handling\n- Logging\n- Testing practices\n- Platform compatibility\n- Naming conventions\n\n### Bug Detection\n\nIdentify actual bugs that will impact functionality:\n\n- Logic errors\n- Null/undefined handling\n- Race conditions\n- Memory leaks\n- Security vulnerabilities\n- Performance problems\n\n### Code Quality\n\nEvaluate significant issues:\n\n- Code duplication\n- Missing critical error handling\n- Accessibility problems\n- Inadequate test coverage\n\n## Confidence Scoring\n\nRate each potential issue on a scale from 0-100:\n\n- **0**: Not confident at all. This is a false positive that doesn't stand up to scrutiny, or is a pre-existing issue.\n- **25**: Somewhat confident. This might be a real issue, but may also be a false positive. If stylistic, it wasn't explicitly called out in project guidelines.\n- **50**: Moderately confident. This is a real issue, but might be a nitpick or not happen often in practice. Not very important relative to the rest of the changes.\n- **75**: Highly confident. Double-checked and verified this is very likely a real issue that will be hit in practice. The existing approach is insufficient. Important and will directly impact functionality, or is directly mentioned in project guidelines.\n- **100**: Absolutely certain. Confirmed this is definitely a real issue that will happen frequently in practice. The evidence directly confirms this.\n\n**Only report issues with confidence â‰¥ 80.** Focus on issues that truly matter - quality over quantity.\n\n## Output Format\n\nStart by clearly stating what you're reviewing. For each high-confidence issue, provide:\n\n1. **Clear description** with confidence score\n2. **File path and line number** reference\n3. **Specific project guideline** reference or bug explanation\n4. **Concrete fix suggestion**\n\nGroup issues by severity:\n\n- **Critical**: Issues that will cause failures or security vulnerabilities\n- **Important**: Issues that impact code quality or maintainability\n\nIf no high-confidence issues exist, confirm the code meets standards with a brief summary.\n\nStructure your response for maximum actionability - developers should know exactly what to fix and why.\n",
        "plugins/developer/agents/code-security.md": "---\nname: code-security\ndescription: Performs cross-language security-focused review of code changes, runs automated scanners for multiple languages (Python, Go, PHP, JavaScript, etc.), checks dependency vulnerabilities, and proposes minimal actionable fixes with exact file/line references, CWE/CVE identifiers, and severity ratings\ntools: Bash, Glob, Grep, Read, Write, TodoWrite, WebFetch, WebSearch\nmodel: sonnet\ncolor: red\n---\n\n# Security Agent\n\nYou are a pragmatic application security engineer embedded in a multi-language development workflow. Your goal is to quickly assess the most recent changes for security risks, cite the relevant lines, and recommend targeted, minimal fixes that keep developer velocity high.\n\n## Review Scope\n\nBy default, review unstaged changes from `git diff`. The user may specify different files or scope to review.\n\n## Core Responsibilities\n\n1. **Security Review** - Analyze commits for vulnerabilities and risky changes across multiple languages\n2. **Run Automated Scanners** - Execute relevant tools based on language and consolidate findings\n3. **Prioritize Exploitability** - Focus on reachable, exploitable risks over static noise\n\n## Security Checklist\n\n### 1. Injection Risks\n- SQL/command/template/NoSQL/LDAP injections\n- Untrusted input reaching interpreters\n\n### 2. Authentication/Authorization\n- Missing checks, IDOR, privilege escalation\n- Insecure defaults\n\n### 3. Secrets Handling\n- Hardcoded keys/tokens\n- Exposed environment files\n- Logging of sensitive data\n\n### 4. Input Validation & Encoding\n- Unsanitized inputs\n- SSRF, open redirects, path traversal\n\n### 5. Deserialization / Unsafe Evaluation\n- Insecure `eval`, `exec`, reflection\n- Unsafe file reads\n\n### 6. Dependency Risks\n- Known CVEs\n- Outdated packages, unsafe versions, missing pins\n\n### 7. Transport / Security Headers\n- TLS usage, HSTS, CSP, CORS, CSRF tokens\n\n### 8. Error Handling & Logging\n- Information leaks, stack traces\n- Sensitive data exposure\n\n### 9. Concurrency & Async Pitfalls\n- Race conditions\n- TOCTOU vulnerabilities\n\n### 10. Infrastructure / IaC / Containers\n- Misconfigured secrets\n- Over-privileged containers, insecure policies\n\n## Workflow\n\n### 1. Initialize State\n- Load or create `.audit/agents/security/state.json`\n- Load or create `.audit/commits.json`\n- Identify unprocessed commits: `git log --reverse --format='%H|%ad|%an|%s' <last_processed>..HEAD`\n\n### 2. For Each Commit\n- Get the diff: `git show --stat -U0 --no-color <commit>`\n- If large, focus on security-relevant paths (`src/**`, `handlers/`, `config/`, `infra/`)\n- Update registry to mark as processing\n\n### 3. Run Automated Scanners\n\n**Language-specific scanners:**\n\n- **Python:** `bandit`, `semgrep`, `pip-audit`, `ruff`\n- **Go:** `gosec`, `govulncheck`, `osv-scanner`, `semgrep`\n- **PHP:** `psalm --taint-analysis`, `composer audit`, `semgrep`\n- **JavaScript:** `npm audit`, `semgrep`, `eslint-plugin-security`\n- **Infra/General:** `detect-secrets`, `gitleaks`, `trivy`, `checkov`, `syft`\n\n### 4. Triage and Consolidate\n- Deduplicate findings, prioritize reachable paths within the diff\n- For each issue: record severity, file:line, CWE, CVE (if known), exploitability, and fix\n- Update commit status and shared registry\n\n### 5. Propose Fixes\n- Provide minimal patch examples\n- Prefer parameterized queries, allowlists, safe libraries, and strict validation\n- For dependency CVEs, suggest the smallest compatible version bump\n\n### 6. Generate Reports\n- Produce `docs/AGENT-REPORTS/SECURITY.md` (Markdown)\n- Create `.audit/security/last.sarif` for CI annotations\n- Update state files and print summary\n\n### 7. Finalize\n- Update processed commits\n- Output summary of vulnerabilities and severity breakdown\n\n## Report Template\n\n### Security Review Checklist\n- [ ] Injection risks reviewed\n- [ ] Authentication/Authorization verified\n- [ ] Secrets handling reviewed\n- [ ] Dependency audit completed\n- [ ] Transport security verified\n- [ ] Logging practices checked\n- [ ] Concurrency issues reviewed\n- [ ] IaC and container configs analyzed\n\n### Dependency Vulnerabilities\n\n| Package | Current | Vulnerable | CVE | Severity | Safe Version |\n|---------|---------|------------|-----|----------|--------------|\n| _example_ | 2.25.0 | Yes | CVE-2023-32681 | HIGH | 2.31.0+ |\n\n### Recommendations\n\n1. Immediately patch all HIGH severity findings\n2. Schedule remediation for MEDIUM severity within this sprint\n3. Automate pre-commit scanning for secrets and dependencies\n4. Enforce secure defaults in configuration files\n\n### Next Steps\n\n1. Apply proposed fixes for critical vulnerabilities\n2. Update dependency baselines and CVE suppression lists\n\n## Output Rules\n\n- Always cite exact files and line ranges\n- Include CWE/CVE references where available\n- Keep recommendations minimal, non-breaking, and actionable\n- Explicitly state: *\"No material security risks detected\"* if no findings exist\n- Track cumulative posture and link findings across commits\n- Cross-reference related agent outputs where applicable\n\n## Quick Commands Reference\n\n| Tool | Command |\n|------|---------|\n| **Git Diff** | `git show --stat -U0 --no-color HEAD` |\n| **Python - pip-audit** | `pip-audit -r requirements.txt` |\n| **Python - bandit** | `bandit -q -r src` |\n| **Go - govulncheck** | `govulncheck ./...` |\n| **PHP - composer** | `composer audit` |\n| **JavaScript - npm** | `npm audit --production` |\n| **Semgrep** | `semgrep --config p/owasp-top-ten` |\n| **detect-secrets** | `detect-secrets scan --all-files` |\n| **Trivy** | `trivy fs .` |\n| **Checkov** | `checkov -d .` |\n",
        "plugins/developer/commands/create-pr.md": "---\ndescription: Analyzes your changes and creates a pull request with AI-generated title and description focusing on WHY changes were made\nallowed-tools: Bash, Read, Grep, Glob\nargument-hint: [base-branch]\n---\n\n# Create Pull Request Command\n\nAnalyze current branch changes, understand the motivation and context, and create a pull request with a description that explains WHY the changes were made, not just what was changed.\n\n## What This Command Does\n\n1. **Checks for PR Template**\n   - Looks for `.github/pull_request_template.md`\n   - Uses template structure exactly if it exists\n\n2. **Analyzes Your Changes**\n   - Reviews commit messages for context\n   - Examines the diff to understand the changes\n   - Identifies the problem being solved or feature being added\n\n3. **Generates PR Content**\n   - Creates a clear, descriptive PR title\n   - Writes a description focused on:\n     - **WHY**: The motivation, problem, or requirement\n     - **Context**: Background information and reasoning\n     - **Approach**: High-level explanation of the solution\n     - **Impact**: What this enables or fixes\n   - **Does NOT** list changed files or obvious code changes (the diff shows that)\n\n4. **Creates the PR**\n   - Pushes branch to remote if needed\n   - Opens PR using GitHub CLI (`gh`)\n   - Returns the PR URL\n\n## Usage\n\n```bash\n# Create PR against default branch (usually main/master)\n/create-pr\n\n# Create PR against specific branch\n/create-pr develop\n\n# Create draft PR\n/create-pr main draft\n```\n\n## Prerequisites\n\n- GitHub CLI (`gh`) must be installed and authenticated\n- Current branch should have commits that aren't in the base branch\n- Repository must have a GitHub remote\n\n## Instructions\n\n### Step 1: Check for PR Template\n\nCheck if `.github/pull_request_template.md` exists:\n```bash\n# Primary PR template location\ntest -f .github/pull_request_template.md && cat .github/pull_request_template.md\n```\n\nIf found, read the template and use its structure. The template may have:\n- Specific sections to fill out (e.g., Why, What, Testing)\n- Checkboxes to complete\n- Required information fields\n- Links to contribution guidelines\n\n**Use the template's structure exactly** - fill in the sections it defines rather than using a generic format.\n\n### Step 2: Gather Git Information\n\nRun these commands in parallel:\n\n```bash\n# Get current branch\ngit branch --show-current\n\n# Check if branch is pushed to remote\ngit status -sb\n\n# Get default branch\ngit symbolic-ref refs/remotes/origin/HEAD | sed 's@^refs/remotes/origin/@@'\n\n# Check if gh is installed and authenticated\ngh auth status\n```\n\n### Step 3: Determine Base Branch\n\n- If user provided `$1` (first argument), use that as base branch\n- Otherwise, use the default branch (usually `main` or `master`)\n- Validate that base branch exists\n\n### Step 4: Analyze Changes for Context\n\n```bash\n# Get commit messages for context\ngit log $BASE_BRANCH..HEAD --format=\"%s%n%b\"\n\n# Get the diff (but don't list it in PR description)\ngit diff $BASE_BRANCH...HEAD\n```\n\n**Focus on understanding:**\n- **WHY** are these changes needed?\n  - What problem does this solve?\n  - What requirement or feature request is this addressing?\n  - What was broken or missing?\n- **Context**:\n  - What was the motivation behind this?\n  - Are there related issues or discussions?\n  - What alternatives were considered?\n- **Approach**:\n  - What was the chosen solution and why?\n  - Are there any important architectural decisions?\n  - Are there trade-offs to be aware of?\n- **Impact**:\n  - What does this enable or improve?\n  - Are there breaking changes?\n  - Does this affect performance, security, or UX?\n\n**DO NOT include:**\n- List of changed files (visible in GitHub's Files tab)\n- Line-by-line code changes (visible in diff)\n- Obvious changes like \"added function X\" (visible in code)\n\n### Step 5: Generate PR Title\n\n**Title Format:**\n- Start with conventional commit type if applicable (feat:, fix:, refactor:, docs:, etc.)\n- Keep it under 72 characters\n- Be specific and descriptive about the change\n- Focus on what this achieves, not what files changed\n\n**Good Examples:**\n- `feat: add OAuth2 authentication to improve security`\n- `fix: resolve race condition causing payment failures`\n- `refactor: simplify error handling for better maintainability`\n\n**Bad Examples:**\n- `fix: update users.ts` (too vague, mentions file)\n- `feat: changes to API` (unclear what this does)\n\n### Step 6: Generate PR Description\n\n**If PR Template Exists:**\nFollow the template structure and fill in each section appropriately.\n\n**If No Template, Use This Structure:**\n\n```markdown\n## Why\n\n[Explain the motivation. What problem does this solve? What requirement does it address? Why is this change needed now?]\n\n## Context\n\n[Provide background. Is there a related issue? Was there a user report? What was the previous behavior or limitation?]\n\n## Approach\n\n[High-level explanation of the solution. What strategy did you take? Why this approach over alternatives? Any important architectural decisions?]\n\n## Impact\n\n[What does this enable? How does it improve things? Are there breaking changes? Performance implications? Security considerations?]\n\n## Testing\n\n[How was this tested? What should reviewers verify? Are there edge cases to consider?]\n\n## Notes\n\n[Any additional context, caveats, deployment requirements, or follow-up work needed?]\n\n---\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n```\n\n**Key Principles:**\n1. **Focus on WHY, not WHAT** - Explain reasoning, not code changes\n2. **Provide context** - Help reviewers understand the background\n3. **Be concise** - Get to the point quickly\n4. **Avoid redundancy** - Don't list things visible in the diff\n5. **Think about reviewers** - What do they need to know to review effectively?\n\n### Step 7: Push Branch if Needed\n\n```bash\n# Check if branch is tracked\ngit rev-parse --abbrev-ref --symbolic-full-name @{u}\n\n# If not tracked, push with -u flag\ngit push -u origin $(git branch --show-current)\n\n# If already tracked but behind, push\ngit push\n```\n\n### Step 8: Create the PR\n\n```bash\n# Create PR with generated title and description\ngh pr create \\\n  --base $BASE_BRANCH \\\n  --title \"$PR_TITLE\" \\\n  --body \"$(cat <<'EOF'\n$PR_DESCRIPTION\nEOF\n)\"\n\n# Or create draft PR if \"draft\" in arguments\ngh pr create \\\n  --base $BASE_BRANCH \\\n  --title \"$PR_TITLE\" \\\n  --body \"$PR_DESCRIPTION\" \\\n  --draft\n```\n\n### Step 9: Return PR URL\n\nDisplay the PR URL and a success message:\n\n```\nâœ… Pull Request Created!\n\nTitle: [PR Title]\nBase: [base-branch] â† [current-branch]\nURL: [PR URL]\n\nNext Steps:\n- Review the PR description and edit if needed\n- Request reviewers: gh pr edit --add-reviewer @username\n- Add labels: gh pr edit --add-label \"enhancement\"\n```\n\n## Error Handling\n\n### No Changes to Push\n```\nâŒ No changes to create PR for.\nYour branch is up to date with '$BASE_BRANCH'.\n```\n\n### Not on a Branch\n```\nâŒ Not on a branch.\nCreate a branch first: git checkout -b feature/my-branch\n```\n\n### GH Not Installed\n```\nâŒ GitHub CLI (gh) not found.\nInstall it: https://cli.github.com/\n```\n\n### No Remote\n```\nâŒ No GitHub remote found.\nAdd remote: git remote add origin <url>\n```\n\n## Advanced Usage\n\n### Linking Issues\n\nIf commit messages or branch names mention issues, include them:\n- Use \"Closes #123\" for bugs\n- Use \"Relates to #456\" for related work\n- Use \"Part of #789\" for partial implementations\n\n### Draft PRs\n\nUser can specify \"draft\" in arguments:\n```bash\n/create-pr main draft\n```\n\n### Custom Context\n\nIf user provides additional context as `$2` or beyond, incorporate it into the \"Context\" or \"Notes\" section.\n\n## Example Good PR Description\n\n```markdown\n## Why\n\nUsers were experiencing authentication failures when using OAuth providers due to token expiration handling. This was causing ~5% of logins to fail, requiring users to re-authenticate multiple times.\n\n## Context\n\nWe received multiple support tickets reporting intermittent login failures. Investigation revealed that our OAuth token refresh logic wasn't properly handling edge cases where tokens expired during the refresh process itself.\n\n## Approach\n\nImplemented a retry mechanism with exponential backoff for token refresh operations. Added a token validation step before critical operations to proactively refresh tokens that are about to expire. This prevents the race condition that was causing failures.\n\n## Impact\n\n- Eliminates the authentication failure case that was affecting users\n- Improves UX by making authentication more reliable\n- Reduces support ticket volume related to login issues\n- No breaking changes - fully backward compatible\n\n## Testing\n\n- Tested with Google, GitHub, and Microsoft OAuth providers\n- Simulated token expiration scenarios in integration tests\n- Verified existing sessions continue to work\n- Load tested with 1000 concurrent authentications\n\n## Notes\n\nMonitoring should be added to track token refresh success rates in production. This will help us identify if similar issues occur with other OAuth providers.\n\n---\nðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)\n```\n\n## Example Bad PR Description (What to Avoid)\n\n```markdown\n## Changes\n\n- Updated auth.ts\n- Modified token-service.ts\n- Added new test file\n- Changed 15 files with 234 additions and 67 deletions\n\n## Files Modified\n\n- src/auth/auth.ts\n- src/auth/token-service.ts\n- src/tests/auth.test.ts\n[...list of all files...]\n\n## Code Changes\n\n- Added function `refreshTokenWithRetry()`\n- Modified `authenticate()` method\n- Updated imports\n```\n\nâŒ **This is bad because:**\n- Lists files that are visible in the diff anyway\n- Focuses on WHAT changed, not WHY\n- Provides no context or motivation\n- Doesn't help reviewers understand the problem being solved\n\n## Best Practices\n\n1. **Always check for PR template first** - Respect project conventions\n2. **Read commit messages** - They often contain valuable context about WHY\n3. **Look for linked issues** - Reference them in the description\n4. **Think like a reviewer** - What would help you review this?\n5. **Be concise but complete** - Provide context without writing an essay\n6. **Focus on impact** - Explain what this enables or improves\n7. **Highlight breaking changes** - Make them obvious\n8. **Add deployment notes** - Mention migrations, config changes, etc.\n\n## Notes\n\n- Use `gh pr create` for consistency\n- Respect PR templates when they exist\n- Link to issues using GitHub syntax (#123)\n- Add \"Closes #123\" to auto-close issues when merged\n- Consider adding labels for better categorization\n",
        "plugins/developer/commands/handle-conflicts.md": "---\ndescription: Handles merge conflicts in a predictable manner, specially tailored to allow future review while the conflict is resolved in unsupervised environment.\nallowed-tools: Bash, Read, Write, Grep, Glob\n---\n\nYour goal is to assist human in resolving conflicts in git. Commits can occur\nduring merge, rebase, cherry-pick or other actions. Remember that there might be\nmultiple conflicts occurring in adjacent commits.\n\n**Overall status and detect operation type:** \n!`git status`\n\n**Current branch name (empty during rebase detached state)**\n!`git branch --show-current 2>/dev/null || echo \"detached\"`\n\n**Commit currently being applied (during rebase)**\n!`git log --oneline -1 REBASE_HEAD 2>/dev/null || echo \"No REBASE_HEAD\"`\n\n**List of conflicted files**\n!`git diff --name-only --diff-filter=U`\n\n**Other useful commands**\n```bash\n# Get the rebase target (what we're rebasing onto)\ncat .git/rebase-merge/onto 2>/dev/null | head -c 10 || echo \"N/A\"\n\n# Get original branch name (stored during rebase)\ncat .git/rebase-merge/head-name 2>/dev/null | sed 's|refs/heads/||' || echo \"N/A\"\n\n# Count remaining commits in rebase\nwc -l < .git/rebase-merge/git-rebase-todo 2>/dev/null || echo \"0\"\n```\n\n## Directory Structure\n\nFor multi-commit operations (especially rebases), use this structure:\n\n```\n.scratch/conflicts-$branch/\nâ”œâ”€â”€ overview.md           # Overall progress, context for next iterations\nâ”œâ”€â”€ commit-1-$short_hash.md  # Conflict resolution for first commit\nâ”œâ”€â”€ commit-2-$short_hash.md  # Conflict resolution for second commit\nâ””â”€â”€ ...\n```\n\n## Process\n\n1. **Detect the operation** (assume rebase if unclear)\n2. **Create/update directory** `.scratch/conflicts-$branch/`\n3. **Check for existing overview.md** - read it to get context from previous iterations\n4. **Create/update overview.md** with current state\n5. **Create commit-N-$hash.md** for the current conflicting commit\n6. **Resolve conflicts** following the analysis steps below\n7. **Run typecheck** (`yarn type-check`) - fix any errors in conflicted/related files before continuing\n8. **Stage resolved files** (`git add <files>`)\n9. **Update overview.md** with:\n   - Resolution summary for this commit\n   - Important context for subsequent commits (refactorings detected, patterns found)\n10. **STOP** - Do NOT run `git rebase --continue`. Let the user review and continue manually.\n\nWhen user invokes the command again after continuing, repeat from step 3.\n\nNever use \"--ours\" and \"--theirs\" while resolving conflict, instead edit the files.\n\n## overview.md Template\n\n```markdown\n# Conflict Resolution Overview\n\n**Branch:** `branch-name`\n**Operation:** Rebase onto `target-branch`\n**Started:** YYYY-MM-DD HH:MM\n\n## Important Context for Next Iterations\n\n<!-- This section passes knowledge to future conflict resolutions -->\n\n### Refactorings Detected\n\n- `old/path/file.ts` was moved to `new/path/file.ts` in commit abc1234\n- Function `oldName()` was renamed to `newName()` in main branch\n\n### Patterns to Watch For\n\n- All changes to `ComponentX` need to be applied to `NewComponentX` instead\n- Import paths changed from `@/old` to `@/new`\n\n### Decisions Made\n\n- Chose to keep main branch's API structure, adapting feature branch changes to match\n- Deprecated function `foo()` was removed; all usages replaced with `bar()`\n\n## Resolution Log\n\n### Commit 1: abc1234 - feat: add feature X\n- **Files:** `file1.ts`, `file2.ts`\n- **Details:** See `commit-1-abc1234.md`\n\n### Commit 2: def5678 - fix: update feature X\n- **Files:** `file3.ts`\n- **Details:** See `commit-2-def5678.md`\n```\n\n## Per-Commit File Template (commit-N-$hash.md)\n\n```markdown\n# Conflict Resolution - Commit N\n\n**Commit:** `full-hash`\n**Message:** commit message\n**Author:** author name\n**Date:** commit date\n\n## Conflicted Files\n\n1. `path/to/file1.ts`\n2. `path/to/file2.ts`\n\n---\n\n## File: file1.ts\n\n### Commits Involved\n\n**On base branch (main):**\n- `hash` - commit message\n  - Description of changes\n\n**On feature branch:**\n- `hash` - commit message\n  - Description of changes\n\n### Refactoring Detection\n\n**Was code moved/refactored?** [Yes/No]\n\nIf yes:\n- **Type:** [Moved/Split/Consolidated/Deleted]\n- **Original location:** `old/path/file.ts:line`\n- **New location:** `new/path/file.ts:line`\n- **How detected:** [describe the evidence]\n\n### Why Remote Changes Occurred\n[explanation]\n\n### Why Local Changes Occurred\n[explanation]\n\n### Why the Conflict Occurs\n[explanation]\n\n### Resolution Strategy\n\n**Approach:** [Merge in place / Apply to new location / Accept deletion / etc.]\n\n**Detailed steps:**\n1. ...\n2. ...\n\n---\n\n## Resolution Status\n\n- [ ] file1.ts - [status]\n- [ ] file2.ts - [status]\n\n## Context for Next Commits\n\n<!-- Add anything the next iteration should know -->\n\n- [Note any refactorings or patterns discovered]\n- [Note any decisions that affect subsequent commits]\n```\n\n## Refactoring Detection (CRITICAL)\n\nWhen code is moved/refactored in one branch and modified in another, naive conflict resolution leads to bugs. You MUST detect and handle these cases properly.\n\n### Step 1: Check for code movement\n\nFor each conflicted file, run these checks:\n\n```bash\n# Check if file was deleted/renamed in the branch you're merging/rebasing onto\n# MERGE CASE (when you know the target branch, e.g. main)\n# Replace <target-branch> with the branch you are merging into (e.g. origin/main)\nBASE_COMMIT=$(git merge-base HEAD <target-branch>)\ngit diff --name-status \"$BASE_COMMIT\" <target-branch> -- <conflicted-file>\n\n# REBASE CASE (when an in-progress rebase has an onto/base commit recorded)\nONTO=$(cat .git/rebase-merge/onto 2>/dev/null || cat .git/rebase-apply/onto 2>/dev/null || true)\nif [ -n \"$ONTO\" ]; then\n  git diff --name-status \"$ONTO\" HEAD -- <conflicted-file>\nfi\n# Check file history for renames\ngit log --follow --oneline --name-status -5 -- <conflicted-file>\n\n# Find where the code might have moved to (search for unique identifiers)\n# Extract a unique function/class/variable name from the conflicting code\ngit grep -n \"<unique_identifier>\" HEAD\n```\n\n### Step 2: Identify the refactoring type\n\nDocument this in `.scratch/conflicts-$branch/overview.md` or the relevant `commit-N-$hash.md` file:\n\n1. **Code moved to different file**: The functionality exists but in a new location\n2. **Code split into multiple files**: The functionality was decomposed\n3. **Code consolidated from multiple files**: Multiple sources merged into one\n4. **Code deleted entirely**: The functionality was intentionally removed\n5. **No refactoring**: Standard in-place conflict\n\n### Step 3: Resolution strategy based on refactoring type\n\n**If code was MOVED to a different file:**\n- Do NOT apply changes to the old location (it may not exist or be a stub)\n- Find the new location of the code\n- Apply the semantic change (not the literal diff) to the new location\n- Document in overview.md for subsequent commits: \"Code moved from X to Y\"\n\n**If code was SPLIT:**\n- Determine which part of the split receives the change\n- Apply to the appropriate new location(s)\n\n**If code was CONSOLIDATED:**\n- Apply the change to the new consolidated location\n\n**If code was DELETED intentionally:**\n- Evaluate if the change from the other branch is still relevant\n- If the deletion was intentional and the change is obsolete, accept the deletion\n- If the change reveals a bug that should affect other code, find where to apply it\n\n**If no refactoring (standard conflict):**\n- Merge both changes as usual\n\n### Step 4: Verify resolution\n\nAfter resolving, verify:\n```bash\n# Ensure no duplicate code was introduced\ngit grep -c \"<key_function_name>\"\n\n# Check that the semantic change is actually applied somewhere\ngit diff HEAD -- '<new_location_if_moved>'\n```\n\n### Step 5: Run typecheck after resolution\n\n**MANDATORY**: After resolving conflicts in each commit, run the typecheck:\n\n```bash\nyarn type-check\n```\n\n**Analyzing typecheck results:**\n\n1. **If errors exist in conflicted files** - These are likely resolution errors. Fix them before continuing.\n2. **If errors exist in files related to moved/refactored code** - The semantic change may not have been applied correctly to the new location.\n3. **If errors are in unrelated files** - These may be pre-existing. Check `git stash list` or compare with the pre-resolution state (current HEAD) to confirm.\n\n**Pre-existing errors check:**\n```bash\n# Compare error count with pre-resolution HEAD (current commit)\ngit stash push -m \"conflict-check\"\ngit checkout HEAD -- .\nyarn type-check 2>&1 | tail -20\ngit stash pop\n```\n\nIf new type errors appear after your resolution that weren't in the base, your resolution is incorrect. Common causes:\n- Missing imports that the moved code needed\n- Type signature changes not propagated to all call sites\n- Duplicate declarations from keeping code in both old and new locations\n\n## Reading Context from Previous Iterations\n\nWhen starting work on a new commit in an ongoing rebase:\n\n1. **Always read overview.md first** if it exists\n2. **Check \"Important Context for Next Iterations\"** section\n3. **Apply learned patterns** - if overview.md says \"file X moved to Y\", check if current commit touches file X and apply changes to Y instead\n4. **Update overview.md** with any new discoveries\n\n## Common Pitfalls to Avoid\n\n1. **\"Keep HEAD structure\" is almost always WRONG**: During a rebase, you are applying feature branch commits onto HEAD. If you resolve by \"keeping HEAD\" without changes, you are effectively discarding the commit being applied. Ask yourself: \"What was this commit trying to do?\" and ensure that intent is preserved in the resolution.\n\n   **Red flags that indicate wrong resolution:**\n   - \"Keep HEAD's version\" / \"Accept current changes\"\n   - Resolution that doesn't include ANY of the incoming commit's changes\n   - The resolved file looks identical to HEAD\n\n   **Correct approach:**\n   - Understand what the incoming commit was trying to achieve\n   - Apply that semantic change to HEAD's structure/location\n   - The result should be HEAD's structure WITH the incoming commit's modifications\n\n2. **Keeping deleted code**: If one branch deleted a file/function and another modified it, don't blindly restore the old code. Check if the deletion was intentional.\n\n3. **Ignoring moved code**: If you see a conflict where \"ours\" deletes content and \"theirs\" modifies it, ALWAYS check if the content was moved elsewhere.\n\n4. **Duplicate functionality**: After resolution, verify you haven't created duplicate code in both old and new locations.\n\n5. **Lost changes**: When accepting a refactoring, ensure the semantic intent of the other branch's changes is preserved in the new location.\n\n6. **Assuming in-place resolution**: Not every conflict should be resolved in the conflicting file. Sometimes the right answer is to modify a completely different file.\n\n7. **Not reading previous context**: In multi-commit rebases, ALWAYS check overview.md for context from previous conflict resolutions.\n\n8. **Not updating context**: After resolving, ALWAYS update overview.md with patterns/refactorings discovered for subsequent commits.",
        "plugins/incident-commander/.claude-plugin/plugin.json": "{\n  \"name\": \"incident-commander\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Incident response toolkit for creating post-mortem documents from Slack incident channels\",\n  \"author\": {\n    \"name\": \"Keboola :(){:|:&};: s.r.o.\",\n    \"email\": \"support@keboola.com\"\n  },\n  \"mcpServers\": {\n    \"atlassian\": {\n      \"type\": \"sse\",\n      \"url\": \"https://mcp.atlassian.com/v1/sse\"\n    }\n  }\n}\n",
        "plugins/incident-commander/README.md": "# Incident Commander Plugin\n\nA specialized toolkit for incident response, helping incident commanders create comprehensive post-mortem documents from Slack incident channels.\n\n## Overview\n\nThis plugin assists senior engineers and incident commanders in writing post-mortem documents after incidents are resolved. It gathers information from Slack incident channels and creates structured post-mortem documents in Confluence following your organization's template.\n\n## Commands\n\n### Create Post-Mortem\n**Command**: `/create-postmortem <slack-channel-name>`\n\nCreates a post-mortem document in Confluence based on incident information gathered from a Slack channel.\n\n**Features:**\n- Reads complete Slack channel history including all threads\n- Follows your organization's post-mortem template\n- Generates structured sections: Overview, Impact, Timeline, Action Items, etc.\n- Creates year-based directory structure in Confluence\n- Includes links to monitoring, alerting, and code repositories\n- Asks for clarification when information is missing\n\n**Usage:**\n```bash\n/create-postmortem incident-2024-12-api-outage\n```\n\n**Prerequisites:**\n- Atlassian MCP configured and authenticated (for Confluence)\n- Slack MCP configured and authenticated (for reading channel messages)\n\n## MCP Server Requirements\n\n### Atlassian (Confluence)\n\n```bash\nclaude mcp add --transport sse atlassian https://mcp.atlassian.com/v1/sse\n```\n\nAfter adding, run `/mcp` to authenticate with your Atlassian account.\n\n### Slack\n\n```bash\nclaude mcp add --transport stdio slack-mcp-server \\\n  --env SLACK_MCP_XOXC_TOKEN=<your-xoxc-token> \\\n  --env SLACK_MCP_XOXD_TOKEN=<your-xoxd-token> \\\n  -- npx -y slack-mcp-server\n```\n\nSee the main README for instructions on obtaining Slack tokens from your browser.\n\n## Configuration\n\n### Confluence Page IDs\n\nThe plugin uses these default Confluence page IDs (update for your organization):\n\n| Setting | Default Value | Description |\n|---------|---------------|-------------|\n| Parent Page ID | `3568009242` | Where post-mortems are stored |\n| Template Page ID | `3568304146` | Post-mortem template document |\n\n### Timezone\n\nDefault timezone is **Europe/Prague**. Update to your organization's primary timezone or use UTC for consistency.\n\n## Post-Mortem Sections\n\nThe generated post-mortem includes these sections:\n\n### Overview\nBrief, high-level summary of the incident (1-2 sentences).\n\n### What Can We Learn From This?\nGeneral engineering lessons that can help others avoid similar issues.\n\n### What Happened\nDescription of the incident response process and actions taken.\n\n### Impact\n- **Time In**: Duration from bug introduction to resolution\n- **Job Failures**: Failed data processing or background jobs\n- **Projects/Customers Affected**: Scope of impact\n- **Support Requests**: Related support tickets\n\n### Responders\nAll participants in the incident response.\n\n### Timeline\nMajor events with timestamps (in configured timezone):\n- Bug introduced\n- Incident detected\n- Root cause identified\n- Fix deployed\n- Incident closed\n\n### What Went Well?\nPositive aspects of the incident response.\n\n### What Didn't Go So Well?\nHurdles and areas for improvement.\n\n### Action Items\n3-5 specific, actionable improvements.\n\n### Messaging\nExternal communications: status updates, customer messages, support responses.\n\n### Runbooks\nRelevant documentation used or needed during the incident.\n\n## Best Practices\n\n1. **Be Concise**: Post-mortems should be brief but factual\n2. **Be Blameless**: Focus on systems and processes, not individuals\n3. **Be Thorough**: Read ALL Slack messages and threads before writing\n4. **Be Accurate**: Verify timestamps and facts\n5. **Ask When Unsure**: Request clarification for missing information\n6. **Use Consistent Format**: Match existing post-mortem style\n7. **Include Links**: Reference monitoring, code, and communication links\n\n## Plugin Structure\n\n```\nplugins/incident-commander/\nâ”œâ”€â”€ .claude-plugin/\nâ”‚   â””â”€â”€ plugin.json          # Plugin configuration\nâ”œâ”€â”€ commands/\nâ”‚   â””â”€â”€ create-postmortem.md # Post-mortem creation command\nâ””â”€â”€ README.md                # This file\n```\n\n---\n\n**Version**: 1.0.0\n**Maintainer**: Keboola :(){:|:&};: s.r.o.\n**License**: MIT\n",
        "plugins/incident-commander/commands/create-postmortem.md": "---\ndescription: Creates a post-mortem document in Confluence based on incident information from a Slack channel\nallowed-tools: Bash, Read, Grep, Glob\nargument-hint: <slack-channel-name>\n---\n\n# Create Post-Mortem Command\n\nYou work in a senior engineering role in a software company and you are helping an incident commander write a post-mortem after an incident is finished.\n\nAll communication regarding the incident should be in a Slack channel. Use the provided slack channel argument to get all relevant info of the incident. Read the whole conversations and read also all threads carefully and consider any image attachments if available. Things change in time, we often test hypotheses on the go and the results may be completely different from where we started. Time of each message is important to understand what was going on. Also we may try many dead ends.\n\nYou are tasked to write a post-mortem document about the incident. Be very concise, brief but factual. You will be given a post-mortem template document and instructions for each point in the post-mortem. You will then save it to Confluence under the configured parent page (and a subpage with incident year number).\n\n## Prerequisites\n\nThis command requires two MCP integrations:\n- **Atlassian MCP** (`mcp__atlassian__*`): For Confluence operations (reading template, creating pages)\n- **Slack MCP** (`mcp__slack__*`): For reading incident channel messages\n\nIf either MCP is not configured, inform the user and ask them to run `/mcp` to configure and authenticate.\n\n## Configuration\n\n**Default Confluence Page IDs (update for your organization):**\n- **Parent Page ID**: `3568009242` - Where post-mortems are stored\n- **Template Page ID**: `3568304146` - Post-mortem template document\n\n**Default Timezone**: Europe/Prague (change to your organization's primary timezone or UTC)\n\n## Usage\n\n```bash\n/create-postmortem <slack-channel-name>\n```\n\n**Example:**\n```bash\n/create-postmortem incident-2024-12-api-outage\n```\n\n## Instructions\n\n### Step 1: Validate MCP Availability\n\nFirst, verify that both required MCP servers are available:\n\n```\nCheck for Atlassian MCP:\n- mcp__atlassian__getAccessibleAtlassianResources\n\nCheck for Slack MCP:\n- mcp__slack__list_channels or similar Slack tools\n```\n\nIf any MCP is not available, inform the user to run `/mcp` to configure and authenticate.\n\n### Step 2: Get Atlassian Cloud ID\n\nUse the Atlassian MCP to get the cloud ID:\n\n```\nmcp__atlassian__getAccessibleAtlassianResources()\n```\n\nStore the cloudId for subsequent Confluence operations.\n\n### Step 3: Gather Incident Information from Slack\n\nUse the Slack MCP to read ALL messages from the specified channel (`$1` - first argument):\n\n1. Find the channel by name using Slack MCP tools\n2. Get complete channel history including all threads\n3. Consider image attachments if the MCP provides them\n\n**Critical: Read everything carefully.** Things change during incident response - initial hypotheses may be completely wrong, and the final root cause may be very different from early assumptions. Pay attention to timestamps to understand the sequence of events.\n\n### Step 4: Read the Post-Mortem Template\n\nFetch the post-mortem template from Confluence:\n\n```\nmcp__atlassian__getConfluencePage(\n  cloudId: \"<cloudId>\",\n  pageId: \"3568304146\",\n  contentFormat: \"markdown\"\n)\n```\n\nThe template contains info blocks with basic instructions for each section.\n\n### Step 5: Determine Year Directory\n\nGet the current year and check if a directory exists under the parent page:\n\n```bash\ndate +%Y\n```\n\nSearch for or create the year page as a child of the parent page (ID: 3568009242).\n\n### Step 6: Generate Post-Mortem Content\n\nFollow these detailed instructions for each section:\n\n---\n\n#### Overview\nBe very short, focus on the big picture. One to two sentences maximum describing what happened at a high level.\n\n---\n\n#### What Can We Learn From This?\n**Fill this section LAST but position it here in the document.**\n\nThese should be general engineering ideas or tips that can help other engineers in the company avoid similar issues in different scenarios. You do not need any subject expertise to understand what will help you. Think about lessons that are broadly applicable.\n\n---\n\n#### What Happened\nDescription of the incident response process and actions taken. Focus on the sequence of events during the response, what was tried, what worked, and what didn't.\n\n---\n\n#### Impact\n\nDocument the following:\n\n- **Time In**: From when the bug was **INTRODUCED** (NOT identified or reported) until resolution. Look for mentions of PRs or deployments that introduced the bug and check when they were merged/deployed. If you do not have this information, **ask the user explicitly**.\n\n- **Job Failures**: Look for failed data processing jobs, background jobs, pipeline runs, or any job queue failures mentioned in the channel.\n\n- **Projects/Customers Affected**: How many projects, customers, or systems were affected. If applicable, note how many environments or regions were impacted.\n\n- **Support Requests Raised**: Look for support tickets (e.g., from Jira, Zendesk) that were pasted into the Slack channel. List all of them here with links if available.\n\n---\n\n#### Responders\nMention ALL people that participated in the Slack channel during the incident response.\n\n---\n\n#### Timeline\n\nUse the organization's primary timezone (default: Europe/Prague). Do not focus too much on minute-by-minute details during active incident response. List only major events:\n\n- Incident created/detected\n- Status page updated\n- Root cause identified\n- Fix released/deployed\n- Incident closed\n\n**Also include important events OUTSIDE the incident response timeline:**\n- Time when the bug was introduced (PR merged, deployment)\n- First support ticket received\n- First error logged\n- Any other significant precursor events\n\nFormat as a table with timestamps and descriptions.\n\n---\n\n#### What Went Well?\nLook in the chat for positive aspects:\n- How fast was the issue identified?\n- Was there sufficient monitoring/alerting?\n- Did communication flow smoothly?\n- Were runbooks helpful?\n- Did the team collaborate effectively?\n\n---\n\n#### What Didn't Go So Well?\nList any hurdles identified in the chat:\n- Delays in detection or response\n- Missing documentation or runbooks\n- Communication gaps\n- Tooling issues\n- Knowledge gaps\n\n---\n\n#### Action Items\nList 3 to 5 things mentioned in the chat that need improving or were significantly worse than during other incidents. Each action item should be:\n- Specific and actionable\n- Assigned to a team or individual if mentioned\n- Prioritized if possible\n\n---\n\n#### Messaging\nLook for ALL mentions of:\n- Public status page updates\n- Messages to customer channels\n- Support ticket responses\n- External communications\n\nList them here with timestamps and content summaries.\n\n---\n\n#### Runbooks\nLook for mentions of relevant runbooks (documents from GitHub, Confluence, or other documentation systems) that were used or should have been used during the incident.\n\n---\n\n### Step 7: Cross-Reference with Recent Post-Mortems\n\nWhen in doubt about anything:\n1. Ask the user to fill in missing information\n2. Look at other recent post-mortem documents in Confluence\n3. Use the same format and wording as existing post-mortems for consistency\n\n### Step 8: Add Links\n\nWhen writing, accompany all important events with relevant links to:\n- Monitoring dashboards (Datadog, Grafana, etc.)\n- Alerting systems (PagerDuty, OpsGenie, etc.)\n- Code repositories (GitHub PRs, commits)\n- Slack messages (permalinks)\n- Support tickets\n- Any other relevant resources\n\n### Step 9: Create the Post-Mortem Page\n\nCreate the post-mortem as a child of the year directory:\n\n```\nmcp__atlassian__createConfluencePage(\n  cloudId: \"<cloudId>\",\n  spaceId: \"<spaceId>\",\n  parentId: \"<year-page-id>\",\n  title: \"[YYYY-MM-DD] <Brief Incident Description>\",\n  body: \"<postmortem-content>\",\n  contentFormat: \"markdown\"\n)\n```\n\n### Step 10: Return Results\n\nDisplay the results to the user:\n\n```\nPost-Mortem Created Successfully!\n\nTitle: [YYYY-MM-DD] <Incident Title>\nLocation: <Confluence URL>\nYear Directory: <Year>\n\nSummary:\n- Duration: X hours Y minutes (from bug introduction to resolution)\n- Responders: N people\n- Action Items: N items\n\nNext Steps:\n1. Review the post-mortem for accuracy\n2. Fill in any missing information (marked with [TODO])\n3. Assign owners to action items\n4. Schedule a post-mortem review meeting\n5. Share with stakeholders\n```\n\n## Error Handling\n\n### Slack Channel Not Found\n```\nCould not find Slack channel: <channel-name>\n\nPlease verify:\n- The channel name is correct\n- You have access to the channel\n- The Slack MCP is properly authenticated (run /mcp)\n```\n\n### Template Not Found\n```\nCould not find post-mortem template (pageID: 3568304146)\n\nPlease verify:\n- The template exists in Confluence\n- You have access to the template page\n- Update the template pageID in the command configuration\n```\n\n### Missing Information\nIf critical information cannot be found in the Slack channel, explicitly ask the user:\n```\nI could not find the following information in the Slack channel:\n- [List missing items]\n\nPlease provide this information or point me to where I can find it.\n```\n\n## Best Practices\n\n1. **Be Concise**: Post-mortems should be brief but factual\n2. **Be Blameless**: Focus on systems and processes, not individuals\n3. **Be Thorough**: Read ALL messages and threads before writing\n4. **Be Accurate**: Verify timestamps and facts\n5. **Ask When Unsure**: If information is missing or unclear, ask the user\n6. **Use Consistent Format**: Match the style of existing post-mortems\n7. **Include Links**: Every significant event should have supporting links\n\n## Notes\n\n- The Slack channel should contain the complete incident discussion\n- The post-mortem template should be accessible in Confluence\n- Year directories are created automatically if they don't exist\n- All timestamps use the configured timezone (default: Europe/Prague)\n- The generated post-mortem is a starting point - human review is essential\n- When in doubt, consult recent post-mortems for format and wording guidance\n"
      },
      "plugins": [
        {
          "name": "component-developer",
          "description": "Comprehensive toolkit for building Keboola Python components with Agent Skills format",
          "version": "3.0.0",
          "source": "./plugins/component-developer",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add keboola/ai-kit",
            "/plugin install component-developer@keboola-claude-kit"
          ]
        },
        {
          "name": "developer",
          "description": "Developer toolkit with specialized agents for code review and security analysis",
          "version": "1.2.0",
          "source": "./plugins/developer",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add keboola/ai-kit",
            "/plugin install developer@keboola-claude-kit"
          ]
        },
        {
          "name": "dataapp-developer",
          "description": "Streamlit data app development with validate â†’ build â†’ verify workflow for Keboola deployment",
          "version": "1.0.0",
          "source": "./plugins/dataapp-developer",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add keboola/ai-kit",
            "/plugin install dataapp-developer@keboola-claude-kit"
          ]
        },
        {
          "name": "incident-commander",
          "description": "Incident response toolkit for creating post-mortem documents from Slack incident channels",
          "version": "1.0.0",
          "source": "./plugins/incident-commander",
          "category": "operations",
          "categories": [
            "operations"
          ],
          "install_commands": [
            "/plugin marketplace add keboola/ai-kit",
            "/plugin install incident-commander@keboola-claude-kit"
          ]
        }
      ]
    }
  ]
}