{
  "author": {
    "id": "grandcamel",
    "display_name": "Jason Krueger",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/7255229?u=b23bfc292d68afb457b89c04dab514b39ca43ca9&v=4",
    "url": "https://github.com/grandcamel",
    "bio": "Site Reliability Engineer | Infrastructure Architecture & Release Management | Leading Enterprise Automation at Scale",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 4,
      "total_skills": 14,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "splunk-assistant-skills",
      "version": null,
      "description": "Comprehensive Splunk automation skills for Claude Code - 14 specialized skills covering search, job management, exports, metadata discovery, and administration. Natural language Splunk automation with zero SPL memorization.",
      "owner_info": {
        "name": "grandcamel"
      },
      "keywords": [],
      "repo_full_name": "grandcamel/Splunk-Assistant-Skills",
      "repo_url": "https://github.com/grandcamel/Splunk-Assistant-Skills",
      "repo_description": "A modular, production-ready Claude Code skills framework for Splunk REST API automation",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-26T03:56:15Z",
        "created_at": "2025-12-29T06:33:16Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1045
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 613
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 15233
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/assistant-skills-setup.md",
          "type": "blob",
          "size": 2732
        },
        {
          "path": "commands/browse-skills.md",
          "type": "blob",
          "size": 2177
        },
        {
          "path": "commands/skill-info.md",
          "type": "blob",
          "size": 1144
        },
        {
          "path": "commands/splunk-discover-instance.md",
          "type": "blob",
          "size": 1310
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/shared",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/shared/docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/shared/docs/DECISION_TREE.md",
          "type": "blob",
          "size": 5688
        },
        {
          "path": "skills/shared/docs/QUICK_REFERENCE.md",
          "type": "blob",
          "size": 7637
        },
        {
          "path": "skills/shared/docs/SAFEGUARDS.md",
          "type": "blob",
          "size": 9894
        },
        {
          "path": "skills/shared/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/shared/tests/live_integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/shared/tests/live_integration/README.md",
          "type": "blob",
          "size": 7786
        },
        {
          "path": "skills/splunk-alert",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-alert/SKILL.md",
          "type": "blob",
          "size": 2845
        },
        {
          "path": "skills/splunk-app",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-app/SKILL.md",
          "type": "blob",
          "size": 1331
        },
        {
          "path": "skills/splunk-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-assistant/SKILL.md",
          "type": "blob",
          "size": 3757
        },
        {
          "path": "skills/splunk-export",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-export/SKILL.md",
          "type": "blob",
          "size": 2373
        },
        {
          "path": "skills/splunk-job",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-job/SKILL.md",
          "type": "blob",
          "size": 4174
        },
        {
          "path": "skills/splunk-kvstore",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-kvstore/SKILL.md",
          "type": "blob",
          "size": 2123
        },
        {
          "path": "skills/splunk-lookup",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-lookup/SKILL.md",
          "type": "blob",
          "size": 1423
        },
        {
          "path": "skills/splunk-metadata",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-metadata/SKILL.md",
          "type": "blob",
          "size": 1630
        },
        {
          "path": "skills/splunk-metrics",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-metrics/SKILL.md",
          "type": "blob",
          "size": 1177
        },
        {
          "path": "skills/splunk-rest-admin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-rest-admin/SKILL.md",
          "type": "blob",
          "size": 1321
        },
        {
          "path": "skills/splunk-savedsearch",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-savedsearch/SKILL.md",
          "type": "blob",
          "size": 1969
        },
        {
          "path": "skills/splunk-search",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-search/SKILL.md",
          "type": "blob",
          "size": 4139
        },
        {
          "path": "skills/splunk-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-security/SKILL.md",
          "type": "blob",
          "size": 1781
        },
        {
          "path": "skills/splunk-tag",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/splunk-tag/SKILL.md",
          "type": "blob",
          "size": 1045
        },
        {
          "path": "tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/e2e",
          "type": "tree",
          "size": null
        },
        {
          "path": "tests/e2e/README.md",
          "type": "blob",
          "size": 2194
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"splunk-assistant-skills\",\n  \"owner\": {\n    \"name\": \"grandcamel\"\n  },\n  \"metadata\": {\n    \"description\": \"Comprehensive Splunk automation skills for Claude Code - 14 specialized skills covering search, job management, exports, metadata discovery, and administration. Natural language Splunk automation with zero SPL memorization.\",\n    \"version\": \"2.0.0\",\n    \"homepage\": \"https://github.com/grandcamel/Splunk-Assistant-Skills\",\n    \"repository\": \"https://github.com/grandcamel/Splunk-Assistant-Skills\",\n    \"license\": \"MIT\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"splunk-assistant-skills\",\n      \"source\": \".\",\n      \"description\": \"Complete Splunk automation suite with 14 specialized skills - search execution, job lifecycle, data export, metadata discovery, lookups, saved searches, alerts, and more\",\n      \"version\": \"2.0.0\",\n      \"category\": \"productivity\",\n      \"keywords\": [\"splunk\", \"search\", \"spl\", \"siem\", \"automation\", \"devops\", \"natural-language\"],\n      \"author\": {\n        \"name\": \"grandcamel\"\n      }\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"splunk-assistant-skills\",\n  \"version\": \"2.0.0\",\n  \"description\": \"14 specialized skills for natural language Splunk automation - search, job management, exports, and administration via Claude Code\",\n  \"author\": {\n    \"name\": \"grandcamel\",\n    \"url\": \"https://github.com/grandcamel\"\n  },\n  \"homepage\": \"https://github.com/grandcamel/Splunk-Assistant-Skills\",\n  \"repository\": \"https://github.com/grandcamel/Splunk-Assistant-Skills\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"splunk\",\n    \"search\",\n    \"spl\",\n    \"siem\",\n    \"automation\",\n    \"devops\",\n    \"natural-language\",\n    \"observability\"\n  ]\n}\n",
        "README.md": "<p align=\"center\">\n  <img src=\"assets/logo.svg\" alt=\"Splunk Assistant Skills\" width=\"140\">\n</p>\n\n<h1 align=\"center\">Splunk Assistant Skills</h1>\n\n<table align=\"center\">\n<tr>\n<td align=\"center\">\n<h2>80%</h2>\n<sub>Faster than manual<br>SPL workflows</sub>\n</td>\n<td align=\"center\">\n<h2>14</h2>\n<sub>Specialized skills<br>one conversation</sub>\n</td>\n<td align=\"center\">\n<h2>83</h2>\n<sub>Production-ready<br>Python scripts</sub>\n</td>\n<td align=\"center\">\n<h2>0</h2>\n<sub>SPL syntax<br>to memorize</sub>\n</td>\n</tr>\n</table>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/tests-348%20passing%2C%207%20xfail-brightgreen?logo=pytest\" alt=\"Tests\">\n  <img src=\"https://img.shields.io/badge/python-3.8+-3776AB?logo=python&logoColor=white\" alt=\"Python 3.8+\">\n  <img src=\"https://img.shields.io/badge/skills-14-FF6900\" alt=\"Skills\">\n  <a href=\"https://pypi.org/project/splunk-as/\"><img src=\"https://img.shields.io/pypi/v/splunk-as?color=blue&logo=pypi&logoColor=white\" alt=\"PyPI\"></a>\n  <img src=\"https://img.shields.io/github/stars/grandcamel/Splunk-Assistant-Skills?style=social\" alt=\"GitHub Stars\">\n  <img src=\"https://img.shields.io/badge/license-MIT-green\" alt=\"MIT License\">\n</p>\n\n<p align=\"center\">\n  <strong>Talk to Splunk like a colleague, not a query language.</strong><br>\n  <sub>Natural language search, job management, and administration for Splunk via Claude Code.</sub>\n</p>\n\n<div align=\"center\">\n\n```\n> \"Show me error patterns in the main index from the last hour\"\n\nClaude: Running SPL query...\n  index=main error earliest=-1h | stats count by host, sourcetype | sort -count\n\nFound 847 errors across 12 hosts. Top sources:\n  web-prod-01  nginx:error     423\n  api-srv-03   application     298\n  db-master    postgresql      126\n```\n\n</div>\n\n<p align=\"center\">\n  <a href=\"#quick-start\"><strong>Get Started</strong></a> &bull;\n  <a href=\"#skills-overview\">Skills</a> &bull;\n  <a href=\"#who-is-this-for\">Use Cases</a> &bull;\n  <a href=\"#architecture\">Architecture</a>\n</p>\n\n---\n\n## The Difference\n\n<table>\n<tr>\n<td width=\"50%\">\n\n### The SPL Way\n```spl\nindex=main sourcetype=access_combined\n| eval response_time=response_time/1000\n| where response_time > 2\n| stats avg(response_time) as avg_rt,\n        max(response_time) as max_rt,\n        count by host\n| sort -count\n| head 10\n```\n*Hope you remembered the syntax...*\n\n</td>\n<td width=\"50%\">\n\n### The Natural Way\n```\n\"Show me slow API responses over 2 seconds,\n grouped by host, top 10\"\n```\n*Just ask.*\n\n</td>\n</tr>\n</table>\n\n### Time Saved\n\n| Task | Traditional Splunk | Splunk Assistant | Saved |\n|------|------------------|----------------|-------|\n| Write complex SPL query | 5-15 min | 30 sec | 90% |\n| Check job status & results | 2-3 min | 10 sec | 95% |\n| Export large dataset | 5-10 min | 1 min | 85% |\n| Create saved search | 3-5 min | 30 sec | 90% |\n| Debug search errors | 5-20 min | 1 min | 80% |\n\n**Typical user:** Save 3-5 hours per week.\n\n---\n\n## Quick Start\n\n### Option A: Install as Claude Code Plugin (Recommended)\n\n```bash\n# Install from GitHub\nclaude plugin add github:grandcamel/Splunk-Assistant-Skills\n```\n\n### Option B: Manual Installation\n\n#### 1. Clone and Install Dependencies\n\n```bash\ngit clone https://github.com/grandcamel/Splunk-Assistant-Skills.git\ncd Splunk-Assistant-Skills\npip install -r requirements.txt\n```\n\n#### 2. Get API Token\n\n1. Log into Splunk Web\n2. Go to **Settings > Tokens**\n3. Click **New Token**, select your user\n4. Copy the generated token\n\n#### 3. Configure\n\n```bash\n# Set environment variables\nexport SPLUNK_TOKEN=\"your-jwt-token\"\nexport SPLUNK_SITE_URL=\"https://splunk.example.com\"\n\n# Or create .claude/settings.local.json for profiles\n```\n\n#### 4. Install CLI\n\n```bash\n# Install the splunk-as CLI\npip install splunk-as\n\n# Verify installation\nsplunk-as --version\n```\n\n#### 5. Start Using\n\n```bash\n# CLI usage (recommended)\nsplunk-as search oneshot \"index=main | stats count by sourcetype\" --earliest -1h\n\n# Or with Claude Code\n> \"Search for errors in the main index from the last hour\"\n```\n\n**That's it.** Claude now has full Splunk access.\n\n<p align=\"center\">\n  <a href=\"CLAUDE.md\"><strong>Full Setup Guide</strong></a>\n</p>\n\n---\n\n## Setup (Assistant Skills)\n\nIf you installed via the plugin system, run the setup wizard:\n\n```bash\n/assistant-skills-setup\n```\n\nThis configures:\n- Shared Python venv at `~/.assistant-skills-venv/`\n- Required dependencies from `requirements.txt`\n- Environment variables (prompts you to set credentials)\n- `claude-as` shell function for running Claude with dependencies\n\nAfter setup, use `claude-as` instead of `claude`:\n```bash\nclaude-as  # Runs Claude with Assistant Skills venv activated\n```\n\n### Environment Variables\n\n| Variable | Required | Description |\n|----------|----------|-------------|\n| `SPLUNK_SITE_URL` | Yes | Splunk server URL (e.g., `https://splunk.example.com`) |\n| `SPLUNK_TOKEN` | Auth* | Bearer token (preferred). Create in Splunk Web: Settings > Tokens |\n| `SPLUNK_USERNAME` | Auth* | Basic auth username (alternative to token) |\n| `SPLUNK_PASSWORD` | Auth* | Basic auth password (use with `SPLUNK_USERNAME`) |\n| `SPLUNK_MANAGEMENT_PORT` | No | Management API port (default: `8089`) |\n| `SPLUNK_VERIFY_SSL` | No | Verify SSL certificates (default: `true`) |\n| `SPLUNK_DEFAULT_APP` | No | Default Splunk app context (default: `search`) |\n| `SPLUNK_DEFAULT_INDEX` | No | Default search index (default: `main`) |\n\n*\\*Authentication: Either `SPLUNK_TOKEN` OR both `SPLUNK_USERNAME` and `SPLUNK_PASSWORD` required.*\n\n---\n\n## What You Can Do\n\n```mermaid\nflowchart LR\n    subgraph Input[\"You Say\"]\n        A[\"Search for errors\"]\n        B[\"Export yesterday's logs\"]\n        C[\"Show my saved searches\"]\n        D[\"Create an alert\"]\n    end\n\n    subgraph Processing[\"Claude Understands\"]\n        E[\"splunk-search\"]\n        F[\"splunk-export\"]\n        G[\"splunk-savedsearch\"]\n        H[\"splunk-alert\"]\n    end\n\n    subgraph Output[\"You Get\"]\n        I[\"Formatted results\"]\n        J[\"CSV/JSON file\"]\n        K[\"Search list\"]\n        L[\"Alert configured\"]\n    end\n\n    A --> E --> I\n    B --> F --> J\n    C --> G --> K\n    D --> H --> L\n```\n\n<details>\n<summary><strong>Example: Security Analyst's Morning</strong></summary>\n\n**Before Splunk Assistant (45 minutes)**\n1. Open Splunk Web, navigate to search\n2. Write SPL for failed logins: `index=security action=failure | stats count by user, src_ip | sort -count`\n3. Copy results, open spreadsheet\n4. Write another query for privilege escalation\n5. Cross-reference with yesterday's baseline\n6. Document findings in ticket\n\n**After Splunk Assistant (5 minutes)**\n> Analyst: \"Show me failed logins in the last 24 hours, group by user and source IP, compare to yesterday's baseline, and flag any anomalies\"\n\nClaude provides a formatted summary with highlighted anomalies.\n\n**Time saved:** 40 minutes every morning\n\n</details>\n\n---\n\n## Skills Overview\n\n| Skill | Purpose | Example Command |\n|-------|---------|-----------------|\n| `splunk-assistant` | Hub router with progressive disclosure | \"Help me search Splunk\" |\n| `splunk-search` | SPL query execution (oneshot/normal/blocking) | \"Search for 404 errors in nginx logs\" |\n| `splunk-job` | Search job lifecycle management | \"Check status of job abc123\" |\n| `splunk-export` | High-volume streaming extraction | \"Export last week's firewall logs to CSV\" |\n| `splunk-metadata` | Index, source, sourcetype discovery | \"List all available indexes\" |\n| `splunk-lookup` | CSV and lookup file management | \"Upload users.csv as a lookup\" |\n| `splunk-tag` | Knowledge object tagging | \"Tag host web-01 as production\" |\n| `splunk-savedsearch` | Reports and scheduled searches | \"Show my saved searches\" |\n| `splunk-alert` | Alert triggering and monitoring | \"Create alert for high error rate\" |\n| `splunk-rest-admin` | REST API configuration access | \"Get server info\" |\n| `splunk-security` | Token management and RBAC | \"List authentication tokens\" |\n| `splunk-metrics` | Real-time metrics (mstats, mcatalog) | \"Show CPU metrics by host\" |\n| `splunk-app` | Application management | \"List installed apps\" |\n| `splunk-kvstore` | App Key Value Store operations | \"Query the threat intel collection\" |\n\n<p align=\"center\">\n  <a href=\"CLAUDE.md\"><strong>Full Scripts Reference</strong></a>\n</p>\n\n---\n\n## Who Is This For?\n\n<details>\n<summary><strong>Developers</strong> — Never leave your terminal</summary>\n\n**Stop context-switching to Splunk Web.**\n\nYou're debugging in your IDE. You need to check logs. Stay in your terminal.\n\n```bash\nclaude \"Show me errors from my-app in the last hour\"\n# Done in 3 seconds, never left your terminal\n```\n\n### Developer Cheat Sheet\n\n| Task | Command |\n|------|---------|\n| Search logs | \"Search for errors in app-name last hour\" |\n| Check deployments | \"Find deployment events today\" |\n| Debug issues | \"Show stack traces from main index\" |\n| Export for analysis | \"Export last 1000 errors to JSON\" |\n\n**Time saved:** ~45 min/week\n\n</details>\n\n<details>\n<summary><strong>Security Analysts</strong> — Investigate faster</summary>\n\n**Accelerate threat hunting and incident response.**\n\n### Quick Investigations\n```\n\"Find all failed SSH attempts in the last 24 hours\"\n\"Show authentication events for user john.doe\"\n\"Search for privilege escalation patterns\"\n```\n\n### Common Operations\n\n| Task | Command |\n|------|---------|\n| Failed logins | \"Show failed logins by user and IP\" |\n| Suspicious activity | \"Find unusual outbound connections\" |\n| Threat hunting | \"Search for indicators: 192.168.1.100, malware.exe\" |\n| Incident timeline | \"Build timeline for host web-01 last 4 hours\" |\n\n**Time saved:** Minutes per investigation\n\n</details>\n\n<details>\n<summary><strong>IT Operations</strong> — Monitor and respond</summary>\n\n**Real-time visibility without the query complexity.**\n\n### Quick Actions\n```\n\"Show critical alerts from last 24 hours\"\n\"Check system health across all hosts\"\n\"Find hosts with high CPU usage\"\n```\n\n### Common Operations\n\n| Task | Command |\n|------|---------|\n| Alert review | \"Show triggered alerts today\" |\n| Performance | \"CPU and memory stats by host\" |\n| Capacity | \"Disk usage trends this week\" |\n| Incidents | \"Errors across production hosts\" |\n\n**Time saved:** Hours per week on routine checks\n\n</details>\n\n<details>\n<summary><strong>Data Engineers</strong> — ETL without the complexity</summary>\n\n**Extract data at scale with simple commands.**\n\n### Data Extraction\n```\n\"Export last week's firewall logs to CSV\"\n\"Stream authentication events to JSON file\"\n\"Download metrics data for analysis\"\n```\n\n### Common Operations\n\n| Task | Command |\n|------|---------|\n| Bulk export | \"Export index=main last 7 days to CSV\" |\n| Filtered extract | \"Export errors from web tier to JSON\" |\n| Metrics dump | \"Download CPU metrics for all hosts\" |\n| Schema discovery | \"Show all fields in sourcetype=nginx\" |\n\n**Time saved:** Hours per data pipeline\n\n</details>\n\n---\n\n## Architecture\n\n```mermaid\nflowchart TD\n    U[\"User Request\"] --> CC[\"Claude Code\"]\n    CC --> HA[\"splunk-assistant<br/>Meta-Router\"]\n\n    HA --> |\"Search queries\"| SS[\"splunk-search\"]\n    HA --> |\"Job management\"| SJ[\"splunk-job\"]\n    HA --> |\"Data export\"| SE[\"splunk-export\"]\n    HA --> |\"Discovery\"| SM[\"splunk-metadata\"]\n    HA --> |\"Lookups\"| SL[\"splunk-lookup\"]\n    HA --> |\"Saved searches\"| SSS[\"splunk-savedsearch\"]\n    HA --> |\"Alerts\"| SA[\"splunk-alert\"]\n    HA --> |\"Security\"| SEC[\"splunk-security\"]\n    HA --> |\"Metrics\"| MET[\"splunk-metrics\"]\n    HA --> |\"Apps\"| APP[\"splunk-app\"]\n    HA --> |\"KV Store\"| KV[\"splunk-kvstore\"]\n    HA --> |\"Tags\"| TAG[\"splunk-tag\"]\n    HA --> |\"REST Admin\"| RA[\"splunk-rest-admin\"]\n\n    SS --> SH[\"Shared Library\"]\n    SJ --> SH\n    SE --> SH\n    SM --> SH\n    SL --> SH\n    SSS --> SH\n    SA --> SH\n    SEC --> SH\n    MET --> SH\n    APP --> SH\n    KV --> SH\n    TAG --> SH\n    RA --> SH\n\n    SH --> API[\"Splunk REST API<br/>Port 8089\"]\n```\n\n### Technical Highlights\n\n- **Dual Authentication**: JWT Bearer tokens (preferred) and Basic Auth support\n- **Environment Config**: Configure via environment variables\n- **Search Modes**: Oneshot (ad-hoc), Normal (async), Blocking (sync), Export (streaming)\n- **Progressive Disclosure**: 3-level optimization guidance\n- **Robust Error Handling**: Custom exception hierarchy with retry logic\n- **Type-Safe**: Full type annotations and input validation\n\n---\n\n## Quality & Security\n\n### Test Coverage\n\n| Category | Tests | Description |\n|----------|------:|-------------|\n| Unit Tests | 180 | Core library and CLI validation |\n| Integration Tests | 175 | Live Splunk API verification |\n| **Total** | **355** | Comprehensive coverage |\n\n> Tests run against live Splunk instances to ensure real-world reliability.\n\n### Security\n\n- **No credentials in code** — Environment variables or `.local.json` files (gitignored)\n- **Token-based auth** — JWT Bearer tokens preferred over Basic Auth\n- **Input validation** — All user input sanitized before API calls\n- **No destructive defaults** — Explicit confirmation required for dangerous operations\n\n---\n\n## Try It\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/grandcamel/Splunk-Assistant-Skills)\n\nOne-click cloud environment with all dependencies pre-installed.\n\n---\n\n## Documentation\n\n| Resource | Description |\n|----------|-------------|\n| [CLAUDE.md](CLAUDE.md) | Comprehensive project documentation |\n| [CHANGELOG.md](CHANGELOG.md) | Version history and release notes |\n| Skill SKILL.md files | Per-skill detailed documentation |\n| [splunk-demo](https://github.com/grandcamel/splunk-demo) | Live demo environment with pre-configured Splunk |\n\n### Need Help?\n\n- [GitHub Discussions](https://github.com/grandcamel/Splunk-Assistant-Skills/discussions)\n- [Report Issues](https://github.com/grandcamel/Splunk-Assistant-Skills/issues)\n\n---\n\n## E2E Testing\n\n### Run E2E Tests\n\nE2E tests validate the plugin with the Claude Code CLI:\n\n```bash\n# Requires ANTHROPIC_API_KEY\n./scripts/run-e2e-tests.sh           # Docker\n./scripts/run-e2e-tests.sh --local   # Local\n```\n\nSee [tests/e2e/README.md](tests/e2e/README.md) for details.\n\n---\n\n## Contributing\n\nContributions are welcome! See our contributing guidelines.\n\n```bash\n# Clone the repository\ngit clone https://github.com/grandcamel/Splunk-Assistant-Skills.git\ncd Splunk-Assistant-Skills\n\n# Install dependencies and CLI\npip install -r requirements.txt\npip install -e .\n\n# Run tests (live_integration excluded by default via pytest.ini)\npytest tests/ skills/*/tests/ -v\n```\n\nFollow [Conventional Commits](https://www.conventionalcommits.org/) for commit messages.\n\n---\n\n## Roadmap\n\n- [ ] **Splunk Cloud** — Native Splunk Cloud API support\n- [ ] **Dashboard Skills** — Create and manage dashboards via natural language\n- [ ] **Data Model Skills** — Accelerated data model queries\n- [ ] **Federated Search** — Cross-instance search capabilities\n- [ ] **Workflow Actions** — Trigger external workflows from search results\n\n---\n\n## License\n\nThis project is licensed under the MIT License — see the [LICENSE](LICENSE) file for details.\n\n---\n\n<p align=\"center\">\n  <strong>Talk to Splunk. Get answers. Ship faster.</strong>\n  <br>\n  <sub>Built for Claude Code by developers who were tired of memorizing SPL syntax.</sub>\n</p>\n",
        "commands/assistant-skills-setup.md": "---\nname: assistant-skills-setup\ndescription: Set up Splunk Assistant Skills - Python environment, dependencies, and credentials\n---\n\n# Splunk Assistant Skills Setup\n\nYou are helping the user set up Splunk Assistant Skills. This is a one-time setup that configures:\n\n1. A shared Python virtual environment at `~/.assistant-skills-venv/`\n2. Required Python dependencies\n3. Splunk connection credentials (URL, authentication)\n4. A `claude-as` shell function for running Claude with the venv activated\n\n## Setup Process\n\n### Step 1: Create Python Virtual Environment\n\nFirst, create the shared virtual environment:\n\n```bash\npython3 -m venv ~/.assistant-skills-venv\n```\n\n### Step 2: Activate and Install Dependencies\n\nActivate the venv and install the required packages:\n\n```bash\nsource ~/.assistant-skills-venv/bin/activate && pip install --upgrade pip && pip install splunk-as\n```\n\n### Step 3: Configure Splunk Credentials\n\nRun the environment setup script to configure Splunk connection:\n\n```bash\nbash \"${CLAUDE_PLUGIN_ROOT}/../../scripts/setup-env.sh\"\n```\n\nThis will prompt for:\n- Splunk URL (e.g., https://splunk.example.com)\n- Management port (default: 8089)\n- Authentication method (Bearer token or Basic auth)\n- Default app and index settings\n\n### Step 4: Create claude-as Shell Function\n\nAdd the `claude-as` function to the user's shell RC file. This function activates the venv before running Claude:\n\nFor bash users (~/.bashrc):\n```bash\ncat >> ~/.bashrc << 'EOF'\n\n# Claude with Assistant Skills venv\nclaude-as() {\n    source ~/.assistant-skills-venv/bin/activate\n    claude \"$@\"\n    deactivate\n}\nEOF\n```\n\nFor zsh users (~/.zshrc):\n```bash\ncat >> ~/.zshrc << 'EOF'\n\n# Claude with Assistant Skills venv\nclaude-as() {\n    source ~/.assistant-skills-venv/bin/activate\n    claude \"$@\"\n    deactivate\n}\nEOF\n```\n\n### Step 5: Verify Installation\n\nTest that the setup is complete:\n\n```bash\nsource ~/.assistant-skills-venv/bin/activate && python -c \"from splunk_as import get_splunk_client; print('Library installed successfully')\"\n```\n\n## Important Notes\n\n- After setup, use `claude-as` instead of `claude` to run with the Assistant Skills venv activated\n- Credentials are stored in `~/.env` with secure permissions (chmod 600)\n- To reconfigure credentials later, run the setup script again: `bash \"${CLAUDE_PLUGIN_ROOT}/../../scripts/setup-env.sh\"`\n- For profile-based configuration, see the CLAUDE.md documentation\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. **Python not found**: Ensure Python 3.8+ is installed\n2. **Permission denied**: Check that ~/.assistant-skills-venv is writable\n3. **Connection failed**: Verify Splunk URL and port are accessible\n4. **Authentication failed**: Check token validity or username/password\n",
        "commands/browse-skills.md": "---\nname: browse-skills\ndescription: Browse all available Splunk Assistant skills with descriptions\n---\n\n# Browse Splunk Assistant Skills\n\nDisplay all 14 available Splunk Assistant Skills with their purpose and trigger keywords.\n\n## Skill Catalog\n\n| Skill | Purpose | Triggers |\n|-------|---------|----------|\n| `splunk-assistant` | Central hub and router - routes requests to specialized skills | splunk, search, query, SPL |\n| `splunk-search` | SPL query execution (oneshot, normal, blocking modes) | search, SPL, query, find, oneshot, blocking |\n| `splunk-job` | Search job lifecycle (create, monitor, pause, cancel) | job, SID, status, progress, cancel, pause |\n| `splunk-export` | High-volume streaming data extraction | export, download, extract, stream, ETL |\n| `splunk-metadata` | Index, source, sourcetype discovery | metadata, index, source, sourcetype, fields |\n| `splunk-lookup` | CSV and lookup file management | lookup, CSV, upload, lookup table |\n| `splunk-kvstore` | App Key Value Store operations | kvstore, collection, key-value, persist |\n| `splunk-savedsearch` | Reports and scheduled searches | saved search, report, schedule |\n| `splunk-alert` | Alert triggering and monitoring | alert, trigger, notification, monitor |\n| `splunk-app` | Application management | app, application, install, package |\n| `splunk-security` | Token management, RBAC, ACL | token, permission, ACL, RBAC, security |\n| `splunk-rest-admin` | REST API configuration access | rest, admin, config, server, settings |\n| `splunk-tag` | Knowledge object tagging | tag, label, classify |\n| `splunk-metrics` | Real-time metrics (mstats, mcatalog) | metrics, mstats, mcatalog, time series |\n\n## Quick Start Commands\n\n```bash\n# Search\nsplunk-as search oneshot \"index=main | head 10\"\nsplunk-as search normal \"index=main | stats count\" --wait\n\n# Jobs\nsplunk-as job list\nsplunk-as job status <SID>\n\n# Metadata\nsplunk-as metadata indexes\nsplunk-as metadata sourcetypes --index main\n\n# Administration\nsplunk-as admin info\nsplunk-as security whoami\n```\n\n## Getting More Info\n\nUse `/skill-info <skill-name>` to see detailed documentation for a specific skill.\n\nExample: `/skill-info splunk-search`\n",
        "commands/skill-info.md": "---\nname: skill-info\ndescription: Show detailed information about a specific Splunk skill\narguments:\n  - name: skill\n    description: \"The skill name (e.g., splunk-search, splunk-job)\"\n    required: true\n---\n\n# Skill Information: {{ skill }}\n\nDisplay detailed documentation for the specified Splunk Assistant skill.\n\n## Instructions\n\nRead the SKILL.md file for the requested skill and present:\n\n1. **Purpose**: What the skill does\n2. **Triggers**: Keywords that activate this skill\n3. **CLI Commands**: Available `splunk-as` commands with examples\n4. **API Endpoints**: Underlying Splunk REST API endpoints\n5. **Best Practices**: Tips for effective usage\n6. **Related Skills**: Other skills that work together\n\n## Skill Location\n\nThe skill documentation is at:\n`${CLAUDE_PLUGIN_ROOT}/skills/{{ skill }}/SKILL.md`\n\n## Available Skills\n\n- splunk-assistant\n- splunk-search\n- splunk-job\n- splunk-export\n- splunk-metadata\n- splunk-lookup\n- splunk-kvstore\n- splunk-savedsearch\n- splunk-alert\n- splunk-app\n- splunk-security\n- splunk-rest-admin\n- splunk-tag\n- splunk-metrics\n\nIf the skill name is not found, suggest similar skills from the list above.\n",
        "commands/splunk-discover-instance.md": "---\nname: splunk-discover-instance\ndescription: Discover Splunk deployment type, version, and capabilities\n---\n\n# Splunk Instance Discovery\n\nDiscover and display information about the connected Splunk instance.\n\n## Discovery Steps\n\n### Step 1: Verify Connection\n\nCheck connectivity to the Splunk Search Head on management port 8089:\n\n```bash\nsplunk-as admin info\n```\n\nThis returns:\n- Server version and build\n- Deployment type (Cloud vs on-prem)\n- Connection status\n- Current user and capabilities\n\n### Step 2: Check Server Health\n\n```bash\nsplunk-as admin health\n```\n\n### Step 3: List Available Indexes\n\n```bash\nsplunk-as metadata indexes\n```\n\n### Step 4: Check User Permissions\n\n```bash\nsplunk-as security whoami\n```\n\n## Expected Output\n\nAfter running discovery, summarize:\n\n1. **Connection Status**: Connected/Not Connected\n2. **Splunk Version**: e.g., 9.1.0\n3. **Deployment Type**: Splunk Enterprise, Splunk Cloud, etc.\n4. **Current User**: Username and roles\n5. **Available Indexes**: List of accessible indexes\n6. **Key Capabilities**: What operations are permitted\n\n## Troubleshooting\n\nIf connection fails:\n- Verify `SPLUNK_SITE_URL` environment variable is set\n- Check `SPLUNK_TOKEN` or `SPLUNK_USERNAME`/`SPLUNK_PASSWORD`\n- Ensure port 8089 is accessible\n- Run `/assistant-skills-setup` to reconfigure\n",
        "skills/shared/docs/DECISION_TREE.md": "# Skill Routing Decision Tree\n\nThis guide helps route requests to the appropriate Splunk skill.\n\n## Quick Decision Flowchart\n\n```\n                         [User Request]\n                              |\n              +---------------+---------------+\n              |                               |\n        [Has SPL query?]               [No SPL query]\n              |                               |\n              v                               v\n    +-------------------+          +--------------------+\n    | splunk-search     |          | What resource?     |\n    | (oneshot/normal/  |          +--------------------+\n    |  blocking)        |                    |\n    +-------------------+          +---------+---------+\n                                   |         |         |\n                              [Data]    [Config]   [Admin]\n                                   |         |         |\n                        +----------+    +----+----+   +----------+\n                        |               |         |              |\n                    [Job?]         [Saved]    [App?]      [Security?]\n                        |          [Search?]     |              |\n                        v              |         v              v\n                 splunk-job            v    splunk-app   splunk-security\n                        |       splunk-savedsearch\n               +--------+--------+\n               |                 |\n          [Export?]         [Metadata?]\n               |                 |\n               v                 v\n         splunk-export    splunk-metadata\n```\n\n## Keyword Routing Table\n\n| Keywords | Route To | Confidence |\n|----------|----------|------------|\n| search, query, SPL, find, execute | splunk-search | High |\n| job, SID, status, poll, cancel, pause | splunk-job | High |\n| export, download, stream, large, ETL | splunk-export | High |\n| index, source, sourcetype, metadata, fields | splunk-metadata | High |\n| lookup, CSV, upload, enrichment | splunk-lookup | High |\n| tag, label, classify | splunk-tag | High |\n| saved search, report, schedule, scheduled | splunk-savedsearch | High |\n| alert, trigger, notification, monitor | splunk-alert | High |\n| token, permission, ACL, RBAC, capability | splunk-security | High |\n| metrics, mstats, mcatalog, time series | splunk-metrics | High |\n| app, application, install, addon | splunk-app | High |\n| kvstore, collection, key-value, persist | splunk-kvstore | High |\n| rest, admin, config, server info | splunk-rest-admin | High |\n| connect, verify, authenticate | splunk-assistant | High |\n\n## Operation Verb Mapping\n\n| Verb | Typical Skills | Notes |\n|------|----------------|-------|\n| get, list, show, view | All skills | Read-only operations |\n| search, query, find | splunk-search | SPL execution |\n| create, add, insert | splunk-savedsearch, splunk-lookup, splunk-kvstore, splunk-alert | Write operations |\n| update, modify, edit | splunk-savedsearch, splunk-kvstore, splunk-tag | Modification operations |\n| delete, remove | All skills with CRUD | Destructive operations |\n| export, download | splunk-export, splunk-lookup | Data extraction |\n| upload | splunk-lookup, splunk-app | Data ingestion |\n| cancel, pause, stop | splunk-job | Job control |\n| enable, disable | splunk-savedsearch, splunk-app | Toggle operations |\n\n## Resource Type Signals\n\n| Resource Type | Signal Words | Primary Skill | Secondary |\n|--------------|--------------|---------------|-----------|\n| Search Job | SID, job ID, 1234567890.12345 | splunk-job | splunk-search |\n| Saved Search | report, scheduled, dashboard | splunk-savedsearch | splunk-alert |\n| Lookup | CSV, lookup table, enrichment | splunk-lookup | - |\n| KV Store | collection, key, record | splunk-kvstore | - |\n| Alert | fired, triggered, notification | splunk-alert | splunk-savedsearch |\n| App | package, addon, .tgz | splunk-app | - |\n| Token | JWT, bearer, credential | splunk-security | - |\n| Index | bucket, event count | splunk-metadata | - |\n\n## Ambiguous Request Handling\n\n### Request: \"Show me the data\"\n1. Ask: \"Do you want to search for data (splunk-search) or explore what data exists (splunk-metadata)?\"\n2. If time-bounded: likely splunk-search\n3. If discovery-focused: likely splunk-metadata\n\n### Request: \"Create a report\"\n1. If one-time: splunk-search with export\n2. If recurring/scheduled: splunk-savedsearch\n3. If alerting needed: splunk-alert\n\n### Request: \"Delete the job\"\n1. If SID provided: splunk-job delete\n2. If name provided: likely splunk-savedsearch delete\n3. Ask for clarification if ambiguous\n\n### Request: \"Upload the file\"\n1. If CSV for enrichment: splunk-lookup\n2. If app package: splunk-app\n3. Ask: \"Is this a lookup CSV or an app package?\"\n\n### Request: \"Get the status\"\n1. Server status: splunk-rest-admin\n2. Job status: splunk-job\n3. Alert status: splunk-alert\n4. Ask: \"Status of what - server, job, or alert?\"\n\n## Multi-Skill Workflows\n\n### ETL Pipeline\n1. splunk-search - Create the search job\n2. splunk-job - Monitor progress\n3. splunk-export - Stream results to file\n\n### Alert Setup\n1. splunk-savedsearch - Create base search\n2. splunk-alert - Configure alert conditions\n3. splunk-security - Set permissions\n\n### Data Enrichment\n1. splunk-lookup - Upload CSV\n2. splunk-search - Test with lookup command\n3. splunk-metadata - Verify field extraction\n\n## Routing Priority\n\nWhen multiple skills match:\n1. **Most specific match wins** - \"cancel job\" -> splunk-job, not splunk-search\n2. **Explicit resource beats action** - \"delete saved search\" -> splunk-savedsearch\n3. **Context from conversation** - Recent job creation -> splunk-job for status\n4. **User's stated intent** - Always respect explicit skill requests\n",
        "skills/shared/docs/QUICK_REFERENCE.md": "# Splunk Assistant Skills Quick Reference\n\nOne-page cheat sheet for common operations.\n\n## CLI Command Quick Reference\n\n### Search Commands\n\n```bash\n# Oneshot (ad-hoc, inline results)\nsplunk-as search oneshot \"index=main | head 10\"\nsplunk-as search oneshot \"index=main | stats count\" --earliest -1h --latest now\n\n# Normal (async, returns SID)\nsplunk-as search normal \"index=main | stats count\" --wait\nsplunk-as search normal \"index=main | stats count\"  # Returns SID only\n\n# Blocking (sync, waits)\nsplunk-as search blocking \"index=main | head 10\" --timeout 60\n\n# Get results from job\nsplunk-as search results 1703779200.12345 --count 100\n\n# Validate SPL\nsplunk-as search validate \"index=main | stats count\"\n```\n\n### Job Management\n\n```bash\nsplunk-as job list                           # List all jobs\nsplunk-as job status 1703779200.12345        # Get job status\nsplunk-as job poll 1703779200.12345          # Wait for completion\nsplunk-as job cancel 1703779200.12345        # Cancel job\nsplunk-as job pause 1703779200.12345         # Pause job\nsplunk-as job unpause 1703779200.12345       # Resume job\nsplunk-as job delete 1703779200.12345        # Delete job\n```\n\n### Metadata Discovery\n\n```bash\nsplunk-as metadata indexes                    # List indexes\nsplunk-as metadata sourcetypes --index main   # List sourcetypes\nsplunk-as metadata sources --index main       # List sources\nsplunk-as metadata fields --index main        # Field summary\n```\n\n### Export\n\n```bash\nsplunk-as export results SID --output-file data.csv\nsplunk-as export results SID --format json --output-file data.json\nsplunk-as export estimate \"index=main\" --earliest -7d\n```\n\n### Saved Searches\n\n```bash\nsplunk-as savedsearch list --app search\nsplunk-as savedsearch get \"My Report\"\nsplunk-as savedsearch create \"Name\" \"SPL query\" --app search\nsplunk-as savedsearch run \"My Report\" --wait\nsplunk-as savedsearch enable \"My Report\"\nsplunk-as savedsearch delete \"My Report\"\n```\n\n### Lookups\n\n```bash\nsplunk-as lookup list --app search\nsplunk-as lookup download users.csv --output-file ./backup.csv\nsplunk-as lookup upload users.csv --app search\nsplunk-as lookup delete users.csv --app search\n```\n\n### KV Store\n\n```bash\nsplunk-as kvstore list --app search\nsplunk-as kvstore create my_collection --app search\nsplunk-as kvstore insert my_collection '{\"key\": \"value\"}'\nsplunk-as kvstore query my_collection --filter '{\"status\": \"active\"}'\nsplunk-as kvstore delete my_collection --app search\n```\n\n### Alerts\n\n```bash\nsplunk-as alert list --app search\nsplunk-as alert triggered --severity 4\nsplunk-as alert get alert_12345\nsplunk-as alert acknowledge alert_12345\n```\n\n### Security\n\n```bash\nsplunk-as security whoami\nsplunk-as security list-tokens\nsplunk-as security create-token --audience \"my-app\"\nsplunk-as security capabilities --user admin\n```\n\n### Administration\n\n```bash\nsplunk-as admin info\nsplunk-as admin health\nsplunk-as admin list-users\nsplunk-as admin rest-get /services/server/info\n```\n\n## Common SPL Patterns\n\n### Basic Searches\n\n```spl\n# Simple search with time\nindex=main earliest=-1h | head 100\n\n# Filter by field\nindex=main status>=400 | stats count by status\n\n# Multiple indexes\nindex=main OR index=web | stats count by index\n```\n\n### Statistics\n\n```spl\n# Count by field\nindex=main | stats count by sourcetype | sort -count\n\n# Multiple aggregations\nindex=main | stats count, avg(response_time), max(response_time) by host\n\n# Top values\nindex=main | top 10 uri\n\n# Rare values\nindex=main | rare status\n```\n\n### Time Analysis\n\n```spl\n# Time chart\nindex=main | timechart span=1h count by sourcetype\n\n# Time-based comparison\nindex=main earliest=-1d@d latest=@d | stats count as today\n| appendcols [search index=main earliest=-2d@d latest=-1d@d | stats count as yesterday]\n\n# Bucketing\nindex=main | bucket _time span=5m | stats count by _time\n```\n\n### Field Extraction\n\n```spl\n# Limit fields (performance)\nindex=main | fields host, status, uri | table host status uri\n\n# Field summary\nindex=main | fieldsummary maxvals=100\n\n# Rex extraction\nindex=main | rex field=_raw \"user=(?<username>\\w+)\"\n```\n\n### Subsearch\n\n```spl\n# Basic subsearch\nindex=main [search index=alerts | fields src_ip | head 100]\n\n# Subsearch with format\nindex=main [search index=users | fields user_id | format]\n```\n\n### Lookup Enrichment\n\n```spl\n# Lookup\nindex=main | lookup users.csv username OUTPUT department\n\n# Automatic lookup (if configured)\nindex=main | table username, department\n\n# Outputlookup\nindex=main | stats count by host | outputlookup host_counts.csv\n```\n\n### Metrics\n\n```spl\n# mstats\n| mstats avg(cpu.percent) WHERE index=metrics BY host span=1h\n\n# mcatalog\n| mcatalog values(metric_name) WHERE index=metrics\n\n# mpreview\n| mpreview index=metrics\n```\n\n### Metadata Commands\n\n```spl\n# Metadata search\n| metadata type=sourcetypes index=main\n\n# Metasearch\n| metasearch index=* sourcetype=access_combined\n\n# REST API\n| rest /services/server/info\n```\n\n## Time Modifier Syntax\n\n| Modifier | Description | Example |\n|----------|-------------|---------|\n| `-1h` | 1 hour ago | `earliest=-1h` |\n| `-1d` | 1 day ago | `earliest=-1d` |\n| `-1w` | 1 week ago | `earliest=-1w` |\n| `-1mon` | 1 month ago | `earliest=-1mon` |\n| `@d` | Snap to day | `earliest=-1d@d` |\n| `@h` | Snap to hour | `earliest=-1h@h` |\n| `now` | Current time | `latest=now` |\n\n### Common Time Ranges\n\n```\nearliest=-1h latest=now          # Last hour\nearliest=-24h latest=now         # Last 24 hours\nearliest=-7d@d latest=@d         # Last 7 complete days\nearliest=-1d@d latest=@d         # Yesterday (complete)\nearliest=@d latest=now           # Today so far\nearliest=-1mon@mon latest=@mon   # Last complete month\n```\n\n## API Endpoint Shortcuts\n\n| Operation | Endpoint |\n|-----------|----------|\n| Oneshot search | `POST /services/search/jobs/oneshot` |\n| Create job | `POST /services/search/v2/jobs` |\n| Job status | `GET /services/search/v2/jobs/{sid}` |\n| Job results | `GET /services/search/v2/jobs/{sid}/results` |\n| Job control | `POST /services/search/v2/jobs/{sid}/control` |\n| Saved searches | `GET/POST /services/saved/searches` |\n| Lookups | `GET/POST /services/data/lookup-table-files` |\n| KV Store | `GET/POST /services/storage/collections/data/{coll}` |\n| Tokens | `GET/POST /services/authorization/tokens` |\n| Server info | `GET /services/server/info` |\n| Indexes | `GET /services/data/indexes` |\n| Apps | `GET/POST /services/apps/local` |\n| Alerts | `GET /services/alerts/fired_alerts` |\n\n## Error Code Meanings\n\n| HTTP Code | Meaning | Common Cause |\n|-----------|---------|--------------|\n| 400 | Bad Request | Invalid SPL syntax |\n| 401 | Unauthorized | Invalid/expired token |\n| 403 | Forbidden | Missing capability |\n| 404 | Not Found | Wrong SID or resource name |\n| 409 | Conflict | Resource already exists |\n| 429 | Too Many Requests | Rate limited |\n| 500 | Server Error | Splunk internal error |\n| 503 | Service Unavailable | Search quota exceeded |\n\n## Search Modes Comparison\n\n| Mode | Best For | Returns SID | Wait | Max Results |\n|------|----------|-------------|------|-------------|\n| Oneshot | Ad-hoc, <50K rows | No | Inline | 50,000 |\n| Normal | Long-running | Yes | Async | No limit |\n| Blocking | Simple, fast | Yes | Sync | No limit |\n| Export | Large data, ETL | Yes | Stream | No limit |\n\n## Environment Variables\n\n```bash\n# Authentication\nSPLUNK_TOKEN=<jwt-token>           # Bearer token (preferred)\nSPLUNK_USERNAME=<user>             # Basic auth\nSPLUNK_PASSWORD=<pass>             # Basic auth\n\n# Connection\nSPLUNK_SITE_URL=https://splunk.example.com\nSPLUNK_MANAGEMENT_PORT=8089\nSPLUNK_VERIFY_SSL=true\n\n# Defaults\nSPLUNK_DEFAULT_APP=search\nSPLUNK_DEFAULT_INDEX=main\n```\n",
        "skills/shared/docs/SAFEGUARDS.md": "# Safeguards and Recovery Procedures\n\nThis document defines risk levels, pre-operation checklists, and recovery procedures for Splunk Assistant Skills.\n\n## Risk Level Definitions\n\n| Level | Indicator | Description | User Confirmation |\n|-------|-----------|-------------|-------------------|\n| Safe | `-` | Read-only operations, no data modification | None required |\n| Caution | `Warning` | Modifiable but easily reversible | Optional |\n| Warning | `WarningWarning` | Destructive but potentially recoverable | Recommended |\n| Danger | `WarningWarningWarning` | **IRREVERSIBLE** data loss | **Required** |\n\n## Pre-Operation Checklists\n\n### Before Any Destructive Operation\n\n- [ ] Confirm the target resource name/ID is correct\n- [ ] Verify you're connected to the intended Splunk instance\n- [ ] Check if the resource is currently in use\n- [ ] Determine if backups exist\n\n### Before Delete Operations\n\n```bash\n# List what will be affected\nsplunk-as <skill> list --app <app>\n\n# Get details of specific resource\nsplunk-as <skill> get <resource-name>\n\n# Check dependencies (if applicable)\nsplunk-as search oneshot \"| rest /services/<endpoint> | search <resource>\"\n```\n\n### Before Bulk Operations\n\n- [ ] Test with a single item first\n- [ ] Review the full list of affected items\n- [ ] Consider time-of-day (avoid production peak hours)\n- [ ] Have rollback plan ready\n\n## Recovery Procedures by Resource Type\n\n### Search Jobs (splunk-job)\n\n| Operation | Recovery Method |\n|-----------|-----------------|\n| Cancel job | Re-run the search |\n| Delete job | Results are gone; re-run search if needed |\n| Finalize job | Accept partial results or re-run |\n\n**Prevention**: Use `--ttl` to set appropriate time-to-live instead of manual deletion.\n\n### Saved Searches (splunk-savedsearch)\n\n| Operation | Recovery Method |\n|-----------|-----------------|\n| Update | No version history; restore from backup |\n| Delete | Restore from backup or recreate |\n| Disable | Re-enable with `splunk-as savedsearch enable` |\n\n**Prevention**: Export before modifying:\n```bash\nsplunk-as savedsearch get \"MySearch\" --output json > backup_mysearch.json\n```\n\n### Lookups (splunk-lookup)\n\n| Operation | Recovery Method |\n|-----------|-----------------|\n| Upload (overwrite) | Restore from local copy or backup |\n| Delete | Restore from backup |\n\n**Prevention**: Always download before upload:\n```bash\nsplunk-as lookup download users.csv --output-file users_backup.csv\nsplunk-as lookup upload users_new.csv --app search\n```\n\n### KV Store (splunk-kvstore)\n\n| Operation | Recovery Method |\n|-----------|-----------------|\n| Update record | Previous value lost; restore from backup |\n| Delete record | Restore from backup |\n| Delete collection | **IRREVERSIBLE** - all data lost |\n\n**Prevention**: Export collection before modifications:\n```bash\nsplunk-as search oneshot \"| inputlookup my_collection\" --output json > backup.json\n```\n\n### Apps (splunk-app)\n\n| Operation | Recovery Method |\n|-----------|-----------------|\n| Disable | Re-enable with `splunk-as app enable` |\n| Uninstall | **IRREVERSIBLE** - reinstall from package |\n\n**Prevention**: Keep original package files. Never uninstall without backup.\n\n### Tokens (splunk-security)\n\n| Operation | Recovery Method |\n|-----------|-----------------|\n| Delete token | Create new token; update all integrations |\n\n**Prevention**: Document all token usages. Use meaningful audience names.\n\n### Alerts (splunk-alert)\n\n| Operation | Recovery Method |\n|-----------|-----------------|\n| Delete | Recreate from documentation |\n| Acknowledge | Alert can re-trigger on next schedule |\n\n**Prevention**: Document alert configurations in version control.\n\n## Confirmation Patterns\n\n### For Warning Operations\n\n```\nWarning: This will modify <resource>.\nCurrent value: <current>\nNew value: <new>\nProceed? [y/N]\n```\n\n### For WarningWarning Operations\n\n```\nWARNING: This will delete <resource>.\nThis action may be recoverable from backups only.\nType the resource name to confirm:\n```\n\n### For WarningWarningWarning Operations\n\n```\nDANGER: This will PERMANENTLY delete <resource>.\nThis action is IRREVERSIBLE. All data will be lost.\nType \"I understand this is irreversible\" to confirm:\n```\n\n## Dry-Run Guidance\n\n### When to Use Dry-Run\n\n- Bulk operations affecting multiple resources\n- First-time execution of automated scripts\n- Production environment modifications\n- When uncertainty exists about scope\n\n### Dry-Run Patterns\n\n```bash\n# Preview what would be affected\nsplunk-as job list --filter \"status=running\"  # See before canceling all\n\n# Use search to preview\nsplunk-as search oneshot \"| rest /services/saved/searches | search disabled=0\"\n\n# Test SPL syntax first\nsplunk-as search validate \"your complex SPL query\"\n```\n\n## Emergency Procedures\n\n### Runaway Search Job\n\n```bash\n# List all running jobs\nsplunk-as job list --filter \"dispatchState=RUNNING\"\n\n# Cancel specific job\nsplunk-as job cancel <SID>\n\n# Cancel all your jobs (emergency)\nsplunk-as search oneshot \"| rest /services/search/jobs | search dispatchState=RUNNING | fields sid\" \\\n  | xargs -I {} splunk-as job cancel {}\n```\n\n### Accidental Token Deletion\n\n1. Immediately create a new token\n2. Update all affected integrations\n3. Document which integrations were affected\n4. Review audit logs for exposure window\n\n### Wrong Lookup Uploaded\n\n```bash\n# If you have backup\nsplunk-as lookup upload backup.csv --app search\n\n# If no backup, check Splunk's backup (if enabled)\n# Look in $SPLUNK_HOME/var/lib/splunk/kvstore/backup/\n```\n\n### App Causing Issues\n\n```bash\n# Disable first, don't uninstall\nsplunk-as app disable problematic_app\n\n# Restart Splunk if needed (admin only)\n# Then investigate before deciding on uninstall\n```\n\n## Audit and Logging\n\n### What Gets Logged\n\n| Action | Log Location | Retention |\n|--------|--------------|-----------|\n| Search execution | `index=_audit action=search` | Per policy |\n| Config changes | `index=_internal component=Conf*` | Per policy |\n| REST API calls | `index=_internal REST` | Per policy |\n| Authentication | `index=_audit action=login` | Per policy |\n\n### Review Before Destructive Actions\n\n```spl\n# Who else is using this saved search?\nindex=_audit action=search savedsearch_name=\"MySearch\"\n| stats count by user\n| sort -count\n\n# Recent modifications to resource\nindex=_internal component=ConfReplicationThread\n| search name=\"transforms-lookup/MyLookup\"\n```\n\n## Best Practices Summary\n\n1. **Always verify the target** - Double-check resource names and environments\n2. **Backup before modify** - Export configurations before changes\n3. **Start small** - Test with single items before bulk operations\n4. **Document changes** - Keep records of what was changed and why\n5. **Use dry-run** - Preview operations when available\n6. **Off-peak timing** - Schedule destructive operations during low-usage periods\n7. **Have rollback plan** - Know how to recover before you execute\n\n---\n\n<!-- PERMISSIONS\npermissions:\n  cli: splunk-as\n  operations:\n    # Safe - Read-only operations\n    - pattern: \"splunk-as search oneshot *\"\n      risk: safe\n    - pattern: \"splunk-as search validate *\"\n      risk: safe\n    - pattern: \"splunk-as job list *\"\n      risk: safe\n    - pattern: \"splunk-as job status *\"\n      risk: safe\n    - pattern: \"splunk-as job results *\"\n      risk: safe\n    - pattern: \"splunk-as metadata indexes *\"\n      risk: safe\n    - pattern: \"splunk-as metadata sources *\"\n      risk: safe\n    - pattern: \"splunk-as metadata sourcetypes *\"\n      risk: safe\n    - pattern: \"splunk-as savedsearch list *\"\n      risk: safe\n    - pattern: \"splunk-as savedsearch get *\"\n      risk: safe\n    - pattern: \"splunk-as lookup list *\"\n      risk: safe\n    - pattern: \"splunk-as lookup download *\"\n      risk: safe\n    - pattern: \"splunk-as app list *\"\n      risk: safe\n    - pattern: \"splunk-as app info *\"\n      risk: safe\n    - pattern: \"splunk-as security list *\"\n      risk: safe\n    - pattern: \"splunk-as metrics *\"\n      risk: safe\n    - pattern: \"splunk-as alert list *\"\n      risk: safe\n    - pattern: \"splunk-as kvstore list *\"\n      risk: safe\n    - pattern: \"splunk-as kvstore get *\"\n      risk: safe\n    - pattern: \"splunk-as export *\"\n      risk: safe\n\n    # Caution - Modifiable but easily reversible\n    - pattern: \"splunk-as job create *\"\n      risk: caution\n    - pattern: \"splunk-as job finalize *\"\n      risk: caution\n    - pattern: \"splunk-as job cancel *\"\n      risk: caution\n    - pattern: \"splunk-as job touch *\"\n      risk: caution\n    - pattern: \"splunk-as savedsearch create *\"\n      risk: caution\n    - pattern: \"splunk-as savedsearch update *\"\n      risk: caution\n    - pattern: \"splunk-as savedsearch enable *\"\n      risk: caution\n    - pattern: \"splunk-as savedsearch disable *\"\n      risk: caution\n    - pattern: \"splunk-as lookup upload *\"\n      risk: caution\n    - pattern: \"splunk-as tag *\"\n      risk: caution\n    - pattern: \"splunk-as app install *\"\n      risk: caution\n    - pattern: \"splunk-as app enable *\"\n      risk: caution\n    - pattern: \"splunk-as app disable *\"\n      risk: caution\n    - pattern: \"splunk-as security create *\"\n      risk: caution\n    - pattern: \"splunk-as alert acknowledge *\"\n      risk: caution\n    - pattern: \"splunk-as kvstore insert *\"\n      risk: caution\n    - pattern: \"splunk-as kvstore update *\"\n      risk: caution\n\n    # Warning - Destructive but potentially recoverable\n    - pattern: \"splunk-as job delete *\"\n      risk: warning\n    - pattern: \"splunk-as savedsearch delete *\"\n      risk: warning\n    - pattern: \"splunk-as lookup delete *\"\n      risk: warning\n    - pattern: \"splunk-as security delete *\"\n      risk: warning\n    - pattern: \"splunk-as alert delete *\"\n      risk: warning\n    - pattern: \"splunk-as kvstore delete *\"\n      risk: warning\n\n    # Danger - IRREVERSIBLE operations\n    - pattern: \"splunk-as app uninstall *\"\n      risk: danger\n    - pattern: \"splunk-as kvstore drop *\"\n      risk: danger\n-->\n",
        "skills/shared/tests/live_integration/README.md": "# Live Integration Tests\n\nThis directory contains the live integration test framework for testing Splunk skills against a real Splunk instance.\n\n## Overview\n\nThe framework supports two modes:\n1. **Docker Mode**: Automatically starts a Splunk container using testcontainers\n2. **External Mode**: Connects to an existing external Splunk instance\n\n## Prerequisites\n\n### Docker Mode\n- Docker installed and running\n- Python 3.8+\n- Dependencies: `pip install -r requirements.txt`\n\n### External Mode\n- Access to a Splunk instance with REST API enabled\n- Authentication token or username/password credentials\n\n## Quick Start\n\n### Using Docker (Default)\n\n```bash\n# Run all live integration tests\npytest skills/shared/tests/live_integration/ -v -m live\n\n# Run specific skill tests\npytest skills/splunk-search/tests/live_integration/ -v\npytest skills/splunk-job/tests/live_integration/ -v\n```\n\n### Using External Splunk\n\n```bash\n# Set environment variables\nexport SPLUNK_TEST_URL=https://your-splunk:8089\nexport SPLUNK_TEST_TOKEN=your-auth-token\n\n# Or with username/password\nexport SPLUNK_TEST_URL=https://your-splunk:8089\nexport SPLUNK_TEST_USERNAME=admin\nexport SPLUNK_TEST_PASSWORD=changeme\n\n# Run tests\npytest skills/splunk-search/tests/live_integration/ -v\n```\n\n## Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `SPLUNK_TEST_URL` | External Splunk management URL | (Docker auto-configured) |\n| `SPLUNK_TEST_TOKEN` | Bearer token for authentication | (Docker auto-generated) |\n| `SPLUNK_TEST_USERNAME` | Username for basic auth | `admin` |\n| `SPLUNK_TEST_PASSWORD` | Password for basic auth | (Docker: `testpassword123`) |\n| `SPLUNK_TEST_IMAGE` | Docker image for Splunk | `splunk/splunk:latest` |\n| `SPLUNK_TEST_INDEX` | Test index name | `splunk_skills_test` |\n\n## Command Line Options\n\n```bash\n# Override Splunk URL\npytest --splunk-url=https://splunk:8089\n\n# Override token\npytest --splunk-token=your-token\n\n# Skip slow tests\npytest --skip-slow\n\n# Custom Docker image\npytest --splunk-image=splunk/splunk:9.1.0\n```\n\n## Test Markers\n\n| Marker | Description |\n|--------|-------------|\n| `@pytest.mark.live` | Requires live Splunk connection |\n| `@pytest.mark.docker_required` | Requires Docker (skipped with external Splunk) |\n| `@pytest.mark.external_splunk` | Requires external Splunk (skipped with Docker) |\n| `@pytest.mark.slow_integration` | Slow tests (skippable with `--skip-slow`) |\n| `@pytest.mark.destructive` | Tests that modify Splunk configuration |\n\n## Architecture\n\n```\nlive_integration/\n├── __init__.py\n├── conftest.py          # Pytest configuration and marker registration\n├── fixtures.py          # Session-scoped fixtures\n├── splunk_container.py  # Docker container management\n├── test_utils.py        # Test data generation utilities\n└── README.md            # This file\n```\n\n## Fixtures\n\n### Connection Fixtures\n\n| Fixture | Scope | Description |\n|---------|-------|-------------|\n| `splunk_connection` | session | SplunkContainer or ExternalSplunkConnection |\n| `splunk_client` | session | Configured SplunkClient instance |\n| `splunk_info` | session | Server information dict |\n\n### Test Data Fixtures\n\n| Fixture | Scope | Description |\n|---------|-------|-------------|\n| `test_index` | session | Dedicated test index (auto-created/cleaned) |\n| `test_index_name` | session | Test index name string |\n| `test_data` | session | Synthetic test events (350 events) |\n| `fresh_test_data` | function | Fresh events unique to each test |\n\n### Helper Fixtures\n\n| Fixture | Scope | Description |\n|---------|-------|-------------|\n| `search_helper` | function | Simplified search operations |\n| `job_helper` | function | Search job management with auto-cleanup |\n\n## Test Data Generation\n\nTest data is generated using SPL `| makeresults` to avoid external file dependencies:\n\n```python\nfrom live_integration.test_utils import generate_test_events\n\ngenerate_test_events(\n    connection,\n    index=\"test_index\",\n    count=100,\n    fields={\n        \"sourcetype\": \"access_combined\",\n        \"host\": [\"web01\", \"web02\", \"web03\"],\n        \"status\": [200, 404, 500],\n    }\n)\n```\n\n### EventBuilder (Fluent API)\n\n```python\nfrom live_integration.test_utils import EventBuilder\n\nspl = (EventBuilder()\n    .with_count(100)\n    .with_index(\"test\")\n    .with_field(\"host\", [\"web01\", \"web02\"])\n    .with_field(\"status\", [200, 404, 500])\n    .build())\n```\n\n## Writing Tests\n\n### Basic Test Example\n\n```python\nimport pytest\nfrom live_integration.fixtures import splunk_client, test_index, test_data\n\nclass TestMyFeature:\n    @pytest.mark.live\n    def test_basic_search(self, splunk_client, test_index, test_data):\n        response = splunk_client.post(\n            \"/search/jobs/oneshot\",\n            data={\n                \"search\": f\"search index={test_index} | head 10\",\n                \"output_mode\": \"json\",\n            },\n            operation=\"test search\",\n        )\n\n        results = response.get(\"results\", [])\n        assert len(results) > 0\n```\n\n### Using Search Helper\n\n```python\n@pytest.mark.live\ndef test_with_helper(self, search_helper, test_index, test_data):\n    # Oneshot search\n    results = search_helper.oneshot(f\"search index={test_index} | head 5\")\n    assert len(results) == 5\n\n    # Count events\n    count = search_helper.count(f\"search index={test_index}\")\n    assert count > 0\n\n    # Check existence\n    assert search_helper.exists(f\"search index={test_index}\")\n```\n\n### Using Job Helper\n\n```python\n@pytest.mark.live\ndef test_with_job_helper(self, job_helper, test_index, test_data):\n    # Create async job\n    sid = job_helper.create(f\"search index={test_index} | stats count\")\n\n    # Wait for completion\n    status = job_helper.wait_for_done(sid, timeout=60)\n\n    assert status[\"isDone\"] is True\n    assert status[\"resultCount\"] > 0\n\n    # Cleanup is automatic via fixture teardown\n```\n\n## Troubleshooting\n\n### Docker Issues\n\n```bash\n# Check Docker is running\ndocker info\n\n# Pull Splunk image manually\ndocker pull splunk/splunk:latest\n\n# Check container logs (if stuck)\ndocker logs <container_id>\n```\n\n### Connection Issues\n\n```bash\n# Test external Splunk connectivity\ncurl -k https://your-splunk:8089/services/server/info \\\n  -H \"Authorization: Bearer your-token\"\n\n# Verify credentials\ncurl -k https://your-splunk:8089/services/server/info \\\n  -u admin:password\n```\n\n### Common Errors\n\n| Error | Solution |\n|-------|----------|\n| \"Docker not available\" | Ensure Docker daemon is running |\n| \"No Splunk connection\" | Set `SPLUNK_TEST_URL` or start Docker |\n| \"Authentication failed\" | Check token/credentials |\n| \"Index not found\" | Index will be auto-created; check permissions |\n| \"Timeout waiting for events\" | Increase `wait_for_indexing` timeout |\n\n## Performance Tips\n\n1. **Use session-scoped fixtures**: Reuses the same Splunk container across all tests\n2. **Skip slow tests during development**: `pytest --skip-slow`\n3. **Use external Splunk for CI**: Faster than starting Docker each time\n4. **Limit test data**: Use `test_data` for shared data, `fresh_test_data` only when isolation is needed\n\n## CI/CD Integration\n\nExample GitHub Actions workflow:\n\n```yaml\njobs:\n  integration-tests:\n    runs-on: ubuntu-latest\n    services:\n      splunk:\n        image: splunk/splunk:latest\n        ports:\n          - 8089:8089\n        env:\n          SPLUNK_START_ARGS: --accept-license\n          SPLUNK_PASSWORD: testpassword123\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run integration tests\n        env:\n          SPLUNK_TEST_URL: https://localhost:8089\n          SPLUNK_TEST_USERNAME: admin\n          SPLUNK_TEST_PASSWORD: testpassword123\n        run: |\n          pip install -r requirements.txt\n          pytest skills/*/tests/live_integration/ -v -m live\n```\n",
        "skills/splunk-alert/SKILL.md": "# splunk-alert\n\nAlert triggering, monitoring, and notification management for Splunk.\n\n## Purpose\n\nCreate and manage alerts, monitor triggered alerts, and configure alert actions.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| List alerts | - | Read-only |\n| Get alert details | - | Read-only |\n| List triggered alerts | - | Read-only |\n| Create alert | ⚠️ | May trigger notifications |\n| Update alert | ⚠️ | Previous config lost |\n| Acknowledge alert | ⚠️ | Can be re-triggered |\n| Delete alert | ⚠️⚠️ | May be recoverable from backup |\n\n## Triggers\n\n- \"alert\", \"trigger\", \"notification\"\n- \"monitor\", \"alerting\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `create_alert.py` | Create alert from saved search |\n| `get_alert.py` | Get alert configuration |\n| `list_alerts.py` | List configured alerts |\n| `get_triggered_alerts.py` | List triggered instances |\n| `acknowledge_alert.py` | Acknowledge triggered alert |\n\n## Examples\n\n```bash\n# Create an alert\nsplunk-as alert create \"High Error Rate\" \\\n  \"index=main sourcetype=app_logs error | stats count\" \\\n  --alert-type \"number of events\" \\\n  --alert-comparator \"greater than\" \\\n  --alert-threshold 100 \\\n  --severity 4 \\\n  --cron \"*/5 * * * *\" \\\n  --actions email \\\n  --email-to ops@example.com\n\n# List all configured alerts\nsplunk-as alert list --app search --count 100\n\n# Get specific alert details\nsplunk-as alert get alert_12345\n\n# List triggered alert instances with filters\nsplunk-as alert triggered --severity 4\nsplunk-as alert triggered --savedsearch \"High Error Rate\"\nsplunk-as alert triggered --app search --count 20\n\n# Acknowledge/delete a triggered alert\nsplunk-as alert acknowledge alert_12345 --force\n```\n\n## Alert Configuration\n\n### Severity Levels\n\n- 1 = debug\n- 2 = info\n- 3 = warn (default)\n- 4 = error\n- 5 = severe\n- 6 = fatal\n\n### Alert Types\n\n- `always` - Trigger on every scheduled execution\n- `number of events` - Trigger when result count meets condition\n- `number of hosts` - Trigger when host count meets condition\n- `number of sources` - Trigger when source count meets condition\n- `custom` - Custom alert condition\n\n### Alert Comparators\n\n- `greater than`\n- `less than`\n- `equal to`\n- `not equal to`\n- `drops by`\n- `rises by`\n\n### Alert Actions\n\n- `email` - Send email notification\n- `webhook` - HTTP POST to webhook URL\n- `script` - Execute custom script\n- Custom actions configured in Splunk\n\n## API Endpoints\n\n- `GET /services/alerts/fired_alerts` - List triggered alerts\n- `GET /services/alerts/fired_alerts/{name}` - Get specific triggered alert\n- `DELETE /services/alerts/fired_alerts/{name}` - Acknowledge/delete triggered alert\n- `POST /servicesNS/nobody/{app}/saved/searches` - Create alert (via saved search)\n- `GET /servicesNS/nobody/{app}/saved/searches` - List alert configurations\n",
        "skills/splunk-app/SKILL.md": "# splunk-app\n\nSplunk application management.\n\n## Purpose\n\nInstall, uninstall, enable, disable, and manage Splunk applications.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| List apps | - | Read-only |\n| Get app details | - | Read-only |\n| Enable app | ⚠️ | Easily reversible |\n| Disable app | ⚠️ | Easily reversible |\n| Install app | ⚠️⚠️ | May affect system behavior |\n| Uninstall app | ⚠️⚠️⚠️ | **IRREVERSIBLE** - app files deleted |\n\n## Triggers\n\n- \"app\", \"application\", \"install\"\n- \"package\", \"addon\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `list_apps.py` | List installed apps |\n| `get_app.py` | Get app details |\n| `install_app.py` | Install app from file |\n| `uninstall_app.py` | Remove app |\n| `enable_app.py` | Enable disabled app |\n| `disable_app.py` | Disable app |\n\n## Examples\n\n```bash\n# List installed apps\nsplunk-as app list\n\n# Get app details\nsplunk-as app get search\n\n# Install app\nsplunk-as app install my_app.tgz\n\n# Uninstall app\nsplunk-as app uninstall my_app\n\n# Enable app\nsplunk-as app enable my_app\n\n# Disable app\nsplunk-as app disable my_app\n```\n\n## API Endpoints\n\n- `GET/POST /services/apps/local` - List/Install\n- `GET/POST/DELETE /services/apps/local/{name}` - CRUD\n- `POST /services/apps/local/{name}/package` - Export\n",
        "skills/splunk-assistant/SKILL.md": "# splunk-assistant\n\nCentral hub and router for Splunk Assistant Skills. Implements 3-level progressive disclosure for optimal Splunk interaction.\n\n## Purpose\n\nRoutes natural language requests to specialized Splunk skills based on intent. Provides connection verification, authentication validation, and execution strategy recommendations.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| Get server info | - | Read-only |\n| Verify connection | - | Read-only |\n| Route to skill | - | Navigation only |\n\n## Triggers\n\n- Any Splunk-related request\n- \"splunk\", \"search\", \"query\", \"SPL\"\n- Connection/authentication issues\n- General Splunk questions\n\n## Progressive Disclosure\n\n### Level 1: Essential Connection & Identification\n\n- Verify Search Head connection on management port 8089 via HTTPS\n- Validate JWT Bearer token or Basic Auth credentials\n- Detect deployment type (Cloud vs on-prem)\n- Route to appropriate specialized skill\n\n### Level 2: Execution Mode Strategy\n\n| Mode | Use Case | Characteristics |\n|------|----------|-----------------|\n| Oneshot | Ad-hoc queries | Results inline, no SID, minimal disk I/O |\n| Normal | Long searches | Returns SID, poll for results, progress tracking |\n| Blocking | Simple queries | Waits for completion, synchronous |\n| Export | Large extracts | Streaming, checkpoint support, ETL |\n\n### Level 3: Advanced Optimization & Resource Governance\n\n- **Time Modifiers**: Always enforce `earliest_time` and `latest_time`\n- **Field Reduction**: Insert `fields` command to limit data transfer\n- **Resource Cleanup**: Issue `/control/cancel` after results consumed\n- **Error Handling**: Use `strict=true` for clear errors vs incomplete data\n\n## Skill Routing\n\n| Intent | Route To |\n|--------|----------|\n| Execute SPL query | `splunk-search` |\n| Job lifecycle management | `splunk-job` |\n| Large data export | `splunk-export` |\n| Index/source discovery | `splunk-metadata` |\n| Lookup management | `splunk-lookup` |\n| Tag operations | `splunk-tag` |\n| Saved searches/reports | `splunk-savedsearch` |\n| Alert management | `splunk-alert` |\n| REST configuration | `splunk-rest-admin` |\n| Token/RBAC/ACL | `splunk-security` |\n| Metrics (mstats) | `splunk-metrics` |\n| App management | `splunk-app` |\n| KV Store | `splunk-kvstore` |\n\n## Connection Verification\n\n```bash\n# Get server information (verify connection)\nsplunk-as admin info --profile production\n\n# Get server health status\nsplunk-as admin health --profile production\n```\n\n## Examples\n\n### Verify Connection\n\n```bash\nsplunk-as admin info\n# Output:\n# ✓ Connected to splunk.example.com:8089\n# ✓ Authentication: Bearer token valid\n# ✓ Deployment: Splunk Enterprise 9.1.0\n# ✓ User: admin (capabilities: search, admin_all_objects)\n```\n\n### Get Server Info\n\n```bash\nsplunk-as admin info --output json\n# Output: Server version, build, OS, cluster status, etc.\n```\n\n### Common CLI Commands\n\n```bash\n# Search commands\nsplunk-as search oneshot \"index=main | head 10\"\nsplunk-as search normal \"index=main | stats count\" --wait\n\n# Job management\nsplunk-as job list\nsplunk-as job status 1703779200.12345\n\n# Metadata discovery\nsplunk-as metadata indexes\nsplunk-as metadata sourcetypes --index main\n\n# Security\nsplunk-as security whoami\n```\n\n## Best Practices\n\n1. **Always include time bounds** - Prevent full index scans\n2. **Use field extraction** - Limit data transfer\n3. **Choose appropriate mode** - Oneshot for ad-hoc, Export for ETL\n4. **Clean up resources** - Cancel jobs when done\n5. **Handle errors gracefully** - Use the error hierarchy\n\n## Related Skills\n\n- [splunk-job](../splunk-job/SKILL.md) - Job lifecycle\n- [splunk-search](../splunk-search/SKILL.md) - Query execution\n- [splunk-security](../splunk-security/SKILL.md) - Authentication\n",
        "skills/splunk-export/SKILL.md": "# splunk-export\n\nHigh-volume streaming data extraction for Splunk.\n\n## Purpose\n\nExport large result sets (>50,000 rows) efficiently using streaming.\nSupports checkpoint-based resume for reliability during long exports.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| Export results | - | Read-only |\n| Export raw events | - | Read-only |\n| Estimate size | - | Read-only |\n| Export with checkpoint | - | Read-only (writes local file) |\n\n## Triggers\n\n- \"export\", \"download\", \"extract\"\n- \"stream\", \"large results\", \"ETL\"\n- \"backup\", \"archive\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `export_results.py` | Stream results to file (CSV/JSON/XML) |\n| `export_raw.py` | Export raw events |\n| `export_with_checkpoint.py` | Resume-capable export |\n| `estimate_export_size.py` | Preview result count |\n\n## Examples\n\n### Basic Export\n\n```bash\n# Export to CSV\nsplunk-as export results 1703779200.12345 --output-file results.csv\n\n# Export to JSON\nsplunk-as export results 1703779200.12345 --format json --output-file data.json\n\n# Export raw events\nsplunk-as export raw 1703779200.12345 --output-file events.json\n```\n\n### Large Export with Checkpoints\n\n```bash\n# Start export with checkpoint file\nsplunk-as export checkpoint \"index=main\" --output large_export.csv --checkpoint export.ckpt\n\n# Resume interrupted export\nsplunk-as export checkpoint --resume export.ckpt\n```\n\n### Estimate Size\n\n```bash\n# Preview count before export\nsplunk-as export estimate \"index=main | stats count by host\" --earliest -7d\n# Output: Estimated 1,234,567 results\n```\n\n## API Endpoints\n\n| Endpoint | Description |\n|----------|-------------|\n| `GET /services/search/v2/jobs/{sid}/results` | Stream results |\n| `GET /services/search/v2/jobs/{sid}/events` | Stream raw events |\n\n## Parameters\n\n| Parameter | Description |\n|-----------|-------------|\n| `count=0` | Return all results (no limit) |\n| `output_mode` | csv, json, xml, raw |\n| `field_list` | Comma-separated fields |\n\n## Best Practices\n\n1. **Use streaming** for >50K results\n2. **Enable checkpoints** for large exports\n3. **Limit fields** to reduce data transfer\n4. **Monitor progress** for long-running exports\n5. **Compress output** for storage efficiency\n\n## Related Skills\n\n- [splunk-search](../splunk-search/SKILL.md) - Query execution\n- [splunk-job](../splunk-job/SKILL.md) - Job management\n",
        "skills/splunk-job/SKILL.md": "# splunk-job\n\nSearch job lifecycle orchestration for Splunk.\n\n## Purpose\n\nManage the complete lifecycle of Splunk search jobs including creation, monitoring, control actions (pause/cancel/finalize), and cleanup.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| Get job status | - | Read-only |\n| List jobs | - | Read-only |\n| Create job | - | Easily reversible via cancel |\n| Pause/unpause job | ⚠️ | Can be undone |\n| Finalize job | ⚠️ | Returns partial results |\n| Cancel job | ⚠️ | Stops execution |\n| Delete job | ⚠️⚠️ | Removes job and results |\n\n## Triggers\n\n- \"job\", \"search job\", \"SID\"\n- \"status\", \"progress\", \"state\"\n- \"cancel\", \"pause\", \"unpause\", \"finalize\"\n- \"list jobs\", \"delete job\"\n\n## Job States (dispatchState)\n\n```\nQUEUED → PARSING → RUNNING → FINALIZING → DONE\n                                        → FAILED\n                 → PAUSED (on pause action)\n```\n\n| State | Description |\n|-------|-------------|\n| QUEUED | Job waiting in queue |\n| PARSING | SPL being parsed |\n| RUNNING | Search executing |\n| FINALIZING | Results being finalized |\n| DONE | Completed successfully |\n| FAILED | Error occurred |\n| PAUSED | Paused by user |\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `create_job.py` | Create search job, return SID |\n| `get_job_status.py` | Get dispatchState, progress, stats |\n| `poll_job.py` | Wait for job completion with timeout |\n| `cancel_job.py` | Issue /control/cancel action |\n| `pause_job.py` | Issue /control/pause action |\n| `unpause_job.py` | Issue /control/unpause action |\n| `finalize_job.py` | Issue /control/finalize action |\n| `set_job_ttl.py` | Extend job time-to-live |\n| `list_jobs.py` | List all search jobs for user |\n| `delete_job.py` | Remove job from dispatch directory |\n\n## Examples\n\n### Create and Monitor Job\n\n```bash\n# Create job\nsplunk-as job create \"index=main | stats count by sourcetype\" --earliest -1h\n# Output: Job created: 1703779200.12345\n\n# Check status\nsplunk-as job status 1703779200.12345\n# Output: State: RUNNING, Progress: 45%, Events: 12345\n\n# Wait for completion\nsplunk-as job poll 1703779200.12345 --timeout 300\n# Output: Job completed: DONE, Results: 42\n```\n\n### Job Control\n\n```bash\n# Pause running job\nsplunk-as job pause 1703779200.12345\n\n# Resume paused job\nsplunk-as job unpause 1703779200.12345\n\n# Cancel job\nsplunk-as job cancel 1703779200.12345\n\n# Finalize (stop and return current results)\nsplunk-as job finalize 1703779200.12345\n```\n\n### Job Management\n\n```bash\n# List all jobs\nsplunk-as job list\n# Output: Table of active jobs with status\n\n# Extend TTL\nsplunk-as job set-ttl 1703779200.12345 --ttl 3600\n\n# Delete job\nsplunk-as job delete 1703779200.12345\n```\n\n## API Endpoints\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/services/search/v2/jobs` | POST | Create job |\n| `/services/search/v2/jobs/{sid}` | GET | Get job status |\n| `/services/search/v2/jobs/{sid}/control` | POST | Control actions |\n| `/services/search/jobs` | GET | List jobs |\n| `/services/search/jobs/{sid}` | DELETE | Delete job |\n\n## Control Actions\n\n```python\n# Available actions for /control endpoint\nactions = ['cancel', 'pause', 'unpause', 'finalize', 'touch', 'setttl', 'enablepreview', 'disablepreview']\n\n# POST /services/search/v2/jobs/{sid}/control\n# data={'action': 'cancel'}\n```\n\n## Job Properties\n\n| Property | Description |\n|----------|-------------|\n| `sid` | Search job ID |\n| `dispatchState` | Current state |\n| `doneProgress` | Completion 0.0-1.0 |\n| `eventCount` | Events scanned |\n| `resultCount` | Results produced |\n| `scanCount` | Buckets scanned |\n| `runDuration` | Execution time |\n| `ttl` | Time to live |\n| `isFailed` | Failure flag |\n| `isPaused` | Pause flag |\n\n## Best Practices\n\n1. **Always set time bounds** in the search query\n2. **Use appropriate timeout** for poll_job.py\n3. **Cancel jobs** when results are no longer needed\n4. **Monitor progress** for long-running searches\n5. **Extend TTL** for jobs you need to keep\n\n## Related Skills\n\n- [splunk-search](../splunk-search/SKILL.md) - Query execution\n- [splunk-export](../splunk-export/SKILL.md) - Result extraction\n",
        "skills/splunk-kvstore/SKILL.md": "# splunk-kvstore\n\nInteraction with App Key Value Store for persistent metadata.\n\n## Purpose\n\nCreate and manage KV store collections and records for persistent data storage.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| List collections | - | Read-only |\n| Get record | - | Read-only |\n| Query collection | - | Read-only |\n| Insert record | ⚠️ | Easily reversible |\n| Create collection | ⚠️ | Easily reversible |\n| Update record | ⚠️ | Previous value lost |\n| Delete record | ⚠️⚠️ | Data loss, may be in backups |\n| Delete collection | ⚠️⚠️⚠️ | **IRREVERSIBLE** - all data lost |\n\n## Triggers\n\n- \"kvstore\", \"collection\", \"key-value\"\n- \"persist\", \"store\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `create_collection.py` | Create KV store collection |\n| `delete_collection.py` | Delete collection |\n| `list_collections.py` | List collections in app |\n| `insert_record.py` | Insert record into collection |\n| `get_record.py` | Get record by _key |\n| `update_record.py` | Update existing record |\n| `delete_record.py` | Delete record |\n| `query_collection.py` | Query with filters |\n\n## Examples\n\n```bash\n# List collections\nsplunk-as kvstore list --app search\n\n# Create collection\nsplunk-as kvstore create my_collection --app search\n\n# Insert record\nsplunk-as kvstore insert my_collection '{\"name\": \"test\", \"value\": 123}'\n\n# Get record\nsplunk-as kvstore get my_collection abc123\n\n# Query collection\nsplunk-as kvstore query my_collection --filter '{\"name\": \"test\"}'\n\n# Update record\nsplunk-as kvstore update my_collection abc123 '{\"name\": \"updated\"}'\n\n# Delete record\nsplunk-as kvstore delete-record my_collection abc123\n\n# Delete collection\nsplunk-as kvstore delete my_collection --app search\n```\n\n## API Endpoints\n\n- `GET/POST/DELETE /services/storage/collections/config` - Collections\n- `GET/POST /services/storage/collections/data/{collection}` - Records\n- `GET/PUT/DELETE /services/storage/collections/data/{collection}/{key}` - Record\n\n## SPL Patterns\n\n```spl\n| inputlookup collection_name\n| outputlookup collection_name append=true\n```\n",
        "skills/splunk-lookup/SKILL.md": "# splunk-lookup\n\nCSV and lookup file management for Splunk.\n\n## Purpose\n\nUpload, download, and manage CSV lookup files and lookup definitions.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| List lookups | - | Read-only |\n| Get lookup info | - | Read-only |\n| Download lookup | - | Read-only |\n| Upload lookup | ⚠️ | Creates new or overwrites |\n| Create lookup definition | ⚠️ | Can be undone |\n| Delete lookup | ⚠️⚠️ | May be recoverable from backup |\n\n## Triggers\n\n- \"lookup\", \"CSV\", \"upload\"\n- \"lookup table\", \"enrichment\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `upload_lookup.py` | Upload CSV lookup file |\n| `download_lookup.py` | Download lookup file |\n| `list_lookups.py` | List lookup files in app |\n| `delete_lookup.py` | Remove lookup file |\n| `create_lookup_definition.py` | Create lookup-table stanza |\n\n## Examples\n\n```bash\n# List lookups\nsplunk-as lookup list --app search\n\n# Get lookup info\nsplunk-as lookup get users.csv --app search\n\n# Upload lookup\nsplunk-as lookup upload users.csv --app search\n\n# Download lookup\nsplunk-as lookup download users.csv --output-file ./users.csv\n\n# Delete lookup\nsplunk-as lookup delete users.csv --app search\n```\n\n## API Endpoints\n\n- `POST /services/data/lookup-table-files` - Upload\n- `GET /services/data/lookup-table-files` - List\n- `GET/DELETE /services/data/lookup-table-files/{name}` - Get/Delete\n",
        "skills/splunk-metadata/SKILL.md": "# splunk-metadata\n\nQuery index, source, and sourcetype configurations for Splunk.\n\n## Purpose\n\nDiscover and explore metadata about indexes, sources, sourcetypes, and fields.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| List indexes | - | Read-only |\n| Get index info | - | Read-only |\n| List sources | - | Read-only |\n| List sourcetypes | - | Read-only |\n| Get field summary | - | Read-only |\n| Metadata search | - | Read-only |\n\n## Triggers\n\n- \"metadata\", \"index\", \"source\", \"sourcetype\"\n- \"fields\", \"discovery\", \"catalog\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `list_indexes.py` | List available indexes |\n| `get_index_info.py` | Index size, event count, time range |\n| `list_sources.py` | Unique sources per index |\n| `list_sourcetypes.py` | Sourcetypes in use |\n| `metadata_search.py` | Execute `\\| metadata` search |\n| `get_field_summary.py` | Field summary for index/sourcetype |\n\n## Examples\n\n```bash\n# List all indexes\nsplunk-as metadata indexes\n\n# Get index details\nsplunk-as metadata index-info main\n\n# List sourcetypes\nsplunk-as metadata sourcetypes --index main\n\n# List sources\nsplunk-as metadata sources --index main\n\n# Field summary\nsplunk-as metadata fields --index main --sourcetype access_combined\n\n# Metadata search\nsplunk-as metadata search --type sourcetypes --index main\n```\n\n## SPL Patterns\n\n```spl\n# Metadata command\n| metadata type=sourcetypes index=main\n\n# Metasearch\n| metasearch index=* sourcetype=access_combined\n\n# Field summary\n| fieldsummary maxvals=100\n```\n\n## Related Skills\n\n- [splunk-search](../splunk-search/SKILL.md) - Query execution\n",
        "skills/splunk-metrics/SKILL.md": "# splunk-metrics\n\nReal-time metrics and data point analysis for Splunk.\n\n## Purpose\n\nQuery and analyze metrics data using mstats and mcatalog commands.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| List metrics | - | Read-only |\n| List metric indexes | - | Read-only |\n| Query with mstats | - | Read-only |\n| Discover with mcatalog | - | Read-only |\n\n## Triggers\n\n- \"metrics\", \"mstats\", \"mcatalog\"\n- \"time series\", \"data points\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `mstats.py` | Execute mstats command |\n| `mcatalog.py` | Query metrics catalog |\n| `list_metric_indexes.py` | List metric indexes |\n| `list_metrics.py` | List metric names |\n\n## Examples\n\n```bash\n# List metrics\nsplunk-as metrics list --index metrics\n\n# List metric indexes\nsplunk-as metrics indexes\n\n# Query with mstats\nsplunk-as metrics mstats cpu.percent --agg avg --by host --span 1h\n\n# Discover metrics with mcatalog\nsplunk-as metrics mcatalog --index metrics --filter \"cpu.*\"\n```\n\n## SPL Patterns\n\n```spl\n| mstats avg(cpu.percent) WHERE index=metrics BY host span=1h\n| mcatalog values(metric_name) WHERE index=metrics\n| mpreview index=metrics\n```\n",
        "skills/splunk-rest-admin/SKILL.md": "# splunk-rest-admin\n\nProgrammatic access to internal configurations via REST command.\n\n## Purpose\n\nQuery and manage Splunk server configurations, users, roles, and system info.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| REST GET request | - | Read-only |\n| Get server info | - | Read-only |\n| List users/roles | - | Read-only |\n| REST POST request | ⚠️⚠️ | May modify server config |\n\n## Triggers\n\n- \"rest\", \"admin\", \"config\"\n- \"server\", \"settings\", \"info\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `rest_get.py` | GET any REST endpoint |\n| `rest_post.py` | POST to REST endpoint |\n| `get_server_info.py` | Server version, build, features |\n| `list_users.py` | List Splunk users |\n| `list_roles.py` | List Splunk roles |\n\n## Examples\n\n```bash\n# Get server info\nsplunk-as admin info\n\n# Get server status\nsplunk-as admin status\n\n# Get server health\nsplunk-as admin health\n\n# List users\nsplunk-as admin list-users\n\n# List roles\nsplunk-as admin list-roles\n\n# REST GET request\nsplunk-as admin rest-get /services/authentication/users\n\n# REST POST request\nsplunk-as admin rest-post /services/saved/searches -d '{\"name\": \"test\"}'\n```\n\n## SPL Patterns\n\n```spl\n| rest /services/server/info\n| rest /services/authentication/users\n| rest /services/admin/conf-times\n```\n",
        "skills/splunk-savedsearch/SKILL.md": "# splunk-savedsearch\n\nCRUD for reports and scheduled searches in Splunk.\n\n## Purpose\n\nCreate, read, update, delete saved searches, reports, and scheduled searches.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| List saved searches | - | Read-only |\n| Get saved search | - | Read-only |\n| Run saved search | - | Read-only execution |\n| Create saved search | ⚠️ | Can be deleted |\n| Update saved search | ⚠️ | Previous version lost |\n| Enable/disable schedule | ⚠️ | Easily reversible |\n| Delete saved search | ⚠️⚠️ | May be recoverable from backup |\n\n## Triggers\n\n- \"saved search\", \"report\", \"schedule\"\n- \"scheduled search\", \"alert\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `create_savedsearch.py` | Create saved search/report |\n| `get_savedsearch.py` | Get saved search details |\n| `update_savedsearch.py` | Modify saved search |\n| `delete_savedsearch.py` | Delete saved search |\n| `list_savedsearches.py` | List saved searches in app |\n| `run_savedsearch.py` | Execute saved search on-demand |\n| `enable_schedule.py` | Enable scheduled execution |\n| `disable_schedule.py` | Disable scheduling |\n\n## Examples\n\n```bash\n# List saved searches\nsplunk-as savedsearch list --app search\n\n# Get saved search details\nsplunk-as savedsearch get \"My Report\"\n\n# Create saved search\nsplunk-as savedsearch create \"My Report\" \"index=main | stats count\" --app search\n\n# Update saved search\nsplunk-as savedsearch update \"My Report\" --search \"index=main | stats count by host\"\n\n# Run saved search\nsplunk-as savedsearch run \"My Report\" --wait\n\n# Enable scheduling\nsplunk-as savedsearch enable \"My Report\"\n\n# Disable scheduling\nsplunk-as savedsearch disable \"My Report\"\n\n# Delete saved search\nsplunk-as savedsearch delete \"My Report\"\n```\n\n## API Endpoints\n\n- `GET/POST /services/saved/searches` - CRUD\n- `POST /services/saved/searches/{name}/dispatch` - Run\n- `GET /services/saved/searches/{name}/history` - History\n",
        "skills/splunk-search/SKILL.md": "# splunk-search\n\nSPL query execution in multiple modes for Splunk.\n\n## Purpose\n\nExecute SPL (Search Processing Language) queries using various execution modes:\noneshot (inline results), normal (async with polling), and blocking (sync wait).\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| Execute search (read) | - | Read-only query |\n| Get results | - | Read-only |\n| Validate SPL | - | Read-only |\n| Execute search (write) | ⚠️⚠️ | SPL with `| outputlookup` or `| collect` modifies data |\n\n## Triggers\n\n- \"search\", \"SPL\", \"query\", \"find\"\n- \"oneshot\", \"blocking\", \"async\"\n- \"execute\", \"run search\"\n\n## Search Modes\n\n| Mode | Use Case | Returns SID | Wait for Results |\n|------|----------|-------------|------------------|\n| Oneshot | Ad-hoc queries < 50K rows | No | Inline |\n| Normal | Long-running searches | Yes | Async (poll) |\n| Blocking | Simple queries | Yes | Sync (waits) |\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `search_oneshot.py` | Execute oneshot search (results inline) |\n| `search_normal.py` | Execute normal search (returns SID) |\n| `search_blocking.py` | Execute blocking search (waits) |\n| `get_results.py` | Get results from completed job |\n| `get_preview.py` | Get partial results during search |\n| `validate_spl.py` | Validate SPL syntax |\n\n## Examples\n\n### Oneshot Search (Recommended for Ad-hoc)\n\n```bash\n# Simple search\nsplunk-as search oneshot \"index=main | stats count by sourcetype\"\n\n# With time range\nsplunk-as search oneshot \"index=main | head 100\" --earliest -1h --latest now\n\n# Output as JSON\nsplunk-as search oneshot \"index=main | top host\" --output json\n```\n\n### Normal Search (Async)\n\n```bash\n# Create job and poll\nsplunk-as search normal \"index=main | stats count\" --wait\n\n# Create job only (returns SID)\nsplunk-as search normal \"index=main | stats count\"\n# Then use: splunk-as search results <SID>\n```\n\n### Blocking Search (Sync)\n\n```bash\n# Wait for completion and return results\nsplunk-as search blocking \"index=main | head 10\" --timeout 60\n```\n\n### Get Results\n\n```bash\n# From completed job\nsplunk-as search results 1703779200.12345\n\n# With pagination\nsplunk-as search results 1703779200.12345 --count 100 --offset 0\n\n# Specific fields only\nsplunk-as search results 1703779200.12345 --fields host,status,uri\n```\n\n### Validate SPL\n\n```bash\n# Validate SPL syntax\nsplunk-as search validate \"index=main | stats count\"\n```\n\n## API Endpoints\n\n| Endpoint | Mode | Description |\n|----------|------|-------------|\n| `POST /services/search/jobs/oneshot` | Oneshot | Inline results |\n| `POST /services/search/v2/jobs` | Normal | Create async job |\n| `POST /services/search/v2/jobs` + `exec_mode=blocking` | Blocking | Sync wait |\n| `GET /services/search/v2/jobs/{sid}/results` | - | Get results |\n| `GET /services/search/v2/jobs/{sid}/results_preview` | - | Get preview |\n\n## Request Parameters\n\n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `search` | SPL query | Required |\n| `earliest_time` | Start time | -24h |\n| `latest_time` | End time | now |\n| `exec_mode` | normal/blocking | normal |\n| `max_count` | Max results | 50000 |\n| `output_mode` | json/csv/xml | json |\n\n## Best Practices\n\n1. **Always include time bounds** - Prevents full index scans\n2. **Use oneshot for ad-hoc** - Minimal resource usage\n3. **Add fields command** - Reduce data transfer\n4. **Validate SPL first** - Catch syntax errors early\n5. **Handle pagination** - Use count/offset for large results\n\n## SPL Quick Reference\n\n```spl\n# Basic search with time\nindex=main earliest=-1h | head 100\n\n# Statistics\nindex=main | stats count by status | sort -count\n\n# Time chart\nindex=main | timechart span=1h count by sourcetype\n\n# Field extraction\nindex=main | fields host, status, uri | table host status uri\n\n# Filtering\nindex=main status>=400 | stats count by status\n\n# Subsearch\nindex=main [search index=alerts | fields src_ip | head 100]\n```\n\n## Related Skills\n\n- [splunk-job](../splunk-job/SKILL.md) - Job lifecycle\n- [splunk-export](../splunk-export/SKILL.md) - Large exports\n- [splunk-metadata](../splunk-metadata/SKILL.md) - Field discovery\n",
        "skills/splunk-security/SKILL.md": "# splunk-security\n\nToken management, RBAC, and ACL verification for Splunk.\n\n## Purpose\n\nManage JWT tokens, check permissions, and configure ACLs on knowledge objects.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| Get current user | - | Read-only |\n| List users/roles | - | Read-only |\n| List tokens | - | Read-only |\n| Get capabilities | - | Read-only |\n| Check permission | - | Read-only |\n| Get ACL | - | Read-only |\n| Create token | ⚠️ | Security credential created |\n| Delete token | ⚠️⚠️ | **Breaks dependent integrations** |\n| Modify ACL | ⚠️⚠️ | Changes access permissions |\n\n## Triggers\n\n- \"token\", \"permission\", \"ACL\"\n- \"security\", \"RBAC\", \"role\"\n- \"access\", \"capabilities\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `create_token.py` | Create new JWT token |\n| `list_tokens.py` | List tokens for user |\n| `delete_token.py` | Revoke token |\n| `get_capabilities.py` | Get user capabilities |\n| `check_permission.py` | Verify access to resource |\n| `get_acl.py` | Get ACL for knowledge object |\n\n## Examples\n\n```bash\n# Get current user info\nsplunk-as security whoami\n\n# List users\nsplunk-as security list-users\n\n# List roles\nsplunk-as security list-roles\n\n# List tokens\nsplunk-as security list-tokens\n\n# Create token\nsplunk-as security create-token --audience \"my-app\" --expires 30d\n\n# Delete token\nsplunk-as security delete-token token_123\n\n# Get capabilities\nsplunk-as security capabilities --user admin\n\n# Check permission\nsplunk-as security check-permission --object saved/searches/MySearch\n\n# Get ACL\nsplunk-as security acl saved/searches/MySearch\n```\n\n## API Endpoints\n\n- `GET/POST/DELETE /services/authorization/tokens` - Tokens\n- `GET/POST /services/data/transforms/lookups/{name}/acl` - ACL\n",
        "skills/splunk-tag/SKILL.md": "# splunk-tag\n\nKnowledge object tags and field/value associations for Splunk.\n\n## Purpose\n\nAdd, remove, and manage tags associated with field values for easier searching.\n\n## Risk Levels\n\n| Operation | Risk | Notes |\n|-----------|------|-------|\n| List tags | - | Read-only |\n| Search by tag | - | Read-only |\n| Add tag | ⚠️ | Easily reversible |\n| Remove tag | ⚠️ | Easily reversible |\n\n## Triggers\n\n- \"tag\", \"label\", \"classify\"\n- \"tag field\", \"add tag\"\n\n## Scripts\n\n| Script | Description |\n|--------|-------------|\n| `add_tag.py` | Add tag to field value |\n| `remove_tag.py` | Remove tag from field value |\n| `list_tags.py` | List all tags |\n| `search_by_tag.py` | Search using tag= syntax |\n\n## Examples\n\n```bash\n# List all tags\nsplunk-as tag list\n\n# Add tag to field value\nsplunk-as tag add production host webserver01\n\n# Remove tag from field value\nsplunk-as tag remove production host webserver01\n\n# Search by tag\nsplunk-as tag search production --earliest -1h\n```\n\n## SPL Patterns\n\n```spl\ntag=web_traffic\ntag::src_ip=internal\n```\n",
        "tests/e2e/README.md": "# E2E Tests for splunk-assistant-skills\n\nEnd-to-end tests that validate the plugin by interacting with the Claude Code CLI.\n\n## Prerequisites\n\n### Authentication (Choose One)\n\n**Option 1: API Key**\n```bash\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n```\n\n**Option 2: OAuth**\n```bash\nclaude auth login\n```\n\n## Quick Start\n\n```bash\n# Run all tests\n./scripts/run-e2e-tests.sh\n\n# Run locally (no Docker)\n./scripts/run-e2e-tests.sh --local\n\n# Verbose output\n./scripts/run-e2e-tests.sh --verbose\n\n# Debug shell\n./scripts/run-e2e-tests.sh --shell\n```\n\n## Test Structure\n\n```\ntests/e2e/\n├── __init__.py\n├── conftest.py          # Pytest fixtures\n├── runner.py            # Test execution engine\n├── test_cases.yaml      # YAML test definitions\n└── test_plugin_e2e.py   # Pytest test classes\n```\n\n## Configuration\n\n| Variable | Default | Description |\n|----------|---------|-------------|\n| `ANTHROPIC_API_KEY` | - | API key |\n| `E2E_TEST_TIMEOUT` | 120 | Timeout per test (seconds) |\n| `E2E_TEST_MODEL` | claude-sonnet-4-20250514 | Model to use |\n| `E2E_VERBOSE` | false | Verbose output |\n\n## Output Formats\n\n```bash\n# JSON report\npython -m tests.e2e.run_tests --json results.json\n\n# JUnit XML (CI integration)\npython -m tests.e2e.run_tests --junit results.xml\n\n# HTML report\npython -m tests.e2e.run_tests --html report.html\n\n# All formats\npython -m tests.e2e.run_tests --all-formats\n```\n\n## Adding Tests\n\n### YAML Test Cases\n\nEdit `test_cases.yaml`:\n\n```yaml\nsuites:\n  my_suite:\n    description: My tests\n    tests:\n      - id: my_test\n        name: Test something\n        prompt: \"Do something\"\n        expect:\n          output_contains:\n            - \"expected\"\n          no_errors: true\n```\n\n### Pytest Classes\n\nEdit `test_plugin_e2e.py`:\n\n```python\nclass TestMyFeature:\n    def test_something(self, claude_runner, e2e_enabled):\n        if not e2e_enabled:\n            pytest.skip(\"E2E disabled\")\n\n        result = claude_runner.send_prompt(\"My prompt\")\n        assert \"expected\" in result[\"output\"]\n```\n\n## Cost Estimates\n\n| Model | Per Test | 20 Tests |\n|-------|----------|----------|\n| Haiku | ~$0.001 | ~$0.02 |\n| Sonnet | ~$0.01 | ~$0.20 |\n| Opus | ~$0.05 | ~$1.00 |\n"
      },
      "plugins": [
        {
          "name": "splunk-assistant-skills",
          "source": ".",
          "description": "Complete Splunk automation suite with 14 specialized skills - search execution, job lifecycle, data export, metadata discovery, lookups, saved searches, alerts, and more",
          "version": "2.0.0",
          "category": "productivity",
          "keywords": [
            "splunk",
            "search",
            "spl",
            "siem",
            "automation",
            "devops",
            "natural-language"
          ],
          "author": {
            "name": "grandcamel"
          },
          "categories": [
            "automation",
            "devops",
            "natural-language",
            "productivity",
            "search",
            "siem",
            "spl",
            "splunk"
          ],
          "install_commands": [
            "/plugin marketplace add grandcamel/Splunk-Assistant-Skills",
            "/plugin install splunk-assistant-skills@splunk-assistant-skills"
          ]
        }
      ]
    }
  ]
}