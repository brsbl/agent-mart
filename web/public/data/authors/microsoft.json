{
  "author": {
    "id": "microsoft",
    "display_name": "Microsoft",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
    "url": "https://github.com/microsoft",
    "bio": "Open source projects and samples from Microsoft",
    "stats": {
      "total_marketplaces": 2,
      "total_plugins": 2,
      "total_commands": 0,
      "total_skills": 29,
      "total_stars": 1755,
      "total_forks": 112
    }
  },
  "marketplaces": [
    {
      "name": "playwright-cli",
      "version": null,
      "description": "Control browsers from the command line. Navigate pages, click elements, fill forms, take screenshots, and inspect network traffic using Playwright.",
      "owner_info": {
        "name": "Microsoft Corporation"
      },
      "keywords": [],
      "repo_full_name": "microsoft/playwright-cli",
      "repo_url": "https://github.com/microsoft/playwright-cli",
      "repo_description": "CLI for common Playwright actions. Record and generate Playwright code, inspect selectors and take screenshots.",
      "homepage": "https://playwright.dev",
      "signals": {
        "stars": 1656,
        "forks": 62,
        "pushed_at": "2026-01-29T10:10:37Z",
        "created_at": "2020-06-19T18:34:03Z",
        "license": "Apache-2.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 341
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 400
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 16292
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/playwright-cli",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/playwright-cli/SKILL.md",
          "type": "blob",
          "size": 3683
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"playwright-cli\",\n  \"owner\": {\n    \"name\": \"Microsoft Corporation\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"playwright-cli\",\n      \"source\": \"./\",\n      \"description\": \"Control browsers from the command line. Navigate pages, click elements, fill forms, take screenshots, and inspect network traffic using Playwright.\"\n    }\n  ]\n}",
        ".claude-plugin/plugin.json": "{\n    \"name\": \"playwright-cli\",\n    \"description\": \"Automates browser interactions for web testing, form filling, screenshots, and data extraction. Use when the user needs to navigate websites, interact with web pages, fill forms, take screenshots, test web applications, or extract information from web pages.\",\n    \"version\": \"0.0.1\",\n    \"author\": {\n        \"name\": \"Microsoft Corporation\"\n    }\n}",
        "README.md": "# playwright-cli\n\nPlaywright CLI with SKILLS\n\n### Playwright CLI vs Playwright MCP\n\nThis package provides CLI interface into Playwright. If you are using **coding agents**, that is the best fit.\n\n- **CLI**: Modern **coding agents** increasingly favor CLIâ€“based workflows exposed as SKILLs over MCP because CLI invocations are more token-efficient: they avoid loading large tool schemas and verbose accessibility trees into the model context, allowing agents to act through concise, purpose-built commands. This makes CLI + SKILLs better suited for high-throughput coding agents that must balance browser automation with large codebases, tests, and reasoning within limited context windows.\n\n- **MCP**: MCP remains relevant for specialized agentic loops that benefit from persistent state, rich introspection, and iterative reasoning over page structure, such as exploratory automation, self-healing tests, or long-running autonomous workflows where maintaining continuous browser context outweighs token cost concerns. Learn more about [Playwright MCP](https://github.com/microsoft/playwright-mcp).\n\n### Key Features\n\n- **Token-efficient**. Does not force page data into LLM.\n\n### Requirements\n- Node.js 18 or newer\n- Claude Code, GitHub Copilot, or any other coding agent.\n\n## Getting Started\n\n## Installation\n\n```bash\nnpm install -g @playwright/cli@latest\nplaywright-cli --help\n```\n\n> Note that you might need to force it to reclaim `playwright-cli` binary from our older MCP package.\n> ```bash\n> npm install -g @playwright/cli@latest --force\n> ```\n\n## Demo\n\n```\n> Use playwright skills to test https://demo.playwright.dev/todomvc/.\n  Take screenshots for all successful and failing scenarios. \n```\n\nYour agent will be running commands, but it does not mean you can't play with it manually:\n\n```\nplaywright-cli open https://demo.playwright.dev/todomvc/ --headed\nplaywright-cli type \"Buy groceries\"\nplaywright-cli press Enter\nplaywright-cli type \"Water flowers\"\nplaywright-cli press Enter\nplaywright-cli check e21\nplaywright-cli check e35\nplaywright-cli screenshot\n```\n\n### Skills-less operation\n\nPoint your agent at the CLI and let it cook. It'll read the skill off `playwright-cli --help` on its own:\n\n```\nTest the \"add todo\" flow on https://demo.playwright.dev/todomvc using playwright-cli.\nCheck playwright-cli --help for available commands.\n```\n\n### Installing skills\n\nClaude Code, GitHub copilot and others will let you install the Playwright skills into the agentic loop.\n\n#### plugin (recommended)\n```bash\n/plugin marketplace add microsoft/playwright-cli\n/plugin install playwright-cli\n```\n\n#### manual\n\n```bash\nmkdir -p .claude/skills/playwright-cli\ncurl -o .claude/skills/playwright-cli/SKILL.md \\\n  https://raw.githubusercontent.com/microsoft/playwright-cli/main/skills/playwright-cli/SKILL.md\n```\n\n## Headed operation\n\nPlaywright CLI is headless by default. If you'd like to see the browser, pass `--headed` to `open`:\n\n```bash\nplaywright-cli open https://playwright.dev --headed\n```\n\n## Sessions\n\nPlaywright CLI will use a dedicated persistent profile by default. It means that\nyour cookies and other storage state will be preserved between the calls. You can use different\ninstances of the browser for different projects with sessions.\n\nFollowing will result in two browsers with separate profiles being available. Pass `--session` to\nthe invocation to talk to a specific browser.\n\n```bash\nplaywright-cli open https://playwright.dev\nplaywright-cli --session=example open https://example.com\nplaywright-cli session-list\n```\n\nYou can run your coding agent with the `PLAYWRIGHT_CLI_SESSION` environment variable:\n\n```bash\nPLAYWRIGHT_CLI_SESSION=todo-app claude .\n```\n\nOr instruct it to prepend `--session` to the calls.\n\nManage your sessions as follows:\n\n```bash\nplaywright-cli session-list             # list all sessions\nplaywright-cli session-stop [name]      # stop session\nplaywright-cli session-stop-all         # stop all sessions\nplaywright-cli session-delete [name]    # delete session data along with the profiles\n```\n\n<!-- BEGIN GENERATED CLI HELP -->\n\n## Commands\n\n### Core\n\n```bash\nplaywright-cli open <url>               # open url\nplaywright-cli close                    # close the page\nplaywright-cli type <text>              # type text into editable element\nplaywright-cli click <ref> [button]     # perform click on a web page\nplaywright-cli dblclick <ref> [button]  # perform double click on a web page\nplaywright-cli fill <ref> <text>        # fill text into editable element\nplaywright-cli drag <startRef> <endRef> # perform drag and drop between two elements\nplaywright-cli hover <ref>              # hover over element on page\nplaywright-cli select <ref> <val>       # select an option in a dropdown\nplaywright-cli upload <file>            # upload one or multiple files\nplaywright-cli check <ref>              # check a checkbox or radio button\nplaywright-cli uncheck <ref>            # uncheck a checkbox or radio button\nplaywright-cli snapshot                 # capture page snapshot to obtain element ref\nplaywright-cli eval <func> [ref]        # evaluate javascript expression on page or element\nplaywright-cli dialog-accept [prompt]   # accept a dialog\nplaywright-cli dialog-dismiss           # dismiss a dialog\nplaywright-cli resize <w> <h>           # resize the browser window\n```\n\n### Navigation\n\n```bash\nplaywright-cli go-back                  # go back to the previous page\nplaywright-cli go-forward               # go forward to the next page\nplaywright-cli reload                   # reload the current page\n```\n\n### Keyboard\n\n```bash\nplaywright-cli press <key>              # press a key on the keyboard, `a`, `arrowleft`\nplaywright-cli keydown <key>            # press a key down on the keyboard\nplaywright-cli keyup <key>              # press a key up on the keyboard\n```\n\n### Mouse\n\n```bash\nplaywright-cli mousemove <x> <y>        # move mouse to a given position\nplaywright-cli mousedown [button]       # press mouse down\nplaywright-cli mouseup [button]         # press mouse up\nplaywright-cli mousewheel <dx> <dy>     # scroll mouse wheel\n```\n\n### Save as\n\n```bash\nplaywright-cli screenshot [ref]         # screenshot of the current page or element\nplaywright-cli pdf                      # save page as pdf\n```\n\n### Tabs\n\n```bash\nplaywright-cli tab-list                 # list all tabs\nplaywright-cli tab-new [url]            # create a new tab\nplaywright-cli tab-close [index]        # close a browser tab\nplaywright-cli tab-select <index>       # select a browser tab\n```\n\n### DevTools\n\n```bash\nplaywright-cli console [min-level]      # list console messages\nplaywright-cli network                  # list all network requests since loading the page\nplaywright-cli run-code <code>          # run playwright code snippet\nplaywright-cli tracing-start            # start trace recording\nplaywright-cli tracing-stop             # stop trace recording\n```\n<!-- END GENERATED CLI HELP -->\n\n## Configuration file\n\nThe Playwright CLI can be configured using a JSON configuration file. You can specify the configuration file using the `--config` command line option:\n\n```bash\nplaywright-cli --config path/to/config.json open example.com\n```\n\nPlaywright CLI will load config from `playwright-cli.json` by default so that you did not need to specify it every time.\n\n<details>\n<summary>Configuration file schema</summary>\n\n```typescript\n{\n  /**\n   * The browser to use.\n   */\n  browser?: {\n    /**\n     * The type of browser to use.\n     */\n    browserName?: 'chromium' | 'firefox' | 'webkit';\n\n    /**\n     * Keep the browser profile in memory, do not save it to disk.\n     */\n    isolated?: boolean;\n\n    /**\n     * Path to a user data directory for browser profile persistence.\n     * Temporary directory is created by default.\n     */\n    userDataDir?: string;\n\n    /**\n     * Launch options passed to\n     * @see https://playwright.dev/docs/api/class-browsertype#browser-type-launch-persistent-context\n     *\n     * This is useful for settings options like `channel`, `headless`, `executablePath`, etc.\n     */\n    launchOptions?: playwright.LaunchOptions;\n\n    /**\n     * Context options for the browser context.\n     *\n     * This is useful for settings options like `viewport`.\n     */\n    contextOptions?: playwright.BrowserContextOptions;\n\n    /**\n     * Chrome DevTools Protocol endpoint to connect to an existing browser instance in case of Chromium family browsers.\n     */\n    cdpEndpoint?: string;\n\n    /**\n     * CDP headers to send with the connect request.\n     */\n    cdpHeaders?: Record<string, string>;\n\n    /**\n     * Timeout in milliseconds for connecting to CDP endpoint. Defaults to 30000 (30 seconds). Pass 0 to disable timeout.\n     */\n    cdpTimeout?: number;\n\n    /**\n     * Remote endpoint to connect to an existing Playwright server.\n     */\n    remoteEndpoint?: string;\n\n    /**\n     * Paths to TypeScript files to add as initialization scripts for Playwright page.\n     */\n    initPage?: string[];\n\n    /**\n     * Paths to JavaScript files to add as initialization scripts.\n     * The scripts will be evaluated in every page before any of the page's scripts.\n     */\n    initScript?: string[];\n  },\n\n  /**\n   * If specified, saves the Playwright video of the session into the output directory.\n   */\n  saveVideo?: {\n    width: number;\n    height: number;\n  };\n\n  /**\n   * The directory to save output files.\n   */\n  outputDir?: string;\n\n  /**\n   * Whether to save snapshots, console messages, network logs and other session logs to a file or to the standard output. Defaults to \"stdout\".\n   */\n  outputMode?: 'file' | 'stdout';\n\n  console?: {\n    /**\n     * The level of console messages to return. Each level includes the messages of more severe levels. Defaults to \"info\".\n     */\n    level?: 'error' | 'warning' | 'info' | 'debug';\n  },\n\n  network?: {\n    /**\n     * List of origins to allow the browser to request. Default is to allow all. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n     */\n    allowedOrigins?: string[];\n\n    /**\n     * List of origins to block the browser to request. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n     */\n    blockedOrigins?: string[];\n  };\n\n  /**\n   * Specify the attribute to use for test ids, defaults to \"data-testid\".\n   */\n  testIdAttribute?: string;\n\n  timeouts?: {\n    /*\n     * Configures default action timeout: https://playwright.dev/docs/api/class-page#page-set-default-timeout. Defaults to 5000ms.\n     */\n    action?: number;\n\n    /*\n     * Configures default navigation timeout: https://playwright.dev/docs/api/class-page#page-set-default-navigation-timeout. Defaults to 60000ms.\n     */\n    navigation?: number;\n  };\n\n  /**\n   * Whether to allow file uploads from anywhere on the file system.\n   * By default (false), file uploads are restricted to paths within the MCP roots only.\n   */\n  allowUnrestrictedFileAccess?: boolean;\n\n  /**\n   * Specify the language to use for code generation.\n   */\n  codegen?: 'typescript' | 'none';\n}\n```\n\n</details>\n\n<details>\n<summary>Configuration via environment</summary>\n\n| Environment |\n|-------------|\n| `PLAYWRIGHT_MCP_ALLOWED_HOSTS` comma-separated list of hosts this server is allowed to serve from. Defaults to the host the server is bound to. Pass '*' to disable the host check. |\n| `PLAYWRIGHT_MCP_ALLOWED_ORIGINS` semicolon-separated list of TRUSTED origins to allow the browser to request. Default is to allow all. Important: *does not* serve as a security boundary and *does not* affect redirects. |\n| `PLAYWRIGHT_MCP_ALLOW_UNRESTRICTED_FILE_ACCESS` allow access to files outside of the workspace roots. Also allows unrestricted access to file:// URLs. By default access to file system is restricted to workspace root directories (or cwd if no roots are configured) only, and navigation to file:// URLs is blocked. |\n| `PLAYWRIGHT_MCP_BLOCKED_ORIGINS` semicolon-separated list of origins to block the browser from requesting. Blocklist is evaluated before allowlist. If used without the allowlist, requests not matching the blocklist are still allowed. Important: *does not* serve as a security boundary and *does not* affect redirects. |\n| `PLAYWRIGHT_MCP_BLOCK_SERVICE_WORKERS` block service workers |\n| `PLAYWRIGHT_MCP_BROWSER` browser or chrome channel to use, possible values: chrome, firefox, webkit, msedge. |\n| `PLAYWRIGHT_MCP_CAPS` comma-separated list of additional capabilities to enable, possible values: vision, pdf. |\n| `PLAYWRIGHT_MCP_CDP_ENDPOINT` CDP endpoint to connect to. |\n| `PLAYWRIGHT_MCP_CDP_HEADER` CDP headers to send with the connect request, multiple can be specified. |\n| `PLAYWRIGHT_MCP_CODEGEN` specify the language to use for code generation, possible values: \"typescript\", \"none\". Default is \"typescript\". |\n| `PLAYWRIGHT_MCP_CONFIG` path to the configuration file. |\n| `PLAYWRIGHT_MCP_CONSOLE_LEVEL` level of console messages to return: \"error\", \"warning\", \"info\", \"debug\". Each level includes the messages of more severe levels. |\n| `PLAYWRIGHT_MCP_DEVICE` device to emulate, for example: \"iPhone 15\" |\n| `PLAYWRIGHT_MCP_EXECUTABLE_PATH` path to the browser executable. |\n| `PLAYWRIGHT_MCP_EXTENSION` Connect to a running browser instance (Edge/Chrome only). Requires the \"Playwright MCP Bridge\" browser extension to be installed. |\n| `PLAYWRIGHT_MCP_GRANT_PERMISSIONS` List of permissions to grant to the browser context, for example \"geolocation\", \"clipboard-read\", \"clipboard-write\". |\n| `PLAYWRIGHT_MCP_HEADLESS` run browser in headless mode, headed by default |\n| `PLAYWRIGHT_MCP_HOST` host to bind server to. Default is localhost. Use 0.0.0.0 to bind to all interfaces. |\n| `PLAYWRIGHT_MCP_IGNORE_HTTPS_ERRORS` ignore https errors |\n| `PLAYWRIGHT_MCP_INIT_PAGE` path to TypeScript file to evaluate on Playwright page object |\n| `PLAYWRIGHT_MCP_INIT_SCRIPT` path to JavaScript file to add as an initialization script. The script will be evaluated in every page before any of the page's scripts. Can be specified multiple times. |\n| `PLAYWRIGHT_MCP_ISOLATED` keep the browser profile in memory, do not save it to disk. |\n| `PLAYWRIGHT_MCP_IMAGE_RESPONSES` whether to send image responses to the client. Can be \"allow\" or \"omit\", Defaults to \"allow\". |\n| `PLAYWRIGHT_MCP_NO_SANDBOX` disable the sandbox for all process types that are normally sandboxed. |\n| `PLAYWRIGHT_MCP_OUTPUT_DIR` path to the directory for output files. |\n| `PLAYWRIGHT_MCP_OUTPUT_MODE` whether to save snapshots, console messages, network logs to a file or to the standard output. Can be \"file\" or \"stdout\". Default is \"stdout\". |\n| `PLAYWRIGHT_MCP_PORT` port to listen on for SSE transport. |\n| `PLAYWRIGHT_MCP_PROXY_BYPASS` comma-separated domains to bypass proxy, for example \".com,chromium.org,.domain.com\" |\n| `PLAYWRIGHT_MCP_PROXY_SERVER` specify proxy server, for example \"http://myproxy:3128\" or \"socks5://myproxy:8080\" |\n| `PLAYWRIGHT_MCP_SAVE_SESSION` Whether to save the Playwright MCP session into the output directory. |\n| `PLAYWRIGHT_MCP_SAVE_TRACE` Whether to save the Playwright Trace of the session into the output directory. |\n| `PLAYWRIGHT_MCP_SAVE_VIDEO` Whether to save the video of the session into the output directory. For example \"--save-video=800x600\" |\n| `PLAYWRIGHT_MCP_SECRETS` path to a file containing secrets in the dotenv format |\n| `PLAYWRIGHT_MCP_SHARED_BROWSER_CONTEXT` reuse the same browser context between all connected HTTP clients. |\n| `PLAYWRIGHT_MCP_SNAPSHOT_MODE` when taking snapshots for responses, specifies the mode to use. Can be \"incremental\", \"full\", or \"none\". Default is incremental. |\n| `PLAYWRIGHT_MCP_STORAGE_STATE` path to the storage state file for isolated sessions. |\n| `PLAYWRIGHT_MCP_TEST_ID_ATTRIBUTE` specify the attribute to use for test ids, defaults to \"data-testid\" |\n| `PLAYWRIGHT_MCP_TIMEOUT_ACTION` specify action timeout in milliseconds, defaults to 5000ms |\n| `PLAYWRIGHT_MCP_TIMEOUT_NAVIGATION` specify navigation timeout in milliseconds, defaults to 60000ms |\n| `PLAYWRIGHT_MCP_USER_AGENT` specify user agent string |\n| `PLAYWRIGHT_MCP_USER_DATA_DIR` path to the user data directory. If not specified, a temporary directory will be created. |\n| `PLAYWRIGHT_MCP_VIEWPORT_SIZE` specify browser viewport size in pixels, for example \"1280x720\" |\n</details>\n",
        "skills/playwright-cli/SKILL.md": "---\nname: playwright-cli\ndescription: Automates browser interactions for web testing, form filling, screenshots, and data extraction. Use when the user needs to navigate websites, interact with web pages, fill forms, take screenshots, test web applications, or extract information from web pages.\nallowed-tools: Bash(playwright-cli:*)\n---\n\n# Browser Automation with playwright-cli\n\n## Quick start\n\n```bash\nplaywright-cli open https://playwright.dev\nplaywright-cli click e15\nplaywright-cli type \"page.click\"\nplaywright-cli press Enter\n```\n\n## Core workflow\n\n1. Navigate: `playwright-cli open https://example.com`\n2. Interact using refs from the snapshot\n3. Re-snapshot after significant changes\n\n## Commands\n\n### Core\n\n```bash\nplaywright-cli open https://example.com/\nplaywright-cli close\nplaywright-cli type \"search query\"\nplaywright-cli click e3\nplaywright-cli dblclick e7\nplaywright-cli fill e5 \"user@example.com\"\nplaywright-cli drag e2 e8\nplaywright-cli hover e4\nplaywright-cli select e9 \"option-value\"\nplaywright-cli upload ./document.pdf\nplaywright-cli check e12\nplaywright-cli uncheck e12\nplaywright-cli snapshot\nplaywright-cli eval \"document.title\"\nplaywright-cli eval \"el => el.textContent\" e5\nplaywright-cli dialog-accept\nplaywright-cli dialog-accept \"confirmation text\"\nplaywright-cli dialog-dismiss\nplaywright-cli resize 1920 1080\n```\n\n### Navigation\n\n```bash\nplaywright-cli go-back\nplaywright-cli go-forward\nplaywright-cli reload\n```\n\n### Keyboard\n\n```bash\nplaywright-cli press Enter\nplaywright-cli press ArrowDown\nplaywright-cli keydown Shift\nplaywright-cli keyup Shift\n```\n\n### Mouse\n\n```bash\nplaywright-cli mousemove 150 300\nplaywright-cli mousedown\nplaywright-cli mousedown right\nplaywright-cli mouseup\nplaywright-cli mouseup right\nplaywright-cli mousewheel 0 100\n```\n\n### Save as\n\n```bash\nplaywright-cli screenshot\nplaywright-cli screenshot e5\nplaywright-cli pdf\n```\n\n### Tabs\n\n```bash\nplaywright-cli tab-list\nplaywright-cli tab-new\nplaywright-cli tab-new https://example.com/page\nplaywright-cli tab-close\nplaywright-cli tab-close 2\nplaywright-cli tab-select 0\n```\n\n### DevTools\n\n```bash\nplaywright-cli console\nplaywright-cli console warning\nplaywright-cli network\nplaywright-cli run-code \"async page => await page.context().grantPermissions(['geolocation'])\"\nplaywright-cli tracing-start\nplaywright-cli tracing-stop\n```\n\n### Configuration\n```bash\n# Configure the session\nplaywright-cli config my-config.json\n# Configure named session\nplaywright-cli --session=mysession config my-config.json\n# Start with configured session\nplaywright-cli open --config=my-config.json\n```\n\n### Sessions\n\n```bash\nplaywright-cli --session=mysession open example.com\nplaywright-cli --session=mysession click e6\nplaywright-cli session-list\nplaywright-cli session-stop mysession\nplaywright-cli session-stop-all\nplaywright-cli session-delete\nplaywright-cli session-delete mysession\n```\n\n## Example: Form submission\n\n```bash\nplaywright-cli open https://example.com/form\nplaywright-cli snapshot\n\nplaywright-cli fill e1 \"user@example.com\"\nplaywright-cli fill e2 \"password123\"\nplaywright-cli click e3\nplaywright-cli snapshot\n```\n\n## Example: Multi-tab workflow\n\n```bash\nplaywright-cli open https://example.com\nplaywright-cli tab-new https://example.com/other\nplaywright-cli tab-list\nplaywright-cli tab-select 0\nplaywright-cli snapshot\n```\n\n## Example: Debugging with DevTools\n\n```bash\nplaywright-cli open https://example.com\nplaywright-cli click e4\nplaywright-cli fill e7 \"test\"\nplaywright-cli console\nplaywright-cli network\n```\n\n```bash\nplaywright-cli open https://example.com\nplaywright-cli tracing-start\nplaywright-cli click e4\nplaywright-cli fill e7 \"test\"\nplaywright-cli tracing-stop\n```\n"
      },
      "plugins": [
        {
          "name": "playwright-cli",
          "source": "./",
          "description": "Control browsers from the command line. Navigate pages, click elements, fill forms, take screenshots, and inspect network traffic using Playwright.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add microsoft/playwright-cli",
            "/plugin install playwright-cli@playwright-cli"
          ]
        }
      ]
    },
    {
      "name": "github-copilot-for-azure",
      "version": null,
      "description": "Microsoft Azure MCP integration for cloud resource management, deployments, and Azure services. Manage your Azure infrastructure, monitor applications, and deploy resources directly from Claude Code.",
      "owner_info": {
        "name": "Microsoft",
        "url": "https://www.microsoft.com"
      },
      "keywords": [],
      "repo_full_name": "microsoft/GitHub-Copilot-for-Azure",
      "repo_url": "https://github.com/microsoft/GitHub-Copilot-for-Azure",
      "repo_description": "GitHub Copilot for Azure",
      "homepage": null,
      "signals": {
        "stars": 99,
        "forks": 50,
        "pushed_at": "2026-01-29T22:25:05Z",
        "created_at": "2024-10-28T13:46:49Z",
        "license": "NOASSERTION"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 629
        },
        {
          "path": "plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 507
        },
        {
          "path": "plugin/README.md",
          "type": "blob",
          "size": 4418
        },
        {
          "path": "plugin/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/appinsights-instrumentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/appinsights-instrumentation/SKILL.md",
          "type": "blob",
          "size": 2462
        },
        {
          "path": "plugin/skills/appinsights-instrumentation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/appinsights-instrumentation/references/ASPNETCORE.md",
          "type": "blob",
          "size": 1711
        },
        {
          "path": "plugin/skills/appinsights-instrumentation/references/AUTO.md",
          "type": "blob",
          "size": 891
        },
        {
          "path": "plugin/skills/appinsights-instrumentation/references/NODEJS.md",
          "type": "blob",
          "size": 1815
        },
        {
          "path": "plugin/skills/appinsights-instrumentation/references/PYTHON.md",
          "type": "blob",
          "size": 1812
        },
        {
          "path": "plugin/skills/azure-ai",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-ai/SKILL.md",
          "type": "blob",
          "size": 2750
        },
        {
          "path": "plugin/skills/azure-aigateway",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-aigateway/SKILL.md",
          "type": "blob",
          "size": 24852
        },
        {
          "path": "plugin/skills/azure-cosmos-db",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-cosmos-db/SKILL.md",
          "type": "blob",
          "size": 4691
        },
        {
          "path": "plugin/skills/azure-cost-optimization",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-cost-optimization/SKILL.md",
          "type": "blob",
          "size": 13195
        },
        {
          "path": "plugin/skills/azure-cost-optimization/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-cost-optimization/references/azure-quick-review.md",
          "type": "blob",
          "size": 1600
        },
        {
          "path": "plugin/skills/azure-cost-optimization/references/azure-redis.md",
          "type": "blob",
          "size": 2707
        },
        {
          "path": "plugin/skills/azure-cost-optimization/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-cost-optimization/templates/redis-detailed-cache-analysis.md",
          "type": "blob",
          "size": 3025
        },
        {
          "path": "plugin/skills/azure-cost-optimization/templates/redis-subscription-level-report.md",
          "type": "blob",
          "size": 1828
        },
        {
          "path": "plugin/skills/azure-create-app",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-create-app/SKILL.md",
          "type": "blob",
          "size": 4872
        },
        {
          "path": "plugin/skills/azure-create-app/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-create-app/reference/aks.md",
          "type": "blob",
          "size": 20823
        },
        {
          "path": "plugin/skills/azure-create-app/reference/app-service.md",
          "type": "blob",
          "size": 19783
        },
        {
          "path": "plugin/skills/azure-create-app/reference/app-type-detection.md",
          "type": "blob",
          "size": 1981
        },
        {
          "path": "plugin/skills/azure-create-app/reference/azure-yaml-config.md",
          "type": "blob",
          "size": 1567
        },
        {
          "path": "plugin/skills/azure-create-app/reference/container-apps.md",
          "type": "blob",
          "size": 39310
        },
        {
          "path": "plugin/skills/azure-create-app/reference/error-handling.md",
          "type": "blob",
          "size": 1351
        },
        {
          "path": "plugin/skills/azure-create-app/reference/functions.md",
          "type": "blob",
          "size": 24783
        },
        {
          "path": "plugin/skills/azure-create-app/reference/service-selection.md",
          "type": "blob",
          "size": 1442
        },
        {
          "path": "plugin/skills/azure-create-app/reference/static-web-apps.md",
          "type": "blob",
          "size": 25770
        },
        {
          "path": "plugin/skills/azure-deploy",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-deploy/SKILL.md",
          "type": "blob",
          "size": 4695
        },
        {
          "path": "plugin/skills/azure-deployment-preflight",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-deployment-preflight/SKILL.md",
          "type": "blob",
          "size": 9103
        },
        {
          "path": "plugin/skills/azure-deployment-preflight/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-deployment-preflight/references/ERROR-HANDLING.md",
          "type": "blob",
          "size": 8896
        },
        {
          "path": "plugin/skills/azure-deployment-preflight/references/REPORT-TEMPLATE.md",
          "type": "blob",
          "size": 7458
        },
        {
          "path": "plugin/skills/azure-deployment-preflight/references/VALIDATION-COMMANDS.md",
          "type": "blob",
          "size": 8790
        },
        {
          "path": "plugin/skills/azure-diagnostics",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-diagnostics/SKILL.md",
          "type": "blob",
          "size": 5686
        },
        {
          "path": "plugin/skills/azure-functions",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-functions/SKILL.md",
          "type": "blob",
          "size": 27051
        },
        {
          "path": "plugin/skills/azure-keyvault-expiration-audit",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-keyvault-expiration-audit/SKILL.md",
          "type": "blob",
          "size": 6150
        },
        {
          "path": "plugin/skills/azure-kusto",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-kusto/SKILL.md",
          "type": "blob",
          "size": 8526
        },
        {
          "path": "plugin/skills/azure-mcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-mcp/SKILL.md",
          "type": "blob",
          "size": 8808
        },
        {
          "path": "plugin/skills/azure-networking",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-networking/SKILL.md",
          "type": "blob",
          "size": 2508
        },
        {
          "path": "plugin/skills/azure-nodejs-production",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-nodejs-production/SKILL.md",
          "type": "blob",
          "size": 7499
        },
        {
          "path": "plugin/skills/azure-observability",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-observability/SKILL.md",
          "type": "blob",
          "size": 3078
        },
        {
          "path": "plugin/skills/azure-postgres-entra-rbac-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-postgres-entra-rbac-setup/REQUIREMENTS.md",
          "type": "blob",
          "size": 17154
        },
        {
          "path": "plugin/skills/azure-postgres-entra-rbac-setup/SKILL.md",
          "type": "blob",
          "size": 8326
        },
        {
          "path": "plugin/skills/azure-postgres-entra-rbac-setup/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-postgres-entra-rbac-setup/references/GROUP-SYNC.md",
          "type": "blob",
          "size": 7380
        },
        {
          "path": "plugin/skills/azure-postgres-entra-rbac-setup/references/PERMISSION-TEMPLATES.md",
          "type": "blob",
          "size": 6587
        },
        {
          "path": "plugin/skills/azure-postgres-entra-rbac-setup/references/SQL-FUNCTIONS.md",
          "type": "blob",
          "size": 3377
        },
        {
          "path": "plugin/skills/azure-postgres-entra-rbac-setup/references/TROUBLESHOOTING.md",
          "type": "blob",
          "size": 7864
        },
        {
          "path": "plugin/skills/azure-quick-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-quick-review/SKILL.md",
          "type": "blob",
          "size": 5999
        },
        {
          "path": "plugin/skills/azure-quick-review/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-quick-review/references/RECOMMENDATIONS.md",
          "type": "blob",
          "size": 5786
        },
        {
          "path": "plugin/skills/azure-quick-review/references/REMEDIATION-PATTERNS.md",
          "type": "blob",
          "size": 7946
        },
        {
          "path": "plugin/skills/azure-redis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-redis/SKILL.md",
          "type": "blob",
          "size": 4142
        },
        {
          "path": "plugin/skills/azure-resource-visualizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-resource-visualizer/SKILL.md",
          "type": "blob",
          "size": 9671
        },
        {
          "path": "plugin/skills/azure-resource-visualizer/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-resource-visualizer/assets/template-architecture.md",
          "type": "blob",
          "size": 970
        },
        {
          "path": "plugin/skills/azure-role-selector",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-role-selector/SKILL.md",
          "type": "blob",
          "size": 860
        },
        {
          "path": "plugin/skills/azure-security-hardening",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-security-hardening/SKILL.md",
          "type": "blob",
          "size": 3863
        },
        {
          "path": "plugin/skills/azure-security",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-security/SKILL.md",
          "type": "blob",
          "size": 2786
        },
        {
          "path": "plugin/skills/azure-sql-database",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-sql-database/SKILL.md",
          "type": "blob",
          "size": 4371
        },
        {
          "path": "plugin/skills/azure-storage",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-storage/SKILL.md",
          "type": "blob",
          "size": 3110
        },
        {
          "path": "plugin/skills/azure-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/azure-validation/SKILL.md",
          "type": "blob",
          "size": 5477
        },
        {
          "path": "plugin/skills/entra-app-registration",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/entra-app-registration/SKILL.md",
          "type": "blob",
          "size": 7093
        },
        {
          "path": "plugin/skills/entra-app-registration/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/entra-app-registration/references/API-PERMISSIONS.md",
          "type": "blob",
          "size": 10198
        },
        {
          "path": "plugin/skills/entra-app-registration/references/CLI-COMMANDS.md",
          "type": "blob",
          "size": 8852
        },
        {
          "path": "plugin/skills/entra-app-registration/references/CONSOLE-APP-EXAMPLE.md",
          "type": "blob",
          "size": 11005
        },
        {
          "path": "plugin/skills/entra-app-registration/references/FIRST-APP-REGISTRATION.md",
          "type": "blob",
          "size": 7415
        },
        {
          "path": "plugin/skills/entra-app-registration/references/OAUTH-FLOWS.md",
          "type": "blob",
          "size": 9540
        },
        {
          "path": "plugin/skills/entra-app-registration/references/TROUBLESHOOTING.md",
          "type": "blob",
          "size": 7594
        },
        {
          "path": "plugin/skills/microsoft-foundry",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/microsoft-foundry/SKILL.md",
          "type": "blob",
          "size": 19959
        },
        {
          "path": "plugin/skills/microsoft-foundry/language",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugin/skills/microsoft-foundry/language/python.md",
          "type": "blob",
          "size": 18718
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n    \"name\": \"github-copilot-for-azure\",\n    \"owner\": {\n        \"name\": \"Microsoft\",\n        \"url\": \"https://www.microsoft.com\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"azure\",\n            \"description\": \"Microsoft Azure MCP integration for cloud resource management, deployments, and Azure services. Manage your Azure infrastructure, monitor applications, and deploy resources directly from Claude Code.\",\n            \"source\": \"./plugin\",\n            \"homepage\": \"https://github.com/microsoft/github-copilot-for-azure\"\n        }\n    ]\n}",
        "plugin/.claude-plugin/plugin.json": "{\n  \"name\": \"azure\",\n  \"description\": \"Microsoft Azure MCP integration for cloud resource management, deployments, and Azure services. Manage your Azure infrastructure, monitor applications, and deploy resources directly from Claude Code.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Microsoft\",\n    \"url\": \"https://www.microsoft.com\"\n  },\n  \"homepage\": \"https://github.com/microsoft/github-copilot-for-azure\",\n  \"keywords\": [\"azure\", \"cloud\", \"infrastructure\", \"deployment\", \"microsoft\", \"devops\"]\n}\n",
        "plugin/README.md": "# Azure\n\n[Microsoft Azure](https://azure.microsoft.com) is Microsoft's cloud computing platform. This plugin connects [GitHub Copilot CLI](https://github.com/github/copilot-cli) or Claude Code to your Azure account, letting you manage resources, deploy applications, and monitor services directly from your development environment.\n\n## Setup\n\n### 1. Create an Azure Account\n\nSign up at [azure.microsoft.com](https://azure.microsoft.com) or use your existing Azure account.\n\n### 2. Install Node.js and NPM\n\nThe Azure MCP Server runs as an NPM package. Ensure you have Node.js 18 or later installed:\n\n- Download from [nodejs.org](https://nodejs.org)\n- Or use a version manager like [nvm](https://github.com/nvm-sh/nvm)\n\n### 3. Authenticate to Azure\n\nThe Azure MCP Server uses the Azure Identity SDK for authentication. You can authenticate using any of these methods:\n\n#### Option A: Azure CLI (Recommended)\n1. Install [Azure CLI](https://docs.microsoft.com/cli/azure/install-azure-cli)\n2. Run `az login` to authenticate\n3. The MCP server will automatically use your CLI credentials\n\n#### Option B: Environment Variables\nSet Azure service principal credentials:\n\n**Bash/Zsh:**\n```bash\nexport AZURE_TENANT_ID=\"your-tenant-id\"\nexport AZURE_CLIENT_ID=\"your-client-id\"\nexport AZURE_CLIENT_SECRET=\"your-client-secret\"\n```\n\n**PowerShell:**\n```powershell\n$env:AZURE_TENANT_ID = \"your-tenant-id\"\n$env:AZURE_CLIENT_ID = \"your-client-id\"\n$env:AZURE_CLIENT_SECRET = \"your-client-secret\"\n```\n\n#### Option C: Managed Identity\nWhen running on Azure resources (VMs, Container Apps, etc.), the server automatically uses managed identity.\n\nFor more authentication options, see the [Azure Identity documentation](https://learn.microsoft.com/azure/developer/azure-mcp-server/).\n\n### 4. Install the Plugins\n# Add the repo as a plugin marketplace\n/plugin marketplace add microsoft/github-copilot-for-azure\n\n# Pull in the Azure plugin\n/plugin install azure@github-copilot-for-azure\n\n## Available Tools\n\nThe Azure MCP Server provides tools for 40+ Azure services:\n\n### AI & Machine Learning\n- Microsoft Foundry (AI models, deployments, knowledge indexes)\n- Azure AI Search (search and vector database)\n- Azure AI Services Speech (speech-to-text, text-to-speech)\n\n### Compute & Containers\n- Azure App Service, Container Apps, AKS\n- Azure Functions, Virtual Desktop\n\n### Storage & Databases\n- Azure Storage (Blob, File Sync)\n- Azure SQL Database, Cosmos DB\n- Azure Database for MySQL & PostgreSQL\n\n### Security & Networking\n- Azure Key Vault (secrets, keys, certificates)\n- Azure RBAC (access control)\n- Azure Confidential Ledger\n\n### DevOps & Management\n- Resource Groups, Subscriptions\n- Azure Monitor (logging, metrics)\n- Azure CLI command generation\n- Bicep templates\n\n### Messaging & Communication\n- Azure Communication Services (SMS, email)\n- Azure Service Bus, Event Grid\n\nFor the complete list of 40+ services, see the [official documentation](https://learn.microsoft.com/azure/developer/azure-mcp-server/).\n\n## Example Usage\n\nAsk GitHub Copilot CLI or Claude Code to:\n- \"List my Azure storage accounts\"\n- \"Show me all containers in my Cosmos DB database\"\n- \"List all secrets in my key vault 'my-vault'\"\n- \"Deploy a web app to Azure App Service\"\n- \"Query my Log Analytics workspace\"\n- \"List my AKS clusters\"\n- \"Send an SMS message to +1234567890 using Azure Communication Services\"\n- \"Generate an Azure CLI command to create a storage account\"\n\nFor more examples, visit the [Azure MCP documentation](https://learn.microsoft.com/azure/developer/azure-mcp-server/).\n\n## Documentation\n\nFor more information, visit:\n- [Azure Documentation](https://docs.microsoft.com/azure)\n- [Azure CLI Reference](https://docs.microsoft.com/cli/azure/)\n- [Azure REST API Reference](https://docs.microsoft.com/rest/api/azure/)\n\n## Troubleshooting\n\n### Authentication Issues\n- Run `az login` to authenticate with Azure CLI\n- Verify you have appropriate Azure RBAC permissions\n- Check that your credentials are not expired\n- See the [Authentication guide](https://learn.microsoft.com/azure/developer/azure-mcp-server/)\n\n### Server Issues\n- Ensure Node.js 18 or later is installed\n- Verify NPM can download packages from npmjs.com\n- Check the [Troubleshooting guide](https://github.com/microsoft/mcp/blob/main/servers/Azure.Mcp.Server/TROUBLESHOOTING.md)\n\n### Telemetry\nTo disable telemetry collection, set:\n```bash\nexport AZURE_MCP_COLLECT_TELEMETRY=false\n```\n",
        "plugin/skills/appinsights-instrumentation/SKILL.md": "---\nname: appinsights-instrumentation\ndescription: 'Instrument a webapp to send useful telemetry data to Azure App Insights'\n---\n\n# AppInsights instrumentation\n\nThis skill enables sending telemetry data of a webapp to Azure App Insights for better observability of the app's health.\n\n## When to use this skill\n\nUse this skill when the user wants to enable telemetry for their webapp.\n\n## Prerequisites\n\nThe app in the workspace must be one of these kinds\n\n- An ASP.NET Core app hosted in Azure\n- A Node.js app hosted in Azure\n\n## Guidelines\n\n### Collect context information\n\nFind out the (programming language, application framework, hosting) tuple of the application the user is trying to add telemetry support in. This determines how the application can be instrumented. Read the source code to make an educated guess. Confirm with the user on anything you don't know. You must always ask the user where the application is hosted (e.g. on a personal computer, in an Azure App Service as code, in an Azure App Service as container, in an Azure Container App, etc.). \n\n### Prefer auto-instrument if possible\n\nIf the app is a C# ASP.NET Core app hosted in Azure App Service, use [AUTO guide](references/AUTO.md) to help user auto-instrument the app.\n\n### Manually instrument\n\nManually instrument the app by creating the AppInsights resource and update the app's code. \n\n#### Create AppInsights resource\n\nUse one of the following options that fits the environment.\n\n- Add AppInsights to existing Bicep template. See [examples/appinsights.bicep](examples/appinsights.bicep) for what to add. This is the best option if there are existing Bicep template files in the workspace.\n- Use Azure CLI. See [scripts/appinsights.ps1](scripts/appinsights.ps1) for what Azure CLI command to execute to create the App Insights resource.\n\nNo matter which option you choose, recommend the user to create the App Insights resource in a meaningful resource group that makes managing resources easier. A good candidate will be the same resource group that contains the resources for the hosted app in Azure.\n\n#### Modify application code\n\n- If the app is an ASP.NET Core app, see [ASPNETCORE guide](references/ASPNETCORE.md) for how to modify the C# code.\n- If the app is a Node.js app, see [NODEJS guide](references/NODEJS.md) for how to modify the JavaScript/TypeScript code.\n- If the app is a Python app, see [PYTHON guide](references/PYTHON.md) for how to modify the Python code.\n",
        "plugin/skills/appinsights-instrumentation/references/ASPNETCORE.md": "## Modify code\n\nMake these necessary changes to the app.\n\n- Install client library\n```\ndotnet add package Azure.Monitor.OpenTelemetry.AspNetCore\n```\n\n- Configure the app to use Azure Monitor\nAn ASP.NET Core app typically has a Program.cs file that \"builds\" the app. Find this file and apply these changes.\n  - Add `using Azure.Monitor.OpenTelemetry.AspNetCore;` at the top\n  - Before calling `builder.Build()`, add this line `builder.Services.AddOpenTelemetry().UseAzureMonitor();`.\n\n> Note: since we modified the code of the app, the app needs to be deployed to take effect.\n\n## Configure App Insights connection string\n\nThe App Insights resource has a connection string. Add the connection string as an environment variable of the running app. You can use Azure CLI to query the connection string of the App Insights resource. See [scripts/appinsights.ps1](scripts/appinsights.ps1) for what Azure CLI command to execute for querying the connection string.\n\nAfter getting the connection string, set this environment variable with its value.\n\n```\n\"APPLICATIONINSIGHTS_CONNECTION_STRING={your_application_insights_connection_string}\"\n```\n\nIf the app has IaC template such as Bicep or terraform files representing its cloud instance, this environment variable should be added to the IaC template to be applied in each deployment. Otherwise, use Azure CLI to manually apply the environment variable to the cloud instance of the app. See [scripts/appinsights.ps1](scripts/appinsights.ps1) for what Azure CLI command to execute for setting this environment variable.\n\n> Important: Don't modify appsettings.json. It was a deprecated way to configure App Insights. The environment variable is the new recommended way.\n",
        "plugin/skills/appinsights-instrumentation/references/AUTO.md": "# Auto-instrument app\n\nUse Azure Portal to auto-instrument a webapp hosted in Azure App Service for App Insights without making any code changes. Only the following types of app can be auto-instrumented. See [supported environments and resource providers](https://learn.microsoft.com/azure/azure-monitor/app/codeless-overview#supported-environments-languages-and-resource-providers).\n\n- ASP.NET Core app hosted in Azure App Service\n- Node.js app hosted in Azure App Service\n\nConstruct a url to bring the user to the Application Insights blade in Azure Portal for the App Service App.\n```\nhttps://portal.azure.com/#resource/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.Web/sites/{app_service_name}/monitoringSettings\n```\n\nUse the context or ask the user to get the subscription_id, resource_group_name, and the app_service_name hosting the webapp.\n",
        "plugin/skills/appinsights-instrumentation/references/NODEJS.md": "## Modify code\n\nMake these necessary changes to the app.\n\n- Install client library\n```\nnpm install @azure/monitor-opentelemetry\n```\n\n- Configure the app to use Azure Monitor\nA Node.js app typically has an entry file that is listed as the \"main\" property in package.json. Find this file and apply these changes in it.\n  - Require the client library at the top. `const { useAzureMonitor } = require(\"@azure/monitor-opentelemetry\");`\n  - Call the setup method. `useAzureMonitor();`\n\n> Note: The setup method should be called as early as possible but it must be after the environment variables are configured since it needs the App Insights connection string from the environment variable. For example, if the app uses dotenv to load environment variables, the setup method should be called after it but before anything else.\n> Note: since we modified the code of the app, it needs to be deployed to take effect.\n\n## Configure App Insights connection string\n\nThe App Insights resource has a connection string. Add the connection string as an environment variable of the running app. You can use Azure CLI to query the connection string of the App Insights resource. See [scripts/appinsights.ps1] for what Azure CLI command to execute for querying the connection string.\n\nAfter getting the connection string, set this environment variable with its value.\n\n```\n\"APPLICATIONINSIGHTS_CONNECTION_STRING={your_application_insights_connection_string}\"\n```\n\nIf the app has IaC template such as Bicep or terraform files representing its cloud instance, this environment variable should be added to the IaC template to be applied in each deployment. Otherwise, use Azure CLI to manually apply the environment variable to the cloud instance of the app. See what Azure CLI command to execute for setting this environment variable.\n",
        "plugin/skills/appinsights-instrumentation/references/PYTHON.md": "## Modify code\n\nMake these necessary changes to the app.\n\n- Install client library\n```\npip install azure-monitor-opentelemetry\n```\n\n- Configure the app to use Azure Monitor\nPython applications send telemetry via the logger class in Python standard library. Create a module that configures and creates a logger that can send telemetry.\n\n```python\nimport logging\nfrom azure.monitor.opentelemetry import configure_azure_monitor\n\nconfigure_azure_monitor(\n    logger_name=\"<your_logger_namespace>\"\n)\nlogger = logging.getLogger(\"<your_logger_namespace>\")\n```\n\n> Note: since we modified the code of the app, it needs to be deployed to take effect.\n\n## Configure App Insights connection string\n\nThe App Insights resource has a connection string. Add the connection string as an environment variable of the running app. You can use Azure CLI to query the connection string of the App Insights resource. See [scripts/appinsights.ps1] for what Azure CLI command to execute for querying the connection string.\n\nAfter getting the connection string, set this environment variable with its value.\n\n```\n\"APPLICATIONINSIGHTS_CONNECTION_STRING={your_application_insights_connection_string}\"\n```\n\nIf the app has IaC template such as Bicep or terraform files representing its cloud instance, this environment variable should be added to the IaC template to be applied in each deployment. Otherwise, use Azure CLI to manually apply the environment variable to the cloud instance of the app. See what Azure CLI command to execute for setting this environment variable.\n\n## Send data\n\nCreate a logger that is configured to send telemetry.\n```python\nlogger = logging.getLogger(\"<your_logger_namespace>\")\nlogger.setLevel(logging.INFO)\n```\n\nThen send telemetry events by calling its logging methods.\n```python\nlogger.info(\"info log\")\n```\n",
        "plugin/skills/azure-ai/SKILL.md": "---\nname: azure-ai\ndescription: Azure AI Services including AI Search, Speech, Foundry, OpenAI, and Document Intelligence. Provides capabilities for full-text/vector/hybrid search, speech-to-text, text-to-speech, AI models, agents, and prompt flows.\n---\n\n# Azure AI Services\n\n## Services\n\n| Service | Use When | MCP Tools | CLI |\n|---------|----------|-----------|-----|\n| AI Search | Full-text, vector, hybrid search | `azure__search` | `az search` |\n| Speech | Speech-to-text, text-to-speech | `azure__speech` | - |\n| Foundry | AI models, agents, prompt flows | `azure__foundry` | `az ml` |\n| OpenAI | GPT models, embeddings, DALL-E | - | `az cognitiveservices` |\n| Document Intelligence | Form extraction, OCR | - | - |\n\n## MCP Server (Preferred)\n\nWhen Azure MCP is enabled:\n\n### AI Search\n- `azure__search` with command `search_index_list` - List search indexes\n- `azure__search` with command `search_index_get` - Get index details\n- `azure__search` with command `search_query` - Query search index\n\n### Speech\n- `azure__speech` with command `speech_transcribe` - Speech to text\n- `azure__speech` with command `speech_synthesize` - Text to speech\n\n### Foundry\n- `azure__foundry` with command `foundry_model_list` - List AI models\n- `azure__foundry` with command `foundry_deployment_list` - List deployments\n- `azure__foundry` with command `foundry_agent_list` - List AI agents\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n## AI Search Capabilities\n\n| Feature | Description |\n|---------|-------------|\n| Full-text search | Linguistic analysis, stemming |\n| Vector search | Semantic similarity with embeddings |\n| Hybrid search | Combined keyword + vector |\n| AI enrichment | Entity extraction, OCR, sentiment |\n\n## Speech Capabilities\n\n| Feature | Description |\n|---------|-------------|\n| Speech-to-text | Real-time and batch transcription |\n| Text-to-speech | Neural voices, SSML support |\n| Speaker diarization | Identify who spoke when |\n| Custom models | Domain-specific vocabulary |\n\n## Foundry Capabilities\n\n| Feature | Description |\n|---------|-------------|\n| Model catalog | GPT-4, Llama, Mistral, custom |\n| AI agents | Multi-turn, tool calling, RAG |\n| Prompt flow | Orchestration, evaluation |\n| Fine-tuning | Custom model training |\n\n## Service Details\n\nFor deep documentation on specific services:\n\n- AI Search indexing and queries -> [Azure AI Search documentation](https://learn.microsoft.com/azure/search/search-what-is-azure-search)\n- Speech transcription patterns -> [Azure AI Speech documentation](https://learn.microsoft.com/azure/ai-services/speech-service/overview)\n- Foundry agents and flows -> [Azure AI Foundry documentation](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio)\n",
        "plugin/skills/azure-aigateway/SKILL.md": "---\nname: azure-aigateway\ndescription: Bootstrap and configure Azure API Management as an AI Gateway for securing, observing, and controlling AI models, tools (MCP Servers), and agents. Use this skill when setting up a gateway for models or tools, rate limiting model/tool requests, adding semantic caching, content safety, or load balancing to AI endpoints.\n---\n\n# Azure AI Gateway\n\nBootstrap and configure Azure API Management (APIM) as an AI Gateway for securing, observing, and controlling AI models, tools (MCP Servers), and agents.\n\n## Skill Activation Triggers\n\n**Use this skill immediately when the user asks to:**\n- \"Set up a gateway for my model\"\n- \"Set up a gateway for my tools\"\n- \"Set up a gateway for my agents\"\n- \"Add a gateway to my MCP server\"\n- \"Protect my AI model with a gateway\"\n- \"Secure my AI agents\"\n- \"Ratelimit my model requests\"\n- \"Ratelimit my tool requests\"\n- \"Limit tokens for my model\"\n- \"Add rate limiting to my MCP server\"\n- \"Enable semantic caching for my AI API\"\n- \"Add content safety to my AI endpoint\"\n- \"Add my model behind gateway\"\n- \"Import API from OpenAPI spec\"\n- \"Add API to gateway from swagger\"\n- \"Convert my API to MCP\"\n- \"Expose my API as MCP server\"\n\n**Key Indicators:**\n- User deploying Azure OpenAI, AI Foundry, or other AI models\n- User creating or managing MCP servers\n- User needs token limits, rate limiting, or quota management\n- User wants to cache AI responses to reduce costs\n- User needs content filtering or safety controls\n- User wants load balancing across multiple AI backends\n\n**Secondary Triggers (Proactive Recommendations):**\n- After model creation: Recommend AI Gateway for security, caching, and token limits\n- After MCP server creation: Recommend AI Gateway for rate limiting, content safety, and auth\n\n## Overview\n\nAzure API Management serves as an AI Gateway that provides:\n- **Security**: Authentication, authorization, and content safety\n- **Observability**: Token metrics, logging, and monitoring\n- **Control**: Rate limiting, token limits, and load balancing\n- **Optimization**: Semantic caching to reduce costs and latency\n\n```\nAI Models â”€â”€â”                      â”Œâ”€â”€ Azure OpenAI\nMCP Tools â”€â”€â”¼â”€â”€ AI Gateway (APIM) â”€â”€â”¼â”€â”€ AI Foundry\nAgents â”€â”€â”€â”€â”€â”˜                      â””â”€â”€ Custom Models\n```\n\n## Key Resources\n\n- **GitHub Repo**: https://github.com/Azure-Samples/AI-Gateway (aka.ms/aigateway)\n- **Docs**:\n  - [GenAI Gateway Capabilities](https://learn.microsoft.com/en-us/azure/api-management/genai-gateway-capabilities)\n  - [MCP Server Overview](https://learn.microsoft.com/en-us/azure/api-management/mcp-server-overview)\n  - [Azure AI Foundry API](https://learn.microsoft.com/en-us/azure/api-management/azure-ai-foundry-api)\n  - [Semantic Caching](https://learn.microsoft.com/en-us/azure/api-management/azure-openai-enable-semantic-caching)\n  - [Token Limits & LLM Logs](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-llm-logs)\n\n## Configuration Rules\n\n**Default to `Basicv2` SKU** when creating new APIM instances:\n- Cheaper than other tiers\n- Creates quickly (~5-10 minutes vs 30+ for Premium)\n- Supports all AI Gateway policies\n\n## Pattern 1: Quick Bootstrap AI Gateway\n\nDeploy APIM with Basicv2 SKU for AI workloads.\n\n```bash\n# Create resource group\naz group create --name rg-aigateway --location eastus\n\n# Deploy APIM with Bicep\naz deployment group create \\\n  --resource-group rg-aigateway \\\n  --template-file main.bicep \\\n  --parameters apimSku=Basicv2\n```\n\n### Bicep Template\n\n```bicep\nparam location string = resourceGroup().location\nparam apimSku string = 'Basicv2'\nparam apimManagedIdentityType string = 'SystemAssigned'\n\n// NOTE: Using 2024-06-01-preview because Basicv2 SKU support currently requires this preview API version.\n//       Update to the latest stable (GA) API version once Basicv2 is available there.\nresource apimService 'Microsoft.ApiManagement/service@2024-06-01-preview' = {\n  name: 'apim-aigateway-${uniqueString(resourceGroup().id)}'\n  location: location\n  sku: {\n    name: apimSku\n    capacity: 1\n  }\n  properties: {\n    publisherEmail: 'admin@contoso.com'\n    publisherName: 'Contoso'\n  }\n  identity: {\n    type: apimManagedIdentityType\n  }\n}\n\noutput gatewayUrl string = apimService.properties.gatewayUrl\noutput principalId string = apimService.identity.principalId\n```\n\n## Pattern 2: Semantic Caching\n\nCache similar prompts to reduce costs and latency.\n\n```xml\n<policies>\n    <inbound>\n        <base />\n        <!-- Cache lookup with 0.8 similarity threshold -->\n        <azure-openai-semantic-cache-lookup \n            score-threshold=\"0.8\" \n            embeddings-backend-id=\"embeddings-backend\" \n            embeddings-backend-auth=\"system-assigned\" />\n        <set-backend-service backend-id=\"{backend-id}\" />\n    </inbound>\n    <outbound>\n        <!-- Cache responses for 120 seconds -->\n        <azure-openai-semantic-cache-store duration=\"120\" />\n        <base />\n    </outbound>\n</policies>\n```\n\n**Options:**\n| Parameter | Range | Description |\n|-----------|-------|-------------|\n| `score-threshold` | 0.7-0.95 | Higher = stricter matching |\n| `duration` | 60-3600 | Cache TTL in seconds |\n\n## Pattern 3: Token Rate Limiting\n\nLimit tokens per minute to control costs and prevent abuse.\n\n```xml\n<policies>\n    <inbound>\n        <base />\n        <set-backend-service backend-id=\"{backend-id}\" />\n        <!-- Limit to 500 tokens per minute per subscription -->\n        <azure-openai-token-limit \n            counter-key=\"@(context.Subscription.Id)\"\n            tokens-per-minute=\"500\" \n            estimate-prompt-tokens=\"false\" \n            remaining-tokens-variable-name=\"remainingTokens\" />\n    </inbound>\n</policies>\n```\n\n**Options:**\n| Parameter | Values | Description |\n|-----------|--------|-------------|\n| `counter-key` | Subscription.Id, Request.IpAddress, custom | Grouping key for limits |\n| `tokens-per-minute` | 100-100000 | Token quota |\n| `estimate-prompt-tokens` | true/false | true = faster but less accurate |\n\n## Pattern 4: Content Safety\n\nFilter harmful content and detect jailbreak attempts.\n\n```xml\n<policies>\n    <inbound>\n        <base />\n        <set-backend-service backend-id=\"{backend-id}\" />\n        <!-- Block severity 4+ content, detect jailbreaks -->\n        <llm-content-safety backend-id=\"content-safety-backend\" shield-prompt=\"true\">\n            <categories output-type=\"EightSeverityLevels\">\n                <category name=\"Hate\" threshold=\"4\" />\n                <category name=\"Sexual\" threshold=\"4\" />\n                <category name=\"SelfHarm\" threshold=\"4\" />\n                <category name=\"Violence\" threshold=\"4\" />\n            </categories>\n            <blocklists>\n                <id>custom-blocklist</id>\n            </blocklists>\n        </llm-content-safety>\n    </inbound>\n</policies>\n```\n\n**Options:**\n| Parameter | Range | Description |\n|-----------|-------|-------------|\n| `threshold` | 0-7 | 0=safe, 7=severe |\n| `shield-prompt` | true/false | Detect jailbreak attempts |\n\n## Pattern 5: Rate Limits for MCPs/OpenAPI Tools\n\nProtect MCP servers and tools with request rate limiting.\n\n```xml\n<policies>\n    <inbound>\n        <base />\n        <!-- 10 calls per 60 seconds per IP -->\n        <rate-limit-by-key \n            calls=\"10\" \n            renewal-period=\"60\" \n            counter-key=\"@(context.Request.IpAddress)\" \n            remaining-calls-variable-name=\"remainingCalls\" />\n    </inbound>\n    <outbound>\n        <set-header name=\"X-Rate-Limit-Remaining\" exists-action=\"override\">\n            <value>@(context.Variables.GetValueOrDefault<int>(\"remainingCalls\", 0).ToString())</value>\n        </set-header>\n        <base />\n    </outbound>\n</policies>\n```\n\n## Pattern 6: Managed Identity Authentication\n\nSecure backend access with managed identity instead of API keys.\n\n```xml\n<policies>\n    <inbound>\n        <base />\n        <!-- Managed identity auth to Azure OpenAI -->\n        <authentication-managed-identity \n            resource=\"https://cognitiveservices.azure.com\" \n            output-token-variable-name=\"managed-id-access-token\" \n            ignore-error=\"false\" />\n        <set-header name=\"Authorization\" exists-action=\"override\">\n            <value>@(\"Bearer \" + (string)context.Variables[\"managed-id-access-token\"])</value>\n        </set-header>\n        <set-backend-service backend-id=\"{backend-id}\" />\n        <!-- Emit token metrics for monitoring -->\n        <azure-openai-emit-token-metric namespace=\"openai\">\n            <dimension name=\"Subscription ID\" value=\"@(context.Subscription.Id)\" />\n            <dimension name=\"Client IP\" value=\"@(context.Request.IpAddress)\" />\n            <dimension name=\"API ID\" value=\"@(context.Api.Id)\" />\n        </azure-openai-emit-token-metric>\n    </inbound>\n</policies>\n```\n\n## Pattern 7: Load Balancing with Retry\n\nDistribute load across multiple backends with automatic failover.\n\n```xml\n<policies>\n    <inbound>\n        <base />\n        <set-backend-service backend-id=\"{backend-pool-id}\" />\n    </inbound>\n    <backend>\n        <!-- Retry on 429 (rate limit) or 503 (service unavailable) -->\n        <retry count=\"2\" interval=\"0\" first-fast-retry=\"true\" \n            condition=\"@(context.Response.StatusCode == 429 || context.Response.StatusCode == 503)\">\n            <set-backend-service backend-id=\"{backend-pool-id}\" />\n            <forward-request buffer-request-body=\"true\" />\n        </retry>\n    </backend>\n    <on-error>\n        <when condition=\"@(context.Response.StatusCode == 503)\">\n            <return-response>\n                <set-status code=\"503\" reason=\"Service Unavailable\" />\n            </return-response>\n        </when>\n    </on-error>\n</policies>\n```\n\n## Pattern 8: Add AI Foundry Model Behind Gateway\n\nWhen user asks to \"add my model behind gateway\", first discover available models from Azure AI Foundry, then ask which model to add.\n\n### Step 1: Discover AI Foundry Projects and Available Models\n\n```bash\n# Set environment variables\naccountName=\"<ai-foundry-resource-name>\"\nresourceGroupName=\"<resource-group>\"\n\n# List AI Foundry resources (AI Services accounts)\naz cognitiveservices account list --query \"[?kind=='AIServices'].{name:name, resourceGroup:resourceGroup, location:location}\" -o table\n\n# List available models in the AI Foundry resource\naz cognitiveservices account list-models \\\n  -n $accountName \\\n  -g $resourceGroupName \\\n  | jq '.[] | { name: .name, format: .format, version: .version, sku: .skus[0].name, capacity: .skus[0].capacity.default }'\n\n# List already deployed models\naz cognitiveservices account deployment list \\\n  -n $accountName \\\n  -g $resourceGroupName\n```\n\n### Step 2: Ask User Which Model to Add\n\nAfter listing the available models, **use the ask_user tool** to present the models as choices and let the user select which model to add behind the gateway.\n\nExample choices to present:\n- Model deployments from the discovered list\n- Include model name, format (provider), version, and SKU info\n\n### Step 3: Deploy the Model (if not already deployed)\n\n```bash\n# Deploy the selected model to AI Foundry\naz cognitiveservices account deployment create \\\n  -n $accountName \\\n  -g $resourceGroupName \\\n  --deployment-name <model-name> \\\n  --model-name <model-name> \\\n  --model-version <version> \\\n  --model-format <format> \\\n  --sku-capacity 1 \\\n  --sku-name <sku>\n```\n\n### Step 4: Configure APIM Backend for Selected Model\n\n```bash\n# Get the AI Foundry inference endpoint\nENDPOINT=$(az cognitiveservices account show \\\n  -n $accountName \\\n  -g $resourceGroupName \\\n  | jq -r '.properties.endpoints[\"Azure AI Model Inference API\"]')\n\n# Create APIM backend for the selected model\naz apim backend create \\\n  --resource-group <apim-resource-group> \\\n  --service-name <apim-service-name> \\\n  --backend-id <model-deployment-name>-backend \\\n  --protocol http \\\n  --url \"${ENDPOINT}\"\n```\n\n### Step 5: Create API and Apply Policies\n\n```bash\n# Import Azure OpenAI API specification\naz apim api import \\\n  --resource-group <apim-resource-group> \\\n  --service-name <apim-service-name> \\\n  --path <model-deployment-name> \\\n  --specification-format OpenApiJson \\\n  --specification-url \"https://raw.githubusercontent.com/Azure/azure-rest-api-specs/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/stable/2024-02-01/inference.json\"\n```\n\n### Step 6: Grant APIM Access to AI Foundry\n\n```bash\n# Get APIM managed identity principal ID\nAPIM_PRINCIPAL_ID=$(az apim show \\\n  --name <apim-service-name> \\\n  --resource-group <apim-resource-group> \\\n  --query \"identity.principalId\" -o tsv)\n\n# Get AI Foundry resource ID\nAI_RESOURCE_ID=$(az cognitiveservices account show \\\n  -n $accountName \\\n  -g $resourceGroupName \\\n  --query \"id\" -o tsv)\n\n# Assign Cognitive Services User role\naz role assignment create \\\n  --assignee $APIM_PRINCIPAL_ID \\\n  --role \"Cognitive Services User\" \\\n  --scope $AI_RESOURCE_ID\n```\n\n### Bicep Template for Backend Configuration\n\n```bicep\nparam apimServiceName string\nparam backendId string\nparam aiFoundryEndpoint string\nparam modelDeploymentName string\n\nresource apimService 'Microsoft.ApiManagement/service@2024-06-01-preview' existing = {\n  name: apimServiceName\n}\n\nresource backend 'Microsoft.ApiManagement/service/backends@2024-06-01-preview' = {\n  parent: apimService\n  name: backendId\n  properties: {\n    protocol: 'http'\n    url: '${aiFoundryEndpoint}openai/deployments/${modelDeploymentName}'\n    credentials: {\n      header: {}\n    }\n    tls: {\n      validateCertificateChain: true\n      validateCertificateName: true\n    }\n  }\n}\n```\n\n## Pattern 9: Import API from OpenAPI Specification\n\nAdd an API to the gateway from an OpenAPI/Swagger specification, either from a local file or web URL.\n\n### Step 1: Import API from Web URL\n\n```bash\n# Import API from a publicly accessible OpenAPI spec URL\naz apim api import \\\n  --resource-group <apim-resource-group> \\\n  --service-name <apim-service-name> \\\n  --api-id <api-id> \\\n  --path <api-path> \\\n  --display-name \"<API Display Name>\" \\\n  --specification-format OpenApiJson \\\n  --specification-url \"https://example.com/openapi.json\"\n```\n\n### Step 2: Import API from Local File\n\n```bash\n# Import API from a local OpenAPI spec file (JSON or YAML)\naz apim api import \\\n  --resource-group <apim-resource-group> \\\n  --service-name <apim-service-name> \\\n  --api-id <api-id> \\\n  --path <api-path> \\\n  --display-name \"<API Display Name>\" \\\n  --specification-format OpenApi \\\n  --specification-path \"./openapi.yaml\"\n```\n\n### Step 3: Configure Backend for the API\n\n```bash\n# Create backend pointing to your API server\naz apim backend create \\\n  --resource-group <apim-resource-group> \\\n  --service-name <apim-service-name> \\\n  --backend-id <backend-id> \\\n  --protocol http \\\n  --url \"https://your-api-server.com\"\n\n# Update API to use the backend\naz apim api update \\\n  --resource-group <apim-resource-group> \\\n  --service-name <apim-service-name> \\\n  --api-id <api-id> \\\n  --set properties.serviceUrl=\"https://your-api-server.com\"\n```\n\n### Step 4: Apply Policies (Optional)\n\n```xml\n<policies>\n    <inbound>\n        <base />\n        <set-backend-service backend-id=\"{backend-id}\" />\n        <!-- Add rate limiting -->\n        <rate-limit-by-key \n            calls=\"100\" \n            renewal-period=\"60\" \n            counter-key=\"@(context.Request.IpAddress)\" />\n    </inbound>\n    <outbound>\n        <base />\n    </outbound>\n</policies>\n```\n\n### Supported Specification Formats\n\n| Format | Value | File Extension |\n|--------|-------|----------------|\n| OpenAPI 3.x JSON | `OpenApiJson` | `.json` |\n| OpenAPI 3.x YAML | `OpenApi` | `.yaml`, `.yml` |\n| Swagger 2.0 JSON | `SwaggerJson` | `.json` |\n| Swagger 2.0 (link) | `SwaggerLinkJson` | URL |\n| WSDL | `Wsdl` | `.wsdl` |\n| WADL | `Wadl` | `.wadl` |\n\n## Pattern 10: Convert API to MCP Server\n\nConvert existing APIM API operations into an MCP (Model Context Protocol) server, enabling AI agents to discover and use your APIs as tools.\n\n### Prerequisites\n\n- APIM instance with Basicv2 SKU or higher\n- Existing API imported into APIM\n- MCP feature enabled on APIM\n\n### Step 1: List Existing APIs in APIM\n\n```bash\n# List all APIs in APIM\naz apim api list \\\n  --resource-group <apim-resource-group> \\\n  --service-name <apim-service-name> \\\n  --query \"[].{id:name, displayName:displayName, path:path}\" \\\n  -o table\n```\n\n### Step 2: Ask User Which API to Convert\n\nAfter listing the APIs, **use the ask_user tool** to let the user select which API to convert to an MCP server.\n\n### Step 3: List API Operations\n\n```bash\n# List all operations for the selected API\naz apim api operation list \\\n  --resource-group <apim-resource-group> \\\n  --service-name <apim-service-name> \\\n  --api-id <api-id> \\\n  --query \"[].{operationId:name, displayName:displayName, method:method, urlTemplate:urlTemplate}\" \\\n  -o table\n```\n\n### Step 4: Ask User Which Operations to Expose as MCP Tools\n\nAfter listing the operations, **use the ask_user tool** to present the operations as choices. Let the user select which operations to expose as MCP tools. Users may want to expose all operations or only a subset.\n\nExample choices to present:\n- All operations (convert entire API)\n- Individual operations from the discovered list\n- Include operation name, method, and URL template\n\n### Step 5: Enable MCP Server on APIM\n\n```bash\n# Enable MCP server capability (via ARM/Bicep or Portal)\n# Note: MCP configuration is done via APIM policies and product configuration\n```\n\n### Step 6: Configure MCP Endpoint for API\n\nCreate an MCP-compatible endpoint that exposes your API operations as tools:\n\n```xml\n<policies>\n    <inbound>\n        <base />\n        <!-- MCP tools/list endpoint handler -->\n        <choose>\n            <when condition=\"@(context.Request.Url.Path.EndsWith(\"/mcp/tools/list\"))\">\n                <return-response>\n                    <set-status code=\"200\" reason=\"OK\" />\n                    <set-header name=\"Content-Type\" exists-action=\"override\">\n                        <value>application/json</value>\n                    </set-header>\n                    <set-body>@{\n                        var tools = new JArray();\n                        // Define your API operations as MCP tools\n                        tools.Add(new JObject(\n                            new JProperty(\"name\", \"operation_name\"),\n                            new JProperty(\"description\", \"Description of what this operation does\"),\n                            new JProperty(\"inputSchema\", new JObject(\n                                new JProperty(\"type\", \"object\"),\n                                new JProperty(\"properties\", new JObject(\n                                    new JProperty(\"param1\", new JObject(\n                                        new JProperty(\"type\", \"string\"),\n                                        new JProperty(\"description\", \"Parameter description\")\n                                    ))\n                                ))\n                            ))\n                        ));\n                        return new JObject(new JProperty(\"tools\", tools)).ToString();\n                    }</set-body>\n                </return-response>\n            </when>\n        </choose>\n    </inbound>\n</policies>\n```\n\n### Step 7: Bicep Template for MCP-Enabled API\n\n```bicep\nparam apimServiceName string\nparam apiId string\nparam apiDisplayName string\nparam apiPath string\nparam backendUrl string\n\nresource apimService 'Microsoft.ApiManagement/service@2024-06-01-preview' existing = {\n  name: apimServiceName\n}\n\nresource api 'Microsoft.ApiManagement/service/apis@2024-06-01-preview' = {\n  parent: apimService\n  name: apiId\n  properties: {\n    displayName: apiDisplayName\n    path: apiPath\n    protocols: ['https']\n    serviceUrl: backendUrl\n    subscriptionRequired: true\n    // MCP endpoints\n    apiType: 'http'\n  }\n}\n\n// MCP tools/list operation\nresource mcpToolsListOperation 'Microsoft.ApiManagement/service/apis/operations@2024-06-01-preview' = {\n  parent: api\n  name: 'mcp-tools-list'\n  properties: {\n    displayName: 'MCP Tools List'\n    method: 'POST'\n    urlTemplate: '/mcp/tools/list'\n    description: 'List available MCP tools'\n  }\n}\n\n// MCP tools/call operation\nresource mcpToolsCallOperation 'Microsoft.ApiManagement/service/apis/operations@2024-06-01-preview' = {\n  parent: api\n  name: 'mcp-tools-call'\n  properties: {\n    displayName: 'MCP Tools Call'\n    method: 'POST'\n    urlTemplate: '/mcp/tools/call'\n    description: 'Call an MCP tool'\n  }\n}\n```\n\n### Step 8: Test MCP Endpoint\n\n```bash\n# Get APIM gateway URL\nGATEWAY_URL=$(az apim show \\\n  --name <apim-service-name> \\\n  --resource-group <apim-resource-group> \\\n  --query \"gatewayUrl\" -o tsv)\n\n# Test MCP tools/list endpoint\ncurl -X POST \"${GATEWAY_URL}/<api-path>/mcp/tools/list\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Ocp-Apim-Subscription-Key: <subscription-key>\" \\\n  -d '{}'\n```\n\n### MCP Tool Definition Schema\n\nWhen converting API operations to MCP tools, use this schema:\n\n```json\n{\n  \"tools\": [\n    {\n      \"name\": \"get_weather\",\n      \"description\": \"Get current weather for a location\",\n      \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"string\",\n            \"description\": \"City name or coordinates\"\n          }\n        },\n        \"required\": [\"location\"]\n      }\n    }\n  ]\n}\n```\n\n### Reference\n\n- [MCP Server Overview](https://learn.microsoft.com/en-us/azure/api-management/mcp-server-overview)\n- [MCP from API Lab](https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/mcp-from-api)\n\n## Lab References (AI-Gateway Repo)\n\n**Essential Labs to Get Started:**\n\n| Scenario | Lab | Description |\n|----------|-----|-------------|\n| Semantic Caching | [semantic-caching](https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/semantic-caching) | Cache similar prompts to reduce costs |\n| Token Rate Limiting | [token-rate-limiting](https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/token-rate-limiting) | Limit tokens per minute |\n| Content Safety | [content-safety](https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/content-safety) | Filter harmful content |\n| Load Balancing | [backend-pool-load-balancing](https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/backend-pool-load-balancing) | Distribute load across backends |\n| MCP from API | [mcp-from-api](https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/mcp-from-api) | Convert OpenAPI to MCP server |\n| Zero to Production | [zero-to-production](https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/zero-to-production) | Complete production setup guide |\n\n**Find more labs at:** https://github.com/Azure-Samples/AI-Gateway/tree/main/labs\n\n## Quick Start Checklist\n\n### Prerequisites\n- [ ] Azure subscription created\n- [ ] Azure CLI installed and authenticated (`az login`)\n- [ ] Resource group created for AI Gateway resources\n\n### Deployment\n- [ ] Deploy APIM with Basicv2 SKU\n- [ ] Configure managed identity\n- [ ] Add backend for Azure OpenAI or AI Foundry\n- [ ] Apply policies (caching, rate limits, content safety)\n\n### Verification\n- [ ] Test API endpoint through gateway\n- [ ] Verify token metrics in Application Insights\n- [ ] Check rate limiting headers in response\n- [ ] Validate content safety filtering\n\n## Best Practices\n\n| Practice | Description |\n|----------|-------------|\n| **Default to Basicv2** | Use Basicv2 SKU for cost/speed optimization |\n| **Use managed identity** | Prefer managed identity over API keys for backend auth |\n| **Enable token metrics** | Use `azure-openai-emit-token-metric` for cost tracking |\n| **Semantic caching** | Cache similar prompts to reduce costs (60-80% savings possible) |\n| **Rate limit by key** | Use subscription ID or IP for granular rate limiting |\n| **Content safety** | Enable `shield-prompt` to detect jailbreak attempts |\n\n## Troubleshooting\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| **Slow APIM creation** | Deployment takes 30+ minutes | Use Basicv2 SKU instead of Premium |\n| **Token limit exceeded** | 429 response | Increase `tokens-per-minute` or add load balancing |\n| **Cache not working** | No cache hits | Lower `score-threshold` (e.g., 0.7) |\n| **Content blocked** | False positives | Increase category thresholds |\n| **Backend auth fails** | 401 from Azure OpenAI | Assign Cognitive Services User role to APIM managed identity |\n| **Rate limit too strict** | Legitimate requests blocked | Increase `calls` or `renewal-period` |\n\n## Additional Resources\n\n- [Azure API Management Documentation](https://learn.microsoft.com/azure/api-management/)\n- [AI Gateway Samples Repository](https://github.com/Azure-Samples/AI-Gateway)\n- [APIM Policies Reference](https://learn.microsoft.com/azure/api-management/api-management-policies)\n- [Azure OpenAI Integration](https://learn.microsoft.com/azure/api-management/azure-openai-api-from-specification)\n",
        "plugin/skills/azure-cosmos-db/SKILL.md": "---\nname: azure-cosmos-db\ndescription: Build globally distributed applications with Azure Cosmos DB, a fully managed NoSQL database with single-digit millisecond latency and multiple API support\n---\n\n# Azure Data Services\n\n## Services\n\n| Service | Use When | MCP Tools | CLI |\n|---------|----------|-----------|-----|\n| Cosmos DB | NoSQL documents, global distribution, vector search | `azure__cosmos` | `az cosmosdb` |\n| SQL Database | Relational data, ACID transactions, complex joins | `azure__sql` | `az sql` |\n| Redis Cache | Caching, sessions, real-time leaderboards | `azure__redis` | `az redis` |\n| PostgreSQL | Open source relational, PostGIS | `azure__postgres` | `az postgres` |\n| MySQL | LAMP stack, WordPress | `azure__mysql` | `az mysql` |\n\n## MCP Server (Preferred)\n\nWhen Azure MCP is enabled, use these tools for data operations:\n\n### Cosmos DB\n- `azure__cosmos` with command `cosmos_account_list` - List Cosmos DB accounts\n- `azure__cosmos` with command `cosmos_database_list` - List databases in account\n- `azure__cosmos` with command `cosmos_container_list` - List containers\n\n### SQL Database\n- `azure__sql` with command `sql_server_list` - List SQL servers\n- `azure__sql` with command `sql_database_list` - List databases on server\n- `azure__sql` with command `sql_firewall_list` - List firewall rules\n\n### Redis\n- `azure__redis` with command `redis_cache_list` - List Redis caches\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n## CLI Fallback\n\n```bash\n# Cosmos DB\naz cosmosdb list --output table\naz cosmosdb sql database list --account-name ACCOUNT -g RG\n\n# SQL Database\naz sql server list --output table\naz sql db list --server SERVER -g RG\n\n# Redis\naz redis list --output table\n```\n\n## Choosing the Right Database\n\n| If you need... | Use |\n|----------------|-----|\n| Global distribution, <10ms latency | Cosmos DB |\n| Complex SQL queries, ACID transactions | SQL Database |\n| Caching layer, session state | Redis Cache |\n| PostgreSQL compatibility | Azure PostgreSQL |\n| MySQL compatibility | Azure MySQL |\n\n---\n\n# Azure Cosmos DB\n\n## Quick Reference\n\n| Property | Value |\n|----------|-------|\n| CLI prefix | `az cosmosdb` |\n| MCP tools | `azure__cosmos` (commands: `cosmos_account_list`, `cosmos_database_list`, `cosmos_container_list`) |\n| Best for | JSON documents, global distribution, <10ms reads |\n\n## API Selection\n\n| API | Data Model | Use Case |\n|-----|-----------|----------|\n| NoSQL (Core) | Document | Most scenarios, native |\n| MongoDB | Document | MongoDB compatibility |\n| PostgreSQL | Relational | Distributed PostgreSQL |\n| Cassandra | Wide-column | Cassandra workloads |\n| Gremlin | Graph | Graph relationships |\n\n## Partitioning Strategy\n\n**Partition key selection is critical:**\n\nGood partition keys:\n- High cardinality (many distinct values)\n- Even distribution\n- Included in most queries\n\nExamples:\n- `userId` for user data\n- `tenantId` for multi-tenant\n- `deviceId` for IoT\n\nBad partition keys:\n- `status` (few values)\n- `timestamp` (hot partition)\n- `region` (uneven distribution)\n\n## Consistency Levels\n\n| Level | Guarantees | Use Case |\n|-------|-----------|----------|\n| Strong | Linearizable | Financial transactions |\n| Bounded Staleness | Bounded delay | Inventory systems |\n| Session | Session consistency | Most applications (default) |\n| Consistent Prefix | Order preserved | Audit logs |\n| Eventual | No guarantees | Recommendations |\n\n## Request Units (RUs)\n\nRU = normalized cost of operations:\n- Point read (1KB): 1 RU\n- Write (1KB): ~5 RUs\n- Query: varies by complexity\n\n**Throughput options:**\n- Provisioned: Set RU/s, consistent cost\n- Autoscale: 10-100% of max, automatic\n- Serverless: Pay per request\n\n## Common Patterns\n\n### Query with Filters\n\nMCP:\n```\nUse azure__cosmos tools to browse accounts, databases, and containers.\nFor queries, use the Azure portal or SDK.\n```\n\nCLI:\n```bash\naz cosmosdb sql query \\\n  --account-name ACCOUNT \\\n  --database-name DB \\\n  --container-name CONTAINER \\\n  --query \"SELECT * FROM c WHERE c.status = 'active'\"\n```\n\n### Cross-Partition Queries\n\nEnable with `--enable-cross-partition-query true` in CLI.\nAvoid when possible - they're expensive.\n\n## Cost Optimization\n\n1. Use autoscale instead of manual provisioning\n2. Optimize partition keys to avoid cross-partition queries\n3. Enable TTL for automatic data expiration\n4. Use analytical store for analytics workloads\n5. Consider reserved capacity for 1-3 year commitment\n\n## Gotchas\n\n1. **RU consumption** - Complex queries consume more RUs\n2. **Partition limits** - 20GB per logical partition\n3. **Index policy** - Review and customize for your queries\n4. **Consistency trade-offs** - Stronger = higher latency\n",
        "plugin/skills/azure-cost-optimization/SKILL.md": "---\nname: azure-cost-optimization\ndescription: Identify and quantify cost savings across Azure subscriptions by analyzing actual costs, utilization metrics, and generating actionable optimization recommendations. Use this skill when analyzing Azure spending, finding waste, rightsizing resources, or generating cost optimization reports.\n---\n\n# Azure Cost Optimization Skill\n\nAnalyze Azure subscriptions to identify cost savings through orphaned resource cleanup, rightsizing, and optimization recommendations based on actual usage data.\n\n## When to Use This Skill\n\nUse this skill when the user asks to:\n- Optimize Azure costs or reduce spending\n- Analyze Azure subscription for cost savings\n- Generate cost optimization report\n- Find orphaned or unused resources\n- Rightsize Azure VMs, containers, or services\n- Identify where they're overspending in Azure\n- **Optimize Redis costs specifically** - See [Azure Redis Cost Optimization](./references/azure-redis.md) for Redis-specific analysis\n\n## Instructions\n\nFollow these steps in conversation with the user:\n\n### Step 0: Validate Prerequisites\n\nBefore starting, verify these tools and permissions are available:\n\n**Required Tools:**\n- Azure CLI installed and authenticated (`az login`)\n- Azure CLI extensions: `costmanagement`, `resource-graph`\n- Azure Quick Review (azqr) installed - See [Azure Quick Review](./references/azure-quick-review.md) for details\n\n**Required Permissions:**\n- Cost Management Reader role\n- Monitoring Reader role\n- Reader role on subscription/resource group\n\n**Verification commands:**\n```powershell\naz --version\naz account show\naz extension show --name costmanagement\nazqr version\n```\n\n### Step 1: Load Best Practices\n\nGet Azure cost optimization best practices to inform recommendations:\n\n```javascript\n// Use Azure MCP best practices tool\nmcp_azure_mcp_get_azure_bestpractices({\n  intent: \"Get cost optimization best practices\",\n  command: \"get_bestpractices\",\n  parameters: { resource: \"cost-optimization\", action: \"all\" }\n})\n```\n\n### Step 1.5: Redis-Specific Analysis (Conditional)\n\n**If the user specifically requests Redis cost optimization**, use the specialized Redis skill:\n\nðŸ“‹ **Reference**: [Azure Redis Cost Optimization](./references/azure-redis.md)\n\n**When to use Redis-specific analysis:**\n- User mentions \"Redis\", \"Azure Cache for Redis\", or \"Azure Managed Redis\"\n- Focus is on Redis resource optimization, not general subscription analysis\n- User wants Redis-specific recommendations (SKU downgrade, failed caches, etc.)\n\n**Key capabilities:**\n- Interactive subscription filtering (prefix, ID, or \"all subscriptions\")\n- Redis-specific optimization rules (failed caches, oversized tiers, missing tags)\n- Pre-built report templates for Redis cost analysis\n- Uses `redis_list` command\n\n**Report templates available:**\n- [Subscription-level Redis summary](./templates/redis-subscription-level-report.md)\n- [Detailed Redis cache analysis](./templates/redis-detailed-cache-analysis.md)\n\n> **Note**: For general subscription-wide cost optimization (including Redis), continue with Step 2. For Redis-only focused analysis, follow the instructions in the Redis-specific reference document.\n### Step 1.6: Choose Analysis Scope (for Redis-specific analysis)\n\n**If performing Redis cost optimization**, ask the user to select their analysis scope:\n\n**Prompt the user with these options:**\n1. **Specific Subscription ID** - Analyze a single subscription\n2. **Subscription Name** - Use display name instead of ID\n3. **Subscription Prefix** - Analyze all subscriptions starting with a prefix (e.g., \"CacheTeam\")\n4. **All My Subscriptions** - Scan all accessible subscriptions\n5. **Tenant-wide** - Analyze entire organization\n\nWait for user response before proceeding to Step 2.\n\n### Step 2: Run Azure Quick Review\n\nRun azqr to find orphaned resources (immediate cost savings):\n\nðŸ“‹ **Reference**: [Azure Quick Review](./references/azure-quick-review.md) - Detailed instructions for running azqr scans\n\n```javascript\n// Use Azure MCP extension_azqr tool\nextension_azqr({\n  subscription: \"<SUBSCRIPTION_ID>\",\n  \"resource-group\": \"<RESOURCE_GROUP>\"  // optional\n})\n```\n\n**What to look for in azqr results:**\n- Orphaned resources: unattached disks, unused NICs, idle NAT gateways\n- Over-provisioned resources: excessive retention periods, oversized SKUs\n- Missing cost tags: resources without proper cost allocation\n\n> **Note**: The Azure Quick Review reference document includes instructions for creating filter configurations, saving output to the `output/` folder, and interpreting results for cost optimization.\n\n### Step 3: Discover Resources\n\nList all resources in the subscription using Azure MCP tools or CLI:\n\n```powershell\n# Get subscription info\naz account show\n\n# List all resources\naz resource list --subscription \"<SUBSCRIPTION_ID>\" --resource-group \"<RESOURCE_GROUP>\"\n\n# Use MCP tools for specific services (preferred):\n# - Storage accounts, Cosmos DB, Key Vaults: use Azure MCP tools\n# - Redis caches: use mcp_azure_mcp_redis tool (see ./references/azure-redis.md)\n# - Web apps, VMs, SQL: use az CLI commands\n```\n\n### Step 4: Query Actual Costs\n\nGet actual cost data from Azure Cost Management API (last 30 days):\n\n**Create cost query file:**\n\nCreate `temp/cost-query.json` with:\n```json\n{\n  \"type\": \"ActualCost\",\n  \"timeframe\": \"Custom\",\n  \"timePeriod\": {\n    \"from\": \"<START_DATE>\",  \n    \"to\": \"<END_DATE>\"\n  },\n  \"dataset\": {\n    \"granularity\": \"None\",\n    \"aggregation\": {\n      \"totalCost\": {\n        \"name\": \"Cost\",\n        \"function\": \"Sum\"\n      }\n    },\n    \"grouping\": [\n      {\n        \"type\": \"Dimension\",\n        \"name\": \"ResourceId\"\n      }\n    ]\n  }\n}\n```\n\n> **Action Required**: Calculate `<START_DATE>` (30 days ago) and `<END_DATE>` (today) in ISO 8601 format (e.g., `2025-11-03T00:00:00Z`).\n\n**Execute cost query:**\n```powershell\n# Create temp folder\nNew-Item -ItemType Directory -Path \"temp\" -Force\n\n# Query using REST API (more reliable than az costmanagement query)\naz rest --method post `\n  --url \"https://management.azure.com/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/providers/Microsoft.CostManagement/query?api-version=2023-11-01\" `\n  --body '@temp/cost-query.json'\n```\n\n**Important:** Save the query results to `output/cost-query-result<timestamp>.json` for audit trail.\n\n### Step 5: Validate Pricing\n\nFetch current pricing from official Azure pricing pages using `fetch_webpage`:\n\n```javascript\n// Validate pricing for key services\nfetch_webpage({\n  urls: [\"https://azure.microsoft.com/en-us/pricing/details/container-apps/\"],\n  query: \"pricing tiers and costs\"\n})\n```\n\n**Key services to validate:**\n- Container Apps: https://azure.microsoft.com/pricing/details/container-apps/\n- Virtual Machines: https://azure.microsoft.com/pricing/details/virtual-machines/\n- App Service: https://azure.microsoft.com/pricing/details/app-service/\n- Log Analytics: https://azure.microsoft.com/pricing/details/monitor/\n\n> **Important**: Check for free tier allowances - many Azure services have generous free limits that may explain $0 costs.\n\n### Step 6: Collect Utilization Metrics\n\nQuery Azure Monitor for utilization data (last 14 days) to support rightsizing recommendations:\n\n```powershell\n# Calculate dates for last 14 days\n$startTime = (Get-Date).AddDays(-14).ToString(\"yyyy-MM-ddTHH:mm:ssZ\")\n$endTime = Get-Date -Format \"yyyy-MM-ddTHH:mm:ssZ\"\n\n# VM CPU utilization\naz monitor metrics list `\n  --resource \"<RESOURCE_ID>\" `\n  --metric \"Percentage CPU\" `\n  --interval PT1H `\n  --aggregation Average `\n  --start-time $startTime `\n  --end-time $endTime\n\n# App Service Plan utilization\naz monitor metrics list `\n  --resource \"<RESOURCE_ID>\" `\n  --metric \"CpuTime,Requests\" `\n  --interval PT1H `\n  --aggregation Total `\n  --start-time $startTime `\n  --end-time $endTime\n\n# Storage capacity\naz monitor metrics list `\n  --resource \"<RESOURCE_ID>\" `\n  --metric \"UsedCapacity,BlobCount\" `\n  --interval PT1H `\n  --aggregation Average `\n  --start-time $startTime `\n  --end-time $endTime\n```\n\n### Step 7: Generate Optimization Report\n\nCreate a comprehensive cost optimization report in the `output/` folder:\n\n**Use the `create_file` tool** with path `output/costoptimizereport<YYYYMMDD_HHMMSS>.md`:\n\n**Report Structure:**\n```markdown\n# Azure Cost Optimization Report\n**Generated**: <timestamp>\n\n## Executive Summary\n- Total Monthly Cost: $X (ðŸ’° ACTUAL DATA)\n- Top Cost Drivers: [List top 3 resources with Azure Portal links]\n\n## Cost Breakdown\n[Table with top 10 resources by cost, including Azure Portal links]\n\n## Free Tier Analysis\n[Resources operating within free tiers showing $0 cost]\n\n## Orphaned Resources (Immediate Savings)\n[From azqr - resources that can be deleted immediately]\n- Resource name with Portal link - $X/month savings\n\n## Optimization Recommendations\n\n### Priority 1: High Impact, Low Risk\n[Example: Delete orphaned resources]\n- ðŸ’° ACTUAL cost: $X/month\n- ðŸ“Š ESTIMATED savings: $Y/month\n- Commands to execute (with warnings)\n\n### Priority 2: Medium Impact, Medium Risk\n[Example: Rightsize VM from D4s_v5 to D2s_v5]\n- ðŸ’° ACTUAL baseline: D4s_v5, $X/month\n- ðŸ“ˆ ACTUAL metrics: CPU 8%, Memory 30%\n- ðŸ’µ VALIDATED pricing: D4s_v5 $Y/hr, D2s_v5 $Z/hr\n- ðŸ“Š ESTIMATED savings: $S/month\n- Commands to execute\n\n### Priority 3: Long-term Optimization\n[Example: Reserved Instances, Storage tiering]\n\n## Total Estimated Savings\n- Monthly: $X\n- Annual: $Y\n\n## Implementation Commands\n[Safe commands with approval warnings]\n\n## Validation Appendix\n\n### Data Sources and Files\n- **Cost Query Results**: `output/cost-query-result<timestamp>.json`\n  - Raw cost data from Azure Cost Management API\n  - Audit trail proving actual costs at report generation time\n  - Keep for at least 12 months for historical comparison\n  - Contains every resource's exact cost over the analysis period\n- **Pricing Sources**: [Links to Azure pricing pages]\n- **Free Tier Allowances**: [Applicable allowances]\n\n> **Note**: The `temp/cost-query.json` file (if present) is a temporary query template and can be safely deleted. All permanent audit data is in the `output/` folder.\n```\n\n**Portal Link Format:**\n```\nhttps://portal.azure.com/#@<TENANT_ID>/resource/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/providers/<RESOURCE_PROVIDER>/<RESOURCE_TYPE>/<RESOURCE_NAME>/overview\n```\n\n### Step 8: Save Audit Trail\n\nSave all cost query results for validation:\n\n**Use the `create_file` tool** with path `output/cost-query-result<YYYYMMDD_HHMMSS>.json`:\n\n```json\n{\n  \"timestamp\": \"<ISO_8601>\",\n  \"subscription\": \"<SUBSCRIPTION_ID>\",\n  \"resourceGroup\": \"<RESOURCE_GROUP>\",\n  \"queries\": [\n    {\n      \"queryType\": \"ActualCost\",\n      \"timeframe\": \"MonthToDate\",\n      \"query\": { },\n      \"response\": { }\n    }\n  ]\n}\n```\n\n### Step 9: Clean Up Temporary Files\n\nRemove temporary query files and folder after the report is generated:\n\n```powershell\n# Delete entire temp folder (no longer needed)\nRemove-Item -Path \"temp\" -Recurse -Force -ErrorAction SilentlyContinue\n```\n\n> **Note**: The `temp/cost-query.json` file is only needed during API execution. The actual query and results are preserved in `output/cost-query-result*.json` for audit purposes.\n\n## Output\n\nThe skill generates:\n1. **Cost Optimization Report** (`output/costoptimizereport<timestamp>.md`)\n   - Executive summary with total costs and top drivers\n   - Detailed cost breakdown with Azure Portal links\n   - Prioritized recommendations with actual data and estimated savings\n   - Implementation commands with safety warnings\n\n2. **Cost Query Results** (`output/cost-query-result<timestamp>.json`)\n   - Audit trail of all cost queries and responses\n   - Validation evidence for recommendations\n\n## Important Notes\n\n### Data Classification\n- ðŸ’° **ACTUAL DATA** = Retrieved from Azure Cost Management API\n- ðŸ“ˆ **ACTUAL METRICS** = Retrieved from Azure Monitor\n- ðŸ’µ **VALIDATED PRICING** = Retrieved from official Azure pricing pages\n- ðŸ“Š **ESTIMATED SAVINGS** = Calculated based on actual data and validated pricing\n\n### Best Practices\n- Always query actual costs first - never estimate or assume\n- Validate pricing from official sources - account for free tiers\n- Use REST API for cost queries (more reliable than `az costmanagement query`)\n- Save audit trail - include all queries and responses\n- Include Azure Portal links for all resources\n- Use UTF-8 encoding when creating report files\n- For costs < $10/month, emphasize operational improvements over financial savings\n- Never execute destructive operations without explicit approval\n\n### Common Pitfalls\n- **Assuming costs**: Always query actual data from Cost Management API\n- **Ignoring free tiers**: Many services have generous allowances (e.g., Container Apps: 180K vCPU-sec free/month)\n- **Using wrong date ranges**: 30 days for costs, 14 days for utilization\n- **Broken Portal links**: Verify tenant ID and resource ID format\n- **Cost query failures**: Use `az rest` with JSON body, not `az costmanagement query`\n\n### Safety Requirements\n- Get approval before deleting resources\n- Test changes in non-production first\n- Provide dry-run commands for validation\n- Include rollback procedures\n- Monitor impact after implementation\n",
        "plugin/skills/azure-cost-optimization/references/azure-quick-review.md": "## Azure Quick Review (azqr) for Cost Optimization\n\nAzure Quick Review (azqr) generates compliance and governance reports that identify cost-impacting issues and orphaned resources.\n\n## Create Filters Configuration\n\nCreate a `filters.yaml` file to focus the scan on cost optimization:\n\n```yaml\nincludeSections:\n  - Costs\n  - Advisor\n  - Inventory\n  - Orphaned\nexcludeSections:\n  - Recommendations\n  - AzurePolicy\n  - DefenderRecommendations\n```\n\n## Run the azqr Scan\n\nExecute the scan using Azure MCP or CLI:\n\n```powershell\n# Via Azure MCP (preferred if available)\n# Use the extension_azqr tool with subscription and optional resource-group parameters\n\n# Or via direct CLI:\nazqr scan --subscription \"<SUBSCRIPTION_ID>\" --resource-group \"<RESOURCE_GROUP>\" --filters ./filters.yaml --output json\n```\n\n## Save Output\n\nSave all generated files to the `output/` folder:\n1. Create the folder: `mkdir output` (if it doesn't exist)\n2. Save the azqr report as: `output/azqr_report_<YYYYMMDD_HHMMSS>.json`\n3. After the scan completes, delete the temporary `filters.yaml` file\n\n## Report Output\n\nThe scan generates a JSON report with recommendations categorized by impact level (High/Medium/Low), including:\n- Orphaned resources (NICs, disks, IPs)\n- Azure Advisor cost recommendations  \n- Resource inventory\n- Cost breakdown by resource\n\n## Notes\n\n- azqr provides qualitative governance recommendations\n- Always validate findings with actual cost data before making changes\n- The tool requires Reader role on the subscription or resource group\n- Save reports to `output/` folder with timestamps for audit trail\n",
        "plugin/skills/azure-cost-optimization/references/azure-redis.md": "## Azure Redis Cost Optimization\n\nReference guide for identifying cost savings opportunities in Azure Redis deployments through analysis and targeted scans.\n\n## Subscription Input Options\n\nAccept any of these identifiers to identify subscriptions for analysis:\n\n| Input Type | Example | Use Case |\n|------------|---------|----------|\n| **Subscription ID** | `a1b2c3d4-...` | Analyze specific subscription |\n| **Subscription Name** | `Production-Environment` | User-friendly identifier |\n| **Subscription Prefix** | `CacheTeam -` | Analyze all team subscriptions |\n| **Tenant ID** | `tenant-guid` | Analyze entire organization |\n| **\"All my subscriptions\"** | (keyword) | Scan all accessible subscriptions |\n\n## Cost Optimization Rules\n\nWhen analyzing each cache, apply these prioritized rules:\n\n| Priority | Rule | Detection Logic | Recommendation | Avg Savings |\n|----------|------|----------------|----------------|-------------|\n| ðŸ”´ Critical | Failed Cache | `provisioningState == 'Failed'` | Delete immediately | $50-300/mo |\n| ðŸ”´ Critical | Stuck Creating | `provisioningState == 'Creating'` AND age >4 hours | Delete/support ticket | $50-300/mo |\n| ðŸŸ  High | Premium in Dev | `sku.name == 'Premium'` AND `tags.environment in ['dev','test','staging']` | Downgrade to Standard | $175/mo |\n| ðŸŸ  High | Enterprise Unused | `sku.name startsWith 'Enterprise'` AND no modules/clustering | Downgrade to Premium/Standard | $300-1000/mo |\n| ðŸŸ  High | Old Test Cache | `tags.purpose == 'test'` AND age >60 days | Delete or downgrade | $50-150/mo |\n| ðŸŸ¡ Medium | Large Dev Cache | `sku.capacity >3` AND `tags.environment == 'dev'` | Reduce size | $100-300/mo |\n| ðŸŸ¡ Medium | No Expiration Tag | Missing `expirationDate` or `ttl` tag | Add cleanup policy | N/A |\n| ðŸŸ¢ Low | Untagged Resource | Missing required tags (`environment`, `owner`) | Apply tags | N/A |\n| ðŸŸ¢ Low | Old Cache | Age >365 days | Review if still needed | Variable |\n\n## Report Templates\n\n### Subscription-Level Summary\nQuick overview of costs and issues per subscription (use for multi-subscription scans).\nSee [redis-subscription-level-report.md](../templates/redis-subscription-level-report.md) for template format.\n\n### Detailed Cache Analysis\nIndividual cache breakdown with specific recommendations.\nSee [redis-detailed-cache-analysis.md](../templates/redis-detailed-cache-analysis.md) for template format.\n\n## Tools & Commands\n\n**MCP Tool:** `mcp_azure_mcp_redis` with command `redis_list` (parameter: `subscription`)\n\n**Azure CLI Equivalents:**\n- `az account list` - List subscriptions\n- `az redis list --subscription <id>` - List Redis caches\n- `az redis show` - Get cache details\n- `az redis delete` - Remove cache\n",
        "plugin/skills/azure-cost-optimization/templates/redis-detailed-cache-analysis.md": "Redis Cost Optimization Report - Detailed Analysis\nSubscription: Example-Subscription (12345678-1234-1234-1234-123456789abc)\nGenerated: January 26, 2026\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSUBSCRIPTION OVERVIEW\n- Total Caches: 5\n- Current Monthly Cost: $850\n- Potential Savings: $425/month (50%)\n- Critical Issues: 3\n\nCRITICAL ISSUES (ðŸ”´ Immediate Action)\n\n[1] example-cache-001\n    SKU: Premium P1 (6GB)\n    State: Failed\n    Location: eastus\n    Age: 12 days\n    Cost: $300/month\n    Tags: environment=dev, owner=user1@example.com\n    \n    âŒ Problem: Cache in Failed state for 12 days\n    ðŸ’¡ Recommendation: Delete immediately\n    ðŸ’° Savings: $300/month\n    \n    Action: az redis delete --name example-cache-001 --resource-group dev-rg\n\n[2] example-cache-002\n    SKU: Premium P1 (6GB)\n    State: Running\n    Location: eastus\n    Age: 120 days\n    Cost: $300/month\n    Tags: environment=dev, owner=user2@example.com\n    \n    âš ï¸ Problem: Premium tier in dev environment\n    ðŸ’¡ Recommendation: Downgrade to Standard C3 (6GB)\n    ðŸ’° Savings: $175/month\n    \n    Next Steps:\n    1. Verify with owner: user2@example.com\n    2. Schedule maintenance window\n    3. az redis update --name example-cache-002 --resource-group dev-rg --sku Standard --vm-size C3\n\nHIGH PRIORITY (ðŸŸ  Review This Week)\n\n[3] example-cache-003\n    SKU: Standard C2 (2.5GB)\n    State: Running\n    Location: westus\n    Age: 180 days\n    Cost: $100/month\n    Tags: purpose=test, temporary=true, created=2025-07-15\n    \n    âš ï¸ Problem: Temporary test cache running for 6 months\n    ðŸ’¡ Recommendation: Delete if no longer needed\n    ðŸ’° Savings: $100/month\n    \n    Action: Confirm with team, then delete\n\nHEALTHY CACHES (ðŸŸ¢ No Action Needed)\n\n[4] example-cache-004\n    SKU: Standard C3 (6GB)\n    State: Running\n    Cost: $125/month\n    Tags: environment=prod, owner=team@example.com\n    âœ“ Appropriate tier for production workload\n\n[5] example-cache-005\n    SKU: Standard C1 (1GB)\n    State: Running\n    Cost: $75/month\n    Tags: environment=staging\n    âœ“ Cost-optimized for staging environment\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSAVINGS SUMMARY\n- Critical issues resolved: $425/month\n- Total potential savings: $425/month (50% reduction)\n- New monthly cost: $425/month\n\nRECOMMENDED ACTIONS\n1. [Immediate] Confirm with the team, then delete example-cache-001 (Failed state)\n2. [This Week] Downgrade example-cache-002 to Standard\n3. [This Week] Confirm example-cache-003 still needed, delete if not\n\nWould you like me to:\n  A. Generate Azure CLI commands for these actions\n  B. Analyze another subscription\n  C. Export full report to CSV\n  D. Set up automated monitoring\n\nPlease select (A/B/C/D):",
        "plugin/skills/azure-cost-optimization/templates/redis-subscription-level-report.md": "Redis Cost Optimization Report\nTenant: Contoso Corp\nGenerated: January 26, 2026\nSubscriptions Analyzed: 3 (filtered by prefix \"CacheTeam -\")\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nEXECUTIVE SUMMARY\n- Total Redis Caches: 20\n- Current Monthly Cost: $3,625\n- Potential Savings: $875/month (24.1%)\n- Critical Issues: 4 caches requiring immediate action\n\nBY SUBSCRIPTION\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Subscription        â”‚Cachesâ”‚  Cost/Mo â”‚  Savings/Mo â”‚ Priority â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ CacheTeam - Alpha   â”‚   5  â”‚   $850   â”‚   $425      â”‚    ðŸ”´    â”‚\nâ”‚ CacheTeam - Beta    â”‚   3  â”‚   $375   â”‚     $0      â”‚    ðŸŸ¢    â”‚\nâ”‚ CacheTeam - Prod    â”‚  12  â”‚ $2,400   â”‚   $450      â”‚    ðŸŸ     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nCRITICAL ISSUES (ðŸ”´ Immediate Action Required)\n- CacheTeam - Alpha: 1 failed cache, 2 Premium in dev\n- CacheTeam - Prod: 1 old test cache (180 days)\n\nNext Steps:\n1. Review detailed analysis for CacheTeam - Alpha (type 'analyze alpha')\n2. Review detailed analysis for CacheTeam - Prod (type 'analyze prod')\n3. Generate full report with all recommendations (type 'full report')",
        "plugin/skills/azure-create-app/SKILL.md": "---\r\nname: azure-create-app\r\ndescription: Create Azure-ready application configurations using Azure Developer CLI (azd). USE THIS SKILL when users want to prepare their application for Azure deployment, create azure.yaml, generate infrastructure files, or set up azd projects. Trigger phrases include \"prepare for Azure\", \"create azure.yaml\", \"set up azd\", \"generate infrastructure\", \"configure for Azure\", \"make this Azure-ready\", \"deploy to Azure\", \"azd init\", etc.\r\n---\r\n\r\n# Azure Create App Skill\r\n\r\nCreate Azure-ready application configurations using Azure Developer CLI (azd). This skill generates the required configuration files for Azure deployment.\r\n\r\n---\r\n\r\n## Execution Flow\r\n\r\nExecute these steps in order.\r\n\r\n### Step 1: Check Existing State\r\n\r\nCheck for existing configuration files:\r\n\r\n**If `azure.yaml` exists:**\r\n- Project is already configured for Azure\r\n- User may need to update configuration or deploy (use azure-deploy skill)\r\n- Ask user if they want to regenerate configuration\r\n\r\n**If `azd-arch-plan.md` exists but no `azure.yaml`:**\r\n- Read `azd-arch-plan.md` to determine last completed phase\r\n- Resume from the incomplete phase\r\n\r\n**If neither file exists:**\r\n- Proceed to Step 2 (Discovery)\r\n\r\n### Step 2: Discovery Analysis\r\n\r\nCall the `azure__azd` MCP tool with the `discovery_analysis` command:\r\n```javascript\r\nawait azure__azd({\r\n  command: \"discovery_analysis\",\r\n  parameters: {}\r\n});\r\n```\r\n\r\nThis tool returns instructions to:\r\n- Scan the file system recursively\r\n- Identify programming languages and frameworks\r\n- Classify components (web apps, APIs, databases, etc.)\r\n- Map dependencies between components\r\n- Create `azd-arch-plan.md` with findings\r\n\r\nExecute the returned instructions before proceeding.\r\n\r\n### Step 3: Architecture Planning\r\n\r\nCall the `azure__azd` MCP tool with the `architecture_planning` command:\r\n```javascript\r\nawait azure__azd({\r\n  command: \"architecture_planning\",\r\n  parameters: {}\r\n});\r\n```\r\n\r\nThis tool returns instructions to:\r\n- Select appropriate Azure services for each component\r\n- Plan hosting strategy\r\n- Design containerization approach if needed\r\n- Update `azd-arch-plan.md` with service selections\r\n\r\nExecute the returned instructions before proceeding.\r\n\r\n### Step 4: File Generation\r\n\r\nCall these MCP tools in sequence using `azure__azd`:\r\n\r\n**4a. Get IaC rules:**\r\n```javascript\r\nawait azure__azd({\r\n  command: \"iac_generation_rules\",\r\n  parameters: {}\r\n});\r\n```\r\n\r\n**4b. Generate Dockerfiles (if containerizing):**\r\n```javascript\r\nawait azure__azd({\r\n  command: \"docker_generation\",\r\n  parameters: {}\r\n});\r\n```\r\n\r\n**4c. Generate Bicep templates:**\r\n```javascript\r\nawait azure__azd({\r\n  command: \"infrastructure_generation\",\r\n  parameters: {}\r\n});\r\n```\r\n\r\n**4d. Generate azure.yaml:**\r\n```javascript\r\nawait azure__azd({\r\n  command: \"azure_yaml_generation\",\r\n  parameters: {}\r\n});\r\n```\r\n\r\nEach tool returns instructions. Execute them before calling the next tool.\r\n\r\n**Required output files:**\r\n- `azure.yaml` - Always required\r\n- `infra/main.bicep` - Always required\r\n- `infra/main.parameters.json` - Always required\r\n- `Dockerfile` - Required for Container Apps or AKS hosts\r\n\r\n### Step 5: Validation (REQUIRED)\r\n\r\n**This step is mandatory. Do not proceed to Step 6 until validation completes without errors.**\r\n\r\nCall the `azure__azd` MCP tool with the `project_validation` command:\r\n```javascript\r\nawait azure__azd({\r\n  command: \"project_validation\",\r\n  parameters: {}\r\n});\r\n```\r\n\r\nThis tool returns instructions to validate:\r\n- azure.yaml against schema\r\n- Bicep template compilation\r\n- AZD environment configuration\r\n- Package building\r\n- Provision preview\r\n\r\n**For quick azure.yaml-only validation:**\r\n```javascript\r\nawait azure__azd({\r\n  command: \"validate_azure_yaml\",\r\n  parameters: { path: \"./azure.yaml\" }\r\n});\r\n```\r\n\r\nResolve ALL validation errors before proceeding. Repeat validation until zero errors are returned.\r\n\r\n### Step 6: Complete\r\n\r\nConfiguration is complete. Inform the user:\r\n- `azure.yaml` and infrastructure files are ready\r\n- To deploy, use the azure-deploy skill\r\n\r\n---\r\n\r\n## Reference Guides\r\n\r\nLoad these guides as needed:\r\n\r\n**Discovery & Planning:**\r\n\r\n- [Application Type Detection](./reference/app-type-detection.md) - Patterns for identifying application types\r\n- [Service Selection Rules](./reference/service-selection.md) - Mapping components to Azure services\r\n\r\n**Configuration:**\r\n\r\n- [azure.yaml Configuration](./reference/azure-yaml-config.md) - Configuration file reference\r\n- [Error Handling](./reference/error-handling.md) - Troubleshooting and common errors\r\n\r\n**Service-Specific Details:**\r\n\r\n- [Static Web Apps Guide](./reference/static-web-apps.md)\r\n- [Container Apps Guide](./reference/container-apps.md)\r\n- [Azure Functions Guide](./reference/functions.md)\r\n- [App Service Guide](./reference/app-service.md)\r\n- [AKS Guide](./reference/aks.md)",
        "plugin/skills/azure-create-app/reference/aks.md": "# Azure Kubernetes Service (AKS) Deployment Guide\n\nComplete reference for deploying and managing containerized workloads on Azure Kubernetes Service requiring full Kubernetes control.\n\n---\n\n## Overview\n\nAzure Kubernetes Service (AKS) is a managed Kubernetes container orchestration service that simplifies deploying and managing containerized applications. AKS provides enterprise-grade Kubernetes with integrated Azure services.\n\n**Key Benefits:**\n- **Managed Kubernetes** - Azure handles control plane management\n- **Enterprise features** - RBAC, Azure AD integration, network policies\n- **Scalability** - Automatic node scaling and pod autoscaling\n- **Integrated monitoring** - Azure Monitor and Container Insights\n- **Security** - Private clusters, workload identity, policy enforcement\n- **Flexible networking** - Azure CNI, Kubenet, Calico network policies\n\n**When to use AKS:**\n- Need full Kubernetes capabilities and control\n- Have Kubernetes expertise in your team\n- Complex multi-service microservices architectures\n- Require custom controllers, operators, or CRDs\n- Need specific Kubernetes features (StatefulSets, DaemonSets, service mesh)\n- Running existing Kubernetes workloads\n\n**For simpler scenarios, consider Azure Container Apps.**\n\n---\n\n## Always Use azd for Deployments\n\n> **Always use `azd` (Azure Developer CLI) for Azure provisioning and AKS deployments.**\n> The `azd` tool provides a complete, reproducible deployment workflow for all AKS scenarios.\n\n```bash\n# Deploy everything - THIS IS THE REQUIRED APPROACH\nazd up --no-prompt\n\n# Or step-by-step:\nazd provision --no-prompt   # Create AKS cluster, ACR, networking\nazd deploy --no-prompt      # Deploy application to cluster\n\n# Preview changes before deployment\nazd provision --preview\n\n# Clean up test environments\nazd down --force --purge\n```\n\n> âš ï¸ **CRITICAL: `azd down` Data Loss Warning**\n>\n> `azd down` **permanently deletes ALL resources** including:\n> - AKS cluster with all workloads and persistent volumes\n> - Container Registry with all images\n> - Key Vault with all secrets (use `--purge` to bypass soft-delete)\n> - Storage accounts and databases\n>\n> Always back up important data before running `azd down`.\n\n**Why azd is required:**\n- **Parallel provisioning** - Deploys in seconds, not minutes\n- **Automatic ACR integration** - No image pull failures or manual credential setup\n- **Single command** - `azd up` replaces 5+ commands\n- **Reproducible** - Infrastructure as Code with Bicep\n- **Environment management** - Easy dev/staging/prod separation\n- **Consistent workflow** - Same commands work across all Azure services\n\n---\n\n## Quick Reference\n\n| Property | Value |\n|----------|-------|\n| Deployment tool | `azd` (Azure Developer CLI) |\n| MCP tools | `azure__aks` (commands: `aks_cluster_list`, `aks_nodepool_list`) |\n| Best for | Complex microservices, full K8s control |\n| Prerequisites | Docker, kubectl, azd |\n\n---\n\n## Prerequisites\n\n### Required Tools\n\n**Azure Developer CLI (azd):**\n```bash\n# macOS\nbrew tap azure/azure-dev && brew install azd\n\n# Windows\nwinget install Microsoft.Azd\n\n# Linux\ncurl -fsSL https://aka.ms/install-azd.sh | bash\n```\n\n**kubectl:**\n```bash\n# macOS\nbrew install kubectl\n\n# Windows\nwinget install Kubernetes.kubectl\n\n# Linux\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n```\n\n**Docker Desktop:**\n```bash\n# Download from https://www.docker.com/products/docker-desktop\n# Verify installation\ndocker version\n```\n\n### Authentication\n\n```bash\n# Login to Azure with azd\nazd auth login\n\n# Verify login status\nazd auth login --check-status\n\n# Set environment and subscription\nazd env new <environment-name>\nazd env set AZURE_SUBSCRIPTION_ID \"<subscription-id>\"\n```\n\n---\n\n## Pre-flight Check\n\n**Run `/azure:preflight` before deploying** to verify:\n- Tools installed (azd, docker, kubectl)\n- Authentication valid\n- Quotas sufficient\n- Docker running\n- Subscription has capacity\n\n---\n\n## Quick Deploy with azd\n\n### MCP Tools for AKS\n\nUse the Azure MCP server's azd tools (`azure__azd`) for validation:\n\n| Command | Description |\n|---------|-------------|\n| `validate_azure_yaml` | Validate azure.yaml before deployment |\n| `docker_generation` | Generate Dockerfiles for AKS containers |\n| `infrastructure_generation` | Generate Bicep templates for AKS |\n| `project_validation` | Comprehensive validation before deployment |\n| `error_troubleshooting` | Diagnose azd errors |\n\n**Validate before deployment:**\n```javascript\nconst validation = await azure__azd({\n  command: \"validate_azure_yaml\",\n  parameters: { path: \"./azure.yaml\" }\n});\n```\n\n### Using AZD Template\n\n```bash\n# 1. Initialize from template\nazd init --template azure-samples/aks-store-quickstart\n\n# 2. Deploy (provisions cluster + deploys workloads in parallel)\n# Use --no-prompt for automation/agent scenarios\nazd up --no-prompt\n\n# 3. Get cluster credentials\nazd env get-value AZURE_AKS_CLUSTER_NAME | xargs -I {} az aks get-credentials --name {} --resource-group $(azd env get-value AZURE_RESOURCE_GROUP)\n\n# 4. Verify deployment\nkubectl get pods --all-namespaces\n\n# 5. Iterate on code changes\nazd deploy --no-prompt\n\n# 6. Clean up test environment (WARNING: deletes all resources)\nazd down --force --purge\n```\n\n### Custom Bicep for AKS\n\nCreate `infra/main.bicep`:\n```bicep\nparam location string = resourceGroup().location\nparam clusterName string = 'aks-${uniqueString(resourceGroup().id)}'\n\nresource aks 'Microsoft.ContainerService/managedClusters@2024-02-01' = {\n  name: clusterName\n  location: location\n  identity: {\n    type: 'SystemAssigned'\n  }\n  properties: {\n    dnsPrefix: clusterName\n    enableRBAC: true\n    networkProfile: {\n      networkPlugin: 'azure'\n      networkPolicy: 'azure'\n    }\n    agentPoolProfiles: [\n      {\n        name: 'systempool'\n        count: 3\n        vmSize: 'Standard_DS2_v2'\n        mode: 'System'\n        osType: 'Linux'\n        enableAutoScaling: true\n        minCount: 1\n        maxCount: 5\n      }\n    ]\n  }\n}\n\noutput clusterName string = aks.name\noutput resourceGroupName string = resourceGroup().name\n```\n\n---\n\n## Cluster Creation (Legacy Reference)\n\n> **âš ï¸ Do not use these commands.** Always use `azd up --no-prompt` for AKS deployments.\n> The commands below are legacy reference only for troubleshooting or when azd absolutely cannot be used.\n\n```bash\n# Set variables\nRESOURCE_GROUP=\"myResourceGroup\"\nCLUSTER_NAME=\"myAKSCluster\"\nLOCATION=\"eastus\"\n\n# Create resource group\naz group create --name $RESOURCE_GROUP --location $LOCATION\n\n# Create AKS cluster with managed identity\naz aks create \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --location $LOCATION \\\n    --node-count 3 \\\n    --node-vm-size Standard_DS2_v2 \\\n    --enable-managed-identity \\\n    --generate-ssh-keys \\\n    --network-plugin azure \\\n    --network-policy azure \\\n    --enable-addons monitoring\n\n# Get cluster credentials\naz aks get-credentials \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP\n```\n\n### Cluster Configuration Options\n\n```bash\n# Create with availability zones\naz aks create \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --zones 1 2 3 \\\n    --node-count 3\n\n# Create with Azure CNI and network policy\naz aks create \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --network-plugin azure \\\n    --network-policy azure\n\n# Create with Azure AD integration\naz aks create \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --enable-aad \\\n    --aad-admin-group-object-ids <group-id>\n\n# Create private cluster\naz aks create \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --enable-private-cluster\n```\n\n---\n\n## Node Pools\n\nAKS supports multiple node pools with different VM sizes and configurations.\n\n### Node Pool Types\n\n| Pool Type | Mode | Use Case |\n|-----------|------|----------|\n| **System** | System | Core cluster services (CoreDNS, metrics-server) |\n| **User (General)** | User | Standard application workloads |\n| **User (Spot)** | User | Batch processing, interruptible workloads |\n| **User (GPU)** | User | Machine learning, graphics processing |\n\n### Managing Node Pools\n\n```bash\n# Add user node pool\naz aks nodepool add \\\n    --cluster-name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --name userpool \\\n    --node-count 3 \\\n    --node-vm-size Standard_DS3_v2 \\\n    --mode User\n\n# Add spot node pool (for cost savings)\naz aks nodepool add \\\n    --cluster-name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --name spotpool \\\n    --priority Spot \\\n    --eviction-policy Delete \\\n    --spot-max-price -1 \\\n    --node-count 3 \\\n    --node-vm-size Standard_DS2_v2\n\n# Add GPU node pool\naz aks nodepool add \\\n    --cluster-name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --name gpupool \\\n    --node-count 1 \\\n    --node-vm-size Standard_NC6 \\\n    --node-taints sku=gpu:NoSchedule\n\n# Scale node pool\naz aks nodepool scale \\\n    --cluster-name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --name userpool \\\n    --node-count 5\n\n# Enable autoscaler on node pool\naz aks nodepool update \\\n    --cluster-name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --name userpool \\\n    --enable-cluster-autoscaler \\\n    --min-count 1 \\\n    --max-count 10\n\n# List node pools\naz aks nodepool list \\\n    --cluster-name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --output table\n\n# Delete node pool\naz aks nodepool delete \\\n    --cluster-name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --name spotpool\n```\n\n---\n\n## Workload Identity (Recommended)\n\nWorkload Identity is the recommended way for pods to access Azure resources. It replaces pod-managed identity.\n\n```bash\n# Enable OIDC issuer and workload identity\naz aks update \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --enable-oidc-issuer \\\n    --enable-workload-identity\n\n# Get OIDC issuer URL\nOIDC_ISSUER=$(az aks show \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --query \"oidcIssuerProfile.issuerUrl\" -o tsv)\n\n# Create managed identity\naz identity create \\\n    --name myworkloadidentity \\\n    --resource-group $RESOURCE_GROUP\n\n# Get identity client ID\nIDENTITY_CLIENT_ID=$(az identity show \\\n    --name myworkloadidentity \\\n    --resource-group $RESOURCE_GROUP \\\n    --query clientId -o tsv)\n\n# Create federated identity credential\naz identity federated-credential create \\\n    --name myfc \\\n    --identity-name myworkloadidentity \\\n    --resource-group $RESOURCE_GROUP \\\n    --issuer $OIDC_ISSUER \\\n    --subject system:serviceaccount:default:myserviceaccount\n```\n\n**Kubernetes manifest with workload identity:**\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: myserviceaccount\n  namespace: default\n  annotations:\n    azure.workload.identity/client-id: <IDENTITY_CLIENT_ID>\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    metadata:\n      labels:\n        azure.workload.identity/use: \"true\"\n    spec:\n      serviceAccountName: myserviceaccount\n      containers:\n      - name: app\n        image: myapp:latest\n```\n\n---\n\n## Deployment Strategies\n\n### Rolling Update (Default)\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1        # Max pods above desired count\n      maxUnavailable: 1   # Max pods unavailable during update\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myapp:v2\n```\n\n### Blue-Green Deployment\n\n```bash\n# Deploy green version\nkubectl apply -f deployment-green.yaml\n\n# Wait for green to be ready\nkubectl wait --for=condition=available --timeout=300s deployment/myapp-green\n\n# Switch service to green\nkubectl patch service myapp -p '{\"spec\":{\"selector\":{\"version\":\"green\"}}}'\n\n# Delete blue version\nkubectl delete deployment myapp-blue\n```\n\n### Canary Deployment\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-stable\nspec:\n  replicas: 9\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp-canary\nspec:\n  replicas: 1  # 10% traffic to canary\n```\n\n---\n\n## Networking\n\n### Network Plugins\n\n| Plugin | Description | Use Case |\n|--------|-------------|----------|\n| **Azure CNI** | Each pod gets Azure VNET IP | Enterprise, network policies |\n| **Kubenet** | Pods use private IPs | Cost-effective, simpler |\n\n### Network Policies\n\n```bash\n# Enable Azure Network Policy\naz aks create \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --network-plugin azure \\\n    --network-policy azure\n\n# Or use Calico\naz aks create \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --network-plugin azure \\\n    --network-policy calico\n```\n\n**Example network policy:**\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-from-other-namespaces\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector: {}\n```\n\n### Ingress Controllers\n\n```bash\n# Install NGINX Ingress Controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.0/deploy/static/provider/cloud/deploy.yaml\n\n# Or use Helm\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm install ingress-nginx ingress-nginx/ingress-nginx\n```\n\n**Ingress resource:**\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: myapp-ingress\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: myapp\n            port:\n              number: 80\n```\n\n---\n\n## Scaling\n\n### Horizontal Pod Autoscaler (HPA)\n\n```bash\n# Create HPA\nkubectl autoscale deployment myapp \\\n    --cpu-percent=50 \\\n    --min=2 \\\n    --max=10\n\n# Or use manifest\nkubectl apply -f - <<EOF\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: myapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\nEOF\n```\n\n### Cluster Autoscaler\n\n```bash\n# Enable on node pool\naz aks nodepool update \\\n    --cluster-name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --name userpool \\\n    --enable-cluster-autoscaler \\\n    --min-count 1 \\\n    --max-count 10\n\n# Update autoscaler settings\naz aks nodepool update \\\n    --cluster-name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --name userpool \\\n    --update-cluster-autoscaler \\\n    --min-count 2 \\\n    --max-count 20\n```\n\n---\n\n## Cluster Management\n\n### Cluster Operations\n\n```bash\n# List clusters\naz aks list --output table\n\n# Get cluster info\naz aks show \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP\n\n# Upgrade cluster\naz aks get-upgrades \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP\n\naz aks upgrade \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --kubernetes-version 1.29.0\n\n# Start/stop cluster (dev/test only)\naz aks stop --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP\naz aks start --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP\n\n# Enable/disable addons\naz aks enable-addons \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --addons monitoring,azure-policy\n\naz aks disable-addons \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --addons http_application_routing\n```\n\n### Cluster Configuration\n\n```bash\n# Enable Azure Policy\naz aks enable-addons \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --addons azure-policy\n\n# Enable monitoring with Container Insights\naz aks enable-addons \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --addons monitoring \\\n    --workspace-resource-id <log-analytics-workspace-id>\n\n# Update to private cluster\naz aks update \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --enable-private-cluster\n\n# Rotate cluster certificates\naz aks rotate-certs \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP\n```\n\n---\n\n## MCP Tools (For Queries Only)\n\nUse MCP tools to **query** existing AKS resources, not deploy:\n\n| Command | Description | Parameters |\n|---------|-------------|------------|\n| `aks_cluster_list` | List AKS clusters in subscription | `subscription`, `resource-group` (optional) |\n| `aks_nodepool_list` | List node pools in a cluster | `cluster-name`, `resource-group` |\n\n**Example usage:**\n```javascript\n// List all AKS clusters\nconst clusters = await azure__aks({\n  intent: \"List AKS clusters\",\n  command: \"aks_cluster_list\",\n  parameters: {\n    subscription: \"my-subscription-id\"\n  }\n});\n\n// List node pools\nconst nodePools = await azure__aks({\n  intent: \"List node pools\",\n  command: \"aks_nodepool_list\",\n  parameters: {\n    \"cluster-name\": \"myAKSCluster\",\n    \"resource-group\": \"myResourceGroup\"\n  }\n});\n```\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n---\n\n## Monitoring and Logging\n\n### Container Insights\n\n```bash\n# Enable Container Insights\naz aks enable-addons \\\n    --name $CLUSTER_NAME \\\n    --resource-group $RESOURCE_GROUP \\\n    --addons monitoring\n\n# Query logs with kubectl\nkubectl logs <pod-name>\nkubectl logs <pod-name> -c <container-name>\nkubectl logs -f <pod-name>  # Follow logs\n\n# View logs for all pods in deployment\nkubectl logs -l app=myapp --all-containers=true\n```\n\n### Azure Monitor Queries\n\n```kusto\n// Pod performance\nlet startTime = ago(1h);\nPerf\n| where TimeGenerated > startTime\n| where ObjectName == \"K8SContainer\"\n| where CounterName == \"cpuUsageNanoCores\"\n| summarize AvgCPU = avg(CounterValue) by bin(TimeGenerated, 5m), InstanceName\n\n// Container logs\nContainerLog\n| where TimeGenerated > ago(1h)\n| where LogEntry contains \"error\"\n| project TimeGenerated, LogEntry, Name\n| order by TimeGenerated desc\n```\n\n---\n\n## Security Best Practices\n\n| Practice | Description |\n|----------|-------------|\n| **Use managed identity** | Enable system-assigned managed identity for cluster |\n| **Enable Azure Policy** | Enforce governance and compliance policies |\n| **Configure network policy** | Restrict pod-to-pod communication |\n| **Use private clusters** | No public endpoint for production workloads |\n| **Enable workload identity** | Secure pod access to Azure resources |\n| **Use availability zones** | Deploy across zones for high availability |\n| **Minimum 3 nodes** | For production clusters |\n| **Set resource quotas** | Limit resource usage per namespace |\n| **Enable RBAC** | Role-based access control for cluster resources |\n| **Scan images** | Use Azure Container Registry with vulnerability scanning |\n\n---\n\n## Best Practices\n\n1. **Use managed identity** for cluster authentication\n2. **Enable Azure Policy** for governance\n3. **Configure network policy** (Calico or Azure)\n4. **Use private clusters** for production\n5. **Enable Container Insights** for monitoring\n6. **Use availability zones** for HA\n7. **Minimum 3 nodes** for production\n8. **Set resource quotas** per namespace\n9. **Implement pod security** standards\n10. **Use workload identity** for Azure resource access\n11. **Enable autoscaling** (HPA and cluster autoscaler)\n12. **Regular cluster upgrades** for security patches\n\n---\n\n## Choosing the Right Compute\n\n| If your app is... | Use | Why |\n|-------------------|-----|-----|\n| HTTP APIs, microservices | **Container Apps** | Serverless, auto-scale, Dapr, simpler |\n| Event-driven | **Functions** | Pay-per-execution, lightweight |\n| Traditional web apps | **App Service** | Managed platform, easy deployment |\n| Complex K8s workloads | **AKS** | Full control, custom operators, CRDs |\n\n---\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| **Cluster creation fails** | Check subscription quotas: `az vm list-usage --location eastus` |\n| **Pods pending** | Check node resources, add/scale node pools |\n| **Image pull errors** | Attach ACR: `az aks update --name CLUSTER -g RG --attach-acr ACR_NAME` |\n| **Network connectivity** | Verify network policies and security groups |\n| **DNS issues** | Check CoreDNS pods: `kubectl get pods -n kube-system -l k8s-app=kube-dns` |\n| **Node not ready** | Check node logs: `kubectl describe node <node-name>` |\n\n---\n\n## Additional Resources\n\n- [AKS Documentation](https://learn.microsoft.com/azure/aks/)\n- [AKS Best Practices](https://learn.microsoft.com/azure/aks/best-practices)\n- [Kubernetes Documentation](https://kubernetes.io/docs/)\n- [Azure Verified Modules](https://aka.ms/avm)\n",
        "plugin/skills/azure-create-app/reference/app-service.md": "# Azure App Service Deployment Guide\n\nComplete reference for deploying and managing traditional web applications using Azure App Service for managed platform hosting.\n\n---\n\n## Overview\n\nAzure App Service is a fully managed platform-as-a-service (PaaS) for hosting web applications, REST APIs, and mobile backends. It provides automatic scaling, load balancing, and integrated deployment tools.\n\n**Key Benefits:**\n- **Fully managed** - No infrastructure management required\n- **Multiple languages** - Node.js, Python, .NET, Java, PHP, Ruby\n- **Built-in CI/CD** - GitHub Actions, Azure DevOps integration\n- **Auto-scaling** - Scale based on metrics or schedule\n- **Deployment slots** - Zero-downtime deployments\n- **Custom domains** - HTTPS with managed certificates\n\n**When to use App Service:**\n- Traditional web applications (MVC, SPA with server-side)\n- REST APIs that don't require containers\n- Mobile backends\n- WordPress, Drupal, and other CMS platforms\n- Applications requiring VNet integration\n- Applications needing always-on availability\n\n**For containerized apps, consider Container Apps. For serverless, consider Azure Functions.**\n\n---\n\n## Always Use azd for Deployments\n\n> **Always use `azd` (Azure Developer CLI) for Azure provisioning and App Service deployments.**\n> The `azd` tool provides a complete, reproducible deployment workflow for all App Service scenarios.\n\n```bash\n# Deploy everything - THIS IS THE REQUIRED APPROACH\nazd up --no-prompt\n\n# Or step-by-step:\nazd provision --no-prompt   # Create App Service, plan, and dependencies\nazd deploy --no-prompt      # Deploy application code\n\n# Preview changes before deployment\nazd provision --preview\n\n# Clean up test environments\nazd down --force --purge\n```\n\n> âš ï¸ **CRITICAL: `azd down` Data Loss Warning**\n>\n> `azd down` **permanently deletes ALL resources** including databases with data, storage accounts, and Key Vaults.\n> - `--force` skips confirmation\n> - `--purge` permanently deletes Key Vault (no soft-delete recovery)\n>\n> Always back up important data before running `azd down`.\n\n**Why azd is required:**\n- **Parallel provisioning** - Deploys in seconds, not minutes\n- **Single command** - `azd up` replaces 5+ commands\n- **Infrastructure as Code** - Reproducible with Bicep\n- **Environment management** - Easy dev/staging/prod separation\n- **Consistent workflow** - Same commands work across all Azure services\n\n---\n\n## Quick Reference\n\n| Property | Value |\n|----------|-------|\n| Deployment tool | `azd` (Azure Developer CLI) |\n| MCP tools | `azure__azd` (commands: `validate_azure_yaml`, `discovery_analysis`) |\n| Best for | Web apps, REST APIs, managed hosting |\n| azd Template | `todo-csharp-sql`, `todo-nodejs-mongo` |\n\n---\n\n## Prerequisites\n\n### Required Tools\n\n**Azure Developer CLI (azd):**\n```bash\n# macOS\nbrew tap azure/azure-dev && brew install azd\n\n# Windows\nwinget install Microsoft.Azd\n\n# Linux\ncurl -fsSL https://aka.ms/install-azd.sh | bash\n```\n\n### Authentication\n\n```bash\n# Login to Azure with azd\nazd auth login\n\n# Verify login status\nazd auth login --check-status\n\n# Set environment and subscription\nazd env new <environment-name>\nazd env set AZURE_SUBSCRIPTION_ID \"<subscription-id>\"\n```\n\n---\n\n## Pre-flight Check\n\n**Run `/azure:preflight` before deploying** to verify:\n- Tools installed (azd)\n- Authentication valid\n- Quotas sufficient\n- Subscription has capacity\n\n---\n\n## Quick Deploy with azd\n\n### MCP Tools for App Service\n\nUse the Azure MCP server's azd tools (`azure__azd`) for validation:\n\n| Command | Description |\n|---------|-------------|\n| `validate_azure_yaml` | Validate azure.yaml before deployment |\n| `project_validation` | Comprehensive validation before deployment |\n| `error_troubleshooting` | Diagnose azd errors |\n\n**Validate before deployment:**\n```javascript\nconst validation = await azure__azd({\n  command: \"validate_azure_yaml\",\n  parameters: { path: \"./azure.yaml\" }\n});\n```\n\n### Using AZD Template\n\n```bash\n# 1. Initialize from template\nazd init --template azure-samples/todo-csharp-sql\n\n# 2. Deploy (provisions + deploys in parallel)\n# Use --no-prompt for automation/agent scenarios\nazd up --no-prompt\n\n# 3. Iterate on code changes\nazd deploy --no-prompt\n\n# 4. View application logs\nazd monitor --logs\n\n# 5. Clean up test environment (WARNING: deletes all resources)\nazd down --force --purge\n```\n\n### Custom Bicep for App Service\n\nCreate `infra/main.bicep`:\n```bicep\nparam location string = resourceGroup().location\nparam appName string = 'webapp-${uniqueString(resourceGroup().id)}'\n\nresource appServicePlan 'Microsoft.Web/serverfarms@2023-12-01' = {\n  name: '${appName}-plan'\n  location: location\n  sku: {\n    name: 'B1'\n    tier: 'Basic'\n  }\n  properties: {\n    reserved: true  // Linux\n  }\n}\n\nresource webApp 'Microsoft.Web/sites@2023-12-01' = {\n  name: appName\n  location: location\n  properties: {\n    serverFarmId: appServicePlan.id\n    siteConfig: {\n      linuxFxVersion: 'NODE|20-lts'\n      appSettings: [\n        {\n          name: 'WEBSITE_NODE_DEFAULT_VERSION'\n          value: '~20'\n        }\n      ]\n    }\n  }\n}\n\noutput webAppUrl string = webApp.properties.defaultHostName\n```\n\n---\n\n## App Service Plans\n\n### Plan Tiers\n\n| Tier | Features | Use Case | Pricing |\n|------|----------|----------|---------|\n| **Free (F1)** | Shared compute, 60 min/day | Development, testing | Free |\n| **Shared (D1)** | Shared compute, 240 min/day | Small personal projects | ~$10/month |\n| **Basic (B1-B3)** | Dedicated VMs, no auto-scale | Small production apps | ~$13-52/month |\n| **Standard (S1-S3)** | Auto-scale, staging slots, backups | Production apps | ~$70-280/month |\n| **Premium (P1v3-P3v3)** | Enhanced performance, VNet | High-scale production | ~$100-400/month |\n| **Isolated (I1v2-I3v2)** | Dedicated environment, ASE | Compliance, security | ~$400-1600/month |\n\n### Creating App Service Plans\n\n> **Note:** Always use `azd up --no-prompt` for deployments. Define plan configuration in Bicep templates. The commands below are legacy reference only.\n\n```bash\n# Create Basic plan\naz appservice plan create \\\n    --name myplan \\\n    --resource-group RG \\\n    --location eastus \\\n    --sku B1 \\\n    --is-linux\n\n# Create Standard plan with auto-scale\naz appservice plan create \\\n    --name myplan \\\n    --resource-group RG \\\n    --location eastus \\\n    --sku S1 \\\n    --is-linux\n\n# Create Premium plan\naz appservice plan create \\\n    --name myplan \\\n    --resource-group RG \\\n    --location eastus \\\n    --sku P1v3 \\\n    --is-linux\n```\n\n---\n\n## Creating Web Apps\n\n### Supported Runtimes\n\n**Linux runtimes:**\n- Node.js: `\"NODE:18-lts\"`, `\"NODE:20-lts\"`\n- Python: `\"PYTHON:3.9\"`, `\"PYTHON:3.10\"`, `\"PYTHON:3.11\"`, `\"PYTHON:3.12\"`\n- .NET: `\"DOTNETCORE:6.0\"`, `\"DOTNETCORE:7.0\"`, `\"DOTNETCORE:8.0\"`\n- Java: `\"JAVA:17-java17\"`, `\"JAVA:21-java21\"`\n- PHP: `\"PHP:8.1\"`, `\"PHP:8.2\"`\n\n**Windows runtimes:**\n- .NET Framework: `\"v4.8\"`, `\"v3.5\"`\n- ASP.NET Core: `\"ASPNET:V4.8\"`, `\"ASPNET:V6.0\"`\n- Node.js: Configured via app settings\n\n### Create Web App\n\n> **Note:** Always use `azd up --no-prompt` for deployments. The commands below are legacy reference only.\n\n```bash\n# Create Node.js web app\naz webapp create \\\n    --name myapp \\\n    --resource-group RG \\\n    --plan myplan \\\n    --runtime \"NODE:20-lts\"\n\n# Create Python web app\naz webapp create \\\n    --name myapp \\\n    --resource-group RG \\\n    --plan myplan \\\n    --runtime \"PYTHON:3.12\"\n\n# Create .NET web app\naz webapp create \\\n    --name myapp \\\n    --resource-group RG \\\n    --plan myplan \\\n    --runtime \"DOTNETCORE:8.0\"\n\n# Create Java web app\naz webapp create \\\n    --name myapp \\\n    --resource-group RG \\\n    --plan myplan \\\n    --runtime \"JAVA:17-java17\"\n```\n\n---\n\n## Deployment Methods\n\n### Method 1: ZIP Deploy (Recommended for CLI)\n\n```bash\n# Package application\nzip -r app.zip . -x \"*.git*\" \"node_modules/*\" \".env\"\n\n# Deploy\naz webapp deploy \\\n    --name myapp \\\n    --resource-group RG \\\n    --src-path app.zip \\\n    --type zip\n\n# For async deployment (doesn't wait)\naz webapp deploy \\\n    --name myapp \\\n    --resource-group RG \\\n    --src-path app.zip \\\n    --type zip \\\n    --async true\n```\n\n### Method 2: Git Deploy\n\n```bash\n# Configure local git deployment\naz webapp deployment source config-local-git \\\n    --name myapp \\\n    --resource-group RG\n\n# Get git URL\nGIT_URL=$(az webapp deployment list-publishing-credentials \\\n    --name myapp \\\n    --resource-group RG \\\n    --query scmUri -o tsv)\n\n# Add remote and push\ngit remote add azure $GIT_URL\ngit push azure main\n```\n\n### Method 3: GitHub Actions\n\n```bash\n# Configure GitHub deployment\naz webapp deployment github-actions add \\\n    --name myapp \\\n    --resource-group RG \\\n    --repo owner/repo \\\n    --branch main \\\n    --runtime node \\\n    --runtime-version 20\n\n# Or manually create workflow\n```\n\n**GitHub Actions workflow:**\n```yaml\nname: Deploy to Azure Web App\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\n\nenv:\n  AZURE_WEBAPP_NAME: myapp\n  NODE_VERSION: '20.x'\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n      \n      - name: npm install and build\n        run: |\n          npm ci\n          npm run build --if-present\n      \n      - name: Deploy to Azure Web App\n        uses: azure/webapps-deploy@v3\n        with:\n          app-name: ${{ env.AZURE_WEBAPP_NAME }}\n          publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE }}\n          package: .\n```\n\n### Method 4: Docker Container\n\n```bash\n# Create web app for containers\naz webapp create \\\n    --name myapp \\\n    --resource-group RG \\\n    --plan myplan \\\n    --deployment-container-image-name myregistry.azurecr.io/myapp:latest\n\n# Configure continuous deployment\naz webapp deployment container config \\\n    --name myapp \\\n    --resource-group RG \\\n    --enable-cd true\n```\n\n---\n\n## Deployment Slots\n\nDeployment slots enable zero-downtime deployments with staging environments.\n\n```bash\n# Create staging slot\naz webapp deployment slot create \\\n    --name myapp \\\n    --resource-group RG \\\n    --slot staging\n\n# Deploy to staging\naz webapp deploy \\\n    --name myapp \\\n    --resource-group RG \\\n    --slot staging \\\n    --src-path app.zip \\\n    --type zip\n\n# Test staging: https://myapp-staging.azurewebsites.net\n\n# Swap slots (zero downtime)\naz webapp deployment slot swap \\\n    --name myapp \\\n    --resource-group RG \\\n    --slot staging \\\n    --target-slot production\n\n# Swap with preview (two-phase swap)\naz webapp deployment slot swap \\\n    --name myapp \\\n    --resource-group RG \\\n    --slot staging \\\n    --action preview\n\n# Complete the swap\naz webapp deployment slot swap \\\n    --name myapp \\\n    --resource-group RG \\\n    --slot staging \\\n    --action swap\n\n# Cancel swap\naz webapp deployment slot swap \\\n    --name myapp \\\n    --resource-group RG \\\n    --slot staging \\\n    --action reset\n```\n\n---\n\n## Configuration\n\n### Application Settings\n\n```bash\n# Set application settings (environment variables)\naz webapp config appsettings set \\\n    --name myapp \\\n    --resource-group RG \\\n    --settings \\\n        NODE_ENV=production \\\n        API_KEY=@Microsoft.KeyVault(SecretUri=https://myvault.vault.azure.net/secrets/apikey/)\n\n# List settings\naz webapp config appsettings list \\\n    --name myapp \\\n    --resource-group RG\n\n# Delete setting\naz webapp config appsettings delete \\\n    --name myapp \\\n    --resource-group RG \\\n    --setting-names NODE_ENV\n```\n\n### Connection Strings\n\n```bash\n# Set connection string\naz webapp config connection-string set \\\n    --name myapp \\\n    --resource-group RG \\\n    --connection-string-type SQLAzure \\\n    --settings MyDb=\"Server=tcp:...\"\n\n# List connection strings\naz webapp config connection-string list \\\n    --name myapp \\\n    --resource-group RG\n```\n\n### General Configuration\n\n```bash\n# Enable always on (prevents cold starts)\naz webapp config set \\\n    --name myapp \\\n    --resource-group RG \\\n    --always-on true\n\n# Set minimum TLS version\naz webapp config set \\\n    --name myapp \\\n    --resource-group RG \\\n    --min-tls-version 1.2\n\n# Configure HTTP/2\naz webapp config set \\\n    --name myapp \\\n    --resource-group RG \\\n    --http20-enabled true\n\n# Set startup command (for custom entry points)\naz webapp config set \\\n    --name myapp \\\n    --resource-group RG \\\n    --startup-file \"npm start\"\n```\n\n---\n\n## Scaling\n\n### Scale Up (Vertical Scaling)\n\nChange to a higher-tier plan with more resources:\n\n```bash\n# Scale up to Standard S1\naz appservice plan update \\\n    --name myplan \\\n    --resource-group RG \\\n    --sku S1\n\n# Scale up to Premium P1v3\naz appservice plan update \\\n    --name myplan \\\n    --resource-group RG \\\n    --sku P1v3\n```\n\n### Scale Out (Horizontal Scaling)\n\nAdd more instances:\n\n```bash\n# Manual scale out\naz appservice plan update \\\n    --name myplan \\\n    --resource-group RG \\\n    --number-of-workers 3\n\n# Enable auto-scale (requires Standard or higher)\naz monitor autoscale create \\\n    --name myautoscale \\\n    --resource-group RG \\\n    --resource /subscriptions/{sub}/resourceGroups/RG/providers/Microsoft.Web/serverfarms/myplan \\\n    --min-count 1 \\\n    --max-count 10 \\\n    --count 2\n\n# Add scale rule based on CPU\naz monitor autoscale rule create \\\n    --resource-group RG \\\n    --autoscale-name myautoscale \\\n    --condition \"Percentage CPU > 70 avg 5m\" \\\n    --scale out 1\n\naz monitor autoscale rule create \\\n    --resource-group RG \\\n    --autoscale-name myautoscale \\\n    --condition \"Percentage CPU < 25 avg 5m\" \\\n    --scale in 1\n```\n\n---\n\n## Custom Domains and SSL\n\n### Add Custom Domain\n\n```bash\n# Map custom domain\naz webapp config hostname add \\\n    --webapp-name myapp \\\n    --resource-group RG \\\n    --hostname www.contoso.com\n\n# Bind SSL certificate\naz webapp config ssl bind \\\n    --name myapp \\\n    --resource-group RG \\\n    --certificate-thumbprint {thumbprint} \\\n    --ssl-type SNI\n```\n\n### Managed Certificate (Free)\n\n```bash\n# Create managed certificate\naz webapp config ssl create \\\n    --name myapp \\\n    --resource-group RG \\\n    --hostname www.contoso.com\n\n# Bind managed certificate\naz webapp config ssl bind \\\n    --name myapp \\\n    --resource-group RG \\\n    --certificate-thumbprint {thumbprint} \\\n    --ssl-type SNI\n```\n\n---\n\n## Monitoring and Logging\n\n### Application Logging\n\n```bash\n# Enable application logging\naz webapp log config \\\n    --name myapp \\\n    --resource-group RG \\\n    --application-logging azureblobstorage\n\n# Stream logs\naz webapp log tail \\\n    --name myapp \\\n    --resource-group RG\n\n# Download logs\naz webapp log download \\\n    --name myapp \\\n    --resource-group RG \\\n    --log-file logs.zip\n```\n\n### Application Insights\n\n```bash\n# Create Application Insights\naz monitor app-insights component create \\\n    --app myapp-insights \\\n    --location eastus \\\n    --resource-group RG \\\n    --application-type web\n\n# Get instrumentation key\nINSTRUMENTATION_KEY=$(az monitor app-insights component show \\\n    --app myapp-insights \\\n    --resource-group RG \\\n    --query instrumentationKey -o tsv)\n\n# Get connection string (preferred)\nCONNECTION_STRING=$(az monitor app-insights component show \\\n    --app myapp-insights \\\n    --resource-group RG \\\n    --query connectionString -o tsv)\n\n# Configure in web app\naz webapp config appsettings set \\\n    --name myapp \\\n    --resource-group RG \\\n    --settings \"APPLICATIONINSIGHTS_CONNECTION_STRING=$CONNECTION_STRING\"\n```\n\n---\n\n## MCP Tools (For Queries Only)\n\nUse MCP tools to **query** existing App Service resources, not deploy:\n\n| Command | Description | Parameters |\n|---------|-------------|------------|\n| `appservice_webapp_list` | List web apps in subscription/resource group | `subscription`, `resource-group` (optional) |\n| `appservice_webapp_get` | Get web app details | `name`, `resource-group` |\n| `appservice_plan_list` | List App Service plans | `subscription`, `resource-group` (optional) |\n\n**Example usage:**\n```javascript\n// List all web apps\nconst webApps = await azure__appservice({\n  intent: \"List web apps\",\n  command: \"appservice_webapp_list\",\n  parameters: {\n    subscription: \"my-subscription-id\"\n  }\n});\n\n// Get web app details\nconst appDetails = await azure__appservice({\n  intent: \"Get web app details\",\n  command: \"appservice_webapp_get\",\n  parameters: {\n    name: \"myapp\",\n    \"resource-group\": \"myResourceGroup\"\n  }\n});\n\n// List App Service plans\nconst plans = await azure__appservice({\n  intent: \"List App Service plans\",\n  command: \"appservice_plan_list\",\n  parameters: {\n    subscription: \"my-subscription-id\",\n    \"resource-group\": \"myResourceGroup\"\n  }\n});\n```\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n---\n\n## Best Practices\n\n| Practice | Description |\n|----------|-------------|\n| **Use deployment slots** | Zero-downtime deployments with staging-to-production swaps |\n| **Store secrets in Key Vault** | Use managed identity to access Key Vault references in app settings |\n| **Enable always on** | Prevents cold starts for production apps (Standard tier+) |\n| **Configure health checks** | App Service can restart unhealthy instances automatically |\n| **Use staging slots** | Test changes in production-like environment before swapping |\n| **Enable Application Insights** | Monitor performance, exceptions, and user behavior |\n| **Set minimum TLS version** | Use TLS 1.2 or higher for security |\n| **Use managed certificates** | Free SSL/TLS certificates for custom domains |\n| **Configure auto-scale** | Scale based on metrics for cost optimization |\n| **Enable diagnostic logs** | Stream logs to Log Analytics or Storage |\n\n---\n\n## Common Issues\n\n### 503 Service Unavailable\n\n**Symptoms:** App returns 503 error\n\n**Solutions:**\n- Check app logs: `az webapp log tail --name myapp -g RG`\n- Verify app is starting correctly\n- Check memory/CPU usage in metrics\n- Ensure dependencies are installed\n- Verify connection strings are correct\n\n### Slow Cold Starts\n\n**Symptoms:** First request after idle takes 5-10 seconds\n\n**Solutions:**\n- Enable \"Always On\" (Standard tier+)\n- Use Premium tier with pre-warmed instances\n- Optimize application startup time\n- Consider using deployment slots for pre-warming\n\n### Application Won't Start\n\n**Symptoms:** App shows \"Application Error\" page\n\n**Solutions:**\n- Check application logs\n- Verify runtime version matches app requirements\n- Check startup command is correct\n- Ensure all required app settings are configured\n- Verify dependencies are included in deployment\n\n### Connection String Issues\n\n**Symptoms:** Database connection errors\n\n**Solutions:**\n- Verify connection string format\n- Check firewall rules allow App Service IPs\n- Use managed identity for Azure SQL\n- Test connection string locally first\n\n---\n\n## Troubleshooting Commands\n\n```bash\n# View app details\naz webapp show --name myapp -g RG\n\n# Check app state\naz webapp show --name myapp -g RG --query \"state\"\n\n# View configuration\naz webapp config show --name myapp -g RG\n\n# View app settings\naz webapp config appsettings list --name myapp -g RG\n\n# Stream logs\naz webapp log tail --name myapp -g RG\n\n# Download logs\naz webapp log download --name myapp -g RG --log-file app-logs.zip\n\n# Restart app\naz webapp restart --name myapp -g RG\n\n# Stop app\naz webapp stop --name myapp -g RG\n\n# Start app\naz webapp start --name myapp -g RG\n\n# View deployment history\naz webapp deployment list --name myapp -g RG\n```\n\n---\n\n## Additional Resources\n\n- [App Service Documentation](https://learn.microsoft.com/azure/app-service/)\n- [App Service Best Practices](https://learn.microsoft.com/azure/app-service/app-service-best-practices)\n- [Deployment Best Practices](https://learn.microsoft.com/azure/app-service/deploy-best-practices)\n- [Monitoring App Service](https://learn.microsoft.com/azure/app-service/web-sites-monitor)\n",
        "plugin/skills/azure-create-app/reference/app-type-detection.md": "# Application Type Detection\r\n\r\nUse these patterns to identify application types during discovery.\r\n\r\n## Node.js Applications\r\n\r\n**Indicator:** `package.json` exists\r\n\r\n| Pattern | Azure Service |\r\n|---------|---------------|\r\n| `next.config.js/mjs/ts` with `output: 'export'` | Static Web Apps |\r\n| `next.config.js/mjs/ts` without export config | Container Apps (SSR) |\r\n| `angular.json` | Static Web Apps |\r\n| `vite.config.*` | Static Web Apps |\r\n| `gatsby-config.js` | Static Web Apps |\r\n| `astro.config.mjs` | Static Web Apps |\r\n| `nest-cli.json` | Container Apps |\r\n| express/fastify/koa/hapi dependency | Container Apps |\r\n\r\n## Python Applications\r\n\r\n**Indicator:** `requirements.txt` or `pyproject.toml` exists\r\n\r\n| Pattern | Azure Service |\r\n|---------|---------------|\r\n| `function_app.py` exists | Azure Functions |\r\n| `azure-functions` dependency | Azure Functions |\r\n| flask/django/fastapi dependency | Container Apps |\r\n\r\n## .NET Applications\r\n\r\n**Indicator:** `*.csproj` or `*.sln` exists\r\n\r\n| Pattern | Azure Service |\r\n|---------|---------------|\r\n| `<AzureFunctionsVersion>` in csproj | Azure Functions |\r\n| Blazor WebAssembly | Static Web Apps |\r\n| ASP.NET Core | Container Apps |\r\n\r\n## Java Applications\r\n\r\n**Indicator:** `pom.xml` or `build.gradle` exists\r\n\r\n| Pattern | Azure Service |\r\n|---------|---------------|\r\n| `azure-functions-*` dependency | Azure Functions |\r\n| spring-boot dependency | Container Apps |\r\n\r\n## Static Sites\r\n\r\n**Indicator:** `index.html` without `package.json` or `requirements.txt`\r\n\r\nâ†’ **Static Web Apps**\r\n\r\n## Containerized Applications\r\n\r\n**Indicator:** `Dockerfile` exists\r\n\r\nâ†’ **Container Apps** (or AKS if complex Kubernetes needs)\r\n\r\n## Multi-Service Indicators\r\n\r\nThese patterns suggest a multi-service application:\r\n\r\n- Monorepo structure (`frontend/`, `backend/`, `api/`)\r\n- `docker-compose.yml` with multiple services\r\n- Multiple `package.json` files in subdirectories\r\n- Database connection strings in config files\r\n",
        "plugin/skills/azure-create-app/reference/azure-yaml-config.md": "# azure.yaml Configuration\r\n\r\nThe `azure.yaml` file defines your application's Azure deployment configuration.\r\n\r\n## Host Property\r\n\r\nThe `host` property determines the Azure service:\r\n\r\n| Value | Azure Service |\r\n|-------|---------------|\r\n| `containerapp` | Azure Container Apps |\r\n| `appservice` | Azure App Service |\r\n| `staticwebapp` | Azure Static Web Apps |\r\n| `function` | Azure Functions |\r\n| `aks` | Azure Kubernetes Service |\r\n\r\n## Language Property\r\n\r\nThe `language` property specifies the application runtime.\r\n\r\n**Valid values:** `js`, `ts`, `python`, `csharp`, `java`, `go`\r\n\r\n**Rules by host type:**\r\n\r\n| Host | Language Requirement |\r\n|------|---------------------|\r\n| `staticwebapp` | Do NOT specify for plain HTML sites |\r\n| `containerapp` | Optional, used for build detection |\r\n| `appservice` | Required for runtime selection |\r\n| `function` | Required for runtime |\r\n\r\n## Example Configurations\r\n\r\n### Multi-Service Application\r\n\r\n```yaml\r\nname: my-application\r\nservices:\r\n  web:\r\n    project: ./src/web\r\n    host: staticwebapp\r\n    dist: ./dist\r\n  api:\r\n    project: ./src/api\r\n    host: containerapp\r\n    language: python\r\n```\r\n\r\n### Single Container App\r\n\r\n```yaml\r\nname: my-api\r\nservices:\r\n  api:\r\n    project: .\r\n    host: containerapp\r\n    language: ts\r\n```\r\n\r\n### Azure Functions\r\n\r\n```yaml\r\nname: my-functions\r\nservices:\r\n  func:\r\n    project: .\r\n    host: function\r\n    language: python\r\n```\r\n\r\n### Static Web App with API\r\n\r\n```yaml\r\nname: my-frontend\r\nservices:\r\n  web:\r\n    project: .\r\n    host: staticwebapp\r\n    dist: ./dist\r\n```\r\n",
        "plugin/skills/azure-create-app/reference/container-apps.md": "# Azure Container Apps Deployment Guide\n\nComplete reference for deploying containerized applications to Azure Container Apps and Container Apps Jobs using Azure MCP tools and azd.\n\n---\n\n## Overview\n\nAzure Container Apps is a fully managed serverless container platform that enables you to run microservices and containerized applications without managing complex infrastructure. This guide provides automated deployment workflows using Azure MCP tools for infrastructure planning, generation, validation, and monitoring.\n\n**Key Benefits:**\n- **Serverless containers** - No VM management, auto-scaling to zero\n- **Built-in HTTPS** - Automatic ingress with managed certificates\n- **Multiple triggers** - HTTP, scheduled jobs, event-driven scaling\n- **Microservices ready** - Service-to-service communication via DNS\n- **Integrated monitoring** - Built-in Log Analytics and Azure Monitor\n- **MCP tool integration** - Automated planning, IaC generation, and validation\n\n**When to use Container Apps:**\n\n### Long-Running Services (Container Apps)\n- **Full-stack web applications** - Deploy complete apps with frontend (HTML/React/Vue/Angular) and backend (Node.js/Python/Java) as separate containers\n- **Todo apps, blogs, e-commerce sites** - Any web application with a UI and API service layer\n- **Single-page applications (SPA) with APIs** - Frontend serving static files + backend API container\n- **Microservices** - Deploy multiple containerized services with inter-service communication\n- **RESTful APIs** - Host APIs with auto-scaling and ingress configuration\n- **Background workers** - Run asynchronous job processors without ingress\n- **Event-driven apps** - Build applications that scale based on queue depth or custom metrics\n\n### Finite-Duration Tasks (Container Apps Jobs)\n- **Scheduled tasks** - Run recurring jobs on a cron schedule (e.g., daily reports, cleanup tasks)\n- **Batch processing** - Process large datasets in parallel with finite execution\n- **Data pipelines** - ETL jobs, data transformation, and migration tasks\n- **Queue processors** - Process messages from Azure Storage Queues or Service Bus\n- **On-demand tasks** - Manual execution for one-time operations or maintenance\n- **CI/CD runners** - Self-hosted GitHub Actions or Azure Pipelines agents\n- **Machine learning** - Training jobs, model evaluation, and batch inference\n\n**Deployment Workflow:**\n```\nPrerequisites â†’ Plan â†’ Generate IaC â†’ Validate â†’ Deploy â†’ Monitor\n     â†‘                                                      â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## Prerequisites and Validation\n\n### Pattern 0: Prerequisites Validation\n\n**Always validate all prerequisites before starting deployment to avoid common failures.**\n\nThe most common failure is attempting to build container images when Docker daemon is not started.\n\n```javascript\nasync function validatePrerequisites() {\n  const checks = [];\n  \n  // Check Docker is installed and running\n  try {\n    await exec('docker ps');\n    checks.push({ name: 'Docker', status: 'running' });\n  } catch (error) {\n    if (error.message.includes('daemon is not running')) {\n      throw new Error('Docker Desktop is not running. Please start Docker Desktop and try again.');\n    }\n    throw new Error('Docker is not installed. Install Docker Desktop from https://www.docker.com/products/docker-desktop');\n  }\n  \n  // Check Azure Developer CLI authentication\n  try {\n    await exec('azd auth login --check-status');\n    checks.push({ name: 'Azure Developer CLI', status: 'authenticated' });\n  } catch (error) {\n    throw new Error('Not authenticated with Azure Developer CLI. Run: azd auth login');\n  }\n  \n  // Check Azure location is set\n  try {\n    const location = process.env.AZURE_LOCATION;\n    if (!location) {\n      throw new Error('AZURE_LOCATION not set. Run: azd env set AZURE_LOCATION eastus');\n    }\n    checks.push({ name: 'Azure Location', status: location });\n  } catch (error) {\n    throw error;\n  }\n  \n  return checks;\n}\n```\n\n### Prerequisites Checklist\n\n**Setup:**\n- [ ] Azure subscription created\n- [ ] **Docker Desktop installed and running** (`docker ps`)\n- [ ] Azure Developer CLI (azd) installed (`azd version`)\n- [ ] Azure Developer CLI authenticated (`azd auth login`)\n- [ ] Azure location configured (`azd env set AZURE_LOCATION eastus`)\n- [ ] Application containerized with Dockerfile\n- [ ] **For full-stack apps**: Verify both frontend and backend can be containerized together or separately\n\n### Required Tools\n\n**Azure Developer CLI (azd):**\n```bash\n# macOS\nbrew install azd\n\n# Windows\nwinget install Microsoft.Azd\n\n# Linux\ncurl -fsSL https://aka.ms/install-azd.sh | bash\n```\n\n**Docker Desktop:**\n```bash\n# Verify Docker is running\ndocker version\n\n# Test Docker works\ndocker run hello-world\n```\n\n**Azure CLI (for queries only):**\n```bash\n# macOS\nbrew install azure-cli\n\n# Windows\nwinget install Microsoft.AzureCLI\n```\n\n### Authentication\n\n```bash\n# Login to Azure\naz login\n\n# Verify subscription\naz account show --query \"{name:name, id:id}\" -o table\n\n# Set subscription if needed\naz account set --subscription \"<name-or-id>\"\n```\n\n---\n\n## MCP Tools Available\n\n### azd-Specific MCP Tools\n\nUse the Azure MCP server's azd tools (`azure__azd`) for validation and guidance:\n\n| Command | Description |\n|---------|-------------|\n| `validate_azure_yaml` | **Validates azure.yaml against official JSON schema** - Use before deployment |\n| `discovery_analysis` | Analyze application components for AZD migration |\n| `architecture_planning` | Select Azure services for discovered components |\n| `docker_generation` | Generate optimized Dockerfiles for Container Apps |\n| `infrastructure_generation` | Generate Bicep templates |\n| `iac_generation_rules` | Get Bicep compliance rules and best practices |\n| `project_validation` | Comprehensive validation before deployment |\n| `error_troubleshooting` | Diagnose and troubleshoot azd errors |\n\n### Deployment Planning Tools\n\nUse the `azure__deploy` hierarchical tool with these commands for automated Container Apps deployment:\n\n| Command | Description | Parameters |\n|---------|-------------|------------|\n| `deploy_plan_get` | Generate deployment plan for Container Apps | `workspace-folder`, `project-name`, `target-app-service: \"ContainerApp\"` |\n| `deploy_iac_rules_get` | Get Bicep/Terraform guidelines for Container Apps | `deployment-tool: \"AZD\"`, `iac-type: \"bicep\"`, `resource-types: \"containerapp\"` |\n| `deploy_app_logs_get` | Fetch logs from deployed Container Apps | `workspace-folder`, `azd-env-name`, `limit` |\n| `deploy_pipeline_guidance_get` | Get CI/CD pipeline guidance | `use-azd-pipeline-config: true`, `subscription` |\n| `deploy_architecture_diagram_generate` | Generate architecture diagram | `workspaceFolder`, `projectName`, `services` |\n\n### Pattern 1: Deployment Planning\n\nGenerate a deployment plan by analyzing your workspace and recommending Azure resources.\n\n```javascript\nasync function planDeployment(workspaceFolder: string, projectName: string) {\n  // Generate deployment plan\n  const plan = await azure__deploy({\n    intent: \"Generate deployment plan for container app\",\n    command: \"deploy_plan_get\",\n    parameters: {\n      \"workspace-folder\": workspaceFolder,\n      \"project-name\": projectName,\n      \"target-app-service\": \"ContainerApp\",\n      \"provisioning-tool\": \"AZD\",\n      \"azd-iac-options\": \"bicep\"\n    }\n  });\n  \n  // Plan saved to: .azure/plan.copilotmd\n  return plan;\n}\n```\n\n**Key insight**: The deployment plan detects services, dependencies, and recommends Azure resources before any infrastructure is created.\n\n### Pattern 2: Infrastructure as Code Generation\n\nRetrieve IaC best practices and generate Bicep files for Container Apps deployment.\n\n```javascript\nasync function generateInfrastructure(projectName: string) {\n  // Get IaC rules\n  const rules = await azure__deploy({\n    intent: \"Get IaC rules\",\n    command: \"deploy_iac_rules_get\",\n    parameters: {\n      \"deployment-tool\": \"AZD\",\n      \"iac-type\": \"bicep\",\n      \"resource-types\": \"containerapp\"\n    }\n  });\n  \n  // Get Bicep schema (uses latest API version automatically)\n  const schema = await azure__bicepschema({\n    intent: \"Get Container Apps schema\",\n    command: \"bicepschema_get\",\n    parameters: {\n      \"resource-type\": \"Microsoft.App/containerApps\"\n    }\n  });\n  \n  // Generate main.bicep based on schema and rules\n  // Files created: infra/main.bicep, infra/main.parameters.json, azure.yaml\n}\n```\n\n**Key insight**: Always validate infrastructure before deploying using preview commands (`azd provision --preview`) to catch configuration issues early.\n\n### Pattern 3: Deployment Validation\n\nMonitor and validate deployments using application logs and health checks.\n\n```javascript\nasync function validateDeployment(workspaceFolder: string, envName: string) {\n  // Get application logs\n  const logs = await azure__deploy({\n    intent: \"Get deployment logs\",\n    command: \"deploy_app_logs_get\",\n    parameters: {\n      \"workspace-folder\": workspaceFolder,\n      \"azd-env-name\": envName,\n      \"limit\": 200\n    }\n  });\n  \n  // Check for errors in logs\n  const hasErrors = logs.some(log => log.includes(\"error\") || log.includes(\"exception\"));\n  \n  if (hasErrors) {\n    throw new Error(\"Deployment validation failed - errors found in logs\");\n  }\n  \n  return { status: \"success\", logs };\n}\n```\n\n### Regional Validation\n\nCheck quota and availability before deploying to avoid failures.\n\n```javascript\nasync function checkRegionalCapacity(subscription: string, location: string) {\n  const availability = await azure__quota({\n    intent: \"Check availability\",\n    command: \"quota_region_availability_list\",\n    parameters: {\n      subscription,\n      \"resource-type\": \"Microsoft.App/containerApps\"\n    }\n  });\n  \n  return availability.regions.includes(location);\n}\n```\n\n### Architecture Visualization\n\nGenerate diagrams to understand service dependencies before deployment.\n\n```javascript\nasync function visualizeArchitecture(workspaceFolder: string, services: Array) {\n  return await azure__deploy({\n    intent: \"Generate architecture diagram\",\n    command: \"deploy_architecture_diagram_generate\",\n    parameters: {\n      workspaceFolder,\n      projectName: \"myapp\",\n      services\n    }\n  });\n}\n```\n\n### CI/CD Automation\n\nGet pipeline configuration guidance for automated deployments.\n\n```javascript\nasync function setupCICD(subscription: string) {\n  return await azure__deploy({\n    intent: \"Get CI/CD guidance\",\n    command: \"deploy_pipeline_guidance_get\",\n    parameters: {\n      \"use-azd-pipeline-config\": true,\n      subscription\n    }\n  });\n}\n```\n\n---\n\n## Deployment Workflows\n\n### Multi-Service Deployment (Full-Stack Apps)\n\nDeploy full-stack applications with both frontend and backend services in the same Container Apps environment.\n\n```javascript\nasync function deployFullStackApp(projectName: string) {\n  // Project structure:\n  // /web       - Frontend (HTML/CSS/JS or React/Vue)\n  // /api       - Backend (Node.js/Python/Java)\n  // /Dockerfile - Multi-stage build or separate Dockerfiles\n  \n  const services = [\n    {\n      name: `${projectName}-web`,\n      path: './web',\n      language: 'JavaScript',\n      type: 'frontend',\n      ingress: {\n        external: true,\n        targetPort: 80,\n        allowInsecure: false\n      }\n    },\n    {\n      name: `${projectName}-api`,\n      path: './api',\n      language: 'JavaScript',\n      type: 'backend',\n      ingress: {\n        external: true,  // or false if only internal access\n        targetPort: 3000,\n        allowInsecure: false\n      },\n      env: [\n        { name: 'NODE_ENV', value: 'production' }\n      ]\n    }\n  ];\n  \n  // Generate deployment plan for multi-service app\n  const plan = await azure__deploy({\n    intent: \"Generate deployment plan for full-stack app\",\n    command: \"deploy_plan_get\",\n    parameters: {\n      \"workspace-folder\": process.cwd(),\n      \"project-name\": projectName,\n      \"target-app-service\": \"ContainerApp\",\n      \"provisioning-tool\": \"AZD\",\n      \"azd-iac-options\": \"bicep\"\n    }\n  });\n  \n  // For single Dockerfile serving both UI and API:\n  // - Frontend files served as static from /web\n  // - Backend API exposed on /api/* routes\n  // - Single container app with one ingress\n  \n  return { plan, services };\n}\n```\n\n**Key insight**: For simple full-stack apps (like todo apps), you can use a single container where the backend serves both the API routes and the static frontend files. For more complex apps, deploy separate container apps for frontend and backend.\n\n**Architecture Options:**\n1. **Single Container**: Backend serves static frontend files + API (simpler, good for small apps)\n2. **Two Containers**: Separate UI and API containers (better for larger apps, independent scaling)\n3. **Microservices**: Multiple backend services + frontend (enterprise applications)\n\n---\n\n## MCP Tools Used\n\n---\n\n## Project Structure Detection\n\n### Containerization Signals\n\nazd will recommend Container Apps when it detects:\n\n**Strong indicators:**\n- `Dockerfile` in project root or service directories\n- `docker-compose.yml` with multiple services\n- `.dockerignore` file present\n- User mentions \"container\", \"Docker\", \"scheduled task\", \"cron job\"\n\n**Multi-container patterns:**\n```\nproject/\nâ”œâ”€â”€ frontend/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â””â”€â”€ package.json\nâ”œâ”€â”€ backend/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â””â”€â”€ requirements.txt\nâ”œâ”€â”€ docker-compose.yml\nâ””â”€â”€ azure.yaml (generated by azd)\n```\n\n---\n\n## Deployment Workflow\n\n### Step 1: Initialize azd Project\n\n**From scratch:**\n```bash\n# Navigate to project directory\ncd my-app\n\n# Initialize with azd\nazd init\n\n# Choose Container Apps template\n# azd will detect your Dockerfile(s) and configure automatically\n```\n\n**Using a template:**\n```bash\n# List available templates\nazd template list --filter container\n\n# Initialize from template\nazd init --template todo-nodejs-mongo-aca\n```\n\n### Step 2: Review azure.yaml Configuration\n\nazd generates `azure.yaml` that defines your application:\n\n```yaml\nname: my-containerized-app\nmetadata:\n  template: azd-init@1.0.0\nservices:\n  web:\n    project: ./frontend\n    language: js\n    host: containerapp\n    docker:\n      path: ./Dockerfile\n      context: ./frontend\n  api:\n    project: ./backend\n    language: python\n    host: containerapp\n    docker:\n      path: ./Dockerfile\n      context: ./backend\n```\n\n**Configuration options:**\n\n| Field | Description | Example |\n|-------|-------------|---------|\n| `host: containerapp` | Deploy as Container App | Required |\n| `docker.path` | Path to Dockerfile | `./Dockerfile` |\n| `docker.context` | Docker build context | `.` or `./backend` |\n| `docker.target` | Multi-stage build target | `production` |\n\n### Step 3: Configure Infrastructure (Optional)\n\nazd creates `infra/` directory with Bicep templates. You can customize:\n\n**Container App configuration (`infra/main.bicep`):**\n```bicep\nmodule containerApp 'core/host/container-app.bicep' = {\n  name: 'container-app'\n  params: {\n    name: 'my-app'\n    location: location\n    containerAppsEnvironmentId: containerAppsEnvironment.outputs.id\n    containerRegistryName: containerRegistry.outputs.name\n    secrets: [\n      {\n        name: 'database-connection-string'\n        value: database.outputs.connectionString\n      }\n    ]\n    env: [\n      {\n        name: 'DATABASE_URL'\n        secretRef: 'database-connection-string'\n      }\n      {\n        name: 'NODE_ENV'\n        value: 'production'\n      }\n    ]\n    ingress: {\n      external: true\n      targetPort: 3000\n      allowInsecure: false\n    }\n    scale: {\n      minReplicas: 1\n      maxReplicas: 10\n      rules: [\n        {\n          name: 'http-rule'\n          http: {\n            metadata: {\n              concurrentRequests: '100'\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n**Using Azure Verified Modules:**\n```bicep\nmodule containerApp 'br/public:avm/res/app/container-app:0.4.0' = {\n  name: 'containerAppDeployment'\n  params: {\n    name: 'my-app'\n    environmentId: containerAppsEnvironment.outputs.resourceId\n    containers: [\n      {\n        name: 'main'\n        image: '${containerRegistry.outputs.loginServer}/my-app:latest'\n        resources: {\n          cpu: json('0.5')\n          memory: '1.0Gi'\n        }\n      }\n    ]\n  }\n}\n```\n\n### Step 4: Deploy\n\n**Deploy everything (provision + deploy code):**\n```bash\n# Interactive deployment\nazd up\n\n# Non-interactive with environment (for automation/agents)\nazd up --no-prompt --environment production\n```\n\n> âš ï¸ **CRITICAL for automation**: Always use `--no-prompt` when azd is called by an agent or in CI/CD pipelines where interactive prompts cannot be answered.\n\n**Or deploy in steps:**\n```bash\n# 1. Preview changes before provisioning\nazd provision --preview\n\n# 2. Provision infrastructure only (with --no-prompt for automation)\nazd provision --no-prompt\n\n# 3. Build and deploy containers\nazd deploy --no-prompt\n\n# 4. Deploy specific service\nazd deploy api --no-prompt\n```\n\n**Validate azure.yaml before deployment:**\n```javascript\n// Use MCP tool to validate azure.yaml\nconst validation = await azure__azd({\n  command: \"validate_azure_yaml\",\n  parameters: { path: \"./azure.yaml\" }\n});\n```\n\n**What azd does automatically:**\n1. âœ… Provisions Container Apps Environment\n2. âœ… Creates Azure Container Registry (ACR)\n3. âœ… Builds Docker images locally\n4. âœ… Pushes images to ACR using managed identity\n5. âœ… Deploys containers to Container Apps\n6. âœ… Configures ingress, scaling, and environment variables\n7. âœ… Sets up Log Analytics workspace\n\n---\n\n## Container Apps Features\n\n### Ingress Configuration\n\n**External ingress (public internet):**\n```bicep\ningress: {\n  external: true\n  targetPort: 8080\n  allowInsecure: false  // HTTPS only\n  transport: 'auto'     // HTTP/1 and HTTP/2\n}\n```\n\n**Internal ingress (within environment):**\n```bicep\ningress: {\n  external: false\n  targetPort: 3000\n  allowInsecure: false\n}\n```\n\n**Custom domain:**\n```bicep\ncustomDomains: [\n  {\n    name: 'api.contoso.com'\n    certificateId: certificate.id\n    bindingType: 'SniEnabled'\n  }\n]\n```\n\n#---\n\n## Azure Resources Reference\n\n### Core Resources for Container Apps\n\n| Resource Type | Purpose | API Version |\n|--------------|---------|-------------|\n| `Microsoft.App/containerApps` | Container App instance (long-running services) | 2024-03-01 |\n| `Microsoft.App/jobs` | Container Apps Job (finite-duration tasks) | 2024-03-01 |\n| `Microsoft.App/managedEnvironments` | Container Apps Environment (shared infrastructure) | 2024-03-01 |\n| `Microsoft.ContainerRegistry/registries` | Container Registry (ACR) for image storage | 2023-11-01-preview |\n| `Microsoft.OperationalInsights/workspaces` | Log Analytics for monitoring and diagnostics | 2023-09-01 |\n\n### Example Bicep Template - Container App\n\n```bicep\nresource containerApp 'Microsoft.App/containerApps@2024-03-01' = {\n  name: projectName\n  location: location\n  identity: {\n    type: 'SystemAssigned'\n  }\n  properties: {\n    environmentId: containerAppEnv.id\n    configuration: {\n      ingress: {\n        external: true\n        targetPort: 80\n        transport: 'auto'\n        allowInsecure: false\n      }\n      registries: [\n        {\n          server: containerRegistry.properties.loginServer\n          identity: 'system'\n        }\n      ]\n    }\n    template: {\n      containers: [\n        {\n          name: projectName\n          image: containerImage\n          resources: {\n            cpu: json('0.5')\n            memory: '1Gi'\n          }\n          env: [\n            {\n              name: 'NODE_ENV'\n              value: 'production'\n            }\n          ]\n        }\n      ]\n      scale: {\n        minReplicas: 1\n        maxReplicas: 10\n        rules: [\n          {\n            name: 'http-scaling'\n            http: {\n              metadata: {\n                concurrentRequests: '100'\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n```\n\n### Example Bicep Templates - Container Apps Jobs\n\n**Scheduled Job (Cron):**\n\n```bicep\nresource containerAppJob 'Microsoft.App/jobs@2024-03-01' = {\n  name: '${projectName}-job'\n  location: location\n  identity: {\n    type: 'SystemAssigned'\n  }\n  properties: {\n    environmentId: containerAppEnv.id\n    configuration: {\n      triggerType: 'Schedule'  // Options: Manual, Schedule, Event\n      replicaTimeout: 1800  // 30 minutes\n      replicaRetryLimit: 3\n      scheduleTriggerConfig: {\n        cronExpression: '0 0 * * *'  // Daily at midnight UTC\n        parallelism: 1\n        replicaCompletionCount: 1\n      }\n      registries: [\n        {\n          server: containerRegistry.properties.loginServer\n          identity: 'system'\n        }\n      ]\n    }\n    template: {\n      containers: [\n        {\n          name: '${projectName}-job'\n          image: containerImage\n          resources: {\n            cpu: json('0.25')\n            memory: '0.5Gi'\n          }\n          env: [\n            {\n              name: 'JOB_TYPE'\n              value: 'scheduled'\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n```\n\n**Event-Driven Job (Queue Trigger):**\n\n```bicep\nresource eventDrivenJob 'Microsoft.App/jobs@2024-03-01' = {\n  name: '${projectName}-queue-job'\n  location: location\n  identity: {\n    type: 'SystemAssigned'\n  }\n  properties: {\n    environmentId: containerAppEnv.id\n    configuration: {\n      triggerType: 'Event'\n      replicaTimeout: 1800  // 30 minutes\n      replicaRetryLimit: 2\n      eventTriggerConfig: {\n        parallelism: 1\n        replicaCompletionCount: 1\n        scale: {\n          minExecutions: 0\n          maxExecutions: 10\n          pollingInterval: 30\n          rules: [\n            {\n              name: 'azure-queue-rule'\n              type: 'azure-queue'\n              metadata: {\n                accountName: storageAccountName\n                queueName: queueName\n                queueLength: '1'\n              }\n              auth: [\n                {\n                  secretRef: 'storage-connection-string'\n                  triggerParameter: 'connection'\n                }\n              ]\n            }\n          ]\n        }\n      }\n      secrets: [\n        {\n          name: 'storage-connection-string'\n          value: storageConnectionString\n        }\n      ]\n      registries: [\n        {\n          server: containerRegistry.properties.loginServer\n          identity: 'system'\n        }\n      ]\n    }\n    template: {\n      containers: [\n        {\n          name: '${projectName}-queue-processor'\n          image: containerImage\n          resources: {\n            cpu: json('0.25')\n            memory: '0.5Gi'\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n**Manual Job (On-Demand):**\n\n```bicep\nresource manualJob 'Microsoft.App/jobs@2024-03-01' = {\n  name: '${projectName}-manual-job'\n  location: location\n  properties: {\n    environmentId: containerAppEnv.id\n    configuration: {\n      triggerType: 'Manual'\n      replicaTimeout: 3600  // 1 hour\n      manualTriggerConfig: {\n        parallelism: 1\n        replicaCompletionCount: 1\n      }\n    }\n    template: {\n      containers: [\n        {\n          name: 'migration'\n          image: containerImage\n        }\n      ]\n    }\n  }\n}\n```\n\n**Start manual job:**\n```bash\naz containerapp job start \\\n  --name \"${projectName}-manual-job\" \\\n  --resource-group my-rg\n```\n\n---\n\n## Environment Variables and Secrets\n\n**HTTP-based scaling:**\n```bicep\nscale: {\n  minReplicas: 0  // Scale to zero when idle\n  maxReplicas: 30\n  rules: [\n    {\n      name: 'http-scaling'\n      http: {\n        metadata: {\n          concurrentRequests: '100'\n        }\n      }\n    }\n  ]\n}\n```\n\n**Custom scaler (KEDA):**\n```bicep\nscale: {\n  minReplicas: 1\n  maxReplicas: 10\n  rules: [\n    {\n      name: 'azure-queue-scaler'\n      azureQueue: {\n        queueName: 'orders'\n        queueLength: 10\n        auth: [\n          {\n            secretRef: 'storage-connection-string'\n            triggerParameter: 'connection'\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\n**Supported KEDA scalers:**\n- Azure Service Bus Queue/Topic\n- Azure Storage Queue\n- Azure Event Hubs\n- HTTP (polling external endpoint)\n- CPU/Memory metrics\n- Cron (scheduled scaling)\n\n### Environment Variables and Secrets\n\n**Environment variables:**\n```bicep\nenv: [\n  {\n    name: 'API_BASE_URL'\n    value: 'https://api.contoso.com'\n  }\n  {\n    name: 'LOG_LEVEL'\n    value: 'info'\n  }\n]\n```\n\n**Secrets (from Key Vault):**\n```bicep\nsecrets: [\n  {\n    name: 'database-password'\n    keyVaultUrl: '${keyVault.properties.vaultUri}secrets/db-password'\n    identity: managedIdentity.id\n  }\n]\nenv: [\n  {\n    name: 'DATABASE_PASSWORD'\n    secretRef: 'database-password'\n  }\n]\n```\n\n**Secrets (from values):**\n```bicep\nsecrets: [\n  {\n    name: 'api-key'\n    value: apiKeySecretValue\n  }\n]\nenv: [\n  {\n    name: 'API_KEY'\n    secretRef: 'api-key'\n  }\n]\n```\n\n---\n\n## Scaling Configuration\n\n**Service-to-service calls within environment:**\n\nFrontend â†’ Backend communication:\n```javascript\n// Frontend container calling backend\n// Use internal FQDN: <app-name>.<environment-name>.internal\nconst API_URL = process.env.API_INTERNAL_URL || \n  'http://api.my-env.internal';\n\nfetch(`${API_URL}/api/users`)\n  .then(res => res.json());\n```\n\n**Dapr integration:**\n```bicep\ndapr: {\n  enabled: true\n  appId: 'backend-api'\n  appPort: 3000\n  appProtocol: 'http'\n}\n```\n\nFrontend calling backend via Dapr:\n```javascript\nconst daprPort = process.env.DAPR_HTTP_PORT || 3500;\nconst response = await fetch(\n  `http://localhost:${daprPort}/v1.0/invoke/backend-api/method/users`\n);\n```\n\n---\n\n## Dockerfile Best Practices\n\n### Node.js Dockerfile Example\n\n```dockerfile\n# Multi-stage build for smaller images\nFROM node:20-alpine AS builder\n\nWORKDIR /app\n\n# Copy package files\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm ci --only=production\n\n# Copy source code\nCOPY . .\n\n# Build if needed\nRUN npm run build\n\n# Production stage\nFROM node:20-alpine AS production\n\n# Run as non-root user\nRUN addgroup -g 1001 -S nodejs && \\\n    adduser -S nodejs -u 1001\n\nWORKDIR /app\n\n# Copy only necessary files from builder\nCOPY --from=builder --chown=nodejs:nodejs /app/node_modules ./node_modules\nCOPY --from=builder --chown=nodejs:nodejs /app/dist ./dist\nCOPY --from=builder --chown=nodejs:nodejs /app/package*.json ./\n\nUSER nodejs\n\n# Expose port (must match Container App ingress.targetPort)\nEXPOSE 3000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD node -e \"require('http').get('http://localhost:3000/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))\"\n\nCMD [\"node\", \"dist/index.js\"]\n```\n\n### Python Dockerfile Example\n\n```dockerfile\nFROM python:3.12-slim AS builder\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# Production stage\nFROM python:3.12-slim AS production\n\n# Run as non-root\nRUN useradd -m -u 1001 appuser\n\nWORKDIR /app\n\n# Copy dependencies from builder\nCOPY --from=builder /root/.local /home/appuser/.local\nCOPY --chown=appuser:appuser . .\n\nUSER appuser\n\n# Add local bin to PATH\nENV PATH=/home/appuser/.local/bin:$PATH\n\nEXPOSE 8000\n\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health')\"\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n### .NET Dockerfile Example\n\n```dockerfile\nFROM mcr.microsoft.com/dotnet/sdk:8.0 AS build\n\nWORKDIR /src\n\n# Copy csproj and restore\nCOPY [\"MyApp.csproj\", \"./\"]\nRUN dotnet restore\n\n# Copy source and build\nCOPY . .\nRUN dotnet build -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish -c Release -o /app/publish\n\n# Runtime stage\nFROM mcr.microsoft.com/dotnet/aspnet:8.0 AS final\n\nWORKDIR /app\nCOPY --from=publish /app/publish .\n\n# Run as non-root\nUSER $APP_UID\n\nEXPOSE 8080\n\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD curl --fail http://localhost:8080/health || exit 1\n\nENTRYPOINT [\"dotnet\", \"MyApp.dll\"]\n```\n\n### Dockerfile Optimization Tips\n\n**1. Use multi-stage builds** - Keep images small\n**2. Run as non-root user** - Security best practice\n**3. Add health checks** - Container Apps will use these\n**4. Use .dockerignore** - Faster builds, smaller context\n\n**.dockerignore example:**\n```\nnode_modules\nnpm-debug.log\n.git\n.gitignore\n.env\n.vscode\n*.md\nDockerfile\n.dockerignore\ndist\nbuild\ncoverage\n```\n\n---\n\n## Monitoring and Logging\n\n### View Logs with azd\n\n```bash\n# Stream logs from specific service\nazd deploy api --logs\n\n# Or use Azure CLI\naz containerapp logs show \\\n  --name api \\\n  --resource-group my-rg \\\n  --follow\n```\n\n### Query Logs with Log Analytics\n\n```kusto\n// Container logs\nContainerAppConsoleLogs_CL\n| where ContainerAppName_s == 'api'\n| where TimeGenerated > ago(1h)\n| project TimeGenerated, Log_s\n| order by TimeGenerated desc\n\n// System logs\nContainerAppSystemLogs_CL\n| where ContainerAppName_s == 'api'\n| where TimeGenerated > ago(1h)\n| project TimeGenerated, Log_s\n\n// Ingress logs (HTTP requests)\nContainerAppConsoleLogs_CL\n| where ContainerAppName_s == 'api'\n| where Log_s contains \"HTTP\"\n| extend StatusCode = extract(@\"HTTP/\\d\\.\\d\\\" (\\d+)\", 1, Log_s)\n| summarize count() by StatusCode, bin(TimeGenerated, 5m)\n```\n\n### Application Insights Integration\n\nAdd to your Bicep:\n```bicep\nresource appInsights 'Microsoft.Insights/components@2020-02-02' = {\n  name: 'app-insights'\n  location: location\n  kind: 'web'\n  properties: {\n    Application_Type: 'web'\n  }\n}\n\n// Add to Container App env\nenv: [\n  {\n    name: 'APPLICATIONINSIGHTS_CONNECTION_STRING'\n    value: appInsights.properties.ConnectionString\n  }\n]\n```\n\n---\n\n## Troubleshooting\n\n### Container App Won't Start\n\n**Check revision status:**\n```bash\naz containerapp revision list \\\n  --name my-app \\\n  --resource-group my-rg \\\n  --query \"[].{Name:name, Active:properties.active, ProvisioningState:properties.provisioningState}\"\n```\n\n### Common Issues\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| **Docker not running** | Error: \"Docker daemon is not running\" | Start Docker Desktop. Windows: Search for Docker Desktop in Start menu. macOS: Open Docker Desktop from Applications. Verify with `docker ps` |\n| **Docker not installed** | Error: \"docker: command not found\" | Install Docker Desktop from https://www.docker.com/products/docker-desktop |\n| **Not authenticated** | Error: \"authentication required\" | Run `az login` and `azd auth login` to authenticate with Azure |\n| **Location not set** | Error: \"location property must be specified\" | Set location: `azd env set AZURE_LOCATION eastus` |\n| **Image pull failures** | Container fails to start | Check ACR is accessible. Verify managed identity has AcrPull role. Ensure image tag is correct. Run: `az containerapp registry set --name APP -g RG --server ACR.azurecr.io --identity system` |\n| **ACR Tasks disabled** | `az acr build` fails with \"TasksOperationsNotAllowed\" | Free/trial subscriptions have ACR Tasks disabled. Build locally: `docker build`, `az acr login`, `docker push` |\n| **Container crashes immediately** | Container exits right after starting | Check logs: `az containerapp logs show --name my-app --resource-group my-rg`. Verify health check endpoint exists. Check environment variables are set correctly |\n| **Port mismatch** | App not accessible after deployment | Ensure `EXPOSE` in Dockerfile matches `ingress.targetPort` in Bicep. Application must listen on correct port |\n| **Health check failures** | Container marked unhealthy | Verify `/health` endpoint returns 200 OK. Check startup time isn't exceeding timeout. Adjust probe timings if needed |\n| **Bicep validation errors** | Error during `azd provision` | Use `get_errors` tool on Bicep files to identify syntax issues |\n| **User/group already exists** | Dockerfile build fails with \"group in use\" | Base images may have users pre-configured. Check if user exists before creating in Dockerfile |\n| **Cold start timeouts** | First request times out after idle period | Set minimum replicas to prevent cold starts: `az containerapp update --name APP -g RG --min-replicas 1` |\n| **Out of memory** | Container crashes or restarts frequently | Increase memory limits in Bicep configuration or `resources` section |\n| **Missing env vars** | Application errors on startup | Configure environment variables in container app settings or Bicep template |\n| **Job execution timeout** | Job fails with timeout error | Increase `replicaTimeout` value (default 1800s, max varies by plan) |\n| **Job not triggering** | Scheduled job doesn't run at expected time | Verify cron expression syntax. Remember times are in UTC timezone. Check logs for execution history |\n| **Event-driven job not scaling** | Queue messages not being processed | Check scale rule configuration, verify secrets and connection strings are correct. Test queue connectivity |\n| **Job replica failures** | Job keeps failing and retrying | Check logs with `az containerapp job execution list`. Verify container exits with code 0 on success. Review retry limits |\n| **Invalid cron expression** | Job creation fails with validation error | Use standard 5-field cron format: `minute hour day-of-month month day-of-week` |\n\n### Performance Issues\n\n**Check replica count:**\n```bash\naz containerapp show \\\n  --name my-app \\\n  --resource-group my-rg \\\n  --query \"properties.template.scale\"\n```\n\n**Adjust scaling rules:**\n- Increase `maxReplicas` for more capacity\n- Decrease `concurrentRequests` to scale out sooner\n- Add CPU/memory-based scaling if needed\n\n### Deployment Failures\n\n**azd deploy fails:**\n```bash\n# Check azd logs\nazd deploy --debug\n\n# Verify Docker is running\ndocker version\n\n# Test Docker build locally\ndocker build -t test .\n\n# Check Azure subscription\naz account show\n```\n\n**ACR authentication issues:**\n```bash\n# Login to ACR manually\naz acr login --name <registry-name>\n\n# Verify managed identity has AcrPull role\naz role assignment list \\\n  --scope /subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.ContainerRegistry/registries/<acr-name> \\\n  --query \"[?principalId=='<identity-id>']\"\n```\n\n---\n\n## Cost Optimization\n\n**Scale to zero:**\n```bicep\nscale: {\n  minReplicas: 0  // No cost when idle\n  maxReplicas: 10\n}\n```\n\n**Right-size containers:**\n```bicep\nresources: {\n  cpu: json('0.25')     // 0.25 vCPU\n  memory: '0.5Gi'       // 512 MB\n}\n```\n\n**Use Consumption plan (default):**\n- Pay only for what you use\n- Billed per vCPU-second and GB-second\n- Includes monthly free grant\n\n**Monitor costs:**\n```bash\n# View resource costs\naz consumption usage list \\\n  --start-date 2024-01-01 \\\n  --end-date 2024-01-31 \\\n  --query \"[?contains(instanceName, 'my-app')]\"\n```\n\n---\n\n## Best Practices\n\n| Practice | Description |\n|----------|-------------|\n| **Validate Docker first** | Always verify Docker is running with `docker ps` before starting deployment to avoid build failures |\n| **Use managed identity** | Always prefer managed identity over key-based authentication for secure resource access |\n| **Preview before deploy** | Run `azd provision --preview` to validate infrastructure changes before deploying |\n| **Test locally** | Build and test Docker images locally with `docker build` before pushing to Azure Container Registry |\n| **Port matching** | Ensure `ingress.targetPort` in configuration matches the port your application listens on (must match Dockerfile `EXPOSE`) |\n| **Structured logging** | Enable Application Insights and Log Analytics for centralized monitoring and troubleshooting |\n| **Health probes** | Configure liveness and readiness probes for reliability. Implement `/health` endpoint in your app |\n| **Auto-scaling** | Set min/max replica counts based on expected workload patterns. Set min=0 for cost savings, min=1 to prevent cold starts |\n| **Use multi-stage builds** | Keep Docker images small by using multi-stage builds and only copying necessary files to production stage |\n| **Run as non-root** | Always run containers as non-root user for security. Create dedicated user in Dockerfile |\n| **Environment-specific config** | Use azd environments for dev/staging/production. Store secrets in Key Vault, not in code |\n| **Resource right-sizing** | Start with small CPU/memory allocations (0.25 vCPU, 0.5Gi) and scale up based on monitoring data |\n| **Job timeout planning** | Set realistic `replicaTimeout` values for jobs. Monitor execution times and adjust accordingly |\n| **UTC timezone awareness** | Remember all cron schedules run in UTC. Adjust expressions for your local timezone |\n| **Container exit codes** | Ensure jobs exit with code 0 on success. Non-zero codes trigger retries and failures |\n\n---\n\n## Security Best Practices\n\n### 1. Use Managed Identity\n\n**Never store credentials in code or environment variables:**\n\n```bicep\nidentity: {\n  type: 'SystemAssigned'\n}\n\n// Grant access to Key Vault\nresource keyVaultPolicy 'Microsoft.KeyVault/vaults/accessPolicies@2023-02-01' = {\n  parent: keyVault\n  name: 'add'\n  properties: {\n    accessPolicies: [\n      {\n        tenantId: subscription().tenantId\n        objectId: containerApp.identity.principalId\n        permissions: {\n          secrets: ['get']\n        }\n      }\n    ]\n  }\n}\n```\n\n### 2. Store Secrets in Key Vault\n\n```bicep\nsecrets: [\n  {\n    name: 'database-password'\n    keyVaultUrl: '${keyVault.properties.vaultUri}secrets/db-password'\n    identity: containerApp.identity.id\n  }\n]\n```\n\n### 3. Use Internal Ingress When Possible\n\n```bicep\ningress: {\n  external: false  // Only accessible within environment\n  targetPort: 3000\n}\n```\n\n### 4. Enable HTTPS Only\n\n```bicep\ningress: {\n  external: true\n  allowInsecure: false  // Redirect HTTP to HTTPS\n  targetPort: 443\n}\n```\n\n### 5. Restrict Network Access\n\n```bicep\nworkloadProfiles: [\n  {\n    name: 'Consumption'\n    workloadProfileType: 'Consumption'\n  }\n]\nvnetConfiguration: {\n  infrastructureSubnetId: subnet.id\n  internal: true  // No public IP\n}\n```\n\n---\n\n## Common Patterns\n\n### Pattern 1: Frontend + Backend\n\n```yaml\n# azure.yaml\nservices:\n  web:\n    project: ./frontend\n    host: containerapp\n    docker:\n      path: ./Dockerfile\n      context: ./frontend\n  api:\n    project: ./backend\n    host: containerapp\n    docker:\n      path: ./Dockerfile\n      context: ./backend\n```\n\n### Pattern 2: API + Background Job\n\n```yaml\nservices:\n  api:\n    project: ./api\n    host: containerapp\n  worker:\n    project: ./worker\n    host: containerjob  # Scheduled job\n```\n\n### Pattern 3: Microservices with Dapr\n\n```yaml\nservices:\n  orders:\n    project: ./services/orders\n    host: containerapp\n  inventory:\n    project: ./services/inventory\n    host: containerapp\n  notifications:\n    project: ./services/notifications\n    host: containerapp\n```\n\n---\n\n## Next Steps\n\n- See [Azure Verified Modules](./azure-verified-modules.md) for Bicep module reference\n- See [Multi-Service Guide](./multi-service.md) for complex deployments\n- See [Troubleshooting Guide](./troubleshooting.md) for more help\n\n---\n\n## Additional Resources\n\n- [Azure Container Apps Documentation](https://learn.microsoft.com/azure/container-apps/)\n- [KEDA Scalers](https://keda.sh/docs/scalers/)\n- [Dapr Documentation](https://docs.dapr.io/)\n- [azd Templates](https://azure.github.io/awesome-azd/)\n",
        "plugin/skills/azure-create-app/reference/error-handling.md": "# Error Handling\r\n\r\n## MCP Tool Failures\r\n\r\nIf any MCP tool call fails, call the `azure__azd` MCP tool for troubleshooting:\r\n\r\n```javascript\r\nawait azure__azd({\r\n  command: \"error_troubleshooting\",\r\n  parameters: {}\r\n});\r\n```\r\n\r\n## Common Error Resolutions\r\n\r\n| Error | Resolution |\r\n|-------|------------|\r\n| \"azure.yaml invalid\" | Call `validate_azure_yaml` and fix reported errors |\r\n| \"Bicep compilation error\" | Check module paths and parameters |\r\n| \"Service not found\" | Verify service name matches `azure.yaml` configuration |\r\n| \"Docker build failed\" | Check Dockerfile syntax and base image availability |\r\n| \"Authentication failed\" | Run `az login` or check service principal credentials |\r\n\r\n## Validation Errors\r\n\r\n### azure.yaml Schema Errors\r\n\r\nCommon issues:\r\n- Missing required `name` property\r\n- Invalid `host` value\r\n- Missing `project` path\r\n- Incorrect `dist` path for static web apps\r\n\r\n### Bicep Compilation Errors\r\n\r\nCommon issues:\r\n- Missing module files\r\n- Parameter type mismatches\r\n- Undefined variables\r\n- Invalid resource API versions\r\n\r\n## Debugging Tips\r\n\r\n1. **Check azure.yaml syntax** - Use `validate_azure_yaml` command\r\n2. **Verify file paths** - Ensure all referenced paths exist\r\n3. **Review Bicep templates** - Check for compilation errors\r\n4. **Test locally first** - Run `azd package` before `azd provision`\r\n",
        "plugin/skills/azure-create-app/reference/functions.md": "# Azure Functions Deployment Guide\n\nComplete reference for deploying serverless functions to Azure Function Apps using `azd` (Azure Developer CLI) and Azure Functions Core Tools.\n\n---\n\n## Overview\n\nAzure Functions is a serverless compute service that enables you to run event-driven code without managing infrastructure. This guide provides comprehensive deployment workflows for serverless applications.\n\n**Key Benefits:**\n- **Serverless execution** - Pay only for execution time\n- **Event-driven** - Respond to triggers from various Azure services\n- **Multiple languages** - Node.js, Python, .NET, Java, PowerShell\n- **Integrated bindings** - Simplified connections to Azure services\n- **Flexible hosting** - Consumption, Premium, or Dedicated plans\n\n**When to use Azure Functions:**\n- **Serverless APIs** - HTTP-triggered functions for REST endpoints\n- **Background processing** - Queue-triggered functions for async workloads\n- **Scheduled tasks** - Timer-triggered functions for cron-like jobs\n- **Event handlers** - Blob, Event Grid, or Service Bus triggered functions\n- **Webhooks** - HTTP endpoints for third-party integrations\n- **Data processing** - Transform and process data in real-time\n\n**Deployment Workflow:**\n```\nInit â†’ Develop â†’ Test Locally â†’ azd up â†’ Monitor\n```\n\n---\n\n## Always Use azd for Deployments\n\n> **Always use `azd` (Azure Developer CLI) for Azure provisioning and Functions deployments.**\n> The `azd` tool provides a complete, reproducible deployment workflow for all Functions scenarios.\n\n```bash\n# Deploy everything in one command\nazd up --no-prompt\n\n# Or step-by-step:\nazd init                    # Create azure.yaml and infra/\nazd provision --no-prompt   # Create Function App, Storage, and dependencies\nazd deploy --no-prompt      # Deploy function code\n\n# Preview changes before deployment\nazd provision --preview\n\n# Clean up test environments\nazd down --force --purge\n```\n\n> âš ï¸ **CRITICAL: `azd down` Data Loss Warning**\n>\n> `azd down` **permanently deletes ALL resources** including Function Apps, Storage accounts, and Key Vaults.\n> Always back up important data before running `azd down`.\n\n**Why azd is required:**\n- **Parallel provisioning** - Deploys in seconds, not minutes\n- **Single command** - `azd up` replaces 5+ commands\n- **Infrastructure as Code** - Reproducible with Bicep\n- **Environment management** - Easy dev/staging/prod separation\n- **Consistent workflow** - Same commands work across all Azure services\n\n---\n\n## Prerequisites and Validation\n\n### Pattern 0: Prerequisites Validation\n\n**Always validate all prerequisites before starting deployment.**\n\n```javascript\nasync function validatePrerequisites() {\n  const checks = [];\n  \n  // Check azd authentication\n  try {\n    await exec('azd auth login --check-status');\n    checks.push({ name: 'Azure Developer CLI', status: 'authenticated' });\n  } catch (error) {\n    throw new Error('Not authenticated with Azure Developer CLI. Run: azd auth login');\n  }\n  \n  // Check Azure Functions Core Tools - install if not present\n  try {\n    await exec('func --version');\n    checks.push({ name: 'Azure Functions Core Tools', status: 'installed' });\n  } catch (error) {\n    console.log('Azure Functions Core Tools not found. Installing...');\n    try {\n      await exec('npm install -g azure-functions-core-tools@4 --unsafe-perm true');\n      checks.push({ name: 'Azure Functions Core Tools', status: 'installed (just now)' });\n    } catch (installError) {\n      throw new Error('Failed to install Azure Functions Core Tools. Please install manually: npm install -g azure-functions-core-tools@4');\n    }\n  }\n  \n  return checks;\n}\n```\n\n**Key insight**: Azure Functions Core Tools (`func`) is required for local development and deployment. The validation will attempt automatic installation via npm if not found.\n\n### Platform-Specific Installation\n\nIf npm installation fails, use platform-specific installers:\n\n```bash\n# Windows (winget)\nwinget install Microsoft.AzureFunctionsCoreTools\n\n# Windows (Chocolatey)\nchoco install azure-functions-core-tools\n\n# macOS (Homebrew)\nbrew tap azure/functions\nbrew install azure-functions-core-tools@4\n\n# Linux (Ubuntu/Debian)\ncurl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\nsudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\nsudo sh -c 'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs)-prod $(lsb_release -cs) main\" > /etc/apt/sources.list.d/dotnetdev.list'\nsudo apt-get update\nsudo apt-get install azure-functions-core-tools-4\n```\n\n### Prerequisites Checklist\n\n**Setup:**\n- [ ] Azure subscription created\n- [ ] Azure Developer CLI installed (`azd version`)\n- [ ] Azure Developer CLI authenticated (`azd auth login`)\n- [ ] Azure Functions Core Tools installed (`func --version`)\n- [ ] Node.js/Python/.NET installed (based on runtime)\n\n---\n\n## Pattern 1: Initialize Function Project\n\nCreate a new Azure Functions project with the desired runtime.\n\n```bash\n# Create new function project\nfunc init MyFunctionApp --worker-runtime node --model V4\n\n# Create a new HTTP-triggered function\ncd MyFunctionApp\nfunc new --name HttpTrigger --template \"HTTP trigger\"\n\n# For TypeScript\nfunc init MyFunctionApp --worker-runtime node --language typescript --model V4\n\n# For Python\nfunc init MyFunctionApp --worker-runtime python --model V2\n\n# For .NET\nfunc init MyFunctionApp --worker-runtime dotnet-isolated\n```\n\n**Supported runtimes:** `node`, `python`, `dotnet`, `dotnet-isolated`, `java`, `powershell`, `custom`\n\n### Project Structure\n\n```\nMyFunctionApp/\nâ”œâ”€â”€ host.json              # Function app configuration\nâ”œâ”€â”€ local.settings.json    # Local development settings\nâ”œâ”€â”€ package.json           # Node.js dependencies\nâ””â”€â”€ src/\n    â””â”€â”€ functions/\n        â””â”€â”€ HttpTrigger.js # Function code\n```\n\n### host.json Configuration\n\n```json\n{\n  \"version\": \"2.0\",\n  \"logging\": {\n    \"applicationInsights\": {\n      \"samplingSettings\": {\n        \"isEnabled\": true,\n        \"maxTelemetryItemsPerSecond\": 20\n      }\n    }\n  },\n  \"functionTimeout\": \"00:05:00\",\n  \"extensions\": {\n    \"http\": {\n      \"routePrefix\": \"api\"\n    }\n  }\n}\n```\n\n---\n\n## Pattern 2: Local Development\n\nTest functions locally before deploying.\n\n```bash\n# Start local development server\nfunc start\n\n# Start with specific port\nfunc start --port 7072\n\n# Start with debugging enabled\nfunc start --verbose\n\n# Start with specific language worker\nfunc start --python\n```\n\n**Local endpoints:**\n- HTTP triggers: `http://localhost:7071/api/{functionName}`\n- Admin API: `http://localhost:7071/admin/functions`\n\n### Example Functions\n\n#### HTTP Function (Node.js v4)\n\n```javascript\nconst { app } = require('@azure/functions');\n\napp.http('HttpTrigger', {\n    methods: ['GET', 'POST'],\n    authLevel: 'anonymous',\n    handler: async (request, context) => {\n        context.log('HTTP function processed a request.');\n        const name = request.query.get('name') || await request.text() || 'World';\n        return { body: `Hello, ${name}!` };\n    }\n});\n```\n\n#### Timer Function\n\n```javascript\nconst { app } = require('@azure/functions');\n\napp.timer('TimerTrigger', {\n    schedule: '0 */5 * * * *', // Every 5 minutes\n    handler: async (myTimer, context) => {\n        context.log('Timer trigger executed at:', new Date().toISOString());\n    }\n});\n```\n\n#### Queue Trigger Function\n\n```javascript\nconst { app } = require('@azure/functions');\n\napp.storageQueue('QueueTrigger', {\n    queueName: 'myqueue',\n    connection: 'AzureWebJobsStorage',\n    handler: async (message, context) => {\n        context.log('Queue message:', message);\n    }\n});\n```\n\n#### Blob Trigger Function\n\n```javascript\nconst { app } = require('@azure/functions');\n\napp.storageBlob('BlobTrigger', {\n    path: 'samples/{name}',\n    connection: 'AzureWebJobsStorage',\n    handler: async (blob, context) => {\n        context.log('Blob name:', context.triggerMetadata.name);\n        context.log('Blob size:', blob.length);\n    }\n});\n```\n\n---\n\n## Pattern 3: Create Azure Resources\n\n### Using azd (Required)\n\n```bash\n# Initialize project with azure.yaml\nazd init\n\n# Provision infrastructure and deploy\nazd up --no-prompt\n\n# Or step-by-step:\nazd provision --no-prompt   # Create Function App, Storage, App Insights\nazd deploy --no-prompt      # Deploy function code\n```\n\n### Hosting Plans\n\nConfigure hosting plan in your Bicep templates under `infra/`:\n\n| Plan | Use Case | Scaling | Pricing |\n|------|----------|---------|---------|\n| **Consumption** | Event-driven, variable load | Auto-scale, scale to zero | Pay per execution |\n| **Premium** | Enhanced performance, VNET | Pre-warmed instances | Fixed hourly rate |\n| **Dedicated (App Service)** | Predictable workloads | Manual/auto-scale | App Service Plan pricing |\n\n**Bicep example for Consumption plan (default):**\n```bicep\nresource hostingPlan 'Microsoft.Web/serverfarms@2023-12-01' = {\n  name: hostingPlanName\n  location: location\n  sku: {\n    name: 'Y1'\n    tier: 'Dynamic'\n  }\n  properties: {}\n}\n\nresource functionApp 'Microsoft.Web/sites@2023-12-01' = {\n  name: functionAppName\n  location: location\n  kind: 'functionapp,linux'\n  properties: {\n    serverFarmId: hostingPlan.id\n    siteConfig: {\n      linuxFxVersion: 'NODE|20'\n      appSettings: [\n        { name: 'FUNCTIONS_WORKER_RUNTIME', value: 'node' }\n        { name: 'FUNCTIONS_EXTENSION_VERSION', value: '~4' }\n        { name: 'AzureWebJobsStorage', value: storageConnectionString }\n      ]\n    }\n  }\n}\n```\n\n**Bicep example for Premium plan:**\n```bicep\nresource hostingPlan 'Microsoft.Web/serverfarms@2023-12-01' = {\n  name: 'myPremiumPlan'\n  location: location\n  sku: {\n    name: 'EP1'\n    tier: 'ElasticPremium'\n  }\n  kind: 'elastic'\n  properties: {\n    reserved: true  // Linux\n  }\n}\n```\n\n**Bicep example for Dedicated plan:**\n```bicep\nresource hostingPlan 'Microsoft.Web/serverfarms@2023-12-01' = {\n  name: 'myAppServicePlan'\n  location: location\n  sku: {\n    name: 'B1'\n    tier: 'Basic'\n  }\n  properties: {\n    reserved: true  // Linux\n  }\n}\n```\n\n---\n\n## Pattern 4: Deploy Functions\n\nDeploy functions to Azure using azd or Azure Functions Core Tools.\n\n### Using azd (Recommended)\n\n```bash\n# Deploy with azd\nazd deploy --no-prompt\n\n# Deploy to specific environment\nazd deploy --environment staging --no-prompt\n```\n\n### Using func CLI\n\n```bash\n# Deploy to Azure (from project root)\nfunc azure functionapp publish $FUNCTION_APP\n\n# Deploy with build (for TypeScript/compiled projects)\nfunc azure functionapp publish $FUNCTION_APP --build remote\n\n# Deploy with verbose output\nfunc azure functionapp publish $FUNCTION_APP --verbose\n\n# Deploy with npm install on remote\nfunc azure functionapp publish $FUNCTION_APP --build-native-deps\n```\n\n### Deployment Options\n\n```bash\n# Deploy using zip deployment\nfunc azure functionapp publish $FUNCTION_APP\n\n# Deploy specific slot\nfunc azure functionapp publish $FUNCTION_APP --slot staging\n\n# Force update function app settings from local.settings.json\nfunc azure functionapp publish $FUNCTION_APP --publish-local-settings\n\n# Publish settings only (no code)\nfunc azure functionapp publish $FUNCTION_APP --publish-settings-only\n\n# Overwrite existing settings\nfunc azure functionapp publish $FUNCTION_APP --publish-local-settings --overwrite-settings\n```\n\n---\n\n## Pattern 5: Configuration Management\n\nManage application settings using azd environment variables and Bicep.\n\n### Using azd (Recommended)\n\n```bash\n# Set environment variables with azd\nazd env set MY_SETTING \"MyValue\"\nazd env set ANOTHER_SETTING \"AnotherValue\"\n\n# Deploy with updated settings\nazd deploy --no-prompt\n\n# Upload local.settings.json to Azure\nfunc azure functionapp publish $FUNCTION_APP --publish-local-settings\n```\n\n### Bicep Configuration\n\nDefine app settings in your Bicep template:\n\n```bicep\nresource functionApp 'Microsoft.Web/sites@2023-12-01' = {\n  name: functionAppName\n  location: location\n  kind: 'functionapp,linux'\n  properties: {\n    siteConfig: {\n      appSettings: [\n        { name: 'FUNCTIONS_WORKER_RUNTIME', value: 'node' }\n        { name: 'MY_SETTING', value: mySetting }\n        { name: 'APPLICATIONINSIGHTS_CONNECTION_STRING', value: appInsights.properties.ConnectionString }\n      ]\n    }\n  }\n}\n```\n\n### local.settings.json\n\n```json\n{\n  \"IsEncrypted\": false,\n  \"Values\": {\n    \"FUNCTIONS_WORKER_RUNTIME\": \"node\",\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n    \"MY_API_KEY\": \"local-dev-key\",\n    \"DATABASE_CONNECTION_STRING\": \"Server=localhost;...\"\n  },\n  \"Host\": {\n    \"LocalHttpPort\": 7071,\n    \"CORS\": \"*\",\n    \"CORSCredentials\": false\n  },\n  \"ConnectionStrings\": {\n    \"SQLConnectionString\": \"Server=localhost;...\"\n  }\n}\n```\n\n---\n\n## Pattern 6: Monitoring and Logs\n\nView function execution logs and diagnostics.\n\n### Using azd\n\n```bash\n# View logs from deployed functions\nazd monitor --logs\n\n# Open Azure Portal to view metrics\nazd monitor --overview\n```\n\n### Using func CLI\n\n```bash\n# Stream live logs\nfunc azure functionapp logstream $FUNCTION_APP\n\n# Stream logs in browser\nfunc azure functionapp logstream $FUNCTION_APP --browser\n```\n\n### Application Insights Integration\n\nApplication Insights is automatically configured when using azd. Define it in Bicep:\n\n```bicep\nresource appInsights 'Microsoft.Insights/components@2020-02-02' = {\n  name: '${functionAppName}-insights'\n  location: location\n  kind: 'web'\n  properties: {\n    Application_Type: 'web'\n  }\n}\n\n// Link to Function App via app settings in the functionApp resource\n// { name: 'APPLICATIONINSIGHTS_CONNECTION_STRING', value: appInsights.properties.ConnectionString }\n```\n\n**Query Application Insights:**\n\n```kusto\n// Recent function executions\nrequests\n| where timestamp > ago(1h)\n| where cloud_RoleName == \"my-function-app\"\n| project timestamp, name, duration, resultCode\n| order by timestamp desc\n\n// Failed executions\nrequests\n| where timestamp > ago(1h)\n| where success == false\n| project timestamp, name, resultCode, customDimensions\n| order by timestamp desc\n\n// Function performance\nrequests\n| where timestamp > ago(24h)\n| summarize \n    Count = count(),\n    AvgDuration = avg(duration),\n    P95Duration = percentile(duration, 95)\n    by name\n| order by Count desc\n```\n\n---\n\n## Pattern 7: Deployment Slots (Premium/Dedicated Plans)\n\nUse deployment slots for zero-downtime deployments. Configure slots in Bicep and deploy with azd.\n\n### Bicep Configuration\n\n```bicep\nresource stagingSlot 'Microsoft.Web/sites/slots@2023-12-01' = {\n  parent: functionApp\n  name: 'staging'\n  location: location\n  kind: 'functionapp,linux'\n  properties: {\n    serverFarmId: hostingPlan.id\n  }\n}\n```\n\n### Deploy to Slots\n\nDeployment slots are Azure resources, not azd environments. Use the Azure Functions Core Tools CLI to deploy to slots:\n\n```bash\n# Deploy to staging slot\nfunc azure functionapp publish <function-app-name> --slot staging\n\n# After testing, swap staging to production via Azure CLI\naz functionapp deployment slot swap \\\n  --name <function-app-name> \\\n  --resource-group <resource-group> \\\n  --slot staging \\\n  --target-slot production\n```\n\n> **Note:** `azd deploy --environment` manages separate azd environments (different resource sets), not deployment slots within the same Function App.\n\n---\n\n## Pattern 8: CI/CD with GitHub Actions\n\nAutomate deployments with GitHub Actions.\n\n**.github/workflows/azure-functions.yml:**\n```yaml\nname: Deploy Azure Functions\n\non:\n  push:\n    branches: [main]\n  workflow_dispatch:\n\nenv:\n  AZURE_FUNCTIONAPP_NAME: 'myFunctionApp'\n  AZURE_FUNCTIONAPP_PACKAGE_PATH: '.'\n  NODE_VERSION: '20.x'\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      \n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n      \n      - name: Install dependencies\n        run: npm ci\n        working-directory: ${{ env.AZURE_FUNCTIONAPP_PACKAGE_PATH }}\n      \n      - name: Build TypeScript (if applicable)\n        run: npm run build --if-present\n        working-directory: ${{ env.AZURE_FUNCTIONAPP_PACKAGE_PATH }}\n      \n      - name: Azure Login\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n      \n      - name: Deploy to Azure Functions\n        uses: Azure/functions-action@v1\n        with:\n          app-name: ${{ env.AZURE_FUNCTIONAPP_NAME }}\n          package: ${{ env.AZURE_FUNCTIONAPP_PACKAGE_PATH }}\n```\n\n**Configure GitHub Actions with azd:**\n\nUse `azd pipeline config` to set up GitHub Actions with proper credentials:\n\n```bash\n# Configure CI/CD pipeline\nazd pipeline config\n```\n\nThis command automatically creates the necessary GitHub Actions workflow and the required secrets (including `AZURE_CREDENTIALS`) for you. No additional manual secret configuration is required when you use `azd pipeline config`.\n\n> **Note:** If you choose to configure GitHub Actions manually instead of using `azd pipeline config`, create a service principal with `az ad sp create-for-rbac` and add its JSON output as an `AZURE_CREDENTIALS` secret in your GitHub repository settings.\n\n---\n\n## Triggers and Bindings\n\n### Common Trigger Types\n\n| Trigger | Description | Use Case |\n|---------|-------------|----------|\n| **HTTP** | Responds to HTTP requests | REST APIs, webhooks |\n| **Timer** | Runs on schedule (cron) | Scheduled jobs, cleanup |\n| **Queue** | Triggered by queue messages | Async processing |\n| **Blob** | Triggered by blob uploads | File processing |\n| **Event Grid** | Triggered by Event Grid events | Event-driven architecture |\n| **Service Bus** | Triggered by Service Bus messages | Messaging patterns |\n| **Cosmos DB** | Triggered by Cosmos DB changes | Change feed processing |\n\n### Input/Output Bindings\n\n```javascript\n// HTTP trigger with Blob output binding\nconst { app } = require('@azure/functions');\n\napp.http('HttpToBlobBinding', {\n    methods: ['POST'],\n    authLevel: 'function',\n    extraOutputs: [\n        {\n            type: 'blob',\n            name: 'outputBlob',\n            path: 'output/{DateTime}.txt',\n            connection: 'AzureWebJobsStorage'\n        }\n    ],\n    handler: async (request, context) => {\n        const data = await request.text();\n        context.extraOutputs.set('outputBlob', data);\n        return { body: 'Data saved to blob' };\n    }\n});\n```\n\n---\n\n## Durable Functions\n\nFor long-running workflows and orchestrations.\n\n```bash\n# Install Durable Functions extension\nnpm install durable-functions\n```\n\n**Example orchestrator:**\n```javascript\nconst df = require('durable-functions');\n\ndf.app.orchestration('HelloCitiesOrchestrator', function* (context) {\n    const outputs = [];\n    outputs.push(yield context.df.callActivity('SayHello', 'Tokyo'));\n    outputs.push(yield context.df.callActivity('SayHello', 'Seattle'));\n    outputs.push(yield context.df.callActivity('SayHello', 'Cairo'));\n    return outputs;\n});\n\ndf.app.activity('SayHello', {\n    handler: (input) => {\n        return `Hello, ${input}!`;\n    }\n});\n\ndf.app.http('StartOrchestration', {\n    route: 'orchestrators/{orchestratorName}',\n    extraInputs: [df.input.durableClient()],\n    handler: async (request, context) => {\n        const client = df.getClient(context);\n        const instanceId = await client.startNew(\n            request.params.orchestratorName\n        );\n        return client.createCheckStatusResponse(request, instanceId);\n    }\n});\n```\n\n---\n\n## Best Practices\n\n| Practice | Description |\n|----------|-------------|\n| **Use managed identity** | Prefer managed identity over connection strings for secure resource access |\n| **Configure timeout** | Set `functionTimeout` in host.json (default 5 min for Consumption, 30 min for Premium) |\n| **Use Application Insights** | Enable for monitoring, tracing, and diagnostics |\n| **Secure HTTP functions** | Use `authLevel: 'function'` or `'admin'` for non-public endpoints |\n| **Environment variables** | Store secrets in App Settings or Key Vault, not in code |\n| **Cold start optimization** | Use Premium plan or keep-alive pings for latency-sensitive apps |\n| **Durable Functions** | Use for long-running orchestrations and stateful workflows |\n| **Resource cleanup** | Implement proper exception handling and cleanup in functions |\n| **Idempotency** | Design functions to handle duplicate messages gracefully |\n| **Batch processing** | Process multiple items per invocation for queue/Service Bus triggers |\n| **Connection pooling** | Reuse connections across function invocations |\n\n---\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| **func not found** | Command not recognized | Install Azure Functions Core Tools: `npm install -g azure-functions-core-tools@4 --unsafe-perm true` |\n| **Storage error** | Function app won't start | Verify `AzureWebJobsStorage` connection string is valid. Check storage account exists |\n| **404 on function** | Function not found after deployment | Check function is exported correctly and route is configured. Verify deployment succeeded |\n| **Cold start delays** | First request slow (5-10 seconds) | Use Premium plan with pre-warmed instances, or implement keep-alive ping |\n| **Timeout** | Function exceeds time limit | Increase `functionTimeout` in host.json (max 10 min for Consumption). Consider Durable Functions for longer workflows |\n| **Binding errors** | Extension not loaded | Run `func extensions install` to install required binding extensions |\n| **Deploy fails** | Publish error | Ensure function app exists, CLI is authenticated, and storage account is accessible |\n| **Runtime mismatch** | Version conflict | Verify `FUNCTIONS_EXTENSION_VERSION` (set to ~4) matches project functions version |\n| **Missing dependencies** | Module not found | Ensure all dependencies are in package.json. Use `--build remote` for native dependencies |\n| **High memory usage** | App crashes or restarts | Optimize function code, consider Premium plan with more memory |\n\n### Debug Commands\n\n```bash\n# Local debugging with verbose output\nfunc start --verbose\n\n# Stream live logs from Azure\nfunc azure functionapp logstream $FUNCTION_APP\n\n# View logs with azd\nazd monitor --logs\n\n# Open Azure Portal for function app\nazd monitor --overview\n```\n\n---\n\n## Azure Resources Reference\n\n### Core Resources for Azure Functions\n\n| Resource Type | Purpose | API Version |\n|--------------|---------|-------------|\n| `Microsoft.Web/sites` | Function App | 2023-12-01 |\n| `Microsoft.Storage/storageAccounts` | Required storage for function metadata | 2023-01-01 |\n| `Microsoft.Web/serverfarms` | App Service Plan (for Premium/Dedicated) | 2023-12-01 |\n| `Microsoft.Insights/components` | Application Insights for monitoring | 2020-02-02 |\n\n### Example Bicep Template\n\n```bicep\nparam location string = resourceGroup().location\nparam functionAppName string\nparam storageAccountName string\n\nresource storageAccount 'Microsoft.Storage/storageAccounts@2023-01-01' = {\n  name: storageAccountName\n  location: location\n  sku: {\n    name: 'Standard_LRS'\n  }\n  kind: 'StorageV2'\n}\n\nresource appInsights 'Microsoft.Insights/components@2020-02-02' = {\n  name: '${functionAppName}-insights'\n  location: location\n  kind: 'web'\n  properties: {\n    Application_Type: 'web'\n  }\n}\n\nresource functionApp 'Microsoft.Web/sites@2023-12-01' = {\n  name: functionAppName\n  location: location\n  kind: 'functionapp,linux'\n  properties: {\n    serverFarmId: null  // Consumption plan\n    siteConfig: {\n      appSettings: [\n        {\n          name: 'AzureWebJobsStorage'\n          value: 'DefaultEndpointsProtocol=https;AccountName=${storageAccount.name};EndpointSuffix=${environment().suffixes.storage};AccountKey=${storageAccount.listKeys().keys[0].value}'\n        }\n        {\n          name: 'FUNCTIONS_EXTENSION_VERSION'\n          value: '~4'\n        }\n        {\n          name: 'FUNCTIONS_WORKER_RUNTIME'\n          value: 'node'\n        }\n        {\n          name: 'WEBSITE_NODE_DEFAULT_VERSION'\n          value: '~20'\n        }\n        {\n          name: 'APPLICATIONINSIGHTS_CONNECTION_STRING'\n          value: appInsights.properties.ConnectionString\n        }\n      ]\n      linuxFxVersion: 'NODE|20'\n    }\n  }\n}\n```\n\n---\n\n## Additional Resources\n\n- [Azure Functions Documentation](https://learn.microsoft.com/azure/azure-functions/)\n- [Azure Functions Core Tools](https://learn.microsoft.com/azure/azure-functions/functions-run-local)\n- [Triggers and Bindings](https://learn.microsoft.com/azure/azure-functions/functions-triggers-bindings)\n- [Durable Functions](https://learn.microsoft.com/azure/azure-functions/durable/)\n- [Best Practices](https://learn.microsoft.com/azure/azure-functions/functions-best-practices)\n- [Performance and Reliability](https://learn.microsoft.com/azure/azure-functions/performance-reliability)\n",
        "plugin/skills/azure-create-app/reference/service-selection.md": "# Service Selection Rules\r\n\r\nUse these rules when mapping application components to Azure services.\r\n\r\n## Static Web Apps\r\n\r\n**Use when:**\r\n- Application is a static frontend (React, Vue, Angular, Svelte)\r\n- Application is a Jamstack site (Gatsby, Hugo, Astro)\r\n- Application needs global CDN distribution\r\n- Application has optional serverless API\r\n\r\n**Requirements:**\r\n- Built files must be in a `dist` folder\r\n- Reference this in `azure.yaml` using the `dist` property\r\n- For plain HTML sites, create a `dist/` folder and copy deployable files there\r\n\r\n## Container Apps\r\n\r\n**Use when:**\r\n- Application is a microservice or API\r\n- Application is a full-stack web application\r\n- Application needs background workers or queue processors\r\n- Application needs scheduled jobs (use Container Apps Jobs)\r\n- Application is already containerized with Docker\r\n\r\n## Azure Functions\r\n\r\n**Use when:**\r\n- Application is event-driven serverless\r\n- Application needs HTTP APIs with per-request billing\r\n- Application needs timer-triggered jobs\r\n- Application uses queue/blob/event triggers\r\n\r\n## App Service\r\n\r\n**Use when:**\r\n- Application is a traditional web application\r\n- Container Apps features are not needed\r\n- Migrating existing App Service application\r\n\r\n## Azure Kubernetes Service (AKS)\r\n\r\n**Use when:**\r\n- Application has complex Kubernetes requirements\r\n- Application needs custom operators or CRDs\r\n- Team has existing Kubernetes expertise\r\n",
        "plugin/skills/azure-create-app/reference/static-web-apps.md": "# Static Web Apps Deployment Guide\n\nDeploy static frontends (React, Vue, Angular, etc.) with optional serverless Azure Functions APIs.\n\n---\n\n## Overview\n\nAzure Static Web Apps (SWA) provides:\n- Global CDN distribution for fast content delivery\n- Free SSL certificates with automatic renewal\n- Integrated serverless APIs (managed Azure Functions)\n- GitHub/Azure DevOps CI/CD integration\n- Automatic preview environments for pull requests\n- Built-in authentication with GitHub, Microsoft Entra ID, Twitter\n\n**Best for:** React, Vue, Angular, Svelte, Next.js (SSG), Gatsby, Hugo, Astro, plain HTML/CSS/JS sites\n\n---\n\n## Prerequisites\n\n### Required Tools\n- **Azure Developer CLI (azd)** - For resource provisioning and deployment\n- **SWA CLI** - Static Web Apps command-line tool (optional, for local development)\n- **Node.js** - For SWA CLI and managed Functions APIs\n\n### Installation\n\n**Azure Developer CLI (azd):**\n```bash\n# macOS\nbrew tap azure/azure-dev && brew install azd\n\n# Windows\nwinget install Microsoft.Azd\n\n# Linux\ncurl -fsSL https://aka.ms/install-azd.sh | bash\n```\n\n**SWA CLI - Option 1: Install locally in project (recommended)**\n```bash\nnpm install -D @azure/static-web-apps-cli\n```\n\nVerify: `npx swa --version`\n\n**SWA CLI - Option 2: Install globally**\n```bash\nnpm install -g @azure/static-web-apps-cli\n```\n\nVerify: `swa --version`\n\n**SWA CLI - Option 3: Use npx without installation (no setup required)**\n```bash\nnpx @azure/static-web-apps-cli --version\n```\n\n> ðŸ’¡ **Best Practice**: Use `npx swa` commands instead of `swa` directly to avoid \"command not found\" errors. This works whether SWA CLI is installed locally, globally, or not at all.\n\n---\n\n## Quick Start with azd\n\n> **Always use `azd` for Static Web Apps deployments.** The `azd` tool provides a complete, reproducible workflow.\n\n```bash\n# Deploy everything in one command\nazd up --no-prompt\n\n# Or step-by-step:\nazd init                    # Create azure.yaml and infra/\nazd provision --no-prompt   # Create Static Web App resource\nazd deploy --no-prompt      # Deploy built content\n\n# Preview changes before deployment\nazd provision --preview\n\n# Clean up test environments\nazd down --force --purge\n```\n\n> âš ï¸ **CRITICAL: SWA CLI Directory Rule**\n> \n> **Always run `swa deploy` from a PARENT directory, pointing to the output folder.** The SWA CLI will fail silently with \"Current directory cannot be identical to or contained within artifact folders\" if you run it from inside the deployment directory.\n> \n> **âœ… Correct:** `cd C:\\projects && swa deploy .\\myapp\\dist`\n> \n> **âŒ Wrong:** `cd C:\\projects\\myapp\\dist && swa deploy .`\n> \n> For plain HTML sites without a build step, copy files to a `dist/` folder first, then deploy from the parent directory.\n\n---\n\n## SKU Options\n\n| SKU | Price | Features |\n|-----|-------|----------|\n| **Free** | $0/month | 2 custom domains, 100GB bandwidth/month, community support |\n| **Standard** | ~$9/month | 5 custom domains, unlimited bandwidth, password protection, custom auth providers, SLA |\n\nTo use Standard SKU with azd, set the SKU in your Bicep template (`infra/main.bicep`):\n\n```bicep\nresource staticWebApp 'Microsoft.Web/staticSites@2022-09-01' = {\n  name: name\n  location: location\n  sku: {\n    name: 'Standard'\n    tier: 'Standard'\n  }\n  // ...\n}\n```\n\n---\n\n## Configuration Files\n\n### swa-cli.config.json (SWA CLI Settings)\n\n**IMPORTANT: Do not use `swa init`. Create this file manually for better control.**\n\nPlace in project root:\n\n```json\n{\n  \"$schema\": \"https://aka.ms/azure/static-web-apps-cli/schema\",\n  \"configurations\": {\n    \"app\": {\n      \"appLocation\": \".\",\n      \"apiLocation\": \"api\",\n      \"outputLocation\": \"dist\",\n      \"appBuildCommand\": \"npm run build\",\n      \"apiBuildCommand\": \"npm run build --if-present\",\n      \"run\": \"npm run dev\",\n      \"appDevserverUrl\": \"http://localhost:5173\",\n      \"appName\": \"myapp\",\n      \"resourceGroup\": \"myapp-rg\"\n    }\n  }\n}\n```\n\n**Key properties:**\n- `appLocation` - Directory containing frontend source code (relative to project root)\n- `apiLocation` - Directory containing Azure Functions API code\n- `outputLocation` - Build output directory (relative to appLocation)\n- `appBuildCommand` - Command to build frontend\n- `apiBuildCommand` - Command to build API\n- `run` - Command to start dev server\n- `appDevserverUrl` - Dev server URL for SWA CLI to proxy\n- `appName` - Azure Static Web App resource name (for deployment)\n- `resourceGroup` - Azure resource group name (for deployment)\n\nFor complete schema reference: https://aka.ms/azure/static-web-apps-cli/schema\n\n### staticwebapp.config.json (Runtime Configuration)\n\nPlace in your build output directory or repository root:\n\n```json\n{\n  \"navigationFallback\": {\n    \"rewrite\": \"/index.html\",\n    \"exclude\": [\"/images/*\", \"/api/*\", \"*.{css,js,png,jpg,svg,ico}\"]\n  },\n  \"routes\": [\n    {\n      \"route\": \"/api/*\",\n      \"allowedRoles\": [\"authenticated\"]\n    },\n    {\n      \"route\": \"/admin/*\",\n      \"allowedRoles\": [\"admin\"]\n    }\n  ],\n  \"responseOverrides\": {\n    \"401\": {\n      \"statusCode\": 302,\n      \"redirect\": \"/.auth/login/aad\"\n    },\n    \"404\": {\n      \"rewrite\": \"/404.html\"\n    }\n  },\n  \"globalHeaders\": {\n    \"X-Frame-Options\": \"DENY\",\n    \"X-Content-Type-Options\": \"nosniff\",\n    \"X-XSS-Protection\": \"1; mode=block\"\n  },\n  \"mimeTypes\": {\n    \".json\": \"application/json\",\n    \".wasm\": \"application/wasm\"\n  },\n  \"platform\": {\n    \"apiRuntime\": \"node:22\"\n  }\n}\n```\n\n**Key sections:**\n- `navigationFallback` - SPA routing support (redirect to index.html for client-side routes)\n- `routes` - Role-based access control for routes\n- `responseOverrides` - Custom error pages and redirects\n- `globalHeaders` - Security and CORS headers\n- `mimeTypes` - Custom MIME type mappings\n- `platform.apiRuntime` - Azure Functions runtime version\n\n---\n\n## SWA CLI Commands\n\n### swa start (Local Development)\n\nStart the local emulator at `http://localhost:4280`:\n\n```bash\n# Serve from build output (uses swa-cli.config.json)\nnpx swa start\n\n# Serve specific folder\nnpx swa start ./dist\n\n# Proxy to development server (recommended for frameworks)\nnpx swa start http://localhost:3000\n\n# With API folder\nnpx swa start ./dist --api-location ./api\n\n# Proxy to dev server with API\nnpx swa start http://localhost:3000 --api-location ./api\n\n# Auto-start dev server and proxy\nnpx swa start http://localhost:3000 --run \"npm start\" --api-location ./api\n\n# Custom emulator port\nnpx swa start --port 4280\n\n# Enable HTTPS\nnpx swa start --ssl\n```\n\n**Common framework development server ports:**\n| Framework | Default Port | Start Command |\n|-----------|--------------|---------------|\n| React (Vite) | 5173 | `npm run dev` |\n| React (CRA) | 3000 | `npm start` |\n| Vue (Vite) | 5173 | `npm run dev` |\n| Angular | 4200 | `ng serve` |\n| Next.js | 3000 | `npm run dev` |\n| Svelte | 5173 | `npm run dev` |\n\n**Key flags:**\n- `--port, -p` - SWA emulator port (default: 4280)\n- `--api-location, -i` - API folder path\n- `--api-port, -j` - Functions API port (default: 7071)\n- `--run, -r` - Command to auto-start dev server\n- `--open, -o` - Open browser automatically\n- `--ssl, -s` - Enable HTTPS with self-signed certificate\n- `--verbose` - Enable verbose logging\n\n### swa build\n\nBuild frontend and API:\n\n```bash\n# Build using config\nnpx swa build\n\n# Auto-detect and build\nnpx swa build --auto\n\n# Build specific configuration\nnpx swa build myApp\n```\n\n**Key flags:**\n- `--app-location, -a` - Frontend source path\n- `--api-location, -i` - API source path\n- `--output-location, -O` - Build output path\n- `--app-build-command, -A` - Frontend build command\n- `--api-build-command, -I` - API build command\n\n### swa deploy\n\nDeploy to Azure Static Web App:\n\n```bash\n# Deploy using config (uses appName and resourceGroup from swa-cli.config.json)\nnpx swa deploy\n\n# Deploy specific folder\nnpx swa deploy ./dist\n\n# Deploy to production environment\nnpx swa deploy --env production\n\n# Deploy with deployment token\nnpx swa deploy --deployment-token <TOKEN>\n\n# Preview without deploying\nnpx swa deploy --dry-run\n\n# Deploy with verbose output (RECOMMENDED for debugging)\nnpx swa deploy --verbose silly\n```\n\n> ðŸ’¡ **Always use `--verbose silly` when troubleshooting deployments** to see all error details and diagnostic information.\n\n**Get deployment token:**\n1. **Azure Portal**: Static Web App â†’ Overview â†’ Manage deployment token\n2. **CLI**: `npx swa deploy --print-token`\n3. **Environment variable**: Set `SWA_CLI_DEPLOYMENT_TOKEN`\n\n**Key flags:**\n- `--env` - Target environment (`preview` or `production`)\n- `--deployment-token, -d` - Deployment token\n- `--app-name, -n` - Azure SWA resource name\n- `--resource-group, -g` - Azure resource group\n- `--verbose` - Logging level (`log`, `silly`)\n- `--dry-run` - Preview deployment without uploading\n\n### swa login\n\nAuthenticate with Azure:\n\n```bash\n# Interactive login\nnpx swa login\n\n# Specific subscription\nnpx swa login --subscription-id <id>\n\n# Clear cached credentials\nnpx swa login --clear-credentials\n```\n\n### swa db\n\nInitialize database connections:\n\n```bash\nnpx swa db init --database-type mssql\nnpx swa db init --database-type postgresql\nnpx swa db init --database-type cosmosdb_nosql\n```\n\n---\n\n## Framework-Specific Setup\n\n### Plain HTML (No Build Step)\n\nFor plain HTML sites without a build process, SWA CLI requires files in a dedicated output folder:\n\n```bash\n# Create output directory and copy files\nmkdir -p dist\ncp -r *.html *.css *.js *.png *.jpg *.svg dist/ 2>/dev/null || true\n\n# Get deployment token\nTOKEN=$(az staticwebapp secrets list \\\n  --name <app-name> \\\n  --resource-group <resource-group> \\\n  --query \"properties.apiKey\" -o tsv)\n\n# Deploy from dist folder\nnpx swa deploy ./dist --deployment-token \"$TOKEN\" --env production\n\n# Clean up temp folder (optional)\nrm -rf dist\n```\n\n**Note:** SWA CLI does not support deploying directly from the root directory for plain HTML sites. Always use an output folder.\n\n### React (Vite)\n\n```bash\n# Build\nnpm run build\n\n# Deploy (Vite outputs to dist/)\nnpx swa deploy ./dist --deployment-token \"$TOKEN\" --env production\n```\n\n**swa-cli.config.json example:**\n```json\n{\n  \"configurations\": {\n    \"app\": {\n      \"appLocation\": \".\",\n      \"outputLocation\": \"dist\",\n      \"appBuildCommand\": \"npm run build\",\n      \"appDevserverUrl\": \"http://localhost:5173\"\n    }\n  }\n}\n```\n\n### React (Create React App)\n\n```bash\n# Build\nnpm run build\n\n# Deploy (CRA outputs to build/)\nnpx swa deploy ./build --deployment-token \"$TOKEN\" --env production\n```\n\n### Vue (Vite)\n\n```bash\n# Build\nnpm run build\n\n# Deploy\nnpx swa deploy ./dist --deployment-token \"$TOKEN\" --env production\n```\n\n### Angular\n\n```bash\n# Build\nnpm run build\n\n# Deploy (output is in dist/<project-name>/browser or dist/<project-name>)\nnpx swa deploy ./dist/<project-name>/browser --deployment-token \"$TOKEN\" --env production\n```\n\n**swa-cli.config.json example:**\n```json\n{\n  \"configurations\": {\n    \"app\": {\n      \"appLocation\": \".\",\n      \"outputLocation\": \"dist/my-angular-app/browser\",\n      \"appBuildCommand\": \"npm run build\",\n      \"appDevserverUrl\": \"http://localhost:4200\"\n    }\n  }\n}\n```\n\n### Next.js (Static Export)\n\nAdd to `next.config.js`:\n```javascript\nmodule.exports = {\n  output: 'export',\n  trailingSlash: true,\n}\n```\n\n```bash\n# Build\nnpm run build\n\n# Deploy (Next.js exports to out/)\nnpx swa deploy ./out --deployment-token \"$TOKEN\" --env production\n```\n\n**Important:** Only static export is supported. Server-side rendering (SSR) is not compatible with Static Web Apps.\n\n### Svelte (Vite)\n\n```bash\n# Build\nnpm run build\n\n# Deploy\nnpx swa deploy ./dist --deployment-token \"$TOKEN\" --env production\n```\n\n### Astro\n\n```bash\n# Build\nnpm run build\n\n# Deploy\nnpx swa deploy ./dist --deployment-token \"$TOKEN\" --env production\n```\n\n### Gatsby\n\n```bash\n# Build\nnpm run build\n\n# Deploy (Gatsby outputs to public/)\nnpx swa deploy ./public --deployment-token \"$TOKEN\" --env production\n```\n\n---\n\n## API Integration (Azure Functions)\n\n### Managed Functions (Built-in)\n\nStatic Web Apps includes managed Azure Functions support. Create an `api/` folder in your project:\n\n**Project structure:**\n```\nproject/\nâ”œâ”€â”€ src/                    # Frontend source\nâ”œâ”€â”€ dist/                   # Build output\nâ”œâ”€â”€ api/                    # Managed Functions\nâ”‚   â”œâ”€â”€ host.json\nâ”‚   â”œâ”€â”€ package.json\nâ”‚   â””â”€â”€ message/\nâ”‚       â”œâ”€â”€ function.json\nâ”‚       â””â”€â”€ index.js\nâ””â”€â”€ staticwebapp.config.json\n```\n\n#### Create API (Node.js v4 Programming Model)\n\n```bash\n# Initialize Functions project\nmkdir api && cd api\nnpm init -y\nnpm install @azure/functions\n\n# Create function using v4 model\nfunc init --worker-runtime node --model V4\nfunc new --name message --template \"HTTP trigger\"\n```\n\n**api/src/functions/message.js:**\n```javascript\nconst { app } = require('@azure/functions');\n\napp.http('message', {\n    methods: ['GET', 'POST'],\n    authLevel: 'anonymous',\n    handler: async (request) => {\n        const name = request.query.get('name') || 'World';\n        return { \n            jsonBody: { \n                message: `Hello, ${name}!`,\n                timestamp: new Date().toISOString()\n            } \n        };\n    }\n});\n```\n\n**api/package.json:**\n```json\n{\n  \"name\": \"api\",\n  \"version\": \"1.0.0\",\n  \"main\": \"src/functions/*.js\",\n  \"dependencies\": {\n    \"@azure/functions\": \"^4.0.0\"\n  }\n}\n```\n\n#### Set API Runtime\n\nIn `staticwebapp.config.json`:\n```json\n{\n  \"platform\": {\n    \"apiRuntime\": \"node:22\"\n  }\n}\n```\n\n**Supported runtimes:**\n- Node.js: `node:18`, `node:20`, `node:22`\n- .NET: `dotnet:8.0`, `dotnet-isolated:8.0`\n- Python: `python:3.10`, `python:3.11`\n\n#### Deploy with API\n\n```bash\n# Local testing\nnpx swa start ./dist --api-location ./api\n\n# Deploy to Azure\nnpx swa deploy ./dist --api-location ./api --deployment-token \"$TOKEN\" --env production\n```\n\nAccess API at: `https://<app-name>.azurestaticapps.net/api/message`\n\n### Linked Backend (Bring Your Own Functions)\n\nLink an existing Azure Functions app, App Service, or Container App:\n\n```bash\naz staticwebapp backends link \\\n  --name <swa-name> \\\n  --resource-group <rg> \\\n  --backend-resource-id \"/subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.Web/sites/<function-app-name>\" \\\n  --backend-region <region>\n```\n\n---\n\n## Authentication\n\n### Built-in Providers\n\nStatic Web Apps provides built-in authentication with:\n- **GitHub**\n- **Microsoft Entra ID (Azure AD)**\n- **Twitter**\n\n#### Login Routes\n\n```json\n{\n  \"routes\": [\n    { \"route\": \"/login\", \"redirect\": \"/.auth/login/github\" },\n    { \"route\": \"/login/aad\", \"redirect\": \"/.auth/login/aad\" },\n    { \"route\": \"/logout\", \"redirect\": \"/.auth/logout\" }\n  ]\n}\n```\n\nAvailable login endpoints:\n- `/.auth/login/github`\n- `/.auth/login/aad` (Microsoft Entra ID)\n- `/.auth/login/twitter`\n\nLogout endpoint:\n- `/.auth/logout`\n\n#### Access User Information\n\n**In frontend JavaScript:**\n```javascript\nasync function getUserInfo() {\n  const response = await fetch('/.auth/me');\n  const { clientPrincipal } = await response.json();\n  \n  if (clientPrincipal) {\n    console.log('User ID:', clientPrincipal.userId);\n    console.log('Roles:', clientPrincipal.userRoles);\n    console.log('Provider:', clientPrincipal.identityProvider);\n    console.log('User details:', clientPrincipal.userDetails);\n  } else {\n    console.log('Not authenticated');\n  }\n}\n```\n\n**In API function (Node.js):**\n```javascript\nmodule.exports = async function (context, req) {\n  const header = req.headers['x-ms-client-principal'];\n  const user = header ? JSON.parse(Buffer.from(header, 'base64').toString()) : null;\n  \n  if (user) {\n    context.res = {\n      body: { \n        message: `Hello, ${user.userDetails}!`,\n        roles: user.userRoles \n      }\n    };\n  } else {\n    context.res = { status: 401, body: 'Not authenticated' };\n  }\n};\n```\n\n#### Role-Based Access Control\n\n```json\n{\n  \"routes\": [\n    {\n      \"route\": \"/api/*\",\n      \"allowedRoles\": [\"authenticated\"]\n    },\n    {\n      \"route\": \"/admin/*\",\n      \"allowedRoles\": [\"admin\"]\n    }\n  ],\n  \"responseOverrides\": {\n    \"401\": {\n      \"statusCode\": 302,\n      \"redirect\": \"/.auth/login/github\"\n    }\n  }\n}\n```\n\n**Built-in roles:**\n- `anonymous` - All users (default)\n- `authenticated` - Logged-in users\n\n**Custom roles:** Configure via invitation links or custom authentication\n\n### Custom Authentication (OpenID Connect)\n\nConfigure custom authentication providers in Azure Portal:\n- **Settings â†’ Authentication â†’ Add identity provider**\n- Supports any OpenID Connect-compatible provider\n\n---\n\n## Routing and Navigation\n\n### SPA Routing (Client-Side Routes)\n\nEnable client-side routing for single-page applications:\n\n```json\n{\n  \"navigationFallback\": {\n    \"rewrite\": \"/index.html\",\n    \"exclude\": [\"/images/*\", \"/api/*\", \"*.{css,js,png,jpg,svg,ico,json}\"]\n  }\n}\n```\n\nThis ensures all routes (except excluded patterns) serve `index.html`, allowing your SPA router to handle navigation.\n\n### Custom Routes\n\n```json\n{\n  \"routes\": [\n    {\n      \"route\": \"/profile\",\n      \"allowedRoles\": [\"authenticated\"]\n    },\n    {\n      \"route\": \"/images/*\",\n      \"headers\": {\n        \"cache-control\": \"public, max-age=31536000, immutable\"\n      }\n    },\n    {\n      \"route\": \"/api/*\"\n    },\n    {\n      \"route\": \"/old-page\",\n      \"redirect\": \"/new-page\",\n      \"statusCode\": 301\n    }\n  ]\n}\n```\n\n### Custom Error Pages\n\n```json\n{\n  \"responseOverrides\": {\n    \"400\": {\n      \"rewrite\": \"/errors/400.html\"\n    },\n    \"404\": {\n      \"rewrite\": \"/404.html\"\n    },\n    \"500\": {\n      \"rewrite\": \"/errors/500.html\"\n    }\n  }\n}\n```\n\n---\n\n## Custom Domains and SSL\n\n### Add Custom Domain\n\n```bash\n# Add custom domain\naz staticwebapp hostname set \\\n  --name <app> \\\n  --resource-group <rg> \\\n  --hostname www.example.com\n\n# List domains\naz staticwebapp hostname list \\\n  --name <app> \\\n  --resource-group <rg>\n\n# Delete domain\naz staticwebapp hostname delete \\\n  --name <app> \\\n  --resource-group <rg> \\\n  --hostname www.example.com\n```\n\n### DNS Configuration\n\n**For subdomain (www.example.com):**\n- Create CNAME record: `www` â†’ `<app-name>.azurestaticapps.net`\n\n**For apex domain (example.com):**\n- Use Azure DNS with ALIAS record, or\n- Use your DNS provider's ALIAS/ANAME record pointing to `<app-name>.azurestaticapps.net`\n\n**SSL certificates:**\n- Automatically provisioned and renewed\n- Free with Let's Encrypt\n- No configuration required\n\n---\n\n## Environment Variables and App Settings\n\n```bash\n# Set environment variable for production\naz staticwebapp appsettings set \\\n  --name <app> \\\n  --resource-group <rg> \\\n  --setting-names \\\n    API_URL=https://api.example.com \\\n    FEATURE_FLAG=true\n\n# List all settings\naz staticwebapp appsettings list \\\n  --name <app> \\\n  --resource-group <rg>\n\n# Delete setting\naz staticwebapp appsettings delete \\\n  --name <app> \\\n  --resource-group <rg> \\\n  --setting-names API_URL\n```\n\n**Access in API functions:**\n```javascript\nconst apiUrl = process.env.API_URL;\n```\n\n**Access in frontend:**\nEnvironment variables are NOT exposed to the frontend. Use your build tool's environment variable support instead (e.g., `VITE_`, `REACT_APP_`, `NEXT_PUBLIC_`).\n\n---\n\n## Preview Environments\n\nAutomatically created for pull requests when using GitHub Actions.\n\n```bash\n# List all environments\naz staticwebapp environment list \\\n  --name <app> \\\n  --resource-group <rg>\n\n# Delete preview environment\naz staticwebapp environment delete \\\n  --name <app> \\\n  --resource-group <rg> \\\n  --environment-name <env-name>\n```\n\n**Preview environment URLs:**\n- Format: `https://<unique-url>.azurestaticapps.net`\n- Created automatically when PR is opened\n- Deleted automatically when PR is closed\n\n---\n\n## GitHub Actions CI/CD\n\n### Auto-Generated Workflow\n\nWhen linking a GitHub repository, Static Web Apps automatically creates a workflow file:\n\n**.github/workflows/azure-static-web-apps-<app-name>.yml:**\n```yaml\nname: Azure Static Web Apps CI/CD\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    types: [opened, synchronize, reopened, closed]\n    branches:\n      - main\n\njobs:\n  build_and_deploy_job:\n    if: github.event_name == 'push' || (github.event_name == 'pull_request' && github.event.action != 'closed')\n    runs-on: ubuntu-latest\n    name: Build and Deploy Job\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          submodules: true\n      \n      - name: Build And Deploy\n        id: builddeploy\n        uses: Azure/static-web-apps-deploy@v1\n        with:\n          azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN }}\n          repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments)\n          action: \"upload\"\n          app_location: \"/\" # App source code path\n          api_location: \"api\" # Api source code path - optional\n          output_location: \"dist\" # Built app content directory - optional\n\n  close_pull_request_job:\n    if: github.event_name == 'pull_request' && github.event.action == 'closed'\n    runs-on: ubuntu-latest\n    name: Close Pull Request Job\n    steps:\n      - name: Close Pull Request\n        id: closepullrequest\n        uses: Azure/static-web-apps-deploy@v1\n        with:\n          azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN }}\n          action: \"close\"\n```\n\n### Workflow Configuration\n\nKey settings:\n- `app_location` - Frontend source path (relative to repo root)\n- `api_location` - API source path (omit if no API)\n- `output_location` - Build output directory (relative to app_location)\n- `skip_app_build: true` - Skip build if already built\n- `app_build_command` - Custom build command\n- `api_build_command` - Custom API build command\n\n### Add Deployment Token Secret\n\n1. Get deployment token:\n   ```bash\n   az staticwebapp secrets list \\\n     --name <app> \\\n     --resource-group <rg> \\\n     --query \"properties.apiKey\" -o tsv\n   ```\n\n2. Add to GitHub repository secrets:\n   - Go to repository **Settings â†’ Secrets and variables â†’ Actions**\n   - Click **New repository secret**\n   - Name: `AZURE_STATIC_WEB_APPS_API_TOKEN`\n   - Value: Paste the deployment token\n\n---\n\n## Monitoring and Diagnostics\n\n### View Logs\n\n```bash\n# Stream logs (requires Azure CLI with SWA extension)\naz staticwebapp log tail \\\n  --name <app> \\\n  --resource-group <rg>\n```\n\n### Application Insights Integration\n\nStatic Web Apps automatically integrates with Application Insights for:\n- Request telemetry\n- Dependency tracking\n- Exception logging\n- Custom metrics\n\n**Enable in Azure Portal:**\n- Static Web App â†’ Settings â†’ Application Insights â†’ Enable\n\n**Access logs:**\n- Azure Portal â†’ Application Insights â†’ Logs\n\n---\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| `swa` command not found | Use `npx swa` instead, or install globally: `npm install -g @azure/static-web-apps-cli` |\n| 404 on client routes | Add `navigationFallback` with `rewrite: \"/index.html\"` to `staticwebapp.config.json` |\n| API returns 404 | Verify `api/` folder structure, ensure `platform.apiRuntime` is set in config, check function exports |\n| Build output not found | Verify `output_location` matches actual build output directory (e.g., `dist/`, `build/`, `out/`) |\n| Auth not working locally | Use `/.auth/login/<provider>` to access auth emulator UI in local dev |\n| CORS errors | APIs under `/api/*` are same-origin by default; external APIs need CORS headers |\n| Deployment token expired | Regenerate in Azure Portal â†’ Static Web App â†’ Manage deployment token |\n| Config not applied | Ensure `staticwebapp.config.json` is in `app_location` or `output_location` |\n| Local API timeout | Default timeout is 45 seconds; optimize function code or check for blocking operations |\n| Preview environment not created | Verify GitHub Actions workflow has PR trigger and correct permissions |\n\n### Debug Commands\n\n```bash\n# Verbose local emulator output\nnpx swa start --verbose log\n\n# Preview deployment without uploading\nnpx swa deploy --dry-run\n\n# Show all deployment issues and diagnostics\nnpx swa deploy --verbose silly\n\n# Show resolved configuration\nnpx swa --print-config\n```\n\n### Validate Configuration\n\n```bash\n# Test local emulator\nnpx swa start ./dist --api-location ./api\n\n# Access emulator\nopen http://localhost:4280\n\n# Test API endpoint\ncurl http://localhost:4280/api/message\n\n# Test authentication\nopen http://localhost:4280/.auth/login/github\n```\n\n---\n\n## Cleanup\n\n```bash\n# Delete Static Web App\naz staticwebapp delete \\\n  --name <app> \\\n  --resource-group <rg> \\\n  --yes\n\n# Delete resource group (deletes all resources)\naz group delete --name <rg> --yes\n```\n\n---\n\n## Best Practices\n\n1. **Always use `npx swa` instead of `swa`** to avoid installation issues\n2. **Create `swa-cli.config.json` manually** instead of using `swa init` for better control\n3. **Use `--verbose silly` when deploying** to catch deployment issues early\n4. **Test locally first** with `npx swa start` before deploying to Azure\n5. **Use `navigationFallback`** for SPAs to support client-side routing\n6. **Set `platform.apiRuntime`** explicitly in `staticwebapp.config.json`\n7. **Exclude static assets** from navigationFallback to serve them directly\n8. **Use role-based access control** to secure API routes and admin pages\n9. **Enable Application Insights** for production apps to monitor performance\n10. **Use preview environments** for testing changes before merging to main\n\n---\n\n## Additional Resources\n\n- [Static Web Apps Documentation](https://learn.microsoft.com/azure/static-web-apps/)\n- [SWA CLI GitHub](https://github.com/Azure/static-web-apps-cli)\n- [Configuration Reference](https://learn.microsoft.com/azure/static-web-apps/configuration)\n- [Authentication and Authorization](https://learn.microsoft.com/azure/static-web-apps/authentication-authorization)\n- [API Routes with Functions](https://learn.microsoft.com/azure/static-web-apps/apis)\n",
        "plugin/skills/azure-deploy/SKILL.md": "---\nname: azure-deploy\ndescription: Deploy applications to Azure using Azure Developer CLI (azd). USE THIS SKILL when users want to deploy, publish, host, or run their application on Azure. Trigger phrases include \"deploy to Azure\", \"host on Azure\", \"publish to Azure\", \"run on Azure\", \"deploy my app\", \"azd up\", etc.\n---\n\n# Azure Deployment Skill\n\nDeploy applications to Azure using Azure Developer CLI (azd).\n\n---\n\n## Execution Flow\n\n### Step 1: Check for azure.yaml\n\nCheck if `azure.yaml` exists in the project root.\n\n**If `azure.yaml` does NOT exist:**\n- Inform user: \"No azure.yaml found. Use the azure-create-app skill to prepare your application for Azure deployment.\"\n- Stop execution\n\n**If `azure.yaml` exists:**\n- Proceed to Step 2\n\n### Step 2: Check Environment\n\nRun:\n```bash\nazd env list\n```\n\n**If no environment exists:**\n- Ask the user: \"What name would you like for your Azure environment? (e.g., dev, staging, prod)\"\n- Create the environment with the user-provided name:\n```bash\nazd env new <user-provided-name>\n```\n\n**If environment exists:**\n- Proceed to Step 3\n\n### Step 3: Check Subscription Configuration\n\nFirst, check for global defaults:\n```bash\nazd config get defaults\n```\n\nThis may return defaults like:\n```json\n{\n  \"subscription\": \"<subscription-id>\",\n  \"location\": \"<location>\"\n}\n```\n\nStore these default values if present.\n\nNext, check environment-specific values:\n```bash\nazd env get-values\n```\n\nCheck if `AZURE_SUBSCRIPTION_ID` is set in the output.\n\n**If `AZURE_SUBSCRIPTION_ID` is NOT set:**\n\n1. Call the `azure__subscription_list` MCP tool to get available subscriptions:\n```json\n{\n  \"command\": \"subscription_list\",\n  \"parameters\": {}\n}\n```\n\n2. Present the list of subscriptions to the user. If a default subscription was found in `azd config get defaults`, include it in the prompt:\n   - With default: \"Which Azure subscription would you like to use? (default from azd config: `<default-subscription-id>`)\"\n   - Without default: \"Which Azure subscription would you like to use for this deployment?\"\n\n3. Set the subscription with the user-selected value (or use default if user accepts):\n```bash\nazd env set AZURE_SUBSCRIPTION_ID <selected-subscription-id>\n```\n\n**If `AZURE_SUBSCRIPTION_ID` is set:**\n- Proceed to Step 4\n\n### Step 4: Check Location Configuration\n\nCheck if `AZURE_LOCATION` is set in the `azd env get-values` output from Step 3.\n\n**If `AZURE_LOCATION` is NOT set:**\n\n1. Get the list of available Azure regions:\n```bash\naz account list-locations --query \"[].{name:name, displayName:displayName}\" --output table\n```\n\n2. Present the list of available regions to the user. If a default location was found in `azd config get defaults`, include it in the prompt:\n   - With default: \"Which Azure region would you like to deploy to? (default from azd config: `<default-location>`)\"\n   - Without default: \"Which Azure region would you like to deploy to?\"\n\n3. Set the location with the user-selected value:\n```bash\nazd env set AZURE_LOCATION <selected-location>\n```\n\n**If `AZURE_LOCATION` is set:**\n- Proceed to Step 5\n\n### Step 5: Deploy\n\nExecute:\n```bash\nazd up --no-prompt\n```\n\nThe `--no-prompt` flag is required to prevent interactive prompts from blocking execution.\n\nThis command provisions all Azure resources defined in `infra/` and deploys the application code.\n\n**Alternative:** To provision and deploy separately:\n```bash\nazd provision --no-prompt   # Create Azure resources\nazd deploy --no-prompt      # Deploy application code\n```\n\n**To preview changes before deployment:**\n```bash\nazd provision --preview\n```\n\n### Step 6: Handle Errors\n\nIf `azd up` fails, call the `azure__azd` MCP tool:\n```json\n{\n  \"command\": \"error_troubleshooting\",\n  \"parameters\": {}\n}\n```\n\nCommon error resolutions:\n- \"Not authenticated\" â†’ Run `azd auth login`\n- \"Environment not found\" â†’ Run `azd env new <name>`\n- \"azure.yaml invalid\" â†’ Use azure-create-app skill to regenerate\n- \"Bicep compilation error\" â†’ Check module paths and parameters\n- \"Provision failed\" â†’ Check resource quotas and permissions\n- \"Package failed\" â†’ Verify Dockerfile and build configuration\n\n---\n\n## Environment Management\n\n```bash\nazd env new <name>              # Create environment\nazd env select <name>           # Switch environment\nazd env set AZURE_LOCATION eastus   # Set variable\nazd env list                    # List environments\n```\n\n---\n\n## Post-Deployment Commands\n\n```bash\nazd monitor --logs      # View logs\nazd monitor --overview  # Open Azure Portal\n```\n\n**Cleanup (DESTRUCTIVE):**\n```bash\nazd down --force --purge\n```\n\nWARNING: `azd down` permanently deletes ALL resources including databases with data, storage accounts with files, and Key Vaults with secrets.\n",
        "plugin/skills/azure-deployment-preflight/SKILL.md": "---\nname: azure-deployment-preflight\ndescription: Performs comprehensive preflight validation of Bicep deployments to Azure, including template syntax validation, what-if analysis, and permission checks. Use this skill before any deployment to Azure to preview changes, identify potential issues, and ensure the deployment will succeed. Activate when users mention deploying to Azure, validating Bicep files, checking deployment permissions, previewing infrastructure changes, running what-if, or preparing for azd provision.\n---\n\n# Azure Deployment Preflight Validation\n\nThis skill validates Bicep deployments before execution, supporting both Azure CLI (`az`) and Azure Developer CLI (`azd`) workflows.\n\n## When to Use This Skill\n\n- Before deploying infrastructure to Azure\n- When preparing or reviewing Bicep files\n- To preview what changes a deployment will make\n- To verify permissions are sufficient for deployment\n- Before running `azd up`, `azd provision`, or `az deployment` commands\n\n## Quick Pre-flight Checks\n\n**Run these checks BEFORE any deployment** to avoid mid-deployment failures:\n\n1. **Tools installed**: az, azd, docker, bicep\n2. **Authentication valid**: `az account show` and `azd auth login`\n3. **Subscription quotas sufficient**: Check resource limits\n4. **Docker daemon running** (for container deployments)\n\n```bash\n# Verify tools\naz --version\nazd version\nbicep --version\ndocker info\n\n# Verify authentication\naz account show\nazd auth login --check-status\n```\n\n## Validation Process\n\nFollow these steps in order. Continue to the next step even if a previous step failsâ€”capture all issues in the final report.\n\n### Step 1: Detect Project Type\n\nDetermine the deployment workflow by checking for project indicators:\n\n1. **Check for azd project**: Look for `azure.yaml` in the project root\n   - If found â†’ Use **azd workflow**\n   - If not found â†’ Use **az CLI workflow**\n\n2. **Locate Bicep files**: Find all `.bicep` files to validate\n   - For azd projects: Check `infra/` directory first, then project root\n   - For standalone: Use the file specified by the user or search common locations (`infra/`, `deploy/`, project root)\n\n3. **Auto-detect parameter files**: For each Bicep file, look for matching parameter files:\n   - `<filename>.bicepparam` (Bicep parameters - preferred)\n   - `<filename>.parameters.json` (JSON parameters)\n   - `parameters.json` or `parameters/<env>.json` in same directory\n\n### Step 1.5: Validate azure.yaml (azd projects only)\n\nFor azd projects, validate the `azure.yaml` configuration using the Azure MCP azd tool:\n\n```javascript\nconst validation = await azure__azd({\n  command: \"validate_azure_yaml\",\n  parameters: { path: \"./azure.yaml\" }\n});\n```\n\n**What to capture:**\n- Schema validation errors\n- Invalid service configurations\n- Missing required fields\n\nIf validation fails, include errors in the report and continue to Step 2.\n\n### Step 2: Validate Bicep Syntax\n\nRun Bicep CLI to check template syntax before attempting deployment validation:\n\n```bash\nbicep build <bicep-file> --stdout\n```\n\n**What to capture:**\n- Syntax errors with line/column numbers\n- Warning messages\n- Build success/failure status\n\n**If Bicep CLI is not installed:**\n- Note the issue in the report\n- Continue to Step 3 (Azure will validate syntax during what-if)\n\n### Step 3: Run Preflight Validation\n\nChoose the appropriate validation based on project type detected in Step 1.\n\n#### For azd Projects (azure.yaml exists)\n\nUse `azd provision --preview` to validate the deployment:\n\n```bash\nazd provision --preview\n```\n\nIf an environment is specified or multiple environments exist:\n```bash\nazd provision --preview --environment <env-name>\n```\n\n#### For Standalone Bicep (no azure.yaml)\n\nDetermine the deployment scope from the Bicep file's `targetScope` declaration:\n\n| Target Scope | Command |\n|--------------|---------|\n| `resourceGroup` (default) | `az deployment group what-if` |\n| `subscription` | `az deployment sub what-if` |\n| `managementGroup` | `az deployment mg what-if` |\n| `tenant` | `az deployment tenant what-if` |\n\n**Run with Provider validation level first:**\n\n```bash\n# Resource Group scope (most common)\naz deployment group what-if \\\n  --resource-group <rg-name> \\\n  --template-file <bicep-file> \\\n  --parameters <param-file> \\\n  --validation-level Provider\n\n# Subscription scope\naz deployment sub what-if \\\n  --location <location> \\\n  --template-file <bicep-file> \\\n  --parameters <param-file> \\\n  --validation-level Provider\n\n# Management Group scope\naz deployment mg what-if \\\n  --location <location> \\\n  --management-group-id <mg-id> \\\n  --template-file <bicep-file> \\\n  --parameters <param-file> \\\n  --validation-level Provider\n\n# Tenant scope\naz deployment tenant what-if \\\n  --location <location> \\\n  --template-file <bicep-file> \\\n  --parameters <param-file> \\\n  --validation-level Provider\n```\n\n**Fallback Strategy:**\n\nIf `--validation-level Provider` fails with permission errors (RBAC), retry with `ProviderNoRbac`:\n\n```bash\naz deployment group what-if \\\n  --resource-group <rg-name> \\\n  --template-file <bicep-file> \\\n  --validation-level ProviderNoRbac\n```\n\nNote the fallback in the reportâ€”the user may lack full deployment permissions.\n\n### Step 4: Capture What-If Results\n\nParse the what-if output to categorize resource changes:\n\n| Change Type | Symbol | Meaning |\n|-------------|--------|---------|\n| Create | `+` | New resource will be created |\n| Delete | `-` | Resource will be deleted |\n| Modify | `~` | Resource properties will change |\n| NoChange | `=` | Resource unchanged |\n| Ignore | `*` | Resource not analyzed (limits reached) |\n| Deploy | `!` | Resource will be deployed (changes unknown) |\n\nFor modified resources, capture the specific property changes.\n\n### Step 5: Generate Report\n\nCreate a Markdown report file in the **project root** named:\n- `preflight-report.md`\n\nUse the template structure from [references/REPORT-TEMPLATE.md](references/REPORT-TEMPLATE.md).\n\n**Report sections:**\n1. **Summary** - Overall status, timestamp, files validated, target scope\n2. **Tools Executed** - Commands run, versions, validation levels used\n3. **Issues** - All errors and warnings with severity and remediation\n4. **What-If Results** - Resources to create/modify/delete/unchanged\n5. **Recommendations** - Actionable next steps\n\n## Required Information\n\nBefore running validation, gather:\n\n| Information | Required For | How to Obtain |\n|-------------|--------------|---------------|\n| Resource Group | `az deployment group` | Ask user or check existing `.azure/` config |\n| Subscription | All deployments | `az account show` or ask user |\n| Location | Sub/MG/Tenant scope | Ask user or use default from config |\n| Environment | azd projects | `azd env list` or ask user |\n\nIf required information is missing, prompt the user before proceeding.\n\n## Error Handling\n\nSee [references/ERROR-HANDLING.md](references/ERROR-HANDLING.md) for detailed error handling guidance.\n\n**Key principle:** Continue validation even when errors occur. Capture all issues in the final report.\n\n| Error Type | Action |\n|------------|--------|\n| Not logged in | Note in report, suggest `az login` or `azd auth login` |\n| Permission denied | Fall back to `ProviderNoRbac`, note in report |\n| Bicep syntax error | Include all errors, continue to other files |\n| Tool not installed | Note in report, skip that validation step |\n| Resource group not found | Note in report, suggest creating it |\n\n## Tool Requirements\n\nThis skill uses the following tools:\n\n- **Azure CLI** (`az`) - Version 2.76.0+ recommended for `--validation-level`\n- **Azure Developer CLI** (`azd`) - For projects with `azure.yaml`\n- **Bicep CLI** (`bicep`) - For syntax validation\n- **Azure MCP Tools** - For documentation lookups and best practices\n\n### Azure MCP azd Tools\n\nUse the Azure MCP server's azd tools (`azure__azd`) for additional validation:\n\n| Command | Description |\n|---------|-------------|\n| `validate_azure_yaml` | **Validates azure.yaml against official JSON schema** |\n| `project_validation` | Comprehensive project validation before deployment |\n| `error_troubleshooting` | Diagnose and troubleshoot azd errors |\n\n**Validate azure.yaml before running `azd provision --preview`:**\n```javascript\nconst validation = await azure__azd({\n  command: \"validate_azure_yaml\",\n  parameters: { path: \"./azure.yaml\" }\n});\n```\n\nCheck tool availability before starting:\n```bash\naz --version\nazd version\nbicep --version\n```\n\n## Example Workflow\n\n1. User: \"Validate my Bicep deployment before I run it\"\n2. Agent detects `azure.yaml` â†’ azd project\n3. Agent validates `azure.yaml` using `azure__azd` â†’ `validate_azure_yaml`\n4. Agent finds `infra/main.bicep` and `infra/main.bicepparam`\n5. Agent runs `bicep build infra/main.bicep --stdout`\n6. Agent runs `azd provision --preview`\n7. Agent generates `preflight-report.md` in project root\n8. Agent summarizes findings to user\n\n## Reference Documentation\n\n- [Validation Commands Reference](references/VALIDATION-COMMANDS.md)\n- [Report Template](references/REPORT-TEMPLATE.md)\n- [Error Handling Guide](references/ERROR-HANDLING.md)\n",
        "plugin/skills/azure-deployment-preflight/references/ERROR-HANDLING.md": "# Error Handling Guide\n\nThis reference documents common errors during preflight validation and how to handle them.\n\n## Core Principle\n\n**Continue on failure.** Capture all issues in the final report rather than stopping at the first error. This gives users a complete picture of what needs to be fixed.\n\n---\n\n## Authentication Errors\n\n### Not Logged In (Azure CLI)\n\n**Detection:**\n```\nERROR: Please run 'az login' to setup account.\nERROR: AADSTS700082: The refresh token has expired\n```\n\n**Exit Codes:** Non-zero\n\n**Handling:**\n1. Note the error in the report\n2. Include remediation steps\n3. Skip remaining Azure CLI commands\n4. Continue with other validation steps if possible\n\n**Report Entry:**\n```markdown\n#### âŒ Azure CLI Authentication Required\n\n- **Severity:** Error\n- **Source:** az cli\n- **Message:** Not logged in to Azure CLI\n- **Remediation:** Run `az login` to authenticate, then re-run preflight validation\n- **Documentation:** https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli\n```\n\n### Not Logged In (azd)\n\n**Detection:**\n```\nERROR: not logged in, run `azd auth login` to login\n```\n\n**Handling:**\n1. Note the error in the report\n2. Skip azd commands\n3. Suggest `azd auth login`\n\n**Report Entry:**\n```markdown\n#### âŒ Azure Developer CLI Authentication Required\n\n- **Severity:** Error\n- **Source:** azd\n- **Message:** Not logged in to Azure Developer CLI\n- **Remediation:** Run `azd auth login` to authenticate, then re-run preflight validation\n```\n\n### Token Expired\n\n**Detection:**\n```\nAADSTS700024: Client assertion is not within its valid time range\nAADSTS50173: The provided grant has expired\n```\n\n**Handling:**\n1. Note the error\n2. Suggest re-authentication\n3. Skip Azure operations\n\n---\n\n## Permission Errors\n\n### Insufficient RBAC Permissions\n\n**Detection:**\n```\nAuthorizationFailed: The client '...' with object id '...' does not have authorization \nto perform action '...' over scope '...'\n```\n\n**Handling:**\n1. **First attempt:** Retry with `--validation-level ProviderNoRbac`\n2. Note the permission limitation in the report\n3. If ProviderNoRbac also fails, report the specific missing permission\n\n**Report Entry:**\n```markdown\n#### âš ï¸ Limited Permission Validation\n\n- **Severity:** Warning\n- **Source:** what-if\n- **Message:** Full RBAC validation failed; using read-only validation\n- **Detail:** Missing permission: `Microsoft.Resources/deployments/write` on scope `/subscriptions/xxx`\n- **Recommendation:** Request Contributor role on the target resource group, or verify deployment permissions with your administrator\n```\n\n### Resource Group Not Found\n\n**Detection:**\n```\nResourceGroupNotFound: Resource group 'xxx' could not be found.\n```\n\n**Handling:**\n1. Note in report\n2. Suggest creating the resource group\n3. Skip what-if for this scope\n\n**Report Entry:**\n```markdown\n#### âŒ Resource Group Does Not Exist\n\n- **Severity:** Error\n- **Source:** what-if\n- **Message:** Resource group 'my-rg' does not exist\n- **Remediation:** Create the resource group before deployment:\n  ```bash\n  az group create --name my-rg --location eastus\n  ```\n```\n\n### Subscription Access Denied\n\n**Detection:**\n```\nSubscriptionNotFound: The subscription 'xxx' could not be found.\nInvalidSubscriptionId: Subscription '...' is not valid\n```\n\n**Handling:**\n1. Note in report\n2. Suggest checking subscription ID\n3. List available subscriptions\n\n---\n\n## Bicep Syntax Errors\n\n### Compilation Errors\n\n**Detection:**\n```\n/path/main.bicep(22,51) : Error BCP064: Found unexpected tokens\n/path/main.bicep(10,5) : Error BCP018: Expected the \"=\" character at this location\n```\n\n**Handling:**\n1. Parse error output for line/column numbers\n2. Include all errors in report (don't stop at first)\n3. Continue to what-if (may provide additional context)\n\n**Report Entry:**\n```markdown\n#### âŒ Bicep Syntax Error\n\n- **Severity:** Error\n- **Source:** bicep build\n- **Location:** `main.bicep:22:51`\n- **Code:** BCP064\n- **Message:** Found unexpected tokens in interpolated expression\n- **Remediation:** Check the string interpolation syntax at line 22\n- **Documentation:** https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/diagnostics/bcp064\n```\n\n### Module Not Found\n\n**Detection:**\n```\nError BCP091: An error occurred reading file. Could not find file '...'\nError BCP190: The module is not valid\n```\n\n**Handling:**\n1. Note missing module\n2. Check if `bicep restore` is needed\n3. Verify module path\n\n### Parameter File Issues\n\n**Detection:**\n```\nError BCP032: The value must be a compile-time constant\nError BCP035: The specified object is missing required properties\n```\n\n**Handling:**\n1. Note parameter issues\n2. Indicate which parameters are problematic\n3. Suggest fixes\n\n---\n\n## Tool Not Installed\n\n### Azure CLI Not Found\n\n**Detection:**\n```\n'az' is not recognized as an internal or external command\naz: command not found\n```\n\n**Handling:**\n1. Note in report\n2. Provide installation instructions.\n  - If available use the Azure MCP `extension_cli_install` tool to get installation instructions.\n  - Otherwise look for instructions at https://learn.microsoft.com/en-us/cli/azure/install-azure-cli.\n3. Skip az commands\n\n**Report Entry:**\n```markdown\n#### â­ï¸ Azure CLI Not Installed\n\n- **Severity:** Warning\n- **Source:** environment\n- **Message:** Azure CLI (az) is not installed or not in PATH\n- **Remediation:** Install the Azure CLI <ADD INSTALLATION INSTRUCTIONS HERE>\n- **Impact:** What-if validation using az commands was skipped\n```\n\n### Bicep CLI Not Found\n\n**Detection:**\n```\n'bicep' is not recognized as an internal or external command\nbicep: command not found\n```\n\n**Handling:**\n1. Note in report\n2. Azure CLI may have built-in Bicep - try `az bicep build`\n3. Provide installation link\n\n**Report Entry:**\n```markdown\n#### â­ï¸ Bicep CLI Not Installed\n\n- **Severity:** Warning\n- **Source:** environment\n- **Message:** Bicep CLI is not installed\n- **Remediation:** Install Bicep CLI: https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/install\n- **Impact:** Syntax validation was skipped; Azure will validate during what-if\n```\n\n### Azure Developer CLI Not Found\n\n**Detection:**\n```\n'azd' is not recognized as an internal or external command\nazd: command not found\n```\n\n**Handling:**\n1. If `azure.yaml` exists, this is required\n2. Fall back to az CLI commands if possible\n3. Note in report\n\n---\n\n## What-If Specific Errors\n\n### Nested Template Limits\n\n**Detection:**\n```\nThe deployment exceeded the nested template limit of 500\n```\n\n**Handling:**\n1. Note as warning (not error)\n2. Explain affected resources show as \"Ignore\"\n3. Suggest manual review\n\n### Template Link Not Supported\n\n**Detection:**\n```\ntemplateLink references in nested deployments won't be visible in what-if\n```\n\n**Handling:**\n1. Note as warning\n2. Explain limitation\n3. Resources will be verified during actual deployment\n\n### Unevaluated Expressions\n\n**Detection:** Properties showing function names like `[utcNow()]` instead of values\n\n**Handling:**\n1. Note as informational\n2. Explain these are evaluated at deployment time\n3. Not an error\n\n---\n\n## Network Errors\n\n### Timeout\n\n**Detection:**\n```\nConnection timed out\nRequest timed out\n```\n\n**Handling:**\n1. Suggest retry\n2. Check network connectivity\n3. May indicate Azure service issues\n\n### SSL/TLS Errors\n\n**Detection:**\n```\nSSL: CERTIFICATE_VERIFY_FAILED\nunable to get local issuer certificate\n```\n\n**Handling:**\n1. Note in report\n2. May indicate proxy or corporate firewall\n3. Suggest checking SSL settings\n\n---\n\n## Fallback Strategy\n\nWhen primary validation fails, attempt fallbacks in order:\n\n```\nProvider (full RBAC validation)\n    â†“ fails with permission error\nProviderNoRbac (validation without write permission check)\n    â†“ fails\nTemplate (static syntax only)\n    â†“ fails\nReport all failures and skip what-if analysis\n```\n\n**Always continue to generate the report**, even if all validation steps fail.\n\n---\n\n## Error Report Aggregation\n\nWhen multiple errors occur, aggregate them logically:\n\n1. **Group by source** (bicep, what-if, permissions)\n2. **Order by severity** (errors before warnings)\n3. **Deduplicate** similar errors\n4. **Provide summary count** at the top\n\nExample:\n```markdown\n## Issues\n\nFound **3 errors** and **2 warnings**\n\n### Errors (3)\n\n1. [Bicep Syntax Error - main.bicep:22:51](#error-1)\n2. [Bicep Syntax Error - main.bicep:45:10](#error-2)\n3. [Resource Group Not Found](#error-3)\n\n### Warnings (2)\n\n1. [Limited Permission Validation](#warning-1)\n2. [Nested Template Limit Reached](#warning-2)\n```\n\n---\n\n## Exit Code Reference\n\n| Tool | Exit Code | Meaning |\n|------|-----------|---------|\n| az | 0 | Success |\n| az | 1 | General error |\n| az | 2 | Command not found |\n| az | 3 | Required argument missing |\n| azd | 0 | Success |\n| azd | 1 | Error |\n| bicep | 0 | Build succeeded |\n| bicep | 1 | Build failed (errors) |\n| bicep | 2 | Build succeeded with warnings |\n",
        "plugin/skills/azure-deployment-preflight/references/REPORT-TEMPLATE.md": "# Preflight Report Template\n\nUse this template structure when generating `preflight-report.md` in the project root.\n\n---\n\n## Template\n\n```markdown\n# Azure Deployment Preflight Report\n\n**Generated:** {timestamp}\n**Status:** {overall-status}\n\n---\n\n## Summary\n\n| Property | Value |\n|----------|-------|\n| **Template File(s)** | {bicep-files} |\n| **Parameter File(s)** | {param-files-or-none} |\n| **Project Type** | {azd-project | standalone-bicep} |\n| **Deployment Scope** | {resourceGroup | subscription | managementGroup | tenant} |\n| **Target** | {resource-group-name | subscription-name | mg-id} |\n| **Validation Level** | {Provider | ProviderNoRbac} |\n\n### Validation Results\n\n| Check | Status | Details |\n|-------|--------|---------|\n| Bicep Syntax | {âœ… Pass | âŒ Fail | âš ï¸ Warnings | â­ï¸ Skipped} | {details} |\n| What-If Analysis | {âœ… Pass | âŒ Fail | â­ï¸ Skipped} | {details} |\n| Permission Check | {âœ… Pass | âš ï¸ Limited | âŒ Fail} | {details} |\n\n---\n\n## Tools Executed\n\n### Commands Run\n\n| Step | Command | Exit Code | Duration |\n|------|---------|-----------|----------|\n| 1 | `{command}` | {0 | non-zero} | {duration} |\n| 2 | `{command}` | {0 | non-zero} | {duration} |\n\n### Tool Versions\n\n| Tool | Version |\n|------|---------|\n| Azure CLI | {version} |\n| Bicep CLI | {version} |\n| Azure Developer CLI | {version-or-n/a} |\n\n---\n\n## Issues\n\n{if-no-issues}\nâœ… **No issues found.** The deployment is ready to proceed.\n{end-if}\n\n{if-issues-exist}\n### Errors\n\n{for-each-error}\n#### âŒ {error-title}\n\n- **Severity:** Error\n- **Source:** {bicep-build | what-if | permissions}\n- **Location:** {file-path}:{line}:{column} (if applicable)\n- **Message:** {error-message}\n- **Remediation:** {suggested-fix}\n- **Documentation:** {link-if-available}\n\n{end-for-each}\n\n### Warnings\n\n{for-each-warning}\n#### âš ï¸ {warning-title}\n\n- **Severity:** Warning\n- **Source:** {source}\n- **Message:** {warning-message}\n- **Recommendation:** {suggested-action}\n\n{end-for-each}\n{end-if}\n\n---\n\n## What-If Results\n\n{if-what-if-succeeded}\n\n### Change Summary\n\n| Change Type | Count |\n|-------------|-------|\n| ðŸ†• Create | {count} |\n| ðŸ“ Modify | {count} |\n| ðŸ—‘ï¸ Delete | {count} |\n| âœ“ No Change | {count} |\n| âš ï¸ Ignore | {count} |\n\n### Resources to Create\n\n{if-resources-to-create}\n| Resource Type | Resource Name |\n|---------------|---------------|\n| {type} | {name} |\n{end-if}\n\n{if-no-resources-to-create}\n*No resources will be created.*\n{end-if}\n\n### Resources to Modify\n\n{if-resources-to-modify}\n#### {resource-type}/{resource-name}\n\n| Property | Current Value | New Value |\n|----------|---------------|-----------|\n| {property-path} | {current} | {new} |\n\n{end-if}\n\n{if-no-resources-to-modify}\n*No resources will be modified.*\n{end-if}\n\n### Resources to Delete\n\n{if-resources-to-delete}\n| Resource Type | Resource Name |\n|---------------|---------------|\n| {type} | {name} |\n\n> âš ï¸ **Warning:** Resources listed for deletion will be permanently removed.\n{end-if}\n\n{if-no-resources-to-delete}\n*No resources will be deleted.*\n{end-if}\n\n{end-if-what-if-succeeded}\n\n{if-what-if-failed}\n### What-If Analysis Failed\n\nThe what-if operation could not complete. See the Issues section for details.\n{end-if}\n\n---\n\n## Recommendations\n\n{generate-based-on-findings}\n\n1. {recommendation-1}\n2. {recommendation-2}\n3. {recommendation-3}\n\n---\n\n## Next Steps\n\n{if-all-passed}\nThe preflight validation passed. You can proceed with deployment:\n\n**For azd projects:**\n```bash\nazd provision\n# or\nazd up\n```\n\n**For standalone Bicep:**\n```bash\naz deployment group create \\\n  --resource-group {rg-name} \\\n  --template-file {bicep-file} \\\n  --parameters {param-file}\n```\n{end-if}\n\n{if-issues-exist}\nPlease resolve the issues listed above before deploying. After fixes:\n\n1. Re-run preflight validation to verify fixes\n2. Proceed with deployment once all checks pass\n{end-if}\n\n---\n\n*Report generated by Azure Deployment Preflight Skill*\n```\n\n---\n\n## Status Values\n\n### Overall Status\n\n| Status | Meaning | Visual |\n|--------|---------|--------|\n| **Pass** | All checks succeeded, safe to deploy | âœ… |\n| **Pass with Warnings** | Checks succeeded but review warnings | âš ï¸ |\n| **Fail** | One or more checks failed | âŒ |\n\n### Individual Check Status\n\n| Status | Meaning |\n|--------|---------|\n| âœ… Pass | Check completed successfully |\n| âŒ Fail | Check found errors |\n| âš ï¸ Warnings | Check passed with warnings |\n| â­ï¸ Skipped | Check was skipped (tool unavailable, etc.) |\n\n---\n\n## Example Report\n\n```markdown\n# Azure Deployment Preflight Report\n\n**Generated:** 2026-01-16T14:32:00Z\n**Status:** âš ï¸ Pass with Warnings\n\n---\n\n## Summary\n\n| Property | Value |\n|----------|-------|\n| **Template File(s)** | `infra/main.bicep` |\n| **Parameter File(s)** | `infra/main.bicepparam` |\n| **Project Type** | azd project |\n| **Deployment Scope** | subscription |\n| **Target** | my-subscription |\n| **Validation Level** | Provider |\n\n### Validation Results\n\n| Check | Status | Details |\n|-------|--------|---------|\n| Bicep Syntax | âœ… Pass | No errors found |\n| What-If Analysis | âš ï¸ Warnings | 1 resource ignored due to nested template limits |\n| Permission Check | âœ… Pass | Full deployment permissions verified |\n\n---\n\n## Tools Executed\n\n### Commands Run\n\n| Step | Command | Exit Code | Duration |\n|------|---------|-----------|----------|\n| 1 | `bicep build infra/main.bicep --stdout` | 0 | 1.2s |\n| 2 | `azd provision --preview --environment dev` | 0 | 8.4s |\n\n### Tool Versions\n\n| Tool | Version |\n|------|---------|\n| Azure CLI | 2.76.0 |\n| Bicep CLI | 0.25.3 |\n| Azure Developer CLI | 1.9.0 |\n\n---\n\n## Issues\n\n### Warnings\n\n#### âš ï¸ Nested Template Limit Reached\n\n- **Severity:** Warning\n- **Source:** what-if\n- **Message:** 1 resource was ignored because nested template expansion limits were reached\n- **Recommendation:** Review the ignored resource manually after deployment\n\n---\n\n## What-If Results\n\n### Change Summary\n\n| Change Type | Count |\n|-------------|-------|\n| ðŸ†• Create | 3 |\n| ðŸ“ Modify | 1 |\n| ðŸ—‘ï¸ Delete | 0 |\n| âœ“ No Change | 2 |\n| âš ï¸ Ignore | 1 |\n\n### Resources to Create\n\n| Resource Type | Resource Name |\n|---------------|---------------|\n| Microsoft.Resources/resourceGroups | rg-myapp-dev |\n| Microsoft.Storage/storageAccounts | stmyappdev |\n| Microsoft.Web/sites | app-myapp-dev |\n\n### Resources to Modify\n\n#### Microsoft.KeyVault/vaults/kv-myapp-dev\n\n| Property | Current Value | New Value |\n|----------|---------------|-----------|\n| properties.sku.name | standard | premium |\n| tags.environment | staging | dev |\n\n### Resources to Delete\n\n*No resources will be deleted.*\n\n---\n\n## Recommendations\n\n1. Review the storage account name `stmyappdev` to ensure it meets naming requirements\n2. Confirm the Key Vault SKU upgrade from standard to premium is intentional\n3. The ignored nested template resource should be verified after deployment\n\n---\n\n## Next Steps\n\nThe preflight validation passed with warnings. Review the warnings above, then proceed:\n\n```bash\nazd provision --environment dev\n```\n\n---\n\n*Report generated by Azure Deployment Preflight Skill*\n```\n\n---\n\n## Formatting Guidelines\n\n1. **Use consistent emoji** for visual scanning\n2. **Include line numbers** when referencing Bicep errors\n3. **Provide actionable remediation** for each issue\n4. **Link to documentation** when available\n5. **Order issues by severity** (errors first, then warnings)\n6. **Include command examples** in Next Steps\n",
        "plugin/skills/azure-deployment-preflight/references/VALIDATION-COMMANDS.md": "# Validation Commands Reference\n\nThis reference documents all commands used for Azure deployment preflight validation.\n\n## Azure Developer CLI (azd)\n\n### azd provision --preview\n\nPreview infrastructure changes for azd projects without deploying.\n\n```bash\nazd provision --preview [options]\n```\n\n**Options:**\n| Option | Description |\n|--------|-------------|\n| `--environment`, `-e` | Name of the environment to use |\n| `--no-prompt` | Accept defaults without prompting |\n| `--debug` | Enable debug logging |\n| `--cwd` | Set working directory |\n\n**Examples:**\n\n```bash\n# Preview with default environment\nazd provision --preview\n\n# Preview specific environment\nazd provision --preview --environment dev\n\n# Preview without prompts (CI/CD)\nazd provision --preview --no-prompt\n```\n\n**Output:** Shows resources that will be created, modified, or deleted.\n\n### azd auth login\n\nAuthenticate to Azure for azd operations.\n\n```bash\nazd auth login [options]\n```\n\n**Options:**\n| Option | Description |\n|--------|-------------|\n| `--check-status` | Check login status without logging in |\n| `--use-device-code` | Use device code flow |\n| `--tenant-id` | Specify tenant |\n| `--client-id` | Service principal client ID |\n\n### azd env list\n\nList available environments.\n\n```bash\nazd env list\n```\n\n---\n\n## Azure CLI (az)\n\n### az deployment group what-if\n\nPreview changes for resource group deployments.\n\n```bash\naz deployment group what-if \\\n  --resource-group <rg-name> \\\n  --template-file <bicep-file> \\\n  [options]\n```\n\n**Required Parameters:**\n| Parameter | Description |\n|-----------|-------------|\n| `--resource-group`, `-g` | Target resource group name |\n| `--template-file`, `-f` | Path to Bicep file |\n\n**Optional Parameters:**\n| Parameter | Description |\n|-----------|-------------|\n| `--parameters`, `-p` | Parameter file or inline values |\n| `--validation-level` | `Provider` (default), `ProviderNoRbac`, or `Template` |\n| `--result-format` | `FullResourcePayloads` (default) or `ResourceIdOnly` |\n| `--no-pretty-print` | Output raw JSON for parsing |\n| `--name`, `-n` | Deployment name |\n| `--exclude-change-types` | Exclude specific change types from output |\n\n**Validation Levels:**\n| Level | Description | Use Case |\n|-------|-------------|----------|\n| `Provider` | Full validation with RBAC checks | Default, most thorough |\n| `ProviderNoRbac` | Full validation, read permissions only | When lacking deploy permissions |\n| `Template` | Static syntax validation only | Quick syntax check |\n\n**Examples:**\n\n```bash\n# Basic what-if\naz deployment group what-if \\\n  --resource-group my-rg \\\n  --template-file main.bicep\n\n# With parameters and full validation\naz deployment group what-if \\\n  --resource-group my-rg \\\n  --template-file main.bicep \\\n  --parameters main.bicepparam \\\n  --validation-level Provider\n\n# Fallback without RBAC checks\naz deployment group what-if \\\n  --resource-group my-rg \\\n  --template-file main.bicep \\\n  --validation-level ProviderNoRbac\n\n# JSON output for parsing\naz deployment group what-if \\\n  --resource-group my-rg \\\n  --template-file main.bicep \\\n  --no-pretty-print\n```\n\n### az deployment sub what-if\n\nPreview changes for subscription-level deployments.\n\n```bash\naz deployment sub what-if \\\n  --location <location> \\\n  --template-file <bicep-file> \\\n  [options]\n```\n\n**Required Parameters:**\n| Parameter | Description |\n|-----------|-------------|\n| `--location`, `-l` | Location for deployment metadata |\n| `--template-file`, `-f` | Path to Bicep file |\n\n**Examples:**\n\n```bash\naz deployment sub what-if \\\n  --location eastus \\\n  --template-file main.bicep \\\n  --parameters main.bicepparam \\\n  --validation-level Provider\n```\n\n### az deployment mg what-if\n\nPreview changes for management group deployments.\n\n```bash\naz deployment mg what-if \\\n  --location <location> \\\n  --management-group-id <mg-id> \\\n  --template-file <bicep-file> \\\n  [options]\n```\n\n**Required Parameters:**\n| Parameter | Description |\n|-----------|-------------|\n| `--location`, `-l` | Location for deployment metadata |\n| `--management-group-id`, `-m` | Target management group ID |\n| `--template-file`, `-f` | Path to Bicep file |\n\n### az deployment tenant what-if\n\nPreview changes for tenant-level deployments.\n\n```bash\naz deployment tenant what-if \\\n  --location <location> \\\n  --template-file <bicep-file> \\\n  [options]\n```\n\n**Required Parameters:**\n| Parameter | Description |\n|-----------|-------------|\n| `--location`, `-l` | Location for deployment metadata |\n| `--template-file`, `-f` | Path to Bicep file |\n\n### az login\n\nAuthenticate to Azure CLI.\n\n```bash\naz login [options]\n```\n\n**Options:**\n| Option | Description |\n|--------|-------------|\n| `--tenant`, `-t` | Tenant ID or domain |\n| `--use-device-code` | Use device code flow |\n| `--service-principal` | Login as service principal |\n\n### az account show\n\nDisplay current subscription context.\n\n```bash\naz account show\n```\n\n### az group exists\n\nCheck if resource group exists.\n\n```bash\naz group exists --name <rg-name>\n```\n\n---\n\n## Bicep CLI\n\n### bicep build\n\nCompile Bicep to ARM JSON and validate syntax.\n\n```bash\nbicep build <bicep-file> [options]\n```\n\n**Options:**\n| Option | Description |\n|--------|-------------|\n| `--stdout` | Output to stdout instead of file |\n| `--outdir` | Output directory |\n| `--outfile` | Output file path |\n| `--no-restore` | Skip module restore |\n\n**Examples:**\n\n**Bash:**\n```bash\n# Validate syntax (output to stdout, no file created)\nbicep build main.bicep --stdout > /dev/null\n\n# Build to specific directory\nbicep build main.bicep --outdir ./build\n\n# Validate multiple files\nfor f in *.bicep; do bicep build \"$f\" --stdout; done\n```\n\n**PowerShell:**\n```powershell\n# Validate syntax (output to stdout, no file created)\nbicep build main.bicep --stdout | Out-Null\n\n# Build to specific directory\nbicep build main.bicep --outdir ./build\n\n# Validate multiple files\nGet-ChildItem -Filter *.bicep | ForEach-Object { bicep build $_.FullName --stdout }\n```\n\n**Error Output Format:**\n```\n/path/to/file.bicep(22,51) : Error BCP064: Found unexpected tokens in interpolated expression.\n/path/to/file.bicep(22,51) : Error BCP004: The string at this location is not terminated.\n```\n\nFormat: `<file>(<line>,<column>) : <severity> <code>: <message>`\n\n### bicep --version\n\nCheck Bicep CLI version.\n\n```bash\nbicep --version\n```\n\n---\n\n## Parameter File Detection\n\n### Bicep Parameters (.bicepparam)\n\nModern Bicep parameter files (recommended):\n\n```bicep\nusing './main.bicep'\n\nparam location = 'eastus'\nparam environment = 'dev'\nparam tags = {\n  environment: 'dev'\n  project: 'myapp'\n}\n```\n\n**Detection pattern:** `<template-name>.bicepparam`\n\n### JSON Parameters (.parameters.json)\n\nTraditional ARM parameter files:\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"location\": { \"value\": \"eastus\" },\n    \"environment\": { \"value\": \"dev\" }\n  }\n}\n```\n\n**Detection patterns:**\n- `<template-name>.parameters.json`\n- `parameters.json`\n- `parameters/<env>.json`\n\n### Using Parameters with Commands\n\n```bash\n# Bicep parameters file\naz deployment group what-if \\\n  --resource-group my-rg \\\n  --template-file main.bicep \\\n  --parameters main.bicepparam\n\n# JSON parameters file\naz deployment group what-if \\\n  --resource-group my-rg \\\n  --template-file main.bicep \\\n  --parameters @parameters.json\n\n# Inline parameter overrides\naz deployment group what-if \\\n  --resource-group my-rg \\\n  --template-file main.bicep \\\n  --parameters main.bicepparam \\\n  --parameters location=westus\n```\n\n---\n\n## Determining Deployment Scope\n\nCheck the Bicep file's `targetScope` declaration:\n\n```bicep\n// Resource Group (default if not specified)\ntargetScope = 'resourceGroup'\n\n// Subscription\ntargetScope = 'subscription'\n\n// Management Group\ntargetScope = 'managementGroup'\n\n// Tenant\ntargetScope = 'tenant'\n```\n\n**Scope to Command Mapping:**\n\n| targetScope | Command | Required Parameters |\n|-------------|---------|---------------------|\n| `resourceGroup` | `az deployment group what-if` | `--resource-group` |\n| `subscription` | `az deployment sub what-if` | `--location` |\n| `managementGroup` | `az deployment mg what-if` | `--location`, `--management-group-id` |\n| `tenant` | `az deployment tenant what-if` | `--location` |\n\n---\n\n## Version Requirements\n\n| Tool | Minimum Version | Recommended Version | Key Features |\n|------|-----------------|---------------------|--------------|\n| Azure CLI | 2.14.0 | 2.76.0+ | `--validation-level` switch |\n| Azure Developer CLI | 1.0.0 | Latest | `--preview` flag |\n| Bicep CLI | 0.4.0 | Latest | Best error messages |\n\n**Check versions:**\n\n**Bash:**\n```bash\naz --version\nazd version\nbicep --version\n```\n\n**PowerShell:**\n```powershell\naz --version\nazd version\nbicep --version\n```\n",
        "plugin/skills/azure-diagnostics/SKILL.md": "---\nname: azure-diagnostics\ndescription: Debug and troubleshoot production issues on Azure. Covers Container Apps diagnostics, App Service troubleshooting, log analysis with KQL, health checks, and common issue resolution for image pulls, cold starts, and health probes.\n---\n\n# Debugging Production Issues\n\n## Quick Diagnosis Flow\n\n1. **Identify symptoms** - What's failing?\n2. **Check resource health** - Is Azure healthy?\n3. **Review logs** - What do logs show?\n4. **Analyze metrics** - Performance patterns?\n5. **Investigate recent changes** - What changed?\n\n## Container Apps Troubleshooting\n\n### Common Issues Matrix\n\n| Symptom | Likely Cause | Quick Fix |\n|---------|--------------|-----------|\n| Image pull failure | ACR credentials missing | `az containerapp registry set --identity system` |\n| ACR build fails | ACR Tasks disabled (free sub) | Build locally with Docker |\n| Cold start timeout | min-replicas=0 | `az containerapp update --min-replicas 1` |\n| Port mismatch | Wrong target port | Check Dockerfile EXPOSE matches ingress |\n| App keeps restarting | Health probe failing | Verify `/health` endpoint |\n\n### Image Pull Failures\n\n**Diagnose:**\n```bash\n# Check registry configuration\naz containerapp show --name APP -g RG --query \"properties.configuration.registries\"\n\n# Check revision status\naz containerapp revision list --name APP -g RG --output table\n```\n\n**Fix:**\n```bash\naz containerapp registry set \\\n  --name APP -g RG \\\n  --server ACR.azurecr.io \\\n  --identity system\n```\n\n### ACR Tasks Disabled (Free Subscriptions)\n\n**Symptom:** `az acr build` fails with \"ACR Tasks is not supported\"\n\n**Fix: Build locally instead:**\n```bash\ndocker build -t ACR.azurecr.io/myapp:v1 .\naz acr login --name ACR\ndocker push ACR.azurecr.io/myapp:v1\n```\n\n### Cold Start Issues\n\n**Symptom:** First request very slow or times out\n\n**Fix:**\n```bash\naz containerapp update --name APP -g RG --min-replicas 1\n```\n\n### Health Probe Failures\n\n**Symptom:** Container keeps restarting\n\n**Check:**\n```bash\n# View health probe config\naz containerapp show --name APP -g RG --query \"properties.configuration.ingress\"\n\n# Check if /health endpoint responds\ncurl https://APP.REGION.azurecontainerapps.io/health\n```\n\n**Fix:** Ensure app has health endpoint returning 200:\n```javascript\napp.get('/health', (req, res) => res.sendStatus(200));\n```\n\n### Port Mismatch\n\n**Symptom:** App starts but returns 502/503\n\n**Check:**\n```bash\naz containerapp show --name APP -g RG --query \"properties.configuration.ingress.targetPort\"\n```\n\n**Verify:** App must listen on this exact port. Check:\n- Dockerfile `EXPOSE` statement\n- `process.env.PORT` or hardcoded port in app\n\n### View Logs\n\n```bash\n# Stream logs (wait for replicas if scale-to-zero)\naz containerapp logs show --name APP -g RG --follow\n\n# Recent logs\naz containerapp logs show --name APP -g RG --tail 100\n\n# System logs (startup issues)\naz containerapp logs show --name APP -g RG --type system\n```\n\n### Get All Diagnostic Info\n\n```bash\n# Combined diagnostic command\necho \"=== Container App Diagnostics ===\" && \\\necho \"Revisions:\" && az containerapp revision list --name APP -g RG -o table && \\\necho \"Registry Config:\" && az containerapp show --name APP -g RG --query \"properties.configuration.registries\" && \\\necho \"Ingress Config:\" && az containerapp show --name APP -g RG --query \"properties.configuration.ingress\" && \\\necho \"Recent Logs:\" && az containerapp logs show --name APP -g RG --tail 20\n```\n\n## Check Azure Resource Health\n\n### Using MCP\n\n```\nUse azure_resourcehealth_* tools to check resource availability status.\n```\n\n### Using CLI\n\n```bash\n# Check specific resource health\naz resource show --ids RESOURCE_ID\n\n# Check recent activity\naz monitor activity-log list -g RG --max-events 20\n```\n\n## Application Logs\n\n### App Service\n\n```bash\naz webapp log tail --name APP -g RG\n```\n\n### Functions\n\n```bash\nfunc azure functionapp logstream FUNCTIONAPP\n```\n\n## Log Analytics (KQL)\n\nCommon diagnostic queries:\n\n```kql\n// Recent errors\nAppExceptions\n| where TimeGenerated > ago(1h)\n| project TimeGenerated, Message, StackTrace\n| order by TimeGenerated desc\n\n// Failed requests\nAppRequests\n| where Success == false\n| where TimeGenerated > ago(1h)\n| summarize count() by Name, ResultCode\n| order by count_ desc\n\n// Slow requests\nAppRequests\n| where TimeGenerated > ago(1h)\n| where DurationMs > 5000\n| project TimeGenerated, Name, DurationMs\n| order by DurationMs desc\n\n// Dependency failures\nAppDependencies\n| where Success == false\n| where TimeGenerated > ago(1h)\n| summarize count() by Name, ResultCode, Target\n```\n\n## Common Issues by Service\n\n### App Service\n\n| Symptom | Check |\n|---------|-------|\n| 503 Service Unavailable | App logs, memory/CPU usage |\n| Slow cold start | Always On setting, app startup |\n| Deployment failures | Deployment logs, slot swap |\n\n### Azure Functions\n\n| Symptom | Check |\n|---------|-------|\n| Not triggering | Trigger configuration, host.json |\n| Timeout errors | Execution time, plan limits |\n| Cold starts | Premium plan, package size |\n\n### Database Issues\n\n| Symptom | Check |\n|---------|-------|\n| Connection failures | Firewall rules, connection limits |\n| Slow queries | Query Performance Insights |\n| Throttling | DTU/RU usage, tier limits |\n\n## Using AppLens (MCP)\n\nFor comprehensive AI-powered diagnostics:\n\n```\nUse azure_applens tools to get:\n- Automated issue detection\n- Root cause analysis\n- Remediation recommendations\n```\n\n## Escalation Checklist\n\nBefore escalating:\n- [ ] Checked resource health status\n- [ ] Reviewed application logs\n- [ ] Analyzed recent deployments\n- [ ] Checked for Azure service issues (status.azure.com)\n- [ ] Reviewed metric dashboards\n- [ ] Attempted basic remediation\n",
        "plugin/skills/azure-functions/SKILL.md": "---\nname: azure-functions\ndescription: Serverless event-driven compute with Azure Functions - pay-per-execution, auto-scaling, multiple trigger types, and deployment workflows\n---\n\n# Azure Functions\n\nAzure Functions is a serverless compute service for event-driven applications. Pay only for execution time with automatic scaling.\n\n## Skill Activation Triggers\n\n**Use this skill immediately when the user asks to:**\n- \"Deploy my function to Azure\"\n- \"Create a serverless API on Azure\"\n- \"Deploy Azure Functions\"\n- \"Set up a timer-triggered function in Azure\"\n- \"Create webhooks in Azure\"\n- Any request involving **serverless functions**, **event-driven processing**, or **Azure Functions**\n\n**Key Indicators:**\n- Project uses Azure Functions (`host.json`, `local.settings.json` present)\n- User mentions serverless, functions, triggers, or bindings\n- User wants to deploy lightweight APIs without container management\n- User needs timer jobs, queue processors, or event handlers\n\n## Quick Reference\n\n| Property | Value |\n|----------|-------|\n| CLI prefix | `az functionapp`, `func` |\n| MCP tools | `azure__functionapp` (command: `functionapp_list`) |\n| Best for | Event-driven, pay-per-execution, serverless |\n\n## Hosting Plans\n\n**ALWAYS USE FLEX CONSUMPTION** for new deployments. All azd templates use Flex Consumption by default.\n\n| Plan | Scaling | VNET | Use Case |\n|------|---------|------|----------|\n| **Flex Consumption** â­ | Auto, pay-per-execution | âœ… | **Default for all new projects** |\n| Premium | Auto, pre-warmed | âœ… | Long-running, consistent load |\n| Dedicated | Manual | âœ… | Predictable workloads |\n\n## Trigger Types\n\n| Trigger | Use Case |\n|---------|----------|\n| HTTP | REST APIs, webhooks |\n| Timer | Scheduled jobs (CRON) |\n| Blob | File processing |\n| Queue | Message processing |\n| Event Grid | Event-driven |\n| Cosmos DB | Change feed processing |\n| Service Bus | Enterprise messaging |\n\n---\n\n## Prerequisites Validation\n\nValidate all prerequisites before starting development or deployment.\n\n```javascript\nasync function validatePrerequisites() {\n  const checks = [];\n\n  // Check Azure CLI authentication\n  try {\n    await exec('az account show');\n    checks.push({ name: 'Azure CLI', status: 'authenticated' });\n  } catch (error) {\n    throw new Error('Not authenticated with Azure CLI. Run: az login');\n  }\n\n  // Check Azure Functions Core Tools - install if not present\n  try {\n    await exec('func --version');\n    checks.push({ name: 'Azure Functions Core Tools', status: 'installed' });\n  } catch (error) {\n    console.log('Azure Functions Core Tools not found. Installing...');\n    try {\n      await exec('npm install -g azure-functions-core-tools@4 --unsafe-perm true');\n      checks.push({ name: 'Azure Functions Core Tools', status: 'installed (just now)' });\n    } catch (installError) {\n      throw new Error('Failed to install Azure Functions Core Tools. Please install manually: npm install -g azure-functions-core-tools@4');\n    }\n  }\n\n  return checks;\n}\n```\n\n### Platform-Specific Installation\n\nIf npm installation fails, use platform-specific installers:\n\n```bash\n# Windows (winget)\nwinget install Microsoft.AzureFunctionsCoreTools\n\n# Windows (Chocolatey)\nchoco install azure-functions-core-tools\n\n# macOS (Homebrew)\nbrew install azure-functions-core-tools@4\n\n> ðŸ’¡ **Note**: `brew install` works directly without needing `brew tap azure/functions` in recent Homebrew versions.\n\n# Linux (Ubuntu/Debian)\ncurl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\nsudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\nsudo sh -c 'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs)-prod $(lsb_release -cs) main\" > /etc/apt/sources.list.d/dotnetdev.list'\nsudo apt-get update\nsudo apt-get install azure-functions-core-tools-4\n```\n\n---\n\n## Local Development\n\n### Initialize Function Project\n\nCreate a new Azure Functions project with the desired runtime.\n\n```bash\n# Create new function project\nfunc init MyFunctionApp --worker-runtime node --model V4\n\n# Create a new HTTP-triggered function\ncd MyFunctionApp\nfunc new --name HttpTrigger --template \"HTTP trigger\"\n\n# For TypeScript\nfunc init MyFunctionApp --worker-runtime node --language typescript --model V4\n```\n\n**Supported runtimes:** `node`, `python`, `dotnet`, `dotnet-isolated`, `java`, `powershell`, `custom`\n\n### Project Structure\n\n```\nMyFunctionApp/\nâ”œâ”€â”€ host.json              # Function app configuration\nâ”œâ”€â”€ local.settings.json    # Local development settings\nâ”œâ”€â”€ package.json           # Node.js dependencies\nâ””â”€â”€ src/\n    â””â”€â”€ functions/\n        â””â”€â”€ HttpTrigger.js # Function code\n```\n\n### Run Locally\n\n```bash\n# Start local development server\nfunc start\n\n# Start with specific port\nfunc start --port 7072\n\n# Start with debugging enabled\nfunc start --verbose\n```\n\n**Local endpoints:**\n- HTTP triggers: `http://localhost:7071/api/{functionName}`\n- Admin API: `http://localhost:7071/admin/functions`\n\n### Example HTTP Function (Node.js v4)\n\n```javascript\nconst { app } = require('@azure/functions');\n\napp.http('HttpTrigger', {\n    methods: ['GET', 'POST'],\n    authLevel: 'anonymous',\n    handler: async (request, context) => {\n        context.log('HTTP function processed a request.');\n        const name = request.query.get('name') || await request.text() || 'World';\n        return { body: `Hello, ${name}!` };\n    }\n});\n```\n\n### Example Timer Function\n\n```javascript\nconst { app } = require('@azure/functions');\n\napp.timer('TimerTrigger', {\n    schedule: '0 */5 * * * *', // Every 5 minutes\n    handler: async (myTimer, context) => {\n        context.log('Timer trigger executed at:', new Date().toISOString());\n    }\n});\n```\n\n### local.settings.json\n\n```json\n{\n  \"IsEncrypted\": false,\n  \"Values\": {\n    \"FUNCTIONS_WORKER_RUNTIME\": \"node\",\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n    \"MY_API_KEY\": \"local-dev-key\"\n  },\n  \"Host\": {\n    \"LocalHttpPort\": 7071,\n    \"CORS\": \"*\"\n  }\n}\n```\n\n---\n\n## Preferred: Deploy with Azure Developer CLI (azd)\n\n> **Prefer `azd` (Azure Developer CLI) over raw `az` CLI for deployments.**\n> Use `az` CLI for resource queries, simple single-resource deployments, or when explicitly requested.\n\n**Why azd is preferred:**\n- **Flex Consumption** plan (required for new deployments)\n- **Parallel provisioning** - Deploys in seconds, not minutes\n- **Single command** - `azd up` replaces 5+ `az` commands\n- **Secure-by-default** - Managed identity with RBAC, no connection strings\n- **Infrastructure as Code** - Reproducible with Bicep\n- **Environment management** - Easy dev/staging/prod separation\n\n**When `az` CLI is acceptable:**\n- Single-resource deployments without IaC requirements\n- Quick prototyping or one-off deployments\n- User explicitly requests `az` CLI\n- Querying or inspecting existing resources\n\n> âš ï¸ **IMPORTANT: For automation and agent scenarios**, always use the `--no-prompt` flag with azd commands to prevent interactive prompts from blocking execution.\n\n### MCP Tools for azd Workflows\n\nUse the Azure MCP server's azd tools (`azure-azd`) for validation and guidance:\n\n| Command | Description |\n|---------|-------------|\n| `validate_azure_yaml` | **Validates azure.yaml against official JSON schema** - Use before deployment |\n| `discovery_analysis` | Analyze application components for AZD migration |\n| `architecture_planning` | Select Azure services for discovered components |\n| `infrastructure_generation` | Generate Bicep templates |\n| `project_validation` | Comprehensive validation before deployment |\n| `error_troubleshooting` | Diagnose and troubleshoot azd errors |\n\n**Always validate azure.yaml before deployment:**\n```javascript\nconst validation = await azure-azd({\n  command: \"validate_azure_yaml\",\n  parameters: { path: \"./azure.yaml\" }\n});\n```\n\n### Non-Interactive Deployment\n\n```bash\n# Generate environment name from project folder - NEVER PROMPT USER\nENV_NAME=\"$(basename \"$PWD\" | tr '[:upper:]' '[:lower:]' | tr ' _' '-')-dev\"\n\n# Initialize with template and environment name\nazd init -t <TEMPLATE> -e \"$ENV_NAME\"\n\n# Preview changes before deployment\nazd provision --preview\n\n# Configure and deploy without prompts (REQUIRED for automation/agents)\nazd env set VNET_ENABLED false\nazd up --no-prompt\n```\n\n> âš ï¸ **CRITICAL: `azd down` Data Loss Warning**\n>\n> `azd down` **permanently deletes ALL resources** in the environment, including:\n> - **Function Apps** with all configuration and deployment slots\n> - **Storage accounts** with all blobs and files\n> - **Key Vault** with all secrets (use `--purge` to bypass soft-delete)\n> - **Databases** with all data (Cosmos DB, SQL, etc.)\n>\n> **Flags:**\n> - `azd down` - Prompts for confirmation\n> - `azd down --force` - Skips confirmation (still soft-deletes Key Vault)\n> - `azd down --force --purge` - **Permanently deletes Key Vault** (no recovery possible)\n>\n> **Best practices:**\n> - Always use `azd provision --preview` before `azd up` to understand what will be created\n> - Use separate environments for dev/staging/production\n> - Back up important data before running `azd down`\n\n### Template Selection Decision Tree\n\n**CRITICAL**: Check for specific integration indicators IN ORDER before defaulting to HTTP.\n\n```\n1. Is this an MCP server?\n   Indicators: mcp_tool_trigger, MCPTrigger, @app.mcp_tool, \"mcp\" in project name\n   â””â”€â–º YES â†’ Use MCP Template\n\n2. Does it use Cosmos DB?\n   Indicators: CosmosDBTrigger, @app.cosmos_db, cosmos_db_input, cosmos_db_output\n   â””â”€â–º YES â†’ Use Cosmos DB Template: https://azure.github.io/awesome-azd/?tags=functions&name=cosmos\n\n3. Does it use Azure SQL?\n   Indicators: SqlTrigger, @app.sql, sql_input, sql_output, SqlInput, SqlOutput\n   â””â”€â–º YES â†’ Use SQL Template: https://azure.github.io/awesome-azd/?tags=functions&name=sql\n\n4. Does it use AI/OpenAI?\n   Indicators: openai, AzureOpenAI, azure-ai-openai, langchain, langgraph, semantic_kernel,\n               Microsoft.Agents, azure-ai-projects, CognitiveServices, text_completion,\n               embeddings_input, ChatCompletions, azure.ai.inference, @azure/openai\n   â””â”€â–º YES â†’ Use AI Template: https://azure.github.io/awesome-azd/?tags=functions&name=ai\n\n5. Is it a full-stack app with SWA?\n   Indicators: staticwebapp.config.json, swa-cli, @azure/static-web-apps\n   â””â”€â–º YES â†’ Use SWA+Functions Template (see Integration Templates below)\n\n6. DEFAULT â†’ Use HTTP Template by runtime\n```\n\n### MCP Server Templates\n\n**Indicators**: `mcp_tool_trigger`, `MCPTrigger`, `@app.mcp_tool`, project name contains \"mcp\"\n\n| Language | MCP Template |\n|----------|--------------|\n| Python | `azd init -t remote-mcp-functions-python` |\n| TypeScript | `azd init -t remote-mcp-functions-typescript` |\n| C# (.NET) | `azd init -t remote-mcp-functions-dotnet` |\n| Java | `azd init -t remote-mcp-functions-java` |\n\n**MCP + API Management (OAuth):**\n| Language | Template |\n|----------|----------|\n| Python | `azd init -t remote-mcp-apim-functions-python` |\n\n**Self-Hosted MCP SDK:**\n| Language | Template |\n|----------|----------|\n| Python | `azd init -t remote-mcp-sdk-functions-hosting-python` |\n| TypeScript | `azd init -t remote-mcp-sdk-functions-hosting-node` |\n| C# | `azd init -t remote-mcp-sdk-functions-hosting-dotnet` |\n\n### Integration Templates (Cosmos DB, SQL, AI, SWA)\n\n**Browse by service to find the right template:**\n| Service | Find Templates |\n|---------|----------------|\n| Cosmos DB | [Awesome AZD Cosmos](https://azure.github.io/awesome-azd/?tags=functions&name=cosmos) |\n| Azure SQL | [Awesome AZD SQL](https://azure.github.io/awesome-azd/?tags=functions&name=sql) |\n| AI/OpenAI | [Awesome AZD AI](https://azure.github.io/awesome-azd/?tags=functions&name=ai) |\n| SWA + Functions | [todo-csharp-sql-swa-func](https://github.com/Azure-Samples/todo-csharp-sql-swa-func), [todo-nodejs-mongo-swa-func](https://github.com/azure-samples/todo-nodejs-mongo-swa-func) |\n\n### HTTP Function Templates (Default - use only if no specific integration)\n\n| Runtime | Template |\n|---------|----------|\n| C# (.NET) | `azd init -t functions-quickstart-dotnet-azd` |\n| JavaScript | `azd init -t functions-quickstart-javascript-azd` |\n| TypeScript | `azd init -t functions-quickstart-typescript-azd` |\n| Python | `azd init -t functions-quickstart-python-http-azd` |\n| Java | `azd init -t azure-functions-java-flex-consumption-azd` |\n| PowerShell | `azd init -t functions-quickstart-powershell-azd` |\n\n**Key flags for non-interactive mode:**\n| Flag | Purpose |\n|------|---------|\n| `-e <name>` | Set environment name (avoids prompt) |\n| `-t <template>` | Specify template |\n| `--no-prompt` | Skip all confirmations (REQUIRED for automation/agents) |\n\n> âš ï¸ **`azd env set` vs Application Environment Variables**\n>\n> **`azd env set`** sets variables for the **azd provisioning process**, NOT application runtime environment variables. These are used by azd and Bicep during deployment:\n> ```bash\n> azd env set AZURE_LOCATION eastus\n> azd env set VNET_ENABLED true\n> ```\n>\n> **Application environment variables** (like `FUNCTIONS_WORKER_RUNTIME`) must be configured:\n> 1. **In Bicep templates** - Define in the resource's app settings\n> 2. **Via Azure CLI** - Use `az functionapp config appsettings set`\n> 3. **In local.settings.json** - For local development only\n\n**Browse all templates:** [Awesome AZD Functions](https://azure.github.io/awesome-azd/?tags=functions)\n\n### What azd Creates (Secure-by-Default)\n- **Flex Consumption plan** (required for new deployments)\n- User-assigned managed identity\n- RBAC role assignments (no connection strings)\n- Storage with `allowSharedKeyAccess: false`\n- App Insights with `disableLocalAuth: true`\n- Optional VNET with private endpoints\n\n---\n\n## Create Azure Resources (az CLI Fallback)\n\n**âš ï¸ FALLBACK ONLY**: Use this section only if `azd` is not available. Always prefer `azd` above.\n\nCreate Flex Consumption resources with managed identity (matching azd secure-by-default).\n\n```bash\n# Set variables\nRESOURCE_GROUP=\"rg-myfunc-$(date +%s)\"\nLOCATION=\"eastus\"\nSTORAGE_ACCOUNT=\"stmyfunc$(date +%s | tail -c 8)\"\nFUNCTION_APP=\"func-myapp-$(date +%s | tail -c 8)\"\nIDENTITY_NAME=\"id-myfunc\"\n\n# Create resource group\naz group create --name $RESOURCE_GROUP --location $LOCATION\n\n# Create user-assigned managed identity\naz identity create --name $IDENTITY_NAME --resource-group $RESOURCE_GROUP\n\n# Get identity details\nIDENTITY_ID=$(az identity show --name $IDENTITY_NAME --resource-group $RESOURCE_GROUP --query id -o tsv)\nIDENTITY_PRINCIPAL=$(az identity show --name $IDENTITY_NAME --resource-group $RESOURCE_GROUP --query principalId -o tsv)\nIDENTITY_CLIENT_ID=$(az identity show --name $IDENTITY_NAME --resource-group $RESOURCE_GROUP --query clientId -o tsv)\n\n# Create storage account with NO local auth (RBAC only)\naz storage account create \\\n    --name $STORAGE_ACCOUNT \\\n    --resource-group $RESOURCE_GROUP \\\n    --location $LOCATION \\\n    --sku Standard_LRS \\\n    --allow-blob-public-access false \\\n    --allow-shared-key-access false\n\n# Get storage account ID and assign RBAC\nSTORAGE_ID=$(az storage account show --name $STORAGE_ACCOUNT --resource-group $RESOURCE_GROUP --query id -o tsv)\naz role assignment create \\\n    --assignee-object-id $IDENTITY_PRINCIPAL \\\n    --assignee-principal-type ServicePrincipal \\\n    --role \"Storage Blob Data Owner\" \\\n    --scope $STORAGE_ID\n\n# Create Function App (Flex Consumption) with managed identity\naz functionapp create \\\n    --name $FUNCTION_APP \\\n    --resource-group $RESOURCE_GROUP \\\n    --storage-account $STORAGE_ACCOUNT \\\n    --flexconsumption-location $LOCATION \\\n    --runtime node \\\n    --runtime-version 20 \\\n    --functions-version 4 \\\n    --assign-identity $IDENTITY_ID\n\n# Configure managed identity storage access\nSTORAGE_BLOB_ENDPOINT=$(az storage account show --name $STORAGE_ACCOUNT --resource-group $RESOURCE_GROUP --query primaryEndpoints.blob -o tsv)\naz functionapp config appsettings set \\\n    --name $FUNCTION_APP \\\n    --resource-group $RESOURCE_GROUP \\\n    --settings \\\n        \"AzureWebJobsStorage__credential=managedidentity\" \\\n        \"AzureWebJobsStorage__clientId=$IDENTITY_CLIENT_ID\" \\\n        \"AzureWebJobsStorage__blobServiceUri=$STORAGE_BLOB_ENDPOINT\"\n```\n\n---\n\n## Deploy Functions\n\nDeploy functions to Azure using Azure Functions Core Tools.\n\n```bash\n# Deploy to Azure (from project root)\nfunc azure functionapp publish $FUNCTION_APP\n\n# Deploy with build (for TypeScript/compiled projects)\nfunc azure functionapp publish $FUNCTION_APP --build remote\n\n# Deploy with verbose output\nfunc azure functionapp publish $FUNCTION_APP --verbose\n\n# Deploy specific slot\nfunc azure functionapp publish $FUNCTION_APP --slot staging\n\n# Force update function app settings\nfunc azure functionapp publish $FUNCTION_APP --publish-settings-only\n\n# Upload local.settings.json to Azure\nfunc azure functionapp publish $FUNCTION_APP --publish-local-settings\n```\n\n---\n\n## Configuration Management\n\nManage application settings and connection strings.\n\n```bash\n# Set application setting\naz functionapp config appsettings set \\\n    --name $FUNCTION_APP \\\n    --resource-group $RESOURCE_GROUP \\\n    --settings \"MySetting=MyValue\"\n\n# Set connection string\naz functionapp config connection-string set \\\n    --name $FUNCTION_APP \\\n    --resource-group $RESOURCE_GROUP \\\n    --connection-string-type SQLAzure \\\n    --settings \"MyConnection=Server=...\"\n\n# List settings\naz functionapp config appsettings list \\\n    --name $FUNCTION_APP \\\n    --resource-group $RESOURCE_GROUP\n\n# Get function keys\naz functionapp keys list -n $FUNCTION_APP -g $RESOURCE_GROUP\n```\n\n---\n\n## Monitoring and Logs\n\nView function execution logs and diagnostics.\n\n```bash\n# Stream live logs\nfunc azure functionapp logstream $FUNCTION_APP\n\n# View deployment logs\naz functionapp log deployment list \\\n    --name $FUNCTION_APP \\\n    --resource-group $RESOURCE_GROUP\n\n# Enable Application Insights (recommended)\naz monitor app-insights component create \\\n    --app $FUNCTION_APP-insights \\\n    --location $LOCATION \\\n    --resource-group $RESOURCE_GROUP\n\n# Link App Insights to Function App (use connection string - instrumentationKey is deprecated)\nAPPINSIGHTS_CONNECTION_STRING=$(az monitor app-insights component show \\\n    --app $FUNCTION_APP-insights \\\n    --resource-group $RESOURCE_GROUP \\\n    --query connectionString -o tsv)\n\naz functionapp config appsettings set \\\n    --name $FUNCTION_APP \\\n    --resource-group $RESOURCE_GROUP \\\n    --settings \"APPLICATIONINSIGHTS_CONNECTION_STRING=$APPINSIGHTS_CONNECTION_STRING\"\n```\n\n---\n\n## Deployment Slots (Premium/Dedicated Plans)\n\nUse deployment slots for zero-downtime deployments.\n\n```bash\n# Create staging slot\naz functionapp deployment slot create \\\n    --name $FUNCTION_APP \\\n    --resource-group $RESOURCE_GROUP \\\n    --slot staging\n\n# Deploy to staging\nfunc azure functionapp publish $FUNCTION_APP --slot staging\n\n# Swap slots\naz functionapp deployment slot swap \\\n    --name $FUNCTION_APP \\\n    --resource-group $RESOURCE_GROUP \\\n    --slot staging \\\n    --target-slot production\n```\n\n---\n\n## CI/CD with GitHub Actions\n\nAutomate deployments with GitHub Actions.\n\n> **Important**: Before creating CI/CD pipelines, get CI/CD guidance with `deploy_pipeline_guidance_get`.\n\n`.github/workflows/azure-functions.yml`:\n```yaml\nname: Deploy Azure Functions\n\non:\n  push:\n    branches: [main]\n\nenv:\n  AZURE_FUNCTIONAPP_NAME: 'myFunctionApp'\n  AZURE_FUNCTIONAPP_PACKAGE_PATH: '.'\n  NODE_VERSION: '20.x'\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n\n      - name: Install dependencies\n        run: npm ci\n        working-directory: ${{ env.AZURE_FUNCTIONAPP_PACKAGE_PATH }}\n\n      - name: Build (if TypeScript)\n        run: npm run build --if-present\n        working-directory: ${{ env.AZURE_FUNCTIONAPP_PACKAGE_PATH }}\n\n      - name: Azure Login\n        uses: azure/login@v2\n        with:\n          creds: ${{ secrets.AZURE_CREDENTIALS }}\n\n      - name: Deploy to Azure Functions\n        uses: Azure/functions-action@v1\n        with:\n          app-name: ${{ env.AZURE_FUNCTIONAPP_NAME }}\n          package: ${{ env.AZURE_FUNCTIONAPP_PACKAGE_PATH }}\n```\n\n**Create Azure credentials secret:**\n```bash\naz ad sp create-for-rbac --name \"github-actions-sp\" \\\n    --role contributor \\\n    --scopes /subscriptions/{subscription-id}/resourceGroups/{resource-group} \\\n    --sdk-auth\n```\n\n---\n\n## Durable Functions\n\nFor long-running orchestrations and stateful workflows:\n\n```javascript\n// Orchestrator\nconst df = require('durable-functions');\n\nmodule.exports = df.orchestrator(function* (context) {\n    const result1 = yield context.df.callActivity('Step1', input);\n    const result2 = yield context.df.callActivity('Step2', result1);\n    return result2;\n});\n```\n\n**Patterns:**\n- Function chaining\n- Fan-out/fan-in\n- Async HTTP APIs\n- Human interaction\n- Aggregator\n\n---\n\n## Best Practices\n\n| Practice | Description |\n|----------|-------------|\n| **Keep functions small** | Single-purpose functions are easier to test and maintain |\n| **Implement idempotency** | At-least-once triggers may execute multiple times |\n| **Use managed identity** | Prefer managed identity over connection strings for secure resource access |\n| **Configure timeout** | Set `functionTimeout` in host.json (default 5 min for Consumption) |\n| **Use Application Insights** | Enable for monitoring, tracing, and diagnostics |\n| **Secure HTTP functions** | Use `authLevel: 'function'` or `'admin'` for non-public endpoints |\n| **Environment variables** | Store secrets in App Settings, not in code |\n| **Cold start optimization** | Use Premium plan or keep-alive pings for latency-sensitive apps |\n| **Use Key Vault** | Store secrets securely with Key Vault references |\n| **Configure retry policies** | Set appropriate retry behavior for triggers |\n\n---\n\n## Quick Start Checklist\n\n### Setup\n- [ ] Azure subscription created\n- [ ] Azure CLI installed (`az --version`)\n- [ ] Azure CLI authenticated (`az login`)\n- [ ] Azure Functions Core Tools installed (`func --version`)\n- [ ] Node.js/Python/dotnet installed (based on runtime)\n\n### Development\n- [ ] Initialize project with `func init`\n- [ ] Create functions with `func new`\n- [ ] Configure `host.json` settings\n- [ ] Test locally with `func start`\n\n### Deployment\n- [ ] Create resource group\n- [ ] Create storage account\n- [ ] Create Function App\n- [ ] Deploy with `func azure functionapp publish`\n- [ ] Configure app settings\n- [ ] Verify function URLs\n\n### Monitoring\n- [ ] Enable Application Insights\n- [ ] Stream logs with `func azure functionapp logstream`\n- [ ] Set up alerts for failures\n\n---\n\n## Troubleshooting\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| **func not found** | Command not recognized | Install Azure Functions Core Tools: `npm install -g azure-functions-core-tools@4` |\n| **Storage error** | Function app won't start | Verify `AzureWebJobsStorage` connection string is valid |\n| **404 on function** | Function not found | Check function is exported correctly and route is configured |\n| **Cold start delays** | First request slow | Use Premium plan or implement warm-up triggers |\n| **Timeout** | Function exceeds limit | Increase `functionTimeout` in host.json or use Durable Functions |\n| **Binding errors** | Extension not loaded | Run `func extensions install` to install required extensions |\n| **Deploy fails** | Publish error | Ensure function app exists and CLI is authenticated |\n| **Runtime mismatch** | Version conflict | Verify `FUNCTIONS_EXTENSION_VERSION` matches project |\n| **Execution limits** | Flex Consumption has 30 min timeout | Use Premium or Dedicated plan for longer executions |\n| **Scaling delays** | Cold starts on first request | Flex Consumption supports always-ready instances |\n\n**Debug commands:**\n```bash\nfunc start --verbose                     # Local debugging\nfunc azure functionapp logstream $APP    # Live logs\naz functionapp show --name $APP          # App details\naz functionapp config show --name $APP   # Configuration\naz functionapp list --output table       # List all function apps\n```\n\n---\n\n## Azure Resources\n\n| Resource Type | Purpose | API Version |\n|--------------|---------|-------------|\n| `Microsoft.Web/sites` | Function App | 2023-12-01 |\n| `Microsoft.Storage/storageAccounts` | Required storage | 2023-01-01 |\n| `Microsoft.Web/serverfarms` | App Service Plan | 2023-12-01 |\n| `Microsoft.Insights/components` | Application Insights | 2020-02-02 |\n\n---\n\n## MCP Server Tools\n\nUse MCP tools to **query** existing resources:\n\n- `azure__functionapp` with command `functionapp_list` - List function apps\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n---\n\n## MCP Server Templates\n\n> **See \"Template Selection Decision Tree\" above for deployment.** This section provides additional context and GitHub links.\n\n**Browse:** [Awesome AZD MCP](https://azure.github.io/awesome-azd/?tags=msft&tags=functions&name=mcp) | [Remote MCP Docs](https://aka.ms/remote-mcp)\n\n**GitHub Repositories:**\n- Python: [remote-mcp-functions-python](https://github.com/Azure-Samples/remote-mcp-functions-python)\n- TypeScript: [remote-mcp-functions-typescript](https://github.com/Azure-Samples/remote-mcp-functions-typescript)\n- C#: [remote-mcp-functions-dotnet](https://github.com/Azure-Samples/remote-mcp-functions-dotnet)\n- Java: [remote-mcp-functions-java](https://github.com/Azure-Samples/remote-mcp-functions-java)\n\n---\n\n## Integration Templates\n\n### Full-Stack (SWA + Functions)\n| Stack | Sample |\n|-------|--------|\n| C# + SQL | [todo-csharp-sql-swa-func](https://github.com/Azure-Samples/todo-csharp-sql-swa-func) |\n| Node + MongoDB | [todo-nodejs-mongo-swa-func](https://github.com/azure-samples/todo-nodejs-mongo-swa-func) |\n\n### Database & AI Templates\n| Service | Templates |\n|---------|-----------|\n| Cosmos DB | [Awesome AZD Cosmos](https://azure.github.io/awesome-azd/?tags=functions&name=cosmos) |\n| Azure SQL | [Awesome AZD SQL](https://azure.github.io/awesome-azd/?tags=functions&name=sql) |\n| OpenAI/AI Foundry | [Awesome AZD AI](https://azure.github.io/awesome-azd/?tags=functions&name=ai) |\n\n### Trigger & Binding Quick Reference\n| Service | Trigger | Input | Output |\n|---------|---------|-------|--------|\n| Cosmos DB | âœ… | âœ… | âœ… |\n| Azure SQL | âœ… | âœ… | âœ… |\n| Storage Blob/Queue | âœ… | âœ… | âœ… |\n| Service Bus | âœ… | âŒ | âœ… |\n| Event Grid/Hubs | âœ… | âŒ | âœ… |\n| Azure OpenAI | âŒ | âœ… | âœ… |\n| SignalR | âœ… | âœ… | âœ… |\n\n---\n\n## Additional Resources\n\n- [Azure Functions Documentation](https://learn.microsoft.com/azure/azure-functions/)\n- [Azure Functions Core Tools](https://learn.microsoft.com/azure/azure-functions/functions-run-local)\n- [Triggers and Bindings](https://learn.microsoft.com/azure/azure-functions/functions-triggers-bindings)\n- [Durable Functions](https://learn.microsoft.com/azure/azure-functions/durable/)\n",
        "plugin/skills/azure-keyvault-expiration-audit/SKILL.md": "---\nname: azure-keyvault-expiration-audit\ndescription: Proactively monitor and audit Azure Key Vault resources for expired or soon-to-expire keys, secrets, and certificates. Use this skill for security compliance, preventing service disruptions, and maintaining key vault hygiene through expiration tracking and reporting.\n---\n\n# Key Vault Expiration Audit & Compliance\n\nAutomated auditing of Azure Key Vault resources to identify expired or expiring keys, secrets, and certificates before they cause service disruptions.\n\n## Skill Activation Triggers\n\n**Use this skill immediately when the user asks to:**\n- \"Show me expired certificates/keys/secrets in my Key Vault\"\n- \"Check what's expiring in the next 30 days\"\n- \"Audit my Key Vault for compliance\"\n- \"Find secrets without expiration dates\"\n- \"Generate a security report for my Key Vault\"\n- \"Which keys have expired in production?\"\n- \"Check certificate expiration dates\"\n\n**Key Indicators:**\n- Mentions \"expired\", \"expiring\", or \"expiration\" with Key Vault\n- Compliance audit or security review requests\n- Questions about Key Vault resource lifecycle\n- Requests to find resources without expiration dates\n- Pre-deployment security checks\n\n## Overview\n\nThis skill monitors Azure Key Vault resources (keys, secrets, certificates) for expiration issues. It helps prevent service disruptions by identifying:\n- **Expired resources** causing active problems\n- **Expiring soon** (within customizable days threshold)\n- **Missing expiration dates** (security risk)\n- **Disabled resources** needing cleanup\n\n## Core Workflow\n\n1. **List Resources**: Enumerate keys, secrets, and certificates in target vault(s)\n2. **Get Details**: Retrieve expiration metadata for each resource\n3. **Analyze Status**: Compare expiration dates against current date and threshold\n4. **Generate Report**: Organize findings by priority with actionable recommendations\n\n## Audit Patterns\n\n### Pattern 1: Single Vault Quick Scan\nCheck one Key Vault for all expiration issues with configurable day threshold (default: 30 days).\n\n**Tools**: `keyvault_key_list`, `keyvault_key_get`, `keyvault_secret_list`, `keyvault_secret_get`, `keyvault_certificate_list`, `keyvault_certificate_get`\n\n### Pattern 2: Multi-Vault Compliance Report\nScan multiple vaults across subscription for comprehensive security review.\n\n**Use for**: Quarterly audits, organization-wide compliance checks\n\n### Pattern 3: Resource Type Focus\nAudit only keys, secrets, OR certificates when specific resource type is mentioned.\n\n**Use for**: Certificate renewal planning, secret rotation tracking\n\n### Pattern 4: Emergency Expired Finder\nQuick scan for already-expired resources (negative days) to troubleshoot active incidents.\n\n**Use for**: Production issues, authentication failures\n\n## Key Data Fields\n\nWhen retrieving resource details, analyze these fields:\n- **expiresOn**: Expiration timestamp (null = no expiration set - security risk!)\n- **enabled**: Resource is active (false = disabled/inactive)\n- **notBefore**: When resource becomes valid\n- **createdOn/updatedOn**: For tracking resource age and last rotation\n- **subject/issuer**: Certificate-specific metadata\n\n## Report Format\n\nOrganize findings into:\n- **Summary Statistics**: Total count, expired count, expiring count, no-expiration count per resource type\n- **Critical Issues**: Expired resources requiring immediate action\n- **Warnings**: Expiring within threshold (e.g., 30 days)\n- **Risks**: Resources without expiration dates\n- **Recommendations**: Set expiration policies, rotate credentials, remove disabled items\n\n## Remediation Priority\n\n**ðŸ”´ Critical** - Expired (< 0 days): Rotate immediately  \n**ðŸŸ  High** - Expiring 0-7 days: Schedule rotation within 24 hours  \n**ðŸŸ¡ Medium** - Expiring 8-30 days: Plan rotation within 1 week  \n**ðŸŸ¡ Medium** - No expiration set: Apply expiration policy  \n**ðŸŸ¢ Low** - Active (> 30 days): Monitor on regular schedule\n\n## Best Practices\n\n- Run weekly audits to catch issues early\n- All resources should have expiration dates (Azure Policy recommendation)\n- Configure Azure Event Grid for 30-day advance notifications\n- Rotation schedule: Secrets every 60-90 days, Keys annually, Certificates per CA requirements (max 1 year)\n- Prioritize production Key Vaults over dev/test\n- Automate rotation with Azure Functions or Logic Apps\n\n## MCP Tools Used\n\n| Tool | Purpose |\n|------|---------|\n| `keyvault_key_list` | List all keys in a vault |\n| `keyvault_key_get` | Get key details including expiration |\n| `keyvault_secret_list` | List all secrets in a vault |\n| `keyvault_secret_get` | Get secret details including expiration |\n| `keyvault_certificate_list` | List all certificates in a vault |\n| `keyvault_certificate_get` | Get certificate details including expiration |\n\n**Required**: `vault` (Key Vault name)  \n**Optional**: `subscription`, `tenant`\n\n## Fallback Strategy: Azure CLI Commands\n\nIf Azure MCP Key Vault tools fail, timeout, or are unavailable, use Azure CLI commands as fallback.\n\n### CLI Command Reference\n\n| Operation | Azure CLI Command |\n|-----------|-------------------|\n| List secrets | `az keyvault secret list --vault-name <vault-name>` |\n| Get secret details | `az keyvault secret show --vault-name <vault-name> --name <secret-name>` |\n| List keys | `az keyvault key list --vault-name <vault-name>` |\n| Get key details | `az keyvault key show --vault-name <vault-name> --name <key-name>` |\n| List certificates | `az keyvault certificate list --vault-name <vault-name>` |\n| Get certificate details | `az keyvault certificate show --vault-name <vault-name> --name <cert-name>` |\n\n### When to Fallback\n\nSwitch to Azure CLI when:\n- MCP tool returns timeout error\n- MCP tool returns \"service unavailable\" or connection errors\n- MCP tool takes longer than 30 seconds to respond\n- Empty response when vault is known to have resources\n\n## Common Issues\n\n- **Access Denied**: Verify RBAC permissions (Key Vault Reader + data plane access)\n- **Vault Not Found**: Check vault name and subscription context\n- **Null expiresOn**: Resource has no expiration (security risk - requires policy)\n- **Time zones**: All timestamps are UTC",
        "plugin/skills/azure-kusto/SKILL.md": "---\nname: azure-kusto\ndescription: Query and analyze big data in Azure Data Explorer (Kusto) using KQL. Use this skill for log analytics, time series analysis, telemetry insights, IoT data exploration, and real-time data investigation across large datasets with sub-second query performance.\n---\n\n# Azure Data Explorer (Kusto) Query & Analytics\n\nExecute KQL queries and manage Azure Data Explorer resources for fast, scalable big data analytics on log, telemetry, and time series data.\n\n## Skill Activation Triggers\n\n**Use this skill immediately when the user asks to:**\n- \"Query my Kusto database for [data pattern]\"\n- \"Show me events in the last hour from Azure Data Explorer\"\n- \"Analyze logs in my ADX cluster\"\n- \"Run a KQL query on [database]\"\n- \"What tables are in my Kusto database?\"\n- \"Show me the schema for [table]\"\n- \"List my Azure Data Explorer clusters\"\n- \"Aggregate telemetry data by [dimension]\"\n- \"Create a time series chart from my logs\"\n\n**Key Indicators:**\n- Mentions \"Kusto\", \"Azure Data Explorer\", \"ADX\", or \"KQL\"\n- Log analytics or telemetry analysis requests\n- Time series data exploration\n- IoT data analysis queries\n- SIEM or security analytics tasks\n- Requests for data aggregation on large datasets\n- Performance monitoring or APM queries\n\n## Overview\n\nThis skill enables querying and managing Azure Data Explorer (Kusto), a fast and highly scalable data exploration service optimized for log and telemetry data. Azure Data Explorer provides sub-second query performance on billions of records using the Kusto Query Language (KQL).\n\nKey capabilities:\n- **Query Execution**: Run KQL queries against massive datasets\n- **Schema Exploration**: Discover tables, columns, and data types\n- **Resource Management**: List clusters and databases\n- **Analytics**: Aggregations, time series, anomaly detection, machine learning\n\n## Core Workflow\n\n1. **Discover Resources**: List available clusters and databases in subscription\n2. **Explore Schema**: Retrieve table structures to understand data model\n3. **Query Data**: Execute KQL queries for analysis, filtering, aggregation\n4. **Analyze Results**: Process query output for insights and reporting\n\n## Query Patterns\n\n### Pattern 1: Basic Data Retrieval\nFetch recent records from a table with simple filtering.\n\n**Example KQL**:\n```kql\nEvents\n| where Timestamp > ago(1h)\n| take 100\n```\n\n**Use for**: Quick data inspection, recent event retrieval\n\n### Pattern 2: Aggregation Analysis\nSummarize data by dimensions for insights and reporting.\n\n**Example KQL**:\n```kql\nEvents\n| summarize count() by EventType, bin(Timestamp, 1h)\n| order by count_ desc\n```\n\n**Use for**: Event counting, distribution analysis, top-N queries\n\n### Pattern 3: Time Series Analytics\nAnalyze data over time windows for trends and patterns.\n\n**Example KQL**:\n```kql\nTelemetry\n| where Timestamp > ago(24h)\n| summarize avg(ResponseTime), percentiles(ResponseTime, 50, 95, 99) by bin(Timestamp, 5m)\n| render timechart\n```\n\n**Use for**: Performance monitoring, trend analysis, anomaly detection\n\n### Pattern 4: Join and Correlation\nCombine multiple tables for cross-dataset analysis.\n\n**Example KQL**:\n```kql\nEvents\n| where EventType == \"Error\"\n| join kind=inner (\n    Logs\n    | where Severity == \"Critical\"\n) on CorrelationId\n| project Timestamp, EventType, LogMessage, Severity\n```\n\n**Use for**: Root cause analysis, correlated event tracking\n\n### Pattern 5: Schema Discovery\nExplore table structure before querying.\n\n**Tools**: `kusto_table_schema_get`\n\n**Use for**: Understanding data model, query planning\n\n## Key Data Fields\n\nWhen executing queries, common field patterns:\n- **Timestamp**: Time of event (datetime) - use `ago()`, `between()`, `bin()` for time filtering\n- **EventType/Category**: Classification field for grouping\n- **CorrelationId/SessionId**: For tracing related events\n- **Severity/Level**: For filtering by importance\n- **Dimensions**: Custom properties for grouping and filtering\n\n## Result Format\n\nQuery results include:\n- **Columns**: Field names and data types\n- **Rows**: Data records matching query\n- **Statistics**: Row count, execution time, resource utilization\n- **Visualization**: Chart rendering hints (timechart, barchart, etc.)\n\n## KQL Best Practices\n\n**ðŸŸ¢ Performance Optimized:**\n- Filter early: Use `where` before joins and aggregations\n- Limit result size: Use `take` or `limit` to reduce data transfer\n- Time filters: Always filter by time range for time series data\n- Indexed columns: Filter on indexed columns first\n\n**ðŸ”µ Query Patterns:**\n- Use `summarize` for aggregations instead of `count()` alone\n- Use `bin()` for time bucketing in time series\n- Use `project` to select only needed columns\n- Use `extend` to add calculated fields\n\n**ðŸŸ¡ Common Functions:**\n- `ago(timespan)`: Relative time (ago(1h), ago(7d))\n- `between(start .. end)`: Range filtering\n- `startswith()`, `contains()`, `matches regex`: String filtering\n- `parse`, `extract`: Extract values from strings\n- `percentiles()`, `avg()`, `sum()`, `max()`, `min()`: Aggregations\n\n## Best Practices\n\n- Always include time range filters to optimize query performance\n- Use `take` or `limit` for exploratory queries to avoid large result sets\n- Leverage `summarize` for aggregations instead of client-side processing\n- Store frequently-used queries as functions in the database\n- Use materialized views for repeated aggregations\n- Monitor query performance and resource consumption\n- Apply data retention policies to manage storage costs\n- Use streaming ingestion for real-time analytics (< 1 second latency)\n- Integrate with Azure Monitor for operational insights\n\n## MCP Tools Used\n\n| Tool | Purpose |\n|------|---------|\n| `kusto_cluster_list` | List all Azure Data Explorer clusters in a subscription |\n| `kusto_database_list` | List all databases in a specific Kusto cluster |\n| `kusto_query` | Execute KQL queries against a Kusto database |\n| `kusto_table_schema_get` | Retrieve schema information for a specific table |\n\n**Required Parameters**:\n- `subscription`: Azure subscription ID or display name\n- `cluster`: Kusto cluster name (e.g., \"mycluster\")\n- `database`: Database name\n- `query`: KQL query string (for query operations)\n- `table`: Table name (for schema operations)\n\n**Optional Parameters**:\n- `resource-group`: Resource group name (for listing operations)\n- `tenant`: Azure AD tenant ID\n\n## Fallback Strategy: Azure CLI Commands\n\nIf Azure MCP Kusto tools fail, timeout, or are unavailable, use Azure CLI commands as fallback.\n\n### CLI Command Reference\n\n| Operation | Azure CLI Command |\n|-----------|-------------------|\n| List clusters | `az kusto cluster list --resource-group <rg-name>` |\n| List databases | `az kusto database list --cluster-name <cluster> --resource-group <rg-name>` |\n| Show cluster | `az kusto cluster show --name <cluster> --resource-group <rg-name>` |\n| Show database | `az kusto database show --cluster-name <cluster> --database-name <db> --resource-group <rg-name>` |\n\n### KQL Query via Azure CLI\n\nFor queries, use the Kusto REST API or direct cluster URL:\n```bash\naz rest --method post \\\n  --url \"https://<cluster>.<region>.kusto.windows.net/v1/rest/query\" \\\n  --body \"{ \\\"db\\\": \\\"<database>\\\", \\\"csl\\\": \\\"<kql-query>\\\" }\"\n```\n\n### When to Fallback\n\nSwitch to Azure CLI when:\n- MCP tool returns timeout error (queries > 60 seconds)\n- MCP tool returns \"service unavailable\" or connection errors\n- Authentication failures with MCP tools\n- Empty response when database is known to have data\n\n## Common Issues\n\n- **Access Denied**: Verify database permissions (Viewer role minimum for queries)\n- **Query Timeout**: Optimize query with time filters, reduce result set, or increase timeout\n- **Syntax Error**: Validate KQL syntax - common issues: missing pipes, incorrect operators\n- **Empty Results**: Check time range filters (may be too restrictive), verify table name\n- **Cluster Not Found**: Check cluster name format (exclude \".kusto.windows.net\" suffix)\n- **High CPU Usage**: Query too broad - add filters, reduce time range, limit aggregations\n- **Ingestion Lag**: Streaming data may have 1-30 second delay depending on ingestion method\n\n## Use Cases\n\n- **Log Analytics**: Application logs, system logs, audit logs\n- **IoT Analytics**: Sensor data, device telemetry, real-time monitoring\n- **Security Analytics**: SIEM data, threat detection, security event correlation\n- **APM**: Application performance metrics, user behavior, error tracking\n- **Business Intelligence**: Clickstream analysis, user analytics, operational KPIs",
        "plugin/skills/azure-mcp/SKILL.md": "---\nname: azure-mcp\ndescription: Set up and use the Azure MCP server for direct access to Azure services through structured tools\n---\n\n# Azure MCP Server\n\n## Overview\n\nThe Azure MCP server provides direct access to Azure services through structured tools, enabling richer data access than CLI commands alone.\n\n## Configuration\n\nAdd to your MCP configuration (`.mcp.json` or settings):\n\n```json\n{\n  \"mcpServers\": {\n    \"azure\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure/mcp@latest\", \"server\", \"start\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n## Authentication\n\nThe Azure MCP server uses Azure CLI credentials by default.\n\n### Login First\n\n```bash\n# Interactive login\naz login\n\n# Device code flow (headless/remote)\naz login --use-device-code\n\n# Set subscription\naz account set --subscription \"Subscription Name\"\n```\n\n### Service Principal (CI/CD)\n\nSet environment variables:\n- `AZURE_TENANT_ID`\n- `AZURE_CLIENT_ID`\n- `AZURE_CLIENT_SECRET`\n\n## Enabling the MCP Server\n\n### In Claude Code\n\n1. Run `/mcp` to view MCP servers\n2. Enable the Azure MCP server\n3. Or run `/azure:setup` for guided configuration\n\n### Verify Connection\n\nAfter enabling, verify tools are available:\n- Check `/azure:status`\n- Try `azure__subscription_list` to list subscriptions\n\n## Benefits Over CLI\n\n| Feature | MCP Server | CLI |\n|---------|------------|-----|\n| Structured data | JSON responses | Text parsing needed |\n| Pagination | Automatic | Manual continuation |\n| Schema info | Built-in | Separate queries |\n| Authentication | Managed | Manual |\n\n## Troubleshooting\n\n### Server Not Starting\n\n1. Ensure Node.js 18+ is installed\n2. Check npx is available\n3. Verify network access to npm registry\n\n### Authentication Errors\n\n1. Run `az login` to refresh credentials\n2. Check subscription is set: `az account show`\n3. Verify permissions on target resources\n\n### Tool Not Available\n\n1. Enable the Azure MCP server first\n2. Check server is connected in `/mcp`\n3. Some tools require specific permissions\n\n## Fallback to CLI\n\nWhen MCP is unavailable, use Azure CLI directly:\n\n```bash\n# List subscriptions\naz account list --output table\n\n# List resources\naz resource list -g RESOURCE_GROUP --output table\n```\n\nSee `cli/cheatsheet.md` for common CLI commands.\n\n---\n\n# Tool Reference\n\nThe Azure MCP server exposes two types of tools:\n- **Standalone tools**: Called directly (e.g., `azure__subscription_list`)\n- **Hierarchical tools**: Called with a `command` parameter (e.g., `azure__deploy` with command `deploy_plan_get`)\n\n## Core Operations\n\n| Tool | Description |\n|------|-------------|\n| `azure__subscription_list` | List all subscriptions |\n| `azure__group_list` | List resource groups in subscription |\n\n## Storage\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__storage` | `storage_account_list` | List storage accounts |\n| `azure__storage` | `storage_container_list` | List containers in account |\n| `azure__storage` | `storage_blob_list` | List blobs in container |\n| `azure__storage` | `storage_blob_get` | Download blob content |\n| `azure__storage` | `storage_blob_put` | Upload blob content |\n\n## SQL Database\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__sql` | `sql_server_list` | List SQL servers |\n| `azure__sql` | `sql_database_list` | List databases on server |\n| `azure__sql` | `sql_firewall_list` | List firewall rules |\n\n## Cosmos DB\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__cosmos` | `cosmos_account_list` | List Cosmos DB accounts |\n| `azure__cosmos` | `cosmos_database_list` | List databases in account |\n| `azure__cosmos` | `cosmos_container_list` | List containers |\n\n## Redis\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__redis` | `redis_cache_list` | List Redis caches |\n\n## Key Vault\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__keyvault` | `keyvault_list` | List Key Vaults |\n| `azure__keyvault` | `keyvault_secret_list` | List secrets in vault |\n| `azure__keyvault` | `keyvault_secret_get` | Get secret value |\n| `azure__keyvault` | `keyvault_key_list` | List keys |\n| `azure__keyvault` | `keyvault_certificate_list` | List certificates |\n\n## RBAC\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__role` | `role_assignment_list` | List role assignments |\n| `azure__role` | `role_definition_list` | List role definitions |\n\n## App Service\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__appservice` | `appservice_webapp_list` | List web apps |\n| `azure__appservice` | `appservice_webapp_get` | Get app details |\n| `azure__appservice` | `appservice_plan_list` | List App Service plans |\n\n## Functions\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__functionapp` | `functionapp_list` | List function apps |\n\n## AKS\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__aks` | `aks_cluster_list` | List AKS clusters |\n| `azure__aks` | `aks_nodepool_list` | List node pools |\n\n## Container Apps\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__appservice` | `containerapp_list` | List container apps |\n\n## AI Search\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__search` | `search_index_list` | List search indexes |\n| `azure__search` | `search_index_get` | Get index details |\n| `azure__search` | `search_query` | Query search index |\n\n## Speech\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__speech` | `speech_transcribe` | Speech to text |\n| `azure__speech` | `speech_synthesize` | Text to speech |\n\n## Foundry\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__foundry` | `foundry_model_list` | List AI models |\n| `azure__foundry` | `foundry_deployment_list` | List deployments |\n| `azure__foundry` | `foundry_agent_list` | List AI agents |\n\n## Monitoring\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__monitor` | `monitor_metrics_query` | Query metrics |\n| `azure__monitor` | `monitor_logs_query` | Query logs with KQL |\n| `azure__applicationinsights` | `applicationinsights_component_list` | List App Insights |\n| `azure__resourcehealth` | *(various)* | Check resource health and availability |\n| `azure__applens` | *(conversational)* | AI-powered diagnostics for troubleshooting Azure resource issues |\n\n## Deployment & Infrastructure\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__deploy` | `deploy_plan_get` | Generate a deployment plan for Azure infrastructure |\n| `azure__deploy` | `deploy_iac_rules_get` | Get IaC (Bicep/Terraform) guidelines |\n| `azure__deploy` | `deploy_app_logs_get` | Fetch logs from deployed apps |\n| `azure__deploy` | `deploy_pipeline_guidance_get` | Get CI/CD pipeline guidance |\n| `azure__deploy` | `deploy_architecture_diagram_generate` | Generate architecture diagrams |\n| `azure__bicepschema` | *(schema queries)* | Get Bicep resource type schemas |\n| `azure__quota` | `quota_region_availability_list` | Check regional quota and availability |\n\n## Azure Developer CLI (azd)\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__azd` | `validate_azure_yaml` | Validate azure.yaml against official JSON schema |\n| `azure__azd` | `discovery_analysis` | Analyze application components for azd migration |\n| `azure__azd` | `architecture_planning` | Select Azure services for discovered components |\n| `azure__azd` | `azure_yaml_generation` | Generate azure.yaml configuration |\n| `azure__azd` | `docker_generation` | Generate Dockerfiles for Container Apps/AKS |\n| `azure__azd` | `infrastructure_generation` | Generate Bicep templates |\n| `azure__azd` | `iac_generation_rules` | Get Bicep compliance rules and best practices |\n| `azure__azd` | `project_validation` | Comprehensive validation before deployment |\n| `azure__azd` | `error_troubleshooting` | Diagnose and troubleshoot azd errors |\n\n## Best Practices\n\n| Tool | Command | Description |\n|------|---------|-------------|\n| `azure__get_azure_bestpractices` | *(various)* | Get Azure best practices for code generation and deployment |\n\n## Documentation\n\n| Tool | Description |\n|------|-------------|\n| `azure__documentation` | Search official Microsoft/Azure documentation |\n\n## CLI Helpers\n\n| Tool | Description |\n|------|-------------|\n| `azure__extension_cli_install` | Get CLI installation instructions |\n| `azure__extension_cli_generate` | Generate Azure CLI commands |\n| `azure__extension_azqr` | Run Azure Quick Review for compliance/security reports |\n\n### CLI Install Tool\n\n```\nParameters:\n  - cli-type: \"az\" | \"azd\" | \"func\"\n```\n\n### CLI Generate Tool\n\n```\nParameters:\n  - intent: Description of what you want to do\n  - cli-type: \"az\"\n```\n",
        "plugin/skills/azure-networking/SKILL.md": "---\nname: azure-networking\ndescription: Azure Networking Services including Virtual Networks, Private Endpoints, Load Balancers, Application Gateway, Front Door, and DNS. Covers hub-spoke topology, private endpoint patterns, and network security layers.\n---\n\n# Azure Networking Services\n\n## Services\n\n| Service | Use When | MCP Tools | CLI |\n|---------|----------|-----------|-----|\n| Virtual Network | Private networking, subnets | - | `az network vnet` |\n| Private Endpoints | Private PaaS access | - | `az network private-endpoint` |\n| Load Balancer | Layer 4 load balancing | - | `az network lb` |\n| Application Gateway | Layer 7 load balancing, WAF | - | `az network application-gateway` |\n| Front Door | Global load balancing, CDN | - | `az afd` |\n| DNS | Domain name resolution | - | `az network dns` |\n\n## Common Patterns\n\n### Hub-Spoke Topology\n\n```\nHub VNet\nâ”œâ”€â”€ Azure Firewall\nâ”œâ”€â”€ VPN/ExpressRoute Gateway\nâ”œâ”€â”€ Bastion Host\nâ””â”€â”€ Central services\n\nSpoke VNets (peered to hub)\nâ”œâ”€â”€ Application Spoke\nâ”œâ”€â”€ Data Spoke\nâ””â”€â”€ Management Spoke\n```\n\n### Private Endpoint Pattern\n\nConnect to PaaS services privately:\n\n1. Create private endpoint in your VNet\n2. Disable public access on PaaS resource\n3. Configure private DNS zone\n4. Access service via private IP\n\n## CLI Reference\n\n```bash\n# Virtual Networks\naz network vnet list --output table\naz network vnet create -g RG -n VNET --address-prefix 10.0.0.0/16\n\n# Subnets\naz network vnet subnet list --vnet-name VNET -g RG --output table\n\n# Private Endpoints\naz network private-endpoint list --output table\n\n# NSGs\naz network nsg list --output table\naz network nsg rule list --nsg-name NSG -g RG --output table\n\n# Load Balancers\naz network lb list --output table\n```\n\n## Security Layers\n\n| Layer | Service | Purpose |\n|-------|---------|---------|\n| 4 | NSG | IP/port filtering |\n| 7 | Azure Firewall | Application rules, threat intel |\n| 7 | WAF | Web application protection |\n| Edge | DDoS Protection | Attack mitigation |\n\n## Service Details\n\nFor deep documentation on specific services:\n\n- VNet design and peering -> [Virtual Network documentation](https://learn.microsoft.com/azure/virtual-network/virtual-networks-overview)\n- Private endpoints setup -> [Private Link documentation](https://learn.microsoft.com/azure/private-link/private-endpoint-overview)\n- Load balancing options -> [Load balancing options overview](https://learn.microsoft.com/azure/architecture/guide/technology-choices/load-balancing-overview)\n",
        "plugin/skills/azure-nodejs-production/SKILL.md": "---\nname: azure-nodejs-production\ndescription: Configure Express/Node.js applications for production deployment on Azure. Covers trust proxy settings, cookie configuration, health checks, port binding, and Dockerfile best practices for Container Apps and App Service.\n---\n\n# Express/Node.js Production Configuration for Azure\n\n## Overview\n\nWhen deploying Express/Node.js apps to Azure (Container Apps, App Service), you MUST configure production settings that aren't needed locally.\n\n## Required Production Settings\n\n### 1. Trust Proxy (CRITICAL)\n\nAzure load balancers and reverse proxies sit in front of your app. Without trust proxy, you'll get:\n- Wrong client IP addresses\n- HTTPS detection failures\n- Cookie issues\n\n```javascript\n// app.js or server.js\nconst app = express();\n\n// REQUIRED for Azure - trust the Azure load balancer\napp.set('trust proxy', 1);  // Trust first proxy\n\n// Or trust all proxies (less secure but simpler)\napp.set('trust proxy', true);\n```\n\n### 2. Cookie Configuration\n\nAzure's infrastructure requires specific cookie settings:\n\n```javascript\n// Session configuration\napp.use(session({\n  secret: process.env.SESSION_SECRET,\n  resave: false,\n  saveUninitialized: false,\n  cookie: {\n    secure: process.env.NODE_ENV === 'production',  // HTTPS only in prod\n    sameSite: 'lax',  // Required for Azure\n    httpOnly: true,\n    maxAge: 24 * 60 * 60 * 1000  // 24 hours\n  }\n}));\n```\n\n**Key settings:**\n- `sameSite: 'lax'` - Required for cookies to work through Azure's proxy\n- `secure: true` - Only in production (HTTPS)\n- `httpOnly: true` - Prevent XSS attacks\n\n### 3. Health Check Endpoint\n\nAzure Container Apps and App Service check your app's health:\n\n```javascript\n// Add health check endpoint\napp.get('/health', (req, res) => {\n  res.status(200).json({ status: 'healthy', timestamp: new Date().toISOString() });\n});\n\n// Or minimal version\napp.get('/health', (req, res) => res.sendStatus(200));\n```\n\n**Configure in Container Apps:**\n```bash\naz containerapp update \\\n  --name APP \\\n  --resource-group RG \\\n  --health-probe-path /health \\\n  --health-probe-interval 30\n```\n\n### 4. Port Configuration\n\nAzure sets the port via environment variable:\n\n```javascript\n// Listen on Azure's port or default to 3000\nconst port = process.env.PORT || process.env.WEBSITES_PORT || 3000;\napp.listen(port, '0.0.0.0', () => {\n  console.log(`Server running on port ${port}`);\n});\n```\n\n**Important:** Bind to `0.0.0.0`, not `localhost` or `127.0.0.1`.\n\n### 5. Environment Detection\n\n```javascript\nconst isProduction = process.env.NODE_ENV === 'production';\nconst isAzure = process.env.WEBSITE_SITE_NAME || process.env.CONTAINER_APP_NAME;\n\nif (isProduction || isAzure) {\n  app.set('trust proxy', 1);\n  // Enable production-only settings\n}\n```\n\n## Complete Production Configuration\n\n```javascript\n// app.js - Production-ready Express configuration for Azure\nconst express = require('express');\nconst session = require('express-session');\n\nconst app = express();\n\n// Environment\nconst isProduction = process.env.NODE_ENV === 'production';\n\n// Trust Azure load balancer\nif (isProduction) {\n  app.set('trust proxy', 1);\n}\n\n// Security headers\napp.use((req, res, next) => {\n  res.setHeader('X-Content-Type-Options', 'nosniff');\n  res.setHeader('X-Frame-Options', 'DENY');\n  next();\n});\n\n// JSON parsing\napp.use(express.json());\napp.use(express.urlencoded({ extended: true }));\n\n// Session (if using)\napp.use(session({\n  secret: process.env.SESSION_SECRET || 'dev-secret-change-in-prod',\n  resave: false,\n  saveUninitialized: false,\n  cookie: {\n    secure: isProduction,\n    sameSite: 'lax',\n    httpOnly: true,\n    maxAge: 24 * 60 * 60 * 1000\n  }\n}));\n\n// Health check\napp.get('/health', (req, res) => {\n  res.status(200).json({ status: 'ok' });\n});\n\n// Your routes here\napp.get('/', (req, res) => {\n  res.json({ message: 'Hello from Azure!' });\n});\n\n// Error handler\napp.use((err, req, res, next) => {\n  console.error(err.stack);\n  res.status(500).json({ error: isProduction ? 'Internal error' : err.message });\n});\n\n// Start server\nconst port = process.env.PORT || 3000;\napp.listen(port, '0.0.0.0', () => {\n  console.log(`Server running on port ${port}`);\n});\n```\n\n## Dockerfile for Azure\n\n```dockerfile\nFROM node:20-alpine\n\nWORKDIR /app\n\n# Install dependencies first (better caching)\nCOPY package*.json ./\nRUN npm ci --only=production\n\n# Copy app\nCOPY . .\n\n# Set production environment\nENV NODE_ENV=production\n\n# Expose port (Azure uses PORT env var)\nEXPOSE 3000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD wget --no-verbose --tries=1 --spider http://localhost:3000/health || exit 1\n\n# Start app\nCMD [\"node\", \"app.js\"]\n```\n\n## Common Issues\n\n### Cookies Not Setting\n\n**Symptom:** Session lost between requests\n\n**Fix:**\n1. Add `app.set('trust proxy', 1)`\n2. Set `sameSite: 'lax'` in cookie config\n3. Set `secure: true` only if using HTTPS\n\n### Wrong Client IP\n\n**Symptom:** `req.ip` returns Azure internal IP\n\n**Fix:**\n```javascript\napp.set('trust proxy', 1);\n// Now req.ip returns actual client IP\n```\n\n### HTTPS Redirect Loop\n\n**Symptom:** Infinite redirects when forcing HTTPS\n\n**Fix:**\n```javascript\n// Check x-forwarded-proto, not req.secure\n// Use a trusted host instead of the untrusted Host header to avoid open redirects\nconst TRUSTED_HOST = process.env.APP_PUBLIC_HOSTNAME; // e.g. \"myapp.contoso.com\"\n\napp.use((req, res, next) => {\n  if (req.get('x-forwarded-proto') !== 'https' && process.env.NODE_ENV === 'production') {\n    const host = TRUSTED_HOST;\n    if (!host) {\n      // If no trusted host is configured, skip redirect to avoid using untrusted Host header\n      return next();\n    }\n    // Optionally enforce an allowlist here for extra safety\n    return res.redirect(`https://${host}${req.originalUrl}`);\n  }\n  next();\n});\n```\n\n### Health Check Failing\n\n**Symptom:** Container restarts repeatedly\n\n**Fix:**\n1. Ensure `/health` endpoint returns 200\n2. Check app starts within startup probe timeout\n3. Verify port matches container configuration\n\n## Environment Variables\n\nSet these in Azure:\n\n```bash\naz containerapp update \\\n  --name APP \\\n  --resource-group RG \\\n  --set-env-vars \\\n    NODE_ENV=production \\\n    SESSION_SECRET=your-secret-here \\\n    PORT=3000\n```\n\n> âš ï¸ **Important distinction**: `azd env set` vs Application Environment Variables\n>\n> **`azd env set`** sets variables for the **azd provisioning process**, NOT application runtime environment variables. These are used by azd and Bicep during deployment (e.g., `AZURE_LOCATION`, `AZURE_SUBSCRIPTION_ID`).\n>\n> **Application environment variables** (like `NODE_ENV`, `SESSION_SECRET`) must be configured in one of these ways:\n> 1. **In Bicep templates** - Define in the resource's `env` property\n> 2. **Via Azure CLI** - Use `az containerapp update --set-env-vars` (shown above)\n> 3. **In azure.yaml** - Use the `env` section in service configuration\n\n**Setting azd provisioning parameters:**\n```bash\n# These are for azd/Bicep configuration, NOT application runtime\nazd env set AZURE_LOCATION eastus\nazd env set AZURE_SUBSCRIPTION_ID <subscription-id>\n```\n\n**Setting application environment variables in azure.yaml:**\n```yaml\nservices:\n  api:\n    host: containerapp\n    # Application runtime environment variables\n    env:\n      NODE_ENV: production\n      PORT: \"3000\"\n```\n\n**Setting application environment variables in Bicep:**\n```bicep\nenv: [\n  { name: 'NODE_ENV', value: 'production' }\n  { name: 'SESSION_SECRET', secretRef: 'session-secret' }\n]\n```\n",
        "plugin/skills/azure-observability/SKILL.md": "---\nname: azure-observability\ndescription: Azure Observability Services including Azure Monitor, Application Insights, Log Analytics, Alerts, and Workbooks. Provides metrics, APM, distributed tracing, KQL queries, and interactive reports.\n---\n\n# Azure Observability Services\n\n## Services\n\n| Service | Use When | MCP Tools | CLI |\n|---------|----------|-----------|-----|\n| Azure Monitor | Metrics, alerts, dashboards | `azure__monitor` | `az monitor` |\n| Application Insights | APM, distributed tracing | `azure__applicationinsights` | `az monitor app-insights` |\n| Log Analytics | Log queries, KQL | `azure__kusto` | `az monitor log-analytics` |\n| Alerts | Notifications, actions | - | `az monitor alert` |\n| Workbooks | Interactive reports | `azure__workbooks` | - |\n\n## MCP Server (Preferred)\n\nWhen Azure MCP is enabled:\n\n### Monitor\n- `azure__monitor` with command `monitor_metrics_query` - Query metrics\n- `azure__monitor` with command `monitor_logs_query` - Query logs with KQL\n\n### Application Insights\n- `azure__applicationinsights` with command `applicationinsights_component_list` - List App Insights resources\n\n### Log Analytics\n- `azure__kusto` with command `kusto_cluster_list` - List clusters\n- `azure__kusto` with command `kusto_query` - Execute KQL queries\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n## CLI Reference\n\n```bash\n# List Log Analytics workspaces\naz monitor log-analytics workspace list --output table\n\n# Query logs with KQL\naz monitor log-analytics query \\\n  --workspace WORKSPACE_ID \\\n  --analytics-query \"AzureActivity | take 10\"\n\n# List Application Insights\naz monitor app-insights component list --output table\n\n# List alerts\naz monitor alert list --output table\n\n# Query metrics\naz monitor metrics list \\\n  --resource RESOURCE_ID \\\n  --metric \"Percentage CPU\"\n```\n\n## Common KQL Queries\n\n```kql\n// Recent errors\nAppExceptions\n| where TimeGenerated > ago(1h)\n| project TimeGenerated, Message, StackTrace\n| order by TimeGenerated desc\n\n// Request performance\nAppRequests\n| where TimeGenerated > ago(1h)\n| summarize avg(DurationMs), count() by Name\n| order by avg_DurationMs desc\n\n// Resource usage\nAzureMetrics\n| where TimeGenerated > ago(1h)\n| where MetricName == \"Percentage CPU\"\n| summarize avg(Average) by Resource\n```\n\n## Monitoring Strategy\n\n| What to Monitor | Service | Metric/Log |\n|-----------------|---------|------------|\n| Application errors | App Insights | Exceptions, failed requests |\n| Performance | App Insights | Response time, dependencies |\n| Infrastructure | Azure Monitor | CPU, memory, disk |\n| Security | Log Analytics | Sign-ins, audit logs |\n| Costs | Cost Management | Budget alerts |\n\n## Service Details\n\nFor deep documentation on specific services:\n\n- Application Insights setup -> `appinsights-instrumentation` skill\n- KQL query patterns -> [Log Analytics KQL documentation](https://learn.microsoft.com/azure/azure-monitor/logs/log-query-overview)\n- Alert configuration -> [Azure Monitor alerts documentation](https://learn.microsoft.com/azure/azure-monitor/alerts/alerts-overview)\n",
        "plugin/skills/azure-postgres-entra-rbac-setup/REQUIREMENTS.md": "# Azure Managed PostgreSQL Entra ID RBAC Setup - Requirements\n\n## Overview\n\nThis skill helps users set up Microsoft Entra ID (formerly Azure AD) authentication for Azure Database for PostgreSQL Flexible Server for the first time. The authentication model is confusing because it involves a two-layer mapping: **Azure Identity â†’ PostgreSQL Role â†’ Database Permissions**. This skill demystifies the process and provides step-by-step guidance.\n\n## Background Resources\n\n### Agent Skills Documentation\n- **Agent Skills Overview**: https://agentskills.io/\n- **Skill Specification**: https://agentskills.io/spec\n- **Best Practices**: https://agentskills.io/best-practices\n- **Examples**: https://agentskills.io/examples\n\n### Azure PostgreSQL Entra Authentication Documentation\n- **Main Authentication Guide**: https://learn.microsoft.com/en-us/azure/postgresql/security/security-entra-configure\n- **Managing Entra Roles**: https://learn.microsoft.com/en-us/azure/postgresql/security/security-manage-entra-users\n- **Entra Authentication Concepts**: https://learn.microsoft.com/en-us/azure/postgresql/security/security-entra-concepts\n\n## Key Pain Points Identified\n\nBased on user experience and documentation analysis, these are the main confusion areas:\n\n1. **Two-Layer Authentication Mapping**\n   - Azure identity (User/Group/Service Principal/Managed Identity) maps to PostgreSQL role\n   - PostgreSQL role must then be granted database permissions\n   - Users often forget the second step\n\n2. **Token-Based Authentication**\n   - Access tokens have 5-60 minute validity\n   - Tokens must be used as passwords in connection strings\n   - Token acquisition command: `az account get-access-token --resource-type oss-rdbms`\n\n3. **Multiple Identity Types with Different Flows**\n   - **User**: Requires UPN (userPrincipalName), case-sensitive\n   - **Group**: Supports group sync (auto-creates member roles), has two modes\n   - **Service Principal**: Requires object ID from Enterprise Applications\n   - **Managed Identity**: Requires system or user-assigned identity setup\n\n4. **Cryptic SQL Functions**\n   - `pgaadauth_create_principal(roleName, isAdmin, isMfa)` - creates role by name\n   - `pgaadauth_create_principal_with_oid(roleName, objectId, objectType, isAdmin, isMfa)` - creates role by object ID\n   - `pgaadauth_list_principals(isAdminValue)` - lists all Entra-mapped roles\n   - `SECURITY LABEL for \"pgaadauth\" on role \"<roleName>\" is 'aadauth,oid=<objectId>,type=<objectType>,admin'` - enables Entra on existing roles\n   - `pgaadauth_sync_roles_for_group_members()` - manually syncs group members\n\n5. **Network Requirements**\n   - **Private endpoint** configurations need NSG rules for `AzureActiveDirectory` service tag\n   - DNS must resolve `login.microsoftonline.com` and `graph.microsoft.com`\n   - Firewall rules don't apply to Entra authentication (uses TLS)\n\n6. **Group Sync Confusion**\n   - **Sync disabled** (`pgaadauth.enable_group_sync=OFF`): Members sign in with access token, use group name as username\n   - **Sync enabled** (`pgaadauth.enable_group_sync=ON`): Members get individual roles, auto-synced every 30 minutes\n   - Group role itself should NOT be deleted even with sync enabled\n\n## Target Use Cases\n\n### Use Case 1: First-Time Developer User Setup\n**Scenario**: Development team needs Azure identity-based access instead of shared passwords\n\n**Requirements**:\n- Check if Entra authentication is enabled on the server\n- Add the first Microsoft Entra admin if not configured\n- Create non-admin PostgreSQL role for a developer using their UPN\n- Grant appropriate database and schema permissions\n- Provide connection string template and token retrieval command\n- Generate shell scripts for both bash and PowerShell\n\n**Success Criteria**:\n- Developer can connect using: `psql \"host=SERVER user=USER@DOMAIN dbname=DB sslmode=require\"` with token as password\n- Developer has SELECT, INSERT, UPDATE, DELETE on application tables\n- No credentials stored in code or config files\n\n---\n\n### Use Case 2: Managed Identity for Applications\n**Scenario**: Azure Container App / App Service / Function needs passwordless database access\n\n**Requirements**:\n- Identify the managed identity (system-assigned or user-assigned)\n- Retrieve the managed identity's object ID\n- Create PostgreSQL role mapped to the managed identity\n- Grant appropriate permissions for the application workload\n- Provide application code snippets showing auto-token retrieval (no password needed)\n- Support both Node.js, Python, .NET examples\n\n**Success Criteria**:\n- Application connects without any stored credentials\n- Application uses Azure Identity SDK to auto-acquire tokens\n- Connection works from Azure-hosted environment (not local dev)\n\n---\n\n### Use Case 3: Group-Based Access Control\n**Scenario**: Manage permissions via Azure AD groups (e.g., \"Database Readers\", \"Database Admins\")\n\n**Requirements**:\n- Retrieve group display name and object ID from Azure AD\n- Create group principal in PostgreSQL\n- Configure group sync behavior (ON vs OFF)\n- Explain sync semantics: auto-sync interval, manual trigger, member role creation\n- Grant permissions to the group role\n- Show how group members connect (varies by sync setting)\n\n**Success Criteria**:\n- Adding/removing users from Azure AD group automatically affects database access\n- Group members understand whether to use group name or individual UPN for connection\n- Clear documentation on sync delay (up to 30 minutes)\n\n---\n\n### Use Case 4: Troubleshooting Connection Failures\n**Scenario**: Users getting \"authentication failed\" or \"role does not exist\" errors\n\n**Requirements**:\n- Diagnostic checklist for common issues:\n  - Role exists in database (`pgaadauth_list_principals`)\n  - Token is fresh (not expired)\n  - Username matches exactly (case-sensitive)\n  - Network allows `AzureActiveDirectory` service tag (for private endpoint)\n  - DNS resolves `login.microsoftonline.com` and `graph.microsoft.com`\n- Step-by-step troubleshooting workflow\n- Commands to verify each aspect\n- Shell scripts for both bash and PowerShell\n\n**Success Criteria**:\n- Clear error categorization (auth error vs permission error vs network error)\n- Actionable remediation steps for each error type\n- Self-service diagnostic commands\n\n---\n\n### Use Case 5: Migration from Password Auth to Entra ID\n**Scenario**: Existing PostgreSQL users need to transition from password-based to Entra ID authentication\n\n**Requirements**:\n- List existing PostgreSQL roles (exclude system roles)\n- Map existing roles to Entra identities using `SECURITY LABEL`\n- Support parallel authentication during transition period\n- Test Entra auth before disabling passwords\n- Switch server to \"Microsoft Entra authentication only\" mode\n- Provide migration plan checklist\n\n**Success Criteria**:\n- Zero-downtime migration (parallel auth during transition)\n- All existing roles mapped to corresponding Entra identities\n- Passwords disabled only after successful Entra auth testing\n- Rollback plan documented\n\n---\n\n## Primary Tools and Commands\n\n### Azure CLI Commands\n```bash\n# Entra Admin Management\naz postgres flexible-server microsoft-entra-admin create\naz postgres flexible-server microsoft-entra-admin list\naz postgres flexible-server microsoft-entra-admin show\naz postgres flexible-server microsoft-entra-admin delete\n\n# Token Acquisition\naz account get-access-token --resource-type oss-rdbms\naz account get-access-token --resource https://ossrdbms-aad.database.windows.net\n\n# Identity Lookups\naz ad user show --id <user@domain.com> --query \"{displayName:displayName, userPrincipalName:userPrincipalName, objectId:id}\"\naz ad group show --group \"Group Name\" --query \"{displayName:displayName, objectId:id}\"\naz identity show --name <identity-name> --resource-group <rg> --query \"{name:name, principalId:principalId}\"\n\n# Server Parameters\naz postgres flexible-server parameter set --name pgaadauth.enable_group_sync --value ON\naz postgres flexible-server parameter show --name pgaadauth.enable_group_sync\n\n# Server Management\naz postgres flexible-server list --resource-group <rg>\naz postgres flexible-server show --name <server> --resource-group <rg>\n```\n\n### Azure MCP Tools\n- **mcp_azure_mcp_postgres**:\n  - `postgres_server_list` - List PostgreSQL servers in subscription\n  - `postgres_database_list` - List databases on a server\n  - `postgres_database_query` - Execute SQL queries (for role creation, permission grants)\n  - `postgres_server_param_get` - Get server parameter value\n  - `postgres_server_param_set` - Set server parameter value\n\n- **mcp_azure_mcp_documentation**:\n  - `microsoft_docs_search` - Search official Azure docs\n  - `microsoft_docs_fetch` - Fetch complete documentation pages\n  - `microsoft_code_sample_search` - Find code examples\n\n### PostgreSQL SQL Functions (via psql or MCP)\n```sql\n-- Create role by name (must match Entra principal name exactly)\nSELECT * FROM pgaadauth_create_principal('user@domain.com', false, false);\n\n-- Create role by object ID (more reliable, works for any identity type)\nSELECT * FROM pgaadauth_create_principal_with_oid('role-name', 'object-id-guid', 'user|group|service', false, false);\n\n-- List all Entra-mapped roles\nSELECT * FROM pgaadauth_list_principals(false); -- false = all users, true = admins only\n\n-- Enable Entra on existing PostgreSQL role\nSECURITY LABEL for \"pgaadauth\" on role \"existing_role\" is 'aadauth,oid=<object-id>,type=user';\n\n-- Sync group members (if group sync enabled)\nSELECT * FROM pgaadauth_sync_roles_for_group_members();\n\n-- Standard PostgreSQL permission grants\nGRANT CONNECT ON DATABASE mydb TO \"user@domain.com\";\nGRANT USAGE ON SCHEMA public TO \"user@domain.com\";\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO \"user@domain.com\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO \"user@domain.com\";\n\n-- List roles\n\\du\n\n-- Check permissions\n\\dp\nSELECT * FROM information_schema.role_table_grants WHERE grantee = 'user@domain.com';\n```\n\n### Connection Commands\n```bash\n# Bash - psql connection\nexport PGPASSWORD=$(az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv)\npsql \"host=myserver.postgres.database.azure.com user=user@domain.com dbname=mydb sslmode=require\"\n\n# Bash - combined token and connection\npsql \"host=myserver.postgres.database.azure.com user=user@domain.com dbname=mydb sslmode=require password=$(az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv)\"\n```\n\n```powershell\n# PowerShell - psql connection\n$env:PGPASSWORD = (az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv)\npsql \"host=myserver.postgres.database.azure.com user=user@domain.com dbname=mydb sslmode=require\"\n```\n\n## Skill Features\n\n### 1. Pre-flight Validation\n- Check if server has Entra authentication enabled\n- Verify network connectivity to Azure AD endpoints\n- Validate DNS resolution for `login.microsoftonline.com` and `graph.microsoft.com`\n- Check if user has required permissions (at least one Entra admin exists)\n\n### 2. Identity Type Detection\nAuto-detect user intent based on input:\n- **User mentioned**: \"user\", \"developer\", \"team member\", UPN format\n- **Group mentioned**: \"group\", \"team\", \"role-based\"\n- **Service Principal mentioned**: \"service principal\", \"app registration\", \"application\"\n- **Managed Identity mentioned**: \"managed identity\", \"MI\", \"system-assigned\", \"user-assigned\", \"container app\", \"app service\"\n\n### 3. Token Management Helper\n- Auto-generate fresh token acquisition commands\n- Display token expiration warning (5-60 minutes)\n- Provide both inline and environment variable approaches\n- Include shell scripts for both bash and PowerShell\n\n### 4. Permission Templates\nPredefined permission sets:\n- **Read-Only**: CONNECT, USAGE, SELECT on all tables\n- **Read-Write**: CONNECT, USAGE, SELECT, INSERT, UPDATE, DELETE on all tables\n- **Admin**: CONNECT, USAGE, ALL PRIVILEGES, plus `azure_pg_admin` role membership\n- **Custom**: Prompt user for specific permissions\n\n### 5. Troubleshooting Wizard\nStep-by-step diagnostics:\n1. Verify role exists in database\n2. Check token freshness\n3. Validate username spelling (exact match, case-sensitive)\n4. Test network connectivity (for private endpoint)\n5. Verify DNS resolution\n6. Check server authentication mode (Entra only vs both)\n7. Review server logs if accessible\n\n### 6. Security Best Practices\n- Recommend MFA enforcement (`isMfa=true`) for admin users\n- Suggest separate admin and app roles (least privilege)\n- Warn about token validity (don't store tokens in code)\n- Recommend group-based access for team management\n- Advise on token refresh strategies for long-running apps\n\n## Technical Specifications\n\n### Object Types in PostgreSQL\n- `user` - Microsoft Entra user (including guests)\n- `group` - Microsoft Entra group\n- `service` - Service principals and managed identities\n\n### Admin Role Implications\nWhen `isAdmin=true`:\n- Role becomes member of `azure_pg_admin`\n- Gets `CREATEROLE` and `CREATEDB` privileges\n- Can manage other Entra roles via SQL functions\n\n### MFA Flag Behavior\nThe `isMfa` parameter:\n- Tests the `mfa` claim in the Entra token\n- Does NOT enforce MFA at tenant level (tenant policy controls that)\n- Only useful if tenant has optional MFA configured\n\n### Token Validity\n- Minimum: 5 minutes\n- Maximum: 60 minutes\n- Recommendation: Acquire fresh token immediately before connection\n- For long-running apps: Implement token refresh logic using Azure Identity SDK\n\n### Group Sync Modes\n\n**Sync Disabled** (`pgaadauth.enable_group_sync=OFF`):\n- Group members sign in using their own access tokens\n- Username must be the group name (not individual UPN)\n- No individual member roles created\n\n**Sync Enabled** (`pgaadauth.enable_group_sync=ON`):\n- Individual member roles auto-created in database\n- Synced every 30 minutes automatically\n- Manual sync: `SELECT * FROM pgaadauth_sync_roles_for_group_members();`\n- Group role should NOT be deleted\n- Members can optionally use group name for login (backward compat)\n\n## Edge Cases and Gotchas\n\n1. **Guest Users**: Must use full UPN with `#EXT#` tag (e.g., `guest_user_example.com#EXT#@tenant.onmicrosoft.com`)\n\n2. **Service Principal Object ID**: Must use the Enterprise Application object ID, NOT the App Registration object ID\n\n3. **Spaces in Names**: Must escape spaces in psql: `\"Group\\ Name\"` or wrap in double quotes\n\n4. **Case Sensitivity**: All Entra principal names are case-sensitive in PostgreSQL\n\n5. **Token Resource URL**: Different for sovereign clouds:\n   - Azure Public: `https://ossrdbms-aad.database.windows.net`\n   - Use `az cloud show` to find correct resource for other clouds\n\n6. **Private Endpoint Network**: Must configure NSG and route table for `AzureActiveDirectory` service tag\n\n7. **Default Privileges**: Use `ALTER DEFAULT PRIVILEGES` for permissions on future tables:\n   ```sql\n   ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO \"user@domain.com\";\n   ```\n\n8. **Connection String Format**: PostgreSQL connection string requires:\n   - `sslmode=require` (TLS mandatory)\n   - Username in UPN format or group name\n   - Password is the access token (not a real password)\n\n## Shell Script Requirement\n\n**CRITICAL**: All shell scripts MUST include both **bash** and **PowerShell** versions to ensure compatibility with Linux, Mac, and Windows environments.\n\nExample format:\n```markdown\n### Bash Script\n\\`\\`\\`bash\n# Commands here\n\\`\\`\\`\n\n### PowerShell Script\n\\`\\`\\`powershell\n# Commands here\n\\`\\`\\`\n```\n\n## Success Metrics\n\nThe skill is successful if:\n1. User can enable Entra authentication on a new or existing PostgreSQL server\n2. User can create PostgreSQL roles for Users, Groups, Service Principals, and Managed Identities\n3. User understands the token-based authentication model\n4. User can grant appropriate database permissions\n5. User can troubleshoot common authentication failures\n6. User can migrate from password-based to Entra ID authentication\n7. All examples work on Linux, Mac, and Windows (via bash and PowerShell scripts)\n\n## References and Links\n\n- **Azure PostgreSQL Flexible Server Overview**: https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/overview\n- **Azure Identity SDK for .NET**: https://learn.microsoft.com/en-us/dotnet/api/overview/azure/identity-readme\n- **Azure Identity SDK for Python**: https://learn.microsoft.com/en-us/python/api/overview/azure/identity-readme\n- **Azure Identity SDK for JavaScript**: https://learn.microsoft.com/en-us/javascript/api/overview/azure/identity-readme\n- **PostgreSQL psql Documentation**: https://www.postgresql.org/docs/current/app-psql.html\n- **Azure RBAC Built-in Roles**: https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles\n- **Microsoft Entra ID Overview**: https://learn.microsoft.com/en-us/entra/fundamentals/whatis\n\n## Next Steps\n\nThis REQUIREMENTS.md document should be handed off to the Plan agent to create a detailed implementation plan including:\n- Skill structure (SKILL.md format)\n- Script files (bash and PowerShell)\n- Example files (connection strings, permission templates)\n- Reference documentation\n- Test scenarios\n",
        "plugin/skills/azure-postgres-entra-rbac-setup/SKILL.md": "---\nname: azure-postgres-entra-rbac-setup\ndescription: Set up Microsoft Entra ID (Azure AD) authentication for Azure Database for PostgreSQL Flexible Server. Use this skill when users need to configure passwordless authentication, map Azure identities to PostgreSQL roles, grant database permissions, set up managed identity access, configure group-based access control, troubleshoot authentication failures, or migrate from password-based authentication to Entra ID.\n---\n\n# Azure PostgreSQL Entra ID RBAC Setup\n\nThis skill helps users set up Microsoft Entra ID (formerly Azure AD) authentication for Azure Database for PostgreSQL Flexible Server. It guides users through the confusing two-layer mapping: **Azure Identity â†’ PostgreSQL Role â†’ Database Permissions**.\n\n## Skill Activation Triggers\n\n**Use this skill immediately when the user asks to:**\n- \"Set up Entra ID authentication for PostgreSQL\"\n- \"Configure passwordless access to my PostgreSQL database\"\n- \"Add a user/developer to my Azure PostgreSQL using their Azure identity\"\n- \"Set up managed identity for my app to access PostgreSQL\"\n- \"Configure group-based access to PostgreSQL\"\n- \"I'm getting authentication errors connecting to PostgreSQL with Entra\"\n- \"Migrate from password authentication to Entra ID for PostgreSQL\"\n- \"How do I connect to Azure PostgreSQL with my Azure account?\"\n- \"Grant my Container App access to PostgreSQL without storing passwords\"\n\n**Key Indicators:**\n- Mentions \"Entra\", \"Azure AD\", \"AAD\" with PostgreSQL\n- Passwordless or identity-based database access requests\n- Managed identity + PostgreSQL configuration\n- PostgreSQL authentication failures with Azure identities\n- Questions about `pgaadauth` functions or security labels\n\n## Overview\n\nAzure Database for PostgreSQL Flexible Server supports Microsoft Entra ID authentication, allowing users to connect using their Azure identities instead of passwords. This involves:\n\n1. **Enabling Entra authentication** on the PostgreSQL server\n2. **Creating a PostgreSQL role** mapped to an Azure identity\n3. **Granting database permissions** to the PostgreSQL role\n4. **Connecting with an access token** instead of a password\n\n### Identity Types Supported\n\n| Identity Type | Use Case | SQL Function |\n|--------------|----------|--------------|\n| **User** | Developer access, interactive queries | `pgaadauth_create_principal` |\n| **Group** | Team-based access management | `pgaadauth_create_principal_with_oid` |\n| **Service Principal** | Application authentication | `pgaadauth_create_principal_with_oid` |\n| **Managed Identity** | Azure-hosted app passwordless access | `pgaadauth_create_principal_with_oid` |\n\n## Core Workflow\n\n### Step 1: Check Current Authentication Status\n\nVerify if Entra authentication is enabled on the server. If empty, no Entra admin is configured yet.\n\n### Step 2: Add First Entra Administrator\n\nEnable Entra authentication by adding the first admin using Azure CLI.\n\n### Step 3: Connect as Entra Admin\n\nGet an access token and connect using psql with the token as password.\n\n### Step 4: Create PostgreSQL Roles for Identities\n\nOnce connected as admin, create roles for other identities using SQL functions.\n\n### Step 5: Grant Database Permissions\n\nGrant appropriate permissions to the new roles using GRANT statements.\n\n**See:** [scripts/az-commands.sh](scripts/az-commands.sh) for Azure CLI commands, [references/SQL-FUNCTIONS.md](references/SQL-FUNCTIONS.md) for SQL functions, and [references/PERMISSION-TEMPLATES.md](references/PERMISSION-TEMPLATES.md) for permission grants.\n\n## Setup Patterns\n\n### Pattern 1: Developer User Access\n\nSet up a developer to access the database with their Azure identity.\n\n**Required Information:**\n- Developer's UPN (e.g., `developer@company.com`)\n- Target database name\n- Permission level (read-only, read-write, admin)\n\n**Script:** See [scripts/setup-user.sh](scripts/setup-user.sh)\n\n---\n\n### Pattern 2: Managed Identity for Applications\n\nConfigure passwordless database access for Azure-hosted applications (Container Apps, App Service, Functions).\n\n**Required Information:**\n- Managed identity name and resource group\n- Target database name\n- Permission level needed\n\n**Steps:**\n1. Get managed identity object ID\n2. Create PostgreSQL role using `pgaadauth_create_principal_with_oid`\n3. Grant permissions\n4. Configure application to use Azure Identity SDK\n\n**Script:** See [scripts/setup-managed-identity.sh](scripts/setup-managed-identity.sh)\n\n---\n\n### Pattern 3: Group-Based Access Control\n\nManage database permissions through Azure AD groups.\n\n**Required Information:**\n- Group display name and object ID\n- Whether to enable group sync (`pgaadauth.enable_group_sync`)\n- Permission level for the group\n\n**Group Sync Modes:**\n\n| Mode | Behavior | Use Case |\n|------|----------|----------|\n| **OFF** (default) | Members use group name as username | Simple setup, no individual tracking |\n| **ON** | Individual member roles auto-created | Audit trails, per-user permissions |\n\n**Script:** See [scripts/setup-group.sh](scripts/setup-group.sh)\n\n---\n\n### Pattern 4: Troubleshooting Connection Failures\n\nDiagnose and fix Entra authentication issues.\n\n**Common Errors:**\n- `role \"user@domain.com\" does not exist` - Role not created in database\n- `password authentication failed` - Token expired or invalid\n- `FATAL: password authentication failed` - Wrong username format\n- `could not connect to server` - Network/firewall issues\n\n**Diagnostic Steps:** See [references/TROUBLESHOOTING.md](references/TROUBLESHOOTING.md)\n\n---\n\n### Pattern 5: Migration from Password Auth\n\nTransition existing password-based roles to Entra ID authentication.\n\n**Steps:**\n1. Enable \"PostgreSQL and Microsoft Entra authentication\" mode (parallel auth)\n2. Map existing roles to Entra identities using `SECURITY LABEL`\n3. Test Entra authentication for each migrated role\n4. Disable passwords: `ALTER ROLE \"username\" PASSWORD NULL`\n5. Switch to \"Microsoft Entra authentication only\" mode\n\n**Script:** See [scripts/migrate-to-entra.sh](scripts/migrate-to-entra.sh)\n\n## MCP Tools Used\n\n| Tool | Purpose |\n|------|---------|\n| `postgres_server_list` | List PostgreSQL servers in subscription |\n| `postgres_database_list` | List databases on a server |\n| `postgres_database_query` | Execute SQL (role creation, permissions) |\n| `postgres_server_param_get` | Get server parameter (e.g., group sync) |\n| `postgres_server_param_set` | Set server parameter |\n\n## Security Best Practices\n\n| Practice | Recommendation |\n|----------|---------------|\n| **Least Privilege** | Grant minimum required permissions; avoid admin roles for apps |\n| **Use Groups** | Manage access via Entra groups for easier administration |\n| **Managed Identity** | Always use managed identity for Azure-hosted apps |\n| **MFA for Admins** | Set `isMfa=true` for admin roles if tenant supports optional MFA |\n| **Token Handling** | Never store tokens; acquire fresh before each connection |\n| **Audit Access** | Use `pgaadauth_list_principals` to review who has access |\n| **Private Endpoint** | Use private endpoint for production; configure NSG for AzureActiveDirectory tag |\n\n## Common Issues\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| `role does not exist` | Role not created in database | Run `pgaadauth_create_principal` or `pgaadauth_create_principal_with_oid` |\n| `password authentication failed` | Token expired (5-60 min validity) | Get fresh token with `az account get-access-token` |\n| `permission denied` | Role exists but lacks permissions | Run `GRANT` statements for required access |\n| Username case mismatch | Entra names are case-sensitive | Use exact case from Azure AD |\n| Network timeout | Private endpoint missing NSG rule | Add outbound rule for `AzureActiveDirectory` service tag |\n| Guest user login fails | Using wrong UPN format | Use full UPN with `#EXT#` tag |\n\n## References\n\n- [Azure CLI Commands](scripts/az-commands.sh) - Token acquisition, identity lookups, admin management\n- [SQL Functions](references/SQL-FUNCTIONS.md) - Role creation, listing, security labels\n- [Permission Templates](references/PERMISSION-TEMPLATES.md) - Copy-paste SQL for common scenarios\n- [Troubleshooting Guide](references/TROUBLESHOOTING.md) - Detailed diagnostic steps\n- [Group Sync Guide](references/GROUP-SYNC.md) - Group sync configuration details\n",
        "plugin/skills/azure-postgres-entra-rbac-setup/references/GROUP-SYNC.md": "# Azure PostgreSQL Group Sync Configuration\n\nThis guide explains how group-based access control works with Microsoft Entra ID authentication in Azure Database for PostgreSQL.\n\n## Overview\n\nWhen you create a PostgreSQL role mapped to an Entra group, group members can access the database. There are two modes:\n\n| Mode | Setting | Behavior | Best For |\n|------|---------|----------|----------|\n| **Sync Disabled** | `pgaadauth.enable_group_sync=OFF` | Members use group name as username | Simple setups, shared audit trail |\n| **Sync Enabled** | `pgaadauth.enable_group_sync=ON` | Individual member roles auto-created | Per-user auditing, fine-grained permissions |\n\n## Mode 1: Group Sync Disabled (Default)\n\n### How It Works\n\n1. Create a group role in PostgreSQL\n2. Grant permissions to the group\n3. Group members sign in using the **group name** as their username\n4. All members share the same PostgreSQL role\n\n### Setup\n\n```bash\n# Get group object ID\nGROUP_ID=$(az ad group show --group \"Database Readers\" --query id -o tsv)\n\n# Connect as admin\nexport PGPASSWORD=$(az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv)\npsql \"host=<server>.postgres.database.azure.com user=admin@domain.com dbname=postgres sslmode=require\"\n```\n\n```sql\n-- Create group role\nSELECT * FROM pgaadauth_create_principal_with_oid('Database Readers', '<group-id>', 'group', false, false);\n\n-- Grant permissions\nGRANT CONNECT ON DATABASE mydb TO \"Database Readers\";\nGRANT USAGE ON SCHEMA public TO \"Database Readers\";\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO \"Database Readers\";\n```\n\n### Member Connection\n\nGroup members connect using the **group name** as their username:\n\n```bash\n# Bash\nexport PGPASSWORD=$(az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv)\npsql \"host=<server>.postgres.database.azure.com user='Database Readers' dbname=mydb sslmode=require\"\n```\n\n```powershell\n# PowerShell\n$env:PGPASSWORD = az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv\npsql \"host=<server>.postgres.database.azure.com user='Database Readers' dbname=mydb sslmode=require\"\n```\n\n**Note:** For group names with spaces, escape or quote the name:\n- `user='Database Readers'`\n- `user=Database\\ Readers`\n\n### Pros and Cons\n\n| Pros | Cons |\n|------|------|\n| Simple setup | Can't distinguish users in audit logs |\n| Instant membership effect | Can't grant per-user permissions |\n| Single role to manage | Username is the group name, not intuitive |\n\n---\n\n## Mode 2: Group Sync Enabled\n\n### How It Works\n\n1. Create a group role in PostgreSQL\n2. Enable group sync server parameter\n3. Individual PostgreSQL roles are auto-created for each group member\n4. Sync runs automatically every 30 minutes\n5. Members sign in with their own UPN\n\n### Setup\n\n```bash\n# Enable group sync\naz postgres flexible-server parameter set \\\n  --resource-group <rg> \\\n  --server-name <server> \\\n  --name pgaadauth.enable_group_sync \\\n  --value ON\n\n# Get group object ID\nGROUP_ID=$(az ad group show --group \"Database Readers\" --query id -o tsv)\n\n# Connect as admin\nexport PGPASSWORD=$(az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv)\npsql \"host=<server>.postgres.database.azure.com user=admin@domain.com dbname=postgres sslmode=require\"\n```\n\n```sql\n-- Create group role\nSELECT * FROM pgaadauth_create_principal_with_oid('Database Readers', '<group-id>', 'group', false, false);\n\n-- Grant permissions to the group (inherited by synced members)\nGRANT CONNECT ON DATABASE mydb TO \"Database Readers\";\nGRANT USAGE ON SCHEMA public TO \"Database Readers\";\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO \"Database Readers\";\n\n-- Trigger manual sync (optional, otherwise wait 30 min)\nSELECT * FROM pgaadauth_sync_roles_for_group_members();\n```\n\n### Member Connection\n\nWith sync enabled, members connect using their **own UPN**:\n\n```bash\n# Bash\nexport PGPASSWORD=$(az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv)\npsql \"host=<server>.postgres.database.azure.com user=developer@company.com dbname=mydb sslmode=require\"\n```\n\n```powershell\n# PowerShell\n$env:PGPASSWORD = az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv\npsql \"host=<server>.postgres.database.azure.com user=developer@company.com dbname=mydb sslmode=require\"\n```\n\n### Pros and Cons\n\n| Pros | Cons |\n|------|------|\n| Individual audit trails | 30-min sync delay for new members |\n| Can grant per-user permissions | More roles to manage |\n| Users sign in with their own name | Group role must NOT be deleted |\n\n---\n\n## Important Considerations\n\n### Group Role Management\n\n**DO NOT delete the group role** when sync is enabled. The group role is needed to maintain the member-group relationship.\n\n```sql\n-- WRONG: Don't do this\nDROP ROLE \"Database Readers\";  -- Breaks sync!\n\n-- CORRECT: Disable login if needed, but keep the role\nALTER ROLE \"Database Readers\" NOLOGIN;\n```\n\n### Manual Sync\n\nTrigger sync immediately instead of waiting 30 minutes:\n\n```sql\nSELECT * FROM pgaadauth_sync_roles_for_group_members();\n```\n\n### Checking Sync Status\n\nView all synced member roles:\n\n```sql\nSELECT * FROM pgaadauth_list_principals(false);\n```\n\n### Permission Inheritance\n\nWhen sync is enabled:\n- Permissions granted to the group are inherited by member roles\n- You can also grant additional permissions to individual member roles\n- Revoking from the group affects all synced members\n\n### Nested Groups\n\n- Nested groups (groups within groups) are **not** supported\n- Only direct members of the group are synced\n- If you need hierarchical access, create separate group roles\n\n---\n\n## Switching Modes\n\n### Enable Sync (OFF â†’ ON)\n\n```bash\naz postgres flexible-server parameter set \\\n  --resource-group <rg> \\\n  --server-name <server> \\\n  --name pgaadauth.enable_group_sync \\\n  --value ON\n```\n\nThen trigger sync:\n```sql\nSELECT * FROM pgaadauth_sync_roles_for_group_members();\n```\n\n### Disable Sync (ON â†’ OFF)\n\n```bash\naz postgres flexible-server parameter set \\\n  --resource-group <rg> \\\n  --server-name <server> \\\n  --name pgaadauth.enable_group_sync \\\n  --value OFF\n```\n\n**Note:** Existing synced member roles remain; they won't be deleted automatically.\n\n---\n\n## Troubleshooting\n\n### New group member can't connect (sync enabled)\n\n1. Wait up to 30 minutes, or trigger manual sync:\n   ```sql\n   SELECT * FROM pgaadauth_sync_roles_for_group_members();\n   ```\n\n2. Verify user is in the Azure AD group:\n   ```bash\n   az ad group member list --group \"Database Readers\" --query \"[].userPrincipalName\"\n   ```\n\n3. Check if role was created:\n   ```sql\n   SELECT * FROM pgaadauth_list_principals(false) WHERE rolename = 'user@domain.com';\n   ```\n\n### Group login fails (sync disabled)\n\n1. Ensure username is the **group name**, not individual UPN\n2. Escape spaces in group name: `user='Group Name'` or `user=Group\\ Name`\n3. Check group role exists:\n   ```sql\n   SELECT * FROM pgaadauth_list_principals(false) WHERE principaltype = 'group';\n   ```\n\n### Changes to group membership not reflected\n\n1. Check the `pgaadauth.enable_group_sync` setting:\n   ```bash\n   az postgres flexible-server parameter show \\\n     --resource-group <rg> \\\n     --server-name <server> \\\n     --name pgaadauth.enable_group_sync\n   ```\n\n2. If sync is OFF, changes are immediate (members use group name)\n3. If sync is ON, wait 30 min or run manual sync\n",
        "plugin/skills/azure-postgres-entra-rbac-setup/references/PERMISSION-TEMPLATES.md": "# PostgreSQL Permission Templates\n\nCopy-paste SQL templates for common permission scenarios when setting up Entra ID authentication.\n\n## Permission Levels\n\n### Read-Only Access\n\nGrants SELECT access to all tables in the public schema.\n\n```sql\n-- Replace <database> and <role-name> with actual values\n-- <role-name> should be the UPN (user@domain.com) or custom role name\n\nGRANT CONNECT ON DATABASE <database> TO \"<role-name>\";\nGRANT USAGE ON SCHEMA public TO \"<role-name>\";\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO \"<role-name>\";\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO \"<role-name>\";\n\n-- Grant permissions on future tables\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO \"<role-name>\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON SEQUENCES TO \"<role-name>\";\n```\n\n---\n\n### Read-Write Access\n\nGrants SELECT, INSERT, UPDATE, DELETE on all tables.\n\n```sql\nGRANT CONNECT ON DATABASE <database> TO \"<role-name>\";\nGRANT USAGE ON SCHEMA public TO \"<role-name>\";\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO \"<role-name>\";\nGRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO \"<role-name>\";\n\n-- Grant permissions on future tables\nALTER DEFAULT PRIVILEGES IN SCHEMA public \n  GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO \"<role-name>\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public \n  GRANT USAGE, SELECT ON SEQUENCES TO \"<role-name>\";\n```\n\n---\n\n### Full Admin Access\n\nGrants all privileges including ability to create objects.\n\n```sql\nGRANT ALL PRIVILEGES ON DATABASE <database> TO \"<role-name>\";\nGRANT ALL PRIVILEGES ON SCHEMA public TO \"<role-name>\";\nGRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO \"<role-name>\";\nGRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO \"<role-name>\";\nGRANT ALL PRIVILEGES ON ALL FUNCTIONS IN SCHEMA public TO \"<role-name>\";\n\n-- Grant permissions on future objects\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO \"<role-name>\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO \"<role-name>\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON FUNCTIONS TO \"<role-name>\";\n\n-- Add to azure_pg_admin role (Azure PostgreSQL admin group)\nGRANT azure_pg_admin TO \"<role-name>\";\n```\n\n---\n\n### Application-Specific Access\n\nFor applications that need access to specific tables only.\n\n```sql\n-- Connect permission\nGRANT CONNECT ON DATABASE <database> TO \"<role-name>\";\nGRANT USAGE ON SCHEMA public TO \"<role-name>\";\n\n-- Specific tables only\nGRANT SELECT, INSERT, UPDATE ON <table1> TO \"<role-name>\";\nGRANT SELECT, INSERT, UPDATE, DELETE ON <table2> TO \"<role-name>\";\nGRANT SELECT ON <readonly_table> TO \"<role-name>\";\n\n-- Specific sequences\nGRANT USAGE, SELECT ON <table1>_id_seq TO \"<role-name>\";\nGRANT USAGE, SELECT ON <table2>_id_seq TO \"<role-name>\";\n```\n\n---\n\n### Schema-Specific Access\n\nFor multi-tenant or multi-schema databases.\n\n```sql\n-- Grant access to a specific schema\nGRANT CONNECT ON DATABASE <database> TO \"<role-name>\";\nGRANT USAGE ON SCHEMA <schema-name> TO \"<role-name>\";\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA <schema-name> TO \"<role-name>\";\nGRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA <schema-name> TO \"<role-name>\";\n\n-- Future tables in that schema\nALTER DEFAULT PRIVILEGES IN SCHEMA <schema-name> \n  GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO \"<role-name>\";\nALTER DEFAULT PRIVILEGES IN SCHEMA <schema-name> \n  GRANT USAGE, SELECT ON SEQUENCES TO \"<role-name>\";\n```\n\n---\n\n## Quick Copy Templates\n\n### For User (developer@company.com)\n\n```sql\n-- Read-Only\nGRANT CONNECT ON DATABASE mydb TO \"developer@company.com\";\nGRANT USAGE ON SCHEMA public TO \"developer@company.com\";\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO \"developer@company.com\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO \"developer@company.com\";\n\n-- Read-Write\nGRANT CONNECT ON DATABASE mydb TO \"developer@company.com\";\nGRANT USAGE ON SCHEMA public TO \"developer@company.com\";\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO \"developer@company.com\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO \"developer@company.com\";\n```\n\n### For Managed Identity (my-app-identity)\n\n```sql\n-- Read-Write (typical for applications)\nGRANT CONNECT ON DATABASE mydb TO \"my-app-identity\";\nGRANT USAGE ON SCHEMA public TO \"my-app-identity\";\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO \"my-app-identity\";\nGRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO \"my-app-identity\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO \"my-app-identity\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE, SELECT ON SEQUENCES TO \"my-app-identity\";\n```\n\n### For Group (Database Readers)\n\n```sql\n-- Note: Group names with spaces must be quoted\nGRANT CONNECT ON DATABASE mydb TO \"Database Readers\";\nGRANT USAGE ON SCHEMA public TO \"Database Readers\";\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO \"Database Readers\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO \"Database Readers\";\n```\n\n---\n\n## Revoking Permissions\n\n### Revoke All Permissions\n\n```sql\n-- Revoke grants\nREVOKE ALL PRIVILEGES ON ALL TABLES IN SCHEMA public FROM \"<role-name>\";\nREVOKE ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public FROM \"<role-name>\";\nREVOKE USAGE ON SCHEMA public FROM \"<role-name>\";\nREVOKE CONNECT ON DATABASE <database> FROM \"<role-name>\";\n\n-- Remove default privileges\nALTER DEFAULT PRIVILEGES IN SCHEMA public REVOKE ALL ON TABLES FROM \"<role-name>\";\nALTER DEFAULT PRIVILEGES IN SCHEMA public REVOKE ALL ON SEQUENCES FROM \"<role-name>\";\n```\n\n### Drop Role Completely\n\n```sql\n-- First revoke all privileges\nREVOKE ALL PRIVILEGES ON ALL TABLES IN SCHEMA public FROM \"<role-name>\";\nREVOKE ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public FROM \"<role-name>\";\nREVOKE ALL PRIVILEGES ON DATABASE <database> FROM \"<role-name>\";\nREVOKE USAGE ON SCHEMA public FROM \"<role-name>\";\n\n-- Then drop the role\nDROP ROLE \"<role-name>\";\n```\n\n---\n\n## Checking Existing Permissions\n\n```sql\n-- List all roles\n\\du\n\n-- Show grants for a specific role\nSELECT \n  grantee,\n  table_schema,\n  table_name,\n  privilege_type\nFROM information_schema.role_table_grants \nWHERE grantee = '<role-name>';\n\n-- Show database-level permissions\nSELECT datname, datacl FROM pg_database WHERE datname = '<database>';\n\n-- Show schema permissions\nSELECT nspname, nspacl FROM pg_namespace WHERE nspname = 'public';\n\n-- List Entra-mapped roles with their properties\nSELECT * FROM pgaadauth_list_principals(false);\n```\n",
        "plugin/skills/azure-postgres-entra-rbac-setup/references/SQL-FUNCTIONS.md": "-- =============================================================================\n-- SQL Functions for Azure PostgreSQL Entra ID Authentication\n-- Run these commands after connecting as an Entra admin\n-- =============================================================================\n\n-- =============================================================================\n-- CREATE ROLES\n-- =============================================================================\n\n-- Create role for a user (by name - must match UPN exactly)\nSELECT * FROM pgaadauth_create_principal('user@domain.com', false, false);\n-- Arguments:\n--   'user@domain.com' = roleName (must match Entra principal name exactly)\n--   false = isAdmin (true = azure_pg_admin member)\n--   false = isMfa (true = require MFA claim in token)\n\n-- Create role for a user as admin\nSELECT * FROM pgaadauth_create_principal('admin@domain.com', true, false);\n\n-- Create role using object ID (more reliable for managed identities, service principals, groups)\nSELECT * FROM pgaadauth_create_principal_with_oid(\n  'my-custom-role-name',  -- roleName: any name you choose\n  'abc12345-1234-1234-1234-123456789012',  -- objectId: Entra object ID (GUID)\n  'service',              -- objectType: 'user', 'group', or 'service'\n  false,                  -- isAdmin\n  false                   -- isMfa\n);\n\n-- Object Types:\n--   'user'    = Entra users (including guests)\n--   'group'   = Entra groups\n--   'service' = Service principals and managed identities\n\n-- =============================================================================\n-- LIST ROLES\n-- =============================================================================\n\n-- List all Entra-mapped roles\nSELECT * FROM pgaadauth_list_principals(false);\n\n-- List only admin roles\nSELECT * FROM pgaadauth_list_principals(true);\n\n-- Returns: rolename, principalType, objectId, tenantId, isMfa, isAdmin\n\n-- =============================================================================\n-- ENABLE ENTRA ON EXISTING ROLE (SECURITY LABEL)\n-- =============================================================================\n\n-- Map an existing PostgreSQL role to an Entra identity\nSECURITY LABEL for \"pgaadauth\" on role \"existing_role\" \n  is 'aadauth,oid=<object-id>,type=user';\n\n-- With admin privileges\nSECURITY LABEL for \"pgaadauth\" on role \"existing_admin\" \n  is 'aadauth,oid=<object-id>,type=user,admin';\n\n-- For a group\nSECURITY LABEL for \"pgaadauth\" on role \"existing_group_role\" \n  is 'aadauth,oid=<group-object-id>,type=group';\n\n-- For a service principal or managed identity\nSECURITY LABEL for \"pgaadauth\" on role \"existing_app_role\" \n  is 'aadauth,oid=<service-principal-object-id>,type=service';\n\n-- =============================================================================\n-- MANUAL GROUP SYNC\n-- =============================================================================\n\n-- Trigger manual sync of group members (if group sync is enabled)\nSELECT * FROM pgaadauth_sync_roles_for_group_members();\n\n-- =============================================================================\n-- DROP ROLES\n-- =============================================================================\n\n-- Drop an Entra-mapped role\nDROP ROLE \"user@domain.com\";\n\n-- Note: For groups with sync enabled, do NOT delete the group role\n-- Instead, disable login if needed:\nALTER ROLE \"Group Name\" NOLOGIN;\n",
        "plugin/skills/azure-postgres-entra-rbac-setup/references/TROUBLESHOOTING.md": "# Troubleshooting Azure PostgreSQL Entra ID Authentication\n\nThis guide helps diagnose and resolve common authentication issues when connecting to Azure Database for PostgreSQL using Microsoft Entra ID.\n\n## Quick Diagnostic Checklist\n\nRun through this checklist when authentication fails:\n\n| Check | Command | Expected |\n|-------|---------|----------|\n| Role exists in database | `SELECT * FROM pgaadauth_list_principals(false);` | Your role appears in list |\n| Token is fresh | Check timestamp from `az account get-access-token` | `expiresOn` is in the future |\n| Username format correct | Compare with role name in database | Exact match (case-sensitive) |\n| Network connectivity | `nslookup login.microsoftonline.com` | Resolves to IP address |\n| DNS for Graph API | `nslookup graph.microsoft.com` | Resolves to IP address |\n| Entra admin exists | `az postgres flexible-server microsoft-entra-admin list` | At least one admin |\n\n## Common Errors and Solutions\n\n### Error: `role \"user@domain.com\" does not exist`\n\n**Cause:** The PostgreSQL role hasn't been created for this Entra identity.\n\n**Solution:**\n\n1. Connect as an Entra admin:\n```bash\nexport PGPASSWORD=$(az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv)\npsql \"host=<server>.postgres.database.azure.com user=admin@domain.com dbname=postgres sslmode=require\"\n```\n\n2. Create the role:\n```sql\n-- By name (for users)\nSELECT * FROM pgaadauth_create_principal('user@domain.com', false, false);\n\n-- By object ID (for managed identities/service principals)\nSELECT * FROM pgaadauth_create_principal_with_oid('my-identity', '<object-id>', 'service', false, false);\n```\n\n---\n\n### Error: `password authentication failed for user \"user@domain.com\"`\n\n**Cause:** Token is expired, invalid, or wrong format.\n\n**Solution:**\n\n1. Get a fresh token:\n```bash\n# Bash\nexport PGPASSWORD=$(az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv)\n\n# PowerShell\n$env:PGPASSWORD = az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv\n```\n\n2. Verify token validity:\n```bash\naz account get-access-token --resource-type oss-rdbms --query expiresOn -o tsv\n```\n\n3. Ensure you're logged in as the correct user:\n```bash\naz account show --query user.name -o tsv\n```\n\n---\n\n### Error: `FATAL: password authentication failed` (no username in error)\n\n**Cause:** Username format is incorrect or doesn't match the database role.\n\n**Solution:**\n\n1. Check the exact role name in the database:\n```sql\nSELECT * FROM pgaadauth_list_principals(false);\n```\n\n2. Use the **exact** role name (case-sensitive) in your connection:\n```bash\n# If role is \"Developer@Company.com\", use exactly that\npsql \"host=<server>.postgres.database.azure.com user=Developer@Company.com dbname=mydb sslmode=require\"\n```\n\n3. For guest users, use the full UPN with `#EXT#`:\n```bash\npsql \"host=<server>.postgres.database.azure.com user=guest_user_example.com#EXT#@tenant.onmicrosoft.com dbname=mydb sslmode=require\"\n```\n\n---\n\n### Error: `could not connect to server: Connection timed out`\n\n**Cause:** Network/firewall blocking connection or incorrect server name.\n\n**Solution:**\n\n1. Verify server FQDN:\n```bash\naz postgres flexible-server show --resource-group <rg> --name <server> --query fullyQualifiedDomainName -o tsv\n```\n\n2. Check firewall rules:\n```bash\naz postgres flexible-server firewall-rule list --resource-group <rg> --name <server>\n```\n\n3. For private endpoint, verify NSG allows outbound to `AzureActiveDirectory` service tag:\n```bash\n# Check NSG rules\naz network nsg rule list --resource-group <rg> --nsg-name <nsg-name>\n```\n\n4. Verify DNS resolution:\n```bash\nnslookup <server>.postgres.database.azure.com\nnslookup login.microsoftonline.com\nnslookup graph.microsoft.com\n```\n\n---\n\n### Error: `SSL SYSCALL error: Connection reset by peer`\n\n**Cause:** TLS/SSL connection issue, often network-related.\n\n**Solution:**\n\n1. Ensure `sslmode=require` is in connection string\n2. Check if proxy/firewall is intercepting TLS traffic\n3. For private endpoint, verify route table has `AzureActiveDirectory` â†’ `Internet`\n\n---\n\n### Error: Token acquisition fails\n\n**Cause:** Not logged into Azure CLI or wrong account.\n\n**Solution:**\n\n1. Log in to Azure:\n```bash\naz login\n```\n\n2. Select the correct subscription:\n```bash\naz account set --subscription <subscription-id>\n```\n\n3. Verify you have access:\n```bash\naz account show\n```\n\n4. For service principal authentication:\n```bash\naz login --service-principal -u <client-id> -p <client-secret> --tenant <tenant-id>\n```\n\n---\n\n### Error: `Cannot validate Microsoft Entra ID user because its name isn't unique`\n\n**Cause:** Multiple objects in Azure AD have the same display name.\n\n**Solution:**\n\nUse `pgaadauth_create_principal_with_oid` instead:\n\n```sql\n-- Get the object ID first\n-- az ad user show --id user@domain.com --query id -o tsv\n\nSELECT * FROM pgaadauth_create_principal_with_oid('unique-role-name', '<object-id>', 'user', false, false);\n```\n\n---\n\n### Error: Group member can't connect (group sync enabled)\n\n**Cause:** Group sync hasn't run yet (runs every 30 minutes).\n\n**Solution:**\n\n1. Manually trigger sync:\n```sql\nSELECT * FROM pgaadauth_sync_roles_for_group_members();\n```\n\n2. Wait a few seconds and check roles:\n```sql\nSELECT * FROM pgaadauth_list_principals(false);\n```\n\n3. Verify the user is actually in the Azure AD group:\n```bash\naz ad group member list --group \"Group Name\" --query \"[].userPrincipalName\" -o tsv\n```\n\n---\n\n### Error: Managed identity can't connect from Azure-hosted app\n\n**Cause:** Application not using Azure Identity SDK correctly.\n\n**Solution:**\n\n1. Verify managed identity is enabled on the app:\n```bash\n# For Container Apps\naz containerapp identity show --name <app> --resource-group <rg>\n\n# For App Service\naz webapp identity show --name <app> --resource-group <rg>\n```\n\n2. Ensure the correct role name is used (must match exactly what was created in PostgreSQL)\n\n3. Check application code uses Azure Identity SDK - see `examples/` folder\n\n4. Verify managed identity object ID matches what's in PostgreSQL:\n```bash\n# Get MI object ID\naz identity show --name <identity> --resource-group <rg> --query principalId -o tsv\n\n# Compare with database\npsql -c \"SELECT * FROM pgaadauth_list_principals(false);\"\n```\n\n---\n\n## Diagnostic Commands Reference\n\n### Check Entra Admin Status\n```bash\naz postgres flexible-server microsoft-entra-admin list \\\n  --resource-group <rg> \\\n  --server-name <server>\n```\n\n### List All Entra-Mapped Roles\n```sql\nSELECT * FROM pgaadauth_list_principals(false);\n```\n\n### Check Role Permissions\n```sql\n-- List all roles\n\\du\n\n-- Check grants on a database\n\\l\n\n-- Check table permissions\n\\dp\n\n-- Detailed permission check\nSELECT * FROM information_schema.role_table_grants WHERE grantee = 'user@domain.com';\n```\n\n### Verify Token\n```bash\n# Get token and check expiration\naz account get-access-token --resource-type oss-rdbms\n\n# Decode token (optional, for debugging)\n# The accessToken is a JWT - you can decode it at jwt.io to verify claims\n```\n\n### Test Network Connectivity\n```bash\n# DNS resolution\nnslookup <server>.postgres.database.azure.com\nnslookup login.microsoftonline.com\nnslookup graph.microsoft.com\n\n# TCP connectivity (port 5432)\nnc -zv <server>.postgres.database.azure.com 5432\n\n# Or using telnet\ntelnet <server>.postgres.database.azure.com 5432\n```\n\n### Check Server Parameters\n```bash\n# Check group sync setting\naz postgres flexible-server parameter show \\\n  --resource-group <rg> \\\n  --server-name <server> \\\n  --name pgaadauth.enable_group_sync\n```\n\n## Still Having Issues?\n\n1. **Enable diagnostic logging** on the PostgreSQL server in Azure Portal\n2. **Check Azure Monitor logs** for authentication failures\n3. **Verify RBAC permissions** - you may need `Contributor` or specific PostgreSQL roles\n4. **Contact support** with diagnostic output from the commands above\n",
        "plugin/skills/azure-quick-review/SKILL.md": "---\nname: azure-quick-review\ndescription: Performs Azure compliance assessments using Azure Quick Review (azqr) to identify resources that don't comply with Azure best practices. Use this skill when users ask to check compliance, assess Azure resources, run azqr, identify best practice violations, find orphaned resources, or review Azure resource configurations. Activate when users mention compliance scan, resource review, Azure assessment, or security posture evaluation.\n---\n\n# Azure Quick Review Compliance Assessment\n\nThis skill enables comprehensive Azure compliance assessments using Azure Quick Review (azqr), analyzing findings against Azure best practices, and providing actionable remediation guidance.\n\n## When to Use This Skill\n\n- User asks to check Azure compliance or best practices\n- User wants to assess Azure resources for configuration issues\n- User mentions running azqr or Azure Quick Review\n- User wants to identify orphaned or misconfigured resources\n- User needs to review Azure security posture\n- Before major deployments to establish a compliance baseline\n- After deployments to verify no compliance regressions\n\n## Prerequisites\n\n- **Azure authentication** - Logged in via Azure CLI (`az login`) or using Service Principal/Managed Identity\n- **Reader permissions** - Minimum Reader role on target subscription or management group\n\n## Assessment Workflow\n\n### Step 1: Determine Scan Scope\n\nAsk the user or detect from context:\n\n| Scope | Use Case | Required Info |\n|-------|----------|---------------|\n| Subscription | Full subscription assessment | Subscription ID |\n| Resource Group | Targeted assessment | Subscription ID + Resource Group name |\n| Management Group | Enterprise-wide assessment | Management Group ID |\n| Specific Service | Deep-dive on one resource type | Subscription ID + Service abbreviation |\n\n### Step 2: Run Compliance Scan\n\nUse the Azure MCP tool to run the scan:\n\n```\nmcp_azure_mcp_extension_azqr\n  subscription: <subscription-id>\n  resource-group: <optional-rg-name>\n```\n\n### Step 3: Analyze Scan Results\n\nThe scan produces an Excel file with these sheets:\n\n| Sheet | Contents | Priority |\n|-------|----------|----------|\n| **Recommendations** | All recommendations with impacted resource count | High |\n| **ImpactedResources** | Resources with specific issues to address | High |\n| **Inventory** | All scanned resources with SKU, Tier, SLA details | Medium |\n| **Advisor** | Azure Advisor recommendations | Medium |\n| **DefenderRecommendations** | Microsoft Defender for Cloud findings | High |\n| **Azure Policy** | Non-compliant resources per Azure Policy | Medium |\n| **Costs** | 3-month cost history by subscription | Low |\n| **Defender** | Defender plan status and tiers | Medium |\n| **OutOfScope** | Resources not scanned | Low |\n\n**Focus analysis on:**\n1. High-severity recommendations from ImpactedResources\n2. Defender recommendations (security-critical)\n3. Advisor recommendations (reliability/performance)\n4. Policy non-compliance (governance)\n\n### Step 4: Categorize Findings\n\nGroup findings by category for prioritized remediation:\n\n| Category | Examples | Severity |\n|----------|----------|----------|\n| **Security** | Public endpoints, missing encryption, no private endpoints | Critical |\n| **Reliability** | No zone redundancy, single instance, no backup | High |\n| **Performance** | Undersized SKUs, missing caching, no CDN | Medium |\n| **Cost** | Orphaned resources, oversized SKUs, unused reservations | Medium |\n| **Operations** | Missing diagnostics, no alerts, no tags | Low |\n\n### Step 5: Generate Remediation Guidance\n\nFor each high-priority finding:\n1. Explain the risk in plain language\n2. Provide remediation options (Portal, CLI, Bicep)\n3. Estimate effort and impact\n\nSee [references/REMEDIATION-PATTERNS.md](references/REMEDIATION-PATTERNS.md) for common fix templates.\n\n### Step 6: Present Summary\n\nProvide a structured summary:\n\n```markdown\n## Compliance Assessment Summary\n\n**Scope:** [Subscription/RG/MG name]\n**Scanned:** [Date/Time]\n**Resources Analyzed:** [Count]\n\n### Key Findings\n\n| Severity | Count | Top Issues |\n|----------|-------|------------|\n| Critical | X | [List top 3] |\n| High | X | [List top 3] |\n| Medium | X | [List top 3] |\n\n### Recommended Actions\n\n1. **[Issue]** - [Brief remediation]\n2. **[Issue]** - [Brief remediation]\n3. **[Issue]** - [Brief remediation]\n\n### Next Steps\n- [ ] Address critical security findings\n- [ ] Review and remediate high-severity items\n- [ ] Schedule follow-up scan to verify fixes\n```\n\n## Supported Azure Services\n\nazqr supports 70+ Azure resource types including:\n\n- Azure Kubernetes Service (AKS)\n- API Management\n- App Configuration\n- App Service\n- Container Apps\n- Cosmos DB\n- Container Registry\n- Key Vault\n- Load Balancer\n- Azure Database for MySQL\n- Azure Database for PostgreSQL\n- Azure Cache for Redis\n- Service Bus\n- Azure SQL Database\n- Storage Accounts\n- Virtual Machines\n- Virtual Networks\n\n## Tools Used\n\n| Tool | Purpose |\n|------|---------|\n| `mcp_azure_mcp_extension_azqr` | Run azqr scans via Azure MCP |\n| `mcp_azure_mcp_subscription_list` | List available subscriptions |\n| `mcp_azure_mcp_group_list` | List resource groups in subscription |\n\n## Troubleshooting\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| Permission denied | 403 errors during scan | Verify Reader role on scope |\n| Not authenticated | `AADSTS` errors | Run `az login` first |\n| Slow scan | Scan takes very long | Use resource group scope |\n\n## Example Prompts\n\n- \"Check my Azure subscription for compliance issues\"\n- \"Run azqr on my production resource group\"\n- \"What Azure resources don't follow best practices?\"\n- \"Assess my storage accounts for security issues\"\n\n## Reference Documentation\n\n- [Recommendation Categories](references/RECOMMENDATIONS.md)\n- [Remediation Patterns](references/REMEDIATION-PATTERNS.md)\n- [Azure Quick Review Documentation](https://azure.github.io/azqr/docs/)\n- [Azure Proactive Resiliency Library](https://aka.ms/aprl)\n",
        "plugin/skills/azure-quick-review/references/RECOMMENDATIONS.md": "# azqr Recommendation Categories\n\nThis document describes how to interpret azqr recommendations and prioritize remediation.\n\n## Recommendation Sources\n\nazqr aggregates recommendations from multiple sources:\n\n| Source | Description | Priority |\n|--------|-------------|----------|\n| **APRL** | Azure Proactive Resiliency Library - reliability-focused best practices | High |\n| **Orphaned Resources** | Resources that are unused or disconnected | Medium |\n| **Azure Advisor** | Microsoft's built-in recommendation engine | Medium |\n| **Defender for Cloud** | Security-focused recommendations | Critical |\n| **Azure Policy** | Governance compliance status | Varies |\n\n## Impact Categories\n\n### Reliability\n\nRecommendations that affect service availability and resiliency:\n\n| Issue | Risk | Example Resources |\n|-------|------|-------------------|\n| No zone redundancy | Single zone failure causes outage | VMs, Storage, SQL, AKS |\n| Single instance | No failover capability | App Service, Redis, VMs |\n| No backup configured | Data loss risk | VMs, SQL, Cosmos DB |\n| No disaster recovery | Regional failure exposure | Storage, SQL, Key Vault |\n\n### Security\n\nRecommendations that affect security posture:\n\n| Issue | Risk | Example Resources |\n|-------|------|-------------------|\n| Public endpoint exposed | Attack surface exposure | Storage, SQL, Key Vault |\n| Missing encryption | Data exposure risk | Storage, Disks, SQL |\n| No private endpoint | Traffic on public internet | PaaS services |\n| Weak TLS version | Protocol vulnerabilities | App Service, API Management |\n| No managed identity | Credential management risk | App Service, Functions, AKS |\n\n### Operational Excellence\n\nRecommendations for better operations:\n\n| Issue | Risk | Example Resources |\n|-------|------|-------------------|\n| No diagnostic settings | Blind to failures | All resources |\n| Missing alerts | Delayed incident response | All resources |\n| No tags | Governance/cost tracking gaps | All resources |\n| Outdated SKU/version | Missing features/security fixes | All resources |\n\n### Cost Optimization\n\nRecommendations to reduce spending:\n\n| Issue | Risk | Example Resources |\n|-------|------|-------------------|\n| Orphaned disk | Paying for unused storage | Managed Disks |\n| Orphaned public IP | Paying for unused IP | Public IP |\n| Oversized SKU | Excess capacity cost | VMs, SQL, App Service |\n| No reserved capacity | Missing discounts | VMs, SQL, Cosmos DB |\n\n## Severity Levels\n\nPrioritize remediation using this severity matrix:\n\n| Severity | Criteria | Response Time |\n|----------|----------|---------------|\n| **Critical** | Security vulnerability with active exploit risk | Immediate |\n| **High** | Reliability risk affecting availability | Within 1 week |\n| **Medium** | Best practice violation with moderate risk | Within 1 month |\n| **Low** | Optimization opportunity | As capacity allows |\n\n## Excel Report Columns\n\n### Recommendations Sheet\n\n| Column | Description |\n|--------|-------------|\n| Recommendation ID | Unique identifier for the recommendation |\n| Category | Reliability, Security, Cost, etc. |\n| Recommendation | Description of the issue |\n| Learn More | Link to documentation |\n| Impacted Resources | Count of affected resources |\n\n### ImpactedResources Sheet\n\n| Column | Description |\n|--------|-------------|\n| Subscription | Subscription ID (may be masked) |\n| Resource Group | Resource group name |\n| Type | Azure resource type |\n| Name | Resource name |\n| Recommendation ID | Links to Recommendations sheet |\n| Recommendation | Issue description |\n| Learn More | Documentation link |\n| Param1-5 | Additional context (varies by recommendation) |\n\n### Inventory Sheet\n\n| Column | Description |\n|--------|-------------|\n| Subscription | Subscription ID |\n| Resource Group | Resource group name |\n| Location | Azure region |\n| Type | Resource type |\n| Name | Resource name |\n| SKU | SKU tier/name |\n| SLA | Calculated SLA percentage |\n| Availability Zones | Zone configuration |\n| Private Endpoint | Private endpoint status |\n| Diagnostic Settings | Diagnostic configuration status |\n\n## Common Recommendation IDs\n\nHigh-impact recommendations to prioritize:\n\n### Storage Accounts\n\n| ID | Issue |\n|----|-------|\n| `st-001` | Enable soft delete for blobs |\n| `st-002` | Enable soft delete for containers |\n| `st-003` | Enable versioning |\n| `st-004` | Use private endpoints |\n| `st-005` | Disable public blob access |\n\n### Virtual Machines\n\n| ID | Issue |\n|----|-------|\n| `vm-001` | Enable Azure Backup |\n| `vm-002` | Use managed disks |\n| `vm-003` | Deploy in availability zones |\n| `vm-004` | Enable boot diagnostics |\n| `vm-005` | Use managed identity |\n\n### Azure Kubernetes Service\n\n| ID | Issue |\n|----|-------|\n| `aks-001` | Enable Azure Policy |\n| `aks-002` | Use managed identity |\n| `aks-003` | Enable Defender for Containers |\n| `aks-004` | Use availability zones |\n| `aks-005` | Enable cluster autoscaler |\n\n### Key Vault\n\n| ID | Issue |\n|----|-------|\n| `kv-001` | Enable soft delete |\n| `kv-002` | Enable purge protection |\n| `kv-003` | Use private endpoints |\n| `kv-004` | Enable diagnostic logging |\n| `kv-005` | Use RBAC for data plane |\n\n### SQL Database\n\n| ID | Issue |\n|----|-------|\n| `sql-001` | Enable Transparent Data Encryption |\n| `sql-002` | Enable auditing |\n| `sql-003` | Use private endpoints |\n| `sql-004` | Enable zone redundancy |\n| `sql-005` | Enable Advanced Threat Protection |\n\n## Additional Resources\n\n- [Azure Proactive Resiliency Library](https://aka.ms/aprl)\n- [Azure Orphaned Resources](https://github.com/dolevshor/azure-orphan-resources)\n- [Azure Advisor Documentation](https://learn.microsoft.com/azure/advisor/)\n- [Defender for Cloud Recommendations](https://learn.microsoft.com/azure/defender-for-cloud/recommendations-reference)\n",
        "plugin/skills/azure-quick-review/references/REMEDIATION-PATTERNS.md": "# Remediation Patterns for Common azqr Findings\n\nThis document provides remediation templates for frequently identified compliance issues.\n\n## Storage Account Issues\n\n### Enable Private Endpoints\n\n**Issue:** Storage account accessible via public endpoint\n\n**Azure CLI:**\n```bash\n# Create private endpoint\naz network private-endpoint create \\\n  --name pe-storage \\\n  --resource-group <rg-name> \\\n  --vnet-name <vnet-name> \\\n  --subnet <subnet-name> \\\n  --private-connection-resource-id $(az storage account show -n <storage-name> -g <rg-name> --query id -o tsv) \\\n  --group-id blob \\\n  --connection-name pe-storage-connection\n\n# Disable public access\naz storage account update \\\n  --name <storage-name> \\\n  --resource-group <rg-name> \\\n  --public-network-access Disabled\n```\n\n**Bicep:**\n```bicep\nresource privateEndpoint 'Microsoft.Network/privateEndpoints@2023-05-01' = {\n  name: 'pe-${storageAccount.name}'\n  location: location\n  properties: {\n    subnet: {\n      id: subnet.id\n    }\n    privateLinkServiceConnections: [\n      {\n        name: 'pe-${storageAccount.name}-connection'\n        properties: {\n          privateLinkServiceId: storageAccount.id\n          groupIds: ['blob']\n        }\n      }\n    ]\n  }\n}\n```\n\n### Enable Soft Delete\n\n**Issue:** No soft delete protection for blobs\n\n**Azure CLI:**\n```bash\naz storage account blob-service-properties update \\\n  --account-name <storage-name> \\\n  --resource-group <rg-name> \\\n  --enable-delete-retention true \\\n  --delete-retention-days 7 \\\n  --enable-container-delete-retention true \\\n  --container-delete-retention-days 7\n```\n\n**Bicep:**\n```bicep\nresource blobServices 'Microsoft.Storage/storageAccounts/blobServices@2023-01-01' = {\n  parent: storageAccount\n  name: 'default'\n  properties: {\n    deleteRetentionPolicy: {\n      enabled: true\n      days: 7\n    }\n    containerDeleteRetentionPolicy: {\n      enabled: true\n      days: 7\n    }\n  }\n}\n```\n\n---\n\n## Key Vault Issues\n\n### Enable Purge Protection\n\n**Issue:** Key Vault can be permanently deleted\n\n**Azure CLI:**\n```bash\naz keyvault update \\\n  --name <vault-name> \\\n  --resource-group <rg-name> \\\n  --enable-purge-protection true\n```\n\n**Bicep:**\n```bicep\nresource keyVault 'Microsoft.KeyVault/vaults@2023-07-01' = {\n  name: keyVaultName\n  location: location\n  properties: {\n    enableSoftDelete: true\n    softDeleteRetentionInDays: 90\n    enablePurgeProtection: true\n    // ... other properties\n  }\n}\n```\n\n### Use RBAC for Data Plane\n\n**Issue:** Using access policies instead of RBAC\n\n**Azure CLI:**\n```bash\naz keyvault update \\\n  --name <vault-name> \\\n  --resource-group <rg-name> \\\n  --enable-rbac-authorization true\n```\n\n---\n\n## Virtual Machine Issues\n\n### Enable Diagnostic Settings\n\n**Issue:** No diagnostics configured for VM\n\n**Azure CLI:**\n```bash\n# Create Log Analytics workspace (if needed)\naz monitor log-analytics workspace create \\\n  --resource-group <rg-name> \\\n  --workspace-name <workspace-name>\n\n# Enable diagnostics\naz monitor diagnostic-settings create \\\n  --name diag-vm \\\n  --resource $(az vm show -g <rg-name> -n <vm-name> --query id -o tsv) \\\n  --workspace $(az monitor log-analytics workspace show -g <rg-name> -n <workspace-name> --query id -o tsv) \\\n  --metrics '[{\"category\": \"AllMetrics\", \"enabled\": true}]'\n```\n\n**Bicep:**\n```bicep\nresource diagnosticSettings 'Microsoft.Insights/diagnosticSettings@2021-05-01-preview' = {\n  name: 'diag-${vm.name}'\n  scope: vm\n  properties: {\n    workspaceId: logAnalyticsWorkspace.id\n    metrics: [\n      {\n        category: 'AllMetrics'\n        enabled: true\n      }\n    ]\n  }\n}\n```\n\n### Enable Azure Backup\n\n**Issue:** VM not protected by Azure Backup\n\n**Azure CLI:**\n```bash\n# Create Recovery Services vault (if needed)\naz backup vault create \\\n  --resource-group <rg-name> \\\n  --name <vault-name> \\\n  --location <location>\n\n# Enable backup with default policy\naz backup protection enable-for-vm \\\n  --resource-group <rg-name> \\\n  --vault-name <vault-name> \\\n  --vm $(az vm show -g <rg-name> -n <vm-name> --query id -o tsv) \\\n  --policy-name DefaultPolicy\n```\n\n---\n\n## AKS Issues\n\n### Enable Defender for Containers\n\n**Issue:** No security monitoring for AKS\n\n**Azure CLI:**\n```bash\naz aks update \\\n  --resource-group <rg-name> \\\n  --name <cluster-name> \\\n  --enable-defender\n```\n\n**Bicep:**\n```bicep\nresource aksCluster 'Microsoft.ContainerService/managedClusters@2024-01-01' = {\n  name: clusterName\n  location: location\n  properties: {\n    securityProfile: {\n      defender: {\n        securityMonitoring: {\n          enabled: true\n        }\n        logAnalyticsWorkspaceResourceId: logAnalyticsWorkspace.id\n      }\n    }\n    // ... other properties\n  }\n}\n```\n\n### Use Managed Identity\n\n**Issue:** AKS using service principal instead of managed identity\n\n**Azure CLI:**\n```bash\naz aks update \\\n  --resource-group <rg-name> \\\n  --name <cluster-name> \\\n  --enable-managed-identity\n```\n\n---\n\n## SQL Database Issues\n\n### Enable Auditing\n\n**Issue:** SQL Server auditing not enabled\n\n**Azure CLI:**\n```bash\n# Enable to Log Analytics\naz sql server audit-policy update \\\n  --resource-group <rg-name> \\\n  --name <server-name> \\\n  --state Enabled \\\n  --lats Enabled \\\n  --lawri $(az monitor log-analytics workspace show -g <rg-name> -n <workspace-name> --query id -o tsv)\n```\n\n**Bicep:**\n```bicep\nresource sqlAudit 'Microsoft.Sql/servers/auditingSettings@2023-05-01-preview' = {\n  parent: sqlServer\n  name: 'default'\n  properties: {\n    state: 'Enabled'\n    isAzureMonitorTargetEnabled: true\n    retentionDays: 90\n  }\n}\n```\n\n### Enable Private Endpoint\n\n**Issue:** SQL Server accessible via public endpoint\n\n**Azure CLI:**\n```bash\n# Create private endpoint\naz network private-endpoint create \\\n  --name pe-sql \\\n  --resource-group <rg-name> \\\n  --vnet-name <vnet-name> \\\n  --subnet <subnet-name> \\\n  --private-connection-resource-id $(az sql server show -g <rg-name> -n <server-name> --query id -o tsv) \\\n  --group-id sqlServer \\\n  --connection-name pe-sql-connection\n\n# Disable public access\naz sql server update \\\n  --resource-group <rg-name> \\\n  --name <server-name> \\\n  --enable-public-network false\n```\n\n---\n\n## App Service Issues\n\n### Use Managed Identity\n\n**Issue:** App Service not using managed identity\n\n**Azure CLI:**\n```bash\naz webapp identity assign \\\n  --resource-group <rg-name> \\\n  --name <app-name>\n```\n\n**Bicep:**\n```bicep\nresource webApp 'Microsoft.Web/sites@2023-01-01' = {\n  name: appName\n  location: location\n  identity: {\n    type: 'SystemAssigned'\n  }\n  properties: {\n    // ... other properties\n  }\n}\n```\n\n### Enforce HTTPS Only\n\n**Issue:** HTTP traffic allowed\n\n**Azure CLI:**\n```bash\naz webapp update \\\n  --resource-group <rg-name> \\\n  --name <app-name> \\\n  --https-only true\n```\n\n### Set Minimum TLS Version\n\n**Issue:** TLS version below 1.2\n\n**Azure CLI:**\n```bash\naz webapp config set \\\n  --resource-group <rg-name> \\\n  --name <app-name> \\\n  --min-tls-version 1.2\n```\n\n---\n\n## Bulk Remediation Script\n\nFor multiple resources of the same type, use a loop:\n\n```powershell\n# Example: Enable soft delete on all storage accounts\n$storageAccounts = az storage account list --query \"[].{name:name, rg:resourceGroup}\" -o json | ConvertFrom-Json\n\nforeach ($sa in $storageAccounts) {\n    Write-Host \"Enabling soft delete on $($sa.name)...\"\n    az storage account blob-service-properties update `\n        --account-name $sa.name `\n        --resource-group $sa.rg `\n        --enable-delete-retention true `\n        --delete-retention-days 7\n}\n```\n\n---\n\n## Remediation Validation\n\nAfter applying fixes, re-run the azqr scan using the Azure MCP tool to verify the issues have been resolved:\n\n```\nmcp_azure_mcp_extension_azqr\n  subscription: <subscription-id>\n```\n\n## Additional Resources\n\n- [Azure CLI Reference](https://learn.microsoft.com/cli/azure/)\n- [Bicep Documentation](https://learn.microsoft.com/azure/azure-resource-manager/bicep/)\n- [Azure Policy Built-in Definitions](https://learn.microsoft.com/azure/governance/policy/samples/built-in-policies)\n",
        "plugin/skills/azure-redis/SKILL.md": "---\nname: azure-redis\ndescription: Implement high-performance caching and real-time data solutions with Azure Cache for Redis for session state, leaderboards, and distributed caching\n---\n\n# Azure Data Services\n\n## Services\n\n| Service | Use When | MCP Tools | CLI |\n|---------|----------|-----------|-----|\n| Cosmos DB | NoSQL documents, global distribution, vector search | `azure__cosmos` | `az cosmosdb` |\n| SQL Database | Relational data, ACID transactions, complex joins | `azure__sql` | `az sql` |\n| Redis Cache | Caching, sessions, real-time leaderboards | `azure__redis` | `az redis` |\n| PostgreSQL | Open source relational, PostGIS | `azure__postgres` | `az postgres` |\n| MySQL | LAMP stack, WordPress | `azure__mysql` | `az mysql` |\n\n## MCP Server (Preferred)\n\nWhen Azure MCP is enabled, use these tools for data operations:\n\n### Cosmos DB\n- `azure__cosmos` with command `cosmos_account_list` - List Cosmos DB accounts\n- `azure__cosmos` with command `cosmos_database_list` - List databases in account\n- `azure__cosmos` with command `cosmos_container_list` - List containers\n\n### SQL Database\n- `azure__sql` with command `sql_server_list` - List SQL servers\n- `azure__sql` with command `sql_database_list` - List databases on server\n- `azure__sql` with command `sql_firewall_list` - List firewall rules\n\n### Redis\n- `azure__redis` with command `redis_cache_list` - List Redis caches\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n## CLI Fallback\n\n```bash\n# Cosmos DB\naz cosmosdb list --output table\naz cosmosdb sql database list --account-name ACCOUNT -g RG\n\n# SQL Database\naz sql server list --output table\naz sql db list --server SERVER -g RG\n\n# Redis\naz redis list --output table\n```\n\n## Choosing the Right Database\n\n| If you need... | Use |\n|----------------|-----|\n| Global distribution, <10ms latency | Cosmos DB |\n| Complex SQL queries, ACID transactions | SQL Database |\n| Caching layer, session state | Redis Cache |\n| PostgreSQL compatibility | Azure PostgreSQL |\n| MySQL compatibility | Azure MySQL |\n\n---\n\n# Azure Cache for Redis\n\n## Quick Reference\n\n| Property | Value |\n|----------|-------|\n| CLI prefix | `az redis` |\n| MCP tools | `azure__redis` (command: `redis_cache_list`) |\n| Best for | Caching, sessions, real-time data |\n\n## Tiers\n\n| Tier | Features | Use Case |\n|------|----------|----------|\n| Basic | Single node, no SLA | Dev/test |\n| Standard | Replicated, 99.9% SLA | Production |\n| Premium | Clustering, VNet, persistence | High scale |\n| Enterprise | Redis modules, 99.99% SLA | Enterprise features |\n\n## Caching Patterns\n\n### Cache-Aside (Lazy Loading)\n\n```\n1. Check cache first\n2. If miss, query database\n3. Store result in cache\n4. Return data\n```\n\n### Write-Through\n\n```\n1. Write to cache\n2. Cache writes to database\n3. Ensures consistency\n```\n\n### Write-Behind\n\n```\n1. Write to cache\n2. Async write to database\n3. Better performance, eventual consistency\n```\n\n## Common Use Cases\n\n| Use Case | Pattern |\n|----------|---------|\n| Session state | String/Hash with TTL |\n| Page caching | String with cache-aside |\n| Rate limiting | INCR with expiry |\n| Leaderboards | Sorted sets |\n| Pub/sub | Pub/sub channels |\n| Distributed locks | SET NX with expiry |\n\n## Best Practices\n\n1. **Set appropriate TTLs** - Avoid stale data\n2. **Use connection pooling** - Reuse connections\n3. **Enable clustering** for scale-out\n4. **Configure eviction policy** (allkeys-lru recommended)\n5. **Monitor memory usage** - Avoid evictions\n\n## Common Operations\n\n```bash\n# List Redis caches\naz redis list --output table\n\n# Get cache details\naz redis show -n CACHE -g RG\n\n# Get access keys\naz redis list-keys -n CACHE -g RG\n\n# Regenerate keys\naz redis regenerate-keys -n CACHE -g RG --key-type Primary\n```\n\n## Connection String Format\n\n```\nCACHE.redis.cache.windows.net:6380,password=KEY,ssl=True,abortConnect=False\n```\n\n## Gotchas\n\n1. **Connection limits** - Pool connections, don't create per-request\n2. **Serialization** - Be consistent, consider MessagePack for performance\n3. **Key naming** - Use prefixes to avoid collisions\n4. **Memory management** - Monitor and configure maxmemory-policy\n",
        "plugin/skills/azure-resource-visualizer/SKILL.md": "---\nname: azure-resource-visualizer\ndescription: Analyze Azure resource groups and generate detailed Mermaid architecture diagrams showing the relationships between individual resources. Use this skill when the user asks for a diagram of their Azure resources or help in understanding how the resources relate to each other.\n---\n\n# Azure Resource Visualizer - Architecture Diagram Generator\n\nA user may ask for help understanding how individual resources fit together, or to create a diagram showing their relationships. Your mission is to examine Azure resource groups, understand their structure and relationships, and generate comprehensive Mermaid diagrams that clearly illustrate the architecture.\n\n## Core Responsibilities\n\n1. **Resource Group Discovery**: List available resource groups when not specified\n2. **Deep Resource Analysis**: Examine all resources, their configurations, and interdependencies\n3. **Relationship Mapping**: Identify and document all connections between resources\n4. **Diagram Generation**: Create detailed, accurate Mermaid diagrams\n5. **Documentation Creation**: Produce clear markdown files with embedded diagrams\n\n## Workflow Process\n\n### Step 1: Resource Group Selection\n\nIf the user hasn't specified a resource group:\n\n1. Use your tools to query available resource groups. If you do not have a tool for this, use `az`.\n2. Present a numbered list of resource groups with their locations\n3. Ask the user to select one by number or name\n4. Wait for user response before proceeding\n\nIf a resource group is specified, validate it exists and proceed.\n\n### Step 2: Resource Discovery & Analysis\n\nOnce you have the resource group:\n\n1. **Query all resources** in the resource group using Azure MCP tools or `az`.\n2. **Analyze each resource** type and capture:\n   - Resource name and type\n   - SKU/tier information\n   - Location/region\n   - Key configuration properties\n   - Network settings (VNets, subnets, private endpoints)\n   - Identity and access (Managed Identity, RBAC)\n   - Dependencies and connections\n\n3. **Map relationships** by identifying:\n   - **Network connections**: VNet peering, subnet assignments, NSG rules, private endpoints\n   - **Data flow**: Apps â†’ Databases, Functions â†’ Storage, API Management â†’ Backends\n   - **Identity**: Managed identities connecting to resources\n   - **Configuration**: App Settings pointing to Key Vaults, connection strings\n   - **Dependencies**: Parent-child relationships, required resources\n\n### Step 3: Diagram Construction\n\nCreate a **detailed Mermaid diagram** using the `graph TB` (top-to-bottom) or `graph LR` (left-to-right) format:\n\n**Diagram Structure Guidelines:**\n\n```mermaid\ngraph TB\n    %% Use subgraphs to group related resources\n    subgraph \"Resource Group: [name]\"\n        subgraph \"Network Layer\"\n            VNET[Virtual Network<br/>10.0.0.0/16]\n            SUBNET1[Subnet: web<br/>10.0.1.0/24]\n            SUBNET2[Subnet: data<br/>10.0.2.0/24]\n            NSG[Network Security Group]\n        end\n        \n        subgraph \"Compute Layer\"\n            APP[App Service<br/>Plan: P1v2]\n            FUNC[Function App<br/>Runtime: .NET 8]\n        end\n        \n        subgraph \"Data Layer\"\n            SQL[Azure SQL Database<br/>DTU: S1]\n            STORAGE[Storage Account<br/>Type: Standard LRS]\n        end\n        \n        subgraph \"Security & Identity\"\n            KV[Key Vault]\n            MI[Managed Identity]\n        end\n    end\n    \n    %% Define relationships with descriptive labels\n    APP -->|\"HTTPS requests\"| FUNC\n    FUNC -->|\"SQL connection\"| SQL\n    FUNC -->|\"Blob/Queue access\"| STORAGE\n    APP -->|\"Uses identity\"| MI\n    MI -->|\"Access secrets\"| KV\n    VNET --> SUBNET1\n    VNET --> SUBNET2\n    SUBNET1 --> APP\n    SUBNET2 --> SQL\n    NSG -->|\"Rules applied to\"| SUBNET1\n```\n\n**Key Diagram Requirements:**\n\n- **Group by layer or purpose**: Network, Compute, Data, Security, Monitoring\n- **Include details**: SKUs, tiers, important settings in node labels (use `<br/>` for line breaks)\n- **Label all connections**: Describe what flows between resources (data, identity, network)\n- **Use meaningful node IDs**: Abbreviations that make sense (APP, FUNC, SQL, KV)\n- **Visual hierarchy**: Subgraphs for logical grouping\n- **Connection types**:\n  - `-->` for data flow or dependencies\n  - `-.->` for optional/conditional connections\n  - `==>` for critical/primary paths\n\n**Resource Type Examples:**\n- App Service: Include plan tier (B1, S1, P1v2)\n- Functions: Include runtime (.NET, Python, Node)\n- Databases: Include tier (Basic, Standard, Premium)\n- Storage: Include redundancy (LRS, GRS, ZRS)\n- VNets: Include address space\n- Subnets: Include address range\n\n### Step 4: File Creation\n\nUse [template-architecture.md](./assets/template-architecture.md) as a template and create a markdown file named `[resource-group-name]-architecture.md` with:\n\n1. **Header**: Resource group name, subscription, region\n2. **Summary**: Brief overview of the architecture (2-3 paragraphs)\n3. **Resource Inventory**: Table listing all resources with types and key properties\n4. **Architecture Diagram**: The complete Mermaid diagram\n5. **Relationship Details**: Explanation of key connections and data flows\n6. **Notes**: Any important observations, potential issues, or recommendations\n\n## Operating Guidelines\n\n### Quality Standards\n\n- **Accuracy**: Verify all resource details before including in diagram\n- **Completeness**: Don't omit resources; include everything in the resource group\n- **Clarity**: Use clear, descriptive labels and logical grouping\n- **Detail Level**: Include configuration details that matter for architecture understanding\n- **Relationships**: Show ALL significant connections, not just obvious ones\n\n### Tool Usage Patterns\n\n1. **Azure MCP Search**: \n   - Use `intent=\"list resource groups\"` to discover resource groups\n   - Use `intent=\"list resources in group\"` with group name to get all resources\n   - Use `intent=\"get resource details\"` for individual resource analysis\n   - Use `command` parameter when you need specific Azure operations\n\n2. **File Creation**:\n   - Always create in workspace root or a `docs/` folder if it exists\n   - Use clear, descriptive filenames: `[rg-name]-architecture.md`\n   - Ensure Mermaid syntax is valid (test syntax mentally before output)\n\n3. **Terminal (when needed)**:\n   - Use Azure CLI for complex queries not available via MCP\n   - Example: `az resource list --resource-group <name> --output json`\n   - Example: `az network vnet show --resource-group <name> --name <vnet-name>`\n\n### Constraints & Boundaries\n\n**Always Do:**\n- âœ… List resource groups if not specified\n- âœ… Wait for user selection before proceeding\n- âœ… Analyze ALL resources in the group\n- âœ… Create detailed, accurate diagrams\n- âœ… Include configuration details in node labels\n- âœ… Group resources logically with subgraphs\n- âœ… Label all connections descriptively\n- âœ… Create a complete markdown file with diagram\n\n**Never Do:**\n- âŒ Skip resources because they seem unimportant\n- âŒ Make assumptions about resource relationships without verification\n- âŒ Create incomplete or placeholder diagrams\n- âŒ Omit configuration details that affect architecture\n- âŒ Proceed without confirming resource group selection\n- âŒ Generate invalid Mermaid syntax\n- âŒ Modify or delete Azure resources (read-only analysis)\n\n### Edge Cases & Error Handling\n\n- **No resources found**: Inform user and verify resource group name\n- **Permission issues**: Explain what's missing and suggest checking RBAC\n- **Complex architectures (50+ resources)**: Consider creating multiple diagrams by layer\n- **Cross-resource-group dependencies**: Note external dependencies in diagram notes\n- **Resources without clear relationships**: Group in \"Other Resources\" section\n\n## Output Format Specifications\n\n### Mermaid Diagram Syntax\n- Use `graph TB` (top-to-bottom) for vertical layouts\n- Use `graph LR` (left-to-right) for horizontal layouts (better for wide architectures)\n- Subgraph syntax: `subgraph \"Descriptive Name\"`\n- Node syntax: `ID[\"Display Name<br/>Details\"]`\n- Connection syntax: `SOURCE -->|\"Label\"| TARGET`\n\n### Markdown Structure\n- Use H1 for main title\n- Use H2 for major sections\n- Use H3 for subsections\n- Use tables for resource inventories\n- Use bullet lists for notes and recommendations\n- Use code blocks with `mermaid` language tag for diagrams\n\n## Example Interaction\n\n**User**: \"Analyze my production resource group\"\n\n**Agent**:\n1. Lists all resource groups in subscription\n2. Asks user to select: \"Which resource group? 1) rg-prod-app, 2) rg-dev-app, 3) rg-shared\"\n3. User selects: \"1\"\n4. Queries all resources in rg-prod-app\n5. Analyzes: App Service, Function App, SQL Database, Storage Account, Key Vault, VNet, NSG\n6. Identifies relationships: App â†’ Function, Function â†’ SQL, Function â†’ Storage, All â†’ Key Vault\n7. Creates detailed Mermaid diagram with subgraphs\n8. Generates `rg-prod-app-architecture.md` with complete documentation\n9. Displays: \"Created architecture diagram in rg-prod-app-architecture.md. Found 7 resources with 8 key relationships.\"\n\n## Success Criteria\n\nA successful analysis includes:\n- âœ… Valid resource group identified\n- âœ… All resources discovered and analyzed\n- âœ… All significant relationships mapped\n- âœ… Detailed Mermaid diagram with proper grouping\n- âœ… Complete markdown file created\n- âœ… Clear, actionable documentation\n- âœ… Valid Mermaid syntax that renders correctly\n- âœ… Professional, architect-level output\n\nYour goal is to provide clarity and insight into Azure architectures, making complex resource relationships easy to understand through excellent visualization.\n",
        "plugin/skills/azure-resource-visualizer/assets/template-architecture.md": "# Azure Architecture: [Resource Group Name]\n\n**Subscription**: [subscription-name]  \n**Region**: [primary-region]  \n**Resource Count**: [count]  \n**Generated**: [date]\n\n## Overview\n\n[2-3 paragraph summary of the architecture, its purpose, and key components]\n\n## Resource Inventory\n\n| Resource Name | Type | Tier/SKU | Location | Notes |\n|--------------|------|----------|----------|-------|\n| app-prod-001 | App Service | P1v2 | East US | Production web app |\n| func-prod-001 | Function App | Y1 | East US | Consumption plan |\n\n## Architecture Diagram\n\n```mermaid\n[full diagram here]\n```\n\n## Relationship Details\n\n### Network Architecture\n[Describe VNets, subnets, network security]\n\n### Data Flow\n[Describe how data moves between components]\n\n### Identity & Access\n[Describe managed identities, key vault access, RBAC]\n\n### Dependencies\n[Describe critical dependencies and their order]\n\n## Notes & Recommendations\n\n[Any observations, potential issues, or suggestions]\n",
        "plugin/skills/azure-role-selector/SKILL.md": "---\nname: azure-role-selector\ndescription: When a user is asking for guidance for which role to assign to an identity given desired permissions, this agent helps them understand the role that will meet the requirements with least privilege access and how to apply that role.\n---\nUse the 'azure__documentation' tool to find the minimal role definition that matches the desired permissions the user wants to assign to an identity. If no built-in role matches the desired permissions, use the 'azure__extension_cli_generate' tool to create a custom role definition with the desired permissions. Then use the 'azure__extension_cli_generate' tool to generate the CLI commands needed to assign that role to the identity. Finally, use the 'azure__bicepschema' and 'azure__get_azure_bestpractices' tools to provide a Bicep code snippet for adding the role assignment.\n",
        "plugin/skills/azure-security-hardening/SKILL.md": "---\nname: azure-security-hardening\ndescription: Secure Azure resources following Zero Trust principles. Covers managed identities, RBAC best practices, Key Vault security, network security with private endpoints and NSGs, encryption, and Microsoft Defender for Cloud configuration.\n---\n\n# Securing Azure Resources\n\n## Security Principles\n\n1. **Zero Trust** - Never trust, always verify\n2. **Least Privilege** - Minimum required permissions\n3. **Defense in Depth** - Multiple security layers\n4. **Encryption Everywhere** - At rest and in transit\n\n## Essential Security Checklist\n\n### Identity and Access\n- [ ] Use managed identities (no credentials)\n- [ ] Enable MFA for all users\n- [ ] Apply least privilege RBAC\n- [ ] Use Azure AD for authentication\n- [ ] Review access regularly\n\n### Network Security\n- [ ] Use private endpoints for PaaS\n- [ ] Configure NSGs on all subnets\n- [ ] Disable public endpoints where possible\n- [ ] Enable DDoS protection\n- [ ] Use Azure Firewall or NVA\n\n### Data Protection\n- [ ] Enable encryption at rest (default)\n- [ ] Use TLS 1.2+ for transit\n- [ ] Store secrets in Key Vault\n- [ ] Enable soft delete for Key Vault\n- [ ] Use customer-managed keys (CMK) for sensitive data\n\n### Monitoring\n- [ ] Enable Microsoft Defender for Cloud\n- [ ] Configure diagnostic logging\n- [ ] Set up security alerts\n- [ ] Enable audit logging\n\n## Key Vault Security\n\n```bash\n# Enable soft delete and purge protection\naz keyvault update \\\n  --name VAULT -g RG \\\n  --enable-soft-delete true \\\n  --enable-purge-protection true\n\n# Enable RBAC permission model\naz keyvault update \\\n  --name VAULT -g RG \\\n  --enable-rbac-authorization true\n```\n\n## Network Security\n\n### Private Endpoints\n\n```bash\n# Create private endpoint for storage\naz network private-endpoint create \\\n  --name myEndpoint -g RG \\\n  --vnet-name VNET --subnet SUBNET \\\n  --private-connection-resource-id STORAGE_ID \\\n  --group-id blob \\\n  --connection-name myConnection\n```\n\n### NSG Rules\n\n```bash\n# Deny all inbound by default\n# Allow only required traffic\naz network nsg rule create \\\n  --nsg-name NSG -g RG \\\n  --name AllowHTTPS \\\n  --priority 100 \\\n  --destination-port-ranges 443 \\\n  --access Allow\n```\n\n## RBAC Best Practices\n\n### Built-in Roles\n\n| Role | Use When |\n|------|----------|\n| Reader | View-only access |\n| Contributor | Full access except IAM |\n| Key Vault Secrets User | Read secrets only |\n| Storage Blob Data Reader | Read blobs only |\n\n### Apply Least Privilege\n\n```bash\n# Grant minimal role at resource scope\naz role assignment create \\\n  --role \"Storage Blob Data Reader\" \\\n  --assignee PRINCIPAL_ID \\\n  --scope /subscriptions/SUB/resourceGroups/RG/providers/Microsoft.Storage/storageAccounts/ACCOUNT\n```\n\n## Managed Identity\n\n### Enable on Services\n\n```bash\n# App Service\naz webapp identity assign --name APP -g RG\n\n# Container Apps\naz containerapp identity assign --name APP -g RG --system-assigned\n\n# Function App\naz functionapp identity assign --name APP -g RG\n```\n\n### Grant Access\n\n```bash\n# Grant Key Vault access\naz role assignment create \\\n  --role \"Key Vault Secrets User\" \\\n  --assignee IDENTITY_PRINCIPAL_ID \\\n  --scope /subscriptions/SUB/resourceGroups/RG/providers/Microsoft.KeyVault/vaults/VAULT\n```\n\n## Microsoft Defender for Cloud\n\n```bash\n# Enable Defender plans\naz security pricing create \\\n  --name VirtualMachines \\\n  --tier Standard\n```\n\n## Security by Service\n\n| Service | Key Security Features |\n|---------|----------------------|\n| SQL Database | TDE, Always Encrypted, AAD auth |\n| Cosmos DB | Encryption, firewall, private endpoint |\n| Storage | Encryption, SAS tokens, private endpoint |\n| AKS | Workload identity, network policy, private cluster |\n| Key Vault | RBAC, soft delete, purge protection |\n\n## Security Assessment\n\nUse Azure Security Center for:\n- Security score\n- Recommendations\n- Compliance assessment\n- Threat detection\n",
        "plugin/skills/azure-security/SKILL.md": "---\nname: azure-security\ndescription: Azure Security Services including Key Vault, Managed Identity, RBAC, Entra ID, and Defender. Provides secrets management, credential-free authentication, role-based access control, and threat protection.\n---\n\n# Azure Security Services\n\n## Services\n\n| Service | Use When | MCP Tools | CLI |\n|---------|----------|-----------|-----|\n| Key Vault | Secrets, keys, certificates | `azure__keyvault` | `az keyvault` |\n| Managed Identity | Credential-free authentication | - | `az identity` |\n| RBAC | Role-based access control | `azure__role` | `az role` |\n| Entra ID | Identity and access management | - | `az ad` |\n| Defender | Threat protection, security posture | - | `az security` |\n\n## MCP Server (Preferred)\n\nWhen Azure MCP is enabled:\n\n### Key Vault\n- `azure__keyvault` with command `keyvault_list` - List Key Vaults\n- `azure__keyvault` with command `keyvault_secret_list` - List secrets in vault\n- `azure__keyvault` with command `keyvault_secret_get` - Get secret value\n- `azure__keyvault` with command `keyvault_key_list` - List keys\n- `azure__keyvault` with command `keyvault_certificate_list` - List certificates\n\n### RBAC\n- `azure__role` with command `role_assignment_list` - List role assignments\n- `azure__role` with command `role_definition_list` - List role definitions\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n## CLI Fallback\n\n```bash\n# Key Vault\naz keyvault list --output table\naz keyvault secret list --vault-name VAULT --output table\naz keyvault secret show --vault-name VAULT --name SECRET\n\n# RBAC\naz role assignment list --output table\naz role definition list --output table\n\n# Managed Identity\naz identity list --output table\n```\n\n## Key Security Principles\n\n1. **Use managed identities** - No credentials to manage\n2. **Apply least privilege** - Minimum required permissions\n3. **Enable Key Vault** - Never hardcode secrets\n4. **Use private endpoints** - No public internet access\n5. **Enable auditing** - Log all access\n\n## Common RBAC Roles\n\n| Role | Permissions |\n|------|-------------|\n| Owner | Full access + assign roles |\n| Contributor | Full access, no role assignment |\n| Reader | Read-only |\n| Key Vault Secrets User | Read secrets only |\n| Storage Blob Data Reader | Read blobs only |\n\n## Service Details\n\nFor deep documentation on specific services:\n\n- Key Vault best practices -> [Key Vault documentation](https://learn.microsoft.com/azure/key-vault/general/overview)\n- Managed identity patterns -> [Managed identities documentation](https://learn.microsoft.com/azure/active-directory/managed-identities-azure-resources/overview)\n- RBAC configuration -> `azure-role-selector` skill or [Azure RBAC documentation](https://learn.microsoft.com/azure/role-based-access-control/overview)\n",
        "plugin/skills/azure-sql-database/SKILL.md": "---\nname: azure-sql-database\ndescription: Build enterprise applications with Azure SQL Database, a fully managed relational database with built-in intelligence, ACID transactions, and high availability\n---\n\n# Azure Data Services\n\n## Services\n\n| Service | Use When | MCP Tools | CLI |\n|---------|----------|-----------|-----|\n| Cosmos DB | NoSQL documents, global distribution, vector search | `azure__cosmos` | `az cosmosdb` |\n| SQL Database | Relational data, ACID transactions, complex joins | `azure__sql` | `az sql` |\n| Redis Cache | Caching, sessions, real-time leaderboards | `azure__redis` | `az redis` |\n| PostgreSQL | Open source relational, PostGIS | `azure__postgres` | `az postgres` |\n| MySQL | LAMP stack, WordPress | `azure__mysql` | `az mysql` |\n\n## MCP Server (Preferred)\n\nWhen Azure MCP is enabled, use these tools for data operations:\n\n### Cosmos DB\n- `azure__cosmos` with command `cosmos_account_list` - List Cosmos DB accounts\n- `azure__cosmos` with command `cosmos_database_list` - List databases in account\n- `azure__cosmos` with command `cosmos_container_list` - List containers\n\n### SQL Database\n- `azure__sql` with command `sql_server_list` - List SQL servers\n- `azure__sql` with command `sql_database_list` - List databases on server\n- `azure__sql` with command `sql_firewall_list` - List firewall rules\n\n### Redis\n- `azure__redis` with command `redis_cache_list` - List Redis caches\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n## CLI Fallback\n\n```bash\n# Cosmos DB\naz cosmosdb list --output table\naz cosmosdb sql database list --account-name ACCOUNT -g RG\n\n# SQL Database\naz sql server list --output table\naz sql db list --server SERVER -g RG\n\n# Redis\naz redis list --output table\n```\n\n## Choosing the Right Database\n\n| If you need... | Use |\n|----------------|-----|\n| Global distribution, <10ms latency | Cosmos DB |\n| Complex SQL queries, ACID transactions | SQL Database |\n| Caching layer, session state | Redis Cache |\n| PostgreSQL compatibility | Azure PostgreSQL |\n| MySQL compatibility | Azure MySQL |\n\n---\n\n# Azure SQL Database\n\n## Quick Reference\n\n| Property | Value |\n|----------|-------|\n| CLI prefix | `az sql` |\n| MCP tools | `azure__sql` (commands: `sql_server_list`, `sql_database_list`, `sql_firewall_list`) |\n| Best for | Relational data, ACID transactions, T-SQL |\n\n## Service Tiers\n\n| Tier | Use Case | vCores | Storage |\n|------|----------|--------|---------|\n| Basic | Dev/test | Shared | 2 GB |\n| Standard | Small production | Shared | 250 GB |\n| Premium | High IOPS | Dedicated | 4 TB |\n| Hyperscale | Large databases | Dedicated | 100 TB |\n| Serverless | Variable workloads | Auto-scale | 4 TB |\n\n## DTU vs vCore\n\n**DTU model:** Bundled compute/storage/IO - simpler pricing\n**vCore model:** Separate compute/storage - more control\n\nUse vCore for:\n- Existing SQL Server licenses (Azure Hybrid Benefit)\n- Fine-grained resource control\n- Reserved capacity pricing\n\n## High Availability\n\n| Option | SLA | Use Case |\n|--------|-----|----------|\n| Zone redundant | 99.995% | Regional HA |\n| Geo-replication | RPO <5s | Disaster recovery |\n| Auto-failover groups | Automatic | Multi-region HA |\n\n## Security Best Practices\n\n1. Enable Azure AD authentication (avoid SQL auth)\n2. Use private endpoints\n3. Enable TDE (Transparent Data Encryption) - default on\n4. Configure auditing to Log Analytics\n5. Use Always Encrypted for sensitive columns\n6. Enable Advanced Threat Protection\n\n## Performance Optimization\n\n1. **Automatic tuning** - Let Azure optimize indexes\n2. **Query performance insights** - Identify slow queries\n3. **Elastic pools** - Share resources across databases\n4. **Read replicas** - Offload read workloads\n\n## Common Operations\n\n```bash\n# List servers\naz sql server list --output table\n\n# List databases\naz sql db list --server SERVER -g RG --output table\n\n# Check firewall rules\naz sql server firewall-rule list --server SERVER -g RG --output table\n\n# Create firewall rule\naz sql server firewall-rule create \\\n  --server SERVER -g RG \\\n  --name AllowMyIP \\\n  --start-ip-address IP --end-ip-address IP\n```\n\n## Migration from SQL Server\n\n1. **Assessment** - Use Data Migration Assistant\n2. **Schema migration** - Generate scripts or use tools\n3. **Data migration** - DMS, bacpac, or replication\n4. **Cutover** - Switch connection strings\n5. **Validation** - Verify data integrity\n",
        "plugin/skills/azure-storage/SKILL.md": "---\nname: azure-storage\ndescription: Azure Storage Services including Blob Storage, File Shares, Queue Storage, Table Storage, and Data Lake. Provides object storage, SMB file shares, async messaging, NoSQL key-value, and big data analytics capabilities.\n---\n\n# Azure Storage Services\n\n## Services\n\n| Service | Use When | MCP Tools | CLI |\n|---------|----------|-----------|-----|\n| Blob Storage | Objects, files, backups, static content | `azure__storage` | `az storage blob` |\n| File Shares | SMB file shares, lift-and-shift | - | `az storage file` |\n| Queue Storage | Async messaging, task queues | - | `az storage queue` |\n| Table Storage | NoSQL key-value (consider Cosmos DB) | - | `az storage table` |\n| Data Lake | Big data analytics, hierarchical namespace | - | `az storage fs` |\n\n## MCP Server (Preferred)\n\nWhen Azure MCP is enabled:\n\n- `azure__storage` with command `storage_account_list` - List storage accounts\n- `azure__storage` with command `storage_container_list` - List containers in account\n- `azure__storage` with command `storage_blob_list` - List blobs in container\n- `azure__storage` with command `storage_blob_get` - Download blob content\n- `azure__storage` with command `storage_blob_put` - Upload blob content\n\n**If Azure MCP is not enabled:** Run `/azure:setup` or enable via `/mcp`.\n\n## CLI Fallback\n\n```bash\n# List storage accounts\naz storage account list --output table\n\n# List containers\naz storage container list --account-name ACCOUNT --output table\n\n# List blobs\naz storage blob list --account-name ACCOUNT --container-name CONTAINER --output table\n\n# Download blob\naz storage blob download --account-name ACCOUNT --container-name CONTAINER --name BLOB --file LOCAL_PATH\n\n# Upload blob\naz storage blob upload --account-name ACCOUNT --container-name CONTAINER --name BLOB --file LOCAL_PATH\n```\n\n## Storage Account Tiers\n\n| Tier | Use Case | Performance |\n|------|----------|-------------|\n| Standard | General purpose, backup | Milliseconds |\n| Premium | Databases, high IOPS | Sub-millisecond |\n\n## Blob Access Tiers\n\n| Tier | Access Frequency | Cost |\n|------|-----------------|------|\n| Hot | Frequent | Higher storage, lower access |\n| Cool | Infrequent (30+ days) | Lower storage, higher access |\n| Cold | Rare (90+ days) | Lower still |\n| Archive | Rarely (180+ days) | Lowest storage, rehydration required |\n\n## Redundancy Options\n\n| Type | Durability | Use Case |\n|------|------------|----------|\n| LRS | 11 nines | Dev/test, recreatable data |\n| ZRS | 12 nines | Regional high availability |\n| GRS | 16 nines | Disaster recovery |\n| GZRS | 16 nines | Best durability |\n\n## Service Details\n\nFor deep documentation on specific services:\n\n- Blob storage patterns and lifecycle -> [Blob Storage documentation](https://learn.microsoft.com/azure/storage/blobs/storage-blobs-overview)\n- File shares and Azure File Sync -> [Azure Files documentation](https://learn.microsoft.com/azure/storage/files/storage-files-introduction)\n- Queue patterns and poison handling -> [Queue Storage documentation](https://learn.microsoft.com/azure/storage/queues/storage-queues-introduction)\n",
        "plugin/skills/azure-validation/SKILL.md": "---\nname: azure-validation\ndescription: Pre-deployment validation for Azure resources including naming constraints, Bicep validation, subscription filtering, and quota checks. Ensures deployments succeed by catching naming errors, character limits, and configuration issues before they fail mid-deployment.\n---\n\n# Pre-Deployment Validation\n\n> **VALIDATE BEFORE DEPLOYING** - Catch naming errors, quota limits, and Bicep issues BEFORE they fail mid-deployment.\n\n## Azure Resource Naming Constraints\n\n**CRITICAL: Many Azure resources have strict naming rules. Validate names BEFORE generating any code or running any commands.**\n\n### Common Naming Limits\n\n| Resource | Min | Max | Allowed Characters | Global Unique |\n|----------|-----|-----|-------------------|---------------|\n| **Storage Account** | 3 | **24** | lowercase letters, numbers only | Yes |\n| **Container Registry** | 5 | 50 | lowercase letters, numbers only | Yes |\n| **Key Vault** | 3 | **24** | alphanumerics, hyphens | Yes |\n| **Container App** | 2 | 32 | lowercase letters, numbers, hyphens | No |\n| **App Service** | 2 | 60 | alphanumerics, hyphens | Yes (for *.azurewebsites.net) |\n| **Function App** | 2 | 60 | alphanumerics, hyphens | Yes |\n| **Resource Group** | 1 | 90 | alphanumerics, hyphens, underscores, periods | No |\n| **Cosmos DB Account** | 3 | 44 | lowercase letters, numbers, hyphens | Yes |\n\n### The 24-Character Problem\n\n**Storage Accounts and Key Vaults are limited to 24 characters.** This is the most common naming failure.\n\n**Bad examples:**\n- `mycompanyproductionstore` (25 chars) - FAILS\n- `dev-my-application-storage` (26 chars) - FAILS\n- `my-key-vault-production` (23 chars but has hyphens) - FAILS for storage\n\n**Good examples:**\n- `mycompprodstore` (15 chars) - OK\n- `devmyappstor` (12 chars) - OK\n- `prodkeyvault01` (14 chars) - OK\n\n### Naming Validation Checklist\n\nBefore generating resource names:\n\n1. **Count characters** - Storage/KeyVault must be <=24 chars\n2. **Check allowed characters**:\n   - Storage: lowercase + numbers ONLY (no hyphens!)\n   - KeyVault: lowercase + numbers + hyphens\n   - ACR: alphanumerics only (no hyphens!)\n3. **Check global uniqueness** - Storage, ACR, KeyVault names must be globally unique\n4. **Use abbreviations** for long names:\n   - `prod` not `production`\n   - `stor` not `storage`\n   - `kv` not `keyvault`\n   - `acr` not `containerregistry`\n\n### Use MCP Tools for Validation\n\n**Before creating Bicep/Terraform, get the schema to understand constraints:**\n\n```\nTool: azure__bicepschema\nParameters:\n  resource-type: \"Microsoft.Storage/storageAccounts\"\n```\n\nThis returns the full schema including naming constraints.\n\n## Bicep Validation\n\n**ALWAYS use the Azure MCP deployment tools to validate Bicep before deploying.**\n\n### Get IaC Rules Before Writing Bicep\n\n```\nTool: azure__deploy\nCommand: deploy_iac_rules_get\nParameters:\n  deployment-tool: \"AZD\"\n  iac-type: \"bicep\"\n  resource-types: \"containerapp,storage\"\n```\n\nThis returns best practices and rules for writing correct Bicep.\n\n### Get Schema for Specific Resources\n\n```\nTool: azure__bicepschema\nParameters:\n  resource-type: \"Microsoft.App/containerApps\"\n```\n\nThis returns the complete Bicep schema so you know all required/optional properties.\n\n### Generate a Deployment Plan\n\nBefore writing any infrastructure code, generate a plan:\n\n```\nTool: azure__deploy\nCommand: deploy_plan_get\nParameters:\n  workspace-folder: \"/path/to/project\"\n  project-name: \"myapp\"\n  target-app-service: \"ContainerApp\"\n  provisioning-tool: \"AZD\"\n  azd-iac-options: \"bicep\"\n```\n\nThis generates a complete deployment plan with recommended services.\n\n## Subscription & Resource Filtering\n\n**CRITICAL: Never dump entire subscription or resource lists into context - this can overflow (140K+ characters).**\n\n### Filtering Subscriptions\n\n```bash\n# GOOD - Clean, limited output\naz account list --query \"[].{Name:name, ID:id}\" -o table\n\n# BAD - Can produce massive output\naz account list   # Full JSON with all metadata\n```\n\n### Filtering Resources\n\n```bash\n# GOOD - Filter and limit\naz resource list --resource-group RG --query \"[].{Name:name, Type:type}\" -o table\n\n# GOOD - Filter by type\naz containerapp list -g RG --query \"[].{Name:name, FQDN:properties.configuration.ingress.fqdn}\" -o table\n\n# BAD - Everything\naz resource list   # Can be 100K+ chars\n```\n\n### MCP Tool Best Practices\n\nWhen using MCP tools that list resources:\n\n1. **Always specify resource group** when possible\n2. **Use query parameters** to filter results\n3. **Paginate** if the tool supports it\n4. **Summarize** results instead of showing raw output\n\n### Common Filters\n\n```bash\n# Only show names and essential info\n--query \"[].{Name:name, Location:location, Status:properties.provisioningState}\"\n\n# Limit results\n--query \"[:10]\"  # First 10 only\n\n# Filter by condition\n--query \"[?properties.provisioningState=='Succeeded']\"\n```\n\n## Pre-flight Command\n\nRun `/azure:preflight` before any deployment to check:\n- Tools installed (az, azd, docker)\n- Authentication valid\n- Quota availability\n- Docker running\n\n## Quick Validation Flow\n\n1. **Name check** - Validate all resource names against limits above\n2. **Get IaC rules** - `azure__deploy` with command `deploy_iac_rules_get` for best practices\n3. **Get schemas** - `azure__bicepschema` for specific resources\n4. **Generate plan** - `azure__deploy` with command `deploy_plan_get` for full deployment plan\n5. **Run preflight** - `/azure:preflight` for tool/auth checks\n6. **Deploy** - `azd up`\n",
        "plugin/skills/entra-app-registration/SKILL.md": "---\nname: entra-app-registration\ndescription: Expert in Microsoft Entra app registration. Use this skill to help with understanding OAuth protocol, Entra concepts, creating the first Entra app registration and integrating OAuth flow in an example console application.\n---\n\n## Overview\n\nMicrosoft Entra ID (formerly Azure Active Directory) is Microsoft's cloud-based identity and access management service. App registrations allow applications to authenticate users and access Azure resources securely.\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **App Registration** | Configuration that allows an app to use Microsoft identity platform |\n| **Application (Client) ID** | Unique identifier for your application |\n| **Tenant ID** | Unique identifier for your Azure AD tenant/directory |\n| **Client Secret** | Password for the application (confidential clients only) |\n| **Redirect URI** | URL where authentication responses are sent |\n| **API Permissions** | Access scopes your app requests |\n| **Service Principal** | Identity created in your tenant when you register an app |\n\n### Application Types\n\n| Type | Use Case |\n|------|----------|\n| **Web Application** | Server-side apps, APIs |\n| **Single Page App (SPA)** | JavaScript/React/Angular apps |\n| **Mobile/Native App** | Desktop, mobile apps |\n| **Daemon/Service** | Background services, APIs |\n\n## Core Workflow\n\n### Step 1: Register the Application\n\nCreate an app registration in the Azure portal or using Azure CLI.\n\n**Portal Method:**\n1. Navigate to Azure Portal â†’ Microsoft Entra ID â†’ App registrations\n2. Click \"New registration\"\n3. Provide name, supported account types, and redirect URI\n4. Click \"Register\"\n\n**CLI Method:** See [references/CLI-COMMANDS.md](references/CLI-COMMANDS.md)\n\n### Step 2: Configure Authentication\n\nSet up authentication settings based on your application type.\n\n- **Web Apps**: Add redirect URIs, enable ID tokens if needed\n- **SPAs**: Add redirect URIs, enable implicit grant flow if necessary\n- **Mobile/Desktop**: Use `http://localhost` or custom URI scheme\n- **Services**: No redirect URI needed for client credentials flow\n\n### Step 3: Configure API Permissions\n\nGrant your application permission to access Microsoft APIs or your own APIs.\n\n**Common Microsoft Graph Permissions:**\n- `User.Read` - Read user profile\n- `User.ReadWrite.All` - Read and write all users\n- `Directory.Read.All` - Read directory data\n- `Mail.Send` - Send mail as a user\n\n**Details:** See [references/API-PERMISSIONS.md](references/API-PERMISSIONS.md)\n\n### Step 4: Create Client Credentials (if needed)\n\nFor confidential client applications (web apps, services), create a client secret, certificate or federated identity credential.\n\n**Client Secret:**\n- Navigate to \"Certificates & secrets\"\n- Create new client secret\n- Copy the value immediately (only shown once)\n- Store securely (Key Vault recommended)\n\n**Certificate:** For production environments, use certificates instead of secrets for enhanced security. Upload certificate via \"Certificates & secrets\" section.\n\n**Federated Identity Credential:** For dynamically authenticating the confidential client to Entra platform.\n\n### Step 5: Implement OAuth Flow\n\nIntegrate the OAuth flow into your application code.\n\n**See:**\n- [references/OAUTH-FLOWS.md](references/OAUTH-FLOWS.md) - OAuth 2.0 flow details\n- [references/CONSOLE-APP-EXAMPLE.md](references/CONSOLE-APP-EXAMPLE.md) - Console app implementation\n\n## Common Patterns\n\n### Pattern 1: First-Time App Registration\n\nWalk user through their first app registration step-by-step.\n\n**Required Information:**\n- Application name\n- Application type (web, SPA, mobile, service)\n- Redirect URIs (if applicable)\n- Required permissions\n\n**Script:** See [references/FIRST-APP-REGISTRATION.md](references/FIRST-APP-REGISTRATION.md)\n\n### Pattern 2: Console Application with User Authentication\n\nCreate a .NET/Python/Node.js console app that authenticates users.\n\n**Required Information:**\n- Programming language (C#, Python, JavaScript, etc.)\n- Authentication library (MSAL recommended)\n- Required permissions\n\n**Example:** See [references/CONSOLE-APP-EXAMPLE.md](references/CONSOLE-APP-EXAMPLE.md)\n\n### Pattern 3: Service-to-Service Authentication\n\nSet up daemon/service authentication without user interaction.\n\n**Required Information:**\n- Service/app name\n- Target API/resource\n- Whether to use secret or certificate\n\n**Implementation:** Use Client Credentials flow (see [references/OAUTH-FLOWS.md#client-credentials-flow](references/OAUTH-FLOWS.md#client-credentials-flow))\n\n## MCP Tools and CLI\n\n### Azure CLI Commands\n\n| Command | Purpose |\n|---------|---------|\n| `az ad app create` | Create new app registration |\n| `az ad app list` | List app registrations |\n| `az ad app show` | Show app details |\n| `az ad app permission add` | Add API permission |\n| `az ad app credential reset` | Generate new client secret |\n| `az ad sp create` | Create service principal |\n\n**Complete reference:** See [references/CLI-COMMANDS.md](references/CLI-COMMANDS.md)\n\n### Microsoft Authentication Library (MSAL)\n\nMSAL is the recommended library for integrating Microsoft identity platform.\n\n**Supported Languages:**\n- .NET/C# - `Microsoft.Identity.Client`\n- JavaScript/TypeScript - `@azure/msal-browser`, `@azure/msal-node`\n- Python - `msal`\n\n**Examples:** See [references/CONSOLE-APP-EXAMPLE.md](references/CONSOLE-APP-EXAMPLE.md)\n\n## Security Best Practices\n\n| Practice | Recommendation |\n|----------|---------------|\n| **Never hardcode secrets** | Use environment variables, Azure Key Vault, or managed identity |\n| **Rotate secrets regularly** | Set expiration, automate rotation |\n| **Use certificates over secrets** | More secure for production |\n| **Least privilege permissions** | Request only required API permissions |\n| **Enable MFA** | Require multi-factor authentication for users |\n| **Use managed identity** | For Azure-hosted apps, avoid secrets entirely |\n| **Validate tokens** | Always validate issuer, audience, expiration |\n| **Use HTTPS only** | All redirect URIs must use HTTPS (except localhost) |\n| **Monitor sign-ins** | Use Entra ID sign-in logs for anomaly detection |\n\n## References\n\n- [OAuth Flows](references/OAUTH-FLOWS.md) - Detailed OAuth 2.0 flow explanations\n- [CLI Commands](references/CLI-COMMANDS.md) - Azure CLI reference for app registrations\n- [Console App Example](references/CONSOLE-APP-EXAMPLE.md) - Complete working examples\n- [First App Registration](references/FIRST-APP-REGISTRATION.md) - Step-by-step guide for beginners\n- [API Permissions](references/API-PERMISSIONS.md) - Understanding and configuring permissions\n- [Troubleshooting](references/TROUBLESHOOTING.md) - Common issues and solutions\n\n## External Resources\n\n- [Microsoft Identity Platform Documentation](https://learn.microsoft.com/entra/identity-platform/)\n- [OAuth 2.0 and OpenID Connect protocols](https://learn.microsoft.com/entra/identity-platform/v2-protocols)\n- [MSAL Documentation](https://learn.microsoft.com/entra/msal/)\n- [Microsoft Graph API](https://learn.microsoft.com/graph/)\n",
        "plugin/skills/entra-app-registration/references/API-PERMISSIONS.md": "# API Permissions Guide\n\nThis document explains how to configure and manage API permissions for your Microsoft Entra app registration.\n\n## Permission Types\n\n### Delegated Permissions (User Context)\n\n**What:** Application acts on behalf of a signed-in user\n\n**When to use:**\n- User is present and can consent\n- App needs to access resources as the user\n- Interactive authentication flows\n\n**Examples:**\n- Read user's email\n- Update user's calendar\n- Access user's OneDrive files\n\n**Scope format:** User must consent (or admin pre-consents)\n\n### Application Permissions (App Context)\n\n**What:** Application acts with its own identity (no user)\n\n**When to use:**\n- Background services, daemons\n- Scheduled jobs\n- API-to-API calls without user\n\n**Examples:**\n- Read all users in organization\n- Send mail as any user\n- Access all SharePoint sites\n\n**Requirement:** Always requires admin consent\n\n## Permission Scopes\n\n### Understanding Scopes\n\n**Scope:** A string that defines what access is granted\n\n**Format:**\n```\n{resource}/{permission_name}\n\nExamples:\nhttps://graph.microsoft.com/User.Read\nhttps://graph.microsoft.com/Mail.Send\napi://myapi-id/access_as_user\n```\n\n### .default Scope\n\nSpecial scope that includes all configured permissions:\n\n```\nhttps://graph.microsoft.com/.default\napi://your-api-id/.default\n```\n\n**When to use:**\n- Client credentials flow (always)\n- Want all pre-configured permissions\n- Migrating from v1.0 endpoint\n\n## Microsoft Graph Permissions\n\n### Common Delegated Permissions\n\n| Permission | What it allows | Admin Consent Required |\n|------------|---------------|----------------------|\n| `User.Read` | Read signed-in user's profile | No |\n| `User.ReadWrite` | Read and update user profile | No |\n| `User.ReadBasic.All` | Read basic info of all users | No |\n| `User.Read.All` | Read all users' full profiles | Yes |\n| `Mail.Read` | Read user's mail | No |\n| `Mail.ReadWrite` | Read and write user's mail | No |\n| `Mail.Send` | Send mail as user | No |\n| `Calendars.Read` | Read user's calendars | No |\n| `Calendars.ReadWrite` | Read and write calendars | No |\n| `Files.Read.All` | Read all files user can access | No |\n| `Sites.Read.All` | Read items in all site collections | Yes |\n| `Directory.Read.All` | Read directory data | Yes |\n| `Directory.ReadWrite.All` | Read and write directory data | Yes |\n\n### Common Application Permissions\n\n| Permission | What it allows | Admin Consent Required |\n|------------|---------------|----------------------|\n| `User.Read.All` | Read all users' full profiles | Yes (Always) |\n| `User.ReadWrite.All` | Read and write all users' profiles | Yes (Always) |\n| `Mail.Read` | Read mail in all mailboxes | Yes (Always) |\n| `Mail.Send` | Send mail as any user | Yes (Always) |\n| `Calendars.Read` | Read calendars in all mailboxes | Yes (Always) |\n| `Directory.Read.All` | Read directory data | Yes (Always) |\n| `Directory.ReadWrite.All` | Read and write directory data | Yes (Always) |\n| `Group.ReadWrite.All` | Read and write all groups | Yes (Always) |\n\n## Adding Permissions\n\n### Azure Portal Method\n\n1. Navigate to your app registration\n2. Click **\"API permissions\"** in left menu\n3. Click **\"+ Add a permission\"**\n4. Choose API source:\n   - **Microsoft APIs** (Graph, Office 365, etc.)\n   - **APIs my organization uses** (custom APIs)\n   - **My APIs** (your own APIs)\n\n5. Select permission type:\n   - **Delegated permissions** (user context)\n   - **Application permissions** (app context)\n\n6. Search and select permissions\n7. Click **\"Add permissions\"**\n\nSee [CLI-COMMANDS.md](CLI-COMMANDS.md) for az cli commands to add API permissions programmatically.\n\n## Finding Permission IDs\n\n### Method 1: Azure Portal\n\n1. Go to Microsoft Entra ID â†’ Enterprise applications\n2. Search for \"Microsoft Graph\"\n3. Click on it â†’ Permissions\n4. Browse available permissions and copy IDs\n\n### Method 2: Microsoft Graph Explorer\n\n1. Visit https://developer.microsoft.com/graph/graph-explorer\n2. Click \"Modify permissions\"\n3. Browse and view permission details\n\n### Method 3: Microsoft Documentation\n\nVisit: https://learn.microsoft.com/en-us/graph/permissions-reference\n\n### Method 4: Azure CLI Query\n\n```bash\n# List all Graph permissions (warning: long output)\naz ad sp list --filter \"appId eq '00000003-0000-0000-c000-000000000000'\" \\\n  --query \"[0].{delegated:oauth2PermissionScopes,application:appRoles}\" -o json\n```\n\n## Granting Admin Consent\n\n### When Admin Consent is Required\n\n**Always required for:**\n- All application permissions\n- High-privilege delegated permissions\n- When organization disables user consent\n\n**Examples requiring admin consent:**\n- `User.Read.All` (read all users)\n- `Directory.Read.All` (read directory)\n- `Mail.Read` (application permission)\n- `Sites.Read.All` (read all SharePoint sites)\n\n### How to Grant Admin Consent\n\n**Portal Method:**\n1. Go to API permissions\n2. Click **\"Grant admin consent for [Your Org]\"**\n3. Confirm the action\n4. Check for green checkmarks next to permissions\n\n**CLI Method:**\n```bash\naz ad app permission admin-consent --id $APP_ID\n```\n\n### Verifying Consent Status\n\n**Portal:** Look for green checkmarks in \"Status\" column\n\n**CLI:**\n```bash\naz ad app permission list --id $APP_ID\n```\n\nLook for `consentType: \"AllPrincipals\"` (admin consented)\n\n## Custom API Permissions\n\n### Exposing Your API\n\nIf you're building an API that other apps will call:\n\n1. In your API's app registration, go to **\"Expose an API\"**\n2. Set **Application ID URI**: `api://your-api-id`\n3. Click **\"+ Add a scope\"**\n4. Configure scope:\n   - **Scope name:** `access_as_user`\n   - **Who can consent:** Admins and users\n   - **Display name:** \"Access MyAPI as user\"\n   - **Description:** Clear description of what this allows\n5. Click **\"Add scope\"**\n\n## Effective Permissions\n\n### User + App Permissions\n\n**Delegated permissions:** Intersection of user's permissions and app's permissions\n\nExample:\n- User can: Read all users\n- App granted: User.Read.All\n- **Effective:** Read all users âœ…\n\n- User can: Only read their own profile\n- App granted: User.Read.All\n- **Effective:** Only read own profile (limited by user's rights)\n\n**Application permissions:** Only app's permissions matter (no user context)\n\n## Troubleshooting Permissions\n\n### \"Insufficient privileges\" Error\n\n**Causes:**\n- Permission not added to app registration\n- Admin consent not granted\n- User lacks permission in directory\n- Accessing resource outside permission scope\n\n**Solutions:**\n1. Check API permissions in portal\n2. Grant admin consent if needed\n3. Verify user has access to resource\n4. Use correct permission scope\n\n### \"Consent required\" Error\n\n**Causes:**\n- User hasn't consented to permissions\n- Admin consent required but not granted\n- Token obtained before permission added\n\n**Solutions:**\n1. Request user consent (interactive flow)\n2. Admin grants consent (portal or CLI)\n3. Acquire new token after adding permissions\n\n### Permission Appears Granted but Doesn't Work\n\n**Possible issues:**\n- Using old cached token (get new one)\n- Permission is delegated but user lacks rights\n- API requires additional configuration\n- Permission deprecated (use new one)\n\n**Debug steps:**\n1. Decode access token: https://jwt.ms\n2. Check `scp` claim (delegated) or `roles` claim (application)\n3. Verify permission is present in token\n4. Check if permission is correct type (delegated vs application)\n\n## Permission Best Practices\n\n### Development\n\nâœ… **Do:**\n- Start with minimal permissions\n- Add incrementally as features require\n- Test with non-admin accounts\n- Document why each permission is needed\n\nâŒ **Don't:**\n- Request all permissions \"just in case\"\n- Use admin account for testing only\n- Forget to grant admin consent for app permissions\n\n### Production\n\nâœ… **Do:**\n- Review permissions quarterly\n- Remove unused permissions\n- Use least privilege principle\n- Monitor permission usage\n- Document all permissions in README\n\nâŒ **Don't:**\n- Grant excessive permissions for convenience\n- Use application permissions when delegated would work\n- Forget to rotate admin consent approvals\n\n### Security\n\nâœ… **Do:**\n- Prefer delegated over application permissions\n- Implement proper scope validation\n- Log permission usage\n- Handle consent errors gracefully\n\nâŒ **Don't:**\n- Hardcode permission scopes in multiple places\n- Skip token validation\n- Ignore scope mismatches\n- Cache permissions indefinitely\n\n## Reference Tables\n\n### Microsoft Graph Permission IDs\n\n**Delegated Permissions:**\n```\nUser.Read                     : e1fe6dd8-ba31-4d61-89e7-88639da4683d\nUser.ReadWrite                : b4e74841-8e56-480b-be8b-910348b18b4c\nUser.ReadBasic.All            : b340eb25-3456-403f-be2f-af7a0d370277\nMail.Read                     : 570282fd-fa5c-430d-a7fd-fc8dc98a9dca\nMail.ReadWrite                : 024d486e-b451-40bb-833d-3e66d98c5c73\nMail.Send                     : e383f46e-2787-4529-855e-0e479a3ffac0\nCalendars.Read                : 465a38f9-76ea-45b9-9f34-9e8b0d4b0b42\nCalendars.ReadWrite           : 1ec239c2-d7c9-4623-a91a-a9775856bb36\nFiles.Read.All                : df85f4d6-205c-4ac5-a5ea-6bf408dba283\n```\n\n**Application Permissions:**\n```\nUser.Read.All                 : df021288-bdef-4463-88db-98f22de89214\nUser.ReadWrite.All            : 741f803b-c850-494e-b5df-cde7c675a1ca\nMail.Read                     : 810c84a8-4a9e-49e6-bf7d-12d183f40d01\nMail.Send                     : b633e1c5-b582-4048-a93e-9f11b44c7e96\nDirectory.Read.All            : 7ab1d382-f21e-4acd-a863-ba3e13f7da61\nDirectory.ReadWrite.All       : 19dbc75e-c2e2-444c-a770-ec69d8559fc7\n```\n\n**Note:** Permission IDs may change. Always verify against the official [Microsoft Graph Permissions Reference](https://learn.microsoft.com/en-us/graph/permissions-reference) for the most current values.\n\n## Additional Resources\n\n- [Microsoft Graph Permissions Reference](https://learn.microsoft.com/en-us/graph/permissions-reference)\n- [Permission Types](https://learn.microsoft.com/en-us/entra/identity-platform/permissions-consent-overview)\n- [Admin Consent Workflow](https://learn.microsoft.com/en-us/entra/identity/enterprise-apps/configure-admin-consent-workflow)\n- [Consent Framework](https://learn.microsoft.com/en-us/entra/identity-platform/consent-framework)\n",
        "plugin/skills/entra-app-registration/references/CLI-COMMANDS.md": "# Azure CLI Commands for App Registration\n\nThis document provides a comprehensive reference for managing Microsoft Entra app registrations using Azure CLI.\n\n## Prerequisites\n\n```bash\n# Ensure Azure CLI is installed\naz version\n\n# Login to Azure\naz login\n\n# Set default subscription (optional)\naz account set --subscription \"Your Subscription Name\"\n```\n\n## App Registration Management\n\n### Create App Registration\n\n**Basic app registration:**\n```bash\naz ad app create --display-name \"MyApplication\"\n```\n\n**Web application with redirect URI:**\n```bash\naz ad app create \\\n  --display-name \"MyWebApp\" \\\n  --web-redirect-uris \"https://myapp.com/callback\" \\\n  --sign-in-audience \"AzureADMyOrg\"\n```\n\n**Single Page Application (SPA):**\n```bash\naz ad app create \\\n  --display-name \"MySpaApp\" \\\n  --spa-redirect-uris \"http://localhost:3000\" \\\n  --sign-in-audience \"AzureADMyOrg\"\n```\n\n**Public client (Desktop/Mobile app):**\n```bash\naz ad app create \\\n  --display-name \"MyDesktopApp\" \\\n  --public-client-redirect-uris \"http://localhost\" \\\n  --sign-in-audience \"AzureADMyOrg\"\n```\n\n**Multi-tenant application:**\n```bash\naz ad app create \\\n  --display-name \"MyMultiTenantApp\" \\\n  --web-redirect-uris \"https://myapp.com/callback\" \\\n  --sign-in-audience \"AzureADMultipleOrgs\"\n```\n\n### Sign-in Audience Options\n\n| Value | Description |\n|-------|-------------|\n| `AzureADMyOrg` | Single tenant (default) |\n| `AzureADMultipleOrgs` | Multi-tenant (any Azure AD) |\n| `AzureADandPersonalMicrosoftAccount` | Multi-tenant + personal Microsoft accounts |\n| `PersonalMicrosoftAccount` | Personal Microsoft accounts only |\n\n## List and Query Apps\n\n### List all app registrations\n\n```bash\naz ad app list --output table\n```\n\n### List apps with custom query\n\n```bash\n# Filter by display name\naz ad app list --display-name \"MyApp\" --output table\n\n# Get specific fields\naz ad app list --query \"[].{Name:displayName, AppId:appId}\" --output table\n```\n\n### Get app details\n\n```bash\n# By display name\naz ad app show --id $(az ad app list --display-name \"MyApp\" --query \"[0].appId\" -o tsv)\n\n# By application ID\naz ad app show --id \"YOUR_APPLICATION_ID\"\n```\n\n### Get Application (Client) ID\n\n```bash\nAPP_ID=$(az ad app list --display-name \"MyApp\" --query \"[0].appId\" -o tsv)\necho \"Application ID: $APP_ID\"\n```\n\n### Get Object ID\n\n```bash\nOBJECT_ID=$(az ad app list --display-name \"MyApp\" --query \"[0].id\" -o tsv)\necho \"Object ID: $OBJECT_ID\"\n```\n\n## Update App Registration\n\n### Add redirect URIs\n\n**Web app:**\n```bash\naz ad app update --id $APP_ID \\\n  --web-redirect-uris \"https://myapp.com/callback\" \"https://myapp.com/auth\"\n```\n\n**SPA:**\n```bash\naz ad app update --id $APP_ID \\\n  --spa-redirect-uris \"http://localhost:3000\" \"http://localhost:5000\"\n```\n\n**Public client:**\n```bash\naz ad app update --id $APP_ID \\\n  --public-client-redirect-uris \"http://localhost\" \"myapp://auth\"\n```\n\n## Client Credentials (Secrets & Certificates)\n\n### Create client secret\n\n```bash\n# Create secret with default expiration\naz ad app credential reset --id $APP_ID\n\n# Create secret with custom expiration\naz ad app credential reset --id $APP_ID --years 1\n\n# Create secret with specific end date\naz ad app credential reset --id $APP_ID --end-date \"2025-12-31\"\n```\n\n**Save the output:**\n```json\n{\n  \"appId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"password\": \"your-secret-value-SAVE-THIS\",\n  \"tenant\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n}\n```\n\n**âš ï¸ Important:** Resetting Client credential will delete all existing credentials.\n**âš ï¸ Important:** The secret value is only shown once. Store it securely (e.g., Azure Key Vault).\n\n### List client credentials\n\n```bash\n# List all credentials (secrets and certificates)\naz ad app credential list --id $APP_ID\n```\n\n### Delete client secret\n\n```bash\n# Get key ID from credential list\naz ad app credential list --id $APP_ID --query \"[].{KeyId:keyId, Type:type}\" -o table\n\n# Delete specific credential\naz ad app credential delete --id $APP_ID --key-id \"KEY_ID_HERE\"\n```\n\n### Upload certificate\n\n```bash\n# Upload certificate from file\naz ad app credential reset --id $APP_ID --cert \"@path/to/cert.pem\"\n```\n\n## API Permissions\n\n### Add API permissions\n\n**Microsoft Graph User.Read:**\n```bash\nGRAPH_RESOURCE_ID=\"00000003-0000-0000-c000-000000000000\"  # Microsoft Graph\nUSER_READ_ID=\"e1fe6dd8-ba31-4d61-89e7-88639da4683d\"      # User.Read permission\n\naz ad app permission add --id $APP_ID \\\n  --api $GRAPH_RESOURCE_ID \\\n  --api-permissions \"$USER_READ_ID=Scope\"\n```\n\n**Microsoft Graph Mail.Read (delegated):**\n```bash\nMAIL_READ_ID=\"570282fd-fa5c-430d-a7fd-fc8dc98a9dca\"      # Mail.Read permission\n\naz ad app permission add --id $APP_ID \\\n  --api $GRAPH_RESOURCE_ID \\\n  --api-permissions \"$MAIL_READ_ID=Scope\"\n```\n\n**Microsoft Graph User.Read.All (application):**\n```bash\nUSER_READ_ALL_ID=\"df021288-bdef-4463-88db-98f22de89214\"  # User.Read.All application permission\n\naz ad app permission add --id $APP_ID \\\n  --api $GRAPH_RESOURCE_ID \\\n  --api-permissions \"$USER_READ_ALL_ID=Role\"\n```\n\n**Note:** Use `Scope` for delegated permissions, `Role` for application permissions.\n\n### Common Permission IDs\n\n**Microsoft Graph (00000003-0000-0000-c000-000000000000):**\n\n| Permission | ID | Type |\n|------------|-----|------|\n| User.Read | e1fe6dd8-ba31-4d61-89e7-88639da4683d | Delegated |\n| User.ReadWrite | b4e74841-8e56-480b-be8b-910348b18b4c | Delegated |\n| Mail.Read | 570282fd-fa5c-430d-a7fd-fc8dc98a9dca | Delegated |\n| Mail.Send | e383f46e-2787-4529-855e-0e479a3ffac0 | Delegated |\n| Calendars.Read | 465a38f9-76ea-45b9-9f34-9e8b0d4b0b42 | Delegated |\n| User.Read.All | df021288-bdef-4463-88db-98f22de89214 | Application |\n| Directory.Read.All | 7ab1d382-f21e-4acd-a863-ba3e13f7da61 | Application |\n\n### Grant admin consent\n\n```bash\n# Grant admin consent for all permissions\naz ad app permission admin-consent --id $APP_ID\n```\n\n**Note:** Admin consent is required for application permissions and some delegated permissions.\n\n### List permissions\n\n```bash\naz ad app permission list --id $APP_ID\n```\n\n### Delete permission\n\n```bash\n# Remove specific permission\naz ad app permission delete --id $APP_ID \\\n  --api $GRAPH_RESOURCE_ID \\\n  --permission-id $USER_READ_ID\n```\n\n## Service Principal Management\n\n### Create service principal\n\n```bash\n# Create service principal for the app\naz ad sp create --id $APP_ID\n```\n\n### List service principals\n\n```bash\naz ad sp list --display-name \"MyApp\"\n```\n\n### Get service principal details\n\n```bash\naz ad sp show --id $APP_ID\n```\n\n### Delete service principal\n\n```bash\naz ad sp delete --id $APP_ID\n```\n\n## App Roles and Claims\n\n### Get app roles\n\n```bash\naz ad app show --id $APP_ID --query \"appRoles\"\n```\n\n### Get optional claims\n\n```bash\naz ad app show --id $APP_ID --query \"optionalClaims\"\n```\n\n## Owners\n\n### List app owners\n\n```bash\naz ad app owner list --id $APP_ID\n```\n\n### Add owner\n\n```bash\n# Add user as owner\nUSER_OBJECT_ID=$(az ad user show --id \"user@domain.com\" --query \"id\" -o tsv)\naz ad app owner add --id $APP_ID --owner-object-id $USER_OBJECT_ID\n```\n\n### Remove owner\n\n```bash\naz ad app owner remove --id $APP_ID --owner-object-id $USER_OBJECT_ID\n```\n\n## Delete App Registration\n\n```bash\n# Delete app registration (and associated service principal)\naz ad app delete --id $APP_ID\n```\n\n## Tenant and Identity Information\n\n### Get tenant ID\n\n```bash\naz account show --query tenantId -o tsv\n```\n\n### Get current user information\n\n```bash\naz ad signed-in-user show\n```\n\n### Get user by email\n\n```bash\naz ad user show --id \"user@domain.com\"\n```\n\n### Get user object ID\n\n```bash\naz ad user show --id \"user@domain.com\" --query \"id\" -o tsv\n```\n\n### List all users\n\n```bash\naz ad user list --output table\n```\n\n## Scripting Examples\n\n### Complete app setup script\n\n```bash\n#!/bin/bash\n\n# Variables\nAPP_NAME=\"MyApplication\"\nREDIRECT_URI=\"http://localhost:3000\"\n\necho \"Creating app registration...\"\nAPP_ID=$(az ad app create \\\n  --display-name \"$APP_NAME\" \\\n  --spa-redirect-uris \"$REDIRECT_URI\" \\\n  --query \"appId\" -o tsv)\n\necho \"App created with ID: $APP_ID\"\n\necho \"Adding Microsoft Graph permissions...\"\nGRAPH_RESOURCE_ID=\"00000003-0000-0000-c000-000000000000\"\nUSER_READ_ID=\"e1fe6dd8-ba31-4d61-89e7-88639da4683d\"\n\naz ad app permission add --id $APP_ID \\\n  --api $GRAPH_RESOURCE_ID \\\n  --api-permissions \"$USER_READ_ID=Scope\"\n\necho \"Granting admin consent...\"\naz ad app permission admin-consent --id $APP_ID\n\necho \"Creating service principal...\"\naz ad sp create --id $APP_ID\n\nTENANT_ID=$(az account show --query tenantId -o tsv)\n\necho \"\"\necho \"App registration complete!\"\necho \"Application (Client) ID: $APP_ID\"\necho \"Tenant ID: $TENANT_ID\"\necho \"Redirect URI: $REDIRECT_URI\"\n```\n\n### Cleanup script\n\n```bash\n#!/bin/bash\n\n# Delete all apps matching pattern\naz ad app list --display-name \"Test*\" --query \"[].appId\" -o tsv | while read APP_ID; do\n  echo \"Deleting app: $APP_ID\"\n  az ad app delete --id $APP_ID\ndone\n```\n",
        "plugin/skills/entra-app-registration/references/CONSOLE-APP-EXAMPLE.md": "# Console Application Examples\n\nThis document provides complete working examples of console applications that authenticate with Microsoft Entra ID using MSAL (Microsoft Authentication Library).\n\n## Table of Contents\n\n- [C# (.NET) Example](#c-net-example)\n- [Python Example](#python-example)\n- [JavaScript (Node.js) Example](#javascript-nodejs-example)\n\n## C# (.NET) Example\n\n### Prerequisites\n\n```bash\ndotnet new console -n EntraAuthConsole\ncd EntraAuthConsole\ndotnet add package Microsoft.Identity.Client\n```\n\n### Complete Code\n\n```csharp\nusing Microsoft.Identity.Client;\nusing System;\nusing System.Linq;\nusing System.Threading.Tasks;\n\nnamespace EntraAuthConsole\n{\n    class Program\n    {\n        // Configuration - replace with your values\n        private const string ClientId = \"YOUR_APPLICATION_CLIENT_ID\";\n        private const string TenantId = \"YOUR_TENANT_ID\";\n        private static readonly string[] Scopes = new[] { \"User.Read\" };\n\n        static async Task Main(string[] args)\n        {\n            try\n            {\n                // Build the MSAL client\n                var app = PublicClientApplicationBuilder\n                    .Create(ClientId)\n                    .WithAuthority(AzureCloudInstance.AzurePublic, TenantId)\n                    .WithRedirectUri(\"http://localhost\")\n                    .Build();\n\n                // Try to get token silently from cache first\n                var accounts = await app.GetAccountsAsync();\n                AuthenticationResult result;\n\n                try\n                {\n                    result = await app.AcquireTokenSilent(Scopes, accounts.FirstOrDefault())\n                        .ExecuteAsync();\n                    Console.WriteLine(\"Token acquired from cache\");\n                }\n                catch (MsalUiRequiredException)\n                {\n                    // Interactive authentication required\n                    result = await app.AcquireTokenInteractive(Scopes)\n                        .WithPrompt(Prompt.SelectAccount)\n                        .ExecuteAsync();\n                    Console.WriteLine(\"Token acquired interactively\");\n                }\n\n                // Display user information\n                Console.WriteLine($\"\\nWelcome, {result.Account.Username}!\");\n                Console.WriteLine($\"Token expires: {result.ExpiresOn}\");\n\n                // Call Microsoft Graph API\n                await CallGraphApiAsync(result.AccessToken);\n            }\n            catch (MsalException ex)\n            {\n                Console.WriteLine($\"Error acquiring token: {ex.Message}\");\n            }\n        }\n\n        private static async Task CallGraphApiAsync(string accessToken)\n        {\n            using var httpClient = new System.Net.Http.HttpClient();\n            httpClient.DefaultRequestHeaders.Authorization = \n                new System.Net.Http.Headers.AuthenticationHeaderValue(\"Bearer\", accessToken);\n\n            var response = await httpClient.GetAsync(\"https://graph.microsoft.com/v1.0/me\");\n            \n            if (response.IsSuccessStatusCode)\n            {\n                var content = await response.Content.ReadAsStringAsync();\n                Console.WriteLine(\"\\nUser profile from Microsoft Graph:\");\n                Console.WriteLine(content);\n            }\n            else\n            {\n                Console.WriteLine($\"API call failed: {response.StatusCode}\");\n            }\n        }\n    }\n}\n```\n\n### Run the Application\n\n```bash\ndotnet run\n```\n\n### Device Code Flow (for headless scenarios)\n\n```csharp\n// Use this for servers or devices without a browser\nresult = await app.AcquireTokenWithDeviceCode(Scopes, deviceCodeResult =>\n{\n    Console.WriteLine(deviceCodeResult.Message);\n    return Task.CompletedTask;\n}).ExecuteAsync();\n```\n\n---\n\n## Python Example\n\n### Prerequisites\n\n```bash\npip install msal requests\n```\n\n### Complete Code\n\n```python\nimport msal\nimport requests\nimport json\n\n# Configuration - replace with your values\nCLIENT_ID = \"YOUR_APPLICATION_CLIENT_ID\"\nTENANT_ID = \"YOUR_TENANT_ID\"\nAUTHORITY = f\"https://login.microsoftonline.com/{TENANT_ID}\"\nSCOPES = [\"User.Read\"]\n\ndef acquire_token_interactive():\n    \"\"\"Acquire token using interactive flow (opens browser)\"\"\"\n    app = msal.PublicClientApplication(\n        CLIENT_ID,\n        authority=AUTHORITY\n    )\n    \n    # Try to get token from cache first\n    accounts = app.get_accounts()\n    result = None\n    \n    if accounts:\n        # Try silent acquisition\n        result = app.acquire_token_silent(SCOPES, account=accounts[0])\n        if result:\n            print(\"Token acquired from cache\")\n    \n    if not result:\n        # Interactive authentication\n        result = app.acquire_token_interactive(\n            scopes=SCOPES,\n            prompt=\"select_account\"\n        )\n        print(\"Token acquired interactively\")\n    \n    return result\n\ndef acquire_token_device_code():\n    \"\"\"Acquire token using device code flow (for headless scenarios)\"\"\"\n    app = msal.PublicClientApplication(\n        CLIENT_ID,\n        authority=AUTHORITY\n    )\n    \n    flow = app.initiate_device_flow(scopes=SCOPES)\n    \n    if \"user_code\" not in flow:\n        raise Exception(f\"Failed to create device flow: {flow.get('error_description')}\")\n    \n    # Display instructions to user\n    print(flow[\"message\"])\n    \n    # Wait for user to complete authentication\n    result = app.acquire_token_by_device_flow(flow)\n    return result\n\ndef call_graph_api(access_token):\n    \"\"\"Call Microsoft Graph API with access token\"\"\"\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Content-Type': 'application/json'\n    }\n    \n    response = requests.get(\n        'https://graph.microsoft.com/v1.0/me',\n        headers=headers\n    )\n    \n    if response.status_code == 200:\n        user_data = response.json()\n        print(\"\\nUser profile from Microsoft Graph:\")\n        print(json.dumps(user_data, indent=2))\n    else:\n        print(f\"API call failed: {response.status_code}\")\n        print(response.text)\n\ndef main():\n    # Choose authentication method\n    print(\"Select authentication method:\")\n    print(\"1. Interactive (opens browser)\")\n    print(\"2. Device code (for headless scenarios)\")\n    choice = input(\"Enter choice (1 or 2): \")\n    \n    try:\n        if choice == \"1\":\n            result = acquire_token_interactive()\n        elif choice == \"2\":\n            result = acquire_token_device_code()\n        else:\n            print(\"Invalid choice\")\n            return\n        \n        if \"access_token\" in result:\n            print(f\"\\nWelcome, {result.get('id_token_claims', {}).get('preferred_username', 'User')}!\")\n            print(f\"Token expires in: {result.get('expires_in')} seconds\")\n            \n            # Call Microsoft Graph API\n            call_graph_api(result[\"access_token\"])\n        else:\n            print(f\"Error acquiring token: {result.get('error')}\")\n            print(f\"Description: {result.get('error_description')}\")\n    \n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Run the Application\n\n```bash\npython console_app.py\n```\n\n---\n\n## JavaScript (Node.js) Example\n\n### Prerequisites\n\n```bash\nnpm init -y\nnpm install @azure/msal-node axios\n```\n\n### Complete Code\n\n```javascript\nconst msal = require('@azure/msal-node');\nconst axios = require('axios');\n\n// Configuration - replace with your values\nconst config = {\n    auth: {\n        clientId: \"YOUR_APPLICATION_CLIENT_ID\",\n        authority: \"https://login.microsoftonline.com/YOUR_TENANT_ID\",\n    }\n};\n\nconst scopes = [\"User.Read\"];\n\n// Interactive authentication (opens browser)\nasync function acquireTokenInteractive() {\n    const pca = new msal.PublicClientApplication(config);\n    \n    const authCodeUrlParameters = {\n        scopes: scopes,\n        redirectUri: \"http://localhost:3000\",\n    };\n\n    // This opens the browser for authentication\n    const response = await pca.acquireTokenInteractive(authCodeUrlParameters);\n    return response;\n}\n\n// Device code flow (for headless scenarios)\nasync function acquireTokenDeviceCode() {\n    const pca = new msal.PublicClientApplication(config);\n    \n    const deviceCodeRequest = {\n        deviceCodeCallback: (response) => {\n            console.log(\"\\n\" + response.message);\n        },\n        scopes: scopes,\n    };\n\n    const response = await pca.acquireTokenByDeviceCode(deviceCodeRequest);\n    return response;\n}\n\n// Client credentials flow (service-to-service, no user)\nasync function acquireTokenClientCredentials() {\n    const confidentialConfig = {\n        auth: {\n            clientId: \"YOUR_APPLICATION_CLIENT_ID\",\n            authority: \"https://login.microsoftonline.com/YOUR_TENANT_ID\",\n            clientSecret: \"YOUR_CLIENT_SECRET\", // From app registration\n        }\n    };\n    \n    const cca = new msal.ConfidentialClientApplication(confidentialConfig);\n    \n    const clientCredentialRequest = {\n        scopes: [\"https://graph.microsoft.com/.default\"],\n    };\n\n    const response = await cca.acquireTokenByClientCredential(clientCredentialRequest);\n    return response;\n}\n\n// Call Microsoft Graph API\nasync function callGraphApi(accessToken) {\n    const options = {\n        headers: {\n            Authorization: `Bearer ${accessToken}`\n        }\n    };\n\n    try {\n        const response = await axios.get('https://graph.microsoft.com/v1.0/me', options);\n        console.log('\\nUser profile from Microsoft Graph:');\n        console.log(JSON.stringify(response.data, null, 2));\n    } catch (error) {\n        console.error('API call failed:', error.response?.status, error.message);\n    }\n}\n\n// Main function\nasync function main() {\n    console.log(\"Select authentication method:\");\n    console.log(\"1. Device code flow (recommended for CLI)\");\n    console.log(\"2. Client credentials (service-to-service)\");\n    \n    // For demonstration, using device code flow\n    // In production, get user input with readline or similar\n    const choice = \"1\";\n    \n    try {\n        let result;\n        \n        if (choice === \"1\") {\n            result = await acquireTokenDeviceCode();\n        } else if (choice === \"2\") {\n            result = await acquireTokenClientCredentials();\n        }\n        \n        if (result.accessToken) {\n            console.log('\\nAuthentication successful!');\n            console.log(`Token expires: ${new Date(result.expiresOn)}`);\n            \n            // Call Microsoft Graph API\n            await callGraphApi(result.accessToken);\n        } else {\n            console.error('Failed to acquire token');\n        }\n    } catch (error) {\n        console.error('Error:', error.message);\n    }\n}\n\nmain();\n```\n\n### Run the Application\n\n```bash\nnode console_app.js\n```\n\n## Next Steps\n\n- Review [OAUTH-FLOWS.md](OAUTH-FLOWS.md) for flow details\n- See [API-PERMISSIONS.md](API-PERMISSIONS.md) for permission setup\n- Check [TROUBLESHOOTING.md](TROUBLESHOOTING.md) for common issues\n\n## Additional Resources\n\n- [MSAL Libraries](https://learn.microsoft.com/entra/msal/)\n",
        "plugin/skills/entra-app-registration/references/FIRST-APP-REGISTRATION.md": "# First App Registration - Step-by-Step Guide\n\nThis guide walks you through creating your first Microsoft Entra app registration from scratch.\n\n## Overview\n\nYou'll learn how to:\n1. Create an app registration in Azure Portal\n2. Configure authentication settings\n3. Add API permissions\n4. Create client credentials\n5. Test the authentication flow\n\n## Prerequisites\n\n- Azure subscription (free tier works)\n- Azure Portal access: https://portal.azure.com\n- Basic understanding of your application type (web, mobile, service)\n\n## Step 1: Navigate to App Registrations\n\n1. Open [Azure Portal](https://portal.azure.com)\n2. Search for **\"Microsoft Entra ID\"**\n3. In the left menu, click **\"App registrations\"**\n4. Click **\"+ New registration\"** at the top\n\n## Step 2: Register Your Application\n\nYou'll see a form with several fields:\n\n### Application Name\n- **What to enter:** A descriptive name for your app\n- **Example:** \"My First Console App\" or \"Product Inventory API\"\n- **Tip:** Use a name that clearly identifies the purpose\n\n### Supported Account Types\n\nChoose who can use your application:\n\n| Option | When to Use |\n|--------|-------------|\n| **Accounts in this organizational directory only (Single tenant)** | Only users from the same tenant of this app registration need access |\n| **Accounts in any organizational directory (Multi-tenant)** | Users from multiple organization tenants need access |\n| **Accounts in any organizational directory + Personal Microsoft accounts** | Users from multiple organization tenants and MSA users need access |\n| **Personal Microsoft accounts only** | Only MSA users need access |\n\n**Note:** Once selected, users whose account type is not allowed will get errors when trying to get access token for the app registration.\n\n### Redirect URI (optional)\n\nThe redirect URI is where authentication responses are sent.\n\n**Platform:** Select the type:\n- **Web** - Server-side web apps\n- **Single-page application (SPA)** - React, Angular, Vue apps\n- **Public client/native** - Mobile, desktop, console apps\n\n**URI examples:**\n- Web app: `https://localhost:5001/signin-oidc`\n- SPA: `http://localhost:3000`\n- Console/Desktop: `http://localhost`\n\n**For your first app:** Select **\"Public client/native\"** and enter `http://localhost`\n\n### Click \"Register\"\n\nAfter clicking, you'll be redirected to your app's overview page.\n\n## Step 3: Save Important Information\n\nOn the **Overview** page, you'll see critical information. **Copy and save these values:**\n\n### Application (client) ID\n- **What it is:** Unique identifier for your app\n- **Format:** `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx` (GUID)\n- **When you need it:** Every time your app authenticates\n- **Where to save:** Environment variables, configuration file\n\n### Directory (tenant) ID\n- **What it is:** Unique identifier for your Azure AD tenant\n- **Format:** `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx` (GUID)\n- **When you need it:** Constructing authentication URLs\n\n### Example values to save:\n```bash\n# Save these in a secure location\nAPPLICATION_CLIENT_ID=\"12345678-1234-1234-1234-123456789012\"\nTENANT_ID=\"87654321-4321-4321-4321-210987654321\"\n```\n\n## Step 4: Configure Authentication (Optional)\n\nClick **\"Authentication\"** in the left menu.\n\n### Advanced Settings\n\n**Allow public client flows:**\n- **What it is:** Enables device code flow, resource owner password flow\n- **For console apps:** Turn this **ON**\n- **For web apps:** Keep **OFF**\n\n### Supported account types\n\nYou can change this later if needed.\n\n### Logout URL (optional)\n\nWhere to redirect users after logout.\n\n**Click \"Save\"** at the top if you made changes.\n\n## Step 5: Add API Permissions\n\nClick **\"API permissions\"** in the left menu.\n\n### Default Permission\n\nYou'll see one default permission:\n- **Microsoft Graph â†’ User.Read (Delegated)**\n\nThis allows your app to read the signed-in user's profile.\n\n### Add More Permissions\n\n1. Click **\"+ Add a permission\"**\n2. Select **\"Microsoft Graph\"**\n3. Choose **\"Delegated permissions\"** (for user context)\n4. Search for and select permissions you need:\n   - **User.Read** - Read user profile (already added)\n   - **Mail.Read** - Read user's mail\n   - **Calendars.Read** - Read user's calendar\n\n5. Click **\"Add permissions\"**\n\n### Admin Consent\n\nSome permissions require admin consent:\n- If you're an admin: Click **\"Grant admin consent for [Your Org]\"**\n- If you're not: Ask your admin to grant consent\n\n**Status indicator:**\n- âœ… Green checkmark = Granted\n- âš ï¸ Yellow warning = Not granted (may still work for user consent)\n\n## Step 6: Create Client Secret (If Needed)\n\n**Skip this if:** You're building a desktop/mobile/console app (public client)\n\n**Do this if:** You're building a web app, API, or service (confidential client)\n\n1. Click **\"Certificates & secrets\"** in the left menu\n2. Click **\"+ New client secret\"**\n3. Enter a description: \"Development Secret\"\n4. Choose expiration:\n   - **Recommended for development:** 6 months\n   - **For production:** 12-24 months (set up rotation)\n5. Click **\"Add\"**\n\n**âš ï¸ CRITICAL:** Copy the secret **Value** immediately!\n- It's only shown once\n- You cannot retrieve it later\n- If you lose it, create a new one\n\n```bash\n# Save this securely (example)\nCLIENT_SECRET=\"abc123~defGHI456jklMNO789pqrSTU\"\n```\n\n**Security tips:**\n- Never commit secrets to source control\n- Use Azure Key Vault for production\n- Use environment variables for development\n\n## Step 7: Test Your App Registration\n\n### Option A: Quick Test with Azure CLI\n\n```bash\n# Set your values\nCLIENT_ID=\"your-client-id-here\"\nTENANT_ID=\"your-tenant-id-here\"\n\n# Interactive login\naz login --scope \"https://graph.microsoft.com/.default\"\n\n# Get an access token\naz account get-access-token --resource \"https://graph.microsoft.com\"\n```\n\n### Option B: Test with MSAL Library\n\nSee the complete code example in [CONSOLE-APP-EXAMPLE.md](CONSOLE-APP-EXAMPLE.md)\n\n### Expected Results\n\n**Success:**\n- Browser opens for authentication (or device code shown)\n- You authenticate with your Azure AD account\n- Access token is returned\n- You can call Microsoft Graph API\n\n**Common first-time issues:**\n- Redirect URI mismatch â†’ Double-check URI in Authentication settings\n- Insufficient permissions â†’ Add required API permissions\n- User consent required â†’ Grant admin consent or user must consent\n\n**Tip:** Once you get the access token, you can use [jwt.ms](https://jwt.ms) to decode it and inspect its claims.\n\n## Step 8: Review Configuration\n\n### Checklist\n\n- âœ… App registered with clear name\n- âœ… Application ID and Tenant ID saved securely\n- âœ… Redirect URI configured correctly\n- âœ… API permissions added\n- âœ… Admin consent granted (if required)\n- âœ… Client secret created and saved (if needed)\n- âœ… Authentication tested successfully\n\n## Next Steps\n\n- In your client app, implement the OAuth flow to acquire access tokens for your app registration.\n- In your server app, implement token validation to protect your resources.\n\n## Troubleshooting\n\n### Redirect URI mismatch\"\n\n**Solution:**\n- Check Authentication â†’ Redirect URIs\n- Ensure exact match (case-sensitive, trailing slash matters)\n- Ensure correct platform (Web vs SPA vs Public client)\n\n### User consent required\n\n**Solution:**\n- Grant admin consent in API permissions\n- Or have user consent during first login\n\n## Additional Resources\n\n- [Microsoft Entra ID Documentation](https://learn.microsoft.com/en-us/entra/identity-platform/)\n",
        "plugin/skills/entra-app-registration/references/OAUTH-FLOWS.md": "# OAuth 2.0 Flows\n\nThis document provides an illustration of OAuth 2.0 authentication flows supported by Microsoft Entra ID.\n\n**Note:** All the following implementation steps are for illustration purposes. It's always recommended to use a library to handle the authentication flow.\n\n## Authorization Code Flow\n\n### Flow Steps\n\n```\n1. User â†’ App: Navigate to app's web UI\n2. App â†’ User: Redirect to Microsoft login\n3. User â†’ Entra ID: Authenticate & consent\n4. Entra ID â†’ App: Authorization code (via redirect URI)\n5. App â†’ Entra ID: Exchange code for tokens (with client secret)\n6. Entra ID â†’ App: Access token + refresh token + ID token\n7. App â†’ API: Call API with access token\n```\n\n### Implementation Steps\n\n#### 1. Build Authorization URL\n\n```\nhttps://login.microsoftonline.com/{tenant}/oauth2/v2.0/authorize?\n  client_id={application_id}\n  &response_type=code\n  &redirect_uri={redirect_uri}\n  &response_mode=query\n  &scope={scopes}\n  &state={random_state}\n```\n\n**Parameters:**\n- `tenant`: Your tenant ID or `common` for multi-tenant\n- `client_id`: Application (client) ID from app registration\n- `redirect_uri`: Must match exactly what's registered\n- `scope`: Space-separated permissions (e.g., `openid profile User.Read`)\n- `state`: Random value to prevent CSRF attacks\n\n#### 2. User Authenticates\n\nUser is redirected to Microsoft login page, authenticates, and grants consent.\n\n#### 3. Receive Authorization Code\n\nApp receives callback at redirect URI:\n```\nhttps://your-app.com/callback?\n  code={authorization_code}\n  &state={state_value}\n```\n\n**Validation:**\n- Verify `state` matches what you sent\n- Extract `code` parameter\n\n#### 4. Exchange Code for Tokens\n\n```http\nPOST https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={application_id}\n&scope={scopes}\n&code={authorization_code}\n&redirect_uri={redirect_uri}\n&grant_type=authorization_code\n&client_secret={client_secret}\n```\n\n**Response:**\n```json\n{\n  \"access_token\": \"eyJ0eXAi...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"refresh_token\": \"M.R3_BAY...\",\n  \"id_token\": \"eyJ0eXAi...\"\n}\n```\n\n#### 5. Use Access Token\n\n```http\nGET https://graph.microsoft.com/v1.0/me\nAuthorization: Bearer {access_token}\n```\n\n## Authorization Code Flow with PKCE\n\nPKCE (Proof Key for Code Exchange) adds security for public clients that cannot securely store a client secret.\n\n### Flow Steps\n\n```\n1. App: Generate code verifier (random string)\n2. App: Generate code challenge (SHA256 hash of verifier)\n3. App â†’ Entra ID: Authorization request with code challenge\n4. User â†’ Entra ID: Authenticate & consent\n5. Entra ID â†’ App: Authorization code\n6. App â†’ Entra ID: Exchange code + code verifier for token\n7. Entra ID: Validates verifier matches challenge\n8. Entra ID â†’ App: Access token + ID token\n```\n\n### Implementation Steps\n\n#### 1. Generate PKCE Values\n\n**Code Verifier:** 43-128 character random string\n```javascript\n// JavaScript example\nconst codeVerifier = generateRandomString(128);\n```\n\n**Code Challenge:** Base64URL-encoded SHA256 hash of verifier\n```javascript\nconst codeChallenge = base64URLEncode(sha256(codeVerifier));\n```\n\n#### 2. Build Authorization URL\n\n```\nhttps://login.microsoftonline.com/{tenant}/oauth2/v2.0/authorize?\n  client_id={application_id}\n  &response_type=code\n  &redirect_uri={redirect_uri}\n  &scope={scopes}\n  &state={state}\n  &code_challenge={code_challenge}\n  &code_challenge_method=S256\n```\n\n#### 3. Exchange Code for Tokens (No Secret)\n\n```http\nPOST https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={application_id}\n&scope={scopes}\n&code={authorization_code}\n&redirect_uri={redirect_uri}\n&grant_type=authorization_code\n&code_verifier={code_verifier}\n```\n\n## Client Credentials Flow\n\n### Flow Steps\n\n```\n1. App â†’ Entra ID: Request token with client ID + secret\n2. Entra ID: Validate credentials\n3. Entra ID â†’ App: Access token (application permissions)\n4. App â†’ API: Call API with token\n```\n\n### Implementation Steps\n\n#### 1. Configure Application Permissions\n\nIn app registration:\n1. Go to \"API permissions\"\n2. Add **Application** permissions (not delegated)\n3. Grant admin consent (required for app permissions)\n\n**Example permissions:**\n- `User.Read.All` (application) - Read all users\n- `Directory.Read.All` (application) - Read directory\n\n#### 2. Request Access Token\n\n```http\nPOST https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={application_id}\n&scope=https://graph.microsoft.com/.default\n&client_secret={client_secret}\n&grant_type=client_credentials\n```\n\n**Parameters:**\n- `scope`: Use `{resource}/.default` format\n  - For Microsoft Graph: `https://graph.microsoft.com/.default`\n  - For your API: `api://{api_app_id}/.default`\n\n**Response:**\n```json\n{\n  \"access_token\": \"eyJ0eXAi...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3599\n}\n```\n\n#### 3. Use Access Token\n\n```http\nGET https://graph.microsoft.com/v1.0/users\nAuthorization: Bearer {access_token}\n```\n\n## Device Code Flow\n\n**Use for:** Devices without browsers (IoT, CLIs), headless environments\n\n### Flow Steps\n\n```\n1. App â†’ Entra ID: Request device code\n2. Entra ID â†’ App: Device code + user code + verification URL\n3. App â†’ User: Display code and URL\n4. User: Opens URL on another device, enters code\n5. User â†’ Entra ID: Authenticates & consents\n6. App â†’ Entra ID: Poll for token\n7. Entra ID â†’ App: Access token (after user completes auth)\n```\n\n### Implementation Steps\n\n#### 1. Request Device Code\n\n```http\nPOST https://login.microsoftonline.com/{tenant}/oauth2/v2.0/devicecode\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={application_id}\n&scope={scopes}\n```\n\n**Response:**\n```json\n{\n  \"user_code\": \"GTHK-QPMN\",\n  \"device_code\": \"GMMhmHCXhWEzkobqIHGG_EnNYYsAkukHspeYUk9E8\",\n  \"verification_uri\": \"https://microsoft.com/devicelogin\",\n  \"expires_in\": 900,\n  \"interval\": 5,\n  \"message\": \"To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code GTHK-QPMN to authenticate.\"\n}\n```\n\n#### 2. Display Instructions to User\n\n```\nTo sign in, open https://microsoft.com/devicelogin\nand enter code: GTHK-QPMN\n```\n\n#### 3. Poll for Token\n\n```http\nPOST https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={application_id}\n&grant_type=urn:ietf:params:oauth:grant-type:device_code\n&device_code={device_code}\n```\n\n**Poll every 5 seconds (use `interval` from response)**\n\n**Pending Response (user hasn't completed auth yet):**\n```json\n{\n  \"error\": \"authorization_pending\",\n  \"error_description\": \"AADSTS70016: Pending end-user authorization...\"\n}\n```\n\n**Success Response:**\n```json\n{\n  \"access_token\": \"eyJ0eXAi...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"refresh_token\": \"M.R3_BAY...\",\n  \"id_token\": \"eyJ0eXAi...\"\n}\n```\n\n## Refresh Token Flow\n\n**Use for:** Refreshing expired access tokens without re-authentication\n\n### When to Refresh\n\n- Access tokens typically expire in 1 hour\n- Refresh tokens are long-lived (14-90 days)\n- Refresh before access token expires for seamless UX\n\n### Implementation\n\n```http\nPOST https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token\nContent-Type: application/x-www-form-urlencoded\n\nclient_id={application_id}\n&scope={scopes}\n&refresh_token={refresh_token}\n&grant_type=refresh_token\n&client_secret={client_secret}\n```\n\n**Note:** `client_secret` only required for confidential clients\n\n**Response:**\n```json\n{\n  \"access_token\": \"eyJ0eXAi...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"refresh_token\": \"M.R3_BAY...\",\n  \"id_token\": \"eyJ0eXAi...\"\n}\n```\n\n**Important:** New refresh token is returned; use it for next refresh\n\n## Token Types\n\n### Access Token\n\n- Used to call APIs\n- Contains claims (user ID, permissions, etc.)\n- Short-lived (typically 1 hour)\n- Format: JWT (JSON Web Token)\n\n**Sample claims:**\n```json\n{\n  \"aud\": \"https://graph.microsoft.com\",\n  \"iss\": \"https://sts.windows.net/{tenant}/\",\n  \"sub\": \"{user_object_id}\",\n  \"scp\": \"User.Read Mail.Read\",\n  \"exp\": 1680000000\n}\n```\n\n### Refresh Token\n\n- Used to get new access tokens\n- Long-lived (days to months)\n- Opaque string (not JWT)\n- Single-use (new one issued with each refresh)\n\n### ID Token\n\n- Contains user identity information\n- Used by the app to authenticate user\n- Format: JWT\n\n**Sample claims:**\n```json\n{\n  \"sub\": \"{user_object_id}\",\n  \"name\": \"Jane Doe\",\n  \"preferred_username\": \"jane@contoso.com\",\n  \"email\": \"jane@contoso.com\",\n  \"oid\": \"{object_id}\"\n}\n```\n\n## Scopes and Permissions\n\n### Scope Format\n\n**Microsoft Graph:**\n```\nhttps://graph.microsoft.com/User.Read\nhttps://graph.microsoft.com/Mail.Send\n```\n\n**Custom API:**\n```\napi://{api_application_id}/access_as_user\n```\n\n## Security Considerations\n\n| Practice | Why |\n|----------|-----|\n| **Use state parameter** | Prevents CSRF attacks |\n| **Use PKCE for public clients** | Prevents authorization code interception |\n| **Validate tokens** | Verify signature, issuer, audience, expiration |\n| **Use HTTPS only** | Protect tokens in transit |\n| **Store tokens securely** | Use secure storage, never in localStorage for sensitive apps |\n| **Implement token refresh** | Seamless UX without repeated logins |\n| **Handle token expiration** | Gracefully refresh or re-authenticate |\n| **Minimal scope principle** | Request only necessary permissions |\n\n## Additional Resources\n\n[OAuth 2.0 spec](https://www.rfc-editor.org/rfc/rfc6749)",
        "plugin/skills/entra-app-registration/references/TROUBLESHOOTING.md": "# Troubleshooting Microsoft Entra App Registration\n\nThis guide helps you diagnose and fix common issues with app registrations and authentication.\n\n## Table of Contents\n\n- [Authentication Errors](#authentication-errors)\n- [Token Issues](#token-issues)\n- [Permission Problems](#permission-problems)\n- [Redirect URI Issues](#redirect-uri-issues)\n- [Application Configuration](#application-configuration)\n- [Debugging Tools](#debugging-tools)\n\n## Authentication Errors\n\n### Redirect URI Mismatch\n\n**Error message:**\n```\nAADSTS50011: The redirect URI 'http://localhost:3000' specified in the request \ndoes not match the redirect URIs configured for the application.\n```\n\n**Cause:** The redirect URI in your authentication request doesn't exactly match what's registered.\n\n**Solutions:**\n\n1. **Check exact match** (case-sensitive, trailing slash matters):\n   ```\n   Registered: https://myapp.com/callback\n   Request:    https://myapp.com/callback/  âŒ (trailing slash)\n   Request:    https://MyApp.com/callback   âŒ (case difference)\n   Request:    https://myapp.com/callback   âœ…\n   ```\n\n2. **Add URI to app registration:**\n   ```bash\n   # Portal: Authentication â†’ Add redirect URI\n   # CLI:\n   az ad app update --id $APP_ID \\\n     --web-redirect-uris \"http://localhost:3000\" \"https://myapp.com/callback\"\n   ```\n\n3. **Check platform type:**\n   - Web URIs go in \"Web\" platform\n   - SPA URIs go in \"Single-page application\"\n   - Desktop/mobile URIs go in \"Public client/native\"\n\n### Invalid Client Secret\n\n**Error message:**\n```\nAADSTS7000215: Invalid client secret provided. \nEnsure the secret being sent in the request is the client secret value, not the client secret ID.\n```\n\n**Causes:**\n- Client secret expired\n- Wrong secret value (copied secret ID instead of value)\n- Secret doesn't match app registration\n\n**Solutions:**\n\n1. **Check expiration:**\n   ```bash\n   az ad app credential list --id $APP_ID\n   ```\n2. **Create new secret:**\n   ```bash\n   az ad app credential reset --id $APP_ID --years 1\n   ```\n   Copy the `password` value (not the `keyId`)\n\n### User Consent Required\n\n**Error message:**\n```\nAADSTS65001: The user or administrator has not consented to use the application\n```\n\n**Causes:**\n- Application permissions require admin consent\n- User hasn't consented to delegated permissions\n- Consent was revoked\n\n**Solutions:**\n\n1. **Grant admin consent (if admin):**\n   ```bash\n   az ad app permission admin-consent --id $APP_ID\n   ```\n\n2. **Request user consent (interactive flow):**\n   This requires the client app to have access to UI such as browser, terminal window, etc. Follow the best practices of your client app to implement the interactive flow.\n\n3. **Check API permissions in portal:**\n   - Ensure permissions are added\n   - Look for green checkmarks (granted)\n   - Yellow warning means not granted\n\n### Grant Declined\n\n**Error message:**\n```\nAADSTS70000: The request was denied because one or more permissions have been declined\n```\n\n**Cause:** User or admin explicitly denied consent.\n\n**Solutions:**\n\n1. **Re-request with explanation:**\n   - Explain why permissions are needed\n   - Request only necessary permissions\n\n2. **Check if admin consent is required:**\n   - Some organizations disable user consent\n   - Contact your admin to grant consent\n\n3. **Reduce permission scope:**\n   - Request minimal permissions initially\n   - Use incremental consent for additional features\n\n### Application Not Found\n\n**Error message:**\n```\nAADSTS700016: Application with identifier '{app-id}' was not found in the directory\n```\n\n**Causes:**\n- Wrong application ID\n- Wrong tenant ID\n- Service principal not created\n- App in different tenant\n\n**Solutions:**\n\n1. **Verify application ID:**\n   ```bash\n   az ad app list --display-name \"MyApp\" --query \"[].{Name:displayName, AppId:appId}\"\n   ```\n\n2. **Verify tenant ID:**\n   ```bash\n   az account show --query tenantId -o tsv\n   ```\n\n### Application Doesn't have a Service Principal\n\n**Error message:**\n```\nThe app is trying to access a service 'your_app_id'(your_app_name) that your organization 'your_tenant_id' lacks a service principal for\n```\n\n**Causes:**\n- Your tenant is not configured to automatically provision the service principal for app registrations in it.\n\n**Solutions:**\n\n1. **Create service principal:**\n   ```bash\n   az ad sp create --id $APP_ID\n   ```\n\n### Missing Required Field\n\n**Error message:**\n```\nAADSTS90014: The required field 'client_id' is missing from the request\n```\n\nThis can happen if the client you are using isn't compatible with Entra. Consult the owner of your client app to see if it supports Entra.\n\n## Token Issues\n\nUnless the the access token is encrypted, you can decode and view its claims securely at https://jwt.ms. **Don't** use any other website to decode an access token. Compare the claims in the token with the app registration's configuration to identify issues.\n\n## Debugging Tools\n\n### JWT Token Decoder\n\n**Tool:** https://jwt.ms\n\n**How to use:**\n1. Copy your access token\n2. Paste into jwt.ms\n3. Review claims:\n   - `aud` - Audience (should match your API)\n   - `iss` - Issuer (should be login.microsoftonline.com)\n   - `scp` - Delegated permissions\n   - `roles` - Application permissions\n   - `exp` - Expiration timestamp\n   - `oid` - User object ID\n\n---\n\n### Fiddler\n\n**Use for:** Inspecting HTTP requests/responses\n\n**What to check:**\n- Authorization header format: `Bearer {token}`\n- Token is being sent\n- Response status codes and error messages\n\n### Entra Sign-in Logs\n\n**Access:** Azure Portal â†’ Microsoft Entra ID â†’ Sign-in logs\n\n**What to check:**\n- Failed sign-in attempts\n- Error codes and messages\n- User consent status\n- Conditional Access policy failures\n\n## Common Error Codes Reference\n\n| Error Code | Meaning | Common Cause |\n|------------|---------|--------------|\n| AADSTS50011 | Redirect URI mismatch | URI not registered or doesn't match |\n| AADSTS50020 | Invalid tenant | Wrong tenant in authority URL |\n| AADSTS50034 | User not found | User doesn't exist in tenant |\n| AADSTS50053 | Account locked | Too many failed attempts |\n| AADSTS50055 | Password expired | User needs to reset password |\n| AADSTS50057 | Account disabled | User account disabled |\n| AADSTS50058 | Silent sign-in failed | Interactive auth required |\n| AADSTS50059 | Tenant not found | Invalid tenant ID |\n| AADSTS65001 | Consent required | User/admin hasn't consented |\n| AADSTS70000 | Grant declined | User denied consent |\n| AADSTS70001 | App disabled | App registration disabled |\n| AADSTS700016 | App not found | Invalid app ID or wrong tenant |\n| AADSTS7000215 | Invalid client secret | Wrong/expired secret |\n| AADSTS90014 | Missing field | Required parameter not sent |\n| AADSTS90072 | Consent needed | Admin consent required |\n\n## Best Practices for Troubleshooting\n\n### Systematic Approach\n\n1. **Collect information:**\n   - Exact error message and code\n   - When it started happening\n   - What changed recently\n   - Environment (dev/test/prod)\n\n2. **Check basics first:**\n   - App ID and tenant ID correct\n   - Permissions added and consented\n   - Redirect URIs configured\n   - Secrets/certificates valid\n\n3. **Use debugging tools:**\n   - Decode tokens (jwt.ms)\n   - Check sign-in logs\n   - Enable MSAL logging\n   - Use network inspector\n\n4. **Test incrementally:**\n   - Test with minimal permissions\n   - Add permissions one at a time\n   - Test different flows separately\n\n## Getting Help\n\n### Microsoft Resources\n\n- [Microsoft Q&A](https://learn.microsoft.com/answers/)\n- [Microsoft Identity Platform Documentation](https://learn.microsoft.com/entra/identity-platform/)\n",
        "plugin/skills/microsoft-foundry/SKILL.md": "---\nname: microsoft-foundry\ndescription: 'Expert in Microsoft Foundry: use this skill to help discover and deploy models, build RAG applications with knowledge indexes, create and evaluate AI agents, and troubleshoot common issues in Microsoft Foundry platform.'\n---\n\n# Microsoft Foundry Skill\n\nThis skill helps developers work with Microsoft Foundry resources, covering model discovery and deployment, RAG (Retrieval-Augmented Generation) applications, AI agent creation, evaluation workflows, and troubleshooting.\n\n## When to Use This Skill\n\nUse this skill when the user wants to:\n\n- **Discover and deploy AI models** from the Microsoft Foundry catalog\n- **Build RAG applications** using knowledge indexes and vector search\n- **Create AI agents** with tools like Azure AI Search, web search, or custom functions\n- **Evaluate agent performance** using built-in evaluators\n- **Set up monitoring** and continuous evaluation for production agents\n- **Troubleshoot issues** with deployments, agents, or evaluations\n\n## Prerequisites\n\n### Azure Resources\n- An Azure subscription with an active account\n- Appropriate permissions to create Microsoft Foundry resources (e.g., Azure AI Owner role)\n- Resource group for organizing Foundry resources\n\n### Tools\n- **Azure CLI** installed and authenticated (`az login`)\n- **Azure Developer CLI (azd)** for deployment workflows (optional but recommended)\n\n### Language-Specific Requirements\n\nFor SDK examples and implementation details in specific programming languages, refer to:\n- **Python**: See [language/python.md](language/python.md) for Python SDK setup, authentication, and examples\n\n## Core Workflows\n\n### 1. Getting Started - Model Discovery and Deployment\n\n#### Use Case\nA developer new to Microsoft Foundry wants to explore available models and deploy their first one.\n\n#### Step 1: List Available Resources\n\nFirst, help the user discover their Microsoft Foundry resources.\n\n**Using Azure CLI:**\n\n##### Bash\n```bash\n# List all Microsoft Foundry resources in subscription\naz resource list \\\n  --resource-type \"Microsoft.CognitiveServices/accounts\" \\\n  --query \"[?kind=='AIServices'].{Name:name, ResourceGroup:resourceGroup, Location:location}\" \\\n  --output table\n\n# List resources in a specific resource group\naz resource list \\\n  --resource-group <resource-group-name> \\\n  --resource-type \"Microsoft.CognitiveServices/accounts\" \\\n  --output table\n```\n\n**Using MCP Tools:**\n\nUse the `foundry_resource_get` MCP tool to get detailed information about a specific Foundry resource, or to list all resources if no name is provided.\n\n#### Step 2: Browse Model Catalog\n\nHelp users discover available models, including information about free playground support.\n\n**Key Points to Explain:**\n- Some models support **free playground** for prototyping without costs\n- Models can be filtered by **publisher** (e.g., OpenAI, Meta, Microsoft)\n- Models can be filtered by **license type**\n- Model availability varies by region\n\n**Using MCP Tools:**\n\nUse the `foundry_models_list` MCP tool:\n- List all models: `foundry_models_list()`\n- List free playground models: `foundry_models_list(search-for-free-playground=true)`\n- Filter by publisher: `foundry_models_list(publisher=\"OpenAI\")`\n- Filter by license: `foundry_models_list(license=\"MIT\")`\n\n**Example Output Explanation:**\nWhen listing models, explain to users:\n- Models with free playground support can be used for prototyping at no cost\n- Some models support GitHub token authentication for easy access\n- Check model capabilities and pricing before production deployment\n\n#### Step 3: Deploy a Model\n\nGuide users through deploying a model to their Foundry resource.\n\n**Using Azure CLI:**\n\n##### Bash\n```bash\n# Deploy a model (e.g., gpt-4o)\naz cognitiveservices account deployment create \\\n  --name <foundry-resource-name> \\\n  --resource-group <resource-group-name> \\\n  --deployment-name gpt-4o-deployment \\\n  --model-name gpt-4o \\\n  --model-version \"2024-05-13\" \\\n  --model-format OpenAI \\\n  --sku-capacity 10 \\\n  --sku-name Standard\n\n# Verify deployment status\naz cognitiveservices account deployment show \\\n  --name <foundry-resource-name> \\\n  --resource-group <resource-group-name> \\\n  --deployment-name gpt-4o-deployment\n```\n\n**Using MCP Tools:**\n\nUse the `foundry_models_deploy` MCP tool with parameters:\n- `resource-group`: Resource group name\n- `deployment`: Deployment name\n- `model-name`: Model to deploy (e.g., \"gpt-4o\")\n- `model-format`: Format (e.g., \"OpenAI\")\n- `azure-ai-services`: Foundry resource name\n- `model-version`: Specific version\n- `sku-capacity`: Capacity units\n- `scale-type`: Scaling type\n\n**Deployment Verification:**\nExplain that when deployment completes, `provisioningState` should be `Succeeded`. If it fails, common issues include:\n- Insufficient quota\n- Region capacity limitations\n- Permission issues\n\n#### Step 4: Get Resource Endpoint\n\nUsers need the project endpoint to connect their code to Foundry.\n\n**Using MCP Tools:**\n\nUse the `foundry_resource_get` MCP tool to retrieve resource details including the endpoint.\n\n**Expected Output:**\nThe endpoint will be in format: `https://<resource>.services.ai.azure.com/api/projects/<project-name>`\n\nSave this endpoint as it's needed for subsequent API and SDK calls.\n\n### 2. Building RAG Applications with Knowledge Indexes\n\n#### Use Case\nA developer wants to build a Retrieval-Augmented Generation (RAG) application using their own documents.\n\n#### Understanding RAG and Knowledge Indexes\n\n**Explain the Concept:**\nRAG enhances AI responses by:\n1. **Retrieving** relevant documents from a knowledge base\n2. **Augmenting** the AI prompt with retrieved context\n3. **Generating** responses grounded in factual information\n\n**Knowledge Index Benefits:**\n- Supports keyword, semantic, vector, and hybrid search\n- Enables efficient retrieval of relevant content\n- Stores metadata for better citations (document titles, URLs, file names)\n- Integrates with Azure AI Search for production scenarios\n\n#### Step 1: List Existing Knowledge Indexes\n\n**Using MCP Tools:**\n\nUse `foundry_knowledge_index_list` with your project endpoint to list knowledge indexes.\n\n#### Step 2: Inspect Index Schema\n\nUnderstanding the index structure helps optimize queries.\n\n**Using MCP Tools:**\n\nUse the `foundry_knowledge_index_schema` MCP tool with your project endpoint and index name to get detailed schema information.\n\n**Schema Information Includes:**\n- Field definitions and data types\n- Searchable attributes\n- Vectorization configuration\n- Retrieval mode support (keyword, semantic, vector, hybrid)\n\n#### Step 3: Create an Agent with Azure AI Search Tool\n\n**Implementation:**\n\nTo create a RAG agent with Azure AI Search tool integration:\n\n1. **Initialize the AI Project Client** with your project endpoint and credentials\n2. **Get the Azure AI Search connection** from your project\n3. **Create the agent** with:\n   - Agent name\n   - Model deployment\n   - Clear instructions (see best practices below)\n   - Azure AI Search tool configuration with:\n     - Connection ID\n     - Index name\n     - Query type (HYBRID recommended)\n\n**For SDK Implementation:** See [language/python.md](language/python.md#rag-applications-with-python-sdk)\n\n**Key Best Practices:**\n- **Always request citations** in agent instructions\n- Use **hybrid search** (AzureAISearchQueryType.HYBRID) for best results\n- Instruct the agent to say \"I don't know\" when information isn't in the index\n- Format citations consistently for easy parsing\n\n#### Step 4: Test the RAG Agent\n\n**Testing Process:**\n\n1. **Query the agent** with a test question\n2. **Stream the response** to get real-time output\n3. **Capture citations** from the response annotations\n4. **Validate** that citations are properly formatted and included\n\n**For SDK Implementation:** See [language/python.md](language/python.md#testing-the-rag-agent)\n\n**Troubleshooting RAG Issues:**\n\n| Issue | Possible Cause | Resolution |\n|-------|---------------|------------|\n| No citations in response | Agent instructions don't request citations | Update instructions to explicitly request citation format |\n| \"Index not found\" error | Wrong index name or connection | Verify `AI_SEARCH_INDEX_NAME` matches index in Azure AI Search |\n| 401/403 authentication error | Missing RBAC permissions | Assign project managed identity **Search Index Data Contributor** role |\n| Poor retrieval quality | Query type not optimal | Try HYBRID query type for better results |\n\n### 3. Creating Your First AI Agent\n\n#### Use Case\nA developer wants to create an AI agent with tools (web search, function calling, file search).\n\n#### Step 1: List Existing Agents\n\n**Using MCP Tools:**\n\nUse `foundry_agents_list` with your project endpoint to list existing agents.\n\n#### Step 2: Create a Basic Agent\n\n**Implementation:**\n\nCreate an agent with:\n- **Model deployment name**: The model to use\n- **Agent name**: Unique identifier\n- **Instructions**: Clear, specific guidance for the agent's behavior\n\n**For SDK Implementation:** See [language/python.md](language/python.md#basic-agent)\n\n#### Step 3: Create an Agent with Custom Function Tools\n\nAgents can call custom functions to perform actions like querying databases, calling APIs, or performing calculations.\n\n**Implementation Steps:**\n\n1. **Define custom functions** with clear docstrings describing their purpose and parameters\n2. **Create a function toolset** with your custom functions\n3. **Create the agent** with the toolset and instructions on when to use the tools\n\n**For SDK Implementation:** See [language/python.md](language/python.md#agent-with-custom-function-tools)\n\n#### Step 4: Create an Agent with Web Search\n\n**Implementation:**\n\nCreate an agent with web search capabilities by adding a Web Search tool:\n- Optionally specify user location for localized results\n- Provide instructions to always cite web sources\n\n**For SDK Implementation:** See [language/python.md](language/python.md#agent-with-web-search)\n\n#### Step 5: Interact with the Agent\n\n**Interaction Process:**\n\n1. **Create a conversation thread** for the agent interaction\n2. **Add user messages** to the thread\n3. **Run the agent** to process the messages and generate responses\n4. **Check run status** for success or failure\n5. **Retrieve messages** to see the agent's responses\n6. **Cleanup** by deleting the agent when done\n\n**For SDK Implementation:** See [language/python.md](language/python.md#interacting-with-agents)\n\n**Agent Best Practices:**\n\n1. **Clear Instructions**: Provide specific, actionable instructions\n2. **Tool Selection**: Only include tools the agent needs\n3. **Error Handling**: Always check `run.status` for failures\n4. **Cleanup**: Delete agents/threads when done to manage costs\n5. **Rate Limits**: Handle rate limit errors gracefully (status code 429)\n\n\n### 4. Evaluating Agent Performance\n\n#### Use Case\nA developer has built an agent and wants to evaluate its quality, safety, and performance.\n\n#### Understanding Agent Evaluators\n\n**Built-in Evaluators:**\n\n1. **IntentResolutionEvaluator**: Measures how well the agent identifies and understands user requests (score 1-5)\n2. **TaskAdherenceEvaluator**: Evaluates whether responses adhere to assigned tasks and system instructions (score 1-5)\n3. **ToolCallAccuracyEvaluator**: Assesses whether the agent makes correct function tool calls (score 1-5)\n\n**Evaluation Output:**\nEach evaluator returns:\n- `{metric_name}`: Numerical score (1-5, higher is better)\n- `{metric_name}_result`: \"pass\" or \"fail\" based on threshold\n- `{metric_name}_threshold`: Binarization threshold (default or user-set)\n- `{metric_name}_reason`: Explanation of the score\n\n#### Step 1: Single Agent Run Evaluation\n\n**Using MCP Tools:**\n\nUse the `foundry_agents_query_and_evaluate` MCP tool to query an agent and evaluate the response in one call. Provide:\n- Agent ID\n- Query text\n- Project endpoint\n- Azure OpenAI endpoint and deployment for evaluation\n- Comma-separated list of evaluators to use\n\n**Example Output:**\n```json\n{\n  \"response\": \"The weather in Seattle is currently sunny and 22Â°C.\",\n  \"evaluation\": {\n    \"intent_resolution\": 5.0,\n    \"intent_resolution_result\": \"pass\",\n    \"intent_resolution_threshold\": 3,\n    \"intent_resolution_reason\": \"The agent correctly identified the user's intent to get weather information and provided a relevant response.\",\n    \"task_adherence\": 4.0,\n    \"task_adherence_result\": \"pass\",\n    \"tool_call_accuracy\": 5.0,\n    \"tool_call_accuracy_result\": \"pass\"\n  }\n}\n```\n\n#### Step 2: Evaluate Existing Response\n\nIf you already have the agent's response, you can evaluate it directly.\n\n**Using MCP Tools:**\n\nUse the `foundry_agents_evaluate` MCP tool to evaluate a specific query/response pair with a single evaluator.\n\n**For SDK Implementation:** See [language/python.md](language/python.md#single-response-evaluation-using-mcp)\n\n#### Step 3: Batch Evaluation\n\nFor evaluating multiple agent runs across multiple conversation threads:\n\n1. **Convert agent thread data** to evaluation format\n2. **Prepare evaluation data** from multiple thread IDs\n3. **Set up evaluators** with appropriate configuration\n4. **Run batch evaluation** and view results in the Foundry portal\n\n**For SDK Implementation:** See [language/python.md](language/python.md#batch-evaluation)\n\n#### Interpreting Evaluation Results\n\n**Score Ranges (1-5 scale):**\n- **5**: Excellent - Agent perfectly understood and executed the task\n- **4**: Good - Minor issues, but overall successful\n- **3**: Acceptable - Threshold for passing (default)\n- **2**: Poor - Significant issues with understanding or execution\n- **1**: Failed - Agent completely misunderstood or failed the task\n\n**Common Evaluation Issues:**\n\n| Issue | Cause | Resolution |\n|-------|-------|------------|\n| Job stuck in \"Running\" | Insufficient model capacity | Increase model quota/capacity and rerun |\n| All metrics zero | Wrong evaluator or unsupported model | Verify evaluator compatibility with your model |\n| Groundedness unexpectedly low | Incomplete context/retrieval | Verify RAG retrieval includes sufficient context |\n| Evaluation missing | Not selected during setup | Rerun evaluation with required metrics |\n\n### 5. Troubleshooting Common Issues\n\n#### Deployment Issues\n\n**Problem: Deployment Stays Pending or Fails**\n\n##### Bash\n```bash\n# Check deployment status and details\naz cognitiveservices account deployment show \\\n  --name <resource-name> \\\n  --resource-group <resource-group> \\\n  --deployment-name <deployment-name> \\\n  --output json\n\n# Check account quota\naz cognitiveservices account show \\\n  --name <resource-name> \\\n  --resource-group <resource-group> \\\n  --query \"properties.quotaLimit\"\n```\n\n**Common Causes:**\n- Insufficient quota in the region\n- Region at capacity for the model\n- Permission issues\n\n**Resolution:**\n1. Check quota limits in Azure Portal\n2. Request quota increase if needed\n3. Try deploying to a different region\n4. Verify you have appropriate RBAC permissions\n\n#### Agent Response Issues\n\n**Problem: Agent Doesn't Return Citations (RAG)**\n\n**Diagnostics:**\n1. Check agent instructions explicitly request citations\n2. Verify the tool choice is set to \"required\" or \"auto\"\n3. Confirm the Azure AI Search connection is configured correctly\n\n**Resolution:**\n\nUpdate the agent's instructions to explicitly request citations in the format `[message_idx:search_idxâ€ source]` and to only use the knowledge base, never the agent's own knowledge.\n\n**For SDK Implementation:** See [language/python.md](language/python.md#update-agent-instructions)\n\n**Problem: \"Index Not Found\" Error**\n\n**Using MCP Tools:**\n\nUse the `foundry_knowledge_index_list` MCP tool to verify the index exists and get the correct name.\n\n**Resolution:**\n1. Verify `AI_SEARCH_INDEX_NAME` environment variable matches actual index name\n2. Check the connection points to correct Azure AI Search resource\n3. Ensure index has been created and populated\n\n**Problem: 401/403 Authentication Errors**\n\n**Common Cause:** Missing RBAC permissions\n\n**Resolution:**\n\n##### Bash\n```bash\n# Assign Search Index Data Contributor role to managed identity\naz role assignment create \\\n  --assignee <managed-identity-principal-id> \\\n  --role \"Search Index Data Contributor\" \\\n  --scope /subscriptions/<subscription-id>/resourceGroups/<rg>/providers/Microsoft.Search/searchServices/<search-service>\n\n# Verify role assignment\naz role assignment list \\\n  --assignee <managed-identity-principal-id> \\\n  --output table\n```\n\n#### Evaluation Issues\n\n**Problem: Evaluation Dashboard Shows No Data**\n\n**Common Causes:**\n- No recent agent traffic\n- Time range excludes the data\n- Ingestion delay\n\n**Resolution:**\n1. Generate new agent traffic (test queries)\n2. Expand the time range filter in the dashboard\n3. Wait a few minutes for data ingestion\n4. Refresh the dashboard\n\n**Problem: Continuous Evaluation Not Running**\n\n**Diagnostics:**\n\nCheck evaluation run status to identify issues. For SDK implementation, see [language/python.md](language/python.md#checking-evaluation-status).\n\n**Resolution:**\n1. Verify the evaluation rule is enabled\n2. Confirm agent traffic is flowing\n3. Check project managed identity has **Azure AI User** role\n4. Verify OpenAI endpoint and deployment are accessible\n\n#### Rate Limiting and Capacity Issues\n\n**Problem: Agent Run Fails with Rate Limit Error**\n\n**Error Message:** `Rate limit is exceeded` or HTTP 429\n\n**Resolution:**\n\n##### Bash\n```bash\n# Check current quota usage\naz cognitiveservices usage list \\\n  --name <resource-name> \\\n  --resource-group <resource-group>\n\n# Request quota increase (manual process in portal)\necho \"Request quota increase in Azure Portal under Quotas section\"\n```\n\n# Request quota increase (manual process in portal)\nWrite-Output \"Request quota increase in Azure Portal under Quotas section\"\n```\n\n**Best Practices:**\n- Implement exponential backoff retry logic\n- Use Dynamic Quota when available\n- Monitor quota usage proactively\n- Consider multiple deployments across regions\n\n## Quick Reference\n\n### Common Environment Variables\n\n```bash\n# Foundry Project\nPROJECT_ENDPOINT=https://<resource>.services.ai.azure.com/api/projects/<project>\nMODEL_DEPLOYMENT_NAME=gpt-4o\n\n# Azure AI Search (for RAG)\nAZURE_AI_SEARCH_CONNECTION_NAME=my-search-connection\nAI_SEARCH_INDEX_NAME=my-index\n\n# Evaluation\nAZURE_OPENAI_ENDPOINT=https://<resource>.openai.azure.com\nAZURE_OPENAI_DEPLOYMENT=gpt-4o\n```\n\n### Useful MCP Tools Quick Reference\n\n**Resource Management**\n- `foundry_resource_get` - Get resource details and endpoint\n\n**Models**\n- `foundry_models_list` - Browse model catalog\n- `foundry_models_deploy` - Deploy a model\n- `foundry_models_deployments_list` - List deployed models\n\n**Knowledge & RAG**\n- `foundry_knowledge_index_list` - List knowledge indexes\n- `foundry_knowledge_index_schema` - Get index schema\n\n**Agents**\n- `foundry_agents_list` - List agents\n- `foundry_agents_connect` - Query an agent\n- `foundry_agents_query_and_evaluate` - Query and evaluate\n\n**OpenAI Operations**\n- `foundry_openai_chat_completions_create` - Create chat completions\n- `foundry_openai_embeddings_create` - Create embeddings\n\n### Language-Specific Quick References\n\nFor SDK-specific details, authentication, and code examples:\n- **Python**: See [language/python.md](language/python.md)\n\n## Additional Resources\n\n### Documentation Links\n- [Microsoft Foundry Documentation](https://learn.microsoft.com/azure/ai-foundry/)\n- [Microsoft Foundry Quickstart](https://learn.microsoft.com/azure/ai-foundry/quickstarts/get-started-code)\n- [RAG and Knowledge Indexes](https://learn.microsoft.com/azure/ai-foundry/concepts/retrieval-augmented-generation)\n- [Agent Evaluation Guide](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)\n\n### GitHub Samples\n- [Microsoft Foundry Samples](https://github.com/azure-ai-foundry/foundry-samples)\n- [Azure Search OpenAI Demo](https://github.com/Azure-Samples/azure-search-openai-demo)\n- [Azure Search Classic RAG](https://github.com/Azure-Samples/azure-search-classic-rag)\n",
        "plugin/skills/microsoft-foundry/language/python.md": "# Microsoft Foundry - Python SDK Guide\n\nThis guide provides Python-specific implementations for working with Microsoft Foundry.\n\n## Prerequisites\n\n### Python Environment\n- **Python 3.8+** required\n- **pip** package manager\n\n### Python Package Installation\n\n```bash\n# Core packages for Microsoft Foundry\npip install azure-ai-projects azure-identity azure-ai-inference openai\n\n# For evaluation\npip install azure-ai-evaluation\n\n# For environment management (recommended)\npip install python-dotenv\n```\n\n### Authentication\n\nAlways use `DefaultAzureCredential` from `azure.identity`:\n\n```python\nfrom azure.identity import DefaultAzureCredential\n\n# This works with:\n# - Azure CLI (az login)\n# - Managed Identity (when running in Azure)\n# - Environment variables\n# - VS Code authentication\ncredential = DefaultAzureCredential()\n```\n\n### Environment Variables\n\nCreate a `.env` file or set these environment variables:\n\n```bash\n# Foundry Project\nPROJECT_ENDPOINT=https://<resource>.services.ai.azure.com/api/projects/<project>\nMODEL_DEPLOYMENT_NAME=gpt-4o\n\n# Azure AI Search (for RAG)\nAZURE_AI_SEARCH_CONNECTION_NAME=my-search-connection\nAI_SEARCH_INDEX_NAME=my-index\n\n# Evaluation\nAZURE_OPENAI_ENDPOINT=https://<resource>.openai.azure.com\nAZURE_OPENAI_DEPLOYMENT=gpt-4o\n```\n\n## Model Discovery and Deployment\n\n### Using MCP Tools in Python\n\n```python\n# List all available models\nfoundry_models_list()\n\n# List models that support free playground\nfoundry_models_list(search_for_free_playground=True)\n\n# Filter by publisher\nfoundry_models_list(publisher=\"OpenAI\")\n\n# Deploy a model\nfoundry_models_deploy(\n    resource_group=\"my-resource-group\",\n    deployment=\"gpt-4o-deployment\",\n    model_name=\"gpt-4o\",\n    model_format=\"OpenAI\",\n    azure_ai_services=\"my-foundry-resource\",\n    model_version=\"2024-05-13\",\n    sku_capacity=10,\n    scale_type=\"Standard\"\n)\n\n# Get resource details\nfoundry_resource_get(\n    resource_name=\"my-foundry-resource\",\n    resource_group=\"my-resource-group\"\n)\n```\n\n## RAG Applications with Python SDK\n\n### Complete RAG Agent Example\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom azure.identity import DefaultAzureCredential\nfrom azure.ai.projects import AIProjectClient\nfrom azure.ai.agents.models import (\n    AzureAISearchToolDefinition,\n    AzureAISearchToolResource,\n    AISearchIndexResource,\n    AzureAISearchQueryType,\n)\n\nload_dotenv()\n\n# Create project client\nproject_client = AIProjectClient(\n    endpoint=os.environ[\"FOUNDRY_PROJECT_ENDPOINT\"],\n    credential=DefaultAzureCredential(),\n)\n\nopenai_client = project_client.get_openai_client()\n\n# Get Azure AI Search connection\nazs_connection = project_client.connections.get(\n    os.environ[\"AZURE_AI_SEARCH_CONNECTION_NAME\"]\n)\nconnection_id = azs_connection.id\n\n# Create agent with Azure AI Search tool\nagent = project_client.agents.create_agent(\n    model=os.environ[\"FOUNDRY_MODEL_DEPLOYMENT_NAME\"],\n    name=\"RAGAgent\",\n    instructions=\"\"\"You are a helpful assistant that uses the knowledge base \n    to answer questions. You must always provide citations using the tool \n    and render them as: `[message_idx:search_idxâ€ source]`. \n    If you cannot find the answer in the knowledge base, say \"I don't know\".\"\"\",\n    tools=[\n        AzureAISearchToolDefinition(\n            azure_ai_search=AzureAISearchToolResource(\n                indexes=[\n                    AISearchIndexResource(\n                        index_connection_id=connection_id,\n                        index_name=os.environ[\"AI_SEARCH_INDEX_NAME\"],\n                        query_type=AzureAISearchQueryType.HYBRID,\n                    ),\n                ]\n            )\n        )\n    ],\n)\n\nprint(f\"Agent created: {agent.name} (ID: {agent.id})\")\n```\n\n### Testing the RAG Agent\n\n```python\n# Query the agent\nuser_query = input(\"Ask a question: \")\n\nstream_response = openai_client.responses.create(\n    stream=True,\n    tool_choice=\"required\",\n    input=user_query,\n    extra_body={\"agent\": {\"name\": agent.name, \"type\": \"agent_reference\"}},\n)\n\n# Process streaming response\nfor event in stream_response:\n    if event.type == \"response.output_text.delta\":\n        print(event.delta, end=\"\", flush=True)\n    elif event.type == \"response.output_item.done\":\n        if event.item.type == \"message\":\n            item = event.item\n            if item.content[-1].type == \"output_text\":\n                text_content = item.content[-1]\n                for annotation in text_content.annotations:\n                    if annotation.type == \"url_citation\":\n                        print(f\"\\nðŸ“Ž Citation: {annotation.url}\")\n    elif event.type == \"response.completed\":\n        print(\"\\nâœ… Response complete\")\n```\n\n### Update Agent Instructions\n\n```python\n# Update agent to request citations properly\nupdated_agent = project_client.agents.update_agent(\n    agent_id=agent.id,\n    model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n    instructions=\"\"\"You are a helpful assistant. You must always provide \n    citations using the tool and render them as: `[message_idx:search_idxâ€ source]`. \n    Never answer from your own knowledge - only use the knowledge base.\"\"\",\n    tools=original_tools\n)\n```\n\n## Creating AI Agents with Python SDK\n\n### Basic Agent\n\n```python\nimport os\nfrom azure.ai.projects import AIProjectClient\nfrom azure.identity import DefaultAzureCredential\n\n# Create project client\nproject_client = AIProjectClient(\n    endpoint=os.environ[\"PROJECT_ENDPOINT\"],\n    credential=DefaultAzureCredential(),\n)\n\n# Create a simple agent\nagent = project_client.agents.create_agent(\n    model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n    name=\"my-helpful-agent\",\n    instructions=\"You are a helpful assistant that can answer questions clearly and concisely.\",\n)\n\nprint(f\"Created agent with ID: {agent.id}\")\n```\n\n### Agent with Custom Function Tools\n\n```python\nfrom azure.ai.agents.models import FunctionTool, ToolSet\n\n# Define custom functions\ndef get_weather(location: str, unit: str = \"celsius\") -> str:\n    \"\"\"Get the current weather for a location.\n    \n    Args:\n        location: The city and state, e.g., 'San Francisco, CA'\n        unit: Temperature unit, either 'celsius' or 'fahrenheit'\n    \"\"\"\n    # Mock implementation\n    return f\"The weather in {location} is sunny and 22Â°{unit[0].upper()}\"\n\ndef search_database(query: str) -> str:\n    \"\"\"Search the product database.\n    \n    Args:\n        query: Search query string\n    \"\"\"\n    # Mock implementation\n    return f\"Found 3 products matching '{query}'\"\n\n# Create function toolset\nuser_functions = [get_weather, search_database]\nfunctions = FunctionTool(user_functions)\ntoolset = ToolSet()\ntoolset.add(functions)\n\n# Create agent with tools\nagent = project_client.agents.create_agent(\n    model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n    name=\"function-agent\",\n    instructions=\"You are a helpful assistant with access to weather and product database tools. Use them to help users.\",\n    toolset=toolset\n)\n\nprint(f\"Created agent with function tools: {agent.id}\")\n```\n\n### Agent with Web Search\n\n```python\nfrom azure.ai.agents.models import BingGroundingToolDefinition\n\n# Create agent with web search capability\nagent = project_client.agents.create_agent(\n    model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n    name=\"WebSearchAgent\",\n    instructions=\"You are a helpful assistant that can search the web for current information. Always provide sources for web-based answers.\",\n    tools=[\n        BingGroundingToolDefinition()\n    ],\n)\n\nprint(f\"Web search agent created: {agent.name} (ID: {agent.id})\")\n```\n\n### Interacting with Agents\n\n```python\nfrom azure.ai.agents.models import ListSortOrder\n\n# Create a conversation thread\nthread = project_client.agents.threads.create()\nprint(f\"Created thread: {thread.id}\")\n\n# Add user message\nmessage = project_client.agents.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"What's the weather in Seattle and what products do you have for rain?\"\n)\n\n# Run the agent\nrun = project_client.agents.runs.create_and_process(\n    thread_id=thread.id,\n    agent_id=agent.id\n)\n\n# Check run status\nif run.status == \"failed\":\n    print(f\"âŒ Run failed: {run.last_error}\")\nelse:\n    print(\"âœ… Run completed successfully\")\n\n# Get and display messages\nmessages = project_client.agents.messages.list(\n    thread_id=thread.id,\n    order=ListSortOrder.ASCENDING\n)\n\nfor msg in messages:\n    if msg.text_messages:\n        print(f\"\\n{msg.role.upper()}: {msg.text_messages[-1].text.value}\")\n\n# Cleanup\nproject_client.agents.delete_agent(agent.id)\nprint(\"\\nðŸ§¹ Agent deleted\")\n```\n\n## Agent Evaluation with Python SDK\n\n### Single Response Evaluation Using MCP\n\n```python\n# Query an agent and evaluate in one call\nfoundry_agents_query_and_evaluate(\n    agent_id=\"<agent-id>\",\n    query=\"What's the weather in Seattle?\",\n    endpoint=\"https://my-foundry.services.ai.azure.com/api/projects/my-project\",\n    azure_openai_endpoint=\"https://my-openai.openai.azure.com\",\n    azure_openai_deployment=\"gpt-4o\",\n    evaluators=\"intent_resolution,task_adherence,tool_call_accuracy\"\n)\n\n# Evaluate existing response\nfoundry_agents_evaluate(\n    query=\"What's the weather in Seattle?\",\n    response=\"The weather in Seattle is sunny and 22Â°C.\",\n    evaluator=\"intent_resolution\",\n    azure_openai_endpoint=\"https://my-openai.openai.azure.com\",\n    azure_openai_deployment=\"gpt-4o\"\n)\n```\n\n### Batch Evaluation\n\n```python\nimport os\nimport json\nfrom azure.ai.evaluation import AIAgentConverter, IntentResolutionEvaluator, evaluate\nfrom azure.ai.projects import AIProjectClient\nfrom azure.identity import DefaultAzureCredential\n\n# Initialize project client\nproject_client = AIProjectClient(\n    endpoint=os.environ[\"PROJECT_ENDPOINT\"],\n    credential=DefaultAzureCredential()\n)\n\n# Convert agent thread data to evaluation format\nconverter = AIAgentConverter(project_client)\n\n# Prepare evaluation data from multiple threads\nthread_ids = [\"thread-1\", \"thread-2\", \"thread-3\"]\nfilename = \"evaluation_input_data.jsonl\"\n\nevaluation_data = converter.prepare_evaluation_data(\n    thread_ids=thread_ids,\n    filename=filename\n)\n\nprint(f\"Evaluation data saved to {filename}\")\n\n# Set up evaluators\nevaluators = {\n    \"intent_resolution\": IntentResolutionEvaluator(\n        azure_openai_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n        azure_openai_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n    ),\n    # Add other evaluators as needed\n}\n\n# Run batch evaluation\nresult = evaluate(\n    data=filename,\n    evaluators=evaluators,\n    output_path=\"./evaluation_results\"\n)\n\nprint(f\"Evaluation complete. View results at: {result['studio_url']}\")\n```\n\n### Continuous Evaluation Setup\n\n```python\n# Note: Continuous evaluation setup requires configuration through \n# the Azure AI Foundry portal or using the azure-ai-evaluation SDK.\n# The evaluation rules API is configured at the project level.\n\n# Example using azure-ai-evaluation for setting up evaluators\nfrom azure.ai.evaluation import IntentResolutionEvaluator, TaskAdherenceEvaluator\n\n# Initialize evaluators for use in your evaluation pipeline\nintent_evaluator = IntentResolutionEvaluator(\n    azure_openai_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_openai_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n)\n\ntask_evaluator = TaskAdherenceEvaluator(\n    azure_openai_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_openai_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n)\n\nprint(\"Evaluators initialized for continuous evaluation\")\n```\n\n**Prerequisites for Continuous Evaluation:**\n- Project managed identity must have **Azure AI User** role\n- Application Insights must be connected to the project\n\n### Checking Evaluation Status\n\n```python\n# List evaluation runs to check status\neval_runs = project_client.evaluations.runs.list(\n    eval_id=eval_rule.id,\n    order=\"desc\",\n    limit=10\n)\n\nfor run in eval_runs.data:\n    print(f\"Run ID: {run.id}, Status: {run.status}\")\n    if run.report_url:\n        print(f\"Report: {run.report_url}\")\n```\n\n## Knowledge Index Operations\n\n### List and Inspect Indexes Using MCP\n\n```python\n# List all knowledge indexes in a project\nfoundry_knowledge_index_list(\n    endpoint=\"https://my-foundry.services.ai.azure.com/api/projects/my-project\"\n)\n\n# Get detailed schema for a specific index\nfoundry_knowledge_index_schema(\n    endpoint=\"https://my-foundry.services.ai.azure.com/api/projects/my-project\",\n    index=\"my-knowledge-index\"\n)\n```\n\n## Best Practices for Python\n\n1. **Use environment variables**: Never hardcode credentials or endpoints\n2. **Use .env files**: Leverage `python-dotenv` for local development\n3. **Proper error handling**: Always check `run.status` and handle exceptions\n4. **Context managers**: Use `with` statements for proper resource cleanup\n5. **Type hints**: Use type hints in custom functions for better tool integration\n6. **Async operations**: Consider async versions of SDK methods for better performance\n7. **Connection pooling**: Reuse `AIProjectClient` instances instead of creating new ones\n\n## Example: Complete RAG Application\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom azure.identity import DefaultAzureCredential\nfrom azure.ai.projects import AIProjectClient\nfrom azure.ai.agents.models import (\n    AzureAISearchToolDefinition,\n    AzureAISearchToolResource,\n    AISearchIndexResource,\n    AzureAISearchQueryType,\n    ListSortOrder,\n)\n\nload_dotenv()\n\ndef create_rag_agent():\n    \"\"\"Create a RAG agent with Azure AI Search.\"\"\"\n    project_client = AIProjectClient(\n        endpoint=os.environ[\"FOUNDRY_PROJECT_ENDPOINT\"],\n        credential=DefaultAzureCredential(),\n    )\n    \n    # Get Azure AI Search connection\n    azs_connection = project_client.connections.get(\n        os.environ[\"AZURE_AI_SEARCH_CONNECTION_NAME\"]\n    )\n    \n    # Create agent\n    agent = project_client.agents.create_agent(\n        model=os.environ[\"FOUNDRY_MODEL_DEPLOYMENT_NAME\"],\n        name=\"RAGAgent\",\n        instructions=\"\"\"You are a helpful assistant that uses the knowledge base \n        to answer questions. Always provide citations as: `[message_idx:search_idxâ€ source]`. \n        If you cannot find the answer, say \"I don't know\".\"\"\",\n        tools=[\n            AzureAISearchToolDefinition(\n                azure_ai_search=AzureAISearchToolResource(\n                    indexes=[\n                        AISearchIndexResource(\n                            index_connection_id=azs_connection.id,\n                            index_name=os.environ[\"AI_SEARCH_INDEX_NAME\"],\n                            query_type=AzureAISearchQueryType.HYBRID,\n                        ),\n                    ]\n                )\n            )\n        ],\n    )\n    \n    return project_client, agent\n\ndef query_agent(project_client, agent, query):\n    \"\"\"Query the agent and return response with citations.\"\"\"\n    openai_client = project_client.get_openai_client()\n    \n    stream_response = openai_client.responses.create(\n        stream=True,\n        tool_choice=\"required\",\n        input=query,\n        extra_body={\"agent\": {\"name\": agent.name, \"type\": \"agent_reference\"}},\n    )\n    \n    response_text = \"\"\n    citations = []\n    \n    for event in stream_response:\n        if event.type == \"response.output_text.delta\":\n            print(event.delta, end=\"\", flush=True)\n            response_text += event.delta\n        elif event.type == \"response.output_item.done\":\n            if event.item.type == \"message\":\n                item = event.item\n                if item.content[-1].type == \"output_text\":\n                    text_content = item.content[-1]\n                    for annotation in text_content.annotations:\n                        if annotation.type == \"url_citation\":\n                            citations.append(annotation.url)\n    \n    return response_text, citations\n\ndef main():\n    \"\"\"Main application entry point.\"\"\"\n    print(\"Creating RAG agent...\")\n    project_client, agent = create_rag_agent()\n    print(f\"âœ… Agent created: {agent.name} (ID: {agent.id})\\n\")\n    \n    while True:\n        query = input(\"\\nAsk a question (or 'quit' to exit): \")\n        if query.lower() in ['quit', 'exit', 'q']:\n            break\n        \n        print(\"\\nAgent response:\")\n        response, citations = query_agent(project_client, agent, query)\n        \n        if citations:\n            print(\"\\n\\nðŸ“Ž Citations:\")\n            for i, citation in enumerate(citations, 1):\n                print(f\"  {i}. {citation}\")\n    \n    # Cleanup\n    print(\"\\n\\nðŸ§¹ Cleaning up...\")\n    project_client.agents.delete_agent(agent.id)\n    print(\"Done!\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Common Python Patterns\n\n### Error Handling\n\n```python\nfrom azure.core.exceptions import HttpResponseError\n\ntry:\n    agent = project_client.agents.create_agent(\n        model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n        name=\"my-agent\",\n        instructions=\"You are helpful.\"\n    )\nexcept HttpResponseError as e:\n    if e.status_code == 429:\n        print(\"Rate limit exceeded. Please wait and retry.\")\n    elif e.status_code == 401:\n        print(\"Authentication failed. Check your credentials.\")\n    else:\n        print(f\"Error: {e.message}\")\n```\n\n### Retry Logic\n\n```python\nimport time\nfrom azure.core.exceptions import HttpResponseError\n\ndef create_agent_with_retry(project_client, max_retries=3):\n    \"\"\"Create agent with exponential backoff retry.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            agent = project_client.agents.create_agent(\n                model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n                name=\"my-agent\",\n                instructions=\"You are helpful.\"\n            )\n            return agent\n        except HttpResponseError as e:\n            if e.status_code == 429 and attempt < max_retries - 1:\n                wait_time = 2 ** attempt  # Exponential backoff\n                print(f\"Rate limited. Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                raise\n    \n    raise Exception(\"Failed to create agent after retries\")\n```\n\n### Context Manager for Cleanup\n\n```python\nfrom contextlib import contextmanager\n\n@contextmanager\ndef temporary_agent(project_client, **agent_kwargs):\n    \"\"\"Context manager for temporary agents that auto-cleanup.\"\"\"\n    agent = project_client.agents.create_agent(**agent_kwargs)\n    try:\n        yield agent\n    finally:\n        project_client.agents.delete_agent(agent.id)\n        print(f\"Agent {agent.id} cleaned up\")\n\n# Usage\nwith temporary_agent(\n    project_client,\n    model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n    name=\"temp-agent\",\n    instructions=\"You are helpful.\"\n) as agent:\n    # Use the agent\n    thread = project_client.agents.threads.create()\n    # ... do work ...\n# Agent is automatically deleted when exiting the with block\n```\n"
      },
      "plugins": [
        {
          "name": "azure",
          "description": "Microsoft Azure MCP integration for cloud resource management, deployments, and Azure services. Manage your Azure infrastructure, monitor applications, and deploy resources directly from Claude Code.",
          "source": "./plugin",
          "homepage": "https://github.com/microsoft/github-copilot-for-azure",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add microsoft/GitHub-Copilot-for-Azure",
            "/plugin install azure@github-copilot-for-azure"
          ]
        }
      ]
    }
  ]
}