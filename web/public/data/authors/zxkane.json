{
  "author": {
    "id": "zxkane",
    "display_name": "Mengxin Zhu",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/843303?v=4",
    "url": "https://github.com/zxkane",
    "bio": "Open source mania.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 5,
      "total_commands": 0,
      "total_skills": 5,
      "total_stars": 108,
      "total_forks": 17
    }
  },
  "marketplaces": [
    {
      "name": "aws-skills",
      "version": null,
      "description": "AWS development skills for Claude Code including CDK, serverless architecture, cost optimization, and Bedrock AgentCore for AI agent deployment",
      "owner_info": {
        "name": "Kane Zhu",
        "email": "me@kane.mx"
      },
      "keywords": [],
      "repo_full_name": "zxkane/aws-skills",
      "repo_url": "https://github.com/zxkane/aws-skills",
      "repo_description": "Claude Agent Skills for AWS",
      "homepage": "",
      "signals": {
        "stars": 108,
        "forks": 17,
        "pushed_at": "2026-01-16T07:01:28Z",
        "created_at": "2025-10-21T03:56:34Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 4981
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/SKILL.md",
          "type": "blob",
          "size": 6633
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/cross-service",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/cross-service/credential-management.md",
          "type": "blob",
          "size": 13844
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/browser",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/browser/README.md",
          "type": "blob",
          "size": 10378
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/code-interpreter",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/code-interpreter/README.md",
          "type": "blob",
          "size": 7792
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/gateway",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/gateway/README.md",
          "type": "blob",
          "size": 3379
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/gateway/deployment-strategies.md",
          "type": "blob",
          "size": 11311
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/gateway/troubleshooting-guide.md",
          "type": "blob",
          "size": 10342
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/identity",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/identity/README.md",
          "type": "blob",
          "size": 6868
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/memory",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/memory/README.md",
          "type": "blob",
          "size": 8309
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/observability",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/observability/README.md",
          "type": "blob",
          "size": 14145
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/runtime",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/runtime/README.md",
          "type": "blob",
          "size": 6815
        },
        {
          "path": "plugins/aws-cdk",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-cdk/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-cdk/skills/aws-cdk-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-cdk/skills/aws-cdk-development/SKILL.md",
          "type": "blob",
          "size": 10353
        },
        {
          "path": "plugins/aws-cdk/skills/aws-cdk-development/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-cdk/skills/aws-cdk-development/references/cdk-patterns.md",
          "type": "blob",
          "size": 9852
        },
        {
          "path": "plugins/aws-common",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-common/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-common/skills/aws-mcp-setup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-common/skills/aws-mcp-setup/SKILL.md",
          "type": "blob",
          "size": 4786
        },
        {
          "path": "plugins/aws-cost-ops",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-cost-ops/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-cost-ops/skills/aws-cost-operations",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-cost-ops/skills/aws-cost-operations/SKILL.md",
          "type": "blob",
          "size": 11165
        },
        {
          "path": "plugins/aws-cost-ops/skills/aws-cost-operations/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/aws-cost-ops/skills/aws-cost-operations/references/cloudwatch-alarms.md",
          "type": "blob",
          "size": 13736
        },
        {
          "path": "plugins/aws-cost-ops/skills/aws-cost-operations/references/operations-patterns.md",
          "type": "blob",
          "size": 10477
        },
        {
          "path": "plugins/serverless-eda",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/serverless-eda/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/serverless-eda/skills/aws-serverless-eda",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/serverless-eda/skills/aws-serverless-eda/SKILL.md",
          "type": "blob",
          "size": 21806
        },
        {
          "path": "plugins/serverless-eda/skills/aws-serverless-eda/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/serverless-eda/skills/aws-serverless-eda/references/deployment-best-practices.md",
          "type": "blob",
          "size": 19664
        },
        {
          "path": "plugins/serverless-eda/skills/aws-serverless-eda/references/eda-patterns.md",
          "type": "blob",
          "size": 24678
        },
        {
          "path": "plugins/serverless-eda/skills/aws-serverless-eda/references/observability-best-practices.md",
          "type": "blob",
          "size": 19049
        },
        {
          "path": "plugins/serverless-eda/skills/aws-serverless-eda/references/performance-optimization.md",
          "type": "blob",
          "size": 16776
        },
        {
          "path": "plugins/serverless-eda/skills/aws-serverless-eda/references/security-best-practices.md",
          "type": "blob",
          "size": 15257
        },
        {
          "path": "plugins/serverless-eda/skills/aws-serverless-eda/references/serverless-patterns.md",
          "type": "blob",
          "size": 20841
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"aws-skills\",\n  \"owner\": {\n    \"name\": \"Kane Zhu\",\n    \"email\": \"me@kane.mx\"\n  },\n  \"metadata\": {\n    \"description\": \"AWS development skills for Claude Code including CDK, serverless architecture, cost optimization, and Bedrock AgentCore for AI agent deployment\",\n    \"version\": \"2.2.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"aws-common\",\n      \"description\": \"Shared AWS agent skills including AWS Documentation MCP configuration for querying up-to-date AWS knowledge\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Kane Zhu\",\n        \"email\": \"me@kane.mx\"\n      },\n      \"homepage\": \"https://github.com/zxkane/aws-skills\",\n      \"repository\": \"https://github.com/zxkane/aws-skills\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"aws\", \"mcp\", \"documentation\", \"common\", \"shared\"],\n      \"category\": \"development\",\n      \"source\": \"./plugins/aws-common\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/aws-mcp-setup\"\n      ]\n    },\n    {\n      \"name\": \"aws-cdk\",\n      \"description\": \"Comprehensive AWS development skills including CDK best practices, Lambda development workflows, and AWS documentation search capabilities\",\n      \"version\": \"1.2.0\",\n      \"author\": {\n        \"name\": \"Kane Zhu\",\n        \"email\": \"me@kane.mx\"\n      },\n      \"homepage\": \"https://github.com/zxkane/aws-skills\",\n      \"repository\": \"https://github.com/zxkane/aws-skills\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"aws\", \"cdk\", \"lambda\", \"infrastructure\", \"cloud\", \"skills\"],\n      \"category\": \"development\",\n      \"source\": \"./plugins/aws-cdk\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/aws-cdk-development\"\n      ],\n      \"mcpServers\": {\n        \"cdk\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\n            \"awslabs.cdk-mcp-server@latest\"\n          ],\n          \"env\": {\n            \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"aws-cost-ops\",\n      \"description\": \"AWS cost optimization, monitoring, and operational excellence with integrated MCP servers for billing, cost analysis, observability, and security assessment\",\n      \"version\": \"1.2.0\",\n      \"author\": {\n        \"name\": \"Kane Zhu\",\n        \"email\": \"me@kane.mx\"\n      },\n      \"homepage\": \"https://github.com/zxkane/aws-skills\",\n      \"repository\": \"https://github.com/zxkane/aws-skills\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"aws\", \"cost\", \"billing\", \"monitoring\", \"cloudwatch\", \"operations\", \"observability\"],\n      \"category\": \"operations\",\n      \"source\": \"./plugins/aws-cost-ops\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/aws-cost-operations\"\n      ],\n      \"mcpServers\": {\n        \"pricing\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\n            \"awslabs.aws-pricing-mcp-server@latest\"\n          ],\n          \"env\": {\n            \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n          }\n        },\n        \"costexp\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\n            \"awslabs.cost-explorer-mcp-server@latest\"\n          ],\n          \"env\": {\n            \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n          }\n        },\n        \"cw\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\n            \"awslabs.cloudwatch-mcp-server@latest\"\n          ],\n          \"env\": {\n            \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n          }\n        }\n      }\n    },\n    {\n      \"name\": \"serverless-eda\",\n      \"description\": \"AWS serverless and event-driven architecture best practices based on Well-Architected Framework with MCP servers for SAM, Lambda, Step Functions, and messaging\",\n      \"version\": \"1.2.0\",\n      \"author\": {\n        \"name\": \"Kane Zhu\",\n        \"email\": \"me@kane.mx\"\n      },\n      \"homepage\": \"https://github.com/zxkane/aws-skills\",\n      \"repository\": \"https://github.com/zxkane/aws-skills\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"aws\", \"serverless\", \"lambda\", \"event-driven\", \"eda\", \"step-functions\", \"eventbridge\", \"sqs\", \"sns\"],\n      \"category\": \"development\",\n      \"source\": \"./plugins/serverless-eda\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/aws-serverless-eda\"\n      ]\n    },\n    {\n      \"name\": \"aws-agentic-ai\",\n      \"description\": \"AWS Bedrock AgentCore comprehensive expert for deploying and managing all AgentCore services including Gateway, Runtime, Memory, Identity, Code Interpreter, Browser, and Observability\",\n      \"version\": \"1.2.0\",\n      \"author\": {\n        \"name\": \"Kane Zhu\",\n        \"email\": \"me@kane.mx\"\n      },\n      \"homepage\": \"https://github.com/zxkane/aws-skills\",\n      \"repository\": \"https://github.com/zxkane/aws-skills\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"aws\", \"bedrock\", \"agentcore\", \"ai-agents\", \"mcp\", \"gateway\", \"runtime\", \"memory\", \"identity\"],\n      \"category\": \"development\",\n      \"source\": \"./plugins/aws-agentic-ai\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/aws-agentic-ai\"\n      ]\n    }\n  ]\n}\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/SKILL.md": "---\nname: aws-agentic-ai\naliases:\n  - bedrock-agentcore\n  - aws-agentic-ai\ndescription: AWS Bedrock AgentCore comprehensive expert for deploying and managing all AgentCore services. Use when working with Gateway, Runtime, Memory, Identity, or any AgentCore component. Covers MCP target deployment, credential management, schema optimization, runtime configuration, memory management, and identity services.\ncontext: fork\nmodel: sonnet\nskills:\n  - aws-mcp-setup\nallowed-tools:\n  - mcp__aws-mcp__*\n  - mcp__awsdocs__*\n  - Bash(aws bedrock-agentcore-control *)\n  - Bash(aws bedrock-agentcore-runtime *)\n  - Bash(aws bedrock *)\n  - Bash(aws s3 cp *)\n  - Bash(aws s3 ls *)\n  - Bash(aws secretsmanager *)\n  - Bash(aws sts get-caller-identity)\nhooks:\n  PreToolUse:\n    - matcher: Bash(aws bedrock-agentcore-control create-*)\n      command: aws sts get-caller-identity --query Account --output text\n      once: true\n---\n\n# AWS Bedrock AgentCore\n\nAWS Bedrock AgentCore provides a complete platform for deploying and scaling AI agents with seven core services. This skill guides you through service selection, deployment patterns, and integration workflows using AWS CLI.\n\n## AWS Documentation Requirement\n\n**CRITICAL**: This skill requires AWS MCP tools for accurate, up-to-date AWS information.\n\n### Before Answering AWS Questions\n\n1. **Always verify** using AWS MCP tools (if available):\n   - `mcp__aws-mcp__aws___search_documentation` or `mcp__*awsdocs*__aws___search_documentation` - Search AWS docs\n   - `mcp__aws-mcp__aws___read_documentation` or `mcp__*awsdocs*__aws___read_documentation` - Read specific pages\n   - `mcp__aws-mcp__aws___get_regional_availability` - Check service availability\n\n2. **If AWS MCP tools are unavailable**:\n   - Guide user to configure AWS MCP using the `aws-mcp-setup` skill (auto-loaded as dependency)\n   - Help determine which option fits their environment:\n     - Has uvx + AWS credentials → Full AWS MCP Server\n     - No Python/credentials → AWS Documentation MCP (no auth)\n   - If cannot determine → Ask user which option to use\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- Deploy REST APIs as MCP tools for AI agents (Gateway)\n- Execute agents in serverless runtime (Runtime)\n- Add conversation memory to agents (Memory)\n- Manage API credentials and authentication (Identity)\n- Enable agents to execute code securely (Code Interpreter)\n- Allow agents to interact with websites (Browser)\n- Monitor and trace agent performance (Observability)\n\n## Available Services\n\n| Service | Use For | Documentation |\n|---------|---------|---------------|\n| **Gateway** | Converting REST APIs to MCP tools | [`services/gateway/README.md`](services/gateway/README.md) |\n| **Runtime** | Deploying and scaling agents | [`services/runtime/README.md`](services/runtime/README.md) |\n| **Memory** | Managing conversation state | [`services/memory/README.md`](services/memory/README.md) |\n| **Identity** | Credential and access management | [`services/identity/README.md`](services/identity/README.md) |\n| **Code Interpreter** | Secure code execution in sandboxes | [`services/code-interpreter/README.md`](services/code-interpreter/README.md) |\n| **Browser** | Web automation and scraping | [`services/browser/README.md`](services/browser/README.md) |\n| **Observability** | Tracing and monitoring | [`services/observability/README.md`](services/observability/README.md) |\n\n## Common Workflows\n\n### Deploying a Gateway Target\n\n**MANDATORY - READ DETAILED DOCUMENTATION**: See [`services/gateway/README.md`](services/gateway/README.md) for complete Gateway setup guide including deployment strategies, troubleshooting, and IAM configuration.\n\n**Quick Workflow**:\n1. Upload OpenAPI schema to S3\n2. *(API Key auth only)* Create credential provider and store API key\n3. Create gateway target linking schema (and credentials if using API key)\n4. Verify target status and test connectivity\n\n> **Note**: Credential provider is only needed for API key authentication. Lambda targets use IAM roles, and MCP servers use OAuth.\n\n### Managing Credentials\n\n**MANDATORY - READ DETAILED DOCUMENTATION**: See [`cross-service/credential-management.md`](cross-service/credential-management.md) for unified credential management patterns across all services.\n\n**Quick Workflow**:\n1. Use Identity service credential providers for all API keys\n2. Link providers to gateway targets via ARN references\n3. Rotate credentials quarterly through credential provider updates\n4. Monitor usage with CloudWatch metrics\n\n### Monitoring Agents\n\n**MANDATORY - READ DETAILED DOCUMENTATION**: See [`services/observability/README.md`](services/observability/README.md) for comprehensive monitoring setup.\n\n**Quick Workflow**:\n1. Enable observability for agents\n2. Configure CloudWatch dashboards for metrics\n3. Set up alarms for error rates and latency\n4. Use X-Ray for distributed tracing\n\n## Service-Specific Documentation\n\nFor detailed documentation on each AgentCore service, see the following resources:\n\n### Gateway Service\n- **Overview**: [`services/gateway/README.md`](services/gateway/README.md)\n- **Deployment Strategies**: [`services/gateway/deployment-strategies.md`](services/gateway/deployment-strategies.md)\n- **Troubleshooting**: [`services/gateway/troubleshooting-guide.md`](services/gateway/troubleshooting-guide.md)\n\n### Runtime, Memory, Identity, Code Interpreter, Browser, Observability\nEach service has comprehensive documentation in its respective directory:\n- [`services/runtime/README.md`](services/runtime/README.md)\n- [`services/memory/README.md`](services/memory/README.md)\n- [`services/identity/README.md`](services/identity/README.md)\n- [`services/code-interpreter/README.md`](services/code-interpreter/README.md)\n- [`services/browser/README.md`](services/browser/README.md)\n- [`services/observability/README.md`](services/observability/README.md)\n\n## Cross-Service Resources\n\nFor patterns and best practices that span multiple AgentCore services:\n\n- **Credential Management**: [`cross-service/credential-management.md`](cross-service/credential-management.md) - Unified credential patterns, security practices, rotation procedures\n\n## Additional Resources\n\n- **AWS Documentation**: [Amazon Bedrock AgentCore](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html)\n- **API Reference**: [Bedrock AgentCore Control Plane API](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/)\n- **AWS CLI Reference**: [bedrock-agentcore-control commands](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/bedrock-agentcore-control/index.html)\n\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/cross-service/credential-management.md": "# Cross-Service Credential Management\n\n**Applies to**: Gateway, Runtime, Memory, Identity\n\n## Overview\n\nCredential management is a cross-cutting concern across all AgentCore services. This guide provides unified patterns for managing API keys, tokens, and authentication credentials across the AgentCore platform.\n\n## Authentication Overview\n\n| Service | Direction | Supported Methods | Use Case |\n|---------|-----------|-------------------|----------|\n| **Gateway** | Inbound | IAM, JWT, No Auth | Who can invoke MCP tools |\n| **Gateway** | Outbound | IAM, OAuth (2LO/3LO), API Key | Accessing external APIs |\n| **Runtime** | Inbound | IAM (SigV4), JWT | Who can invoke agents |\n| **Runtime** | Outbound | OAuth, API Key | Accessing third-party services |\n| **Memory** | - | IAM Role | Data access permissions |\n| **Identity** | - | AWS KMS | Secret encryption |\n\n### Inbound Authorization (Who Can Access Your Services)\n\n**Gateway Options** ([docs](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-inbound-auth.html)):\n- **IAM Identity**: Uses AWS IAM credentials for authorization\n- **JWT**: Tokens from identity providers (Cognito, Microsoft Entra ID, etc.)\n- **No Authorization**: Open access - only for production with proper security controls\n\n**Runtime Options** ([docs](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-oauth.html)):\n- **IAM (SigV4)**: Default authentication (works automatically)\n- **JWT Bearer Token**: Token-based auth with discovery URL and audience validation\n\n> **Note**: A Runtime can only use one inbound auth type (IAM or JWT), not both simultaneously.\n\n### Outbound Authorization (Accessing External Services)\n\n**Gateway Options** ([docs](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-outbound-auth.html)):\n\n| Target Type | IAM (Service Role) | OAuth 2LO | OAuth 3LO | API Key |\n|-------------|-------------------|-----------|-----------|---------|\n| Lambda function | ✅ | ❌ | ❌ | ❌ |\n| API Gateway | ✅ | ❌ | ❌ | ✅ |\n| OpenAPI schema | ❌ | ✅ | ✅ | ✅ |\n| Smithy schema | ✅ | ✅ | ✅ | ❌ |\n| MCP server | ❌ | ✅ | ❌ | ❌ |\n\n- **OAuth 2LO**: Client credentials grant (machine-to-machine)\n- **OAuth 3LO**: Authorization code grant (user-delegated access)\n\n**Runtime Options**:\n- **OAuth**: Tokens on behalf of users via Identity Service\n- **API Key**: Key-based authentication via Identity Service\n\n## Best Practices\n\n### ✅ DO's\n\n1. **Use Identity Service**: Always manage credentials through the Identity service\n   ```bash\n   # ✅ Correct - Use Identity API\n   aws bedrock-agentcore-control create-api-key-credential-provider \\\n     --name MyCredentialProvider \\\n     --api-key \"YOUR_API_KEY_VALUE\"\n   ```\n\n2. **Separate by Environment**: Use different providers for different environments\n   ```bash\n   - dev-api-key-provider      # Development\n   - staging-api-key-provider  # Staging\n   - prod-api-key-provider     # Production\n   ```\n\n3. **Rotate Regularly**: Implement quarterly credential rotation\n   ```bash\n   aws bedrock-agentcore-control update-api-key-credential-provider \\\n     --name MyCredentialProvider \\\n     --api-key \"NEW_API_KEY\"\n   ```\n\n4. **Least Privilege**: Grant minimal required permissions to each credential\n   ```bash\n   # API key should only have necessary API permissions\n   # IAM roles should have scoped-down policies\n   ```\n\n5. **Monitor Usage**: Track credential usage and set up alerts\n   ```json\n   {\n     \"CloudWatch Alarms\": {\n       \"HighErrorRate\": \"Alert if > 10% failed requests\",\n       \"UnusualActivity\": \"Alert on usage spikes\"\n     }\n   }\n   ```\n\n### ❌ DON'Ts\n\n1. **Never Hardcode**: Don't embed credentials in code or configuration files\n   ```bash\n   # ❌ Bad - Hardcoded API key\n   const apiKey = \"sk-1234567890abcdef\"\n\n   # ✅ Good - Reference credential provider\n   const credentialProvider = \"MyCredentialProvider\"\n   ```\n\n2. **Don't Share Across Environments**: Avoid using production keys in development\n   ```bash\n   # ❌ Bad - Same key everywhere\n   dev:  third-party-api-key: prod-key\n   prod: third-party-api-key: prod-key\n\n   # ✅ Good - Separate keys\n   dev:  third-party-api-key: dev-key\n   prod: third-party-api-key: prod-key\n   ```\n\n3. **Don't Commit to Git**: Exclude credential files from version control\n   ```bash\n   # .gitignore\n   *.env\n   *.secret\n   credential-*.json\n   ```\n\n4. **Don't Use Long-Lived Tokens**: Implement token refresh for OAuth\n   ```bash\n   # OAuth tokens should auto-refresh\n   # Don't use tokens with > 30 day expiration\n   ```\n\n## Multi-Service Credential Patterns\n\n### Pattern 1: Centralized Identity, Distributed Usage\n\n```\n┌─────────────────────────────────────┐\n│  Identity Service                   │\n│  - Stores ALL credentials           │\n│  - Manages rotation                 │\n│  - Provides audit logs              │\n└──────────┬──────────────────────────┘\n           │\n           ├────────────┬────────────┬────────────┐\n           ▼            ▼            ▼            ▼\n    ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐\n    │ Gateway  │ │ Runtime  │ │  Memory  │ │  Other   │\n    │  Uses    │ │  Uses    │ │  Uses    │ │  Uses    │\n    └──────────┘ └──────────┘ └──────────┘ └──────────┘\n```\n\n**Benefits**:\n- Single source of truth for all credentials\n- Unified rotation and audit\n- Consistent access patterns\n\n**Setup**:\n```bash\n# 1. Create master credential in Identity\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name MasterAPICredentials \\\n  --api-key \"YOUR_MASTER_API_KEY\"\n\n# 2. Grant access to each service\n# - Gateway: can read MasterAPICredentials\n# - Runtime: can read MasterAPICredentials\n# - Memory: can read MasterAPICredentials\n```\n\n### Pattern 2: Service-Specific Credentials\n\n```\n┌─────────────────────────────────────┐\n│  Identity Service                   │\n│  - Stores credentials per service   │\n└──────────┬──────────────────────────┘\n           │\n    ┌──────┴──────┬────────┬─────────┐\n    ▼             ▼        ▼         ▼\n┌─────────┐ ┌─────────┐ ┌──────┐ ┌─────┐\n│ Gateway │ │ Runtime │ │Memory││Other│\n│  Cred   │ │  Cred   │ │ Cred ││Cred │\n└─────────┘ └─────────┘ └──────┘ └─────┘\n```\n\n**Benefits**:\n- Isolation between services\n- Independent rotation per service\n- Service-specific permissions\n\n**Setup**:\n```bash\n# Create separate providers\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name GatewayAPICredentials \\\n  --api-key \"YOUR_GATEWAY_API_KEY\"\n\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name RuntimeCredentials \\\n  --api-key \"YOUR_RUNTIME_API_KEY\"\n```\n\n### Pattern 3: Tiered (Master + Service)\n\n```\n┌─────────────────────────────────────┐\n│  Identity Service                   │\n│  - Master credential                │\n│  - Per-service credentials          │\n└──────────┬──────────────────────────┘\n           │\n    ┌──────┴──────┐\n    ▼             ▼\n┌─────────┐ ┌─────────────┐\n│ Master  │ │   Services  │\n│  Cred   │ │   - Gateway │ │\n└────┬────┘ │   - Runtime │\n     │      │   - Memory  │\n     └──────┤   (each has │\n            │ own creds)  │\n            └─────────────┘\n```\n\n**Use Cases**:\n- Production: Master credential for critical APIs\n- Development: Service-specific credentials for testing\n- Emergency: Master credential as backup\n\n## Security Best Practices\n\n### Encryption\n\n```bash\n# Use KMS for secret encryption\naws secretsmanager create-secret \\\n  --name MySecret \\\n  --kms-key-id arn:aws:kms:us-west-2:123456789012:key/12345678-abcd-ef12-3456-7890abcdef12 \\\n  --secret-string \"my-secret-value\"\n```\n\n### Access Control\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"bedrock-agentcore:GetResourceApiKey\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:PrincipalTag/Service\": \"gateway\"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Audit Logging\n\n```bash\n# Enable CloudTrail for Bedrock AgentCore\naws cloudtrail create-trail \\\n  --name agentcore-audit \\\n  --s3-bucket-name agentcore-audit-logs \\\n  --include-global-service-events true\n```\n\n## Rotation Strategy\n\n### Automated Rotation\n\n```bash\n# Enable automatic rotation (when supported)\naws secretsmanager rotate-secret \\\n  --secret-id MySecret \\\n  --lambda-arn arn:aws:lambda:us-west-2:123456789012:function:MyRotationFunction\n\n# Rotation schedule (every 30 days)\naws secretsmanager rotate-secret \\\n  --secret-id MySecret \\\n  --rotation-rules AutomaticAfterDays=30\n```\n\n### Manual Rotation Process\n\n```bash\n#!/bin/bash\n# rotate-credentials.sh\n\necho \"Step 1: Generate new credential\"\nNEW_KEY=$(generate-new-api-key)\n\necho \"Step 2: Update in Identity service\"\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name MyCredentialProvider \\\n  --api-key \"$NEW_KEY\"\n\necho \"Step 3: Verify all services work\"\n./test-all-services.sh\n\necho \"Step 4: Delete old credential\"\n# Old credential is automatically deprecated\n```\n\n## Common Patterns\n\n### Pattern: Credential Fallback\n\n```typescript\n// Try primary credential, fallback to backup\nasync function callWithFallback(provider: string) {\n  try {\n    return await callAPI(provider);\n  } catch (error) {\n    if (error.code === 'InvalidAPICredentials') {\n      // Fallback to backup provider\n      return await callAPI(`${provider}-backup`);\n    }\n    throw error;\n  }\n}\n```\n\n### Pattern: Rate Limiting with Credential Pool\n\n```typescript\n// Rotate through multiple credentials to avoid rate limits\nconst credentialPool = [\n  'cred-1',\n  'cred-2',\n  'cred-3'\n];\n\nlet currentIndex = 0;\n\nfunction getNextCredential(): string {\n  const credential = credentialPool[currentIndex];\n  currentIndex = (currentIndex + 1) % credentialPool.length;\n  return credential;\n}\n```\n\n## Troubleshooting Credential Issues\n\n### Issue: \"Credential not found\"\n\n**Diagnosis**:\n```bash\n# Check if provider exists\naws bedrock-agentcore-control get-api-key-credential-provider \\\n  --name MyCredentialProvider\n\n# Check IAM permissions\naws iam simulate-principal-policy \\\n  --policy-source-arn arn:aws:iam::123456789012:role/MyRole \\\n  --action-names bedrock-agentcore:GetResourceApiKey \\\n  --resource-arns arn:aws:bedrock-agentcore:us-west-2:123456789012:*\n```\n\n**Solution**: Create provider or grant IAM permissions\n\n---\n\n### Issue: \"Invalid credentials\" after rotation\n\n**Diagnosis**:\n```bash\n# Check secret value format\naws secretsmanager get-secret-value \\\n  --secret-id arn:aws:secretsmanager:us-west-2:123456789012:secret:MySecret\n\n# Should be: {\"apiKey\": \"valid-key\"}\n```\n\n**Solution**: Use correct update API\n```bash\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name MyCredentialProvider \\\n  --api-key \"VALID_KEY\"\n```\n\n---\n\n### Issue: Cross-service access denied\n\n**Diagnosis**:\n```bash\n# Check which services can access the credential\naws bedrock-agentcore-control get-api-key-credential-provider \\\n  --name MyCredentialProvider\n\n# Review service IAM policies\n```\n\n**Solution**: Add cross-service access policy\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"bedrock-agentcore:GetResourceApiKey\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"ArnLike\": {\n          \"aws:PrincipalArn\": [\n            \"arn:aws:iam::*:role/*gateway*\",\n            \"arn:aws:iam::*:role/*runtime*\"\n          ]\n        }\n      }\n    }\n  ]\n}\n```\n\n## Cost Considerations\n\n### Secrets Manager Costs\n\n- **Per secret**: ~$0.40/month\n- **Per 10,000 API calls**: ~$0.05\n- **Cross-region replication**: Additional costs\n\n**Optimization**:\n- Share credentials across services when possible\n- Use regional replication only when necessary\n- Cache credential retrieval (respect security requirements)\n\n### Identity Service Costs\n\n- **Credential provider storage**: Included in Secrets Manager\n- **API calls**: Same as Secrets Manager pricing\n- **Cross-account access**: No additional cost\n\n## References\n\n- **[Identity Service](../services/identity/README.md)**: Credential provider management\n- **[Gateway Service](../services/gateway/README.md)**: Uses credentials for API authentication\n- **AWS Secrets Manager**: [Pricing](https://aws.amazon.com/secrets-manager/pricing/)\n- **AWS Documentation**: [Managing AWS Secrets Manager secrets](https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_create-basic-secret.html)\n\n---\n\n**Related Guides**:\n- [Observability Service](../services/observability/README.md)\n- [AWS AgentCore Identity Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity.html)\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/browser/README.md": "# AgentCore Browser Service\n\n> **Status**: ✅ Available\n\n## Overview\n\nAmazon Bedrock AgentCore Browser provides a fast, secure, cloud-based browser runtime enabling AI agents to interact with websites at scale without infrastructure management.\n\n## Core Capabilities\n\n### Cloud-Based Runtime\n- **Fast Execution**: High-performance browser instances with minimal latency\n- **Auto Scaling**: Automatic scaling based on demand without configuration\n- **Zero Infrastructure**: No servers or containers to manage\n- **Multi-Region**: Deploy browser instances across AWS regions globally\n\n### Security and Compliance\n- **Enterprise Security**: Industry-standard security controls and encryption\n- **Isolated Sessions**: Each browsing session runs in complete isolation\n- **Data Protection**: Secure data handling and privacy controls\n- **Compliance Ready**: Meets enterprise compliance requirements (SOC, HIPAA, GDPR)\n\n### Web Interaction Capabilities\n- **Full Automation**: Complete browser automation capabilities\n- **JavaScript Support**: Execute JavaScript in browser context\n- **Form Interaction**: Fill forms, click buttons, navigate pages\n- **Content Extraction**: Extract text, data, and media from pages\n- **Session Management**: Handle cookies, local storage, and sessions\n- **Screenshot Capture**: Take screenshots of pages and elements\n\n### Observability\n- **Execution Logging**: Comprehensive logs of browser actions\n- **Performance Metrics**: Track page load times and operation latency\n- **Error Tracking**: Detailed error capture and debugging information\n- **Request Monitoring**: Monitor network requests and responses\n\n## Use Cases\n\n### Web Scraping and Data Extraction\nEnable agents to:\n- Extract data from websites at scale\n- Scrape content from dynamic pages\n- Collect structured data from multiple sources\n- Monitor website changes over time\n\n### Automated Testing and QA\nSupport scenarios like:\n- Automated UI testing of web applications\n- Regression testing for web features\n- Cross-browser compatibility testing\n- Performance testing and monitoring\n\n### Form Filling and Workflow Automation\nAllow agents to:\n- Automate form submissions\n- Complete multi-step workflows\n- Handle authentication and logins\n- Process batch operations on web interfaces\n\n### Real-Time Monitoring\nEnable agents to:\n- Monitor website availability and uptime\n- Track content changes and updates\n- Verify website functionality\n- Gather competitive intelligence\n\n### Content Verification\nSupport tasks like:\n- Validate web content accuracy\n- Check link integrity\n- Verify page rendering\n- Test responsive designs\n\n## Architecture\n\n### Browser Execution Flow\n\n```\nAgent Request\n    ↓\n┌─────────────────────────────────────────┐\n│  Browser Service API                    │\n│  - Parse browser action request         │\n│  - Validate parameters                  │\n│  - Allocate browser instance            │\n└─────────────────────────────────────────┘\n    ↓\n┌─────────────────────────────────────────┐\n│  Browser Instance                       │\n│  - Navigate to URL                      │\n│  - Execute JavaScript                   │\n│  - Interact with page elements          │\n│  - Extract content and data             │\n└─────────────────────────────────────────┘\n    ↓\n┌─────────────────────────────────────────┐\n│  Result Processing                      │\n│  - Package extracted data               │\n│  - Capture screenshots (if requested)   │\n│  - Log execution details                │\n│  - Return results to agent              │\n└─────────────────────────────────────────┘\n```\n\n### Security Architecture\n\n1. **Session Isolation**: Each browser session runs in isolated environment\n2. **Network Security**: Controlled outbound internet access\n3. **Data Encryption**: All data encrypted in transit and at rest\n4. **Resource Limits**: CPU, memory, and time limits per session\n5. **Access Control**: IAM-based authentication and authorization\n\n## Configuration\n\n### Basic Browser Session\n\n```bash\n# Configure browser service for agent\naws bedrock-agentcore-control configure-browser \\\n  --agent-id <AGENT_ID> \\\n  --session-timeout 600 \\\n  --viewport-width 1920 \\\n  --viewport-height 1080 \\\n  --region <REGION>\n```\n\n### Advanced Configuration\n\n```bash\n# Set browser preferences and capabilities\naws bedrock-agentcore-control update-browser-config \\\n  --agent-id <AGENT_ID> \\\n  --browser-config '{\n    \"headless\": true,\n    \"javascript\": true,\n    \"images\": true,\n    \"cookies\": true,\n    \"userAgent\": \"CustomUserAgent/1.0\",\n    \"timeout\": 30000\n  }' \\\n  --region <REGION>\n```\n\n## Browser Actions\n\n### Navigation\n```javascript\n// Navigate to URL\n{\n  \"action\": \"navigate\",\n  \"url\": \"https://example.com\"\n}\n\n// Go back\n{\n  \"action\": \"goBack\"\n}\n\n// Refresh page\n{\n  \"action\": \"reload\"\n}\n```\n\n### Element Interaction\n```javascript\n// Click element\n{\n  \"action\": \"click\",\n  \"selector\": \"#submit-button\"\n}\n\n// Fill input field\n{\n  \"action\": \"type\",\n  \"selector\": \"#username\",\n  \"text\": \"user@example.com\"\n}\n\n// Select option\n{\n  \"action\": \"select\",\n  \"selector\": \"#country\",\n  \"value\": \"US\"\n}\n```\n\n### Content Extraction\n```javascript\n// Extract text\n{\n  \"action\": \"getText\",\n  \"selector\": \".article-content\"\n}\n\n// Get element attribute\n{\n  \"action\": \"getAttribute\",\n  \"selector\": \"img.logo\",\n  \"attribute\": \"src\"\n}\n\n// Evaluate JavaScript\n{\n  \"action\": \"evaluate\",\n  \"script\": \"return document.title;\"\n}\n```\n\n### Screenshots\n```javascript\n// Full page screenshot\n{\n  \"action\": \"screenshot\",\n  \"fullPage\": true\n}\n\n// Element screenshot\n{\n  \"action\": \"screenshot\",\n  \"selector\": \"#chart-container\"\n}\n```\n\n## Best Practices\n\n### Performance Optimization\n- Use headless mode for non-visual operations\n- Disable unnecessary resources (images, stylesheets)\n- Set appropriate timeouts for page loads\n- Reuse browser sessions when possible\n- Implement exponential backoff for retries\n\n### Reliability\n- Handle network failures gracefully\n- Implement proper error handling\n- Use explicit waits for dynamic content\n- Verify element existence before interaction\n- Set reasonable timeout values\n\n### Security\n- Validate all URLs before navigation\n- Sanitize extracted data\n- Use secure credential storage\n- Implement rate limiting\n- Monitor for suspicious patterns\n\n### Cost Optimization\n- Close browser sessions when done\n- Use session pooling for frequent operations\n- Set appropriate resource limits\n- Monitor usage patterns\n- Implement caching where appropriate\n\n## Integration Patterns\n\n### With Memory Service\n```\nBrowser ←→ Memory Service\n- Store extracted data in memory\n- Cache frequently accessed pages\n- Share session state across agents\n```\n\n### With Identity Service\n```\nBrowser ←→ Identity Service\n- Authenticate browser sessions\n- Access credentials for protected sites\n- Manage authentication tokens\n```\n\n### With Code Interpreter\n```\nBrowser ←→ Code Interpreter\n- Process scraped data with code\n- Transform extracted content\n- Analyze website data\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**Page Load Timeout**\n- Symptom: Page takes too long to load\n- Solution: Increase timeout or optimize target page\n\n**Element Not Found**\n- Symptom: Cannot locate page element\n- Solution: Use explicit waits or verify selector\n\n**JavaScript Errors**\n- Symptom: Page JavaScript fails\n- Solution: Check console logs, handle errors\n\n**Session Terminated**\n- Symptom: Browser session unexpectedly ends\n- Solution: Check resource limits and session timeout\n\n**Authentication Required**\n- Symptom: Cannot access protected pages\n- Solution: Configure credentials via Identity service\n\n## Monitoring\n\n### Key Metrics\n- **Session Count**: Number of active browser sessions\n- **Success Rate**: Percentage of successful operations\n- **Page Load Time**: Average time to load pages\n- **Error Rate**: Percentage of failed operations\n- **Resource Usage**: CPU and memory utilization\n\n### CloudWatch Integration\n```bash\n# Query browser metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/BedrockAgentCore/Browser \\\n  --metric-name SessionCount \\\n  --dimensions Name=AgentId,Value=<AGENT_ID> \\\n  --start-time <START> \\\n  --end-time <END> \\\n  --period 3600 \\\n  --statistics Average\n```\n\n### Logging\n```bash\n# View browser execution logs\naws logs tail /aws/bedrock-agentcore/browser/<AGENT_ID> \\\n  --follow \\\n  --format short\n```\n\n## Performance Considerations\n\n### Optimization Techniques\n1. **Disable Unnecessary Resources**: Turn off images/stylesheets when not needed\n2. **Use Headless Mode**: Faster execution without rendering overhead\n3. **Implement Caching**: Cache static resources and repeated queries\n4. **Parallel Execution**: Run multiple browser sessions concurrently\n5. **Smart Waiting**: Use explicit waits instead of fixed delays\n\n### Scaling Patterns\n- **Horizontal Scaling**: Launch multiple browser instances\n- **Session Pooling**: Reuse browser sessions for efficiency\n- **Request Queuing**: Queue browser operations during high load\n- **Regional Distribution**: Distribute load across AWS regions\n\n## Additional Resources\n\n- **AWS Documentation**: [Bedrock AgentCore Browser](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/browser.html)\n- **Best Practices**: [Browser Automation Guide](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/browser-best-practices.html)\n- **API Reference**: [Browser API](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/)\n- **Selector Reference**: [CSS Selectors](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)\n\n---\n\n**Related Services**:\n- [Runtime Service](../runtime/README.md) - Agent execution\n- [Code Interpreter](../code-interpreter/README.md) - Data processing\n- [Memory Service](../memory/README.md) - State management\n- [Observability Service](../observability/README.md) - Monitoring\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/code-interpreter/README.md": "# AgentCore Code Interpreter Service\n\n> **Status**: ✅ Available\n\n## Overview\n\nAmazon Bedrock AgentCore Code Interpreter enables agents to securely execute code in isolated sandbox environments, supporting complex data analysis workflows and computational tasks.\n\n## Core Capabilities\n\n### Secure Execution\n- **Isolated Sandboxes**: Each code execution runs in a completely isolated environment\n- **No Cross-Contamination**: Sessions are independent with no shared state\n- **Enterprise Security**: Meets enterprise security and compliance requirements\n- **Resource Controls**: Configurable limits and timeout controls for execution\n\n### Framework Integration\n- **Popular Frameworks**: Seamless integration with LangGraph, CrewAI, Strands, and other agent frameworks\n- **Multi-Language Support**: Execute code in Python, JavaScript, and other languages\n- **Advanced Configuration**: Extensive customization options for runtime environments\n- **Custom Runtimes**: Support for specialized runtime configurations\n\n### Data Processing\n- **File Operations**: Upload and download files for processing\n- **Multi-Modal Data**: Handle structured and unstructured data\n- **Result Formatting**: Format and visualize execution results\n- **Error Handling**: Comprehensive error reporting and debugging support\n\n## Use Cases\n\n### Data Analysis and Transformation\nEnable agents to:\n- Process and analyze datasets\n- Transform data formats\n- Perform statistical calculations\n- Generate data insights\n\n### Complex Computational Workflows\nSupport scenarios like:\n- Running scientific computations\n- Executing business logic calculations\n- Processing batch operations\n- Performing iterative algorithms\n\n### Visualization and Reporting\nAllow agents to:\n- Generate charts and graphs\n- Create formatted reports\n- Build visualizations from data\n- Export results in various formats\n\n### Dynamic Code Testing\nEnable agents to:\n- Test code snippets dynamically\n- Validate logic and algorithms\n- Debug code execution issues\n- Prototype solutions quickly\n\n## Architecture\n\n### Execution Flow\n\n```\nAgent Request\n    ↓\n┌─────────────────────────────────────────┐\n│  Code Interpreter Service               │\n│  - Parse code execution request         │\n│  - Validate code and parameters         │\n│  - Allocate isolated sandbox            │\n└─────────────────────────────────────────┘\n    ↓\n┌─────────────────────────────────────────┐\n│  Sandbox Environment                    │\n│  - Execute code in isolation            │\n│  - Process data and files               │\n│  - Generate outputs                     │\n│  - Capture errors and logs              │\n└─────────────────────────────────────────┘\n    ↓\n┌─────────────────────────────────────────┐\n│  Result Processing                      │\n│  - Format execution results             │\n│  - Package outputs and artifacts        │\n│  - Return to agent                      │\n└─────────────────────────────────────────┘\n```\n\n### Security Model\n\n1. **Sandbox Isolation**: Each execution runs in a completely isolated environment\n2. **Resource Limits**: CPU, memory, and time limits prevent resource exhaustion\n3. **Network Restrictions**: Controlled network access from sandbox environments\n4. **Data Encryption**: Data at rest and in transit is encrypted\n5. **Audit Logging**: All code executions are logged for compliance\n\n## Configuration\n\n### Basic Setup\n\n```bash\n# Configure code interpreter for agent\naws bedrock-agentcore-control configure-code-interpreter \\\n  --agent-id <AGENT_ID> \\\n  --execution-timeout 300 \\\n  --memory-limit 2048 \\\n  --region <REGION>\n```\n\n### Custom Runtime Configuration\n\n```bash\n# Set custom runtime environment\naws bedrock-agentcore-control update-code-interpreter-runtime \\\n  --agent-id <AGENT_ID> \\\n  --runtime-config '{\n    \"language\": \"python3.11\",\n    \"packages\": [\"pandas\", \"numpy\", \"matplotlib\"],\n    \"environment\": {\n      \"CUSTOM_VAR\": \"value\"\n    }\n  }' \\\n  --region <REGION>\n```\n\n## Best Practices\n\n### Code Security\n- Validate all code inputs before execution\n- Implement input sanitization for user-provided code\n- Use resource limits to prevent denial of service\n- Monitor code execution patterns for anomalies\n\n### Performance Optimization\n- Cache common dependencies in runtime images\n- Use appropriate timeout values for expected workload\n- Optimize code for execution within timeout limits\n- Batch similar operations when possible\n\n### Error Handling\n- Implement comprehensive error catching in code\n- Provide clear error messages for debugging\n- Log execution details for troubleshooting\n- Use structured output formats for results\n\n### Data Management\n- Minimize data transfer in and out of sandboxes\n- Use streaming for large data processing\n- Clean up temporary files after execution\n- Implement data validation before processing\n\n## Integration Patterns\n\n### With Memory Service\n```\nCode Interpreter ←→ Memory Service\n- Store execution results in memory\n- Retrieve past computation results\n- Share data across agent sessions\n```\n\n### With Identity Service\n```\nCode Interpreter ←→ Identity Service\n- Authenticate code execution requests\n- Access credentials for external APIs\n- Manage permissions for data access\n```\n\n### With Observability Service\n```\nCode Interpreter ←→ Observability Service\n- Trace code execution workflows\n- Monitor performance metrics\n- Log execution events\n- Alert on execution failures\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**Execution Timeout**\n- Symptom: Code execution exceeds timeout limit\n- Solution: Increase timeout or optimize code performance\n\n**Memory Limit Exceeded**\n- Symptom: Code runs out of memory\n- Solution: Increase memory limit or process data in chunks\n\n**Package Import Errors**\n- Symptom: Required packages not found\n- Solution: Configure custom runtime with needed packages\n\n**Permission Denied**\n- Symptom: Cannot access required resources\n- Solution: Configure IAM permissions for code interpreter\n\n## Monitoring\n\n### Key Metrics\n- **Execution Count**: Number of code executions\n- **Success Rate**: Percentage of successful executions\n- **Average Duration**: Mean execution time\n- **Error Rate**: Percentage of failed executions\n- **Resource Utilization**: CPU and memory usage\n\n### CloudWatch Integration\n```bash\n# Query execution metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/BedrockAgentCore/CodeInterpreter \\\n  --metric-name ExecutionCount \\\n  --dimensions Name=AgentId,Value=<AGENT_ID> \\\n  --start-time <START> \\\n  --end-time <END> \\\n  --period 3600 \\\n  --statistics Sum\n```\n\n## Additional Resources\n\n- **AWS Documentation**: [Bedrock AgentCore Code Interpreter](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter.html)\n- **Security Best Practices**: [Secure Code Execution](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter-security.html)\n- **API Reference**: [Code Interpreter API](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/)\n\n---\n\n**Related Services**:\n- [Runtime Service](../runtime/README.md) - Agent execution environment\n- [Memory Service](../memory/README.md) - State management\n- [Observability Service](../observability/README.md) - Monitoring and tracing\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/gateway/README.md": "# Gateway Service\n\nThe Gateway service converts REST APIs into MCP tools that AI agents can use. It handles authentication, schema validation, and request routing.\n\n## Quick Start\n\n### Prerequisites\n- AWS CLI configured with appropriate permissions\n- An existing Gateway (created via AWS Console or CLI)\n- OpenAPI schema for your target API\n\n### Deploy a Gateway Target\n\n**Step 1: Upload OpenAPI schema to S3**\n```bash\naws s3 cp my-api-openapi.yaml s3://<BUCKET_NAME>/schemas/\n```\n\n**Step 2: Create credential provider (API Key auth only)**\n```bash\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --api-key \"YOUR_API_KEY\" \\\n  --region us-west-2\n```\n\n**Step 3: Create gateway target**\n```bash\naws bedrock-agentcore-control create-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --name MyAPITarget \\\n  --endpoint-configuration '{\"openApiSchema\": {\"s3\": {\"uri\": \"s3://<BUCKET_NAME>/schemas/my-api-openapi.yaml\"}}}' \\\n  --credential-provider-configurations '[{\"credentialProviderType\": \"GATEWAY_API_KEY_CREDENTIAL_PROVIDER\", \"apiKeyCredentialProvider\": {\"providerArn\": \"arn:aws:bedrock-agentcore:us-west-2:<ACCOUNT_ID>:api-key-credential-provider/MyAPICredentialProvider\"}}]' \\\n  --region us-west-2\n```\n\n**Step 4: Verify deployment**\n```bash\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --target-identifier <TARGET_ID> \\\n  --region us-west-2\n```\n\n## Authentication Options\n\n### Outbound (Accessing External APIs)\n\n| Target Type | IAM | OAuth 2LO | OAuth 3LO | API Key |\n|-------------|-----|-----------|-----------|---------|\n| Lambda function | Yes | No | No | No |\n| API Gateway | Yes | No | No | Yes |\n| OpenAPI schema | No | Yes | Yes | Yes |\n| Smithy schema | Yes | Yes | Yes | No |\n| MCP server | No | Yes | No | No |\n\n### Inbound (Who Can Invoke Tools)\n- **IAM**: AWS IAM credentials\n- **JWT**: Tokens from identity providers (Cognito, Entra ID)\n- **No Auth**: Open access (use with caution)\n\n## Documentation\n\n| Document | Description |\n|----------|-------------|\n| [Deployment Strategies](deployment-strategies.md) | Credential provider patterns, multi-environment setup, key rotation |\n| [Troubleshooting Guide](troubleshooting-guide.md) | Common errors, diagnosis steps, solutions |\n| [Deploy Template Script](deploy-template.sh) | Automated deployment script |\n| [Validate Deployment Script](validate-deployment.sh) | Post-deployment verification |\n\n## Common Operations\n\n### List Gateway Targets\n```bash\naws bedrock-agentcore-control list-gateway-targets \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --region us-west-2\n```\n\n### Update Credential Provider\n```bash\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --api-key \"NEW_API_KEY\" \\\n  --region us-west-2\n```\n\n### Delete Gateway Target\n```bash\naws bedrock-agentcore-control delete-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --target-identifier <TARGET_ID> \\\n  --region us-west-2\n```\n\n## Related Resources\n\n- [Cross-Service Credential Management](../../cross-service/credential-management.md)\n- [AWS Gateway Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html)\n- [Bedrock AgentCore CLI Reference](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/bedrock-agentcore-control/index.html)\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/gateway/deployment-strategies.md": "# AgentCore Gateway Deployment Strategies\n\n## Overview\n\nThis reference guide covers different deployment strategies for AWS Bedrock AgentCore Gateway targets, including credential management approaches, multi-environment patterns, and rollback procedures.\n\n## Credential Provider Strategies\n\n### Strategy 1: Shared Provider (Recommended for Most Cases)\n\n**Concept**: Create ONE credential provider and share across all gateway targets\n\n**Setup**:\n```bash\n# Create shared provider with API key (run once)\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name SharedAPICredentialProvider \\\n  --api-key \"YOUR_API_KEY\" \\\n  --profile default --region us-west-2\n```\n\n**Environment Configuration**:\n```bash\n# .env.gateway-a\nGATEWAY_IDENTIFIER=gateway-a-abc123xyz\nCREDENTIAL_PROVIDER_NAME=SharedAPICredentialProvider  # Same for all\n\n# .env.gateway-b\nGATEWAY_IDENTIFIER=gateway-b-def456uvw\nCREDENTIAL_PROVIDER_NAME=SharedAPICredentialProvider  # Same for all\n```\n\n**Benefits**:\n- ✅ Simplified key management - single key to rotate\n- ✅ Reduced operational overhead\n- ✅ Consistent authentication across all gateways\n- ✅ Easier compliance and auditing\n\n**Use Cases**:\n- Same API, multiple gateway deployments\n- Development/Testing/Production gateways\n- Regional deployments (us-west-2, eu-west-1)\n\n**Trade-offs**:\n- Less isolation between gateways (all or nothing key rotation)\n\n### Strategy 2: Isolated Provider (Per-Gateway)\n\n**Concept**: Create UNIQUE credential provider for each gateway\n\n**Setup**:\n```bash\n# Create provider for Gateway A\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name GatewayAAPICredentialProvider \\\n  --api-key \"API_KEY_A\" \\\n  --profile default --region us-west-2\n\n# Create provider for Gateway B\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name GatewayBAPICredentialProvider \\\n  --api-key \"API_KEY_B\" \\\n  --profile default --region us-west-2\n```\n\n**Environment Configuration**:\n```bash\n# .env.gateway-a\nGATEWAY_IDENTIFIER=gateway-a-abc123xyz\nCREDENTIAL_PROVIDER_NAME=GatewayAAPICredentialProvider  # Unique\n\n# .env.gateway-b\nGATEWAY_IDENTIFIER=gateway-b-def456uvw\nCREDENTIAL_PROVIDER_NAME=GatewayBAPICredentialProvider  # Unique\n```\n\n**Benefits**:\n- ✅ Complete isolation between gateways\n- ✅ Independent key rotation per environment\n- ✅ Different API keys for different use cases\n- ✅ Better security boundaries\n\n**Use Cases**:\n- Production vs Development with different API keys\n- Different APIs for different gateways\n- Compliance requiring environment separation\n- Testing new API versions in isolation\n\n**Trade-offs**:\n- More complex key management\n- Multiple keys to rotate and maintain\n\n### Strategy 3: Tiered (Shared + Isolated)\n\n**Concept**: Hybrid approach with shared provider for non-prod, isolated for production\n\n**Setup**:\n```bash\n# Shared provider for dev/test\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name DevTestAPICredentialProvider \\\n  --api-key \"DEV_TEST_API_KEY\" \\\n  --profile default --region us-west-2\n\n# Isolated provider for production\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name ProdAPICredentialProvider \\\n  --api-key \"PROD_API_KEY\" \\\n  --profile default --region us-west-2\n```\n\n**Environment Configuration**:\n```bash\n# .env.development\nGATEWAY_IDENTIFIER=dev-gateway-abc123xyz\nCREDENTIAL_PROVIDER_NAME=DevTestAPICredentialProvider\n\n# .env.staging\nGATEWAY_IDENTIFIER=staging-gateway-def456uvw\nCREDENTIAL_PROVIDER_NAME=DevTestAPICredentialProvider\n\n# .env.production\nGATEWAY_IDENTIFIER=prod-gateway-ghi789rst\nCREDENTIAL_PROVIDER_NAME=ProdAPICredentialProvider\n```\n\n**Benefits**:\n- ✅ Balance of simplicity and security\n- ✅ Production isolation with dev/test convenience\n- ✅ Easier testing in non-prod environments\n- ✅ Production key remains protected\n\n**Use Cases**:\n- Most common enterprise pattern\n- Clear separation between environments\n- Controlled production access\n\n## Multi-Account Deployment\n\nWhen deploying across multiple AWS accounts:\n\n### Setup\n\n1. **Credential Provider per Account**:\n   ```bash\n   # Account 1 (Dev)\n   aws bedrock-agentcore-control create-api-key-credential-provider \\\n     --name APICredentialProvider \\\n     --api-key \"DEV_API_KEY\" \\\n     --profile dev-account\n\n   # Account 2 (Prod)\n   aws bedrock-agentcore-control create-api-key-credential-provider \\\n     --name APICredentialProvider \\\n     --api-key \"PROD_API_KEY\" \\\n     --profile prod-account\n   ```\n\n2. **Centralized Configuration**:\n   ```bash\n   # .env.dev\n   ACCOUNT_ID=123456789012\n   GATEWAY_IDENTIFIER=dev-gateway-abc123xyz\n   AWS_PROFILE=dev-account\n\n   # .env.prod\n   ACCOUNT_ID=987654321098\n   GATEWAY_IDENTIFIER=prod-gateway-abc123xyz\n   AWS_PROFILE=prod-account\n   ```\n\n3. **Cross-Account Deployment Script**:\n   ```bash\n   #!/bin/bash\n   ENV_FILE=$1\n\n   # Load environment\n   export $(cat $ENV_FILE | xargs)\n\n   # Get AWS account ID\n   export CDK_DEFAULT_ACCOUNT=$(aws sts get-caller-identity \\\n     --profile $AWS_PROFILE \\\n     --query Account --output text)\n\n   # Deploy\n   npm run build && cdk deploy --profile $AWS_PROFILE --require-approval never\n   ```\n\n## Key Rotation Procedures\n\n### Shared Provider Strategy\n\n**Manual Rotation**:\n```bash\n# 1. Update key in provider\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name SharedAPICredentialProvider \\\n  --api-key \"NEW_API_KEY\" \\\n  --profile default --region us-west-2\n\n# 2. Restart gateway targets (if needed) to pick up new key\n```\n\n**Automated Rotation**:\n- Use AWS Secrets Manager rotation (if supported by credential provider)\n- Triggered by CloudWatch Events schedule\n- Lambda function handles key generation/update\n\n### Isolated Provider Strategy\n\n**Per-Gateway Rotation**:\n```bash\n# Rotate dev environment only\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name DevAPICredentialProvider \\\n  --api-key \"NEW_DEV_KEY\"\n\n# Production remains unchanged\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name ProdAPICredentialProvider \\\n  --api-key \"EXISTING_PROD_KEY\"\n```\n\n## Rollback Procedures\n\n### Rollback Failed Deployment\n\n```bash\n# List CloudFormation stacks\naws cloudformation list-stacks --stack-status-filter UPDATE_ROLLBACK_FAILED\n\n# Get previous stack configuration\naws cloudformation describe-stack-resources --stack-name StackName\n\n# Rollback to previous version\naws cloudformation continue-update-rollback --stack-name StackName\n```\n\n### Rollback Credential Changes\n\n```bash\n# If new API key is causing issues, restore previous key\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name APICredentialProvider \\\n  --api-key \"PREVIOUS_WORKING_KEY\"\n```\n\n## Monitoring and Alerting\n\n### CloudWatch Metrics to Monitor\n\n- **Gateway Target Status**: Monitor target health\n- **API Request Count**: Track usage per gateway\n- **Error Rates**: 4xx and 5xx errors\n- **Latency**: P95, P99 response times\n- **Credential Provider Errors**: Secret access failures\n\n### CloudWatch Alarms\n\n```typescript\n// CDK Example: Create alarm for high error rate\nnew cloudwatch.Alarm(this, 'HighErrorRate', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/BedrockAgentCore',\n    metricName: 'TargetErrorRate',\n    dimensionsMap: {\n      GatewayId: gatewayId,\n      TargetId: targetId,\n    },\n    statistic: 'avg',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 2,\n  datapointsToAlarm: 2,\n  comparisonOperator: cloudwatch.ComparisonOperator.GREATER_THAN_THRESHOLD,\n  alarmDescription: 'Error rate exceeds 10%',\n  actionsEnabled: true,\n});\n```\n\n## Cost Optimization\n\n### Cost Considerations\n\n1. **AgentCore Gateway**: Pay per tool invocation\n   - Optimize schema to reduce unnecessary API calls\n   - Cache frequently accessed data in schema descriptions\n   - Use batch operations where available\n\n2. **S3 Asset Storage**: Negligible (<$0.01/month)\n   - Schema files are small\n   - CDK automatically cleans up old versions\n\n3. **Lambda (GatewayRoleUpdater)**: ~$0.01 per deployment\n   - Covered by AWS Lambda free tier (1M requests/month)\n   - Only runs during deployment/update\n\n4. **Secrets Manager**: ~$0.40/month per secret\n   - Use shared provider to minimize secret count\n   - Rotate secrets on schedule to maintain security\n\n5. **API Calls (RapidAPI Example)**:\n   - Free tier: 100 requests/day\n   - Paid tiers: From $10/month\n   - Optimize by embedding IDs in schema\n\n### Optimization Strategies\n\n**Schema Optimization**:\n```yaml\n# Embed common IDs to reduce API calls by 50%\ninfo:\n  description: |\n    COMMON LEAGUE IDs:\n    - Premier League: 39\n    - Champions League: 2\n    - World Cup: 1\n```\n\n**Credential Provider Sharing**:\n- Single provider for all gateways = 1 secret = $0.40/month\n- Separate providers = N secrets = $0.40N/month\n\n## Security Best Practices\n\n### Credential Management\n- Never commit API keys to source control\n- Use AWS Secrets Manager via credential providers\n- Rotate keys regularly (quarterly minimum)\n- Use different keys for different environments\n\n### IAM Permissions\n- Custom Resource Lambda has scoped permissions\n- Only allows access to Gateway service roles\n- Follows principle of least privilege\n- Audit policy versions regularly\n\n### Network Security\n- Ensure Gateway is in VPC if required\n- Use AWS PrivateLink for on-premises integrations\n- Enable encryption in transit (TLS 1.2+)\n- Verify API endpoints use HTTPS\n\n## Common Patterns\n\n### Pattern 1: Development Pipeline\n\n```bash\n# Branch-based deployment\nif [ \"$BRANCH\" = \"main\" ]; then\n  ./deploy.sh .env.production\nelif [ \"$BRANCH\" = \"develop\" ]; then\n  ./deploy.sh .env.staging\nelse\n  ./deploy.sh .env.development\nfi\n```\n\n### Pattern 2: Regional Deployment\n\n```bash\n# Deploy to multiple regions\nfor region in us-west-2 eu-west-1 ap-southeast-1; do\n  export AWS_REGION=$region\n  export GATEWAY_IDENTIFIER=\"my-gateway-${region}\"\n  npm run build && cdk deploy --require-approval never\ndone\n```\n\n### Pattern 3: Blue-Green Deployment\n\n```bash\n# Deploy to blue environment\n./deploy.sh .env.blue\n\n# Test blue environment\n./test-target.sh blue\n\n# Switch to green if blue is healthy\n./deploy.sh .env.green\n```\n\n## Migration Strategies\n\n### Migrating from Manual to CDK Management\n\n1. **Discovery Phase**:\n   ```bash\n   # Document existing targets\n   aws bedrock-agentcore-control list-gateway-targets \\\n     --gateway-identifier existing-gateway \\\n     --profile default --region us-west-2\n   ```\n\n2. **Schema Extraction**:\n   - Export existing OpenAPI schemas\n   - Audit and optimize schema descriptions\n   - Embed common IDs for performance\n\n3. **CDK Implementation**:\n   - Create stack with existing target configuration\n   - Import existing credential provider\n   - Add GatewayRoleUpdater for IAM automation\n\n4. **Cutover**:\n   - Deploy to new gateway first (test)\n   - Update AI agents to use new target\n   - Decommission old target after verification\n\n## Additional References\n\n- **Main Skill Documentation**: [`../../SKILL.md`](../../SKILL.md)\n- **Troubleshooting Guide**: [`./troubleshooting-guide.md`](./troubleshooting-guide.md)\n- **Deployment Template Script**: [`./deploy-template.sh`](./deploy-template.sh)\n- **Credential Management**: [`../../cross-service/credential-management.md`](../../cross-service/credential-management.md)\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/gateway/troubleshooting-guide.md": "# AgentCore Gateway Troubleshooting Guide\n\n## Quick Diagnosis\n\n### Symptom: Target Creation Fails\n\n**Error Message**: `\"Gateway target creation failed\"`\n\n**Diagnosis Steps**:\n1. Verify gateway exists\n   ```bash\n   aws bedrock-agentcore-control get-gateway \\\n     --gateway-identifier <GATEWAY_ID> \\\n     --profile default --region us-west-2\n   ```\n\n2. Check credential provider exists (if using API key auth)\n   ```bash\n   aws bedrock-agentcore-control get-api-key-credential-provider \\\n     --name <PROVIDER_NAME> \\\n     --profile default --region us-west-2\n   ```\n\n3. List existing targets\n   ```bash\n   aws bedrock-agentcore-control list-gateway-targets \\\n     --gateway-identifier <GATEWAY_ID> \\\n     --profile default --region us-west-2\n   ```\n\n**Common Causes**:\n- Gateway ID incorrect or gateway doesn't exist\n- Credential provider name misspelled (for API key auth)\n- OpenAPI schema syntax error\n- S3 bucket permissions issue for schema\n\n---\n\n## Permission Errors\n\n### Error: \"User is not authorized to perform: bedrock-agentcore:GetResourceApiKey\"\n\n**Full Error**:\n```\nUser: arn:aws:sts::<ACCOUNT_ID>:assumed-role/GatewayServiceRole is not\nauthorized to perform: bedrock-agentcore:GetResourceApiKey on resource: *\n```\n\n**Root Cause**: Gateway service role missing credential provider access permissions\n\n**Diagnosis**:\n```bash\n# Get gateway role ARN\nGATEWAY_ROLE=$(aws bedrock-agentcore-control get-gateway \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --query 'roleArn' --output text \\\n  --profile default --region us-west-2)\n\necho $GATEWAY_ROLE\n\n# Extract role name\nROLE_NAME=$(echo $GATEWAY_ROLE | cut -d'/' -f2)\n\n# Check attached policies\naws iam list-attached-role-policies \\\n  --role-name $ROLE_NAME \\\n  --profile default --region us-west-2\n```\n\n**Solution**: Add required permissions to gateway role:\n```bash\ncat > /tmp/gateway-policy.json <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"GetResourceApiKey\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"bedrock-agentcore:GetResourceApiKey\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"GetWorkloadAccessToken\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"bedrock-agentcore:GetWorkloadAccessToken\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"GetCredentials\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"secretsmanager:GetSecretValue\"],\n      \"Resource\": [\"arn:aws:secretsmanager:us-west-2:<ACCOUNT_ID>:secret:bedrock-agentcore-identity!*\"]\n    }\n  ]\n}\nEOF\n\n# Get policy ARN from role\nPOLICY_ARN=$(aws iam list-attached-role-policies \\\n  --role-name $ROLE_NAME \\\n  --query 'AttachedPolicies[0].PolicyArn' \\\n  --output text \\\n  --profile default --region us-west-2)\n\n# Create new policy version\naws iam create-policy-version \\\n  --policy-arn $POLICY_ARN \\\n  --policy-document file:///tmp/gateway-policy.json \\\n  --set-as-default \\\n  --profile default --region us-west-2\n```\n\n---\n\n### Error: \"AccessDeniedException: Secrets Manager\"\n\n**Full Error**:\n```\nAccessDeniedException: User: arn:aws:sts::<ACCOUNT_ID>:assumed-role/GatewayServiceRole\nis not authorized to perform: secretsmanager:GetSecretValue on resource: ...\n```\n\n**Root Cause**: Gateway cannot access Secrets Manager for credential provider\n\n**Solution**: Add `secretsmanager:GetSecretValue` permission to gateway role (see above)\n\n---\n\n## Credential Provider Issues\n\n### Error: \"Credential provider not found\"\n\n**Diagnosis**:\n```bash\n# Check if provider exists\naws bedrock-agentcore-control get-api-key-credential-provider \\\n  --name <PROVIDER_NAME> \\\n  --profile default --region us-west-2\n\n# List all providers\naws bedrock-agentcore-control list-api-key-credential-providers \\\n  --profile default --region us-west-2\n```\n\n**Solution**:\n```bash\n# Create provider if missing\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name <PROVIDER_NAME> \\\n  --api-key \"YOUR_API_KEY\" \\\n  --profile default --region us-west-2\n```\n\n---\n\n### Error: \"Invalid API key\"\n\n**Symptom**: API calls return 403 or authentication errors\n\n**Diagnosis**:\n```bash\n# Check API key format in secret\nSECRET=$(aws secretsmanager get-secret-value \\\n  --secret-id \"arn:aws:secretsmanager:us-west-2:<ACCOUNT_ID>:secret:bedrock-agentcore-identity/default/apikeycredentialprovider/<PROVIDER_NAME>-AbCdEf\" \\\n  --query 'SecretString' --output text \\\n  --profile default --region us-west-2)\n\necho $SECRET | jq '.'\n# Should be: {\"apiKey\": \"your-key-here\"}\n```\n\n**Solution**:\n```bash\n# Use the correct update command (not secretsmanager put-secret-value)\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name <PROVIDER_NAME> \\\n  --api-key \"YOUR_VALID_API_KEY\" \\\n  --profile default --region us-west-2\n```\n\n**Common Mistake**: Using `secretsmanager put-secret-value` directly bypasses credential provider validation\n\n---\n\n## OpenAPI Schema Issues\n\n### Error: \"Invalid OpenAPI schema\"\n\n**Diagnosis**:\n```bash\n# Validate OpenAPI schema locally\nnpm install -g @apidevtools/swagger-cli\nswagger-cli validate schemas/my-api-openapi.yaml\n\n# Check for unsupported constructs\ngrep -n \"oneOf\\|anyOf\\|allOf\" schemas/my-api-openapi.yaml\n# These constructs are not supported in Gateway\n```\n\n**Common Issues**:\n1. **oneOf/anyOf/allOf**: Not supported, use simple types\n2. **Missing operationId**: All operations must have operationId\n3. **Invalid $ref references**: Must point to valid components\n4. **YAML syntax errors**: Use online YAML validator\n\n**Solution**: Use OpenAPI 3.0 simple types only:\n```yaml\n# ❌ Bad - unsupported\nschema:\n  oneOf:\n    - type: string\n    - type: number\n\n# ✅ Good - simple type\nschema:\n  type: string\n```\n\n---\n\n### Error: \"Schema size exceeds limit\"\n\n**Root Cause**: OpenAPI schema too large for Gateway\n\n**Solutions**:\n1. Remove unused endpoints from schema\n2. Simplify descriptions (keep essential info only)\n3. Remove redundant component definitions\n4. Split into multiple targets if necessary\n\n---\n\n## API Call Issues\n\n### Error: \"Rate limit exceeded\"\n\n**Symptom**: API calls return 429 Too Many Requests\n\n**Root Cause**: Gateway or upstream API rate limits\n\n**Diagnosis**:\n```bash\n# Check gateway target status\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --target-identifier <TARGET_ID> \\\n  --profile default --region us-west-2\n```\n\n**Solutions**:\n1. **Check Gateway limits**: Verify gateway quota in AWS Console\n2. **Upstream API limits**: Check your API provider dashboard for limits\n3. **Optimize calls**: Use embedded IDs in schema to reduce API queries\n4. **Implement caching**: Cache responses when possible\n5. **Request limit increase**: Contact AWS support if needed\n\n---\n\n### Error: \"Invalid API host header\"\n\n**Root Cause**: API endpoint or host header configuration mismatch\n\n**Diagnosis**:\n```bash\n# Check schema for correct server URL\ngrep -A5 \"servers:\" schemas/my-api-openapi.yaml\n\n# Verify host header in security scheme\ngrep -A10 \"securitySchemes:\" schemas/my-api-openapi.yaml\n```\n\n**Solution**: Update OpenAPI schema with correct server URL and host header\n\n---\n\n### Error: \"Connection timeout\"\n\n**Symptom**: Gateway target calls fail with timeout errors\n\n**Root Cause**: Upstream API not responding within timeout limit\n\n**Solutions**:\n1. Verify upstream API is accessible\n2. Check network connectivity from Gateway\n3. Increase timeout in target configuration (if supported)\n4. Check if upstream API requires VPC configuration\n\n---\n\n## Target Status Issues\n\n### Target Status: \"FAILED\"\n\n**Diagnosis**:\n```bash\n# Get target details including status reason\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --target-identifier <TARGET_ID> \\\n  --query '{status: status, statusReason: statusReason}' \\\n  --profile default --region us-west-2\n```\n\n**Common Status Reasons**:\n- `SCHEMA_VALIDATION_FAILED`: OpenAPI schema has errors\n- `CREDENTIAL_PROVIDER_NOT_FOUND`: API key provider doesn't exist\n- `PERMISSION_DENIED`: IAM permissions missing\n\n---\n\n### Target Status: \"PENDING\" for too long\n\n**Root Cause**: Target creation stuck\n\n**Solutions**:\n1. Check if all dependencies exist (gateway, credential provider)\n2. Delete and recreate target\n3. Check AWS service health dashboard\n\n---\n\n## Testing and Verification\n\n### How to Test Target Deployment\n\n```bash\n#!/bin/bash\n# test-target.sh\n\nGATEWAY_ID=$1\nTARGET_ID=$2\n\n# Test 1: List targets\necho \"Listing gateway targets...\"\naws bedrock-agentcore-control list-gateway-targets \\\n  --gateway-identifier $GATEWAY_ID \\\n  --profile default --region us-west-2\n\n# Test 2: Get target details\necho \"Getting target details...\"\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier $GATEWAY_ID \\\n  --target-identifier $TARGET_ID \\\n  --profile default --region us-west-2\n\n# Test 3: Get tools from target\necho \"Getting tools...\"\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier $GATEWAY_ID \\\n  --target-identifier $TARGET_ID \\\n  --query 'tools' \\\n  --profile default --region us-west-2\n```\n\n---\n\n## Common Error Summary Table\n\n| Error | Likely Cause | Solution |\n|-------|-------------|----------|\n| \"not authorized to perform: bedrock-agentcore:GetResourceApiKey\" | Missing IAM permissions | Add permissions to gateway role |\n| \"Credential provider not found\" | Provider doesn't exist or name typo | Create provider with `create-api-key-credential-provider` |\n| \"Invalid API key\" | Key format wrong or key invalid | Use `update-api-key-credential-provider` to update |\n| \"Invalid OpenAPI schema\" | Unsupported constructs (oneOf/anyOf/allOf) | Remove unsupported constructs, use simple types |\n| \"Invalid API host header\" | Host header doesn't match endpoint | Update OpenAPI schema with correct host |\n| \"Rate limit exceeded\" | Too many API calls | Check limits, implement caching |\n| \"Connection timeout\" | Upstream API not responding | Verify API accessibility |\n| \"Target status FAILED\" | Schema or credential issues | Check statusReason in get-gateway-target |\n\n---\n\n## Escalation Path\n\nIf issues persist after troubleshooting:\n\n1. **Check AWS Documentation**: https://docs.aws.amazon.com/bedrock-agentcore/\n2. **AWS Support**: Open case with Bedrock AgentCore service\n3. **Community**: AWS Developer Forums\n\n**Information to gather for AWS Support**:\n- Gateway ID and target ID\n- Error messages and timestamps\n- OpenAPI schema (sanitized)\n- IAM role and policy configuration\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/identity/README.md": "# AgentCore Identity Service\n\n> **Status**: ✅ Available\n\n## Overview\n\nAmazon Bedrock AgentCore Identity is an identity and credential management service designed specifically for AI agents and automated workloads. It provides secure authentication, authorization, and credential management capabilities that enable agents and tools to access AWS resources and third-party services on behalf of users while maintaining strict security controls and audit trails.\n\n## Core Capabilities\n\n### Centralized Agent Identity Management\n- **Workload Identities**: Agent identities implemented as workload identities with specialized attributes\n- **Unified Directory**: Create, manage, and organize agent identities through unified directory service\n- **Hierarchical Organization**: Group-based access controls and hierarchical organization\n- **Cross-Environment**: Consistent identity management regardless of deployment location\n\n### Secure Credential Storage\n- **Token Vault**: Securely store OAuth 2.0 tokens, client credentials, and API keys\n- **Encryption**: Comprehensive encryption at rest and in transit\n- **Access Controls**: Strict access controls with independent request validation\n- **Defense-in-Depth**: Protects end-user data from malicious or misbehaving agent code\n\n### OAuth 2.0 Flow Support\n- **Client Credentials Grant**: Machine-to-machine authentication (2LO)\n- **Authorization Code Grant**: User-delegated access (3LO)\n- **Built-in Providers**: Pre-configured providers for Google, GitHub, Slack, Salesforce\n- **Custom Providers**: Configurable OAuth 2.0 credential providers for custom integrations\n\n### Credential Provider Management\n- **API Key Providers**: Securely store and manage API keys\n- **OAuth Credential Providers**: Handle OAuth flow and token management\n- **Token Lifecycle**: Automatic token refresh and expiration handling\n- **Provider Discovery**: Automatically discover available credential providers\n\n### Agent Identity and Access Controls\n- **Impersonation Flow**: Agents access resources using provided credentials\n- **Audit Trails**: Maintain audit trails for all actions performed on behalf of users\n- **Request Verification**: Token signature verification, expiration checks, scope validation\n- **Identity-Aware Authorization**: Pass user context to agent code for dynamic decisions\n\n## Use Cases\n\n### Securing AI Agent Access\nEnable agents to:\n- Authenticate with external services securely\n- Access resources on behalf of users\n- Maintain proper audit trails\n- Implement least-privilege access patterns\n\n### Multi-Provider Authentication\nSupport scenarios like:\n- Different authentication methods for different APIs\n- Unified credential management across services\n- OAuth flows for user-delegated access\n- API key management for service accounts\n\n### Zero-Trust Security Models\nAllow implementation of:\n- No long-lived credentials in application code\n- Centralized, audited credential vault\n- Automated rotation to reduce attack window\n- Comprehensive access logging\n\n### Compliance and Auditing\nEnable teams to:\n- Generate reports for compliance audits (SOC2, ISO27001)\n- Implement periodic access reviews\n- Maintain secrets inventory\n- Enforce credential policies\n\n## Quick Start\n\n### Create API Key Credential Provider\n\n```bash\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --api-key \"YOUR_API_KEY\" \\\n  --region us-west-2\n```\n\n### Create OAuth Credential Provider\n\n```bash\naws bedrock-agentcore-control create-oauth2-credential-provider \\\n  --name MyOAuthProvider \\\n  --client-id \"YOUR_CLIENT_ID\" \\\n  --client-secret \"YOUR_CLIENT_SECRET\" \\\n  --authorization-url \"https://provider.com/oauth/authorize\" \\\n  --token-url \"https://provider.com/oauth/token\" \\\n  --scopes '[\"read\", \"write\"]' \\\n  --region us-west-2\n```\n\n### Using Credentials with SDK\n\n```python\nfrom bedrock_agentcore.identity import CredentialProvider\n\n# Get credentials for external API\nprovider = CredentialProvider(\"MyAPICredentialProvider\")\napi_key = provider.get_api_key()\n\n# Get OAuth token\noauth_provider = CredentialProvider(\"MyOAuthProvider\")\ntoken = oauth_provider.get_access_token()\n```\n\n## Common Operations\n\n### List Credential Providers\n\n```bash\naws bedrock-agentcore-control list-api-key-credential-providers \\\n  --region us-west-2\n```\n\n### Update Credential Provider\n\n```bash\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --api-key \"NEW_API_KEY\" \\\n  --region us-west-2\n```\n\n### Delete Credential Provider\n\n```bash\naws bedrock-agentcore-control delete-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --region us-west-2\n```\n\n## Built-in OAuth Providers\n\nAgentCore Identity includes built-in providers for popular services:\n\n| Provider | Use Case |\n|----------|----------|\n| **Google** | Google Workspace, Gmail, Drive |\n| **GitHub** | Repository access, Actions |\n| **Slack** | Messaging, channel access |\n| **Salesforce** | CRM data access |\n\n## Best Practices\n\n### Security\n- Use credential providers instead of hardcoded credentials\n- Implement least-privilege access for each credential\n- Rotate credentials regularly (quarterly minimum)\n- Monitor credential usage with CloudWatch\n\n### Development\n- Use separate credential providers per environment\n- Implement proper error handling for credential access\n- Test credential flows in non-production first\n- Use SDK annotations for cleaner code\n\n### Operations\n- Set up alerts for credential access failures\n- Audit credential usage periodically\n- Implement automated rotation where possible\n- Document credential ownership and purpose\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Credential not found | Provider doesn't exist or name typo | Verify provider name with list command |\n| Invalid API key | Key expired or incorrect | Update credential provider with new key |\n| OAuth token expired | Token refresh failed | Check OAuth provider configuration |\n| Access denied | Insufficient permissions | Verify IAM policy for credential access |\n\n## Related Services\n\n- **[Gateway Service](../gateway/README.md)**: Uses Identity for MCP target authentication\n- **[Runtime Service](../runtime/README.md)**: Uses Identity for agent authentication\n- **[Memory Service](../memory/README.md)**: May use Identity for data encryption\n\n## References\n\n- [AWS Identity Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity.html)\n- [Credential Provider Setup](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-outbound-credential-provider.html)\n- [Identity Features](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/key-features-and-benefits.html)\n- [Securing AI Agents Blog](https://aws.amazon.com/blogs/security/securing-ai-agents-with-amazon-bedrock-agentcore-identity/)\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/memory/README.md": "# AgentCore Memory Service\n\n> **Status**: ✅ Available\n\n## Overview\n\nAmazon Bedrock AgentCore Memory is a fully managed service that gives AI agents the ability to remember past interactions, enabling more intelligent, context-aware, and personalized conversations. It addresses a fundamental challenge in agentic AI: statelessness. Without memory capabilities, AI agents treat each interaction as a new instance with no knowledge of previous conversations.\n\n## Memory Types\n\nAgentCore Memory offers two types of memory that work together:\n\n### Short-Term Memory\nCaptures turn-by-turn interactions within a single session, allowing agents to maintain immediate context without requiring users to repeat information.\n\n**Example**: When a user asks \"What's the weather like in Seattle?\" and follows up with \"What about tomorrow?\", the agent relies on recent conversation history to understand that \"tomorrow\" refers to Seattle weather.\n\n### Long-Term Memory\nAutomatically extracts and stores key insights from conversations across multiple sessions, including user preferences, important facts, and session summaries for persistent knowledge retention.\n\n**Example**: If a customer mentions they prefer window seats during flight booking, the agent stores this preference and proactively offers window seats in future interactions.\n\n## Core Capabilities\n\n### Memory Resource Management\n- **Logical Containers**: Encapsulate both raw events and processed long-term memories\n- **Retention Policies**: Define how long data is retained\n- **Security Configuration**: Control access and encryption\n- **Data Transformation**: Transform raw interactions into meaningful insights\n\n### Short-Term Memory Features\n- **Event Storage**: Store conversations, system events, and state changes as immutable events\n- **Session Organization**: Organize by actor and session\n- **Context Preservation**: Maintain immediate context within sessions\n- **Structured Storage**: Support structured storage of interaction data\n\n### Long-Term Memory Features\n- **Insight Extraction**: Automatically extract insights, preferences, and knowledge\n- **Asynchronous Processing**: Extract memories asynchronously using memory strategies\n- **Cross-Session Persistence**: Retain information across multiple sessions\n- **Semantic Search**: Search memories by meaning and context\n\n### Memory Strategies\nDefine the intelligence layer that transforms raw events into meaningful memories:\n\n| Strategy | Description |\n|----------|-------------|\n| **Semantic** | Extract meaningful facts and information |\n| **Summary** | Generate conversation summaries |\n| **User Preference** | Extract and store user preferences |\n| **Custom** | Define custom extraction logic |\n\n### Advanced Features\n- **Branching**: Create alternative conversation paths from specific points\n- **Checkpointing**: Save and mark specific states for later reference\n- **Memory Consolidation**: Merge related memories without duplicates\n- **Audit Trail**: Immutable audit trail for all memory operations\n\n## Use Cases\n\n### Conversational Agents\nEnable chatbots to:\n- Remember previous issues and preferences\n- Provide relevant assistance based on history\n- Create personalized customer experiences\n- Maintain context across session breaks\n\n### Task-Oriented Agents\nSupport workflows like:\n- Track multi-step business process status\n- Maintain workflow progress across sessions\n- Remember task context for resumption\n- Store intermediate results\n\n### Multi-Agent Systems\nAllow agent teams to:\n- Share memory for synchronized operations\n- Coordinate inventory levels and logistics\n- Maintain shared context\n- Optimize collaborative workflows\n\n### Autonomous Agents\nEnable agents to:\n- Plan routes based on past experiences\n- Learn from previous interactions\n- Improve decision-making over time\n- Build persistent knowledge bases\n\n## Quick Start\n\n### Create Memory Resource\n\n```bash\naws bedrock-agentcore-control create-memory \\\n  --memory-name my-agent-memory \\\n  --memory-strategies '[{\"strategyName\": \"SEMANTIC\", \"configuration\": {}}]' \\\n  --region us-west-2\n```\n\n### Using Memory with SDK\n\n```python\nfrom bedrock_agentcore.memory import MemoryClient\n\n# Initialize memory client\nmemory = MemoryClient(memory_id=\"my-agent-memory\")\n\n# Add short-term memory event\nmemory.add_event(\n    session_id=\"session-123\",\n    actor_id=\"user-456\",\n    event_type=\"message\",\n    content={\"role\": \"user\", \"message\": \"Book a flight to Seattle\"}\n)\n\n# Retrieve conversation history\nhistory = memory.get_session_events(session_id=\"session-123\")\n\n# Search long-term memories\nmemories = memory.search_memories(\n    query=\"user flight preferences\",\n    limit=5\n)\n```\n\n### Store and Retrieve Memories\n\n```python\n# Store long-term memory\nmemory.store_memory(\n    memory_type=\"user_preference\",\n    content={\"preference\": \"window_seat\", \"context\": \"flights\"}\n)\n\n# Retrieve relevant memories\nrelevant = memory.search_memories(\n    query=\"seating preferences for flights\",\n    actor_id=\"user-456\"\n)\n```\n\n## Common Operations\n\n### List Memories\n\n```bash\naws bedrock-agentcore-control list-memories \\\n  --region us-west-2\n```\n\n### Get Memory Details\n\n```bash\naws bedrock-agentcore-control get-memory \\\n  --memory-id <MEMORY_ID> \\\n  --region us-west-2\n```\n\n### Update Memory Configuration\n\n```bash\naws bedrock-agentcore-control update-memory \\\n  --memory-id <MEMORY_ID> \\\n  --memory-strategies '[{\"strategyName\": \"SEMANTIC\"}, {\"strategyName\": \"USER_PREFERENCE\"}]' \\\n  --region us-west-2\n```\n\n### Delete Memory\n\n```bash\naws bedrock-agentcore-control delete-memory \\\n  --memory-id <MEMORY_ID> \\\n  --region us-west-2\n```\n\n## Memory Strategies Configuration\n\n### Built-in Strategies\n\n```bash\n# Use semantic strategy\naws bedrock-agentcore-control create-memory \\\n  --memory-name semantic-memory \\\n  --memory-strategies '[{\n    \"strategyName\": \"SEMANTIC\",\n    \"configuration\": {}\n  }]'\n```\n\n### Custom Strategies\n\n```bash\n# Create custom strategy with specific model\naws bedrock-agentcore-control create-memory \\\n  --memory-name custom-memory \\\n  --memory-strategies '[{\n    \"strategyName\": \"CUSTOM\",\n    \"configuration\": {\n      \"modelId\": \"anthropic.claude-3-sonnet\",\n      \"extractionPrompt\": \"Extract key user preferences from this conversation\"\n    }\n  }]'\n```\n\n## Best Practices\n\n### Memory Architecture\n- Design memory architecture intentionally\n- Choose appropriate strategies for use case\n- Implement proper retention policies\n- Consider memory costs and storage\n\n### Performance\n- Use appropriate time-to-live settings\n- Extract only relevant information\n- Implement rhythm of memory operations\n- Monitor memory search latency\n\n### Security\n- Implement proper access controls\n- Encrypt sensitive memories\n- Audit memory access\n- Follow data privacy regulations (GDPR, HIPAA)\n\n### Operations\n- Monitor memory usage and costs\n- Set up alerts for memory failures\n- Implement backup strategies\n- Test memory operations regularly\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Memory not found | Incorrect memory ID | Verify memory ID with list command |\n| Search returns empty | No matching memories | Check query and memory content |\n| Slow memory retrieval | Large memory size | Implement pagination and filters |\n| Strategy extraction fails | Invalid configuration | Check strategy configuration |\n\n## Related Services\n\n- **[Gateway Service](../gateway/README.md)**: Expose APIs as tools for agents\n- **[Runtime Service](../runtime/README.md)**: Execute agents that generate conversation data\n- **[Identity Service](../identity/README.md)**: Secure access to conversation data\n- **[Observability Service](../observability/README.md)**: Monitor memory operations\n\n## References\n\n- [AWS Memory Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html)\n- [Memory Types](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory-types.html)\n- [Memory Strategies](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory-strategies.html)\n- [Building Context-Aware Agents Blog](https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-agentcore-memory-building-context-aware-agents/)\n- [Long-Term Memory Deep Dive](https://aws.amazon.com/blogs/machine-learning/building-smarter-ai-agents-agentcore-long-term-memory-deep-dive/)\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/observability/README.md": "# AgentCore Observability Service\n\n> **Status**: ✅ Available\n\n## Overview\n\nAmazon Bedrock AgentCore Observability helps developers trace, debug, and monitor agent performance in production through unified operational dashboards and OpenTelemetry-compatible telemetry.\n\n## Core Capabilities\n\n### Distributed Tracing\n- **End-to-End Tracing**: Complete request tracing across all AgentCore services\n- **Workflow Visualization**: Detailed step-by-step workflow execution views\n- **Service Dependencies**: Automatic mapping of service interactions\n- **Bottleneck Detection**: Identify performance bottlenecks in agent workflows\n- **Error Attribution**: Pinpoint exact failure points in complex workflows\n\n### Metrics and Monitoring\n- **Real-Time Metrics**: Live operational metrics for all agent activities\n- **Token Tracking**: Monitor token consumption and costs\n- **Latency Measurements**: Track P50, P95, P99 response times\n- **Session Monitoring**: Track session duration and status\n- **Error Rates**: Monitor error rates by service and operation\n- **Throughput**: Measure requests per second and operation counts\n\n### Logging\n- **Centralized Aggregation**: All service logs in one place\n- **Structured Logging**: Consistent log format with correlation IDs\n- **Search and Filter**: Query logs by service, operation, or time\n- **Real-Time Streaming**: Live log tailing for debugging\n- **Log Retention**: Configurable retention policies\n\n### Dashboards and Alerting\n- **Unified Dashboards**: Pre-built operational dashboards\n- **Custom Metrics**: Define and visualize custom metrics\n- **CloudWatch Integration**: Native AWS CloudWatch support\n- **Configurable Alerts**: Set up alerts for critical conditions\n- **Multi-Service Views**: Consolidated view across all services\n\n### OpenTelemetry Support\n- **Industry Standard**: Compatible with OpenTelemetry specification\n- **Tool Integration**: Works with existing observability tools\n- **Custom Instrumentation**: Add custom traces and metrics\n- **External Export**: Export telemetry to external systems\n\n## Use Cases\n\n### Production Debugging\nEnable teams to:\n- Debug agent execution issues in real-time\n- Identify root causes of failures quickly\n- Trace request flows across services\n- Analyze error patterns and trends\n\n### Performance Monitoring\nSupport scenarios like:\n- Monitor agent response times\n- Track token usage and costs\n- Identify slow operations\n- Optimize agent workflows\n\n### Behavior Analysis\nAllow teams to:\n- Analyze agent behavior patterns\n- Understand user interaction flows\n- Identify usage trends\n- Detect anomalies\n\n### Quality Assurance\nEnable teams to:\n- Ensure SLA compliance\n- Monitor service reliability\n- Track quality metrics\n- Validate performance standards\n\n### Capacity Planning\nSupport activities like:\n- Forecast resource needs\n- Identify scaling requirements\n- Optimize resource allocation\n- Plan for growth\n\n## Architecture\n\n### Observability Data Flow\n\n```\n┌─────────────────────────────────────────┐\n│  AgentCore Services                     │\n│  - Gateway, Runtime, Memory, etc.       │\n│  - Emit traces, logs, metrics           │\n└─────────────────────────────────────────┘\n           ↓\n┌─────────────────────────────────────────┐\n│  OpenTelemetry Collector                │\n│  - Receive telemetry data               │\n│  - Process and enrich                   │\n│  - Route to destinations                │\n└─────────────────────────────────────────┘\n           ↓\n    ┌──────┴──────┐\n    ↓             ↓\n┌─────────┐  ┌─────────────┐\n│CloudWatch│  │  X-Ray      │\n│ Logs    │  │  Traces     │\n│ Metrics │  │  Service Map│\n└─────────┘  └─────────────┘\n    ↓             ↓\n┌─────────────────────────────────────────┐\n│  Unified Dashboards                     │\n│  - Service health                       │\n│  - Performance metrics                  │\n│  - Error analysis                       │\n│  - Cost tracking                        │\n└─────────────────────────────────────────┘\n```\n\n### Data Collection Model\n\n1. **Automatic Instrumentation**: Built-in instrumentation for all services\n2. **Context Propagation**: Correlation IDs passed across service boundaries\n3. **Sampling**: Intelligent sampling for high-volume operations\n4. **Buffering**: Local buffering for reliability\n5. **Batch Export**: Efficient batch transmission to backends\n\n## Configuration\n\n### Enable Observability\n\n```bash\n# Enable observability for agent\naws bedrock-agentcore-control update-observability-config \\\n  --agent-id <AGENT_ID> \\\n  --config '{\n    \"tracing\": {\n      \"enabled\": true,\n      \"samplingRate\": 1.0\n    },\n    \"metrics\": {\n      \"enabled\": true,\n      \"interval\": 60\n    },\n    \"logging\": {\n      \"enabled\": true,\n      \"level\": \"INFO\"\n    }\n  }' \\\n  --region <REGION>\n```\n\n### Configure Sampling\n\n```bash\n# Set trace sampling rate\naws bedrock-agentcore-control update-tracing-config \\\n  --agent-id <AGENT_ID> \\\n  --sampling-rate 0.1 \\\n  --region <REGION>\n```\n\n### Custom Metrics\n\n```bash\n# Define custom metric\naws bedrock-agentcore-control create-custom-metric \\\n  --agent-id <AGENT_ID> \\\n  --metric-name \"CustomOperationCount\" \\\n  --metric-type \"Counter\" \\\n  --description \"Count of custom operations\" \\\n  --region <REGION>\n```\n\n## Traces\n\n### View Traces\n\n```bash\n# Query recent traces\naws xray get-trace-summaries \\\n  --start-time <START_TIMESTAMP> \\\n  --end-time <END_TIMESTAMP> \\\n  --filter-expression 'service(id(name: \"AgentCore\", type: \"AWS::Service\"))'\n```\n\n### Trace Details\n\n```bash\n# Get specific trace\naws xray batch-get-traces \\\n  --trace-ids <TRACE_ID_1> <TRACE_ID_2>\n```\n\n### Service Map\n\n```bash\n# Get service map\naws xray get-service-graph \\\n  --start-time <START_TIMESTAMP> \\\n  --end-time <END_TIMESTAMP>\n```\n\n## Metrics\n\n### Common Metrics\n\n**Gateway Metrics**:\n- `TargetInvocations`: Number of target invocations\n- `TargetErrors`: Number of target errors\n- `TargetLatency`: Target response latency\n\n**Runtime Metrics**:\n- `AgentExecutions`: Number of agent executions\n- `ExecutionDuration`: Agent execution duration\n- `ExecutionErrors`: Number of execution failures\n\n**Memory Metrics**:\n- `MemoryReads`: Number of memory read operations\n- `MemoryWrites`: Number of memory write operations\n- `MemorySize`: Total memory storage size\n\n**Token Metrics**:\n- `TokensConsumed`: Total tokens used\n- `TokenCost`: Estimated cost in dollars\n\n### Query Metrics\n\n```bash\n# Get metric statistics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/BedrockAgentCore \\\n  --metric-name TargetInvocations \\\n  --dimensions Name=AgentId,Value=<AGENT_ID> \\\n  --start-time <START> \\\n  --end-time <END> \\\n  --period 300 \\\n  --statistics Sum Average\n```\n\n### Custom Metrics\n\n```bash\n# Put custom metric data\naws cloudwatch put-metric-data \\\n  --namespace AgentCore/Custom \\\n  --metric-name CustomMetric \\\n  --value 1.0 \\\n  --dimensions AgentId=<AGENT_ID>\n```\n\n## Logs\n\n### Query Logs\n\n```bash\n# Tail agent logs\naws logs tail /aws/bedrock-agentcore/<AGENT_ID> \\\n  --follow \\\n  --format short\n\n# Query logs with filter\naws logs filter-log-events \\\n  --log-group-name /aws/bedrock-agentcore/<AGENT_ID> \\\n  --filter-pattern \"ERROR\" \\\n  --start-time <TIMESTAMP>\n```\n\n### Log Insights\n\n```bash\n# Run Insights query\naws logs start-query \\\n  --log-group-name /aws/bedrock-agentcore/<AGENT_ID> \\\n  --start-time <START_TIMESTAMP> \\\n  --end-time <END_TIMESTAMP> \\\n  --query-string 'fields @timestamp, @message\n    | filter @message like /ERROR/\n    | sort @timestamp desc\n    | limit 20'\n```\n\n## Dashboards\n\n### Create Dashboard\n\n```bash\n# Create CloudWatch dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name AgentCore-<AGENT_ID> \\\n  --dashboard-body file://dashboard-definition.json\n```\n\n### Dashboard Definition Example\n\n```json\n{\n  \"widgets\": [\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"metrics\": [\n          [\"AWS/BedrockAgentCore\", \"TargetInvocations\", {\"stat\": \"Sum\"}]\n        ],\n        \"period\": 300,\n        \"stat\": \"Sum\",\n        \"region\": \"us-west-2\",\n        \"title\": \"Target Invocations\"\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"metrics\": [\n          [\"AWS/BedrockAgentCore\", \"TargetErrors\", {\"stat\": \"Sum\"}]\n        ],\n        \"period\": 300,\n        \"stat\": \"Sum\",\n        \"region\": \"us-west-2\",\n        \"title\": \"Target Errors\"\n      }\n    }\n  ]\n}\n```\n\n## Alerting\n\n### Create Alarm\n\n```bash\n# Create CloudWatch alarm\naws cloudwatch put-metric-alarm \\\n  --alarm-name high-error-rate-<AGENT_ID> \\\n  --alarm-description \"Alert when error rate exceeds threshold\" \\\n  --metric-name TargetErrors \\\n  --namespace AWS/BedrockAgentCore \\\n  --statistic Sum \\\n  --period 300 \\\n  --evaluation-periods 2 \\\n  --threshold 10 \\\n  --comparison-operator GreaterThanThreshold \\\n  --dimensions Name=AgentId,Value=<AGENT_ID> \\\n  --alarm-actions <SNS_TOPIC_ARN>\n```\n\n### Alarm Templates\n\n**High Error Rate**:\n```bash\n# Alert on >5% error rate\naws cloudwatch put-metric-alarm \\\n  --alarm-name error-rate-high \\\n  --metric-name ErrorRate \\\n  --threshold 5 \\\n  --comparison-operator GreaterThanThreshold\n```\n\n**High Latency**:\n```bash\n# Alert on P95 latency >2s\naws cloudwatch put-metric-alarm \\\n  --alarm-name latency-high \\\n  --metric-name TargetLatency \\\n  --statistic p95 \\\n  --threshold 2000 \\\n  --comparison-operator GreaterThanThreshold\n```\n\n**High Token Usage**:\n```bash\n# Alert on excessive token usage\naws cloudwatch put-metric-alarm \\\n  --alarm-name tokens-high \\\n  --metric-name TokensConsumed \\\n  --statistic Sum \\\n  --threshold 1000000 \\\n  --comparison-operator GreaterThanThreshold\n```\n\n## Best Practices\n\n### Instrumentation\n- Enable observability for all production agents\n- Use appropriate sampling rates (1.0 for dev, 0.1 for prod)\n- Add custom metrics for business-critical operations\n- Include context in log messages\n- Use structured logging formats\n\n### Performance\n- Use appropriate metric aggregation periods\n- Implement metric sampling for high-volume operations\n- Set reasonable log retention periods\n- Use log filtering to reduce noise\n- Archive old traces and logs\n\n### Cost Optimization\n- Adjust sampling rates based on traffic\n- Use metric filters to create custom metrics\n- Set appropriate log retention (7-30 days)\n- Archive infrequently accessed data to S3\n- Use CloudWatch Insights for complex queries\n\n### Alerting\n- Define clear SLOs and SLIs\n- Set meaningful alert thresholds\n- Avoid alert fatigue with proper tuning\n- Use composite alarms for complex conditions\n- Implement escalation policies\n\n### Security\n- Encrypt logs and metrics at rest\n- Use IAM for access control\n- Implement least privilege access\n- Audit observability data access\n- Protect sensitive data in logs\n\n## Integration Patterns\n\n### With All Services\n\nObservability is automatically integrated with all AgentCore services:\n\n```\nGateway ──→ Observability\nRuntime ──→ Observability\nMemory ──→ Observability\nIdentity ──→ Observability\nCode Interpreter ──→ Observability\nBrowser ──→ Observability\n```\n\n### With External Tools\n\nExport telemetry to external observability platforms:\n\n```\nAgentCore Observability\n    ↓\nOpenTelemetry Collector\n    ↓\n┌────────┬────────┬────────┐\n│Datadog │New Relic│Grafana│\n└────────┴────────┴────────┘\n```\n\n## Troubleshooting\n\n### No Traces Appearing\n\n**Diagnosis**:\n```bash\n# Check if tracing is enabled\naws bedrock-agentcore-control get-observability-config \\\n  --agent-id <AGENT_ID>\n```\n\n**Solution**: Enable tracing in observability configuration\n\n### High Cardinality Metrics\n\n**Symptom**: Too many unique metric combinations\n**Solution**: Reduce dimension cardinality, use metric filters\n\n### Missing Logs\n\n**Diagnosis**:\n```bash\n# Check log group exists\naws logs describe-log-groups \\\n  --log-group-name-prefix /aws/bedrock-agentcore\n```\n\n**Solution**: Verify IAM permissions for CloudWatch Logs\n\n### High Costs\n\n**Symptom**: Excessive CloudWatch costs\n**Solution**: Adjust sampling rates, reduce log retention, archive old data\n\n## Performance Monitoring\n\n### Key Performance Indicators\n\n**Availability**:\n- Service uptime percentage\n- Error rate by service\n- Failed request percentage\n\n**Performance**:\n- P50, P95, P99 latency\n- Request throughput\n- Operation duration\n\n**Efficiency**:\n- Token consumption rate\n- Cost per operation\n- Resource utilization\n\n**Quality**:\n- Agent success rate\n- User satisfaction metrics\n- Workflow completion rate\n\n## Additional Resources\n\n- **AWS Documentation**: [Bedrock AgentCore Observability](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability.html)\n- **CloudWatch Guide**: [CloudWatch User Guide](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/)\n- **X-Ray Guide**: [AWS X-Ray Developer Guide](https://docs.aws.amazon.com/xray/latest/devguide/)\n- **OpenTelemetry**: [OpenTelemetry Documentation](https://opentelemetry.io/docs/)\n- **Best Practices**: [Observability Best Practices](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability-best-practices.html)\n\n---\n\n**Related Services**:\n- [Gateway Service](../gateway/README.md) - Gateway monitoring\n- [Runtime Service](../runtime/README.md) - Runtime tracing\n- [Memory Service](../memory/README.md) - Memory metrics\n- [Credential Management](../../cross-service/credential-management.md) - Cross-service credential patterns\n",
        "plugins/aws-agentic-ai/skills/aws-agentic-ai/services/runtime/README.md": "# Runtime Service\n\nThe Runtime service provides a secure, serverless hosting environment for deploying and running AI agents or tools. It handles scaling, session management, security isolation, and infrastructure management.\n\n## Key Features\n\n| Feature | Description |\n|---------|-------------|\n| **Framework Agnostic** | Works with LangGraph, Strands, CrewAI, or custom agents |\n| **Model Flexibility** | Supports any LLM (Bedrock, Claude, Gemini, OpenAI) |\n| **Protocol Support** | MCP (Model Context Protocol) and A2A (Agent to Agent) |\n| **Session Isolation** | Dedicated microVM per session with isolated CPU, memory, filesystem |\n| **Extended Execution** | Up to 8 hours for long-running workloads |\n| **100MB Payloads** | Handle multimodal content (text, images, audio, video) |\n| **Bidirectional Streaming** | HTTP API and WebSocket for real-time interactions |\n\n## Quick Start\n\n### Prerequisites\n- AWS CLI configured with appropriate permissions\n- Docker installed for container builds\n- Python 3.9+ for SDK usage\n\n### Deploy an Agent\n\n**Step 1: Install AgentCore SDK**\n```bash\npip install bedrock-agentcore\n```\n\n**Step 2: Create agent code**\n```python\nfrom bedrock_agentcore.runtime import BedrockAgentCoreApp\n\napp = BedrockAgentCoreApp()\n\n@app.handler()\nasync def handle_request(request, context):\n    user_input = request.get(\"input\", \"\")\n    # Your agent logic here\n    return {\"response\": f\"Processed: {user_input}\"}\n```\n\n**Step 3: Create AgentCore Runtime**\n```bash\naws bedrock-agentcore-control create-agent-runtime \\\n  --agent-runtime-name my-agent \\\n  --runtime-artifact '{\"containerConfiguration\": {\"containerUri\": \"<ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com/my-agent:latest\"}}' \\\n  --role-arn arn:aws:iam::<ACCOUNT_ID>:role/AgentRuntimeExecutionRole \\\n  --network-configuration '{\"networkMode\": \"PUBLIC\"}' \\\n  --region us-west-2\n```\n\n**Step 4: Invoke agent**\n```bash\naws bedrock-agentcore-runtime invoke-agent-runtime \\\n  --agent-runtime-endpoint-arn arn:aws:bedrock-agentcore:us-west-2:<ACCOUNT_ID>:runtime/my-agent/endpoint/DEFAULT \\\n  --payload '{\"input\": \"Hello, agent!\"}' \\\n  --region us-west-2\n```\n\n## Core Components\n\n### AgentCore Runtime\nContainerized application hosting your AI agent or tool code. Each runtime:\n- Has a unique identity\n- Is versioned for controlled deployment and updates\n- Can use popular frameworks or custom implementations\n\n### Versions\nImmutable snapshots of configuration:\n- Version 1 (V1) created automatically with new runtime\n- Each update creates a new version\n- Enables rollback capabilities\n\n### Endpoints\nAddressable access points to runtime versions:\n- **DEFAULT**: Automatically created, points to latest version\n- Custom endpoints for different environments (dev, test, prod)\n- Unique ARN for invocation\n\nEndpoint states: `CREATING` → `READY` (or `CREATE_FAILED`) → `UPDATING` → `READY`\n\n### Sessions\nIndividual interaction contexts with complete isolation:\n- Dedicated microVM per session\n- Preserves context across interactions\n- Persists up to 8 hours\n- Auto-terminates after 15 minutes idle\n\nSession states: `Active` → `Idle` → `Terminated`\n\n## Authentication\n\n### Inbound (Who Can Access Your Agent)\n\n| Method | Description |\n|--------|-------------|\n| **IAM (SigV4)** | AWS credentials for identity verification |\n| **OAuth 2.0** | External identity providers (Cognito, Okta, Entra ID) |\n\n**OAuth Flow**:\n1. User authenticates with identity provider\n2. Client receives bearer token\n3. Token passed in authorization header\n4. Runtime validates token\n5. Request processed or rejected\n\n### Outbound (Accessing External Services)\n\n| Method | Use Case |\n|--------|----------|\n| **OAuth** | Services supporting OAuth flows |\n| **API Keys** | Key-based authentication |\n\n**Modes**:\n- **User-delegated**: Acting on behalf of end user\n- **Autonomous**: Acting with service-level credentials\n\n## Common Operations\n\n### List Agent Runtimes\n```bash\naws bedrock-agentcore-control list-agent-runtimes \\\n  --region us-west-2\n```\n\n### Get Runtime Details\n```bash\naws bedrock-agentcore-control get-agent-runtime \\\n  --agent-runtime-id <RUNTIME_ID> \\\n  --region us-west-2\n```\n\n### Update Runtime\n```bash\naws bedrock-agentcore-control update-agent-runtime \\\n  --agent-runtime-id <RUNTIME_ID> \\\n  --runtime-artifact '{\"containerConfiguration\": {\"containerUri\": \"<NEW_IMAGE_URI>\"}}' \\\n  --region us-west-2\n```\n\n### Delete Runtime\n```bash\naws bedrock-agentcore-control delete-agent-runtime \\\n  --agent-runtime-id <RUNTIME_ID> \\\n  --region us-west-2\n```\n\n## Long-Running Agents\n\nFor workloads exceeding request/response cycles (up to 8 hours):\n\n```python\nfrom bedrock_agentcore.runtime import BedrockAgentCoreApp\n\napp = BedrockAgentCoreApp()\n\n@app.handler()\nasync def handle_request(request, context):\n    # Add async task\n    task_id = context.add_async_task(\"background-processing\")\n\n    # Start background work\n    # ... long-running operation ...\n\n    # Complete task when done\n    context.complete_async_task(task_id)\n\n    return {\"status\": \"Task started\", \"task_id\": task_id}\n```\n\n## Streaming Responses\n\nEnable real-time partial results:\n\n```python\n@app.handler()\nasync def handle_request(request, context):\n    async for chunk in generate_response(request):\n        yield {\"partial\": chunk}\n    yield {\"complete\": True}\n```\n\n## Supported Frameworks\n\n| Framework | Description |\n|-----------|-------------|\n| **LangGraph** | Graph-based agent workflows |\n| **Strands** | AWS-native agent framework |\n| **CrewAI** | Multi-agent collaboration |\n| **Custom** | Any Python-based agent |\n\n## Troubleshooting\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| 504 Gateway Timeout | Container issues, ARM64 compatibility | Ensure container exposes port 8080, use ARM64 image |\n| 403 AccessDeniedException | Missing permissions | Verify IAM role and policies |\n| exec format error | Wrong architecture | Build ARM64 containers with buildx |\n| Session terminated after 15min | Idle timeout | Implement ping handler with HEALTHY_BUSY status |\n\n## Related Services\n\n- **[Gateway Service](../gateway/README.md)**: Expose APIs as tools for agents\n- **[Memory Service](../memory/README.md)**: Store agent conversation history\n- **[Identity Service](../identity/README.md)**: Manage agent credentials\n- **[Observability Service](../observability/README.md)**: Monitor agent performance\n\n## References\n\n- [AWS Runtime Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agents-tools-runtime.html)\n- [How Runtime Works](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-how-it-works.html)\n- [Runtime Troubleshooting](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-troubleshooting.html)\n- [Runtime API Reference](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/)\n",
        "plugins/aws-cdk/skills/aws-cdk-development/SKILL.md": "---\nname: aws-cdk-development\ndescription: AWS Cloud Development Kit (CDK) expert for building cloud infrastructure with TypeScript/Python. Use when creating CDK stacks, defining CDK constructs, implementing infrastructure as code, or when the user mentions CDK, CloudFormation, IaC, cdk synth, cdk deploy, or wants to define AWS infrastructure programmatically. Covers CDK app structure, construct patterns, stack composition, and deployment workflows.\ncontext: fork\nskills:\n  - aws-mcp-setup\nallowed-tools:\n  - mcp__cdk__*\n  - mcp__aws-mcp__*\n  - mcp__awsdocs__*\n  - Bash(cdk *)\n  - Bash(npm *)\n  - Bash(npx *)\n  - Bash(aws cloudformation *)\n  - Bash(aws sts get-caller-identity)\nhooks:\n  PreToolUse:\n    - matcher: Bash(cdk deploy*)\n      command: aws sts get-caller-identity --query Account --output text\n      once: true\n---\n\n# AWS CDK Development\n\nThis skill provides comprehensive guidance for developing AWS infrastructure using the Cloud Development Kit (CDK), with integrated MCP servers for accessing latest AWS knowledge and CDK utilities.\n\n## AWS Documentation Requirement\n\n**CRITICAL**: This skill requires AWS MCP tools for accurate, up-to-date AWS information.\n\n### Before Answering AWS Questions\n\n1. **Always verify** using AWS MCP tools (if available):\n   - `mcp__aws-mcp__aws___search_documentation` or `mcp__*awsdocs*__aws___search_documentation` - Search AWS docs\n   - `mcp__aws-mcp__aws___read_documentation` or `mcp__*awsdocs*__aws___read_documentation` - Read specific pages\n   - `mcp__aws-mcp__aws___get_regional_availability` - Check service availability\n\n2. **If AWS MCP tools are unavailable**:\n   - Guide user to configure AWS MCP using the `aws-mcp-setup` skill (auto-loaded as dependency)\n   - Help determine which option fits their environment:\n     - Has uvx + AWS credentials → Full AWS MCP Server\n     - No Python/credentials → AWS Documentation MCP (no auth)\n   - If cannot determine → Ask user which option to use\n\n## Integrated MCP Servers\n\nThis skill includes the CDK MCP server automatically configured with the plugin:\n\n### AWS CDK MCP Server\n**When to use**: For CDK-specific guidance and utilities\n- Get CDK construct recommendations\n- Retrieve CDK best practices\n- Access CDK pattern suggestions\n- Validate CDK configurations\n- Get help with CDK-specific APIs\n\n**Important**: Leverage this server for CDK construct guidance and advanced CDK operations.\n\n## When to Use This Skill\n\nUse this skill when:\n- Creating new CDK stacks or constructs\n- Refactoring existing CDK infrastructure\n- Implementing Lambda functions within CDK\n- Following AWS CDK best practices\n- Validating CDK stack configurations before deployment\n- Verifying AWS service capabilities and regional availability\n\n## Core CDK Principles\n\n### Resource Naming\n\n**CRITICAL**: Do NOT explicitly specify resource names when they are optional in CDK constructs.\n\n**Why**: CDK-generated names enable:\n- **Reusable patterns**: Deploy the same construct/pattern multiple times without conflicts\n- **Parallel deployments**: Multiple stacks can deploy simultaneously in the same region\n- **Cleaner shared logic**: Patterns and shared code can be initialized multiple times without name collision\n- **Stack isolation**: Each stack gets uniquely identified resources automatically\n\n**Pattern**: Let CDK generate unique names automatically using CloudFormation's naming mechanism.\n\n```typescript\n// ❌ BAD - Explicit naming prevents reusability and parallel deployments\nnew lambda.Function(this, 'MyFunction', {\n  functionName: 'my-lambda',  // Avoid this\n  // ...\n});\n\n// ✅ GOOD - Let CDK generate unique names\nnew lambda.Function(this, 'MyFunction', {\n  // No functionName specified - CDK generates: StackName-MyFunctionXXXXXX\n  // ...\n});\n```\n\n**Security Note**: For different environments (dev, staging, prod), follow AWS Security Pillar best practices by using separate AWS accounts rather than relying on resource naming within a single account. Account-level isolation provides stronger security boundaries.\n\n### Lambda Function Development\n\nUse the appropriate Lambda construct based on runtime:\n\n**TypeScript/JavaScript**: Use `@aws-cdk/aws-lambda-nodejs`\n```typescript\nimport { NodejsFunction } from 'aws-cdk-lib/aws-lambda-nodejs';\n\nnew NodejsFunction(this, 'MyFunction', {\n  entry: 'lambda/handler.ts',\n  handler: 'handler',\n  // Automatically handles bundling, dependencies, and transpilation\n});\n```\n\n**Python**: Use `@aws-cdk/aws-lambda-python`\n```typescript\nimport { PythonFunction } from '@aws-cdk/aws-lambda-python-alpha';\n\nnew PythonFunction(this, 'MyFunction', {\n  entry: 'lambda',\n  index: 'handler.py',\n  handler: 'handler',\n  // Automatically handles dependencies and packaging\n});\n```\n\n**Benefits**:\n- Automatic bundling and dependency management\n- Transpilation handled automatically\n- No manual packaging required\n- Consistent deployment patterns\n\n### Pre-Deployment Validation\n\nUse a **multi-layer validation strategy** for comprehensive CDK quality checks:\n\n#### Layer 1: Real-Time IDE Feedback (Recommended)\n\n**For TypeScript/JavaScript projects**:\n\nInstall [cdk-nag](https://github.com/cdklabs/cdk-nag) for synthesis-time validation:\n```bash\nnpm install --save-dev cdk-nag\n```\n\nAdd to your CDK app:\n```typescript\nimport { Aspects } from 'aws-cdk-lib';\nimport { AwsSolutionsChecks } from 'cdk-nag';\n\nconst app = new App();\nAspects.of(app).add(new AwsSolutionsChecks());\n```\n\n**Optional - VS Code users**: Install [CDK NAG Validator extension](https://marketplace.visualstudio.com/items?itemName=alphacrack.cdk-nag-validator) for faster feedback on file save.\n\n**For Python/Java/C#/Go projects**: cdk-nag is available in all CDK languages and provides the same synthesis-time validation.\n\n#### Layer 2: Synthesis-Time Validation (Required)\n\n1. **Synthesis with cdk-nag**: Validate stack with comprehensive rules\n   ```bash\n   cdk synth  # cdk-nag runs automatically via Aspects\n   ```\n\n2. **Suppress legitimate exceptions** with documented reasons:\n   ```typescript\n   import { NagSuppressions } from 'cdk-nag';\n\n   // Document WHY the exception is needed\n   NagSuppressions.addResourceSuppressions(resource, [\n     {\n       id: 'AwsSolutions-L1',\n       reason: 'Lambda@Edge requires specific runtime for CloudFront compatibility'\n     }\n   ]);\n   ```\n\n#### Layer 3: Pre-Commit Safety Net\n\n1. **Build**: Ensure compilation succeeds\n   ```bash\n   npm run build  # or language-specific build command\n   ```\n\n2. **Tests**: Run unit and integration tests\n   ```bash\n   npm test  # or pytest, mvn test, etc.\n   ```\n\n3. **Validation Script**: Meta-level checks\n   ```bash\n   ./scripts/validate-stack.sh\n   ```\n\nThe validation script now focuses on:\n- Language detection\n- Template size and resource count analysis\n- Synthesis success verification\n- (Note: Detailed anti-pattern checks are handled by cdk-nag)\n\n## Workflow Guidelines\n\n### Development Workflow\n\n1. **Design**: Plan infrastructure resources and relationships\n2. **Verify AWS Services**: Use AWS Documentation MCP to confirm service availability and features\n   - Check regional availability for all required services\n   - Verify service limits and quotas\n   - Confirm latest API specifications\n3. **Implement**: Write CDK constructs following best practices\n   - Use CDK MCP server for construct recommendations\n   - Reference CDK best practices via MCP tools\n4. **Validate**: Run pre-deployment checks (see above)\n5. **Synthesize**: Generate CloudFormation templates\n6. **Review**: Examine synthesized templates for correctness\n7. **Deploy**: Deploy to target environment\n8. **Verify**: Confirm resources are created correctly\n\n### Stack Organization\n\n- Use nested stacks for complex applications\n- Separate concerns into logical construct boundaries\n- Export values that other stacks may need\n- Use CDK context for environment-specific configuration\n\n### Testing Strategy\n\n- Unit test individual constructs\n- Integration test stack synthesis\n- Snapshot test CloudFormation templates\n- Validate resource properties and relationships\n\n## Using MCP Servers Effectively\n\n### When to Use AWS Documentation MCP\n\n**Always verify before implementing**:\n- New AWS service features or configurations\n- Service availability in target regions\n- API parameter specifications\n- Service limits and quotas\n- Security best practices for AWS services\n\n**Example scenarios**:\n- \"Check if Lambda supports Python 3.13 runtime\"\n- \"Verify DynamoDB is available in eu-south-2\"\n- \"What are the current Lambda timeout limits?\"\n- \"Get latest S3 encryption options\"\n\n### When to Use CDK MCP Server\n\n**Leverage for CDK-specific guidance**:\n- CDK construct selection and usage\n- CDK API parameter options\n- CDK best practice patterns\n- Construct property configurations\n- CDK-specific optimizations\n\n**Example scenarios**:\n- \"What's the recommended CDK construct for API Gateway REST API?\"\n- \"How to configure NodejsFunction bundling options?\"\n- \"Best practices for CDK stack organization\"\n- \"CDK construct for DynamoDB with auto-scaling\"\n\n### MCP Usage Best Practices\n\n1. **Verify First**: Always check AWS Documentation MCP before implementing new features\n2. **Regional Validation**: Check service availability in target deployment regions\n3. **CDK Guidance**: Use CDK MCP for construct-specific recommendations\n4. **Stay Current**: MCP servers provide latest information beyond knowledge cutoff\n5. **Combine Sources**: Use both skill patterns and MCP servers for comprehensive guidance\n\n## CDK Patterns Reference\n\nFor detailed CDK patterns, anti-patterns, and architectural guidance, refer to the comprehensive reference:\n\n**File**: `references/cdk-patterns.md`\n\nThis reference includes:\n- Common CDK patterns and their use cases\n- Anti-patterns to avoid\n- Security best practices\n- Cost optimization strategies\n- Performance considerations\n\n## Additional Resources\n\n- **Validation Script**: `scripts/validate-stack.sh` - Pre-deployment validation\n- **CDK Patterns**: `references/cdk-patterns.md` - Detailed pattern library\n- **AWS Documentation MCP**: Integrated for latest AWS information\n- **CDK MCP Server**: Integrated for CDK-specific guidance\n\n## GitHub Actions Integration\n\nWhen GitHub Actions workflow files exist in the repository, ensure all checks defined in `.github/workflows/` pass before committing. This prevents CI/CD failures and maintains code quality standards.\n",
        "plugins/aws-cdk/skills/aws-cdk-development/references/cdk-patterns.md": "# AWS CDK Patterns and Best Practices\n\nThis reference provides detailed patterns, anti-patterns, and best practices for AWS CDK development.\n\n## Table of Contents\n\n- [Naming Conventions](#naming-conventions)\n- [Construct Patterns](#construct-patterns)\n- [Security Patterns](#security-patterns)\n- [Lambda Integration](#lambda-integration)\n- [Testing Patterns](#testing-patterns)\n- [Cost Optimization](#cost-optimization)\n- [Anti-Patterns](#anti-patterns)\n\n## Naming Conventions\n\n### Automatic Resource Naming (Recommended)\n\nLet CDK and CloudFormation generate unique resource names automatically:\n\n**Benefits**:\n- Enables multiple deployments in the same region/account\n- Supports parallel environments (dev, staging, prod)\n- Prevents naming conflicts\n- Allows stack cloning and testing\n\n**Example**:\n```typescript\n// ✅ GOOD - Automatic naming\nconst bucket = new s3.Bucket(this, 'DataBucket', {\n  // No bucketName specified\n  encryption: s3.BucketEncryption.S3_MANAGED,\n});\n```\n\n### When Explicit Naming is Required\n\nSome scenarios require explicit names:\n- Resources referenced by external systems\n- Resources that must maintain consistent names across deployments\n- Cross-stack references requiring stable names\n\n**Pattern**: Use logical prefixes and environment suffixes\n```typescript\n// Only when absolutely necessary\nconst bucket = new s3.Bucket(this, 'DataBucket', {\n  bucketName: `${props.projectName}-data-${props.environment}`,\n});\n```\n\n## Construct Patterns\n\n### L3 Constructs (Patterns)\n\nPrefer high-level patterns that encapsulate best practices:\n\n```typescript\nimport * as patterns from 'aws-cdk-lib/aws-apigateway';\n\nnew patterns.LambdaRestApi(this, 'MyApi', {\n  handler: myFunction,\n  // Includes CloudWatch Logs, IAM roles, and API Gateway configuration\n});\n```\n\n### Custom Constructs\n\nCreate reusable constructs for repeated patterns:\n\n```typescript\nexport class ApiWithDatabase extends Construct {\n  public readonly api: apigateway.RestApi;\n  public readonly table: dynamodb.Table;\n\n  constructor(scope: Construct, id: string, props: ApiWithDatabaseProps) {\n    super(scope, id);\n\n    this.table = new dynamodb.Table(this, 'Table', {\n      partitionKey: { name: 'id', type: dynamodb.AttributeType.STRING },\n      billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,\n    });\n\n    const handler = new NodejsFunction(this, 'Handler', {\n      entry: props.handlerEntry,\n      environment: {\n        TABLE_NAME: this.table.tableName,\n      },\n    });\n\n    this.table.grantReadWriteData(handler);\n\n    this.api = new apigateway.LambdaRestApi(this, 'Api', {\n      handler,\n    });\n  }\n}\n```\n\n## Security Patterns\n\n### IAM Least Privilege\n\nUse grant methods instead of broad policies:\n\n```typescript\n// ✅ GOOD - Specific grants\nconst table = new dynamodb.Table(this, 'Table', { /* ... */ });\nconst lambda = new lambda.Function(this, 'Function', { /* ... */ });\n\ntable.grantReadWriteData(lambda);\n\n// ❌ BAD - Overly broad permissions\nlambda.addToRolePolicy(new iam.PolicyStatement({\n  actions: ['dynamodb:*'],\n  resources: ['*'],\n}));\n```\n\n### Secrets Management\n\nUse Secrets Manager for sensitive data:\n\n```typescript\nimport * as secretsmanager from 'aws-cdk-lib/aws-secretsmanager';\n\nconst secret = new secretsmanager.Secret(this, 'DbPassword', {\n  generateSecretString: {\n    secretStringTemplate: JSON.stringify({ username: 'admin' }),\n    generateStringKey: 'password',\n    excludePunctuation: true,\n  },\n});\n\n// Grant read access to Lambda\nsecret.grantRead(myFunction);\n```\n\n### VPC Configuration\n\nFollow VPC best practices:\n\n```typescript\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 2,\n  natGateways: 1, // Cost optimization: use 1 for dev, 2+ for prod\n  subnetConfiguration: [\n    {\n      name: 'Public',\n      subnetType: ec2.SubnetType.PUBLIC,\n      cidrMask: 24,\n    },\n    {\n      name: 'Private',\n      subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS,\n      cidrMask: 24,\n    },\n    {\n      name: 'Isolated',\n      subnetType: ec2.SubnetType.PRIVATE_ISOLATED,\n      cidrMask: 24,\n    },\n  ],\n});\n```\n\n## Lambda Integration\n\n### NodejsFunction (TypeScript/JavaScript)\n\n```typescript\nimport { NodejsFunction } from 'aws-cdk-lib/aws-lambda-nodejs';\n\nconst fn = new NodejsFunction(this, 'Function', {\n  entry: 'src/handlers/process.ts',\n  handler: 'handler',\n  runtime: lambda.Runtime.NODEJS_20_X,\n  timeout: Duration.seconds(30),\n  memorySize: 512,\n  environment: {\n    TABLE_NAME: table.tableName,\n  },\n  bundling: {\n    minify: true,\n    sourceMap: true,\n    externalModules: ['@aws-sdk/*'], // Use AWS SDK from Lambda runtime\n  },\n});\n```\n\n### PythonFunction\n\n```typescript\nimport { PythonFunction } from '@aws-cdk/aws-lambda-python-alpha';\n\nconst fn = new PythonFunction(this, 'Function', {\n  entry: 'src/handlers',\n  index: 'process.py',\n  handler: 'handler',\n  runtime: lambda.Runtime.PYTHON_3_12,\n  timeout: Duration.seconds(30),\n  memorySize: 512,\n});\n```\n\n### Lambda Layers\n\nShare code across functions:\n\n```typescript\nconst layer = new lambda.LayerVersion(this, 'CommonLayer', {\n  code: lambda.Code.fromAsset('layers/common'),\n  compatibleRuntimes: [lambda.Runtime.NODEJS_20_X],\n  description: 'Common utilities',\n});\n\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  layers: [layer],\n});\n```\n\n## Testing Patterns\n\n### Snapshot Testing\n\n```typescript\nimport { Template } from 'aws-cdk-lib/assertions';\n\ntest('Stack creates expected resources', () => {\n  const app = new cdk.App();\n  const stack = new MyStack(app, 'TestStack');\n\n  const template = Template.fromStack(stack);\n  expect(template.toJSON()).toMatchSnapshot();\n});\n```\n\n### Fine-Grained Assertions\n\n```typescript\ntest('Lambda has correct environment', () => {\n  const app = new cdk.App();\n  const stack = new MyStack(app, 'TestStack');\n\n  const template = Template.fromStack(stack);\n\n  template.hasResourceProperties('AWS::Lambda::Function', {\n    Runtime: 'nodejs20.x',\n    Timeout: 30,\n    Environment: {\n      Variables: {\n        TABLE_NAME: { Ref: Match.anyValue() },\n      },\n    },\n  });\n});\n```\n\n### Resource Count Validation\n\n```typescript\ntest('Stack has correct number of functions', () => {\n  const app = new cdk.App();\n  const stack = new MyStack(app, 'TestStack');\n\n  const template = Template.fromStack(stack);\n  template.resourceCountIs('AWS::Lambda::Function', 3);\n});\n```\n\n## Cost Optimization\n\n### Right-Sizing Lambda\n\n```typescript\n// Development\nconst devFunction = new NodejsFunction(this, 'DevFunction', {\n  memorySize: 256, // Lower for dev\n  timeout: Duration.seconds(30),\n});\n\n// Production\nconst prodFunction = new NodejsFunction(this, 'ProdFunction', {\n  memorySize: 1024, // Higher for prod performance\n  timeout: Duration.seconds(10),\n  reservedConcurrentExecutions: 10, // Prevent runaway costs\n});\n```\n\n### DynamoDB Billing Modes\n\n```typescript\n// Development/Low Traffic\nconst devTable = new dynamodb.Table(this, 'DevTable', {\n  billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,\n});\n\n// Production/Predictable Load\nconst prodTable = new dynamodb.Table(this, 'ProdTable', {\n  billingMode: dynamodb.BillingMode.PROVISIONED,\n  readCapacity: 5,\n  writeCapacity: 5,\n  autoScaling: { /* ... */ },\n});\n```\n\n### S3 Lifecycle Policies\n\n```typescript\nconst bucket = new s3.Bucket(this, 'DataBucket', {\n  lifecycleRules: [\n    {\n      id: 'MoveToIA',\n      transitions: [\n        {\n          storageClass: s3.StorageClass.INFREQUENT_ACCESS,\n          transitionAfter: Duration.days(30),\n        },\n        {\n          storageClass: s3.StorageClass.GLACIER,\n          transitionAfter: Duration.days(90),\n        },\n      ],\n    },\n    {\n      id: 'CleanupOldVersions',\n      noncurrentVersionExpiration: Duration.days(30),\n    },\n  ],\n});\n```\n\n## Anti-Patterns\n\n### ❌ Hardcoded Values\n\n```typescript\n// BAD\nnew lambda.Function(this, 'Function', {\n  functionName: 'my-function', // Prevents multiple deployments\n  code: lambda.Code.fromAsset('lambda'),\n  handler: 'index.handler',\n  runtime: lambda.Runtime.NODEJS_20_X,\n});\n\n// GOOD\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  // Let CDK generate the name\n});\n```\n\n### ❌ Overly Broad IAM Permissions\n\n```typescript\n// BAD\nfunction.addToRolePolicy(new iam.PolicyStatement({\n  actions: ['*'],\n  resources: ['*'],\n}));\n\n// GOOD\ntable.grantReadWriteData(function);\n```\n\n### ❌ Manual Dependency Management\n\n```typescript\n// BAD - Manual bundling\nnew lambda.Function(this, 'Function', {\n  code: lambda.Code.fromAsset('lambda.zip'), // Pre-bundled manually\n  // ...\n});\n\n// GOOD - Let CDK handle it\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  // CDK handles bundling automatically\n});\n```\n\n### ❌ Missing Environment Variables\n\n```typescript\n// BAD\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  // Table name hardcoded in Lambda code\n});\n\n// GOOD\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  environment: {\n    TABLE_NAME: table.tableName,\n  },\n});\n```\n\n### ❌ Ignoring Stack Outputs\n\n```typescript\n// BAD - No way to reference resources\nnew MyStack(app, 'Stack', {});\n\n// GOOD - Export important values\nclass MyStack extends Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    const api = new apigateway.RestApi(this, 'Api', {});\n\n    new CfnOutput(this, 'ApiUrl', {\n      value: api.url,\n      description: 'API Gateway URL',\n      exportName: 'MyApiUrl',\n    });\n  }\n}\n```\n\n## Summary\n\n- **Always** let CDK generate resource names unless explicitly required\n- **Use** high-level constructs (L2/L3) over low-level (L1)\n- **Prefer** grant methods for IAM permissions\n- **Leverage** `NodejsFunction` and `PythonFunction` for automatic bundling\n- **Test** stacks with assertions and snapshots\n- **Optimize** costs based on environment (dev vs prod)\n- **Validate** infrastructure before deployment\n- **Document** custom constructs and patterns\n",
        "plugins/aws-common/skills/aws-mcp-setup/SKILL.md": "---\nname: aws-mcp-setup\ndescription: Configure AWS Documentation MCP server to query up-to-date AWS knowledge, APIs, and best practices\n---\n\n# AWS MCP Server Configuration Guide\n\n## Overview\n\nThis guide helps you configure AWS MCP tools for AI agents. Two options are available:\n\n| Option | Requirements | Capabilities |\n|--------|--------------|--------------|\n| **Full AWS MCP Server** | Python 3.10+, uvx, AWS credentials | Execute AWS API calls + documentation search |\n| **AWS Documentation MCP** | None | Documentation search only |\n\n## Step 1: Check Existing Configuration\n\nBefore configuring, check if AWS MCP tools are already available using either method:\n\n### Method A: Check Available Tools (Recommended)\n\nLook for these tool name patterns in your agent's available tools:\n- `mcp__aws-mcp__*` or `mcp__aws__*` → Full AWS MCP Server configured\n- `mcp__*awsdocs*__aws___*` → AWS Documentation MCP configured\n\n**How to check**: Run `/mcp` command to list all active MCP servers.\n\n### Method B: Check Configuration Files\n\nAgent tools use hierarchical configuration (precedence: local → project → user → enterprise):\n\n| Scope | File Location | Use Case |\n|-------|---------------|----------|\n| Local | `.claude.json` (in project) | Personal/experimental |\n| Project | `.mcp.json` (project root) | Team-shared |\n| User | `~/.claude.json` | Cross-project personal |\n| Enterprise | System managed directories | Organization-wide |\n\nCheck these files for `mcpServers` containing `aws-mcp`, `aws`, or `awsdocs` keys:\n\n```bash\n# Check project config\ncat .mcp.json 2>/dev/null | grep -E '\"(aws-mcp|aws|awsdocs)\"'\n\n# Check user config\ncat ~/.claude.json 2>/dev/null | grep -E '\"(aws-mcp|aws|awsdocs)\"'\n\n# Or use Claude CLI\nclaude mcp list\n```\n\nIf AWS MCP is already configured, no further setup needed.\n\n## Step 2: Choose Configuration Method\n\n### Automatic Detection\n\nRun these commands to determine which option to use:\n\n```bash\n# Check for uvx (requires Python 3.10+)\nwhich uvx || echo \"uvx not available\"\n\n# Check for valid AWS credentials\naws sts get-caller-identity || echo \"AWS credentials not configured\"\n```\n\n### Option A: Full AWS MCP Server (Recommended)\n\n**Use when**: uvx available AND AWS credentials valid\n\n**Prerequisites**:\n- Python 3.10+ with `uv` package manager\n- AWS credentials configured (via profile, environment variables, or IAM role)\n\n**Required IAM Permissions**:\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Action\": [\n      \"aws-mcp:InvokeMCP\",\n      \"aws-mcp:CallReadOnlyTool\",\n      \"aws-mcp:CallReadWriteTool\"\n    ],\n    \"Resource\": \"*\"\n  }]\n}\n```\n\n**Configuration** (add to your MCP settings):\n```json\n{\n  \"mcpServers\": {\n    \"aws-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"mcp-proxy-for-aws@latest\",\n        \"https://aws-mcp.us-east-1.api.aws/mcp\",\n        \"--metadata\", \"AWS_REGION=us-west-2\"\n      ]\n    }\n  }\n}\n```\n\n**Credential Configuration Options**:\n\n1. **AWS Profile** (recommended for development):\n   ```json\n   \"args\": [\n     \"mcp-proxy-for-aws@latest\",\n     \"https://aws-mcp.us-east-1.api.aws/mcp\",\n     \"--profile\", \"my-profile\",\n     \"--metadata\", \"AWS_REGION=us-west-2\"\n   ]\n   ```\n\n2. **Environment Variables**:\n   ```json\n   \"env\": {\n     \"AWS_ACCESS_KEY_ID\": \"...\",\n     \"AWS_SECRET_ACCESS_KEY\": \"...\",\n     \"AWS_REGION\": \"us-west-2\"\n   }\n   ```\n\n3. **IAM Role** (for EC2/ECS/Lambda): No additional config needed - uses instance credentials\n\n**Additional Options**:\n- `--region <region>`: Override AWS region\n- `--read-only`: Restrict to read-only tools\n- `--log-level <level>`: Set logging level (debug, info, warning, error)\n\n**Reference**: https://github.com/aws/mcp-proxy-for-aws\n\n### Option B: AWS Documentation MCP Server (No Auth)\n\n**Use when**:\n- No Python/uvx environment\n- No AWS credentials\n- Only need documentation search (no API execution)\n\n**Configuration**:\n```json\n{\n  \"mcpServers\": {\n    \"awsdocs\": {\n      \"type\": \"http\",\n      \"url\": \"https://knowledge-mcp.global.api.aws\"\n    }\n  }\n}\n```\n\n## Step 3: Verification\n\nAfter configuration, verify tools are available:\n\n**For Full AWS MCP**:\n- Look for tools: `mcp__aws-mcp__aws___search_documentation`, `mcp__aws-mcp__aws___call_aws`\n\n**For Documentation MCP**:\n- Look for tools: `mcp__awsdocs__aws___search_documentation`, `mcp__awsdocs__aws___read_documentation`\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| `uvx: command not found` | uv not installed | Install with `pip install uv` or use Option B |\n| `AccessDenied` error | Missing IAM permissions | Add aws-mcp:* permissions to IAM policy |\n| `InvalidSignatureException` | Credential issue | Check `aws sts get-caller-identity` |\n| Tools not appearing | MCP not started | Restart your agent after config change |\n",
        "plugins/aws-cost-ops/skills/aws-cost-operations/SKILL.md": "---\nname: aws-cost-operations\ndescription: This skill provides AWS cost optimization, monitoring, and operational best practices with integrated MCP servers for billing analysis, cost estimation, observability, and security assessment.\nskills:\n  - aws-mcp-setup\nallowed-tools:\n  - mcp__pricing__*\n  - mcp__costexp__*\n  - mcp__cw__*\n  - mcp__aws-mcp__*\n  - mcp__awsdocs__*\n  - Bash(aws ce *)\n  - Bash(aws cloudwatch *)\n  - Bash(aws logs *)\n  - Bash(aws budgets *)\n  - Bash(aws cloudtrail *)\n  - Bash(aws sts get-caller-identity)\nhooks:\n  PreToolUse:\n    - matcher: Bash(aws ce *)\n      command: aws sts get-caller-identity --query Account --output text\n      once: true\n---\n\n# AWS Cost & Operations\n\nThis skill provides comprehensive guidance for AWS cost optimization, monitoring, observability, and operational excellence with integrated MCP servers.\n\n## AWS Documentation Requirement\n\n**CRITICAL**: This skill requires AWS MCP tools for accurate, up-to-date AWS information.\n\n### Before Answering AWS Questions\n\n1. **Always verify** using AWS MCP tools (if available):\n   - `mcp__aws-mcp__aws___search_documentation` or `mcp__*awsdocs*__aws___search_documentation` - Search AWS docs\n   - `mcp__aws-mcp__aws___read_documentation` or `mcp__*awsdocs*__aws___read_documentation` - Read specific pages\n   - `mcp__aws-mcp__aws___get_regional_availability` - Check service availability\n\n2. **If AWS MCP tools are unavailable**:\n   - Guide user to configure AWS MCP using the `aws-mcp-setup` skill (auto-loaded as dependency)\n   - Help determine which option fits their environment:\n     - Has uvx + AWS credentials → Full AWS MCP Server\n     - No Python/credentials → AWS Documentation MCP (no auth)\n   - If cannot determine → Ask user which option to use\n\n## Integrated MCP Servers\n\nThis skill includes 8 MCP servers automatically configured with the plugin:\n\n### Cost Management Servers\n\n#### 1. AWS Billing and Cost Management MCP Server\n**Purpose**: Real-time billing and cost management\n- View current AWS spending and trends\n- Analyze billing details across services\n- Track budget utilization\n- Monitor cost allocation tags\n- Review consolidated billing for organizations\n\n#### 2. AWS Pricing MCP Server\n**Purpose**: Pre-deployment cost estimation and optimization\n- Estimate costs before deploying resources\n- Compare pricing across regions\n- Calculate Total Cost of Ownership (TCO)\n- Evaluate different service options for cost efficiency\n- Get current pricing information for AWS services\n\n#### 3. AWS Cost Explorer MCP Server\n**Purpose**: Detailed cost analysis and reporting\n- Analyze historical spending patterns\n- Create custom cost reports\n- Identify cost anomalies and trends\n- Forecast future costs\n- Analyze cost by service, region, or tag\n- Generate cost optimization recommendations\n\n### Monitoring & Observability Servers\n\n#### 4. Amazon CloudWatch MCP Server\n**Purpose**: Metrics, alarms, and logs analysis\n- Query CloudWatch metrics and logs\n- Create and manage CloudWatch alarms\n- Analyze application performance metrics\n- Troubleshoot operational issues\n- Set up custom dashboards\n- Monitor resource utilization\n\n#### 5. Amazon CloudWatch Application Signals MCP Server\n**Purpose**: Application monitoring and performance insights\n- Monitor application health and performance\n- Analyze service-level objectives (SLOs)\n- Track application dependencies\n- Identify performance bottlenecks\n- Monitor service map and traces\n\n#### 6. AWS Managed Prometheus MCP Server\n**Purpose**: Prometheus-compatible monitoring\n- Query Prometheus metrics\n- Monitor containerized applications\n- Analyze Kubernetes workload metrics\n- Create PromQL queries\n- Track custom application metrics\n\n### Audit & Security Servers\n\n#### 7. AWS CloudTrail MCP Server\n**Purpose**: AWS API activity and audit analysis\n- Analyze AWS API calls and user activity\n- Track resource changes and modifications\n- Investigate security incidents\n- Audit compliance requirements\n- Identify unusual access patterns\n- Review who made what changes when\n\n#### 8. AWS Well-Architected Security Assessment Tool MCP Server\n**Purpose**: Security assessment against Well-Architected Framework\n- Assess security posture against AWS best practices\n- Identify security gaps and vulnerabilities\n- Get security improvement recommendations\n- Review security pillar compliance\n- Generate security assessment reports\n\n## When to Use This Skill\n\nUse this skill when:\n- Optimizing AWS costs and reducing spending\n- Estimating costs before deployment\n- Monitoring application and infrastructure performance\n- Setting up observability and alerting\n- Analyzing spending patterns and trends\n- Investigating operational issues\n- Auditing AWS activity and changes\n- Assessing security posture\n- Implementing operational excellence\n\n## Cost Optimization Best Practices\n\n### Pre-Deployment Cost Estimation\n\n**Always estimate costs before deploying**:\n1. Use **AWS Pricing MCP** to estimate resource costs\n2. Compare pricing across different regions\n3. Evaluate alternative service options\n4. Calculate expected monthly costs\n5. Plan for scaling and growth\n\n**Example workflow**:\n```\n\"Estimate the monthly cost of running a Lambda function with\n1 million invocations, 512MB memory, 3-second duration in us-east-1\"\n```\n\n### Cost Analysis and Optimization\n\n**Regular cost reviews**:\n1. Use **Cost Explorer MCP** to analyze spending trends\n2. Identify cost anomalies and unexpected charges\n3. Review costs by service, region, and environment\n4. Compare actual vs. budgeted costs\n5. Generate cost optimization recommendations\n\n**Cost optimization strategies**:\n- Right-size over-provisioned resources\n- Use appropriate storage classes (S3, EBS)\n- Implement auto-scaling for dynamic workloads\n- Leverage Savings Plans and Reserved Instances\n- Delete unused resources and snapshots\n- Use cost allocation tags effectively\n\n### Budget Monitoring\n\n**Track spending against budgets**:\n1. Use **Billing and Cost Management MCP** to monitor budgets\n2. Set up budget alerts for threshold breaches\n3. Review budget utilization regularly\n4. Adjust budgets based on trends\n5. Implement cost controls and governance\n\n## Monitoring and Observability Best Practices\n\n### CloudWatch Metrics and Alarms\n\n**Implement comprehensive monitoring**:\n1. Use **CloudWatch MCP** to query metrics and logs\n2. Set up alarms for critical metrics:\n   - CPU and memory utilization\n   - Error rates and latency\n   - Queue depths and processing times\n   - API gateway throttling\n   - Lambda errors and timeouts\n3. Create CloudWatch dashboards for visualization\n4. Use log insights for troubleshooting\n\n**Example alarm scenarios**:\n- Lambda error rate > 1%\n- EC2 CPU utilization > 80%\n- API Gateway 4xx/5xx error spike\n- DynamoDB throttled requests\n- ECS task failures\n\n### Application Performance Monitoring\n\n**Monitor application health**:\n1. Use **CloudWatch Application Signals MCP** for APM\n2. Track service-level objectives (SLOs)\n3. Monitor application dependencies\n4. Identify performance bottlenecks\n5. Set up distributed tracing\n\n### Container and Kubernetes Monitoring\n\n**For containerized workloads**:\n1. Use **AWS Managed Prometheus MCP** for metrics\n2. Monitor container resource utilization\n3. Track pod and node health\n4. Create PromQL queries for custom metrics\n5. Set up alerts for container anomalies\n\n## Audit and Security Best Practices\n\n### CloudTrail Activity Analysis\n\n**Audit AWS activity**:\n1. Use **CloudTrail MCP** to analyze API activity\n2. Track who made changes to resources\n3. Investigate security incidents\n4. Monitor for suspicious activity patterns\n5. Audit compliance with policies\n\n**Common audit scenarios**:\n- \"Who deleted this S3 bucket?\"\n- \"Show all IAM role changes in the last 24 hours\"\n- \"List failed login attempts\"\n- \"Find all actions by a specific user\"\n- \"Track modifications to security groups\"\n\n### Security Assessment\n\n**Regular security reviews**:\n1. Use **Well-Architected Security Assessment MCP**\n2. Assess security posture against best practices\n3. Identify security gaps and vulnerabilities\n4. Implement recommended security improvements\n5. Document security compliance\n\n**Security assessment areas**:\n- Identity and Access Management (IAM)\n- Detective controls and monitoring\n- Infrastructure protection\n- Data protection and encryption\n- Incident response preparedness\n\n## Using MCP Servers Effectively\n\n### Cost Analysis Workflow\n\n1. **Pre-deployment**: Use Pricing MCP to estimate costs\n2. **Post-deployment**: Use Billing MCP to track actual spending\n3. **Analysis**: Use Cost Explorer MCP for detailed cost analysis\n4. **Optimization**: Implement recommendations from Cost Explorer\n\n### Monitoring Workflow\n\n1. **Setup**: Configure CloudWatch metrics and alarms\n2. **Monitor**: Use CloudWatch MCP to track key metrics\n3. **Analyze**: Use Application Signals for APM insights\n4. **Troubleshoot**: Query CloudWatch Logs for issue resolution\n\n### Security Workflow\n\n1. **Audit**: Use CloudTrail MCP to review activity\n2. **Assess**: Use Well-Architected Security Assessment\n3. **Remediate**: Implement security recommendations\n4. **Monitor**: Track security events via CloudWatch\n\n### MCP Usage Best Practices\n\n1. **Cost Awareness**: Check pricing before deploying resources\n2. **Proactive Monitoring**: Set up alarms for critical metrics\n3. **Regular Reviews**: Analyze costs and performance weekly\n4. **Audit Trails**: Review CloudTrail logs for compliance\n5. **Security First**: Run security assessments regularly\n6. **Optimize Continuously**: Act on cost and performance recommendations\n\n## Operational Excellence Guidelines\n\n### Cost Optimization\n\n- **Tag Everything**: Use consistent cost allocation tags\n- **Review Monthly**: Analyze spending trends and anomalies\n- **Right-size**: Match resources to actual usage\n- **Automate**: Use auto-scaling and scheduling\n- **Monitor Budgets**: Set alerts for cost overruns\n\n### Monitoring and Alerting\n\n- **Critical Metrics**: Alert on business-critical metrics\n- **Noise Reduction**: Fine-tune thresholds to reduce false positives\n- **Actionable Alerts**: Ensure alerts have clear remediation steps\n- **Dashboard Visibility**: Create dashboards for key stakeholders\n- **Log Retention**: Balance cost and compliance needs\n\n### Security and Compliance\n\n- **Least Privilege**: Grant minimum required permissions\n- **Audit Regularly**: Review CloudTrail logs for anomalies\n- **Encrypt Data**: Use encryption at rest and in transit\n- **Assess Continuously**: Run security assessments frequently\n- **Incident Response**: Have procedures for security events\n\n## Additional Resources\n\nFor detailed operational patterns and best practices, refer to the comprehensive reference:\n\n**File**: `references/operations-patterns.md`\n\nThis reference includes:\n- Cost optimization strategies\n- Monitoring and alerting patterns\n- Observability best practices\n- Security and compliance guidelines\n- Troubleshooting workflows\n\n## CloudWatch Alarms Reference\n\n**File**: `references/cloudwatch-alarms.md`\n\nCommon alarm configurations for:\n- Lambda functions\n- EC2 instances\n- RDS databases\n- DynamoDB tables\n- API Gateway\n- ECS services\n- Application Load Balancers\n",
        "plugins/aws-cost-ops/skills/aws-cost-operations/references/cloudwatch-alarms.md": "# CloudWatch Alarms Reference\n\nCommon CloudWatch alarm configurations for AWS services.\n\n## Lambda Functions\n\n### Error Rate Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'LambdaErrorAlarm', {\n  metric: lambdaFunction.metricErrors({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 1,\n  treatMissingData: cloudwatch.TreatMissingData.NOT_BREACHING,\n  alarmDescription: 'Lambda error count exceeded threshold',\n});\n```\n\n### Duration Alarm (Approaching Timeout)\n```typescript\nnew cloudwatch.Alarm(this, 'LambdaDurationAlarm', {\n  metric: lambdaFunction.metricDuration({\n    statistic: 'Maximum',\n    period: Duration.minutes(5),\n  }),\n  threshold: lambdaFunction.timeout.toMilliseconds() * 0.8, // 80% of timeout\n  evaluationPeriods: 2,\n  alarmDescription: 'Lambda duration approaching timeout',\n});\n```\n\n### Throttle Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'LambdaThrottleAlarm', {\n  metric: lambdaFunction.metricThrottles({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 5,\n  evaluationPeriods: 1,\n  alarmDescription: 'Lambda function is being throttled',\n});\n```\n\n### Concurrent Executions Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'LambdaConcurrencyAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/Lambda',\n    metricName: 'ConcurrentExecutions',\n    dimensionsMap: {\n      FunctionName: lambdaFunction.functionName,\n    },\n    statistic: 'Maximum',\n    period: Duration.minutes(1),\n  }),\n  threshold: 100, // Adjust based on reserved concurrency\n  evaluationPeriods: 2,\n  alarmDescription: 'Lambda concurrent executions high',\n});\n```\n\n## API Gateway\n\n### 5XX Error Rate Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'Api5xxAlarm', {\n  metric: api.metricServerError({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 1,\n  alarmDescription: 'API Gateway 5XX errors exceeded threshold',\n});\n```\n\n### 4XX Error Rate Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'Api4xxAlarm', {\n  metric: api.metricClientError({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 50,\n  evaluationPeriods: 2,\n  alarmDescription: 'API Gateway 4XX errors exceeded threshold',\n});\n```\n\n### Latency Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ApiLatencyAlarm', {\n  metric: api.metricLatency({\n    statistic: 'p99',\n    period: Duration.minutes(5),\n  }),\n  threshold: 2000, // 2 seconds\n  evaluationPeriods: 2,\n  alarmDescription: 'API Gateway p99 latency exceeded threshold',\n});\n```\n\n## DynamoDB\n\n### Read Throttle Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'DynamoDBReadThrottleAlarm', {\n  metric: table.metricUserErrors({\n    dimensions: {\n      Operation: 'GetItem',\n    },\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 5,\n  evaluationPeriods: 1,\n  alarmDescription: 'DynamoDB read operations being throttled',\n});\n```\n\n### Write Throttle Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'DynamoDBWriteThrottleAlarm', {\n  metric: table.metricUserErrors({\n    dimensions: {\n      Operation: 'PutItem',\n    },\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 5,\n  evaluationPeriods: 1,\n  alarmDescription: 'DynamoDB write operations being throttled',\n});\n```\n\n### Consumed Capacity Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'DynamoDBCapacityAlarm', {\n  metric: table.metricConsumedReadCapacityUnits({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: provisionedCapacity * 0.8, // 80% of provisioned\n  evaluationPeriods: 2,\n  alarmDescription: 'DynamoDB consumed capacity approaching limit',\n});\n```\n\n## EC2 Instances\n\n### CPU Utilization Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'EC2CpuAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/EC2',\n    metricName: 'CPUUtilization',\n    dimensionsMap: {\n      InstanceId: instance.instanceId,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 80,\n  evaluationPeriods: 3,\n  alarmDescription: 'EC2 CPU utilization high',\n});\n```\n\n### Status Check Failed Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'EC2StatusCheckAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/EC2',\n    metricName: 'StatusCheckFailed',\n    dimensionsMap: {\n      InstanceId: instance.instanceId,\n    },\n    statistic: 'Maximum',\n    period: Duration.minutes(1),\n  }),\n  threshold: 1,\n  evaluationPeriods: 2,\n  alarmDescription: 'EC2 status check failed',\n});\n```\n\n### Disk Space Alarm (Requires CloudWatch Agent)\n```typescript\nnew cloudwatch.Alarm(this, 'EC2DiskAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'CWAgent',\n    metricName: 'disk_used_percent',\n    dimensionsMap: {\n      InstanceId: instance.instanceId,\n      path: '/',\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 85,\n  evaluationPeriods: 2,\n  alarmDescription: 'EC2 disk space usage high',\n});\n```\n\n## RDS Databases\n\n### CPU Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'RDSCpuAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/RDS',\n    metricName: 'CPUUtilization',\n    dimensionsMap: {\n      DBInstanceIdentifier: dbInstance.instanceIdentifier,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 80,\n  evaluationPeriods: 3,\n  alarmDescription: 'RDS CPU utilization high',\n});\n```\n\n### Connection Count Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'RDSConnectionAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/RDS',\n    metricName: 'DatabaseConnections',\n    dimensionsMap: {\n      DBInstanceIdentifier: dbInstance.instanceIdentifier,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: maxConnections * 0.8, // 80% of max connections\n  evaluationPeriods: 2,\n  alarmDescription: 'RDS connection count approaching limit',\n});\n```\n\n### Free Storage Space Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'RDSStorageAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/RDS',\n    metricName: 'FreeStorageSpace',\n    dimensionsMap: {\n      DBInstanceIdentifier: dbInstance.instanceIdentifier,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10 * 1024 * 1024 * 1024, // 10 GB in bytes\n  comparisonOperator: cloudwatch.ComparisonOperator.LESS_THAN_THRESHOLD,\n  evaluationPeriods: 1,\n  alarmDescription: 'RDS free storage space low',\n});\n```\n\n## ECS Services\n\n### Task Count Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ECSTaskCountAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'ECS/ContainerInsights',\n    metricName: 'RunningTaskCount',\n    dimensionsMap: {\n      ServiceName: service.serviceName,\n      ClusterName: cluster.clusterName,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1,\n  comparisonOperator: cloudwatch.ComparisonOperator.LESS_THAN_THRESHOLD,\n  evaluationPeriods: 2,\n  alarmDescription: 'ECS service has no running tasks',\n});\n```\n\n### CPU Utilization Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ECSCpuAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ECS',\n    metricName: 'CPUUtilization',\n    dimensionsMap: {\n      ServiceName: service.serviceName,\n      ClusterName: cluster.clusterName,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 80,\n  evaluationPeriods: 3,\n  alarmDescription: 'ECS service CPU utilization high',\n});\n```\n\n### Memory Utilization Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ECSMemoryAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ECS',\n    metricName: 'MemoryUtilization',\n    dimensionsMap: {\n      ServiceName: service.serviceName,\n      ClusterName: cluster.clusterName,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 85,\n  evaluationPeriods: 2,\n  alarmDescription: 'ECS service memory utilization high',\n});\n```\n\n## SQS Queues\n\n### Queue Depth Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'SQSDepthAlarm', {\n  metric: queue.metricApproximateNumberOfMessagesVisible({\n    statistic: 'Maximum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1000,\n  evaluationPeriods: 2,\n  alarmDescription: 'SQS queue depth exceeded threshold',\n});\n```\n\n### Age of Oldest Message Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'SQSAgeAlarm', {\n  metric: queue.metricApproximateAgeOfOldestMessage({\n    statistic: 'Maximum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 300, // 5 minutes in seconds\n  evaluationPeriods: 1,\n  alarmDescription: 'SQS messages not being processed timely',\n});\n```\n\n## Application Load Balancer\n\n### Target Health Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ALBUnhealthyTargetAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ApplicationELB',\n    metricName: 'UnHealthyHostCount',\n    dimensionsMap: {\n      LoadBalancer: loadBalancer.loadBalancerFullName,\n      TargetGroup: targetGroup.targetGroupFullName,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1,\n  evaluationPeriods: 2,\n  alarmDescription: 'ALB has unhealthy targets',\n});\n```\n\n### HTTP 5XX Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ALB5xxAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ApplicationELB',\n    metricName: 'HTTPCode_Target_5XX_Count',\n    dimensionsMap: {\n      LoadBalancer: loadBalancer.loadBalancerFullName,\n    },\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 1,\n  alarmDescription: 'ALB target 5XX errors exceeded threshold',\n});\n```\n\n### Response Time Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ALBLatencyAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ApplicationELB',\n    metricName: 'TargetResponseTime',\n    dimensionsMap: {\n      LoadBalancer: loadBalancer.loadBalancerFullName,\n    },\n    statistic: 'p99',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1, // 1 second\n  evaluationPeriods: 2,\n  alarmDescription: 'ALB p99 response time exceeded threshold',\n});\n```\n\n## Composite Alarms\n\n### Service Health Composite Alarm\n```typescript\nconst errorAlarm = new cloudwatch.Alarm(this, 'ErrorAlarm', { /* ... */ });\nconst latencyAlarm = new cloudwatch.Alarm(this, 'LatencyAlarm', { /* ... */ });\nconst throttleAlarm = new cloudwatch.Alarm(this, 'ThrottleAlarm', { /* ... */ });\n\nnew cloudwatch.CompositeAlarm(this, 'ServiceHealthAlarm', {\n  compositeAlarmName: 'service-health',\n  alarmRule: cloudwatch.AlarmRule.anyOf(\n    errorAlarm,\n    latencyAlarm,\n    throttleAlarm\n  ),\n  alarmDescription: 'Overall service health degraded',\n});\n```\n\n## Alarm Actions\n\n### SNS Topic Integration\n```typescript\nconst topic = new sns.Topic(this, 'AlarmTopic', {\n  displayName: 'CloudWatch Alarms',\n});\n\n// Email subscription\ntopic.addSubscription(new subscriptions.EmailSubscription('ops@example.com'));\n\n// Add action to alarm\nalarm.addAlarmAction(new actions.SnsAction(topic));\nalarm.addOkAction(new actions.SnsAction(topic));\n```\n\n### Auto Scaling Action\n```typescript\nconst scalingAction = targetGroup.scaleOnMetric('ScaleUp', {\n  metric: targetGroup.metricTargetResponseTime(),\n  scalingSteps: [\n    { upper: 1, change: 0 },\n    { lower: 1, change: +1 },\n    { lower: 2, change: +2 },\n  ],\n});\n```\n\n## Alarm Best Practices\n\n### Threshold Selection\n\n**CPU/Memory Alarms**:\n- Warning: 70-80%\n- Critical: 80-90%\n- Consider burst patterns and normal usage\n\n**Error Rate Alarms**:\n- Threshold based on SLA (e.g., 99.9% = 0.1% error rate)\n- Account for normal error rates\n- Different thresholds for different error types\n\n**Latency Alarms**:\n- p99 latency for user-facing APIs\n- Warning: 80% of SLA target\n- Critical: 100% of SLA target\n\n### Evaluation Periods\n\n**Fast-changing metrics** (1-2 periods):\n- Error counts\n- Failed health checks\n- Critical application errors\n\n**Slow-changing metrics** (3-5 periods):\n- CPU utilization\n- Memory usage\n- Disk usage\n\n**Cost-related metrics** (longer periods):\n- Daily spending\n- Resource count changes\n- Usage patterns\n\n### Missing Data Handling\n\n```typescript\n// For intermittent workloads\nalarm.treatMissingData(cloudwatch.TreatMissingData.NOT_BREACHING);\n\n// For always-on services\nalarm.treatMissingData(cloudwatch.TreatMissingData.BREACHING);\n\n// To distinguish from data issues\nalarm.treatMissingData(cloudwatch.TreatMissingData.MISSING);\n```\n\n### Alarm Naming Conventions\n\n```typescript\n// Pattern: <service>-<metric>-<severity>\n'lambda-errors-critical'\n'api-latency-warning'\n'rds-cpu-warning'\n'ecs-tasks-critical'\n```\n\n### Alarm Actions Best Practices\n\n1. **Separate topics by severity**:\n   - Critical alarms → PagerDuty/on-call\n   - Warning alarms → Slack/email\n   - Info alarms → Metrics dashboard\n\n2. **Include context in alarm description**:\n   - Service name\n   - Expected threshold\n   - Troubleshooting runbook link\n\n3. **Auto-remediation where possible**:\n   - Lambda errors → automatic retry\n   - CPU high → auto-scaling trigger\n   - Disk full → automated cleanup\n\n4. **Alarm fatigue prevention**:\n   - Tune thresholds based on actual patterns\n   - Use composite alarms to reduce noise\n   - Implement proper evaluation periods\n   - Regularly review and adjust alarms\n\n## Monitoring Dashboard\n\n### Recommended Dashboard Layout\n\n**Service Overview**:\n- Request count and rate\n- Error count and percentage\n- Latency (p50, p95, p99)\n- Availability percentage\n\n**Resource Utilization**:\n- CPU utilization by service\n- Memory utilization by service\n- Network throughput\n- Disk I/O\n\n**Cost Metrics**:\n- Daily spending by service\n- Month-to-date costs\n- Budget utilization\n- Cost anomalies\n\n**Security Metrics**:\n- Failed login attempts\n- IAM policy changes\n- Security group modifications\n- GuardDuty findings\n",
        "plugins/aws-cost-ops/skills/aws-cost-operations/references/operations-patterns.md": "# AWS Cost & Operations Patterns\n\nComprehensive patterns and best practices for AWS cost optimization, monitoring, and operational excellence.\n\n## Table of Contents\n\n- [Cost Optimization Patterns](#cost-optimization-patterns)\n- [Monitoring Patterns](#monitoring-patterns)\n- [Observability Patterns](#observability-patterns)\n- [Security and Audit Patterns](#security-and-audit-patterns)\n- [Troubleshooting Workflows](#troubleshooting-workflows)\n\n## Cost Optimization Patterns\n\n### Pattern 1: Cost Estimation Before Deployment\n\n**When**: Before deploying any new infrastructure\n\n**MCP Server**: AWS Pricing MCP\n\n**Steps**:\n1. List all resources to be deployed\n2. Query pricing for each resource type\n3. Calculate monthly costs based on expected usage\n4. Compare pricing across regions\n5. Document cost estimates in architecture docs\n\n**Example**:\n```\nResource: Lambda Function\n- Invocations: 1,000,000/month\n- Duration: 3 seconds avg\n- Memory: 512 MB\n- Region: us-east-1\nEstimated cost: $X/month\n```\n\n### Pattern 2: Monthly Cost Review\n\n**When**: First week of every month\n\n**MCP Servers**: Cost Explorer MCP, Billing and Cost Management MCP\n\n**Steps**:\n1. Review total spending vs. budget\n2. Analyze cost by service (top 5 services)\n3. Identify cost anomalies (>20% increase)\n4. Review cost by environment (dev/staging/prod)\n5. Check cost allocation tag coverage\n6. Generate cost optimization recommendations\n\n**Key Metrics**:\n- Month-over-month cost change\n- Cost per environment\n- Cost per application/project\n- Untagged resource costs\n\n### Pattern 3: Right-Sizing Resources\n\n**When**: Quarterly or when utilization alerts trigger\n\n**MCP Servers**: CloudWatch MCP, Cost Explorer MCP\n\n**Steps**:\n1. Query CloudWatch for resource utilization metrics\n2. Identify over-provisioned resources (< 40% utilization)\n3. Identify under-provisioned resources (> 80% utilization)\n4. Calculate potential savings from right-sizing\n5. Plan and execute right-sizing changes\n6. Monitor post-change performance\n\n**Common Right-Sizing Scenarios**:\n- EC2 instances with low CPU utilization\n- RDS instances with excess capacity\n- DynamoDB tables with low read/write usage\n- Lambda functions with excessive memory allocation\n\n### Pattern 4: Unused Resource Cleanup\n\n**When**: Monthly or triggered by cost anomalies\n\n**MCP Servers**: Cost Explorer MCP, CloudTrail MCP\n\n**Steps**:\n1. Identify resources with zero usage\n2. Query CloudTrail for last access time\n3. Tag resources for deletion review\n4. Notify resource owners\n5. Delete confirmed unused resources\n6. Track cost savings\n\n**Common Unused Resources**:\n- Unattached EBS volumes\n- Old EBS snapshots\n- Idle Load Balancers\n- Unused Elastic IPs\n- Old AMIs and snapshots\n- Stopped EC2 instances (long-term)\n\n## Monitoring Patterns\n\n### Pattern 1: Critical Service Monitoring\n\n**When**: All production services\n\n**MCP Server**: CloudWatch MCP\n\n**Metrics to Monitor**:\n- **Availability**: Service uptime, health checks\n- **Performance**: Latency, response time\n- **Errors**: Error rate, failed requests\n- **Saturation**: CPU, memory, disk, network utilization\n\n**Alarm Thresholds** (adjust based on SLAs):\n- Error rate: > 1% for 2 consecutive periods\n- Latency: p99 > 1 second for 5 minutes\n- CPU: > 80% for 10 minutes\n- Memory: > 85% for 5 minutes\n\n### Pattern 2: Lambda Function Monitoring\n\n**MCP Server**: CloudWatch MCP\n\n**Key Metrics**:\n```\n- Invocations (Count)\n- Errors (Count, %)\n- Duration (Average, p99)\n- Throttles (Count)\n- ConcurrentExecutions (Max)\n- IteratorAge (for stream processing)\n```\n\n**Recommended Alarms**:\n- Error rate > 1%\n- Duration > 80% of timeout\n- Throttles > 0\n- ConcurrentExecutions > 80% of reserved\n\n### Pattern 3: API Gateway Monitoring\n\n**MCP Server**: CloudWatch MCP\n\n**Key Metrics**:\n```\n- Count (Total requests)\n- 4XXError, 5XXError\n- Latency (p50, p95, p99)\n- IntegrationLatency\n- CacheHitCount, CacheMissCount\n```\n\n**Recommended Alarms**:\n- 5XX error rate > 0.5%\n- 4XX error rate > 5%\n- Latency p99 > 2 seconds\n- Integration latency spike\n\n### Pattern 4: Database Monitoring\n\n**MCP Server**: CloudWatch MCP\n\n**RDS Metrics**:\n```\n- CPUUtilization\n- DatabaseConnections\n- FreeableMemory\n- ReadLatency, WriteLatency\n- ReadIOPS, WriteIOPS\n- FreeStorageSpace\n```\n\n**DynamoDB Metrics**:\n```\n- ConsumedReadCapacityUnits\n- ConsumedWriteCapacityUnits\n- UserErrors\n- SystemErrors\n- ThrottledRequests\n```\n\n**Recommended Alarms**:\n- RDS CPU > 80% for 10 minutes\n- RDS connections > 80% of max\n- RDS free storage < 10 GB\n- DynamoDB throttled requests > 0\n- DynamoDB user errors spike\n\n## Observability Patterns\n\n### Pattern 1: Distributed Tracing Setup\n\n**MCP Server**: CloudWatch Application Signals MCP\n\n**Components**:\n1. **Service Map**: Visualize service dependencies\n2. **Traces**: Track requests across services\n3. **Metrics**: Monitor latency and errors per service\n4. **SLOs**: Define and track service level objectives\n\n**Implementation**:\n- Enable X-Ray tracing on Lambda functions\n- Add X-Ray SDK to application code\n- Configure sampling rules\n- Create service lens dashboards\n\n### Pattern 2: Log Aggregation and Analysis\n\n**MCP Server**: CloudWatch MCP\n\n**Log Strategy**:\n1. **Centralize Logs**: Send all application logs to CloudWatch Logs\n2. **Structure Logs**: Use JSON format for structured logging\n3. **Log Insights**: Use CloudWatch Logs Insights for queries\n4. **Retention**: Set appropriate retention periods\n\n**Example Log Insights Queries**:\n```\n# Find errors in last hour\nfields @timestamp, @message\n| filter @message like /ERROR/\n| sort @timestamp desc\n| limit 100\n\n# Count errors by type\nstats count() by error_type\n| sort count desc\n\n# Calculate p99 latency\nstats percentile(duration, 99) by service_name\n```\n\n### Pattern 3: Custom Metrics\n\n**MCP Server**: CloudWatch MCP\n\n**When to Use Custom Metrics**:\n- Business-specific KPIs (orders/minute, revenue/hour)\n- Application-specific metrics (cache hit rate, queue depth)\n- Performance metrics not provided by AWS\n\n**Best Practices**:\n- Use consistent namespace: `CompanyName/ApplicationName`\n- Include relevant dimensions (environment, region, version)\n- Publish metrics at appropriate intervals\n- Use metric filters for log-derived metrics\n\n## Security and Audit Patterns\n\n### Pattern 1: API Activity Auditing\n\n**MCP Server**: CloudTrail MCP\n\n**Regular Audit Queries**:\n```\n# Find all IAM changes\neventName: CreateUser, DeleteUser, AttachUserPolicy, etc.\nTime: Last 24 hours\n\n# Track S3 bucket deletions\neventName: DeleteBucket\nTime: Last 7 days\n\n# Find failed login attempts\neventName: ConsoleLogin\nerrorCode: Failure\n\n# Monitor privileged actions\nuserIdentity.arn: *admin* OR *root*\n```\n\n**Audit Schedule**:\n- Daily: Review privileged user actions\n- Weekly: Audit IAM changes and security group modifications\n- Monthly: Comprehensive security review\n\n### Pattern 2: Security Posture Assessment\n\n**MCP Server**: Well-Architected Security Assessment Tool MCP\n\n**Assessment Areas**:\n1. **Identity and Access Management**\n   - Least privilege implementation\n   - MFA enforcement\n   - Role-based access control\n   - Service control policies\n\n2. **Detective Controls**\n   - CloudTrail enabled in all regions\n   - GuardDuty findings review\n   - Config rule compliance\n   - Security Hub findings\n\n3. **Infrastructure Protection**\n   - VPC security groups review\n   - Network ACLs configuration\n   - AWS WAF rules\n   - Security group ingress rules\n\n4. **Data Protection**\n   - Encryption at rest (S3, EBS, RDS)\n   - Encryption in transit (TLS/SSL)\n   - KMS key usage and rotation\n   - Secrets Manager utilization\n\n5. **Incident Response**\n   - IR playbooks documented\n   - Automated response procedures\n   - Contact information current\n   - Regular IR drills\n\n**Assessment Frequency**:\n- Quarterly: Full Well-Architected review\n- Monthly: High-priority findings review\n- Weekly: Critical security findings\n\n### Pattern 3: Compliance Monitoring\n\n**MCP Servers**: CloudTrail MCP, CloudWatch MCP\n\n**Compliance Requirements**:\n- Data residency (ensure data stays in approved regions)\n- Access logging (all access logged and retained)\n- Encryption requirements (data encrypted at rest and in transit)\n- Change management (all changes tracked in CloudTrail)\n\n**Compliance Dashboards**:\n- Encryption coverage by service\n- CloudTrail logging status\n- Failed login attempts\n- Privileged access usage\n- Non-compliant resources\n\n## Troubleshooting Workflows\n\n### Workflow 1: High Lambda Error Rate\n\n**MCP Servers**: CloudWatch MCP, CloudWatch Application Signals MCP\n\n**Steps**:\n1. Query CloudWatch for Lambda error metrics\n2. Check error logs in CloudWatch Logs\n3. Identify error patterns (timeout, memory, permission)\n4. Check Lambda configuration (memory, timeout, permissions)\n5. Review recent code deployments\n6. Check downstream service health\n7. Implement fix and monitor\n\n### Workflow 2: Increased Latency\n\n**MCP Servers**: CloudWatch MCP, CloudWatch Application Signals MCP\n\n**Steps**:\n1. Identify latency spike in CloudWatch metrics\n2. Check service map for slow dependencies\n3. Query distributed traces for slow requests\n4. Check database query performance\n5. Review API Gateway integration latency\n6. Check Lambda cold starts\n7. Identify bottleneck and optimize\n\n### Workflow 3: Cost Spike Investigation\n\n**MCP Servers**: Cost Explorer MCP, CloudWatch MCP, CloudTrail MCP\n\n**Steps**:\n1. Use Cost Explorer to identify service causing spike\n2. Check CloudWatch metrics for usage increase\n3. Review CloudTrail for recent resource creation\n4. Identify root cause (misconfiguration, runaway process, attack)\n5. Implement cost controls (budgets, alarms, service quotas)\n6. Clean up unnecessary resources\n\n### Workflow 4: Security Incident Response\n\n**MCP Servers**: CloudTrail MCP, GuardDuty (via CloudWatch), Well-Architected Assessment MCP\n\n**Steps**:\n1. Identify security event in GuardDuty or CloudWatch\n2. Query CloudTrail for related API activity\n3. Determine scope and impact\n4. Isolate affected resources\n5. Revoke compromised credentials\n6. Implement remediation\n7. Conduct post-incident review\n8. Update security controls\n\n## Summary\n\n- **Cost Optimization**: Use Pricing, Cost Explorer, and Billing MCPs for proactive cost management\n- **Monitoring**: Set up comprehensive CloudWatch alarms for all critical services\n- **Observability**: Implement distributed tracing and structured logging\n- **Security**: Regular CloudTrail audits and Well-Architected assessments\n- **Proactive**: Don't wait for incidents - monitor and optimize continuously\n",
        "plugins/serverless-eda/skills/aws-serverless-eda/SKILL.md": "---\nname: aws-serverless-eda\ndescription: AWS serverless and event-driven architecture expert based on Well-Architected Framework. Use when building serverless APIs, Lambda functions, REST APIs, microservices, or async workflows. Covers Lambda with TypeScript/Python, API Gateway (REST/HTTP), DynamoDB, Step Functions, EventBridge, SQS, SNS, and serverless patterns. Essential when user mentions serverless, Lambda, API Gateway, event-driven, async processing, queues, pub/sub, or wants to build scalable serverless applications with AWS best practices.\ncontext: fork\nskills:\n  - aws-mcp-setup\n  - aws-cdk-development\nallowed-tools:\n  - mcp__aws-mcp__*\n  - mcp__awsdocs__*\n  - mcp__cdk__*\n  - Bash(sam *)\n  - Bash(aws lambda *)\n  - Bash(aws apigateway *)\n  - Bash(aws apigatewayv2 *)\n  - Bash(aws dynamodb *)\n  - Bash(aws stepfunctions *)\n  - Bash(aws events *)\n  - Bash(aws sqs *)\n  - Bash(aws sns *)\n  - Bash(aws sts get-caller-identity)\nhooks:\n  PreToolUse:\n    - matcher: Bash(sam deploy*)\n      command: aws sts get-caller-identity --query Account --output text\n      once: true\n---\n\n# AWS Serverless & Event-Driven Architecture\n\nThis skill provides comprehensive guidance for building serverless applications and event-driven architectures on AWS based on Well-Architected Framework principles.\n\n## AWS Documentation Requirement\n\n**CRITICAL**: This skill requires AWS MCP tools for accurate, up-to-date AWS information.\n\n### Before Answering AWS Questions\n\n1. **Always verify** using AWS MCP tools (if available):\n   - `mcp__aws-mcp__aws___search_documentation` or `mcp__*awsdocs*__aws___search_documentation` - Search AWS docs\n   - `mcp__aws-mcp__aws___read_documentation` or `mcp__*awsdocs*__aws___read_documentation` - Read specific pages\n   - `mcp__aws-mcp__aws___get_regional_availability` - Check service availability\n\n2. **If AWS MCP tools are unavailable**:\n   - Guide user to configure AWS MCP using the `aws-mcp-setup` skill (auto-loaded as dependency)\n   - Help determine which option fits their environment:\n     - Has uvx + AWS credentials → Full AWS MCP Server\n     - No Python/credentials → AWS Documentation MCP (no auth)\n   - If cannot determine → Ask user which option to use\n\n## Serverless MCP Servers\n\nThis skill can leverage serverless-specific MCP servers for enhanced development workflows:\n\n### AWS Serverless MCP Server\n**Purpose**: Complete serverless application lifecycle with SAM CLI\n- Initialize new serverless applications\n- Deploy serverless applications\n- Test Lambda functions locally\n- Generate SAM templates\n- Manage serverless application lifecycle\n\n### AWS Lambda Tool MCP Server\n**Purpose**: Execute Lambda functions as tools\n- Invoke Lambda functions directly\n- Test Lambda integrations\n- Execute workflows requiring private resource access\n- Run Lambda-based automation\n\n### AWS Step Functions MCP Server\n**Purpose**: Execute complex workflows and orchestration\n- Create and manage state machines\n- Execute workflow orchestrations\n- Handle distributed transactions\n- Implement saga patterns\n- Coordinate microservices\n\n### Amazon SNS/SQS MCP Server\n**Purpose**: Event-driven messaging and queue management\n- Publish messages to SNS topics\n- Send/receive messages from SQS queues\n- Manage event-driven communication\n- Implement pub/sub patterns\n- Handle asynchronous processing\n\n## When to Use This Skill\n\nUse this skill when:\n- Building serverless applications with Lambda\n- Designing event-driven architectures\n- Implementing microservices patterns\n- Creating asynchronous processing workflows\n- Orchestrating multi-service transactions\n- Building real-time data processing pipelines\n- Implementing saga patterns for distributed transactions\n- Designing for scale and resilience\n\n## AWS Well-Architected Serverless Design Principles\n\n### 1. Speedy, Simple, Singular\n\n**Functions should be concise and single-purpose**\n\n```typescript\n// ✅ GOOD - Single purpose, focused function\nexport const processOrder = async (event: OrderEvent) => {\n  // Only handles order processing\n  const order = await validateOrder(event);\n  await saveOrder(order);\n  await publishOrderCreatedEvent(order);\n  return { statusCode: 200, body: JSON.stringify({ orderId: order.id }) };\n};\n\n// ❌ BAD - Function does too much\nexport const handleEverything = async (event: any) => {\n  // Handles orders, inventory, payments, shipping...\n  // Too many responsibilities\n};\n```\n\n**Keep functions environmentally efficient and cost-aware**:\n- Minimize cold start times\n- Optimize memory allocation\n- Use provisioned concurrency only when needed\n- Leverage connection reuse\n\n### 2. Think Concurrent Requests, Not Total Requests\n\n**Design for concurrency, not volume**\n\nLambda scales horizontally - design considerations should focus on:\n- Concurrent execution limits\n- Downstream service throttling\n- Shared resource contention\n- Connection pool sizing\n\n```typescript\n// Consider concurrent Lambda executions accessing DynamoDB\nconst table = new dynamodb.Table(this, 'Table', {\n  billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, // Auto-scales with load\n});\n\n// Or with provisioned capacity + auto-scaling\nconst table = new dynamodb.Table(this, 'Table', {\n  billingMode: dynamodb.BillingMode.PROVISIONED,\n  readCapacity: 5,\n  writeCapacity: 5,\n});\n\n// Enable auto-scaling for concurrent load\ntable.autoScaleReadCapacity({ minCapacity: 5, maxCapacity: 100 });\ntable.autoScaleWriteCapacity({ minCapacity: 5, maxCapacity: 100 });\n```\n\n### 3. Share Nothing\n\n**Function runtime environments are short-lived**\n\n```typescript\n// ❌ BAD - Relying on local file system\nexport const handler = async (event: any) => {\n  fs.writeFileSync('/tmp/data.json', JSON.stringify(data)); // Lost after execution\n};\n\n// ✅ GOOD - Use persistent storage\nexport const handler = async (event: any) => {\n  await s3.putObject({\n    Bucket: process.env.BUCKET_NAME,\n    Key: 'data.json',\n    Body: JSON.stringify(data),\n  });\n};\n```\n\n**State management**:\n- Use DynamoDB for persistent state\n- Use Step Functions for workflow state\n- Use ElastiCache for session state\n- Use S3 for file storage\n\n### 4. Assume No Hardware Affinity\n\n**Applications must be hardware-agnostic**\n\nInfrastructure can change without notice:\n- Lambda functions can run on different hardware\n- Container instances can be replaced\n- No assumption about underlying infrastructure\n\n**Design for portability**:\n- Use environment variables for configuration\n- Avoid hardware-specific optimizations\n- Test across different environments\n\n### 5. Orchestrate with State Machines, Not Function Chaining\n\n**Use Step Functions for orchestration**\n\n```typescript\n// ❌ BAD - Lambda function chaining\nexport const handler1 = async (event: any) => {\n  const result = await processStep1(event);\n  await lambda.invoke({\n    FunctionName: 'handler2',\n    Payload: JSON.stringify(result),\n  });\n};\n\n// ✅ GOOD - Step Functions orchestration\nconst stateMachine = new stepfunctions.StateMachine(this, 'OrderWorkflow', {\n  definition: stepfunctions.Chain\n    .start(validateOrder)\n    .next(processPayment)\n    .next(shipOrder)\n    .next(sendConfirmation),\n});\n```\n\n**Benefits of Step Functions**:\n- Visual workflow representation\n- Built-in error handling and retries\n- Execution history and debugging\n- Parallel and sequential execution\n- Service integrations without code\n\n### 6. Use Events to Trigger Transactions\n\n**Event-driven over synchronous request/response**\n\n```typescript\n// Pattern: Event-driven processing\nconst bucket = new s3.Bucket(this, 'DataBucket');\n\nbucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.LambdaDestination(processFunction),\n  { prefix: 'uploads/' }\n);\n\n// Pattern: EventBridge integration\nconst rule = new events.Rule(this, 'OrderRule', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n});\n\nrule.addTarget(new targets.LambdaFunction(processOrderFunction));\n```\n\n**Benefits**:\n- Loose coupling between services\n- Asynchronous processing\n- Better fault tolerance\n- Independent scaling\n\n### 7. Design for Failures and Duplicates\n\n**Operations must be idempotent**\n\n```typescript\n// ✅ GOOD - Idempotent operation\nexport const handler = async (event: SQSEvent) => {\n  for (const record of event.Records) {\n    const orderId = JSON.parse(record.body).orderId;\n\n    // Check if already processed (idempotency)\n    const existing = await dynamodb.getItem({\n      TableName: process.env.TABLE_NAME,\n      Key: { orderId },\n    });\n\n    if (existing.Item) {\n      console.log('Order already processed:', orderId);\n      continue; // Skip duplicate\n    }\n\n    // Process order\n    await processOrder(orderId);\n\n    // Mark as processed\n    await dynamodb.putItem({\n      TableName: process.env.TABLE_NAME,\n      Item: { orderId, processedAt: Date.now() },\n    });\n  }\n};\n```\n\n**Implement retry logic with exponential backoff**:\n```typescript\nasync function withRetry<T>(fn: () => Promise<T>, maxRetries = 3): Promise<T> {\n  for (let i = 0; i < maxRetries; i++) {\n    try {\n      return await fn();\n    } catch (error) {\n      if (i === maxRetries - 1) throw error;\n      await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));\n    }\n  }\n  throw new Error('Max retries exceeded');\n}\n```\n\n## Event-Driven Architecture Patterns\n\n### Pattern 1: Event Router (EventBridge)\n\nUse EventBridge for event routing and filtering:\n\n```typescript\n// Create custom event bus\nconst eventBus = new events.EventBus(this, 'AppEventBus', {\n  eventBusName: 'application-events',\n});\n\n// Define event schema\nconst schema = new events.Schema(this, 'OrderSchema', {\n  schemaName: 'OrderPlaced',\n  definition: events.SchemaDefinition.fromInline({\n    openapi: '3.0.0',\n    info: { version: '1.0.0', title: 'Order Events' },\n    paths: {},\n    components: {\n      schemas: {\n        OrderPlaced: {\n          type: 'object',\n          properties: {\n            orderId: { type: 'string' },\n            customerId: { type: 'string' },\n            amount: { type: 'number' },\n          },\n        },\n      },\n    },\n  }),\n});\n\n// Create rules for different consumers\nnew events.Rule(this, 'ProcessOrderRule', {\n  eventBus,\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n  targets: [new targets.LambdaFunction(processOrderFunction)],\n});\n\nnew events.Rule(this, 'NotifyCustomerRule', {\n  eventBus,\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n  targets: [new targets.LambdaFunction(notifyCustomerFunction)],\n});\n```\n\n### Pattern 2: Queue-Based Processing (SQS)\n\nUse SQS for reliable asynchronous processing:\n\n```typescript\n// Standard queue for at-least-once delivery\nconst queue = new sqs.Queue(this, 'ProcessingQueue', {\n  visibilityTimeout: Duration.seconds(300),\n  retentionPeriod: Duration.days(14),\n  deadLetterQueue: {\n    queue: dlq,\n    maxReceiveCount: 3,\n  },\n});\n\n// FIFO queue for ordered processing\nconst fifoQueue = new sqs.Queue(this, 'OrderedQueue', {\n  fifo: true,\n  contentBasedDeduplication: true,\n  deduplicationScope: sqs.DeduplicationScope.MESSAGE_GROUP,\n});\n\n// Lambda consumer\nnew lambda.EventSourceMapping(this, 'QueueConsumer', {\n  target: processingFunction,\n  eventSourceArn: queue.queueArn,\n  batchSize: 10,\n  maxBatchingWindow: Duration.seconds(5),\n});\n```\n\n### Pattern 3: Pub/Sub (SNS + SQS Fan-Out)\n\nImplement fan-out pattern for multiple consumers:\n\n```typescript\n// Create SNS topic\nconst topic = new sns.Topic(this, 'OrderTopic', {\n  displayName: 'Order Events',\n});\n\n// Multiple SQS queues subscribe to topic\nconst inventoryQueue = new sqs.Queue(this, 'InventoryQueue');\nconst shippingQueue = new sqs.Queue(this, 'ShippingQueue');\nconst analyticsQueue = new sqs.Queue(this, 'AnalyticsQueue');\n\ntopic.addSubscription(new subscriptions.SqsSubscription(inventoryQueue));\ntopic.addSubscription(new subscriptions.SqsSubscription(shippingQueue));\ntopic.addSubscription(new subscriptions.SqsSubscription(analyticsQueue));\n\n// Each queue has its own Lambda consumer\nnew lambda.EventSourceMapping(this, 'InventoryConsumer', {\n  target: inventoryFunction,\n  eventSourceArn: inventoryQueue.queueArn,\n});\n```\n\n### Pattern 4: Saga Pattern with Step Functions\n\nImplement distributed transactions:\n\n```typescript\nconst reserveFlight = new tasks.LambdaInvoke(this, 'ReserveFlight', {\n  lambdaFunction: reserveFlightFunction,\n  outputPath: '$.Payload',\n});\n\nconst reserveHotel = new tasks.LambdaInvoke(this, 'ReserveHotel', {\n  lambdaFunction: reserveHotelFunction,\n  outputPath: '$.Payload',\n});\n\nconst processPayment = new tasks.LambdaInvoke(this, 'ProcessPayment', {\n  lambdaFunction: processPaymentFunction,\n  outputPath: '$.Payload',\n});\n\n// Compensating transactions\nconst cancelFlight = new tasks.LambdaInvoke(this, 'CancelFlight', {\n  lambdaFunction: cancelFlightFunction,\n});\n\nconst cancelHotel = new tasks.LambdaInvoke(this, 'CancelHotel', {\n  lambdaFunction: cancelHotelFunction,\n});\n\n// Define saga with compensation\nconst definition = reserveFlight\n  .next(reserveHotel)\n  .next(processPayment)\n  .addCatch(cancelHotel.next(cancelFlight), {\n    resultPath: '$.error',\n  });\n\nnew stepfunctions.StateMachine(this, 'BookingStateMachine', {\n  definition,\n  timeout: Duration.minutes(5),\n});\n```\n\n### Pattern 5: Event Sourcing\n\nStore events as source of truth:\n\n```typescript\n// Event store with DynamoDB\nconst eventStore = new dynamodb.Table(this, 'EventStore', {\n  partitionKey: { name: 'aggregateId', type: dynamodb.AttributeType.STRING },\n  sortKey: { name: 'version', type: dynamodb.AttributeType.NUMBER },\n  stream: dynamodb.StreamViewType.NEW_IMAGE,\n});\n\n// Lambda function stores events\nexport const handleCommand = async (event: any) => {\n  const { aggregateId, eventType, eventData } = event;\n\n  // Get current version\n  const items = await dynamodb.query({\n    TableName: process.env.EVENT_STORE,\n    KeyConditionExpression: 'aggregateId = :id',\n    ExpressionAttributeValues: { ':id': aggregateId },\n    ScanIndexForward: false,\n    Limit: 1,\n  });\n\n  const nextVersion = items.Items?.[0]?.version + 1 || 1;\n\n  // Append new event\n  await dynamodb.putItem({\n    TableName: process.env.EVENT_STORE,\n    Item: {\n      aggregateId,\n      version: nextVersion,\n      eventType,\n      eventData,\n      timestamp: Date.now(),\n    },\n  });\n};\n\n// Projections read from event stream\neventStore.grantStreamRead(projectionFunction);\n```\n\n## Serverless Architecture Patterns\n\n### Pattern 1: API-Driven Microservices\n\nREST APIs with Lambda backend:\n\n```typescript\nconst api = new apigateway.RestApi(this, 'Api', {\n  restApiName: 'microservices-api',\n  deployOptions: {\n    throttlingRateLimit: 1000,\n    throttlingBurstLimit: 2000,\n    tracingEnabled: true,\n  },\n});\n\n// User service\nconst users = api.root.addResource('users');\nusers.addMethod('GET', new apigateway.LambdaIntegration(getUsersFunction));\nusers.addMethod('POST', new apigateway.LambdaIntegration(createUserFunction));\n\n// Order service\nconst orders = api.root.addResource('orders');\norders.addMethod('GET', new apigateway.LambdaIntegration(getOrdersFunction));\norders.addMethod('POST', new apigateway.LambdaIntegration(createOrderFunction));\n```\n\n### Pattern 2: Stream Processing\n\nReal-time data processing with Kinesis:\n\n```typescript\nconst stream = new kinesis.Stream(this, 'DataStream', {\n  shardCount: 2,\n  retentionPeriod: Duration.days(7),\n});\n\n// Lambda processes stream records\nnew lambda.EventSourceMapping(this, 'StreamProcessor', {\n  target: processFunction,\n  eventSourceArn: stream.streamArn,\n  batchSize: 100,\n  maxBatchingWindow: Duration.seconds(5),\n  parallelizationFactor: 10,\n  startingPosition: lambda.StartingPosition.LATEST,\n  retryAttempts: 3,\n  bisectBatchOnError: true,\n  onFailure: new lambdaDestinations.SqsDestination(dlq),\n});\n```\n\n### Pattern 3: Async Task Processing\n\nBackground job processing:\n\n```typescript\n// SQS queue for tasks\nconst taskQueue = new sqs.Queue(this, 'TaskQueue', {\n  visibilityTimeout: Duration.minutes(5),\n  receiveMessageWaitTime: Duration.seconds(20), // Long polling\n  deadLetterQueue: {\n    queue: dlq,\n    maxReceiveCount: 3,\n  },\n});\n\n// Lambda worker processes tasks\nconst worker = new lambda.Function(this, 'TaskWorker', {\n  // ... configuration\n  reservedConcurrentExecutions: 10, // Control concurrency\n});\n\nnew lambda.EventSourceMapping(this, 'TaskConsumer', {\n  target: worker,\n  eventSourceArn: taskQueue.queueArn,\n  batchSize: 10,\n  reportBatchItemFailures: true, // Partial batch failure handling\n});\n```\n\n### Pattern 4: Scheduled Jobs\n\nPeriodic processing with EventBridge:\n\n```typescript\n// Daily cleanup job\nnew events.Rule(this, 'DailyCleanup', {\n  schedule: events.Schedule.cron({ hour: '2', minute: '0' }),\n  targets: [new targets.LambdaFunction(cleanupFunction)],\n});\n\n// Process every 5 minutes\nnew events.Rule(this, 'FrequentProcessing', {\n  schedule: events.Schedule.rate(Duration.minutes(5)),\n  targets: [new targets.LambdaFunction(processFunction)],\n});\n```\n\n### Pattern 5: Webhook Processing\n\nHandle external webhooks:\n\n```typescript\n// API Gateway endpoint for webhooks\nconst webhookApi = new apigateway.RestApi(this, 'WebhookApi', {\n  restApiName: 'webhooks',\n});\n\nconst webhook = webhookApi.root.addResource('webhook');\nwebhook.addMethod('POST', new apigateway.LambdaIntegration(webhookFunction, {\n  proxy: true,\n  timeout: Duration.seconds(29), // API Gateway max\n}));\n\n// Lambda handler validates and queues webhook\nexport const handler = async (event: APIGatewayProxyEvent) => {\n  // Validate webhook signature\n  const isValid = validateSignature(event.headers, event.body);\n  if (!isValid) {\n    return { statusCode: 401, body: 'Invalid signature' };\n  }\n\n  // Queue for async processing\n  await sqs.sendMessage({\n    QueueUrl: process.env.QUEUE_URL,\n    MessageBody: event.body,\n  });\n\n  // Return immediately\n  return { statusCode: 202, body: 'Accepted' };\n};\n```\n\n## Best Practices\n\n### Error Handling\n\n**Implement comprehensive error handling**:\n\n```typescript\nexport const handler = async (event: SQSEvent) => {\n  const failures: SQSBatchItemFailure[] = [];\n\n  for (const record of event.Records) {\n    try {\n      await processRecord(record);\n    } catch (error) {\n      console.error('Failed to process record:', record.messageId, error);\n      failures.push({ itemIdentifier: record.messageId });\n    }\n  }\n\n  // Return partial batch failures for retry\n  return { batchItemFailures: failures };\n};\n```\n\n### Dead Letter Queues\n\n**Always configure DLQs for error handling**:\n\n```typescript\nconst dlq = new sqs.Queue(this, 'DLQ', {\n  retentionPeriod: Duration.days(14),\n});\n\nconst queue = new sqs.Queue(this, 'Queue', {\n  deadLetterQueue: {\n    queue: dlq,\n    maxReceiveCount: 3,\n  },\n});\n\n// Monitor DLQ depth\nnew cloudwatch.Alarm(this, 'DLQAlarm', {\n  metric: dlq.metricApproximateNumberOfMessagesVisible(),\n  threshold: 1,\n  evaluationPeriods: 1,\n  alarmDescription: 'Messages in DLQ require attention',\n});\n```\n\n### Observability\n\n**Enable tracing and monitoring**:\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  tracing: lambda.Tracing.ACTIVE, // X-Ray tracing\n  environment: {\n    POWERTOOLS_SERVICE_NAME: 'order-service',\n    POWERTOOLS_METRICS_NAMESPACE: 'MyApp',\n    LOG_LEVEL: 'INFO',\n  },\n});\n```\n\n## Using MCP Servers Effectively\n\n### AWS Serverless MCP Usage\n\n**Lifecycle management**:\n- Initialize new serverless projects\n- Generate SAM templates\n- Deploy applications\n- Test locally before deployment\n\n### Lambda Tool MCP Usage\n\n**Function execution**:\n- Test Lambda functions directly\n- Execute automation workflows\n- Access private resources\n- Validate integrations\n\n### Step Functions MCP Usage\n\n**Workflow orchestration**:\n- Create state machines for complex workflows\n- Execute distributed transactions\n- Implement saga patterns\n- Coordinate microservices\n\n### SNS/SQS MCP Usage\n\n**Messaging operations**:\n- Test pub/sub patterns\n- Send test messages to queues\n- Validate event routing\n- Debug message processing\n\n## Additional Resources\n\nThis skill includes comprehensive reference documentation based on AWS best practices:\n\n- **Serverless Patterns**: `references/serverless-patterns.md`\n  - Core serverless architectures and API patterns\n  - Data processing and integration patterns\n  - Orchestration with Step Functions\n  - Anti-patterns to avoid\n\n- **Event-Driven Architecture Patterns**: `references/eda-patterns.md`\n  - Event routing and processing patterns\n  - Event sourcing and saga patterns\n  - Idempotency and error handling\n  - Message ordering and deduplication\n\n- **Security Best Practices**: `references/security-best-practices.md`\n  - Shared responsibility model\n  - IAM least privilege patterns\n  - Data protection and encryption\n  - Network security with VPC\n\n- **Observability Best Practices**: `references/observability-best-practices.md`\n  - Three pillars: metrics, logs, traces\n  - Structured logging with Lambda Powertools\n  - X-Ray distributed tracing\n  - CloudWatch alarms and dashboards\n\n- **Performance Optimization**: `references/performance-optimization.md`\n  - Cold start optimization techniques\n  - Memory and CPU optimization\n  - Package size reduction\n  - Provisioned concurrency patterns\n\n- **Deployment Best Practices**: `references/deployment-best-practices.md`\n  - CI/CD pipeline design\n  - Testing strategies (unit, integration, load)\n  - Deployment strategies (canary, blue/green)\n  - Rollback and safety mechanisms\n\n**External Resources**:\n- **AWS Well-Architected Serverless Lens**: https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/\n- **ServerlessLand.com**: Pre-built serverless patterns\n- **AWS Serverless Workshops**: https://serverlessland.com/learn?type=Workshops\n\nFor detailed implementation patterns, anti-patterns, and code examples, refer to the comprehensive references in the skill directory.\n",
        "plugins/serverless-eda/skills/aws-serverless-eda/references/deployment-best-practices.md": "# Serverless Deployment Best Practices\n\nDeployment best practices for serverless applications including CI/CD, testing, and deployment strategies.\n\n## Table of Contents\n\n- [Software Release Process](#software-release-process)\n- [Infrastructure as Code](#infrastructure-as-code)\n- [CI/CD Pipeline Design](#cicd-pipeline-design)\n- [Testing Strategies](#testing-strategies)\n- [Deployment Strategies](#deployment-strategies)\n- [Rollback and Safety](#rollback-and-safety)\n\n## Software Release Process\n\n### Four Stages of Release\n\n**1. Source Phase**:\n- Developers commit code changes\n- Code review (peer review)\n- Version control (Git)\n\n**2. Build Phase**:\n- Compile code\n- Run unit tests\n- Style checking and linting\n- Create deployment packages\n- Build container images\n\n**3. Test Phase**:\n- Integration tests with other systems\n- Load testing\n- UI testing\n- Security testing (penetration testing)\n- Acceptance testing\n\n**4. Production Phase**:\n- Deploy to production environment\n- Monitor for errors\n- Validate deployment success\n- Rollback if needed\n\n### CI/CD Maturity Levels\n\n**Continuous Integration (CI)**:\n- Automated build on code commit\n- Automated unit testing\n- Manual deployment to test/production\n\n**Continuous Delivery (CD)**:\n- Automated deployment to test environments\n- Manual approval for production\n- Automated testing in non-prod\n\n**Continuous Deployment**:\n- Fully automated pipeline\n- Automated deployment to production\n- No manual intervention after code commit\n\n## Infrastructure as Code\n\n### Framework Selection\n\n**AWS SAM (Serverless Application Model)**:\n\n```yaml\n# template.yaml\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  OrderFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: app.handler\n      Runtime: nodejs20.x\n      CodeUri: src/\n      Events:\n        Api:\n          Type: Api\n          Properties:\n            Path: /orders\n            Method: post\n```\n\n**Benefits**:\n- Simple, serverless-focused syntax\n- Built-in best practices\n- SAM CLI for local testing\n- Integrates with CodeDeploy\n\n**AWS CDK**:\n\n```typescript\nnew NodejsFunction(this, 'OrderFunction', {\n  entry: 'src/orders/handler.ts',\n  environment: {\n    TABLE_NAME: ordersTable.tableName,\n  },\n});\n\nordersTable.grantReadWriteData(orderFunction);\n```\n\n**Benefits**:\n- Type-safe, programmatic\n- Reusable constructs\n- Rich AWS service support\n- Better for complex infrastructure\n\n**When to use**:\n- **SAM**: Serverless-only applications, simpler projects\n- **CDK**: Complex infrastructure, multiple services, reusable patterns\n\n### Environment Management\n\n**Separate environments**:\n\n```typescript\n// CDK App\nconst app = new cdk.App();\n\nnew ServerlessStack(app, 'DevStack', {\n  env: { account: '111111111111', region: 'us-east-1' },\n  environment: 'dev',\n  logLevel: 'DEBUG',\n});\n\nnew ServerlessStack(app, 'ProdStack', {\n  env: { account: '222222222222', region: 'us-east-1' },\n  environment: 'prod',\n  logLevel: 'INFO',\n});\n```\n\n**SAM with parameters**:\n\n```yaml\nParameters:\n  Environment:\n    Type: String\n    Default: dev\n    AllowedValues:\n      - dev\n      - staging\n      - prod\n\nResources:\n  Function:\n    Type: AWS::Serverless::Function\n    Properties:\n      Environment:\n        Variables:\n          ENVIRONMENT: !Ref Environment\n          LOG_LEVEL: !If [IsProd, INFO, DEBUG]\n```\n\n## CI/CD Pipeline Design\n\n### AWS CodePipeline\n\n**Comprehensive pipeline**:\n\n```typescript\nimport * as codepipeline from 'aws-cdk-lib/aws-codepipeline';\nimport * as codepipeline_actions from 'aws-cdk-lib/aws-codepipeline-actions';\n\nconst sourceOutput = new codepipeline.Artifact();\nconst buildOutput = new codepipeline.Artifact();\n\nconst pipeline = new codepipeline.Pipeline(this, 'Pipeline', {\n  pipelineName: 'serverless-pipeline',\n});\n\n// Source stage\npipeline.addStage({\n  stageName: 'Source',\n  actions: [\n    new codepipeline_actions.CodeStarConnectionsSourceAction({\n      actionName: 'GitHub_Source',\n      owner: 'myorg',\n      repo: 'myrepo',\n      branch: 'main',\n      output: sourceOutput,\n      connectionArn: githubConnection.connectionArn,\n    }),\n  ],\n});\n\n// Build stage\npipeline.addStage({\n  stageName: 'Build',\n  actions: [\n    new codepipeline_actions.CodeBuildAction({\n      actionName: 'Build',\n      project: buildProject,\n      input: sourceOutput,\n      outputs: [buildOutput],\n    }),\n  ],\n});\n\n// Test stage\npipeline.addStage({\n  stageName: 'Test',\n  actions: [\n    new codepipeline_actions.CloudFormationCreateUpdateStackAction({\n      actionName: 'Deploy_Test',\n      templatePath: buildOutput.atPath('packaged.yaml'),\n      stackName: 'test-stack',\n      adminPermissions: true,\n    }),\n    new codepipeline_actions.CodeBuildAction({\n      actionName: 'Integration_Tests',\n      project: testProject,\n      input: buildOutput,\n      runOrder: 2,\n    }),\n  ],\n});\n\n// Production stage (with manual approval)\npipeline.addStage({\n  stageName: 'Production',\n  actions: [\n    new codepipeline_actions.ManualApprovalAction({\n      actionName: 'Approve',\n    }),\n    new codepipeline_actions.CloudFormationCreateUpdateStackAction({\n      actionName: 'Deploy_Prod',\n      templatePath: buildOutput.atPath('packaged.yaml'),\n      stackName: 'prod-stack',\n      adminPermissions: true,\n      runOrder: 2,\n    }),\n  ],\n});\n```\n\n### GitHub Actions\n\n**Serverless deployment workflow**:\n\n```yaml\n# .github/workflows/deploy.yml\nname: Deploy Serverless Application\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '20'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run tests\n        run: npm test\n\n      - name: Setup SAM CLI\n        uses: aws-actions/setup-sam@v2\n\n      - name: Build SAM application\n        run: sam build\n\n      - name: Deploy to Dev\n        if: github.ref != 'refs/heads/main'\n        run: |\n          sam deploy \\\n            --no-confirm-changeset \\\n            --no-fail-on-empty-changeset \\\n            --stack-name dev-stack \\\n            --parameter-overrides Environment=dev\n\n      - name: Run integration tests\n        run: npm run test:integration\n\n      - name: Deploy to Prod\n        if: github.ref == 'refs/heads/main'\n        run: |\n          sam deploy \\\n            --no-confirm-changeset \\\n            --no-fail-on-empty-changeset \\\n            --stack-name prod-stack \\\n            --parameter-overrides Environment=prod\n```\n\n## Testing Strategies\n\n### Unit Testing\n\n**Test business logic independently**:\n\n```typescript\n// handler.ts\nexport const processOrder = (order: Order): ProcessedOrder => {\n  // Pure business logic (easily testable)\n  validateOrder(order);\n  calculateTotal(order);\n  return transformOrder(order);\n};\n\nexport const handler = async (event: any) => {\n  const order = parseEvent(event);\n  const processed = processOrder(order); // Testable function\n  await saveToDatabase(processed);\n  return formatResponse(processed);\n};\n\n// handler.test.ts\nimport { processOrder } from './handler';\n\ndescribe('processOrder', () => {\n  it('calculates total correctly', () => {\n    const order = {\n      items: [\n        { price: 10, quantity: 2 },\n        { price: 5, quantity: 3 },\n      ],\n    };\n\n    const result = processOrder(order);\n\n    expect(result.total).toBe(35);\n  });\n\n  it('throws on invalid order', () => {\n    const invalid = { items: [] };\n    expect(() => processOrder(invalid)).toThrow();\n  });\n});\n```\n\n### Integration Testing\n\n**Test in actual AWS environment**:\n\n```typescript\n// integration.test.ts\nimport { LambdaClient, InvokeCommand } from '@aws-sdk/client-lambda';\nimport { DynamoDBClient, GetItemCommand } from '@aws-sdk/client-dynamodb';\n\ndescribe('Order Processing Integration', () => {\n  const lambda = new LambdaClient({});\n  const dynamodb = new DynamoDBClient({});\n\n  it('processes order end-to-end', async () => {\n    // Invoke Lambda\n    const response = await lambda.send(new InvokeCommand({\n      FunctionName: process.env.FUNCTION_NAME,\n      Payload: JSON.stringify({\n        orderId: 'test-123',\n        items: [{ productId: 'prod-1', quantity: 2 }],\n      }),\n    }));\n\n    const result = JSON.parse(Buffer.from(response.Payload!).toString());\n\n    expect(result.statusCode).toBe(200);\n\n    // Verify database write\n    const dbResult = await dynamodb.send(new GetItemCommand({\n      TableName: process.env.TABLE_NAME,\n      Key: { orderId: { S: 'test-123' } },\n    }));\n\n    expect(dbResult.Item).toBeDefined();\n    expect(dbResult.Item?.status.S).toBe('PROCESSED');\n  });\n});\n```\n\n### Local Testing with SAM\n\n**Test locally before deployment**:\n\n```bash\n# Start local API\nsam local start-api\n\n# Invoke function locally\nsam local invoke OrderFunction -e events/create-order.json\n\n# Generate sample events\nsam local generate-event apigateway aws-proxy > event.json\n\n# Debug locally\nsam local invoke OrderFunction -d 5858\n\n# Test with Docker\nsam local start-api --docker-network my-network\n```\n\n### Load Testing\n\n**Test under production load**:\n\n```bash\n# Install Artillery\nnpm install -g artillery\n\n# Create load test\ncat > load-test.yml <<EOF\nconfig:\n  target: https://api.example.com\n  phases:\n    - duration: 300 # 5 minutes\n      arrivalRate: 50 # 50 requests/second\n      rampTo: 200 # Ramp to 200 req/sec\nscenarios:\n  - flow:\n      - post:\n          url: /orders\n          json:\n            orderId: \"{{ $randomString() }}\"\nEOF\n\n# Run load test\nartillery run load-test.yml --output report.json\n\n# Generate HTML report\nartillery report report.json\n```\n\n## Deployment Strategies\n\n### All-at-Once Deployment\n\n**Simple, fast, risky**:\n\n```yaml\n# SAM template\nResources:\n  OrderFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      DeploymentPreference:\n        Type: AllAtOnce # Deploy immediately\n```\n\n**Use for**:\n- Development environments\n- Non-critical applications\n- Quick hotfixes (with caution)\n\n### Blue/Green Deployment\n\n**Zero-downtime deployment**:\n\n```yaml\nResources:\n  OrderFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      AutoPublishAlias: live\n      DeploymentPreference:\n        Type: Linear10PercentEvery1Minute\n        Alarms:\n          - !Ref ErrorAlarm\n          - !Ref LatencyAlarm\n```\n\n**Deployment types**:\n- **Linear10PercentEvery1Minute**: 10% traffic shift every minute\n- **Linear10PercentEvery2Minutes**: Slower, more conservative\n- **Linear10PercentEvery3Minutes**: Even slower\n- **Linear10PercentEvery10Minutes**: Very gradual\n- **Canary10Percent5Minutes**: 10% for 5 min, then 100%\n- **Canary10Percent10Minutes**: 10% for 10 min, then 100%\n- **Canary10Percent30Minutes**: 10% for 30 min, then 100%\n\n### Canary Deployment\n\n**Test with subset of traffic**:\n\n```yaml\nResources:\n  OrderFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      AutoPublishAlias: live\n      DeploymentPreference:\n        Type: Canary10Percent10Minutes\n        Alarms:\n          - !Ref ErrorAlarm\n          - !Ref LatencyAlarm\n        Hooks:\n          PreTraffic: !Ref PreTrafficHook\n          PostTraffic: !Ref PostTrafficHook\n\n  PreTrafficHook:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: hooks.pre_traffic\n      Runtime: python3.12\n      # Runs before traffic shift\n      # Validates new version\n\n  PostTrafficHook:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: hooks.post_traffic\n      Runtime: python3.12\n      # Runs after traffic shift\n      # Validates deployment success\n```\n\n**CDK with CodeDeploy**:\n\n```typescript\nimport * as codedeploy from 'aws-cdk-lib/aws-codedeploy';\n\nconst alias = fn.currentVersion.addAlias('live');\n\nnew codedeploy.LambdaDeploymentGroup(this, 'DeploymentGroup', {\n  alias,\n  deploymentConfig: codedeploy.LambdaDeploymentConfig.CANARY_10PERCENT_10MINUTES,\n  alarms: [errorAlarm, latencyAlarm],\n  autoRollback: {\n    failedDeployment: true,\n    stoppedDeployment: true,\n    deploymentInAlarm: true,\n  },\n});\n```\n\n### Deployment Hooks\n\n**Pre-traffic hook (validation)**:\n\n```python\n# hooks.py\nimport boto3\n\nlambda_client = boto3.client('lambda')\ncodedeploy = boto3.client('codedeploy')\n\ndef pre_traffic(event, context):\n    \"\"\"\n    Validate new version before traffic shift\n    \"\"\"\n    function_name = event['DeploymentId']\n    version = event['NewVersion']\n\n    try:\n        # Invoke new version with test payload\n        response = lambda_client.invoke(\n            FunctionName=f\"{function_name}:{version}\",\n            InvocationType='RequestResponse',\n            Payload=json.dumps({'test': True})\n        )\n\n        # Validate response\n        if response['StatusCode'] == 200:\n            codedeploy.put_lifecycle_event_hook_execution_status(\n                deploymentId=event['DeploymentId'],\n                lifecycleEventHookExecutionId=event['LifecycleEventHookExecutionId'],\n                status='Succeeded'\n            )\n        else:\n            raise Exception('Validation failed')\n\n    except Exception as e:\n        print(f'Pre-traffic validation failed: {e}')\n        codedeploy.put_lifecycle_event_hook_execution_status(\n            deploymentId=event['DeploymentId'],\n            lifecycleEventHookExecutionId=event['LifecycleEventHookExecutionId'],\n            status='Failed'\n        )\n```\n\n**Post-traffic hook (verification)**:\n\n```python\ndef post_traffic(event, context):\n    \"\"\"\n    Verify deployment success after traffic shift\n    \"\"\"\n    try:\n        # Check CloudWatch metrics\n        cloudwatch = boto3.client('cloudwatch')\n\n        metrics = cloudwatch.get_metric_statistics(\n            Namespace='AWS/Lambda',\n            MetricName='Errors',\n            Dimensions=[{'Name': 'FunctionName', 'Value': function_name}],\n            StartTime=deployment_start_time,\n            EndTime=datetime.utcnow(),\n            Period=300,\n            Statistics=['Sum']\n        )\n\n        # Validate no errors\n        total_errors = sum(point['Sum'] for point in metrics['Datapoints'])\n\n        if total_errors == 0:\n            codedeploy.put_lifecycle_event_hook_execution_status(\n                deploymentId=event['DeploymentId'],\n                lifecycleEventHookExecutionId=event['LifecycleEventHookExecutionId'],\n                status='Succeeded'\n            )\n        else:\n            raise Exception(f'{total_errors} errors detected')\n\n    except Exception as e:\n        print(f'Post-traffic verification failed: {e}')\n        codedeploy.put_lifecycle_event_hook_execution_status(\n            deploymentId=event['DeploymentId'],\n            lifecycleEventHookExecutionId=event['LifecycleEventHookExecutionId'],\n            status='Failed'\n        )\n```\n\n## Rollback and Safety\n\n### Automatic Rollback\n\n**Configure rollback triggers**:\n\n```yaml\nDeploymentPreference:\n  Type: Canary10Percent10Minutes\n  Alarms:\n    - !Ref ErrorAlarm\n    - !Ref LatencyAlarm\n  # Automatically rolls back if alarms trigger\n```\n\n**Rollback scenarios**:\n- CloudWatch alarm triggers during deployment\n- Pre-traffic hook fails\n- Post-traffic hook fails\n- Deployment manually stopped\n\n### CloudWatch Alarms for Deployment\n\n**Critical alarms during deployment**:\n\n```typescript\n// Error rate alarm\nconst errorAlarm = new cloudwatch.Alarm(this, 'ErrorAlarm', {\n  metric: fn.metricErrors({\n    statistic: 'Sum',\n    period: Duration.minutes(1),\n  }),\n  threshold: 5,\n  evaluationPeriods: 2,\n  treatMissingData: cloudwatch.TreatMissingData.NOT_BREACHING,\n});\n\n// Duration alarm (regression)\nconst durationAlarm = new cloudwatch.Alarm(this, 'DurationAlarm', {\n  metric: fn.metricDuration({\n    statistic: 'Average',\n    period: Duration.minutes(1),\n  }),\n  threshold: previousAvgDuration * 1.2, // 20% increase\n  evaluationPeriods: 2,\n  comparisonOperator: cloudwatch.ComparisonOperator.GREATER_THAN_THRESHOLD,\n});\n\n// Throttle alarm\nconst throttleAlarm = new cloudwatch.Alarm(this, 'ThrottleAlarm', {\n  metric: fn.metricThrottles({\n    statistic: 'Sum',\n    period: Duration.minutes(1),\n  }),\n  threshold: 1,\n  evaluationPeriods: 1,\n});\n```\n\n### Version Management\n\n**Use Lambda versions and aliases**:\n\n```typescript\nconst version = fn.currentVersion;\n\nconst prodAlias = version.addAlias('prod');\nconst devAlias = version.addAlias('dev');\n\n// Gradual rollout with weighted aliases\nnew lambda.Alias(this, 'LiveAlias', {\n  aliasName: 'live',\n  version: newVersion,\n  additionalVersions: [\n    { version: oldVersion, weight: 0.9 }, // 90% old\n    // 10% automatically goes to main version (new)\n  ],\n});\n```\n\n## Best Practices Checklist\n\n### Pre-Deployment\n\n- [ ] Code review completed\n- [ ] Unit tests passing\n- [ ] Integration tests passing\n- [ ] Security scan completed\n- [ ] Dependencies updated\n- [ ] Infrastructure validated (CDK synth, SAM validate)\n- [ ] Environment variables configured\n\n### Deployment\n\n- [ ] Use IaC (SAM, CDK, Terraform)\n- [ ] Separate environments (dev, staging, prod)\n- [ ] Automate deployments via CI/CD\n- [ ] Use gradual deployment (canary or linear)\n- [ ] Configure CloudWatch alarms\n- [ ] Enable automatic rollback\n- [ ] Use deployment hooks for validation\n\n### Post-Deployment\n\n- [ ] Monitor CloudWatch metrics\n- [ ] Check CloudWatch Logs for errors\n- [ ] Verify X-Ray traces\n- [ ] Validate business metrics\n- [ ] Check alarm status\n- [ ] Review deployment logs\n- [ ] Document any issues\n\n### Rollback Preparation\n\n- [ ] Keep previous version available\n- [ ] Document rollback procedure\n- [ ] Test rollback in non-prod\n- [ ] Configure automatic rollback\n- [ ] Monitor during rollback\n- [ ] Communication plan for rollback\n\n## Deployment Patterns\n\n### Multi-Region Deployment\n\n**Active-Passive**:\n\n```typescript\n// Primary region\nnew ServerlessStack(app, 'PrimaryStack', {\n  env: { region: 'us-east-1' },\n  isPrimary: true,\n});\n\n// Secondary region (standby)\nnew ServerlessStack(app, 'SecondaryStack', {\n  env: { region: 'us-west-2' },\n  isPrimary: false,\n});\n\n// Route 53 health check and failover\nconst healthCheck = new route53.CfnHealthCheck(this, 'HealthCheck', {\n  type: 'HTTPS',\n  resourcePath: '/health',\n  fullyQualifiedDomainName: 'api.example.com',\n});\n```\n\n**Active-Active**:\n\n```typescript\n// Deploy to multiple regions\nconst regions = ['us-east-1', 'us-west-2', 'eu-west-1'];\n\nfor (const region of regions) {\n  new ServerlessStack(app, `Stack-${region}`, {\n    env: { region },\n  });\n}\n\n// Route 53 geolocation routing\nnew route53.ARecord(this, 'GeoRecord', {\n  zone: hostedZone,\n  recordName: 'api',\n  target: route53.RecordTarget.fromAlias(\n    new targets.ApiGatewayDomain(domain)\n  ),\n  geoLocation: route53.GeoLocation.country('US'),\n});\n```\n\n### Feature Flags with AppConfig\n\n**Safe feature rollout**:\n\n```typescript\nimport { AppConfigData } from '@aws-sdk/client-appconfigdata';\n\nconst appconfig = new AppConfigData({});\n\nexport const handler = async (event: any) => {\n  // Fetch feature flags\n  const config = await appconfig.getLatestConfiguration({\n    ConfigurationToken: token,\n  });\n\n  const features = JSON.parse(config.Configuration.toString());\n\n  if (features.newFeatureEnabled) {\n    return newFeatureHandler(event);\n  }\n\n  return legacyHandler(event);\n};\n```\n\n## Summary\n\n- **IaC**: Use SAM or CDK for all deployments\n- **Environments**: Separate dev, staging, production\n- **CI/CD**: Automate build, test, and deployment\n- **Testing**: Unit, integration, and load testing\n- **Gradual Deployment**: Use canary or linear for production\n- **Alarms**: Configure and monitor during deployment\n- **Rollback**: Enable automatic rollback on failures\n- **Hooks**: Validate before and after traffic shifts\n- **Versioning**: Use Lambda versions and aliases\n- **Multi-Region**: Plan for disaster recovery\n",
        "plugins/serverless-eda/skills/aws-serverless-eda/references/eda-patterns.md": "# Event-Driven Architecture Patterns\n\nComprehensive patterns for building event-driven systems on AWS with serverless technologies.\n\n## Table of Contents\n\n- [Core EDA Concepts](#core-eda-concepts)\n- [Event Routing Patterns](#event-routing-patterns)\n- [Event Processing Patterns](#event-processing-patterns)\n- [Event Sourcing Patterns](#event-sourcing-patterns)\n- [Saga Patterns](#saga-patterns)\n- [Best Practices](#best-practices)\n\n## Core EDA Concepts\n\n### Event Types\n\n**Domain Events**: Represent business facts\n```json\n{\n  \"source\": \"orders\",\n  \"detailType\": \"OrderPlaced\",\n  \"detail\": {\n    \"orderId\": \"12345\",\n    \"customerId\": \"customer-1\",\n    \"amount\": 100.00,\n    \"timestamp\": \"2025-01-15T10:30:00Z\"\n  }\n}\n```\n\n**System Events**: Technical occurrences\n```json\n{\n  \"source\": \"aws.s3\",\n  \"detailType\": \"Object Created\",\n  \"detail\": {\n    \"bucket\": \"my-bucket\",\n    \"key\": \"data/file.json\"\n  }\n}\n```\n\n### Event Contracts\n\nDefine clear contracts between producers and consumers:\n\n```typescript\n// schemas/order-events.ts\nexport interface OrderPlacedEvent {\n  orderId: string;\n  customerId: string;\n  items: Array<{\n    productId: string;\n    quantity: number;\n    price: number;\n  }>;\n  totalAmount: number;\n  timestamp: string;\n}\n\n// Register schema with EventBridge\nconst registry = new events.EventBusSchemaRegistry(this, 'SchemaRegistry');\n\nconst schema = new events.Schema(this, 'OrderPlacedSchema', {\n  schemaName: 'OrderPlaced',\n  definition: events.SchemaDefinition.fromInline(/* JSON Schema */),\n});\n```\n\n## Event Routing Patterns\n\n### Pattern 1: Content-Based Routing\n\nRoute events based on content:\n\n```typescript\n// Route by order amount\nnew events.Rule(this, 'HighValueOrders', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n    detail: {\n      totalAmount: [{ numeric: ['>', 1000] }],\n    },\n  },\n  targets: [new targets.LambdaFunction(highValueOrderFunction)],\n});\n\nnew events.Rule(this, 'StandardOrders', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n    detail: {\n      totalAmount: [{ numeric: ['<=', 1000] }],\n    },\n  },\n  targets: [new targets.LambdaFunction(standardOrderFunction)],\n});\n```\n\n### Pattern 2: Event Filtering\n\nFilter events before processing:\n\n```typescript\n// Filter by multiple criteria\nnew events.Rule(this, 'FilteredRule', {\n  eventPattern: {\n    source: ['inventory'],\n    detailType: ['StockUpdate'],\n    detail: {\n      warehouseId: ['WH-1', 'WH-2'], // Specific warehouses\n      quantity: [{ numeric: ['<', 10] }], // Low stock only\n      productCategory: ['electronics'], // Specific category\n    },\n  },\n  targets: [new targets.LambdaFunction(reorderFunction)],\n});\n```\n\n### Pattern 3: Event Replay and Archive\n\nStore events for replay and audit:\n\n```typescript\n// Archive all events\nconst archive = new events.Archive(this, 'EventArchive', {\n  eventPattern: {\n    account: [this.account],\n  },\n  retention: Duration.days(365),\n});\n\n// Replay events when needed\n// Use AWS Console or CLI to replay from archive\n```\n\n### Pattern 4: Cross-Account Event Routing\n\nRoute events to other AWS accounts:\n\n```typescript\n// Event bus in Account A\nconst eventBus = new events.EventBus(this, 'SharedBus');\n\n// Grant permission to Account B\neventBus.addToResourcePolicy(new iam.PolicyStatement({\n  effect: iam.Effect.ALLOW,\n  principals: [new iam.AccountPrincipal('ACCOUNT-B-ID')],\n  actions: ['events:PutEvents'],\n  resources: [eventBus.eventBusArn],\n}));\n\n// Rule forwards to Account B event bus\nnew events.Rule(this, 'ForwardToAccountB', {\n  eventBus,\n  eventPattern: {\n    source: ['shared-service'],\n  },\n  targets: [new targets.EventBus(\n    events.EventBus.fromEventBusArn(\n      this,\n      'AccountBBus',\n      'arn:aws:events:us-east-1:ACCOUNT-B-ID:event-bus/default'\n    )\n  )],\n});\n```\n\n## Event Processing Patterns\n\n### Pattern 1: Event Transformation\n\nTransform events before routing:\n\n```typescript\n// EventBridge input transformer\nnew events.Rule(this, 'TransformRule', {\n  eventPattern: {\n    source: ['orders'],\n  },\n  targets: [new targets.LambdaFunction(processFunction, {\n    event: events.RuleTargetInput.fromObject({\n      orderId: events.EventField.fromPath('$.detail.orderId'),\n      customerEmail: events.EventField.fromPath('$.detail.customer.email'),\n      amount: events.EventField.fromPath('$.detail.totalAmount'),\n      // Transformed structure\n    }),\n  })],\n});\n```\n\n### Pattern 2: Event Aggregation\n\nAggregate multiple events:\n\n```typescript\n// DynamoDB stores partial results\nexport const handler = async (event: any) => {\n  const { transactionId, step, data } = event;\n\n  // Store step result\n  await dynamodb.updateItem({\n    TableName: process.env.TABLE_NAME,\n    Key: { transactionId },\n    UpdateExpression: 'SET #step = :data',\n    ExpressionAttributeNames: { '#step': step },\n    ExpressionAttributeValues: { ':data': data },\n  });\n\n  // Check if all steps complete\n  const item = await dynamodb.getItem({\n    TableName: process.env.TABLE_NAME,\n    Key: { transactionId },\n  });\n\n  if (allStepsComplete(item)) {\n    // Trigger final processing\n    await eventBridge.putEvents({\n      Entries: [{\n        Source: 'aggregator',\n        DetailType: 'AllStepsComplete',\n        Detail: JSON.stringify(item),\n      }],\n    });\n  }\n};\n```\n\n### Pattern 3: Event Enrichment\n\nEnrich events with additional data:\n\n```typescript\nexport const enrichEvent = async (event: any) => {\n  const { customerId } = event.detail;\n\n  // Fetch additional customer data\n  const customer = await dynamodb.getItem({\n    TableName: process.env.CUSTOMER_TABLE,\n    Key: { customerId },\n  });\n\n  // Publish enriched event\n  await eventBridge.putEvents({\n    Entries: [{\n      Source: 'orders',\n      DetailType: 'OrderEnriched',\n      Detail: JSON.stringify({\n        ...event.detail,\n        customerName: customer.Item?.name,\n        customerTier: customer.Item?.tier,\n        customerEmail: customer.Item?.email,\n      }),\n    }],\n  });\n};\n```\n\n### Pattern 4: Event Fork and Join\n\nProcess event multiple ways then aggregate:\n\n```typescript\n// Step Functions parallel + aggregation\nconst parallel = new stepfunctions.Parallel(this, 'ForkProcessing');\n\nparallel.branch(new tasks.LambdaInvoke(this, 'ValidateInventory', {\n  lambdaFunction: inventoryFunction,\n  resultPath: '$.inventory',\n}));\n\nparallel.branch(new tasks.LambdaInvoke(this, 'CheckCredit', {\n  lambdaFunction: creditFunction,\n  resultPath: '$.credit',\n}));\n\nparallel.branch(new tasks.LambdaInvoke(this, 'CalculateShipping', {\n  lambdaFunction: shippingFunction,\n  resultPath: '$.shipping',\n}));\n\nconst definition = parallel.next(\n  new tasks.LambdaInvoke(this, 'AggregateResults', {\n    lambdaFunction: aggregateFunction,\n  })\n);\n```\n\n## Event Sourcing Patterns\n\n### Pattern: Event Store with DynamoDB\n\nStore all events as source of truth:\n\n```typescript\nconst eventStore = new dynamodb.Table(this, 'EventStore', {\n  partitionKey: { name: 'aggregateId', type: dynamodb.AttributeType.STRING },\n  sortKey: { name: 'version', type: dynamodb.AttributeType.NUMBER },\n  stream: dynamodb.StreamViewType.NEW_IMAGE,\n  pointInTimeRecovery: true, // Important for audit\n});\n\n// Append events\nexport const appendEvent = async (aggregateId: string, event: any) => {\n  const version = await getNextVersion(aggregateId);\n\n  await dynamodb.putItem({\n    TableName: process.env.EVENT_STORE,\n    Item: {\n      aggregateId,\n      version,\n      eventType: event.type,\n      eventData: event.data,\n      timestamp: Date.now(),\n      userId: event.userId,\n    },\n    ConditionExpression: 'attribute_not_exists(version)', // Optimistic locking\n  });\n};\n\n// Rebuild state from events\nexport const rebuildState = async (aggregateId: string) => {\n  const events = await dynamodb.query({\n    TableName: process.env.EVENT_STORE,\n    KeyConditionExpression: 'aggregateId = :id',\n    ExpressionAttributeValues: { ':id': aggregateId },\n    ScanIndexForward: true, // Chronological order\n  });\n\n  let state = initialState();\n  for (const event of events.Items) {\n    state = applyEvent(state, event);\n  }\n\n  return state;\n};\n```\n\n### Pattern: Materialized Views\n\nCreate read-optimized projections:\n\n```typescript\n// Event store stream triggers projection\neventStore.grantStreamRead(projectionFunction);\n\nnew lambda.EventSourceMapping(this, 'Projection', {\n  target: projectionFunction,\n  eventSourceArn: eventStore.tableStreamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n});\n\n// Projection function updates read model\nexport const updateProjection = async (event: DynamoDBStreamEvent) => {\n  for (const record of event.Records) {\n    if (record.eventName !== 'INSERT') continue;\n\n    const eventData = record.dynamodb?.NewImage;\n    const aggregateId = eventData?.aggregateId.S;\n\n    // Rebuild current state\n    const currentState = await rebuildState(aggregateId);\n\n    // Update read model\n    await readModelTable.putItem({\n      TableName: process.env.READ_MODEL_TABLE,\n      Item: currentState,\n    });\n  }\n};\n```\n\n### Pattern: Snapshots\n\nOptimize event replay with snapshots:\n\n```typescript\nexport const createSnapshot = async (aggregateId: string) => {\n  // Rebuild state from all events\n  const state = await rebuildState(aggregateId);\n  const version = await getLatestVersion(aggregateId);\n\n  // Store snapshot\n  await snapshotTable.putItem({\n    TableName: process.env.SNAPSHOT_TABLE,\n    Item: {\n      aggregateId,\n      version,\n      state: JSON.stringify(state),\n      createdAt: Date.now(),\n    },\n  });\n};\n\n// Rebuild from snapshot + newer events\nexport const rebuildFromSnapshot = async (aggregateId: string) => {\n  // Get latest snapshot\n  const snapshot = await getLatestSnapshot(aggregateId);\n\n  let state = JSON.parse(snapshot.state);\n  const snapshotVersion = snapshot.version;\n\n  // Apply only events after snapshot\n  const events = await getEventsSinceVersion(aggregateId, snapshotVersion);\n\n  for (const event of events) {\n    state = applyEvent(state, event);\n  }\n\n  return state;\n};\n```\n\n## Saga Patterns\n\n### Pattern: Choreography-Based Saga\n\nServices coordinate through events:\n\n```typescript\n// Order Service publishes event\nexport const placeOrder = async (order: Order) => {\n  await saveOrder(order);\n\n  await eventBridge.putEvents({\n    Entries: [{\n      Source: 'orders',\n      DetailType: 'OrderPlaced',\n      Detail: JSON.stringify({ orderId: order.id }),\n    }],\n  });\n};\n\n// Inventory Service reacts to event\nnew events.Rule(this, 'ReserveInventory', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n  targets: [new targets.LambdaFunction(reserveInventoryFunction)],\n});\n\n// Inventory Service publishes result\nexport const reserveInventory = async (event: any) => {\n  const { orderId } = event.detail;\n\n  try {\n    await reserve(orderId);\n\n    await eventBridge.putEvents({\n      Entries: [{\n        Source: 'inventory',\n        DetailType: 'InventoryReserved',\n        Detail: JSON.stringify({ orderId }),\n      }],\n    });\n  } catch (error) {\n    await eventBridge.putEvents({\n      Entries: [{\n        Source: 'inventory',\n        DetailType: 'InventoryReservationFailed',\n        Detail: JSON.stringify({ orderId, error: error.message }),\n      }],\n    });\n  }\n};\n\n// Payment Service reacts to inventory event\nnew events.Rule(this, 'ProcessPayment', {\n  eventPattern: {\n    source: ['inventory'],\n    detailType: ['InventoryReserved'],\n  },\n  targets: [new targets.LambdaFunction(processPaymentFunction)],\n});\n```\n\n### Pattern: Orchestration-Based Saga\n\nCentral coordinator manages saga:\n\n```typescript\n// Step Functions orchestrates saga\nconst definition = new tasks.LambdaInvoke(this, 'ReserveInventory', {\n  lambdaFunction: reserveInventoryFunction,\n  resultPath: '$.inventory',\n})\n  .next(new tasks.LambdaInvoke(this, 'ProcessPayment', {\n    lambdaFunction: processPaymentFunction,\n    resultPath: '$.payment',\n  }))\n  .next(new tasks.LambdaInvoke(this, 'ShipOrder', {\n    lambdaFunction: shipOrderFunction,\n    resultPath: '$.shipment',\n  }))\n  .addCatch(\n    // Compensation flow\n    new tasks.LambdaInvoke(this, 'RefundPayment', {\n      lambdaFunction: refundFunction,\n    })\n      .next(new tasks.LambdaInvoke(this, 'ReleaseInventory', {\n        lambdaFunction: releaseFunction,\n      })),\n    {\n      errors: ['States.TaskFailed'],\n      resultPath: '$.error',\n    }\n  );\n\nnew stepfunctions.StateMachine(this, 'OrderSaga', {\n  definition,\n  tracingEnabled: true,\n});\n```\n\n**Comparison**:\n\n| Aspect | Choreography | Orchestration |\n|--------|--------------|---------------|\n| Coordination | Decentralized | Centralized |\n| Coupling | Loose | Tighter |\n| Visibility | Distributed logs | Single execution history |\n| Debugging | Harder (trace across services) | Easier (single workflow) |\n| Best for | Simple flows | Complex flows |\n\n## Best Practices\n\n### Idempotency\n\n**Always make event handlers idempotent**:\n\n```typescript\n// Use idempotency keys\nexport const handler = async (event: any) => {\n  const idempotencyKey = event.requestId || event.messageId;\n\n  // Check if already processed\n  try {\n    const existing = await dynamodb.getItem({\n      TableName: process.env.IDEMPOTENCY_TABLE,\n      Key: { idempotencyKey },\n    });\n\n    if (existing.Item) {\n      console.log('Already processed:', idempotencyKey);\n      return existing.Item.result; // Return cached result\n    }\n  } catch (error) {\n    // First time processing\n  }\n\n  // Process event\n  const result = await processEvent(event);\n\n  // Store result\n  await dynamodb.putItem({\n    TableName: process.env.IDEMPOTENCY_TABLE,\n    Item: {\n      idempotencyKey,\n      result,\n      processedAt: Date.now(),\n    },\n    // Optional: Set TTL for cleanup\n    ExpirationTime: Math.floor(Date.now() / 1000) + 86400, // 24 hours\n  });\n\n  return result;\n};\n```\n\n### Event Versioning\n\n**Handle event schema evolution**:\n\n```typescript\n// Version events\ninterface OrderPlacedEventV1 {\n  version: '1.0';\n  orderId: string;\n  amount: number;\n}\n\ninterface OrderPlacedEventV2 {\n  version: '2.0';\n  orderId: string;\n  amount: number;\n  currency: string; // New field\n}\n\n// Handler supports multiple versions\nexport const handler = async (event: any) => {\n  const eventVersion = event.detail.version || '1.0';\n\n  switch (eventVersion) {\n    case '1.0':\n      return processV1(event.detail as OrderPlacedEventV1);\n    case '2.0':\n      return processV2(event.detail as OrderPlacedEventV2);\n    default:\n      throw new Error(`Unsupported event version: ${eventVersion}`);\n  }\n};\n\nconst processV1 = async (event: OrderPlacedEventV1) => {\n  // Upgrade to V2 internally\n  const v2Event: OrderPlacedEventV2 = {\n    ...event,\n    version: '2.0',\n    currency: 'USD', // Default value\n  };\n  return processV2(v2Event);\n};\n```\n\n### Eventual Consistency\n\n**Design for eventual consistency**:\n\n```typescript\n// Service A writes to its database\nexport const createOrder = async (order: Order) => {\n  // Write to Order database\n  await orderTable.putItem({ Item: order });\n\n  // Publish event\n  await eventBridge.putEvents({\n    Entries: [{\n      Source: 'orders',\n      DetailType: 'OrderCreated',\n      Detail: JSON.stringify({ orderId: order.id }),\n    }],\n  });\n};\n\n// Service B eventually updates its database\nexport const onOrderCreated = async (event: any) => {\n  const { orderId } = event.detail;\n\n  // Fetch additional data\n  const orderDetails = await getOrderDetails(orderId);\n\n  // Update inventory database (eventual consistency)\n  await inventoryTable.updateItem({\n    Key: { productId: orderDetails.productId },\n    UpdateExpression: 'SET reserved = reserved + :qty',\n    ExpressionAttributeValues: { ':qty': orderDetails.quantity },\n  });\n};\n```\n\n### Error Handling in EDA\n\n**Comprehensive error handling strategy**:\n\n```typescript\n// Dead Letter Queue for failed events\nconst dlq = new sqs.Queue(this, 'EventDLQ', {\n  retentionPeriod: Duration.days(14),\n});\n\n// EventBridge rule with DLQ\nnew events.Rule(this, 'ProcessRule', {\n  eventPattern: { /* ... */ },\n  targets: [\n    new targets.LambdaFunction(processFunction, {\n      deadLetterQueue: dlq,\n      maxEventAge: Duration.hours(2),\n      retryAttempts: 2,\n    }),\n  ],\n});\n\n// Monitor DLQ\nnew cloudwatch.Alarm(this, 'DLQAlarm', {\n  metric: dlq.metricApproximateNumberOfMessagesVisible(),\n  threshold: 1,\n  evaluationPeriods: 1,\n});\n\n// DLQ processor for manual review\nnew lambda.EventSourceMapping(this, 'DLQProcessor', {\n  target: dlqProcessorFunction,\n  eventSourceArn: dlq.queueArn,\n  enabled: false, // Enable manually when reviewing\n});\n```\n\n### Message Ordering\n\n**When order matters**:\n\n```typescript\n// SQS FIFO for strict ordering\nconst fifoQueue = new sqs.Queue(this, 'OrderedQueue', {\n  fifo: true,\n  contentBasedDeduplication: true,\n  deduplicationScope: sqs.DeduplicationScope.MESSAGE_GROUP,\n  fifoThroughputLimit: sqs.FifoThroughputLimit.PER_MESSAGE_GROUP_ID,\n});\n\n// Publish with message group ID\nawait sqs.sendMessage({\n  QueueUrl: process.env.QUEUE_URL,\n  MessageBody: JSON.stringify(event),\n  MessageGroupId: customerId, // All messages for same customer in order\n  MessageDeduplicationId: eventId, // Prevent duplicates\n});\n\n// Kinesis for ordered streams\nconst stream = new kinesis.Stream(this, 'Stream', {\n  shardCount: 1, // Single shard = strict ordering\n});\n\n// Partition key ensures same partition\nawait kinesis.putRecord({\n  StreamName: process.env.STREAM_NAME,\n  Data: Buffer.from(JSON.stringify(event)),\n  PartitionKey: customerId, // Same key = same shard\n});\n```\n\n### Deduplication\n\n**Prevent duplicate event processing**:\n\n```typescript\n// Content-based deduplication with SQS FIFO\nconst queue = new sqs.Queue(this, 'Queue', {\n  fifo: true,\n  contentBasedDeduplication: true, // Hash of message body\n});\n\n// Manual deduplication with DynamoDB\nexport const handler = async (event: any) => {\n  const eventId = event.id || event.messageId;\n\n  try {\n    // Conditional write (fails if exists)\n    await dynamodb.putItem({\n      TableName: process.env.DEDUP_TABLE,\n      Item: {\n        eventId,\n        processedAt: Date.now(),\n        ttl: Math.floor(Date.now() / 1000) + 86400, // 24h TTL\n      },\n      ConditionExpression: 'attribute_not_exists(eventId)',\n    });\n\n    // Event is unique, process it\n    await processEvent(event);\n  } catch (error) {\n    if (error.code === 'ConditionalCheckFailedException') {\n      console.log('Duplicate event ignored:', eventId);\n      return; // Already processed\n    }\n    throw error; // Other error\n  }\n};\n```\n\n### Backpressure Handling\n\n**Prevent overwhelming downstream systems**:\n\n```typescript\n// Control Lambda concurrency\nconst consumerFunction = new lambda.Function(this, 'Consumer', {\n  reservedConcurrentExecutions: 10, // Max 10 concurrent\n});\n\n// SQS visibility timeout + retry logic\nconst queue = new sqs.Queue(this, 'Queue', {\n  visibilityTimeout: Duration.seconds(300), // 5 minutes\n  receiveMessageWaitTime: Duration.seconds(20), // Long polling\n});\n\nnew lambda.EventSourceMapping(this, 'Consumer', {\n  target: consumerFunction,\n  eventSourceArn: queue.queueArn,\n  batchSize: 10,\n  maxConcurrency: 5, // Process 5 batches concurrently\n  reportBatchItemFailures: true,\n});\n\n// Circuit breaker pattern\nlet consecutiveFailures = 0;\nconst FAILURE_THRESHOLD = 5;\n\nexport const handler = async (event: any) => {\n  // Check circuit breaker\n  if (consecutiveFailures >= FAILURE_THRESHOLD) {\n    console.error('Circuit breaker open, skipping processing');\n    throw new Error('Circuit breaker open');\n  }\n\n  try {\n    await processEvent(event);\n    consecutiveFailures = 0; // Reset on success\n  } catch (error) {\n    consecutiveFailures++;\n    throw error;\n  }\n};\n```\n\n## Advanced Patterns\n\n### Pattern: Event Replay\n\nReplay events for recovery or testing:\n\n```typescript\n// Archive events for replay\nconst archive = new events.Archive(this, 'Archive', {\n  sourceEventBus: eventBus,\n  eventPattern: {\n    account: [this.account],\n  },\n  retention: Duration.days(365),\n});\n\n// Replay programmatically\nexport const replayEvents = async (startTime: Date, endTime: Date) => {\n  // Use AWS SDK to start replay\n  await eventBridge.startReplay({\n    ReplayName: `replay-${Date.now()}`,\n    EventSourceArn: archive.archiveArn,\n    EventStartTime: startTime,\n    EventEndTime: endTime,\n    Destination: {\n      Arn: eventBus.eventBusArn,\n    },\n  });\n};\n```\n\n### Pattern: Event Time vs Processing Time\n\nHandle late-arriving events:\n\n```typescript\n// Include event timestamp\ninterface Event {\n  eventId: string;\n  eventTime: string; // When event occurred\n  processingTime?: string; // When event processed\n  data: any;\n}\n\n// Windowed aggregation\nexport const aggregateWindow = async (events: Event[]) => {\n  // Group by event time window (not processing time)\n  const windows = new Map<string, Event[]>();\n\n  for (const event of events) {\n    const window = getWindowForTime(new Date(event.eventTime), Duration.minutes(5));\n    const key = window.toISOString();\n\n    if (!windows.has(key)) {\n      windows.set(key, []);\n    }\n    windows.get(key)!.push(event);\n  }\n\n  // Process each window\n  for (const [window, eventsInWindow] of windows) {\n    await processWindow(window, eventsInWindow);\n  }\n};\n```\n\n### Pattern: Transactional Outbox\n\nEnsure event publishing with database writes:\n\n```typescript\n// Single DynamoDB transaction\nexport const createOrderWithEvent = async (order: Order) => {\n  await dynamodb.transactWriteItems({\n    TransactItems: [\n      {\n        // Write order\n        Put: {\n          TableName: process.env.ORDERS_TABLE,\n          Item: marshall(order),\n        },\n      },\n      {\n        // Write outbox event\n        Put: {\n          TableName: process.env.OUTBOX_TABLE,\n          Item: marshall({\n            eventId: uuid(),\n            eventType: 'OrderPlaced',\n            eventData: order,\n            status: 'PENDING',\n            createdAt: Date.now(),\n          }),\n        },\n      },\n    ],\n  });\n};\n\n// Separate Lambda processes outbox\nnew lambda.EventSourceMapping(this, 'OutboxProcessor', {\n  target: outboxFunction,\n  eventSourceArn: outboxTable.tableStreamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n});\n\nexport const processOutbox = async (event: DynamoDBStreamEvent) => {\n  for (const record of event.Records) {\n    if (record.eventName !== 'INSERT') continue;\n\n    const outboxEvent = unmarshall(record.dynamodb?.NewImage);\n\n    // Publish to EventBridge\n    await eventBridge.putEvents({\n      Entries: [{\n        Source: 'orders',\n        DetailType: outboxEvent.eventType,\n        Detail: JSON.stringify(outboxEvent.eventData),\n      }],\n    });\n\n    // Mark as processed\n    await dynamodb.updateItem({\n      TableName: process.env.OUTBOX_TABLE,\n      Key: { eventId: outboxEvent.eventId },\n      UpdateExpression: 'SET #status = :status',\n      ExpressionAttributeNames: { '#status': 'status' },\n      ExpressionAttributeValues: { ':status': 'PUBLISHED' },\n    });\n  }\n};\n```\n\n## Testing Event-Driven Systems\n\n### Pattern: Event Replay for Testing\n\n```typescript\n// Publish test events\nexport const publishTestEvents = async () => {\n  const testEvents = [\n    { source: 'orders', detailType: 'OrderPlaced', detail: { orderId: '1' } },\n    { source: 'orders', detailType: 'OrderPlaced', detail: { orderId: '2' } },\n  ];\n\n  for (const event of testEvents) {\n    await eventBridge.putEvents({ Entries: [event] });\n  }\n};\n\n// Monitor processing\nexport const verifyProcessing = async () => {\n  // Check downstream databases\n  const order1 = await orderTable.getItem({ Key: { orderId: '1' } });\n  const order2 = await orderTable.getItem({ Key: { orderId: '2' } });\n\n  expect(order1.Item).toBeDefined();\n  expect(order2.Item).toBeDefined();\n};\n```\n\n### Pattern: Event Mocking\n\n```typescript\n// Mock EventBridge in tests\nconst mockEventBridge = {\n  putEvents: jest.fn().mockResolvedValue({}),\n};\n\n// Test event publishing\ntest('publishes event on order creation', async () => {\n  await createOrder(mockEventBridge, order);\n\n  expect(mockEventBridge.putEvents).toHaveBeenCalledWith({\n    Entries: [\n      expect.objectContaining({\n        Source: 'orders',\n        DetailType: 'OrderPlaced',\n      }),\n    ],\n  });\n});\n```\n\n## Summary\n\n- **Loose Coupling**: Services communicate via events, not direct calls\n- **Async Processing**: Use queues and event buses for asynchronous workflows\n- **Idempotency**: Always handle duplicate events gracefully\n- **Dead Letter Queues**: Configure DLQs for error handling\n- **Event Contracts**: Define clear schemas for events\n- **Observability**: Enable tracing and monitoring across services\n- **Eventual Consistency**: Design for it, don't fight it\n- **Saga Patterns**: Use for distributed transactions\n- **Event Sourcing**: Store events as source of truth when needed\n",
        "plugins/serverless-eda/skills/aws-serverless-eda/references/observability-best-practices.md": "# Serverless Observability Best Practices\n\nComprehensive observability patterns for serverless applications based on AWS best practices.\n\n## Table of Contents\n\n- [Three Pillars of Observability](#three-pillars-of-observability)\n- [Metrics](#metrics)\n- [Logging](#logging)\n- [Tracing](#tracing)\n- [Unified Observability](#unified-observability)\n- [Alerting](#alerting)\n\n## Three Pillars of Observability\n\n### Metrics\n**Numeric data measured at intervals (time series)**\n- Request rate, error rate, duration\n- CPU%, memory%, disk%\n- Custom business metrics\n- Service Level Indicators (SLIs)\n\n### Logs\n**Timestamped records of discrete events**\n- Application events and errors\n- State transformations\n- Debugging information\n- Audit trails\n\n### Traces\n**Single user's journey across services**\n- Request flow through distributed system\n- Service dependencies\n- Latency breakdown\n- Error propagation\n\n## Metrics\n\n### CloudWatch Metrics for Lambda\n\n**Out-of-the-box metrics** (automatically available):\n```\n- Invocations\n- Errors\n- Throttles\n- Duration\n- ConcurrentExecutions\n- IteratorAge (for streams)\n```\n\n**CDK Configuration**:\n```typescript\nconst fn = new NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n});\n\n// Create alarms on metrics\nnew cloudwatch.Alarm(this, 'ErrorAlarm', {\n  metric: fn.metricErrors({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 1,\n});\n\nnew cloudwatch.Alarm(this, 'DurationAlarm', {\n  metric: fn.metricDuration({\n    statistic: 'p99',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1000, // 1 second\n  evaluationPeriods: 2,\n});\n```\n\n### Custom Metrics\n\n**Use CloudWatch Embedded Metric Format (EMF)**:\n\n```typescript\nexport const handler = async (event: any) => {\n  const startTime = Date.now();\n\n  try {\n    const result = await processOrder(event);\n\n    // Emit custom metrics\n    console.log(JSON.stringify({\n      _aws: {\n        Timestamp: Date.now(),\n        CloudWatchMetrics: [{\n          Namespace: 'MyApp/Orders',\n          Dimensions: [['ServiceName', 'Operation']],\n          Metrics: [\n            { Name: 'ProcessingTime', Unit: 'Milliseconds' },\n            { Name: 'OrderValue', Unit: 'None' },\n          ],\n        }],\n      },\n      ServiceName: 'OrderService',\n      Operation: 'ProcessOrder',\n      ProcessingTime: Date.now() - startTime,\n      OrderValue: result.amount,\n    }));\n\n    return result;\n  } catch (error) {\n    // Emit error metric\n    console.log(JSON.stringify({\n      _aws: {\n        CloudWatchMetrics: [{\n          Namespace: 'MyApp/Orders',\n          Dimensions: [['ServiceName']],\n          Metrics: [{ Name: 'Errors', Unit: 'Count' }],\n        }],\n      },\n      ServiceName: 'OrderService',\n      Errors: 1,\n    }));\n\n    throw error;\n  }\n};\n```\n\n**Using Lambda Powertools**:\n\n```typescript\nimport { Metrics, MetricUnits } from '@aws-lambda-powertools/metrics';\n\nconst metrics = new Metrics({\n  namespace: 'MyApp',\n  serviceName: 'OrderService',\n});\n\nexport const handler = async (event: any) => {\n  metrics.addMetric('Invocation', MetricUnits.Count, 1);\n\n  const startTime = Date.now();\n\n  try {\n    const result = await processOrder(event);\n\n    metrics.addMetric('Success', MetricUnits.Count, 1);\n    metrics.addMetric('ProcessingTime', MetricUnits.Milliseconds, Date.now() - startTime);\n    metrics.addMetric('OrderValue', MetricUnits.None, result.amount);\n\n    return result;\n  } catch (error) {\n    metrics.addMetric('Error', MetricUnits.Count, 1);\n    throw error;\n  } finally {\n    metrics.publishStoredMetrics();\n  }\n};\n```\n\n## Logging\n\n### Structured Logging\n\n**Use JSON format for logs**:\n\n```typescript\n// ✅ GOOD - Structured JSON logging\nexport const handler = async (event: any) => {\n  console.log(JSON.stringify({\n    level: 'INFO',\n    message: 'Processing order',\n    orderId: event.orderId,\n    customerId: event.customerId,\n    timestamp: new Date().toISOString(),\n    requestId: context.requestId,\n  }));\n\n  try {\n    const result = await processOrder(event);\n\n    console.log(JSON.stringify({\n      level: 'INFO',\n      message: 'Order processed successfully',\n      orderId: event.orderId,\n      duration: Date.now() - startTime,\n      timestamp: new Date().toISOString(),\n    }));\n\n    return result;\n  } catch (error) {\n    console.error(JSON.stringify({\n      level: 'ERROR',\n      message: 'Order processing failed',\n      orderId: event.orderId,\n      error: {\n        name: error.name,\n        message: error.message,\n        stack: error.stack,\n      },\n      timestamp: new Date().toISOString(),\n    }));\n\n    throw error;\n  }\n};\n\n// ❌ BAD - Unstructured logging\nconsole.log('Processing order ' + orderId + ' for customer ' + customerId);\n```\n\n**Using Lambda Powertools Logger**:\n\n```typescript\nimport { Logger } from '@aws-lambda-powertools/logger';\n\nconst logger = new Logger({\n  serviceName: 'OrderService',\n  logLevel: 'INFO',\n});\n\nexport const handler = async (event: any, context: Context) => {\n  logger.addContext(context);\n\n  logger.info('Processing order', {\n    orderId: event.orderId,\n    customerId: event.customerId,\n  });\n\n  try {\n    const result = await processOrder(event);\n\n    logger.info('Order processed', {\n      orderId: event.orderId,\n      amount: result.amount,\n    });\n\n    return result;\n  } catch (error) {\n    logger.error('Order processing failed', {\n      orderId: event.orderId,\n      error,\n    });\n\n    throw error;\n  }\n};\n```\n\n### Log Levels\n\n**Use appropriate log levels**:\n- **ERROR**: Errors requiring immediate attention\n- **WARN**: Warnings or recoverable errors\n- **INFO**: Important business events\n- **DEBUG**: Detailed debugging information (disable in production)\n\n```typescript\nconst logger = new Logger({\n  serviceName: 'OrderService',\n  logLevel: process.env.LOG_LEVEL || 'INFO',\n});\n\nlogger.debug('Detailed processing info', { data });\nlogger.info('Business event occurred', { event });\nlogger.warn('Recoverable error', { error });\nlogger.error('Critical failure', { error });\n```\n\n### Log Insights Queries\n\n**Common CloudWatch Logs Insights queries**:\n\n```\n# Find errors in last hour\nfields @timestamp, @message, level, error.message\n| filter level = \"ERROR\"\n| sort @timestamp desc\n| limit 100\n\n# Count errors by type\nstats count() by error.name as ErrorType\n| sort count desc\n\n# Calculate p99 latency\nstats percentile(duration, 99) by serviceName\n\n# Find slow requests\nfields @timestamp, orderId, duration\n| filter duration > 1000\n| sort duration desc\n| limit 50\n\n# Track specific customer requests\nfields @timestamp, @message, orderId\n| filter customerId = \"customer-123\"\n| sort @timestamp desc\n```\n\n## Tracing\n\n### Enable X-Ray Tracing\n\n**Configure X-Ray for Lambda**:\n\n```typescript\nconst fn = new NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  tracing: lambda.Tracing.ACTIVE, // Enable X-Ray\n});\n\n// API Gateway tracing\nconst api = new apigateway.RestApi(this, 'Api', {\n  deployOptions: {\n    tracingEnabled: true,\n  },\n});\n\n// Step Functions tracing\nnew stepfunctions.StateMachine(this, 'StateMachine', {\n  definition,\n  tracingEnabled: true,\n});\n```\n\n**Instrument application code**:\n\n```typescript\nimport { captureAWSv3Client } from 'aws-xray-sdk-core';\nimport { DynamoDBClient } from '@aws-sdk/client-dynamodb';\n\n// Wrap AWS SDK clients\nconst client = captureAWSv3Client(new DynamoDBClient({}));\n\n// Custom segments\nimport AWSXRay from 'aws-xray-sdk-core';\n\nexport const handler = async (event: any) => {\n  const segment = AWSXRay.getSegment();\n\n  // Custom subsegment\n  const subsegment = segment.addNewSubsegment('ProcessOrder');\n\n  try {\n    // Add annotations (indexed for filtering)\n    subsegment.addAnnotation('orderId', event.orderId);\n    subsegment.addAnnotation('customerId', event.customerId);\n\n    // Add metadata (not indexed, detailed info)\n    subsegment.addMetadata('orderDetails', event);\n\n    const result = await processOrder(event);\n\n    subsegment.addAnnotation('status', 'success');\n    subsegment.close();\n\n    return result;\n  } catch (error) {\n    subsegment.addError(error);\n    subsegment.close();\n    throw error;\n  }\n};\n```\n\n**Using Lambda Powertools Tracer**:\n\n```typescript\nimport { Tracer } from '@aws-lambda-powertools/tracer';\n\nconst tracer = new Tracer({ serviceName: 'OrderService' });\n\nexport const handler = async (event: any) => {\n  const segment = tracer.getSegment();\n\n  // Automatically captures and traces\n  const result = await tracer.captureAWSv3Client(dynamodb).getItem({\n    TableName: process.env.TABLE_NAME,\n    Key: { orderId: event.orderId },\n  });\n\n  // Custom annotation\n  tracer.putAnnotation('orderId', event.orderId);\n  tracer.putMetadata('orderDetails', event);\n\n  return result;\n};\n```\n\n### Service Map\n\n**Visualize service dependencies** with X-Ray:\n- Shows service-to-service communication\n- Identifies latency bottlenecks\n- Highlights error rates between services\n- Tracks downstream dependencies\n\n### Distributed Tracing Best Practices\n\n1. **Enable tracing everywhere**: Lambda, API Gateway, Step Functions\n2. **Use annotations for filtering**: Indexed fields for queries\n3. **Use metadata for details**: Non-indexed detailed information\n4. **Sample appropriately**: 100% for low traffic, sampled for high traffic\n5. **Correlate with logs**: Include trace ID in log entries\n\n## Unified Observability\n\n### Correlation Between Pillars\n\n**Include trace ID in logs**:\n\n```typescript\nexport const handler = async (event: any, context: Context) => {\n  const traceId = process.env._X_AMZN_TRACE_ID;\n\n  console.log(JSON.stringify({\n    level: 'INFO',\n    message: 'Processing order',\n    traceId,\n    requestId: context.requestId,\n    orderId: event.orderId,\n  }));\n};\n```\n\n### CloudWatch ServiceLens\n\n**Unified view of traces and metrics**:\n- Automatically correlates X-Ray traces with CloudWatch metrics\n- Shows service map with metrics overlay\n- Identifies performance and availability issues\n- Provides end-to-end request view\n\n### Lambda Powertools Integration\n\n**All three pillars in one**:\n\n```typescript\nimport { Logger } from '@aws-lambda-powertools/logger';\nimport { Tracer } from '@aws-lambda-powertools/tracer';\nimport { Metrics, MetricUnits } from '@aws-lambda-powertools/metrics';\n\nconst logger = new Logger({ serviceName: 'OrderService' });\nconst tracer = new Tracer({ serviceName: 'OrderService' });\nconst metrics = new Metrics({ namespace: 'MyApp', serviceName: 'OrderService' });\n\nexport const handler = async (event: any, context: Context) => {\n  // Automatically adds trace context to logs\n  logger.addContext(context);\n\n  logger.info('Processing order', { orderId: event.orderId });\n\n  // Add trace annotations\n  tracer.putAnnotation('orderId', event.orderId);\n\n  // Add metrics\n  metrics.addMetric('Invocation', MetricUnits.Count, 1);\n\n  const startTime = Date.now();\n\n  try {\n    const result = await processOrder(event);\n\n    metrics.addMetric('Success', MetricUnits.Count, 1);\n    metrics.addMetric('Duration', MetricUnits.Milliseconds, Date.now() - startTime);\n\n    logger.info('Order processed', { orderId: event.orderId });\n\n    return result;\n  } catch (error) {\n    metrics.addMetric('Error', MetricUnits.Count, 1);\n    logger.error('Processing failed', { orderId: event.orderId, error });\n    throw error;\n  } finally {\n    metrics.publishStoredMetrics();\n  }\n};\n```\n\n## Alerting\n\n### Effective Alerting Strategy\n\n**Alert on what matters**:\n- **Critical**: Customer-impacting issues (errors, high latency)\n- **Warning**: Approaching thresholds (80% capacity)\n- **Info**: Trends and anomalies (cost spikes)\n\n**Alarm fatigue prevention**:\n- Tune thresholds based on actual patterns\n- Use composite alarms to reduce noise\n- Set appropriate evaluation periods\n- Include clear remediation steps\n\n### CloudWatch Alarms\n\n**Common alarm patterns**:\n\n```typescript\n// Error rate alarm\nnew cloudwatch.Alarm(this, 'ErrorRateAlarm', {\n  metric: new cloudwatch.MathExpression({\n    expression: 'errors / invocations * 100',\n    usingMetrics: {\n      errors: fn.metricErrors({ statistic: 'Sum' }),\n      invocations: fn.metricInvocations({ statistic: 'Sum' }),\n    },\n  }),\n  threshold: 1, // 1% error rate\n  evaluationPeriods: 2,\n  alarmDescription: 'Error rate exceeded 1%',\n});\n\n// Latency alarm (p99)\nnew cloudwatch.Alarm(this, 'LatencyAlarm', {\n  metric: fn.metricDuration({\n    statistic: 'p99',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1000, // 1 second\n  evaluationPeriods: 2,\n  alarmDescription: 'p99 latency exceeded 1 second',\n});\n\n// Concurrent executions approaching limit\nnew cloudwatch.Alarm(this, 'ConcurrencyAlarm', {\n  metric: fn.metricConcurrentExecutions({\n    statistic: 'Maximum',\n  }),\n  threshold: 800, // 80% of 1000 default limit\n  evaluationPeriods: 1,\n  alarmDescription: 'Approaching concurrency limit',\n});\n```\n\n### Composite Alarms\n\n**Reduce alert noise**:\n\n```typescript\nconst errorAlarm = new cloudwatch.Alarm(this, 'Errors', {\n  metric: fn.metricErrors(),\n  threshold: 10,\n  evaluationPeriods: 1,\n});\n\nconst throttleAlarm = new cloudwatch.Alarm(this, 'Throttles', {\n  metric: fn.metricThrottles(),\n  threshold: 5,\n  evaluationPeriods: 1,\n});\n\nconst latencyAlarm = new cloudwatch.Alarm(this, 'Latency', {\n  metric: fn.metricDuration({ statistic: 'p99' }),\n  threshold: 2000,\n  evaluationPeriods: 2,\n});\n\n// Composite alarm (any of the above)\nnew cloudwatch.CompositeAlarm(this, 'ServiceHealthAlarm', {\n  compositeAlarmName: 'order-service-health',\n  alarmRule: cloudwatch.AlarmRule.anyOf(\n    errorAlarm,\n    throttleAlarm,\n    latencyAlarm\n  ),\n  alarmDescription: 'Overall service health degraded',\n});\n```\n\n## Dashboard Best Practices\n\n### Service Dashboard Layout\n\n**Recommended sections**:\n\n1. **Overview**:\n   - Total invocations\n   - Error rate percentage\n   - P50, P95, P99 latency\n   - Availability percentage\n\n2. **Resource Utilization**:\n   - Concurrent executions\n   - Memory utilization\n   - Duration distribution\n   - Throttles\n\n3. **Business Metrics**:\n   - Orders processed\n   - Revenue per minute\n   - Customer activity\n   - Feature usage\n\n4. **Errors and Alerts**:\n   - Error count by type\n   - Active alarms\n   - DLQ message count\n   - Failed transactions\n\n### CloudWatch Dashboard CDK\n\n```typescript\nconst dashboard = new cloudwatch.Dashboard(this, 'ServiceDashboard', {\n  dashboardName: 'order-service',\n});\n\ndashboard.addWidgets(\n  // Row 1: Overview\n  new cloudwatch.GraphWidget({\n    title: 'Invocations',\n    left: [fn.metricInvocations()],\n  }),\n  new cloudwatch.SingleValueWidget({\n    title: 'Error Rate',\n    metrics: [\n      new cloudwatch.MathExpression({\n        expression: 'errors / invocations * 100',\n        usingMetrics: {\n          errors: fn.metricErrors({ statistic: 'Sum' }),\n          invocations: fn.metricInvocations({ statistic: 'Sum' }),\n        },\n      }),\n    ],\n  }),\n  new cloudwatch.GraphWidget({\n    title: 'Latency (p50, p95, p99)',\n    left: [\n      fn.metricDuration({ statistic: 'p50', label: 'p50' }),\n      fn.metricDuration({ statistic: 'p95', label: 'p95' }),\n      fn.metricDuration({ statistic: 'p99', label: 'p99' }),\n    ],\n  })\n);\n\n// Row 2: Errors\ndashboard.addWidgets(\n  new cloudwatch.LogQueryWidget({\n    title: 'Recent Errors',\n    logGroupNames: [fn.logGroup.logGroupName],\n    queryLines: [\n      'fields @timestamp, @message',\n      'filter level = \"ERROR\"',\n      'sort @timestamp desc',\n      'limit 20',\n    ],\n  })\n);\n```\n\n## Monitoring Serverless Architectures\n\n### End-to-End Monitoring\n\n**Monitor the entire flow**:\n\n```\nAPI Gateway → Lambda → DynamoDB → EventBridge → Lambda\n     ↓           ↓          ↓            ↓           ↓\n  Metrics    Traces     Metrics      Metrics     Logs\n```\n\n**Key metrics per service**:\n\n| Service | Key Metrics |\n|---------|-------------|\n| API Gateway | Count, 4XXError, 5XXError, Latency, CacheHitCount |\n| Lambda | Invocations, Errors, Duration, Throttles, ConcurrentExecutions |\n| DynamoDB | ConsumedReadCapacity, ConsumedWriteCapacity, UserErrors, SystemErrors |\n| SQS | NumberOfMessagesSent, NumberOfMessagesReceived, ApproximateAgeOfOldestMessage |\n| EventBridge | Invocations, FailedInvocations, TriggeredRules |\n| Step Functions | ExecutionsStarted, ExecutionsFailed, ExecutionTime |\n\n### Synthetic Monitoring\n\n**Use CloudWatch Synthetics for API monitoring**:\n\n```typescript\nimport { Canary, Test, Code, Schedule } from '@aws-cdk/aws-synthetics-alpha';\n\nnew Canary(this, 'ApiCanary', {\n  canaryName: 'api-health-check',\n  schedule: Schedule.rate(Duration.minutes(5)),\n  test: Test.custom({\n    code: Code.fromInline(`\n      const synthetics = require('Synthetics');\n\n      const apiCanaryBlueprint = async function () {\n        const response = await synthetics.executeHttpStep('Verify API', {\n          url: 'https://api.example.com/health',\n          method: 'GET',\n        });\n\n        return response.statusCode === 200 ? 'success' : 'failure';\n      };\n\n      exports.handler = async () => {\n        return await apiCanaryBlueprint();\n      };\n    `),\n    handler: 'index.handler',\n  }),\n  runtime: synthetics.Runtime.SYNTHETICS_NODEJS_PUPPETEER_6_2,\n});\n```\n\n## OpenTelemetry Integration\n\n### Amazon Distro for OpenTelemetry (ADOT)\n\n**Use ADOT for vendor-neutral observability**:\n\n```typescript\n// Lambda Layer with ADOT\nconst adotLayer = lambda.LayerVersion.fromLayerVersionArn(\n  this,\n  'AdotLayer',\n  `arn:aws:lambda:${this.region}:901920570463:layer:aws-otel-nodejs-amd64-ver-1-18-1:4`\n);\n\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  layers: [adotLayer],\n  tracing: lambda.Tracing.ACTIVE,\n  environment: {\n    AWS_LAMBDA_EXEC_WRAPPER: '/opt/otel-handler',\n    OPENTELEMETRY_COLLECTOR_CONFIG_FILE: '/var/task/collector.yaml',\n  },\n});\n```\n\n**Benefits of ADOT**:\n- Vendor-neutral (works with Datadog, New Relic, Honeycomb, etc.)\n- Automatic instrumentation\n- Consistent format across services\n- Export to multiple backends\n\n## Best Practices Summary\n\n### Metrics\n- ✅ Use CloudWatch Embedded Metric Format (EMF)\n- ✅ Track business metrics, not just technical metrics\n- ✅ Set alarms on error rate, latency, and throughput\n- ✅ Use p99 for latency, not average\n- ✅ Create dashboards for key services\n\n### Logging\n- ✅ Use structured JSON logging\n- ✅ Include correlation IDs (request ID, trace ID)\n- ✅ Use appropriate log levels\n- ✅ Never log sensitive data (PII, secrets)\n- ✅ Use CloudWatch Logs Insights for analysis\n\n### Tracing\n- ✅ Enable X-Ray tracing on all services\n- ✅ Instrument AWS SDK calls\n- ✅ Add custom annotations for business context\n- ✅ Use service map to understand dependencies\n- ✅ Correlate traces with logs and metrics\n\n### Alerting\n- ✅ Alert on customer-impacting issues\n- ✅ Tune thresholds to reduce false positives\n- ✅ Use composite alarms to reduce noise\n- ✅ Include clear remediation steps\n- ✅ Escalate critical alarms appropriately\n\n### Tools\n- ✅ Use Lambda Powertools for unified observability\n- ✅ Use CloudWatch ServiceLens for service view\n- ✅ Use Synthetics for proactive monitoring\n- ✅ Consider ADOT for vendor-neutral observability\n",
        "plugins/serverless-eda/skills/aws-serverless-eda/references/performance-optimization.md": "# Serverless Performance Optimization\n\nPerformance optimization best practices for AWS Lambda and serverless architectures.\n\n## Table of Contents\n\n- [Lambda Execution Lifecycle](#lambda-execution-lifecycle)\n- [Cold Start Optimization](#cold-start-optimization)\n- [Memory and CPU Optimization](#memory-and-cpu-optimization)\n- [Package Size Optimization](#package-size-optimization)\n- [Initialization Optimization](#initialization-optimization)\n- [Runtime Performance](#runtime-performance)\n\n## Lambda Execution Lifecycle\n\n### Execution Environment Phases\n\n**Three phases of Lambda execution**:\n\n1. **Init Phase** (Cold Start):\n   - Download and unpack function package\n   - Create execution environment\n   - Initialize runtime\n   - Execute initialization code (outside handler)\n\n2. **Invoke Phase**:\n   - Execute handler code\n   - Return response\n   - Freeze execution environment\n\n3. **Shutdown Phase**:\n   - Runtime shutdown (after period of inactivity)\n   - Execution environment destroyed\n\n### Concurrency and Scaling\n\n**Key concepts**:\n- **Concurrency**: Number of execution environments serving requests simultaneously\n- **One event per environment**: Each environment processes one event at a time\n- **Automatic scaling**: Lambda creates new environments as needed\n- **Environment reuse**: Warm starts reuse existing environments\n\n**Example**:\n- Function takes 100ms to execute\n- Single environment can handle 10 requests/second\n- 100 concurrent requests = 10 environments needed\n- Default account limit: 1,000 concurrent executions (can be raised)\n\n## Cold Start Optimization\n\n### Understanding Cold Starts\n\n**Cold start components**:\n```\nTotal Cold Start = Download Package + Init Environment + Init Code + Handler\n```\n\n**Cold start frequency**:\n- Development: Every code change creates new environments (frequent)\n- Production: Typically < 1% of invocations\n- Optimize for p95/p99 latency, not average\n\n### Package Size Optimization\n\n**Minimize deployment package**:\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  bundling: {\n    minify: true, // Minify production code\n    sourceMap: false, // Disable in production\n    externalModules: [\n      '@aws-sdk/*', // Use AWS SDK from runtime\n    ],\n    // Tree-shaking removes unused code\n  },\n});\n```\n\n**Tools for optimization**:\n- **esbuild**: Automatic tree-shaking and minification\n- **Webpack**: Bundle optimization\n- **Maven**: Dependency analysis\n- **Gradle**: Unused dependency detection\n\n**Best practices**:\n1. Avoid monolithic functions\n2. Bundle only required dependencies\n3. Use tree-shaking to remove unused code\n4. Minify production code\n5. Exclude AWS SDK (provided by runtime)\n\n### Provisioned Concurrency\n\n**Pre-initialize environments for predictable latency**:\n\n```typescript\nconst fn = new NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n});\n\n// Static provisioned concurrency\nfn.currentVersion.addAlias('live', {\n  provisionedConcurrentExecutions: 10,\n});\n\n// Auto-scaling provisioned concurrency\nconst alias = fn.currentVersion.addAlias('prod');\n\nconst target = new applicationautoscaling.ScalableTarget(this, 'ScalableTarget', {\n  serviceNamespace: applicationautoscaling.ServiceNamespace.LAMBDA,\n  maxCapacity: 100,\n  minCapacity: 10,\n  resourceId: `function:${fn.functionName}:${alias.aliasName}`,\n  scalableDimension: 'lambda:function:ProvisionedConcurrentExecutions',\n});\n\ntarget.scaleOnUtilization({\n  utilizationTarget: 0.7, // 70% utilization\n});\n```\n\n**When to use**:\n- **Consistent traffic patterns**: Predictable load\n- **Latency-sensitive APIs**: Sub-100ms requirements\n- **Cost consideration**: Compare cold start frequency vs. provisioned cost\n\n**Cost comparison**:\n- **On-demand**: Pay only for actual usage\n- **Provisioned**: Pay for provisioned capacity + invocations\n- **Breakeven**: When cold starts > ~20% of invocations\n\n### Lambda SnapStart (Java)\n\n**Instant cold starts for Java**:\n\n```typescript\nnew lambda.Function(this, 'JavaFunction', {\n  runtime: lambda.Runtime.JAVA_17,\n  code: lambda.Code.fromAsset('target/function.jar'),\n  handler: 'com.example.Handler::handleRequest',\n  snapStart: lambda.SnapStartConf.ON_PUBLISHED_VERSIONS,\n});\n```\n\n**Benefits**:\n- Up to 10x faster cold starts for Java\n- No code changes required\n- Works with published versions\n- No additional cost\n\n## Memory and CPU Optimization\n\n### Memory = CPU Allocation\n\n**Key principle**: Memory and CPU are proportionally allocated\n\n| Memory | vCPU |\n|--------|------|\n| 128 MB | 0.07 vCPU |\n| 512 MB | 0.28 vCPU |\n| 1,024 MB | 0.57 vCPU |\n| 1,769 MB | 1.00 vCPU |\n| 3,538 MB | 2.00 vCPU |\n| 10,240 MB | 6.00 vCPU |\n\n### Cost vs. Performance Balancing\n\n**Example - Compute-intensive function**:\n\n| Memory | Duration | Cost |\n|--------|----------|------|\n| 128 MB | 11.72s | $0.0246 |\n| 256 MB | 6.68s | $0.0280 |\n| 512 MB | 3.19s | $0.0268 |\n| 1024 MB | 1.46s | $0.0246 |\n\n**Key insight**: More memory = faster execution = similar or lower cost\n\n**Formula**:\n```\nDuration = Allocated Memory (GB) × Execution Time (seconds)\nCost = Duration × Number of Invocations × Price per GB-second\n```\n\n### Finding Optimal Memory\n\n**Use Lambda Power Tuning**:\n\n```bash\n# Deploy power tuning state machine\nsam deploy --template-file template.yml --stack-name lambda-power-tuning\n\n# Run power tuning\naws lambda invoke \\\n  --function-name powerTuningFunction \\\n  --payload '{\"lambdaARN\": \"arn:aws:lambda:...\", \"powerValues\": [128, 256, 512, 1024, 1536, 3008]}' \\\n  response.json\n```\n\n**Manual testing approach**:\n1. Test function at different memory levels\n2. Measure execution time at each level\n3. Calculate cost for each configuration\n4. Choose optimal balance for your use case\n\n### Multi-Core Optimization\n\n**Leverage multiple vCPUs** (at 1,769 MB+):\n\n```typescript\n// Use Worker Threads for parallel processing\nimport { Worker } from 'worker_threads';\n\nexport const handler = async (event: any) => {\n  const items = event.items;\n\n  // Process in parallel using multiple cores\n  const workers = items.map(item =>\n    new Promise((resolve, reject) => {\n      const worker = new Worker('./worker.js', {\n        workerData: item,\n      });\n\n      worker.on('message', resolve);\n      worker.on('error', reject);\n    })\n  );\n\n  const results = await Promise.all(workers);\n  return results;\n};\n```\n\n**Python multiprocessing**:\n\n```python\nimport multiprocessing as mp\n\ndef handler(event, context):\n    items = event['items']\n\n    # Use multiple cores for CPU-bound work\n    with mp.Pool(mp.cpu_count()) as pool:\n        results = pool.map(process_item, items)\n\n    return {'results': results}\n```\n\n## Initialization Optimization\n\n### Code Outside Handler\n\n**Initialize once, reuse across invocations**:\n\n```typescript\n// ✅ GOOD - Initialize outside handler\nimport { DynamoDBClient } from '@aws-sdk/client-dynamodb';\nimport { S3Client } from '@aws-sdk/client-s3';\n\n// Initialized once per execution environment\nconst dynamodb = new DynamoDBClient({});\nconst s3 = new S3Client({});\n\n// Connection pool initialized once\nconst pool = createConnectionPool({\n  host: process.env.DB_HOST,\n  max: 1, // One connection per execution environment\n});\n\nexport const handler = async (event: any) => {\n  // Reuse connections across invocations\n  const data = await dynamodb.getItem({ /* ... */ });\n  const file = await s3.getObject({ /* ... */ });\n  return processData(data, file);\n};\n\n// ❌ BAD - Initialize in handler\nexport const handler = async (event: any) => {\n  const dynamodb = new DynamoDBClient({}); // Created every invocation\n  const s3 = new S3Client({}); // Created every invocation\n  // ...\n};\n```\n\n### Lazy Loading\n\n**Load dependencies only when needed**:\n\n```typescript\n// ✅ GOOD - Conditional loading\nexport const handler = async (event: any) => {\n  if (event.operation === 'generatePDF') {\n    // Load heavy PDF library only when needed\n    const pdfLib = await import('./pdf-generator');\n    return pdfLib.generatePDF(event.data);\n  }\n\n  if (event.operation === 'processImage') {\n    const sharp = await import('sharp');\n    return processImage(sharp, event.data);\n  }\n\n  // Default operation (no heavy dependencies)\n  return processDefault(event);\n};\n\n// ❌ BAD - Load everything upfront\nimport pdfLib from './pdf-generator'; // 50MB\nimport sharp from 'sharp'; // 20MB\n// Even if not used!\n\nexport const handler = async (event: any) => {\n  if (event.operation === 'generatePDF') {\n    return pdfLib.generatePDF(event.data);\n  }\n};\n```\n\n### Connection Reuse\n\n**Enable connection reuse**:\n\n```typescript\nimport { DynamoDBClient } from '@aws-sdk/client-dynamodb';\n\nconst client = new DynamoDBClient({\n  // Enable keep-alive for connection reuse\n  requestHandler: {\n    connectionTimeout: 3000,\n    socketTimeout: 3000,\n  },\n});\n\n// For Node.js AWS SDK\nprocess.env.AWS_NODEJS_CONNECTION_REUSE_ENABLED = '1';\n```\n\n## Runtime Performance\n\n### Choose the Right Runtime\n\n**Runtime comparison**:\n\n| Runtime | Cold Start | Execution Speed | Ecosystem | Best For |\n|---------|------------|-----------------|-----------|----------|\n| Node.js 20 | Fast | Fast | Excellent | APIs, I/O-bound |\n| Python 3.12 | Fast | Medium | Excellent | Data processing |\n| Java 17 + SnapStart | Fast (w/SnapStart) | Fast | Good | Enterprise apps |\n| .NET 8 | Medium | Fast | Good | Enterprise apps |\n| Go | Very Fast | Very Fast | Good | High performance |\n| Rust | Very Fast | Very Fast | Growing | High performance |\n\n### Optimize Handler Code\n\n**Efficient code patterns**:\n\n```typescript\n// ✅ GOOD - Batch operations\nconst items = ['item1', 'item2', 'item3'];\n\n// Single batch write\nawait dynamodb.batchWriteItem({\n  RequestItems: {\n    [tableName]: items.map(item => ({\n      PutRequest: { Item: item },\n    })),\n  },\n});\n\n// ❌ BAD - Multiple single operations\nfor (const item of items) {\n  await dynamodb.putItem({\n    TableName: tableName,\n    Item: item,\n  }); // Slow, multiple round trips\n}\n```\n\n### Async Processing\n\n**Use async/await effectively**:\n\n```typescript\n// ✅ GOOD - Parallel async operations\nconst [userData, orderData, inventoryData] = await Promise.all([\n  getUserData(userId),\n  getOrderData(orderId),\n  getInventoryData(productId),\n]);\n\n// ❌ BAD - Sequential async operations\nconst userData = await getUserData(userId);\nconst orderData = await getOrderData(orderId); // Waits unnecessarily\nconst inventoryData = await getInventoryData(productId); // Waits unnecessarily\n```\n\n### Caching Strategies\n\n**Cache frequently accessed data**:\n\n```typescript\n// In-memory cache (persists in warm environments)\nconst cache = new Map<string, any>();\n\nexport const handler = async (event: any) => {\n  const key = event.key;\n\n  // Check cache first\n  if (cache.has(key)) {\n    console.log('Cache hit');\n    return cache.get(key);\n  }\n\n  // Fetch from database\n  const data = await fetchFromDatabase(key);\n\n  // Store in cache\n  cache.set(key, data);\n\n  return data;\n};\n```\n\n**ElastiCache for shared cache**:\n\n```typescript\nimport Redis from 'ioredis';\n\n// Initialize once\nconst redis = new Redis({\n  host: process.env.REDIS_HOST,\n  port: 6379,\n  lazyConnect: true,\n  enableOfflineQueue: false,\n});\n\nexport const handler = async (event: any) => {\n  const key = `order:${event.orderId}`;\n\n  // Try cache\n  const cached = await redis.get(key);\n  if (cached) {\n    return JSON.parse(cached);\n  }\n\n  // Fetch and cache\n  const data = await fetchOrder(event.orderId);\n  await redis.setex(key, 300, JSON.stringify(data)); // 5 min TTL\n\n  return data;\n};\n```\n\n## Performance Testing\n\n### Load Testing\n\n**Use Artillery for load testing**:\n\n```yaml\n# load-test.yml\nconfig:\n  target: https://api.example.com\n  phases:\n    - duration: 60\n      arrivalRate: 10\n      rampTo: 100 # Ramp from 10 to 100 req/sec\nscenarios:\n  - flow:\n      - post:\n          url: /orders\n          json:\n            orderId: \"{{ $randomString() }}\"\n            amount: \"{{ $randomNumber(10, 1000) }}\"\n```\n\n```bash\nartillery run load-test.yml\n```\n\n### Benchmarking\n\n**Test different configurations**:\n\n```typescript\n// benchmark.ts\nimport { Lambda } from '@aws-sdk/client-lambda';\n\nconst lambda = new Lambda({});\n\nconst testConfigurations = [\n  { memory: 128, name: 'Function-128' },\n  { memory: 256, name: 'Function-256' },\n  { memory: 512, name: 'Function-512' },\n  { memory: 1024, name: 'Function-1024' },\n];\n\nfor (const config of testConfigurations) {\n  const times: number[] = [];\n\n  // Warm up\n  for (let i = 0; i < 5; i++) {\n    await lambda.invoke({ FunctionName: config.name });\n  }\n\n  // Measure\n  for (let i = 0; i < 100; i++) {\n    const start = Date.now();\n    await lambda.invoke({ FunctionName: config.name });\n    times.push(Date.now() - start);\n  }\n\n  const p99 = times.sort()[99];\n  const avg = times.reduce((a, b) => a + b) / times.length;\n\n  console.log(`${config.memory}MB - Avg: ${avg}ms, p99: ${p99}ms`);\n}\n```\n\n## Cost Optimization\n\n### Right-Sizing Memory\n\n**Balance cost and performance**:\n\n**CPU-bound workloads**:\n- More memory = more CPU = faster execution\n- Often results in lower cost overall\n- Test at 1769MB (1 vCPU) and above\n\n**I/O-bound workloads**:\n- Less sensitive to memory allocation\n- May not benefit from higher memory\n- Test at lower memory levels (256-512MB)\n\n**Simple operations**:\n- Minimal CPU required\n- Use minimum memory (128-256MB)\n- Fast execution despite low resources\n\n### Billing Granularity\n\n**Lambda bills in 1ms increments**:\n- Precise billing (7ms execution = 7ms cost)\n- Optimize even small improvements\n- Consider trade-offs carefully\n\n**Cost calculation**:\n```\nCost = (Memory GB) × (Duration seconds) × (Invocations) × ($0.0000166667/GB-second)\n     + (Invocations) × ($0.20/1M requests)\n```\n\n### Cost Reduction Strategies\n\n1. **Optimize execution time**: Faster = cheaper\n2. **Right-size memory**: Balance CPU needs with cost\n3. **Reduce invocations**: Batch processing, caching\n4. **Use Graviton2**: 20% better price/performance\n5. **Reserved Concurrency**: Only when needed\n6. **Compression**: Reduce data transfer costs\n\n## Advanced Optimization\n\n### Lambda Extensions\n\n**Use extensions for cross-cutting concerns**:\n\n```typescript\n// Lambda layer with extension\nconst extensionLayer = lambda.LayerVersion.fromLayerVersionArn(\n  this,\n  'Extension',\n  'arn:aws:lambda:us-east-1:123456789:layer:my-extension:1'\n);\n\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  layers: [extensionLayer],\n});\n```\n\n**Common extensions**:\n- Secrets caching\n- Configuration caching\n- Custom logging\n- Security scanning\n- Performance monitoring\n\n### Graviton2 Architecture\n\n**20% better price/performance**:\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  architecture: lambda.Architecture.ARM_64, // Graviton2\n});\n```\n\n**Considerations**:\n- Most runtimes support ARM64\n- Test thoroughly before migrating\n- Dependencies must support ARM64\n- Native extensions may need recompilation\n\n### VPC Optimization\n\n**Hyperplane ENIs** (automatic since 2019):\n- No ENI per function\n- Faster cold starts in VPC\n- Scales instantly\n\n```typescript\n// Modern VPC configuration (fast)\nnew NodejsFunction(this, 'VpcFunction', {\n  entry: 'src/handler.ts',\n  vpc,\n  vpcSubnets: { subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS },\n  // Fast scaling, no ENI limitations\n});\n```\n\n## Performance Monitoring\n\n### Key Metrics\n\n**Monitor these metrics**:\n- **Duration**: p50, p95, p99, max\n- **Cold Start %**: ColdStartDuration / TotalDuration\n- **Error Rate**: Errors / Invocations\n- **Throttles**: Indicates concurrency limit reached\n- **Iterator Age**: For stream processing lag\n\n### Performance Dashboards\n\n```typescript\nconst dashboard = new cloudwatch.Dashboard(this, 'PerformanceDashboard');\n\ndashboard.addWidgets(\n  new cloudwatch.GraphWidget({\n    title: 'Latency Distribution',\n    left: [\n      fn.metricDuration({ statistic: 'p50', label: 'p50' }),\n      fn.metricDuration({ statistic: 'p95', label: 'p95' }),\n      fn.metricDuration({ statistic: 'p99', label: 'p99' }),\n      fn.metricDuration({ statistic: 'Maximum', label: 'max' }),\n    ],\n  }),\n  new cloudwatch.GraphWidget({\n    title: 'Memory Utilization',\n    left: [fn.metricDuration()],\n    right: [fn.metricErrors()],\n  })\n);\n```\n\n## Summary\n\n- **Cold Starts**: Optimize package size, use provisioned concurrency for critical paths\n- **Memory**: More memory often = faster execution = lower cost\n- **Initialization**: Initialize connections outside handler\n- **Lazy Loading**: Load dependencies only when needed\n- **Connection Reuse**: Enable for AWS SDK clients\n- **Testing**: Test at different memory levels to find optimal configuration\n- **Monitoring**: Track p99 latency, not average\n- **Graviton2**: Consider ARM64 for better price/performance\n- **Batch Operations**: Reduce round trips to services\n- **Caching**: Cache frequently accessed data\n",
        "plugins/serverless-eda/skills/aws-serverless-eda/references/security-best-practices.md": "# Serverless Security Best Practices\n\nSecurity best practices for serverless applications based on AWS Well-Architected Framework.\n\n## Table of Contents\n\n- [Shared Responsibility Model](#shared-responsibility-model)\n- [Identity and Access Management](#identity-and-access-management)\n- [Function Security](#function-security)\n- [API Security](#api-security)\n- [Data Protection](#data-protection)\n- [Network Security](#network-security)\n\n## Shared Responsibility Model\n\n### Serverless Shifts Responsibility to AWS\n\nWith serverless, AWS takes on more security responsibilities:\n\n**AWS Responsibilities**:\n- Compute infrastructure\n- Execution environment\n- Runtime language and patches\n- Networking infrastructure\n- Server software and OS\n- Physical hardware and facilities\n- Automatic security patches (like Log4Shell mitigation)\n\n**Customer Responsibilities**:\n- Function code and dependencies\n- Resource configuration\n- Identity and Access Management (IAM)\n- Data encryption (at rest and in transit)\n- Application-level security\n- Secure coding practices\n\n### Benefits of Shifted Responsibility\n\n- **Automatic Patching**: AWS applies security patches automatically (e.g., Log4Shell fixed within 3 days)\n- **Infrastructure Security**: No OS patching, server hardening, or vulnerability scanning\n- **Operational Agility**: Quick security response at scale\n- **Focus on Code**: Spend time on business logic, not infrastructure security\n\n## Identity and Access Management\n\n### Least Privilege Principle\n\n**Always use least privilege IAM policies**:\n\n```typescript\n// ✅ GOOD - Specific grant\nconst table = new dynamodb.Table(this, 'Table', {});\nconst function = new lambda.Function(this, 'Function', {});\n\ntable.grantReadData(function); // Only read access\n\n// ❌ BAD - Overly broad\nfunction.addToRolePolicy(new iam.PolicyStatement({\n  actions: ['dynamodb:*'],\n  resources: ['*'],\n}));\n```\n\n### Function Execution Role\n\n**Separate roles per function**:\n\n```typescript\n// ✅ GOOD - Each function has its own role\nconst readFunction = new NodejsFunction(this, 'ReadFunction', {\n  entry: 'src/read.ts',\n  // Gets its own execution role\n});\n\nconst writeFunction = new NodejsFunction(this, 'WriteFunction', {\n  entry: 'src/write.ts',\n  // Gets its own execution role\n});\n\ntable.grantReadData(readFunction);\ntable.grantReadWriteData(writeFunction);\n\n// ❌ BAD - Shared role with excessive permissions\nconst sharedRole = new iam.Role(this, 'SharedRole', {\n  assumedBy: new iam.ServicePrincipal('lambda.amazonaws.com'),\n  managedPolicies: [\n    iam.ManagedPolicy.fromAwsManagedPolicyName('AdministratorAccess'), // Too broad!\n  ],\n});\n```\n\n### Resource-Based Policies\n\nControl who can invoke functions:\n\n```typescript\n// Allow API Gateway to invoke function\nmyFunction.grantInvoke(new iam.ServicePrincipal('apigateway.amazonaws.com'));\n\n// Allow specific account\nmyFunction.addPermission('AllowAccountInvoke', {\n  principal: new iam.AccountPrincipal('123456789012'),\n  action: 'lambda:InvokeFunction',\n});\n\n// Conditional invoke (only from specific VPC endpoint)\nmyFunction.addPermission('AllowVPCInvoke', {\n  principal: new iam.ServicePrincipal('lambda.amazonaws.com'),\n  action: 'lambda:InvokeFunction',\n  sourceArn: vpcEndpoint.vpcEndpointId,\n});\n```\n\n### IAM Policies Best Practices\n\n1. **Use grant methods**: Prefer `.grantXxx()` over manual policies\n2. **Condition keys**: Use IAM conditions for fine-grained control\n3. **Resource ARNs**: Always specify resource ARNs, avoid wildcards\n4. **Session policies**: Use for temporary elevated permissions\n5. **Service Control Policies (SCPs)**: Enforce organization-wide guardrails\n\n## Function Security\n\n### Lambda Isolation Model\n\n**Each function runs in isolated sandbox**:\n- Built on Firecracker microVMs\n- Dedicated execution environment per function\n- No shared memory between functions\n- Isolated file system and network namespace\n- Strong workload isolation\n\n**Execution Environment Security**:\n- One concurrent invocation per environment\n- Environment may be reused (warm starts)\n- `/tmp` storage persists between invocations\n- Sensitive data in memory may persist\n\n### Secure Coding Practices\n\n**Handle sensitive data securely**:\n\n```typescript\n// ✅ GOOD - Clean up sensitive data\nexport const handler = async (event: any) => {\n  const apiKey = process.env.API_KEY;\n\n  try {\n    const result = await callApi(apiKey);\n    return result;\n  } finally {\n    // Clear sensitive data from memory\n    delete process.env.API_KEY;\n  }\n};\n\n// ✅ GOOD - Use Secrets Manager\nimport { SecretsManagerClient, GetSecretValueCommand } from '@aws-sdk/client-secrets-manager';\n\nconst secretsClient = new SecretsManagerClient({});\n\nexport const handler = async (event: any) => {\n  const secret = await secretsClient.send(\n    new GetSecretValueCommand({ SecretId: process.env.SECRET_ARN })\n  );\n\n  const apiKey = secret.SecretString;\n  // Use apiKey\n};\n```\n\n### Dependency Management\n\n**Scan dependencies for vulnerabilities**:\n\n```json\n// package.json\n{\n  \"scripts\": {\n    \"audit\": \"npm audit\",\n    \"audit:fix\": \"npm audit fix\"\n  },\n  \"devDependencies\": {\n    \"snyk\": \"^1.0.0\"\n  }\n}\n```\n\n**Keep dependencies updated**:\n- Run `npm audit` or `pip-audit` regularly\n- Use Dependabot or Snyk for automated scanning\n- Update dependencies promptly when vulnerabilities found\n- Use minimal dependency sets\n\n### Environment Variable Security\n\n**Never store secrets in environment variables**:\n\n```typescript\n// ❌ BAD - Secret in environment variable\nnew NodejsFunction(this, 'Function', {\n  environment: {\n    API_KEY: 'sk-1234567890abcdef', // Never do this!\n  },\n});\n\n// ✅ GOOD - Reference to secret\nnew NodejsFunction(this, 'Function', {\n  environment: {\n    SECRET_ARN: secret.secretArn,\n  },\n});\n\nsecret.grantRead(myFunction);\n```\n\n## API Security\n\n### API Gateway Security\n\n**Authentication and Authorization**:\n\n```typescript\n// Cognito User Pool authorizer\nconst authorizer = new apigateway.CognitoUserPoolsAuthorizer(this, 'Authorizer', {\n  cognitoUserPools: [userPool],\n});\n\napi.root.addMethod('GET', integration, {\n  authorizer,\n  authorizationType: apigateway.AuthorizationType.COGNITO,\n});\n\n// Lambda authorizer for custom auth\nconst customAuthorizer = new apigateway.TokenAuthorizer(this, 'CustomAuth', {\n  handler: authorizerFunction,\n  resultsCacheTtl: Duration.minutes(5),\n});\n\n// IAM authorization for service-to-service\napi.root.addMethod('POST', integration, {\n  authorizationType: apigateway.AuthorizationType.IAM,\n});\n```\n\n### Request Validation\n\n**Validate requests at API Gateway**:\n\n```typescript\nconst validator = new apigateway.RequestValidator(this, 'Validator', {\n  api,\n  validateRequestBody: true,\n  validateRequestParameters: true,\n});\n\nconst model = api.addModel('Model', {\n  schema: {\n    type: apigateway.JsonSchemaType.OBJECT,\n    required: ['email', 'name'],\n    properties: {\n      email: {\n        type: apigateway.JsonSchemaType.STRING,\n        format: 'email',\n      },\n      name: {\n        type: apigateway.JsonSchemaType.STRING,\n        minLength: 1,\n        maxLength: 100,\n      },\n    },\n  },\n});\n\nresource.addMethod('POST', integration, {\n  requestValidator: validator,\n  requestModels: {\n    'application/json': model,\n  },\n});\n```\n\n### Rate Limiting and Throttling\n\n```typescript\nconst api = new apigateway.RestApi(this, 'Api', {\n  deployOptions: {\n    throttlingRateLimit: 1000, // requests per second\n    throttlingBurstLimit: 2000, // burst capacity\n  },\n});\n\n// Per-method throttling\nresource.addMethod('POST', integration, {\n  methodResponses: [{ statusCode: '200' }],\n  requestParameters: {\n    'method.request.header.Authorization': true,\n  },\n  throttling: {\n    rateLimit: 100,\n    burstLimit: 200,\n  },\n});\n```\n\n### API Keys and Usage Plans\n\n```typescript\nconst apiKey = api.addApiKey('ApiKey', {\n  apiKeyName: 'customer-key',\n});\n\nconst plan = api.addUsagePlan('UsagePlan', {\n  name: 'Standard',\n  throttle: {\n    rateLimit: 100,\n    burstLimit: 200,\n  },\n  quota: {\n    limit: 10000,\n    period: apigateway.Period.MONTH,\n  },\n});\n\nplan.addApiKey(apiKey);\nplan.addApiStage({\n  stage: api.deploymentStage,\n});\n```\n\n## Data Protection\n\n### Encryption at Rest\n\n**DynamoDB encryption**:\n\n```typescript\n// Default: AWS-owned CMK (no additional cost)\nconst table = new dynamodb.Table(this, 'Table', {\n  encryption: dynamodb.TableEncryption.AWS_MANAGED, // AWS managed CMK\n});\n\n// Customer-managed CMK (for compliance)\nconst kmsKey = new kms.Key(this, 'Key', {\n  enableKeyRotation: true,\n});\n\nconst table = new dynamodb.Table(this, 'Table', {\n  encryption: dynamodb.TableEncryption.CUSTOMER_MANAGED,\n  encryptionKey: kmsKey,\n});\n```\n\n**S3 encryption**:\n\n```typescript\n// SSE-S3 (default, no additional cost)\nconst bucket = new s3.Bucket(this, 'Bucket', {\n  encryption: s3.BucketEncryption.S3_MANAGED,\n});\n\n// SSE-KMS (for fine-grained access control)\nconst bucket = new s3.Bucket(this, 'Bucket', {\n  encryption: s3.BucketEncryption.KMS,\n  encryptionKey: kmsKey,\n});\n```\n\n**SQS/SNS encryption**:\n\n```typescript\nconst queue = new sqs.Queue(this, 'Queue', {\n  encryption: sqs.QueueEncryption.KMS,\n  encryptionMasterKey: kmsKey,\n});\n\nconst topic = new sns.Topic(this, 'Topic', {\n  masterKey: kmsKey,\n});\n```\n\n### Encryption in Transit\n\n**All AWS service APIs use TLS**:\n- API Gateway endpoints use HTTPS by default\n- Lambda to AWS service communication encrypted\n- EventBridge, SQS, SNS use TLS\n- Custom domains can use ACM certificates\n\n```typescript\n// API Gateway with custom domain\nconst certificate = new acm.Certificate(this, 'Certificate', {\n  domainName: 'api.example.com',\n  validation: acm.CertificateValidation.fromDns(hostedZone),\n});\n\nconst api = new apigateway.RestApi(this, 'Api', {\n  domainName: {\n    domainName: 'api.example.com',\n    certificate,\n  },\n});\n```\n\n### Data Sanitization\n\n**Validate and sanitize inputs**:\n\n```typescript\nimport DOMPurify from 'isomorphic-dompurify';\nimport { z } from 'zod';\n\n// Schema validation\nconst OrderSchema = z.object({\n  orderId: z.string().uuid(),\n  amount: z.number().positive(),\n  email: z.string().email(),\n});\n\nexport const handler = async (event: any) => {\n  const body = JSON.parse(event.body);\n\n  // Validate schema\n  const result = OrderSchema.safeParse(body);\n  if (!result.success) {\n    return {\n      statusCode: 400,\n      body: JSON.stringify({ error: result.error }),\n    };\n  }\n\n  // Sanitize HTML inputs\n  const sanitized = {\n    ...result.data,\n    description: DOMPurify.sanitize(result.data.description),\n  };\n\n  await processOrder(sanitized);\n};\n```\n\n## Network Security\n\n### VPC Configuration\n\n**Lambda in VPC for private resources**:\n\n```typescript\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 2,\n  natGateways: 1,\n});\n\n// Lambda in private subnet\nconst vpcFunction = new NodejsFunction(this, 'VpcFunction', {\n  entry: 'src/handler.ts',\n  vpc,\n  vpcSubnets: {\n    subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS,\n  },\n  securityGroups: [securityGroup],\n});\n\n// Security group for Lambda\nconst securityGroup = new ec2.SecurityGroup(this, 'LambdaSG', {\n  vpc,\n  description: 'Security group for Lambda function',\n  allowAllOutbound: false, // Restrict outbound\n});\n\n// Only allow access to RDS\nsecurityGroup.addEgressRule(\n  ec2.Peer.securityGroupId(rdsSecurityGroup.securityGroupId),\n  ec2.Port.tcp(3306),\n  'Allow MySQL access'\n);\n```\n\n### VPC Endpoints\n\n**Use VPC endpoints for AWS services**:\n\n```typescript\n// S3 VPC endpoint (gateway endpoint, no cost)\nvpc.addGatewayEndpoint('S3Endpoint', {\n  service: ec2.GatewayVpcEndpointAwsService.S3,\n});\n\n// DynamoDB VPC endpoint (gateway endpoint, no cost)\nvpc.addGatewayEndpoint('DynamoDBEndpoint', {\n  service: ec2.GatewayVpcEndpointAwsService.DYNAMODB,\n});\n\n// Secrets Manager VPC endpoint (interface endpoint, cost applies)\nvpc.addInterfaceEndpoint('SecretsManagerEndpoint', {\n  service: ec2.InterfaceVpcEndpointAwsService.SECRETS_MANAGER,\n  privateDnsEnabled: true,\n});\n```\n\n### Security Groups\n\n**Principle of least privilege for network access**:\n\n```typescript\n// Lambda security group\nconst lambdaSG = new ec2.SecurityGroup(this, 'LambdaSG', {\n  vpc,\n  allowAllOutbound: false,\n});\n\n// RDS security group\nconst rdsSG = new ec2.SecurityGroup(this, 'RDSSG', {\n  vpc,\n  allowAllOutbound: false,\n});\n\n// Allow Lambda to access RDS only\nrdsSG.addIngressRule(\n  ec2.Peer.securityGroupId(lambdaSG.securityGroupId),\n  ec2.Port.tcp(3306),\n  'Allow Lambda access'\n);\n\nlambdaSG.addEgressRule(\n  ec2.Peer.securityGroupId(rdsSG.securityGroupId),\n  ec2.Port.tcp(3306),\n  'Allow RDS access'\n);\n```\n\n## Security Monitoring\n\n### CloudWatch Logs\n\n**Enable and encrypt logs**:\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  logRetention: logs.RetentionDays.ONE_WEEK,\n  logGroup: new logs.LogGroup(this, 'LogGroup', {\n    encryptionKey: kmsKey, // Encrypt logs\n    retention: logs.RetentionDays.ONE_WEEK,\n  }),\n});\n```\n\n### CloudTrail\n\n**Enable CloudTrail for audit**:\n\n```typescript\nconst trail = new cloudtrail.Trail(this, 'Trail', {\n  isMultiRegionTrail: true,\n  includeGlobalServiceEvents: true,\n  managementEvents: cloudtrail.ReadWriteType.ALL,\n});\n\n// Log Lambda invocations\ntrail.addLambdaEventSelector([{\n  includeManagementEvents: true,\n  readWriteType: cloudtrail.ReadWriteType.ALL,\n}]);\n```\n\n### GuardDuty\n\n**Enable GuardDuty for threat detection**:\n- Analyzes VPC Flow Logs, DNS logs, CloudTrail events\n- Detects unusual API activity\n- Identifies compromised credentials\n- Monitors for cryptocurrency mining\n\n## Security Best Practices Checklist\n\n### Development\n\n- [ ] Validate and sanitize all inputs\n- [ ] Scan dependencies for vulnerabilities\n- [ ] Use least privilege IAM permissions\n- [ ] Store secrets in Secrets Manager or Parameter Store\n- [ ] Never log sensitive data\n- [ ] Enable encryption for all data stores\n- [ ] Use environment variables for configuration, not secrets\n\n### Deployment\n\n- [ ] Enable CloudTrail in all regions\n- [ ] Configure VPC for sensitive workloads\n- [ ] Use VPC endpoints for AWS service access\n- [ ] Enable GuardDuty for threat detection\n- [ ] Implement resource-based policies\n- [ ] Use AWS WAF for API protection\n- [ ] Enable access logging for API Gateway\n\n### Operations\n\n- [ ] Monitor CloudTrail for unusual activity\n- [ ] Set up alarms for security events\n- [ ] Rotate secrets regularly\n- [ ] Review IAM policies periodically\n- [ ] Audit function permissions\n- [ ] Monitor GuardDuty findings\n- [ ] Implement automated security responses\n\n### Testing\n\n- [ ] Test with least privilege policies\n- [ ] Validate error handling for security failures\n- [ ] Test input validation and sanitization\n- [ ] Verify encryption configurations\n- [ ] Test with malicious payloads\n- [ ] Audit logs for security events\n\n## Summary\n\n- **Shared Responsibility**: AWS handles infrastructure, you handle application security\n- **Least Privilege**: Use IAM grant methods, avoid wildcards\n- **Encryption**: Enable encryption at rest and in transit\n- **Input Validation**: Validate and sanitize all inputs\n- **Dependency Security**: Scan and update dependencies regularly\n- **Monitoring**: Enable CloudTrail, GuardDuty, and CloudWatch\n- **Secrets Management**: Use Secrets Manager, never environment variables\n- **Network Security**: Use VPC, security groups, and VPC endpoints appropriately\n",
        "plugins/serverless-eda/skills/aws-serverless-eda/references/serverless-patterns.md": "# Serverless Architecture Patterns\n\nComprehensive patterns for building serverless applications on AWS based on Well-Architected Framework principles.\n\n## Table of Contents\n\n- [Core Serverless Patterns](#core-serverless-patterns)\n- [API Patterns](#api-patterns)\n- [Data Processing Patterns](#data-processing-patterns)\n- [Integration Patterns](#integration-patterns)\n- [Orchestration Patterns](#orchestration-patterns)\n- [Anti-Patterns](#anti-patterns)\n\n## Core Serverless Patterns\n\n### Pattern: Serverless Microservices\n\n**Use case**: Independent, scalable services with separate databases\n\n**Architecture**:\n```\nAPI Gateway → Lambda Functions → DynamoDB/RDS\n              ↓ (events)\n         EventBridge → Other Services\n```\n\n**CDK Implementation**:\n```typescript\n// User Service\nconst userTable = new dynamodb.Table(this, 'Users', {\n  partitionKey: { name: 'userId', type: dynamodb.AttributeType.STRING },\n  billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,\n});\n\nconst userFunction = new NodejsFunction(this, 'UserHandler', {\n  entry: 'src/services/users/handler.ts',\n  environment: {\n    TABLE_NAME: userTable.tableName,\n  },\n});\n\nuserTable.grantReadWriteData(userFunction);\n\n// Order Service (separate database)\nconst orderTable = new dynamodb.Table(this, 'Orders', {\n  partitionKey: { name: 'orderId', type: dynamodb.AttributeType.STRING },\n  billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,\n});\n\nconst orderFunction = new NodejsFunction(this, 'OrderHandler', {\n  entry: 'src/services/orders/handler.ts',\n  environment: {\n    TABLE_NAME: orderTable.tableName,\n    EVENT_BUS: eventBus.eventBusName,\n  },\n});\n\norderTable.grantReadWriteData(orderFunction);\neventBus.grantPutEventsTo(orderFunction);\n```\n\n**Benefits**:\n- Independent deployment and scaling\n- Database per service (data isolation)\n- Technology diversity\n- Fault isolation\n\n### Pattern: Serverless API Backend\n\n**Use case**: REST or GraphQL API with serverless compute\n\n**REST API with API Gateway**:\n```typescript\nconst api = new apigateway.RestApi(this, 'Api', {\n  restApiName: 'serverless-api',\n  deployOptions: {\n    stageName: 'prod',\n    tracingEnabled: true,\n    loggingLevel: apigateway.MethodLoggingLevel.INFO,\n    dataTraceEnabled: true,\n    metricsEnabled: true,\n  },\n  defaultCorsPreflightOptions: {\n    allowOrigins: apigateway.Cors.ALL_ORIGINS,\n    allowMethods: apigateway.Cors.ALL_METHODS,\n  },\n});\n\n// Resource-based routing\nconst items = api.root.addResource('items');\nitems.addMethod('GET', new apigateway.LambdaIntegration(listFunction));\nitems.addMethod('POST', new apigateway.LambdaIntegration(createFunction));\n\nconst item = items.addResource('{id}');\nitem.addMethod('GET', new apigateway.LambdaIntegration(getFunction));\nitem.addMethod('PUT', new apigateway.LambdaIntegration(updateFunction));\nitem.addMethod('DELETE', new apigateway.LambdaIntegration(deleteFunction));\n```\n\n**GraphQL API with AppSync**:\n```typescript\nconst api = new appsync.GraphqlApi(this, 'Api', {\n  name: 'serverless-graphql-api',\n  schema: appsync.SchemaFile.fromAsset('schema.graphql'),\n  authorizationConfig: {\n    defaultAuthorization: {\n      authorizationType: appsync.AuthorizationType.API_KEY,\n    },\n  },\n  xrayEnabled: true,\n});\n\n// Lambda resolver\nconst dataSource = api.addLambdaDataSource('lambda-ds', resolverFunction);\n\ndataSource.createResolver('QueryGetItem', {\n  typeName: 'Query',\n  fieldName: 'getItem',\n});\n```\n\n### Pattern: Serverless Data Lake\n\n**Use case**: Ingest, process, and analyze large-scale data\n\n**Architecture**:\n```\nS3 (raw data) → Lambda (transform) → S3 (processed)\n                  ↓ (catalog)\n               AWS Glue → Athena (query)\n```\n\n**Implementation**:\n```typescript\nconst rawBucket = new s3.Bucket(this, 'RawData');\nconst processedBucket = new s3.Bucket(this, 'ProcessedData');\n\n// Trigger Lambda on file upload\nrawBucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.LambdaDestination(transformFunction),\n  { prefix: 'incoming/' }\n);\n\n// Transform function\nexport const transform = async (event: S3Event) => {\n  for (const record of event.Records) {\n    const key = record.s3.object.key;\n\n    // Get raw data\n    const raw = await s3.getObject({\n      Bucket: record.s3.bucket.name,\n      Key: key,\n    });\n\n    // Transform data\n    const transformed = await transformData(raw.Body);\n\n    // Write to processed bucket\n    await s3.putObject({\n      Bucket: process.env.PROCESSED_BUCKET,\n      Key: `processed/${key}`,\n      Body: JSON.stringify(transformed),\n    });\n  }\n};\n```\n\n## API Patterns\n\n### Pattern: Authorizer Pattern\n\n**Use case**: Custom authentication and authorization\n\n```typescript\n// Lambda authorizer\nconst authorizer = new apigateway.TokenAuthorizer(this, 'Authorizer', {\n  handler: authorizerFunction,\n  identitySource: 'method.request.header.Authorization',\n  resultsCacheTtl: Duration.minutes(5),\n});\n\n// Apply to API methods\nconst resource = api.root.addResource('protected');\nresource.addMethod('GET', new apigateway.LambdaIntegration(protectedFunction), {\n  authorizer,\n});\n```\n\n### Pattern: Request Validation\n\n**Use case**: Validate requests before Lambda invocation\n\n```typescript\nconst requestModel = api.addModel('RequestModel', {\n  contentType: 'application/json',\n  schema: {\n    type: apigateway.JsonSchemaType.OBJECT,\n    required: ['name', 'email'],\n    properties: {\n      name: { type: apigateway.JsonSchemaType.STRING, minLength: 1 },\n      email: { type: apigateway.JsonSchemaType.STRING, format: 'email' },\n    },\n  },\n});\n\nresource.addMethod('POST', integration, {\n  requestValidator: new apigateway.RequestValidator(this, 'Validator', {\n    api,\n    validateRequestBody: true,\n    validateRequestParameters: true,\n  }),\n  requestModels: {\n    'application/json': requestModel,\n  },\n});\n```\n\n### Pattern: Response Caching\n\n**Use case**: Reduce backend load and improve latency\n\n```typescript\nconst api = new apigateway.RestApi(this, 'Api', {\n  deployOptions: {\n    cachingEnabled: true,\n    cacheTtl: Duration.minutes(5),\n    cacheClusterEnabled: true,\n    cacheClusterSize: '0.5', // GB\n  },\n});\n\n// Enable caching per method\nresource.addMethod('GET', integration, {\n  methodResponses: [{\n    statusCode: '200',\n    responseParameters: {\n      'method.response.header.Cache-Control': true,\n    },\n  }],\n});\n```\n\n## Data Processing Patterns\n\n### Pattern: S3 Event Processing\n\n**Use case**: Process files uploaded to S3\n\n```typescript\nconst bucket = new s3.Bucket(this, 'DataBucket');\n\n// Process images\nbucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.LambdaDestination(imageProcessingFunction),\n  { suffix: '.jpg' }\n);\n\n// Process CSV files\nbucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.LambdaDestination(csvProcessingFunction),\n  { suffix: '.csv' }\n);\n\n// Large file processing with Step Functions\nbucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.SfnDestination(processingStateMachine),\n  { prefix: 'large-files/' }\n);\n```\n\n### Pattern: DynamoDB Streams Processing\n\n**Use case**: React to database changes\n\n```typescript\nconst table = new dynamodb.Table(this, 'Table', {\n  partitionKey: { name: 'id', type: dynamodb.AttributeType.STRING },\n  stream: dynamodb.StreamViewType.NEW_AND_OLD_IMAGES,\n});\n\n// Process stream changes\nnew lambda.EventSourceMapping(this, 'StreamConsumer', {\n  target: streamProcessorFunction,\n  eventSourceArn: table.tableStreamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n  batchSize: 100,\n  maxBatchingWindow: Duration.seconds(5),\n  bisectBatchOnError: true,\n  retryAttempts: 3,\n});\n\n// Example: Sync to search index\nexport const processStream = async (event: DynamoDBStreamEvent) => {\n  for (const record of event.Records) {\n    if (record.eventName === 'INSERT' || record.eventName === 'MODIFY') {\n      const newImage = record.dynamodb?.NewImage;\n      await elasticSearch.index({\n        index: 'items',\n        id: newImage?.id.S,\n        body: unmarshall(newImage),\n      });\n    } else if (record.eventName === 'REMOVE') {\n      await elasticSearch.delete({\n        index: 'items',\n        id: record.dynamodb?.Keys?.id.S,\n      });\n    }\n  }\n};\n```\n\n### Pattern: Kinesis Stream Processing\n\n**Use case**: Real-time data streaming and analytics\n\n```typescript\nconst stream = new kinesis.Stream(this, 'EventStream', {\n  shardCount: 2,\n  streamMode: kinesis.StreamMode.PROVISIONED,\n});\n\n// Fan-out with multiple consumers\nconst consumer1 = new lambda.EventSourceMapping(this, 'Analytics', {\n  target: analyticsFunction,\n  eventSourceArn: stream.streamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n  batchSize: 100,\n  parallelizationFactor: 10, // Process 10 batches per shard in parallel\n});\n\nconst consumer2 = new lambda.EventSourceMapping(this, 'Alerting', {\n  target: alertingFunction,\n  eventSourceArn: stream.streamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n  filters: [\n    lambda.FilterCriteria.filter({\n      eventName: lambda.FilterRule.isEqual('CRITICAL_EVENT'),\n    }),\n  ],\n});\n```\n\n## Integration Patterns\n\n### Pattern: Service Integration with EventBridge\n\n**Use case**: Decouple services with events\n\n```typescript\nconst eventBus = new events.EventBus(this, 'AppBus');\n\n// Service A publishes events\nconst serviceA = new NodejsFunction(this, 'ServiceA', {\n  entry: 'src/services/a/handler.ts',\n  environment: {\n    EVENT_BUS: eventBus.eventBusName,\n  },\n});\n\neventBus.grantPutEventsTo(serviceA);\n\n// Service B subscribes to events\nnew events.Rule(this, 'ServiceBRule', {\n  eventBus,\n  eventPattern: {\n    source: ['service.a'],\n    detailType: ['EntityCreated'],\n  },\n  targets: [new targets.LambdaFunction(serviceBFunction)],\n});\n\n// Service C subscribes to same events\nnew events.Rule(this, 'ServiceCRule', {\n  eventBus,\n  eventPattern: {\n    source: ['service.a'],\n    detailType: ['EntityCreated'],\n  },\n  targets: [new targets.LambdaFunction(serviceCFunction)],\n});\n```\n\n### Pattern: API Gateway + SQS Integration\n\n**Use case**: Async API requests without Lambda\n\n```typescript\nconst queue = new sqs.Queue(this, 'RequestQueue');\n\nconst api = new apigateway.RestApi(this, 'Api');\n\n// Direct SQS integration (no Lambda)\nconst sqsIntegration = new apigateway.AwsIntegration({\n  service: 'sqs',\n  path: `${process.env.AWS_ACCOUNT_ID}/${queue.queueName}`,\n  integrationHttpMethod: 'POST',\n  options: {\n    credentialsRole: sqsRole,\n    requestParameters: {\n      'integration.request.header.Content-Type': \"'application/x-www-form-urlencoded'\",\n    },\n    requestTemplates: {\n      'application/json': 'Action=SendMessage&MessageBody=$input.body',\n    },\n    integrationResponses: [{\n      statusCode: '200',\n    }],\n  },\n});\n\napi.root.addMethod('POST', sqsIntegration, {\n  methodResponses: [{ statusCode: '200' }],\n});\n```\n\n### Pattern: EventBridge + Step Functions\n\n**Use case**: Event-triggered workflow orchestration\n\n```typescript\n// State machine for order processing\nconst orderStateMachine = new stepfunctions.StateMachine(this, 'OrderFlow', {\n  definition: /* ... */,\n});\n\n// EventBridge triggers state machine\nnew events.Rule(this, 'OrderPlacedRule', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n  targets: [new targets.SfnStateMachine(orderStateMachine)],\n});\n```\n\n## Orchestration Patterns\n\n### Pattern: Sequential Workflow\n\n**Use case**: Multi-step process with dependencies\n\n```typescript\nconst definition = new tasks.LambdaInvoke(this, 'Step1', {\n  lambdaFunction: step1Function,\n  outputPath: '$.Payload',\n})\n  .next(new tasks.LambdaInvoke(this, 'Step2', {\n    lambdaFunction: step2Function,\n    outputPath: '$.Payload',\n  }))\n  .next(new tasks.LambdaInvoke(this, 'Step3', {\n    lambdaFunction: step3Function,\n    outputPath: '$.Payload',\n  }));\n\nnew stepfunctions.StateMachine(this, 'Sequential', {\n  definition,\n});\n```\n\n### Pattern: Parallel Execution\n\n**Use case**: Execute independent tasks concurrently\n\n```typescript\nconst parallel = new stepfunctions.Parallel(this, 'ParallelProcessing');\n\nparallel.branch(new tasks.LambdaInvoke(this, 'ProcessA', {\n  lambdaFunction: functionA,\n}));\n\nparallel.branch(new tasks.LambdaInvoke(this, 'ProcessB', {\n  lambdaFunction: functionB,\n}));\n\nparallel.branch(new tasks.LambdaInvoke(this, 'ProcessC', {\n  lambdaFunction: functionC,\n}));\n\nconst definition = parallel.next(new tasks.LambdaInvoke(this, 'Aggregate', {\n  lambdaFunction: aggregateFunction,\n}));\n\nnew stepfunctions.StateMachine(this, 'Parallel', { definition });\n```\n\n### Pattern: Map State (Dynamic Parallelism)\n\n**Use case**: Process array of items in parallel\n\n```typescript\nconst mapState = new stepfunctions.Map(this, 'ProcessItems', {\n  maxConcurrency: 10,\n  itemsPath: '$.items',\n});\n\nmapState.iterator(new tasks.LambdaInvoke(this, 'ProcessItem', {\n  lambdaFunction: processItemFunction,\n}));\n\nconst definition = mapState.next(new tasks.LambdaInvoke(this, 'Finalize', {\n  lambdaFunction: finalizeFunction,\n}));\n```\n\n### Pattern: Choice State (Conditional Logic)\n\n**Use case**: Branching logic based on input\n\n```typescript\nconst choice = new stepfunctions.Choice(this, 'OrderType');\n\nchoice.when(\n  stepfunctions.Condition.stringEquals('$.orderType', 'STANDARD'),\n  standardProcessing\n);\n\nchoice.when(\n  stepfunctions.Condition.stringEquals('$.orderType', 'EXPRESS'),\n  expressProcessing\n);\n\nchoice.otherwise(defaultProcessing);\n```\n\n### Pattern: Wait State\n\n**Use case**: Delay between steps or wait for callbacks\n\n```typescript\n// Fixed delay\nconst wait = new stepfunctions.Wait(this, 'Wait30Seconds', {\n  time: stepfunctions.WaitTime.duration(Duration.seconds(30)),\n});\n\n// Wait until timestamp\nconst waitUntil = new stepfunctions.Wait(this, 'WaitUntil', {\n  time: stepfunctions.WaitTime.timestampPath('$.expiryTime'),\n});\n\n// Wait for callback (.waitForTaskToken)\nconst waitForCallback = new tasks.LambdaInvoke(this, 'WaitForApproval', {\n  lambdaFunction: approvalFunction,\n  integrationPattern: stepfunctions.IntegrationPattern.WAIT_FOR_TASK_TOKEN,\n  payload: stepfunctions.TaskInput.fromObject({\n    token: stepfunctions.JsonPath.taskToken,\n    data: stepfunctions.JsonPath.entirePayload,\n  }),\n});\n```\n\n## Anti-Patterns\n\n### ❌ Lambda Monolith\n\n**Problem**: Single Lambda handling all operations\n\n```typescript\n// BAD\nexport const handler = async (event: any) => {\n  switch (event.operation) {\n    case 'createUser': return createUser(event);\n    case 'getUser': return getUser(event);\n    case 'updateUser': return updateUser(event);\n    case 'deleteUser': return deleteUser(event);\n    case 'createOrder': return createOrder(event);\n    // ... 20 more operations\n  }\n};\n```\n\n**Solution**: Separate Lambda functions per operation\n\n```typescript\n// GOOD - Separate functions\nexport const createUser = async (event: any) => { /* ... */ };\nexport const getUser = async (event: any) => { /* ... */ };\nexport const updateUser = async (event: any) => { /* ... */ };\n```\n\n### ❌ Recursive Lambda Pattern\n\n**Problem**: Lambda invoking itself (runaway costs)\n\n```typescript\n// BAD\nexport const handler = async (event: any) => {\n  await processItem(event);\n\n  if (hasMoreItems()) {\n    await lambda.invoke({\n      FunctionName: process.env.AWS_LAMBDA_FUNCTION_NAME,\n      InvocationType: 'Event',\n      Payload: JSON.stringify({ /* next batch */ }),\n    });\n  }\n};\n```\n\n**Solution**: Use SQS or Step Functions\n\n```typescript\n// GOOD - Use SQS for iteration\nexport const handler = async (event: SQSEvent) => {\n  for (const record of event.Records) {\n    await processItem(record);\n  }\n  // SQS handles iteration automatically\n};\n```\n\n### ❌ Lambda Chaining\n\n**Problem**: Lambda directly invoking another Lambda\n\n```typescript\n// BAD\nexport const handler1 = async (event: any) => {\n  const result = await processStep1(event);\n\n  // Directly invoking next Lambda\n  await lambda.invoke({\n    FunctionName: 'handler2',\n    Payload: JSON.stringify(result),\n  });\n};\n```\n\n**Solution**: Use EventBridge, SQS, or Step Functions\n\n```typescript\n// GOOD - Publish to EventBridge\nexport const handler1 = async (event: any) => {\n  const result = await processStep1(event);\n\n  await eventBridge.putEvents({\n    Entries: [{\n      Source: 'service.step1',\n      DetailType: 'Step1Completed',\n      Detail: JSON.stringify(result),\n    }],\n  });\n};\n```\n\n### ❌ Synchronous Waiting in Lambda\n\n**Problem**: Lambda waiting for slow operations\n\n```typescript\n// BAD - Blocking on slow operation\nexport const handler = async (event: any) => {\n  await startBatchJob(); // Returns immediately\n\n  // Wait for job to complete (wastes Lambda time)\n  while (true) {\n    const status = await checkJobStatus();\n    if (status === 'COMPLETE') break;\n    await sleep(1000);\n  }\n};\n```\n\n**Solution**: Use Step Functions with callback pattern\n\n```typescript\n// GOOD - Step Functions waits, not Lambda\nconst waitForJob = new tasks.LambdaInvoke(this, 'StartJob', {\n  lambdaFunction: startJobFunction,\n  integrationPattern: stepfunctions.IntegrationPattern.WAIT_FOR_TASK_TOKEN,\n  payload: stepfunctions.TaskInput.fromObject({\n    token: stepfunctions.JsonPath.taskToken,\n  }),\n});\n```\n\n### ❌ Large Deployment Packages\n\n**Problem**: Large Lambda packages increase cold start time\n\n**Solution**:\n- Use layers for shared dependencies\n- Externalize AWS SDK\n- Minimize bundle size\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  bundling: {\n    minify: true,\n    externalModules: ['@aws-sdk/*'], // Provided by runtime\n    nodeModules: ['only-needed-deps'], // Selective bundling\n  },\n});\n```\n\n## Performance Optimization\n\n### Cold Start Optimization\n\n**Techniques**:\n1. Minimize package size\n2. Use provisioned concurrency for critical paths\n3. Lazy load dependencies\n4. Reuse connections outside handler\n5. Use Lambda SnapStart (Java)\n\n```typescript\n// For latency-sensitive APIs\nconst apiFunction = new NodejsFunction(this, 'ApiFunction', {\n  entry: 'src/api.ts',\n  memorySize: 1769, // 1 vCPU for faster initialization\n});\n\nconst alias = apiFunction.currentVersion.addAlias('live');\nalias.addAutoScaling({\n  minCapacity: 2,\n  maxCapacity: 10,\n}).scaleOnUtilization({\n  utilizationTarget: 0.7,\n});\n```\n\n### Right-Sizing Memory\n\n**Test different memory configurations**:\n\n```typescript\n// CPU-bound workload\nnew NodejsFunction(this, 'ComputeFunction', {\n  memorySize: 1769, // 1 vCPU\n  timeout: Duration.seconds(30),\n});\n\n// I/O-bound workload\nnew NodejsFunction(this, 'IOFunction', {\n  memorySize: 512, // Less CPU needed\n  timeout: Duration.seconds(60),\n});\n\n// Simple operations\nnew NodejsFunction(this, 'SimpleFunction', {\n  memorySize: 256,\n  timeout: Duration.seconds(10),\n});\n```\n\n### Concurrent Execution Control\n\n```typescript\n// Protect downstream services\nnew NodejsFunction(this, 'Function', {\n  reservedConcurrentExecutions: 10, // Max 10 concurrent\n});\n\n// Unreserved concurrency (shared pool)\nnew NodejsFunction(this, 'Function', {\n  // Uses unreserved account concurrency\n});\n```\n\n## Testing Strategies\n\n### Unit Testing\n\nTest business logic separate from AWS services:\n\n```typescript\n// handler.ts\nexport const processOrder = async (order: Order): Promise<Result> => {\n  // Business logic (easily testable)\n  const validated = validateOrder(order);\n  const priced = calculatePrice(validated);\n  return transformResult(priced);\n};\n\nexport const handler = async (event: any): Promise<any> => {\n  const order = parseEvent(event);\n  const result = await processOrder(order);\n  await saveToDatabase(result);\n  return formatResponse(result);\n};\n\n// handler.test.ts\ntest('processOrder calculates price correctly', () => {\n  const order = { items: [{ price: 10, quantity: 2 }] };\n  const result = processOrder(order);\n  expect(result.total).toBe(20);\n});\n```\n\n### Integration Testing\n\nTest with actual AWS services:\n\n```typescript\n// integration.test.ts\nimport { LambdaClient, InvokeCommand } from '@aws-sdk/client-lambda';\n\ntest('Lambda processes order correctly', async () => {\n  const lambda = new LambdaClient({});\n\n  const response = await lambda.send(new InvokeCommand({\n    FunctionName: process.env.FUNCTION_NAME,\n    Payload: JSON.stringify({ orderId: '123' }),\n  }));\n\n  const result = JSON.parse(Buffer.from(response.Payload!).toString());\n  expect(result.statusCode).toBe(200);\n});\n```\n\n### Local Testing with SAM\n\n```bash\n# Test API locally\nsam local start-api\n\n# Invoke function locally\nsam local invoke MyFunction -e events/test-event.json\n\n# Generate sample event\nsam local generate-event apigateway aws-proxy > event.json\n```\n\n## Summary\n\n- **Single Purpose**: One function, one responsibility\n- **Concurrent Design**: Think concurrency, not volume\n- **Stateless**: Use external storage for state\n- **State Machines**: Orchestrate with Step Functions\n- **Event-Driven**: Use events over direct calls\n- **Idempotent**: Handle failures and duplicates gracefully\n- **Observability**: Enable tracing and structured logging\n"
      },
      "plugins": [
        {
          "name": "aws-common",
          "description": "Shared AWS agent skills including AWS Documentation MCP configuration for querying up-to-date AWS knowledge",
          "version": "1.0.0",
          "author": {
            "name": "Kane Zhu",
            "email": "me@kane.mx"
          },
          "homepage": "https://github.com/zxkane/aws-skills",
          "repository": "https://github.com/zxkane/aws-skills",
          "license": "MIT",
          "keywords": [
            "aws",
            "mcp",
            "documentation",
            "common",
            "shared"
          ],
          "category": "development",
          "source": "./plugins/aws-common",
          "strict": false,
          "skills": [
            "./skills/aws-mcp-setup"
          ],
          "categories": [
            "aws",
            "common",
            "development",
            "documentation",
            "mcp",
            "shared"
          ],
          "install_commands": [
            "/plugin marketplace add zxkane/aws-skills",
            "/plugin install aws-common@aws-skills"
          ]
        },
        {
          "name": "aws-cdk",
          "description": "Comprehensive AWS development skills including CDK best practices, Lambda development workflows, and AWS documentation search capabilities",
          "version": "1.2.0",
          "author": {
            "name": "Kane Zhu",
            "email": "me@kane.mx"
          },
          "homepage": "https://github.com/zxkane/aws-skills",
          "repository": "https://github.com/zxkane/aws-skills",
          "license": "MIT",
          "keywords": [
            "aws",
            "cdk",
            "lambda",
            "infrastructure",
            "cloud",
            "skills"
          ],
          "category": "development",
          "source": "./plugins/aws-cdk",
          "strict": false,
          "skills": [
            "./skills/aws-cdk-development"
          ],
          "mcpServers": {
            "cdk": {
              "type": "stdio",
              "command": "uvx",
              "args": [
                "awslabs.cdk-mcp-server@latest"
              ],
              "env": {
                "FASTMCP_LOG_LEVEL": "ERROR"
              }
            }
          },
          "categories": [
            "aws",
            "cdk",
            "cloud",
            "development",
            "infrastructure",
            "lambda",
            "skills"
          ],
          "install_commands": [
            "/plugin marketplace add zxkane/aws-skills",
            "/plugin install aws-cdk@aws-skills"
          ]
        },
        {
          "name": "aws-cost-ops",
          "description": "AWS cost optimization, monitoring, and operational excellence with integrated MCP servers for billing, cost analysis, observability, and security assessment",
          "version": "1.2.0",
          "author": {
            "name": "Kane Zhu",
            "email": "me@kane.mx"
          },
          "homepage": "https://github.com/zxkane/aws-skills",
          "repository": "https://github.com/zxkane/aws-skills",
          "license": "MIT",
          "keywords": [
            "aws",
            "cost",
            "billing",
            "monitoring",
            "cloudwatch",
            "operations",
            "observability"
          ],
          "category": "operations",
          "source": "./plugins/aws-cost-ops",
          "strict": false,
          "skills": [
            "./skills/aws-cost-operations"
          ],
          "mcpServers": {
            "pricing": {
              "type": "stdio",
              "command": "uvx",
              "args": [
                "awslabs.aws-pricing-mcp-server@latest"
              ],
              "env": {
                "FASTMCP_LOG_LEVEL": "ERROR"
              }
            },
            "costexp": {
              "type": "stdio",
              "command": "uvx",
              "args": [
                "awslabs.cost-explorer-mcp-server@latest"
              ],
              "env": {
                "FASTMCP_LOG_LEVEL": "ERROR"
              }
            },
            "cw": {
              "type": "stdio",
              "command": "uvx",
              "args": [
                "awslabs.cloudwatch-mcp-server@latest"
              ],
              "env": {
                "FASTMCP_LOG_LEVEL": "ERROR"
              }
            }
          },
          "categories": [
            "aws",
            "billing",
            "cloudwatch",
            "cost",
            "monitoring",
            "observability",
            "operations"
          ],
          "install_commands": [
            "/plugin marketplace add zxkane/aws-skills",
            "/plugin install aws-cost-ops@aws-skills"
          ]
        },
        {
          "name": "serverless-eda",
          "description": "AWS serverless and event-driven architecture best practices based on Well-Architected Framework with MCP servers for SAM, Lambda, Step Functions, and messaging",
          "version": "1.2.0",
          "author": {
            "name": "Kane Zhu",
            "email": "me@kane.mx"
          },
          "homepage": "https://github.com/zxkane/aws-skills",
          "repository": "https://github.com/zxkane/aws-skills",
          "license": "MIT",
          "keywords": [
            "aws",
            "serverless",
            "lambda",
            "event-driven",
            "eda",
            "step-functions",
            "eventbridge",
            "sqs",
            "sns"
          ],
          "category": "development",
          "source": "./plugins/serverless-eda",
          "strict": false,
          "skills": [
            "./skills/aws-serverless-eda"
          ],
          "categories": [
            "aws",
            "development",
            "eda",
            "event-driven",
            "eventbridge",
            "lambda",
            "serverless",
            "sns",
            "sqs",
            "step-functions"
          ],
          "install_commands": [
            "/plugin marketplace add zxkane/aws-skills",
            "/plugin install serverless-eda@aws-skills"
          ]
        },
        {
          "name": "aws-agentic-ai",
          "description": "AWS Bedrock AgentCore comprehensive expert for deploying and managing all AgentCore services including Gateway, Runtime, Memory, Identity, Code Interpreter, Browser, and Observability",
          "version": "1.2.0",
          "author": {
            "name": "Kane Zhu",
            "email": "me@kane.mx"
          },
          "homepage": "https://github.com/zxkane/aws-skills",
          "repository": "https://github.com/zxkane/aws-skills",
          "license": "MIT",
          "keywords": [
            "aws",
            "bedrock",
            "agentcore",
            "ai-agents",
            "mcp",
            "gateway",
            "runtime",
            "memory",
            "identity"
          ],
          "category": "development",
          "source": "./plugins/aws-agentic-ai",
          "strict": false,
          "skills": [
            "./skills/aws-agentic-ai"
          ],
          "categories": [
            "agentcore",
            "ai-agents",
            "aws",
            "bedrock",
            "development",
            "gateway",
            "identity",
            "mcp",
            "memory",
            "runtime"
          ],
          "install_commands": [
            "/plugin marketplace add zxkane/aws-skills",
            "/plugin install aws-agentic-ai@aws-skills"
          ]
        }
      ]
    }
  ]
}