{
  "author": {
    "id": "BrandCast-Signage",
    "display_name": "BrandCast & FamilyCast Platforms",
    "avatar_url": "https://avatars.githubusercontent.com/u/235915822?v=4"
  },
  "marketplaces": [
    {
      "name": "agent-benchmark-kit",
      "version": null,
      "description": "Automated quality assurance for Claude Code agents using LLM-as-judge evaluation",
      "repo_full_name": "BrandCast-Signage/agent-benchmark-kit",
      "repo_url": "https://github.com/BrandCast-Signage/agent-benchmark-kit",
      "repo_description": "Automated quality assurance for Claude Code agents using LLM-as-judge evaluation. Built by BrandCast.",
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2025-11-09T23:08:01Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"agent-benchmark-kit\",\n  \"owner\": {\n    \"name\": \"BrandCast\",\n    \"email\": \"hello@brandcast.app\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"agent-benchmark-kit\",\n      \"source\": \"./\",\n      \"description\": \"Automated quality assurance for Claude Code agents using LLM-as-judge evaluation\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"agent-benchmark-kit\",\n  \"description\": \"Automated quality assurance for Claude Code agents using LLM-as-judge evaluation\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"BrandCast\",\n    \"url\": \"https://brandcast.app\"\n  },\n  \"repository\": \"https://github.com/BrandCast-Signage/agent-benchmark-kit\",\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"benchmarking\",\n    \"testing\",\n    \"quality-assurance\",\n    \"llm-as-judge\",\n    \"agents\",\n    \"evaluation\"\n  ],\n  \"homepage\": \"https://github.com/BrandCast-Signage/agent-benchmark-kit\"\n}\n",
        "README.md": "# Agent Benchmark Kit\n\n**Automated quality assurance for Claude Code agents using LLM-as-judge evaluation.**\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![GitHub stars](https://img.shields.io/github/stars/BrandCast-Signage/agent-benchmark-kit?style=social)](https://github.com/BrandCast-Signage/agent-benchmark-kit/stargazers)\n\n---\n\n## Why This Exists\n\nWe built AI agents at [BrandCast](https://brandcast.app) for SEO optimization, content publishing, weekly planning, as well as our technical agent fleet. They needed rigorous quality checks and continuous improvement, but manual testing was time-consuming and inconsistent.\n\nSo we built an automated benchmarking system using **AI to evaluate AI**.\n\nWe're still very early, but the approach shows promise. We're open-sourcing what we've built so far.\n\n---\n\n## What You Get\n\n‚úÖ **Slash command** - `/benchmark-agent` for one-command testing\n\n‚úÖ **Test suite creator** - Generate your first benchmark in < 1 hour\n\n‚úÖ **LLM-as-judge** - Automated, objective scoring\n\n‚úÖ **Performance tracking** - JSON-based history over time\n\n‚úÖ **Test rotation** - Keep agents challenged with fresh tests\n\n‚úÖ **Complete examples** - 2 production-tested benchmark suites\n\n---\n\n## Quick Start\n\n```bash\n# 1. Install via Claude Code Marketplace\n/plugin add https://github.com/BrandCast-Signage/agent-benchmark-kit\n\n# 2. Create your first benchmark\n/benchmark-agent --create my-agent\n\n# 3. Answer 5 questions about your agent\n# [Interactive prompts guide you through test creation]\n\n# 4. Run the benchmark\n/benchmark-agent my-agent\n\n# 5. View results and iterate\n# Results show score breakdown and recommendations\n```\n\n---\n\n## Real-World Results\n\nWe use this framework internally at BrandCast for **7 production agents**:\n\n| Agent | Baseline | Current | Improvement |\n|-------|----------|---------|-------------|\n| **SEO Specialist** | 88/100 | 90/100 | +2.3% in 8 days |\n| **Content Publisher** | 97.5/100 | 97.5/100 | Excellent baseline |\n| **Weekly Planner** | 85/100 | 87/100 | Tracked over 12 weeks |\n\nThese aren't toy examples. **These are production agents serving real users.**\n\n---\n\n## How It Works\n\n```mermaid\ngraph TD\n    A[Create Test Suite] --> B[Define Test Cases]\n    B --> C[Set Ground Truth]\n    C --> D[Run Benchmarks]\n    D --> E[Judge Scores Results]\n    E --> F[Track Performance]\n    F --> G[Iterate & Improve]\n    G --> D\n```\n\n### 1. **Create Test Cases**\nDefine inputs that test your agent's capabilities. The `test-suite-creator` agent helps you design 5 diverse, challenging tests.\n\n### 2. **Set Ground Truth**\nDefine expected outputs in JSON format. What should the agent detect? What decisions should it make?\n\n### 3. **Run Benchmarks**\nExecute tests via the `/benchmark-agent` command. Your agent processes each test case.\n\n### 4. **Judge Scores Results**\nThe `benchmark-judge` agent compares actual output to ground truth, scoring objectively (0-100).\n\n### 5. **Track Performance**\nResults stored in `performance-history.json`. See trends over time, detect regressions.\n\n### 6. **Iterate & Improve**\nUse data to guide prompt improvements. Re-run to validate changes.\n\n---\n\n## Key Features\n\n### üéØ Interactive Test Suite Creator\n\n**Problem:** Creating test cases manually is hard and time-consuming.\n\n**Solution:** Answer 5 questions about your agent, get a complete benchmark suite.\n\n```bash\n/benchmark-agent --create my-agent\n\n# Questions you'll answer:\n# 1. What does your agent do?\n# 2. What validations does it perform?\n# 3. What are common edge cases?\n# 4. What would perfect output look like?\n# 5. What would failing output look like?\n\n# Generates:\n# ‚úì 5 diverse test cases\n# ‚úì Ground truth expectations (JSON)\n# ‚úì Scoring rubric (METRICS.md)\n# ‚úì Complete documentation\n```\n\n**Time to first benchmark: < 1 hour**\n\n---\n\n### üìä LLM-as-Judge Evaluation\n\n**Consistent, objective scoring** using AI to evaluate AI output.\n\nThe `benchmark-judge` agent:\n- Compares actual output to expected results\n- Scores using your custom rubric (0-100 scale)\n- Identifies false positives and missed issues\n- Provides detailed feedback\n\n**Agreement rate with manual scoring: 95%+**\n\n---\n\n### üìà Performance Tracking\n\n**Track improvements over time** with JSON-based history.\n\n```json\n{\n  \"seo-specialist\": {\n    \"baseline\": { \"version\": \"v1\", \"score\": 88 },\n    \"current\": { \"version\": \"v2\", \"score\": 90 },\n    \"trend\": \"improving\",\n    \"runs\": [...]\n  }\n}\n```\n\n**See at a glance:**\n- Current score vs. baseline\n- Trend (improving/stable/regressing)\n- Individual test performance\n- Prompt changes and their impact\n\n---\n\n### üîÑ Intelligent Test Rotation\n\n**Keep benchmarks challenging** with automated test rotation.\n\n**When agent scores 95+ on all tests:**\n- Add new challenging test cases\n- Keep agent from \"gaming\" the tests\n\n**When agent scores 100 three times:**\n- Retire test (agent has mastered it)\n- Focus effort on remaining challenges\n\n**Real-world failures:**\n- Add as regression tests\n- Prevent same issues in future\n\n---\n\n## Examples\n\n### Content Quality Agent\n\nValidates blog posts, documentation, and marketing content.\n\n**Test cases:**\n1. Perfect content (no issues)\n2. Missing metadata (frontmatter errors)\n3. Broken citations (statistics without sources)\n4. Missing resources (hero image doesn't exist)\n5. Format errors (YAML syntax, structure issues)\n\n**Score:** 97.5/100 baseline\n\n[See complete example ‚Üí](examples/content-quality-agent/)\n\n---\n\n### Code Review Agent\n\nReviews TypeScript code for style violations and best practices.\n\n**Test cases:**\n1. Perfect code (follows all rules)\n2. Naming violations (camelCase issues)\n3. Import organization (unsorted imports)\n4. Complex types (formatting edge cases)\n5. Multiple violations (comprehensive test)\n\n**Score:** 85/100 baseline\n\n[See complete example ‚Üí](examples/code-review-agent/)\n\n---\n\n## Installation\n\n### Prerequisites\n\n- [Claude Code](https://claude.com/claude-code) installed\n- Git (for manual installation only)\n\n### Option 1: Plugin Marketplace (Recommended)\n\n```bash\n# In Claude Code, run:\n/plugin add https://github.com/BrandCast-Signage/agent-benchmark-kit\n```\n\nThis will automatically:\n1. Install the plugin with all components\n2. Make `/benchmark-agent` command available\n3. Install 3 benchmark agents\n4. Set up templates in your project\n\n**Note:** After installation, you'll need to create the `~/.agent-benchmarks/` directory for test suites:\n\n```bash\nmkdir -p ~/.agent-benchmarks/{templates,examples}\ncp ~/.claude/plugins/agent-benchmark-kit/templates/* ~/.agent-benchmarks/templates/\n```\n\n### Option 2: Install Script\n\nFor a complete setup including examples and templates:\n\n```bash\ngit clone https://github.com/BrandCast-Signage/agent-benchmark-kit.git\ncd agent-benchmark-kit\n./scripts/install.sh\n```\n\nThe install script:\n1. Copies agents to `.claude/agents/`\n2. Copies slash command to `.claude/commands/`\n3. Creates `~/.agent-benchmarks/` directory\n4. Sets up templates and examples\n\n### Option 3: Manual Installation\n\nSee [docs/getting-started.md](docs/getting-started.md) for manual setup instructions.\n\n---\n\n## Usage\n\n### Creating Your First Benchmark\n\n```bash\n# Start interactive creation\n/benchmark-agent --create my-content-agent\n\n# Answer guided questions\n# > What does your agent do?\n# > What validations does it perform?\n# > ...\n\n# Review generated suite\nls ~/.agent-benchmarks/my-content-agent/\n# test-cases/\n# ground-truth/\n# METRICS.md\n# README.md\n\n# Run the benchmark\n/benchmark-agent my-content-agent\n```\n\n---\n\n### Running Benchmarks\n\n```bash\n# Run specific agent\n/benchmark-agent seo-specialist\n\n# Run all agents\n/benchmark-agent --all\n\n# Run with test rotation\n/benchmark-agent seo-specialist --rotate\n\n# Generate report without running tests\n/benchmark-agent --report-only\n```\n\n---\n\n### Interpreting Results\n\n```markdown\n# Benchmark Results: seo-specialist\n\n**Score:** 90/100 ‚úÖ PASS (threshold: 80)\n\n## Individual Tests\n- Test #01 (mediocre content): 82/100 ‚úì\n- Test #02 (excellent content): 96/100 ‚úì\n- Test #03 (keyword stuffing): 92/100 ‚úì\n\n## Trend\n- Baseline (v1): 88/100\n- Current (v2): 90/100\n- **Improvement: +2 points (+2.3%)**\n\n## Recommendations\n- ‚úÖ PASS - Deploy v2\n- Agent shows consistent improvement\n- No regressions detected\n```\n\n---\n\n## Documentation\n\n- **[Getting Started](docs/getting-started.md)** - Installation and first benchmark\n- **[Creating Test Suites](docs/test-creation-guide.md)** - How to design effective tests\n- **[Scoring Rubrics](docs/scoring-rubrics.md)** - How to create fair scoring\n- **[Advanced Usage](docs/advanced-usage.md)** - Test rotation, tracking, tips\n- **[Architecture](docs/architecture.md)** - How the system works\n\n---\n\n## Contributing\n\nWe welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n**Ideas for contributions:**\n- üìö New example benchmark suites (different agent types)\n- üéØ Improved test rotation strategies\n- üìä Alternative storage backends (SQLite, PostgreSQL)\n- üîç Enhanced judge scoring accuracy\n- üìñ Documentation improvements\n- üêõ Bug fixes\n\n**Recognition:** Contributors featured in README and blog posts.\n\n---\n\n## Community\n\n- **GitHub Issues:** [Report bugs or request features](https://github.com/BrandCast-Signage/agent-benchmark-kit/issues)\n- **Discussions:** [Share your benchmark suites](https://github.com/BrandCast-Signage/agent-benchmark-kit/discussions)\n- **Twitter:** [@BrandCastApp](https://twitter.com/BrandCastApp)\n- **Blog:** [BrandCast Engineering Blog](https://news.brandcast.app)\n\n---\n\n## Roadmap\n\n### v1.0 (Current)\n- ‚úÖ Core framework (slash command, agents, templates)\n- ‚úÖ Test suite creator\n- ‚úÖ JSON-based performance tracking\n- ‚úÖ 2 complete examples\n\n### v1.1 (Next)\n- [ ] SQLite migration tool (optional upgrade from JSON)\n- [ ] Web dashboard for viewing trends\n- [ ] GitHub Actions integration (CI/CD)\n- [ ] More example benchmark suites\n\n### v2.0 (Future)\n- [ ] Automated test generation (LLM suggests new tests)\n- [ ] Comparative benchmarking (compare agents)\n- [ ] Team collaboration features\n- [ ] Plugin ecosystem\n\n**Vote on features:** [GitHub Discussions](https://github.com/BrandCast-Signage/agent-benchmark-kit/discussions)\n\n---\n\n## FAQ\n\n### How is this different from PromptFoo?\n\n**PromptFoo** focuses on single-shot LLM prompts.\n**Agent Benchmark Kit** focuses on multi-step Claude Code agents with complex workflows.\n\nKey differences:\n- Native Claude Code integration (Task tool, slash commands)\n- Test suite creator (guided benchmark creation)\n- Production examples (real agent use cases)\n\n### Does this work with other AI frameworks?\n\nCurrently optimized for **Claude Code agents specifically**.\n\nThe methodology could be adapted to other frameworks (LangChain, AutoGPT, etc.), but integration would require custom work.\n\n### How accurate is LLM-as-judge scoring?\n\nIn our testing: **95%+ agreement with manual human scoring**.\n\nThe judge agent:\n- Compares objective criteria (did agent detect issue X?)\n- Uses clear rubrics (defined in METRICS.md)\n- Flags ambiguous cases for human review\n\n### What's the performance overhead?\n\n**Minimal.** Running a 5-test benchmark:\n- Time: ~2-5 minutes (depending on agent complexity)\n- Cost: ~$0.10-0.25 in API costs (Claude Sonnet pricing)\n\nFor weekly runs: **~$1-2/month per agent**\n\n### Can I keep my test cases private?\n\n**Yes!** The framework is open source, but your implementation is private.\n\n**Public:** Framework code, examples, documentation\n**Private:** Your test cases, ground truth, agent prompts, performance data\n\n---\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.\n\n**Use freely, even commercially.** We built this to help the community.\n\n---\n\n## Acknowledgments\n\n**Built with ‚ù§Ô∏è at [BrandCast](https://brandcast.app)**\n\nBrandCast is AI-powered digital signage for small businesses. We use Claude Code agents in production every day, and this framework ensures they work correctly.\n\n**Inspired by:**\n- [Google Cloud: Stop Guessing and Start Benchmarking](https://medium.com/google-cloud/stop-guessing-and-start-benchmarking-your-ai-prompts-312d4f01f65c)\n- The LLM-as-judge methodology from research papers\n- Our own 3+ months of production agent use\n\n**Special thanks:**\n- Claude Code team for building an amazing agent development platform\n- Early testers who provided feedback\n- Contributors who improve this framework\n\n---\n\n## Star This Repo ‚≠ê\n\nIf you find this useful, **star the repository** to show support!\n\nIt helps others discover the project and validates our decision to open source.\n\n---\n\n**Questions?** [Open an issue](https://github.com/BrandCast-Signage/agent-benchmark-kit/issues) or [start a discussion](https://github.com/BrandCast-Signage/agent-benchmark-kit/discussions).\n\n**Want to contribute?** See [CONTRIBUTING.md](CONTRIBUTING.md).\n\n**Follow our journey:** [@BrandCastApp on Twitter](https://twitter.com/BrandCastApp)\n"
      },
      "plugins": [
        {
          "name": "agent-benchmark-kit",
          "source": "./",
          "description": "Automated quality assurance for Claude Code agents using LLM-as-judge evaluation",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add BrandCast-Signage/agent-benchmark-kit",
            "/plugin install agent-benchmark-kit@agent-benchmark-kit"
          ]
        }
      ]
    }
  ]
}