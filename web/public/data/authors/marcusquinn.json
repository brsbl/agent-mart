{
  "author": {
    "id": "marcusquinn",
    "display_name": "Marcus Quinn",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/6428977?u=135e0cc0e5b6e10572147e5ad35c0b563c4135c3&v=4",
    "url": "https://github.com/marcusquinn",
    "bio": "Open-Source wins. You can't lose what you give freely. Knowledge shared, multiplies.",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 22,
      "total_skills": 0,
      "total_stars": 22,
      "total_forks": 4
    }
  },
  "marketplaces": [
    {
      "name": "aidevops",
      "version": null,
      "description": "AI DevOps Framework - comprehensive DevOps automation with 25+ service integrations",
      "owner_info": {
        "name": "Marcus Quinn",
        "email": "marcus@aidevops.sh"
      },
      "keywords": [],
      "repo_full_name": "marcusquinn/aidevops",
      "repo_url": "https://github.com/marcusquinn/aidevops",
      "repo_description": "Vibe-Coding is easy. DevOps is hard. AI DevOps gives you and your army of AI assistants god-level 100x developer superpowers, with guided managed infrastructure & automation, through AI chat in OpenCode, Claude Code, and others. Opinionated tools & services tech-stack for speed, reliability, security, open-source-preferred and SOTA everything.",
      "homepage": "https://www.aidevops.sh",
      "signals": {
        "stars": 22,
        "forks": 4,
        "pushed_at": "2026-01-29T19:21:05Z",
        "created_at": "2025-11-09T05:15:11Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".agent",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/memory",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/memory/README.md",
          "type": "blob",
          "size": 7032
        },
        {
          "path": ".agent/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/scripts/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/scripts/commands/add-skill.md",
          "type": "blob",
          "size": 5014
        },
        {
          "path": ".agent/scripts/commands/code-simplifier.md",
          "type": "blob",
          "size": 2179
        },
        {
          "path": ".agent/scripts/commands/email-health-check.md",
          "type": "blob",
          "size": 2364
        },
        {
          "path": ".agent/scripts/commands/full-loop.md",
          "type": "blob",
          "size": 10583
        },
        {
          "path": ".agent/scripts/commands/humanise.md",
          "type": "blob",
          "size": 2460
        },
        {
          "path": ".agent/scripts/commands/list-todo.md",
          "type": "blob",
          "size": 3321
        },
        {
          "path": ".agent/scripts/commands/log-issue-aidevops.md",
          "type": "blob",
          "size": 4098
        },
        {
          "path": ".agent/scripts/commands/neuronwriter.md",
          "type": "blob",
          "size": 4679
        },
        {
          "path": ".agent/scripts/commands/performance.md",
          "type": "blob",
          "size": 2893
        },
        {
          "path": ".agent/scripts/commands/postflight-loop.md",
          "type": "blob",
          "size": 2267
        },
        {
          "path": ".agent/scripts/commands/pr-loop.md",
          "type": "blob",
          "size": 3701
        },
        {
          "path": ".agent/scripts/commands/readme.md",
          "type": "blob",
          "size": 3422
        },
        {
          "path": ".agent/scripts/commands/recall.md",
          "type": "blob",
          "size": 4163
        },
        {
          "path": ".agent/scripts/commands/remember.md",
          "type": "blob",
          "size": 3476
        },
        {
          "path": ".agent/scripts/commands/save-todo.md",
          "type": "blob",
          "size": 4301
        },
        {
          "path": ".agent/scripts/commands/seo-analyze.md",
          "type": "blob",
          "size": 1589
        },
        {
          "path": ".agent/scripts/commands/seo-audit.md",
          "type": "blob",
          "size": 4831
        },
        {
          "path": ".agent/scripts/commands/seo-export.md",
          "type": "blob",
          "size": 1653
        },
        {
          "path": ".agent/scripts/commands/seo-opportunities.md",
          "type": "blob",
          "size": 1885
        },
        {
          "path": ".agent/scripts/commands/session-review.md",
          "type": "blob",
          "size": 3444
        },
        {
          "path": ".agent/scripts/commands/show-plan.md",
          "type": "blob",
          "size": 3370
        },
        {
          "path": ".agent/scripts/commands/yt-dlp.md",
          "type": "blob",
          "size": 3342
        },
        {
          "path": ".agent/services",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/agents-sdk",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/agents-sdk/README.md",
          "type": "blob",
          "size": 1008
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/ai-gateway",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/ai-gateway/README.md",
          "type": "blob",
          "size": 19858
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/ai-search",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/ai-search/README.md",
          "type": "blob",
          "size": 560
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/analytics-engine",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/analytics-engine/README.md",
          "type": "blob",
          "size": 637
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/api-shield",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/api-shield/README.md",
          "type": "blob",
          "size": 973
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/api",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/api/README.md",
          "type": "blob",
          "size": 739
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/argo-smart-routing",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/argo-smart-routing/README.md",
          "type": "blob",
          "size": 742
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/bindings",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/bindings/README.md",
          "type": "blob",
          "size": 552
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/bot-management",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/bot-management/README.md",
          "type": "blob",
          "size": 3081
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/browser-rendering",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/browser-rendering/README.md",
          "type": "blob",
          "size": 741
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/c3",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/c3/README.md",
          "type": "blob",
          "size": 5556
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/cache-reserve",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/cache-reserve/README.md",
          "type": "blob",
          "size": 3130
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/containers",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/containers/README.md",
          "type": "blob",
          "size": 734
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/cron-triggers",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/cron-triggers/README.md",
          "type": "blob",
          "size": 2302
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/d1",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/d1/README.md",
          "type": "blob",
          "size": 3080
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/ddos",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/ddos/README.md",
          "type": "blob",
          "size": 1175
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/do-storage",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/do-storage/README.md",
          "type": "blob",
          "size": 2262
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/durable-objects",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/durable-objects/README.md",
          "type": "blob",
          "size": 4800
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/email-routing",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/email-routing/README.md",
          "type": "blob",
          "size": 737
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/email-workers",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/email-workers/README.md",
          "type": "blob",
          "size": 14005
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/hyperdrive",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/hyperdrive/README.md",
          "type": "blob",
          "size": 1790
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/images",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/images/README.md",
          "type": "blob",
          "size": 598
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/kv",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/kv/README.md",
          "type": "blob",
          "size": 1922
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/miniflare",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/miniflare/README.md",
          "type": "blob",
          "size": 1767
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/network-interconnect",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/network-interconnect/README.md",
          "type": "blob",
          "size": 1947
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/observability",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/observability/README.md",
          "type": "blob",
          "size": 728
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/pages-functions",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/pages-functions/README.md",
          "type": "blob",
          "size": 1736
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/pages",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/pages/README.md",
          "type": "blob",
          "size": 2287
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/pipelines",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/pipelines/README.md",
          "type": "blob",
          "size": 16133
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/pulumi",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/pulumi/README.md",
          "type": "blob",
          "size": 2523
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/queues",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/queues/README.md",
          "type": "blob",
          "size": 1942
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/r2-data-catalog",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/r2-data-catalog/README.md",
          "type": "blob",
          "size": 739
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/r2-sql",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/r2-sql/README.md",
          "type": "blob",
          "size": 13088
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/r2",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/r2/README.md",
          "type": "blob",
          "size": 1963
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/realtime-sfu",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/realtime-sfu/README.md",
          "type": "blob",
          "size": 972
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/realtimekit",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/realtimekit/README.md",
          "type": "blob",
          "size": 3122
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/sandbox",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/sandbox/README.md",
          "type": "blob",
          "size": 2725
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/smart-placement",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/smart-placement/README.md",
          "type": "blob",
          "size": 3187
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/snippets",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/snippets/README.md",
          "type": "blob",
          "size": 717
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/spectrum",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/spectrum/README.md",
          "type": "blob",
          "size": 732
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/static-assets",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/static-assets/README.md",
          "type": "blob",
          "size": 676
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/stream",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/stream/README.md",
          "type": "blob",
          "size": 3232
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/tail-workers",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/tail-workers/README.md",
          "type": "blob",
          "size": 16367
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/terraform",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/terraform/README.md",
          "type": "blob",
          "size": 2448
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/tunnel",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/tunnel/README.md",
          "type": "blob",
          "size": 2153
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/turn",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/turn/README.md",
          "type": "blob",
          "size": 17906
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/turnstile",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/turnstile/README.md",
          "type": "blob",
          "size": 609
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/vectorize",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/vectorize/README.md",
          "type": "blob",
          "size": 19113
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/waf",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/waf/README.md",
          "type": "blob",
          "size": 590
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/web-analytics",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/web-analytics/README.md",
          "type": "blob",
          "size": 737
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workerd",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workerd/README.md",
          "type": "blob",
          "size": 1626
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers-ai",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers-ai/README.md",
          "type": "blob",
          "size": 4390
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers-for-platforms",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers-for-platforms/README.md",
          "type": "blob",
          "size": 1793
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers-playground",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers-playground/README.md",
          "type": "blob",
          "size": 724
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers-vpc",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers-vpc/README.md",
          "type": "blob",
          "size": 13538
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workers/README.md",
          "type": "blob",
          "size": 2906
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/workflows/README.md",
          "type": "blob",
          "size": 2121
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/wrangler",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/wrangler/README.md",
          "type": "blob",
          "size": 2538
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/zaraz",
          "type": "tree",
          "size": null
        },
        {
          "path": ".agent/services/hosting/cloudflare-platform/references/zaraz/README.md",
          "type": "blob",
          "size": 7031
        },
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 544
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"aidevops\",\n  \"owner\": {\n    \"name\": \"Marcus Quinn\",\n    \"email\": \"marcus@aidevops.sh\"\n  },\n  \"metadata\": {\n    \"description\": \"AI DevOps Framework - comprehensive DevOps automation with 25+ service integrations\",\n    \"version\": \"2.93.2\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"aidevops\",\n      \"source\": \"./.agent\",\n      \"description\": \"Complete AI DevOps framework - 14 domain agents, 80+ subagents, 100+ helper scripts for WordPress, SEO, hosting, code quality, browser automation, and more\",\n      \"strict\": false\n    }\n  ]\n}\n",
        ".agent/memory/README.md": "---\ndescription: Memory template directory documentation\nmode: subagent\ntools:\n  read: true\n  write: false\n  edit: false\n  bash: true\n  glob: true\n  grep: true\n  webfetch: false\n  task: true\n---\n\n# Memory System\n\nCross-session memory for AI assistants using SQLite FTS5 for fast full-text search.\n\n**Motto**: \"Compound, then clear\" - Sessions should build on each other.\n\n## Quick Start\n\n```bash\n# Store a memory\n~/.aidevops/agents/scripts/memory-helper.sh store --type \"WORKING_SOLUTION\" --content \"Fixed CORS with nginx headers\" --tags \"cors,nginx\"\n\n# Recall memories\n~/.aidevops/agents/scripts/memory-helper.sh recall \"cors\"\n\n# Show recent memories\n~/.aidevops/agents/scripts/memory-helper.sh recall --recent\n\n# View statistics\n~/.aidevops/agents/scripts/memory-helper.sh stats\n```\n\n## Slash Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/remember {content}` | Store a memory with AI-assisted categorization |\n| `/recall {query}` | Search memories by keyword |\n| `/recall --recent` | Show 10 most recent memories |\n| `/recall --stats` | Show memory statistics |\n\nSee `scripts/commands/remember.md` and `scripts/commands/recall.md` for full documentation.\n\n## Memory Types\n\n| Type | Use For |\n|------|---------|\n| `WORKING_SOLUTION` | Fixes that worked |\n| `FAILED_APPROACH` | What didn't work (avoid repeating) |\n| `CODEBASE_PATTERN` | Project conventions |\n| `USER_PREFERENCE` | Developer preferences |\n| `TOOL_CONFIG` | Tool setup notes |\n| `DECISION` | Architecture decisions |\n| `CONTEXT` | Background info |\n\n## Storage Location\n\n```text\n~/.aidevops/.agent-workspace/memory/\n├── memory.db           # SQLite database with FTS5\n└── preferences/        # Optional: markdown preference files\n```\n\n## CLI Reference\n\n```bash\n# Store with project context\nmemory-helper.sh store --type \"TYPE\" --content \"content\" --tags \"tags\" --project \"project-name\"\n\n# Search with filters\nmemory-helper.sh recall \"query\" --type WORKING_SOLUTION --project myapp --limit 20\n\n# Show recent memories\nmemory-helper.sh recall --recent 10\n\n# Maintenance\nmemory-helper.sh validate          # Check for stale entries\nmemory-helper.sh prune --dry-run   # Preview cleanup\nmemory-helper.sh prune             # Remove stale entries\n\n# Export\nmemory-helper.sh export --format json   # Export as JSON\nmemory-helper.sh export --format toon   # Export as TOON (token-efficient)\n```\n\n## Legacy: File-Based Preferences\n\nFor detailed preference files (optional, complements SQLite):\n\n```text\n~/.aidevops/.agent-workspace/memory/preferences/\n├── coding-style.md\n├── workflow.md\n└── project-specific/\n    └── wordpress.md\n```\n\n## Developer Preferences Memory\n\n### Purpose\n\nMaintain a consistent record of developer preferences across coding sessions to:\n\n- Ensure AI assistants provide assistance aligned with the developer's preferred style\n- Reduce the need for developers to repeatedly explain their preferences\n- Create a persistent context across tools and sessions\n\n### How AI Assistants Should Use Preferences\n\n1. **Before starting work**: Check `~/.aidevops/.agent-workspace/memory/preferences/` for relevant preferences\n2. **During development**: Apply established preferences to suggestions and code\n3. **When feedback is given**: Update preference files to record new preferences\n4. **When switching projects**: Check for project-specific preference files\n\n### Preference Categories to Track\n\n#### Code Style Preferences\n\n```markdown\n# ~/.aidevops/.agent-workspace/memory/preferences/coding-style.md\n\n## General\n- Preferred indentation: [tabs/spaces, count]\n- Line length limit: [80/100/120]\n- Quote style: [single/double]\n\n## Language-Specific\n### JavaScript/TypeScript\n- Semicolons: [yes/no]\n- Arrow functions: [preferred/when-appropriate]\n\n### Python\n- Type hints: [always/public-only/never]\n- Docstring style: [Google/NumPy/Sphinx]\n\n### PHP\n- WordPress coding standards: [yes/no]\n- PSR-12: [yes/no]\n```\n\n#### Documentation Preferences\n\n```markdown\n# ~/.aidevops/.agent-workspace/memory/preferences/documentation.md\n\n## Code Comments\n- Prefer: [minimal/moderate/extensive]\n- JSDoc/PHPDoc: [always/public-only/never]\n\n## Project Documentation\n- README format: [brief/comprehensive]\n- Changelog style: [Keep a Changelog/custom]\n\n## AI Assistant Documentation\n- Token-efficient: [yes/no]\n- Reference external files: [yes/no]\n```\n\n#### Workflow Preferences\n\n```markdown\n# ~/.aidevops/.agent-workspace/memory/preferences/workflow.md\n\n## Git\n- Commit message style: [conventional/descriptive]\n- Branch naming: [feature/issue-123/kebab-case]\n- Squash commits: [yes/no]\n\n## Testing\n- Test coverage minimum: [80%/90%/100%]\n- TDD approach: [yes/no]\n\n## CI/CD\n- Auto-fix on commit: [yes/no]\n- Required checks: [list]\n```\n\n#### Tool Preferences\n\n```markdown\n# ~/.aidevops/.agent-workspace/memory/preferences/tools.md\n\n## Editors/IDEs\n- Primary: [VS Code/Cursor/etc]\n- Extensions: [list relevant]\n\n## Terminal\n- Shell: [zsh/bash/fish]\n- Custom aliases: [note any that affect commands]\n\n## Environment\n- Node.js manager: [nvm/n/fnm]\n- Python manager: [pyenv/conda/system]\n- Package managers: [npm/yarn/pnpm]\n```\n\n### Project-Specific Preferences\n\nFor projects with unique requirements:\n\n```markdown\n# ~/.aidevops/.agent-workspace/memory/preferences/project-specific/wordpress.md\n\n## WordPress Development\n- Prefer simpler solutions over complex ones\n- Follow WordPress coding standards\n- Use OOP best practices\n- Admin functionality in admin/lib/\n- Core functionality in includes/\n- Assets in /assets organized by admin folders\n- Version updates require language file updates (POT/PO)\n\n## Plugin Release Process\n- Create version branch from main\n- Update all version references\n- Run quality checks before merge\n- Create GitHub tag and release\n- Ensure readme.txt is updated (Git Updater uses main branch)\n```\n\n### Potential Issues to Track\n\nDocument environment-specific issues that affect AI assistance:\n\n```markdown\n# ~/.aidevops/.agent-workspace/memory/preferences/environment-issues.md\n\n## Terminal Customizations\n- Non-standard prompt: [describe]\n- Custom aliases that might confuse: [list]\n- Shell integrations: [starship/oh-my-zsh/etc]\n\n## Multiple Runtime Versions\n- Node.js versions: [list, note if Homebrew]\n- Python versions: [list, note manager]\n- PHP versions: [list]\n\n## Known Conflicts\n- [Document any tool conflicts discovered]\n```\n\n## Security Guidelines\n\n- **Never store credentials** in memory files\n- **Use configuration references** instead of actual API keys\n- **Keep sensitive data** in separate secure locations (`~/.config/aidevops/mcp-env.sh`)\n- **Regular cleanup** of outdated information\n- **No personal identifiable information** in shareable templates\n\n## Important Reminders\n\n- **Never store personal data** in this template directory\n- **Use ~/.aidevops/.agent-workspace/memory/** for all actual operations\n- **This directory is version controlled** - keep it clean\n- **Respect privacy** - be mindful of what you store\n- **Update preferences** when developer feedback indicates a change\n",
        ".agent/scripts/commands/add-skill.md": "---\ndescription: Import external skills from GitHub or ClawdHub into aidevops\nagent: Build+\nmode: subagent\n---\n\nImport an external skill from GitHub or ClawdHub, convert it to aidevops format, and register it for update tracking.\n\nURL/Repo: $ARGUMENTS\n\n## Quick Reference\n\n```bash\n# Import skill from GitHub (saved as *-skill.md)\n/add-skill dmmulroy/cloudflare-skill\n# → .agent/services/hosting/cloudflare-skill.md\n\n# Import specific skill from multi-skill repo\n/add-skill anthropics/skills/pdf\n# → .agent/tools/pdf-skill.md\n\n# Import from ClawdHub (shorthand)\n/add-skill clawdhub:caldav-calendar\n# → .agent/tools/productivity/caldav-calendar-skill.md\n\n# Import from ClawdHub (full URL)\n/add-skill https://clawdhub.com/Asleep123/caldav-calendar\n\n# Import with custom name\n/add-skill vercel-labs/agent-skills --name vercel-deploy\n# → .agent/tools/deployment/vercel-deploy-skill.md\n\n# List imported skills\n/add-skill list\n\n# Check for updates\n/add-skill check-updates\n```\n\n## Naming Convention\n\nImported skills are saved with a `-skill` suffix to distinguish them from native aidevops subagents:\n\n| Type | Example | Managed by |\n|------|---------|------------|\n| Native subagent | `playwright.md` | aidevops team, evolves with framework |\n| Imported skill | `playwright-skill.md` | Upstream repo, checked for updates |\n\nThis means:\n- No name clashes between imports and native subagents\n- `*-skill.md` glob finds all imports instantly\n- `aidevops skill check` knows which files to check for upstream updates\n- Issues with imported skills → check upstream; issues with native → evolve locally\n\n## Workflow\n\n### Step 1: Parse Input\n\nDetermine if the input is:\n- A GitHub shorthand: `owner/repo` or `owner/repo/subpath`\n- A full GitHub URL: `https://github.com/owner/repo`\n- A ClawdHub shorthand: `clawdhub:<slug>`\n- A ClawdHub URL: `https://clawdhub.com/owner/slug`\n- A command: `list`, `check-updates`, `remove <name>`\n\n### Step 2: Run Helper Script\n\n```bash\n~/.aidevops/agents/scripts/add-skill-helper.sh add \"$ARGUMENTS\"\n```\n\nFor other commands:\n\n```bash\n# List all imported skills\n~/.aidevops/agents/scripts/add-skill-helper.sh list\n\n# Check for upstream updates\n~/.aidevops/agents/scripts/add-skill-helper.sh check-updates\n\n# Remove a skill\n~/.aidevops/agents/scripts/add-skill-helper.sh remove <name>\n```\n\n### Step 3: Handle Conflicts\n\nIf the skill conflicts with existing files, the helper will prompt:\n\n1. **Merge** - Add new content to existing file (preserves both)\n2. **Replace** - Overwrite existing with imported skill\n3. **Separate** - Use a different name for the imported skill\n4. **Skip** - Cancel the import\n\n### Step 4: Post-Import\n\nAfter successful import:\n\n1. The skill is placed in `.agent/` following aidevops conventions\n2. Registered in `.agent/configs/skill-sources.json` for update tracking\n3. Run `./setup.sh` to create symlinks for other AI assistants\n\n## Supported Sources & Formats\n\n| Source | Detection | Fetch Method |\n|--------|-----------|--------------|\n| GitHub | `owner/repo` or github.com URL | `git clone --depth 1` |\n| ClawdHub | `clawdhub:slug` or clawdhub.com URL | Playwright browser extraction |\n\n| Format | Detection | Conversion |\n|--------|-----------|------------|\n| SKILL.md | OpenSkills/Claude Code/ClawdHub | Frontmatter preserved, content adapted |\n| AGENTS.md | aidevops/Windsurf | Direct copy with mode: subagent |\n| .cursorrules | Cursor | Wrapped in markdown with frontmatter |\n| README.md | Generic | Copied as-is |\n\n## Examples\n\n```bash\n# Import Cloudflare skill (60+ products)\n/add-skill dmmulroy/cloudflare-skill\n\n# Import PDF manipulation skill from Anthropic\n/add-skill anthropics/skills/pdf\n\n# Import Vercel deployment skill\n/add-skill vercel-labs/agent-skills\n\n# Import from ClawdHub\n/add-skill clawdhub:caldav-calendar\n/add-skill clawdhub:proxmox-full\n/add-skill https://clawdhub.com/mSarheed/proxmox-full\n\n# Import with force (overwrite existing)\n/add-skill dmmulroy/cloudflare-skill --force\n\n# Dry run (show what would happen)\n/add-skill dmmulroy/cloudflare-skill --dry-run\n```\n\n## Update Tracking\n\nImported skills are tracked in `.agent/configs/skill-sources.json`:\n\n```json\n{\n  \"skills\": [\n    {\n      \"name\": \"cloudflare\",\n      \"upstream_url\": \"https://github.com/dmmulroy/cloudflare-skill\",\n      \"upstream_commit\": \"abc123...\",\n      \"local_path\": \".agent/services/hosting/cloudflare-skill.md\",\n      \"format_detected\": \"skill-md\",\n      \"imported_at\": \"2026-01-21T00:00:00Z\",\n      \"last_checked\": \"2026-01-21T00:00:00Z\",\n      \"merge_strategy\": \"added\"\n    }\n  ]\n}\n```\n\nRun `/add-skill check-updates` periodically to see if upstream skills have changed.\n\n## Related\n\n- `tools/build-agent/add-skill.md` - Detailed conversion logic and merge strategies\n- `scripts/add-skill-helper.sh` - Main import implementation\n- `scripts/clawdhub-helper.sh` - ClawdHub browser-based fetcher (Playwright)\n- `scripts/skill-update-helper.sh` - Automated update checking\n- `scripts/generate-skills.sh` - SKILL.md stub generation for aidevops agents\n",
        ".agent/scripts/commands/code-simplifier.md": "---\ndescription: Simplify and refine code for clarity, consistency, and maintainability\nagent: Build+\nmode: subagent\n---\n\nSimplify and refine code for clarity, consistency, and maintainability while preserving all functionality.\n\nTarget: $ARGUMENTS\n\n## Quick Reference\n\n- **Purpose**: Simplify code without changing functionality\n- **Priority**: Clarity over brevity\n- **Scope**: Recently modified code unless target specified\n\n## Process\n\n1. **Identify scope**: Use $ARGUMENTS if provided, otherwise find recently modified files\n2. **Read the code-simplifier agent**: `tools/code-review/code-simplifier.md`\n3. **Apply refinements** following the agent's principles\n4. **Verify** functionality is preserved\n5. **Report** changes made\n\n## Scope Detection\n\nIf no target specified ($ARGUMENTS is empty):\n\n```bash\n# Find recently modified files (last commit or staged)\ngit diff --name-only HEAD~1\ngit diff --name-only --staged\n```\n\nIf target specified:\n- Directory path: Simplify all code files in directory\n- File path: Simplify specific file\n- `--all`: Review entire codebase (use sparingly)\n\n## Refinement Principles\n\nFrom `tools/code-review/code-simplifier.md`:\n\n1. **Preserve Functionality**: Never change what code does\n2. **Apply Project Standards**: Follow CLAUDE.md/AGENTS.md conventions\n3. **Enhance Clarity**: Reduce complexity, eliminate redundancy\n4. **Maintain Balance**: Avoid over-simplification\n5. **Focus Scope**: Only refine recently modified code unless instructed otherwise\n\n## Key Patterns to Fix\n\n- Nested ternaries -> switch/if-else chains\n- Dense one-liners -> readable multi-line\n- Redundant abstractions -> direct code\n- Obvious comments -> remove\n- Inconsistent naming -> standardize\n\n## Output Format\n\n```text\nCode Simplification Report\n==========================\n\nFiles reviewed: {count}\nChanges made: {count}\n\n{file1}:\n  - Line {n}: {before} -> {after}\n  - Line {m}: Removed redundant {description}\n\n{file2}:\n  - Line {n}: Simplified nested ternary to if/else\n\nFunctionality: Preserved\n```\n\n## Integration\n\nAfter simplification, suggest:\n\n```text\nNext steps:\n1. Run /linters-local to validate\n2. Review changes with git diff\n3. Commit if satisfied\n```\n",
        ".agent/scripts/commands/email-health-check.md": "---\ndescription: Check email deliverability health for a domain (SPF, DKIM, DMARC, MX, blacklists)\nagent: Build+\nmode: subagent\n---\n\nCheck email authentication and deliverability configuration for a domain.\n\nDomain: $ARGUMENTS\n\n## Workflow\n\n### Step 1: Run Health Check\n\n```bash\n~/.aidevops/agents/scripts/email-health-check-helper.sh check \"$ARGUMENTS\"\n```\n\n### Step 2: Present Results\n\nFormat the output as a clear report:\n\n```text\nEmail Health Check: {domain}\n\nSPF:       {status} - {details}\nDKIM:      {status} - {selectors found}\nDMARC:     {status} - {policy}\nMX:        {status} - {record count}\nBlacklist: {status} - {listed/clean}\n\nIssues Found:\n- {issue 1}\n- {issue 2}\n\nRecommendations:\n1. {recommendation 1}\n2. {recommendation 2}\n```\n\n### Step 3: Offer Follow-up Actions\n\n```text\nActions:\n1. Check specific DKIM selector\n2. View detailed blacklist report\n3. Get mail-tester.com instructions\n4. Show DNS records to add/fix\n```\n\n## Options\n\n| Command | Purpose |\n|---------|---------|\n| `/email-health-check example.com` | Full health check |\n| `/email-health-check example.com spf` | SPF only |\n| `/email-health-check example.com dkim google` | DKIM with selector |\n| `/email-health-check example.com dmarc` | DMARC only |\n\n## Examples\n\n**Basic check:**\n\n```text\nUser: /email-health-check example.com\nAI: Running email health check for example.com...\n\n    Email Health Check: example.com\n    \n    SPF:       OK - v=spf1 include:_spf.google.com ~all\n    DKIM:      OK - Found: google, selector1\n    DMARC:     WARN - p=none (monitoring only)\n    MX:        OK - 2 records (redundant)\n    Blacklist: OK - Not listed\n    \n    Recommendations:\n    1. Upgrade DMARC policy from p=none to p=quarantine\n    2. Consider adding rua= for DMARC reports\n    \n    Would you like to:\n    1. See the recommended DMARC record\n    2. Run a mail-tester.com deliverability test\n    3. Check another domain\n```\n\n**DKIM with specific selector:**\n\n```text\nUser: /email-health-check example.com dkim k1\nAI: Checking DKIM for selector 'k1' on example.com...\n\n    DKIM Record Found:\n    Selector: k1._domainkey.example.com\n    Key Type: RSA\n    Status: Valid\n    \n    This is a Mailchimp DKIM selector.\n```\n\n## Related\n\n- `services/email/email-health-check.md` - Full documentation\n- `services/email/ses.md` - Amazon SES integration\n- `services/hosting/dns.md` - DNS management\n",
        ".agent/scripts/commands/full-loop.md": "---\ndescription: Start end-to-end development loop (task → preflight → PR → postflight → deploy)\nagent: Build+\nmode: subagent\n---\n\nStart a full development loop that chains all phases from task implementation to deployment.\n\nTask/Prompt: $ARGUMENTS\n\n## Step 0: Resolve Task ID and Set Session Title\n\n**IMPORTANT**: Before proceeding, extract the first positional argument from `$ARGUMENTS` (ignoring flags like `--max-task-iterations`). Check if it matches the task ID pattern `t\\d+` (e.g., `t061`).\n\nIf the first argument is a task ID (e.g., `t061`):\n\n1. Extract the task ID and look up its description from TODO.md:\n\n   ```bash\n   # Extract first argument (the task ID)\n   TASK_ID=$(echo \"$ARGUMENTS\" | awk '{print $1}')\n   \n   # Look up description (matches open, completed, or declined tasks)\n   TASK_DESC=$(grep -E \"^- \\[( |x|-)\\] $TASK_ID \" TODO.md 2>/dev/null | head -1 | sed -E 's/^- \\[( |x|-)\\] [^ ]* //')\n   ```\n\n2. Set the session title using the `session-rename` MCP tool:\n\n   ```text\n   # Call the session-rename tool with the title parameter\n   session-rename(title: \"t061: Improve session title to include task description\")\n   ```\n\n   - Good: `\"t061: Improve session title to include task description\"`\n   - Bad: `\"Full loop development for t061\"`\n\n3. **Fallback**: If `$TASK_DESC` is empty (task not found in TODO.md), use: `\"t061: (task not found in TODO.md)\"`\n\n4. Store the full task description for use in subsequent steps.\n\nIf the first argument is NOT a task ID (it's a description):\n- Use the description directly for the session title\n- Call `session-rename` tool with a concise version if the description is very long (truncate to ~60 chars)\n\n**Example session titles:**\n- Task ID `t061` with description \"Improve session title format\" → `\"t061: Improve session title format\"`\n- Task ID `t999` not found → `\"t999: (task not found in TODO.md)\"`\n- Description \"Add JWT authentication\" → `\"Add JWT authentication\"`\n\n## Full Loop Phases\n\n```text\nTask Development → Preflight → PR Create → PR Review → Postflight → Deploy\n```\n\n## Workflow\n\n### Step 1: Auto-Branch Setup\n\nThe loop automatically handles branch setup when on main/master:\n\n```bash\n# Run pre-edit check in loop mode with task description\n~/.aidevops/agents/scripts/pre-edit-check.sh --loop-mode --task \"$ARGUMENTS\"\n```\n\n**Exit codes:**\n- `0` - Already on feature branch OR docs-only task (proceed)\n- `1` - Interactive mode fallback (shouldn't happen in loop)\n- `2` - Code task on main (auto-create worktree)\n\n**Auto-decision logic:**\n- **Docs-only tasks** (README, CHANGELOG, docs/, typos): Stay on main\n- **Code tasks** (features, fixes, refactors, enhancements): Auto-create worktree\n\n**Detection keywords:**\n- Docs-only: `readme`, `changelog`, `documentation`, `docs/`, `typo`, `spelling`\n- Code (overrides docs): `feature`, `fix`, `bug`, `implement`, `refactor`, `add`, `update`, `enhance`, `port`, `ssl`\n\n**When worktree is needed:**\n\n```bash\n# Generate branch name from task (sanitized, truncated to 40 chars)\nbranch_name=$(echo \"$ARGUMENTS\" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | cut -c1-40)\n\n# Preferred: Use Worktrunk (wt) if installed\nwt switch -c \"feature/$branch_name\"\n\n# Fallback: Use worktree-helper.sh if wt not available\n~/.aidevops/agents/scripts/worktree-helper.sh add \"feature/$branch_name\"\n# Continue in new worktree directory\n```\n\nAlso verify:\n- **Clean working directory**: Uncommitted changes should be committed or stashed\n- **Git remote configured**: Need to push and create PR\n\n```bash\ngit status --short\n```\n\n### Step 2: Start Full Loop\n\n**Recommended: Background mode** (avoids timeout issues):\n\n```bash\n~/.aidevops/agents/scripts/full-loop-helper.sh start \"$ARGUMENTS\" --background\n```\n\nThis starts the loop in the background and returns immediately. Use these commands to monitor:\n\n```bash\n# Check status\n~/.aidevops/agents/scripts/full-loop-helper.sh status\n\n# View logs\n~/.aidevops/agents/scripts/full-loop-helper.sh logs\n\n# Cancel if needed\n~/.aidevops/agents/scripts/full-loop-helper.sh cancel\n```\n\n**Foreground mode** (may timeout in MCP tools):\n\n```bash\n~/.aidevops/agents/scripts/full-loop-helper.sh start \"$ARGUMENTS\"\n```\n\nThis will:\n1. Initialize the Ralph loop for task development\n2. Set up state tracking in `.agent/loop-state/full-loop.local.md`\n3. Begin iterating on the task\n\n**Note**: Foreground mode may timeout when called via MCP Bash tool (default 120s timeout). Use `--background` for long-running tasks.\n\n### Step 3: Task Development (Ralph Loop)\n\nThe AI will iterate on the task until outputting:\n\n```text\n<promise>TASK_COMPLETE</promise>\n```\n\n**Completion criteria:**\n- All requirements implemented\n- Tests passing (if applicable)\n- Code quality acceptable\n- README.md updated (if adding features/APIs) - use `/readme --sections` for targeted updates\n- Conventional commits used (for auto-changelog)\n\n**README check (aidevops repo only):**\n\n```bash\n# Check if README counts are stale after adding agents/scripts\n~/.aidevops/agents/scripts/readme-helper.sh check\n```\n\n### Step 4: Automatic Phase Progression\n\nAfter task completion, the loop automatically:\n\n1. **Preflight**: Runs quality checks, auto-fixes issues\n2. **PR Create**: Creates pull request with `gh pr create --fill`\n3. **PR Review**: Monitors CI checks and review status\n4. **Merge**: Squash merge (without `--delete-branch` when in worktree)\n5. **Worktree Cleanup**: Return to main repo, pull, clean merged worktrees\n6. **Postflight**: Verifies release health after merge\n7. **Deploy**: Runs `setup.sh` (aidevops repos only)\n\n**Worktree cleanup after merge:**\n\n```bash\n# When in a worktree, merge without --delete-branch\ngh pr merge --squash\n\n# Then clean up from main repo\ncd ~/Git/$(basename \"$PWD\" | cut -d. -f1)  # Return to main repo\ngit pull origin main                        # Get merged changes\nwt prune                                    # Clean merged worktrees\n```\n\n### Step 5: Human Decision Points\n\nThe loop pauses for human input at:\n\n| Point | When | Action Required |\n|-------|------|-----------------|\n| Merge approval | If repo requires human approval | Approve PR in GitHub |\n| Rollback | If postflight detects issues | Decide whether to rollback |\n| Scope change | If task evolves beyond original | Confirm new scope |\n\n### Step 6: Completion\n\nWhen all phases complete:\n\n```text\n<promise>FULL_LOOP_COMPLETE</promise>\n```\n\n## Commands\n\n```bash\n# Start new loop\n/full-loop \"Implement feature X with tests\"\n\n# Check status\n~/.aidevops/agents/scripts/full-loop-helper.sh status\n\n# Resume after interruption\n~/.aidevops/agents/scripts/full-loop-helper.sh resume\n\n# Cancel loop\n~/.aidevops/agents/scripts/full-loop-helper.sh cancel\n```\n\n## Options\n\nPass options after the prompt:\n\n```bash\n/full-loop \"Fix bug Y\" --max-task-iterations 30 --skip-postflight\n```\n\n| Option | Description |\n|--------|-------------|\n| `--background`, `--bg` | Run in background (recommended for long tasks) |\n| `--max-task-iterations N` | Max iterations for task (default: 50) |\n| `--max-preflight-iterations N` | Max iterations for preflight (default: 5) |\n| `--max-pr-iterations N` | Max iterations for PR review (default: 20) |\n| `--skip-preflight` | Skip preflight checks |\n| `--skip-postflight` | Skip postflight monitoring |\n| `--no-auto-pr` | Pause for manual PR creation |\n| `--no-auto-deploy` | Don't auto-run setup.sh |\n\n## Examples\n\n```bash\n# Basic feature implementation (background mode recommended)\n/full-loop \"Add user authentication with JWT tokens\" --background\n\n# Foreground mode (may timeout for long tasks)\n/full-loop \"Add user authentication with JWT tokens\"\n\n# Bug fix with limited iterations\n/full-loop \"Fix memory leak in connection pool\" --max-task-iterations 20 --background\n\n# Skip postflight for quick iteration\n/full-loop \"Update documentation\" --skip-postflight\n\n# Manual PR creation\n/full-loop \"Refactor database layer\" --no-auto-pr --background\n\n# View background loop progress\n~/.aidevops/agents/scripts/full-loop-helper.sh logs\n```\n\n## Documentation & Changelog\n\n### README Updates\n\nWhen implementing features or APIs, include README updates in your task:\n\n```bash\n/full-loop \"Add user authentication with JWT tokens and update README\"\n```\n\nThe task development phase should update README.md with:\n- New feature documentation\n- Usage examples\n- API endpoint descriptions\n- Configuration options\n\n### Changelog (Auto-Generated)\n\nThe release workflow auto-generates CHANGELOG.md from conventional commits. Use proper commit prefixes during task development:\n\n| Prefix | Changelog Section | Example |\n|--------|-------------------|---------|\n| `feat:` | Added | `feat: add JWT authentication` |\n| `fix:` | Fixed | `fix: resolve token expiration bug` |\n| `docs:` | Changed | `docs: update API documentation` |\n| `perf:` | Changed | `perf: optimize database queries` |\n| `refactor:` | Changed | `refactor: simplify auth middleware` |\n| `chore:` | (excluded) | `chore: update dependencies` |\n\nSee `workflows/changelog.md` for format details.\n\n## OpenProse Orchestration\n\nFor complex multi-phase workflows, consider expressing the full loop in OpenProse DSL:\n\n```prose\nagent developer:\n  model: opus\n  prompt: \"You are a senior developer\"\n\n# Phase 1: Task Development\nloop until **task is complete** (max: 50):\n  session: developer\n    prompt: \"Implement the feature, run tests, fix issues\"\n\n# Phase 2: Preflight (parallel quality checks)\nparallel:\n  lint = session \"Run linters and fix issues\"\n  types = session \"Check types and fix issues\"\n  tests = session \"Run tests and fix failures\"\n\nif **any checks failed**:\n  loop until **all checks pass** (max: 5):\n    session \"Fix remaining issues\"\n      context: { lint, types, tests }\n\n# Phase 3: PR Creation\nlet pr = session \"Create pull request with gh pr create --fill\"\n\n# Phase 4: PR Review Loop\nloop until **PR is merged** (max: 20):\n  parallel:\n    ci = session \"Check CI status\"\n    review = session \"Check review status\"\n  \n  if **CI failed**:\n    session \"Fix CI issues and push\"\n  \n  if **changes requested**:\n    session \"Address review feedback and push\"\n\n# Phase 5: Postflight\nsession \"Verify release health\"\n```\n\nSee `tools/ai-orchestration/openprose.md` for full OpenProse documentation.\n\n## Related\n\n- `workflows/ralph-loop.md` - Ralph loop technique details\n- `workflows/preflight.md` - Pre-commit quality checks\n- `workflows/pr.md` - PR creation workflow\n- `workflows/postflight.md` - Post-release verification\n- `workflows/changelog.md` - Changelog format and validation\n- `tools/ai-orchestration/openprose.md` - OpenProse DSL for multi-agent orchestration\n",
        ".agent/scripts/commands/humanise.md": "---\ndescription: Remove AI writing patterns from text to make it sound more natural and human\nagent: Build+\nmode: subagent\n---\n\nRemove signs of AI-generated writing from text, making it sound more natural and human-written.\n\nText to humanise: $ARGUMENTS\n\n## Quick Reference\n\n- **Purpose**: Remove AI writing patterns, add human voice\n- **Source**: Adapted from [blader/humanizer](https://github.com/blader/humanizer)\n- **Based on**: Wikipedia's \"Signs of AI writing\" guide\n\n## Process\n\n1. **Read the humanise subagent**: `content/humanise.md`\n2. **Identify AI patterns** in the provided text\n3. **Rewrite problematic sections** with natural alternatives\n4. **Add voice and personality** - don't just remove patterns\n5. **Present the humanised version** with optional change summary\n\n## Usage\n\n```text\n/humanise [paste text here]\n\n/humanise The new software update serves as a testament to the company's commitment to innovation.\n```\n\nOr provide a file path:\n\n```text\n/humanise path/to/content.md\n```\n\n## Key Patterns to Fix\n\nFrom `content/humanise.md`:\n\n**Content patterns:**\n- Inflated significance (\"pivotal moment\", \"testament to\")\n- Promotional language (\"nestled\", \"vibrant\", \"breathtaking\")\n- Vague attributions (\"experts believe\", \"industry reports\")\n- Superficial -ing analyses (\"highlighting\", \"showcasing\")\n\n**Language patterns:**\n- AI vocabulary (delve, tapestry, landscape, pivotal, crucial)\n- Copula avoidance (\"serves as\" instead of \"is\")\n- Rule of three overuse\n- Synonym cycling\n\n**Style patterns:**\n- Em dash overuse\n- Excessive boldface\n- Title Case In Headings\n- Emojis in professional content\n\n**Communication patterns:**\n- Chatbot artifacts (\"I hope this helps!\")\n- Sycophantic tone (\"Great question!\")\n- Knowledge-cutoff disclaimers\n\n## Output Format\n\n```text\nHumanised Text\n==============\n\n[The rewritten text]\n\n---\n\nChanges made:\n- Removed \"serves as a testament\" (inflated symbolism)\n- Replaced \"Moreover\" with natural transition\n- Simplified rule of three to specific details\n- Added concrete examples instead of vague claims\n```\n\n## Integration with Content Workflow\n\nThe humanise command fits into the content creation workflow:\n\n```text\n1. Draft content\n2. /humanise [content]  <- You are here\n3. /linters-local (if code/markdown)\n4. Publish\n```\n\n## Checking for Updates\n\nThe humanise subagent tracks upstream changes:\n\n```bash\n# Check for updates to the source skill\n~/.aidevops/agents/scripts/humanise-update-helper.sh check\n```\n",
        ".agent/scripts/commands/list-todo.md": "---\ndescription: List tasks from TODO.md with sorting and filtering options\nagent: Build+\nmode: subagent\n---\n\nDisplay tasks from TODO.md and optionally PLANS.md with fast script-based output.\n\nArguments: $ARGUMENTS\n\n## Quick Output (Default)\n\nRun the helper script for instant output:\n\n```bash\n~/.aidevops/agents/scripts/list-todo-helper.sh $ARGUMENTS\n```\n\nDisplay the output directly to the user. The script handles all formatting.\n\n## Fallback (Script Unavailable)\n\nIf the script fails or is unavailable, read and parse the files manually:\n\n1. Read `TODO.md` and `todo/PLANS.md`\n2. Parse tasks by status (In Progress, Backlog, Done)\n3. Apply any filters from arguments\n4. Format as Markdown tables\n\n## Arguments\n\n**Sorting options:**\n- `--priority` or `-p` - Sort by priority (security/bugfix first)\n- `--estimate` or `-e` - Sort by time estimate (shortest first)\n- `--date` or `-d` - Sort by logged date (newest first)\n- `--alpha` or `-a` - Sort alphabetically\n\n**Filtering options:**\n- `--tag <tag>` or `-t <tag>` - Filter by tag (seo, security, etc.)\n- `--owner <name>` or `-o <name>` - Filter by assignee (marcus, etc.)\n- `--status <status>` - Filter by status (pending, in-progress, done)\n- `--estimate-filter <range>` - Filter by estimate (<2h, >1d, 1h-4h)\n\n**Display options:**\n- `--plans` - Include plan details from PLANS.md\n- `--done` - Include completed tasks\n- `--all` - Show everything (pending + done + plans)\n- `--compact` - One-line per task (no tables)\n- `--limit <n>` - Limit results\n- `--json` - Output as JSON\n\n## Examples\n\n```bash\n/list-todo                           # All pending, grouped by status\n/list-todo --priority                # Sorted by priority\n/list-todo -t seo                    # Only #seo tasks\n/list-todo -o marcus -e              # Marcus's tasks, shortest first\n/list-todo --estimate-filter \"<2h\"   # Quick wins under 2 hours\n/list-todo --plans                   # Include plan details\n/list-todo --all --compact           # Everything, one line each\n```\n\n## Output Format\n\nThe script outputs Markdown tables:\n\n```markdown\n## Tasks Overview\n\n### In Progress (N)\n\n| # | ID | Task | Est | Tags | Owner |\n|---|-----|------|-----|------|-------|\n| 1 | t001 | Task description | ~2h | #tag | @owner |\n\n---\n\n### Backlog (N pending)\n\n| # | ID | Task | Est | Tags | Owner | Logged |\n|---|-----|------|-----|------|-------|--------|\n| 1 | t002 | Another task | ~4h | #feature | - | 2025-01-15 |\n\n**Blocked tasks** (N):\n- t003 blocked-by:t001\n\n---\n\n**Summary:** N pending | N in progress | N done | N active plans\n\n---\n\n**Options:**\n1. Work on a specific task (enter task ID like `t014` or row number from `#` column)\n2. Filter/sort differently (e.g., `--priority`, `-t seo`)\n3. Done browsing\n```\n\n## After Display\n\nWait for user input:\n\n1. **Task ID or row number** - Start working on that task (e.g., `t014` or `5`)\n2. **Filter command** - Re-run with new filters (e.g., `-t seo`)\n3. **\"3\" or \"done\"** - End browsing\n\nWhen user selects a task:\n- Check if it's a plan reference (`#plan` tag or `→ PLANS.md`)\n- If plan: suggest `/show-plan <name>`\n- If task: offer to start work (check branch, create if needed)\n\n## Related Commands\n\n- `/show-plan <name>` - Show detailed plan information\n- `/ready` - Show only tasks with no blockers\n- `/save-todo` - Save current discussion as task\n",
        ".agent/scripts/commands/log-issue-aidevops.md": "---\ndescription: Log an issue with aidevops to GitHub for the maintainers to address\nagent: Build+\nmode: subagent\ntools:\n  read: true\n  write: false\n  edit: false\n  bash: true\n  glob: false\n  grep: false\n  webfetch: false\n  task: false\n---\n\nLog an issue with the aidevops framework to GitHub.\n\n**Arguments**: Optional issue title in quotes, e.g., `/log-issue-aidevops \"Update check not working\"`\n\n## Purpose\n\nWhen users encounter problems with aidevops (bugs, unexpected behavior, missing features), this command:\n\n1. Gathers diagnostic information automatically\n2. Helps the user describe the issue clearly\n3. Creates a GitHub issue on `marcusquinn/aidevops`\n4. Provides the issue URL for tracking\n\n## Workflow\n\n### Step 1: Gather Diagnostics\n\nRun the helper script to collect system and aidevops info:\n\n```bash\n~/.aidevops/agents/scripts/log-issue-helper.sh diagnostics\n```\n\nThis collects:\n- aidevops version (local and latest)\n- AI assistant being used\n- OS and shell info\n- Current repo context\n- GitHub CLI version\n\n### Step 2: Understand the Issue\n\nAsk the user to describe:\n\n1. **What happened?** (the problem)\n2. **What did you expect?** (expected behavior)\n3. **Steps to reproduce** (if known)\n\nIf the user provided an argument, use that as the starting point for the title.\n\nReview the current session context:\n- What commands/actions led to the issue?\n- Any error messages displayed?\n- What was the user trying to accomplish?\n\n### Step 3: Check for Duplicates\n\nSearch existing issues:\n\n```bash\ngh issue list -R marcusquinn/aidevops --state all --search \"KEYWORDS\" --limit 10\n```\n\nIf potential duplicates found, show them to the user:\n\n```text\nFound similar issues:\n1. #123 - \"Update check fails on npm install\" (open)\n2. #98 - \"Version mismatch after update\" (closed)\n\nIs your issue related to any of these?\n1. Yes, add comment to existing issue\n2. No, create new issue\n3. Not sure, let me review them first\n```\n\n### Step 4: Compose the Issue\n\nBuild the issue with this structure:\n\n```markdown\n## Description\n\n{User's description of the problem}\n\n## Expected Behavior\n\n{What should have happened}\n\n## Steps to Reproduce\n\n1. {Step 1}\n2. {Step 2}\n3. {Step 3}\n\n## Environment\n\n{Output from diagnostics script}\n\n## Additional Context\n\n{Session context, error messages, screenshots if mentioned}\n```\n\n### Step 5: Confirm Before Submitting\n\nPresent the composed issue to the user:\n\n```text\nReady to create issue on marcusquinn/aidevops:\n\nTitle: {title}\n\nBody:\n---\n{body preview, truncated if long}\n---\n\nLabels: bug (or enhancement, question, documentation)\n\n1. Create issue\n2. Edit title\n3. Edit description\n4. Cancel\n```\n\n### Step 6: Create the Issue\n\n```bash\ngh issue create -R marcusquinn/aidevops \\\n  --title \"TITLE\" \\\n  --body \"$(cat <<'EOF'\nBODY_CONTENT\nEOF\n)\" \\\n  --label \"LABEL\"\n```\n\n### Step 7: Confirm Success\n\n```text\nIssue created successfully!\n\nURL: https://github.com/marcusquinn/aidevops/issues/XXX\n\nThe maintainers will review your issue. You can:\n- Add more details by commenting on the issue\n- Subscribe to notifications for updates\n- Reference this issue in related PRs with \"Fixes #XXX\"\n```\n\n## Label Selection\n\n| Issue Type | Label |\n|------------|-------|\n| Something broken | `bug` |\n| New feature request | `enhancement` |\n| Question/help needed | `question` |\n| Documentation issue | `documentation` |\n| Performance problem | `performance` |\n\n## Examples\n\n```bash\n# Interactive - will prompt for details\n/log-issue-aidevops\n\n# With title hint\n/log-issue-aidevops \"Update check not showing new versions\"\n\n# For feature requests\n/log-issue-aidevops \"Feature: Add support for GitLab\"\n```\n\n## Privacy Notes\n\n- The diagnostic info does NOT include credentials or tokens\n- File paths are included (may reveal username)\n- No file contents are uploaded\n- User can review everything before submission\n\n## Error Handling\n\nIf `gh` is not authenticated:\n\n```text\nGitHub CLI not authenticated. Please run:\n\n    gh auth login\n\nThen try /log-issue-aidevops again.\n```\n\nIf network issues:\n\n```text\nCould not connect to GitHub. Please check your internet connection and try again.\n```\n",
        ".agent/scripts/commands/neuronwriter.md": "---\ndescription: NeuronWriter content optimization - analyze keywords, score content, get NLP recommendations\nagent: Build+\nmode: subagent\n---\n\nOptimize content for SEO using NeuronWriter's NLP analysis API.\n\nArguments: $ARGUMENTS\n\n## Workflow\n\n### Step 1: Parse Arguments\n\nParse `$ARGUMENTS` to determine the action:\n\n```text\n/neuronwriter analyze <keyword> [options]     → Create query + get NLP recommendations\n/neuronwriter score <query-id> <url-or-html>  → Score content against a query\n/neuronwriter import <query-id> <url-or-html> → Import content into NeuronWriter editor\n/neuronwriter get <query-id>                  → Get recommendations for existing query\n/neuronwriter content <query-id>              → Get saved content from editor\n/neuronwriter projects                        → List all projects\n/neuronwriter queries <project-id> [options]  → List queries in a project\n/neuronwriter status                          → Check API key configuration\n```\n\nIf only a keyword is provided (no subcommand), default to `analyze`.\n\n### Step 2: Check API Key\n\n```bash\nsource ~/.config/aidevops/mcp-env.sh\nif [[ -z \"$NEURONWRITER_API_KEY\" ]]; then\n  echo \"NEURONWRITER_API_KEY not configured.\" >&2\n  echo \"Set it with: bash ~/.aidevops/agents/scripts/setup-local-api-keys.sh set NEURONWRITER_API_KEY \\\"your_key\\\"\" >&2\n  echo \"Get your key from: NeuronWriter profile > Neuron API access tab\" >&2\n  echo \"Requires Gold plan or higher.\" >&2\n  exit 1\nfi\n```\n\n### Step 3: Read the NeuronWriter Subagent\n\nRead `seo/neuronwriter.md` for full API reference, then execute the appropriate endpoint.\n\n### Step 4: Execute Action\n\n**For `analyze` (default):**\n\n1. List projects to find the right one (or use `--project <id>` if provided)\n2. Call `/new-query` with the keyword\n3. Poll `/get-query` every 15 seconds until `status == \"ready\"` (max 5 minutes)\n4. Present NLP recommendations in a structured format\n\n**For `score`:**\n\n1. Call `/evaluate-content` with the query ID and URL or HTML\n2. Report the content score\n\n**For `import`:**\n\n1. Call `/import-content` with the query ID and URL or HTML\n2. Report the content score and editor URL\n\n**For `get`:**\n\n1. Call `/get-query` with the query ID\n2. Present recommendations (terms, ideas, competitors)\n\n**For `content`:**\n\n1. Call `/get-content` with the query ID\n2. Return the saved HTML content, title, and description\n\n**For `projects`:**\n\n1. Call `/list-projects`\n2. Display project names, IDs, languages, and engines\n\n**For `queries`:**\n\n1. Call `/list-queries` with the project ID\n2. Optionally filter by `--status`, `--keyword`, `--tag`\n\n### Step 5: Report Results\n\n**For `analyze`, present recommendations as:**\n\n```text\nKeyword: \"trail running shoes\"\nQuery ID: 32dee2a89374a722\nQuery URL: https://app.neuronwriter.com/analysis/view/32dee2a89374a722\n\nNLP Terms (Content):\n  - trail running shoes (use 8-12 times)\n  - running shoe (use 3-5 times)\n  - trail runners (use 2-4 times)\n  ...\n\nSuggested Headings (H2):\n  - Best Trail Running Shoes for 2025\n  - How to Choose Trail Running Shoes\n  ...\n\nContent Ideas:\n  - What makes trail running shoes different from road shoes?\n  - How often should you replace trail running shoes?\n  ...\n\nCompetitors (Top 5):\n  1. runnersworld.com - Score: 87\n  2. rei.com - Score: 82\n  ...\n\nWord Count Target: 2,500-3,000\n```\n\n**For `score` / `import`:**\n\n```text\nContent Score: 72/100\nQuery: 32dee2a89374a722\n```\n\n## Options\n\n| Option | Description |\n|--------|-------------|\n| `--project <id>` | Project ID (skips project selection) |\n| `--engine <engine>` | Search engine (default: `google.com`) |\n| `--language <lang>` | Content language (default: `English`) |\n\n## Examples\n\n**Analyze a keyword:**\n\n```text\n/neuronwriter trail running shoes\n```\n\n**Analyze with specific project and engine:**\n\n```text\n/neuronwriter analyze \"best running gear\" --project ed0b47151fb35b02 --engine google.co.uk\n```\n\n**Score a URL against an existing query:**\n\n```text\n/neuronwriter score 32dee2a89374a722 https://example.com/running-shoes\n```\n\n**Import content into the editor:**\n\n```text\n/neuronwriter import 32dee2a89374a722 https://example.com/running-shoes\n```\n\n**Get recommendations for an existing query:**\n\n```text\n/neuronwriter get 32dee2a89374a722\n```\n\n**List all projects:**\n\n```text\n/neuronwriter projects\n```\n\n**List ready queries in a project:**\n\n```text\n/neuronwriter queries ed0b47151fb35b02 --status ready\n```\n\n**Check API key status:**\n\n```text\n/neuronwriter status\n```\n\n## Related\n\n- `seo/neuronwriter.md` - Full API reference and authentication details\n- NeuronWriter requires Gold plan or higher for API access\n- API requests consume the same monthly limits as the NeuronWriter UI\n",
        ".agent/scripts/commands/performance.md": "---\ndescription: Run comprehensive web performance analysis (Core Web Vitals, network, accessibility)\nagent: Build+\nmode: subagent\n---\n\nAnalyze web performance for the specified URL using Chrome DevTools MCP.\n\nURL/Target: $ARGUMENTS\n\n## Workflow\n\n### Step 1: Parse Arguments\n\n```text\nDefault: Full audit (performance + accessibility + network)\nOptions:\n  --categories=performance,accessibility,network  Specific categories\n  --device=mobile|desktop         Device emulation (default: mobile)\n  --iterations=N                  Number of runs for averaging (default: 3)\n  --compare=baseline.json         Compare against baseline\n  --local                         Assume localhost URL\n```\n\n### Step 2: Check Prerequisites\n\n```bash\n# Verify Chrome DevTools MCP is available\nwhich npx && npx chrome-devtools-mcp@latest --version || echo \"Install: npm i -g chrome-devtools-mcp\"\n```\n\n### Step 3: Read Performance Subagent\n\nRead `~/.aidevops/agents/tools/performance/performance.md` for:\n- Core Web Vitals thresholds\n- Common issues and fixes\n- Actionable output format\n\n### Step 4: Run Analysis\n\nUsing Chrome DevTools MCP:\n\n1. **Lighthouse Audit** - Performance, accessibility, best practices, SEO scores\n2. **Core Web Vitals** - FCP, LCP, CLS, FID, TTFB measurements\n3. **Network Analysis** - Third-party scripts, request chains, bundle sizes\n4. **Accessibility Check** - WCAG compliance issues\n\n### Step 5: Generate Report\n\nOutput in actionable format:\n\n```markdown\n## Performance Report: [URL]\n\n### Core Web Vitals\n| Metric | Value | Status | Target |\n|--------|-------|--------|--------|\n| LCP | X.Xs | GOOD/NEEDS WORK/POOR | <2.5s |\n| FID | Xms | GOOD/NEEDS WORK/POOR | <100ms |\n| CLS | X.XX | GOOD/NEEDS WORK/POOR | <0.1 |\n| TTFB | Xms | GOOD/NEEDS WORK/POOR | <800ms |\n\n### Top Issues (Priority Order)\n1. **Issue** - Description\n   - File: `path/to/file:line`\n   - Fix: Specific recommendation\n\n### Network Dependencies\n- X third-party scripts\n- Longest chain: X requests\n- Total blocking time: Xms\n\n### Accessibility\n- Score: X/100\n- X issues found\n```\n\n### Step 6: Provide Fixes\n\nFor each issue, provide:\n1. **What**: The specific problem\n2. **Where**: File path and line number (if in repo)\n3. **How**: Code snippet or configuration change\n4. **Impact**: Expected improvement\n\n## Examples\n\n```bash\n# Full audit of production site\n/performance https://example.com\n\n# Local dev server\n/performance http://localhost:3000 --local\n\n# Mobile-specific audit\n/performance https://example.com --device=mobile\n\n# Compare against baseline\n/performance https://example.com --compare=baseline.json\n\n# Specific categories only\n/performance https://example.com --categories=performance,accessibility\n```\n\n## Related\n\n- `tools/performance/performance.md` - Full performance subagent\n- `tools/browser/pagespeed.md` - PageSpeed Insights integration\n- `tools/browser/chrome-devtools.md` - Chrome DevTools MCP\n",
        ".agent/scripts/commands/postflight-loop.md": "---\ndescription: Monitor release health for a specified duration\nagent: Build+\nmode: subagent\n---\n\nMonitor release health after deployment using iterative checks.\n\nArguments: $ARGUMENTS\n\n## Usage\n\n```bash\n/postflight-loop [--monitor-duration Nm] [--max-iterations N]\n```\n\n## Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--monitor-duration <t>` | How long to monitor (e.g., 5m, 10m, 1h) | 5m |\n| `--max-iterations <n>` | Max checks during monitoring | 5 |\n\n## Workflow\n\n### Step 1: Parse Arguments\n\nExtract from $ARGUMENTS:\n- `monitor_duration` - Duration string (e.g., \"10m\", \"1h\")\n- `max_iterations` - Number of check iterations\n\n### Step 2: Run Postflight Loop\n\nExecute the quality loop helper:\n\n```bash\n~/.aidevops/agents/scripts/quality-loop-helper.sh postflight $ARGUMENTS\n```\n\n### Step 3: Report Results\n\nThe script performs these checks each iteration:\n\n1. **CI Workflow Status** - Latest GitHub Actions workflow state\n2. **Release Tag Exists** - Verify the release tag was created\n3. **Version Consistency** - VERSION file matches release tag\n\n## Completion Promise\n\nWhen all checks pass: `<promise>RELEASE_HEALTHY</promise>`\n\n## Examples\n\n**Monitor for 10 minutes:**\n\n```bash\n/postflight-loop --monitor-duration 10m\n```\n\n**Extended monitoring with more checks:**\n\n```bash\n/postflight-loop --monitor-duration 1h --max-iterations 10\n```\n\n**Quick verification:**\n\n```bash\n/postflight-loop --monitor-duration 2m --max-iterations 3\n```\n\n## State Tracking\n\nProgress is tracked in `.agent/loop-state/quality-loop.local.md`:\n\n```markdown\n## Postflight Loop State\n\n- **Status:** monitoring\n- **Iteration:** 3/5\n- **Elapsed:** 180s/600s\n- **Last Check:** 2025-01-11T14:30:00Z\n\n### Check Results\n- [x] CI workflow: passing\n- [x] Release tag: v2.44.0 exists\n- [x] Version consistency: matched\n```\n\n## When to Use\n\n- After running `/release` to verify deployment health\n- After manual releases to confirm everything is working\n- As part of CI/CD pipeline verification\n\n## Related Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/preflight` | Quality checks before release |\n| `/release` | Full release workflow |\n| `/postflight` | Single postflight check (no loop) |\n| `/preflight-loop` | Iterative preflight until passing |\n",
        ".agent/scripts/commands/pr-loop.md": "---\ndescription: Iterate on PR until approved or merged\nagent: Build+\nmode: subagent\n---\n\nMonitor and iterate on a PR until it is approved or merged.\n\nArguments: $ARGUMENTS\n\n## Usage\n\n```bash\n/pr-loop [--pr N] [--wait-for-ci] [--max-iterations N] [--no-auto-trigger]\n```\n\n## Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--pr <n>` | PR number (auto-detects from current branch if omitted) | auto |\n| `--wait-for-ci` | Wait for CI checks to complete before checking review status | false |\n| `--max-iterations <n>` | Max check iterations | 10 |\n| `--no-auto-trigger` | Disable automatic re-review trigger for stale reviews | false |\n\n## Workflow\n\n### Step 1: Parse Arguments\n\nExtract from $ARGUMENTS:\n- `pr_number` - PR number (or auto-detect from branch)\n- `wait_for_ci` - Whether to wait for CI before checking reviews\n- `max_iterations` - Maximum iterations before giving up\n- `auto_trigger` - Whether to trigger re-review if stale\n\n### Step 2: Run PR Review Loop\n\nExecute the quality loop helper:\n\n```bash\n~/.aidevops/agents/scripts/quality-loop-helper.sh pr-review $ARGUMENTS\n```\n\n### Step 3: Monitor and Iterate\n\nThe script performs these checks each iteration:\n\n1. **CI Status** - Check all GitHub Actions workflows\n2. **Review Status** - Check for approvals or change requests\n3. **Merge Readiness** - Verify PR can be merged\n\nIf issues are found:\n- CI failures: Report and wait for fixes\n- Changes requested: Report feedback for addressing\n- Stale review: Auto-trigger re-review (unless `--no-auto-trigger`)\n\n## Completion Promises\n\n| Outcome | Promise |\n|---------|---------|\n| PR approved | `<promise>PR_APPROVED</promise>` |\n| PR merged | `<promise>PR_MERGED</promise>` |\n| Max iterations reached | Exit with status report |\n\n## Intelligent Timing\n\nThe loop uses evidence-based timing for different CI services:\n\n| Service Category | Initial Wait | Poll Interval |\n|------------------|--------------|---------------|\n| Fast (CodeFactor, Version) | 10s | 5s |\n| Medium (SonarCloud, Codacy, Qlty) | 60s | 15s |\n| Slow (CodeRabbit) | 120s | 30s |\n\n## Examples\n\n**Monitor current branch's PR:**\n\n```bash\n/pr-loop\n```\n\n**Monitor specific PR with CI wait:**\n\n```bash\n/pr-loop --pr 123 --wait-for-ci\n```\n\n**Extended monitoring:**\n\n```bash\n/pr-loop --pr 123 --max-iterations 20\n```\n\n**Disable auto re-review trigger:**\n\n```bash\n/pr-loop --no-auto-trigger\n```\n\n## State Tracking\n\nProgress is tracked in `.agent/loop-state/quality-loop.local.state`:\n\n```markdown\n## PR Review Loop State\n\n- **Status:** monitoring\n- **PR:** #123\n- **Iteration:** 3/10\n- **Last Check:** <timestamp>\n\n### Check Results\n- [x] CI: all checks passing\n- [ ] Review: awaiting approval\n- [ ] Merge: blocked (needs approval)\n```\n\n## When to Use\n\n- After creating a PR to monitor until merge\n- When waiting for CI checks and reviews\n- As part of `/full-loop` workflow (automatic)\n\n## Timeout Recovery\n\nIf the loop times out before completion:\n\n1. **Check current status:**\n\n   ```bash\n   gh pr view --json state,reviewDecision,statusCheckRollup\n   ```\n\n2. **Review what's pending** - usually one of:\n   - CI checks still running (wait and re-check)\n   - Review requested but not completed (ping reviewer)\n   - Failing checks that need manual intervention\n\n3. **Fix and continue:**\n\n   ```bash\n   # Re-run single review cycle\n   /pr review\n   \n   # Or restart loop\n   /pr-loop\n   ```\n\n## Related Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/pr review` | Single PR review (no loop) |\n| `/pr create` | Create PR with pre-checks |\n| `/preflight-loop` | Iterative preflight until passing |\n| `/postflight-loop` | Monitor release health |\n| `/full-loop` | Complete development cycle |\n",
        ".agent/scripts/commands/readme.md": "---\ndescription: Create or update README.md for the current project\nagent: Build+\nmode: subagent\n---\n\nCreate or update a comprehensive README.md file for the current project.\n\n**Arguments**: Optional flags like `--sections \"installation,usage\"` for partial updates. Without arguments, generates/updates the full README.\n\n## Workflow\n\n### Step 1: Parse Arguments\n\nCheck for `--sections` flag:\n\n```bash\n# Full README (default)\n/readme\n\n# Partial update\n/readme --sections \"installation,usage\"\n/readme --sections \"troubleshooting\"\n```\n\n**When to use `--sections`**:\n- After adding a feature → `--sections \"usage\"`\n- After changing install process → `--sections \"installation\"`\n- After discovering common issue → `--sections \"troubleshooting\"`\n- When full regeneration would lose custom content\n\n**When to use full `/readme`**:\n- New project without README\n- README is significantly outdated\n- Major restructuring needed\n- User explicitly requests full regeneration\n\n### Step 2: Load Workflow\n\nRead the full workflow guidance:\n\n```text\nRead: workflows/readme-create-update.md\n```\n\n### Step 3: Explore Codebase\n\nBefore writing anything:\n\n1. **Detect project type** (package.json, Cargo.toml, go.mod, etc.)\n2. **Detect deployment platform** (Dockerfile, fly.toml, vercel.json, etc.)\n3. **Read existing README** (if updating)\n4. **Gather key info** (scripts, entry points, config files)\n\n### Step 4: Generate/Update README\n\n**For new README**: Follow recommended section order from workflow.\n\n**For updates with `--sections`**:\n1. Read entire existing README\n2. Preserve structure and custom content\n3. Update only specified sections\n4. Maintain consistent style\n\n### Step 5: Confirm\n\nPresent the changes and ask for confirmation before writing:\n\n```text\nREADME changes:\n- [Section]: [Brief description of change]\n- [Section]: [Brief description of change]\n\n1. Apply changes\n2. Show full diff first\n3. Modify before applying\n```\n\n## Section Mapping\n\n| Argument | Sections Updated |\n|----------|------------------|\n| `installation` | Installation, Prerequisites, Quick Start |\n| `usage` | Usage, Commands, Examples, API |\n| `config` | Configuration, Environment Variables |\n| `architecture` | Architecture, Project Structure |\n| `troubleshooting` | Troubleshooting |\n| `deployment` | Deployment, Production Setup |\n| `badges` | Badge section only |\n| `all` | Full regeneration (same as no flag) |\n\nMultiple sections: `--sections \"installation,usage,config\"`\n\n## Examples\n\n```bash\n# New project - create full README\n/readme\n\n# Added new CLI commands\n/readme --sections \"usage\"\n\n# Changed environment variables\n/readme --sections \"config\"\n\n# Added Docker support\n/readme --sections \"installation,deployment\"\n\n# Fixed common user issue\n/readme --sections \"troubleshooting\"\n\n# Major update needed\n/readme --sections \"all\"\n```\n\n## Dynamic Counts (aidevops repo)\n\nWhen working in the aidevops repository, use the helper script to manage counts:\n\n```bash\n# Check if README counts are stale\n~/.aidevops/agents/scripts/readme-helper.sh check\n\n# Preview count updates\n~/.aidevops/agents/scripts/readme-helper.sh update\n\n# Apply count updates\n~/.aidevops/agents/scripts/readme-helper.sh update --apply\n```\n\n## Related\n\n- `workflows/readme-create-update.md` - Full workflow guidance\n- `workflows/changelog.md` - Changelog updates\n- `workflows/wiki-update.md` - Wiki documentation\n- `scripts/readme-helper.sh` - Dynamic count management\n",
        ".agent/scripts/commands/recall.md": "---\ndescription: Search and retrieve memories from previous sessions\nagent: Build+\nmode: subagent\n---\n\nSearch stored memories for relevant knowledge.\n\nSearch query: $ARGUMENTS\n\n## Workflow\n\n### Step 1: Search Memories\n\nRun the search:\n\n```bash\n~/.aidevops/agents/scripts/memory-helper.sh recall \"{query}\"\n```\n\n### Step 2: Present Results\n\nIf results found:\n\n```text\nFound {count} memories for \"{query}\":\n\n1. [{type}] {content}\n   Tags: {tags} | Project: {project} | {age}\n   \n2. [{type}] {content}\n   Tags: {tags} | Project: {project} | {age}\n\n---\nActions:\n- Use memory #1 in current context\n- Search with different terms\n- Show more results\n```\n\nIf no results:\n\n```text\nNo memories found for \"{query}\"\n\nTry:\n- Different keywords\n- Broader search terms\n- /recall --recent (show latest memories)\n```\n\n### Step 3: Apply Memory\n\nWhen user selects a memory, incorporate it into the current context:\n\n```text\nApplying memory: \"{content}\"\n\nThis suggests: {interpretation for current task}\n```\n\n## Search Options\n\n| Command | Purpose |\n|---------|---------|\n| `/recall {query}` | Search by keywords |\n| `/recall --type WORKING_SOLUTION` | Filter by type |\n| `/recall --project myapp` | Filter by project |\n| `/recall --recent` | Show 10 most recent |\n| `/recall --stats` | Show memory statistics |\n\n## Examples\n\n**Basic search:**\n\n```text\nUser: /recall cors\nAI: Found 2 memories for \"cors\":\n\n    1. [WORKING_SOLUTION] Fixed CORS by adding Access-Control-Allow-Origin header to nginx.conf\n       Tags: cors,nginx,headers | Project: api-gateway | 3 days ago\n       \n    2. [FAILED_APPROACH] Setting CORS in Express middleware didn't work with nginx proxy\n       Tags: cors,express,nginx | Project: api-gateway | 3 days ago\n\n    Which memory is relevant to your current task?\n```\n\n**Type-filtered search:**\n\n```text\nUser: /recall --type USER_PREFERENCE\nAI: Found 5 user preferences:\n\n    1. [USER_PREFERENCE] Use conventional commit format\n       Tags: git,commits | Global | 1 week ago\n       \n    2. [USER_PREFERENCE] Prefer tabs over spaces (width: 2)\n       Tags: formatting,style | Global | 2 weeks ago\n    ...\n```\n\n**Project-specific search:**\n\n```text\nUser: /recall --project wordpress-plugin\nAI: Found 8 memories for project \"wordpress-plugin\":\n\n    1. [CODEBASE_PATTERN] All hooks use prefix 'myplugin_'\n    2. [DECISION] Using SCF instead of ACF for custom fields\n    3. [WORKING_SOLUTION] Fixed activation hook by checking PHP version first\n    ...\n```\n\n**Recent memories:**\n\n```text\nUser: /recall --recent\nAI: 10 most recent memories:\n\n    1. [WORKING_SOLUTION] Memory system uses SQLite FTS5 (today)\n    2. [DECISION] Chose SQLite over Postgres for zero dependencies (today)\n    3. [TOOL_CONFIG] ShellCheck requires local var pattern (yesterday)\n    ...\n```\n\n## Memory Statistics\n\n```text\nUser: /recall --stats\nAI: Memory Statistics:\n    \n    Total entries: 47\n    By type:\n      WORKING_SOLUTION: 15\n      CODEBASE_PATTERN: 12\n      USER_PREFERENCE: 8\n      FAILED_APPROACH: 6\n      DECISION: 4\n      TOOL_CONFIG: 2\n    \n    By project:\n      global: 20\n      api-gateway: 12\n      wordpress-plugin: 8\n      aidevops: 7\n    \n    Oldest: 45 days ago\n    Most accessed: \"conventional commits\" (12 accesses)\n```\n\n## Proactive Recall\n\nAI assistants should proactively search memories when:\n\n1. Starting work on a project (check project-specific memories)\n2. Encountering an error (search for similar issues)\n3. Making architecture decisions (check past decisions)\n4. Setting up tools (check TOOL_CONFIG memories)\n\n```text\nAI: Before we start, let me check for relevant memories...\n    [Searches: /recall --project {current-project}]\n    \n    Found 3 relevant memories:\n    - This project uses conventional commits\n    - API routes follow /api/v1/{resource} pattern\n    - Tests require DATABASE_URL env var\n```\n\n## Memory Maintenance\n\nMemories track access patterns. Stale memories (>90 days, never accessed) can be pruned:\n\n```bash\n# Validate memory health\n~/.aidevops/agents/scripts/memory-helper.sh validate\n\n# Prune stale entries (dry-run first)\n~/.aidevops/agents/scripts/memory-helper.sh prune --dry-run\n~/.aidevops/agents/scripts/memory-helper.sh prune\n```\n",
        ".agent/scripts/commands/remember.md": "---\ndescription: Store a memory entry for cross-session recall\nagent: Build+\nmode: subagent\n---\n\nStore knowledge, patterns, or learnings for future sessions.\n\nContent to remember: $ARGUMENTS\n\n## Memory Types\n\n| Type | Use For | Example |\n|------|---------|---------|\n| `WORKING_SOLUTION` | Fixes that worked | \"Fixed CORS by adding headers to nginx\" |\n| `FAILED_APPROACH` | What didn't work (avoid repeating) | \"Don't use sync fs in Lambda\" |\n| `CODEBASE_PATTERN` | Project conventions | \"All API routes use /api/v1 prefix\" |\n| `USER_PREFERENCE` | Developer preferences | \"Prefers tabs over spaces\" |\n| `TOOL_CONFIG` | Tool setup notes | \"SonarCloud needs SONAR_TOKEN in CI\" |\n| `DECISION` | Architecture decisions | \"Chose SQLite over Postgres for simplicity\" |\n| `CONTEXT` | Background info | \"Legacy API deprecated in Q3\" |\n\n## Workflow\n\n### Step 1: Analyze Content\n\nFrom the conversation or $ARGUMENTS, extract:\n- **Content**: What to remember (concise, actionable)\n- **Type**: Best matching type from above\n- **Tags**: Relevant keywords (comma-separated)\n- **Project**: Current project name (optional)\n\n### Step 2: Confirm with User\n\n```text\nStoring memory:\n\nType: {type}\nContent: \"{content}\"\nTags: {tags}\nProject: {project or \"global\"}\n\n1. Confirm\n2. Change type\n3. Edit content\n4. Cancel\n```\n\n### Step 3: Store Memory\n\nAfter confirmation, run:\n\n```bash\n~/.aidevops/agents/scripts/memory-helper.sh store --type \"{type}\" --content \"{content}\" --tags \"{tags}\" --project \"{project}\"\n```\n\n### Step 4: Confirm Storage\n\n```text\nRemembered: \"{content}\" ({type})\nTags: {tags}\n\nRecall anytime with: /recall {keyword}\n```\n\n## Examples\n\n**Storing a working solution:**\n\n```text\nUser: /remember The CORS fix was adding Access-Control-Allow-Origin to nginx.conf\nAI: Storing memory:\n    Type: WORKING_SOLUTION\n    Content: \"Fixed CORS by adding Access-Control-Allow-Origin header to nginx.conf location block\"\n    Tags: cors,nginx,headers\n    Project: api-gateway\n    \n    1. Confirm  2. Change type  3. Edit  4. Cancel\nUser: 1\nAI: Remembered: \"Fixed CORS by adding...\" (WORKING_SOLUTION)\n    Recall anytime with: /recall cors\n```\n\n**Storing a failed approach:**\n\n```text\nUser: /remember Don't use moment.js - it's huge and deprecated\nAI: Storing memory:\n    Type: FAILED_APPROACH\n    Content: \"Avoid moment.js - large bundle size (300KB+) and deprecated. Use date-fns or dayjs instead\"\n    Tags: javascript,dates,bundle-size\n    Project: global\n    \n    1. Confirm  2. Change type  3. Edit  4. Cancel\nUser: 1\nAI: Remembered: \"Avoid moment.js...\" (FAILED_APPROACH)\n```\n\n**Storing a user preference:**\n\n```text\nUser: /remember I prefer conventional commits\nAI: Storing memory:\n    Type: USER_PREFERENCE\n    Content: \"Use conventional commit format: type(scope): description\"\n    Tags: git,commits,style\n    Project: global\n    \n    1. Confirm  2. Change type  3. Edit  4. Cancel\n```\n\n## Auto-Remember Triggers\n\nAI assistants should proactively suggest `/remember` when:\n\n1. A solution is found after debugging\n2. User explicitly states a preference\n3. A workaround is discovered for a tool limitation\n4. An architecture decision is made\n5. A failed approach is identified\n\n```text\nAI: That fixed it! Want me to remember this solution for future sessions?\n    /remember {suggested content}\n```\n\n## Storage Location\n\nMemories are stored in SQLite with FTS5 for fast search:\n`~/.aidevops/.agent-workspace/memory/memory.db`\n\nView stats: `~/.aidevops/agents/scripts/memory-helper.sh stats`\n",
        ".agent/scripts/commands/save-todo.md": "---\ndescription: Save current discussion as task or plan (auto-detects complexity)\nagent: Build+\nmode: subagent\n---\n\nAnalyze the current conversation and save appropriately based on complexity.\n\nTopic/context: $ARGUMENTS\n\n## Auto-Detection Logic\n\nAnalyze the conversation for complexity signals:\n\n| Signal | Indicates | Action |\n|--------|-----------|--------|\n| Single action item | Simple | TODO.md only |\n| < 2 hour estimate | Simple | TODO.md only |\n| User says \"quick\" or \"simple\" | Simple | TODO.md only |\n| Multiple distinct steps | Complex | PLANS.md + TODO.md |\n| Research/design needed | Complex | PLANS.md + TODO.md |\n| > 2 hour estimate | Complex | PLANS.md + TODO.md |\n| Multi-session work | Complex | PLANS.md + TODO.md |\n| PRD mentioned or needed | Complex | PLANS.md + TODO.md + PRD |\n\n## Workflow\n\n### Step 1: Analyze Conversation\n\nExtract from the discussion:\n- **Title**: Concise task/plan name\n- **Description**: What needs to be done\n- **Estimate**: Time estimate with breakdown `~Xh (ai:Xh test:Xh read:Xm)`\n- **Tags**: Relevant categories (#seo, #security, #feature, etc.)\n- **Context**: Key decisions, research findings, constraints discussed\n\n### Step 2: Determine Complexity\n\nBased on signals above, classify as Simple or Complex.\n\n### Step 3: Save with Confirmation\n\n**For Simple tasks** (TODO.md only):\n\n```text\nSaving to TODO.md: \"{title}\" ~{estimate}\n\n1. Confirm\n2. Add more details first\n3. Create full plan instead (PLANS.md)\n```\n\nAfter confirmation, add to TODO.md Backlog:\n\n```markdown\n- [ ] {title} #{tag} ~{estimate} logged:{YYYY-MM-DD}\n```\n\nRespond:\n\n```text\nSaved: \"{title}\" to TODO.md (~{estimate})\nStart anytime with: \"Let's work on {title}\"\n```\n\n**For Complex work** (PLANS.md + TODO.md):\n\n```text\nThis looks like complex work. Creating execution plan.\n\nTitle: {title}\nEstimate: ~{estimate}\nPhases: {count} identified\n\n1. Confirm and create plan\n2. Simplify to TODO.md only\n3. Add more context first\n```\n\nAfter confirmation:\n\n1. Create entry in `todo/PLANS.md`:\n\n```markdown\n### [{YYYY-MM-DD}] {Title}\n\n**Status:** Planning\n**Estimate:** ~{estimate}\n**PRD:** [todo/tasks/prd-{slug}.md](tasks/prd-{slug}.md) (if needed)\n**Tasks:** [todo/tasks/tasks-{slug}.md](tasks/tasks-{slug}.md) (if needed)\n\n#### Purpose\n\n{Why this work matters - from conversation context}\n\n#### Progress\n\n- [ ] ({timestamp}) Phase 1: {description} ~{est}\n- [ ] ({timestamp}) Phase 2: {description} ~{est}\n\n#### Context from Discussion\n\n{Key decisions, research findings, constraints from conversation}\n\n#### Decision Log\n\n(To be populated during implementation)\n\n#### Surprises & Discoveries\n\n(To be populated during implementation)\n```\n\n2. Add reference to TODO.md Backlog:\n\n```markdown\n- [ ] {title} #plan → [todo/PLANS.md#{slug}] ~{estimate} logged:{YYYY-MM-DD}\n```\n\n3. Optionally create PRD/tasks files if scope warrants\n\nRespond:\n\n```text\nSaved: \"{title}\"\n- Plan: todo/PLANS.md\n- Reference: TODO.md\n{- PRD: todo/tasks/prd-{slug}.md (if created)}\n{- Tasks: todo/tasks/tasks-{slug}.md (if created)}\n\nStart anytime with: \"Let's work on {title}\"\n```\n\n## Context Preservation\n\nAlways capture from the conversation:\n- Decisions made and their rationale\n- Research findings\n- Constraints identified\n- Open questions\n- Related links or references mentioned\n\nThis context goes into the PLANS.md entry under \"Context from Discussion\" so future sessions have full context.\n\n## Examples\n\n**Simple task:**\n\n```text\nUser: We discussed adding a CSV export button\nAI: Saving to TODO.md: \"Add CSV export button\" ~2h (ai:1.5h test:30m)\n    1. Confirm  2. More details  3. Full plan\nUser: 1\nAI: Saved: \"Add CSV export button\" to TODO.md (~2h)\n    Start anytime with: \"Let's work on CSV export\"\n```\n\n**Complex work:**\n\n```text\nUser: We discussed the authentication overhaul with OAuth, session management, and migration\nAI: This looks like complex work. Creating execution plan.\n    Title: Authentication Overhaul\n    Estimate: ~2w (ai:1w test:0.5w read:0.5w)\n    Phases: 4 identified (OAuth, sessions, migration, testing)\n    \n    1. Confirm and create plan  2. Simplify to TODO.md  3. Add context\nUser: 1\nAI: Saved: \"Authentication Overhaul\"\n    - Plan: todo/PLANS.md\n    - Reference: TODO.md\n    - PRD: todo/tasks/prd-auth-overhaul.md\n    \n    Start anytime with: \"Let's work on auth overhaul\"\n```\n",
        ".agent/scripts/commands/seo-analyze.md": "---\ndescription: Analyze exported SEO data for ranking opportunities\nagent: SEO\nmode: subagent\n---\n\nAnalyze exported SEO data for ranking opportunities, content cannibalization, and optimization targets.\n\nTarget: $ARGUMENTS\n\n## Quick Reference\n\n- **Purpose**: Find actionable SEO opportunities\n- **Input**: TOON files from `/seo-export`\n- **Output**: Analysis report in TOON format\n\n## Usage\n\n```bash\n# Full analysis\n/seo-analyze example.com\n\n# Specific analyses\n/seo-analyze example.com quick-wins\n/seo-analyze example.com striking-distance\n/seo-analyze example.com low-ctr\n/seo-analyze example.com cannibalization\n\n# View data summary\n/seo-analyze example.com summary\n```\n\n## Process\n\n1. Parse $ARGUMENTS to extract domain and analysis type\n2. Run the analysis script:\n\n```bash\n~/.aidevops/agents/scripts/seo-analysis-helper.sh $ARGUMENTS\n```\n\n3. Present results with actionable recommendations\n\n## Analysis Types\n\n| Type | Criteria | Action |\n|------|----------|--------|\n| Quick Wins | Position 4-20, high impressions | On-page optimization |\n| Striking Distance | Position 11-30, high volume | Content expansion, backlinks |\n| Low CTR | CTR < 2%, high impressions | Title/meta optimization |\n| Cannibalization | Same query, multiple URLs | Consolidate content |\n\n## Output\n\nResults are saved to:\n\n```text\n~/.aidevops/.agent-workspace/work/seo-data/{domain}/analysis-{date}.toon\n```\n\n## Prerequisites\n\nRequires exported data. If no data exists, suggest:\n\n```bash\n/seo-export all example.com --days 90\n```\n\n## Documentation\n\nFor full documentation, read `seo/ranking-opportunities.md`.\n",
        ".agent/scripts/commands/seo-audit.md": "---\ndescription: Run comprehensive SEO audit (technical, on-page, content quality, E-E-A-T)\nagent: Build+\nmode: subagent\n---\n\nRun a comprehensive SEO audit for the specified URL or domain.\n\nURL/Target: $ARGUMENTS\n\n## Workflow\n\n### Step 1: Parse Arguments\n\n```text\nDefault: Full audit (technical + on-page + content)\nOptions:\n  --scope=full|technical|on-page|content  Audit scope (default: full)\n  --pages=N                               Max pages to analyze (default: 10)\n  --gsc                                   Include Search Console data if available\n  --compare=competitor.com                Compare against competitor\n  --output=report.md                      Save report to file\n```\n\n### Step 2: Read SEO Audit Subagent\n\nRead `~/.aidevops/agents/seo/seo-audit-skill.md` for:\n- Complete audit framework and priority order\n- Technical SEO checklist (crawlability, indexation, Core Web Vitals)\n- On-page optimization checklist (titles, meta, headings, content)\n- Content quality assessment (E-E-A-T signals)\n- Common issues by site type\n\nAlso read reference files:\n- `~/.aidevops/agents/seo/seo-audit-skill/references/ai-writing-detection.md`\n- `~/.aidevops/agents/seo/seo-audit-skill/references/aeo-geo-patterns.md`\n\n### Step 3: Gather Data\n\n**Technical checks** (using browser automation or curl):\n\n```bash\n# Check robots.txt\ncurl -s \"https://$DOMAIN/robots.txt\"\n\n# Check sitemap\ncurl -s \"https://$DOMAIN/sitemap.xml\" | head -50\n\n# Check meta tags on homepage\ncurl -s \"https://$DOMAIN\" | grep -E '<(title|meta)' | head -20\n```\n\n**If --gsc flag provided**, use Google Search Console data:\n\n```bash\n# Export GSC data for the domain\n~/.aidevops/agents/scripts/seo-export-gsc.sh \"$DOMAIN\"\n```\n\n**For deeper analysis**, use browser automation to:\n- Check Core Web Vitals via PageSpeed Insights\n- Validate structured data via Rich Results Test\n- Check mobile-friendliness\n- Analyze internal linking\n\n### Step 4: Run Audit\n\nFollow the priority order from seo-audit-skill.md:\n\n1. **Crawlability & Indexation**\n   - robots.txt analysis\n   - Sitemap validation\n   - Index status (site:domain.com)\n   - Canonical tag check\n\n2. **Technical Foundations**\n   - HTTPS check\n   - Core Web Vitals (via /performance or PageSpeed)\n   - Mobile-friendliness\n   - URL structure\n\n3. **On-Page Optimization**\n   - Title tags (unique, 50-60 chars, keyword placement)\n   - Meta descriptions (unique, 150-160 chars, compelling)\n   - Heading structure (single H1, logical hierarchy)\n   - Image optimization (alt text, file sizes)\n\n4. **Content Quality**\n   - E-E-A-T signals (experience, expertise, authority, trust)\n   - Content depth and uniqueness\n   - AI writing patterns to avoid (from references)\n\n5. **Authority & Links**\n   - Internal linking structure\n   - External link profile (if Ahrefs/DataForSEO available)\n\n### Step 5: Generate Report\n\nOutput in actionable format:\n\n```markdown\n## SEO Audit Report: [DOMAIN]\n\n**Audit Date:** YYYY-MM-DD\n**Scope:** Full / Technical / On-Page / Content\n\n### Executive Summary\n\n- **Overall Health:** Good / Needs Work / Critical Issues\n- **Top 3 Priority Issues:**\n  1. [Issue] - [Impact: High/Medium/Low]\n  2. [Issue] - [Impact: High/Medium/Low]\n  3. [Issue] - [Impact: High/Medium/Low]\n\n### Technical SEO\n\n| Check | Status | Notes |\n|-------|--------|-------|\n| HTTPS | PASS/FAIL | |\n| robots.txt | PASS/FAIL | |\n| Sitemap | PASS/FAIL | |\n| Core Web Vitals | PASS/FAIL | LCP: Xs, CLS: X.XX |\n| Mobile-Friendly | PASS/FAIL | |\n\n### On-Page SEO\n\n| Element | Status | Recommendation |\n|---------|--------|----------------|\n| Title Tag | PASS/FAIL | |\n| Meta Description | PASS/FAIL | |\n| H1 Tag | PASS/FAIL | |\n| Image Alt Text | PASS/FAIL | |\n\n### Content Quality\n\n- **E-E-A-T Score:** X/10\n- **Content Depth:** Adequate / Thin / Comprehensive\n- **AI Writing Patterns:** None detected / [Issues found]\n\n### Prioritized Action Plan\n\n**Critical (Fix Immediately):**\n1. [Issue] - [Specific fix]\n\n**High Priority (This Week):**\n1. [Issue] - [Specific fix]\n\n**Quick Wins (Easy, Immediate Benefit):**\n1. [Issue] - [Specific fix]\n\n**Long-Term Recommendations:**\n1. [Recommendation]\n```\n\n## Examples\n\n```bash\n# Full audit of a domain\n/seo-audit example.com\n\n# Technical-only audit\n/seo-audit example.com --scope=technical\n\n# Include Search Console data\n/seo-audit example.com --gsc\n\n# Compare against competitor\n/seo-audit example.com --compare=competitor.com\n\n# Audit specific page\n/seo-audit https://example.com/blog/article\n\n# Save report to file\n/seo-audit example.com --output=seo-report.md\n```\n\n## Related\n\n- `seo/seo-audit-skill.md` - Full SEO audit subagent (imported skill)\n- `seo/google-search-console.md` - GSC integration\n- `seo/dataforseo.md` - DataForSEO API\n- `seo/ahrefs.md` - Ahrefs API\n- `tools/performance/pagespeed.md` - PageSpeed Insights\n- `commands/performance.md` - Performance audit command\n",
        ".agent/scripts/commands/seo-export.md": "---\ndescription: Export SEO data from multiple platforms to TOON format\nagent: SEO\nmode: subagent\n---\n\nExport SEO ranking data from configured platforms to a common TOON format for analysis.\n\nTarget: $ARGUMENTS\n\n## Quick Reference\n\n- **Purpose**: Export SEO data for analysis\n- **Platforms**: GSC, Bing, Ahrefs, DataForSEO\n- **Output**: `~/.aidevops/.agent-workspace/work/seo-data/{domain}/`\n\n## Usage\n\n```bash\n# Export from all platforms\n/seo-export all example.com\n\n# Export from specific platform\n/seo-export gsc example.com\n/seo-export bing example.com\n/seo-export ahrefs example.com\n/seo-export dataforseo example.com\n\n# With date range\n/seo-export all example.com --days 30\n\n# List available platforms\n/seo-export list\n\n# List exports for a domain\n/seo-export exports example.com\n```\n\n## Process\n\n1. Parse $ARGUMENTS to extract platform, domain, and options\n2. Run the appropriate export script:\n\n```bash\n~/.aidevops/agents/scripts/seo-export-helper.sh $ARGUMENTS\n```\n\n3. Report results including:\n   - Number of rows exported\n   - Output file location\n   - Any errors or warnings\n\n## Platform Requirements\n\n| Platform | Credential | Location |\n|----------|------------|----------|\n| GSC | Service account JSON | `GOOGLE_APPLICATION_CREDENTIALS` |\n| Bing | API key | `BING_WEBMASTER_API_KEY` |\n| Ahrefs | API key | `AHREFS_API_KEY` |\n| DataForSEO | Username/password | `DATAFORSEO_USERNAME`, `DATAFORSEO_PASSWORD` |\n\nAll credentials should be set in `~/.config/aidevops/mcp-env.sh`.\n\n## Next Steps\n\nAfter export, suggest running analysis:\n\n```bash\n/seo-analyze example.com\n```\n\n## Documentation\n\nFor full documentation, read `seo/data-export.md`.\n",
        ".agent/scripts/commands/seo-opportunities.md": "---\ndescription: Export SEO data and analyze for ranking opportunities in one step\nagent: SEO\nmode: subagent\n---\n\nExport SEO data from all configured platforms and run full analysis in one step.\n\nTarget: $ARGUMENTS\n\n## Quick Reference\n\n- **Purpose**: Complete SEO opportunity analysis workflow\n- **Combines**: `/seo-export all` + `/seo-analyze`\n- **Output**: Full analysis report with actionable opportunities\n\n## Usage\n\n```bash\n# Full workflow with default 90 days\n/seo-opportunities example.com\n\n# Custom date range\n/seo-opportunities example.com --days 30\n```\n\n## Process\n\n1. Parse $ARGUMENTS to extract domain and options\n2. Export from all configured platforms:\n\n```bash\n~/.aidevops/agents/scripts/seo-export-helper.sh all $DOMAIN --days $DAYS\n```\n\n3. Run full analysis:\n\n```bash\n~/.aidevops/agents/scripts/seo-analysis-helper.sh $DOMAIN\n```\n\n4. Present summary of findings:\n   - Top 10 quick wins\n   - Top 10 striking distance opportunities\n   - Low CTR pages needing optimization\n   - Content cannibalization issues\n\n## Output\n\nTwo types of files are created:\n\n**Export files** (one per platform):\n\n```text\n~/.aidevops/.agent-workspace/work/seo-data/{domain}/{platform}-{start}-{end}.toon\n```\n\n**Analysis file**:\n\n```text\n~/.aidevops/.agent-workspace/work/seo-data/{domain}/analysis-{date}.toon\n```\n\n## Recommendations\n\nAfter analysis, provide prioritized recommendations:\n\n1. **Quick Wins** (do first)\n   - Fastest ROI\n   - Minimal effort required\n   - On-page changes only\n\n2. **Low CTR** (do second)\n   - Title/meta changes are quick\n   - Can significantly increase traffic\n\n3. **Cannibalization** (do third)\n   - Prevents wasted effort\n   - Consolidates ranking signals\n\n4. **Striking Distance** (longer term)\n   - Requires more effort\n   - Higher potential reward\n\n## Documentation\n\n- Export details: `seo/data-export.md`\n- Analysis details: `seo/ranking-opportunities.md`\n",
        ".agent/scripts/commands/session-review.md": "---\ndescription: Review session for completeness, best practices, and knowledge capture\nagent: Build+\nmode: subagent\n---\n\nReview the current session for completeness and aidevops workflow adherence.\n\nFocus area: $ARGUMENTS\n\n## Review Process\n\n### Step 1: Gather Context\n\nRun the helper script to collect session data:\n\n```bash\n~/.aidevops/agents/scripts/session-review-helper.sh gather\n```\n\n### Step 2: Analyze Objectives\n\nReview what was accomplished in this session:\n\n1. **Initial Request**: What did the user originally ask for?\n2. **Branch Purpose**: Does the branch name reflect the work done?\n3. **Commits Made**: Do commits align with the objective?\n4. **Outstanding Items**: What remains incomplete?\n\nCalculate completion score:\n- 100%: All objectives met, no outstanding items\n- 75-99%: Primary objective met, minor items remain\n- 50-74%: Partial completion, significant work remains\n- <50%: Major objectives incomplete\n\n### Step 3: Check Workflow Adherence\n\nVerify aidevops best practices:\n\n| Practice | Check | Required |\n|----------|-------|----------|\n| Feature branch | Not on main/master | Yes |\n| Pre-edit check | Ran before first edit | Yes |\n| Atomic commits | Each commit is focused | Yes |\n| TODO tracking | Tasks logged appropriately | Recommended |\n| Quality checks | Linters run before commit | Recommended |\n\n### Step 4: Identify Knowledge to Capture\n\nLook for learnings that should be preserved:\n\n1. **Corrections Made**: Did the AI make mistakes that were corrected?\n2. **New Patterns**: Were new approaches discovered?\n3. **Tool Issues**: Did any tools not work as expected?\n4. **User Preferences**: Did the user express preferences?\n\nFor each learning, suggest where to document:\n- Agent improvements → `@agent-review`\n- Code patterns → Code comments or docs\n- User preferences → `memory/` files\n- Temporary workarounds → TODO.md\n\n### Step 5: Session Health Assessment\n\nDetermine recommendation:\n\n**End Session When:**\n- All objectives complete\n- PR merged\n- Blocked on external factors\n- Context becoming stale (long session)\n- Topic shift to unrelated work\n\n**Continue Session When:**\n- More work in scope\n- User wants to continue\n- Related follow-up tasks\n\n**Start New Session For:**\n- Unrelated topics\n- Clean context needed\n- Parallel work on different branch\n\n## Output Format\n\n```text\n# Session Review\n\n**Branch**: {branch-name}\n**Duration**: {approximate}\n**Date**: {YYYY-MM-DD}\n\n---\n\n## Objective Completion: {score}%\n\n### Completed\n- [x] {item 1}\n- [x] {item 2}\n\n### Outstanding\n- [ ] {item} - {reason/next step}\n\n---\n\n## Workflow Adherence\n\n### Followed\n- [x] {practice}\n\n### Improvements Needed\n- [ ] {practice} - {recommendation}\n\n---\n\n## Knowledge Capture\n\n### Should Document\n- {learning}: {suggested location}\n\n### Action Items\n1. {specific action}\n\n---\n\n## Session Recommendation\n\n**Verdict**: {Continue | End Session | Start New Session}\n\n### Immediate Actions\n1. {action}\n\n### For Future Sessions\n- {topic}: suggest branch `{type}/{name}`\n\n---\n\n*Review generated by /session-review*\n```\n\n## Quick Review (Summary Only)\n\nFor a quick status check without full analysis:\n\n```bash\n~/.aidevops/agents/scripts/session-review-helper.sh summary\n```\n\n## Integration Points\n\n- **Before PR**: Run to ensure nothing forgotten\n- **Before ending session**: Capture learnings\n- **After Ralph loop**: Verify completion promise was truly met\n- **On topic shift**: Decide whether to continue or start fresh\n",
        ".agent/scripts/commands/show-plan.md": "---\ndescription: Show detailed information about a specific plan from PLANS.md\nagent: Build+\nmode: subagent\n---\n\nDisplay detailed plan information including purpose, progress, decisions, and related tasks.\n\nArguments: $ARGUMENTS\n\n## Quick Output (Default)\n\nRun the helper script for instant output:\n\n```bash\n~/.aidevops/agents/scripts/show-plan-helper.sh $ARGUMENTS\n```\n\nDisplay the output directly to the user. The script handles all formatting.\n\n## Fallback (Script Unavailable)\n\nIf the script fails or is unavailable:\n\n1. Read `todo/PLANS.md`\n2. Find the matching plan section by fuzzy title match or plan ID\n3. Extract and format all sections (Purpose, Progress, Decisions, etc.)\n4. Find related tasks in `TODO.md`\n\n## Arguments\n\n**Plan identifier (required unless --list or --current):**\n- Plan name (fuzzy match): `opencode`, `destructive`, `beads`\n- Plan ID: `p001`, `p002`, etc.\n\n**Options:**\n- `--current` - Show plan related to current git branch\n- `--list` - List all active plans briefly\n- `--json` - Output as JSON\n\n## Examples\n\n```bash\n/show-plan opencode              # Show aidevops-opencode Plugin plan\n/show-plan p001                  # Show plan by ID\n/show-plan --current             # Show plan for current branch\n/show-plan --list                # List all plans\n/show-plan \"destructive\"         # Fuzzy match \"Destructive Command Hooks\"\n/show-plan beads                 # Show Beads Integration plan\n```\n\n## Output Format\n\nThe script outputs formatted Markdown:\n\n```markdown\n# Plan Title\n\n**Status:** Planning (Phase 0/4)\n**Estimate:** ~2d (ai:1d test:0.5d read:0.5d)\n**Progress:** Phase 0 of 4\n\n## Purpose\n\nBrief description of why this work matters and what problem it solves.\n\n## Progress\n\n- [ ] Phase 1: Description ~Xh\n- [ ] Phase 2: Description ~Xh\n- [x] Phase 3: Description ~Xh (completed)\n\n## Context\n\nKey decisions, research findings, constraints from conversation.\n\n## Decisions\n\n- **Decision:** What was decided\n  **Rationale:** Why this choice was made\n  **Date:** YYYY-MM-DD\n\n## Discoveries\n\n- **Observation:** What was unexpected\n  **Evidence:** How we know this\n  **Impact:** How it affects the plan\n\n## Related Tasks\n\n- t008: aidevops-opencode Plugin\n- t009: Claude Code Destructive Command Hooks\n\n---\n\n**Options:**\n1. Start working on this plan\n2. View another plan\n3. Back to task list (`/list-todo`)\n```\n\n## After Display\n\nWait for user input:\n\n1. **\"1\"** - Begin working on the plan\n   - Run pre-edit check\n   - Create/switch to appropriate branch\n   - Mark first pending phase as in-progress\n\n2. **\"2\"** - View another plan\n   - Prompt for plan name, then run `/show-plan <name>`\n\n3. **\"3\"** - Return to task list\n   - Run `/list-todo`\n\n## Starting Work on a Plan\n\nWhen user chooses to start:\n\n1. **Check branch status:**\n\n   ```bash\n   ~/.aidevops/agents/scripts/pre-edit-check.sh\n   ```\n\n2. **Create branch if needed:**\n   - Derive branch name from plan title\n   - Use worktree: `wt switch -c feature/<plan-slug>`\n\n3. **Update plan status:**\n   - Change `**Status:** Planning` to `**Status:** In Progress (Phase 1/N)`\n   - Add `started:` timestamp to first phase\n\n4. **Show next steps:**\n   - Display first phase description\n   - List any blockers or dependencies\n\n## Related Commands\n\n- `/list-todo` - List all tasks and plans\n- `/save-todo` - Save current discussion as task/plan\n- `/ready` - Show tasks with no blockers\n",
        ".agent/scripts/commands/yt-dlp.md": "---\ndescription: Download YouTube video, audio, playlist, channel, or transcript using yt-dlp\nagent: Build+\nmode: subagent\n---\n\nDownload media from YouTube (or other supported sites) using yt-dlp.\n\nArguments: $ARGUMENTS\n\n## Workflow\n\n### Step 1: Parse Arguments\n\nParse `$ARGUMENTS` to determine the mode and URL:\n\n```text\n/yt-dlp <url>                          → Auto-detect (video/playlist/channel)\n/yt-dlp video <url> [options]          → Download video\n/yt-dlp audio <url> [options]          → Extract audio (MP3)\n/yt-dlp playlist <url> [options]       → Download playlist\n/yt-dlp channel <url> [options]        → Download channel\n/yt-dlp transcript <url> [options]     → Download subtitles only\n/yt-dlp info <url>                     → Show video info\n/yt-dlp convert <path> [options]       → Extract audio from local file(s)\n/yt-dlp install                        → Install yt-dlp + ffmpeg\n/yt-dlp status                         → Check installation\n/yt-dlp config                         → Generate default config\n```\n\nIf only a URL is provided (no subcommand), auto-detect:\n- Playlist URL (`playlist?list=`) → `playlist`\n- Channel URL (`/@`, `/c/`, `/channel/`) → `channel`\n- Otherwise → `video`\n\n### Step 2: Check Dependencies\n\n```bash\n~/.aidevops/agents/scripts/yt-dlp-helper.sh status\n```\n\nIf yt-dlp or ffmpeg is missing, offer to install:\n\n```bash\n~/.aidevops/agents/scripts/yt-dlp-helper.sh install\n```\n\n### Step 3: Execute Download\n\nRun the appropriate helper command:\n\n```bash\n~/.aidevops/agents/scripts/yt-dlp-helper.sh <command> <url> [options]\n```\n\n### Step 4: Report Results\n\nShow the output directory and downloaded files:\n\n```text\nDownloaded to: ~/Downloads/yt-dlp-{type}-{name}-{timestamp}/\nFiles:\n  - Video Title.mp4 (1.2GB)\n  - Video Title.en.srt\n  - Video Title.info.json\n```\n\n## Options\n\nPass through to the helper script:\n\n| Option | Description |\n|--------|-------------|\n| `--output-dir <path>` | Custom output directory |\n| `--format <fmt>` | Format: `4k`, `1080p`, `720p`, `480p`, `mp3`, `m4a`, `opus`, `wav`, `flac` |\n| `--cookies` | Use Chrome cookies (private/age-restricted content) |\n| `--no-archive` | Allow re-downloading already-fetched videos |\n| `--no-sponsorblock` | Keep sponsor segments |\n| `--no-metadata` | Skip metadata/thumbnail embedding |\n| `--sub-langs <langs>` | Subtitle languages (default: `en`, use `all` for all) |\n\n## Examples\n\n**Download a video (auto-detect):**\n\n```text\n/yt-dlp https://www.youtube.com/watch?v=dQw4w9WgXcQ\n```\n\n**Extract audio as MP3:**\n\n```text\n/yt-dlp audio https://www.youtube.com/watch?v=dQw4w9WgXcQ\n```\n\n**Download in 4K:**\n\n```text\n/yt-dlp video https://www.youtube.com/watch?v=dQw4w9WgXcQ --format 4k\n```\n\n**Download playlist as audio:**\n\n```text\n/yt-dlp audio https://www.youtube.com/playlist?list=PLxxx --format m4a\n```\n\n**Get transcript only:**\n\n```text\n/yt-dlp transcript https://www.youtube.com/watch?v=dQw4w9WgXcQ --sub-langs en,es\n```\n\n**Convert local video to audio:**\n\n```text\n/yt-dlp convert ~/Videos/lecture.mp4 --format flac\n```\n\n**Download private video with cookies:**\n\n```text\n/yt-dlp https://www.youtube.com/watch?v=PRIVATE --cookies\n```\n\n**Check what's installed:**\n\n```text\n/yt-dlp status\n```\n\n## Related\n\n- `tools/video/yt-dlp.md` - Full agent documentation\n- `scripts/yt-dlp-helper.sh` - Helper script reference\n",
        ".agent/services/hosting/cloudflare-platform/references/agents-sdk/README.md": "# Cloudflare Agents SDK\n\nCloudflare Agents SDK enables building AI-powered agents on Durable Objects with state, WebSockets, SQL, scheduling, and AI integration.\n\n## Core Value\n\nBuild stateful, globally distributed AI agents with persistent memory, real-time connections, scheduled tasks, and async workflows.\n\n## When to Use\n\n- Persistent state + memory required\n- Real-time WebSocket connections\n- Long-running workflows (minutes/hours)\n- Chat interfaces with AI models\n- Scheduled/recurring tasks with state\n- DB queries with agent state\n\n## Quick Start\n\n```typescript\nimport { Agent } from \"agents\";\n\nexport class MyAgent extends Agent<Env> {\n  onStart() {\n    this.sql`CREATE TABLE IF NOT EXISTS users (id TEXT PRIMARY KEY)`;\n  }\n  \n  async onRequest(request: Request) {\n    return Response.json({ state: this.state });\n  }\n}\n```\n\n## See Also\n\n- durable-objects - Agent infrastructure\n- d1 - External database integration\n- workers-ai - AI model integration\n- vectorize - Vector search for RAG patterns\n",
        ".agent/services/hosting/cloudflare-platform/references/ai-gateway/README.md": "# Cloudflare AI Gateway Skill\n\nExpert guidance for implementing and configuring Cloudflare AI Gateway - a universal gateway for AI model providers with analytics, caching, rate limiting, and routing capabilities.\n\n## When to Use This Skill\n\n- Setting up AI Gateway for any AI provider (OpenAI, Anthropic, Workers AI, etc.)\n- Implementing caching, rate limiting, or request retry/fallback\n- Configuring dynamic routing with A/B testing or model fallbacks\n- Managing provider API keys securely with BYOK\n- Setting up observability with logging and custom metadata\n- Integrating AI Gateway with Cloudflare Workers or external applications\n- Debugging AI Gateway requests or optimizing configurations\n\n## Core Concepts\n\n### Gateway Architecture\n\nAI Gateway acts as a proxy between your application and AI providers:\n\n```\nYour App → AI Gateway → AI Provider (OpenAI, Anthropic, etc.)\n         ↓\n    Analytics, Caching, Rate Limiting, Logging\n```\n\n**Key URL patterns:**\n- Unified API (OpenAI-compatible): `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/compat/chat/completions`\n- Provider-specific: `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/{provider}/{endpoint}`\n- Dynamic routes: Use route name instead of model: `dynamic/{route-name}`\n\n### Gateway Types\n\n1. **Unauthenticated Gateway**: Open access (not recommended for production)\n2. **Authenticated Gateway**: Requires `cf-aig-authorization` header with Cloudflare API token (recommended)\n\n### Provider Authentication Options\n\n1. **Unified Billing**: Use AI Gateway billing to pay for inference\n2. **BYOK (Store Keys)**: Store provider API keys in Cloudflare dashboard\n3. **Request Headers**: Include provider API key in each request\n\n## Common Patterns\n\n### Pattern 1: OpenAI SDK with Unified API Endpoint\n\nMost common pattern - drop-in replacement for OpenAI API with multi-provider support.\n\n```typescript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY, // or any provider's key\n  baseURL: `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/compat`,\n  defaultHeaders: {\n    // Only needed for authenticated gateways\n    'cf-aig-authorization': `Bearer ${cfToken}`\n  }\n});\n\n// Switch providers by changing model format: {provider}/{model}\nconst response = await client.chat.completions.create({\n  model: 'openai/gpt-4o-mini', // or 'anthropic/claude-sonnet-4-5'\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n**Benefits:**\n- Works with existing OpenAI SDK tooling\n- Switch providers without code changes (just change model param)\n- Compatible with most OpenAI-compatible tools\n\n### Pattern 2: Provider-Specific Endpoints\n\nUse when you need the original provider's API schema.\n\n```typescript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  baseURL: `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/openai`\n});\n\n// Standard OpenAI request - AI Gateway features still apply\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o-mini',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Pattern 3: Workers AI Binding with Gateway\n\nFor Cloudflare Workers using Workers AI.\n\n```typescript\nexport default {\n  async fetch(request, env, ctx) {\n    const response = await env.AI.run(\n      '@cf/meta/llama-3-8b-instruct',\n      { \n        messages: [{ role: 'user', content: 'Hello!' }]\n      },\n      { \n        gateway: { \n          id: 'my-gateway',\n          metadata: { userId: '123', team: 'engineering' }\n        } \n      }\n    );\n    \n    return new Response(JSON.stringify(response));\n  }\n};\n```\n\n### Pattern 4: Custom Metadata for Tracking\n\nTag requests with user IDs, teams, or other identifiers (max 5 metadata entries).\n\n```typescript\nconst response = await openai.chat.completions.create(\n  {\n    model: 'gpt-4o-mini',\n    messages: [{ role: 'user', content: 'Hello!' }]\n  },\n  {\n    headers: {\n      'cf-aig-metadata': JSON.stringify({\n        userId: 'user123',\n        team: 'engineering',\n        environment: 'production',\n        requestType: 'chat',\n        internal: true\n      })\n    }\n  }\n);\n```\n\n### Pattern 5: Per-Request Caching Control\n\nOverride default gateway caching settings per request.\n\n```bash\n# Skip cache for this request\ncurl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \\\n  --header 'Authorization: Bearer $TOKEN' \\\n  --header 'cf-aig-skip-cache: true' \\\n  --data '{\"model\": \"gpt-4o-mini\", \"messages\": [...]}'\n\n# Custom cache TTL (1 hour)\ncurl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \\\n  --header 'Authorization: Bearer $TOKEN' \\\n  --header 'cf-aig-cache-ttl: 3600' \\\n  --data '{\"model\": \"gpt-4o-mini\", \"messages\": [...]}'\n\n# Custom cache key for deterministic caching\ncurl https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/openai/chat/completions \\\n  --header 'Authorization: Bearer $TOKEN' \\\n  --header 'cf-aig-cache-key: greeting-response' \\\n  --data '{\"model\": \"gpt-4o-mini\", \"messages\": [...]}'\n```\n\n**Cache headers:**\n- `cf-aig-skip-cache: true` - Bypass cache\n- `cf-aig-cache-ttl: <seconds>` - Custom TTL (min: 60s, max: 1 month)\n- `cf-aig-cache-key: <key>` - Custom cache key\n- Response header `cf-aig-cache-status: HIT|MISS` indicates cache status\n\n### Pattern 6: BYOK (Bring Your Own Keys)\n\nStore provider keys in dashboard, remove from code.\n\n**Setup:**\n1. Enable authentication on gateway\n2. Dashboard → AI Gateway → Select gateway → Provider Keys → Add API Key\n3. Remove provider API keys from code:\n\n```typescript\n// Before BYOK: Include provider key in every request\nconst client = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY, // Provider key\n  baseURL: `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/openai`,\n  defaultHeaders: {\n    'cf-aig-authorization': `Bearer ${cfToken}` // Gateway auth\n  }\n});\n\n// After BYOK: Only gateway auth needed\nconst client = new OpenAI({\n  // No apiKey needed - stored in dashboard\n  baseURL: `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/openai`,\n  defaultHeaders: {\n    'cf-aig-authorization': `Bearer ${cfToken}` // Only gateway auth\n  }\n});\n```\n\n### Pattern 7: Dynamic Routing with Fallbacks\n\nConfigure routing logic in dashboard, not code.\n\n```typescript\n// Use route name instead of model\nconst response = await client.chat.completions.create({\n  model: 'dynamic/support', // Route name from dashboard\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n**Dynamic routing use cases:**\n- A/B testing between models\n- Rate/budget limits per user/team\n- Model fallbacks on errors\n- Conditional routing (paid vs free users)\n\n**Route configuration (in dashboard):**\n1. Create route: Dashboard → Gateway → Dynamic Routes → Add Route\n2. Define flow with nodes:\n   - **Conditional**: Branch on metadata (e.g., `user.plan == \"paid\"`)\n   - **Percentage**: A/B split (e.g., 80% model A, 20% model B)\n   - **Rate Limit**: Quota enforcement, fallback when exceeded\n   - **Budget Limit**: Cost quota enforcement\n   - **Model**: Call specific provider/model\n3. Save & deploy version\n\n### Pattern 8: Error Handling\n\n```typescript\ntry {\n  const response = await client.chat.completions.create({\n    model: 'gpt-4o-mini',\n    messages: [{ role: 'user', content: 'Hello!' }]\n  });\n} catch (error) {\n  // Rate limit exceeded\n  if (error.status === 429) {\n    console.error('Rate limit exceeded:', error.message);\n    // Implement backoff or use dynamic routing with fallback\n  }\n  \n  // Gateway authentication failed\n  if (error.status === 401) {\n    console.error('Gateway authentication failed - check cf-aig-authorization token');\n  }\n  \n  // Provider authentication failed\n  if (error.status === 403) {\n    console.error('Provider authentication failed - check API key or BYOK setup');\n  }\n  \n  throw error;\n}\n```\n\n## Configuration Reference\n\n### Dashboard Setup\n\n**Create gateway:**\n\n```bash\n# Via Dashboard: AI > AI Gateway > Create Gateway\n# Or via API:\ncurl https://api.cloudflare.com/client/v4/accounts/{account_id}/ai-gateway/gateways \\\n  -H \"Authorization: Bearer $CF_API_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"id\": \"my-gateway\",\n    \"cache_ttl\": 3600,\n    \"cache_invalidate_on_update\": true,\n    \"rate_limiting_interval\": 60,\n    \"rate_limiting_limit\": 100,\n    \"rate_limiting_technique\": \"sliding\",\n    \"collect_logs\": true\n  }'\n```\n\n### Feature Configuration\n\n**Caching:**\n- Dashboard: Settings → Cache Responses → Enable\n- Default TTL: Set in gateway settings\n- Cache behavior: Only for identical requests (text & image responses)\n- Use case: Support bots with limited prompt options\n\n**Rate Limiting:**\n- Dashboard: Settings → Rate-limiting → Enable\n- Parameters:\n  - Limit: Number of requests\n  - Interval: Time period (seconds)\n  - Technique: `fixed` or `sliding` window\n- Response: `429 Too Many Requests` when exceeded\n\n**Logging:**\n- Dashboard: Settings → Logs\n- Default: Enabled (up to 10M logs per gateway)\n- Per-request: `cf-aig-collect-log: false` to skip\n- Auto-delete: Enable to remove oldest logs when limit reached\n- Filter logs by: status, cache, provider, model, cost, tokens, duration, metadata\n\n### Wrangler Integration\n\n**Gateway with Workers AI:**\n\n```toml\n# wrangler.toml\nname = \"my-worker\"\nmain = \"src/index.ts\"\ncompatibility_date = \"2024-01-01\"\n\n[ai]\nbinding = \"AI\"\n\n[[ai.gateway]]\nid = \"my-gateway\"\n```\n\n```typescript\n// src/index.ts\nexport default {\n  async fetch(request, env, ctx): Promise<Response> {\n    const response = await env.AI.run(\n      '@cf/meta/llama-3-8b-instruct',\n      { prompt: 'Hello!' },\n      { gateway: { id: 'my-gateway' } }\n    );\n    \n    return Response.json(response);\n  }\n} satisfies ExportedHandler<Env>;\n```\n\n**Environment variables for gateways:**\n\n```toml\n# wrangler.toml\n[vars]\nCF_ACCOUNT_ID = \"your-account-id\"\nGATEWAY_ID = \"my-gateway\"\n\n# Secrets (use wrangler secret put)\n# CF_API_TOKEN - for authenticated gateways\n# OPENAI_API_KEY - if not using BYOK\n```\n\n```bash\n# Set secrets\nwrangler secret put CF_API_TOKEN\nwrangler secret put OPENAI_API_KEY\n```\n\n### API Token Permissions\n\n**For gateway management:**\n- AI Gateway - Read\n- AI Gateway - Edit\n\n**For authenticated gateway access:**\n- Create API token with appropriate permissions\n- Pass in `cf-aig-authorization: Bearer {token}` header\n\n## Supported Providers\n\nAI Gateway works with 15+ providers via unified API or provider-specific endpoints:\n\n| Provider | Unified API | Provider Endpoint | Notes |\n|----------|-------------|-------------------|-------|\n| OpenAI | ✅ `openai/gpt-4o` | `/openai/*` | Full support |\n| Anthropic | ✅ `anthropic/claude-3-5-sonnet` | `/anthropic/*` | Full support |\n| Google AI Studio | ✅ `google-ai-studio/gemini-2.0-flash` | `/google-ai-studio/*` | Full support |\n| Workers AI | ✅ `workersai/@cf/meta/llama-3` | `/workers-ai/*` | Native integration |\n| Azure OpenAI | ✅ `azure-openai/*` | `/azure-openai/*` | Deployment names |\n| AWS Bedrock | ❌ | `/bedrock/*` | Provider endpoint only |\n| Groq | ✅ `groq/*` | `/groq/*` | Fast inference |\n| Mistral | ✅ `mistral/*` | `/mistral/*` | Full support |\n| Cohere | ✅ `cohere/*` | `/cohere/*` | Full support |\n| Perplexity | ✅ `perplexity/*` | `/perplexity/*` | Full support |\n| xAI (Grok) | ✅ `grok/*` | `/grok/*` | Full support |\n| DeepSeek | ✅ `deepseek/*` | `/deepseek/*` | Full support |\n| Cerebras | ✅ `cerebras/*` | `/cerebras/*` | Fast inference |\n| Replicate | ❌ | `/replicate/*` | Provider endpoint only |\n| HuggingFace | ❌ | `/huggingface/*` | Provider endpoint only |\n\nSee [full provider list](https://developers.cloudflare.com/ai-gateway/usage/providers/)\n\n## Observability\n\n### Analytics Dashboard\n\nView in Dashboard → AI Gateway → Select gateway:\n- Request count over time\n- Token usage (input/output)\n- Cost tracking (estimated or custom)\n- Cache hit rate\n- Error rates by provider/model\n- Latency percentiles\n\n### Log Structure\n\nEach log entry contains:\n- User prompt & model response\n- Provider & model\n- Timestamp\n- Request status (success/error)\n- Token usage (input/output/total)\n- Cost\n- Duration (ms)\n- Cache status (HIT/MISS)\n- Custom metadata\n- Request/Event ID\n\n### Custom Cost Tracking\n\nFor custom models or providers not in Cloudflare's pricing database:\n\n```bash\n# Dashboard: Gateway → Settings → Custom Costs\n# Or via API:\ncurl https://api.cloudflare.com/client/v4/accounts/{account_id}/ai-gateway/gateways/{gateway_id}/custom-costs \\\n  -H \"Authorization: Bearer $CF_API_TOKEN\" \\\n  -d '{\n    \"model\": \"custom-model-v1\",\n    \"input_cost\": 0.01,\n    \"output_cost\": 0.03\n  }'\n```\n\n## Advanced Use Cases\n\n### Multi-Model Chat with Fallbacks\n\n```typescript\n// Configure in dashboard:\n// Route: dynamic/smart-chat\n// - Try GPT-4 first\n// - Fallback to Claude if error\n// - Fallback to Llama if both fail\n\nconst response = await client.chat.completions.create({\n  model: 'dynamic/smart-chat',\n  messages: [{ role: 'user', content: 'Complex reasoning task' }]\n});\n```\n\n### A/B Testing Models\n\n```typescript\n// Dashboard: Create route with Percentage node\n// - 50% to gpt-4o-mini\n// - 50% to claude-sonnet-4-5\n// Analyze logs to compare quality/cost/latency\n\nconst response = await client.chat.completions.create({\n  model: 'dynamic/ab-test',\n  messages: [{ role: 'user', content: prompt }],\n  // Add metadata to track experiments\n  headers: {\n    'cf-aig-metadata': JSON.stringify({ experiment: 'model-comparison-v1' })\n  }\n});\n```\n\n### User-Based Rate Limiting\n\n```typescript\n// Dashboard: Create route with Rate Limit node\n// - Condition: Check metadata.userId\n// - Limit: 100 requests/hour per user\n// - Fallback: Return error or use cheaper model\n\nconst response = await client.chat.completions.create(\n  {\n    model: 'dynamic/user-limited',\n    messages: [{ role: 'user', content: prompt }]\n  },\n  {\n    headers: {\n      'cf-aig-metadata': JSON.stringify({ userId })\n    }\n  }\n);\n```\n\n### Semantic Caching (Future)\n\nCurrently, caching requires identical requests. Semantic caching (similar but not identical requests) is planned.\n\n**Current workaround:**\n\n```typescript\n// Use cf-aig-cache-key for grouped responses\nconst normalizedPrompt = normalizePrompt(userInput); // Your logic\nconst cacheKey = hashPrompt(normalizedPrompt);\n\nconst response = await fetch(gatewayUrl, {\n  headers: {\n    'cf-aig-cache-key': cacheKey,\n    'cf-aig-cache-ttl': '3600'\n  },\n  // ... rest of request\n});\n```\n\n## Debugging & Troubleshooting\n\n### Check Gateway Status\n\n```bash\n# List all gateways\ncurl https://api.cloudflare.com/client/v4/accounts/{account_id}/ai-gateway/gateways \\\n  -H \"Authorization: Bearer $CF_API_TOKEN\"\n\n# Get specific gateway\ncurl https://api.cloudflare.com/client/v4/accounts/{account_id}/ai-gateway/gateways/{gateway_id} \\\n  -H \"Authorization: Bearer $CF_API_TOKEN\"\n```\n\n### Inspect Request Logs\n\nDashboard → Gateway → Logs\n\n**Filter examples:**\n- `status: error` - All failed requests\n- `provider: openai` - OpenAI requests only\n- `metadata.userId: user123` - Specific user\n- `cache: not cached` - Cache misses only\n- `cost > 0.01` - Expensive requests\n\n### Common Issues\n\n**401 Unauthorized:**\n- Authenticated gateway without `cf-aig-authorization` header\n- Invalid/expired CF API token\n- Check token permissions (AI Gateway - Read)\n\n**403 Forbidden:**\n- Provider API key invalid/missing\n- BYOK key not configured or expired\n- Provider quota exceeded\n\n**429 Rate Limited:**\n- Gateway rate limit exceeded\n- Check settings: Dashboard → Gateway → Settings → Rate-limiting\n- Implement backoff or use dynamic routing\n\n**Cache not working:**\n- Requests must be identical (body, model, parameters)\n- Caching only supports text/image responses\n- Check `cf-aig-cache-status` header in response\n- Verify caching enabled: Dashboard → Settings → Cache Responses\n\n**Logs not appearing:**\n- Check log limit (default: 10M per gateway)\n- Verify logs enabled: Dashboard → Settings → Logs\n- Per-request `cf-aig-collect-log: false` bypasses logging\n- Wait 30-60s for logs to appear\n\n## API Reference\n\n### Gateway Management\n\n```bash\n# Create gateway\nPOST /accounts/{account_id}/ai-gateway/gateways\n\n# Update gateway\nPUT /accounts/{account_id}/ai-gateway/gateways/{gateway_id}\n\n# Delete gateway\nDELETE /accounts/{account_id}/ai-gateway/gateways/{gateway_id}\n\n# List gateways\nGET /accounts/{account_id}/ai-gateway/gateways\n```\n\n### Log Management\n\n```bash\n# Get logs\nGET /accounts/{account_id}/ai-gateway/gateways/{gateway_id}/logs\n\n# Delete logs\nDELETE /accounts/{account_id}/ai-gateway/gateways/{gateway_id}/logs\n\n# Filter logs (query params)\n?status=error&provider=openai&cache=not_cached\n```\n\n### Headers Reference\n\n**Gateway authentication:**\n- `cf-aig-authorization: Bearer {token}` - Required for authenticated gateways\n\n**Caching:**\n- `cf-aig-cache-ttl: {seconds}` - Cache duration (60s - 1 month)\n- `cf-aig-skip-cache: true` - Bypass cache\n- `cf-aig-cache-key: {key}` - Custom cache key\n- Response: `cf-aig-cache-status: HIT|MISS`\n\n**Logging:**\n- `cf-aig-collect-log: false` - Skip logging for this request\n\n**Metadata:**\n- `cf-aig-metadata: {json}` - Custom tracking data (max 5 entries, string/number/boolean values)\n\n## Best Practices\n\n1. **Always use authenticated gateways in production**\n   - Prevents unauthorized access\n   - Protects against log storage abuse\n   - Required for BYOK\n\n2. **Use BYOK for provider keys**\n   - Removes keys from codebase\n   - Easier key rotation\n   - Centralized management\n\n3. **Add custom metadata to all requests**\n   - Track users, teams, environments\n   - Filter logs effectively\n   - Debug production issues faster\n\n4. **Configure appropriate rate limits**\n   - Prevent runaway costs\n   - Use dynamic routing for per-user limits\n   - Combine with budget limits\n\n5. **Enable caching for deterministic prompts**\n   - Support bots with fixed options\n   - Static content generation\n   - Reduces costs & latency\n\n6. **Use dynamic routing for resilience**\n   - Model fallbacks on errors\n   - A/B testing without code changes\n   - Gradual rollouts\n\n7. **Monitor logs regularly**\n   - Set up automatic log deletion\n   - Export logs for long-term analysis\n   - Track cost trends\n\n8. **Test with provider-specific endpoints first**\n   - Validates provider integration\n   - Easier debugging\n   - Migrate to unified API after validation\n\n## Examples Repository\n\nSee real-world usage:\n- [NextChat](https://github.com/ChatGPTNextWeb/NextChat/blob/main/app/utils/cloudflare.ts) - URL parsing utilities\n- [LibreChat](https://github.com/danny-avila/LibreChat) - Multi-provider chat with AI Gateway\n- [Continue.dev](https://github.com/continuedev/continue/blob/main/core/llm/llms/Cloudflare.ts) - IDE integration\n- [Big-AGI](https://github.com/enricoros/big-AGI) - Complex gateway path handling\n\n## Resources\n\n- [Official Docs](https://developers.cloudflare.com/ai-gateway/)\n- [API Reference](https://developers.cloudflare.com/api/resources/ai_gateway/)\n- [Provider Guides](https://developers.cloudflare.com/ai-gateway/usage/providers/)\n- [Workers AI Integration](https://developers.cloudflare.com/workers-ai/)\n- [Discord Community](https://discord.cloudflare.com)\n\n## Quick Reference\n\n**Create gateway:**\n\n```bash\nDashboard → AI → AI Gateway → Create Gateway\n```\n\n**Basic request:**\n\n```typescript\nconst client = new OpenAI({\n  baseURL: `https://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/compat`\n});\n```\n\n**Check cache status:**\n\n```bash\n# Response header: cf-aig-cache-status: HIT|MISS\n```\n\n**Get account/gateway IDs:**\n\n```bash\n# Account ID: Dashboard → Overview → Account ID\n# Gateway ID: Dashboard → AI Gateway → Gateway name/ID\n```\n\n**Required env vars:**\n\n```bash\nCF_ACCOUNT_ID=xxx\nGATEWAY_ID=xxx\nCF_API_TOKEN=xxx  # For authenticated gateways\nPROVIDER_API_KEY=xxx  # If not using BYOK\n```\n",
        ".agent/services/hosting/cloudflare-platform/references/ai-search/README.md": "# Cloudflare AI Search Skill Reference\n\nExpert guidance for implementing Cloudflare AI Search (formerly AutoRAG), Cloudflare's managed semantic search and RAG service....\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/analytics-engine/README.md": "# Cloudflare Workers Analytics Engine Reference\n\nExpert guidance for implementing unlimited-cardinality analytics at scale using Cloudflare Workers Analytics Engine. This skill covers ONLY Analytics Engine - NOT the broader Cloudflare platform....\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/api-shield/README.md": "# Cloudflare API Shield Reference\n\nExpert guidance for API Shield - comprehensive API security suite for discovery, protection, and monitoring.\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, session identifiers, rules, token/mTLS configs\n- **[api.md](./api.md)** - Endpoint management, discovery, validation APIs, GraphQL operations\n- **[patterns.md](./patterns.md)** - Common patterns, progressive rollout, OWASP mappings, workflows\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, false positives, performance, best practices\n\n## Quick Start\n\nAPI Shield: Enterprise-grade API security (Discovery, Schema Validation, JWT, mTLS, Sequence Enforcement). Available as Enterprise add-on with preview access.\n\n## See Also\n\n- [API Shield Docs](https://developers.cloudflare.com/api-shield/)\n- [API Reference](https://developers.cloudflare.com/api/resources/api_gateway/)\n- [OWASP API Security Top 10](https://owasp.org/www-project-api-security/)\n",
        ".agent/services/hosting/cloudflare-platform/references/api/README.md": "# Cloudflare API Integration Skill Reference\n\nGuide for working with Cloudflare's REST API - covering authentication, SDK usage, common patterns, and API categories.\n\n## When to Use This Skill\n\nUse when working with:\n- Cloudflare API authentication and tokens\n- Official Cloudflare SDKs (TypeScript, Python, Go)\n- Zone management, DNS, Workers, o...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/argo-smart-routing/README.md": "# Cloudflare Argo Smart Routing Skill Reference\n\n## Overview\n\nCloudflare Argo Smart Routing is a performance optimization service that detects real-time network issues and routes web traffic across the most efficient network path. This skill provides comprehensive guidance for implementing, configuring, and managing Argo Smart Routing via API, CLI...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/bindings/README.md": "# Cloudflare Bindings Skill Reference\n\nExpert guidance on Cloudflare Workers Bindings - the runtime APIs that connect Workers to Cloudflare platform resources....\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/bot-management/README.md": "# Cloudflare Bot Management\n\nEnterprise-grade bot detection, protection, and mitigation using ML/heuristics, bot scores, JavaScript detections, and verified bot handling.\n\n## Overview\n\nBot Management provides multi-tier protection:\n- **Free (Bot Fight Mode)**: Auto-blocks definite bots, no config\n- **Pro/Business (Super Bot Fight Mode)**: Configurable actions, static resource protection, analytics groupings\n- **Enterprise (Bot Management)**: Granular 1-99 scores, WAF integration, JA3/JA4 fingerprinting, Workers API, Advanced Analytics\n\n## Quick Start\n\n```txt\n# Dashboard: Security > Bots\n# Enterprise: Deploy rule template\n(cf.bot_management.score eq 1 and not cf.bot_management.verified_bot) → Block\n(cf.bot_management.score le 29 and not cf.bot_management.verified_bot) → Managed Challenge\n```\n\n## Core Concepts\n\n**Bot Scores**: 1-99 (1 = definitely automated, 99 = definitely human). Threshold: <30 indicates bot traffic. Enterprise gets granular 1-99; Pro/Business get groupings only.\n\n**Detection Engines**: Heuristics (known fingerprints, assigns score=1), ML (majority of detections, supervised learning on billions of requests), Anomaly Detection (optional, baseline traffic analysis), JavaScript Detections (headless browser detection).\n\n**Verified Bots**: Allowlisted good bots (search engines, AI crawlers) verified via reverse DNS or Web Bot Auth. Access via `cf.bot_management.verified_bot` or `cf.verified_bot_category`.\n\n## Platform Limits\n\n| Plan | Bot Scores | JA3/JA4 | Custom Rules | Analytics Retention |\n|------|------------|---------|--------------|---------------------|\n| Free | No (auto-block only) | No | 5 | N/A (no analytics) |\n| Pro/Business | Groupings only | No | 20/100 | 30 days (72h at a time) |\n| Enterprise | 1-99 granular | Yes | 1,000+ | 30 days (1 week at a time) |\n\n## Basic Patterns\n\n```typescript\n// Workers: Check bot score\nexport default {\n  async fetch(request: Request): Promise<Response> {\n    const botScore = request.cf?.botManagement?.score;\n    if (botScore && botScore < 30 && !request.cf?.botManagement?.verifiedBot) {\n      return new Response('Bot detected', { status: 403 });\n    }\n    return fetch(request);\n  }\n};\n```\n\n```txt\n# WAF: Block definite bots\n(cf.bot_management.score eq 1 and not cf.bot_management.verified_bot)\n\n# WAF: Protect sensitive endpoints\n(cf.bot_management.score lt 50 and http.request.uri.path in {\"/login\" \"/checkout\"} and not cf.bot_management.verified_bot)\n```\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - Product tiers, WAF rule setup, JavaScript Detections, ML auto-updates\n- [api.md](./api.md) - Workers BotManagement interface, WAF fields, JA4 Signals\n- [patterns.md](./patterns.md) - E-commerce, API protection, mobile app allowlisting, SEO-friendly handling\n- [gotchas.md](./gotchas.md) - False positives/negatives, score=0 issues, JSD limitations, CSP requirements\n\n## See Also\n\n- [waf](../waf/) - WAF custom rules for bot enforcement\n- [workers](../workers/) - Workers request.cf.botManagement API\n- [api-shield](../api-shield/) - API-specific bot protection\n",
        ".agent/services/hosting/cloudflare-platform/references/browser-rendering/README.md": "# Cloudflare Browser Rendering Skill Reference\n\n**Description**: Expert knowledge for Cloudflare Browser Rendering - control headless Chrome on Cloudflare's global network for browser automation, screenshots, PDFs, web scraping, testing, and content generation.\n\n**When to use**: Any task involving Cloudflare Browser Rendering including: taking sc...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/c3/README.md": "# Cloudflare C3 (create-cloudflare) Skill\n\nExpert guidance for using C3, the official CLI tool for scaffolding Cloudflare projects.\n\n## What is C3?\n\nC3 (`create-cloudflare`) is Cloudflare's CLI for project initialization. Creates Workers, Pages, and full-stack applications with templates, TypeScript, and instant deployment.\n\n## Installation & Usage\n\n```bash\n# NPM\nnpm create cloudflare@latest\n\n# Yarn\nyarn create cloudflare\n\n# PNPM\npnpm create cloudflare@latest\n\n# With project name\nnpm create cloudflare@latest my-app\n\n# With template\nnpm create cloudflare@latest -- --template=cloudflare/templates/workers-template\n```\n\n## Common Templates\n\n### Workers\n\n```bash\n# Basic Worker\nnpm create cloudflare@latest -- --template=worker\n\n# TypeScript Worker\nnpm create cloudflare@latest -- --template=worker-typescript\n\n# Durable Objects\nnpm create cloudflare@latest -- --template=hello-world-do-template\n```\n\n### Full-Stack Apps\n\n```bash\n# Next.js\nnpm create cloudflare@latest -- --template=next-starter-template\n\n# React Router\nnpm create cloudflare@latest -- --template=react-router-starter-template\n\n# Remix\nnpm create cloudflare@latest -- --template=remix-starter-template\n\n# Astro\nnpm create cloudflare@latest -- --template=astro-blog-starter-template\n```\n\n### Database & Storage\n\n```bash\n# D1 SQLite\nnpm create cloudflare@latest -- --template=d1-template\n\n# R2 Explorer\nnpm create cloudflare@latest -- --template=r2-explorer-template\n\n# Postgres + Hyperdrive\nnpm create cloudflare@latest -- --template=postgres-hyperdrive-template\n```\n\n### AI & ML\n\n```bash\n# Workers AI Chat\nnpm create cloudflare@latest -- --template=llm-chat-app-template\n\n# Text-to-Image\nnpm create cloudflare@latest -- --template=text-to-image-template\n```\n\n### Real-time & Multiplayer\n\n```bash\n# Chat with Durable Objects\nnpm create cloudflare@latest -- --template=durable-chat-template\n\n# Multiplayer Globe\nnpm create cloudflare@latest -- --template=multiplayer-globe-template\n```\n\n## Interactive Mode\n\n```bash\nnpm create cloudflare@latest my-app\n```\n\nC3 prompts for:\n1. Application type (Website/API/scheduled worker/etc.)\n2. Framework choice (if applicable)\n3. TypeScript preference\n4. Git initialization\n5. Deployment option\n\n## Non-Interactive Mode\n\n```bash\nnpm create cloudflare@latest my-worker \\\n  --type=web-app \\\n  --framework=next \\\n  --ts \\\n  --git \\\n  --deploy\n```\n\n### Flags\n\n- `--type` - Application type: `web-app`, `hello-world`, `pre-existing`, `remote-template`\n- `--framework` - Framework: `next`, `remix`, `astro`, `react-router`, `solid`, `svelte`, etc.\n- `--template` - GitHub template URL or path\n- `--ts` / `--no-ts` - TypeScript\n- `--git` / `--no-git` - Initialize git repo\n- `--deploy` / `--no-deploy` - Deploy after creation\n- `--open` - Open in browser after deploy\n- `--existing-script` - Path to existing Worker script\n\n## CI/CD Usage\n\n```yaml\n# GitHub Actions\n- name: Create Cloudflare Project\n  run: |\n    npm create cloudflare@latest my-app -- \\\n      --type=web-app \\\n      --framework=next \\\n      --ts \\\n      --no-git \\\n      --no-deploy\n```\n\n## Custom Templates\n\n```bash\n# From GitHub repo\nnpm create cloudflare@latest -- --template=user/repo\n\n# From local path\nnpm create cloudflare@latest -- --template=../my-template\n\n# From Cloudflare official templates\nnpm create cloudflare@latest -- --template=cloudflare/templates/workers-template\n```\n\n## Project Structure Created\n\n```\nmy-app/\n├── src/\n│   └── index.ts       # Worker entry point\n├── wrangler.toml      # Configuration\n├── package.json\n├── tsconfig.json\n└── README.md\n```\n\n## Post-Creation\n\n```bash\ncd my-app\n\n# Local development\nnpm run dev\n\n# Deploy\nnpm run deploy\n\n# Type check\nnpm run cf-typegen\n```\n\n## wrangler.toml Configuration\n\nC3 generates:\n\n```toml\nname = \"my-app\"\nmain = \"src/index.ts\"\ncompatibility_date = \"2024-01-01\"\n\n# Bindings added based on template\n[[kv_namespaces]]\nbinding = \"MY_KV\"\nid = \"...\"\n\n[[d1_databases]]\nbinding = \"DB\"\ndatabase_id = \"...\"\n```\n\n## Best Practices\n\n1. **Use latest C3:** `npm create cloudflare@latest`\n2. **Choose TypeScript** for type safety\n3. **Enable git** for version control\n4. **Review generated wrangler.toml** before first deploy\n5. **Update dependencies** post-creation: `npm update`\n\n## Common Workflows\n\n### Quick Worker\n\n```bash\nnpm create cloudflare@latest my-worker -- --type=hello-world --ts --deploy\n```\n\n### Full-Stack App with D1\n\n```bash\nnpm create cloudflare@latest my-app -- \\\n  --template=react-router-starter-template \\\n  --ts \\\n  --git\ncd my-app\nnpx wrangler d1 create my-db\n# Add to wrangler.toml, then:\nnpm run deploy\n```\n\n### Convert Existing Project\n\n```bash\ncd existing-project\nnpm create cloudflare@latest . -- --existing-script=./dist/index.js\n```\n\n## Troubleshooting\n\n### `npm create cloudflare failed`\n\n- Ensure Node.js 16.13+\n- Clear npm cache: `npm cache clean --force`\n- Try with `npx`: `npx create-cloudflare@latest`\n\n### Template not found\n\n- Verify template name/path\n- Check network connection\n- Use full GitHub URL: `https://github.com/user/repo`\n\n### Deployment fails\n\n- Check Cloudflare account authentication: `wrangler login`\n- Verify `wrangler.toml` configuration\n- Check for naming conflicts in dashboard\n\n## Reference\n\n- [C3 GitHub](https://github.com/cloudflare/workers-sdk/tree/main/packages/create-cloudflare)\n- [Templates Repository](https://github.com/cloudflare/templates)\n- [Workers Docs](https://developers.cloudflare.com/workers/)\n\n---\n\nThis skill focuses exclusively on C3 CLI tool usage. For Workers development, see `cloudflare-workers` skill.\n",
        ".agent/services/hosting/cloudflare-platform/references/cache-reserve/README.md": "# Cloudflare Cache Reserve\n\n**Persistent cache storage built on R2 for long-term content retention**\n\n## Overview\n\nCache Reserve is Cloudflare's persistent, large-scale cache storage layer built on R2. It acts as the ultimate upper-tier cache, storing cacheable content for extended periods (30+ days) to maximize cache hits, reduce origin egress fees, and shield origins from repeated requests for long-tail content.\n\n## Core Concepts\n\n### What is Cache Reserve?\n\n- **Persistent storage layer**: Built on R2, sits above tiered cache hierarchy\n- **Long-term retention**: 30-day default retention, extended on each access\n- **Automatic operation**: Works seamlessly with existing CDN, no code changes required\n- **Origin shielding**: Dramatically reduces origin egress by serving cached content longer\n- **Usage-based pricing**: Pay only for storage + read/write operations\n\n### Cache Hierarchy\n\n```\nVisitor Request\n    ↓\nLower-Tier Cache (closest to visitor)\n    ↓ (on miss)\nUpper-Tier Cache (closest to origin)\n    ↓ (on miss)\nCache Reserve (R2 persistent storage)\n    ↓ (on miss)\nOrigin Server\n```\n\n### How It Works\n\n1. **On cache miss**: Content fetched from origin �� written to Cache Reserve + edge caches simultaneously\n2. **On edge eviction**: Content may be evicted from edge cache but remains in Cache Reserve\n3. **On subsequent request**: If edge cache misses but Cache Reserve hits → content restored to edge caches\n4. **Retention**: Assets remain in Cache Reserve for 30 days since last access (configurable via TTL)\n\n## Asset Eligibility\n\nCache Reserve only stores assets meeting **ALL** criteria:\n\n- Cacheable per Cloudflare's standard rules\n- Minimum 10-hour TTL (36000 seconds)\n- `Content-Length` header present\n- Original files only (not transformed images)\n\n### Not Eligible\n\n- Assets with TTL < 10 hours\n- Responses without `Content-Length` header\n- Image transformation variants (original images are eligible)\n- Responses with `Set-Cookie` headers\n- Responses with `Vary: *` header\n- Assets from R2 public buckets on same zone\n- O2O (Orange-to-Orange) setup requests\n\n## Quick Start\n\n```bash\n# Enable via Dashboard\nhttps://dash.cloudflare.com/caching/cache-reserve\n# Click \"Enable Storage Sync\" or \"Purchase\" button\n```\n\n**Prerequisites:**\n- Paid Cache Reserve plan required\n- Tiered Cache strongly recommended\n\n## Essential Commands\n\n```bash\n# Check Cache Reserve status\ncurl -X GET \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/cache/cache_reserve\" \\\n  -H \"Authorization: Bearer $API_TOKEN\"\n\n# Enable Cache Reserve\ncurl -X PATCH \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/cache/cache_reserve\" \\\n  -H \"Authorization: Bearer $API_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"value\": \"on\"}'\n\n# Check asset cache status\ncurl -I https://example.com/asset.jpg | grep -i cache\n```\n\n## See Also\n\n- [Configuration](./configuration.md) - Setup, API, and Cache Rules\n- [API Reference](./api.md) - Purging, monitoring, and management APIs\n- [Patterns](./patterns.md) - Best practices and architecture patterns\n- [Gotchas](./gotchas.md) - Common issues, troubleshooting, limits\n",
        ".agent/services/hosting/cloudflare-platform/references/containers/README.md": "# Cloudflare Containers Skill Reference\n\n**APPLIES TO: Cloudflare Containers ONLY - NOT general Cloudflare Workers**\n\nUse when working with Cloudflare Containers: deploying containerized apps on Workers platform, configuring container-enabled Durable Objects, managing container lifecycle, or implementing stateful/stateless container patter...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/cron-triggers/README.md": "# Cloudflare Cron Triggers\n\nSchedule Workers execution using cron expressions. Runs on Cloudflare's global network during underutilized periods.\n\n## Key Features\n\n- **UTC-only execution** - All schedules run on UTC time\n- **5-field cron syntax** - Quartz scheduler extensions (L, W, #)\n- **Global propagation** - 15min deployment delay\n- **At-least-once delivery** - Rare duplicate executions possible\n- **Workflow integration** - Trigger long-running multi-step tasks\n\n## Cron Syntax\n\n```\n ┌─────────── minute (0-59)\n │ ┌───────── hour (0-23)\n │ │ ┌─────── day of month (1-31)\n │ │ │ ┌───── month (1-12, JAN-DEC)\n │ │ │ │ ┌─── day of week (1-7, SUN-SAT, 1=Sunday)\n * * * * *\n```\n\n**Special chars:** `*` (any), `,` (list), `-` (range), `/` (step), `L` (last), `W` (weekday), `#` (nth)\n\n## Common Schedules\n\n```bash\n*/5 * * * *        # Every 5 minutes\n0 * * * *          # Hourly\n0 2 * * *          # Daily 2am UTC (off-peak)\n0 9 * * MON-FRI    # Weekdays 9am UTC\n0 0 1 * *          # Monthly 1st midnight UTC\n0 9 L * *          # Last day of month 9am UTC\n0 10 * * MON#2     # 2nd Monday 10am UTC\n*/10 9-17 * * MON-FRI  # Every 10min, 9am-5pm weekdays\n```\n\n## Quick Start\n\n**wrangler.jsonc:**\n\n```jsonc\n{\n  \"name\": \"my-cron-worker\",\n  \"triggers\": {\n    \"crons\": [\"*/5 * * * *\", \"0 2 * * *\"]\n  }\n}\n```\n\n**Handler:**\n\n```typescript\nexport default {\n  async scheduled(\n    controller: ScheduledController,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<void> {\n    console.log(\"Cron:\", controller.cron);\n    console.log(\"Time:\", new Date(controller.scheduledTime));\n    \n    ctx.waitUntil(asyncTask(env)); // Non-blocking\n  },\n};\n```\n\n**Test locally:**\n\n```bash\nnpx wrangler dev\ncurl \"http://localhost:8787/__scheduled?cron=*/5+*+*+*+*\"\n```\n\n## Limits\n\n- **Free:** 3 triggers/worker, 10ms CPU\n- **Paid:** Unlimited triggers, 50ms CPU\n- **Propagation:** 15min global deployment\n- **Timezone:** UTC only\n\n## See Also\n\n- [configuration.md](./configuration.md) - wrangler config, env-specific schedules\n- [api.md](./api.md) - ScheduledController, handler params\n- [patterns.md](./patterns.md) - Use cases, batch processing, monitoring\n- [gotchas.md](./gotchas.md) - Timezone issues, debugging, limits\n",
        ".agent/services/hosting/cloudflare-platform/references/d1/README.md": "# Cloudflare D1 Database\n\nExpert guidance for Cloudflare D1, a serverless SQLite database designed for horizontal scale-out across multiple databases.\n\n## Overview\n\nD1 is Cloudflare's managed, serverless database with:\n- SQLite SQL semantics and compatibility\n- Built-in disaster recovery via Time Travel (30-day point-in-time recovery)\n- Horizontal scale-out architecture (10 GB per database)\n- Worker and HTTP API access\n- Pricing based on query and storage costs only\n\n**Architecture Philosophy**: D1 is optimized for per-user, per-tenant, or per-entity database patterns rather than single large databases.\n\n## Quick Start\n\n```bash\n# Create database\nwrangler d1 create <database-name>\n\n# Execute migration\nwrangler d1 execute <db-name> --remote --file=./migrations/0001_schema.sql\n\n# Local development\nwrangler dev\n```\n\n## Core Query Methods\n\n```typescript\n// .all() - Returns all rows; .first() - First row or null; .first(col) - Single column value\n// .run() - INSERT/UPDATE/DELETE; .raw() - Array of arrays (efficient)\nconst { results, success, meta } = await env.DB.prepare('SELECT * FROM users WHERE active = ?').bind(true).all();\nconst user = await env.DB.prepare('SELECT * FROM users WHERE id = ?').bind(userId).first();\n```\n\n## Batch Operations\n\n```typescript\n// Multiple queries in single round trip (atomic transaction)\nconst results = await env.DB.batch([\n  env.DB.prepare('SELECT * FROM users WHERE id = ?').bind(1),\n  env.DB.prepare('SELECT * FROM posts WHERE author_id = ?').bind(1),\n  env.DB.prepare('UPDATE users SET last_access = ? WHERE id = ?').bind(Date.now(), 1)\n]);\n```\n\n## Platform Limits\n\n| Limit | Value |\n|-------|-------|\n| Database size | 10 GB per database |\n| Row size | 1 MB maximum |\n| Query timeout | 30 seconds |\n| Batch size | 10,000 statements |\n| Time Travel retention | 30 days |\n\n## CLI Commands\n\n```bash\n# Database management\nwrangler d1 create <db-name>\nwrangler d1 list\nwrangler d1 delete <db-name>\n\n# Execute queries\nwrangler d1 execute <db-name> --remote --command=\"SELECT * FROM users\"\nwrangler d1 execute <db-name> --local --file=./migrations/0001_schema.sql\n\n# Backups\nwrangler d1 export <db-name> --remote --output=./backup.sql\nwrangler d1 time-travel restore <db-name> --timestamp=\"2024-01-15T14:30:00Z\"\n\n# Development\nwrangler dev --persist-to=./.wrangler/state\n```\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - wrangler.toml setup, TypeScript types, binding configuration\n- [api.md](./api.md) - D1Database API, prepared statements, batch operations, testing\n- [patterns.md](./patterns.md) - Pagination, bulk operations, caching, multi-tenant patterns\n- [gotchas.md](./gotchas.md) - SQL injection prevention, error handling, performance pitfalls\n\n## See Also\n\n- [workers](../workers/) - Worker runtime and fetch handler patterns\n- [kv](../kv/) - Workers KV for caching D1 results\n- [r2](../r2/) - R2 for storing binary data instead of D1 BLOBs\n- [queues](../queues/) - Queue writes to D1 for high-throughput scenarios\n- [durable-objects](../durable-objects/) - Coordinate D1 writes with strong consistency\n",
        ".agent/services/hosting/cloudflare-platform/references/ddos/README.md": "# Cloudflare DDoS Protection\n\nAutonomous, always-on protection against DDoS attacks across L3/4 and L7.\n\n## Protection Types\n\n- **HTTP DDoS (L7)**: Protects HTTP/HTTPS traffic, phase `ddos_l7`, zone/account level\n- **Network DDoS (L3/4)**: UDP/SYN/DNS floods, phase `ddos_l4`, account level only\n- **Adaptive DDoS**: Learns 7-day baseline, detects deviations (Enterprise)\n\n## Key Features\n\n- Always-on managed rulesets (cannot disable)\n- Sensitivity levels: `default` (high), `medium`, `low`, `eoff` (essentially off)\n- Actions: `block`, `managed_challenge`, `challenge`, `log` (Enterprise Advanced only)\n- Override by category/tag or individual rule ID\n- Custom expressions for traffic filtering (Enterprise Advanced)\n- Alert configuration for attack notifications\n\n## Rule Limits\n\n- Free/Pro/Business: 1 override rule\n- Enterprise Advanced: Up to 10 rules\n\n## Scope Hierarchy\n\nZone overrides take precedence over account overrides.\n\n## See Also\n\n- [configuration.md](./configuration.md) - Dashboard & rule setup\n- [api.md](./api.md) - API endpoints & management\n- [patterns.md](./patterns.md) - Protection strategies\n- [gotchas.md](./gotchas.md) - False positives & tuning\n",
        ".agent/services/hosting/cloudflare-platform/references/do-storage/README.md": "# Cloudflare Durable Objects Storage\n\nPersistent storage API for Durable Objects with SQLite and KV backends, PITR, and automatic concurrency control.\n\n## Overview\n\nDO Storage provides:\n- SQLite-backed (recommended) or KV-backed\n- SQL API + synchronous/async KV APIs\n- Automatic input/output gates (race-free)\n- 30-day point-in-time recovery (PITR)\n- Transactions and alarms\n\n**Use cases:** Stateful coordination, real-time collaboration, counters, sessions, rate limiters\n\n## Quick Start\n\n```typescript\nexport class Counter extends DurableObject {\n  sql: SqlStorage;\n  \n  constructor(ctx: DurableObjectState, env: Env) {\n    super(ctx, env);\n    this.sql = ctx.storage.sql;\n    this.sql.exec('CREATE TABLE IF NOT EXISTS data(key TEXT PRIMARY KEY, value INTEGER)');\n  }\n  \n  async increment(): Promise<number> {\n    this.sql.exec('INSERT OR REPLACE INTO data VALUES (?, ?) RETURNING value', 'counter', \n      (this.sql.exec('SELECT value FROM data WHERE key = ?', 'counter').one()?.value || 0) + 1);\n  }\n}\n```\n\n## Storage Backends\n\n| Backend | Create Method | APIs | PITR |\n|---------|---------------|------|------|\n| SQLite (recommended) | `new_sqlite_classes` | SQL + sync KV + async KV | ✅ |\n| KV (legacy) | `new_classes` | async KV only | ❌ |\n\n## Core APIs\n\n- **SQL API** (`ctx.storage.sql`): Full SQLite with extensions (FTS5, JSON, math)\n- **Sync KV** (`ctx.storage.kv`): Synchronous key-value (SQLite only)\n- **Async KV** (`ctx.storage`): Asynchronous key-value (both backends)\n- **Transactions** (`transactionSync()`, `transaction()`)\n- **PITR** (`getBookmarkForTime()`, `onNextSessionRestoreBookmark()`)\n- **Alarms** (`setAlarm()`, `alarm()` handler)\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - wrangler.jsonc migrations, SQLite vs KV setup\n- [api.md](./api.md) - SQL exec/cursors, KV methods, transactions, alarms, PITR\n- [patterns.md](./patterns.md) - Schema migrations, caching, rate limiting, batch processing\n- [gotchas.md](./gotchas.md) - Concurrency gates, transaction rules, SQL limits, async pitfalls\n\n## See Also\n\n- [durable-objects](../durable-objects/) - DO fundamentals and coordination patterns\n- [workers](../workers/) - Worker runtime for DO stubs\n- [d1](../d1/) - Shared database alternative to per-DO storage\n",
        ".agent/services/hosting/cloudflare-platform/references/durable-objects/README.md": "# Cloudflare Durable Objects\n\nExpert guidance for building stateful applications with Cloudflare Durable Objects.\n\n## Overview\n\nDurable Objects combine compute with storage in globally-unique, strongly-consistent packages:\n- **Globally unique instances**: Each DO has unique ID for multi-client coordination\n- **Co-located storage**: Fast, strongly-consistent storage with compute\n- **Automatic placement**: Objects spawn near first request location\n- **Stateful serverless**: In-memory state + persistent storage\n- **Single-threaded**: Serial request processing (no race conditions)\n\n## When to Use DOs\n\nUse DOs for **stateful coordination**, not stateless request handling:\n- **Coordination**: Multiple clients interacting with shared state (chat rooms, multiplayer games)\n- **Strong consistency**: Operations must serialize to avoid races (booking systems, inventory)\n- **Per-entity storage**: Each user/tenant/resource needs isolated database (multi-tenant SaaS)\n- **Persistent connections**: Long-lived WebSockets that survive across requests\n- **Per-entity scheduled work**: Each entity needs its own timer (subscription renewals, game timeouts)\n\n## When NOT to Use DOs\n\n| Scenario | Use Instead |\n|----------|-------------|\n| Stateless request handling | Workers |\n| Maximum global distribution | Workers |\n| High fan-out (independent requests) | Workers |\n| Global singleton handling all traffic | Shard across multiple DOs |\n| High-frequency pub/sub | Queues |\n| Long-running continuous processes | Workers + Alarms |\n| Chatty microservice (every request) | Reconsider architecture |\n| Eventual consistency OK, read-heavy | KV |\n| Relational queries across entities | D1 |\n\n## Design Heuristics\n\nModel each DO around your **atom of coordination**—the logical unit needing serialized access (user, room, document, session).\n\n| Characteristic | Feels Right | Question It | Reconsider |\n|----------------|-------------|-------------|------------|\n| Requests/sec (sustained) | < 100 | 100-500 | > 500 |\n| Storage keys | < 100 | 100-1000 | > 1000 |\n| Total state size | < 10MB | 10MB-100MB | > 1GB |\n| Alarm frequency | Minutes-hours | Every 30s | Every few seconds |\n| WebSocket duration | Short bursts | Hours (hibernating) | Days always-on |\n| Fan-out from this DO | Never/rarely | To < 10 DOs | To 100+ DOs |\n\n## Core Concepts\n\n### Class Structure\n\nAll DOs extend `DurableObject` base class with constructor receiving `DurableObjectState` (storage, WebSockets, alarms) and `Env` (bindings).\n\n### Accessing from Workers\n\nWorkers use bindings to get stubs, then call RPC methods directly (recommended) or use fetch handler (legacy).\n\n### ID Generation\n\n- `idFromName()`: Deterministic, named coordination\n- `newUniqueId()`: Random IDs for sharding\n- `idFromString()`: Derive from existing IDs\n- Jurisdiction option: Data locality\n\n### Storage Options\n\n- **SQLite** (recommended): Structured data, transactions, 10GB/DO\n- **Synchronous KV API**: Simple key-value on SQLite objects\n- **Asynchronous KV API**: Legacy/advanced use cases\n\n### Special Features\n\n- **Alarms**: Schedule future execution per-DO\n- **WebSocket Hibernation**: Zero-cost idle connections\n- **Point-in-Time Recovery**: Restore to any point in 30 days\n\n## Quick Start\n\n```typescript\nimport { DurableObject } from \"cloudflare:workers\";\n\nexport class Counter extends DurableObject<Env> {\n  async increment(): Promise<number> {\n    const result = this.ctx.storage.sql.exec(\n      `INSERT INTO counters (id, value) VALUES (1, 1)\n       ON CONFLICT(id) DO UPDATE SET value = value + 1\n       RETURNING value`\n    ).one();\n    return result.value;\n  }\n}\n\n// Worker access\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const id = env.COUNTER.idFromName(\"global\");\n    const stub = env.COUNTER.get(id);\n    const count = await stub.increment();\n    return new Response(`Count: ${count}`);\n  }\n};\n```\n\n## Essential Commands\n\n```bash\nnpx wrangler dev              # Local dev with DOs\nnpx wrangler dev --remote     # Test against prod DOs\nnpx wrangler deploy           # Deploy + auto-apply migrations\n```\n\n## Resources\n\n**Docs**: https://developers.cloudflare.com/durable-objects/  \n**API Reference**: https://developers.cloudflare.com/durable-objects/api/  \n**Examples**: https://developers.cloudflare.com/durable-objects/examples/\n\n## In This Reference\n\n- [Configuration](./configuration.md) - wrangler.jsonc setup, migrations, bindings\n- [API](./api.md) - Class structure, storage APIs, alarms, WebSockets\n- [Patterns](./patterns.md) - Rate limiting, locks, real-time collab, sessions\n- [Gotchas](./gotchas.md) - Limits, common issues, troubleshooting\n\n## See Also\n\n- [Workers](../workers/README.md) - Core Workers runtime\n- [DO Storage](../do-storage/README.md) - Deep dive on storage APIs\n",
        ".agent/services/hosting/cloudflare-platform/references/email-routing/README.md": "# Cloudflare Email Routing Skill Reference\n\n## Overview\n\nCloudflare Email Routing enables custom email addresses for your domain that route to verified destination addresses. It's free, privacy-focused (no storage/access), and includes Email Workers for programmatic email processing.\n\n**Available to all Cloudflare customers using Cloudflare a...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/email-workers/README.md": "# Cloudflare Email Workers Skill\n\nExpert guidance for building, configuring, and deploying Cloudflare Email Workers.\n\n## Overview\n\nEmail Workers let you programmatically process incoming emails using Cloudflare Workers runtime. Use them to build custom email routing logic, spam filters, auto-responders, ticket systems, notification handlers, and more.\n\n## Core Architecture\n\n### Event Handler (ES Modules)\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    // Process email\n    await message.forward(\"destination@example.com\");\n  },\n};\n```\n\n### Event Handler (Service Worker - Deprecated)\n\n```typescript\naddEventListener(\"email\", async (event) => {\n  await event.message.forward(\"destination@example.com\");\n});\n```\n\n**Use ES modules format for all new projects.**\n\n## ForwardableEmailMessage API\n\n### Properties\n\n```typescript\ninterface ForwardableEmailMessage {\n  readonly from: string;        // Envelope From\n  readonly to: string;          // Envelope To\n  readonly headers: Headers;    // Message headers\n  readonly raw: ReadableStream; // Raw message stream\n  readonly rawSize: number;     // Message size in bytes\n  \n  setReject(reason: string): void;\n  forward(rcptTo: string, headers?: Headers): Promise<void>;\n  reply(message: EmailMessage): Promise<void>;\n}\n```\n\n### Key Methods\n\n- **`setReject(reason)`**: Reject with permanent SMTP error\n- **`forward(rcptTo, headers?)`**: Forward to verified destination (only `X-*` headers allowed)\n- **`reply(message)`**: Reply to sender with new EmailMessage\n\n### EmailMessage for Sending\n\n```typescript\ninterface EmailMessage {\n  readonly from: string;\n  readonly to: string;\n}\n\n// Usage\nimport { EmailMessage } from \"cloudflare:email\";\nconst msg = new EmailMessage(from, to, rawMimeContent);\n```\n\n## Common Patterns\n\n### 1. Allowlist\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    const allowList = [\"friend@example.com\", \"coworker@example.com\"];\n    if (!allowList.includes(message.from)) {\n      message.setReject(\"Address not allowed\");\n    } else {\n      await message.forward(\"inbox@corp.example.com\");\n    }\n  },\n};\n```\n\n### 2. Blocklist\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    const blockList = [\"spam@example.com\", \"badactor@example.com\"];\n    if (blockList.includes(message.from)) {\n      message.setReject(\"Blocked sender\");\n    } else {\n      await message.forward(\"inbox@corp.example.com\");\n    }\n  },\n};\n```\n\n### 3. Parse Email with postal-mime\n\n```typescript\nimport * as PostalMime from 'postal-mime';\n\nexport default {\n  async email(message, env, ctx) {\n    const parser = new PostalMime.default();\n    const rawEmail = new Response(message.raw);\n    const email = await parser.parse(await rawEmail.arrayBuffer());\n    \n    // email contains: headers, from, to, subject, html, text, attachments\n    console.log(email.subject, email.from);\n    \n    await message.forward(\"inbox@example.com\");\n  },\n};\n```\n\n### 4. Auto-Reply\n\n```typescript\nimport { EmailMessage } from \"cloudflare:email\";\nimport { createMimeMessage } from 'mimetext';\n\nexport default {\n  async email(message, env, ctx) {\n    const msg = createMimeMessage();\n    msg.setSender({ name: 'Support Team', addr: 'support@example.com' });\n    msg.setRecipient(message.from);\n    msg.setHeader('In-Reply-To', message.headers.get('Message-ID'));\n    msg.setSubject('Re: Your inquiry');\n    msg.addMessage({\n      contentType: 'text/plain',\n      data: 'Thank you for contacting us. We will respond within 24 hours.',\n    });\n\n    const replyMessage = new EmailMessage(\n      'support@example.com',\n      message.from,\n      msg.asRaw()\n    );\n\n    await message.reply(replyMessage);\n    await message.forward(\"team@example.com\");\n  },\n};\n```\n\n### 5. Conditional Routing by Subject\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    const subject = message.headers.get('Subject') || '';\n    \n    if (subject.toLowerCase().includes('billing')) {\n      await message.forward(\"billing@example.com\");\n    } else if (subject.toLowerCase().includes('support')) {\n      await message.forward(\"support@example.com\");\n    } else {\n      await message.forward(\"general@example.com\");\n    }\n  },\n};\n```\n\n### 6. Store Email in KV/R2\n\n```typescript\nimport * as PostalMime from 'postal-mime';\n\nexport default {\n  async email(message, env, ctx) {\n    const parser = new PostalMime.default();\n    const rawEmail = new Response(message.raw);\n    const email = await parser.parse(await rawEmail.arrayBuffer());\n    \n    // Store in KV\n    const key = `email:${Date.now()}:${message.from}`;\n    await env.EMAIL_ARCHIVE.put(key, JSON.stringify({\n      from: email.from,\n      subject: email.subject,\n      receivedAt: new Date().toISOString(),\n    }));\n    \n    await message.forward(\"inbox@example.com\");\n  },\n};\n```\n\n### 7. Webhook Notification\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    const subject = message.headers.get('Subject');\n    \n    // Notify via webhook\n    ctx.waitUntil(\n      fetch('https://hooks.slack.com/services/YOUR/WEBHOOK/URL', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n          text: `New email from ${message.from}: ${subject}`,\n        }),\n      })\n    );\n    \n    await message.forward(\"inbox@example.com\");\n  },\n};\n```\n\n### 8. Size-Based Filtering\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    const MAX_SIZE = 10 * 1024 * 1024; // 10 MB\n    \n    if (message.rawSize > MAX_SIZE) {\n      message.setReject(\"Message too large\");\n    } else {\n      await message.forward(\"inbox@example.com\");\n    }\n  },\n};\n```\n\n## Wrangler Configuration\n\n### Minimal Config (Local Dev)\n\n```jsonc\n// wrangler.jsonc\n{\n  \"send_email\": [\n    {\n      \"name\": \"EMAIL\"\n    }\n  ]\n}\n```\n\n```toml\n# wrangler.toml\n[[send_email]]\nname = \"EMAIL\"\n```\n\n### Full Production Config\n\n```toml\nname = \"email-worker\"\nmain = \"src/index.ts\"\ncompatibility_date = \"2024-01-01\"\n\n# Email binding\n[[send_email]]\nname = \"EMAIL\"\n\n# Add KV for email archival\n[[kv_namespaces]]\nbinding = \"EMAIL_ARCHIVE\"\nid = \"your-kv-namespace-id\"\n\n# Add secrets for API keys\n[vars]\nWEBHOOK_URL = \"https://example.com/webhook\"\n```\n\n## Local Development\n\n### Test Receiving Email\n\n```bash\nnpx wrangler dev\n```\n\n```bash\ncurl --request POST 'http://localhost:8787/cdn-cgi/handler/email' \\\n  --url-query 'from=sender@example.com' \\\n  --url-query 'to=recipient@example.com' \\\n  --header 'Content-Type: application/json' \\\n  --data-raw 'From: sender@example.com\nTo: recipient@example.com\nSubject: Test Email\n\nHello world'\n```\n\n### Test Sending Email\n\nWrangler writes sent emails to local `.eml` files:\n\n```typescript\nimport { EmailMessage } from \"cloudflare:email\";\nimport { createMimeMessage } from 'mimetext';\n\nexport default {\n  async fetch(request, env, ctx) {\n    const msg = createMimeMessage();\n    msg.setSender({ name: 'Test', addr: 'sender@example.com' });\n    msg.setRecipient('recipient@example.com');\n    msg.setSubject('Test from Worker');\n    msg.addMessage({\n      contentType: 'text/plain',\n      data: 'Hello from Email Worker',\n    });\n\n    const message = new EmailMessage(\n      'sender@example.com',\n      'recipient@example.com',\n      msg.asRaw()\n    );\n    await env.EMAIL.send(message);\n    \n    return Response.json({ ok: true });\n  }\n};\n```\n\nVisit `http://localhost:8787/` to trigger. Check terminal for `.eml` file path.\n\n## Deployment\n\n### Prerequisites\n\n1. Enable Email Routing in Cloudflare dashboard\n2. Add verified destination address\n3. Configure wrangler.toml\n\n### Deploy\n\n```bash\nnpx wrangler deploy\n```\n\n### Bind to Route\n\nIn Cloudflare dashboard:\n1. Go to Email Routing → Email Workers\n2. Create route (e.g., `hello@yourdomain.com`)\n3. Bind route to your deployed Worker\n\n## Limits\n\n| Limit | Value |\n|-------|-------|\n| Max message size | 25 MiB |\n| Max rules | 200 |\n| Max destination addresses | 200 |\n| Workers CPU (free tier) | Limited (upgrade for more) |\n\n### CPU Limit Errors\n\nMonitor with `wrangler tail`:\n\n```bash\nnpx wrangler tail\n```\n\nLook for `EXCEEDED_CPU` errors. Consider:\n- Upgrading to Workers Paid plan\n- Optimizing email parsing logic\n- Using `ctx.waitUntil()` for non-critical operations\n\n## Best Practices\n\n### 1. Use Verified Destinations Only\n\n`forward()` only works with verified destination addresses in your Cloudflare account.\n\n### 2. Handle Large Emails\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    if (message.rawSize > 20 * 1024 * 1024) {\n      // Don't parse huge emails synchronously\n      ctx.waitUntil(processLargeEmail(message, env));\n      await message.forward(\"inbox@example.com\");\n      return;\n    }\n    \n    // Normal processing\n  },\n};\n```\n\n### 3. Use ctx.waitUntil for Async Operations\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    // Forward immediately\n    await message.forward(\"inbox@example.com\");\n    \n    // Non-blocking operations\n    ctx.waitUntil(\n      Promise.all([\n        logToAnalytics(message),\n        notifySlack(message),\n        updateDatabase(message),\n      ])\n    );\n  },\n};\n```\n\n### 4. Add Custom Headers When Forwarding\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    const customHeaders = new Headers();\n    customHeaders.set('X-Processed-By', 'Email-Worker');\n    customHeaders.set('X-Original-To', message.to);\n    \n    await message.forward(\"inbox@example.com\", customHeaders);\n  },\n};\n```\n\n### 5. Parse Headers Safely\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    const subject = message.headers.get('Subject') || '(no subject)';\n    const messageId = message.headers.get('Message-ID') || '';\n    \n    // Avoid throwing on missing headers\n  },\n};\n```\n\n### 6. Type Safety\n\n```typescript\ninterface Env {\n  EMAIL: SendEmail;\n  EMAIL_ARCHIVE: KVNamespace;\n  WEBHOOK_URL: string;\n}\n\nexport default {\n  async email(message: ForwardableEmailMessage, env: Env, ctx: ExecutionContext) {\n    // Fully typed\n  },\n};\n```\n\n## Common Use Cases\n\n1. **Spam/Allowlist Filtering**: Block/allow senders\n2. **Auto-Responders**: Reply with canned responses\n3. **Ticket Creation**: Parse email, create support ticket\n4. **Email Archival**: Store emails in KV/R2/D1\n5. **Notification Routing**: Forward to Slack/Discord/webhooks\n6. **Size Filtering**: Reject oversized attachments\n7. **Domain Routing**: Route by sender domain\n8. **Subject-Based Routing**: Route by keywords in subject\n9. **Attachment Handling**: Extract and store attachments\n10. **Email Analytics**: Track email metrics\n\n## Dependencies\n\n### Recommended npm Packages\n\n```json\n{\n  \"dependencies\": {\n    \"postal-mime\": \"^2.3.3\",    // Parse incoming emails\n    \"mimetext\": \"^4.0.0\"         // Compose outgoing emails\n  },\n  \"devDependencies\": {\n    \"@cloudflare/workers-types\": \"^4.0.0\",\n    \"wrangler\": \"^3.0.0\"\n  }\n}\n```\n\n## Troubleshooting\n\n### Email Not Forwarding\n\n- Verify destination address in Cloudflare dashboard\n- Check Email Routing is enabled\n- Verify route binding in dashboard\n- Check `wrangler tail` for errors\n\n### CPU Limit Errors\n\n- Upgrade to Workers Paid plan\n- Use `ctx.waitUntil()` for heavy operations\n- Avoid synchronous parsing of large emails\n\n### Local Dev Not Working\n\n- Ensure `send_email` binding in wrangler config\n- Use correct curl format with `--data-raw`\n- Check wrangler version (`npx wrangler --version`)\n\n## Advanced Patterns\n\n### Multi-Tenant Email Processing\n\n```typescript\nexport default {\n  async email(message, env, ctx) {\n    const [localPart, domain] = message.to.split('@');\n    \n    // Route based on subdomain or local part\n    const tenantId = extractTenantId(localPart);\n    const config = await env.TENANT_CONFIG.get(tenantId, 'json');\n    \n    if (config?.forwardTo) {\n      await message.forward(config.forwardTo);\n    } else {\n      message.setReject(\"Unknown recipient\");\n    }\n  },\n};\n```\n\n### Attachment Extraction\n\n```typescript\nimport * as PostalMime from 'postal-mime';\n\nexport default {\n  async email(message, env, ctx) {\n    const parser = new PostalMime.default();\n    const rawEmail = new Response(message.raw);\n    const email = await parser.parse(await rawEmail.arrayBuffer());\n    \n    // Process attachments\n    for (const attachment of email.attachments) {\n      const key = `attachments/${Date.now()}-${attachment.filename}`;\n      ctx.waitUntil(\n        env.ATTACHMENTS.put(key, attachment.content, {\n          metadata: {\n            contentType: attachment.mimeType,\n            from: email.from.address,\n          },\n        })\n      );\n    }\n    \n    await message.forward(\"inbox@example.com\");\n  },\n};\n```\n\n### Conditional Auto-Reply with Rate Limiting\n\n```typescript\nimport { EmailMessage } from \"cloudflare:email\";\nimport { createMimeMessage } from 'mimetext';\n\nexport default {\n  async email(message, env, ctx) {\n    const rateKey = `rate:${message.from}`;\n    const lastReply = await env.RATE_LIMIT.get(rateKey);\n    \n    if (!lastReply) {\n      // Send auto-reply\n      const msg = createMimeMessage();\n      msg.setSender({ name: 'Auto Reply', addr: 'noreply@example.com' });\n      msg.setRecipient(message.from);\n      msg.setSubject('Received your message');\n      msg.addMessage({\n        contentType: 'text/plain',\n        data: 'Thank you for contacting us.',\n      });\n      \n      const reply = new EmailMessage('noreply@example.com', message.from, msg.asRaw());\n      await message.reply(reply);\n      \n      // Rate limit: 1 reply per hour\n      ctx.waitUntil(\n        env.RATE_LIMIT.put(rateKey, Date.now().toString(), { expirationTtl: 3600 })\n      );\n    }\n    \n    await message.forward(\"inbox@example.com\");\n  },\n};\n```\n\n## Related Documentation\n\n- [Email Routing Setup](https://developers.cloudflare.com/email-routing/get-started/enable-email-routing/)\n- [Workers Platform](https://developers.cloudflare.com/workers/)\n- [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/)\n- [Workers Limits](https://developers.cloudflare.com/workers/platform/limits/)\n",
        ".agent/services/hosting/cloudflare-platform/references/hyperdrive/README.md": "# Hyperdrive\n\nAccelerates database queries from Workers via connection pooling, edge setup, query caching.\n\n## Key Features\n\n- **Connection Pooling**: Persistent connections eliminate TCP/TLS/auth handshakes (~7 round-trips)\n- **Edge Setup**: Connection negotiation at edge, pooling near origin\n- **Query Caching**: Auto-cache non-mutating queries (default 60s TTL)\n- **Support**: PostgreSQL, MySQL + compatibles (CockroachDB, Timescale, PlanetScale, Neon, Supabase)\n\n## Architecture\n\n```\nWorker → Edge (setup) → Pool (near DB) → Origin\n         ↓ cached reads\n         Cache\n```\n\n## Quick Start\n\n```bash\n# Create config\nnpx wrangler hyperdrive create my-db \\\n  --connection-string=\"postgres://user:pass@host:5432/db\"\n\n# wrangler.jsonc\n{\n  \"compatibility_flags\": [\"nodejs_compat\"],\n  \"hyperdrive\": [{\"binding\": \"HYPERDRIVE\", \"id\": \"<ID>\"}]\n}\n```\n\n```typescript\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n    await client.connect();\n    const result = await client.query(\"SELECT * FROM users WHERE id = $1\", [123]);\n    await client.end();\n    return Response.json(result.rows);\n  },\n};\n```\n\n## When to Use\n\n✅ Global access to single-region DBs, high read ratios, popular queries, connection-heavy loads\n❌ Write-heavy, real-time data (<1s), single-region apps close to DB\n\n## See Also\n\n- [configuration.md](./configuration.md) - Setup, wrangler config\n- [api.md](./api.md) - Binding APIs, query patterns\n- [patterns.md](./patterns.md) - Use cases, ORMs\n- [gotchas.md](./gotchas.md) - Limits, troubleshooting\n\n[Docs](https://developers.cloudflare.com/hyperdrive/) | [Discord #hyperdrive](https://discord.cloudflare.com)\n",
        ".agent/services/hosting/cloudflare-platform/references/images/README.md": "# Cloudflare Images Skill Reference\n\n**Cloudflare Images** is an end-to-end image management solution providing storage, transformation, optimization, and delivery at scale via Cloudflare's global network....\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/kv/README.md": "# Cloudflare Workers KV\n\nGlobally-distributed, eventually-consistent key-value store optimized for high read volume and low latency.\n\n## Overview\n\nKV provides:\n- Eventual consistency (60s global propagation)\n- Read-optimized performance\n- 25 MiB value limit per key\n- Auto-replication to Cloudflare edge\n- Metadata support (1024 bytes)\n\n**Use cases:** Config storage, user sessions, feature flags, caching, A/B testing\n\n## Quick Start\n\n```bash\nwrangler kv namespace create MY_NAMESPACE\n# Add binding to wrangler.jsonc\n```\n\n```typescript\n// Write\nawait env.MY_KV.put(\"key\", \"value\", { expirationTtl: 300 });\n\n// Read\nconst value = await env.MY_KV.get(\"key\");\nconst json = await env.MY_KV.get<Config>(\"config\", \"json\");\n```\n\n## Core Operations\n\n| Method | Purpose | Returns |\n|--------|---------|---------|\n| `get(key, type?)` | Single read | `string \\| null` |\n| `get(keys, type?)` | Bulk read (≤100) | `Map<string, T \\| null>` |\n| `put(key, value, options?)` | Write | `Promise<void>` |\n| `delete(key)` | Delete | `Promise<void>` |\n| `list(options?)` | List keys | `{ keys, list_complete, cursor? }` |\n| `getWithMetadata(key)` | Get + metadata | `{ value, metadata }` |\n\n## Consistency Model\n\n- **Write visibility:** Immediate in same location, ≤60s globally\n- **Read path:** Eventually consistent\n- **Write rate:** 1 write/second per key (429 on exceed)\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - wrangler.jsonc setup, namespace creation, TypeScript types\n- [api.md](./api.md) - KV methods, bulk operations, cacheTtl, content types\n- [patterns.md](./patterns.md) - Caching, sessions, rate limiting, A/B testing\n- [gotchas.md](./gotchas.md) - Eventual consistency, concurrent writes, value limits\n\n## See Also\n\n- [workers](../workers/) - Worker runtime for KV access\n- [d1](../d1/) - Use D1 for strong consistency needs\n- [durable-objects](../durable-objects/) - Strongly consistent alternative\n",
        ".agent/services/hosting/cloudflare-platform/references/miniflare/README.md": "# Miniflare\n\nLocal simulator for Cloudflare Workers development/testing. Runs Workers in workerd sandbox implementing runtime APIs - no internet required.\n\n## Features\n\n- Full-featured: KV, Durable Objects, R2, D1, WebSockets, Queues\n- Fully-local: test without internet, instant reload\n- TypeScript-native: detailed logging, source maps\n- Advanced testing: dispatch events without HTTP, simulate Worker connections\n\n## When to Use\n\n- Integration tests for Workers\n- Advanced use cases requiring fine-grained control\n- Testing bindings/storage locally\n- Multiple Workers with service bindings\n\n**Note:** Most users should use Wrangler. Miniflare for advanced testing.\n\n## Setup\n\n```bash\nnpm i -D miniflare\n```\n\nRequires ES modules in `package.json`:\n\n```json\n{\"type\": \"module\"}\n```\n\n## Quick Start\n\n```js\nimport { Miniflare } from \"miniflare\";\n\nconst mf = new Miniflare({\n  modules: true,\n  script: `\n    export default {\n      async fetch(request, env, ctx) {\n        return new Response(\"Hello Miniflare!\");\n      }\n    }\n  `,\n});\n\nconst res = await mf.dispatchFetch(\"http://localhost:8787/\");\nconsole.log(await res.text()); // Hello Miniflare!\nawait mf.dispose();\n```\n\n## See Also\n\n- [configuration.md](./configuration.md) - Config options, bindings, wrangler.toml\n- [api.md](./api.md) - Programmatic API, methods, event dispatching\n- [patterns.md](./patterns.md) - Testing patterns, CI, mocking\n- [gotchas.md](./gotchas.md) - Compatibility issues, limits, debugging\n\n## Resources\n\n- [Miniflare Docs](https://developers.cloudflare.com/workers/testing/miniflare/)\n- [Miniflare GitHub](https://github.com/cloudflare/workers-sdk/tree/main/packages/miniflare)\n- [Vitest Integration](https://developers.cloudflare.com/workers/testing/vitest-integration/) (recommended)\n",
        ".agent/services/hosting/cloudflare-platform/references/network-interconnect/README.md": "# Cloudflare Network Interconnect (CNI)\n\nPrivate, high-performance connectivity to Cloudflare's network. **Enterprise-only**.\n\n## Connection Types\n\n**Direct**: Physical fiber in shared datacenter. 10/100 Gbps. You order cross-connect.\n\n**Partner**: Virtual via Console Connect, Equinix, Megaport, etc. Managed via partner SDN.\n\n**Cloud**: AWS Direct Connect or GCP Cloud Interconnect. Magic WAN only.\n\n## Dataplane Versions\n\n**v1 (Classic)**: GRE tunnel support, VLAN/BFD/LACP, asymmetric MTU (1500↓/1476↑), peering support.\n\n**v2 (Beta)**: No GRE, 1500 MTU both ways, no VLAN/BFD/LACP yet, ECMP instead.\n\n## Use Cases\n\n- **Magic Transit DSR**: DDoS protection, egress via ISP (v1/v2)\n- **Magic Transit + Egress**: DDoS + egress via CF (v1/v2)\n- **Magic WAN + Zero Trust**: Private backbone (v1 needs GRE, v2 native)\n- **Peering**: Public routes at PoP (v1 only)\n- **App Security**: WAF/Cache/LB (v1/v2 over Magic Transit)\n\n## Prerequisites\n\n- Enterprise plan\n- IPv4 /24+ or IPv6 /48+ prefixes\n- BGP ASN for v1\n- See [locations PDF](https://developers.cloudflare.com/network-interconnect/static/cni-locations-30-10-2025.pdf)\n\n## Specs\n\n- /31 point-to-point subnets\n- 10km max optical distance\n- 10G: 10GBASE-LR single-mode\n- 100G: 100GBASE-LR4 single-mode\n- **No SLA** (free service)\n- Backup Internet required\n\n## Throughput\n\n| Direction | 10G | 100G |\n|-----------|-----|------|\n| CF → Customer | 10 Gbps | 100 Gbps |\n| Customer → CF (peering) | 10 Gbps | 100 Gbps |\n| Customer → CF (Magic) | 1 Gbps/tunnel or CNI | 1 Gbps/tunnel or CNI |\n\n## Timeline\n\n2-4 weeks typical. Steps: request → config review → order connection → configure → test → enable health checks → activate → monitor.\n\n## See Also\n\n- [configuration.md](./configuration.md) - BGP, routing, setup\n- [api.md](./api.md) - API endpoints, SDKs\n- [patterns.md](./patterns.md) - HA, hybrid cloud, failover\n- [gotchas.md](./gotchas.md) - Troubleshooting, limits\n",
        ".agent/services/hosting/cloudflare-platform/references/observability/README.md": "# Cloudflare Observability Skill Reference\n\n**Purpose**: Comprehensive guidance for implementing observability in Cloudflare Workers, covering traces, logs, metrics, and analytics.\n\n**Scope**: Cloudflare Observability features ONLY - Workers Logs, Traces, Analytics Engine, Logpush, Metrics & Analytics, and OpenTelemetry exports.\n\n---...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/pages-functions/README.md": "# Cloudflare Pages Functions\n\nServerless functions on Cloudflare Pages using Workers runtime. Full-stack dev with file-based routing.\n\n## File-Based Routing\n\n```\n/functions\n  ├── index.js              → /\n  ├── api.js                → /api\n  ├── users/\n  │   ├── index.js          → /users/\n  │   ├── [user].js         → /users/:user\n  │   └── [[catchall]].js   → /users/*\n  └── _middleware.js        → runs on all routes\n```\n\n**Rules:**\n- `index.js` → directory root\n- Trailing slash optional\n- Specific routes precede catch-alls\n- Falls back to static if no match\n\n## Dynamic Routes\n\n**Single segment** `[param]` → string:\n\n```js\n// /functions/users/[user].js\nexport function onRequest(context) {\n  return new Response(`Hello ${context.params.user}`);\n}\n// Matches: /users/nevi\n```\n\n**Multi-segment** `[[param]]` → array:\n\n```js\n// /functions/users/[[catchall]].js\nexport function onRequest(context) {\n  return new Response(JSON.stringify(context.params.catchall));\n}\n// Matches: /users/nevi/foobar → [\"nevi\", \"foobar\"]\n```\n\n## Key Features\n\n- **Method handlers:** `onRequestGet`, `onRequestPost`, etc.\n- **Middleware:** `_middleware.js` for cross-cutting concerns\n- **Bindings:** KV, D1, R2, Durable Objects, Workers AI, Service bindings\n- **TypeScript:** Full type support via `@cloudflare/workers-types`\n- **Advanced mode:** Use `_worker.js` for custom routing logic\n\n## See Also\n\n- [configuration.md](./configuration.md) - Routes, headers, redirects, wrangler config\n- [api.md](./api.md) - EventContext, handlers, bindings\n- [patterns.md](./patterns.md) - Auth, CORS, rate limiting, forms, caching\n- [gotchas.md](./gotchas.md) - Common issues, debugging, limits\n",
        ".agent/services/hosting/cloudflare-platform/references/pages/README.md": "# Cloudflare Pages\n\nJAMstack platform for full-stack apps on Cloudflare's global network.\n\n## Key Features\n\n- **Git-based deploys**: Auto-deploy from GitHub/GitLab\n- **Preview deployments**: Unique URL per branch/PR\n- **Pages Functions**: File-based serverless routing (Workers runtime)\n- **Static + dynamic**: Smart asset caching + edge compute\n- **Framework optimized**: Next.js, SvelteKit, Remix, Astro, Nuxt, Qwik\n\n## Deployment Methods\n\n### 1. Git Integration (Production)\n\nDashboard → Workers & Pages → Create → Connect to Git → Configure build\n\n### 2. Direct Upload\n\n```bash\nnpx wrangler pages deploy ./dist --project-name=my-project\nnpx wrangler pages deploy ./dist --project-name=my-project --branch=staging\n```\n\n### 3. C3 CLI\n\n```bash\nnpm create cloudflare@latest my-app\n# Select framework → auto-setup + deploy\n```\n\n## vs Workers\n\n- **Pages**: Static sites, JAMstack, frameworks, git workflow, file-based routing\n- **Workers**: Pure APIs, complex routing, WebSockets, scheduled tasks, email handlers\n- **Combine**: Pages Functions use Workers runtime, can bind to Workers\n\n## Quick Start\n\n```bash\n# Create\nnpm create cloudflare@latest\n\n# Local dev\nnpx wrangler pages dev ./dist\n\n# Deploy\nnpx wrangler pages deploy ./dist --project-name=my-project\n\n# Types\nnpx wrangler types --path='./functions/types.d.ts'\n\n# Secrets\necho \"value\" | npx wrangler pages secret put KEY --project-name=my-project\n\n# Logs\nnpx wrangler pages deployment tail --project-name=my-project\n```\n\n## Resources\n\n- [Pages Docs](https://developers.cloudflare.com/pages/)\n- [Functions API](https://developers.cloudflare.com/pages/functions/api-reference/)\n- [Framework Guides](https://developers.cloudflare.com/pages/framework-guides/)\n- [Discord #functions](https://discord.com/channels/595317990191398933/910978223968518144)\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - wrangler.jsonc, build, env vars\n- [api.md](./api.md) - Functions API, bindings, context\n- [patterns.md](./patterns.md) - Full-stack patterns, frameworks\n- [gotchas.md](./gotchas.md) - Build issues, limits, debugging\n\n## See Also\n\n- [pages-functions](../pages-functions/) - File-based routing, middleware\n- [d1](../d1/) - SQL database for Pages Functions\n- [kv](../kv/) - Key-value storage for caching/state\n",
        ".agent/services/hosting/cloudflare-platform/references/pipelines/README.md": "# Cloudflare Pipelines Skill\n\nExpert guidance for working with Cloudflare Pipelines - ETL streaming data platform for ingesting, transforming, and loading data into R2.\n\n## Overview\n\nCloudflare Pipelines ingests events, transforms them with SQL, and delivers to R2 as Apache Iceberg tables or Parquet/JSON files. It provides:\n\n- **Streams**: Durable, buffered queues for event ingestion via HTTP or Workers\n- **Pipelines**: SQL-based transformations between streams and sinks\n- **Sinks**: Destinations for processed data (R2 Data Catalog or R2 storage)\n\n**Status**: Open beta (Workers Paid plan required)\n**Pricing**: Currently no charge beyond standard R2 storage/operations\n\n## Architecture\n\n```\nData Sources → Streams → Pipelines (SQL) → Sinks → R2\n                 ↑          ↓                 ↓\n            HTTP/Workers   Transform      Iceberg/Parquet\n```\n\n### Core Components\n\n1. **Streams**: Buffer and store incoming events\n   - Structured (with schema validation) or unstructured (raw JSON)\n   - HTTP endpoints and Worker bindings\n   - Can be read by multiple pipelines\n\n2. **Pipelines**: Execute SQL transformations\n   - Filter, transform, enrich data\n   - Cannot be modified after creation (delete/recreate required)\n   - SQL reference: SELECT, INSERT, scalar functions\n\n3. **Sinks**: Write to destinations\n   - **R2 Data Catalog**: Apache Iceberg tables with ACID guarantees\n   - **R2 Storage**: Parquet or JSON files\n   - Exactly-once delivery guarantee\n\n## Common Use Cases\n\n- **Analytics pipelines**: Server logs, clickstream, telemetry\n- **Data warehousing**: ETL into queryable Iceberg tables\n- **Event processing**: Mobile/IoT events with enrichment\n- **Ecommerce analytics**: User events, purchases, product views\n- **Log aggregation**: Application/server logs with filtering\n\n## Setup & Configuration\n\n### Quick Start\n\n```bash\n# Interactive setup (recommended)\nnpx wrangler pipelines setup\n\n# Manual setup\nnpx wrangler r2 bucket create my-bucket\nnpx wrangler r2 bucket catalog enable my-bucket\nnpx wrangler pipelines streams create my-stream --schema-file schema.json\nnpx wrangler pipelines sinks create my-sink --type r2-data-catalog \\\n  --bucket my-bucket --namespace default --table my_table \\\n  --catalog-token YOUR_TOKEN\nnpx wrangler pipelines create my-pipeline \\\n  --sql \"INSERT INTO my_sink SELECT * FROM my_stream\"\n```\n\n### Schema Definition\n\n**Structured streams** (recommended for validation):\n\n```json\n{\n  \"fields\": [\n    {\n      \"name\": \"user_id\",\n      \"type\": \"string\",\n      \"required\": true\n    },\n    {\n      \"name\": \"event_type\",\n      \"type\": \"string\",\n      \"required\": true\n    },\n    {\n      \"name\": \"amount\",\n      \"type\": \"float64\",\n      \"required\": false\n    },\n    {\n      \"name\": \"tags\",\n      \"type\": \"list\",\n      \"required\": false,\n      \"items\": {\n        \"type\": \"string\"\n      }\n    },\n    {\n      \"name\": \"metadata\",\n      \"type\": \"struct\",\n      \"required\": false,\n      \"fields\": [\n        {\n          \"name\": \"source\",\n          \"type\": \"string\",\n          \"required\": false\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Supported types**: `string`, `int32`, `int64`, `float32`, `float64`, `bool`, `timestamp`, `json`, `binary`, `list`, `struct`\n\n**Unstructured streams** (no validation, single `value` column):\n\n```bash\nnpx wrangler pipelines streams create my-stream\n```\n\n## Writing Data to Streams\n\n### Via Workers (Recommended)\n\n**Configuration** (`wrangler.toml`):\n\n```toml\n[[pipelines]]\npipeline = \"<STREAM_ID>\"\nbinding = \"STREAM\"\n```\n\n**Or JSON** (`wrangler.jsonc`):\n\n```jsonc\n{\n  \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n  \"pipelines\": [\n    {\n      \"pipeline\": \"<STREAM_ID>\",\n      \"binding\": \"STREAM\"\n    }\n  ]\n}\n```\n\n**Worker code**:\n\n```typescript\nexport default {\n  async fetch(request, env, ctx): Promise<Response> {\n    const event = {\n      user_id: \"12345\",\n      event_type: \"purchase\",\n      product_id: \"widget-001\",\n      amount: 29.99\n    };\n    \n    // Send single or multiple events\n    await env.STREAM.send([event]);\n    \n    return new Response('Event sent');\n  },\n} satisfies ExportedHandler<Env>;\n```\n\n**Batch sending**:\n\n```typescript\nconst events = [\n  { user_id: \"user1\", event_type: \"view\" },\n  { user_id: \"user2\", event_type: \"purchase\", amount: 50 }\n];\nawait env.STREAM.send(events);\n```\n\n### Via HTTP\n\n**Endpoint format**: `https://{stream-id}.ingest.cloudflare.com`\n\n**Without auth** (for testing):\n\n```bash\ncurl -X POST https://{stream-id}.ingest.cloudflare.com \\\n  -H \"Content-Type: application/json\" \\\n  -d '[\n    {\n      \"user_id\": \"user_12345\",\n      \"event_type\": \"purchase\",\n      \"product_id\": \"widget-001\",\n      \"amount\": 29.99\n    }\n  ]'\n```\n\n**With authentication**:\n\n```bash\ncurl -X POST https://{stream-id}.ingest.cloudflare.com \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_API_TOKEN\" \\\n  -d '[{\"event\": \"data\"}]'\n```\n\n**Required permission**: Workers Pipeline Send\n\n## SQL Transformations\n\n### Basic Patterns\n\n**Pass-through**:\n\n```sql\nINSERT INTO my_sink SELECT * FROM my_stream\n```\n\n**Filtering**:\n\n```sql\nINSERT INTO my_sink\nSELECT * FROM my_stream\nWHERE event_type = 'purchase' AND amount > 100\n```\n\n**Field selection**:\n\n```sql\nINSERT INTO my_sink\nSELECT user_id, event_type, timestamp, amount\nFROM my_stream\n```\n\n**Field transformation**:\n\n```sql\nINSERT INTO my_sink\nSELECT\n  user_id,\n  UPPER(event_type) as event_type,\n  timestamp,\n  amount * 1.1 as amount_with_tax,\n  CONCAT(user_id, '_', product_id) as unique_key\nFROM my_stream\n```\n\n**Conditional logic**:\n\n```sql\nINSERT INTO my_sink\nSELECT\n  user_id,\n  event_type,\n  CASE\n    WHEN amount > 1000 THEN 'high_value'\n    WHEN amount > 100 THEN 'medium_value'\n    ELSE 'low_value'\n  END as customer_tier\nFROM my_stream\nWHERE event_type IN ('purchase', 'refund')\n```\n\n## Sink Configuration\n\n### R2 Data Catalog (Iceberg Tables)\n\n**Create sink**:\n\n```bash\nnpx wrangler pipelines sinks create my-sink \\\n  --type r2-data-catalog \\\n  --bucket my-bucket \\\n  --namespace my_namespace \\\n  --table my_table \\\n  --catalog-token YOUR_CATALOG_TOKEN \\\n  --compression zstd \\\n  --roll-interval 60 \\\n  --roll-size 100\n```\n\n**Options**:\n- `--compression`: `zstd` (default), `snappy`, `gzip`, `lz4`, `uncompressed`\n- `--roll-interval`: Seconds between writes (default: 300)\n- `--roll-size`: Max file size in MB before rolling\n- `--target-row-group-size`: Parquet row group size in MB (default: 256)\n\n**Querying with R2 SQL**:\n\n```bash\nexport WRANGLER_R2_SQL_AUTH_TOKEN=YOUR_API_TOKEN\n\nnpx wrangler r2 sql query \"warehouse_name\" \"\nSELECT user_id, event_type, COUNT(*) as event_count\nFROM default.my_table\nWHERE event_type = 'purchase'\nGROUP BY user_id, event_type\nLIMIT 100\"\n```\n\n### R2 Storage (Raw Files)\n\n**JSON format**:\n\n```bash\nnpx wrangler pipelines sinks create my-sink \\\n  --type r2 \\\n  --bucket my-bucket \\\n  --format json \\\n  --path analytics/events \\\n  --partitioning \"year=%Y/month=%m/day=%d\" \\\n  --roll-interval 60 \\\n  --roll-size 100 \\\n  --access-key-id YOUR_KEY \\\n  --secret-access-key YOUR_SECRET\n```\n\n**Parquet format** (better compression/performance):\n\n```bash\nnpx wrangler pipelines sinks create my-sink \\\n  --type r2 \\\n  --bucket my-bucket \\\n  --format parquet \\\n  --compression zstd \\\n  --path analytics/events \\\n  --partitioning \"year=%Y/month=%m/day=%d/hour=%H\" \\\n  --target-row-group-size 256 \\\n  --roll-interval 300 \\\n  --roll-size 100 \\\n  --access-key-id YOUR_KEY \\\n  --secret-access-key YOUR_SECRET\n```\n\n**File organization**:\n\n```\nbucket/analytics/events/\n  year=2025/\n    month=01/\n      day=11/\n        uuid-1.parquet\n        uuid-2.parquet\n```\n\n## Wrangler Commands Reference\n\n### Setup & Management\n\n```bash\n# Interactive setup (creates stream, sink, pipeline)\nnpx wrangler pipelines setup\n\n# List all pipelines\nnpx wrangler pipelines list\n\n# Get pipeline details\nnpx wrangler pipelines get <PIPELINE_ID>\n\n# Delete pipeline\nnpx wrangler pipelines delete <PIPELINE_ID>\n```\n\n### Streams\n\n```bash\n# Create stream with schema\nnpx wrangler pipelines streams create my-stream --schema-file schema.json\n\n# Create unstructured stream\nnpx wrangler pipelines streams create my-stream\n\n# List streams\nnpx wrangler pipelines streams list\n\n# Get stream details\nnpx wrangler pipelines streams get <STREAM_ID>\n\n# Delete stream (deletes dependent pipelines and buffered events!)\nnpx wrangler pipelines streams delete <STREAM_ID>\n```\n\n### Sinks\n\n```bash\n# Create R2 Data Catalog sink\nnpx wrangler pipelines sinks create my-sink \\\n  --type r2-data-catalog \\\n  --bucket my-bucket \\\n  --namespace default \\\n  --table my_table \\\n  --catalog-token TOKEN\n\n# Create R2 storage sink\nnpx wrangler pipelines sinks create my-sink \\\n  --type r2 \\\n  --bucket my-bucket \\\n  --format parquet \\\n  --compression zstd\n\n# List sinks\nnpx wrangler pipelines sinks list\n\n# Get sink details\nnpx wrangler pipelines sinks get <SINK_ID>\n\n# Delete sink\nnpx wrangler pipelines sinks delete <SINK_ID>\n```\n\n### Pipelines\n\n```bash\n# Create with inline SQL\nnpx wrangler pipelines create my-pipeline \\\n  --sql \"INSERT INTO my_sink SELECT * FROM my_stream\"\n\n# Create with SQL file\nnpx wrangler pipelines create my-pipeline \\\n  --sql-file transform.sql\n\n# View pipeline\nnpx wrangler pipelines get <PIPELINE_ID>\n\n# List all pipelines\nnpx wrangler pipelines list\n\n# Delete pipeline\nnpx wrangler pipelines delete <PIPELINE_ID>\n```\n\n## Authentication & Permissions\n\n### R2 Data Catalog Token\n\nRequired permissions: **R2 Admin Read & Write**\n\nCreate in dashboard:\n1. Go to R2 > Manage API tokens\n2. Create Account API Token\n3. Select \"Admin Read & Write\" permission\n4. Save token value\n\n### R2 Storage Credentials\n\nRequired permissions: **Object Read & Write**\n\nCreate via Wrangler or dashboard for access key ID and secret access key.\n\n### HTTP Ingest Token\n\nRequired permissions: **Workers Pipeline Send**\n\nFor authenticated HTTP ingestion endpoints.\n\n## Best Practices\n\n### Schema Design\n\n- ✅ Use structured streams for validation\n- ✅ Mark critical fields as `required: true`\n- ✅ Use appropriate types (`int64` for timestamps, `float64` for decimals)\n- ❌ Avoid overly nested structs (query performance)\n- ❌ Don't change schemas after creation (recreate stream)\n\n### Performance\n\n- **Low latency**: Set `--roll-interval 10` (smaller files, more frequent)\n- **Query performance**: Set `--roll-interval 300` and `--roll-size 100` (larger files, less frequent)\n- Use `zstd` compression for best ratio, `snappy` for speed\n- Increase `--target-row-group-size` for analytical workloads\n\n### SQL Transformations\n\n- ✅ Filter early (`WHERE` clauses reduce data volume)\n- ✅ Select only needed fields (reduces storage costs)\n- ✅ Use functions for enrichment (CONCAT, UPPER, CASE)\n- ❌ Cannot modify pipelines after creation (plan carefully)\n- ❌ No JOINs across streams (single stream per pipeline)\n\n### Workers Integration\n\n- ✅ Use Worker bindings (no token management)\n- ✅ Batch events when possible (`send([event1, event2, ...])`)\n- ✅ Handle send errors gracefully\n- ❌ Don't await send in critical path if latency matters (use `ctx.waitUntil()`)\n\n```typescript\n// Fire-and-forget pattern\nexport default {\n  async fetch(request, env, ctx) {\n    const event = { /* ... */ };\n    \n    // Don't block response on send\n    ctx.waitUntil(env.STREAM.send([event]));\n    \n    return new Response('OK');\n  }\n};\n```\n\n### HTTP Ingestion\n\n- ✅ Enable auth for production endpoints\n- ✅ Configure CORS if sending from browsers\n- ✅ Send arrays (not single objects) for batch efficiency\n- ✅ Handle 4xx/5xx responses with retries\n\n### Monitoring\n\n- Check stream buffer status (dashboard or API)\n- Monitor pipeline processing rate\n- Review R2 storage growth\n- Query data regularly to verify pipeline health\n\n## Limits (Open Beta)\n\n| Resource | Limit |\n|----------|-------|\n| Streams per account | 20 |\n| Sinks per account | 20 |\n| Pipelines per account | 20 |\n| Payload size per request | 1 MB |\n| Ingest rate per stream | 5 MB/s |\n\nRequest increases: [Limit Increase Form](https://forms.gle/ukpeZVLWLnKeixDu7)\n\n## Troubleshooting\n\n### Events not appearing in R2\n\n- Wait 10-300 seconds (depends on `--roll-interval`)\n- Check pipeline status: `npx wrangler pipelines get <ID>`\n- Verify stream has data (check dashboard metrics)\n- Confirm sink credentials are valid\n\n### Schema validation failures\n\n- Events accepted but dropped if invalid\n- Check event structure matches schema exactly\n- Verify required fields are present\n- Check data types (e.g., strings not numbers)\n\n### Worker binding not found\n\n- Verify `wrangler.toml`/`wrangler.jsonc` has correct `pipeline` ID\n- Redeploy Worker after adding binding\n- Check binding name matches code (`env.STREAM`)\n\n### SQL errors\n\n- SQL cannot be modified after creation\n- Recreate pipeline with corrected SQL\n- Verify stream and sink names in SQL match actual resources\n- Check SQL syntax against reference docs\n\n## Complete Example: Ecommerce Analytics\n\n**1. Create schema** (`ecommerce-schema.json`):\n\n```json\n{\n  \"fields\": [\n    {\n      \"name\": \"user_id\",\n      \"type\": \"string\",\n      \"required\": true\n    },\n    {\n      \"name\": \"event_type\",\n      \"type\": \"string\",\n      \"required\": true\n    },\n    {\n      \"name\": \"product_id\",\n      \"type\": \"string\",\n      \"required\": false\n    },\n    {\n      \"name\": \"amount\",\n      \"type\": \"float64\",\n      \"required\": false\n    },\n    {\n      \"name\": \"timestamp\",\n      \"type\": \"timestamp\",\n      \"required\": true\n    }\n  ]\n}\n```\n\n**2. Setup infrastructure**:\n\n```bash\n# Create bucket and enable catalog\nnpx wrangler r2 bucket create ecommerce-data\nnpx wrangler r2 bucket catalog enable ecommerce-data\n\n# Create stream\nnpx wrangler pipelines streams create ecommerce-stream \\\n  --schema-file ecommerce-schema.json\n\n# Create sink\nnpx wrangler pipelines sinks create ecommerce-sink \\\n  --type r2-data-catalog \\\n  --bucket ecommerce-data \\\n  --namespace default \\\n  --table events \\\n  --catalog-token $CATALOG_TOKEN \\\n  --roll-interval 60\n\n# Create pipeline with transformation\nnpx wrangler pipelines create ecommerce-pipeline \\\n  --sql \"INSERT INTO ecommerce_sink \n         SELECT \n           user_id,\n           UPPER(event_type) as event_type,\n           product_id,\n           amount,\n           timestamp,\n           CASE \n             WHEN amount > 100 THEN 'high_value'\n             ELSE 'standard'\n           END as transaction_tier\n         FROM ecommerce_stream\n         WHERE event_type IN ('purchase', 'add_to_cart', 'view_product')\"\n```\n\n**3. Configure Worker** (`wrangler.toml`):\n\n```toml\nname = \"ecommerce-api\"\nmain = \"src/index.ts\"\n\n[[pipelines]]\npipeline = \"<STREAM_ID>\"\nbinding = \"EVENTS\"\n```\n\n**4. Send events** (`src/index.ts`):\n\n```typescript\ninterface Env {\n  EVENTS: Pipeline;\n}\n\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    if (request.method === 'POST') {\n      const data = await request.json();\n      \n      const event = {\n        user_id: data.userId,\n        event_type: data.eventType,\n        product_id: data.productId,\n        amount: data.amount,\n        timestamp: new Date().toISOString()\n      };\n      \n      try {\n        await env.EVENTS.send([event]);\n        return new Response('Event tracked', { status: 200 });\n      } catch (error) {\n        return new Response('Failed to track event', { status: 500 });\n      }\n    }\n    \n    return new Response('Method not allowed', { status: 405 });\n  }\n} satisfies ExportedHandler<Env>;\n```\n\n**5. Query results**:\n\n```bash\nexport WRANGLER_R2_SQL_AUTH_TOKEN=$CATALOG_TOKEN\n\nnpx wrangler r2 sql query \"ecommerce-warehouse\" \"\nSELECT \n  event_type,\n  transaction_tier,\n  COUNT(*) as event_count,\n  SUM(amount) as total_revenue\nFROM default.events\nWHERE event_type = 'PURCHASE'\nGROUP BY event_type, transaction_tier\nORDER BY total_revenue DESC\"\n```\n\n## Additional Resources\n\n- [Pipelines Documentation](https://developers.cloudflare.com/pipelines/)\n- [SQL Reference](https://developers.cloudflare.com/pipelines/sql-reference/)\n- [R2 Data Catalog](https://developers.cloudflare.com/r2/data-catalog/)\n- [Wrangler Commands](https://developers.cloudflare.com/workers/wrangler/commands/#pipelines)\n- [Apache Iceberg](https://iceberg.apache.org/)\n",
        ".agent/services/hosting/cloudflare-platform/references/pulumi/README.md": "# Cloudflare Pulumi Provider\n\nExpert guidance for Cloudflare Pulumi Provider (@pulumi/cloudflare).\n\n## Overview\n\nProgrammatic management of Cloudflare resources: Workers, Pages, D1, KV, R2, DNS, Queues, etc.\n\n**Packages:**\n- TypeScript/JS: `@pulumi/cloudflare`\n- Python: `pulumi-cloudflare`\n- Go: `github.com/pulumi/pulumi-cloudflare/sdk/v6/go/cloudflare`\n- .NET: `Pulumi.Cloudflare`\n\n**Version:** v6.x\n\n## Core Principles\n\n1. Use API tokens (not legacy API keys)\n2. Store accountId in stack config\n3. Match binding names across code/config\n4. Use `module: true` for ES modules\n5. Set `compatibilityDate` to lock behavior\n\n## Authentication\n\nThree methods (mutually exclusive):\n\n**1. API Token (Recommended)**\n\n```typescript\nimport * as cloudflare from \"@pulumi/cloudflare\";\n\nconst provider = new cloudflare.Provider(\"cf\", {\n    apiToken: process.env.CLOUDFLARE_API_TOKEN,\n});\n```\n\nEnv: `CLOUDFLARE_API_TOKEN`\n\n**2. API Key (Legacy)**\n\n```typescript\nconst provider = new cloudflare.Provider(\"cf\", {\n    apiKey: process.env.CLOUDFLARE_API_KEY,\n    email: process.env.CLOUDFLARE_EMAIL,\n});\n```\n\nEnv: `CLOUDFLARE_API_KEY`, `CLOUDFLARE_EMAIL`\n\n**3. API User Service Key**\n\n```typescript\nconst provider = new cloudflare.Provider(\"cf\", {\n    apiUserServiceKey: process.env.CLOUDFLARE_API_USER_SERVICE_KEY,\n});\n```\n\nEnv: `CLOUDFLARE_API_USER_SERVICE_KEY`\n\n## Setup\n\n**Pulumi.yaml:**\n\n```yaml\nname: my-cloudflare-app\nruntime: nodejs\nconfig:\n  cloudflare:apiToken:\n    value: ${CLOUDFLARE_API_TOKEN}\n```\n\n**Pulumi.<stack>.yaml:**\n\n```yaml\nconfig:\n  cloudflare:accountId: \"abc123...\"\n```\n\n**index.ts:**\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as cloudflare from \"@pulumi/cloudflare\";\n\nconst config = new pulumi.Config(\"cloudflare\");\nconst accountId = config.require(\"accountId\");\n```\n\n## Essential Imports\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as cloudflare from \"@pulumi/cloudflare\";\n```\n\n## Common Resource Types\n\n- `Provider` - Provider config\n- `WorkerScript` - Worker\n- `WorkersKvNamespace` - KV\n- `R2Bucket` - R2\n- `D1Database` - D1\n- `Queue` - Queue\n- `PagesProject` - Pages\n- `DnsRecord` - DNS\n- `WorkerRoute` - Worker route\n- `WorkersDomain` - Custom domain\n\n## Key Properties\n\n- `accountId` - Required for most resources\n- `zoneId` - Required for DNS/domain\n- `name`/`title` - Resource identifier\n- `*Bindings` - Connect resources to Workers\n\n---\nSee: [configuration.md](./configuration.md), [api.md](./api.md), [patterns.md](./patterns.md), [gotchas.md](./gotchas.md)\n",
        ".agent/services/hosting/cloudflare-platform/references/queues/README.md": "# Cloudflare Queues\n\nFlexible message queuing for async task processing with guaranteed at-least-once delivery and configurable batching.\n\n## Overview\n\nQueues provide:\n- At-least-once delivery guarantee\n- Push-based (Worker) and pull-based (HTTP) consumers\n- Configurable batching and retries\n- Dead Letter Queues (DLQ)\n- Delays up to 12 hours\n\n**Use cases:** Async processing, API buffering, rate limiting, event workflows, deferred jobs\n\n## Quick Start\n\n```bash\nwrangler queues create my-queue\nwrangler queues consumer add my-queue my-worker\n```\n\n```typescript\n// Producer\nawait env.MY_QUEUE.send({ userId: 123, action: 'notify' });\n\n// Consumer\nexport default {\n  async queue(batch: MessageBatch, env: Env): Promise<void> {\n    for (const msg of batch.messages) {\n      await process(msg.body);\n      msg.ack();\n    }\n  }\n};\n```\n\n## Core Operations\n\n| Operation | Purpose | Limit |\n|-----------|---------|-------|\n| `send(body, options?)` | Publish message | 128 KB |\n| `sendBatch(messages)` | Bulk publish | 100 msgs/256 KB |\n| `message.ack()` | Acknowledge success | - |\n| `message.retry(options?)` | Retry with delay | - |\n| `batch.ackAll()` | Ack entire batch | - |\n\n## Architecture\n\n```\n[Producer Worker] → [Queue] → [Consumer Worker/HTTP] → [Processing]\n```\n\n- Max 10,000 queues per account\n- 5,000 msgs/second per queue\n- 4-14 day retention (configurable)\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - wrangler.jsonc setup, producer/consumer config, DLQ\n- [api.md](./api.md) - Send/batch methods, queue handler, ack/retry, pull API\n- [patterns.md](./patterns.md) - Async tasks, buffering, rate limiting, event workflows\n- [gotchas.md](./gotchas.md) - Idempotency, retry limits, content types, cost optimization\n\n## See Also\n\n- [workers](../workers/) - Worker runtime for producers/consumers\n- [r2](../r2/) - Process R2 event notifications via queues\n- [d1](../d1/) - Batch write to D1 from queue consumers\n",
        ".agent/services/hosting/cloudflare-platform/references/r2-data-catalog/README.md": "# Cloudflare R2 Data Catalog Skill Reference\n\nExpert guidance for Cloudflare R2 Data Catalog - Apache Iceberg catalog built into R2 buckets.\n\n## Overview\n\nR2 Data Catalog is a managed Apache Iceberg REST catalog built directly into R2 buckets. It enables analytical workloads like log analytics, BI, and data pipelines with zero egress fees. Curr...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/r2-sql/README.md": "# Cloudflare R2 SQL Skill\n\nGuide for using Cloudflare R2 SQL - serverless distributed query engine for Apache Iceberg tables in R2 Data Catalog.\n\n## Overview\n\nR2 SQL is Cloudflare's serverless distributed analytics query engine for querying Apache Iceberg tables in R2 Data Catalog. Features:\n- Serverless - no clusters to manage\n- Distributed - leverages Cloudflare's global network\n- Zero egress fees - query from any cloud/region\n- Open beta - free during beta (standard R2 storage costs apply)\n\n## Core Concepts\n\n### Apache Iceberg Table Format\n\n- Open table format for large-scale analytics datasets\n- ACID transactions for reliable concurrent reads/writes\n- Schema evolution - add/rename/drop columns without rewriting data\n- Optimized metadata - avoids full table scans via indexed metadata\n- Supported by Spark, Trino, Snowflake, DuckDB, ClickHouse, PyIceberg\n\n### R2 Data Catalog\n\n- Managed Apache Iceberg catalog built into R2 bucket\n- Exposes standard Iceberg REST catalog interface\n- Single source of truth for table metadata\n- Tracks table state via immutable snapshots\n- Supports multiple query engines safely accessing same tables\n\n### Architecture\n\n**Query Planner**:\n- Top-down metadata investigation\n- Multi-layer pruning (partition-level, column-level, row-group level)\n- Streaming pipeline - execution starts before planning completes\n- Early termination - stops when result complete without full scan\n- Uses partition stats and column stats (min/max, null counts)\n\n**Query Execution**:\n- Coordinator distributes work to workers across Cloudflare network\n- Workers run Apache DataFusion for parallel query execution\n- Arrow IPC format for inter-process communication\n- Parquet column pruning - reads only required columns\n- Ranged reads from R2 for efficiency\n\n**Aggregation Strategies**:\n- Scatter-gather - for simple aggregations (sum, count, avg)\n- Shuffling - for ORDER BY/HAVING on aggregates via hash partitioning\n\n## Setup & Configuration\n\n### 1. Enable R2 Data Catalog\n\nCLI:\n\n```bash\nnpx wrangler r2 bucket catalog enable <bucket-name>\n```\n\nNote the Warehouse name and Catalog URI from output.\n\nDashboard:\n1. R2 Object Storage → Select bucket\n2. Settings tab → R2 Data Catalog → Enable\n3. Note Catalog URI and Warehouse name\n\n### 2. Create API Token\n\nRequired permissions: R2 Admin Read & Write (includes R2 SQL Read)\n\nDashboard:\n1. R2 Object Storage → Manage API tokens\n2. Create API token → Admin Read & Write\n3. Save token value\n\n### 3. Configure Environment\n\n```bash\nexport WRANGLER_R2_SQL_AUTH_TOKEN=<your-token>\n```\n\nOr `.env` file:\n\n```\nWRANGLER_R2_SQL_AUTH_TOKEN=<your-token>\n```\n\n## Common Code Patterns\n\n### Wrangler CLI Query\n\n```bash\nnpx wrangler r2 sql query \"<warehouse-name>\" \"\n  SELECT * \n  FROM namespace.table_name \n  WHERE condition \n  LIMIT 10\"\n```\n\n### PyIceberg Setup\n\n```python\nfrom pyiceberg.catalog.rest import RestCatalog\n\ncatalog = RestCatalog(\n    name=\"my_catalog\",\n    warehouse=\"<WAREHOUSE>\",\n    uri=\"<CATALOG_URI>\",\n    token=\"<TOKEN>\",\n)\n\n# Create namespace\ncatalog.create_namespace_if_not_exists(\"default\")\n```\n\n### Create Table\n\n```python\nimport pyarrow as pa\n\n# Define schema\ndf = pa.table({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"score\": [80.0, 92.5, 88.0],\n})\n\n# Create table\ntable = catalog.create_table(\n    (\"default\", \"people\"),\n    schema=df.schema,\n)\n```\n\n### Append Data\n\n```python\ntable.append(df)\n```\n\n### Query Table\n\n```python\n# Scan and convert to Pandas\nscanned = table.scan().to_arrow()\nprint(scanned.to_pandas())\n```\n\n## SQL Reference\n\n### Query Structure\n\n```sql\nSELECT column_list | aggregation_function\nFROM table_name\nWHERE conditions\n[GROUP BY column_list]\n[HAVING conditions]\n[ORDER BY partition_key [DESC | ASC]]\n[LIMIT number]\n```\n\n### Schema Discovery\n\n```sql\n-- List namespaces\nSHOW DATABASES;\nSHOW NAMESPACES;\n\n-- List tables\nSHOW TABLES IN namespace_name;\n\n-- Describe table\nDESCRIBE namespace_name.table_name;\n```\n\n### SELECT Patterns\n\n```sql\n-- All columns\nSELECT * FROM ns.table;\n\n-- Specific columns\nSELECT user_id, timestamp, status FROM ns.table;\n\n-- With conditions\nSELECT * FROM ns.table \nWHERE timestamp BETWEEN '2025-01-01T00:00:00Z' AND '2025-01-31T23:59:59Z'\n  AND status = 200\nLIMIT 100;\n\n-- Complex conditions\nSELECT * FROM ns.table \nWHERE (status = 404 OR status = 500) \n  AND method = 'POST'\n  AND user_agent IS NOT NULL\nORDER BY timestamp DESC;\n```\n\n### Aggregations\n\nSupported functions: COUNT(*), SUM(col), AVG(col), MIN(col), MAX(col)\n\n```sql\n-- Count by group\nSELECT department, COUNT(*)\nFROM ns.sales_data\nGROUP BY department;\n\n-- Multiple aggregates\nSELECT region, MIN(price), MAX(price), AVG(price)\nFROM ns.products\nGROUP BY region\nORDER BY AVG(price) DESC;\n\n-- With HAVING filter\nSELECT category, SUM(amount)\nFROM ns.sales\nWHERE sale_date >= '2024-01-01'\nGROUP BY category\nHAVING SUM(amount) > 10000\nLIMIT 10;\n```\n\n### Data Types\n\n| Type | Description | Example |\n|------|-------------|---------|\n| integer | Whole numbers | 1, 42, -10 |\n| float | Decimals | 1.5, 3.14 |\n| string | Text (quoted) | 'hello', 'GET' |\n| boolean | true/false | true, false |\n| timestamp | RFC3339 | '2025-01-01T00:00:00Z' |\n| date | YYYY-MM-DD | '2025-01-01' |\n\n### Operators\n\nComparison: =, !=, <, <=, >, >=, LIKE, BETWEEN, IS NULL, IS NOT NULL\nLogical: AND (higher precedence), OR (lower precedence)\n\n### ORDER BY Limitations\n\n**CRITICAL**: ORDER BY only supports partition key columns\n\n```sql\n-- Valid if timestamp is partition key\nSELECT * FROM ns.logs ORDER BY timestamp DESC LIMIT 100;\n\n-- Invalid if column not in partition key\nSELECT * FROM ns.logs ORDER BY user_id;  -- ERROR\n```\n\n### LIMIT Defaults\n\n- Range: 1 to 10,000\n- Default: 500 if not specified\n\n## Pipelines Integration\n\n### Create Pipeline with Data Catalog Sink\n\nSchema file (`schema.json`):\n\n```json\n{\n  \"fields\": [\n    {\"name\": \"user_id\", \"type\": \"string\", \"required\": true},\n    {\"name\": \"event_type\", \"type\": \"string\", \"required\": true},\n    {\"name\": \"amount\", \"type\": \"float64\", \"required\": false}\n  ]\n}\n```\n\nSetup:\n\n```bash\nnpx wrangler pipelines setup\n```\n\nConfiguration:\n- Pipeline name: ecommerce\n- Enable HTTP endpoint: yes\n- Schema: Load from file → schema.json\n- Destination: Data Catalog Table\n- R2 bucket: your-bucket\n- Namespace: default\n- Table name: events\n- Catalog token: <your-token>\n- Compression: zstd\n- Roll file time: 10 seconds (dev), 300+ (prod)\n\n### Send Data to Pipeline\n\n```bash\ncurl -X POST https://{stream-id}.ingest.cloudflare.com \\\n  -H \"Content-Type: application/json\" \\\n  -d '[\n    {\n      \"user_id\": \"user_123\",\n      \"event_type\": \"purchase\",\n      \"amount\": 29.99\n    }\n  ]'\n```\n\n## Common Use Cases\n\n### Log Analytics\n\n- Ingest logs via Pipelines to Iceberg table\n- Partition by day(timestamp) for efficient queries\n- Query specific time ranges with automatic pruning\n- Aggregate by status codes, endpoints, user agents\n\n```sql\nSELECT status, COUNT(*)\nFROM logs.http_requests\nWHERE timestamp BETWEEN '2025-01-01T00:00:00Z' AND '2025-01-31T23:59:59Z'\n  AND method = 'GET'\nGROUP BY status\nORDER BY COUNT(*) DESC;\n```\n\n### Fraud Detection\n\n- Stream transaction events to catalog\n- Query suspicious patterns with WHERE filters\n- Aggregate by location, merchant, time windows\n\n```sql\nSELECT location, COUNT(*), AVG(amount)\nFROM fraud.transactions\nWHERE is_fraud = true\n  AND transaction_timestamp >= '2025-01-01'\nGROUP BY location\nHAVING COUNT(*) > 10;\n```\n\n### Business Intelligence\n\n- ETL data into partitioned Iceberg tables\n- Run analytical queries across large datasets\n- Generate reports with GROUP BY aggregations\n- No egress fees when querying from BI tools\n\n```sql\nSELECT \n  department, \n  SUM(revenue) as total_revenue,\n  AVG(revenue) as avg_revenue\nFROM sales.transactions\nWHERE sale_date >= '2024-01-01'\nGROUP BY department\nORDER BY SUM(revenue) DESC\nLIMIT 10;\n```\n\n## Performance Optimization\n\n### Partitioning Strategy\n\n- Choose partition key based on common query patterns\n- Typical: day(timestamp), hour(timestamp), region, category\n- Enables metadata pruning to skip entire partitions\n- Required for ORDER BY optimization\n\n### Query Optimization\n\n- Use WHERE filters to leverage partition/column stats\n- Specify LIMIT to enable early termination\n- ORDER BY partition key columns only\n- Filter on high-selectivity columns first\n\n### Data Organization\n\n- Smaller files → slower queries (overhead)\n- Larger files → better compression, fewer metadata ops\n- Recommended: 100-500MB Parquet files after compression\n- Use appropriate roll intervals in Pipelines (300+ seconds for prod)\n\n### File Pruning\n\nAutomatic at three levels:\n1. Partition-level: Skip manifests not matching query\n2. File-level: Skip Parquet files via column stats\n3. Row-group level: Skip row groups within files\n\n## Iceberg Metadata Structure\n\n```\nbucket/\n  metadata/\n    snap-{id}.avro          # Snapshot (points to manifest list)\n    {uuid}-m0.avro          # Manifest file (lists data files + stats)\n    version-hint.text       # Current metadata version\n    v{n}.metadata.json      # Table metadata (schema, snapshots)\n  data/\n    00000-0-{uuid}.parquet  # Data files\n```\n\n**Metadata hierarchy**:\n1. Table metadata JSON - schema, partition spec, snapshot log\n2. Snapshot - points to manifest list\n3. Manifest list - partition stats for each manifest\n4. Manifest files - column stats for each data file\n5. Parquet files - row group stats in footer\n\n## Limitations & Best Practices\n\n### Current Limitations (Open Beta)\n\n- ORDER BY only on partition key columns\n- COUNT(*) only - COUNT(column) not supported\n- No aliases in SELECT\n- No subqueries, joins, or CTEs\n- No nested column access\n- LIMIT max 10,000\n\n### Best Practices\n\n- Partition by time dimension for time-series data\n- Use BETWEEN for time ranges (leverages partition pruning)\n- Combine filters with AND for better pruning\n- Set appropriate LIMIT based on use case\n- Use compression (zstd recommended)\n- Monitor query performance and adjust partitioning\n\n### Type Safety\n\n- Quote string values: 'value'\n- Use RFC3339 for timestamps: '2025-01-01T00:00:00Z'\n- Use YYYY-MM-DD for dates: '2025-01-01'\n- No implicit type conversions\n\n## Connecting Other Engines\n\nR2 Data Catalog supports standard Iceberg REST catalog API.\n\n### Spark (Scala)\n\n```scala\nval spark = SparkSession.builder()\n  .config(\"spark.sql.catalog.my_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n  .config(\"spark.sql.catalog.my_catalog.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\")\n  .config(\"spark.sql.catalog.my_catalog.uri\", catalogUri)\n  .config(\"spark.sql.catalog.my_catalog.token\", token)\n  .config(\"spark.sql.catalog.my_catalog.warehouse\", warehouse)\n  .getOrCreate()\n```\n\n### Snowflake\n\n- Create external Iceberg catalog connection\n- Configure with Catalog URI and R2 credentials\n- Query tables via SQL interface\n\n### DuckDB, Trino, ClickHouse\n\n- Supported via Iceberg REST catalog protocol\n- Refer to engine-specific documentation for configuration\n\n## Pricing (Future)\n\nCurrently in open beta - no charges beyond standard R2 costs.\n\nPlanned future pricing:\n- R2 storage: $0.015/GB-month\n- Class A operations: $4.50/million\n- Class B operations: $0.36/million\n- Catalog operations: $9.00/million (create table, get metadata, etc)\n- Compaction: $0.05/GB + $4.00/million objects processed\n- Egress: $0 (always free)\n\n30+ days notice before billing begins.\n\n## Troubleshooting\n\n### Common Errors\n\n**\"ORDER BY column not in partition key\"**\n- Only partition key columns can be used in ORDER BY\n- Check table partition spec with DESCRIBE\n- Remove ORDER BY or adjust table partitioning\n\n**\"Token authentication failed\"**\n- Verify WRANGLER_R2_SQL_AUTH_TOKEN is set\n- Ensure token has R2 Admin Read & Write + SQL Read permissions\n- Token may be expired - create new one\n\n**\"Table not found\"**\n- Verify namespace exists: SHOW DATABASES\n- Check table name: SHOW TABLES IN namespace\n- Ensure catalog enabled on bucket\n\n**\"No data returned\"**\n- Check WHERE conditions match data\n- Verify time range in BETWEEN clause\n- Try removing filters to confirm data exists\n\n### Performance Issues\n\n**Slow queries**:\n- Check partition pruning effectiveness\n- Reduce LIMIT if scanning too much data\n- Ensure filters on partition key columns\n- Review Parquet file sizes (aim for 100-500MB)\n\n**Query timeout**:\n- Add more restrictive WHERE filters\n- Reduce LIMIT\n- Consider better partitioning strategy\n\n## Resources\n\n- Docs: https://developers.cloudflare.com/r2-sql/\n- Data Catalog: https://developers.cloudflare.com/r2/data-catalog/\n- Blog: https://blog.cloudflare.com/r2-sql-deep-dive/\n- Discord: https://discord.cloudflare.com/\n\n## Key Reminders\n\n1. R2 SQL queries ONLY Apache Iceberg tables in R2 Data Catalog\n2. Enable catalog on bucket before use\n3. Create API token with R2 + catalog permissions\n4. Partition by time for time-series data\n5. ORDER BY limited to partition key columns\n6. Use LIMIT and WHERE for optimal performance\n7. Zero egress fees - query from anywhere\n8. Open beta - free during testing phase\n9. Serverless - no infrastructure management\n10. Leverage Cloudflare's global network for distributed execution\n",
        ".agent/services/hosting/cloudflare-platform/references/r2/README.md": "# Cloudflare R2 Object Storage\n\nS3-compatible object storage with zero egress fees, optimized for large file storage and delivery.\n\n## Overview\n\nR2 provides:\n- S3-compatible API (Workers API + S3 REST)\n- Zero egress fees globally\n- Strong consistency for writes/deletes\n- Storage classes (Standard/Infrequent Access)\n- SSE-C encryption support\n\n**Use cases:** Media storage, backups, static assets, user uploads, data lakes\n\n## Quick Start\n\n```bash\nwrangler r2 bucket create my-bucket --location=enam\nwrangler r2 object put my-bucket/file.txt --file=./local.txt\n```\n\n```typescript\n// Upload\nawait env.MY_BUCKET.put(key, data, {\n  httpMetadata: { contentType: 'image/jpeg' }\n});\n\n// Download\nconst object = await env.MY_BUCKET.get(key);\nif (object) return new Response(object.body);\n```\n\n## Core Operations\n\n| Method | Purpose | Returns |\n|--------|---------|---------|\n| `put(key, value, options?)` | Upload object | `R2Object \\| null` |\n| `get(key, options?)` | Download object | `R2ObjectBody \\| R2Object \\| null` |\n| `head(key)` | Get metadata only | `R2Object \\| null` |\n| `delete(keys)` | Delete object(s) | `Promise<void>` |\n| `list(options?)` | List objects | `R2Objects` |\n\n## Storage Classes\n\n- **Standard**: Frequent access, low latency reads\n- **InfrequentAccess**: 30-day minimum storage, retrieval fees, lower storage cost\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - wrangler.jsonc bindings, S3 SDK setup, location hints\n- [api.md](./api.md) - Workers API methods, multipart uploads, conditional requests\n- [patterns.md](./patterns.md) - Streaming, caching, presigned URLs, storage transitions\n- [gotchas.md](./gotchas.md) - List truncation, etag format, checksum limits, multipart pitfalls\n\n## See Also\n\n- [workers](../workers/) - Worker runtime and fetch handlers\n- [kv](../kv/) - Metadata storage for R2 objects\n- [d1](../d1/) - Store R2 URLs in relational database\n- [queues](../queues/) - Process R2 uploads asynchronously\n",
        ".agent/services/hosting/cloudflare-platform/references/realtime-sfu/README.md": "# Cloudflare Realtime SFU Reference\n\nExpert guidance for building real-time audio/video/data applications using Cloudflare Realtime SFU (Selective Forwarding Unit).\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, environment variables, Wrangler config\n- **[api.md](./api.md)** - Sessions, tracks, endpoints, request/response patterns\n- **[patterns.md](./patterns.md)** - Architecture patterns, use cases, integration examples\n- **[gotchas.md](./gotchas.md)** - Common issues, debugging, performance, security\n\n## Quick Start\n\nCloudflare Realtime SFU: WebRTC infrastructure on global network (310+ cities). Anycast routing, no regional constraints, pub/sub model.\n\n## See Also\n\n- [Orange Meets Demo](https://demo.orange.cloudflare.dev/)\n- [Orange Source](https://github.com/cloudflare/orange)\n- [Calls Examples](https://github.com/cloudflare/calls-examples)\n- [API Reference](https://developers.cloudflare.com/api/resources/calls/)\n",
        ".agent/services/hosting/cloudflare-platform/references/realtimekit/README.md": "# Cloudflare RealtimeKit\n\nExpert guidance for building real-time video and audio applications using **Cloudflare RealtimeKit** - a comprehensive SDK suite for adding customizable live video and voice to web or mobile applications.\n\n## Overview\n\nRealtimeKit is Cloudflare's SDK suite built on Realtime SFU, abstracting WebRTC complexity with fast integration, pre-built UI components, global performance (300+ cities), and production features (recording, transcription, chat, polls).\n\n**Use cases**: Team meetings, webinars, social video, audio calls, interactive plugins\n\n## Core Concepts\n\n- **App**: Workspace grouping meetings, participants, presets, recordings. Use separate Apps for staging/production\n- **Meeting**: Re-usable virtual room. Each join creates new **Session**\n- **Session**: Live meeting instance. Created on first join, ends after last leave\n- **Participant**: User added via REST API. Returns `authToken` for client SDK. **Do not reuse tokens**\n- **Preset**: Reusable permission/UI template (permissions, meeting type, theme). Applied at participant creation\n- **Peer ID** (`id`): Unique per session, changes on rejoin\n- **Participant ID** (`userId`): Persistent across sessions\n\n## Quick Start\n\n### 1. Create App & Meeting (Backend)\n\n```bash\n# Create app\ncurl -X POST 'https://api.cloudflare.com/client/v4/accounts/<account_id>/realtime/kit/apps' \\\n  -H 'Authorization: Bearer <api_token>' \\\n  -d '{\"name\": \"My RealtimeKit App\"}'\n\n# Create meeting\ncurl -X POST 'https://api.cloudflare.com/client/v4/accounts/<account_id>/realtime/kit/<app_id>/meetings' \\\n  -H 'Authorization: Bearer <api_token>' \\\n  -d '{\"title\": \"Team Standup\"}'\n\n# Add participant\ncurl -X POST 'https://api.cloudflare.com/client/v4/accounts/<account_id>/realtime/kit/<app_id>/meetings/<meeting_id>/participants' \\\n  -H 'Authorization: Bearer <api_token>' \\\n  -d '{\"name\": \"Alice\", \"preset_name\": \"host\"}'\n# Returns: { authToken }\n```\n\n### 2. Client Integration\n\n**React**:\n\n```tsx\nimport { RtkMeeting } from '@cloudflare/realtimekit-react-ui';\n\nfunction App() {\n  return <RtkMeeting authToken=\"<participant_auth_token>\" onLeave={() => {}} />;\n}\n```\n\n**Core SDK**:\n\n```typescript\nimport RealtimeKitClient from '@cloudflare/realtimekit';\n\nconst meeting = new RealtimeKitClient({ authToken: '<token>', video: true, audio: true });\nawait meeting.join();\n```\n\n## In This Reference\n\n- [Configuration](./configuration.md) - Setup, installation, wrangler config\n- [API](./api.md) - Meeting object, REST API, SDK methods\n- [Patterns](./patterns.md) - Common workflows, code examples\n- [Gotchas](./gotchas.md) - Common issues, troubleshooting\n\n## See Also\n\n- [Workers](../workers/) - Backend integration\n- [D1](../d1/) - Meeting metadata storage\n- [R2](../r2/) - Recording storage\n- [KV](../kv/) - Session management\n\n## Reference Links\n\n- **Official Docs**: https://developers.cloudflare.com/realtime/realtimekit/\n- **API Reference**: https://developers.cloudflare.com/api/resources/realtime_kit/\n- **Examples**: https://github.com/cloudflare/realtimekit-web-examples\n- **Dashboard**: https://dash.cloudflare.com/?to=/:account/realtime/kit\n",
        ".agent/services/hosting/cloudflare-platform/references/sandbox/README.md": "# Cloudflare Sandbox SDK\n\nSecure isolated code execution in containers on Cloudflare's edge. Run untrusted code, manage files, expose services, integrate with AI agents.\n\n**Use cases**: AI code execution, interactive dev environments, data analysis, CI/CD, code interpreters, multi-tenant execution.\n\n## Architecture\n\n- Each sandbox = Durable Object + Container\n- Persistent across requests (same ID = same sandbox)\n- Isolated filesystem/processes/network\n- Configurable sleep/wake for cost optimization\n\n## Quick Start\n\n```typescript\nimport { getSandbox, proxyToSandbox, type Sandbox } from '@cloudflare/sandbox';\nexport { Sandbox } from '@cloudflare/sandbox';\n\ntype Env = { Sandbox: DurableObjectNamespace<Sandbox>; };\n\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    // CRITICAL: proxyToSandbox MUST be called first for preview URLs\n    const proxyResponse = await proxyToSandbox(request, env);\n    if (proxyResponse) return proxyResponse;\n\n    const sandbox = getSandbox(env.Sandbox, 'my-sandbox');\n    const result = await sandbox.exec('python3 -c \"print(2 + 2)\"');\n    return Response.json({ output: result.stdout });\n  }\n};\n```\n\n**wrangler.jsonc**:\n\n```jsonc\n{\n  \"name\": \"my-sandbox-worker\",\n  \"main\": \"src/index.ts\",\n  \"compatibility_date\": \"2024-01-01\",\n  \n  \"containers\": [{\n    \"class_name\": \"Sandbox\",\n    \"image\": \"./Dockerfile\",\n    \"instance_type\": \"lite\",        // lite | standard | heavy\n    \"max_instances\": 5\n  }],\n  \n  \"durable_objects\": {\n    \"bindings\": [{ \"class_name\": \"Sandbox\", \"name\": \"Sandbox\" }]\n  },\n  \n  \"migrations\": [{\n    \"tag\": \"v1\",\n    \"new_sqlite_classes\": [\"Sandbox\"]\n  }]\n}\n```\n\n**Dockerfile**:\n\n```dockerfile\nFROM docker.io/cloudflare/sandbox:latest\nRUN pip3 install --no-cache-dir pandas numpy matplotlib\nEXPOSE 8080 3000  # Required for wrangler dev\n```\n\n## Core APIs\n\n- `getSandbox(namespace, id, options?)` → Get/create sandbox\n- `sandbox.exec(command, options?)` → Execute command\n- `sandbox.readFile(path)` / `writeFile(path, content)` → File ops\n- `sandbox.startProcess(command, options)` → Background process\n- `sandbox.exposePort(port, options)` → Get preview URL\n- `sandbox.createSession(options)` → Isolated session\n\n## Critical Rules\n\n- ALWAYS call `proxyToSandbox()` first\n- Same ID = reuse sandbox\n- Use `/workspace` for persistent files\n- `normalizeId: true` for preview URLs\n- Retry on `CONTAINER_NOT_READY`\n\n## Resources\n\n- [Configuration](./configuration.md) - Config, CLI, environment\n- [API Reference](./api.md) - Programmatic API, testing\n- [Patterns](./patterns.md) - Common workflows, CI/CD\n- [Gotchas](./gotchas.md) - Issues, limits, best practices\n- [Official Docs](https://developers.cloudflare.com/sandbox/)\n",
        ".agent/services/hosting/cloudflare-platform/references/smart-placement/README.md": "# Cloudflare Workers Smart Placement\n\nAutomatic workload placement optimization to minimize latency by running Workers closer to backend infrastructure rather than end users.\n\n## Core Concept\n\nSmart Placement automatically analyzes Worker request duration across Cloudflare's global network and intelligently routes requests to optimal data center locations. Instead of defaulting to the location closest to the end user, Smart Placement can forward requests to locations closer to backend infrastructure when this reduces overall request duration.\n\n### When to Use\n\n**Enable Smart Placement when:**\n- Worker makes multiple round trips to backend services/databases\n- Backend infrastructure is geographically concentrated\n- Request duration dominated by backend latency rather than network latency from user\n- Running backend logic in Workers (APIs, data aggregation, SSR with DB calls)\n\n**Do NOT enable for:**\n- Workers serving only static content or cached responses\n- Workers without significant backend communication\n- Pure edge logic (auth checks, redirects, simple transformations)\n- Workers without fetch event handlers\n\n### Key Architecture Pattern\n\n**Recommended:** Split full-stack applications into separate Workers:\n\n```\nUser → Frontend Worker (at edge, close to user)\n         ↓ Service Binding\n       Backend Worker (Smart Placement enabled, close to DB/API)\n         ↓\n       Database/Backend Service\n```\n\nThis maintains fast, reactive frontends while optimizing backend latency.\n\n## Quick Start\n\n```toml\n# wrangler.toml\n[placement]\nmode = \"smart\"\nhint = \"wnam\"  # Optional: West North America\n```\n\nDeploy and wait 15 minutes for analysis. Check status via API or dashboard metrics.\n\n## Requirements\n\n- Wrangler 2.20.0+\n- Analysis time: Up to 15 minutes after enabling\n- Traffic requirements: Consistent traffic from multiple global locations\n- Available on all Workers plans (Free, Paid, Enterprise)\n\n## Placement Status Values\n\n```typescript\ntype PlacementStatus = \n  | undefined  // Not yet analyzed\n  | 'SUCCESS'  // Successfully optimized\n  | 'INSUFFICIENT_INVOCATIONS'  // Not enough traffic\n  | 'UNSUPPORTED_APPLICATION';  // Made Worker slower (reverted)\n```\n\n## CLI Commands\n\n```bash\n# Deploy with Smart Placement\nwrangler deploy\n\n# Check placement status\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  https://api.cloudflare.com/client/v4/accounts/$ACCOUNT_ID/workers/services/$WORKER_NAME \\\n  | jq .result.placement_status\n\n# Monitor\nwrangler tail your-worker-name --header cf-placement\n```\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - wrangler.toml setup, placement hints, dashboard config\n- [api.md](./api.md) - Placement Status API, cf-placement header, monitoring\n- [patterns.md](./patterns.md) - Frontend/backend split, database workers, SSR patterns\n- [gotchas.md](./gotchas.md) - Troubleshooting INSUFFICIENT_INVOCATIONS, performance issues\n\n## See Also\n\n- [workers](../workers/) - Worker runtime and fetch handlers\n- [d1](../d1/) - D1 database that benefits from Smart Placement\n- [durable-objects](../durable-objects/) - Durable Objects with backend logic\n- [bindings](../bindings/) - Service bindings for frontend/backend split\n",
        ".agent/services/hosting/cloudflare-platform/references/snippets/README.md": "# Cloudflare Snippets Skill Reference\n\n## Description\n\nExpert guidance for **Cloudflare Snippets ONLY** - a lightweight JavaScript-based edge logic platform for modifying HTTP requests and responses. Snippets run as part of the Ruleset Engine and are included at no additional cost on paid plans (Pro, Business, Enterprise)....\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/spectrum/README.md": "# Cloudflare Spectrum Skill Reference\n\n## Overview\n\nCloudflare Spectrum provides security and acceleration for ANY TCP or UDP-based application. It's a global Layer 4 (L4) reverse proxy running on Cloudflare's edge nodes that routes MQTT, email, file transfer, version control, games, and more through Cloudflare to mask origins and protec...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/static-assets/README.md": "# Cloudflare Static Assets Skill Reference\n\nExpert guidance for deploying and configuring static assets with Cloudflare Workers. This skill covers configuration patterns, routing architectures, asset binding usage, and best practices for SPAs, SSG sites, and full-stack applications....\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/stream/README.md": "# Cloudflare Stream\n\nServerless live and on-demand video streaming platform with one API.\n\n## Overview\n\nCloudflare Stream provides video upload, storage, encoding, and delivery without managing infrastructure. Runs on Cloudflare's global network.\n\n### Key Features\n\n- **On-demand video**: Upload, encode, store, deliver\n- **Live streaming**: RTMPS/SRT ingestion with ABR\n- **Direct creator uploads**: End users upload without API keys\n- **Signed URLs**: Token-based access control\n- **Analytics**: Server-side metrics via GraphQL\n- **Webhooks**: Processing notifications\n- **Captions**: Upload or AI-generate subtitles\n- **Watermarks**: Apply branding to videos\n- **Downloads**: Enable MP4 offline viewing\n\n## Core Concepts\n\n### Video Upload Methods\n\n1. **API Upload (TUS protocol)**: Direct server upload\n2. **Upload from URL**: Import from external source\n3. **Direct Creator Uploads**: User-generated content (recommended)\n\n### Playback Options\n\n1. **Stream Player (iframe)**: Built-in, optimized player\n2. **Custom Player (HLS/DASH)**: Video.js, HLS.js integration\n3. **Thumbnails**: Static or animated previews\n\n### Access Control\n\n- **Public**: No restrictions\n- **requireSignedURLs**: Token-based access\n- **allowedOrigins**: Domain restrictions\n- **Access Rules**: Geo/IP restrictions in tokens\n\n### Live Streaming\n\n- RTMPS/SRT ingest from OBS, FFmpeg\n- Automatic recording to on-demand\n- Simulcast to YouTube, Twitch, etc.\n- WebRTC support for browser streaming\n\n## Quick Start\n\n**Upload video via API**\n\n```bash\ncurl -X POST \\\n  \"https://api.cloudflare.com/client/v4/accounts/{account_id}/stream/copy\" \\\n  -H \"Authorization: Bearer <TOKEN>\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com/video.mp4\"}'\n```\n\n**Embed player**\n\n```html\n<iframe\n  src=\"https://customer-<CODE>.cloudflarestream.com/<VIDEO_ID>/iframe\"\n  style=\"border: none;\"\n  height=\"720\" width=\"1280\"\n  allow=\"accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;\"\n  allowfullscreen=\"true\"\n></iframe>\n```\n\n**Create live input**\n\n```bash\ncurl -X POST \\\n  \"https://api.cloudflare.com/client/v4/accounts/{account_id}/stream/live_inputs\" \\\n  -H \"Authorization: Bearer <TOKEN>\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"recording\": {\"mode\": \"automatic\"}}'\n```\n\n## Limits\n\n- Max file size: 30 GB\n- Max frame rate: 60 fps (recommended)\n- Supported formats: MP4, MKV, MOV, AVI, FLV, MPEG-2 TS/PS, MXF, LXF, GXF, 3GP, WebM, MPG, QuickTime\n\n## Pricing\n\n- $5/1000 min stored\n- $1/1000 min delivered\n\n## Resources\n\n- Dashboard: https://dash.cloudflare.com/?to=/:account/stream\n- API Docs: https://developers.cloudflare.com/api/resources/stream/\n- Stream Docs: https://developers.cloudflare.com/stream/\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - Setup, environment variables, wrangler config\n- [api.md](./api.md) - Upload, playback, live streaming, management APIs\n- [patterns.md](./patterns.md) - Full-stack flows, state management, best practices\n- [gotchas.md](./gotchas.md) - Error codes, troubleshooting, limits\n\n## See Also\n\n- [workers](../workers/) - Deploy Stream APIs in Workers\n- [pages](../pages/) - Integrate Stream with Pages\n- [workers-ai](../workers-ai/) - AI-generate captions\n",
        ".agent/services/hosting/cloudflare-platform/references/tail-workers/README.md": "# Cloudflare Tail Workers Skill\n\n## Purpose\n\nExpert guidance on Cloudflare Tail Workers—specialized Workers that consume execution events from producer Workers for logging, debugging, analytics, and observability.\n\n## When to Use\n\n- User implements observability/logging for Cloudflare Workers\n- User needs to process Worker execution events, logs, exceptions\n- User builds custom analytics or error tracking\n- User configures real-time event streaming\n- User mentions tail handlers, tail consumers, or producer Workers\n\n## Core Concepts\n\n### What Are Tail Workers?\n\nTail Workers automatically process events from producer Workers (the Workers being monitored). They receive:\n- HTTP request/response info\n- Console logs (console.log/error/warn/debug)\n- Uncaught exceptions\n- Execution outcomes (ok, exception, exceededCpu, etc.)\n- Diagnostic channel events\n\n**Key characteristics:**\n- Invoked AFTER producer finishes executing\n- Capture entire request lifecycle including Service Bindings and Dynamic Dispatch sub-requests\n- Billed by CPU time, not request count\n- Available on Workers Paid and Enterprise tiers\n\n### Alternative: OpenTelemetry Export\n\nFor batch exports to observability tools (Sentry, Grafana, Honeycomb):\n- Consider OTEL export instead of Tail Workers\n- OTEL sends logs/traces in batches (more efficient)\n- Tail Workers = advanced mode for custom processing\n\n## Implementation Patterns\n\n### Basic Tail Handler Structure\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    // Process events from producer Worker\n  }\n};\n```\n\n**Parameters:**\n- `events`: Array of `TailItem` objects (one per producer invocation)\n- `env`: Bindings (KV, D1, R2, env vars, etc.)\n- `ctx`: Context with `waitUntil()` for async work\n\n**CRITICAL:** Tail handlers don't return values. Use `ctx.waitUntil()` for async operations.\n\n### Event Structure (`TailItem`)\n\n```typescript\ninterface TailItem {\n  scriptName: string;           // Producer Worker name\n  eventTimestamp: number;        // Epoch time\n  outcome: 'ok' | 'exception' | 'exceededCpu' | 'exceededMemory' \n         | 'canceled' | 'scriptNotFound' | 'responseStreamDisconnected' | 'unknown';\n  \n  event: {\n    request?: {\n      url: string;               // Redacted by default\n      method: string;\n      headers: Record<string, string>;  // Sensitive headers redacted\n      cf: IncomingRequestCfProperties;\n      getUnredacted(): TailRequest;     // Bypass redaction (use carefully)\n    };\n    response?: {\n      status: number;\n    };\n  };\n  \n  logs: Array<{\n    timestamp: number;\n    level: 'debug' | 'info' | 'log' | 'warn' | 'error';\n    message: any[];              // Args passed to console function\n  }>;\n  \n  exceptions: Array<{\n    timestamp: number;\n    name: string;                // Error type (Error, TypeError, etc.)\n    message: string;             // Error description\n  }>;\n  \n  diagnosticsChannelEvents: Array<{\n    channel: string;\n    message: any;\n    timestamp: number;\n  }>;\n}\n```\n\n### Configuration\n\n**Producer Worker wrangler.toml:**\n\n```toml\nname = \"my-producer-worker\"\ntail_consumers = [{service = \"my-tail-worker\"}]\n```\n\n**Producer Worker wrangler.jsonc:**\n\n```json\n{\n  \"name\": \"my-producer-worker\",\n  \"tail_consumers\": [\n    {\n      \"service\": \"my-tail-worker\"\n    }\n  ]\n}\n```\n\n**Tail Worker wrangler.toml:**\n\n```toml\nname = \"my-tail-worker\"\n# No special config needed, just must have tail() handler\n```\n\n## Common Use Cases\n\n### 1. Send Logs to HTTP Endpoint\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    const payload = events.map(event => ({\n      script: event.scriptName,\n      timestamp: event.eventTimestamp,\n      outcome: event.outcome,\n      url: event.event?.request?.url,\n      status: event.event?.response?.status,\n      logs: event.logs,\n      exceptions: event.exceptions,\n    }));\n    \n    ctx.waitUntil(\n      fetch(env.LOG_ENDPOINT, {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify(payload),\n      })\n    );\n  }\n};\n```\n\n### 2. Error Tracking to External Service\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    for (const event of events) {\n      // Only process errors\n      if (event.outcome === 'exception' || event.exceptions.length > 0) {\n        ctx.waitUntil(\n          fetch(\"https://error-tracker.example.com/errors\", {\n            method: \"POST\",\n            headers: {\n              \"Authorization\": `Bearer ${env.ERROR_TRACKER_TOKEN}`,\n              \"Content-Type\": \"application/json\",\n            },\n            body: JSON.stringify({\n              script: event.scriptName,\n              timestamp: event.eventTimestamp,\n              exceptions: event.exceptions,\n              request: event.event?.request?.getUnredacted?.(),  // If needed\n              logs: event.logs,\n            }),\n          })\n        );\n      }\n    }\n  }\n};\n```\n\n### 3. Store Logs in KV\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    const promises = events.map(event => {\n      const key = `log:${event.scriptName}:${event.eventTimestamp}`;\n      const value = JSON.stringify({\n        outcome: event.outcome,\n        logs: event.logs,\n        exceptions: event.exceptions,\n      });\n      \n      // TTL: 24 hours\n      return env.LOGS_KV.put(key, value, { expirationTtl: 86400 });\n    });\n    \n    ctx.waitUntil(Promise.all(promises));\n  }\n};\n```\n\n### 4. Analytics Engine for Aggregated Metrics\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    const writes = events.map(event => \n      env.ANALYTICS.writeDataPoint({\n        // String dimensions\n        blobs: [\n          event.scriptName,\n          event.outcome,\n          event.event?.request?.method ?? 'unknown',\n        ],\n        // Numeric metrics\n        doubles: [\n          1,  // Count\n          event.event?.response?.status ?? 0,\n        ],\n        // Indexed fields for filtering\n        indexes: [\n          event.event?.request?.cf?.colo ?? 'unknown',\n        ],\n      })\n    );\n    \n    ctx.waitUntil(Promise.all(writes));\n  }\n};\n```\n\n### 5. Filter Specific Routes/Patterns\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    // Only process API routes\n    const apiEvents = events.filter(event => \n      event.event?.request?.url?.includes('/api/')\n    );\n    \n    if (apiEvents.length === 0) return;\n    \n    ctx.waitUntil(\n      fetch(env.API_LOGS_ENDPOINT, {\n        method: \"POST\",\n        body: JSON.stringify(apiEvents),\n      })\n    );\n  }\n};\n```\n\n### 6. Multi-Destination Logging\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    // Send errors to one place, everything else to another\n    const errors = events.filter(e => e.outcome === 'exception');\n    const success = events.filter(e => e.outcome === 'ok');\n    \n    const tasks = [];\n    \n    if (errors.length > 0) {\n      tasks.push(\n        fetch(env.ERROR_ENDPOINT, {\n          method: \"POST\",\n          body: JSON.stringify(errors),\n        })\n      );\n    }\n    \n    if (success.length > 0) {\n      tasks.push(\n        fetch(env.SUCCESS_ENDPOINT, {\n          method: \"POST\",\n          body: JSON.stringify(success),\n        })\n      );\n    }\n    \n    ctx.waitUntil(Promise.all(tasks));\n  }\n};\n```\n\n### 7. Performance Monitoring\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    const metrics = events.map(event => ({\n      script: event.scriptName,\n      timestamp: event.eventTimestamp,\n      duration: calculateDuration(event),  // Custom logic\n      outcome: event.outcome,\n      status: event.event?.response?.status,\n      colo: event.event?.request?.cf?.colo,\n    }));\n    \n    ctx.waitUntil(\n      fetch(env.METRICS_ENDPOINT, {\n        method: \"POST\",\n        headers: { \"X-API-Key\": env.METRICS_API_KEY },\n        body: JSON.stringify(metrics),\n      })\n    );\n  }\n};\n```\n\n## Security & Privacy\n\n### Automatic Redaction\n\nBy default, sensitive data is redacted from `TailRequest`:\n\n**Header redaction:**\n- Headers containing: `auth`, `key`, `secret`, `token`, `jwt` (case-insensitive)\n- `cookie` and `set-cookie` headers\n- Redacted values show as `\"REDACTED\"`\n\n**URL redaction:**\n- Hex IDs: 32+ hex digits → `\"REDACTED\"`\n- Base-64 IDs: 21+ chars with 2+ upper, 2+ lower, 2+ digits → `\"REDACTED\"`\n\n### Bypassing Redaction\n\n```typescript\n// Use with extreme caution\nconst unredacted = event.event?.request?.getUnredacted();\n// unredacted.url and unredacted.headers contain raw values\n```\n\n**Best practices:**\n- Only call `getUnredacted()` when absolutely necessary\n- Never log unredacted sensitive data\n- Implement additional filtering before external transmission\n- Use environment variables for API keys, never hardcode\n\n## Wrangler CLI Usage\n\n### Deploy Tail Worker\n\n```bash\nwrangler deploy\n```\n\n### View Live Tail Locally (NOT Tail Workers)\n\n```bash\n# This streams logs to terminal, different from Tail Workers\nwrangler tail <producer-worker-name>\n```\n\n### Update Producer Configuration\n\n```bash\n# Edit wrangler.toml to add tail_consumers\nwrangler deploy\n```\n\n### Remove Tail Consumer\n\n```toml\n# Remove from wrangler.toml or set empty array\ntail_consumers = []\n```\n\n## TypeScript Types\n\n```typescript\n// Add to your Tail Worker\nexport default {\n  async tail(\n    events: TailItem[],\n    env: Env,\n    ctx: ExecutionContext\n  ): Promise<void> {\n    // Implementation\n  }\n} satisfies ExportedHandler<Env>;\n\ninterface Env {\n  // Your bindings\n  LOGS_KV: KVNamespace;\n  ANALYTICS: AnalyticsEngineDataset;\n  LOG_ENDPOINT: string;\n  API_TOKEN: string;\n}\n```\n\n## Testing & Development\n\n### Local Testing\n\nTail Workers cannot be fully tested locally with `wrangler dev`. Deploy to staging environment for testing.\n\n### Testing Strategy\n\n1. Deploy producer Worker to staging\n2. Deploy Tail Worker to staging\n3. Configure `tail_consumers` in producer\n4. Trigger producer Worker requests\n5. Verify Tail Worker receives events (check destination logs/storage)\n\n### Debugging Tips\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    // Log to console for debugging (won't be captured by self)\n    console.log('Received events:', events.length);\n    \n    try {\n      // Your logic\n      await processEvents(events, env);\n    } catch (error) {\n      // Log errors\n      console.error('Tail Worker error:', error);\n      // Consider sending errors to monitoring service\n    }\n  }\n};\n```\n\n## Advanced Patterns\n\n### Batching Events\n\n```typescript\n// Use KV or Durable Objects to batch events before sending\nexport default {\n  async tail(events, env, ctx) {\n    const batch = await env.BATCH_DO.get(env.BATCH_DO.idFromName(\"batch\"));\n    ctx.waitUntil(batch.addEvents(events));\n  }\n};\n```\n\n### Sampling\n\n```typescript\n// Only process a percentage of events\nexport default {\n  async tail(events, env, ctx) {\n    const sampleRate = 0.1;  // 10%\n    const sampledEvents = events.filter(() => Math.random() < sampleRate);\n    \n    if (sampledEvents.length > 0) {\n      ctx.waitUntil(sendToEndpoint(sampledEvents, env));\n    }\n  }\n};\n```\n\n### Workers for Platforms\n\nFor dynamic dispatch Workers, `events` array contains TWO elements:\n1. Dynamic dispatch Worker event\n2. User Worker event\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    for (const event of events) {\n      // Distinguish between dispatch and user Worker\n      if (event.scriptName === 'dispatch-worker') {\n        // Handle dispatch Worker event\n      } else {\n        // Handle user Worker event\n      }\n    }\n  }\n};\n```\n\n## Common Pitfalls\n\n1. **Not using `ctx.waitUntil()`:**\n\n   ```typescript\n   // ❌ WRONG - async work may not complete\n   export default {\n     async tail(events) {\n       fetch(endpoint, { body: JSON.stringify(events) });\n     }\n   };\n   \n   // ✅ CORRECT\n   export default {\n     async tail(events, env, ctx) {\n       ctx.waitUntil(\n         fetch(endpoint, { body: JSON.stringify(events) })\n       );\n     }\n   };\n   ```\n\n2. **Missing tail() handler:**\n   Producer Worker deployment will fail if tail_consumers references a Worker without tail() handler.\n\n3. **Outcome vs HTTP Status:**\n   `outcome` is script execution status, NOT HTTP status. A Worker can return 500 but have outcome='ok' if script completed successfully.\n\n4. **Excessive logging:**\n   Tail Workers are invoked on EVERY producer invocation. Be mindful of volume and costs.\n\n5. **Blocking operations:**\n   Don't await in tail handler unless necessary. Use `ctx.waitUntil()` for fire-and-forget operations.\n\n## Integration Examples\n\n### Sentry\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    const errors = events.filter(e => \n      e.outcome === 'exception' || e.exceptions.length > 0\n    );\n    \n    for (const event of errors) {\n      ctx.waitUntil(\n        fetch(`https://sentry.io/api/${env.SENTRY_PROJECT}/store/`, {\n          method: \"POST\",\n          headers: {\n            \"X-Sentry-Auth\": `Sentry sentry_key=${env.SENTRY_KEY}`,\n            \"Content-Type\": \"application/json\",\n          },\n          body: JSON.stringify({\n            message: event.exceptions[0]?.message,\n            level: \"error\",\n            tags: { worker: event.scriptName },\n            extra: { event },\n          }),\n        })\n      );\n    }\n  }\n};\n```\n\n### Datadog\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    const logs = events.flatMap(event => \n      event.logs.map(log => ({\n        ddsource: \"cloudflare-worker\",\n        ddtags: `worker:${event.scriptName},outcome:${event.outcome}`,\n        hostname: event.event?.request?.cf?.colo,\n        message: log.message.join(\" \"),\n        status: log.level,\n        timestamp: log.timestamp,\n      }))\n    );\n    \n    ctx.waitUntil(\n      fetch(\"https://http-intake.logs.datadoghq.com/v1/input\", {\n        method: \"POST\",\n        headers: {\n          \"DD-API-KEY\": env.DATADOG_API_KEY,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify(logs),\n      })\n    );\n  }\n};\n```\n\n## Related Resources\n\n- Tail Workers Docs: https://developers.cloudflare.com/workers/observability/logs/tail-workers/\n- Tail Handler API: https://developers.cloudflare.com/workers/runtime-apis/handlers/tail/\n- Analytics Engine: https://developers.cloudflare.com/analytics/analytics-engine/\n- OpenTelemetry Export: https://developers.cloudflare.com/workers/observability/exporting-opentelemetry-data/\n\n## Decision Tree\n\n```\nNeed observability for Workers?\n├─ Batch export to known tools (Sentry/Grafana/Honeycomb)?\n│  └─ Use OpenTelemetry export (not Tail Workers)\n├─ Custom real-time processing needed?\n│  ├─ Aggregated metrics?\n│  │  └─ Use Tail Worker + Analytics Engine\n│  ├─ Error tracking?\n│  │  └─ Use Tail Worker + external service\n│  ├─ Custom logging/debugging?\n│  │  └─ Use Tail Worker + KV/HTTP endpoint\n│  └─ Complex event processing?\n│     └─ Use Tail Worker + Durable Objects\n└─ Quick debugging?\n   └─ Use `wrangler tail` (different from Tail Workers)\n```\n\n## Code Quality Guidelines\n\n### Type Safety\n\n```typescript\n// ✅ Use proper types\ninterface Env {\n  LOG_ENDPOINT: string;\n  API_TOKEN: string;\n}\n\nexport default {\n  async tail(\n    events: TailItem[],\n    env: Env,\n    ctx: ExecutionContext\n  ): Promise<void> {\n    // Type-safe implementation\n  }\n} satisfies ExportedHandler<Env>;\n\n// ❌ Avoid any\nexport default {\n  async tail(events: any, env: any, ctx: any) {\n    // Unsafe\n  }\n};\n```\n\n### Error Handling\n\n```typescript\nexport default {\n  async tail(events, env, ctx) {\n    ctx.waitUntil(\n      (async () => {\n        try {\n          await fetch(env.ENDPOINT, {\n            method: \"POST\",\n            body: JSON.stringify(events),\n          });\n        } catch (error) {\n          // Log to console or send to fallback\n          console.error(\"Failed to send events:\", error);\n        }\n      })()\n    );\n  }\n};\n```\n\n### Minimal, Surgical Changes\n\n- Process only necessary events (filter early)\n- Avoid unnecessary data transformations\n- Keep handlers focused and simple\n\n## Summary\n\nTail Workers provide real-time, custom event processing for Cloudflare Workers. Use them when you need fine-grained control over logging, error tracking, or analytics that goes beyond standard OTEL export. Always use `ctx.waitUntil()` for async work, be mindful of sensitive data redaction, and consider Analytics Engine for aggregated metrics.\n",
        ".agent/services/hosting/cloudflare-platform/references/terraform/README.md": "# Cloudflare Terraform Provider\n\n**Expert guidance for Cloudflare Terraform Provider - infrastructure as code for Cloudflare resources.**\n\n## Core Principles\n\n- **Provider-first**: Use Terraform provider for ALL infrastructure - never mix with wrangler.toml for the same resources\n- **State management**: Always use remote state (S3, Terraform Cloud, etc.) for team environments\n- **Modular architecture**: Create reusable modules for common patterns (zones, workers, pages)\n- **Version pinning**: Always pin provider version with `~>` for predictable upgrades\n- **Secret management**: Use variables + environment vars for sensitive data - never hardcode API tokens\n\n## Provider Setup\n\n### Basic Configuration\n\n```hcl\nterraform {\n  required_version = \">= 1.0\"\n  \n  required_providers {\n    cloudflare = {\n      source  = \"cloudflare/cloudflare\"\n      version = \"~> 5.15.0\"\n    }\n  }\n}\n\nprovider \"cloudflare\" {\n  api_token = var.cloudflare_api_token  # or CLOUDFLARE_API_TOKEN env var\n}\n```\n\n### Authentication Methods (priority order)\n\n1. **API Token** (RECOMMENDED): `api_token` or `CLOUDFLARE_API_TOKEN`\n   - Create: Dashboard → My Profile → API Tokens\n   - Scope to specific accounts/zones for security\n\n2. **Global API Key** (LEGACY): `api_key` + `api_email` or `CLOUDFLARE_API_KEY` + `CLOUDFLARE_EMAIL`\n   - Less secure, use tokens instead\n\n3. **User Service Key**: `user_service_key` for Origin CA certificates\n\n### Backend Configuration\n\n```hcl\nterraform {\n  backend \"s3\" {\n    bucket = \"terraform-state\"\n    key    = \"cloudflare/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n```\n\n## Quick Reference: Common Commands\n\n```bash\nterraform init          # Initialize provider\nterraform plan          # Plan changes\nterraform apply         # Apply changes\nterraform destroy       # Destroy resources\nterraform import cloudflare_zone.example <zone-id>  # Import existing\nterraform state list    # List resources in state\nterraform output        # Show outputs\nterraform fmt -recursive  # Format code\nterraform validate      # Validate configuration\n```\n\n## See Also\n\n- [Configuration Reference](./configuration.md) - Resources for zones, DNS, workers, KV, R2, D1, Pages, rulesets\n- [API Reference](./api.md) - Data sources for existing resources\n- [Patterns & Use Cases](./patterns.md) - Architecture patterns, multi-env setup, CI/CD integration\n- [Troubleshooting & Best Practices](./gotchas.md) - Common issues, security, best practices\n",
        ".agent/services/hosting/cloudflare-platform/references/tunnel/README.md": "# Cloudflare Tunnel\n\nSecure outbound-only connections between infrastructure and Cloudflare's global network.\n\n## Overview\n\nCloudflare Tunnel (formerly Argo Tunnel) enables:\n- **Outbound-only connections** - No inbound ports or firewall changes\n- **Public hostname routing** - Expose local services to internet\n- **Private network access** - Connect internal networks via WARP\n- **Zero Trust integration** - Built-in access policies\n\n**Architecture**: Tunnel (persistent object) → Connector (`cloudflared` process) → Origin services\n\n## Quick Start\n\n```bash\n# Install cloudflared\nbrew install cloudflared  # macOS\n\n# Authenticate\ncloudflared tunnel login\n\n# Create tunnel\ncloudflared tunnel create my-tunnel\n\n# Route DNS\ncloudflared tunnel route dns my-tunnel app.example.com\n\n# Run tunnel\ncloudflared tunnel run my-tunnel\n```\n\n## Core Commands\n\n```bash\n# Tunnel lifecycle\ncloudflared tunnel create <name>\ncloudflared tunnel list\ncloudflared tunnel info <name>\ncloudflared tunnel delete <name>\n\n# DNS routing\ncloudflared tunnel route dns <tunnel> <hostname>\ncloudflared tunnel route list\n\n# Private network\ncloudflared tunnel route ip add 10.0.0.0/8 <tunnel>\n\n# Run tunnel\ncloudflared tunnel run <name>\n```\n\n## Configuration Example\n\n```yaml\n# ~/.cloudflared/config.yml\ntunnel: 6ff42ae2-765d-4adf-8112-31c55c1551ef\ncredentials-file: /root/.cloudflared/6ff42ae2-765d-4adf-8112-31c55c1551ef.json\n\ningress:\n  - hostname: app.example.com\n    service: http://localhost:8000\n  - hostname: api.example.com\n    service: https://localhost:8443\n    originRequest:\n      noTLSVerify: true\n  - service: http_status:404\n```\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - Config file options, ingress rules, TLS settings\n- [api.md](./api.md) - Cloudflare API, remotely-managed tunnels, programmatic control\n- [patterns.md](./patterns.md) - Docker, Kubernetes, HA, service types, use cases\n- [gotchas.md](./gotchas.md) - Troubleshooting, limitations, best practices\n\n## See Also\n\n- [workers](../workers/) - Workers with Tunnel integration\n- [access](../access/) - Zero Trust access policies\n- [warp](../warp/) - WARP client for private networks\n",
        ".agent/services/hosting/cloudflare-platform/references/turn/README.md": "# Cloudflare TURN Service\n\nExpert guidance for implementing Cloudflare TURN Service in WebRTC applications.\n\n## Overview\n\nCloudflare TURN (Traversal Using Relays around NAT) Service is a managed relay service for WebRTC applications. TURN acts as a relay point for traffic between WebRTC clients and SFUs, particularly when direct peer-to-peer communication is obstructed by NATs or firewalls. The service runs on Cloudflare's global anycast network across 310+ cities.\n\n## Key Characteristics\n\n- **Anycast Architecture**: Automatically connects clients to the closest Cloudflare location\n- **Global Network**: Available across Cloudflare's entire network (excluding China Network)\n- **Zero Configuration**: No need to manually select regions or servers\n- **Protocol Support**: STUN/TURN over UDP, TCP, and TLS\n- **Free Tier**: Free when used with Cloudflare Calls SFU, otherwise $0.05/GB outbound\n\n## Service Addresses and Ports\n\n### STUN over UDP\n\n- **Primary**: `stun.cloudflare.com:3478/udp`\n- **Alternate**: `stun.cloudflare.com:53/udp` (not recommended as primary, blocked by many ISPs/browsers)\n\n### TURN over UDP\n\n- **Primary**: `turn.cloudflare.com:3478/udp`\n- **Alternate**: `turn.cloudflare.com:53/udp`\n\n### TURN over TCP\n\n- **Primary**: `turn.cloudflare.com:3478/tcp`\n- **Alternate**: `turn.cloudflare.com:80/tcp`\n\n### TURN over TLS\n\n- **Primary**: `turn.cloudflare.com:5349/tcp`\n- **Alternate**: `turn.cloudflare.com:443/tcp`\n\n## API Endpoints\n\nAll API endpoints require authentication with a Cloudflare API token with \"Calls Write\" permission.\n\nBase URL: `https://api.cloudflare.com/client/v4`\n\n### List TURN Keys\n\n```\nGET /accounts/{account_id}/calls/turn_keys\n```\n\n### Get TURN Key Details\n\n```\nGET /accounts/{account_id}/calls/turn_keys/{key_id}\n```\n\n### Create TURN Key\n\n```\nPOST /accounts/{account_id}/calls/turn_keys\nContent-Type: application/json\n\n{\n  \"name\": \"my-turn-key\"\n}\n```\n\n**Response includes**:\n- `uid`: Key identifier\n- `key`: The actual secret key (only returned on creation)\n- `name`: Human-readable name\n- `created`: ISO 8601 timestamp\n- `modified`: ISO 8601 timestamp\n\n### Update TURN Key\n\n```\nPUT /accounts/{account_id}/calls/turn_keys/{key_id}\nContent-Type: application/json\n\n{\n  \"name\": \"updated-name\"\n}\n```\n\n### Delete TURN Key\n\n```\nDELETE /accounts/{account_id}/calls/turn_keys/{key_id}\n```\n\n## Generate Temporary Credentials\n\nTo use TURN, clients need temporary credentials. Generate them via:\n\n```\nPOST https://rtc.live.cloudflare.com/v1/turn/keys/{key_id}/credentials/generate\nAuthorization: Bearer {key_secret}\nContent-Type: application/json\n\n{\n  \"ttl\": 86400  // optional, defaults to reasonable value\n}\n```\n\n**Response**:\n\n```json\n{\n  \"iceServers\": {\n    \"urls\": [\n      \"stun:stun.cloudflare.com:3478\",\n      \"turn:turn.cloudflare.com:3478?transport=udp\",\n      \"turn:turn.cloudflare.com:3478?transport=tcp\",\n      \"turns:turn.cloudflare.com:5349?transport=tcp\"\n    ],\n    \"username\": \"generated-username\",\n    \"credential\": \"generated-credential\"\n  }\n}\n```\n\n## Implementation Patterns\n\n### Basic TURN Configuration (Browser)\n\n```typescript\ninterface RTCIceServer {\n  urls: string | string[];\n  username?: string;\n  credential?: string;\n  credentialType?: \"password\" | \"oauth\";\n}\n\nasync function getTURNConfig(): Promise<RTCIceServer[]> {\n  const response = await fetch('/api/turn-credentials');\n  const data = await response.json();\n  \n  return [\n    {\n      urls: 'stun:stun.cloudflare.com:3478'\n    },\n    {\n      urls: [\n        'turn:turn.cloudflare.com:3478?transport=udp',\n        'turn:turn.cloudflare.com:3478?transport=tcp',\n        'turns:turn.cloudflare.com:5349?transport=tcp'\n      ],\n      username: data.username,\n      credential: data.credential,\n      credentialType: 'password'\n    }\n  ];\n}\n\n// Use in RTCPeerConnection\nconst iceServers = await getTURNConfig();\nconst peerConnection = new RTCPeerConnection({ iceServers });\n```\n\n### Credential Generation (Backend - Node.js/TypeScript)\n\n```typescript\nasync function generateTURNCredentials(\n  turnKeyId: string,\n  turnKeySecret: string,\n  ttl: number = 86400\n): Promise<{ username: string; credential: string; urls: string[] }> {\n  const response = await fetch(\n    `https://rtc.live.cloudflare.com/v1/turn/keys/${turnKeyId}/credentials/generate`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${turnKeySecret}`,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({ ttl })\n    }\n  );\n\n  if (!response.ok) {\n    throw new Error(`Failed to generate TURN credentials: ${response.statusText}`);\n  }\n\n  const data = await response.json();\n  return {\n    username: data.iceServers.username,\n    credential: data.iceServers.credential,\n    urls: data.iceServers.urls\n  };\n}\n```\n\n### Cloudflare Worker Integration\n\n```typescript\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const url = new URL(request.url);\n    \n    if (url.pathname !== '/turn-credentials') {\n      return new Response('Not found', { status: 404 });\n    }\n\n    // Validate client (implement your auth logic)\n    const authHeader = request.headers.get('Authorization');\n    if (!authHeader) {\n      return new Response('Unauthorized', { status: 401 });\n    }\n\n    // Generate credentials\n    const response = await fetch(\n      `https://rtc.live.cloudflare.com/v1/turn/keys/${env.TURN_KEY_ID}/credentials/generate`,\n      {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${env.TURN_KEY_SECRET}`,\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({ ttl: 3600 })\n      }\n    );\n\n    if (!response.ok) {\n      return new Response('Failed to generate credentials', { status: 500 });\n    }\n\n    const data = await response.json();\n    \n    return new Response(JSON.stringify({\n      iceServers: [\n        {\n          urls: 'stun:stun.cloudflare.com:3478'\n        },\n        {\n          urls: data.iceServers.urls,\n          username: data.iceServers.username,\n          credential: data.iceServers.credential\n        }\n      ]\n    }), {\n      headers: { 'Content-Type': 'application/json' }\n    });\n  }\n};\n```\n\n### Credentials Caching Pattern\n\n```typescript\nclass TURNCredentialsManager {\n  private credentials: {\n    username: string;\n    credential: string;\n    urls: string[];\n    expiresAt: number;\n  } | null = null;\n\n  async getCredentials(\n    turnKeyId: string,\n    turnKeySecret: string\n  ): Promise<RTCIceServer[]> {\n    const now = Date.now();\n    \n    // Return cached credentials if still valid\n    if (this.credentials && this.credentials.expiresAt > now) {\n      return this.buildIceServers(this.credentials);\n    }\n\n    // Generate new credentials\n    const ttl = 3600; // 1 hour\n    const data = await this.generateCredentials(turnKeyId, turnKeySecret, ttl);\n    \n    this.credentials = {\n      username: data.username,\n      credential: data.credential,\n      urls: data.urls,\n      expiresAt: now + (ttl * 1000) - 60000 // Refresh 1 min early\n    };\n\n    return this.buildIceServers(this.credentials);\n  }\n\n  private async generateCredentials(\n    turnKeyId: string,\n    turnKeySecret: string,\n    ttl: number\n  ) {\n    const response = await fetch(\n      `https://rtc.live.cloudflare.com/v1/turn/keys/${turnKeyId}/credentials/generate`,\n      {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${turnKeySecret}`,\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({ ttl })\n      }\n    );\n\n    const data = await response.json();\n    return {\n      username: data.iceServers.username,\n      credential: data.iceServers.credential,\n      urls: data.iceServers.urls\n    };\n  }\n\n  private buildIceServers(creds: {\n    username: string;\n    credential: string;\n    urls: string[];\n  }): RTCIceServer[] {\n    return [\n      { urls: 'stun:stun.cloudflare.com:3478' },\n      {\n        urls: creds.urls,\n        username: creds.username,\n        credential: creds.credential,\n        credentialType: 'password' as const\n      }\n    ];\n  }\n}\n```\n\n### Type-Safe Configuration\n\n```typescript\ninterface CloudflareTURNConfig {\n  keyId: string;\n  keySecret: string;\n  ttl?: number;\n  protocols?: ('udp' | 'tcp' | 'tls')[];\n}\n\ninterface TURNCredentials {\n  username: string;\n  credential: string;\n  urls: string[];\n  expiresAt: Date;\n}\n\nfunction validateRTCIceServer(obj: unknown): obj is RTCIceServer {\n  if (!obj || typeof obj !== 'object') {\n    return false;\n  }\n\n  const server = obj as Record<string, unknown>;\n\n  if (typeof server.urls !== 'string' && !Array.isArray(server.urls)) {\n    return false;\n  }\n\n  if (server.username && typeof server.username !== 'string') {\n    return false;\n  }\n\n  if (server.credential && typeof server.credential !== 'string') {\n    return false;\n  }\n\n  return true;\n}\n\nasync function fetchTURNServers(\n  config: CloudflareTURNConfig\n): Promise<RTCIceServer[]> {\n  const response = await fetch(\n    `https://rtc.live.cloudflare.com/v1/turn/keys/${config.keyId}/credentials/generate`,\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${config.keySecret}`,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({ ttl: config.ttl ?? 3600 })\n    }\n  );\n\n  if (!response.ok) {\n    throw new Error(`TURN credential generation failed: ${response.status}`);\n  }\n\n  const data = await response.json();\n  const iceServers = [\n    { urls: 'stun:stun.cloudflare.com:3478' },\n    {\n      urls: data.iceServers.urls,\n      username: data.iceServers.username,\n      credential: data.iceServers.credential,\n      credentialType: 'password' as const\n    }\n  ];\n\n  // Validate before returning\n  if (!iceServers.every(validateRTCIceServer)) {\n    throw new Error('Invalid ICE server configuration received');\n  }\n\n  return iceServers;\n}\n```\n\n## Common Use Cases\n\n### 1. Video Conferencing\n\nUse TURN as fallback when direct peer-to-peer fails due to restrictive NATs/firewalls.\n\n```typescript\nconst config: RTCConfiguration = {\n  iceServers: await getTURNConfig(),\n  iceTransportPolicy: 'all' // Try direct connection first\n};\n```\n\n### 2. Screen Sharing Applications\n\nEnsure connectivity for high-bandwidth screen sharing streams.\n\n```typescript\nconst iceServers = await getTURNConfig();\nconst pc = new RTCPeerConnection({ \n  iceServers,\n  bundlePolicy: 'max-bundle' // Reduce overhead\n});\n```\n\n### 3. IoT Device Communication\n\nEnable WebRTC for devices behind restrictive NATs.\n\n```typescript\n// Prefer relay for predictable connectivity\nconst config: RTCConfiguration = {\n  iceServers: await getTURNConfig(),\n  iceTransportPolicy: 'relay' // Force TURN usage\n};\n```\n\n### 4. Live Streaming (WHIP/WHEP)\n\nIntegrate with Cloudflare Stream for low-latency broadcasting.\n\n```typescript\nconst turnServers = await getTURNConfig();\n// Use in WHIP/WHEP workflow with Cloudflare Stream\n```\n\n## Limits and Quotas\n\nPer TURN allocation (per user):\n- **IP addresses**: >5 new unique IPs per second\n- **Packet rate**: 5-10k packets per second (inbound/outbound)\n- **Data rate**: 50-100 Mbps (inbound/outbound)\n- **MTU**: No specific limit\n- **Burst rates**: Higher than documented limits\n\nLimits apply per allocation, not account-wide. Exceeding limits results in packet drops.\n\n## TLS Configuration\n\n### Supported TLS Versions\n\n- TLS 1.1\n- TLS 1.2\n- TLS 1.3\n\n### Recommended Ciphers (TLS 1.3)\n\n- AEAD-AES128-GCM-SHA256\n- AEAD-AES256-GCM-SHA384\n- AEAD-CHACHA20-POLY1305-SHA256\n\n### Recommended Ciphers (TLS 1.2)\n\n- ECDHE-ECDSA-AES128-GCM-SHA256\n- ECDHE-RSA-AES128-GCM-SHA256\n- ECDHE-RSA-AES128-SHA (also TLS 1.1)\n- AES128-GCM-SHA256\n\n## Environment Variables Pattern\n\n```bash\n# .env\nCLOUDFLARE_ACCOUNT_ID=your_account_id\nCLOUDFLARE_API_TOKEN=your_api_token\nTURN_KEY_ID=your_turn_key_id\nTURN_KEY_SECRET=your_turn_key_secret\n```\n\n```typescript\n// config.ts\nimport { z } from 'zod';\n\nconst envSchema = z.object({\n  CLOUDFLARE_ACCOUNT_ID: z.string().min(1),\n  CLOUDFLARE_API_TOKEN: z.string().min(1),\n  TURN_KEY_ID: z.string().min(1),\n  TURN_KEY_SECRET: z.string().min(1)\n});\n\nexport const config = envSchema.parse(process.env);\n```\n\n## Wrangler Configuration\n\n```toml\n# wrangler.toml\nname = \"turn-credentials-api\"\nmain = \"src/index.ts\"\ncompatibility_date = \"2024-01-01\"\n\n[vars]\nTURN_KEY_ID = \"your-turn-key-id\"\n\n[[env.production.kv_namespaces]]\nbinding = \"CREDENTIALS_CACHE\"\nid = \"your-kv-namespace-id\"\n\n[env.production]\nvars = { ENVIRONMENT = \"production\" }\n\n# Store TURN_KEY_SECRET in secrets\n# wrangler secret put TURN_KEY_SECRET\n```\n\n## Security Best Practices\n\n1. **Never expose TURN key secrets client-side**\n   - Always generate credentials server-side\n   - Use a backend API endpoint\n\n2. **Implement rate limiting**\n\n   ```typescript\n   // Limit credential generation per client\n   const rateLimiter = new Map<string, number>();\n   \n   function checkRateLimit(clientId: string): boolean {\n     const lastRequest = rateLimiter.get(clientId) ?? 0;\n     const now = Date.now();\n     \n     if (now - lastRequest < 5000) { // 5 second cooldown\n       return false;\n     }\n     \n     rateLimiter.set(clientId, now);\n     return true;\n   }\n   ```\n\n3. **Set appropriate TTLs**\n   - Short-lived sessions: 1800-3600 seconds (30 min - 1 hour)\n   - Long-lived sessions: 86400 seconds (24 hours max recommended)\n\n4. **Validate client authentication**\n\n   ```typescript\n   async function validateClient(request: Request): Promise<boolean> {\n     const token = request.headers.get('Authorization')?.split(' ')[1];\n     if (!token) return false;\n     \n     // Implement JWT validation or session check\n     return validateToken(token);\n   }\n   ```\n\n5. **Monitor usage**\n   - Track credential generation requests\n   - Alert on unusual patterns\n   - Log failed authentication attempts\n\n## Troubleshooting\n\n### Issue: TURN credentials not working\n\n**Check:**\n- Key ID and secret are correct\n- Credentials haven't expired (check TTL)\n- Server can reach rtc.live.cloudflare.com\n- Network allows outbound HTTPS\n\n### Issue: Slow connection establishment\n\n**Solutions:**\n- Ensure proper ICE candidate gathering\n- Check network latency to Cloudflare edge\n- Verify firewall allows WebRTC ports\n- Consider using TURN over TLS (port 443)\n\n### Issue: High packet loss\n\n**Check:**\n- Not exceeding rate limits (5-10k pps)\n- Not exceeding bandwidth limits (50-100 Mbps)\n- Not connecting to too many unique IPs (>5/sec)\n- Client network quality\n\n### Debugging ICE Connectivity\n\n```typescript\npc.addEventListener('icecandidate', (event) => {\n  if (event.candidate) {\n    console.log('ICE candidate:', {\n      type: event.candidate.type,\n      protocol: event.candidate.protocol,\n      address: event.candidate.address,\n      port: event.candidate.port\n    });\n  }\n});\n\npc.addEventListener('iceconnectionstatechange', () => {\n  console.log('ICE connection state:', pc.iceConnectionState);\n});\n\n// Check which candidate pair was selected\nconst stats = await pc.getStats();\nstats.forEach(report => {\n  if (report.type === 'candidate-pair' && report.selected) {\n    console.log('Selected candidate pair:', report);\n  }\n});\n```\n\n## Monitoring and Analytics\n\n```typescript\ninterface TURNMetrics {\n  totalRequests: number;\n  failedRequests: number;\n  averageLatency: number;\n  activeConnections: number;\n}\n\nclass TURNMonitor {\n  private metrics: TURNMetrics = {\n    totalRequests: 0,\n    failedRequests: 0,\n    averageLatency: 0,\n    activeConnections: 0\n  };\n\n  async trackRequest<T>(\n    operation: () => Promise<T>\n  ): Promise<T> {\n    const start = Date.now();\n    this.metrics.totalRequests++;\n\n    try {\n      const result = await operation();\n      this.updateLatency(Date.now() - start);\n      return result;\n    } catch (error) {\n      this.metrics.failedRequests++;\n      throw error;\n    }\n  }\n\n  private updateLatency(latency: number): void {\n    this.metrics.averageLatency = \n      (this.metrics.averageLatency + latency) / 2;\n  }\n\n  getMetrics(): TURNMetrics {\n    return { ...this.metrics };\n  }\n}\n```\n\n## Architecture Considerations\n\n### Anycast Benefits\n\n- **Automatic routing**: Clients connect to nearest location\n- **No region selection**: BGP handles routing\n- **Low latency**: 95% of users within 50ms of edge\n- **Fault tolerance**: Network handles failover\n\n### When to Use TURN\n\n- **Restrictive NATs**: Symmetric NATs that block direct connections\n- **Corporate firewalls**: Environments blocking WebRTC ports\n- **Mobile networks**: Carrier-grade NAT scenarios\n- **Predictable connectivity**: When reliability > efficiency\n\n### Integration with Cloudflare Calls SFU\n\n```typescript\n// TURN is automatically used when needed\n// Cloudflare Calls handles TURN + SFU coordination\nconst session = await callsClient.createSession({\n  appId: 'your-app-id',\n  sessionId: 'meeting-123'\n});\n```\n\n## Cost Optimization\n\n1. **Use appropriate TTLs**: Don't over-provision credential lifetime\n2. **Implement credential caching**: Reuse credentials when possible\n3. **Set iceTransportPolicy wisely**:\n   - `'all'`: Try direct first (recommended for most cases)\n   - `'relay'`: Force TURN (only when necessary)\n4. **Monitor usage**: Track bandwidth to avoid surprises\n5. **Use with Cloudflare Calls**: Free when used with SFU\n\n## Additional Resources\n\n- [Cloudflare Calls Documentation](https://developers.cloudflare.com/calls/)\n- [Cloudflare TURN Service Docs](https://developers.cloudflare.com/realtime/turn/)\n- [Cloudflare API Reference](https://developers.cloudflare.com/api/resources/calls/subresources/turn/)\n- [WebRTC for the Curious](https://webrtcforthecurious.com/)\n- [Orange Meets (Open Source Example)](https://github.com/cloudflare/orange)\n\n## Related Cloudflare Services\n\n- **Cloudflare Calls SFU**: Managed Selective Forwarding Unit\n- **Cloudflare Stream**: Video streaming with WHIP/WHEP support\n- **Cloudflare Workers**: Backend for credential generation\n- **Cloudflare KV**: Credential caching\n- **Cloudflare Durable Objects**: Session state management\n",
        ".agent/services/hosting/cloudflare-platform/references/turnstile/README.md": "# Cloudflare Turnstile Implementation Skill Reference\n\nExpert guidance for implementing Cloudflare Turnstile - a smart CAPTCHA alternative that protects websites from bots without showing traditional CAPTCHA puzzles....\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/vectorize/README.md": "# Cloudflare Vectorize Skill\n\nExpert guidance for Cloudflare Vectorize - globally distributed vector database for AI applications.\n\n## Overview\n\nVectorize is Cloudflare's vector database that enables building full-stack AI-powered applications with Workers. It stores and queries vector embeddings for semantic search, recommendations, classification, and anomaly detection.\n\n**Key Features:**\n- Globally distributed vector database\n- Seamless integration with Workers AI\n- Support for dimensions up to 1536 (32-bit float precision)\n- Metadata filtering (up to 10 indexes per Vectorize index)\n- Namespace support for index segmentation\n- Three distance metrics: euclidean, cosine, dot-product\n- Up to 5M vectors per index (V2)\n\n**Status:** Generally Available (GA)\n\n## Index Configuration\n\n### Creating Indexes\n\nUse `wrangler vectorize create` with required parameters:\n\n```bash\n# Wrangler 3.71.0+ required for V2 indexes\nnpx wrangler@latest vectorize create <index-name> \\\n  --dimensions=<number> \\\n  --metric=<euclidean|cosine|dot-product>\n```\n\n**CRITICAL: Index configuration is immutable after creation. Cannot change dimensions or metric.**\n\n#### Distance Metrics\n\n| Metric | Best For | Score Interpretation |\n|--------|----------|---------------------|\n| `euclidean` | Absolute distance, spatial data | Lower = closer (0.0 = identical) |\n| `cosine` | Text embeddings, semantic similarity | Higher = closer (1.0 = identical) |\n| `dot-product` | Recommendation systems, normalized vectors | Higher = closer |\n\n**Metric Selection:**\n- Text/semantic search: `cosine` (most common)\n- Image similarity: `euclidean`\n- Pre-normalized vectors: `dot-product`\n\n#### Naming Conventions\n\nIndex names must:\n- Be lowercase and/or numeric ASCII\n- Start with a letter\n- Use dashes (-) instead of spaces\n- Be < 32 characters\n- Be descriptive: `production-doc-search`, `dev-recommendation-engine`\n\n### Metadata Indexes\n\nEnable filtering on metadata properties (up to 10 per index):\n\n```bash\n# Create metadata index BEFORE inserting vectors\nnpx wrangler vectorize create-metadata-index <index-name> \\\n  --property-name=<field-name> \\\n  --type=<string|number|boolean>\n```\n\n**Important:**\n- Create metadata indexes BEFORE inserting vectors\n- Existing vectors won't be indexed retroactively (must re-upsert)\n- String fields: first 64 bytes indexed (UTF-8 boundary)\n- Number fields: float64 precision\n- Max 10 metadata indexes per Vectorize index\n\n**Cardinality Considerations:**\n- **High cardinality** (UUIDs, millisecond timestamps): Good for `$eq`, poor for range queries\n- **Low cardinality** (enum values, status): Good for filters, less selective\n- **Best practice**: Bucket high-cardinality data (e.g., round timestamps to 5-min windows)\n\n### Management Commands\n\n```bash\n# List metadata indexes\nnpx wrangler vectorize list-metadata-index <index-name>\n\n# Delete metadata index\nnpx wrangler vectorize delete-metadata-index <index-name> --property-name=<field>\n\n# Get index info (vector count, processed mutations)\nnpx wrangler vectorize info <index-name>\n\n# List vector IDs (paginated, 1-1000 per page)\nnpx wrangler vectorize list-vectors <index-name> \\\n  --count=100 \\\n  --cursor=<pagination-cursor>\n```\n\n## Worker Binding\n\n### Configuration\n\n**wrangler.jsonc:**\n\n```jsonc\n{\n  \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n  \"vectorize\": [\n    {\n      \"binding\": \"VECTORIZE\",\n      \"index_name\": \"production-index\"\n    }\n  ]\n}\n```\n\n**wrangler.toml:**\n\n```toml\n[[vectorize]]\nbinding = \"VECTORIZE\"  # Available as env.VECTORIZE\nindex_name = \"production-index\"\n```\n\n### TypeScript Types\n\n```typescript\nexport interface Env {\n  VECTORIZE: Vectorize;\n}\n\n// Generate types after config changes\n// npx wrangler types\n```\n\n## Vector Operations\n\n### Vector Format\n\n```typescript\ninterface VectorizeVector {\n  id: string;              // Unique identifier (max 64 bytes)\n  values: number[] | Float32Array | Float64Array;  // Match index dimensions\n  namespace?: string;      // Optional partition key (max 64 bytes)\n  metadata?: Record<string, string | number | boolean | null>;  // Max 10 KiB\n}\n```\n\n**Vector Values:**\n- Array of numbers, Float32Array, or Float64Array\n- Must match index dimensions exactly\n- Stored as Float32 (Float64 converted on insert)\n- Dense arrays only (no sparse vectors)\n\n### Insert vs Upsert\n\n```typescript\n// INSERT: Ignore duplicates (keeps first)\nconst inserted = await env.VECTORIZE.insert([\n  {\n    id: \"1\",\n    values: [0.12, 0.45, 0.67, ...],\n    metadata: { url: \"/products/sku/123\", category: \"electronics\" }\n  }\n]);\n\n// UPSERT: Overwrite existing (keeps last)\nconst upserted = await env.VECTORIZE.upsert([\n  {\n    id: \"1\",\n    values: [0.15, 0.48, 0.70, ...],\n    metadata: { url: \"/products/sku/123\", category: \"electronics\", updated: true }\n  }\n]);\n```\n\n**Key Differences:**\n- `insert()`: Duplicate IDs ignored, first insert wins\n- `upsert()`: Overwrites completely (no merge), last upsert wins\n- Both return `{ mutationId: string }`\n- Asynchronous: Takes a few seconds to be queryable\n\n**Batch Limits:**\n- Workers: 1000 vectors per batch\n- HTTP API: 5000 vectors per batch\n- File upload: 100 MB max\n\n### Querying\n\n#### Basic Query\n\n```typescript\n// Query vector: must match index dimensions\nconst queryVector: number[] = [0.13, 0.25, 0.44, ...];\n\nconst matches = await env.VECTORIZE.query(queryVector, {\n  topK: 5,                    // Default: 5, Max: 100 (or 20 with values/metadata)\n  returnValues: false,        // Default: false\n  returnMetadata: \"none\",     // \"none\" | \"indexed\" | \"all\"\n  namespace?: \"user-123\",     // Optional namespace filter\n  filter?: { category: \"electronics\" }  // Optional metadata filter\n});\n```\n\n**Response:**\n\n```typescript\ninterface VectorizeMatches {\n  count: number;\n  matches: Array<{\n    id: string;\n    score: number;           // Distance score (interpretation depends on metric)\n    values?: number[];       // If returnValues: true\n    metadata?: Record<string, any>;  // If returnMetadata != \"none\"\n  }>;\n}\n```\n\n#### Query by ID\n\n```typescript\n// Query using existing vector in index\nconst matches = await env.VECTORIZE.queryById(\"some-vector-id\", {\n  topK: 5,\n  returnValues: true,\n  returnMetadata: \"all\"\n});\n```\n\n#### Get Vectors by ID\n\n```typescript\n// Retrieve specific vectors with values and metadata\nconst ids = [\"11\", \"22\", \"33\"];\nconst vectors = await env.VECTORIZE.getByIds(ids);\n```\n\n### Metadata Filtering\n\n```typescript\n// Implicit $eq\nconst matches = await env.VECTORIZE.query(queryVector, {\n  topK: 10,\n  filter: { category: \"electronics\" }\n});\n\n// Explicit operators\nconst matches = await env.VECTORIZE.query(queryVector, {\n  filter: {\n    category: { $ne: \"deprecated\" },\n    price: { $gte: 10, $lt: 100 },\n    tags: { $in: [\"featured\", \"sale\"] },\n    discontinued: { $ne: true }\n  }\n});\n\n// Nested metadata with dot notation\nconst matches = await env.VECTORIZE.query(queryVector, {\n  filter: { \"product.brand\": \"acme\" }\n});\n\n// Range query for prefix search (strings)\nconst matches = await env.VECTORIZE.query(queryVector, {\n  filter: { \n    category: { $gte: \"elec\", $lt: \"eled\" }  // Matches \"electronics\"\n  }\n});\n```\n\n**Operators:**\n- `$eq`: Equals (implicit if no operator)\n- `$ne`: Not equals\n- `$in`: In array\n- `$nin`: Not in array\n- `$lt`, `$lte`: Less than (or equal)\n- `$gt`, `$gte`: Greater than (or equal)\n\n**Filter Constraints:**\n- Max 2048 bytes (compact JSON)\n- Keys: no empty, no dots, no `$` prefix, no double-quotes, max 512 chars\n- Values: string, number, boolean, null\n- Range queries: Can combine upper/lower bounds on same field\n- Namespaces filtered before metadata\n\n### Deletion\n\n```typescript\n// Delete by IDs (asynchronous)\nconst deleted = await env.VECTORIZE.deleteByIds([\"11\", \"22\", \"33\"]);\n// Returns: { mutationId: string }\n```\n\n### Index Inspection\n\n```typescript\n// Get index configuration\nconst details = await env.VECTORIZE.describe();\n// Returns: { dimensions: number, metric: string, vectorCount?: number }\n```\n\n## Namespaces\n\nPartition vectors within a single index by customer, tenant, or category.\n\n```typescript\n// Insert with namespace\nawait env.VECTORIZE.insert([\n  { id: \"1\", values: [...], namespace: \"customer-abc\" },\n  { id: \"2\", values: [...], namespace: \"customer-xyz\" }\n]);\n\n// Query within namespace (applied before vector search)\nconst matches = await env.VECTORIZE.query(queryVector, {\n  namespace: \"customer-abc\"\n});\n```\n\n**Limits:**\n- 50,000 namespaces (Paid) / 1,000 (Free)\n- Max 64 bytes per namespace name\n- Namespace filter applied before metadata filters\n\n## Integration Patterns\n\n### Workers AI Integration\n\n```typescript\nimport { Ai } from '@cloudflare/ai';\n\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const ai = new Ai(env.AI);\n    \n    // Generate embedding\n    const userQuery = \"what is a vector database\";\n    const embeddings = await ai.run(\"@cf/baai/bge-base-en-v1.5\", {\n      text: [userQuery]\n    });\n    \n    // embeddings.data is number[][]\n    // Pass embeddings.data[0], NOT embeddings or embeddings.data\n    const matches = await env.VECTORIZE.query(embeddings.data[0], {\n      topK: 3,\n      returnMetadata: \"all\"\n    });\n    \n    return Response.json({ matches });\n  }\n};\n```\n\n**Common Embedding Models:**\n- `@cf/baai/bge-base-en-v1.5`: 768 dimensions, English\n- `@cf/baai/bge-large-en-v1.5`: 1024 dimensions, English\n- `@cf/baai/bge-small-en-v1.5`: 384 dimensions, English\n\n### OpenAI Integration\n\n```typescript\nimport OpenAI from 'openai';\n\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const openai = new OpenAI({ apiKey: env.OPENAI_KEY });\n    \n    const userQuery = \"semantic search query\";\n    const response = await openai.embeddings.create({\n      model: \"text-embedding-ada-002\",\n      input: userQuery\n    });\n    \n    // Pass response.data[0].embedding, NOT response\n    const matches = await env.VECTORIZE.query(response.data[0].embedding, {\n      topK: 5,\n      returnMetadata: \"all\"\n    });\n    \n    return Response.json({ matches });\n  }\n};\n```\n\n### RAG Pattern\n\n```typescript\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    const { query } = await request.json();\n    \n    // 1. Generate query embedding\n    const embeddings = await env.AI.run(\"@cf/baai/bge-base-en-v1.5\", {\n      text: [query]\n    });\n    \n    // 2. Search Vectorize\n    const matches = await env.VECTORIZE.query(embeddings.data[0], {\n      topK: 5,\n      returnMetadata: \"all\"\n    });\n    \n    // 3. Fetch full documents from R2/D1/KV\n    const documents = await Promise.all(\n      matches.matches.map(async (match) => {\n        const key = match.metadata?.r2_key as string;\n        const obj = await env.R2_BUCKET.get(key);\n        return obj?.text();\n      })\n    );\n    \n    // 4. Build context for LLM\n    const context = documents.filter(Boolean).join(\"\\n\\n\");\n    \n    // 5. Generate response with context\n    const llmResponse = await env.AI.run(\"@cf/meta/llama-3-8b-instruct\", {\n      prompt: `Context: ${context}\\n\\nQuestion: ${query}\\n\\nAnswer:`\n    });\n    \n    return Response.json({ answer: llmResponse, sources: matches.matches });\n  }\n};\n```\n\n## CLI Operations\n\n### Bulk Upload (NDJSON)\n\n```bash\n# File: embeddings.ndjson\n# { \"id\": \"1\", \"values\": [0.1, 0.2, ...], \"metadata\": {\"url\": \"/doc/1\"}}\n# { \"id\": \"2\", \"values\": [0.3, 0.4, ...], \"metadata\": {\"url\": \"/doc/2\"}}\n\nnpx wrangler vectorize insert <index-name> --file=embeddings.ndjson\n```\n\n**Rate Limits:**\n- Max 5000 vectors per file (Cloudflare API rate limit)\n- Use multiple files for larger batches\n\n### Python HTTP API Example\n\n```python\nimport requests\n\nurl = f\"https://api.cloudflare.com/client/v4/accounts/{account_id}/vectorize/v2/indexes/{index_name}/insert\"\nheaders = {\"Authorization\": f\"Bearer {api_token}\"}\n\nwith open('embeddings.ndjson', 'rb') as f:\n    resp = requests.post(url, headers=headers, files=dict(vectors=f))\n    print(resp.json())\n```\n\n## Performance Optimization\n\n### Write Throughput\n\n**Batching Strategy:**\n- Vectorize batches up to 200K vectors OR 1000 operations per job\n- Inserting 1 vector at a time = 1000 vectors per job = slow\n- Inserting 2500 vectors at a time = 200K+ vectors per job = fast\n\n**Example:**\n\n```typescript\n// BAD: 250,000 individual inserts = 250 jobs = ~1 hour\nfor (const vector of vectors) {\n  await env.VECTORIZE.insert([vector]);\n}\n\n// GOOD: 100 batches of 2,500 = 2-3 jobs = minutes\nfor (let i = 0; i < vectors.length; i += 2500) {\n  const batch = vectors.slice(i, i + 2500);\n  await env.VECTORIZE.insert(batch);\n}\n```\n\n### Query Performance\n\n**High-Precision vs. Approximate:**\n- Default: Approximate scoring (faster, good trade-off)\n- `returnValues: true`: High-precision scoring (slower, more accurate)\n\n**topK Limits:**\n- Default limit: 100 without values/metadata\n- With `returnValues: true` or `returnMetadata: \"all\"`: Max 20\n- Balance accuracy vs. latency\n\n**Metadata Filter Performance:**\n- Namespace filters applied first (fastest)\n- High-cardinality range queries degrade performance\n- Bucket high-cardinality values when possible\n\n### Mutation Tracking\n\n```bash\n# Check if mutations are processed\nnpx wrangler vectorize info <index-name>\n\n# Returns processedUpToMutation and processedUpToDatetime\n# Compare with insert/upsert mutationId\n```\n\n## Limits (V2)\n\n| Resource | Limit |\n|----------|-------|\n| Indexes per account | 50,000 (Paid) / 100 (Free) |\n| Max dimensions | 1536 (32-bit float) |\n| Max vector ID length | 64 bytes |\n| Metadata per vector | 10 KiB |\n| Max topK (no values/metadata) | 100 |\n| Max topK (with values/metadata) | 20 |\n| Insert batch size (Workers) | 1000 |\n| Insert batch size (HTTP API) | 5000 |\n| List vectors page size | 1000 |\n| Max index name length | 64 bytes |\n| Max vectors per index | 5,000,000 |\n| Max namespaces | 50,000 (Paid) / 1000 (Free) |\n| Max namespace length | 64 bytes |\n| Max upload size | 100 MB |\n| Max metadata indexes | 10 |\n| Indexed metadata per field | 64 bytes (strings, UTF-8) |\n\n## Common Patterns\n\n### Multi-Tenant Architecture\n\n```typescript\n// Option 1: Separate indexes per tenant (if < 50K tenants)\nconst tenantIndex = env[`VECTORIZE_${tenantId.toUpperCase()}`];\n\n// Option 2: Namespaces (up to 50K namespaces)\nawait env.VECTORIZE.insert([\n  { id: \"doc-1\", values: [...], namespace: `tenant-${tenantId}` }\n]);\n\nconst matches = await env.VECTORIZE.query(queryVector, {\n  namespace: `tenant-${tenantId}`\n});\n\n// Option 3: Metadata filtering (flexible but slower)\nconst matches = await env.VECTORIZE.query(queryVector, {\n  filter: { tenantId: tenantId }\n});\n```\n\n### Semantic Search with Metadata\n\n```typescript\n// Index documents with rich metadata\nawait env.VECTORIZE.upsert([\n  {\n    id: doc.id,\n    values: embedding,\n    metadata: {\n      title: doc.title,\n      category: doc.category,\n      published: Math.floor(doc.date / 1000), // Unix timestamp\n      tags: doc.tags.join(\",\"),\n      url: doc.url\n    }\n  }\n]);\n\n// Search with filters\nconst matches = await env.VECTORIZE.query(queryVector, {\n  topK: 10,\n  returnMetadata: \"all\",\n  filter: {\n    category: \"tutorials\",\n    published: { $gte: thirtyDaysAgo }\n  }\n});\n```\n\n### Hybrid Search (Vector + Metadata)\n\n```typescript\n// 1. Create metadata indexes for common filters\n// wrangler vectorize create-metadata-index my-index --property-name=category --type=string\n// wrangler vectorize create-metadata-index my-index --property-name=published --type=number\n\n// 2. Query with both semantic similarity and filters\nconst results = await env.VECTORIZE.query(queryVector, {\n  topK: 20,\n  returnMetadata: \"all\",\n  filter: {\n    category: { $in: [\"tech\", \"science\"] },\n    published: { $gte: lastMonth },\n    status: \"published\"\n  }\n});\n```\n\n## Error Handling\n\n```typescript\ntry {\n  const matches = await env.VECTORIZE.query(queryVector, { topK: 5 });\n} catch (error) {\n  // Common errors:\n  // - Dimension mismatch\n  // - Invalid filter syntax\n  // - topK exceeds limits\n  // - Index not found/not bound\n  console.error(\"Vectorize query failed:\", error);\n  \n  // Fallback strategy\n  return Response.json({ \n    error: \"Search unavailable\", \n    matches: [] \n  }, { status: 503 });\n}\n```\n\n## Best Practices\n\n1. **Create metadata indexes BEFORE inserting vectors** - existing vectors not retroactively indexed\n2. **Use upsert for updates** - insert ignores duplicates\n3. **Batch operations** - 1000-2500 vectors per batch for optimal throughput\n4. **Monitor mutations** - Use `wrangler vectorize info` to track processing\n5. **Choose appropriate metric** - cosine for text, euclidean for images\n6. **Design for cardinality** - Bucket high-cardinality metadata for better range queries\n7. **Namespace for tenant isolation** - Faster than metadata filters\n8. **Return metadata strategically** - Use \"indexed\" for speed, \"all\" when needed\n9. **Validate dimensions** - Must match index configuration exactly\n10. **Handle async operations** - Inserts/upserts take seconds to be queryable\n\n## Common Mistakes\n\n1. **Passing wrong data shape to query():**\n   - Workers AI: Pass `embeddings.data[0]`, not `embeddings`\n   - OpenAI: Pass `response.data[0].embedding`, not `response`\n\n2. **Creating metadata indexes after inserting vectors** - Won't index existing vectors\n\n3. **Using insert when upsert is needed** - Duplicates ignored with insert\n\n4. **Not batching operations** - 1 vector per request is extremely slow\n\n5. **Returning all values/metadata by default** - Impacts performance and topK limit\n\n6. **High-cardinality range queries** - Use bucketing or discrete values\n\n7. **Exceeding topK limits** - 20 with values/metadata, 100 without\n\n8. **Forgetting to run wrangler types** - Missing TypeScript types after config changes\n\n## Troubleshooting\n\n### Vectors not appearing in queries\n\n- Check mutation processed: `wrangler vectorize info <index>`\n- Wait 5-10 seconds after insert/upsert\n- Verify mutationId matches processedUpToMutation\n\n### Dimension mismatch errors\n\n- Ensure query vector length matches index dimensions exactly\n- Check embedding model output dimensions\n\n### Filter not working\n\n- Verify metadata index created: `wrangler vectorize list-metadata-index <index>`\n- Re-upsert vectors after creating metadata index\n- Check filter syntax and operator constraints\n\n### Performance issues\n\n- Reduce topK if using returnValues or returnMetadata=\"all\"\n- Simplify metadata filters (avoid high-cardinality ranges)\n- Use namespace filtering instead of metadata when possible\n- Batch insert/upsert operations properly\n\n## Resources\n\n- [Official Docs](https://developers.cloudflare.com/vectorize/)\n- [Client API Reference](https://developers.cloudflare.com/vectorize/reference/client-api/)\n- [Metadata Filtering](https://developers.cloudflare.com/vectorize/reference/metadata-filtering/)\n- [Limits](https://developers.cloudflare.com/vectorize/platform/limits/)\n- [Workers AI Models](https://developers.cloudflare.com/workers-ai/models/#text-embeddings)\n- [Wrangler Commands](https://developers.cloudflare.com/workers/wrangler/commands/#vectorize)\n- [Discord: #vectorize](https://discord.cloudflare.com)\n\n---\n\n**Version:** V2 (GA) - Requires Wrangler 3.71.0+\n**Last Updated:** 2025-01-11\n",
        ".agent/services/hosting/cloudflare-platform/references/waf/README.md": "# Cloudflare WAF Expert Skill Reference\n\n**Expertise**: Cloudflare Web Application Firewall (WAF) configuration, custom rules, managed rulesets, rate limiting, attack detection, and API integration...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/web-analytics/README.md": "# Cloudflare Web Analytics Skill Reference\n\nComprehensive guide for implementing Cloudflare Web Analytics - a privacy-first web analytics solution that provides performance metrics and user insights without compromising visitor privacy.\n\n## Overview\n\nCloudflare Web Analytics is a free, privacy-focused analytics service that:\n- Measures Core W...\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/workerd/README.md": "# Workerd Runtime\n\nV8-based JS/Wasm runtime powering Cloudflare Workers. Use as app server, dev tool, or HTTP proxy.\n\n## When to Use\n\n- Local Workers development (via Wrangler)\n- Self-hosted Workers runtime\n- Custom embedded runtime\n- Debugging runtime-specific issues\n\n## Key Features\n\n- **Standards-based**: Fetch API, Web Crypto, Streams, WebSocket\n- **Nanoservices**: Service bindings with local call performance\n- **Capability security**: Explicit bindings prevent SSRF\n- **Backwards compatible**: Version = max compat date supported\n\n## Architecture\n\n```\nConfig (workerd.capnp)\n├── Services (workers/endpoints)\n├── Sockets (HTTP/HTTPS listeners)\n└── Extensions (global capabilities)\n```\n\n## Quick Start\n\n```bash\nworkerd serve config.capnp\nworkerd compile config.capnp myConfig -o binary\nworkerd test config.capnp\n```\n\n## Core Concepts\n\n- **Service**: Named endpoint (worker/network/disk/external)\n- **Binding**: Capability-based resource access (KV/DO/R2/services)\n- **Compatibility date**: Feature gate (always set!)\n- **Modules**: ES modules (recommended) or service worker syntax\n\n## See Also\n\n- [configuration.md](./configuration.md) - Config format, services, bindings\n- [api.md](./api.md) - Runtime APIs, C++ embedding\n- [patterns.md](./patterns.md) - Multi-service, DO, proxies\n- [gotchas.md](./gotchas.md) - Common errors, debugging\n\n## References\n\n- [GitHub](https://github.com/cloudflare/workerd)\n- [Compat Dates](https://developers.cloudflare.com/workers/configuration/compatibility-dates/)\n- [workerd.capnp](https://github.com/cloudflare/workerd/blob/main/src/workerd/server/workerd.capnp)\n",
        ".agent/services/hosting/cloudflare-platform/references/workers-ai/README.md": "# Cloudflare Workers AI Skill\n\nA comprehensive OpenCode skill for working with Cloudflare Workers AI.\n\n## What This Skill Covers\n\nThis skill focuses **exclusively on Cloudflare Workers AI** - the serverless AI inference platform. It does NOT cover the broader Cloudflare platform (Workers, Pages, CDN, etc.) unless directly related to Workers AI usage.\n\n### Topics Included\n\n- **Core Concepts**: Bindings, model invocation patterns, naming conventions\n- **Task-Specific Implementations**: Text generation, embeddings, image generation, speech recognition, translation, etc.\n- **REST API Usage**: Authentication, endpoints, OpenAI compatibility\n- **Wrangler Integration**: Setup, configuration, deployment\n- **Pricing & Limits**: Neurons, rate limits, cost optimization\n- **Common Patterns**: RAG, streaming, batch processing, error handling\n- **AI Gateway**: Caching, rate limiting, analytics integration\n- **Function Calling**: Both traditional and embedded approaches\n- **Model Selection**: Guidelines for choosing the right model\n- **TypeScript Support**: Types, interfaces, best practices\n\n## How to Use This Skill\n\n### Installation\n\n1. Place `README.md` in your OpenCode skills directory\n2. Load the skill when working on Workers AI projects\n\n### When to Load This Skill\n\nLoad this skill when:\n- Implementing AI inference in Cloudflare Workers\n- Building RAG systems with Workers AI + Vectorize\n- Optimizing Workers AI costs or performance\n- Debugging Workers AI integration issues\n- Setting up AI Gateway with Workers AI\n- Implementing function calling with LLMs\n\n### Example Usage\n\n```bash\n# In OpenCode CLI\nload-skill cloudflare-workers-ai\n\n# Then ask questions like:\n\"How do I implement streaming with Workers AI?\"\n\"What's the best embedding model for semantic search?\"\n\"Show me how to do RAG with Vectorize\"\n\"How do I handle rate limits?\"\n```\n\n## Skill Structure\n\nThe skill is organized into these sections:\n\n1. **Overview** - What Workers AI is and when to use this skill\n2. **Core Concepts** - Bindings, invocation patterns, model naming\n3. **Task-Specific Patterns** - Code examples for each AI task type\n4. **REST API** - Using Workers AI via HTTP endpoints\n5. **Wrangler CLI** - Setup, config, deployment\n6. **Pricing & Neurons** - Cost model and optimization\n7. **Rate Limits** - Per-task and per-model limits\n8. **RAG Pattern** - Complete retrieval-augmented generation example\n9. **AI Gateway** - Integration patterns\n10. **Common Patterns** - Error handling, streaming, batching, etc.\n11. **Model Selection** - Choosing the right model\n12. **Debugging** - Monitoring and troubleshooting\n13. **Common Issues** - Known problems and solutions\n14. **Pages Integration** - Using Workers AI in Pages Functions\n15. **Advanced Topics** - LoRA adapters\n16. **Architecture Patterns** - System design approaches\n\n## Key Features\n\n- ✅ **Complete Code Examples**: Every pattern has working code\n- ✅ **TypeScript First**: Proper typing for all examples\n- ✅ **Real-World Patterns**: RAG, streaming, batch processing\n- ✅ **Cost Optimization**: Pricing info and model selection guidance\n- ✅ **Troubleshooting**: Common issues and solutions\n- ✅ **Best Practices**: Error handling, type safety, monitoring\n\n## What's NOT Covered\n\nThis skill does NOT cover:\n- General Cloudflare Workers programming (use Workers skill)\n- Cloudflare Pages (unless specifically Workers AI integration)\n- Cloudflare CDN, DNS, security features\n- Vectorize (unless in context of Workers AI RAG)\n- D1, KV, R2, Durable Objects (unless AI-specific usage)\n\n## Maintenance\n\nTo keep this skill up to date:\n- Check official docs: https://developers.cloudflare.com/workers-ai/\n- Monitor model catalog: https://developers.cloudflare.com/workers-ai/models/\n- Track pricing changes: https://developers.cloudflare.com/workers-ai/platform/pricing/\n\n## Contributing\n\nFound an issue or want to improve this skill?\n1. Test changes against official Cloudflare Workers AI docs\n2. Verify code examples work with latest Wrangler\n3. Update pricing/limits if changed\n4. Add new model examples as they're released\n\n## Version History\n\n- **v1.0** (2026-01-11): Initial comprehensive skill\n  - Full coverage of Workers AI API\n  - Code patterns for all task types\n  - Pricing, limits, troubleshooting\n  - RAG, streaming, function calling examples\n\n## License\n\nThis skill documentation is provided as-is for use with OpenCode.\n",
        ".agent/services/hosting/cloudflare-platform/references/workers-for-platforms/README.md": "# Cloudflare Workers for Platforms\n\nMulti-tenant platform with isolated customer code execution at scale.\n\n## Use Cases\n\n- Multi-tenant SaaS running customer code\n- AI-generated code execution in secure sandboxes\n- Programmable platforms with isolated compute\n- Edge functions/serverless platforms\n- Website builders with static + dynamic content\n- Unlimited app deployment at scale\n\n**NOT for general Workers** - only for Workers for Platforms architecture.\n\n## Architecture\n\n**4 Components:**\n1. **Dispatch Namespace** - Container for unlimited customer Workers, automatic isolation, untrusted mode\n2. **Dynamic Dispatch Worker** - Entry point, routes requests, enforces platform logic (auth, limits, validation)\n3. **User Workers** - Customer code in isolated sandboxes, API-deployed, optional bindings (KV/D1/R2/DO)\n4. **Outbound Worker** (optional) - Intercepts external fetch, controls egress, logs subrequests\n\n**Request Flow:**\n\n```\nRequest → Dispatch Worker → Determines user Worker → env.DISPATCHER.get(\"customer\") \n→ User Worker executes (Outbound Worker for external fetch) → Response → Dispatch Worker → Client\n```\n\n## Key Features\n\n- Unlimited Workers per namespace (no script limits)\n- Automatic tenant isolation\n- Custom CPU/subrequest limits per customer\n- Hostname routing (subdomains/vanity domains)\n- Egress/ingress control\n- Static assets support\n- Tags for bulk operations\n\n## Quick Start\n\nSee [configuration.md](./configuration.md), [api.md](./api.md), [patterns.md](./patterns.md), [gotchas.md](./gotchas.md)\n\n## Refs\n\n- [Docs](https://developers.cloudflare.com/cloudflare-for-platforms/workers-for-platforms/)\n- [Starter Kit](https://github.com/cloudflare/templates/tree/main/worker-publisher-template)\n- [VibeSDK](https://github.com/cloudflare/vibesdk)\n",
        ".agent/services/hosting/cloudflare-platform/references/workers-playground/README.md": "# Cloudflare Workers Playground Skill Reference\n\n## Overview\n\nCloudflare Workers Playground is a browser-based sandbox for instantly experimenting with, testing, and deploying Cloudflare Workers without authentication or setup. This skill provides patterns, APIs, and best practices specifically for Workers Playground development....\n\n## In This Reference\n\n- **[configuration.md](./configuration.md)** - Setup, deployment, configuration\n- **[api.md](./api.md)** - API endpoints, methods, interfaces\n- **[patterns.md](./patterns.md)** - Common patterns, use cases, examples\n- **[gotchas.md](./gotchas.md)** - Troubleshooting, best practices, limitations\n\n## See Also\n\n- [Cloudflare Docs](https://developers.cloudflare.com/)\n",
        ".agent/services/hosting/cloudflare-platform/references/workers-vpc/README.md": "# Cloudflare Workers VPC Skill\n\nExpert guidance for connecting Cloudflare Workers to private networks (AWS/Azure/GCP/on-prem) using TCP Sockets, Cloudflare Tunnel, and related technologies.\n\n## What is Workers VPC Connectivity?\n\nWorkers VPC connectivity enables Workers to communicate with resources in private networks through:\n\n1. **TCP Sockets API** (`connect()`) - Direct outbound TCP connections from Workers\n2. **Cloudflare Tunnel** - Secure connections to private networks without exposing public IPs\n3. **Hyperdrive** - Optimized connections to external databases with pooling\n4. **Smart Placement** - Automatic Worker placement near backend services\n\n## Core APIs\n\n### TCP Sockets (`connect()`)\n\nCreate outbound TCP connections to private resources:\n\n```typescript\nimport { connect } from 'cloudflare:sockets';\n\nexport default {\n  async fetch(req: Request): Promise<Response> {\n    const socket = connect({\n      hostname: \"internal-db.private.com\",\n      port: 5432\n    }, {\n      secureTransport: \"starttls\" // or \"on\" for immediate TLS\n    });\n\n    // Get readable/writable streams\n    const writer = socket.writable.getWriter();\n    const reader = socket.readable.getReader();\n\n    // Write data\n    const encoder = new TextEncoder();\n    await writer.write(encoder.encode(\"QUERY\\r\\n\"));\n    await writer.close();\n\n    // Read response\n    const { value } = await reader.read();\n    \n    await socket.close();\n    return new Response(value);\n  }\n};\n```\n\n### SocketOptions\n\n```typescript\ninterface SocketOptions {\n  secureTransport?: \"off\" | \"on\" | \"starttls\"; // Default: \"off\"\n  allowHalfOpen?: boolean; // Default: false\n}\n\ninterface SocketAddress {\n  hostname: string; // e.g., \"db.private.net\"\n  port: number;     // e.g., 5432\n}\n```\n\n### Socket Interface\n\n```typescript\ninterface Socket {\n  readable: ReadableStream<Uint8Array>;\n  writable: WritableStream<Uint8Array>;\n  opened: Promise<SocketInfo>;\n  closed: Promise<void>;\n  close(): Promise<void>;\n  startTls(): Socket; // Upgrade to TLS\n}\n```\n\n## Common Use Cases\n\n### 1. Connect to Internal Database\n\n```typescript\nimport { connect } from 'cloudflare:sockets';\n\nexport default {\n  async fetch(req: Request) {\n    const socket = connect(\n      { hostname: \"10.0.1.50\", port: 5432 },\n      { secureTransport: \"on\" }\n    );\n\n    try {\n      await socket.opened; // Wait for connection\n      \n      const writer = socket.writable.getWriter();\n      await writer.write(new TextEncoder().encode(\"SELECT 1\\n\"));\n      await writer.close();\n\n      return new Response(socket.readable);\n    } catch (error) {\n      return new Response(`Connection failed: ${error}`, { status: 500 });\n    } finally {\n      await socket.close();\n    }\n  }\n};\n```\n\n### 2. StartTLS Pattern (Opportunistic TLS)\n\nMany databases require starting insecure then upgrading:\n\n```typescript\nimport { connect } from 'cloudflare:sockets';\n\nconst socket = connect(\n  { hostname: \"postgres.internal\", port: 5432 },\n  { secureTransport: \"starttls\" }\n);\n\n// Initially insecure connection\nconst writer = socket.writable.getWriter();\nawait writer.write(new TextEncoder().encode(\"STARTTLS\\n\"));\n\n// Upgrade to TLS\nconst secureSocket = socket.startTls();\n\n// Now use secureSocket for encrypted communication\nconst secureWriter = secureSocket.writable.getWriter();\nawait secureWriter.write(new TextEncoder().encode(\"AUTH\\n\"));\n```\n\n### 3. SSH/MQTT/SMTP Protocols\n\n```typescript\n// SSH connection example\nimport { connect } from 'cloudflare:sockets';\n\nexport default {\n  async fetch(req: Request) {\n    const socket = connect(\n      { hostname: \"bastion.internal\", port: 22 },\n      { secureTransport: \"on\" }\n    );\n\n    const writer = socket.writable.getWriter();\n    const reader = socket.readable.getReader();\n\n    // SSH handshake\n    await writer.write(new TextEncoder().encode(\"SSH-2.0-CloudflareWorker\\r\\n\"));\n\n    // Read server response\n    const { value } = await reader.read();\n    const response = new TextDecoder().decode(value);\n\n    await socket.close();\n    return new Response(response);\n  }\n};\n```\n\n### 4. Connection with Error Handling\n\n```typescript\nimport { connect } from 'cloudflare:sockets';\n\nasync function connectToPrivateService(\n  host: string,\n  port: number,\n  data: string\n): Promise<string> {\n  let socket: ReturnType<typeof connect> | null = null;\n\n  try {\n    socket = connect({ hostname: host, port }, { secureTransport: \"on\" });\n    \n    await socket.opened; // Throws if connection fails\n\n    const writer = socket.writable.getWriter();\n    await writer.write(new TextEncoder().encode(data));\n    await writer.close();\n\n    const reader = socket.readable.getReader();\n    const chunks: Uint8Array[] = [];\n    \n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) break;\n      chunks.push(value);\n    }\n\n    const combined = new Uint8Array(\n      chunks.reduce((acc, chunk) => acc + chunk.length, 0)\n    );\n    let offset = 0;\n    chunks.forEach(chunk => {\n      combined.set(chunk, offset);\n      offset += chunk.length;\n    });\n\n    return new TextDecoder().decode(combined);\n  } catch (error) {\n    throw new Error(`Socket error: ${error}`);\n  } finally {\n    if (socket) await socket.close();\n  }\n}\n```\n\n## Integration with Cloudflare Tunnel\n\nConnect Workers to private networks via Cloudflare Tunnel:\n\n### Architecture Pattern\n\n```\nWorker → TCP Socket → Cloudflare Tunnel → Private Network\n```\n\n### Setup\n\n1. **Install cloudflared in your private network:**\n\n```bash\n# On private network server\ncloudflared tunnel create my-private-network\ncloudflared tunnel route ip add 10.0.0.0/24 my-private-network\n```\n\n2. **Configure tunnel:**\n\n```yaml\n# config.yml\ntunnel: <TUNNEL_ID>\ncredentials-file: /path/to/credentials.json\n\ningress:\n  - hostname: db.internal.example.com\n    service: tcp://10.0.1.50:5432\n  - hostname: api.internal.example.com\n    service: http://10.0.1.100:8080\n  - service: http_status:404\n```\n\n3. **Connect from Worker:**\n\n```typescript\nimport { connect } from 'cloudflare:sockets';\n\nexport default {\n  async fetch(req: Request) {\n    // Connect through Tunnel to private resource\n    const socket = connect({\n      hostname: \"db.internal.example.com\",\n      port: 5432\n    }, {\n      secureTransport: \"on\"\n    });\n\n    // Use socket...\n  }\n};\n```\n\n## Wrangler Configuration\n\n### Enable TCP Sockets\n\n```toml\n# wrangler.toml\nname = \"private-network-worker\"\nmain = \"src/index.ts\"\ncompatibility_date = \"2024-01-01\"\n\n# No special configuration needed - TCP sockets are available by default\n# in Workers runtime\n\n[env.production]\nroutes = [\n  { pattern = \"api.example.com/*\", zone_name = \"example.com\" }\n]\n```\n\n### Environment Variables for Endpoints\n\n```toml\n[vars]\nDB_HOST = \"10.0.1.50\"\nDB_PORT = \"5432\"\nAPI_HOST = \"internal-api.private.net\"\nAPI_PORT = \"8080\"\n```\n\n```typescript\ninterface Env {\n  DB_HOST: string;\n  DB_PORT: string;\n}\n\nexport default {\n  async fetch(req: Request, env: Env) {\n    const socket = connect({\n      hostname: env.DB_HOST,\n      port: parseInt(env.DB_PORT)\n    });\n    // ...\n  }\n};\n```\n\n## Smart Placement\n\nAuto-locate Workers near backend services:\n\n```toml\n# wrangler.toml\n[placement]\nmode = \"smart\"\n```\n\n```typescript\nexport default {\n  async fetch(req: Request, env: Env, ctx: ExecutionContext) {\n    // Worker automatically runs closest to your backend\n    const socket = connect({ hostname: \"backend.internal\", port: 8080 });\n    // Minimized latency to private network\n  }\n};\n```\n\n## Hyperdrive for Databases\n\nFor PostgreSQL/MySQL, use Hyperdrive instead of raw TCP sockets:\n\n```toml\n# wrangler.toml\n[[hyperdrive]]\nbinding = \"DB\"\nid = \"<HYPERDRIVE_ID>\"\n```\n\n```typescript\nimport { Client } from 'pg';\n\ninterface Env {\n  DB: Hyperdrive;\n}\n\nexport default {\n  async fetch(req: Request, env: Env) {\n    const client = new Client({\n      connectionString: env.DB.connectionString\n    });\n    \n    await client.connect();\n    const result = await client.query('SELECT * FROM users');\n    await client.end();\n\n    return Response.json(result.rows);\n  }\n};\n```\n\n## Limits and Considerations\n\n### TCP Socket Limits\n\n- **Max simultaneous connections:** 6 per Worker execution\n- **Blocked destinations:**\n  - Cloudflare IPs\n  - `localhost` / `127.0.0.1`\n  - Port 25 (SMTP - use Email Workers instead)\n  - Cannot connect back to the calling Worker (loop detection)\n- **Scope:** Sockets must be created in handlers (fetch/scheduled/queue), not global scope\n\n### Performance\n\n```typescript\n// ❌ BAD: Creating socket in global scope\n// import { connect } from 'cloudflare:sockets';\n// const globalSocket = connect({ hostname: \"db\", port: 5432 }); // ERROR\n\n// ✅ GOOD: Create in handler\nexport default {\n  async fetch(req: Request) {\n    const socket = connect({ hostname: \"db\", port: 5432 });\n    // Use socket\n    await socket.close();\n  }\n};\n```\n\n### Security\n\n```typescript\n// Validate destinations\nfunction isAllowedHost(hostname: string): boolean {\n  const allowed = [\n    'internal-db.company.com',\n    'api.private.net',\n    /^10\\.0\\.1\\.\\d+$/ // Private subnet regex\n  ];\n  \n  return allowed.some(pattern => \n    pattern instanceof RegExp \n      ? pattern.test(hostname)\n      : pattern === hostname\n  );\n}\n\nexport default {\n  async fetch(req: Request) {\n    const url = new URL(req.url);\n    const target = url.searchParams.get('target');\n    \n    if (!target || !isAllowedHost(target)) {\n      return new Response('Forbidden', { status: 403 });\n    }\n    \n    const socket = connect({ hostname: target, port: 443 });\n    // ...\n  }\n};\n```\n\n## Common Errors\n\n### `proxy request failed, cannot connect to the specified address`\n\n**Cause:** Attempting to connect to disallowed address (Cloudflare IPs, localhost, blocked IPs)\n\n**Solution:** Use public internet addresses or properly configured Tunnel endpoints\n\n### `TCP Loop detected`\n\n**Cause:** Worker connecting back to itself\n\n**Solution:** Ensure destination is external service, not the Worker's own URL\n\n### `Connections to port 25 are prohibited`\n\n**Cause:** Attempting SMTP on port 25\n\n**Solution:** Use [Email Workers](https://developers.cloudflare.com/email-routing/email-workers/)\n\n### `socket is not open`\n\n**Cause:** Trying to read/write after socket closed\n\n**Solution:** Check socket state, use try/finally with close()\n\n## Testing\n\n### Local Development with Wrangler\n\n```bash\nwrangler dev\n```\n\n### Test TCP Connection\n\n```typescript\n// test.ts\nimport { connect } from 'cloudflare:sockets';\n\nexport default {\n  async fetch(req: Request) {\n    const socket = connect({ hostname: \"google.com\", port: 80 });\n    \n    const writer = socket.writable.getWriter();\n    await writer.write(\n      new TextEncoder().encode(\"GET / HTTP/1.0\\r\\n\\r\\n\")\n    );\n    await writer.close();\n\n    return new Response(socket.readable, {\n      headers: { \"Content-Type\": \"text/plain\" }\n    });\n  }\n};\n```\n\n## Best Practices\n\n1. **Always close sockets:**\n\n   ```typescript\n   const socket = connect(...);\n   try {\n     // Use socket\n   } finally {\n     await socket.close();\n   }\n   ```\n\n2. **Use Hyperdrive for databases** - Better performance, connection pooling\n\n3. **Validate destinations** - Prevent connections to unintended hosts\n\n4. **Handle errors gracefully:**\n\n   ```typescript\n   try {\n     const socket = connect(...);\n     await socket.opened;\n     // Use socket\n   } catch (error) {\n     console.error('Socket error:', error);\n     return new Response('Service unavailable', { status: 503 });\n   }\n   ```\n\n5. **Use Smart Placement** for latency-sensitive applications\n\n6. **Prefer fetch() for HTTP** - Use TCP sockets only when necessary\n\n## Real-World Patterns\n\n### Multi-Protocol Gateway\n\n```typescript\nimport { connect } from 'cloudflare:sockets';\n\ninterface Protocol {\n  connect(host: string, port: number): Promise<string>;\n}\n\nclass SSHProtocol implements Protocol {\n  async connect(host: string, port: number): Promise<string> {\n    const socket = connect({ hostname: host, port }, { secureTransport: \"on\" });\n    // SSH implementation\n    await socket.close();\n    return \"SSH connection established\";\n  }\n}\n\nclass PostgresProtocol implements Protocol {\n  async connect(host: string, port: number): Promise<string> {\n    const socket = connect(\n      { hostname: host, port },\n      { secureTransport: \"starttls\" }\n    );\n    \n    // Postgres wire protocol\n    const secureSocket = socket.startTls();\n    await secureSocket.close();\n    return \"Postgres connection established\";\n  }\n}\n\nexport default {\n  async fetch(req: Request) {\n    const url = new URL(req.url);\n    const protocol = url.pathname.slice(1); // /ssh or /postgres\n    \n    const protocols: Record<string, Protocol> = {\n      ssh: new SSHProtocol(),\n      postgres: new PostgresProtocol()\n    };\n    \n    const handler = protocols[protocol];\n    if (!handler) {\n      return new Response('Unknown protocol', { status: 400 });\n    }\n    \n    const result = await handler.connect('internal.net', 22);\n    return new Response(result);\n  }\n};\n```\n\n## Reference\n\n- [TCP Sockets Documentation](https://developers.cloudflare.com/workers/runtime-apis/tcp-sockets/)\n- [Cloudflare Tunnel](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/)\n- [Hyperdrive](https://developers.cloudflare.com/hyperdrive/)\n- [Smart Placement](https://developers.cloudflare.com/workers/configuration/smart-placement/)\n- [Email Workers](https://developers.cloudflare.com/email-routing/email-workers/)\n\n---\n\nThis skill focuses exclusively on connecting Workers to private networks and VPCs. For general Workers development, see the `cloudflare-workers` skill.\n",
        ".agent/services/hosting/cloudflare-platform/references/workers/README.md": "# Cloudflare Workers\n\nExpert guidance for building, deploying, and optimizing Cloudflare Workers applications.\n\n## Overview\n\nCloudflare Workers run on V8 isolates (NOT containers/VMs):\n- Extremely fast cold starts (< 1ms)\n- Global deployment across 300+ locations\n- Web standards compliant (fetch, URL, Headers, Request, Response)\n- Support JS/TS, Python, Rust, and WebAssembly\n\n**Key principle**: Workers use web platform APIs wherever possible for portability.\n\n## Module Worker Pattern (Recommended)\n\n```typescript\nexport default {\n  async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {\n    return new Response('Hello World!');\n  },\n};\n```\n\n**Handler parameters**:\n- `request`: Incoming HTTP request (standard Request object)\n- `env`: Environment bindings (KV, D1, R2, secrets, vars)\n- `ctx`: Execution context (`waitUntil`, `passThroughOnException`)\n\n## Essential Commands\n\n```bash\nnpx wrangler dev                    # Local dev\nnpx wrangler dev --remote           # Remote dev (actual resources)\nnpx wrangler deploy                 # Production\nnpx wrangler deploy --env staging   # Specific environment\nnpx wrangler tail                   # Stream logs\nnpx wrangler secret put API_KEY     # Set secret\n```\n\n## When to Use Workers\n\n- API endpoints at the edge\n- Request/response transformation\n- Authentication/authorization layers\n- Static asset optimization\n- A/B testing and feature flags\n- Rate limiting and security\n- Proxy/routing logic\n- WebSocket applications\n\n## Quick Start\n\n```bash\nnpm create cloudflare@latest my-worker -- --type hello-world\ncd my-worker\nnpx wrangler dev\n```\n\n## Handler Signatures\n\n```typescript\n// HTTP requests\nasync fetch(request: Request, env: Env, ctx: ExecutionContext): Promise<Response>\n\n// Cron triggers\nasync scheduled(event: ScheduledEvent, env: Env, ctx: ExecutionContext): Promise<void>\n\n// Queue consumer\nasync queue(batch: MessageBatch, env: Env, ctx: ExecutionContext): Promise<void>\n\n// Tail consumer\nasync tail(events: TraceItem[], env: Env, ctx: ExecutionContext): Promise<void>\n```\n\n## Resources\n\n**Docs**: https://developers.cloudflare.com/workers/  \n**Examples**: https://developers.cloudflare.com/workers/examples/  \n**Runtime APIs**: https://developers.cloudflare.com/workers/runtime-apis/\n\n## In This Reference\n\n- [Configuration](./configuration.md) - wrangler.jsonc setup, bindings, environments\n- [API](./api.md) - Runtime APIs, bindings, execution context\n- [Patterns](./patterns.md) - Common workflows, testing, optimization\n- [Gotchas](./gotchas.md) - Common issues, limits, troubleshooting\n\n## See Also\n\n- [KV](../kv/README.md) - Key-value storage\n- [D1](../d1/README.md) - SQL database\n- [R2](../r2/README.md) - Object storage\n- [Durable Objects](../durable-objects/README.md) - Stateful coordination\n- [Queues](../queues/README.md) - Message queues\n- [Wrangler](../wrangler/README.md) - CLI tool reference\n",
        ".agent/services/hosting/cloudflare-platform/references/workflows/README.md": "# Cloudflare Workflows\n\nDurable multi-step applications with automatic retries, state persistence, and long-running execution.\n\n## What It Does\n\n- Chain steps with automatic retry logic\n- Persist state between steps (minutes → weeks)\n- Handle failures without losing progress\n- Wait for external events/approvals\n- Sleep without consuming resources\n\n**Available:** Free & Paid Workers plans\n\n## Core Concepts\n\n**Workflow**: Class extending `WorkflowEntrypoint` with `run` method\n**Instance**: Single execution with unique ID & independent state\n**Steps**: Independently retriable units via `step.do()` - API calls, DB queries, AI invocations\n**State**: Persisted from step returns; step name = cache key\n\n## Quick Start\n\n```typescript\nimport { WorkflowEntrypoint, WorkflowStep, WorkflowEvent } from 'cloudflare:workers';\n\ntype Env = { MY_WORKFLOW: Workflow; DB: D1Database };\ntype Params = { userId: string };\n\nexport class MyWorkflow extends WorkflowEntrypoint<Env, Params> {\n  async run(event: WorkflowEvent<Params>, step: WorkflowStep) {\n    const user = await step.do('fetch user', async () => {\n      return await this.env.DB.prepare('SELECT * FROM users WHERE id = ?')\n        .bind(event.payload.userId).first();\n    });\n    \n    await step.sleep('wait 7 days', '7 days');\n    \n    await step.do('send reminder', async () => {\n      await sendEmail(user.email, 'Reminder!');\n    });\n  }\n}\n```\n\n## Key Features\n\n- **Durability**: Failed steps don't re-run successful ones\n- **Retries**: Configurable backoff (constant/linear/exponential)\n- **Events**: `waitForEvent()` for webhooks/approvals (timeout: 1h → 365d)\n- **Sleep**: `sleep()` / `sleepUntil()` for scheduling (max 365d)\n- **Parallel**: `Promise.all()` for concurrent steps\n- **Idempotency**: Check-then-execute patterns\n\n## Further Reading\n\n- [configuration.md](./configuration.md) - wrangler.toml, step config\n- [api.md](./api.md) - Step APIs, instance management\n- [patterns.md](./patterns.md) - Common workflows, orchestration\n- [gotchas.md](./gotchas.md) - Timeouts, limits, debugging\n\n[Official Docs](https://developers.cloudflare.com/workflows/)\n",
        ".agent/services/hosting/cloudflare-platform/references/wrangler/README.md": "# Cloudflare Wrangler\n\nOfficial CLI for Cloudflare Workers - develop, manage, and deploy Workers from the command line.\n\n## What is Wrangler?\n\nWrangler is the Cloudflare Developer Platform CLI that allows you to:\n- Create, develop, and deploy Workers\n- Manage bindings (KV, D1, R2, Durable Objects, etc.)\n- Configure routing and environments\n- Run local development servers\n- Execute migrations and manage resources\n- Perform integration testing\n\n## Installation\n\n```bash\nnpm install wrangler --save-dev\n# or globally\nnpm install -g wrangler\n```\n\nRun commands: `npx wrangler <command>` (or `pnpm`/`yarn wrangler`)\n\n## Essential Commands\n\n### Project & Development\n\n```bash\nwrangler init [name]              # Create new project\nwrangler dev                      # Local dev server\nwrangler dev --remote             # Dev with remote resources\nwrangler deploy                   # Deploy to production\nwrangler deploy --env staging     # Deploy to environment\nwrangler versions list            # List versions\nwrangler rollback [id]            # Rollback deployment\nwrangler login                    # OAuth login\nwrangler whoami                   # Check auth status\n```\n\n## Resource Management\n\n### KV\n\n```bash\nwrangler kv namespace create NAME\nwrangler kv key put \"key\" \"value\" --namespace-id=<id>\nwrangler kv key get \"key\" --namespace-id=<id>\n```\n\n### D1\n\n```bash\nwrangler d1 create NAME\nwrangler d1 execute NAME --command \"SQL\"\nwrangler d1 migrations create NAME \"description\"\nwrangler d1 migrations apply NAME\n```\n\n### R2\n\n```bash\nwrangler r2 bucket create NAME\nwrangler r2 object put BUCKET/key --file path\nwrangler r2 object get BUCKET/key\n```\n\n### Other Resources\n\n```bash\nwrangler queues create NAME\nwrangler vectorize create NAME --dimensions N --metric cosine\nwrangler hyperdrive create NAME --connection-string \"...\"\n```\n\n### Secrets\n\n```bash\nwrangler secret put NAME          # Set secret\nwrangler secret list              # List secrets\nwrangler secret delete NAME       # Delete secret\n```\n\n### Monitoring\n\n```bash\nwrangler tail                     # Real-time logs\nwrangler tail --env production    # Tail specific env\nwrangler tail --status error      # Filter by status\n```\n\n## In This Reference\n\n- [configuration.md](./configuration.md) - wrangler.jsonc setup, environments, bindings\n- [api.md](./api.md) - Programmatic API (`unstable_startWorker`, `getPlatformProxy`)\n- [patterns.md](./patterns.md) - Common workflows and development patterns\n- [gotchas.md](./gotchas.md) - Common pitfalls, limits, and troubleshooting\n",
        ".agent/services/hosting/cloudflare-platform/references/zaraz/README.md": "# Cloudflare Zaraz Skill\n\nExpert guidance for Cloudflare Zaraz - server-side tag manager for loading third-party tools at the edge.\n\n## What is Zaraz?\n\nZaraz offloads third-party scripts (analytics, ads, chat, marketing) to Cloudflare's edge, improving site speed, privacy, and security. Zero client-side performance impact.\n\n## Core Concepts\n\n- **Server-side execution** - Scripts run on Cloudflare, not user's browser\n- **Single HTTP request** - All tools loaded via one endpoint\n- **Privacy-first** - Control data sent to third parties\n- **No client-side JS** - Minimal browser overhead\n\n## Zaraz Dashboard Setup\n\n1. Navigate to domain > Zaraz\n2. Click \"Start setup\"\n3. Add tools (Google Analytics, Facebook Pixel, etc.)\n4. Configure triggers and actions\n\n## Web API\n\nZaraz provides `zaraz` object in browser:\n\n### Track Events\n\n```javascript\n// Basic event\nzaraz.track('button_click');\n\n// Event with properties\nzaraz.track('purchase', {\n  value: 99.99,\n  currency: 'USD',\n  item_id: '12345'\n});\n\n// E-commerce events\nzaraz.track('add_to_cart', {\n  product_name: 'Widget',\n  price: 29.99,\n  quantity: 1\n});\n```\n\n### Set User Properties\n\n```javascript\nzaraz.set('userId', 'user_12345');\nzaraz.set('plan', 'premium');\nzaraz.set({\n  email: '[email protected]',\n  country: 'US',\n  age: 30\n});\n```\n\n### E-commerce Tracking\n\n```javascript\n// Product view\nzaraz.ecommerce('Product Viewed', {\n  product_id: 'SKU123',\n  name: 'Blue Widget',\n  price: 49.99,\n  currency: 'USD'\n});\n\n// Add to cart\nzaraz.ecommerce('Product Added', {\n  product_id: 'SKU123',\n  quantity: 2,\n  price: 49.99\n});\n\n// Purchase\nzaraz.ecommerce('Order Completed', {\n  order_id: 'ORD-789',\n  total: 149.98,\n  revenue: 149.98,\n  shipping: 10.00,\n  tax: 12.50,\n  currency: 'USD',\n  products: [\n    { product_id: 'SKU123', quantity: 2, price: 49.99 }\n  ]\n});\n```\n\n## Consent Management\n\n```javascript\n// Check consent status\nif (zaraz.consent.getAll().analytics) {\n  zaraz.track('page_view');\n}\n\n// Request consent\nzaraz.consent.modal = true; // Show consent modal\n\n// Set consent programmatically\nzaraz.consent.setAll({\n  analytics: true,\n  marketing: false,\n  preferences: true\n});\n\n// Listen for consent changes\nzaraz.consent.addEventListener('consentChanged', () => {\n  console.log('Consent updated:', zaraz.consent.getAll());\n});\n```\n\n## Workers Integration\n\nAccess Zaraz data in Workers:\n\n```typescript\nexport default {\n  async fetch(req: Request): Promise<Response> {\n    const url = new URL(req.url);\n    \n    // Inject Zaraz tracking\n    if (url.pathname === '/checkout') {\n      const response = await fetch(req);\n      const html = await response.text();\n      \n      const tracking = `\n        <script>\n          zaraz.track('checkout_started', {\n            cart_value: 99.99\n          });\n        </script>\n      `;\n      \n      const modified = html.replace('</body>', tracking + '</body>');\n      return new Response(modified, response);\n    }\n    \n    return fetch(req);\n  }\n};\n```\n\n## Triggers\n\nConfigure when tools fire:\n\n### Page Rules\n\n- **Pageview** - On every page load\n- **DOM Ready** - When DOM is ready\n- **Click** - Element clicks (CSS selector)\n- **Form submission** - Form submits\n- **Scroll depth** - User scrolls percentage\n- **Timer** - After time elapsed\n- **Variable match** - Custom conditions\n\n### Example Trigger\n\n```\nTrigger: Button Click\nMatch rule: CSS Selector = .buy-button\nAction: Track event \"purchase_intent\"\n```\n\n## Common Tools\n\n### Google Analytics 4\n\n```javascript\n// Track page view (automatic)\nzaraz.track('pageview');\n\n// Custom event\nzaraz.track('sign_up', {\n  method: 'email'\n});\n```\n\n### Facebook Pixel\n\n```javascript\nzaraz.track('PageView');\nzaraz.track('Purchase', {\n  value: 99.99,\n  currency: 'USD'\n});\n```\n\n### Google Ads Conversion\n\n```javascript\nzaraz.track('conversion', {\n  send_to: 'AW-XXXXXXXXX/YYYYYY',\n  value: 1.00,\n  currency: 'USD'\n});\n```\n\n## Custom Managed Components\n\nBuild custom tools:\n\n```javascript\n// Example: Custom analytics tool\nexport default class CustomAnalytics {\n  async handleEvent(event) {\n    const { type, payload } = event;\n    \n    await fetch('https://analytics.example.com/track', {\n      method: 'POST',\n      body: JSON.stringify({\n        event: type,\n        properties: payload,\n        timestamp: Date.now()\n      })\n    });\n  }\n}\n```\n\n## Data Layer\n\nUse `zaraz.dataLayer` for structured data:\n\n```javascript\n// Set data layer\nwindow.zaraz.dataLayer = {\n  user_id: '12345',\n  page_type: 'product',\n  category: 'electronics'\n};\n\n// Access in triggers\n// Variable: {{client.__zarazTrack.page_type}}\n```\n\n## Server-Side Configuration\n\n### zaraz.toml\n\n```toml\n[settings]\nauto_inject = true\ndebug_mode = false\n\n[[tools]]\ntype = \"google-analytics\"\nid = \"G-XXXXXXXXXX\"\n\n[[tools.triggers]]\nmatch_rule = \"Pageview\"\n```\n\n## Privacy Features\n\n1. **IP anonymization** - Automatic\n2. **Cookie control** - Consent-based\n3. **Data minimization** - Send only necessary data\n4. **Regional compliance** - GDPR, CCPA support\n\n## Performance\n\n```javascript\n// Zaraz automatically:\n// - Batches requests\n// - Defers non-critical scripts\n// - Proxies third-party requests\n// - Caches tool configurations\n\n// Result: ~0ms client-side overhead\n```\n\n## Debugging\n\n```javascript\n// Enable debug mode in dashboard, then:\nzaraz.debug = true;\n\n// View events in console\nzaraz.track('test_event', { debug: true });\n\n// Check loaded tools\nconsole.log(zaraz.tools);\n```\n\n## Best Practices\n\n1. **Use triggers** instead of inline `zaraz.track()` when possible\n2. **Batch events** for related actions\n3. **Test with debug mode** before production\n4. **Implement consent** for GDPR compliance\n5. **Monitor performance** in dashboard analytics\n6. **Use data layer** for structured data\n\n## Common Patterns\n\n### SPA Tracking\n\n```javascript\n// On route change\nrouter.afterEach((to, from) => {\n  zaraz.track('pageview', {\n    page_path: to.path,\n    page_title: to.meta.title\n  });\n});\n```\n\n### User Identification\n\n```javascript\n// On login\nzaraz.set('user_id', user.id);\nzaraz.set('user_email', user.email);\nzaraz.track('login', { method: 'password' });\n```\n\n### A/B Testing\n\n```javascript\nconst variant = Math.random() < 0.5 ? 'A' : 'B';\nzaraz.set('ab_test_variant', variant);\nzaraz.track('ab_test_view', { variant });\n```\n\n## Limits\n\n- **Tools**: Unlimited\n- **Events**: Unlimited\n- **Data retention**: Per tool's policy\n- **Request size**: 100KB per request\n\n## Troubleshooting\n\n### Events not firing\n\n- Check trigger conditions in dashboard\n- Verify tool is enabled\n- Enable debug mode\n- Check browser console for errors\n\n### Consent issues\n\n- Verify consent modal configuration\n- Check `zaraz.consent.getAll()` status\n- Ensure tools respect consent settings\n\n## Reference\n\n- [Zaraz Docs](https://developers.cloudflare.com/zaraz/)\n- [Web API](https://developers.cloudflare.com/zaraz/web-api/)\n- [Managed Components](https://developers.cloudflare.com/zaraz/advanced/load-custom-managed-component/)\n\n---\n\nThis skill focuses exclusively on Zaraz. For Workers development, see `cloudflare-workers` skill.\n"
      },
      "plugins": [
        {
          "name": "aidevops",
          "source": "./.agent",
          "description": "Complete AI DevOps framework - 14 domain agents, 80+ subagents, 100+ helper scripts for WordPress, SEO, hosting, code quality, browser automation, and more",
          "strict": false,
          "categories": [],
          "install_commands": [
            "/plugin marketplace add marcusquinn/aidevops",
            "/plugin install aidevops@aidevops"
          ]
        }
      ]
    }
  ]
}