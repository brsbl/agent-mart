{
  "author": {
    "id": "tobi",
    "display_name": "Tobias Lütke",
    "avatar_url": "https://avatars.githubusercontent.com/u/347?u=7960534ac523f60d241bb7c7ef986d263ea96170&v=4"
  },
  "marketplaces": [
    {
      "name": "qmd",
      "version": null,
      "description": "Search and retrieve documents from local markdown files.",
      "repo_full_name": "tobi/qmd",
      "repo_url": "https://github.com/tobi/qmd",
      "repo_description": "mini cli search engine for your docs, knowledge bases, meeting notes, whatever. Tracking current sota approaches while being all local",
      "signals": {
        "stars": 9824,
        "forks": 537,
        "pushed_at": "2026-02-19T12:02:07Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n\t\"name\": \"qmd\",\n\t\"owner\": {\n\t\t\"name\": \"tobi\",\n\t\t\"email\": \"tobi@lutke.com\"\n\t},\n\t\"plugins\": [\n\t\t{\n\t\t\t\"name\": \"qmd\",\n\t\t\t\"source\": \"./\",\n\t\t\t\"description\": \"Search and retrieve documents from local markdown files.\",\n\t\t\t\"version\": \"0.1.0\",\n\t\t\t\"author\": {\n\t\t\t\t\"name\": \"tobi\",\n\t\t\t\t\"email\": \"tobi@lutke.com\"\n\t\t\t},\n\t\t\t\"repository\": \"https://github.com/tobi/qmd\",\n\t\t\t\"license\": \"MIT\",\n\t\t\t\"keywords\": [\"markdown\", \"search\", \"qmd\"],\n\t\t\t\"skills\": [\"./skills/\"],\n\t\t\t\"mcpServers\": {\n\t\t\t\t\"qmd\": {\n\t\t\t\t\t\"command\": \"qmd\",\n\t\t\t\t\t\"args\": [\"mcp\"]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t]\n}\n",
        "README.md": "# QMD - Query Markup Documents\n\nAn on-device search engine for everything you need to remember. Index your markdown notes, meeting transcripts, documentation, and knowledge bases. Search with keywords or natural language. Ideal for your agentic flows.\n\nQMD combines BM25 full-text search, vector semantic search, and LLM re-ranking—all running locally via node-llama-cpp with GGUF models.\n\n![QMD Architecture](assets/qmd-architecture.png)\n\nYou can read more about QMD's progress in the [CHANGELOG](CHANGELOG.md).\n\n## Quick Start\n\n```sh\n# Install globally (Node or Bun)\nnpm install -g @tobilu/qmd\n# or\nbun install -g @tobilu/qmd\n\n# Or run directly\nnpx @tobilu/qmd ...\nbunx @tobilu/qmd ...\n\n# Create collections for your notes, docs, and meeting transcripts\nqmd collection add ~/notes --name notes\nqmd collection add ~/Documents/meetings --name meetings\nqmd collection add ~/work/docs --name docs\n\n# Add context to help with search results, each piece of context will be returned when matching sub documents are returned. This works as a tree. This is the key feature of QMD as it allows LLMs to make much better contextual choices when selecting documents. Don't sleep on it!\nqmd context add qmd://notes \"Personal notes and ideas\"\nqmd context add qmd://meetings \"Meeting transcripts and notes\"\nqmd context add qmd://docs \"Work documentation\"\n\n# Generate embeddings for semantic search\nqmd embed\n\n# Search across everything\nqmd search \"project timeline\"           # Fast keyword search\nqmd vsearch \"how to deploy\"             # Semantic search\nqmd query \"quarterly planning process\"  # Hybrid + reranking (best quality)\n\n# Get a specific document\nqmd get \"meetings/2024-01-15.md\"\n\n# Get a document by docid (shown in search results)\nqmd get \"#abc123\"\n\n# Get multiple documents by glob pattern\nqmd multi-get \"journals/2025-05*.md\"\n\n# Search within a specific collection\nqmd search \"API\" -c notes\n\n# Export all matches for an agent\nqmd search \"API\" --all --files --min-score 0.3\n```\n\n### Using with AI Agents\n\nQMD's `--json` and `--files` output formats are designed for agentic workflows:\n\n```sh\n# Get structured results for an LLM\nqmd search \"authentication\" --json -n 10\n\n# List all relevant files above a threshold\nqmd query \"error handling\" --all --files --min-score 0.4\n\n# Retrieve full document content\nqmd get \"docs/api-reference.md\" --full\n```\n\n### MCP Server\n\nAlthough the tool works perfectly fine when you just tell your agent to use it on the command line, it also exposes an MCP (Model Context Protocol) server for tighter integration.\n\n**Tools exposed:**\n- `qmd_search` - Fast BM25 keyword search (supports collection filter)\n- `qmd_vector_search` - Semantic vector search (supports collection filter)\n- `qmd_deep_search` - Deep search with query expansion and reranking (supports collection filter)\n- `qmd_get` - Retrieve document by path or docid (with fuzzy matching suggestions)\n- `qmd_multi_get` - Retrieve multiple documents by glob pattern, list, or docids\n- `qmd_status` - Index health and collection info\n\n**Claude Desktop configuration** (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"qmd\": {\n      \"command\": \"qmd\",\n      \"args\": [\"mcp\"]\n    }\n  }\n}\n```\n\n**Claude Code** — Install the plugin (recommended):\n\n```bash\nclaude marketplace add tobi/qmd\nclaude plugin add qmd@qmd\n```\n\nOr configure MCP manually in `~/.claude/settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"qmd\": {\n      \"command\": \"qmd\",\n      \"args\": [\"mcp\"]\n    }\n  }\n}\n```\n\n#### HTTP Transport\n\nBy default, QMD's MCP server uses stdio (launched as a subprocess by each client). For a shared, long-lived server that avoids repeated model loading, use the HTTP transport:\n\n```sh\n# Foreground (Ctrl-C to stop)\nqmd mcp --http                    # localhost:8181\nqmd mcp --http --port 8080        # custom port\n\n# Background daemon\nqmd mcp --http --daemon           # start, writes PID to ~/.cache/qmd/mcp.pid\nqmd mcp stop                      # stop via PID file\nqmd status                        # shows \"MCP: running (PID ...)\" when active\n```\n\nThe HTTP server exposes two endpoints:\n- `POST /mcp` — MCP Streamable HTTP (JSON responses, stateless)\n- `GET /health` — liveness check with uptime\n\nLLM models stay loaded in VRAM across requests. Embedding/reranking contexts are disposed after 5 min idle and transparently recreated on the next request (~1s penalty, models remain loaded).\n\nPoint any MCP client at `http://localhost:8181/mcp` to connect.\n\n## Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         QMD Hybrid Search Pipeline                          │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n                              ┌─────────────────┐\n                              │   User Query    │\n                              └────────┬────────┘\n                                       │\n                        ┌──────────────┴──────────────┐\n                        ▼                             ▼\n               ┌────────────────┐            ┌────────────────┐\n               │ Query Expansion│            │  Original Query│\n               │  (fine-tuned)  │            │   (×2 weight)  │\n               └───────┬────────┘            └───────┬────────┘\n                       │                             │\n                       │ 2 alternative queries       │\n                       └──────────────┬──────────────┘\n                                      │\n              ┌───────────────────────┼───────────────────────┐\n              ▼                       ▼                       ▼\n     ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n     │ Original Query  │     │ Expanded Query 1│     │ Expanded Query 2│\n     └────────┬────────┘     └────────┬────────┘     └────────┬────────┘\n              │                       │                       │\n      ┌───────┴───────┐       ┌───────┴───────┐       ┌───────┴───────┐\n      ▼               ▼       ▼               ▼       ▼               ▼\n  ┌───────┐       ┌───────┐ ┌───────┐     ┌───────┐ ┌───────┐     ┌───────┐\n  │ BM25  │       │Vector │ │ BM25  │     │Vector │ │ BM25  │     │Vector │\n  │(FTS5) │       │Search │ │(FTS5) │     │Search │ │(FTS5) │     │Search │\n  └───┬───┘       └───┬───┘ └───┬───┘     └───┬───┘ └───┬───┘     └───┬───┘\n      │               │         │             │         │             │\n      └───────┬───────┘         └──────┬──────┘         └──────┬──────┘\n              │                        │                       │\n              └────────────────────────┼───────────────────────┘\n                                       │\n                                       ▼\n                          ┌───────────────────────┐\n                          │   RRF Fusion + Bonus  │\n                          │  Original query: ×2   │\n                          │  Top-rank bonus: +0.05│\n                          │     Top 30 Kept       │\n                          └───────────┬───────────┘\n                                      │\n                                      ▼\n                          ┌───────────────────────┐\n                          │    LLM Re-ranking     │\n                          │  (qwen3-reranker)     │\n                          │  Yes/No + logprobs    │\n                          └───────────┬───────────┘\n                                      │\n                                      ▼\n                          ┌───────────────────────┐\n                          │  Position-Aware Blend │\n                          │  Top 1-3:  75% RRF    │\n                          │  Top 4-10: 60% RRF    │\n                          │  Top 11+:  40% RRF    │\n                          └───────────────────────┘\n```\n\n## Score Normalization & Fusion\n\n### Search Backends\n\n| Backend | Raw Score | Conversion | Range |\n|---------|-----------|------------|-------|\n| **FTS (BM25)** | SQLite FTS5 BM25 | `Math.abs(score)` | 0 to ~25+ |\n| **Vector** | Cosine distance | `1 / (1 + distance)` | 0.0 to 1.0 |\n| **Reranker** | LLM 0-10 rating | `score / 10` | 0.0 to 1.0 |\n\n### Fusion Strategy\n\nThe `query` command uses **Reciprocal Rank Fusion (RRF)** with position-aware blending:\n\n1. **Query Expansion**: Original query (×2 for weighting) + 1 LLM variation\n2. **Parallel Retrieval**: Each query searches both FTS and vector indexes\n3. **RRF Fusion**: Combine all result lists using `score = Σ(1/(k+rank+1))` where k=60\n4. **Top-Rank Bonus**: Documents ranking #1 in any list get +0.05, #2-3 get +0.02\n5. **Top-K Selection**: Take top 30 candidates for reranking\n6. **Re-ranking**: LLM scores each document (yes/no with logprobs confidence)\n7. **Position-Aware Blending**:\n   - RRF rank 1-3: 75% retrieval, 25% reranker (preserves exact matches)\n   - RRF rank 4-10: 60% retrieval, 40% reranker\n   - RRF rank 11+: 40% retrieval, 60% reranker (trust reranker more)\n\n**Why this approach**: Pure RRF can dilute exact matches when expanded queries don't match. The top-rank bonus preserves documents that score #1 for the original query. Position-aware blending prevents the reranker from destroying high-confidence retrieval results.\n\n### Score Interpretation\n\n| Score | Meaning |\n|-------|---------|\n| 0.8 - 1.0 | Highly relevant |\n| 0.5 - 0.8 | Moderately relevant |\n| 0.2 - 0.5 | Somewhat relevant |\n| 0.0 - 0.2 | Low relevance |\n\n## Requirements\n\n### System Requirements\n\n- **Node.js** >= 22\n- **Bun** >= 1.0.0\n- **macOS**: Homebrew SQLite (for extension support)\n  ```sh\n  brew install sqlite\n  ```\n\n### GGUF Models (via node-llama-cpp)\n\nQMD uses three local GGUF models (auto-downloaded on first use):\n\n| Model | Purpose | Size |\n|-------|---------|------|\n| `embeddinggemma-300M-Q8_0` | Vector embeddings | ~300MB |\n| `qwen3-reranker-0.6b-q8_0` | Re-ranking | ~640MB |\n| `qmd-query-expansion-1.7B-q4_k_m` | Query expansion (fine-tuned) | ~1.1GB |\n\nModels are downloaded from HuggingFace and cached in `~/.cache/qmd/models/`.\n\n## Installation\n\n```sh\nnpm install -g @tobilu/qmd\n# or\nbun install -g @tobilu/qmd\n```\n\n### Development\n\n```sh\ngit clone https://github.com/tobi/qmd\ncd qmd\nnpm install\nnpm link\n```\n\n## Usage\n\n### Collection Management\n\n```sh\n# Create a collection from current directory\nqmd collection add . --name myproject\n\n# Create a collection with explicit path and custom glob mask\nqmd collection add ~/Documents/notes --name notes --mask \"**/*.md\"\n\n# List all collections\nqmd collection list\n\n# Remove a collection\nqmd collection remove myproject\n\n# Rename a collection\nqmd collection rename myproject my-project\n\n# List files in a collection\nqmd ls notes\nqmd ls notes/subfolder\n```\n\n### Generate Vector Embeddings\n\n```sh\n# Embed all indexed documents (900 tokens/chunk, 15% overlap)\nqmd embed\n\n# Force re-embed everything\nqmd embed -f\n```\n\n### Context Management\n\nContext adds descriptive metadata to collections and paths, helping search understand your content.\n\n```sh\n# Add context to a collection (using qmd:// virtual paths)\nqmd context add qmd://notes \"Personal notes and ideas\"\nqmd context add qmd://docs/api \"API documentation\"\n\n# Add context from within a collection directory\ncd ~/notes && qmd context add \"Personal notes and ideas\"\ncd ~/notes/work && qmd context add \"Work-related notes\"\n\n# Add global context (applies to all collections)\nqmd context add / \"Knowledge base for my projects\"\n\n# List all contexts\nqmd context list\n\n# Remove context\nqmd context rm qmd://notes/old\n```\n\n### Search Commands\n\n```\n┌──────────────────────────────────────────────────────────────────┐\n│                        Search Modes                              │\n├──────────┬───────────────────────────────────────────────────────┤\n│ search   │ BM25 full-text search only                           │\n│ vsearch  │ Vector semantic search only                          │\n│ query    │ Hybrid: FTS + Vector + Query Expansion + Re-ranking  │\n└──────────┴───────────────────────────────────────────────────────┘\n```\n\n```sh\n# Full-text search (fast, keyword-based)\nqmd search \"authentication flow\"\n\n# Vector search (semantic similarity)\nqmd vsearch \"how to login\"\n\n# Hybrid search with re-ranking (best quality)\nqmd query \"user authentication\"\n```\n\n### Options\n\n```sh\n# Search options\n-n <num>           # Number of results (default: 5, or 20 for --files/--json)\n-c, --collection   # Restrict search to a specific collection\n--all              # Return all matches (use with --min-score to filter)\n--min-score <num>  # Minimum score threshold (default: 0)\n--full             # Show full document content\n--line-numbers     # Add line numbers to output\n--index <name>     # Use named index\n\n# Output formats (for search and multi-get)\n--files            # Output: docid,score,filepath,context\n--json             # JSON output with snippets\n--csv              # CSV output\n--md               # Markdown output\n--xml              # XML output\n\n# Get options\nqmd get <file>[:line]  # Get document, optionally starting at line\n-l <num>               # Maximum lines to return\n--from <num>           # Start from line number\n\n# Multi-get options\n-l <num>           # Maximum lines per file\n--max-bytes <num>  # Skip files larger than N bytes (default: 10KB)\n```\n\n### Output Format\n\nDefault output is colorized CLI format (respects `NO_COLOR` env):\n\n```\ndocs/guide.md:42 #a1b2c3\nTitle: Software Craftsmanship\nContext: Work documentation\nScore: 93%\n\nThis section covers the **craftsmanship** of building\nquality software with attention to detail.\nSee also: engineering principles\n\n\nnotes/meeting.md:15 #d4e5f6\nTitle: Q4 Planning\nContext: Personal notes and ideas\nScore: 67%\n\nDiscussion about code quality and craftsmanship\nin the development process.\n```\n\n- **Path**: Collection-relative path (e.g., `docs/guide.md`)\n- **Docid**: Short hash identifier (e.g., `#a1b2c3`) - use with `qmd get #a1b2c3`\n- **Title**: Extracted from document (first heading or filename)\n- **Context**: Path context if configured via `qmd context add`\n- **Score**: Color-coded (green >70%, yellow >40%, dim otherwise)\n- **Snippet**: Context around match with query terms highlighted\n\n### Examples\n\n```sh\n# Get 10 results with minimum score 0.3\nqmd query -n 10 --min-score 0.3 \"API design patterns\"\n\n# Output as markdown for LLM context\nqmd search --md --full \"error handling\"\n\n# JSON output for scripting\nqmd query --json \"quarterly reports\"\n\n# Use separate index for different knowledge base\nqmd --index work search \"quarterly reports\"\n```\n\n### Index Maintenance\n\n```sh\n# Show index status and collections with contexts\nqmd status\n\n# Re-index all collections\nqmd update\n\n# Re-index with git pull first (for remote repos)\nqmd update --pull\n\n# Get document by filepath (with fuzzy matching suggestions)\nqmd get notes/meeting.md\n\n# Get document by docid (from search results)\nqmd get \"#abc123\"\n\n# Get document starting at line 50, max 100 lines\nqmd get notes/meeting.md:50 -l 100\n\n# Get multiple documents by glob pattern\nqmd multi-get \"journals/2025-05*.md\"\n\n# Get multiple documents by comma-separated list (supports docids)\nqmd multi-get \"doc1.md, doc2.md, #abc123\"\n\n# Limit multi-get to files under 20KB\nqmd multi-get \"docs/*.md\" --max-bytes 20480\n\n# Output multi-get as JSON for agent processing\nqmd multi-get \"docs/*.md\" --json\n\n# Clean up cache and orphaned data\nqmd cleanup\n```\n\n## Data Storage\n\nIndex stored in: `~/.cache/qmd/index.sqlite`\n\n### Schema\n\n```sql\ncollections     -- Indexed directories with name and glob patterns\npath_contexts   -- Context descriptions by virtual path (qmd://...)\ndocuments       -- Markdown content with metadata and docid (6-char hash)\ndocuments_fts   -- FTS5 full-text index\ncontent_vectors -- Embedding chunks (hash, seq, pos, 900 tokens each)\nvectors_vec     -- sqlite-vec vector index (hash_seq key)\nllm_cache       -- Cached LLM responses (query expansion, rerank scores)\n```\n\n## Environment Variables\n\n| Variable | Default | Description |\n|----------|---------|-------------|\n| `XDG_CACHE_HOME` | `~/.cache` | Cache directory location |\n\n## How It Works\n\n### Indexing Flow\n\n```\nCollection ──► Glob Pattern ──► Markdown Files ──► Parse Title ──► Hash Content\n    │                                                   │              │\n    │                                                   │              ▼\n    │                                                   │         Generate docid\n    │                                                   │         (6-char hash)\n    │                                                   │              │\n    └──────────────────────────────────────────────────►└──► Store in SQLite\n                                                                       │\n                                                                       ▼\n                                                                  FTS5 Index\n```\n\n### Embedding Flow\n\nDocuments are chunked into ~900-token pieces with 15% overlap using smart boundary detection:\n\n```\nDocument ──► Smart Chunk (~900 tokens) ──► Format each chunk ──► node-llama-cpp ──► Store Vectors\n                │                           \"title | text\"        embedBatch()\n                │\n                └─► Chunks stored with:\n                    - hash: document hash\n                    - seq: chunk sequence (0, 1, 2...)\n                    - pos: character position in original\n```\n\n### Smart Chunking\n\nInstead of cutting at hard token boundaries, QMD uses a scoring algorithm to find natural markdown break points. This keeps semantic units (sections, paragraphs, code blocks) together.\n\n**Break Point Scores:**\n\n| Pattern | Score | Description |\n|---------|-------|-------------|\n| `# Heading` | 100 | H1 - major section |\n| `## Heading` | 90 | H2 - subsection |\n| `### Heading` | 80 | H3 |\n| `#### Heading` | 70 | H4 |\n| `##### Heading` | 60 | H5 |\n| `###### Heading` | 50 | H6 |\n| ` ``` ` | 80 | Code block boundary |\n| `---` / `***` | 60 | Horizontal rule |\n| Blank line | 20 | Paragraph boundary |\n| `- item` / `1. item` | 5 | List item |\n| Line break | 1 | Minimal break |\n\n**Algorithm:**\n\n1. Scan document for all break points with scores\n2. When approaching the 900-token target, search a 200-token window before the cutoff\n3. Score each break point: `finalScore = baseScore × (1 - (distance/window)² × 0.7)`\n4. Cut at the highest-scoring break point\n\nThe squared distance decay means a heading 200 tokens back (score ~30) still beats a simple line break at the target (score 1), but a closer heading wins over a distant one.\n\n**Code Fence Protection:** Break points inside code blocks are ignored—code stays together. If a code block exceeds the chunk size, it's kept whole when possible.\n\n### Query Flow (Hybrid)\n\n```\nQuery ──► LLM Expansion ──► [Original, Variant 1, Variant 2]\n                │\n      ┌─────────┴─────────┐\n      ▼                   ▼\n   For each query:     FTS (BM25)\n      │                   │\n      ▼                   ▼\n   Vector Search      Ranked List\n      │\n      ▼\n   Ranked List\n      │\n      └─────────┬─────────┘\n                ▼\n         RRF Fusion (k=60)\n         Original query ×2 weight\n         Top-rank bonus: +0.05/#1, +0.02/#2-3\n                │\n                ▼\n         Top 30 candidates\n                │\n                ▼\n         LLM Re-ranking\n         (yes/no + logprob confidence)\n                │\n                ▼\n         Position-Aware Blend\n         Rank 1-3:  75% RRF / 25% reranker\n         Rank 4-10: 60% RRF / 40% reranker\n         Rank 11+:  40% RRF / 60% reranker\n                │\n                ▼\n         Final Results\n```\n\n## Model Configuration\n\nModels are configured in `src/llm.ts` as HuggingFace URIs:\n\n```typescript\nconst DEFAULT_EMBED_MODEL = \"hf:ggml-org/embeddinggemma-300M-GGUF/embeddinggemma-300M-Q8_0.gguf\";\nconst DEFAULT_RERANK_MODEL = \"hf:ggml-org/Qwen3-Reranker-0.6B-Q8_0-GGUF/qwen3-reranker-0.6b-q8_0.gguf\";\nconst DEFAULT_GENERATE_MODEL = \"hf:tobil/qmd-query-expansion-1.7B-gguf/qmd-query-expansion-1.7B-q4_k_m.gguf\";\n```\n\n### EmbeddingGemma Prompt Format\n\n```\n// For queries\n\"task: search result | query: {query}\"\n\n// For documents\n\"title: {title} | text: {content}\"\n```\n\n### Qwen3-Reranker\n\nUses node-llama-cpp's `createRankingContext()` and `rankAndSort()` API for cross-encoder reranking. Returns documents sorted by relevance score (0.0 - 1.0).\n\n### Qwen3 (Query Expansion)\n\nUsed for generating query variations via `LlamaChatSession`.\n\n## License\n\nMIT\n"
      },
      "plugins": [
        {
          "name": "qmd",
          "source": "./",
          "description": "Search and retrieve documents from local markdown files.",
          "version": "0.1.0",
          "author": {
            "name": "tobi",
            "email": "tobi@lutke.com"
          },
          "repository": "https://github.com/tobi/qmd",
          "license": "MIT",
          "keywords": [
            "markdown",
            "search",
            "qmd"
          ],
          "skills": [
            "./skills/"
          ],
          "mcpServers": {
            "qmd": {
              "command": "qmd",
              "args": [
                "mcp"
              ]
            }
          },
          "categories": [
            "markdown",
            "qmd",
            "search"
          ],
          "install_commands": [
            "/plugin marketplace add tobi/qmd",
            "/plugin install qmd@qmd"
          ]
        }
      ]
    }
  ]
}