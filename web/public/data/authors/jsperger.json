{
  "author": {
    "id": "jsperger",
    "display_name": "John Sperger",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/1226744?v=4",
    "url": "https://github.com/jsperger",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 8,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "r-skills",
      "version": null,
      "description": "Skills for R programming with Claude Code",
      "owner_info": {
        "name": "jsperger"
      },
      "keywords": [],
      "repo_full_name": "jsperger/llm-r-skills",
      "repo_url": "https://github.com/jsperger/llm-r-skills",
      "repo_description": "Coding agent skills for R programming",
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-28T22:22:39Z",
        "created_at": "2026-01-13T20:16:12Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 682
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 483
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 1885
        },
        {
          "path": "hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "hooks/hooks.json",
          "type": "blob",
          "size": 301
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/designing-tidy-r-functions",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/designing-tidy-r-functions/SKILL.md",
          "type": "blob",
          "size": 9039
        },
        {
          "path": "skills/ggplot2",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ggplot2/SKILL.md",
          "type": "blob",
          "size": 8132
        },
        {
          "path": "skills/hardhat",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hardhat/SKILL.md",
          "type": "blob",
          "size": 7560
        },
        {
          "path": "skills/metaprogramming",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/metaprogramming/SKILL.md",
          "type": "blob",
          "size": 9260
        },
        {
          "path": "skills/metaprogramming/topic-metaprogramming.md",
          "type": "blob",
          "size": 10415
        },
        {
          "path": "skills/metaprogramming/topic-multiple-columns.md",
          "type": "blob",
          "size": 4845
        },
        {
          "path": "skills/metaprogramming/topic-quosure.md",
          "type": "blob",
          "size": 7829
        },
        {
          "path": "skills/rlang-conditions",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/rlang-conditions/SKILL.md",
          "type": "blob",
          "size": 8023
        },
        {
          "path": "skills/rlang-conditions/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/rlang-conditions/references/customisation.md",
          "type": "blob",
          "size": 4164
        },
        {
          "path": "skills/rlang-conditions/references/error-chaining.md",
          "type": "blob",
          "size": 6893
        },
        {
          "path": "skills/targets-pipelines",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/targets-pipelines/SKILL.md",
          "type": "blob",
          "size": 9943
        },
        {
          "path": "skills/targets-pipelines/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/targets-pipelines/references/pattern-composition.md",
          "type": "blob",
          "size": 5326
        },
        {
          "path": "skills/targets-pipelines/references/tarchetypes-functions.md",
          "type": "blob",
          "size": 6379
        },
        {
          "path": "skills/targets-pipelines/references/targets-utilities.md",
          "type": "blob",
          "size": 2973
        },
        {
          "path": "skills/targets-pipelines/targetopia-packages.md",
          "type": "blob",
          "size": 1374
        },
        {
          "path": "skills/targets-pipelines/topic-dynamic-branching.md",
          "type": "blob",
          "size": 7689
        },
        {
          "path": "skills/targets-pipelines/topic-hybrid-branching.md",
          "type": "blob",
          "size": 8700
        },
        {
          "path": "skills/targets-pipelines/topic-static-branching.md",
          "type": "blob",
          "size": 6202
        },
        {
          "path": "skills/targets-pipelines/topic-target-factories.md",
          "type": "blob",
          "size": 11760
        },
        {
          "path": "skills/tidy-evaluation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/tidy-evaluation/SKILL.md",
          "type": "blob",
          "size": 9477
        },
        {
          "path": "skills/tidymodels-overview",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/tidymodels-overview/SKILL.md",
          "type": "blob",
          "size": 6224
        },
        {
          "path": "skills/tidymodels-overview/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/tidymodels-overview/references/common-problems.md",
          "type": "blob",
          "size": 8657
        },
        {
          "path": "skills/tidymodels-overview/references/packages.md",
          "type": "blob",
          "size": 9983
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"r-skills\",\n  \"owner\": {\n    \"name\": \"jsperger\"\n  },\n  \"metadata\": {\n    \"description\": \"Skills for R programming with Claude Code\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"r-skills\",\n      \"source\": \"./\",\n      \"description\": \"Skills for R programming covering tidyverse patterns, rlang conditions, targets pipelines, metaprogramming, ggplot2, and tidymodels\",\n      \"version\": \"0.1.2\",\n      \"author\": {\n        \"name\": \"jsperger\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"r\",\n        \"rlang\",\n        \"tidyverse\",\n        \"tidymodels\",\n        \"targets\",\n        \"ggplot2\",\n        \"programming\"\n      ],\n      \"category\": \"languages\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"r-skills\",\n  \"version\": \"0.1.2\",\n  \"description\": \"Skills for R programming covering tidyverse patterns, rlang conditions, targets pipelines, metaprogramming, and more\",\n  \"author\": {\n    \"name\": \"jsperger\",\n    \"url\": \"https://github.com/jsperger\"\n  },\n  \"repository\": \"https://github.com/jsperger/llm-r-skills\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"r\", \"rlang\", \"tidyverse\", \"tidymodels\", \"targets\", \"ggplot2\"],\n  \"skills\": \"./skills/\",\n  \"lspServers\": \"./.lsp.json\"\n}\n",
        "README.md": "# r-skills\n\nClaude Code plugin providing skills for R programming. Complementary to [Posit skills](https://github.com/posit-dev/skills).\n\n## Installation\n\n### Via Marketplace (recommended)\n\nAfter pushing to GitHub:\n\n```\n/plugin marketplace add jsperger/llm-r-skills\n/plugin install r-skills@r-skills\n```\n\n### Local Development\n\n```bash\nclaude --plugin-dir /path/to/r-skills\n```\n\n### Dependencies\n\nFor full functionality, the following are required:\n\n- [languageserver](https://github.com/REditorSupport/languageserver) - R package for LSP support\n- [jq](https://jqlang.org/download/) - JSON processor for hooks\n- [air](https://github.com/posit-dev/air) - R code formatter\n\n## Skills\n\n### [designing-tidy-r-functions](skills/designing-tidy-r-functions)\nGuidelines for designing user-friendly R function APIs, covering naming conventions, argument ordering, and output stability.\n\n### [ggplot2](skills/ggplot2)\nggplot2 4.0+ features including S7 migration, theme defaults, and new scale/position aesthetics.\n\n### [hardhat](skills/hardhat)\nInfrastructure for building `tidymodels`-compatible modeling packages using `mold()` and `forge()`.\n\n### [metaprogramming](skills/metaprogramming)\nTechniques for manipulating R expressions using `rlang`: defuse-and-inject pattern, quosures, and symbol construction.\n\n### [rlang-conditions](skills/rlang-conditions)\nError handling with `rlang` and `cli`: formatted output, error chaining, and input validation.\n\n### [targets-pipelines](skills/targets-pipelines)\nComplex `targets` patterns: static branching, dynamic branching, hybrid patterns, and custom target factories.\n\n### [tidy-evaluation](skills/tidy-evaluation)\nProgramming patterns for data-masked functions in the tidyverse using `{{}}` and managing variable ambiguity.\n\n### [tidymodels-overview](skills/tidymodels-overview)\nOverview of the tidymodels ecosystem for machine learning in R.\n",
        "hooks/hooks.json": "{\n  \"description\": \"Automatic code formatting with air\",\n  \"hooks\": {\n    \"PostToolUse\": [\n      {\n        \"matcher\": \"Write|Edit\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"air format .\",\n            \"timeout\": 20\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "skills/designing-tidy-r-functions/SKILL.md": "---\nname: designing-tidy-r-functions\ndescription: >\n  Use when designing R function APIs, reviewing R code for design issues,\n  writing functions for R packages, or evaluating argument ordering and\n  naming decisions. Does NOT cover: style/linting, error handling\n  (rlang-conditions), CLI output (r-lib:cli), testing (testing-r-packages),\n  CRAN compliance (cran-extrachecks).\n---\n\n# Tidy R Function Design\n\nDesign R functions for humans, not computers. Optimize for cognitive load reduction, predictability, and composability. These principles apply to any R code, not just tidyverse packages.\n\n**Core principle:** The less a user needs to think to use your function correctly, the better.\n\n## Quick Reference\n\n| Design Goal | Pattern |\n|-------------|---------|\n| Predictable names | Verb in imperative mood, prefixes for families |\n| Clear arguments | Most important first, optional with defaults last |\n| Pipe-friendly | Primary data as first argument |\n| Type stability | Output type predictable from input types |\n| Enumerated options | Use `arg_match()` with character vector defaults |\n| Side effects | Return input invisibly; partition from computation |\n| Complex strategies | Extract to strategy objects (not boolean flags) |\n\n## Function Naming\n\n### Use Verbs in Imperative Mood\n\n```r\n# Good: imperative verbs\nmutate()\nfilter()\nsummarize()\n\n# Exception: noun-y builders\ngeom_point()\nrecipe()\n```\n\n### Prefer Prefixes Over Suffixes\n\nPrefixes enable autocomplete discovery:\n\n```r\n# Good: common prefix groups related functions\nstr_detect(), str_replace(), str_extract()\nread_csv(), read_tsv(), read_delim()\n\n# Suffixes for variations on a theme\nmap_int(), map_chr(), map_dbl()\n```\n\n### Length Inversely Proportional to Frequency\n\n```r\n# Very frequent -> short\nc(), n(), df\n\n# Less frequent -> descriptive\ncreate_bootstrap_samples()\nvalidate_model_specification()\n```\n\n## Argument Design\n\n### Most Important Arguments First\n\n```r\n# Good: transformed data first (pipe-friendly)\nstr_replace(string, pattern, replacement)\nleft_join(x, y, by)\n\n# Output-determining args early\nread_csv(file, col_types, col_names)\n```\n\n### Required Arguments Have No Defaults\n\n```r\n# Good: required args have no defaults\nmy_function <- function(data, columns, method = \"default\") {\n  # data and columns required, method optional\n}\n\n# Bad: everything has defaults\nmy_function <- function(data = NULL, columns = NULL, method = \"default\")\n```\n\n### Dots Position Matters\n\nPlace `...` between required and optional arguments:\n\n```r\n# Good: forces explicit naming of optional args\nmy_function <- function(x, y, ..., verbose = FALSE, na.rm = TRUE) {\n  # x, y required; anything after ... must be named\n}\n```\n\n### Keep Defaults Short\n\nUse `NULL` for complex defaults, compute in body:\n\n```r\n# Good: NULL signals \"computed if not provided\"\nmy_function <- function(x, weights = NULL) {\n  weights <- weights %||% rep(1, length(x))\n}\n\n# Bad: complex default in signature\nmy_function <- function(x, weights = rep(1, length(x)))\n```\n\n### Enumerate String Options\n\nUse `arg_match()` with character vector defaults:\n\n```r\nmy_function <- function(x, method = c(\"fast\", \"accurate\", \"balanced\")) {\n  method <- rlang::arg_match(method)\n  # method is now validated, first value is default\n}\n```\n\n### Standardize Common Argument Names\n\n| Purpose | Use | Not |\n|---------|-----|-----|\n| New data for prediction | `new_data` | `newdata`, `newData` |\n| Missing value handling | `na_rm` | `na.rm`, `rm.na` |\n| Case weights | `weights` | `wts`, `w` |\n| Predictors (data frame) | `x` | `predictors`, `features` |\n| Outcome (data frame) | `y` | `response`, `target` |\n| Formula interface data | `data` | `df`, `dataset` |\n\n## Output Patterns\n\n### Type Stability\n\nOutput type should be predictable from input types, not values:\n\n```r\n# Bad: type depends on VALUE\nifelse(TRUE, 1L, 2)   # returns integer\nifelse(FALSE, 1L, 2)  # returns double\n\n# Good: type predictable from input types\ndplyr::if_else(TRUE, 1L, 2L)   # always integer\ndplyr::if_else(FALSE, 1L, 2L)  # always integer\n```\n\n### Tibble Predictions\n\nFor modeling functions, predictions should return tibbles:\n- Same number of rows as input\n- Same row order as input\n- Standardized column names: `.pred`, `.pred_class`, `.pred_lower`\n\n```r\n# Good prediction output\npredict(model, new_data)\n#> # A tibble: 100 x 1\n#>     .pred\n#>     <dbl>\n#>  1   3.45\n#>  2   2.89\n```\n\n### Side-Effect Functions Return Invisibly\n\nFunctions called for side effects should return the first argument invisibly:\n\n```r\n# Good: enables piping\nwrite_csv <- function(x, file, ...) {\n  # write the file\n  invisible(x)\n}\n\n# Enables this pattern:\ndata |>\n  write_csv(\"backup.csv\") |>\n  filter(important) |>\n  write_csv(\"filtered.csv\")\n```\n\n## Side Effects\n\n### Partition Side Effects from Computation\n\n```r\n# Bad: computation mixed with side effects\nanalyze <- function(x) {\n  result <- expensive_computation(x)\n  cat(\"Computed result:\", result, \"\\n\")  # side effect buried\n  options(my_option = result)            # hidden state change\n  result\n}\n\n# Good: side effects isolated\nanalyze <- function(x, verbose = FALSE) {\n  result <- expensive_computation(x)\n  if (verbose) cli::cli_inform(\"Computed result: {result}\")\n  result\n}\n```\n\n### Make Side Effects Easy to Undo\n\nFunctions that change global state should return previous values:\n\n```r\n# Good: returns previous value for restoration\nold <- options(digits = 3)\n# ... do work ...\noptions(old)  # restore\n```\n\n## Strategy Patterns\n\n### Avoid Boolean Strategy Flags\n\n```r\n# Bad: boolean flags for strategies\ngrepl(pattern, x, perl = TRUE, fixed = FALSE, ignore.case = TRUE)\n# Which combinations are valid? What does perl + fixed mean?\n\n# Good: strategy objects\nstr_detect(x, regex(pattern, ignore_case = TRUE))\nstr_detect(x, fixed(pattern))\n```\n\n### Strategy Objects for Complex Options\n\nWhen strategies need different arguments, create helper functions:\n\n```r\n# Strategy helpers with strategy-specific arguments\nregex <- function(pattern, ignore_case = FALSE, multiline = FALSE) {\n  structure(list(pattern = pattern, ignore_case = ignore_case,\n                 multiline = multiline), class = \"regex\")\n}\n\nfixed <- function(pattern) {\n  structure(list(pattern = pattern), class = \"fixed\")\n}\n\n# Main function accepts strategy objects\nstr_detect <- function(string, pattern) {\n  if (inherits(pattern, \"regex\")) {\n    # regex-specific handling\n  } else if (inherits(pattern, \"fixed\")) {\n    # fixed-specific handling\n  }\n}\n```\n\n## Explicit Over Implicit\n\n### Avoid Global Option Dependencies\n\n```r\n# Bad: behavior depends on global option\nmy_function <- function(x) {\n  na_action <- getOption(\"na.action\")  # implicit input\n  # ...\n}\n\n# Good: explicit argument with informative default\nmy_function <- function(x, na_action = na.omit) {\n  # ...\n}\n```\n\n### Inform Users of Important Defaults\n\nWhen defaults matter, tell the user:\n\n```r\nmy_function <- function(x, tz = Sys.timezone()) {\n  if (missing(tz)) {\n    cli::cli_inform(\"Using timezone: {.val {tz}}\")\n  }\n  # ...\n}\n```\n\n## Model Object Design\n\n### Minimize Stored Data\n\n```r\n# Bad: stores entire training set\nmodel$training_data <- training_set  # memory bloat\n\n# Good: store only what's needed for prediction\nmodel$coefficients <- coefs\nmodel$levels <- factor_levels\n```\n\n### Never Save Call Objects\n\nCall objects can embed entire datasets and environments:\n\n```r\n# Bad: call may contain data\nmodel$call <- match.call()\n\n# Good: omit call or store only essential info\n```\n\n### Use Proper S3 Constructors\n\n```r\n# Constructor (internal)\nnew_my_model <- function(coefficients, levels) {\n  structure(\n    list(coefficients = coefficients, levels = levels),\n    class = \"my_model\"\n  )\n}\n\n# Validator (internal)\nvalidate_my_model <- function(x) {\n  stopifnot(is.numeric(x$coefficients))\n  x\n}\n\n# Helper (user-facing)\nmy_model <- function(...) {\n  result <- new_my_model(...)\n  validate_my_model(result)\n}\n```\n\n### Matrix Subsetting Discipline\n\nAlways preserve matrix structure:\n\n```r\n# Bad: may return vector\nX[, 1]\n\n# Good: always returns matrix\nX[, 1, drop = FALSE]\n```\n\n## Design Review Checklist\n\nWhen reviewing R function design:\n\n- [ ] Function names are verbs in imperative mood (or nouns for builders)\n- [ ] Related functions share a prefix\n- [ ] Most important arguments come first\n- [ ] Primary data is first argument (pipe-friendly)\n- [ ] Required arguments have no defaults\n- [ ] `...` comes between required and optional arguments\n- [ ] String options use `arg_match()` with enumerated defaults\n- [ ] Output type is predictable from input types\n- [ ] Side-effect functions return input invisibly\n- [ ] No hidden dependencies on global options or locale\n- [ ] Strategy variations use objects, not boolean flags\n- [ ] Model objects don't store training data or calls\n- [ ] Matrix subsetting uses `drop = FALSE`\n\n## Resources\n\n- [Tidy Design Principles](https://design.tidyverse.org)\n- [Tidymodels Implementation Principles](https://tidymodels.github.io/model-implementation-principles/)\n- [Advanced R: S3](https://adv-r.hadley.nz/s3.html)\n",
        "skills/ggplot2/SKILL.md": "---\nname: ggplot2\ndescription: Use when working with R ggplot2 package, especially ggplot2 4.0+ features. Covers S7 migration (@ property access), theme defaults with ink/paper/accent, element_geom(), from_theme(), theme shortcuts (theme_sub_*), palette themes, labels with dictionary/attributes, discrete scale improvements (palette, continuous.limits, minor_breaks, sec.axis), position aesthetics (nudge_x/nudge_y, order), facet_wrap dir/space/layout, boxplot/violin/label styling, stat_manual(), stat_connect(), coord reversal.\n---\n\n# ggplot2 Reference\n\nggplot2 is an R package for producing visualizations using a grammar of graphics. You compose plots from data, mappings, layers, scales, facets, coordinates, and themes.\n\n## Core Components\n\n### Data and Mapping\n\n```r\nggplot(data = mpg, mapping = aes(x = cty, y = hwy))\n```\n\n- **Data**: Tidy data frame (rows = observations, columns = variables)\n- **Mapping**: `aes()` links data columns to visual properties (x, y, colour, size, etc.)\n\n### Layers\n\nLayers display data using geometry, statistical transformation, and position adjustment:\n\n```r\nggplot(mpg, aes(cty, hwy)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method = \"lm\")\n```\n\n### Scales\n\nControl how data maps to visual properties and create legends/axes:\n\n```r\nggplot(mpg, aes(cty, hwy, colour = class)) +\n  geom_point() +\n  scale_colour_viridis_d()\n```\n\n### Facets\n\nSplit data into panels by variables:\n\n```r\nggplot(mpg, aes(cty, hwy)) +\n  geom_point() +\n  facet_grid(year ~ drv)\n```\n\n### Coordinates\n\nInterpret position aesthetics (Cartesian, polar, map projections):\n\n```r\nggplot(mpg, aes(cty, hwy)) +\n  geom_point() +\n  coord_fixed()\n```\n\n### Theme\n\nControl non-data visual elements:\n\n```r\nggplot(mpg, aes(cty, hwy, colour = class)) +\n  geom_point() +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n```\n\n---\n\n# ggplot2 4.0 Features\n\nggplot2 4.0.0 (September 2025) introduced S7 classes and major new features.\n\n## S7 Migration\n\nAccess properties with `@` instead of `$`:\n\n```r\n# ggplot2 4.0+\nggplot()@data\n\n# Deprecated (still works temporarily)\nggplot()$data\n```\n\nStricter type validation:\n\n```r\nelement_text(hjust = \"foo\")\n#> Error: @hjust must be <NULL>, <integer>, or <double>, not <character>\n```\n\n## Theme-Based Layer Defaults\n\n### Ink, Paper, and Accent\n\nBuilt-in themes accept `ink` (foreground), `paper` (background), `accent` (highlight):\n\n```r\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  theme_gray(paper = \"cornsilk\", ink = \"navy\", accent = \"tomato\")\n```\n\n### element_geom() and from_theme()\n\nSet layer defaults via `theme(geom)`:\n\n```r\nggplot(mpg, aes(class, displ)) +\n  geom_boxplot(aes(colour = from_theme(accent))) +\n  theme(geom = element_geom(\n    accent = \"tomato\",\n    paper = \"cornsilk\",\n    bordertype = \"dashed\",\n    borderwidth = 0.2,\n    linewidth = 2,\n    linetype = \"solid\"\n  ))\n```\n\n### Theme Palettes\n\nSet default palettes in themes:\n\n```r\ntheme(\n  palette.colour.continuous = c(\"chartreuse\", \"forestgreen\"),\n  palette.shape.discrete = c(\"triangle\", \"triangle open\")\n)\n```\n\n## Theme Shortcuts\n\nNew `theme_sub_*()` functions reduce verbosity:\n\n| Shortcut | Prefix replaced |\n|----------|-----------------|\n| `theme_sub_axis()` | `axis.*` |\n| `theme_sub_axis_x()` | `axis.*.x` |\n| `theme_sub_axis_bottom()` | `axis.*.x.bottom` |\n| `theme_sub_legend()` | `legend.*` |\n| `theme_sub_panel()` | `panel.*` |\n| `theme_sub_plot()` | `plot.*` |\n| `theme_sub_strip()` | `strip.*` |\n\n```r\n# Concise\ntheme_sub_axis_x(\n  ticks = element_line(colour = \"red\"),\n  ticks.length = unit(5, \"mm\")\n) +\ntheme_sub_panel(\n  widths = unit(5, \"cm\"),\n  spacing.x = unit(5, \"mm\")\n)\n```\n\n### Margin Helpers\n\n```r\nmargin_auto(1)           # all sides = 1\nmargin_auto(1, 2)        # t/b=1, l/r=2\nmargin_auto(1, 2, 3)     # t=1, l/r=2, b=3\nmargin_part(r = 20)      # partial (NA inherits)\n```\n\n### Panel Sizes\n\n```r\ntheme_sub_panel(widths = unit(c(2, 3, 4), \"cm\"))  # per-panel\ntheme_sub_panel(widths = unit(9, \"cm\"))           # total area\n```\n\n## Labels\n\n### Label Attributes\n\nVariables with `\"label\"` attribute auto-populate axis labels:\n\n```r\nattr(df$bill_dep, \"label\") <- \"Bill depth (mm)\"\nggplot(df, aes(bill_dep, bill_len)) + geom_point()\n```\n\n### Dictionary Labels\n\n```r\ndict <- c(species = \"Species\", bill_dep = \"Bill depth (mm)\")\nggplot(penguins, aes(bill_dep, bill_len, colour = species)) +\n  geom_point() +\n  labs(dictionary = dict)\n```\n\n### Function Labels\n\n```r\nscale_colour_discrete(name = toupper)\nguides(x = guide_axis(title = tools::toTitleCase))\nlabs(y = \\(x) paste0(x, \" variable\"))\n```\n\nLabel hierarchy (lowest to highest): `aes()` < `labs(dictionary)` < column attribute < `labs()` < `scale_*(name)` < `guide_*(title)`\n\n### Named Breaks\n\n```r\nscale_colour_discrete(breaks = c(\n  \"Pygoscelis adeliae\" = \"Adelie\",\n  \"Pygoscelis papua\" = \"Gentoo\"\n))\n```\n\n## Discrete Scale Improvements\n\n```r\n# Palette for spacing\nscale_x_discrete(palette = scales::pal_manual(c(1:3, 5:7)))\n\n# Consistent limits across facets\nscale_x_discrete(continuous.limits = c(1, 5))\n\n# Minor breaks\nscale_x_discrete(\n  minor_breaks = scales::breaks_width(1, offset = 0.5),\n  guide = guide_axis(minor.ticks = TRUE)\n)\n\n# Secondary axis\nscale_x_discrete(sec.axis = dup_axis(\n  name = \"Counts\",\n  breaks = seq_len(7),\n  labels = paste0(\"n = \", table(mpg$class))\n))\n```\n\n## Position Aesthetics\n\n### Nudge Aesthetics\n\n```r\ngeom_text(aes(nudge_x = sign(value) * 3, label = value))\n```\n\n### Dodge Order\n\n```r\nggplot(data, aes(x, y, fill = group)) +\n  geom_boxplot(position = position_dodge(preserve = \"single\")) +\n  aes(order = group)\n```\n\n## Facets\n\n### Wrapping Directions\n\n8 direction options for `facet_wrap(dir)`:\n\n| dir | Start | Fill |\n|-----|-------|------|\n| `\"lt\"` | top-left | left-to-right |\n| `\"tl\"` | top-left | top-to-bottom |\n| `\"lb\"` | bottom-left | left-to-right |\n| `\"bl\"` | bottom-left | bottom-to-top |\n| `\"rt\"` | top-right | right-to-left |\n| `\"tr\"` | top-right | top-to-bottom |\n| `\"rb\"` | bottom-right | right-to-left |\n| `\"br\"` | bottom-right | bottom-to-top |\n\n### Free Space\n\n```r\nfacet_wrap(~ island, scales = \"free_x\", space = \"free_x\")\n```\n\n### Layer Layout\n\n```r\ngeom_point(colour = \"grey\", layout = \"fixed_rows\")  # repeat in rows\ngeom_point(layout = NULL)                            # use facet vars\nannotate(\"text\", label = \"X\", layout = 6)            # specific panel\n```\n\nOptions: `NULL`, `\"fixed\"`, `<integer>`, `\"fixed_cols\"`, `\"fixed_rows\"`\n\n## Styling\n\n### Boxplot Parts\n\n```r\ngeom_boxplot(\n  whisker.linetype = \"dashed\",\n  box.colour = \"black\",\n  median.linewidth = 2,\n  staplewidth = 0.5,\n  staple.colour = \"grey50\"\n)\n```\n\n### Violin Quantiles\n\n```r\ngeom_violin(\n  quantiles = c(0.1, 0.9),\n  quantile.linetype = 1,\n  quantile.colour = \"red\"\n)\n```\n\n### Labels\n\n```r\ngeom_label(\n  aes(linetype = factor(vs), linewidth = factor(am)),\n  text.colour = \"black\",\n  border.colour = \"blue\"\n)\n```\n\n### Varying Fill\n\n```r\ngeom_area(aes(fill = continuous_var))  # gradient (R 4.1+)\n```\n\n## New Stats\n\n### stat_manual()\n\n```r\nmake_centroids <- function(df) {\n  transform(df, xend = mean(x), yend = mean(y))\n}\nstat_manual(geom = \"segment\", fun = make_centroids)\n```\n\n### stat_connect()\n\n```r\ngeom_line(stat = \"connect\")           # stairstep\ngeom_ribbon(stat = \"connect\", alpha = 0.4)\n\n# Custom connection shape\nsmooth <- cbind(x = seq(0, 1, length.out = 20)[-1],\n                y = scales::rescale(plogis(x, 0.5, 0.1)))\nstat_connect(connection = smooth)\n```\n\n## Coord Reversal\n\n```r\ncoord_cartesian(reverse = \"x\")   # \"y\", \"xy\", \"none\"\ncoord_sf(reverse = \"y\")\ncoord_radial(reverse = \"theta\")  # \"r\", \"thetar\", \"none\"\n```\n\n## Deprecations\n\n| Old | New |\n|-----|-----|\n| `fatten` | `median.linewidth` / `middle.linewidth` |\n| `draw_quantiles` | `quantiles` |\n| `geom_errorbarh()` | `geom_errorbar(orientation = \"y\")` |\n| `coord_trans()` | `coord_transform()` |\n| `borders()` | `annotation_borders()` |\n| `facet_wrap(as.table)` | `facet_wrap(dir)` |\n| `theme_get/set/update/replace()` | `get/set/update/replace_theme()` |\n| `last_plot()` | `get_last_plot()` |\n| `layer_data/grob/scales()` | `get_layer_data/grob()`, `get_panel_scales()` |\n",
        "skills/hardhat/SKILL.md": "---\nname: hardhat\ndescription: >\n  Use when creating an R modeling package that needs standardized preprocessing\n  for formula, data frame, matrix, and recipe interfaces. Covers: mold() for\n  training data preprocessing, forge() for prediction data validation,\n  blueprints, model constructors, spruce functions for output formatting.\ndependencies: R>=4.3, hardhat>=1.4.2\n---\n\n# Creating Modeling Packages with hardhat\n\nThe hardhat package provides infrastructure for building modeling packages with consistent interfaces. It standardizes preprocessing via `mold()` (training) and `forge()` (prediction), handling formula, XY, and recipe inputs uniformly.\n\n## Quick Reference\n\n| Task | Function |\n|------|----------|\n| Preprocess training data | `mold(x, y)` or `mold(formula, data)` |\n| Preprocess prediction data | `forge(new_data, blueprint)` |\n| Create model object | `new_model(..., blueprint, class)` |\n| XY blueprint | `default_xy_blueprint(intercept = TRUE)` |\n| Formula blueprint | `default_formula_blueprint(intercept = TRUE)` |\n| Recipe blueprint | `default_recipe_blueprint(intercept = TRUE)` |\n| Format numeric predictions | `spruce_numeric(pred)` |\n| Format class predictions | `spruce_class(pred)` |\n| Format probability predictions | `spruce_prob(pred)` |\n| Validate univariate outcome | `validate_outcomes_are_univariate(outcomes)` |\n| Validate prediction size | `validate_prediction_size(pred, new_data)` |\n\n## Package Architecture\n\n### Stage 1: Model Fitting\n\n```\nUser → simple_lm() methods → bridge → implementation → constructor\n         (formula/xy/recipe)    ↓           ↓              ↓\n                            mold()    lm.fit()      new_model()\n```\n\n### Stage 2: Model Prediction\n\n```\nUser → predict.simple_lm() → bridge → implementation\n              ↓                ↓            ↓\n          forge()          switch()   predict_*_numeric()\n```\n\n## Model Constructor\n\nCreate objects of your model class. Name: `new_<model_class>()`.\n\n```r\nnew_simple_lm <- function(coefs, coef_names, blueprint) {\n  if (!is.numeric(coefs)) {\n    stop(\"`coefs` should be a numeric vector.\", call. = FALSE)\n  }\n  if (!is.character(coef_names)) {\n    stop(\"`coef_names` should be a character vector.\", call. = FALSE)\n  }\n\n  new_model(\n    coefs = coefs,\n    coef_names = coef_names,\n    blueprint = blueprint,\n    class = \"simple_lm\"\n  )\n}\n```\n\n## Implementation Function\n\nCore algorithm. Name: `<model_class>_impl()`. Returns named list of model elements.\n\n```r\nsimple_lm_impl <- function(predictors, outcomes) {\n  lm_fit <- lm.fit(predictors, outcomes)\n  coefs <- lm_fit$coefficients\n\n  list(\n    coefs = unname(coefs),\n    coef_names = names(coefs)\n  )\n}\n```\n\n## Bridge Function\n\nConnects user-facing methods to implementation. Converts `mold()` output to implementation format.\n\n```r\nsimple_lm_bridge <- function(processed) {\n  validate_outcomes_are_univariate(processed$outcomes)\n\n  predictors <- as.matrix(processed$predictors)\n  outcomes <- processed$outcomes[[1]]\n\n  fit <- simple_lm_impl(predictors, outcomes)\n\n  new_simple_lm(\n    coefs = fit$coefs,\n    coef_names = fit$coef_names,\n    blueprint = processed$blueprint\n  )\n}\n```\n\n## User-Facing Fitting Function\n\nGeneric with methods for each interface. Each method calls `mold()` then the bridge.\n\n```r\nsimple_lm <- function(x, ...) {\n UseMethod(\"simple_lm\")\n}\n\nsimple_lm.default <- function(x, ...) {\n  stop(\"`simple_lm()` is not defined for a '\", class(x)[1], \"'.\", call. = FALSE)\n}\n\nsimple_lm.data.frame <- function(x, y, intercept = TRUE, ...) {\n  blueprint <- default_xy_blueprint(intercept = intercept)\n  processed <- mold(x, y, blueprint = blueprint)\n  simple_lm_bridge(processed)\n}\n\nsimple_lm.matrix <- function(x, y, intercept = TRUE, ...) {\n  blueprint <- default_xy_blueprint(intercept = intercept)\n  processed <- mold(x, y, blueprint = blueprint)\n  simple_lm_bridge(processed)\n}\n\nsimple_lm.formula <- function(formula, data, intercept = TRUE, ...) {\n  blueprint <- default_formula_blueprint(intercept = intercept)\n  processed <- mold(formula, data, blueprint = blueprint)\n  simple_lm_bridge(processed)\n}\n\nsimple_lm.recipe <- function(x, data, intercept = TRUE, ...) {\n  blueprint <- default_recipe_blueprint(intercept = intercept)\n  processed <- mold(x, data, blueprint = blueprint)\n  simple_lm_bridge(processed)\n}\n```\n\n## Prediction Implementation\n\nOne function per prediction type. Use `spruce_*()` for standardized output.\n\n```r\npredict_simple_lm_numeric <- function(object, predictors) {\n  coefs <- object$coefs\n  pred <- as.vector(predictors %*% coefs)\n  spruce_numeric(pred)  # Returns tibble with .pred column\n}\n```\n\n## Prediction Bridge\n\nConverts `forge()` output and switches on type.\n\n```r\npredict_simple_lm_bridge <- function(type, object, predictors) {\n  type <- rlang::arg_match(type, \"numeric\")\n  predictors <- as.matrix(predictors)\n\n  switch(\n    type,\n    numeric = predict_simple_lm_numeric(object, predictors)\n  )\n}\n```\n\n## User-Facing Predict Method\n\nCall `forge()` with blueprint, then bridge, then validate.\n\n```r\npredict.simple_lm <- function(object, new_data, type = \"numeric\", ...) {\n  processed <- forge(new_data, object$blueprint)\n  out <- predict_simple_lm_bridge(type, object, processed$predictors)\n  validate_prediction_size(out, new_data)\n  out\n}\n```\n\n## mold() Details\n\nReturns: `predictors` (tibble), `outcomes` (tibble), `extras`, `blueprint`.\n\n### Blueprint Options\n\n| Blueprint | Key Options |\n|-----------|-------------|\n| `default_xy_blueprint()` | `intercept` |\n| `default_formula_blueprint()` | `intercept`, `indicators` (\"traditional\", \"none\", \"one_hot\") |\n| `default_recipe_blueprint()` | `intercept` |\n\n### Formula Special Behaviors\n\n- No intercept by default (unlike base R)\n- `indicators = \"none\"` keeps factors unexpanded\n- Multivariate outcomes: `y1 + y2 ~ x1 + x2` (not `cbind()`)\n\n## forge() Validation\n\nAutomatically validates new data matches training data:\n- Column names must match\n- Column types must be compatible\n- Factor levels must be subset of training levels\n- Lossy conversions emit warnings (novel levels → NA)\n\n```r\n# Missing column → error\n# Wrong type (double for factor) → error\n# Character for factor → silent conversion\n# Novel factor level → warning + NA\n```\n\n## Spruce Functions\n\nStandardize prediction output to tidymodels conventions:\n\n| Function | Output Column |\n|----------|---------------|\n| `spruce_numeric(pred)` | `.pred` |\n| `spruce_class(pred)` | `.pred_class` |\n| `spruce_prob(pred_matrix)` | `.pred_{class_name}` |\n\n## Validation Functions\n\n| Function | Checks |\n|----------|--------|\n| `validate_outcomes_are_univariate()` | Single outcome column |\n| `validate_prediction_size()` | Output rows == input rows |\n| `validate_outcomes_are_numeric()` | Numeric outcomes |\n| `validate_predictors_are_numeric()` | Numeric predictors |\n\n## See Also\n\n- **designing-tidy-r-functions**: Function API design\n- **r-metaprogramming**: Expression manipulation (if customizing blueprints)\n- **testing-r-packages**: Testing patterns\n\n## Vignettes\n\nAccess detailed documentation via R:\n\n```r\n# Open vignette in browser\nRShowDoc(\"mold\", package = \"hardhat\")    # Molding data for modeling\nRShowDoc(\"forge\", package = \"hardhat\")   # Forging data for predictions\nRShowDoc(\"package\", package = \"hardhat\") # Creating modeling packages\n\n# Or browse all vignettes\nbrowseVignettes(\"hardhat\")\n```\n\n## External Resources\n\n- [tidymodels implementation principles](https://tidymodels.github.io/model-implementation-principles/)\n- [hardhat documentation](https://hardhat.tidymodels.org/)\n",
        "skills/metaprogramming/SKILL.md": "---\nname: metaprogramming\ndescription: >\n  Use when writing R code that manipulates expressions, builds code\n  programmatically, or needs to understand rlang's defuse/inject mechanics.\n  Covers: defusing with expr()/enquo()/enquos(), quosure environment tracking,\n  injection with !!/!!!/{{, symbol construction with sym()/syms(). Does NOT\n  cover: data-mask programming patterns (tidy-evaluation), error handling\n  (rlang-conditions), function design (designing-tidy-r-functions).\ndependencies: R>=4.3, rlang>=1.1.3\n---\n\n# R Metaprogramming with rlang\n\nMetaprogramming is the ability to defuse, create, and inject R expressions. The core pattern is **defuse-and-inject**: capture code as data, optionally transform it, then inject it into another context for evaluation.\n\n## Quick Reference\n\n| Task | Function/Operator |\n|------|-------------------|\n| Defuse your own expression | `expr(x + 1)` |\n| Defuse user's single argument | `enquo(arg)` |\n| Defuse user's `...` arguments | `enquos(...)` |\n| Inject single expression | `!!` or `{{` |\n| Splice list of expressions | `!!!` |\n| Get expression from quosure | `quo_get_expr(q)` |\n| Get environment from quosure | `quo_get_env(q)` |\n| Build symbol from string | `sym(\"name\")` |\n| Build symbol with .data pronoun | `data_sym(\"name\")` |\n| Build symbols from vector | `syms(names)` / `data_syms(names)` |\n| Auto-label expression | `as_label(quo)` |\n| Format argument as string | `englue(\"{{ x }}\")` |\n| Interpolate name in dynamic dots | `\"{name}\" := value` |\n| Interpolate argument in name | `\"{{ arg }}\" := value` |\n\n## Defusing Expressions\n\nDefusing stops evaluation and returns the expression as a tree-like object (a \"blueprint\" for computation).\n\n```r\n# Normal evaluation returns result\n1 + 1\n#> [1] 2\n\n# Defusing returns the expression\nexpr(1 + 1)\n#> 1 + 1\n```\n\n### expr() vs enquo()\n\n| Function | Defuses | Returns | Use When |\n|----------|---------|---------|----------|\n| `expr()` | Your own code | Expression | Building expressions locally |\n| `enquo()` | User's argument | Quosure | Forwarding function arguments |\n| `enquos()` | User's `...` | List of quosures | Forwarding multiple arguments |\n\n```r\n# Defuse your own expression\nmy_expr <- expr(mean(x, na.rm = TRUE))\n\n# Defuse user's function argument (returns quosure)\nmy_function <- function(var) {\n\n  enquo(var)\n}\nmy_function(cyl + am)\n#> <quosure>\n#> expr: ^cyl + am\n#> env:  global\n```\n\n### enquos() with .named\n\nAuto-label unnamed arguments:\n\n```r\ng <- function(...) {\n\n  vars <- enquos(..., .named = TRUE)\n  names(vars)\n}\ng(cyl, 1 + 1)\n#> [1] \"cyl\"   \"1 + 1\"\n\ng(foo = cyl, bar = 1 + 1)\n#> [1] \"foo\" \"bar\"\n```\n\n### Types of Defused Expressions\n\n- **Calls**: `f(x, y)`, `1 + 1` - function invocations\n- **Symbols**: `x`, `df` - named object references\n- **Constants**: `1`, `\"text\"`, `NULL` - literal values\n\n## Quosures\n\nA quosure wraps an expression with its original environment. This is critical for correct evaluation when expressions travel across function and package boundaries.\n\n### Why Environments Matter\n\n```r\n# In package A\nmy_function <- function(data, var) {\n  # 'var' was defined in the user's environment\n  # The quosure tracks that environment\n  var <- enquo(var)\n\n  # When passed to package B's function, the quosure\n  # ensures symbols resolve in the correct environment\n  pkg_b_function(data, !!var)\n}\n```\n\nWithout environment tracking, symbols might resolve to wrong objects when code crosses package boundaries.\n\n### When You Need Quosures\n\n| Situation | Use Quosure? |\n|-----------|--------------|\n| Defusing function arguments | Yes - use `enquo()` |\n| Building local expressions | No - use `expr()` |\n| Cross-package composition | Yes - environments matter |\n| Simple local evaluation | No - `expr()` + `eval()` suffices |\n\n### Quosure Operations\n\n```r\nq <- enquo(x + 1)\n\nquo_get_expr(q)   # Extract expression: x + 1\nquo_get_env(q)    # Extract environment\n\n# Create quosure manually\nnew_quosure(expr(x + 1), env = global_env())\n\n# Convert expression to quosure\nas_quosure(expr(x + 1), env = global_env())\n```\n\n## Injection Operators\n\nInjection inserts defused expressions back into code before evaluation.\n\n### `{{` (Embrace)\n\nDefuses and injects in one step. Equivalent to `!!enquo(arg)`:\n\n```r\n# These are equivalent:\nmy_summarise <- function(data, var) {\n\n  data |> dplyr::summarise({{ var }})\n}\n\nmy_summarise <- function(data, var) {\n  data |> dplyr::summarise(!!enquo(var))\n}\n```\n\nUse `{{` when you simply need to forward an argument. Use `enquo()` + `!!` when you need to inspect or transform the expression first.\n\n### `!!` (Bang-Bang)\n\nInjects a single expression:\n\n```r\nvar <- expr(cyl)\nmtcars |> dplyr::summarise(mean(!!var))\n#> Equivalent to: summarise(mean(cyl))\n\n# Inject a value to avoid name collisions\nx <- 100\ndf |> dplyr::mutate(x = x / !!x)\n#> Uses column x divided by env value 100\n```\n\n### `!!!` (Splice)\n\nInjects each element of a list as separate arguments:\n\n```r\nvars <- exprs(cyl, am, vs)\nmtcars |> dplyr::select(!!!vars)\n#> Equivalent to: select(cyl, am, vs)\n\n# With enquos()\nmy_group_by <- function(.data, ...) {\n  .data |> dplyr::group_by(!!!enquos(...))\n}\n```\n\n### Where Operators Work\n\n- **Data-masked arguments**: Implicitly enabled (dplyr, ggplot2, etc.)\n- **inject()**: Explicitly enables operators in any context\n- **Dynamic dots**: `!!!` and `{name}` work in functions using `list2()`\n\n```r\n# Enable injection in base functions\ninject(\n  with(mtcars, mean(!!sym(\"cyl\")))\n)\n```\n\n## Building Expressions from Data\n\n### sym() and syms()\n\nConvert strings to symbols:\n\n```r\nvar <- \"cyl\"\nsym(var)\n#> cyl\n\nvars <- c(\"cyl\", \"am\")\nsyms(vars)\n#> [[1]]\n#> cyl\n#> [[2]]\n#> am\n```\n\n### data_sym() and data_syms()\n\nCreate `.data$col` expressions (safer in tidy eval, avoids collisions):\n\n```r\ndata_sym(\"cyl\")\n#> .data$cyl\n\ndata_syms(c(\"cyl\", \"am\"))\n#> [[1]]\n#> .data$cyl\n#> [[2]]\n#> .data$am\n```\n\nUse `sym()` for base R functions; use `data_sym()` for tidy eval functions.\n\n### Building Calls\n\n```r\n# With call()\ncall(\"mean\", sym(\"x\"), na.rm = TRUE)\n#> mean(x, na.rm = TRUE)\n\n# With expr() and injection\nvar <- sym(\"x\")\nexpr(mean(!!var, na.rm = TRUE))\n#> mean(x, na.rm = TRUE)\n```\n\n## Name Interpolation (Glue Operators)\n\nIn dynamic dots, use glue syntax for names.\n\n### `{` for Variable Values\n\n```r\nname <- \"foo\"\ntibble::tibble(\"{name}\" := 1:3)\n#> # A tibble: 3 x 1\n#>     foo\n#>   <int>\n#> 1     1\n#> 2     2\n#> 3     3\n\ntibble::tibble(\"prefix_{name}\" := 1:3)\n#> Column named: prefix_foo\n```\n\n### `{{` for Argument Labels\n\n```r\nmy_mutate <- function(data, var) {\n  data |> dplyr::mutate(\"mean_{{ var }}\" := mean({{ var }}))\n}\nmtcars |> my_mutate(cyl)\n#> Creates column: mean_cyl\n```\n\n### englue() for String Formatting\n\n```r\nmy_function <- function(var) {\n  englue(\"Column: {{ var }}\")\n}\nmy_function(some_column)\n#> [1] \"Column: some_column\"\n```\n\n## Advanced: Manual Expression Transformation\n\nWhen you need to modify expressions before injection:\n\n```r\nmy_mean <- function(data, var) {\n  # 1. Defuse\n\n  var <- enquo(var)\n\n  # 2. Transform: wrap in mean()\n  wrapped <- expr(mean(!!var, na.rm = TRUE))\n\n  # 3. Inject\n  data |> dplyr::summarise(mean = !!wrapped)\n}\n```\n\nFor multiple arguments:\n\n```r\nmy_mean <- function(.data, ...) {\n  vars <- enquos(..., .named = TRUE)\n\n  # Transform each expression\n  vars <- purrr::map(vars, ~ expr(mean(!!.x, na.rm = TRUE)))\n\n  .data |> dplyr::summarise(!!!vars)\n}\n```\n\n## Base R Equivalents\n\n| rlang | Base R | Notes |\n|-------|--------|-------|\n| `expr()` | `bquote()` | bquote uses `.()` for injection |\n| `enquo()` | `substitute()` | substitute returns naked expr, not quosure |\n| `enquos(...)` | `eval(substitute(alist(...)))` | Workaround for dots |\n| `!!` | `.()` in bquote | Only inside bquote |\n| `eval_tidy()` | `eval()` | eval_tidy supports .data/.env pronouns |\n\n## Pitfalls\n\n### `{{` on Non-Arguments\n\n`{{` should only wrap function arguments. On regular objects, it captures the value, not the expression:\n\n```r\n# Correct: var is a function argument\nmy_fn <- function(var) {{ var }}\n\n# Problematic: x is not an argument\nx <- 1\n{{ x }}  # Returns 1, not the expression\n```\n\n### Operators Out of Context\n\nOutside tidy eval/inject contexts, operators have different meanings:\n\n| Operator | Intended | Outside Context |\n|----------|----------|-----------------|\n| `{{` | Embrace | Double braces (returns value) |\n| `!!` | Inject | Double negation (logical) |\n| `!!!` | Splice | Triple negation (logical) |\n\nThese fail silently. See the [tidy-evaluation](../tidy-evaluation/SKILL.md) skill for details on proper usage contexts.\n\n## See Also\n\n- **tidy-evaluation**: Programming patterns for data-masked functions\n- **designing-tidy-r-functions**: Function API design principles\n- **rlang-conditions**: Error handling with rlang\n\n## Reference Files\n\n- [topic-quosure.md](topic-quosure.md) - Complete quosure reference\n- [topic-metaprogramming.md](topic-metaprogramming.md) - Advanced transformation patterns\n- [topic-multiple-columns.md](topic-multiple-columns.md) - Multiple columns patterns\n\n## Vignettes\n\nAccess detailed rlang documentation via R:\n\n```r\n# Defusing expressions\nvignette(\"topic-defuse\", package = \"rlang\")\n\n# Injection operators\nvignette(\"topic-inject\", package = \"rlang\")\n\n# Or browse all vignettes\nbrowseVignettes(\"rlang\")\n```\n",
        "skills/metaprogramming/topic-metaprogramming.md": "# Metaprogramming Patterns\n\nThe patterns covered in this article rely on _metaprogramming_, the ability to defuse, create, expand, and inject R expressions. A good place to start if you're new to programming on the language is the [Metaprogramming chapter](https://adv-r.hadley.nz/metaprogramming.html) of the [Advanced R](https://adv-r.hadley.nz) book.\n\nIf you haven't already, read the data mask programming topic which covers simpler patterns that do not require as much theory to get up to speed. It covers concepts like argument behaviours and the various patterns you can add to your toolbox (forwarding, names, bridge, and transformative patterns).\n\n\n## Forwarding patterns\n\n### Defuse and inject\n\n`{{` and `...` are sufficient for most purposes. Sometimes however, it is necessary to decompose the forwarding action into its two constitutive steps, defusing and injecting.\n\n`{{` is the combination of `enquo()` and `!!`. These functions are completely equivalent:\n\n```r\nmy_summarise <- function(data, var) {\n  data |> dplyr::summarise({{ var }})\n}\nmy_summarise <- function(data, var) {\n  data |> dplyr::summarise(!!enquo(var))\n}\n```\n\nPassing `...` is equivalent to the combination of `enquos()` and `!!!`:\n\n```r\nmy_group_by <- function(.data, ...) {\n  .data |> dplyr::group_by(...)\n}\nmy_group_by <- function(.data, ...) {\n  .data |> dplyr::group_by(!!!enquos(...))\n}\n```\n\nThe advantage of decomposing the steps is that you gain access to the defused expressions. Once defused, you can inspect or modify the expressions before injecting them in their target context.\n\n\n### Inspecting input labels\n\nFor instance, here is how to create an automatic name for a defused argument using `as_label()`:\n\n```r\nf <- function(var) {\n  var <- enquo(var)\n  as_label(var)\n}\n\nf(cyl)\n#> [1] \"cyl\"\n\nf(1 + 1)\n#> [1] \"1 + 1\"\n```\n\nThis is essentially equivalent to formatting an argument using `englue()`:\n\n```r\nf2 <- function(var) {\n  englue(\"{{ var }}\")\n}\n\nf2(1 + 1)\n#> [1] \"1 + 1\"\n```\n\nWith multiple arguments, use the plural variant `enquos()`. Set `.named` to `TRUE` to automatically call `as_label()` on the inputs for which the user has not provided a name (the same behaviour as in most dplyr verbs):\n\n```r\ng <- function(...) {\n  vars <- enquos(..., .named = TRUE)\n  names(vars)\n}\n\ng(cyl, 1 + 1)\n#> [1] \"cyl\"   \"1 + 1\"\n```\n\nJust like with `dplyr::mutate()`, the user can override automatic names by supplying explicit names:\n\n```r\ng(foo = cyl, bar = 1 + 1)\n#> [1] \"foo\" \"bar\"\n```\n\nDefuse-and-inject patterns are most useful for transforming inputs. Some applications are explored in the Transformation patterns section.\n\n\n## Names patterns\n\n### Symbolise and inject\n\nThe symbolise-and-inject pattern is a _names pattern_ that you can use when `across(all_of())` is not supported. It consists in creating defused expressions that refer to the data-variables represented in the names vector. These are then injected in the data mask context.\n\nSymbolise a single string with `sym()` or `data_sym()`:\n\n```r\nvar <- \"cyl\"\n\nsym(var)\n#> cyl\n\ndata_sym(var)\n#> .data$cyl\n```\n\nSymbolise a character vector with `syms()` or `data_syms()`.\n\n```r\nvars <- c(\"cyl\", \"am\")\n\nsyms(vars)\n#> [[1]]\n#> cyl\n#>\n#> [[2]]\n#> am\n\ndata_syms(vars)\n#> [[1]]\n#> .data$cyl\n#>\n#> [[2]]\n#> .data$am\n```\n\nSimple symbols returned by `sym()` and `syms()` work in a wider variety of cases (with base functions in particular) but we'll use mostly use `data_sym()` and `data_syms()` because they are more robust (see the data mask ambiguity topic). Note that these do not return _symbols_ per se, instead they create _calls_ to `$` that subset the `.data` pronoun.\n\nSince the `.data` pronoun is a tidy eval feature, you can't use it in base functions. As a rule, prefer the `data_`-prefixed variants when you're injecting in tidy eval functions and the unprefixed functions for base functions.\n\nA list of symbols can be injected in data-masked dots with the splice operator `!!!`, which injects each element of the list as a separate argument. For instance, to implement a `group_by()` variant that takes a character vector of column names, you might write:\n\n```r\nmy_group_by <- function(data, vars) {\n  data |> dplyr::group_by(!!!data_syms(vars))\n}\n\nmy_group_by(vars)\n```\n\nIn more complex case, you might want to add R code around the symbols. This requires _transformation_ patterns, see the section below.\n\n\n## Bridge patterns\n\n### `mutate()` as a data-mask to selection bridge\n\nThis is a variant of the `transmute()` bridge pattern described in the data mask programming topic that does not materialise `...` in the intermediate step. Instead, the `...` expressions are defused and inspected. Then the expressions, rather than the columns, are spliced in `mutate()`.\n\n```r\nmy_pivot_longer <- function(data, ...) {\n  # Defuse the dots and inspect the names\n  dots <- enquos(..., .named = TRUE)\n  names <- names(dots)\n\n  # Pass the inputs to `mutate()`\n  data <- data |> dplyr::mutate(!!!dots)\n\n  # Select `...` inputs by name with `all_of()`\n  data |>\n    tidyr::pivot_longer(cols = all_of(names))\n}\n\nmtcars |> my_pivot_longer(cyl, am = am * 100)\n```\n\n1. Defuse the `...` expressions. The `.named` argument ensures unnamed inputs get a default name, just like they would if passed to `mutate()`. Take the names of the list of inputs.\n\n2. Once we have the names, inject the argument expressions into `mutate()` to update the data frame.\n\n3. Finally, pass the names to the tidy selection via `all_of()`.\n\n\n## Transformation patterns\n\n### Transforming inputs manually\n\nIf `across()` and variants are not available, you will need to transform the inputs yourself using metaprogramming techniques. To illustrate the technique we'll reimplement `my_mean()` and without using `across()`. The pattern consists in defusing the input expression, building larger calls around them, and finally inject the modified expressions inside the data-masking functions.\n\nWe'll start with a single named argument for simplicity:\n\n```r\nmy_mean <- function(data, var) {\n  # Defuse the expression\n  var <- enquo(var)\n\n  # Wrap it in a call to `mean()`\n  var <- expr(mean(!!var, na.rm = TRUE))\n\n  # Inject the expanded expression\n  data |> dplyr::summarise(mean = !!var)\n}\n\nmtcars |> my_mean(cyl)\n#> # A tibble: 1 x 1\n#>    mean\n#>   <dbl>\n#> 1  6.19\n```\n\nWith `...` the technique is similar, though a little more involved. We'll use the plural variants `enquos()` and `!!!`. We'll also loop over the variable number of inputs using `purrr::map()`. But the pattern is otherwise basically the same:\n\n```r\nmy_mean <- function(.data, ...) {\n  # Defuse the dots. Make sure they are automatically named.\n  vars <- enquos(..., .named = TRUE)\n\n  # Map over each defused expression and wrap it in a call to `mean()`\n  vars <- purrr::map(vars, ~ expr(mean(!!.x, na.rm = TRUE)))\n\n  # Inject the expressions\n  .data |> dplyr::summarise(!!!vars)\n}\n\nmtcars |> my_mean(cyl)\n#> # A tibble: 1 x 1\n#>     cyl\n#>   <dbl>\n#> 1  6.19\n```\n\nNote that we are inheriting the data-masking behaviour of `summarise()` because we have effectively forwarded `...` inside that verb. This is different than transformation patterns based on `across()` which inherit tidy selection behaviour. In practice, this means the function doesn't support selection helpers and syntax. Instead, it gains the ability to create new vectors on the fly:\n\n```r\nmtcars |> my_mean(cyl = cyl * 100)\n#> # A tibble: 1 x 1\n#>     cyl\n#>   <dbl>\n#> 1  619.\n```\n\n\n## Base patterns\n\nIn this section, we review patterns for programming with _base_ data-masking functions. They essentially consist in building and evaluating expressions in the data mask. We review these patterns and compare them to rlang idioms.\n\n\n### Data-masked `get()`\n\nIn the simplest version of this pattern, `get()` is called with a variable name to retrieve objects from the data mask:\n\n```r\nvar <- \"cyl\"\n\nwith(mtcars, mean(get(var)))\n#> [1] 6.1875\n```\n\nThis sort of pattern is susceptible to names collisions. For instance, the input data frame might contain a variable called `var`:\n\n```r\ndf <- data.frame(var = \"wrong\")\n\nwith(df, mean(get(var)))\n#> Error in mean.default(get(var)): argument is not numeric or logical:\n#>   returning NA\n```\n\nIn general, prefer symbol injection over `get()` to prevent this sort of collisions. With base functions you will need to enable injection operators explicitly using `inject()`:\n\n```r\ninject(\n  with(mtcars, mean(!!sym(var)))\n)\n#> [1] 6.1875\n```\n\nSee the data mask ambiguity topic for more information about names collisions.\n\n\n### Data-masked `parse()` and `eval()`\n\nA more involved pattern consists in building R code in a string and evaluating it in the mask:\n\n```r\nvar1 <- \"am\"\nvar2 <- \"vs\"\n\ncode <- paste(var1, \"==\", var2)\nwith(mtcars, mean(eval(parse(text = code))))\n#> [1] 0.59375\n```\n\nAs before, the `code` variable is vulnerable to names collisions. More importantly, if `var1` and `var2` are user inputs, they could contain adversarial code. Evaluating code assembled from strings is always a risky business:\n\n```r\nvar1 <- \"(function() {\n  Sys.sleep(Inf)  # Could be a coin mining routine\n})()\"\nvar2 <- \"vs\"\n\ncode <- paste(var1, \"==\", var2)\nwith(mtcars, mean(eval(parse(text = code))))\n```\n\nThis is not a big deal if your code is only used internally. However, this code could be part of a public Shiny app which Internet users could exploit. But even internally, parsing is a source of bugs when variable names contain syntactic symbols like `-` or `:`.\n\n```r\nvar1 <- \":var:\"\nvar2 <- \"vs\"\n\ncode <- paste(var1, \"==\", var2)\nwith(mtcars, mean(eval(parse(text = code))))\n#> Error in parse(text = code): <text>:1:1: unexpected ':'\n```\n\nFor these reasons, always prefer to _build_ code instead of parsing code. Building variable names with `sym()` is a way of sanitising inputs.\n\n```r\nvar1 <- \"(function() {\n  Sys.sleep(Inf)  # Could be a coin mining routine\n})()\"\nvar2 <- \"vs\"\n\ncode <- call(\"==\", sym(var1), sym(var2))\n\ncode\n#> `(function() {\\n  Sys.sleep(Inf)\n#> })()` == vs\n```\n\nThe adversarial input now produces an error:\n\n```r\nwith(mtcars, mean(eval(code)))\n#> Error in eval(code): object '(function() {\\n\n#>   Sys.sleep(Inf)  # Could be a coin mining routine\\n})()' not found\n```\n\nFinally, it is recommended to inject the code instead of evaluating it to avoid names collisions:\n\n```r\nvar1 <- \"am\"\nvar2 <- \"vs\"\n\ncode <- call(\"==\", sym(var1), sym(var2))\ninject(\n  with(mtcars, mean(!!code))\n)\n#> [1] 0.59375\n```\n",
        "skills/metaprogramming/topic-multiple-columns.md": "# Multiple Columns Patterns\n\nIn this guide we compare ways of taking multiple columns in a single function argument.\n\nAs a refresher (see the programming patterns article), there are two common ways of passing arguments to data-masking functions. For single arguments, embrace with `{{`:\n\n```r\nmy_group_by <- function(data, var) {\n  data |> dplyr::group_by({{ var }})\n}\n\nmy_pivot_longer <- function(data, var) {\n  data |> tidyr::pivot_longer({{ var }})\n}\n```\n\nFor multiple arguments in `...`, pass them on to functions that also take `...` like `group_by()`, or pass them within `c()` for functions taking tidy selection in a single argument like `pivot_longer()`:\n\n```r\n# Pass dots through\nmy_group_by <- function(.data, ...) {\n  .data |> dplyr::group_by(...)\n}\n\nmy_pivot_longer <- function(.data, ...) {\n  .data |> tidyr::pivot_longer(c(...))\n}\n```\n\nBut what if you want to take multiple columns in a single named argument rather than in `...`?\n\n\n## Using tidy selections\n\nThe idiomatic tidyverse way of taking multiple columns in a single argument is to take a _tidy selection_ (see the Argument behaviours section). In tidy selections, the syntax for passing multiple columns in a single argument is `c()`:\n\n```r\nmtcars |> tidyr::pivot_longer(c(am, cyl, vs))\n```\n\nSince `{{` inherits behaviour, this implementation of `my_pivot_longer()` automatically allows multiple columns passing:\n\n```r\nmy_pivot_longer <- function(data, var) {\n  data |> tidyr::pivot_longer({{ var }})\n}\n\nmtcars |> my_pivot_longer(c(am, cyl, vs))\n```\n\nFor `group_by()`, which takes data-masked arguments, we'll use `across()` as a _bridge_ (see Bridge patterns).\n\n```r\nmy_group_by <- function(data, var) {\n  data |> dplyr::group_by(across({{ var }}))\n}\n\nmtcars |> my_group_by(c(am, cyl, vs))\n```\n\nWhen embracing in tidyselect context or using `across()` is not possible, you might have to implement tidyselect behaviour manually with `tidyselect::eval_select()`.\n\n\n## Using external defusal\n\nTo implement an argument with tidyselect behaviour, it is necessary to defuse the argument. However defusing an argument which had historically behaved like a regular argument is a rather disruptive breaking change. This is why we could not implement tidy selections in ggplot2 facetting functions like `facet_grid()` and `facet_wrap()`.\n\nAn alternative is to use external defusal of arguments. This is what formula interfaces do for instance. A modelling function takes a formula in a regular argument and the formula defuses the user code:\n\n```r\nmy_lm <- function(data, f, ...) {\n  lm(f, data, ...)\n}\n\nmtcars |> my_lm(disp ~ drat)\n```\n\nOnce created, the defused expressions contained in the formula are passed around like a normal argument. A similar approach was taken to update `facet_` functions to tidy eval. The `vars()` function (a simple alias to `quos()`) is provided so that users can defuse their arguments externally.\n\n```r\nggplot2::facet_grid(\n  ggplot2::vars(cyl),\n  ggplot2::vars(am, vs)\n)\n```\n\nYou can implement this approach by simply taking a list of defused expressions as argument. This list can be passed the usual way to other functions taking such lists:\n\n```r\nmy_facet_grid <- function(rows, cols, ...) {\n  ggplot2::facet_grid(rows, cols, ...)\n}\n```\n\nOr it can be spliced with `!!!`:\n\n```r\nmy_group_by <- function(data, vars) {\n  stopifnot(is_quosures(vars))\n  data |> dplyr::group_by(!!!vars)\n}\n\nmtcars |> my_group_by(dplyr::vars(cyl, am))\n```\n\n\n## A non-approach: Parsing lists\n\nIntuitively, many programmers who want to take a list of expressions in a single argument try to defuse an argument and parse it. The user is expected to supply multiple arguments within a `list()` expression. When such a call is detected, the arguments are retrieved and spliced with `!!!`. Otherwise, the user is assumed to have supplied a single argument which is injected with `!!`. An implementation along these lines might look like this:\n\n```r\nmy_group_by <- function(data, vars) {\n  vars <- enquo(vars)\n\n  if (quo_is_call(vars, \"list\")) {\n    expr <- quo_get_expr(vars)\n    env <- quo_get_env(vars)\n    args <- as_quosures(call_args(expr), env = env)\n    data |> dplyr::group_by(!!!args)\n  } else {\n    data |> dplyr::group_by(!!vars)\n  }\n}\n```\n\nThis does work in simple cases:\n\n```r\nmtcars |> my_group_by(cyl) |> dplyr::group_vars()\n#> [1] \"cyl\"\n\nmtcars |> my_group_by(list(cyl, am)) |> dplyr::group_vars()\n#> [1] \"cyl\" \"am\"\n```\n\nHowever this parsing approach quickly shows limits:\n\n```r\nmtcars |> my_group_by(list2(cyl, am))\n#> Error in `group_by()`: Can't add columns.\n#> i `..1 = list2(cyl, am)`.\n#> i `..1` must be size 32 or 1, not 2.\n```\n\nAlso, it would be better for overall consistency of interfaces to use the tidyselect syntax `c()` for passing multiple columns. In general, we recommend to use either the tidyselect or the external defusal approaches.\n",
        "skills/metaprogramming/topic-quosure.md": "# Quosures\n\nA quosure is a special type of defused expression that keeps track of the original context the expression was written in. The tracking capabilities of quosures is important when interfacing data-masking functions together because the functions might come from two unrelated environments, like two different packages.\n\n\n## Blending environments\n\nLet's take an example where the R user calls the function `summarise_bmi()` from the foo package to summarise a data frame with statistics of a BMI value. Because the `height` variable of their data frame is not in metres, they use a custom function `div100()` to rescale the column.\n\n```r\n# Global environment of user\n\ndiv100 <- function(x) {\n  x / 100\n}\n\ndplyr::starwars |>\n  foo::summarise_bmi(mass, div100(height))\n```\n\nThe `summarise_bmi()` function is a data-masking function defined in the namespace of the foo package which looks like this:\n\n```r\n# Namespace of package foo\n\nbmi <- function(mass, height) {\n  mass / height^2\n}\n\nsummarise_bmi <- function(data, mass, height) {\n  data |>\n    bar::summarise_stats(bmi({{ mass }}, {{ height }}))\n}\n```\n\nThe foo package uses the custom function `bmi()` to perform a computation on two vectors. It interfaces with `summarise_stats()` defined in bar, another package whose namespace looks like this:\n\n```r\n# Namespace of package bar\n\ncheck_numeric <- function(x) {\n  stopifnot(is.numeric(x))\n  x\n}\n\nsummarise_stats <- function(data, var) {\n  data |>\n    dplyr::transmute(\n      var = check_numeric({{ var }})\n    ) |>\n    dplyr::summarise(\n      mean = mean(var, na.rm = TRUE),\n      sd = sd(var, na.rm = TRUE)\n    )\n}\n```\n\nAgain the package bar uses a custom function, `check_numeric()`, to validate its input. It also interfaces with data-masking functions from dplyr (using the define-a-constant trick to avoid issues of double evaluation).\n\nThere are three data-masking functions simultaneously interfacing in this snippet:\n\n- At the bottom, `dplyr::transmute()` takes a data-masked input, and creates a data frame of a single column named `var`.\n\n- Before this, `bar::summarise_stats()` takes a data-masked input inside `dplyr::transmute()` and checks it is numeric.\n\n- And first of all, `foo::summarise_bmi()` takes two data-masked inputs inside `bar::summarise_stats()` and transforms them to a single BMI value.\n\nThere is a fourth context, the global environment where `summarise_bmi()` is called with two columns defined in a data frame, one of which is transformed on the fly with the user function `div100()`.\n\nAll of these contexts (except to some extent the global environment) contain functions that are private and invisible to foreign functions. Yet, the final expanded data-masked expression that is evaluated down the line looks like this (with caret characters indicating the quosure boundaries):\n\n```r\ndplyr::transmute(\n  var = ^check_numeric(^bmi(^mass, ^div100(height)))\n)\n```\n\nThe role of quosures is to let R know that `check_numeric()` should be found in the bar package, `bmi()` in the foo package, and `div100()` in the global environment.\n\n\n## When should I create quosures?\n\nAs a tidyverse user you generally don't need to worry about quosures because `{{` and `...` will create them for you. Introductory texts like [Programming with dplyr](https://dplyr.tidyverse.org/articles/programming.html) or the standard data-mask programming patterns don't even mention the term. In more complex cases you might need to create quosures with `enquo()` or `enquos()` (even though you generally don't need to know or care that these functions return quosures). In this section, we explore when quosures are necessary in these more advanced applications.\n\n\n### Foreign and local expressions\n\nAs a rule of thumb, quosures are only needed for arguments defused with `enquo()` or `enquos()` (or with `{{` which calls `enquo()` implicitly):\n\n```r\nmy_function <- function(var) {\n  var <- enquo(var)\n  their_function(!!var)\n}\n\n# Equivalently\nmy_function <- function(var) {\n  their_function({{ var }})\n}\n```\n\nWrapping defused arguments in quosures is needed because expressions supplied as argument comes from a different environment, the environment of your user. For local expressions created in your function, you generally don't need to create quosures:\n\n```r\nmy_mean <- function(data, var) {\n  # `expr()` is sufficient, no need for `quo()`\n  expr <- expr(mean({{ var }}))\n  dplyr::summarise(data, !!expr)\n}\n\nmy_mean(mtcars, cyl)\n#> # A tibble: 1 x 1\n#>   `mean(cyl)`\n#>         <dbl>\n#> 1        6.19\n```\n\nUsing `quo()` instead of `expr()` would have worked too but it is superfluous because `dplyr::summarise()`, which uses `enquos()`, is already in charge of wrapping your expression within a quosure scoped in your environment.\n\nThe same applies if you evaluate manually. By default, `eval()` and `eval_tidy()` capture your environment:\n\n```r\nmy_mean <- function(data, var) {\n  expr <- expr(mean({{ var }}))\n  eval_tidy(expr, data)\n}\n\nmy_mean(mtcars, cyl)\n#> [1] 6.1875\n```\n\n\n### External defusing\n\nAn exception to this rule of thumb (wrap foreign expressions in quosures, not your own expressions) arises when your function takes multiple expressions in a list instead of `...`. The preferred approach in that case is to take a tidy selection so that users can combine multiple columns using `c()`. If that is not possible, you can take a list of externally defused expressions:\n\n```r\nmy_group_by <- function(data, vars) {\n  stopifnot(is_quosures(vars))\n  data |> dplyr::group_by(!!!vars)\n}\n\nmtcars |> my_group_by(dplyr::vars(cyl, am))\n```\n\nIn this pattern, `dplyr::vars()` defuses expressions externally. It creates a list of quosures because the expressions are passed around from function to function like regular arguments. In fact, `dplyr::vars()` and `ggplot2::vars()` are simple aliases of `quos()`.\n\n```r\ndplyr::vars(cyl, am)\n#> <list_of<quosure>>\n#>\n#> [[1]]\n#> <quosure>\n#> expr: ^cyl\n#> env:  global\n#>\n#> [[2]]\n#> <quosure>\n#> expr: ^am\n#> env:  global\n```\n\nFor more information about external defusing, see topic-multiple-columns.\n\n\n## Technical description of quosures\n\nA quosure carries two things:\n\n- An expression (get it with `quo_get_expr()`).\n- An environment (get it with `quo_get_env()`).\n\nAnd implements these behaviours:\n\n- It is _callable_. Evaluation produces a result.\n\n  For historical reasons, `base::eval()` doesn't support quosure evaluation. Quosures currently require `eval_tidy()`. We would like to fix this limitation in the future.\n\n- It is _hygienic_. It evaluates in the tracked environment.\n\n- It is _maskable_. If evaluated in a data mask (currently only masks created with `eval_tidy()` or `new_data_mask()`), the mask comes first in scope before the quosure environment.\n\n  Conceptually, a quosure inherits from two chains of environments, the data mask and the user environment. In practice rlang implements this special scoping by rechaining the top of the data mask to the quosure environment currently under evaluation.\n\nThere are similarities between promises (the ones R uses to implement lazy evaluation, not the async expressions from the promises package) and quosures. One important difference is that promises are only evaluated once and cache the result for subsequent evaluation. Quosures behave more like calls and can be evaluated repeatedly, potentially in a different data mask. This property is useful to implement split-apply-combine evaluations.\n\n\n## See also\n\n- `enquo()` and `enquos()` to defuse function arguments as quosures. This is the main way quosures are created.\n\n- `quo()` which is like `expr()` but wraps in a quosure. Usually it is not needed to wrap local expressions yourself.\n\n- `quo_get_expr()` and `quo_get_env()` to access quosure components.\n\n- `new_quosure()` and `as_quosure()` to assemble a quosure from components.\n",
        "skills/rlang-conditions/SKILL.md": "---\nname: rlang-conditions\ndescription: >\n  Use when developing R packages that need to handle errors, warnings, and\n  conditions properly. Covers: (1) cli_abort/cli_warn/cli_inform for throwing\n  conditions with formatting, (2) Error call context with caller_env() and\n  caller_arg(), (3) Input validation helpers that report the right function,\n  (4) Error chaining with try_fetch() and parent argument,\n  (5) Testing error conditions with testthat snapshots.\ndependencies: R>=4.3, rlang>=1.1.3\n---\n\n# Condition Handling in R Packages\n\n## When to Use What\n\n| Task | Use |\n|------|-----|\n| Throw formatted error | `cli_abort()` with message and bullets |\n| Throw formatted warning | `cli_warn()` with message and bullets |\n| Display informative message | `cli_inform()` with message and bullets |\n| Show correct function in error | `call = caller_env()` argument |\n| Get argument name dynamically | `arg = caller_arg(x)` in input checkers |\n| Catch and rethrow with context | `try_fetch()` with `parent = cnd` |\n| Take ownership of low-level error | `try_fetch()` with `parent = NA` |\n| Test error messages | `expect_snapshot(error = TRUE, ...)` |\n\n## CLI Conditions Essentials\n\nUse `cli_abort()`, `cli_warn()`, and `cli_inform()` instead of base R's `stop()`, `warning()`, and `message()` for formatted condition messages.\n\n### Basic Usage\n\n```r\n# Simple error\ncli_abort(\"Column {.field name} is required\")\n\n# Simple warning\ncli_warn(\"Column {.field {col}} has missing values\")\n\n# Simple message\ncli_inform(\"Processing {n} file{?s}\")\n```\n\n### Structured Messages with Bullets\n\nUse named character vectors for multi-part messages:\n\n```r\ncli_abort(c(\n\n  \"File not found\",\n  \"x\" = \"Cannot read {.file {path}}\",\n\n  \"i\" = \"Check that the file exists\"\n))\n```\n\n**Bullet types:**\n- `\"x\"` - Error/problem (red X)\n- `\"!\"` - Warning (yellow !)\n- `\"i\"` - Information (blue i)\n- `\"v\"` - Success (green checkmark)\n- `\"*\"` - Bullet point\n- `\">\"` - Arrow/pointer\n\n### Essential Inline Markup\n\nFormat values in condition messages for clarity:\n\n```r\n# Arguments\ncli_abort(\"{.arg x} must be numeric\")\n\n# Files and paths\ncli_abort(\"Cannot find {.file {path}}\")\n\n# Values\ncli_abort(\"Expected {.val TRUE}, got {.val {x}}\")\n\n# Classes/types\ncli_abort(\"Expected numeric, got {.cls {class(x)}}\")\n\n# Code\ncli_abort(\"Use {.code na.rm = TRUE} to ignore missing values\n\")\n\n# Function names\ncli_abort(\"See {.fn mypackage::helper} for details\")\n```\n\n### Pluralization\n\nUse `{?}` for automatic singular/plural handling:\n\n```r\ncli_abort(\"Found {n} error{?s}\")\n#> Found 1 error\n#> Found 3 errors\n\ncli_abort(\"Found {n} director{?y/ies}\")\n#> Found 1 directory\n#> Found 5 directories\n```\n\n## Error Call Context\n\nBy default, `abort()` and `cli_abort()` show the function where they are called. When using error helpers, pass `call = caller_env()` to show the user's function instead.\n\n### The Problem\n\n```r\n# Error helper without call context\nstop_my_class <- function(message) {\n\n  abort(message, class = \"my_class\")\n}\n\nmy_function <- function(x) {\n  stop_my_class(\"Something went wrong\")\n}\n\nmy_function(\"test\")\n#> Error in `stop_my_class()`:\n#> ! Something went wrong\n```\nThe error shows `stop_my_class()` but users called `my_function()`.\n\n### The Solution\n\n```r\nstop_my_class <- function(message, call = caller_env()) {\n  abort(message, class = \"my_class\", call = call)\n}\n\nmy_function <- function(x) {\n  stop_my_class(\"Something went wrong\")\n}\n\nmy_function(\"test\")\n#> Error in `my_function()`:\n#> ! Something went wrong\n```\n\nNow the error correctly shows `my_function()`.\n\n### Pattern: Abort Wrapper\n\n```r\nstop_my_class <- function(message, call = caller_env()) {\n  abort(message, class = \"my_class\", call = call)\n}\n```\n\n## Input Checkers\n\nInput checkers validate function arguments. Use `caller_arg()` to get the argument name dynamically and `caller_env()` for proper error context.\n\n### Basic Input Checker\n\n```r\ncheck_string <- function(x,\n                         arg = caller_arg(x),\n                         call = caller_env()) {\n  if (!is_string(x)) {\n    cli_abort(\"{.arg {arg}} must be a string.\", call = call)\n  }\n}\n```\n\n### Using the Checker\n\n```r\nmy_function <- function(my_arg) {\n  check_string(my_arg)\n  # ... rest of function\n}\n\nmy_function(NA)\n#> Error in `my_function()`:\n#> ! `my_arg` must be a string.\n```\n\nThe error shows:\n- The correct function (`my_function`, not `check_string`)\n- The correct argument name (`my_arg`, not `x`)\n\n### Pattern: Complete Input Checker\n\n```r\ncheck_scalar_numeric <- function(x,\n                                  arg = caller_arg(x),\n                                  call = caller_env()) {\n  if (!is.numeric(x) || length(x) != 1) {\n    cli_abort(c(\n      \"{.arg {arg}} must be a single number\",\n      \"x\" = \"You supplied {.cls {class(x)}} of length {length(x)}\"\n    ), call = call)\n  }\n}\n```\n\n### Built-in Checkers from rlang\n\nrlang provides `check_required()` for mandatory arguments:\n\n```r\nmy_function <- function(data) {\n  check_required(data)\n  # ... rest of function\n}\n\nmy_function()\n#> Error in `my_function()`:\n#> ! `data` is absent but must be supplied.\n```\n\n## Error Chaining\n\nError chaining adds context when catching and rethrowing errors. Use `try_fetch()` to catch errors and `parent` to chain them.\n\n### Basic Error Chaining\n\n```r\nwith_chained_errors <- function(expr, call = caller_env()) {\n  try_fetch(\n    expr,\n    error = function(cnd) {\n      abort(\"Problem during step.\", parent = cnd, call = call)\n    }\n  )\n}\n\nmy_verb <- function(expr) {\n  with_chained_errors(expr)\n}\n\nmy_verb(1 + \"a\")\n#> Error in `my_verb()`:\n#> ! Problem during step.\n#> Caused by error in `1 + \"a\"`:\n#> ! non-numeric argument to binary operator\n```\n\n### Why try_fetch() Over tryCatch()\n\n- Preserves full backtrace context for debugging\n- Allows `options(error = recover)` to work\n- Catches stack overflow errors (R >= 4.2.0)\n\n### Taking Ownership of Errors\n\nUse `parent = NA` to completely replace a low-level error:\n\n```r\nwith_user_friendly_errors <- function(expr, call = caller_env()) {\n  try_fetch(\n    expr,\n    vctrs_error_scalar_type = function(cnd) {\n      abort(\n        \"Must supply a vector.\",\n        parent = NA,\n        error = cnd,  # Store original for debugging\n        call = call\n      )\n    }\n  )\n}\n```\n\n**For advanced error chaining patterns**: See [references/error-chaining.md](references/error-chaining.md) for iteration context, modifying error calls, and case studies.\n\n## Testing Error Conditions\n\nUse testthat snapshot tests to verify error messages.\n\n### Basic Snapshot Test\n\n```r\ntest_that(\"validation errors are clear\", {\n  expect_snapshot(error = TRUE, {\n    my_function(NULL)\n  })\n})\n```\n\nThis creates a snapshot file recording the exact error output.\n\n### Testing Multiple Cases\n\n```r\ntest_that(\"input validation catches bad inputs\", {\n  expect_snapshot(error = TRUE, {\n    check_string(123)\n    check_string(NA)\n    check_string(c(\"a\", \"b\"))\n  })\n})\n```\n\n### Enabling rlang Error Display\n\nTo see the `call` field in error snapshots, depend on rlang >= 1.0.0 in your package. With testthat >= 3.1.2, this enables the enhanced error display automatically.\n\n### Testing Error Classes\n\n```r\ntest_that(\"errors have correct class\", {\n  expect_error(\n    validate_input(NULL),\n    class = \"validation_error\"\n  )\n})\n```\n\n## Resources\n\n### rlang Documentation\n- [Error call documentation](https://rlang.r-lib.org/reference/topic-error-call.html)\n- [Error chaining documentation](https://rlang.r-lib.org/reference/topic-error-chaining.html)\n- [Condition customisation](https://rlang.r-lib.org/reference/topic-condition-customisation.html)\n\n### cli Documentation\n- [cli package](https://cli.r-lib.org/)\n- [Semantic CLI article](https://cli.r-lib.org/articles/semantic-cli.html)\n\n### Style Guide\n- [Tidyverse error style guide](https://style.tidyverse.org/errors.html)\n\n### Reference Files\n- [references/error-chaining.md](references/error-chaining.md) - Advanced error chaining patterns\n- [references/customisation.md](references/customisation.md) - Condition appearance customization\n",
        "skills/rlang-conditions/references/customisation.md": "# Condition Customisation Reference\n\nCustomize the appearance of condition messages from `abort()`, `warn()`, `inform()`, and their cli equivalents using cli package options.\n\n## Table of Contents\n\n1. [Unicode Bullets](#unicode-bullets)\n2. [Bullet Symbols](#bullet-symbols)\n3. [Error Call Colors](#error-call-colors)\n4. [Setting Options](#setting-options)\n\n## Unicode Bullets\n\nBy default, condition messages use unicode bullet symbols:\n\n```r\nrlang::abort(c(\n  \"The error message.\",\n  \"*\" = \"Regular bullet.\",\n  \"i\" = \"Informative bullet.\",\n  \"x\" = \"Cross bullet.\",\n  \"v\" = \"Victory bullet.\",\n  \">\" = \"Arrow bullet.\"\n))\n#> Error:\n#> ! The error message.\n#> * Regular bullet.\n#> i Informative bullet.\n#> x Cross bullet.\n#> v Victory bullet.\n#> > Arrow bullet.\n```\n\n### Disabling Unicode Bullets\n\nUse simple ASCII letters instead:\n\n```r\noptions(cli.condition_unicode_bullets = FALSE)\n\nrlang::abort(c(\n  \"The error message.\",\n  \"*\" = \"Regular bullet.\",\n  \"i\" = \"Informative bullet.\",\n  \"x\" = \"Cross bullet.\",\n  \"v\" = \"Victory bullet.\",\n  \">\" = \"Arrow bullet.\"\n))\n#> Error:\n#> ! The error message.\n#> * Regular bullet.\n#> i Informative bullet.\n#> x Cross bullet.\n#> v Victory bullet.\n#> > Arrow bullet.\n```\n\n## Bullet Symbols\n\nCustomize bullet symbols through cli user themes.\n\n### Uniform Bullets (Except Header)\n\nUse the same symbol for all bullet types:\n\n```r\noptions(cli.user_theme = list(\n  \".cli_rlang .bullet-*\" = list(before = \"* \"),\n  \".cli_rlang .bullet-i\" = list(before = \"* \"),\n  \".cli_rlang .bullet-x\" = list(before = \"* \"),\n  \".cli_rlang .bullet-v\" = list(before = \"* \"),\n  \".cli_rlang .bullet->\" = list(before = \"* \")\n))\n\nrlang::abort(c(\n  \"The error message.\",\n  \"*\" = \"Regular bullet.\",\n  \"i\" = \"Informative bullet.\",\n  \"x\" = \"Cross bullet.\"\n))\n#> Error:\n#> ! The error message.\n#> * Regular bullet.\n#> * Informative bullet.\n#> * Cross bullet.\n```\n\n### All Bullets Including Header\n\nTo change all bullets including the leading `!`:\n\n```r\noptions(cli.user_theme = list(\n  \".cli_rlang .bullet\" = list(before = \"* \")\n))\n\nrlang::abort(c(\n  \"The error message.\",\n  \"*\" = \"Regular bullet.\",\n  \"i\" = \"Informative bullet.\"\n))\n#> Error:\n#> * The error message.\n#> * Regular bullet.\n#> * Informative bullet.\n```\n\n### Custom Symbols Per Type\n\nMix custom symbols:\n\n```r\noptions(cli.user_theme = list(\n  \".cli_rlang .bullet-x\" = list(before = \"[-] \"),\n  \".cli_rlang .bullet-v\" = list(before = \"[+] \"),\n  \".cli_rlang .bullet-i\" = list(before = \"[i] \")\n))\n```\n\n## Error Call Colors\n\nWhen `abort()` is called inside a function, it displays the function call. This is formatted as a `code` element with background highlighting.\n\n### Default Behavior\n\n```r\nsplash <- function() {\n  abort(\"Can't splash without water.\")\n}\n\nsplash()\n#> Error in `splash()`:\n#> ! Can't splash without water.\n```\n\nThe `splash()` text has a highlighted background (light or dark theme aware).\n\n### Custom Code Colors\n\nOverride the code element styling in your cli theme:\n\n```r\noptions(cli.user_theme = list(\n  span.code = list(\n    \"background-color\" = \"#3B4252\",\n    color = \"#E5E9F0\"\n  )\n))\n```\n\n### Theme Properties\n\nAvailable CSS-like properties for `span.code`:\n\n| Property | Description | Example |\n|----------|-------------|---------|\n| `color` | Text color | `\"#E5E9F0\"` |\n| `background-color` | Background color | `\"#3B4252\"` |\n| `font-weight` | Bold/normal | `\"bold\"` |\n| `font-style` | Italic/normal | `\"italic\"` |\n\n## Setting Options\n\n### In .Rprofile\n\nFor personal settings, add to `~/.Rprofile`:\n```r\noptions(\n  cli.condition_unicode_bullets = FALSE,\n  cli.user_theme = list(\n    span.code = list(\n      \"background-color\" = \"#3B4252\",\n      color = \"#E5E9F0\"\n    )\n  )\n)\n```\n\n### In Package Code\n\nFor package defaults, set options in `.onLoad()`:\n\n```r\n.onLoad <- function(libname, pkgname) {\n  # Only if not already set by user\n  if (is.null(getOption(\"my_pkg.bullet_style\"))) {\n    options(my_pkg.bullet_style = \"unicode\")\n  }\n}\n```\n\n**Note**: Generally avoid overriding user's cli theme in packages. These customizations are intended for end-user configuration.\n\n### Checking Current Settings\n\n```r\ngetOption(\"cli.condition_unicode_bullets\")\ngetOption(\"cli.user_theme\")\n```\n",
        "skills/rlang-conditions/references/error-chaining.md": "# Error Chaining Reference\n\nError chaining provides contextual information when errors occur. Chain errors to show high-level context, pipeline steps, or iteration state alongside the original error.\n\n## Table of Contents\n\n1. [Concepts](#concepts)\n2. [try_fetch() Basics](#try_fetch-basics)\n3. [Chaining Patterns](#chaining-patterns)\n4. [Taking Ownership](#taking-ownership)\n5. [Iteration Context](#iteration-context)\n6. [Modifying Error Calls](#modifying-error-calls)\n\n## Concepts\n\n### Causal vs Contextual Errors\n\nError chains have two types of errors:\n\n1. **Causal error** - The original error that interrupted execution\n2. **Contextual error** - Higher-level information about what was happening\n\n```r\n#> Error in `my_verb()`:\n#> ! Problem while processing data.\n#> Caused by error in `1 + \"a\"`:\n#> ! non-numeric argument to binary operator\n```\n\nHere:\n- \"Problem while processing data\" is the **contextual error**\n- \"non-numeric argument to binary operator\" is the **causal error**\n\n### When to Chain Errors\n\nChain errors when you can provide useful context:\n\n- **High-level context**: What operation was being attempted\n- **Pipeline step**: Which step in a multi-step process failed\n- **Iteration context**: Which element/file/iteration failed\n\n## try_fetch() Basics\n\nUse `try_fetch()` instead of `tryCatch()` or `withCallingHandlers()`:\n\n```r\ntry_fetch(\n  expr,\n  error = function(cnd) {\n    # Handle error\n  }\n)\n```\n\n### Why try_fetch()?\n\n| Feature | try_fetch() | tryCatch() | withCallingHandlers() |\n|---------|------------|------------|----------------------|\n| Preserves backtrace | Yes | No | Yes |\n| Catches stack overflow | Yes (R >= 4.2) | No | No |\n| Works with recover | Yes | No | Yes |\n\n### Basic try_fetch() Usage\n\n```r\nresult <- try_fetch(\n  risky_operation(),\n  error = function(cnd) {\n    # cnd is the error condition object\n    # Return a value or rethrow\n    NULL\n  }\n)\n```\n\n## Chaining Patterns\n\n### Pattern 1: Simple Context\n\n```r\nwith_context <- function(expr, context, call = caller_env()) {\n  try_fetch(\n    expr,\n    error = function(cnd) {\n      abort(context, parent = cnd, call = call)\n    }\n  )\n}\n\nload_data <- function(path) {\n  with_context(\n    read.csv(path),\n    sprintf(\"Failed to load data from '%s'\", path)\n  )\n}\n```\n\n### Pattern 2: With Helper Function\n\nCreate reusable error context helpers:\n\n```r\nwith_step_context <- function(expr, step_name, call = caller_env()) {\n  try_fetch(\n    expr,\n    error = function(cnd) {\n      cli_abort(\n        c(\"Problem during {.field {step_name}} step.\"),\n        parent = cnd,\n        call = call\n      )\n    }\n  )\n}\n\nprocess_pipeline <- function(data) {\n  data <- with_step_context(clean(data), \"cleaning\")\n  data <- with_step_context(transform(data), \"transformation\")\n  data <- with_step_context(validate(data), \"validation\")\n  data\n}\n```\n\n### Pattern 3: User-Facing Verb\n\n```r\nmy_verb <- function(expr) {\n  check_required(expr)  # Check required args BEFORE error context\n  with_chained_errors(expr)\n}\n\nwith_chained_errors <- function(expr, call = caller_env()) {\n  try_fetch(\n    expr,\n    error = function(cnd) {\n      abort(\"Problem during step.\", parent = cnd, call = call)\n    }\n  )\n}\n```\n\n**Important**: Check required arguments before setting up error context to avoid confusing error chains for missing arguments.\n\n## Taking Ownership\n\nSometimes you want to completely replace a low-level error with a user-friendly message. Use `parent = NA`:\n\n```r\nwith_friendly_errors <- function(expr, call = caller_env()) {\n  try_fetch(\n    expr,\n    vctrs_error_scalar_type = function(cnd) {\n      abort(\n        \"Must supply a vector.\",\n        parent = NA,       # Don't chain - replace entirely\n        error = cnd,       # Store original for debugging\n        call = call\n      )\n    }\n  )\n}\n```\n\n### When to Take Ownership\n\n- **Do**: Replace low-level technical errors (HTTP, vctrs internal)\n- **Don't**: Hide user errors (wrong input types, etc.)\n- **Don't**: Replace errors indiscriminately\n\n### Accessing the Original Error\n\nStore the original error for debugging:\n\n```r\n# Throw with stored error\nabort(\"User-friendly message\", parent = NA, error = original_cnd)\n\n# Later, access it\nrlang::last_error()$error\n```\n\n## Iteration Context\n\nAdd iteration information when processing multiple items:\n\n### Basic Pattern\n\n```r\nmy_map <- function(.xs, .fn, ...) {\n  out <- vector(\"list\", length(.xs))\n  i <- 0L\n\n  try_fetch(\n    for (i in seq_along(.xs)) {\n      out[[i]] <- .fn(.xs[[i]], ...)\n    },\n    error = function(cnd) {\n      abort(\n        sprintf(\"Problem while mapping element %d.\", i),\n        parent = cnd\n      )\n    }\n  )\n\n  out\n}\n\nlist(1, \"foo\") |> my_map(\\(x) x + 1)\n#> Error:\n#> ! Problem while mapping element 2.\n#> Caused by error in `x + 1`:\n#> ! non-numeric argument to binary operator\n```\n\n### Performance Note\n\nPlace `try_fetch()` **outside** the loop. Wrapping each iteration is expensive:\n\n```r\n# Good - try_fetch outside loop\ntry_fetch(\n  for (i in seq_along(xs)) { ... },\n  error = function(cnd) { ... }\n)\n\n# Bad - try_fetch inside loop (slow!)\nfor (i in seq_along(xs)) {\n  try_fetch(\n    process(xs[[i]]),\n    error = function(cnd) { ... }\n  )\n}\n```\n\n### Named Elements\n\nFor named lists, include the name:\n\n```r\nmy_map <- function(.xs, .fn, ...) {\n  nms <- names(.xs) %||% seq_along(.xs)\n  out <- vector(\"list\", length(.xs))\n  i <- 0L\n\n  try_fetch(\n    for (i in seq_along(.xs)) {\n      out[[i]] <- .fn(.xs[[i]], ...)\n    },\n    error = function(cnd) {\n      abort(\n        sprintf(\"Problem while mapping element `%s`.\", nms[[i]]),\n        parent = cnd\n      )\n    }\n  )\n\n  out\n}\n```\n\n## Modifying Error Calls\n\nSometimes the error call shows an internal function name. You can modify it:\n\n### The Problem\n\n```r\nmy_map <- function(.xs, .fn, ...) {\n  for (i in seq_along(.xs)) {\n    out[[i]] <- .fn(.xs[[i]], ...)  # Error shows `.fn()`\n  }\n}\n\nmy_function <- function(x) {\n  if (!is_string(x)) abort(\"`x` must be a string.\")\n}\n\nlist(1) |> my_map(my_function)\n#> Error in `.fn()`:\n#> ! `x` must be a string.\n```\n\nUsers see `.fn()` but they passed `my_function`.\n\n### The Solution\n\nInspect and modify the error's `call` field:\n\n```r\nmy_map <- function(.xs, .fn, ...) {\n  fn_code <- substitute(.fn)  # Capture the expression passed as .fn\n  out <- vector(\"list\", length(.xs))\n\n  for (i in seq_along(.xs)) {\n    try_fetch(\n      out[[i]] <- .fn(.xs[[i]], ...),\n      error = function(cnd) {\n        # If error call starts with .fn, replace with actual function\n        if (is_call(cnd$call, \".fn\")) {\n          cnd$call[[1]] <- fn_code\n        }\n        abort(\n          sprintf(\"Problem while mapping element %d.\", i),\n          parent = cnd\n        )\n      }\n    )\n  }\n\n  out\n}\n\nlist(1) |> my_map(my_function)\n#> Error:\n#> ! Problem while mapping element 1.\n#> Caused by error in `my_function()`:\n#> ! `x` must be a string.\n```\n\nNow the error shows `my_function()` instead of `.fn()`.\n",
        "skills/targets-pipelines/SKILL.md": "---\nname: targets-pipelines\ndescription: >\n  Use when writing complex targets pipelines with branching, batching, or custom\n  target factories. Symptoms: multiple similar targets, parameter sweeps, method\n  comparisons, simulation studies. Covers: static branching (tar_map/tar_combine),\n  dynamic branching (pattern argument), hybrid patterns, custom target factories,\n  pipeline debugging. Does NOT cover: basic targets usage, HPC deployment,\n  package development workflows (see targetopia-packages.md).\ndependencies: R>=4.3, targets>=1.9.0, tarchetypes>=0.12.0\n---\n\n# Complex targets Pipelines\n\nBranching creates multiple targets from a single definition. Choose your approach based on when iterations are known and what naming you need.\n\n## Quick Reference\n\n| Task | Approach | Key Function/Pattern |\n|------|----------|---------------------|\n| Variants from known parameters | Static | `tarchetypes::tar_map()` |\n| Aggregate static variants | Static | `tarchetypes::tar_combine()` |\n| Runtime-determined iterations | Dynamic | `pattern = map(x)` |\n| All combinations (cartesian) | Dynamic | `pattern = cross(x, y)` |\n| Batched simulation reps | Dynamic | `tarchetypes::tar_rep()` |\n| Static variants + batched reps | Hybrid | `tarchetypes::tar_map_rep()` |\n| Branch over row groups | Dynamic | `tarchetypes::tar_group_by()` |\n| Reusable multi-target pattern | Factory | Custom function + `tar_target_raw()` |\n\n## Decision Tree\n\n```\nNeed multiple similar targets?\n│\n├─ NO ──► Single target, or consider a target factory\n│         for reusable patterns\n│\n└─ YES ─► Iterations known before tar_make()?\n          │\n          ├─ NO ──► DYNAMIC BRANCHING\n          │         pattern = map() / cross()\n          │         → topic-dynamic-branching.md\n          │\n          └─ YES ─► Need friendly target names in DAG?\n                    │\n                    ├─ YES ──► STATIC BRANCHING\n                    │          tarchetypes::tar_map()\n                    │          → topic-static-branching.md\n                    │\n                    └─ NO ───► Either works; prefer dynamic\n                               for simplicity at scale\n\nAdditional decisions:\n\nStatic variants + many replications?\n  └─► HYBRID: tarchetypes::tar_map_rep()\n      → topic-hybrid-branching.md\n\nEncapsulating a reusable multi-target pattern?\n  └─► TARGET FACTORY: tar_target_raw()\n      → topic-target-factories.md\n```\n\n## Approach Comparison\n\n| Aspect | Static | Dynamic | Hybrid |\n|--------|--------|---------|--------|\n| When defined | Before `tar_make()` | During `tar_make()` | Both |\n| Target names | Friendly (`analysis_bayesian`) | Hashed (`analysis_3a7f2b`) | Mixed |\n| Scale | <1000 targets | 100k+ targets | Moderate |\n| Validation | Full `tar_manifest()` | Must run to see branches | Partial |\n| Use case | Method comparison | Simulations, file processing | Methods x replications |\n\n## Static Branching Overview\n\nUse `tarchetypes::tar_map()` when iterations are known at define-time and you want readable target names.\n\n```r\nvalues <- tibble::tibble(\n  method = rlang::syms(c(\"analyze_bayesian\", \"analyze_freq\")),\n  method_name = c(\"bayesian\", \"freq\")\n)\n\ntarchetypes::tar_map(\n  values = values,\n  names = \"method_name\",\n  targets::tar_target(result, method(data)),\n  targets::tar_target(summary, summarize_result(result))\n)\n# Creates: result_bayesian, result_freq, summary_bayesian, summary_freq\n```\n\nAggregate with `tarchetypes::tar_combine()`:\n\n```r\nmapped <- tarchetypes::tar_map(\n  unlist = FALSE,\n  values = values,\n  targets::tar_target(result, method(data))\n)\n\ntarchetypes::tar_combine(\n  all_results,\n  mapped[[\"result\"]],\n  command = dplyr::bind_rows(!!!.x, .id = \"method\")\n)\n```\n\n**Key points:**\n- Function names must be symbols: use `rlang::syms()`, not strings\n- Use `unlist = FALSE` when you need `tar_combine()` afterward\n- `!!!.x` splices the list of upstream results into the command\n\nSee [topic-static-branching.md](topic-static-branching.md) for details on `tar_eval()`, `tar_sub()`, and complex value construction.\n\n## Dynamic Branching Overview\n\nUse the `pattern` argument when branch count depends on data or you need massive scale.\n\n```r\nlist(\n  targets::tar_target(datasets, c(\"trial_A\", \"trial_B\", \"trial_C\")),\n  targets::tar_target(\n    analysis,\n    analyze_data(datasets),\n    pattern = map(datasets)\n  )\n)\n```\n\n**Pattern types:**\n- `map(x, y)` - parallel iteration (zip)\n- `cross(x, y)` - cartesian product\n- `slice(x, index = c(1, 3))` - specific elements\n- `head(x, n = 5)` / `tail()` / `sample()` - subsets\n\n**Critical:** Patterns reference *target names*, not R objects or expressions.\n\n```r\n# WRONG: Can't reference external objects\nmy_vec <- 1:5\ntargets::tar_target(x, process(my_vec), pattern = map(my_vec))\n\n# CORRECT: Reference upstream target by name\nlist(\n  targets::tar_target(params, 1:5),\n  targets::tar_target(result, process(params), pattern = map(params))\n)\n```\n\nSee [topic-dynamic-branching.md](topic-dynamic-branching.md) for iteration modes, row grouping, and batching.\n\n## Hybrid Patterns Overview\n\nCombine static branching (for methods/variants) with dynamic branching (for replications):\n\n```r\nvalues <- tibble::tibble(\n  method = rlang::syms(c(\"method_a\", \"method_b\")),\n  method_name = c(\"a\", \"b\")\n)\n\ntarchetypes::tar_map_rep(\n  name = sim_result,\n  command = method(simulate_data()),\n  values = values,\n  names = \"method_name\",\n  batches = 10,\n\n  reps = 100\n)\n```\n\nSee [topic-hybrid-branching.md](topic-hybrid-branching.md) for complete patterns.\n\n## Target Factories Overview\n\nA target factory is a function that returns pre-configured target objects:\n\n```r\n#' @export\nanalyze_dataset <- function(name, file_path) {\n  name <- targets::tar_deparse_language(substitute(name))\n  name_data <- paste0(name, \"_data\")\n  sym_data <- as.symbol(name_data)\n\n  command <- substitute(analyze(data), env = list(data = sym_data))\n\n  list(\n    targets::tar_target_raw(name_data, substitute(load_csv(file_path))),\n    targets::tar_target_raw(name, command)\n  )\n}\n```\n\nSee [topic-target-factories.md](topic-target-factories.md) for metaprogramming patterns and validation.\n\n## Debugging and Troubleshooting\n\n### Validation Commands\n\n```r\n# Check target definitions before running\ntargets::tar_manifest()\ntargets::tar_manifest(fields = c(\"name\", \"command\", \"pattern\"))\n\n# Visualize DAG (slow with many targets)\ntargets::tar_visnetwork(targets_only = TRUE)\n\n# Test pattern logic without running pipeline\ntargets::tar_pattern(\n  cross(method, map(dataset, seed)),\n  method = 3,\n  dataset = 5,\n  seed = 10\n)\n```\n\n### Common Errors\n\n| Error | Cause | Fix |\n|-------|-------|-----|\n| \"Target 'x' not found\" in pattern | Pattern references non-existent target | Check spelling; ensure upstream target exists |\n| \"object 'x' not found\" in command | External object in pattern | Create upstream target; patterns reference target names only |\n| Strings not callable | Using `c(\"func_a\", \"func_b\")` | Use `rlang::syms()` for function names |\n| Can't selectively combine | Missing `unlist = FALSE` | Add to `tar_map()` when using `tar_combine()` |\n| Cryptic target names | Dynamic branching default | Add metadata to output or use static branching |\n\n### Identifying Failed Branches\n\n```r\n# Check which targets have errors\ntargets::tar_meta() |>\n  dplyr::filter(error != \"\")\n\n# Read specific branch by index\ntargets::tar_read(target_name, branches = 1)\n\n# Inspect all branch metadata\ntargets::tar_branches(target_name)\n```\n\n### Debugging Strategies\n\n1. **Start with `tar_manifest()`** - verify target count and commands before running\n2. **Test with subset** - use `pattern = head(x, n = 3)` for initial runs\n3. **Include provenance** - add parameter values to branch outputs for traceability\n4. **Check upstream first** - use `tar_read()` on dependencies before investigating branching\n\n## Common Mistakes\n\n| Mistake | Problem | Fix |\n|---------|---------|-----|\n| `pattern = map(df$col)` | Can't reference external objects | Create upstream target |\n| `method = c(\"fn_a\", \"fn_b\")` | Strings aren't callable | `rlang::syms(c(\"fn_a\", \"fn_b\"))` |\n| Manual `lapply` + aggregation | Reinventing batching | Use `tar_rep()` or `tar_map_rep()` |\n| `tar_make()` without validation | Errors discovered late | Always `tar_manifest()` first |\n| `expand_grid()` with `syms()` inside | `syms()` doesn't expand in grid | Apply `syms()` after grid creation |\n\n## See Also\n\n- **r-metaprogramming**: Expression manipulation for target factories\n- **tidy-evaluation**: Programming with data-masked functions\n- [targetopia-packages.md](targetopia-packages.md): Package development workflows\n\n## Reference Files\n\n- [topic-static-branching.md](topic-static-branching.md) - Full static branching guide\n- [topic-dynamic-branching.md](topic-dynamic-branching.md) - Full dynamic branching guide\n- [topic-hybrid-branching.md](topic-hybrid-branching.md) - Dynamic within static patterns\n- [topic-target-factories.md](topic-target-factories.md) - Custom factory functions\n- [references/tarchetypes-functions.md](references/tarchetypes-functions.md) - Function reference\n- [references/pattern-composition.md](references/pattern-composition.md) - Pattern algebra\n- [references/targets-utilities.md](references/targets-utilities.md) - Package author utilities\n- [templates/static-branching.R](templates/static-branching.R) - Static branching code template\n- [templates/target-factory.R](templates/target-factory.R) - Target factory code template\n- [targetopia-packages.md](targetopia-packages.md) - Targetopia package development guide\n\n## External Resources\n\n- [targets documentation](https://docs.ropensci.org/targets)\n- [tarchetypes package](https://docs.ropensci.org/tarchetypes)\n- [targets manual: dynamic branching](https://books.ropensci.org/targets/dynamic.html)\n- [targets manual: static branching](https://books.ropensci.org/targets/static.html)\n",
        "skills/targets-pipelines/references/pattern-composition.md": "# Pattern Composition Reference\n\nPatterns in dynamic branching define how targets are sliced and combined. Understanding pattern algebra helps design efficient branching strategies.\n\n## Pattern Types\n\n| Pattern | Behavior | Analogy |\n|---------|----------|---------|\n| `map(x)` | One branch per element | `purrr::map()` |\n| `map(x, y)` | Parallel iteration (zip) | `purrr::map2()` |\n| `cross(x, y)` | All combinations | `tidyr::crossing()` |\n| `head(x, n)` | First n elements | `head()` |\n| `tail(x, n)` | Last n elements | `tail()` |\n| `slice(x, index)` | Specific indices | `dplyr::slice()` |\n| `sample(x, n)` | Random sample | `sample()` |\n\n## Pattern Algebra\n\n### map() - Parallel Iteration\n\nElements from multiple targets are zipped together:\n\n```r\n# x = [1, 2, 3], y = [a, b, c]\npattern = map(x, y)\n# Branches: (1,a), (2,b), (3,c)\n```\n\n**Requirement:** All targets in `map()` must have the same length.\n\n### cross() - Cartesian Product\n\nEvery combination of elements:\n\n```r\n# x = [1, 2], y = [a, b]\npattern = cross(x, y)\n# Branches: (1,a), (1,b), (2,a), (2,b)\n```\n\n### Composing Patterns\n\nPatterns can be nested. The outer pattern determines the primary structure:\n\n```r\n# Cross methods with mapped (dataset, seed) pairs\npattern = cross(method, map(dataset, seed))\n\n# With method = [A, B], dataset = [d1, d2], seed = [s1, s2]:\n# Branches:\n#   A with (d1, s1)\n#   A with (d2, s2)\n#   B with (d1, s1)\n#   B with (d2, s2)\n```\n\n```r\n# Map over methods, cross datasets and seeds within each\npattern = map(method, cross(dataset, seed))\n\n# With method = [A, B], dataset = [d1, d2], seed = [s1, s2]:\n# Branches:\n#   A with (d1, s1)\n#   A with (d1, s2)\n#   A with (d2, s1)\n#   A with (d2, s2)\n#   B with (d1, s1)\n#   ... (8 total)\n```\n\n## Testing Patterns\n\nAlways test complex patterns before running:\n\n```r\ntargets::tar_pattern(\n  cross(method, map(dataset, seed)),\n  method = 3,\n  dataset = 5,\n  seed = 5\n)\n```\n\nOutput shows the expected branch structure and count.\n\n## Equivalent tidyr Operations\n\n| Pattern | tidyr Equivalent |\n|---------|------------------|\n| `map(x, y)` | `tibble(x, y)` (same length vectors) |\n| `cross(x, y)` | `tidyr::crossing(x, y)` |\n| `cross(a, map(b, c))` | `tidyr::crossing(a, tibble(b, c))` |\n\nThis mental model helps predict branch counts:\n\n```r\n# How many branches?\n# cross(method, map(dataset, seed))\n# = nrow(crossing(method, tibble(dataset, seed)))\n# = length(method) * length(dataset)  # assuming dataset == seed length\n```\n\n## Subset Patterns\n\n### head() and tail()\n\nUseful for testing with a subset:\n\n```r\n# Run only first 3 branches\npattern = head(large_dataset, n = 3)\n\n# Run last 5 branches\npattern = tail(large_dataset, n = 5)\n```\n\n### slice()\n\nSelect specific indices:\n\n```r\n# Run branches 1, 5, and 10\npattern = slice(dataset, index = c(1, 5, 10))\n```\n\n### sample()\n\nRandom subset (reproducible with pipeline seed):\n\n```r\n# Run 10 random branches\npattern = sample(dataset, n = 10)\n```\n\n## Aggregation Behavior\n\nDynamic branches auto-aggregate based on the `iteration` argument:\n\n| `iteration` | Split function | Combine function |\n|-------------|----------------|------------------|\n| `\"vector\"` (default) | `vctrs::vec_slice()` | `vctrs::vec_c()` |\n| `\"list\"` | `[[` | `list()` |\n| `\"group\"` | Row groups | `vctrs::vec_rbind()` |\n\n### Reading Aggregated Results\n\n```r\n# All branches combined\ntargets::tar_read(result)\n\n# Specific branches only\ntargets::tar_read(result, branches = 1)\ntargets::tar_read(result, branches = c(2, 5, 7))\n```\n\n## Common Patterns\n\n### File Processing\n\n```r\nlist(\n  tarchetypes::tar_files(data_files, list.files(\"data/\", full.names = TRUE)),\n  targets::tar_target(\n    processed,\n    process_file(data_files),\n    pattern = map(data_files)\n  )\n)\n```\n\n### Parameter Grid\n\n```r\nlist(\n  targets::tar_target(alphas, c(0.1, 0.5, 1.0)),\n  targets::tar_target(lambdas, c(0.01, 0.1, 1.0)),\n  targets::tar_target(\n    model,\n    fit_model(data, alpha = alphas, lambda = lambdas),\n    pattern = cross(alphas, lambdas)\n  )\n)\n```\n\n### Grouped Analysis\n\n```r\nlist(\n  tarchetypes::tar_group_by(grouped_data, load_data(), region),\n  targets::tar_target(\n    regional_analysis,\n    analyze(grouped_data),\n    pattern = map(grouped_data)\n  )\n)\n```\n\n### Parallel with Zip\n\n```r\nlist(\n  targets::tar_target(train_files, list.files(\"train/\")),\n  targets::tar_target(test_files, list.files(\"test/\")),\n  targets::tar_target(\n    evaluation,\n    evaluate(train_files, test_files),\n    pattern = map(train_files, test_files)\n  )\n)\n```\n\n## Anti-Patterns\n\n### External Objects in Patterns\n\n```r\n# WRONG: Patterns can't reference R objects\nmy_vec <- 1:10\ntargets::tar_target(x, f(my_vec), pattern = map(my_vec))\n\n# CORRECT: Create upstream target\nlist(\n  targets::tar_target(params, 1:10),\n  targets::tar_target(x, f(params), pattern = map(params))\n)\n```\n\n### Expressions in Patterns\n\n```r\n# WRONG: Can't use expressions\ntargets::tar_target(x, f(data), pattern = map(data$column))\n\n# CORRECT: Extract column as upstream target\nlist(\n  targets::tar_target(values, data$column),\n  targets::tar_target(x, f(values), pattern = map(values))\n)\n```\n\n### Mismatched Lengths in map()\n\n```r\n# ERROR: x has 3 elements, y has 5\ntargets::tar_target(result, f(x, y), pattern = map(x, y))\n\n# If you want all combinations, use cross()\ntargets::tar_target(result, f(x, y), pattern = cross(x, y))\n```\n",
        "skills/targets-pipelines/references/tarchetypes-functions.md": "# tarchetypes Function Reference\n\nQuick reference for `tarchetypes` functions used in branching patterns.\n\n## Static Branching\n\n| Function | Purpose | Key Arguments |\n|----------|---------|---------------|\n| `tar_map()` | Create target variants from values data frame | `values`, `names`, `unlist` |\n| `tar_combine()` | Aggregate results from multiple targets | `name`, `...targets`, `command` |\n| `tar_eval()` | Evaluate expression with substitution | `expr`, `values` |\n| `tar_sub()` | Substitute values into expression (no eval) | `expr`, `values` |\n\n### tar_map()\n\n```r\ntarchetypes::tar_map(\n  values,\n  ...,\n  names = tidyselect::everything(),\n  descriptions = tidyselect::everything(),\n  unlist = TRUE,\n  delimiter = \"_\"\n)\n```\n\n| Argument | Description |\n|----------|-------------|\n| `values` | Data frame where each row defines one variant |\n| `...` | Target definitions using column names from `values` |\n| `names` | Columns to use for target name suffixes |\n| `unlist` | If `FALSE`, return nested list (required for `tar_combine()`) |\n| `delimiter` | Separator between base name and suffix (default `\"_\"`) |\n\n### tar_combine()\n\n```r\ntarchetypes::tar_combine(\n  name,\n  ...,\n  command = NULL,\n  use_names = TRUE,\n  pattern = NULL,\n  ...\n)\n```\n\n| Argument | Description |\n|----------|-------------|\n| `name` | Name of combined target |\n| `...` | Targets to combine (or list from `tar_map(..., unlist = FALSE)`) |\n| `command` | Aggregation expression; use `!!!.x` to splice upstream results |\n| `use_names` | Include target names in combined result |\n\n## Dynamic Data Frame Grouping\n\n| Function | Strategy | Key Arguments |\n|----------|----------|---------------|\n| `tar_group_by()` | Group by columns | `name`, `command`, `...columns` |\n| `tar_group_count()` | Fixed number of groups | `name`, `command`, `count` |\n| `tar_group_size()` | Fixed rows per group | `name`, `command`, `size` |\n| `tar_group_select()` | Group with tidyselect | `name`, `command`, `...selection` |\n\n### tar_group_by()\n\n```r\ntarchetypes::tar_group_by(\n  name,\n  command,\n  ...,\n  tidy_eval = targets::tar_option_get(\"tidy_eval\"),\n  packages = targets::tar_option_get(\"packages\"),\n  ...\n)\n```\n\nCreates a target with `iteration = \"group\"`. Downstream targets use `pattern = map(grouped_target)` to branch over groups.\n\n```r\nlist(\n  tarchetypes::tar_group_by(grouped_data, load_data(), region, year),\n  targets::tar_target(\n    analysis,\n    analyze(grouped_data),\n    pattern = map(grouped_data)\n  )\n)\n```\n\n### tar_group_count()\n\n```r\ntarchetypes::tar_group_count(\n  name,\n  command,\n  count,\n  ...\n)\n```\n\nSplits data into exactly `count` groups (rows distributed as evenly as possible).\n\n### tar_group_size()\n\n```r\ntarchetypes::tar_group_size(\n  name,\n  command,\n  size,\n  ...\n)\n```\n\nSplits data into groups of approximately `size` rows each.\n\n## Batched Replication\n\n| Function | Purpose | Key Arguments |\n|----------|---------|---------------|\n| `tar_rep()` | Batched replication | `name`, `command`, `batches`, `reps` |\n| `tar_map_rep()` | Static variants + batching | `name`, `command`, `values`, `batches`, `reps` |\n| `tar_rep_raw()` | Raw version of `tar_rep()` | Same as `tar_rep()` |\n| `tar_rep_index()` | Get current rep index in batch | (no args) |\n\n### tar_rep()\n\n```r\ntarchetypes::tar_rep(\n  name,\n  command,\n  batches = 1,\n  reps = 1,\n  rep_workers = 1,\n  tidy_eval = targets::tar_option_get(\"tidy_eval\"),\n  packages = targets::tar_option_get(\"packages\"),\n  iteration = targets::tar_option_get(\"iteration\"),\n  ...\n)\n```\n\n| Argument | Description |\n|----------|-------------|\n| `batches` | Number of dynamic branches |\n| `reps` | Replications per batch |\n| `rep_workers` | Max parallel workers within a batch |\n\nOutput includes `tar_batch`, `tar_rep`, and `tar_seed` columns.\n\n**Seed invariance:** Same results regardless of `batches`/`reps` split:\n\n```r\n# All produce identical results:\ntarchetypes::tar_rep(x, rnorm(1), batches = 100, reps = 1)\ntarchetypes::tar_rep(x, rnorm(1), batches = 10, reps = 10)\ntarchetypes::tar_rep(x, rnorm(1), batches = 1, reps = 100)\n```\n\n### tar_map_rep()\n\n```r\ntarchetypes::tar_map_rep(\n  name,\n  command,\n  values = NULL,\n  names = NULL,\n  descriptions = NULL,\n  batches = 1,\n  reps = 1,\n  rep_workers = 1,\n  ...\n)\n```\n\nCombines static branching (via `values`) with batched replication.\n\n```r\nvalues <- tibble::tibble(\n  method = rlang::syms(c(\"method_a\", \"method_b\")),\n  method_name = c(\"a\", \"b\")\n)\n\ntarchetypes::tar_map_rep(\n  sim,\n  method(simulate_data()),\n  values = values,\n  names = \"method_name\",\n  batches = 10,\n  reps = 100\n)\n# Creates: sim_a, sim_b (each with 10 batches x 100 reps = 1000 total)\n```\n\n### tar_rep_index()\n\nReturns the current replication index within a batch. Useful for conditional logic:\n\n```r\ntarchetypes::tar_rep(\n  sim,\n  {\n    idx <- tarchetypes::tar_rep_index()\n    if (idx == 1) {\n      # First rep: full output\n      list(full = TRUE, result = simulate())\n    } else {\n      # Subsequent: just result\n      list(full = FALSE, result = simulate())\n    }\n  },\n  batches = 10,\n  reps = 100\n)\n```\n\n## File Tracking\n\n| Function | Purpose |\n|----------|---------|\n| `tar_files()` | Track multiple files with dynamic branching |\n| `tar_files_input()` | Track input files only |\n| `tar_files_raw()` | Raw version of `tar_files()` |\n\n### tar_files()\n\n```r\ntarchetypes::tar_files(\n  name,\n  command,\n  tidy_eval = targets::tar_option_get(\"tidy_eval\"),\n  packages = targets::tar_option_get(\"packages\"),\n  ...\n)\n```\n\nCreates file-tracking target that branches dynamically:\n\n```r\nlist(\n  tarchetypes::tar_files(data_files, list.files(\"data/\", full.names = TRUE)),\n  targets::tar_target(\n    loaded,\n    readr::read_csv(data_files),\n    pattern = map(data_files)\n  )\n)\n```\n\n## Pattern Testing\n\n| Function | Purpose |\n|----------|---------|\n| `targets::tar_pattern()` | Test pattern logic without running |\n\n```r\n# Test how patterns compose\ntargets::tar_pattern(\n  cross(method, map(dataset, seed)),\n  method = 3,\n  dataset = 5,\n  seed = 5\n)\n#> Shows expected branch structure\n```\n\n## Hook Functions\n\n| Function | Purpose |\n|----------|---------|\n| `tar_hook_before()` | Insert code before target command |\n| `tar_hook_after()` | Insert code after target command |\n| `tar_hook_inner()` | Wrap command expression |\n| `tar_hook_outer()` | Wrap entire target |\n\nUseful for logging, timing, or resource management across many targets.\n",
        "skills/targets-pipelines/references/targets-utilities.md": "# `targets` utilities for package authors\n## Metaprogramming\n`tar_deparse_language()` is a wrapper around `tar_deparse_safe()` which leaves character vectors and NULL objects alone, which helps with subsequent user input validation.\n\n`tar_deparse_safe()` is a wrapper around base::deparse() with a custom set of fast default settings and guardrails to ensure the output always has length 1.\n\n`tar_tidy_eval()` applies tidy evaluation to a language object and returns another language object.\n\n`tar_tidyselect_eval()` applies tidyselect selection with some special guardrails around NULL inputs.\n\n## Conditions\nThrow custom targets-specific error conditions\n\n```R\ntargets::tar_print(...)\n\ntargets::tar_error(message, class)\n\ntargets::tar_warning(message, class)\n\ntargets::tar_message(message, class)\n```\n\n\n## Assertions\nUse `targets` assertions to check the correctness of user inputs and generate custom error conditions as needed. \n\n```R\ntargets::tar_assert_chr(x, msg = NULL)\n\ntargets::tar_assert_dbl(x, msg = NULL)\n\ntargets::tar_assert_df(x, msg = NULL)\n\ntargets::tar_assert_equal_lengths(x, msg = NULL)\n\ntargets::tar_assert_envir(x, msg = NULL)\n\ntargets::tar_assert_expr(x, msg = NULL)\n\ntargets::tar_assert_flag(x, choices, msg = NULL)\n\ntargets::tar_assert_file(x)\n\ntargets::tar_assert_finite(x, msg = NULL)\n\ntargets::tar_assert_function(x, msg = NULL)\n\ntargets::tar_assert_function_arguments(x, args, msg = NULL)\n\ntargets::tar_assert_ge(x, threshold, msg = NULL)\n\ntargets::tar_assert_identical(x, y, msg = NULL)\n\ntargets::tar_assert_in(x, choices, msg = NULL)\n\ntargets::tar_assert_not_dirs(x, msg = NULL)\n\ntargets::tar_assert_not_dir(x, msg = NULL)\n\ntargets::tar_assert_not_in(x, choices, msg = NULL)\n\ntargets::tar_assert_inherits(x, class, msg = NULL)\n\ntargets::tar_assert_int(x, msg = NULL)\n\ntargets::tar_assert_internet(msg = NULL)\n\ntargets::tar_assert_lang(x, msg = NULL)\n\ntargets::tar_assert_le(x, threshold, msg = NULL)\n\ntargets::tar_assert_list(x, msg = NULL)\n\ntargets::tar_assert_lgl(x, msg = NULL)\n\ntargets::tar_assert_name(x)\n\ntargets::tar_assert_named(x, msg = NULL)\n\ntargets::tar_assert_names(x, msg = NULL)\n\ntargets::tar_assert_nonempty(x, msg = NULL)\n\ntargets::tar_assert_null(x, msg = NULL)\n\ntargets::tar_assert_not_expr(x, msg = NULL)\n\ntargets::tar_assert_nzchar(x, msg = NULL)\n\ntargets::tar_assert_package(package, msg = NULL)\n\ntargets::tar_assert_path(path, msg = NULL)\n\ntargets::tar_assert_match(x, pattern, msg = NULL)\n\ntargets::tar_assert_nonmissing(x, msg = NULL)\n\ntargets::tar_assert_positive(x, msg = NULL)\n\ntargets::tar_assert_scalar(x, msg = NULL)\n\ntargets::tar_assert_store(store)\n\ntargets::tar_assert_targets::target(x, msg = NULL)\n\ntargets::tar_assert_targets::target_list(x)\n\ntargets::tar_assert_true(x, msg = NULL)\n\ntargets::tar_assert_unique(x, msg = NULL)\n\ntargets::tar_assert_unique_targets::targets(x)\n```\n\n### Examples\n\n```R\ntar_assert_chr(\"123\") #succeeds invisbly\ntry(tar_assert_chr(123))\n#> Error : 123 must be a character.\n```\n\n",
        "skills/targets-pipelines/targetopia-packages.md": "# Targetopia package development\n\n## Overview \n\n## Quick Reference\n\n| Task | Function/Pattern |\n|------|------------------|\n| Create target in factory | `tar_target_raw(name, command, ...)` |\n| Test in temp directory | `targets::tar_test()` |\n| Check pipeline structure | `tar_manifest()`, `tar_network()` |\n\n## Documentation\n\n### Writing examples\n\nKeep `@examples` fast and use temporary files. Use `tar_dir()` for runnable examples:\n\n```r\n#' @examples\n#' targets::tar_dir({\n#'   targets::tar_script(target_factory(example, \"data.csv\"))\n#'   targets::tar_make()\n#' })\n```\n\n## Testing\n\n### What to Test\n\n1. **Results**: Run pipeline, check outputs\n2. **Manifest**: Verify target count, commands, settings\n3. **Dependencies**: Check graph edges between targets\n\n### tar_test() Pattern\nRuns a test_that() unit test inside a temporary directory to avoid writing to the user's file space\n\n```r\ntargets::tar_test(\"factory creates correct targets\", {\n  # Runs in temp directory, resets options after\n  targets <- target_factory(test, \"data.csv\")\n  expect_length(targets, 3)\n  expect_equal(targets[[1]]$settings$name, \"test_file\")\n})\n```\n\n### Speed Tips\n\n- Use `callr_function = NULL` in `tar_make()` for faster tests (but sensitive to test environment)\n- Use `testthat::skip_on_cran()` for slow tests\n\n## See Also\n\n[utilities for package authors](references/targets-utilities.md)\n",
        "skills/targets-pipelines/topic-dynamic-branching.md": "# Dynamic Branching in targets\n\nDynamic branching creates targets at runtime based on upstream data. Use when the number or nature of branches isn't known until the pipeline runs.\n\n## When to Use Dynamic Branching\n\n- Branch count depends on data (files in directory, rows in table)\n- Scaling to hundreds of thousands of branches\n- Don't need human-readable target names\n- Runtime efficiency more important than pre-run validation\n\n## Core Concept: The pattern Argument\n\nThe `pattern` argument of `tar_target()` defines how to split upstream targets:\n\n```r\nlist(\n  tar_target(values, 1:5),\n  tar_target(\n    doubled,\n    values * 2,\n    pattern = map(values)  # One branch per element of 'values'\n  )\n)\n```\n\n**Critical:** `pattern` references **target names**, not R objects or expressions.\n\n```r\n# ❌ WRONG: Can't use external objects\nmy_vector <- 1:5\ntar_target(x, process(my_vector), pattern = map(my_vector))\n\n# ❌ WRONG: Can't use expressions\ntar_target(x, process(data), pattern = map(data$column))\n\n# ✅ CORRECT: Reference upstream target by name\nlist(\n  tar_target(params, 1:5),\n  tar_target(result, process(params), pattern = map(params))\n)\n```\n\n## Pattern Types\n\n### map() - Parallel Iteration\n\nOne branch per tuple of elements (like `purrr::map2` or `zip`):\n\n```r\nlist(\n  tar_target(x, 1:3),\n  tar_target(y, c(\"a\", \"b\", \"c\")),\n  tar_target(\n    result,\n    paste(x, y),\n    pattern = map(x, y)  # (1,\"a\"), (2,\"b\"), (3,\"c\")\n  )\n)\n```\n\n### cross() - Cartesian Product\n\nOne branch per combination:\n\n```r\nlist(\n  tar_target(x, 1:2),\n  tar_target(y, c(\"a\", \"b\")),\n  tar_target(\n    result,\n    paste(x, y),\n    pattern = cross(x, y)  # All 4 combinations\n  )\n)\n```\n\n### Subset Patterns\n\n```r\n# Specific indices\npattern = slice(x, index = c(1, 3, 5))\n\n# First/last N\npattern = head(x, n = 10)\npattern = tail(x, n = 5)\n\n# Random sample\npattern = sample(x, n = 100)\n```\n\n### Composing Patterns\n\nPatterns are composable:\n\n```r\n# Cross outer, map inner\npattern = cross(method, map(dataset, seed))\n\n# Test with tar_pattern() first\ntar_pattern(\n  cross(method, map(dataset, seed)),\n  method = 3,\n  dataset = 5,\n  seed = 5\n)\n```\n\n## Iteration Modes\n\nThe `iteration` argument controls how targets split and aggregate:\n\n### iteration = \"vector\" (default)\n\nUses `vctrs::vec_slice()` to split, `vctrs::vec_c()` to combine.\n\n```r\ntar_target(\n  result,\n  process(data),\n  pattern = map(data),\n  iteration = \"vector\"  # Default\n)\n```\n\n- Vectors → vectors\n- Data frames → data frames (row binding)\n- Type-consistent\n\n### iteration = \"list\"\n\nUses `[[` to split, `list()` to combine.\n\n```r\ntar_target(\n  plots,\n  make_plot(data),\n  pattern = map(data),\n  iteration = \"list\"  # Required for ggplot objects\n)\n```\n\nUse when:\n- Return values can't be vectorized (ggplot2, model objects)\n- Need list structure preserved\n\n### iteration = \"group\"\n\nBranch over row groups of a data frame:\n\n```r\nlist(\n  tar_target(\n    grouped_data,\n    data |>\n      group_by(category) |>\n      tar_group(),\n    iteration = \"group\"  # Required\n  ),\n  tar_target(\n    result,\n    analyze_group(grouped_data),\n    pattern = map(grouped_data)\n  )\n)\n```\n\n**Important:** Target with `iteration = \"group\"` must NOT have a `pattern` argument itself.\n\n## Row Grouping Functions\n\n`tarchetypes` provides helper functions for grouping:\n\n### tar_group_by()\n\nGroup by specific columns:\n\n```r\ntar_group_by(\n  grouped_data,\n  load_data(),\n  region, year  # Group by these columns\n)\n```\n\n### tar_group_count()\n\nFixed number of groups (variable row counts):\n\n```r\ntar_group_count(\n  batched_data,\n  load_data(),\n  count = 10  # Split into 10 groups\n)\n```\n\n### tar_group_size()\n\nFixed rows per group (variable number of groups):\n\n```r\ntar_group_size(\n  batched_data,\n  load_data(),\n  size = 1000  # ~1000 rows per group\n)\n```\n\n## Batching for Performance\n\nWith many branches (>1000), overhead accumulates. Batch work:\n\n### tar_rep() - Batched Replication\n\nFor simulation studies:\n\n```r\ntar_rep(\n  simulation,\n  {\n    data <- simulate_data()\n    fit_model(data)\n  },\n  batches = 10,   # 10 dynamic branches\n  reps = 100      # 100 reps per batch\n)\n# Total: 1000 replications in 10 targets\n```\n\nOutput includes `tar_batch`, `tar_rep`, and `tar_seed` columns.\n\n**Seed invariance:** Results are the same regardless of batch structure:\n```r\n# Same results:\ntar_rep(x, rnorm(1), batches = 100, reps = 1)\ntar_rep(x, rnorm(1), batches = 10, reps = 10)\ntar_rep(x, rnorm(1), batches = 1, reps = 100)\n```\n\n### tar_map_rep() - Static Variants + Batching\n\nCombines static branching over parameters with batched replication:\n\n```r\ntar_map_rep(\n  simulation,\n  simulate_and_fit(method, n_obs),\n  values = tibble(\n    method = syms(c(\"method_a\", \"method_b\")),\n    n_obs = c(100, 500)\n  ),\n  batches = 10,\n  reps = 50\n)\n```\n\n## Branching Over Files\n\nFiles require special handling because `format = \"file\"` treats all files as one unit:\n\n```r\n# ❌ WRONG: Can't branch over file target directly\nlist(\n  tar_target(files, list.files(\"data/\"), format = \"file\"),\n  tar_target(data, read_csv(files), pattern = map(files))\n)\n\n# ✅ CORRECT: Create one file target per path\nlist(\n  tar_target(paths, list.files(\"data/\", full.names = TRUE)),\n  tar_target(files, paths, format = \"file\", pattern = map(paths)),\n  tar_target(data, read_csv(files), pattern = map(files))\n)\n\n# ✅ SIMPLER: Use tar_files()\nlist(\n  tar_files(files, list.files(\"data/\", full.names = TRUE)),\n  tar_target(data, read_csv(files), pattern = map(files))\n)\n```\n\n## Implementation Steps\n\n1. **Create upstream target**: Define data that determines branches\n2. **Add pattern argument**: Reference upstream target(s) by name\n3. **Choose pattern type**: `map()` for parallel, `cross()` for combinations\n4. **Set iteration mode**: `\"list\"` for non-vectorizable returns\n5. **Consider batching**: Use `tar_rep()` for >1000 branches\n6. **Test pattern**: Use `tar_pattern()` before running\n7. **Validate**: Run small subset first with `head()` or `slice()`\n\n## Complete Example\n\nProcessing multiple data files with parameter grid:\n\n```r\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\n\nlist(\n  # Discover files dynamically\n  tar_target(file_paths, list.files(\"data/\", pattern = \"\\\\.csv$\", full.names = TRUE)),\n\n  # Track each file\n  tar_files(data_files, file_paths),\n\n  # Load each file\n  tar_target(\n    raw_data,\n    read_csv(data_files),\n    pattern = map(data_files)\n  ),\n\n  # Parameters for analysis\n  tar_target(thresholds, c(0.01, 0.05, 0.1)),\n\n  # Cross files with thresholds\n  tar_target(\n    analysis,\n    analyze(raw_data, threshold = thresholds),\n    pattern = cross(raw_data, thresholds)\n  ),\n\n  # Aggregate results\n  tar_target(\n    summary,\n    analysis |>\n      group_by(threshold) |>\n      summarize(mean_result = mean(result))\n  )\n)\n```\n\n## Provenance and Debugging\n\nInclude metadata in branch outputs for traceability:\n\n```r\ntar_target(\n  result,\n  {\n    out <- process(data, param)\n    out$param_value <- param  # Track which param created this\n    out$data_source <- attr(data, \"source\")\n    out\n  },\n  pattern = cross(data, param)\n)\n```\n\nWhen a branch fails, you can identify which inputs caused it.\n\n## Reading Branch Results\n\n```r\n# All branches combined (uses iteration mode)\ntar_read(result)\n\n# Specific branch by index\ntar_read(result, branches = 1)\ntar_read(result, branches = c(2, 5, 7))\n\n# Load into environment\ntar_load(result)\ntar_load(result, branches = 1:3)\n```\n\n## Validation\n\n```r\n# Test pattern logic with mock sizes\ntar_pattern(\n  cross(x, map(y, z)),\n  x = 3,\n  y = 5,\n  z = 5\n)\n\n# Run subset first\nlist(\n  tar_target(params, 1:1000),\n  tar_target(\n    result,\n    slow_computation(params),\n    pattern = head(params, n = 10)  # Test with 10 first\n  )\n)\n```\n",
        "skills/targets-pipelines/topic-hybrid-branching.md": "# Dynamic Within Static Branching\n\nHybrid branching combines static branching (for known variants like methods or scenarios) with dynamic branching (for runtime-determined iterations like replications or data-driven splits).\n\n## When to Use Hybrid Patterns\n\n- **Simulation studies**: Compare methods across many replications\n- **Bootstrap analysis**: Multiple methods, each with bootstrap samples\n- **Cross-validation**: Known fold structure with data-dependent splits\n- **Parameter sweeps**: Known parameter grid with batched execution\n\nThe key insight: static branching gives you friendly target names for high-level variants, while dynamic branching handles the scale of replications efficiently.\n\n## tar_map_rep(): The Primary Hybrid Function\n\n`tarchetypes::tar_map_rep()` combines `tar_map()` with `tar_rep()`:\n\n```r\nvalues <- tibble::tibble(\n  method = rlang::syms(c(\"method_glm\", \"method_rf\", \"method_xgb\")),\n  method_name = c(\"glm\", \"rf\", \"xgb\")\n)\n\ntarchetypes::tar_map_rep(\n  name = simulation,\n  command = {\n    data <- simulate_data(n = 500)\n    method(data)\n  },\n  values = values,\n  names = \"method_name\",\n  batches = 10,\n  reps = 100\n)\n```\n\n**Result:** Creates `simulation_glm`, `simulation_rf`, `simulation_xgb`, each with 10 dynamic branches containing 100 replications (1000 total reps per method).\n\n### Key Arguments\n\n| Argument | Purpose |\n|----------|---------|\n| `name` | Base name for generated targets |\n| `command` | Expression to execute (can reference columns from `values`) |\n| `values` | Data frame of static variants (same as `tar_map()`) |\n| `names` | Column(s) for target name suffixes |\n| `batches` | Number of dynamic branches |\n| `reps` | Replications per batch |\n| `rep_workers` | Max parallel workers per target (default 1) |\n\n### Output Structure\n\nEach batch includes metadata columns:\n\n```r\n# After tar_make(), reading a result:\ntargets::tar_read(simulation_glm)\n#> # A tibble: 1,000 x 5\n#>    tar_batch tar_rep tar_seed   estimate std_error\n#>        <int>   <int>    <int>      <dbl>     <dbl>\n#>  1         1       1 12345678      0.502    0.0312\n#>  2         1       2 23456789      0.498    0.0298\n#>  ...\n```\n\n- `tar_batch`: Which dynamic branch (1 to `batches`)\n- `tar_rep`: Which replication within batch (1 to `reps`)\n- `tar_seed`: Seed used for reproducibility\n\n### Seed Invariance\n\nResults are deterministic regardless of batch structure:\n\n```r\n# These produce identical results (same seeds):\ntarchetypes::tar_rep(x, rnorm(1), batches = 100, reps = 1)\ntarchetypes::tar_rep(x, rnorm(1), batches = 10, reps = 10)\ntarchetypes::tar_rep(x, rnorm(1), batches = 1, reps = 100)\n```\n\nThis allows tuning batch size for performance without affecting reproducibility.\n\n## Manual Hybrid Construction\n\nFor cases `tar_map_rep()` doesn't cover, construct manually with `tar_map()` + `pattern`:\n\n```r\n# Static branching over methods\nmethods <- tibble::tibble(\n  method = rlang::syms(c(\"method_a\", \"method_b\")),\n  method_name = c(\"a\", \"b\")\n)\n\nlist(\n  # Upstream target that determines dynamic branch count\n  targets::tar_target(seeds, 1:100),\n\n  # Static map with dynamic pattern inside\n  tarchetypes::tar_map(\n    values = methods,\n    names = \"method_name\",\n    targets::tar_target(\n      result,\n      {\n        set.seed(seeds)\n        method(simulate_data())\n      },\n      pattern = map(seeds)\n    )\n  )\n)\n```\n\n**Result:** Creates `result_a` and `result_b`, each with 100 dynamic branches.\n\n### When to Use Manual Construction\n\n- Need custom aggregation at the dynamic level\n- Dynamic branch count varies by static variant\n- Complex dependencies between static variants\n\n## Aggregating Hybrid Results\n\n### Within Static Variants\n\nDynamic branches auto-aggregate based on `iteration` mode:\n\n```r\n# All 1000 reps combined (iteration = \"vector\" default)\ntargets::tar_read(simulation_glm)\n\n# Specific batches\ntargets::tar_read(simulation_glm, branches = 1:3)\n```\n\n### Across Static Variants\n\nCombine static variants with explicit aggregation:\n\n```r\nlist(\n  tarchetypes::tar_map_rep(\n    name = sim,\n    command = method(simulate_data()),\n    values = values,\n    names = \"method_name\",\n    batches = 10,\n    reps = 100\n  ),\n\n  targets::tar_target(\n    comparison,\n    {\n      results <- list(\n        glm = targets::tar_read(sim_glm),\n        rf = targets::tar_read(sim_rf),\n        xgb = targets::tar_read(sim_xgb)\n      )\n      dplyr::bind_rows(results, .id = \"method\") |>\n        dplyr::group_by(method) |>\n        dplyr::summarize(\n          mean_est = mean(estimate),\n          bias = mean(estimate - true_value),\n          rmse = sqrt(mean((estimate - true_value)^2)),\n          .groups = \"drop\"\n        )\n    }\n  )\n)\n```\n\n### Programmatic Aggregation\n\nWhen method names are dynamic:\n\n```r\ntargets::tar_target(\n  comparison,\n  {\n    method_names <- c(\"glm\", \"rf\", \"xgb\")\n    purrr::map_dfr(method_names, function(m) {\n      targets::tar_read_raw(paste0(\"sim_\", m)) |>\n        dplyr::mutate(method = m)\n    }) |>\n      dplyr::group_by(method) |>\n      dplyr::summarize(\n        mean_est = mean(estimate),\n        coverage = mean(ci_lower <= true_value & true_value <= ci_upper),\n        .groups = \"drop\"\n      )\n  }\n)\n```\n\n## Complete Example: Method Comparison Study\n\nCompare three estimation methods across 1000 bootstrap replications:\n\n```r\n# _targets.R\nvalues <- tibble::tibble(\n  estimator = rlang::syms(c(\"est_mle\", \"est_bayes\", \"est_robust\")),\n  estimator_name = c(\"mle\", \"bayes\", \"robust\")\n)\n\nlist(\n  # Load real data once\n  targets::tar_target(\n    observed_data,\n    load_study_data(\"data/observations.csv\")\n  ),\n\n  # Bootstrap each estimator\n  tarchetypes::tar_map_rep(\n    name = bootstrap,\n    command = {\n      boot_sample <- observed_data[sample(nrow(observed_data), replace = TRUE), ]\n      estimator(boot_sample)\n    },\n    values = values,\n    names = \"estimator_name\",\n    batches = 20,\n    reps = 50\n  ),\n\n  # Compare estimators\n  targets::tar_target(\n    comparison,\n    {\n      results <- dplyr::bind_rows(\n        mle = targets::tar_read(bootstrap_mle),\n        bayes = targets::tar_read(bootstrap_bayes),\n        robust = targets::tar_read(bootstrap_robust),\n        .id = \"estimator\"\n      )\n\n      results |>\n        dplyr::group_by(estimator) |>\n        dplyr::summarize(\n          estimate = mean(value),\n          se = sd(value),\n          ci_lower = quantile(value, 0.025),\n          ci_upper = quantile(value, 0.975),\n          .groups = \"drop\"\n        )\n    }\n  ),\n\n  # Visualization\n  targets::tar_target(\n    plot,\n    {\n      ggplot2::ggplot(\n        dplyr::bind_rows(\n          mle = targets::tar_read(bootstrap_mle),\n          bayes = targets::tar_read(bootstrap_bayes),\n          robust = targets::tar_read(bootstrap_robust),\n          .id = \"estimator\"\n        ),\n        ggplot2::aes(x = value, fill = estimator)\n      ) +\n        ggplot2::geom_density(alpha = 0.5) +\n        ggplot2::labs(title = \"Bootstrap Distributions by Estimator\")\n    },\n    packages = \"ggplot2\"\n  )\n)\n```\n\n## Performance Considerations\n\n### Batch Size Tuning\n\n| Scenario | Recommendation |\n|----------|----------------|\n| Fast iterations (<1s each) | Larger batches (50-100 reps) |\n| Slow iterations (>10s each) | Smaller batches (5-10 reps) |\n| Memory-intensive | Smaller batches to limit memory |\n| Distributed computing | Match batches to worker count |\n\n### Memory Management\n\nFor memory-intensive simulations:\n\n```r\ntarchetypes::tar_map_rep(\n  name = sim,\n  command = run_simulation(),\n  values = values,\n  batches = 100,\n  reps = 10,\n  garbage_collection = TRUE,\n  memory = \"transient\"\n)\n```\n\n## Debugging Hybrid Pipelines\n\n### Validate Structure First\n\n```r\n# Check static structure\ntargets::tar_manifest() |>\n  dplyr::filter(stringr::str_detect(name, \"^sim_\"))\n\n# Test pattern composition\ntargets::tar_pattern(\n  map(seeds),\n  seeds = 100\n)\n```\n\n### Inspect Specific Branches\n\n```r\n# Read specific batch\ntargets::tar_read(sim_glm, branches = 1)\n\n# Check batch metadata\ntargets::tar_read(sim_glm) |>\n  dplyr::count(tar_batch)\n```\n\n### Common Issues\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| Missing `tar_batch` column | Not using `tar_map_rep()` | Use `tar_map_rep()` or add manually |\n| Non-deterministic results | Missing seed management | Use `tar_rep()` family for automatic seeding |\n| Memory exhaustion | Batches too large | Reduce `reps`, increase `batches` |\n| Slow aggregation | Reading all branches | Use `branches` argument for subset |\n\n## See Also\n\n- [topic-static-branching.md](topic-static-branching.md) - Static branching details\n- [topic-dynamic-branching.md](topic-dynamic-branching.md) - Dynamic branching details\n- [references/tarchetypes-functions.md](references/tarchetypes-functions.md) - Function reference\n",
        "skills/targets-pipelines/topic-static-branching.md": "# Static Branching in targets\n\nStatic branching defines all targets before `tar_make()` runs. Use when you know your iterations at define-time: methods to compare, models to fit, datasets to analyze.\n\n## When to Use Static Branching\n\n- Parameter values known when writing `_targets.R`\n- Need friendly, readable target names\n- Want to validate with `tar_manifest()` before running\n- Fewer than ~1000 combinations\n\n## Core Functions\n\n### tar_map() - Create Target Variants\n\nCreates copies of target definitions, substituting values from a data frame:\n\n```r\nlibrary(tarchetypes)\nlibrary(tibble)\nlibrary(rlang)\n\nvalues <- tibble(\n  method = syms(c(\"analyze_bayesian\", \"analyze_frequentist\")),\n  prior = c(\"weak\", \"strong\")\n)\n\ntar_map(\n  values = values,\n  names = \"prior\",  # Column(s) for target name suffixes\n  tar_target(result, method(data, prior_type = prior)),\n  tar_target(summary, summarize_result(result))\n)\n```\n\n**Result:** Creates `result_weak`, `result_strong`, `summary_weak`, `summary_strong`.\n\n#### Key Arguments\n\n| Argument | Purpose | Example |\n|----------|---------|---------|\n| `values` | Data frame of parameters | `tibble(x = 1:3)` |\n| `names` | Columns for target name suffixes | `names = c(\"method\", \"dataset\")` |\n| `unlist` | Return nested list (for `tar_combine()`) | `unlist = FALSE` |\n| `delimiter` | Separator in target names | `delimiter = \"_\"` (default) |\n\n#### Working with Function Names\n\n**Critical:** Function names must be symbols, not strings:\n\n```r\n# ❌ WRONG: Strings can't be called\nvalues <- tibble(method = c(\"method_a\", \"method_b\"))\n\n# ✅ CORRECT: Use rlang::syms()\nvalues <- tibble(method = syms(c(\"method_a\", \"method_b\")))\n\n# ✅ ALSO CORRECT: Use rlang::sym() for single values\nvalues <- tibble(method = list(sym(\"method_a\"), sym(\"method_b\")))\n```\n\n#### All Combinations with expand_grid\n\n```r\nlibrary(tidyr)\n\nvalues <- expand_grid(\n  method = syms(c(\"method_a\", \"method_b\")),\n  dataset = c(\"train\", \"test\"),\n  seed = 1:3\n)\n# Creates 2 × 2 × 3 = 12 rows\n```\n\n### tar_combine() - Aggregate Results\n\nCombines results from multiple targets into one:\n\n```r\n# Simple case: combine two explicit targets\ntarget1 <- tar_target(head_data, head(mtcars))\ntarget2 <- tar_target(tail_data, tail(mtcars))\n\ntar_combine(\n  combined,\n  target1, target2,\n  command = dplyr::bind_rows(!!!.x)\n)\n```\n\n#### With tar_map()\n\n```r\n# Must use unlist = FALSE for selective combining\nmapped <- tar_map(\n  unlist = FALSE,\n  values = tibble(method = syms(c(\"method_a\", \"method_b\"))),\n  tar_target(analysis, method(data)),\n  tar_target(summary, summarize(analysis))\n)\n\n# Combine only summaries, not analyses\ntar_combine(\n  all_summaries,\n  mapped[[\"summary\"]],\n  command = dplyr::bind_rows(!!!.x, .id = \"method\")\n)\n```\n\n#### The `!!!.x` Syntax\n\n`!!!.x` is rlang's unquote-splice operator. It expands `.x` (the list of upstream target results) into separate arguments:\n\n```r\n# This command:\ncommand = dplyr::bind_rows(!!!.x)\n\n# Becomes (with 3 upstream targets):\ndplyr::bind_rows(result_a, result_b, result_c)\n```\n\nCustom aggregation:\n\n```r\n# Mean across all results\ncommand = mean(c(!!!.x))\n\n# Custom function\ncommand = my_combine_function(!!!.x, weights = c(0.5, 0.3, 0.2))\n```\n\n### tar_eval() and tar_sub() - Custom Metaprogramming\n\nWhen `tar_map()` isn't flexible enough:\n\n```r\nlibrary(rlang)\n\n# tar_sub: create expressions with substitution\ntar_sub(\n  tar_target(symbol, get_data(string)),\n  values = list(\n    string = c(\"data_a\", \"data_b\"),\n    symbol = syms(c(\"data_a\", \"data_b\"))\n  )\n)\n\n# tar_eval: create AND evaluate (returns target objects)\ntar_eval(\n  tar_target(symbol, get_data(string)),\n  values = list(\n    string = c(\"data_a\", \"data_b\"),\n    symbol = syms(c(\"data_a\", \"data_b\"))\n  )\n)\n```\n\n## Combining Static + Dynamic\n\nStatic branching for outer layer (methods), dynamic for inner (seeds/reps):\n\n```r\nrandom_seeds <- tar_target(seeds, 1:100)\n\nmapped <- tar_map(\n  values = tibble(method = syms(c(\"method_a\", \"method_b\"))),\n  tar_target(\n    analysis,\n    method(data, seed = seeds),\n    pattern = map(seeds)  # Dynamic branching over seeds\n  ),\n  tar_target(\n    summary,\n    summarize(analysis),\n    pattern = map(analysis)\n  )\n)\n\nlist(random_seeds, mapped)\n```\n\n## Implementation Steps\n\n1. **Identify what varies**: List parameters that differ across targets\n2. **Build values data frame**: Use `tibble()`, convert functions with `syms()`\n3. **Write base targets**: Create `tar_target()` calls using parameter names\n4. **Wrap in tar_map()**: Pass values and targets\n5. **Add tar_combine() if needed**: Use `unlist = FALSE` for selective combining\n6. **Validate**: Run `tar_manifest()` to check generated targets\n\n## Complete Example\n\n```r\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(rlang)\n\n# Define parameter grid\nmethods <- c(\"glm\", \"rf\", \"xgb\")\ndatasets <- c(\"train\", \"validation\", \"test\")\n\nvalues <- expand_grid(\n  method = syms(paste0(\"fit_\", methods)),\n  method_name = methods,\n  dataset = datasets\n)\n\nlist(\n  # Data loading (static over datasets)\n  tar_map(\n    values = tibble(dataset = datasets),\n    tar_target(data, load_data(dataset))\n  ),\n\n  # Model fitting (static over methods × datasets)\n  tar_map(\n    unlist = FALSE,\n    values = values,\n    names = c(\"method_name\", \"dataset\"),\n    tar_target(\n      model,\n      method(get(paste0(\"data_\", dataset)))\n    ),\n    tar_target(\n      predictions,\n      predict(model, newdata = get(paste0(\"data_\", dataset)))\n    ),\n    tar_target(\n      metrics,\n      compute_metrics(predictions, get(paste0(\"data_\", dataset)))\n    )\n  ),\n\n  # Combine all metrics\n  tar_combine(\n    all_metrics,\n    mapped[[\"metrics\"]],\n    command = bind_rows(!!!.x, .id = \"model_dataset\") |>\n      separate(model_dataset, c(\"method\", \"dataset\"), sep = \"_\")\n  )\n)\n```\n\n## Limitations\n\n- Complex nested objects may not substitute correctly (use `quote()`)\n- Scales poorly beyond ~1000 targets (use dynamic branching)\n- `tar_visnetwork()` becomes slow with many targets\n\n## Validation\n\n```r\n# Check target definitions\ntar_manifest()\n\n# Check specific fields\ntar_manifest(fields = c(\"name\", \"command\"))\n\n# Visualize (may be slow with many targets)\ntar_visnetwork(targets_only = TRUE)\n```\n",
        "skills/targets-pipelines/topic-target-factories.md": "# Writing Custom Target Factory Functions\n\nA target factory is a function that accepts simple inputs and returns a list of pre-configured target objects. Use factories to encapsulate reusable multi-target patterns with domain-specific defaults.\n\n## When to Write a Factory\n\n- **Repeated pattern**: Same multi-step workflow appears across projects\n- **Domain encapsulation**: Hide complexity from pipeline authors\n- **Consistent configuration**: Enforce format, storage, and deployment settings\n- **Package distribution**: Share patterns via R packages\n\n## Core Pattern\n\n```r\n#' Analyze a dataset with preprocessing and modeling\n#'\n#' @param name Symbol. Base name for generated targets.\n#' @param file Character. Path to input data file.\n#' @param model_fn Symbol. Modeling function to apply.\n#' @export\nanalyze_dataset <- function(name, file, model_fn = fit_default) {\n  # 1. Convert symbol arguments to strings\n name <- targets::tar_deparse_language(substitute(name))\n  model_fn <- targets::tar_deparse_language(substitute(model_fn))\n\n  # 2. Build derived target names\n  name_file <- paste0(name, \"_file\")\n  name_data <- paste0(name, \"_data\")\n  name_model <- paste0(name, \"_model\")\n\n  # 3. Create symbols for cross-target references\n  sym_data <- as.symbol(name_data)\n  sym_model_fn <- as.symbol(model_fn)\n\n  # 4. Build commands with substitute()\n  command_data <- substitute(\n    preprocess(readr::read_csv(file)),\n    env = list(file = file)\n  )\n  command_model <- substitute(\n    model_fn(data),\n    env = list(model_fn = sym_model_fn, data = sym_data)\n  )\n\n  # 5. Return list of target objects\n  list(\n    targets::tar_target_raw(\n      name = name_file,\n      command = quote(file),\n      format = \"file\",\n      deployment = \"main\"\n    ),\n    targets::tar_target_raw(\n      name = name_data,\n      command = command_data,\n      format = \"qs\"\n    ),\n    targets::tar_target_raw(\n      name = name_model,\n      command = command_model,\n      format = \"qs\"\n    )\n  )\n}\n```\n\n**Usage in `_targets.R`:**\n\n```r\nlist(\n  analyze_dataset(sales, \"data/sales.csv\", model_fn = fit_xgboost),\n  analyze_dataset(customers, \"data/customers.csv\")\n)\n```\n\n## tar_target_raw() vs tar_target()\n\n| Aspect | `tar_target()` | `tar_target_raw()` |\n|--------|----------------|-------------------|\n| Audience | End users | Factory authors |\n| Name | Symbol: `tar_target(foo, ...)` | String: `tar_target_raw(\"foo\", ...)` |\n| Command | Expression: `tar_target(x, f(y))` | Quoted: `tar_target_raw(\"x\", quote(f(y)))` |\n| Pattern | Expression: `pattern = map(x)` | Quoted: `pattern = quote(map(x))` |\n\n**Key rule:** Use `tar_target_raw()` in factories because you're programmatically constructing names and commands.\n\n## Metaprogramming Essentials\n\n### Capturing User Input\n\n```r\n# User writes: my_factory(analysis, data_file = \"data.csv\")\n# Factory sees: substitute(name) -> quote(analysis)\n\nmy_factory <- function(name, data_file) {\n  # Convert symbol to string\n  name_str <- targets::tar_deparse_language(substitute(name))\n  # Result: \"analysis\"\n}\n```\n\n### Base R Metaprogramming\n\n| Function | Purpose | Example |\n|----------|---------|---------|\n| `quote(expr)` | Capture expression literally | `quote(f(x + y))` → `f(x + y)` |\n| `substitute(expr, env)` | Replace symbols in expression | `substitute(f(x), list(x = quote(y)))` → `f(y)` |\n| `deparse(expr)` | Expression to string | `deparse(quote(foo))` → `\"foo\"` |\n| `as.symbol(str)` | String to symbol | `as.symbol(\"foo\")` → `foo` |\n| `bquote(expr)` | Quote with `.()` splicing | `bquote(f(.(x)))` with `x <- quote(y)` → `f(y)` |\n\n### Building Commands with substitute()\n\n```r\n# Pattern: substitute(template, env = list(placeholder = value))\n\n# Simple substitution\nsubstitute(analyze(data), env = list(data = as.symbol(\"my_data\")))\n#> analyze(my_data)\n\n# Multiple substitutions\nsubstitute(\n  model_fn(data, params),\n  env = list(\n    model_fn = as.symbol(\"fit_glm\"),\n    data = as.symbol(\"clean_data\"),\n    params = quote(list(family = \"binomial\"))\n  )\n)\n#> fit_glm(clean_data, list(family = \"binomial\"))\n```\n\n### Building Commands with bquote()\n\n`bquote()` uses `.(expr)` for splicing (more readable for complex expressions):\n\n```r\ndata_sym <- as.symbol(\"my_data\")\nmodel_sym <- as.symbol(\"fit_glm\")\n\nbquote(.(model_sym)(.(data_sym), family = \"binomial\"))\n#> fit_glm(my_data, family = \"binomial\")\n```\n\n## Input Validation\n\nUse `targets::tar_assert_*()` functions for consistent error messages:\n```r\nmy_factory <- function(name, file, n_folds = 5) {\n  name <- targets::tar_deparse_language(substitute(name))\n\n  # Validate inputs\n  targets::tar_assert_chr(name)\n  targets::tar_assert_nzchar(name)\n  targets::tar_assert_scalar(file)\n  targets::tar_assert_chr(file)\n  targets::tar_assert_scalar(n_folds)\n  targets::tar_assert_dbl(n_folds)\n  targets::tar_assert_ge(n_folds, 2)\n\n  # ... build targets\n}\n```\n\n### Common Assertions\n\n| Function | Checks |\n|----------|--------|\n| `tar_assert_chr(x)` | Character type |\n| `tar_assert_nzchar(x)` | Non-empty string |\n| `tar_assert_scalar(x)` | Length 1 |\n| `tar_assert_dbl(x)` | Numeric type |\n| `tar_assert_lgl(x)` | Logical type |\n| `tar_assert_ge(x, min)` | x >= min |\n| `tar_assert_le(x, max)` | x <= max |\n| `tar_assert_in(x, choices)` | x in choices |\n| `tar_assert_path(x)` | File exists |\n\n## Pre-configuring Settings\n\nApply domain knowledge to set sensible defaults:\n\n```r\n#' Target factory for Bayesian models\n#'\n#' Pre-configures settings appropriate for Stan/brms models.\nfit_bayesian_model <- function(name, formula, data_target) {\n  name <- targets::tar_deparse_language(substitute(name))\n  data_target <- targets::tar_deparse_language(substitute(data_target))\n\n  command <- substitute(\n    brms::brm(formula, data = data, chains = 4, cores = 4),\n    env = list(formula = formula, data = as.symbol(data_target))\n  )\n\n  targets::tar_target_raw(\n    name = name,\n    command = command,\n    format = \"qs\",\n    memory = \"transient\",\n    garbage_collection = TRUE,\n    deployment = \"worker\",\n    resources = targets::tar_resources(\n      crew = targets::tar_resources_crew(\n        controller = \"high_memory\"\n      )\n    )\n  )\n}\n```\n\n## Forwarding tar_target Arguments\n\nAllow users to override defaults:\n\n```r\nmy_factory <- function(\n  name,\n  command,\n  format = targets::tar_option_get(\"format\"),\n  packages = targets::tar_option_get(\"packages\"),\n  ...\n) {\n  name <- targets::tar_deparse_language(substitute(name))\n  command <- substitute(command)\n\n  targets::tar_target_raw(\n    name = name,\n    command = command,\n    format = format,\n    packages = packages,\n    ...\n  )\n}\n```\n\n## Factories with Branching\n\n### Static Branching in Factory\n\n```r\n#' Create cross-validated model targets\nfit_cv_model <- function(name, data_target, n_folds = 5) {\n  name <- targets::tar_deparse_language(substitute(name))\n  data_target <- targets::tar_deparse_language(substitute(data_target))\n\n  values <- tibble::tibble(\n    fold = seq_len(n_folds),\n    fold_name = sprintf(\"fold%d\", seq_len(n_folds))\n  )\n\n  # Use tarchetypes for static branching within factory\n  tarchetypes::tar_map(\n    values = values,\n    names = \"fold_name\",\n    targets::tar_target_raw(\n      name = name,\n      command = substitute(\n        fit_fold(data, fold_id = fold),\n        env = list(\n          data = as.symbol(data_target),\n          fold = fold\n        )\n      )\n    )\n  )\n}\n```\n\n### Dynamic Branching in Factory\n\n```r\n#' Create target with dynamic branching over upstream\nprocess_branches <- function(name, upstream_target) {\n  name <- targets::tar_deparse_language(substitute(name))\n  upstream <- targets::tar_deparse_language(substitute(upstream_target))\n\n  command <- substitute(\n    process_one(upstream),\n    env = list(upstream = as.symbol(upstream))\n  )\n\n  pattern <- substitute(\n    map(upstream),\n    env = list(upstream = as.symbol(upstream))\n  )\n\n  targets::tar_target_raw(\n    name = name,\n    command = command,\n    pattern = pattern\n  )\n}\n```\n\n## Testing Factories\n\n### Using tar_dir() for Isolation\n\n```r\n# In tests or examples\ntargets::tar_dir({\n  targets::tar_script({\n    list(\n      my_factory(test_analysis, \"data.csv\")\n    )\n  })\n\n  # Verify manifest\n  manifest <- targets::tar_manifest()\n  testthat::expect_true(\"test_analysis_data\" %in% manifest$name)\n  testthat::expect_true(\"test_analysis_model\" %in% manifest$name)\n})\n```\n\n### Using tar_test() Pattern\n\n```r\ntargets::tar_test(\"my_factory creates expected targets\", {\n  targets::tar_script({\n    list(\n      my_factory(foo, \"input.csv\")\n    )\n  })\n\n  manifest <- targets::tar_manifest()\n  expect_equal(nrow(manifest), 3)\n  expect_true(all(c(\"foo_file\", \"foo_data\", \"foo_model\") %in% manifest$name))\n})\n```\n\n## Documenting Factories\n\n```r\n#' Analyze survey data with standard preprocessing\n#'\n#' Creates targets for loading, cleaning, and analyzing survey data.\n#'\n#' @param name Symbol. Base name for generated targets. Creates:\n#'   - `{name}_raw`: Raw data from file\n#'   - `{name}_clean`: Cleaned data\n#'   - `{name}_summary`: Analysis summary\n#' @param file Character. Path to survey CSV file.\n#' @param weights Character. Column name for survey weights (optional).\n#'\n#' @return List of target objects to include in `_targets.R`.\n#'\n#' @examples\n#' # In _targets.R:\n#' list(\n#'   analyze_survey(customer_survey, \"data/customers.csv\"),\n#'   analyze_survey(employee_survey, \"data/employees.csv\", weights = \"sample_weight\")\n#' )\n#'\n#' @export\nanalyze_survey <- function(name, file, weights = NULL) {\n  # ... implementation\n}\n```\n\n## Complete Example: Multi-Step Analysis Factory\n\n```r\n#' Create complete analysis pipeline for a dataset\n#'\n#' @param name Base name (symbol)\n#' @param file Data file path\n#' @param model_fn Modeling function (symbol)\n#' @param preprocess_fn Preprocessing function (symbol, default: identity)\n#' @export\nfull_analysis <- function(\n  name,\n  file,\n  model_fn,\n  preprocess_fn = identity\n) {\n  # Parse inputs\n  name <- targets::tar_deparse_language(substitute(name))\n  model_fn_str <- targets::tar_deparse_language(substitute(model_fn))\n  preprocess_fn_str <- targets::tar_deparse_language(substitute(preprocess_fn))\n\n  # Validate\n  targets::tar_assert_chr(name)\n  targets::tar_assert_nzchar(name)\n  targets::tar_assert_chr(file)\n  targets::tar_assert_scalar(file)\n\n  # Build target names\n  name_file <- paste0(name, \"_file\")\n  name_raw <- paste0(name, \"_raw\")\n  name_clean <- paste0(name, \"_clean\")\n  name_model <- paste0(name, \"_model\")\n  name_diagnostics <- paste0(name, \"_diagnostics\")\n\n  # Build symbols\n  sym_raw <- as.symbol(name_raw)\n  sym_clean <- as.symbol(name_clean)\n  sym_model <- as.symbol(name_model)\n  sym_model_fn <- as.symbol(model_fn_str)\n  sym_preprocess <- as.symbol(preprocess_fn_str)\n\n  # Build commands\n  cmd_raw <- bquote(readr::read_csv(.(file)))\n  cmd_clean <- bquote(.(sym_preprocess)(.(sym_raw)))\n  cmd_model <- bquote(.(sym_model_fn)(.(sym_clean)))\n  cmd_diagnostics <- bquote(compute_diagnostics(.(sym_model)))\n\n  list(\n    targets::tar_target_raw(\n      name_file,\n      command = bquote(.(file)),\n      format = \"file\"\n    ),\n    targets::tar_target_raw(\n      name_raw,\n      command = cmd_raw,\n      format = \"qs\"\n    ),\n    targets::tar_target_raw(\n      name_clean,\n      command = cmd_clean,\n      format = \"qs\"\n    ),\n    targets::tar_target_raw(\n      name_model,\n      command = cmd_model,\n      format = \"qs\"\n    ),\n    targets::tar_target_raw(\n      name_diagnostics,\n      command = cmd_diagnostics\n    )\n  )\n}\n```\n\n## See Also\n\n- **r-metaprogramming**: Advanced expression manipulation\n- [targetopia-packages.md](targetopia-packages.md): Package development workflows\n- [references/targets-utilities.md](references/targets-utilities.md): Utility functions for factories\n- [templates/target-factory.R](templates/target-factory.R): Starter template\n",
        "skills/tidy-evaluation/SKILL.md": "---\nname: tidy-evaluation\ndescription: >\n  Use when programming with tidyverse data-masked functions (dplyr, ggplot2,\n  tidyr) and need to pass column references through functions. Covers:\n  forwarding patterns with {{ and ..., names patterns with .data/.env pronouns,\n  bridge patterns with across()/all_of(), double evaluation and ambiguity\n  pitfalls. Does NOT cover: expression mechanics (r-metaprogramming), error\n  handling (rlang-conditions), function design (designing-tidy-r-functions).\ndependencies: R>=4.3, rlang>=1.1.3\n---\n\n# Tidy Evaluation Programming Patterns\n\nData masking lets you refer to data frame columns as if they were regular objects. Programming with data-masked functions requires special patterns to pass column references through your functions.\n\n## Quick Reference\n\n| Goal | Pattern |\n|------|---------|\n| Forward single argument | `{{ var }}` |\n| Forward `...` to data-mask | `...` directly |\n| Forward `...` to tidy-select (single arg) | `c(...)` |\n| Use column name from string | `.data[[var]]` |\n| Use column names from vector | `across(all_of(vars))` |\n| Disambiguate env-variable | `.env$x` |\n| Disambiguate data-variable | `.data$x` |\n| Bridge selection to data-mask | `across({{ var }})` |\n| Bridge names to data-mask | `across(all_of(vars))` |\n| Bridge data-mask to selection | `transmute()` then `all_of()` |\n| Prevent double evaluation | Assign to column first |\n\n## What is Data Masking?\n\nData masking inserts a data frame at the bottom of the environment chain, giving columns precedence over user-defined variables:\n\n```r\n# Without masking: must use $ notation\nmean(mtcars$cyl + mtcars$am)\n\n# With masking: columns are directly accessible\nwith(mtcars, mean(cyl + am))\ndplyr::summarise(mtcars, mean(cyl + am))\n```\n\n### Why Injection is Needed\n\nData-masking functions defuse their arguments. When you wrap them, you must inject the user's expression:\n\n```r\n# Without injection: summarise sees literal \"var1 + var2\"\nmy_mean <- function(data, var1, var2) {\n  dplyr::summarise(data, mean(var1 + var2))  # Error!\n}\n\n# With injection: summarise sees \"cyl + am\"\nmy_mean <- function(data, var1, var2) {\n  dplyr::summarise(data, mean({{ var1 }} + {{ var2 }}))\n}\n```\n\n## Argument Behaviors\n\n| Behavior | Example Functions | Key Features |\n|----------|-------------------|--------------|\n| Data-masked | `mutate()`, `summarise()`, `filter()` | Column refs, `.data`/`.env`, `{{`, `!!` |\n| Tidy-select | `select()`, `pivot_longer(cols=)` | Helpers (`starts_with`), `c()`, `:`, `{{` |\n| Dynamic dots | `list2()`, `tibble()` | `!!!` splice, `{name}` interpolation |\n\n### Documenting Argument Behaviors\n\n```r\n#' @param var <[`data-masked`][dplyr::dplyr_data_masking]> Column to summarize.\n#' @param cols <[`tidy-select`][dplyr::dplyr_tidy_select]> Columns to pivot.\n#' @param ... <[`dynamic-dots`][rlang::dyn-dots]> Name-value pairs.\n```\n\n## Forwarding Patterns\n\n### Single Argument with `{{}}`\n\nThe embrace operator forwards an argument, inheriting behavior from the wrapped function:\n\n```r\n# Forwarding to data-masked context\nmy_summarise <- function(data, var) {\n  data |> dplyr::summarise({{ var }})\n}\nmtcars |> my_summarise(mean(cyl))\n\n# Forwarding to tidy-select context\nmy_pivot <- function(data, cols) {\n  data |> tidyr::pivot_longer(cols = {{ cols }})\n}\nmtcars |> my_pivot(starts_with(\"c\"))\n```\n\n### Multiple Arguments with `...`\n\nPass `...` directly to data-masked functions:\n\n```r\nmy_group_by <- function(.data, ...) {\n  .data |> dplyr::group_by(...)\n}\nmtcars |> my_group_by(cyl, am)\n```\n\nFor tidy-select functions taking a single argument, wrap in `c()`:\n\n```r\nmy_pivot <- function(.data, ...) {\n  .data |> tidyr::pivot_longer(c(...))\n}\nmtcars |> my_pivot(cyl, am, vs)\n```\n\n## Names Patterns\n\nUse strings or character vectors instead of expressions. Your function becomes \"regular\" with no data-masking complications.\n\n### `.data[[var]]` for Single Column\n\n```r\nmy_mean <- function(data, var) {\n  data |> dplyr::summarise(mean = mean(.data[[var]]))\n}\nmy_mean(mtcars, \"cyl\")\n\n# No masking surprises\nam <- \"cyl\"\nmy_mean(mtcars, am)  # Uses \"cyl\", not masked\n```\n\n### `all_of()` for Character Vectors\n\n```r\nvars <- c(\"cyl\", \"am\")\nmtcars |> tidyr::pivot_longer(all_of(vars))\nmtcars |> dplyr::select(all_of(vars))\n```\n\n### Loop Pattern\n\n```r\nvars <- c(\"cyl\", \"am\", \"vs\")\nfor (var in vars) {\n  result <- mtcars |>\n    dplyr::summarise(mean = mean(.data[[var]]))\n  print(result)\n}\n\n# Or with purrr\npurrr::map(vars, ~ dplyr::summarise(mtcars, mean = mean(.data[[.x]])))\n```\n\n## Bridge Patterns\n\nConvert between argument behaviors when the wrapped function doesn't match your desired interface.\n\n### Selection to Data-Mask: `across()`\n\nGive your function tidy-select behavior when wrapping a data-masked function:\n\n```r\nmy_group_by <- function(data, var) {\n  data |> dplyr::group_by(across({{ var }}))\n}\n# Now supports selection helpers:\nmtcars |> my_group_by(starts_with(\"c\"))\n```\n\nFor `...`, wrap in `c()`:\n\n```r\nmy_group_by <- function(.data, ...) {\n  .data |> dplyr::group_by(across(c(...)))\n}\n```\n\n### Names to Data-Mask: `across(all_of())`\n\nAccept character vectors for data-masked operations:\n\n```r\nmy_group_by <- function(data, vars) {\n  data |> dplyr::group_by(across(all_of(vars)))\n}\nmy_group_by(mtcars, c(\"cyl\", \"am\"))\n```\n\n### Data-Mask to Selection: `transmute()` Bridge\n\nThree-step pattern for data-masked input to tidy-select functions:\n\n```r\nmy_pivot_longer <- function(data, ...) {\n  # 1. Capture inputs in data-masked context, get names\n  inputs <- dplyr::transmute(data, ...)\n  names <- names(inputs)\n\n  # 2. Update data with the expressions\n  data <- dplyr::mutate(data, !!!inputs)\n\n  # 3. Pass names to tidy-select\n tidyr::pivot_longer(data, cols = all_of(names))\n}\n\nmtcars |> my_pivot_longer(cyl, am_scaled = am * 100)\n```\n\n## Transformation Patterns\n\n### Named Arguments: Code Around `{{}}`\n\nAdd code around embraced arguments:\n\n```r\nmy_mean <- function(data, var) {\n  data |> dplyr::summarise(mean = mean({{ var }}, na.rm = TRUE))\n}\n```\n\n### `...` Arguments: Use `across()`\n\nMap an expression across multiple columns:\n\n```r\nmy_mean <- function(data, ...) {\n  data |> dplyr::summarise(\n    across(c(...), ~ mean(.x, na.rm = TRUE))\n  )\n}\nmtcars |> my_mean(cyl, disp, hp)\n```\n\n### Filter with `if_all()` / `if_any()`\n\nCombine logical conditions across columns:\n```r\nfilter_non_min <- function(.data, ...) {\n  .data |> dplyr::filter(\n    if_all(c(...), ~ .x != min(.x, na.rm = TRUE))\n  )\n}\n\nfilter_any_max <- function(.data, ...) {\n  .data |> dplyr::filter(\n    if_any(c(...), ~ .x == max(.x, na.rm = TRUE))\n  )\n}\n```\n\n## Disambiguation: .data and .env Pronouns\n\nData masking can cause collisions when variable names exist in both the data and environment.\n\n### Column Collisions\n\n```r\nx <- 100\ndf <- data.frame(x = 1:3, y = 4:6)\n\n# Ambiguous: which x?\ndf |> dplyr::mutate(z = y / x)\n#> Uses column x (data takes precedence)\n\n# Explicit: use environment x\ndf |> dplyr::mutate(z = y / .env$x)\n```\n\n### In Functions (Critical)\n\nAlways use `.env` for function parameters that might collide:\n\n```r\nmy_rescale <- function(data, var, factor = 10) {\n  # Safe even if data has a 'factor' column\n  data |> dplyr::mutate(\n    \"{{ var }}\" := {{ var }} / .env$factor\n  )\n}\n```\n\n### Full Disambiguation\n\n```r\ndf |> dplyr::mutate(\n  result = .data$y / .env$x\n)\n```\n\n## Pitfalls\n\n### Double Evaluation\n\nExpressions injected multiple times execute multiple times:\n\n```r\n# BAD: times100() runs twice\nsummarise_stats <- function(data, var) {\n  data |> dplyr::summarise(\n    mean = mean({{ var }}),\n    sd = sd({{ var }})\n  )\n}\n# If var = times100(cyl), function executes twice\n\n# GOOD: Evaluate once, reference result\nsummarise_stats <- function(data, var) {\n  data |>\n    dplyr::transmute(var = {{ var }}) |>\n    dplyr::summarise(mean = mean(var), sd = sd(var))\n}\n```\n\nException: Glue strings (`\"{{ var }}\"`) don't suffer from double evaluation.\n\n### `{{` Out of Context\n\nOutside data-masking, `{{` becomes literal double-braces and silently returns the value:\n\n```r\n# In non-tidy-eval function:\nf <- function(x) {{ x }}\nf(1 + 1)\n#> [1] 2  # No error, but not defuse-and-inject\n```\n\n### `!!` and `!!!` Out of Context\n\nOutside injection context, these become logical negation:\n\n```r\nx <- TRUE\n!!x    # Double negation: TRUE\n!!!x   # Triple negation: FALSE\n```\n\nNo error, just wrong semantics.\n\n### `{{` on Non-Arguments\n\n`{{` should only wrap function arguments:\n\n```r\n# Correct\nmy_fn <- function(arg) {\n  summarise(data, {{ arg }})\n}\n\n# Problematic: x is not a defused argument\nx <- expr(cyl)\nsummarise(data, {{ x }})  # Captures x's VALUE, not expression\n```\n\n## Tidy Selection vs Data Masking\n\nTidy selection is *not* data masking. It interprets expressions rather than masking environments, so there's no ambiguity:\n\n```r\ndata <- data.frame(x = 1, data = 2)\n\n# Tidy selection: no collision\ndata |> dplyr::select(data:ncol(data))\n#> Works correctly\n\n# Data masking: potential collision\ndata |> dplyr::mutate(y = data + 1)\n#> Uses column 'data', not the data frame\n```\n\n## See Also\n\n- **r-metaprogramming**: Defusing, quosures, expression building mechanics\n- **designing-tidy-r-functions**: Function API design principles\n- **rlang-conditions**: Error handling with rlang\n\n## Vignettes\n\nAccess detailed rlang documentation via R:\n\n```r\n# Data masking concepts\nvignette(\"data-mask\", package = \"rlang\")\n\n# Programming with data masking\nvignette(\"data-mask-programming\", package = \"rlang\")\n\n# Or browse all vignettes\nbrowseVignettes(\"rlang\")\n```\n",
        "skills/tidymodels-overview/SKILL.md": "---\nname: tidymodels-overview\ndescription: This skill should be used when working with R tidymodels packages, including when the user asks to \"create a tidymodels workflow\", \"build a recipe\", \"tune a model\", \"use parsnip\", \"set up resampling\", \"create a workflow_set\", \"compare models\", \"stack models\", or mentions tidymodels packages like recipes, parsnip, workflows, workflowsets, tune, rsample, yardstick, or stacks. Provides ecosystem context before package-specific skills.\n---\n\n# Tidymodels Overview\n\nThe tidymodels ecosystem provides a consistent, modular framework for machine learning in R. Understanding the ecosystem context helps when working with any tidymodels pipeline before diving into package-specific details.\n\n## Core Principle: Recipes Are Plans, Not Actions\n\n**Critical**: A `recipe` object is a *specification* of preprocessing steps. Adding steps like `step_normalize()` does **not** transform data immediately. Transformations execute only when:\n\n1. `prep()` estimates parameters from training data\n2. `bake()` applies the prepped recipe to new data\n\n```r\n# This does NOT transform data - it creates a plan\nrec <- recipe(outcome ~ ., data = train) |>\n  step_normalize(all_numeric_predictors())\n\n# This estimates parameters (means, sds) from training data\nprepped <- prep(rec, training = train)\n\n# This applies transformations to new data\nprocessed <- bake(prepped, new_data = test)\n```\n\n## The Tidymodels Workflow\n\nFollow this standard workflow for modeling projects:\n\n### 1. Data Splitting (rsample)\n\nAllocate data to training, validation, and test sets before any modeling:\n\n```r\nset.seed(123)\ndata_split <- initial_split(data, prop = 0.8, strata = outcome)\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\n# For iterative evaluation during development\nresamples <- vfold_cv(train_data, v = 10)\n```\n\n### 2. Preprocessing (recipes)\n\nDefine feature engineering as a recipe specification:\n\n```r\nrec_spec <- recipe(outcome ~ ., data = train_data) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_factor_predictors()) |>\n  step_zv(all_predictors())\n```\n\nUse tidyselect helpers for column selection:\n- `all_predictors()`, `all_outcomes()` - by role\n- `all_numeric_predictors()`, `all_nominal_predictors()` - by type and role\n- `has_role()`, `has_type()` - explicit queries\n\n### 3. Model Specification (parsnip)\n\nDefine the model type, engine, and mode separately from fitting:\n\n```r\nmodel_spec <- rand_forest(mtry = tune(), trees = 1000) |>\n  set_engine(\"ranger\") |>\n  set_mode(\"regression\")\n```\n\n### 4. Bundling (workflows)\n\nCombine preprocessing and model into a single object:\n\n```r\nwflow <- workflow() |>\n  add_recipe(rec_spec) |>\n  add_model(model_spec)\n```\n\n### 5. Evaluation (tune + yardstick)\n\nUse resampling or validation sets to assess performance:\n\n```r\n# Define metrics\nmetrics <- metric_set(rmse, rsq, mae)\n\n# Tune hyperparameters\ntuned <- tune_grid(\n  wflow,\n  resamples = resamples,\n  grid = 10,\n  metrics = metrics\n)\n\n# Select best parameters\nbest_params <- select_best(tuned, metric = \"rmse\")\n```\n\n### 6. Finalization\n\nFinalize the workflow and fit to full training data:\n\n```r\nfinal_wflow <- finalize_workflow(wflow, best_params)\nfinal_fit <- last_fit(final_wflow, split = data_split)\n\n# Extract test set metrics\ncollect_metrics(final_fit)\n```\n\n## Package Roles\n\n| Package | Purpose | Key Functions |\n|---------|---------|---------------|\n| **rsample** | Data splitting and resampling | `initial_split()`, `vfold_cv()`, `bootstraps()` |\n| **recipes** | Preprocessing specification | `recipe()`, `step_*()`, `prep()`, `bake()` |\n| **parsnip** | Model specification | Model functions, `set_engine()`, `set_mode()` |\n| **workflows** | Bundle recipe + model | `workflow()`, `add_recipe()`, `add_model()` |\n| **tune** | Hyperparameter optimization | `tune_grid()`, `tune_bayes()`, `select_best()` |\n| **yardstick** | Performance metrics | `metric_set()`, `rmse()`, `accuracy()` |\n| **workflowsets** | Compare multiple pipelines | `workflow_set()`, `workflow_map()` |\n| **stacks** | Model ensembling | `stacks()`, `add_candidates()`, `blend_predictions()` |\n| **hardhat** | Internal infrastructure | `mold()`, `forge()`, blueprints |\n\n## Key Principles\n\n### Use Package Functions, Not Direct Access\n\nNever directly modify tidymodels object internals. Always use provided functions:\n\n```r\n# WRONG - directly modifying internals\nrecipe_obj$steps[[1]]$means <- new_means\n\n# CORRECT - use proper functions\nrec <- recipe(...) |>\n  step_normalize(...) |>\n  prep()\n```\n\n### Use Selectors, Not String Matching\n\nAvoid constructing variable lists manually:\n\n```r\n# WRONG - manual string matching\nnumeric_cols <- names(data)[sapply(data, is.numeric)]\nrec |> step_normalize(all_of(numeric_cols))\n\n# CORRECT - use tidyselect helpers\nrec |> step_normalize(all_numeric_predictors())\n```\n\n### Understand Role Requirements\n\nCustom roles are required at `bake()` time by default. When using `step_rm()` with custom roles, update requirements:\n\n```r\nrec <- recipe(...) |>\n  update_role(id_column, new_role = \"id\") |>\n  update_role_requirements(\"id\", bake = FALSE) |>\n  step_rm(has_role(\"id\"))\n```\n\n### workflowsets Require Same Outcome\n\nAll workflows in a `workflow_set` must predict the same outcome variable. For different outcomes, create separate workflow sets.\n\n## When to Use Each Package\n\n- **Simple model**: recipes + parsnip + workflows\n- **Hyperparameter tuning**: Add tune\n- **Model comparison**: Add workflowsets\n- **Ensemble models**: Add stacks (requires `save_pred = TRUE`, `save_workflow = TRUE`)\n- **Custom preprocessing interfaces**: Use hardhat\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed information, consult:\n\n- **`references/packages.md`** - Detailed package documentation including object structures, creation processes, and deep knowledge links\n- **`references/common-problems.md`** - Common pitfalls when working with tidymodels and how to avoid them\n\n### External Documentation\n\n- [tidymodels.org](https://www.tidymodels.org) - Official documentation and tutorials\n- [recipes.tidymodels.org](https://recipes.tidymodels.org) - Recipe step reference\n- [parsnip.tidymodels.org](https://parsnip.tidymodels.org) - Model specifications\n",
        "skills/tidymodels-overview/references/common-problems.md": "# Common Problems When Working with Tidymodels\n\nThis reference documents frequently observed problems when working with tidymodels packages and how to avoid them.\n\n## General Problems\n\n### Directly Modifying Internals\n\n**Problem**: Directly accessing tidymodels objects by modifying lists and changing attributes instead of using proper constructors, accessors, and functions.\n\n**Why this is bad**:\n- Bypasses validation that package functions provide\n- Creates brittle code that breaks when internal representations change\n- Violates encapsulation principles\n\n**Wrong approach**:\n```r\n# Directly modifying recipe internals\nrecipe_obj$steps[[1]]$means <- new_means\nrecipe_obj$var_info$role[3] <- \"outcome\"\n\n# Directly modifying workflow internals\nworkflow_obj$fit$fit$fit <- new_model\n```\n\n**Correct approach**:\n```r\n# Use proper functions to create new objects\nrec <- recipe(...) |>\n  step_normalize(...) |>\n  prep()\n\n# Use extraction functions\nextract_fit_engine(workflow_fit)\nextract_recipe(workflow_fit)\n```\n\n### Ignoring Package Functions / Writing Custom Implementations\n\n**Problem**: Writing custom functions for operations that tidymodels packages already provide, especially for common tasks like variable selection, data extraction, or model evaluation.\n\n**Why this is bad**:\n- Custom implementations miss edge cases that package authors have handled\n- Creates maintenance burden when tidymodels updates\n- Often less efficient than optimized package functions\n\n**Examples of reinventing the wheel**:\n```r\n# WRONG - custom extraction\nmy_data <- workflow_obj$pre$mold$predictors\n\n# CORRECT - use extraction function\nmy_data <- extract_mold(workflow_fit)$predictors\n```\n\n---\n\n## recipes Package Problems\n\n### Expecting Immediate Changes After Adding Steps\n\n**Problem**: Expecting data to be transformed immediately after adding a step to a recipe.\n\n**Why this happens**: Misunderstanding that recipes are a *plan* and don't execute until `prep()` and `bake()`.\n\n**Wrong understanding**:\n```r\nrec <- recipe(y ~ ., data = train) |>\n  step_normalize(all_numeric_predictors())\n\n# Expecting 'rec' to now contain normalized data - IT DOES NOT\n```\n\n**Correct understanding**:\n```r\n# Recipe is just a specification\nrec <- recipe(y ~ ., data = train) |>\n  step_normalize(all_numeric_predictors())\n\n# prep() estimates parameters (means, sds) from training data\nprepped <- prep(rec, training = train)\n\n# bake() applies the transformations\nnormalized_train <- bake(prepped, new_data = NULL)\nnormalized_test  <- bake(prepped, new_data = test)\n```\n\n**Exception**: `update_role()` and `update_role_requirements()` immediately update the recipe; these are distinct from `step_*` functions.\n\n### Writing Custom Variable Selection Logic\n\n**Problem**: Constructing lists of variable names with string matching (e.g., `grepl`) and type checking instead of using tidyselect helpers.\n\n**Wrong approach**:\n```r\n# Manual string matching for numeric predictors\nnumeric_cols <- names(train)[sapply(train, is.numeric)]\npredictor_cols <- setdiff(numeric_cols, \"outcome\")\n\nrec <- recipe(outcome ~ ., data = train) |>\n  step_normalize(all_of(predictor_cols))\n```\n\n**Correct approach**:\n```r\n# Use tidyselect helpers\nrec <- recipe(outcome ~ ., data = train) |>\n  step_normalize(all_numeric_predictors())\n```\n\n**Available selectors by role**:\n- `has_role()`, `all_predictors()`, `all_outcomes()`\n\n**Available selectors by type**:\n- `has_type()`, `all_numeric()`, `all_integer()`, `all_double()`\n- `all_nominal()`, `all_ordered()`, `all_unordered()`, `all_factor()`, `all_string()`\n- `all_date()`, `all_datetime()`\n\n**Combined selectors**:\n- `all_numeric_predictors()`, `all_nominal_predictors()`\n\n**References**:\n- https://recipes.tidymodels.org/articles/Selecting_Variables.html\n- https://recipes.tidymodels.org/reference/selections.html\n\n### Misunderstanding Roles and New Data Requirements\n\n**Problem**: Custom roles are required at `bake()` time by default. When using `step_rm()` to remove columns with custom roles, new data will fail if it doesn't have those columns.\n\n**Scenario**: You have an ID column that you want to use for tracking but remove before modeling:\n\n```r\n# This will FAIL when baking new data that lacks 'id_column'\nrec <- recipe(outcome ~ ., data = train) |>\n  update_role(id_column, new_role = \"id\") |>\n  step_rm(has_role(\"id\"))\n\nprepped <- prep(rec)\nbake(prepped, new_data = new_data)  # Error: id_column not found\n```\n\n**Solution**: Use `update_role_requirements()` to make the role optional at bake time:\n\n```r\nrec <- recipe(outcome ~ ., data = train) |>\n  update_role(id_column, new_role = \"id\") |>\n  update_role_requirements(\"id\", bake = FALSE) |>\n  step_rm(has_role(\"id\"))\n```\n\n**Reference**: https://recipes.tidymodels.org/reference/update_role_requirements.html\n\n---\n\n## workflows and workflowsets Problems\n\n### Using Bare Lists Instead of workflow Objects\n\n**Problem**: Creating ad-hoc lists to store model/recipe combinations instead of using `workflow()` objects.\n\n**Wrong approach**:\n```r\n# Ad-hoc list structure\nmy_pipeline <- list(\n  recipe = my_recipe,\n\n  model = my_model\n)\n```\n\n**Correct approach**:\n```r\nwflow <- workflow() |>\n  add_recipe(my_recipe) |>\n  add_model(my_model)\n```\n\n**Why workflows are better**:\n- Consistent interface for fitting and prediction\n- Proper handling of preprocessing during prediction\n- Integration with tune, workflowsets, and other tidymodels packages\n\n### workflowsets With Different Outcomes\n\n**Problem**: Attempting to create a `workflow_set` where workflows predict different outcome variables.\n\n**Constraint**: All workflows in a `workflow_set` must have the same outcome.\n\n**Wrong approach**:\n```r\n# These recipes have different outcomes\nrec_price <- recipe(price ~ ., data = train)\nrec_quality <- recipe(quality ~ ., data = train)\n\n# This will cause problems\nworkflow_set(\n  preproc = list(price_rec = rec_price, quality_rec = rec_quality),\n  models = list(lm = linear_reg())\n)\n```\n\n**Correct approach**: Create separate workflow sets for different outcomes:\n\n```r\nprice_set <- workflow_set(\n  preproc = list(basic = rec_price),\n  models = list(lm = linear_reg(), rf = rand_forest())\n)\n\nquality_set <- workflow_set(\n  preproc = list(basic = rec_quality),\n  models = list(lm = linear_reg(), rf = rand_forest())\n)\n```\n\n---\n\n## stacks Package Problems\n\n### Missing Required Control Flags\n\n**Problem**: Candidate models cannot be added to a stack because they were fit without the required metadata.\n\n**Required flags**:\n- `save_pred = TRUE` - Meta-model trains on out-of-sample predictions\n- `save_workflow = TRUE` - `fit_members()` needs model specifications\n\n**Wrong approach**:\n```r\n# Default control doesn't save what stacks needs\nresults <- tune_grid(wflow, resamples, grid = 10)\nstack <- stacks() |>\n  add_candidates(results)  # Will fail or produce incomplete stack\n```\n\n**Correct approach**:\n```r\n# Use stacks-specific control functions\nctrl <- control_stack_grid()  # or control_stack_resamples()\n\nresults <- tune_grid(wflow, resamples, grid = 10, control = ctrl)\nstack <- stacks() |>\n  add_candidates(results)  # Works correctly\n```\n\n---\n\n## Data Leakage Problems\n\n### Preprocessing Before Splitting\n\n**Problem**: Applying preprocessing (normalization, imputation, etc.) to the entire dataset before splitting into train/test sets.\n\n**Why this is bad**: Information from the test set \"leaks\" into the training process, leading to overly optimistic performance estimates.\n\n**Wrong approach**:\n```r\n# Normalize entire dataset first\ndata$x <- scale(data$x)\n\n# Then split - test set statistics influenced training\nsplit <- initial_split(data)\n```\n\n**Correct approach**:\n```r\n# Split first\nsplit <- initial_split(data)\ntrain <- training(split)\n\n# Preprocessing parameters estimated ONLY from training data\nrec <- recipe(y ~ ., data = train) |>\n  step_normalize(all_numeric_predictors())\n\n# prep() uses only training data\nprepped <- prep(rec, training = train)\n```\n\n### Using Test Set During Model Development\n\n**Problem**: Repeatedly evaluating models on the test set during development, then reporting test set performance as final results.\n\n**Correct approach**: Reserve test set for final evaluation only. Use validation sets or cross-validation during development:\n\n```r\n# For larger datasets: validation set approach\nsplit <- initial_validation_split(data, prop = c(0.6, 0.2))\ntrain <- training(split)\nval <- validation(split)\ntest <- testing(split)\n\n# For smaller datasets: cross-validation approach\nsplit <- initial_split(data, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\nfolds <- vfold_cv(train, v = 10)\n\n# Develop models using folds, THEN evaluate final model on test\n```\n",
        "skills/tidymodels-overview/references/packages.md": "# Tidymodels Package Reference\n\nDetailed documentation for each tidymodels package including object structures, key functions, and integration points.\n\n## recipes\n\nThe `recipes` package defines preprocessing data and feature engineering steps.\n\n### recipe Object Structure\n\n- **var_info**: Tibble with information about variables in the original data (name, type, role)\n- **term_info**: Tibble with information about terms used in the recipe (name, type, role, source)\n- **steps**: List of step objects (e.g., `step_normalize`, `step_dummy`), each defining a transformation\n- **template**: Tibble (often 0-row) capturing the names and types of data used to train the recipe (after `prep()`)\n- **trained**: Logical flag, `FALSE` initially, becomes `TRUE` after `prep()` is called\n\n### Creation and Use\n\n1. Initialize with `recipe(formula, data)`\n2. Add transformation steps sequentially (`step_normalize()`, `step_dummy()`, etc.)\n3. This creates an **unprepped** recipe object\n4. `prep()` estimates parameters from training data\n5. `bake()` applies the prepped recipe to new data\n6. `juice()` extracts the preprocessed training data (deprecated in favor of `bake(new_data = NULL)`)\n\n### Variable Selection\n\nRole-based selectors:\n- `all_predictors()`, `all_outcomes()` - select by assigned role\n- `has_role(\"custom_role\")` - select by specific role\n\nType-based selectors:\n- `all_numeric()`, `all_nominal()`, `all_factor()`, `all_string()`\n- `all_integer()`, `all_double()`, `all_ordered()`, `all_unordered()`\n- `all_date()`, `all_datetime()`\n\nCombined selectors:\n- `all_numeric_predictors()`, `all_nominal_predictors()` - type AND role\n\n### Deep Knowledge\n\n- [Selecting Variables](https://recipes.tidymodels.org/articles/Selecting_Variables.html)\n- [Recipe Internals](https://recipes.tidymodels.org/articles/internals.html)\n- [Selection Reference](https://recipes.tidymodels.org/reference/selections.html)\n\n---\n\n## parsnip\n\nThe `parsnip` package provides a consistent interface for specifying models via the `model_spec` S3 object.\n\n### model_spec Object Structure\n\n- **args**: Named list of primary model arguments (e.g., `penalty`, `mtry`), stored as quosures to be evaluated at fit time\n- **engine**: Character string specifying the computational engine (e.g., `\"lm\"`, `\"glmnet\"`)\n- **mode**: Character string for the modeling task (`\"regression\"`, `\"classification\"`)\n- **method**: Internal list with engine-specific details for fitting and prediction (\"translation layer\")\n\n### Creation Process\n\n1. Instantiate a base model: `linear_reg()`, `rand_forest()`, etc.\n2. Specify the computational engine: `set_engine(\"glmnet\")`\n3. Confirm the modeling mode: `set_mode(\"regression\")`\n\n### Key Model Functions\n\n| Function | Model Type |\n|----------|------------|\n| `linear_reg()` | Linear regression |\n| `logistic_reg()` | Logistic regression |\n| `rand_forest()` | Random forest |\n| `boost_tree()` | Gradient boosting |\n| `decision_tree()` | Single decision tree |\n| `svm_rbf()`, `svm_linear()`, `svm_poly()` | Support vector machines |\n| `mlp()` | Neural network (multilayer perceptron) |\n| `cubist_rules()` | Cubist rule-based model |\n\n---\n\n## hardhat\n\nThe `hardhat` package provides foundational tools for applying data transformations consistently using \"blueprints.\" Often used internally by `workflows`.\n\n### Blueprints\n\nInstruction sets defining how data is processed:\n- `formula_blueprint` - for formula-based preprocessing\n- `recipe_blueprint` - for recipe-based preprocessing\n\nPurpose: Store learned parameters from training data to ensure consistent application.\n\n### recipe_blueprint Key Slots (after mold())\n\n- **recipe**: The *prepped* `recipes::recipe` object\n- **ptypes**: Prototypes (empty tibbles) of processed predictors and outcomes\n- **intercept**: Logical flag indicating if an intercept is added post-recipe\n- **allow_novel_levels**: Logical flag for handling new factor levels\n\n### Core Process\n\n1. **`mold()`**: Used on training data\n   - Preps the preprocessor (e.g., a recipe)\n   - Creates the blueprint\n   - Processes the data\n   - Outputs: processed predictors, outcomes, and the prepped blueprint\n\n2. **`forge()`**: Used on new data\n   - Applies stored transformations from the prepped blueprint\n   - Ensures consistency between training and prediction\n\n---\n\n## workflows\n\nThe `workflows` package bundles preprocessing (recipes/formulas) and model specifications into a single `workflow` S3 object.\n\n### workflow Object Structure\n\n- **actions**: List defining operations\n  - `preprocessor` holds the recipe/formula\n  - `model` holds the `model_spec`\n- **pre$mold** (after fit): Contains output of `hardhat::mold()`, including the prepped blueprint and processed data\n- **fit$fit** (after fit): Holds the fitted `parsnip::model_fit` object\n- **trained**: Logical flag, `FALSE` initially, set to `TRUE` after fitting\n\n### Key Processes\n\n- **`fit.workflow()`**: Orchestrates training\n  1. Calls `hardhat::mold()` on training data\n  2. Passes processed data to `parsnip::fit()`\n\n- **`predict.workflow()`**: Ensures consistent prediction\n  1. Retrieves prepped blueprint from fitted workflow\n  2. Uses `hardhat::forge()` to process new data\n  3. Calls `predict()` on the fitted model\n\n---\n\n## workflowsets\n\nThe `workflowsets` package creates, manages, and evaluates collections of multiple workflows.\n\n### workflow_set Object Structure\n\nA tibble where each row represents a unique workflow, using list-columns:\n\n- **wflow_id**: Character string providing unique identifier\n- **info**: List-column containing the unevaluated `workflow` object\n- **option**: List-column for evaluation options (e.g., tuning grids)\n- **result**: List-column populated with evaluation results (e.g., `tune_results` objects)\n\n### Evaluation Process\n\n`workflow_map()` applies an evaluation function to each workflow:\n1. Iterates through each row\n2. Extracts the workflow\n3. Applies the specified function with its options\n4. Stores output in the `result` column\n\n### Important Constraint\n\n**All workflows in a workflow_set must have the same outcome variable.** For different outcomes, create separate workflow sets.\n\n---\n\n## rsample\n\nThe `rsample` package provides standardized framework for creating and managing data splits.\n\n### rset and rsplit Object Structure\n\n- **rset**: Tibble representing a collection of resamples\n- **rsplit**: S3 object representing a single split\n\nrsplit stores:\n- Reference to original data (not a copy)\n- Integer indices for **analysis** (training) and **assessment** (testing/holdout) sets\n\n### Memory Efficiency\n\nrsplit objects do not copy the data; they store a reference to the original environment's data frame and specific integer pointers.\n\n### Core Functions\n\n- `initial_split()` - Single train/test partition\n- `initial_validation_split()` - Train/validation/test partition\n- `vfold_cv()` - V-fold cross-validation\n- `bootstraps()` - Bootstrap resampling\n- `training()`, `testing()` - Extract data from split\n- `analysis()`, `assessment()` - Extract data from rsplit\n\n---\n\n## tune\n\nThe `tune` package provides tools for hyperparameter optimization.\n\n### tune_results Object Structure\n\nA subclass of `rset` that augments the resample tibble with evaluation data:\n\n- **.metrics**: List-column of tibbles containing performance values for each parameter combination\n- **.notes**: List-column of tibbles capturing warnings or errors during fit\n- **.predictions**: List-column (optional) of out-of-sample predictions\n- **parameters** (attribute): A `dials` parameter object defining the search space\n\n### Tuning Algorithms\n\n**Grid Search (`tune_grid()`)**: Evaluates pre-defined hyperparameter combinations\n- Regular grids: Cartesian product of parameter levels\n- Space-filling designs: Latin Hypercube for better coverage\n\n**Bayesian Optimization (`tune_bayes()`)**: Uses Gaussian Process surrogate model with acquisition function (e.g., Expected Improvement)\n\n### Key Functions\n\n- `tune()` - Placeholder for tunable parameters\n- `tune_grid()`, `tune_bayes()` - Tuning functions\n- `select_best()`, `select_by_one_std_err()`, `select_by_pct_loss()` - Select best parameters\n- `finalize_workflow()` - Apply selected parameters\n- `last_fit()` - Final fit on training, evaluate on test\n\n---\n\n## yardstick\n\nThe `yardstick` package quantifies model performance with tidyverse-compliant output.\n\n### Metric Tibble Structure\n\nAll functions return a tibble with exactly three columns:\n- **.metric**: Name of the metric (e.g., `\"rmse\"`, `\"roc_auc\"`)\n- **.estimator**: Type of estimator (e.g., `\"standard\"`, `\"macro\"`, `\"macro_weighted\"`)\n- **.estimate**: Calculated numerical value\n\n### metric_set\n\nBundle multiple metrics for consistent evaluation:\n\n```r\nmetrics <- metric_set(rmse, rsq, mae)\nmetrics(data, truth = actual, estimate = predicted)\n```\n\n### Common Metrics\n\n**Regression**: `rmse()`, `rsq()`, `mae()`, `mape()`, `huber_loss()`\n\n**Classification**: `accuracy()`, `roc_auc()`, `pr_auc()`, `f_meas()`, `kap()`\n\n**Multiclass**: Use `.estimator` argument (`\"macro\"`, `\"micro\"`, `\"weighted\"`)\n\n---\n\n## stacks\n\nThe `stacks` package builds model ensembles using stacking.\n\n### Stacking Workflow\n\n1. **`stacks()`**: Create empty `data_stack` object\n2. **`add_candidates()`**: Add tuning/resampling results\n3. **`blend_predictions()`**: Fit meta-model (LASSO) to determine weights\n4. **`fit_members()`**: Train retained models on full training data\n5. **`predict()`**: Generate ensemble predictions\n\n### Critical Requirements\n\nCandidate model results must contain specific metadata:\n\n```r\n# Use these control functions\nctrl <- control_stack_grid()    # for tune_grid()\nctrl <- control_stack_resamples()  # for fit_resamples()\n```\n\nThese helpers set:\n- `save_pred = TRUE` - Required because meta-model trains on out-of-sample predictions\n- `save_workflow = TRUE` - Required so `fit_members()` knows model specifications\n\n### Integration with workflowsets\n\nPass a trained `workflow_set` directly to `add_candidates()` to batch-add all successful workflows.\n"
      },
      "plugins": [
        {
          "name": "r-skills",
          "source": "./",
          "description": "Skills for R programming covering tidyverse patterns, rlang conditions, targets pipelines, metaprogramming, ggplot2, and tidymodels",
          "version": "0.1.2",
          "author": {
            "name": "jsperger"
          },
          "license": "MIT",
          "keywords": [
            "r",
            "rlang",
            "tidyverse",
            "tidymodels",
            "targets",
            "ggplot2",
            "programming"
          ],
          "category": "languages",
          "categories": [
            "ggplot2",
            "languages",
            "programming",
            "r",
            "rlang",
            "targets",
            "tidymodels",
            "tidyverse"
          ],
          "install_commands": [
            "/plugin marketplace add jsperger/llm-r-skills",
            "/plugin install r-skills@r-skills"
          ]
        }
      ]
    }
  ]
}