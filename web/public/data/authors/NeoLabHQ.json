{
  "author": {
    "id": "NeoLabHQ",
    "display_name": "NeoLab",
    "avatar_url": "https://avatars.githubusercontent.com/u/208055522?v=4"
  },
  "marketplaces": [
    {
      "name": "context-engineering-kit",
      "version": "1.10.2",
      "description": "Hand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint focused on improving agent result quality.",
      "repo_full_name": "NeoLabHQ/context-engineering-kit",
      "repo_url": "https://github.com/NeoLabHQ/context-engineering-kit",
      "repo_description": "Hand-crafted plugin marketplace focused on improving agent results quality. Supports Claude Code, OpenCode, Cursor, Windsurf, and Cline.",
      "signals": {
        "stars": 378,
        "forks": 31,
        "pushed_at": "2026-02-02T20:44:26Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"context-engineering-kit\",\n  \"version\": \"1.10.2\",\n  \"description\": \"Hand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint focused on improving agent result quality.\",\n  \"owner\": {\n    \"name\": \"NeoLabHQ\",\n    \"email\": \"vlad.goncharov@neolab.finance\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"reflexion\",\n      \"description\": \"Collection of commands that force LLM to reflect on previous response and output. Based on papers like Self-Refine and Reflexion. These techniques improve the output of large language models by introducing feedback and refinement loops.\",\n      \"version\": \"1.1.4\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/reflexion\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"code-review\",\n      \"description\": \"Introduce codebase and PR review commands and skills using multiple specialized agents.\",\n      \"version\": \"1.0.8\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/code-review\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"git\",\n      \"description\": \"Introduces commands for commit and PRs creation, plus skills for git worktrees and notes.\",\n      \"version\": \"1.2.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/git\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"tdd\",\n      \"description\": \"Introduces commands for test-driven development, common anti-patterns and skills for testing using subagents.\",\n      \"version\": \"1.1.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/tdd\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"sadd\",\n      \"description\": \"Introduces skills for subagent-driven development, dispatches fresh subagent for each task with code review between tasks, enabling fast iteration with quality gates.\",\n      \"version\": \"1.2.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/sadd\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"ddd\",\n      \"description\": \"Introduces command to update CLAUDE.md with best practices for domain-driven development, focused on quality of code, includes Clean Architecture, SOLID principles, and other design patterns.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/ddd\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"sdd\",\n      \"description\": \"Specification Driven Development workflow commands and agents, based on Github Spec Kit and OpenSpec. Uses specialized agents for effective context management and quality review.\",\n      \"version\": \"1.1.5\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/sdd\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"kaizen\",\n      \"description\": \"Inspired by Japanese continuous improvement philosophy, Agile and Lean development practices. Introduces commands for analysis of root cause of issues and problems, including 5 Whys, Cause and Effect Analysis, and other techniques.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/kaizen\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"customaize-agent\",\n      \"description\": \"Commands and skills for writing and refining commands, hooks, skills for Claude Code, includes Anthropic Best Practices and Agent Persuasion Principles that can be useful for sub-agent workflows.\",\n      \"version\": \"1.3.2\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/customaize-agent\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"docs\",\n      \"description\": \"Commands for analysing project, writing and refining documentation.\",\n      \"version\": \"1.1.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/docs\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"tech-stack\",\n      \"description\": \"Commands for setup or update of CLAUDE.md file with best practices for specific language or framework.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/tech-stack\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"mcp\",\n      \"description\": \"Commands for setup well known MCP server integration if needed and update CLAUDE.md file with requirement to use this MCP server for current project.\",\n      \"version\": \"1.2.1\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/mcp\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"fpf\",\n      \"description\": \"First Principles Framework (FPF) for structured reasoning. Implements ADI (Abduction-Deduction-Induction) cycle for hypothesis generation, logical verification, empirical validation, and auditable decision-making.\",\n      \"version\": \"1.1.1\",\n      \"author\": {\n        \"name\": \"Vlad Goncharov\",\n        \"email\": \"vlad.goncharov@neolab.finance\"\n      },\n      \"source\": \"./plugins/fpf\",\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        "README.md": "<picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/assets/CEK-header.png\">\n    <img src=\"docs/assets/CEK-header.png\" alt=\"Context Engineering Kit - advanced context engineering techniques\" />\n</picture>\n\n<div align=\"center\">\n\n[![License](https://img.shields.io/badge/license-GPL%203.0-blue.svg)](LICENSE)\n[![agentskills.io](https://img.shields.io/badge/format-agentskills.io-purple.svg)](https://agentskills.io)\n[![Mentioned in Awesome Claude Code](https://awesome.re/mentioned-badge.svg)](https://github.com/hesreallyhim/awesome-claude-code)\n\n[Install](#quick-start) · [Plugins](#plugins-list) · [Github Action](#usage-in-github-actions) · [Docs](https://cek.neolab.finance/)\n\n</div>\n\n# [Context Engineering Kit](https://cek.neolab.finance)\n\nHand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint, focused on improving agent result quality and predictability.\n\nMarketplace is based on prompts used daily by our company developers for a long time, while adding plugins from benchmarked papers and high-quality projects.\n\n## Supported Agents\n\nUniversal support based on [agentskills.io](https://agentskills.io) and [openskills](https://github.com/numman-ali/openskills) standards.\n\n| Agent | How it works | Status |\n|:------|:-------------|:------:|\n| **Claude Code** | Native plugin system | ✅ Native |\n| **Cursor** | openskills → AGENTS.md | ✅ Universal |\n| **Windsurf** | openskills → AGENTS.md | ✅ Universal |\n| **Cline** | openskills → AGENTS.md | ✅ Universal |\n| **OpenCode** | Native skill support | ✅ Native |\n| **Amp Code** | openskills → AGENTS.md | ✅ Universal |\n\n## Key Features\n\n- **Simple to Use** - Easy to install and use without any dependencies. Contains automatically used skills and self-explanatory commands.\n- **Token-Efficient** - Carefully crafted prompts and architecture, preferring commands with sub-agents over skills when possible, to minimize populating context with unnecessary information.\n- **Quality-Focused** - Each plugin is focused on meaningfully improving agent results in a specific area.\n- **Granular** - Install only the plugins you need. Each plugin loads only its specific agents, commands, and skills. Each without overlap and redundant skills.\n- **Scientifically proven** - Plugins are based on proven techniques and patterns that were tested by well-trusted benchmarks and studies.\n\n## Quick Start\n\n### Step 1: Install Marketplaces and Plugin\n\n#### Claude Code\n\nOpen Claude Code and add the Context Engineering Kit marketplace\n\n```bash\n/plugin marketplace add NeoLabHQ/context-engineering-kit\n```\n\nThis makes all plugins available for installation, but does not load any agents or skills into your context.\n\nInstall any plugins, for example reflexion\n\n```bash\n/plugin install reflexion@NeoLabHQ/context-engineering-kit\n```\n\nEach installed plugin loads only its specific agents, commands, and skills into Claude's context.\n\n#### Cursor, Windsurf, Cline, OpenCode\n\nCommands installation is available for Cursor and OpenCode:\n\n<details>\n<summary>Cursor</summary>\n\nCursor does not support sub-agents, so the script installs only commands that do not use them.\n\nInstall per project to the `.claude/commands` directory by default (Cursor uses the `.claude/commands` folder for some reason):\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/NeoLabHQ/context-engineering-kit/refs/heads/master/.bin/install-commands.sh | bash\n```\n\nInstall globally to `~/.cursor/commands` directory:\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/NeoLabHQ/context-engineering-kit/refs/heads/master/.bin/install-commands.sh | bash -s -- --global\n```\n\n</details>\n\n<details>\n<summary>OpenCode</summary>\n\nInstall per project to `.opencode/commands` directory:\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/NeoLabHQ/context-engineering-kit/refs/heads/master/.bin/install-commands.sh | bash -s -- --agent opencode\n```\n\nInstall globally to `~/.config/opencode/commands` directory:\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/NeoLabHQ/context-engineering-kit/refs/heads/master/.bin/install-commands.sh | bash -s -- --agent opencode --global\n```\n\n</details>\n\nUse [OpenSkills](https://github.com/numman-ali/openskills) to install skills for broad range of agents:\n\n```bash\nnpx openskills install NeoLabHQ/context-engineering-kit\nnpx openskills sync\n```\n\n### Step 2: Use Plugin\n\n```bash\n> claude \"implement user authentication\"\n# Claude implements user authentication, then you can ask it to reflect on implementation\n\n> /reflexion:reflect\n# It analyses results and suggests improvements\n# If issues are obvious, it will fix them immediately\n# If they are minor, it will suggest improvements that you can respond to\n> fix the issues\n\n# If you would like it to avoid issues that were found during reflection to appear again,\n# ask claude to extract resolution strategies and save the insights to project memory\n> /reflexion:memorize\n```\n\nAlternatively, you can use the `reflect` word in intial prompt:\n\n```bash\n> claude \"implement user authentication, then reflect\"\n# Claude implements user authentication,\n# then hook automatically runs /reflexion:reflect\n```\n\nIn order to use this hook, you need to have `bun` installed. However, it is not required for the overall command.\n\n## Documentation\n\nYou can find the complete Context Engineering Kit documentation [here](https://cek.neolab.finance).\n\n## Plugins List\n\nTo view all available plugins:\n\n```bash\n/plugin\n```\n\n- [Reflexion](https://cek.neolab.finance/plugins/reflexion) - Introduces feedback and refinement loops to improve output quality.\n- [Code Review](https://cek.neolab.finance/plugins/code-review) - Introduces codebase and PR review commands and skills using multiple specialized agents.\n- [Git](https://cek.neolab.finance/plugins/git) - Introduces commands for commit and PRs creation.\n- [Test-Driven Development](https://cek.neolab.finance/plugins/tdd) - Introduces commands for test-driven development, common anti-patterns and skills for testing using subagents.\n- [Subagent-Driven Development](https://cek.neolab.finance/plugins/sadd) - Introduces skills for subagent-driven development, dispatches fresh subagent for each task with code review between tasks, enabling fast iteration with quality gates.\n- [Domain-Driven Development](https://cek.neolab.finance/plugins/ddd) - Introduces commands to update CLAUDE.md with best practices for domain-driven development, focused on code quality, and includes Clean Architecture, SOLID principles, and other design patterns.\n- [Spec-Driven Development](https://cek.neolab.finance/plugins/sdd) - Introduces commands for specification-driven development, based on Github Spec Kit, OpenSpec and BMad Method. Uses specialized agents for effective context management and quality review.\n- [FPF - First Principles Framework](https://cek.neolab.finance/plugins/fpf) - Introduces structured reasoning using ADI cycle (Abduction-Deduction-Induction) with knowledge layer progression. Uses workflow command pattern with fpf-agent for hypothesis generation, verification, and auditable decision-making.\n- [Kaizen](https://cek.neolab.finance/plugins/kaizen) - Inspired by Japanese continuous improvement philosophy, Agile and Lean development practices. Introduces commands for analysis of root causes of issues and problems, including 5 Whys, Cause and Effect Analysis, and other techniques.\n- [Customaize Agent](https://cek.neolab.finance/plugins/customaize-agent) - Commands and skills for writing and refining commands, hooks, and skills for Claude Code. Includes Anthropic Best Practices and [Agent Persuasion Principles](https://arxiv.org/abs/2508.00614) that can be useful for sub-agent workflows.\n- [Docs](https://cek.neolab.finance/plugins/docs) - Commands for analyzing projects, writing and refining documentation.\n- [Tech Stack](https://cek.neolab.finance/plugins/tech-stack) - Commands for setting up or updating CLAUDE.md file with best practices for specific languages or frameworks.\n- [MCP](https://cek.neolab.finance/plugins/mcp) - Commands for setting up well-known MCP server integration if needed and updating CLAUDE.md file with requirements to use this MCP server for the current project.\n\n### Reflexion\n\nCollection of commands that force the LLM to reflect on previous response and output. Includes **automatic reflection hooks** that trigger when you include \"reflect\" in your prompt.\n\n**How to install**\n\n```bash\n/plugin install reflexion@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/reflexion:reflect` - Reflect on previous response and output, based on Self-refinement framework for iterative improvement with complexity triage and verification\n- `/reflexion:memorize` - Memorize insights from reflections and update the CLAUDE.md file with this knowledge. Curates insights from reflections and critiques into CLAUDE.md using Agentic Context Engineering\n- `/reflexion:critique` - Comprehensive multi-perspective review using specialized judges with debate and consensus building\n\n**Hooks**\n\n- **Automatic Reflection Hook** - Triggers `/reflexion:reflect` automatically when \"reflect\" appears in your prompt\n\n#### Theoretical Foundation\n\nBased on papers like [Self-Refine](https://arxiv.org/abs/2303.17651) and [Reflexion](https://arxiv.org/abs/2303.11366). These techniques improve the output of large language models by introducing feedback and refinement loops.\n\nThey are proven to **increase output quality by 8–21%** based on both automatic metrics and human preferences across seven diverse tasks, including dialogue generation, coding, and mathematical reasoning, when compared to standard one-step model outputs.\n\nFull list of included patterns and techniques:\n\n- [Self-Refinement / Iterative Refinement](https://arxiv.org/abs/2303.17651) - One model generates, then reviews and improves its own output\n- [Constitutional AI (CAI) / RLAIF](https://arxiv.org/abs/2212.08073) - One model generates responses, another critiques them based on principles\n- [Critic-Generator or Verifier-Generator Architecture](https://arxiv.org/abs/2510.14660v1) - Generator model creates outputs, Critic/verifier model evaluates and provides feedback\n- [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) - One LLM evaluates/scores outputs from another LLM\n- [Debate / Multi-Agent Debate](https://arxiv.org/abs/2305.14325) - Multiple models propose and critique solutions\n- [Generate-Verify-Refine (GVR)](https://arxiv.org/abs/2204.05511) - Three-stage process: generate → verify → refine based on verification\n\nOn top of that, the plugin is based on the [Agentic Context Engineering](https://arxiv.org/abs/2510.04618) paper that uses memory updates after reflection, and **consistently outperforms strong baselines by 10.6%** on agents.\n\nAlso includes the following techniques:\n\n- [Chain-of-Verification (CoVe)](https://arxiv.org/abs/2309.11495) - Model generates answer, then verification questions, then revises\n- [Tree of Thoughts (ToT)](https://arxiv.org/abs/2305.10601) - Explores multiple reasoning paths with evaluation\n- [Process Reward Models (PRM)](https://arxiv.org/abs/2305.20050) - Evaluates reasoning steps rather than just final answers\n\n### Code Review\n\nComprehensive code review commands using multiple specialized agents for thorough code quality evaluation.\n\n**How to install**\n\n```bash\n/plugin install code-review@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/code-review:review-local-changes` - Comprehensive review of local uncommitted changes using specialized agents with code improvement suggestions\n- `/code-review:review-pr` - Comprehensive pull request review using specialized agents\n\n**Agents**\n\nThis plugin uses multiple specialized agents for comprehensive code quality analysis:\n\n- **bug-hunter** - Identifies potential bugs, edge cases, and error-prone patterns\n- **code-quality-reviewer** - Evaluates code structure, readability, and maintainability\n- **contracts-reviewer** - Reviews interfaces, API contracts, and data models\n- **historical-context-reviewer** - Analyzes changes in relation to codebase history and patterns\n- **security-auditor** - Identifies security vulnerabilities and potential attack vectors\n- **test-coverage-reviewer** - Evaluates test coverage and suggests missing test cases\n\n#### Usage in github actions\n\nYou can use [anthropics/claude-code-action](https://github.com/marketplace/actions/claude-code-action-official) to run this plugin for PR reviews in github actions.\n\n1. Use `/install-github-app` command to setup workflow and secrets.\n2. Set content of `.github/workflows/claude-code-review.yml` to the following:\n\n```yaml\nname: Claude Code Review\n\non:\n  pull_request:\n    types:\n    - opened\n    - synchronize # remove if want to run only, when PR is opened\n    - ready_for_review\n    - reopened\n    # Uncomment to limit which files can trigger the workflow\n    # paths:\n    #   - \"**/*.ts\"\n    #   - \"**/*.tsx\"\n    #   - \"**/*.js\"\n    #   - \"**/*.jsx\"\n    #   - \"**/*.py\"\n    #   - \"**/*.sql\"\n    #   - \"**/*.SQL\"\n    #   - \"**/*.sh\"\n\njobs:\n  claude-review:\n    name: Claude Code Review\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      pull-requests: read\n      issues: write\n      id-token: write\n      actions: read\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n\n      - name: Run Claude Code Review\n        id: claude-review\n        uses: anthropics/claude-code-action@v1\n        with:\n          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}\n          track_progress: true # attach tracking comment\n          use_sticky_comment: true\n\n          plugin_marketplaces: https://github.com/NeoLabHQ/context-engineering-kit.git\n          plugins: \"code-review@context-engineering-kit\\ngit@context-engineering-kit\\ntdd@context-engineering-kit\\nsadd@context-engineering-kit\\nddd@context-engineering-kit\\nsdd@context-engineering-kit\\nkaizen@context-engineering-kit\"\n\n          prompt: '/code-review:review-pr ${{ github.repository }}/pull/${{ github.event.pull_request.number }} Note: The PR branch is already checked out in the current working directory.'\n\n          # Skill and Bash(gh pr comment:*) is required for review, the rest is optional, but recommended for better context and quality of the review.\n          claude_args: '--allowed-tools \"Skill,Bash,Glob,Grep,Read,Task,mcp__github_inline_comment__create_inline_comment,Bash(gh issue view:*),Bash(gh search:*),Bash(gh issue list:*),Bash(gh pr comment:*),Bash(gh pr edit:*),Bash(gh pr diff:*),Bash(gh pr view:*),Bash(gh pr list:*),Bash(gh api:*)\"'\n```\n\n### Git\n\nCommands and skills for streamlined Git operations including commits, pull request creation, and advanced workflow patterns.\n\n**How to install**\n\n```bash\n/plugin install git@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/git:commit` - Create well-formatted commits with conventional commit messages and emoji\n- `/git:create-pr` - Create pull requests using GitHub CLI with proper templates and formatting\n- `/git:analyze-issue` - Analyze a GitHub issue and create a detailed technical specification\n- `/git:load-issues` - Load all open issues from GitHub and save them as markdown files\n- `/git:create-worktree` - Create git worktrees for parallel development with automatic dependency installation\n- `/git:compare-worktrees` - Compare files and directories between git worktrees\n- `/git:merge-worktree` - Merge changes from worktrees with selective checkout, cherry-picking, or patch selection\n\n**Skills**\n\n- **worktrees** - Git worktree commands and workflow patterns for parallel branch development\n- **notes** - Git notes commands for attaching non-invasive metadata to commits\n\n### Test-Driven Development\n\nCommands and skills for test-driven development with anti-pattern detection.\n\n**How to install**\n\n```bash\n/plugin install tdd@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/tdd:write-tests` - Systematically add test coverage for local code changes using specialized review and development agents\n- `/tdd:fix-tests` - Fix failing tests after business logic changes or refactoring using orchestrated agents\n\n**Skills**\n\n- **test-driven-development** - Introduces TDD methodology, best practices, and skills for testing using subagents\n\n### Subagent-Driven Development\n\nExecution framework for competitive generation, multi-agent evaluation, and subagent-driven development with quality gates.\n\n**How to install**\n\n```bash\n/plugin install sadd@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/sadd:launch-sub-agent` - Launch focused sub-agents with intelligent model selection, Zero-shot CoT reasoning, and self-critique verification\n- `/sadd:do-and-judge` - Execute a single task with implementation sub-agent, independent judge verification, and automatic retry loop until passing\n- `/sadd:do-in-parallel` - Execute the same task across multiple independent targets in parallel with context isolation\n- `/sadd:do-in-steps` - Execute complex tasks through sequential sub-agent orchestration with automatic decomposition and context passing\n- `/sadd:do-competitively` - Execute tasks through competitive generation, multi-judge evaluation, and evidence-based synthesis to produce superior results\n- `/sadd:tree-of-thoughts` - Execute complex reasoning through systematic exploration of solution space, pruning unpromising branches, and synthesizing the best solution\n- `/sadd:judge-with-debate` - Evaluate solutions through iterative multi-judge debate with consensus building or disagreement reporting\n- `/sadd:judge` - Evaluate completed work using LLM-as-Judge with structured rubrics and evidence-based scoring\n\n**Skills**\n\n- **subagent-driven-development** - Dispatches fresh subagent for each task with code review between tasks, enabling fast iteration with quality gates\n- **multi-agent-patterns** - Design multi-agent architectures (supervisor, peer-to-peer, hierarchical) for complex tasks exceeding single-agent context limits\n\n### Domain-Driven Development\n\nCommands for setting up domain-driven development best practices focused on code quality.\n\n**How to install**\n\n```bash\n/plugin install ddd@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/ddd:setup-code-formating` - Sets up code formatting rules and style guidelines in CLAUDE.md\n\n**Skills**\n\n- **software-architecture** - Includes Clean Architecture, SOLID principles, and other design patterns\n\n### Spec-Driven Development\n\nComprehensive specification-driven development workflow using specialized agents.\n\n**How to install**\n\n```bash\n/plugin install sdd@NeoLabHQ/context-engineering-kit\n```\n\n#### Usage workflow\n\n```bash\n# start claude code\nclaude\n# setup project constitution\n/sdd:00-setup Use NestJS as backend framework, strictly follow SOLID principles and Clean Architecture.\n\n# Generate new feature specification\n/sdd:01-specify Add user authentication with OAuth\n\n# Plan feature development\n/sdd:02-plan\n\n# Create detailed implementation tasks\n/sdd:03-tasks\n\n# Execute feature implementation\n/sdd:04-implement\n\n# Document completed feature implementation\n/sdd:05-document Focus on API documentation\n\n```\n\n**Commands**\n\n- `/sdd:00-setup` - Create or update the project constitution from interactive or provided principle inputs\n- `/sdd:01-specify` - Create or update the feature specification from a natural language feature description\n- `/sdd:02-plan` - Plan the feature development based on the feature specification\n- `/sdd:03-tasks` - Create detailed implementation tasks from feature plans with complexity analysis\n- `/sdd:04-implement` - Execute feature implementation following task list with TDD approach and quality review\n- `/sdd:05-document` - Document completed feature implementation with API guides, architecture updates, and lessons learned\n- `/sdd:brainstorm` - Refines rough ideas into fully-formed designs through collaborative questioning and exploration\n- `/sdd:create-ideas` - Generate diverse ideas using creative sampling based on Verbalized Sampling technique. 2-3x diversity improvement while maintaining quality.\n\n**Agents**\n\n- **code-architect** - Designs system architecture and technical solutions\n- **code-explorer** - Navigates and understands existing codebase structure\n- **code-reviewer** - Reviews implementations against specifications and quality standards\n\n#### Theoretical Foundation\n\nThe SDD plugin implements a structured software development methodology combining proven frameworks:\n\n- [GitHub Spec Kit](https://github.com/github/spec-kit) - Specification-driven development templates and workflows\n- [OpenSpec](https://openspec.ai/) - Open specification format for software requirements\n- [BMad Method](https://github.com/bmadcode/BMAD-METHOD) - Structured approach to breaking down complex features into manageable tasks\n\nSupporting research and techniques:\n\n- [Specification-Driven Development](https://en.wikipedia.org/wiki/Design_by_contract) - Design by contract and formal specification approaches\n- [Verbalized Sampling](https://arxiv.org/abs/2510.01171) - Training-free prompting for diverse idea generation. Achieves **2-3x diversity improvement** while maintaining quality. Used for `create-ideas`, `brainstorm` and `plan` commands.\n\n### FPF - First Principles Framework\n\nStructured reasoning plugin implements the **[First Principles Framework (FPF)](https://github.com/ailev/FPF)** by Anatoly Levenchuk — a methodology for rigorous, auditable reasoning. The killer feature is turning the black box of AI reasoning into a transparent, evidence-backed audit trail. The plugin makes AI decision-making transparent and auditable. Instead of jumping to solutions, FPF enforces generating competing hypotheses, checking them logically, testing against evidence, then letting developers choose.\n\nKey principles:\n\n- **Transparent reasoning** - Full audit trail from hypothesis to decision\n- **Hypothesis-driven** - Generate 3-5 competing alternatives before evaluating\n- **Evidence-based** - Computed trust scores, not estimates\n- **Human-in-the-loop** - AI generates options; humans decide (Transformer Mandate)\n\nThe core cycle follows three modes of inference:\n\n1. **Abduction** — Generate competing hypotheses (don't anchor on the first idea).\n2. **Deduction** — Verify logic and constraints (does the idea make sense?).\n3. **Induction** — Gather evidence through tests or research (does the idea work in reality?).\n\nThen, audit for bias, decide, and document the rationale in a durable record.\n\n> **Warning:** This plugin loads the core FPF specification into context, which is large (~600k tokens). As a result, it is loaded into a subagent with the Sonnet[1m] model. However, such an agent can consume your token limit quickly.\n\n**How to install**\n\n```bash\n/plugin install fpf@NeoLabHQ/context-engineering-kit\n```\n\n#### Usage workflow\n\n```bash\n# Execute complete FPF cycle from hypothesis to decision\n/fpf:propose-hypotheses What caching strategy should we use?\n\n# The workflow will:\n# 1. Initialize context and .fpf/ directory\n# 2. Generate competing hypotheses\n# 3. Allow you to add your own alternatives\n# 4. Verify each against project constraints (parallel)\n# 5. Validate with evidence (parallel)\n# 6. Compute trust scores (parallel)\n# 7. Present comparison for your decision\n```\n\n**Commands**\n\n- `/fpf:propose-hypotheses` - Execute complete FPF cycle from hypothesis to decision (main workflow)\n- `/fpf:status` - Show current FPF phase and hypothesis counts\n- `/fpf:query` - Search knowledge base with assurance info\n- `/fpf:decay` - Manage evidence freshness (refresh/deprecate/waive)\n- `/fpf:actualize` - Reconcile knowledge with codebase changes\n- `/fpf:reset` - Archive session and return to IDLE\n\n**Agent**\n\n- **fpf-agent** - FPF reasoning specialist for hypothesis generation, verification, validation, and trust calculus using ADI cycle and knowledge layer progression\n\n#### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **ADI Cycle** | Abduction-Deduction-Induction reasoning loop |\n| **Knowledge Layers** | L0 (Conjecture) -> L1 (Substantiated) -> L2 (Corroborated) |\n| **WLNK** | Weakest Link principle: R_eff = min(evidence_scores) |\n| **Transformer Mandate** | AI generates options; humans decide |\n\n#### Theoretical Foundation\n\n- [FPF Repository](https://github.com/ailev/FPF) - Original methodology by Anatoly Levenchuk\n- [quint-code](https://github.com/m0n0x41d/quint-code) - Implementation that this plugin is based on\n\n### Kaizen\n\nContinuous improvement methodology inspired by Japanese philosophy and Agile practices.\n\n**How to install**\n\n```bash\n/plugin install kaizen@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/kaizen:analyse` - Auto-selects best Kaizen method (Gemba Walk, Value Stream, or Muda) for target analysis\n- `/kaizen:analyse-problem` - Comprehensive A3 one-page problem analysis with root cause and action plan\n- `/kaizen:why` - Iterative Five Whys root cause analysis drilling from symptoms to fundamentals\n- `/kaizen:root-cause-tracing` - Systematically traces bugs backward through call stack to identify source of invalid data or incorrect behavior\n- `/kaizen:cause-and-effect` - Systematic Fishbone analysis exploring problem causes across six categories\n- `/kaizen:plan-do-check-act` - Iterative PDCA cycle for systematic experimentation and continuous improvement\n\n**Skills**\n\n- **kaizen** - Continuous improvement methodology with multiple analysis techniques\n\n### Customaize Agent\n\nCommands and skills for creating and refining Claude Code extensions.\n\n**How to install**\n\n```bash\n/plugin install customaize-agent@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/customaize-agent:create-agent` - Comprehensive guide for creating Claude Code agents with proper structure, triggering conditions, system prompts, and validation\n- `/customaize-agent:create-command` - Interactive assistant for creating new Claude commands with proper structure and patterns\n- `/customaize-agent:create-workflow-command` - Create workflow commands that orchestrate multi-step execution through sub-agents with file-based task prompts\n- `/customaize-agent:create-skill` - Guide for creating effective skills with test-driven approach\n- `/customaize-agent:create-hook` - Create and configure git hooks with intelligent project analysis and automated testing\n- `/customaize-agent:test-skill` - Verify skills work under pressure and resist rationalization using RED-GREEN-REFACTOR cycle\n- `/customaize-agent:test-prompt` - Test any prompt (commands, hooks, skills, subagent instructions) using RED-GREEN-REFACTOR cycle with subagents\n- `/customaize-agent:apply-anthropic-skill-best-practices` - Comprehensive guide for skill development based on Anthropic's official best practices\n\n**Skills**\n\n- **prompt-engineering** - Well known prompt engineering techniques and patterns, includes Anthropic Best Practices and Agent Persuasion Principles\n- **context-engineering** - Deep understanding of context mechanics: attention budget, progressive disclosure, lost-in-middle effect, and practical optimization patterns\n- **agent-evaluation** - Evaluation frameworks for agent systems: LLM-as-Judge, multi-dimensional rubrics, bias mitigation, and the 95% performance finding\n\n### Docs\n\nCommands for project analysis and documentation management.\n\n**How to install**\n\n```bash\n/plugin install docs@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/docs:update-docs` - Update implementation documentation after completing development phases\n\n### Tech Stack\n\nCommands for setting up language and framework-specific best practices.\n\n**How to install**\n\n```bash\n/plugin install tech-stack@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/tech-stack:add-typescript-best-practices` - Setup TypeScript best practices and code style rules in CLAUDE.md\n\n### MCP\n\nCommands for integrating Model Context Protocol servers with your project. Each setup command supports configuration at multiple levels:\n\n- **Project level (shared)** - Configuration tracked in git, shared with team via `./CLAUDE.md`\n- **Project level (personal)** - Local configuration in `./CLAUDE.local.md`, not tracked in git\n- **User level (global)** - Configuration in `~/.claude/CLAUDE.md`, applies to all projects\n\n**How to install**\n\n```bash\n/plugin install mcp@NeoLabHQ/context-engineering-kit\n```\n\n**Commands**\n\n- `/mcp:setup-context7-mcp` - Guide for setup Context7 MCP server to load documentation for specific technologies\n- `/mcp:setup-serena-mcp` - Guide for setup Serena MCP server for semantic code retrieval and editing capabilities\n- `/mcp:setup-codemap-cli` - Guide for setup Codemap CLI for intelligent codebase visualization and navigation\n- `/mcp:setup-arxiv-mcp` - Guide for setup arXiv/Paper Search MCP server via Docker MCP for academic paper search and retrieval from multiple sources\n- `/mcp:build-mcp` - Guide for creating high-quality MCP servers that enable LLMs to interact with external services\n",
        "plugins/reflexion/README.md": "# Reflexion Plugin\n\nSelf-refinement framework that introduces feedback and refinement loops to improve output quality through iterative improvement, complexity triage, and verification.\n\nFocused on:\n\n- **Self-refinement** - Agents review and improve their own outputs\n- **Multi-agent review** - Specialized agents critique from different perspectives\n- **Iterative improvement** - Systematic loops that converge on higher quality\n- **Memory integration** - Lessons learned persist across interactions\n\n## Plugin Target\n\n- Decrease hallucinations - reflection usually allows you to get rid of hallucinations by verifying the output\n- Make output quality more predictable - same model usually produces more similar output after reflection, rather than after one shot prompt\n- Improve output quality - reflection usually allows you to improve the output by identifying areas that were missed or misunderstood in one shot prompt\n\n## Overview\n\nThe Reflexion plugin implements multiple scientifically-proven techniques for improving LLM outputs through self-reflection, critique, and memory updates. It enables Claude to evaluate its own work, identify weaknesses, and generate improved versions.\n\nPlugin is based on papers like [Self-Refine](https://arxiv.org/abs/2303.17651) and [Reflexion](https://arxiv.org/abs/2303.11366). These techniques improve the output of large language models by introducing feedback and refinement loops.\n\nThey are proven to **increase output quality by 8–21%** based on both automatic metrics and human preferences across seven diverse tasks, including dialogue generation, coding, and mathematical reasoning, when compared to standard one-step model outputs.\n\nOn top of that, the plugin is based on the [Agentic Context Engineering](https://arxiv.org/abs/2510.04618) paper that uses memory updates after reflection, and **consistently outperforms strong baselines by 10.6%** on agents.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install reflexion@NeoLabHQ/context-engineering-kit\n```\n\n```bash\n> claude \"implement user authentication\"\n# Claude implements user authentication, then you can ask it to reflect on implementation\n\n> /reflexion:reflect\n# It analyses results and suggests improvements\n# If issues are obvious, it will fix them immediately\n# If they are minor, it will suggest improvements that you can respond to\n> fix the issues\n\n# If you would like it to avoid issues that were found during reflection to appear again,\n# ask claude to extract resolution strategies and save the insights to project memory\n> /reflexion:memorize\n```\n\nAlternatively, you can use the `reflect` word in initial prompt:\n\n```bash\n> claude \"implement user authentication, then reflect\"\n# Claude implements user authentication,\n# then hook automatically runs /reflexion:reflect\n```\n\nIn order to use this hook, need to have `bun` installed. But for overall command it is not required.\n\n[Usage Examples](./usage-examples.md)\n\n## Automatic Reflection with Hooks\n\nThe plugin includes optional hooks that automatically trigger reflection when you include the word \"reflect\" in your prompt. This removes the need to manually run `/reflexion:reflect` after each task.\n\n### How It Works\n\n1. Include the word \"reflect\" anywhere in your prompt\n2. Claude completes your task\n3. The hook automatically triggers `/reflexion:reflect`\n4. Claude reviews and improves its work\n\n```bash\n# Automatic reflection triggered by \"reflect\" keyword\n> Fix the bug in auth.ts then reflect\n# Claude fixes the bug, then automatically reflects on the work\n\n> Implement the feature, reflect on your work\n# Same behavior - \"reflect\" triggers automatic reflection\n```\n\n**Important**: Only the exact word \"reflect\" triggers automatic reflection. Words like \"reflection\", \"reflective\", or \"reflects\" do not trigger it.\n\n## Commands Overview\n\n### /reflexion:reflect - Self-Refinement\n\nReflect on previous response and output, based on Self-refinement framework for iterative improvement with complexity triage and verification\n\n- Purpose - Review and improve previous response\n- Output - Refined output with improvements\n\n```bash\n/reflexion:reflect [\"focus area or threshold\"]\n```\n\n#### Arguments\n\nOptional areas to focus or confidence threshold to use, for example \"security\" or \"deep reflect if less than 90% confidence\"\n\n#### How It Works\n\n1. **Complexity Triage**: Automatically determines appropriate reflection depth\n   - Quick Path (5s): Simple tasks get fast verification\n   - Standard Path: Multi-file changes get full reflection\n   - Deep Path: Critical systems get comprehensive analysis\n\n2. **Self-Assessment**: Evaluates output against quality criteria\n   - Completeness check\n   - Quality assessment\n   - Correctness verification\n   - Fact-checking\n\n3. **Refinement Planning**: If improvements needed, generates specific plan\n   - Identifies issues\n   - Proposes solutions\n   - Prioritizes fixes\n\n4. **Implementation**: Produces refined output addressing identified issues\n\n**Confidence Thresholds**\n\nThe command uses confidence levels to determine if further iteration is needed:\n\n- **Quick Path**: No specific threshold (fast verification only)\n- **Standard Path**: Requires >70% confidence\n- **Deep Reflection**: Requires >90% confidence\n\nIf confidence threshold isn't met, the command will iterate automatically.\n\n#### Usage Examples\n\n```bash\n# Basic reflection on previous response\n> claude \"implement user authentication\"\n> /reflexion:reflect\n\n# Focused reflection on specific aspect\n> /reflexion:reflect security\n\n# After complex feature implementation\n> claude \"add payment processing with Stripe\"\n> /reflexion:reflect\n```\n\n#### Best practices\n\n- Reflect after significant work - Don't reflect on trivial tasks\n- Be specific - Provide context about what to focus on\n- Iterate when needed - Sometimes multiple reflection cycles are valuable\n- Capture learnings - Use `/reflexion:memorize` to preserve insights\n\n### /reflexion:critique - Multi-Perspective Critique\n\nMemorize insights from reflections and updates CLAUDE.md file with this knowledge. Curates insights from reflections and critiques into CLAUDE.md using Agentic Context Engineering\n\n- Purpose - Multi-perspective comprehensive review\n- Output - Structured feedback from multiple judges\n\n```bash\n/reflexion:critique [\"scope or focus area\"]\n```\n\n#### Arguments\n\nOptional file paths, commits, or context to review (defaults to recent changes)\n\n#### How It Works\n\n1. **Context Gathering**: Identifies scope of work to review\n2. **Parallel Review**: Spawns three specialized judge agents\n   - **Requirements Validator**: Checks alignment with original requirements\n   - **Solution Architect**: Evaluates technical approach and design\n   - **Code Quality Reviewer**: Assesses implementation quality\n3. **Cross-Review & Debate**: Judges review each other's findings and debate disagreements\n4. **Consensus Report**: Generates comprehensive report with actionable recommendations\n\n**Judge Scoring**\n\nEach judge provides a score out of 10:\n\n- **9-10**: Exceptional quality, minimal improvements needed\n- **7-8**: Good quality, minor improvements suggested\n- **5-6**: Acceptable quality, several improvements recommended\n- **3-4**: Below standards, significant rework needed\n- **1-2**: Major issues, substantial rework required\n\n#### Usage Examples\n\n```bash\n# Review recent work from conversation\n> /reflexion:critique\n\n# Review specific files\n> /reflexion:critique src/auth/*.ts\n\n# Review with security focus\n> /reflexion:critique --focus=security\n\n# Review a git commit range\n> /reflexion:critique HEAD~3..HEAD\n```\n\n#### Best practices\n\n- For important decisions - Use critique for architectural or design choices\n- Before major commits - Get multi-perspective review before committing\n- Learn from debates - Pay attention to different perspectives in the critique\n- Address all concerns - Don't cherry-pick feedback\n\n### /reflexion:memorize - Memory Updates\n\nComprehensive multi-perspective review using specialized judges with debate and consensus building\n\n- Purpose - Save insights to project memory\n- Output - Updated CLAUDE.md with learnings\n\n```bash\n/reflexion:memorize [\"source or scope\"]\n```\n\n#### Arguments\n\nOptional source specification (last, selection, chat:<id>) or --dry-run for preview\n\n#### How It Works\n\n1. **Context Harvesting**: Gathers insights from recent work\n   - Reflection outputs\n   - Critique findings\n   - Problem-solving patterns\n   - Failed approaches and lessons\n\n2. **Curation Process**: Transforms raw insights into structured knowledge\n   - Extracts key insights\n   - Categorizes by impact\n   - Applies curation rules (relevance, non-redundancy, actionability)\n   - Prevents context collapse\n\n3. **CLAUDE.md Updates**: Adds curated insights to appropriate sections\n   - Project Context\n   - Code Quality Standards\n   - Architecture Decisions\n   - Testing Strategies\n   - Development Guidelines\n   - Strategies and Hard Rules\n\n4. **Memory Validation**: Ensures quality of updates\n   - Coherence check\n   - Actionability test\n   - Consolidation review\n   - Evidence verification\n\n#### Usage Examples\n\n```bash\n# Memorize from most recent work\n> /reflexion:reflect\n> /reflexion:memorize\n\n# Preview without writing\n> /reflexion:memorize --dry-run\n\n# Limit insights\n> /reflexion:memorize --max=3\n\n# Target specific section\n> /reflexion:memorize --section=\"Testing Strategies\"\n\n# Memorize from critique\n> /reflexion:critique\n> /reflexion:memorize\n```\n\n#### Best practices\n\n- Regular memorization - Periodically save insights to CLAUDE.md\n- Review memory - Occasionally review CLAUDE.md to ensure it stays relevant\n- Curate carefully - Only memorize significant, reusable insights\n- Organize by topic - Keep CLAUDE.md well-structured\n\n## Theoretical Foundation\n\nThe Reflexion plugin is based on peer-reviewed research demonstrating **8-21% improvement in output quality** across diverse tasks:\n\n### Core Papers\n\n- **[Self-Refine](https://arxiv.org/abs/2303.17651)** - Iterative refinement where the model reviews and improves its own output\n- **[Reflexion](https://arxiv.org/abs/2303.11366)** - Self-reflection for autonomous agents with memory\n- **[Constitutional AI (CAI)](https://arxiv.org/abs/2212.08073)** - Critique based on principles and guidelines\n- **[LLM-as-a-Judge](https://arxiv.org/abs/2306.05685)** - Using LLMs to evaluate other LLM outputs\n- **[Multi-Agent Debate](https://arxiv.org/abs/2305.14325)** - Multiple models proposing and critiquing solutions\n- **[Agentic Context Engineering](https://arxiv.org/abs/2510.04618)** - Memory updates after reflection (**10.6% improvement**)\n\n### Additional Techniques\n\n- **[Chain-of-Verification (CoVe)](https://arxiv.org/abs/2309.11495)** - Generate, verify, revise cycle\n- **[Tree of Thoughts (ToT)](https://arxiv.org/abs/2305.10601)** - Multiple reasoning path exploration\n- **[Process Reward Models](https://arxiv.org/abs/2305.20050)** - Step-by-step evaluation\n",
        "plugins/code-review/README.md": "# Code Review Plugin\n\nComprehensive multi-agent code review system that examines code from multiple specialized perspectives to catch bugs, security issues, and quality problems before they reach production.\n\n## Focused on\n\n- **Multi-perspective analysis** - Six specialized agents examine code from different angles\n- **Early bug detection** - Catch bugs before commits and pull requests\n- **Security auditing** - Identify vulnerabilities and attack vectors\n- **Quality enforcement** - Maintain code standards and best practices\n\n## Overview\n\nThe Code Review plugin implements a multi-agent code review system where specialized AI agents examine code from different perspectives. Six agents work in parallel: Bug Hunter, Security Auditor, Test Coverage Reviewer, Code Quality Reviewer, Contracts Reviewer, and Historical Context Reviewer. This provides comprehensive, professional-grade code review before commits or pull requests.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install code-review@NeoLabHQ/context-engineering-kit\n\n# Review uncommitted local changes\n> /code-review:review-local-changes\n\n# Review a pull request\n> /code-review:review-pr #123\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Agent Architecture\n\n```\nCode Review Command\n        │\n        ├──> Bug Hunter (parallel)\n        ├──> Security Auditor (parallel)\n        ├──> Test Coverage Reviewer (parallel)\n        ├──> Code Quality Reviewer (parallel)\n        ├──> Contracts Reviewer (parallel)\n        └──> Historical Context Reviewer (parallel)\n                │\n                ▼\n        Aggregated Report\n```\n\n\n## Commands Overview\n\n### /code-review:review-local-changes - Local Changes Review\n\nReview uncommitted local changes using all specialized agents with code improvement suggestions.\n\n- Purpose - Comprehensive review before committing\n- Output - Structured report with findings by severity\n\n```bash\n/code-review:review-local-changes [\"review-aspects\"]\n```\n\n#### Arguments\n\nOptional review aspects to focus on (e.g., \"security\", \"bugs\", \"tests\")\n\n#### How It Works\n\n1. **Change Detection**: Identifies all uncommitted changes in the working directory\n   - Staged changes\n   - Unstaged modifications\n   - New files\n\n2. **Parallel Agent Analysis**: Spawns six specialized agents simultaneously\n   - Bug Hunter - Identifies potential bugs and edge cases\n   - Security Auditor - Finds security vulnerabilities\n   - Test Coverage Reviewer - Evaluates test coverage\n   - Code Quality Reviewer - Assesses code structure\n   - Contracts Reviewer - Reviews API contracts\n   - Historical Context Reviewer - Analyzes codebase patterns\n\n3. **Finding Aggregation**: Combines all agent reports\n   - Categorizes by severity (Critical, High, Medium, Low)\n   - Removes duplicates\n   - Adds file and line references\n\n4. **Report Generation**: Produces actionable report with prioritized findings\n\n#### Usage Examples\n\n```bash\n# Review all local changes\n> /code-review:review-local-changes\n\n# Focus on security aspects\n> /code-review:review-local-changes security\n\n# After implementing a feature\n> claude \"implement user authentication\"\n> /code-review:review-local-changes\n```\n\n#### Best Practices\n\n- Review before committing - Run review on local changes before `git commit`\n- Address critical issues first - Fix Critical and High priority findings immediately\n- Iterate after fixes - Run again to verify issues are resolved\n- Combine with reflexion - Use `/reflexion:memorize` to save patterns for future reference\n\n### /code-review:review-pr - Pull Request Review\n\nComprehensive pull request review using all specialized agents. Posts only high-confidence, high-value inline comments directly on PR lines - no overall review report.\n\n- Purpose - Review PR changes before merge with minimal noise\n- Output - Inline comments on specific lines (only issues that pass confidence/impact thresholds)\n\n```bash\n/code-review:review-pr [\"PR number or review-aspects\"]\n```\n\n#### Arguments\n\nPR number (e.g., #123, 123) and/or review aspects to focus on\n\n#### How It Works\n\n1. **PR Context Loading**: Fetches PR details and diff\n   - Changed files\n   - Commit messages\n   - PR description\n   - Base branch context\n\n2. **Parallel Agent Analysis**: Same six agents analyze the PR diff\n   - Each agent examines changes from their specialty perspective\n   - Considers PR context and commit messages\n\n3. **Confidence & Impact Scoring**: Each issue is scored on two dimensions\n   - **Confidence (0-100)**: How likely is this a real issue vs false positive?\n   - **Impact (0-100)**: How severe is the consequence if left unfixed?\n   - Progressive threshold: Critical issues (81-100 impact) need 50% confidence, Low issues (0-20 impact) need 95% confidence\n\n4. **Inline Comment Posting**: Only issues passing thresholds get posted\n   - Uses GitHub inline comments on specific PR lines\n   - Low impact issues (0-20) are never posted, even with high confidence\n\n\n#### Usage Examples\n\n```bash\n# Review PR by number\n> /code-review:review-pr #123\n\n# Review PR with focus on security\n> /code-review:review-pr #123 security\n\n# Review current branch's PR\n> /code-review:review-pr\n```\n\n\n\n## Review Agents\n\n### Bug Hunter\n\n**Focus**: Identifies potential bugs and edge cases through root cause analysis\n\n**What it catches:**\n- Null pointer exceptions\n- Off-by-one errors\n- Race conditions\n- Memory and resource leaks\n- Unhandled error cases\n- Logic errors\n\n### Security Auditor\n\n**Focus**: Security vulnerabilities and attack vectors\n\n**What it catches:**\n- SQL injection risks\n- XSS vulnerabilities\n- CSRF missing protection\n- Authentication/authorization bypasses\n- Exposed secrets or credentials\n- Insecure cryptography usage\n\n### Test Coverage Reviewer\n\n**Focus**: Test quality and coverage\n\n**What it evaluates:**\n- Test coverage gaps\n- Missing edge case tests\n- Integration test needs\n- Test quality and meaningfulness\n\n### Code Quality Reviewer\n\n**Focus**: Code structure and maintainability\n\n**What it evaluates:**\n- Code complexity\n- Naming conventions\n- Code duplication\n- Design patterns usage\n- Code smells\n\n### Contracts Reviewer\n\n**Focus**: API contracts and interfaces\n\n**What it reviews:**\n- API endpoint definitions\n- Request/response schemas\n- Breaking changes\n- Backward compatibility\n- Type safety\n\n### Historical Context Reviewer\n\n**Focus**: Changes relative to codebase history\n\n**What it analyzes:**\n- Consistency with existing patterns\n- Previous bug patterns\n- Architectural drift\n- Technical debt indicators\n\n## CI/CD Integration\n\n### GitHub Actions\n\nYou can use [anthropics/claude-code-action](https://github.com/marketplace/actions/claude-code-action-official) to run this plugin for PR reviews in github actions.\n\n1. Use `/install-github-app` command to setup workflow and secrets.\n2. Set content of `.github/workflows/claude-code-review.yml` to the following:\n\n```yaml\nname: Claude Code Review\n\non:\n  pull_request:\n    types:\n    - opened\n    - synchronize # remove if want to run only, when PR is opened\n    - ready_for_review\n    - reopened\n    # Uncomment to limit which files can trigger the workflow\n    # paths:\n    #   - \"**/*.ts\"\n    #   - \"**/*.tsx\"\n    #   - \"**/*.js\"\n    #   - \"**/*.jsx\"\n    #   - \"**/*.py\"\n    #   - \"**/*.sql\"\n    #   - \"**/*.SQL\"\n    #   - \"**/*.sh\"\n\njobs:\n  claude-review:\n    name: Claude Code Review\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      pull-requests: read\n      issues: write\n      id-token: write\n      actions: read\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 1\n      \n      - name: Run Claude Code Review\n        id: claude-review\n        uses: anthropics/claude-code-action@v1\n        with:\n          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}\n          track_progress: true # attach tracking comment\n          use_sticky_comment: true\n\n          plugin_marketplaces: https://github.com/NeoLabHQ/context-engineering-kit.git\n          plugins: \"code-review@context-engineering-kit\\ngit@context-engineering-kit\\ntdd@context-engineering-kit\\nsadd@context-engineering-kit\\nddd@context-engineering-kit\\nsdd@context-engineering-kit\\nkaizen@context-engineering-kit\"\n\n          prompt: '/code-review:review-pr ${{ github.repository }}/pull/${{ github.event.pull_request.number }} Note: The PR branch is already checked out in the current working directory.'\n\n          # Skill and Bash(gh pr comment:*) is required for review, the rest is optional, but recommended for better context and quality of the review.\n          claude_args: '--allowed-tools \"Skill,Bash,Glob,Grep,Read,Task,mcp__github_inline_comment__create_inline_comment,Bash(gh issue view:*),Bash(gh search:*),Bash(gh issue list:*),Bash(gh pr comment:*),Bash(gh pr edit:*),Bash(gh pr diff:*),Bash(gh pr view:*),Bash(gh pr list:*),Bash(gh api:*)\"'\n```\n\n## Output Formats\n\n### Local Changes Review (`review-local-changes`)\n\nProduces a structured report organized by severity:\n\n```markdown\n# Code Review Report\n\n## Executive Summary\n[Overview of changes and quality assessment]\n\n## Critical Issues (Must Fix)\n- [Issue with location and suggested fix]\n\n## High Priority (Should Fix)\n- [Issue with location and suggested fix]\n\n## Medium Priority (Consider Fixing)\n- [Issue with location]\n\n## Low Priority (Nice to Have)\n- [Issue with location]\n\n## Action Items\n- [ ] Critical action 1\n- [ ] High priority action 1\n```\n\n### PR Review (`review-pr`)\n\nPosts inline comments directly on PR lines - no overall report. Each comment follows this format:\n\n```markdown\n🔴/🟠/🟡 [Critical/High/Medium]: [Brief description]\n\n[Evidence: What was observed and consequence if unfixed]\n\n```suggestion\n[code fix if applicable]\n```\n```\n\n",
        "plugins/git/README.md": "# Git Plugin\n\nCommands for streamlined Git operations including commits and pull request creation with conventional commit messages.\n\n## Plugin Target\n\n- Maintain consistent commit history - Every commit follows conventional commit format\n- Reduce PR creation friction - Automated formatting, templates, and linking\n- Improve issue-to-code workflow - Clear technical specs from issue descriptions\n- Ensure team consistency - Standardized Git operations across the team\n\n## Overview\n\nThe Git plugin provides commands that automate and standardize Git workflows, ensuring consistent commit messages, proper PR formatting, and efficient issue management. It integrates GitHub best practices and conventional commits with emoji.\n\nMost commands require GitHub CLI (`gh`) for full functionality including creating PRs, loading issues, and setting labels/reviewers.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install git@NeoLabHQ/context-engineering-kit\n\n# Create a well-formatted commit\n> /git:commit\n\n# Create a pull request\n> /git:create-pr\n```\n\n#### Analyze Open GitHub issues\n\n```bash\n# Load all open issues\n> /git:load-issues\n\n# Analyze a GitHub issue\n> /git:analyze-issue 123\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands Overview\n\n### /git:commit - Conventional Commits\n\nCreate well-formatted commits with conventional commit messages and emoji.\n\n- Purpose - Standardize commit messages across the team\n- Output - Git commit with conventional format\n\n```bash\n/git:commit [flags]\n```\n\n#### Arguments\n\nOptional flags like `--no-verify` to skip pre-commit checks.\n\n#### How It Works\n\n1. **Change Analysis**: Reviews staged changes to understand what was modified\n2. **Type Detection**: Determines commit type (feat, fix, refactor, etc.)\n3. **Message Generation**: Creates descriptive commit message following conventions\n4. **Emoji Selection**: Adds appropriate emoji for the commit type\n5. **Commit Creation**: Executes git commit with formatted message\n\n**Commit Types with Emoji**\n\n| Emoji | Type | Description |\n|-------|------|-------------|\n| ✨ | `feat` | New feature |\n| 🐛 | `fix` | Bug fix |\n| 📝 | `docs` | Documentation changes |\n| 💄 | `style` | Code style changes (formatting) |\n| ♻️ | `refactor` | Code refactoring |\n| ⚡ | `perf` | Performance improvements |\n| ✅ | `test` | Adding or updating tests |\n| 🔧 | `chore` | Maintenance tasks |\n| 🔨 | `build` | Build system changes |\n| 👷 | `ci` | CI/CD changes |\n\n#### Usage Examples\n\n```bash\n# Basic commit after making changes\n> git add .\n> /git:commit\n\n# Skip pre-commit hooks\n> /git:commit --no-verify\n\n# After code review\n> /code-review:review-local-changes\n> /git:commit\n```\n\n#### Best Practices\n\n- Keep commits focused - One logical change per commit\n- Reference issues - Include issue numbers when applicable\n- Review before commit - Use code review commands first\n\n### /git:create-pr - Pull Request Creation\n\nCreate pull requests using GitHub CLI with proper templates and formatting.\n\n- Purpose - Streamline PR creation with consistent formatting\n- Output - GitHub pull request with template\n\n```bash\n/git:create-pr\n```\n\n#### Arguments\n\nNone required - interactive guide for PR creation.\n\n#### How It Works\n\n1. **Branch Detection**: Identifies current branch and target base branch\n2. **Template Search**: Looks for PR templates in `.github/` directory\n3. **Change Summary**: Analyzes commits to generate description\n4. **PR Creation**: Uses `gh pr create` with formatted content\n5. **Issue Linking**: Automatically links related issues\n\n#### Usage Examples\n\n```bash\n# Create PR for current branch\n> /git:create-pr\n\n# After completing feature\n> /git:commit\n> /git:create-pr\n\n# Full workflow\n> /git:analyze-issue 123\n> claude \"implement feature\"\n> /git:commit\n> /git:create-pr\n```\n\n#### Best Practices\n\n- Push branch first - Ensure branch is pushed to remote\n- Use descriptive titles - Clear summary of changes\n- Link issues - Reference related issues in description\n- Request reviewers - Add appropriate team members\n\n### /git:analyze-issue - Issue Analysis\n\nAnalyze a GitHub issue and create a detailed technical specification.\n\n- Purpose - Transform issues into actionable development tasks\n- Output - Technical specification with requirements\n\n```bash\n/git:analyze-issue <issue-number>\n```\n\n#### Arguments\n\nIssue number (e.g., 42) - required.\n\n#### How It Works\n\n1. **Issue Fetching**: Retrieves issue details from GitHub\n2. **Requirements Extraction**: Identifies user stories and acceptance criteria\n3. **Technical Analysis**: Determines APIs, data models, and dependencies\n4. **Task Breakdown**: Creates actionable subtasks\n5. **Complexity Assessment**: Estimates implementation effort\n\n#### Usage Examples\n\n```bash\n# Analyze issue before starting work\n> /git:analyze-issue 123\n\n# Use with SDD workflow\n> /git:analyze-issue 123\n> /sdd:01-specify\n\n# Plan sprint work\n> /git:load-issues\n> /git:analyze-issue 45\n> /git:analyze-issue 67\n```\n\n#### Best Practices\n\n- Analyze before coding - Understand requirements first\n- Check issue completeness - Request clarification if needed\n- Note dependencies - Identify related issues or PRs\n- Use for planning - Helps estimate and prioritize work\n\n### /git:load-issues - Load Open Issues\n\nLoad all open issues from GitHub and save them as markdown files.\n\n- Purpose - Bulk import issues for planning and analysis\n- Output - Markdown files for each open issue\n\n```bash\n/git:load-issues\n```\n\n#### Arguments\n\nNone required - loads all open issues automatically.\n\n#### How It Works\n\n1. **Issue Retrieval**: Fetches all open issues from repository\n2. **Content Extraction**: Parses issue title, body, labels, and metadata\n3. **File Generation**: Creates markdown file for each issue\n4. **Organization**: Structures files in designated directory\n\n#### Usage Examples\n\n```bash\n# Load all issues for sprint planning\n> /git:load-issues\n\n# Then analyze specific issues\n> /git:analyze-issue 123\n```\n\n#### Best Practices\n\n- Use for sprint planning - Get overview of all open work\n- Combine with analysis - Analyze high-priority issues in detail\n- Regular updates - Reload periodically to stay current\n\n### /git:attach-review-to-pr - PR Review Comments\n\nAdd line-specific review comments to pull requests using GitHub CLI API.\n\n- Purpose - Attach detailed code review feedback to PRs\n- Output - Review comments on specific lines\n\n```bash\n/git:attach-review-to-pr [pr-number]\n```\n\n#### Arguments\n\nPR number or URL (optional - can work with current branch).\n\n#### Usage Examples\n\n```bash\n# Add review comments to PR\n> /git:attach-review-to-pr 456\n\n# After code review\n> /code-review:review-pr 456\n> /git:attach-review-to-pr 456\n```\n\n### /git:create-worktree - Create Worktrees\n\nCreate and setup git worktrees for parallel development with automatic dependency installation.\n\n- Purpose - Enable parallel branch development without stashing or context switching\n- Output - New worktree with dependencies installed\n\n```bash\n/git:create-worktree <name> | --list\n```\n\n#### Arguments\n\n- `<name>` - Descriptive name for the worktree (e.g., \"refactor auth system\", \"fix login bug\")\n- `--list` - Show existing worktrees\n\n#### How It Works\n\n1. **Type Detection**: Auto-detects branch type from name (feature, fix, hotfix, refactor, etc.)\n2. **Branch Resolution**: Creates or tracks existing local/remote branch\n3. **Worktree Creation**: Creates sibling directory with pattern `../<project>-<name>`\n4. **Dependency Installation**: Detects project type and runs appropriate install command\n\n**Supported Project Types**: Node.js (npm/yarn/pnpm/bun), Python (pip/poetry), Rust (cargo), Go, Ruby, PHP\n\n#### Usage Examples\n\n```bash\n# Create feature worktree (default type)\n> /git:create-worktree auth system\n# Branch: feature/auth-system → ../myproject-auth-system\n\n# Create fix worktree\n> /git:create-worktree fix login error\n# Branch: fix/login-error → ../myproject-login-error\n\n# Create hotfix while feature work continues\n> /git:create-worktree hotfix critical bug\n\n# List existing worktrees\n> /git:create-worktree --list\n```\n\n### /git:compare-worktrees - Compare Worktrees\n\nCompare files and directories between git worktrees or worktree and current branch.\n\n- Purpose - Understand differences across branches/worktrees before merging\n- Output - Diff output with clear headers and statistics\n\n```bash\n/git:compare-worktrees [paths...] [--stat]\n```\n\n#### Arguments\n\n- `<paths>` - File(s) or directory(ies) to compare\n- `<worktree>` - Worktree path or branch name to compare\n- `--stat` - Show summary statistics only\n\n#### Usage Examples\n\n```bash\n# Compare specific file\n> /git:compare-worktrees src/app.js\n\n# Compare multiple paths\n> /git:compare-worktrees src/app.js src/utils/ package.json\n\n# Compare entire directory\n> /git:compare-worktrees src/\n\n# Get summary statistics\n> /git:compare-worktrees --stat\n\n# Interactive mode (lists worktrees)\n> /git:compare-worktrees\n```\n\n### /git:merge-worktree - Merge from Worktrees\n\nMerge changes from worktrees into current branch with selective file checkout, cherry-picking, interactive patch selection, or manual merge.\n\n- Purpose - Selectively merge changes without full branch merges\n- Output - Merged files with optional cleanup\n\n```bash\n/git:merge-worktree [path|commit] [--from <worktree>] [--patch] [--interactive]\n```\n\n#### Arguments\n\n- `<path>` - File or directory to merge\n- `<commit>` - Commit name to cherry-pick\n- `--from <worktree>` - Source worktree path\n- `--patch` / `-p` - Interactive patch selection mode\n- `--interactive` - Guided mode\n\n#### Merge Strategies\n\n| Strategy | Use When | Command Pattern |\n|----------|----------|-----------------|\n| **Selective File** | Need complete file(s) from another branch | `git checkout <branch> -- <path>` |\n| **Interactive Patch** | Need specific changes within a file | `git checkout -p <branch> -- <path>` |\n| **Cherry-Pick Selective** | Need a commit but not all its changes | `git cherry-pick --no-commit` + selective staging |\n| **Manual Merge** | Full branch merge with control | `git merge --no-commit` + selective staging |\n| **Multi-Source** | Combining files from multiple branches | Multiple `git checkout <branch> -- <path>` |\n\n#### Usage Examples\n\n```bash\n# Merge single file\n> /git:merge-worktree src/app.js --from ../project-feature\n\n# Interactive patch selection (select specific hunks)\n> /git:merge-worktree src/utils.js --patch\n\n# Cherry-pick specific commit\n> /git:merge-worktree abc1234\n\n# Full guided mode\n> /git:merge-worktree --interactive\n```\n\n## Skills Overview\n\n### worktrees - Parallel Branch Development\n\nUse when working on multiple branches simultaneously, context switching without stashing, reviewing PRs while developing, testing in isolation, or comparing implementations across branches.\n\n- Purpose - Provide git worktree commands and workflow patterns for parallel development\n- Core Principle - One worktree per active branch; switch contexts by changing directories\n\n**Key Concepts**\n\n| Concept | Description |\n|---------|-------------|\n| Main worktree | Original working directory from `git clone` or `git init` |\n| Linked worktree | Additional directories created with `git worktree add` |\n| Shared `.git` | All worktrees share same Git object database (no duplication) |\n| Branch lock | Each branch can only be checked out in ONE worktree at a time |\n\n**Quick Reference**\n\n| Task | Command |\n|------|---------|\n| Create worktree (existing branch) | `git worktree add <path> <branch>` |\n| Create worktree (new branch) | `git worktree add -b <branch> <path>` |\n| List all worktrees | `git worktree list` |\n| Remove worktree | `git worktree remove <path>` |\n\n**Common Workflows**\n\n- **Feature + Hotfix in Parallel** - Create worktree for hotfix while feature work continues\n- **PR Review While Working** - Create temporary worktree to review PRs without stashing\n- **Compare Implementations** - Create worktrees for different versions to diff side-by-side\n- **Long-Running Tasks** - Run tests in isolated worktree while continuing development\n\n### notes - Commit Metadata Annotations\n\nUse when adding metadata to commits without changing history, tracking review status, test results, code quality annotations, or supplementing commit messages post-hoc.\n\n- Purpose - Attach non-invasive metadata to Git objects without modifying commit history\n- Core Principle - Add information to commits after creation without rewriting history\n\n**Key Concepts**\n\n| Concept | Description |\n|---------|-------------|\n| Notes ref | Storage location, default `refs/notes/commits` |\n| Non-invasive | Notes never modify SHA of original object |\n| Namespaces | Use `--ref` for different note categories (reviews, testing, audit) |\n| Display | Notes appear in `git log` and `git show` output |\n\n**Quick Reference**\n\n| Task | Command |\n|------|---------|\n| Add note | `git notes add -m \"message\" <sha>` |\n| View note | `git notes show <sha>` |\n| Append to note | `git notes append -m \"message\" <sha>` |\n| Use namespace | `git notes --ref=<name> <command>` |\n| Push notes | `git push origin refs/notes/<name>` |\n\n**Common Use Cases**\n\n- **Code Review Tracking** - Mark commits as reviewed with reviewer attribution\n- **Test Results Annotation** - Record test pass/fail status and coverage\n- **Audit Trail** - Attach security review or compliance information\n- **Sharing Notes** - Push/fetch notes to share metadata with team\n\n## Conventional Commit Format\n\nThe plugin follows the [conventional commits specification](https://www.conventionalcommits.org/):\n\n```\n<type>(<scope>): <description>\n\n[optional body]\n\n[optional footer(s)]\n```\n\n### Example Commit Messages\n\n**Feature Commit**\n```\n✨ feat(auth): add OAuth2 authentication\n\nImplement OAuth2 with Google and GitHub providers\n- Add OAuthController for callback handling\n- Implement token exchange and validation\n- Add user profile synchronization\n\nCloses #123\n```\n\n**Bug Fix Commit**\n```\n🐛 fix(cart): prevent duplicate items in shopping cart\n\nFix race condition when adding items concurrently\n- Add distributed lock for cart operations\n- Implement idempotency key validation\n\nFixes #456\n```\n\n**Refactoring Commit**\n```\n♻️ refactor(order): extract order processing logic\n\nImprove code organization and testability\n- Extract OrderProcessor from OrderController\n- Implement strategy pattern for order types\n\nRelated to #789\n```\n",
        "plugins/tdd/README.md": "# Test-Driven Development (TDD) Plugin\n\nA disciplined approach to software development that ensures every line of production code is validated by tests written first. Introduces TDD methodology, anti-pattern detection, and orchestrated test coverage using specialized agents.\n\nFocused on:\n\n- **Test-first development** - Write tests before implementation, ensuring every feature is verified\n- **Red-Green-Refactor cycle** - Systematic approach that builds confidence through failing tests\n- **Anti-pattern detection** - Identifies common testing mistakes like mock abuse and test-only methods\n- **Agent-orchestrated coverage** - Parallel test writing using specialized subagents for complex changes\n\n## Plugin Target\n\n- **Prevent regressions** - Every change is backed by tests that catch future breaks\n- **Improve design quality** - Hard-to-test code reveals design problems early\n- **Build confidence** - Watching tests fail then pass proves they actually test something\n- **Accelerate development** - TDD is faster than debugging untested code in production\n\n## Overview\n\nThe TDD plugin implements Kent Beck's Test-Driven Development methodology, proven over two decades to produce higher-quality, more maintainable software. The core principle is simple but transformative: **write the test first, watch it fail, then write minimal code to pass**.\n\nThe plugin is based on foundational works including Kent Beck's *Test-Driven Development: By Example* and the extensive research on TDD effectiveness.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install tdd@NeoLabHQ/context-engineering-kit\n\n> claude \"Use TDD skill to implement email validation for user registration\"\n\n# Manually make some changes that cause test failures\n\n# Fix failing tests\n> /tdd:fix-tests\n```\n\n### After Implementation\n\nIf you implemented a new feature but have not written tests, you can use the `write-tests` command to cover it.\n\n```bash\n> claude \"implement email validation for user registration\"\n\n# Write tests after you made changes\n> /tdd:write-tests\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands Overview\n\n### /tdd:write-tests - Cover Local Changes with Tests\n\nSystematically add test coverage for all local code changes using specialized review and development agents.\n\n- Purpose - Ensure comprehensive test coverage for new or modified code\n- Output - New test files covering all critical business logic\n\n```bash\n/tdd:write-tests [\"focus area or modules\"]\n```\n\n#### Arguments\n\nOptional focus area specification. Defaults to all uncommitted changes. If everything is committed, covers the latest commit.\n\n#### How It Works\n\n1. **Preparation Phase**\n   - Discovers test infrastructure (test commands, coverage tools)\n   - Runs full test suite to establish baseline\n   - Reads project conventions and patterns\n\n2. **Analysis Phase** (parallel)\n   - Verifies single test execution capability\n   - Analyzes local changes via `git status` or latest commit\n   - Filters non-code files and identifies logic changes\n   - Assesses complexity to determine workflow path\n\n3. **Test Writing Phase**\n   - **Simple changes** (single file, straightforward logic): Writes tests directly\n   - **Complex changes** (multiple files or complex logic): Orchestrates specialized agents\n     - Coverage reviewer agents analyze each file for test needs\n     - Developer agents write comprehensive tests in parallel\n     - Verification agents confirm coverage completeness\n\n4. **Verification Phase**\n   - Runs full test suite\n   - Generates coverage report if available\n   - Iterates on gaps until all critical logic is covered\n\n**Complexity Decision:**\n- 1 simple file: Write tests directly\n- 2+ files or complex logic: Orchestrate parallel agents\n\n#### Usage Examples\n\n```bash\n# Cover all uncommitted changes\n> /tdd:write-tests\n\n# Focus on specific module\n> /tdd:write-tests Focus on payment processing edge cases\n\n# Cover authentication changes\n> /tdd:write-tests authentication module\n\n# Focus on error handling\n> /tdd:write-tests Focus on error paths and validations\n```\n\n#### Best practices\n\n- **Run before committing** - Ensure all changes have test coverage before commit\n- **Be specific** - Provide focus areas for more targeted test generation\n- **Review generated tests** - Verify tests actually test behavior, not implementation\n- **Iterate on gaps** - Re-run if coverage reviewer identifies missing cases\n- **Prioritize critical logic** - Not every line needs 100% coverage, focus on business logic\n\n### /tdd:fix-tests - Fix Failing Tests\n\nSystematically fix all failing tests after business logic changes or refactoring using orchestrated agents.\n\n- Purpose - Update tests to match current business logic after changes\n- Output - Fixed tests that pass while preserving test intent\n\n```bash\n/tdd:fix-tests [\"focus area or modules\"]\n```\n\n#### Arguments\n\nOptional specification of which tests or modules to focus on. Defaults to all failing tests.\n\n#### How It Works\n\n1. **Discovery Phase**\n   - Reads test infrastructure configuration\n   - Runs full test suite to identify all failures\n   - Groups failing tests by file for parallel processing\n\n2. **Analysis Phase**\n   - Verifies ability to run individual test files\n   - Understands why tests are failing (outdated expectations vs. bugs)\n\n3. **Fixing Phase**\n   - **Simple changes**: Fixes tests directly\n   - **Complex changes**: Launches parallel developer agents per failing test file\n   - Each agent:\n     - Reads test file and TDD skill\n     - Analyzes failure type (expectations, setup, or actual bug)\n     - Fixes test while preserving intent\n     - Iterates until test passes\n\n4. **Verification Phase**\n   - Runs full test suite after all agents complete\n   - Iterates on any remaining failures\n   - Continues until 100% pass rate\n\n**Agent Decision Logic:**\n- Outdated test expectations: Fix assertions\n- Broken test setup/mocks: Fix setup code\n- Actual business logic bug (rare): Fix logic\n\n#### Usage Examples\n\n```bash\n# Fix all failing tests\n> /tdd:fix-tests\n\n# Focus on specific test files\n> /tdd:fix-tests user authentication tests\n\n# Fix tests in specific module\n> /tdd:fix-tests payment module tests\n\n# Focus on integration tests\n> /tdd:fix-tests integration tests only\n```\n\n#### Best practices\n\n- **Preserve test intent** - Fix assertions, not the behavior being tested\n- **Avoid changing business logic** - Unless you discover an actual bug\n- **Understand before fixing** - Know why the test fails before changing it\n- **Run full suite** - Ensure fixes don't break other tests\n- **Review agent changes** - Verify fixes maintain test quality\n\n## Skills Overview\n\n### test-driven-development - TDD Methodology Skill\n\nComprehensive TDD methodology and anti-pattern detection guide that ensures rigorous test-first development.\n\n#### The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrite code before the test? Delete it. Start over. No exceptions.\n\n#### Red-Green-Refactor Cycle\n\n```\n    ┌─────────────────┐\n    │                 │\n    │   RED           │\n    │   Write         │◄────────────────────┐\n    │   failing test  │                     │\n    │                 │                     │\n    └────────┬────────┘                     │\n             │                              │\n             │ Verify fails correctly       │\n             ▼                              │\n    ┌─────────────────┐                     │\n    │                 │                     │\n    │   GREEN         │                     │\n    │   Write minimal │                     │\n    │   code to pass  │                     │\n    │                 │                     │\n    └────────┬────────┘                     │\n             │                              │\n             │ Verify all tests pass        │\n             ▼                              │\n    ┌─────────────────┐                     │\n    │                 │                     │\n    │   REFACTOR      │─────────────────────┘\n    │   Clean up      │    Next test\n    │   (stay green)  │\n    │                 │\n    └─────────────────┘\n```\n\n**RED - Write Failing Test:**\n- Write one minimal test showing expected behavior\n- Clear, descriptive test name\n- Tests real code, not mocks\n\n**Verify RED:**\n- Run test and confirm it fails\n- Failure should be for expected reason (feature missing, not typo)\n- Test passes immediately? Fix the test, you're testing existing behavior\n\n**GREEN - Minimal Code:**\n- Write simplest code to pass the test\n- No extra features, no over-engineering\n- YAGNI (You Aren't Gonna Need It)\n\n**Verify GREEN:**\n- All tests pass\n- No errors or warnings\n- Other tests still green\n\n**REFACTOR:**\n- Remove duplication\n- Improve names\n- Extract helpers\n- Keep tests passing throughout\n\n#### Testing Anti-Patterns\n\nThe skill includes comprehensive anti-pattern detection:\n\n| Anti-Pattern | Problem | Fix |\n|--------------|---------|-----|\n| Testing mock behavior | Verifies mock works, not code | Test real component or unmock |\n| Test-only methods | Pollutes production class | Move to test utilities |\n| Mocking without understanding | Breaks behavior test depends on | Understand dependencies first |\n| Incomplete mocks | Silent failures downstream | Mirror real API completely |\n| Tests as afterthought | Proves nothing about correctness | Follow TDD - tests first |\n\n#### Common Rationalizations (Rejected)\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Already manually tested\" | Ad-hoc does not equal systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Unverified code is technical debt. |\n| \"TDD is dogmatic\" | TDD is pragmatic. Faster than debugging in production. |\n\n## Foundation\n\nThe TDD plugin is based on decades of research and practice demonstrating significant improvements in code quality and development efficiency:\n\n### Foundational Works\n\n- **[Test-Driven Development: By Example](https://www.oreilly.com/library/view/test-driven-development/0321146530/)** by Kent Beck - The definitive guide to TDD methodology, introducing Red-Green-Refactor\n- **[Refactoring: Improving the Design of Existing Code](https://martinfowler.com/books/refactoring.html)** by Martin Fowler - Companion work on safe code transformation under test coverage\n",
        "plugins/sadd/README.md": "# SADD Plugin (Subagent-Driven Development)\n\nExecution framework that dispatches fresh subagents for each task with quality gates between iterations, enabling fast parallel development while maintaining code quality.\n\nFocused on:\n\n- **Fresh context per task** - Each subagent starts clean without context pollution from previous tasks\n- **Quality gates** - Code review between tasks catches issues early before they compound\n- **Parallel execution** - Independent tasks run concurrently for faster completion\n- **Sequential execution** - Dependent tasks execute in order with review checkpoints\n\n## Plugin Target\n\n- Prevent context pollution - Fresh subagents avoid accumulated confusion from long sessions\n- Catch issues early - Code review between tasks prevents bugs from compounding\n- Faster iteration - Parallel execution of independent tasks saves time\n- Maintain quality at scale - Quality gates ensure standards are met on every task\n\n## Overview\n\nThe SADD plugin provides skills and commands for executing work through coordinated subagents. Instead of executing all tasks in a single long session where context accumulates and quality degrades, SADD dispatches fresh subagents with quality gates.\n\n**Core capabilities:**\n\n- **Sequential/Parallel Execution** - Execute implementation plans task-by-task with code review gates\n- **Competitive Execution** - Generate multiple solutions, evaluate with judges, synthesize best elements\n- **Work Evaluation** - Assess completed work using LLM-as-Judge with structured rubrics\n\nThis approach solves the \"context pollution\" problem - when an agent accumulates confusion, outdated assumptions, or implementation drift over long sessions. Each fresh subagent starts clean, implements its specific scope, and reports back for quality validation.\n\nThe plugin supports multiple execution strategies based on task characteristics, all with built-in quality gates.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install sadd@NeoLabHQ/context-engineering-kit\n\n# Use competitive execution for high-stakes tasks\n/do-competitively \"Design and implement authentication middleware with JWT support\"\n\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands Overview\n\n### launch-sub-agent\n\nThis command launches a focused sub-agent to execute the provided task. Analyze the task to intelligently select the optimal model and agent configuration, then dispatch a sub-agent with Zero-shot Chain-of-Thought reasoning at the beginning and mandatory self-critique verification at the end. It implements the **Supervisor/Orchestrator pattern** from multi-agent architectures where you (the orchestrator) dispatch focused sub-agents with isolated context. The primary benefit is **context isolation** - each sub-agent operates in a clean context window focused on its specific task without accumulated context pollution.\n\n#### Usage\n\n```bash\n`/launch-sub-agent Design a caching strategy for our API that handles 10k requests/second`\n```\n\nAgent output:\n\n```markdown\n**Analysis:**\n- Task type: Architecture / design\n- Complexity: High (performance requirements, system design)\n- Output size: Medium (design document)\n- Domain match: software-architect\n\n**Selection:** Opus + software-architect agent\n\n**Dispatch:** Task tool with Opus model, software-architect prompt, CoT prefix, critique suffix\n```\n\n#### Advanced Options\n\n**Explicit Model Override**\n\nWhen you know the appropriate model tier, override automatic selection:\n\n```bash\n/launch-sub-agent \"Task description\" --model opus|sonnet|haiku\n```\n\n**Explicit Agent Selection**\n\nForce use of a specific specialized agent:\n\n```bash\n/launch-sub-agent \"Task description\" --agent developer|researcher|software-architect|tech-writer|business-analyst|code-explorer|tech-lead|security-auditor\n```\n\n**Output Location**\n\nSpecify where results should be written:\n\n```bash\n/launch-sub-agent \"Task description\" --output path/to/output.md\n```\n\n**Combined Options**\n\n```bash\n/launch-sub-agent \"Implement the payment flow\" --agent developer --model opus --output src/services/payment.ts\n```\n\n#### Core design principles\n\n- **Context isolation**: Sub-agents operate with fresh context, preventing confirmation bias and attention scarcity\n- **Intelligent model selection**: Match model capability to task complexity for optimal quality/cost tradeoff\n- **Specialized agent routing**: Domain experts handle domain-specific tasks\n- **Zero-shot CoT**: Systematic reasoning at task start improves quality by 20-60%\n- **Self-critique**: Verification loop catches 40-60% of issues before delivery\n\n#### When to use this command\n\n- Tasks that benefit from fresh, focused context\n- Tasks where model selection matters (quality vs. cost tradeoffs)\n- Delegating work while maintaining quality gates\n- Single, well-defined tasks with clear deliverables\n\n#### When NOT to use\n\n- Simple tasks you can complete directly (overhead not justified)\n- Tasks requiring conversation history or accumulated session context\n- Exploratory work where scope is undefined\n\n#### Theoretical Foundation\n\n**Zero-shot Chain-of-Thought** (Kojima et al., 2022)\n\n- Adding \"Let's think step by step\" improves reasoning by 20-60%\n- Explicit reasoning steps reduce errors and catch edge cases\n- Reference: [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n\n**Constitutional AI / Self-Critique** (Bai et al., 2022)\n\n- Self-critique loops catch 40-60% of issues before delivery\n- Verification questions force explicit quality checking\n- Reference: [Constitutional AI](https://arxiv.org/abs/2212.08073)\n\n**Multi-Agent Context Isolation** (Multi-agent architecture patterns)\n\n- Fresh context prevents accumulated confusion and attention scarcity\n- Focused tasks produce better results than context-polluted sessions\n- Supervisor pattern enables quality gates between delegated work\n\n### /do-and-judge\n\nExecute a single task with implementation sub-agent, independent judge verification, and automatic retry loop until passing or max retries exceeded.\n\n- Purpose - Execute a single task with quality verification and feedback-driven iteration\n- Pattern - Implement → Judge → Iterate (if needed) → Report\n- Output - Verified implementation with judge scores and improvement suggestions\n- Quality - Two-layer verification: self-critique (internal) + LLM-as-a-judge (external)\n- Iteration - Retry with judge feedback until passing (≥4/5.0) or max retries (2)\n\n#### Pattern: Single-Task Execution with Judge Verification\n\n```\nPhase 1: Task Analysis and Model Selection\n         Complexity + Risk + Scope → Model Selection\n                     │\nPhase 2: Dispatch Implementation Agent\n         [CoT Prefix] + [Task Body] + [Self-Critique Suffix]\n                     │\nPhase 3: Dispatch Judge Agent\n         Independent verification with structured criteria\n                     │\nPhase 4: Parse Verdict and Iterate\n         ├─ PASS (≥4) → Report Success\n         └─ FAIL (<4) → Retry with Feedback (max 2)\n                            └─ Return to Phase 3\n                     │\nPhase 5: Final Report or Escalation\n         Success summary OR escalate to user after max retries\n```\n\n#### Usage\n\n```bash\n# Basic usage\n/do-and-judge \"Refactor the UserService class to use dependency injection\"\n\n# Complex implementation\n/do-and-judge \"Implement rate limiting middleware with configurable limits per endpoint\"\n\n# Architecture change\n/do-and-judge \"Extract validation logic from UserController into separate UserValidator class\"\n```\n\n#### When to Use\n\n**Good use cases:**\n\n- Single, well-defined tasks that benefit from quality verification\n- Changes that should meet a quality threshold before shipping\n- Tasks where feedback-driven iteration improves results\n- Any implementation where you want an independent quality gate\n\n**Do NOT use when:**\n\n- Multi-step tasks with dependencies → use `/do-in-steps` instead\n- Independent parallel tasks → use `/do-in-parallel` instead\n- High-stakes tasks needing multiple approaches → use `/do-competitively` instead\n- Simple tasks where verification overhead isn't justified → use `/launch-sub-agent` instead\n\n#### Quality Enhancement Techniques\n\n| Phase | Technique | Benefit |\n|-------|-----------|---------|\n| **Phase 2** | Zero-shot CoT | Systematic reasoning improves quality by 20-60% |\n| **Phase 2** | Self-Critique | Implementation agents verify own work before submission |\n| **Phase 3** | LLM-as-a-Judge | Independent judge catches blind spots self-critique misses |\n| **Phase 4** | Feedback Loop | Retry with specific issues until passing or max retries |\n\n#### Theoretical Foundation\n\n- **[Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)** (Wei et al., 2022) - Step-by-step reasoning improves accuracy\n- **[Constitutional AI](https://arxiv.org/abs/2212.08073)** (Bai et al., 2022) - Self-critique loops before submission\n- **[LLM-as-a-Judge](https://arxiv.org/abs/2306.05685)** (Zheng et al., 2023) - Independent evaluation with structured rubrics\n\n### /do-in-parallel\n\nExecute tasks in parallel across multiple targets with intelligent model selection, independence validation, and quality-focused prompting.\n\n- Purpose - Execute the same task across multiple independent targets in parallel\n- Pattern - Supervisor/Orchestrator with parallel dispatch and context isolation\n- Output - Multiple solutions, one per target, with aggregated summary\n- Quality - Enhanced with Zero-shot CoT, Constitutional AI self-critique, and intelligent model selection\n- Efficiency - Dramatic time savings through concurrent execution of independent work\n\n#### Pattern: Parallel Orchestration with Independence Validation\n\nThis command implements a six-phase parallel orchestration pattern:\n\n```\nPhase 1: Parse Input and Identify Targets\n                     │\nPhase 2: Task Analysis with Zero-shot CoT\n         ┌─ Task Type Identification ─────────────────┐\n         │ (transformation, analysis, documentation)  │\n         ├─ Per-Target Complexity Assessment ─────────┤\n         │ (high/medium/low)                          │\n         ├─ Independence Validation ──────────────────┤\n         │ ⚠️ CRITICAL: Must pass before proceeding   │\n         └────────────────────────────────────────────┘\n                     │\nPhase 3: Model and Agent Selection\n         Is task COMPLEX? → Opus\n         Is task SIMPLE/MECHANICAL? → Haiku\n         Otherwise → Opus (default for balanced work)\n                     │\nPhase 4: Construct Per-Target Prompts\n         [CoT Prefix] + [Task Body] + [Self-Critique Suffix]\n         (Same structure for ALL agents, customized per target)\n                     │\nPhase 5: Parallel Dispatch (ALL agents in SINGLE response)\n         ┌─ Agent 1 (target A) ─┐\n         ├─ Agent 2 (target B) ─┼─→ Concurrent Execution\n         └─ Agent 3 (target C) ─┘\n                     │\nPhase 6: Collect and Summarize Results\n         Aggregate outcomes, report failures, suggest remediation\n```\n\n#### Usage\n\n```bash\n# Inferred targets from task description\n/do-in-parallel \"Apply consistent logging format to src/handlers/user.ts, src/handlers/order.ts, and src/handlers/product.ts\"\n```\n\n#### Advanced Options\n\n```bash\n# Basic usage with file targets\n/do-in-parallel \"Simplify error handling to use early returns\" \\\n  --files \"src/services/user.ts,src/services/order.ts,src/services/payment.ts\"\n\n# With named targets\n/do-in-parallel \"Generate unit tests achieving 80% coverage\" \\\n  --targets \"UserService,OrderService,PaymentService\"\n\n# With model override\n/do-in-parallel \"Security audit for injection vulnerabilities\" \\\n  --files \"src/db/queries.ts,src/api/search.ts\" \\\n  --model opus\n```\n\n#### When to Use\n\n**Good use cases:**\n\n- Same operation across multiple files (refactoring, formatting)\n- Independent transformations (each file stands alone)\n- Batch documentation generation (API docs per module)\n- Parallel analysis tasks (security audit per component)\n- Multi-file code generation (tests per service)\n\n**Do NOT use when:**\n\n- Only one target → use `/launch-sub-agent` instead\n- Targets have dependencies → use `/do-in-steps` instead\n- Tasks require sequential ordering → use `/do-in-steps` instead\n- Shared state needed between executions → use `/do-in-steps` instead\n- Quality-critical tasks needing comparison → use `/do-competitively` instead\n\n#### Context Isolation Best Practices\n\n- **Minimal context**: Each sub-agent receives only what it needs for its target\n- **No cross-references**: Don't tell Agent A about Agent B's target\n- **Let them discover**: Sub-agents read files to understand local patterns\n- **File system as truth**: Changes are coordinated through the filesystem\n\n#### Theoretical Foundation\n\n**Zero-shot Chain-of-Thought** (Kojima et al., 2022)\n\n- \"Let's think step by step\" improves reasoning by 20-60%\n- Applied to each parallel agent independently\n- Reference: [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n\n**Constitutional AI / Self-Critique** (Bai et al., 2022)\n\n- Each agent self-verifies before completing\n- Catches issues without coordinator overhead\n- Reference: [Constitutional AI](https://arxiv.org/abs/2212.08073)\n\n**Multi-Agent Context Isolation** (Multi-agent architecture patterns)\n\n- Fresh context prevents accumulated confusion\n- Focused tasks produce better results than context-polluted sessions\n- Reference: [Multi-Agent Debate](https://arxiv.org/abs/2305.14325) (Du et al., 2023)\n\n### /do-in-steps\n\nExecute complex tasks through sequential sub-agent orchestration with intelligent model selection and LLM-as-a-judge verification.\n\n- Purpose - Execute dependent tasks sequentially where each step builds on previous outputs\n- Pattern - Supervisor/Orchestrator with sequential dispatch, judge verification, and iteration loop\n- Output - Comprehensive report with all step results, judge scores, and integration summary\n- Quality - Two-layer verification: self-critique (internal) + LLM-as-a-judge (external) with iteration until passing\n- Key Benefit - Prevents context pollution while ensuring quality through independent verification\n\n#### Pattern: Sequential Orchestration with Judge Verification\n\n```\nPhase 1: Task Analysis and Decomposition\n         Task → Identify Dependencies → Define Step Boundaries\n                     │\nPhase 2: Model Selection\n         For each step: Assess Complexity + Scope + Risk → Select Model\n                     │\nPhase 3: Sequential Execution with Judge Verification\n         ┌─────────────────────────────────────────────────────────────┐\n         │ For each Step N:                                           │\n         │   Implementer → Self-Critique → Judge → Parse Verdict      │\n         │        ▲                                    │               │\n         │        │                                    ▼               │\n         │        │                         PASS (≥3.5)? → Next Step  │\n         │        │                         FAIL? → Retry (max 2)     │\n         │        └──────── feedback ───────┘         or Escalate     │\n         └─────────────────────────────────────────────────────────────┘\n         Step 1 → Judge ✓ → Step 2 → Judge ✓ → Step 3 → Judge ✓ → ...\n                     │\nPhase 4: Final Summary and Report\n         Aggregate results, judge scores, files modified, decisions made\n```\n\n#### Usage\n\n```bash\n# Interface change with consumer updates\n/do-in-steps \"Change return type of UserService.getUser() from User to UserDTO and update all consumers\"\n\n# Feature addition across layers\n/do-in-steps \"Add email notification capability to the order processing system\"\n\n# Multi-file refactoring with breaking changes\n/do-in-steps \"Rename 'userId' to 'accountId' across the codebase - affects interfaces, implementations, and callers\"\n```\n\n#### When to Use\n\n**Good use cases:**\n\n- Changes that cascade through multiple files/layers\n- Interface modifications with consumers to update\n- Feature additions spanning multiple components\n- Refactoring with dependency chains\n- Any task where \"Step N depends on Step N-1\"\n\n**Do NOT use when:**\n\n- Independent tasks that could run in parallel → use `/do-in-parallel`\n- Single-step tasks → use `/launch-sub-agent`\n- Tasks needing exploration before commitment → use `/tree-of-thoughts`\n- High-stakes tasks needing multiple approaches → use `/do-competitively`\n\n#### Quality Enhancement Techniques\n\n| Phase | Technique | Benefit |\n|-------|-----------|---------|\n| **Phase 3** | Self-Critique | Implementation agents verify own work before submission, catching 40-60% of issues |\n| **Phase 3** | LLM-as-a-Judge | Independent judge verifies each step, catching blind spots self-critique misses |\n| **Phase 3** | Iteration Loop | Failed steps retry with judge feedback until passing (max 2 retries) or escalate |\n| **Phase 3** | Context Passing | Each step receives summary of previous outputs without full context pollution |\n\n#### Theoretical Foundation\n\n- **[Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)** (Wei et al., 2022) - Step-by-step reasoning improves accuracy\n- **[Constitutional AI](https://arxiv.org/abs/2212.08073)** (Bai et al., 2022) - Self-critique loops before submission\n- **[LLM-as-a-Judge](https://arxiv.org/abs/2306.05685)** (Zheng et al., 2023) - Independent evaluation with structured rubrics\n- **[Multi-Agent Debate](https://arxiv.org/abs/2305.14325)** (Du et al., 2023) - Fresh context prevents accumulated confusion\n\n### /do-competitively - Competitive Multi-Agent Synthesis\n\nExecute tasks through competitive generation, multi-judge evaluation, and evidence-based synthesis to produce superior results.\n\n- Purpose - Generate multiple solutions competitively, evaluate with independent judges, synthesize best elements\n- Pattern - Generate-Critique-Synthesize (GCS) with self-critique, verification loops, and adaptive strategy selection\n- Output - Superior solution combining best elements from all candidates\n- Quality - Enhanced with Constitutional AI self-critique, Chain of Verification, and intelligent strategy selection\n- Efficiency - 15-20% average cost savings through adaptive strategy (polish clear winners, redesign failures)\n\n#### Pattern: Generate-Critique-Synthesize (GCS)\n\nThis command implements a four-phase adaptive competitive orchestration pattern with quality enhancement loops:\n\n```\nPhase 1: Competitive Generation with Self-Critique\n         ┌─ Agent 1 → Draft → Self-Critique → Revise → Solution A ─┐\nTask ───┼─ Agent 2 → Draft → Self-Critique → Revise → Solution B ─┼─┐\n         └─ Agent 3 → Draft → Self-Critique → Revise → Solution C ─┘ │\n                                                                  │\nPhase 2: Multi-Judge Evaluation with Verification                │\n         ┌─ Judge 1 → Evaluate → Verify → Revise → Report A ─┐  │\n         ├─ Judge 2 → Evaluate → Verify → Revise → Report B ─┼──┤\n         └─ Judge 3 → Evaluate → Verify → Revise → Report C ─┘  │\n                                                                  │\nPhase 2.5: Adaptive Strategy Selection                           │\n         Analyze Consensus ──────────────────────────────────────┤\n                ├─ Clear Winner? → SELECT_AND_POLISH             │\n                ├─ All Flawed (<3.0)? → REDESIGN (return Phase 1)│\n                └─ Split Decision? → FULL_SYNTHESIS              │\n                                          │                       │\nPhase 3: Evidence-Based Synthesis        │                       │\n         (Only if FULL_SYNTHESIS)         │                       │\n         Synthesizer ─────────────────────┴───────────────────────┴─→ Final Solution\n```\n\n#### Usage\n\n```bash\n# Basic usage\n/do-competitively <task-description>\n\n# With explicit output specification\n/do-competitively \"Create authentication middleware\" --output \"src/middleware/auth.ts\"\n\n# With specific evaluation criteria\n/do-competitively \"Design user schema\" --criteria \"scalability,security,developer-experience\"\n```\n\n#### When to Use\n\nUse this command when:\n\n- **Quality is critical** - Multiple perspectives catch flaws single agents miss\n- **Novel/ambiguous tasks** - No clear \"right answer\", exploration needed\n- **High-stakes decisions** - Architecture choices, API design, critical algorithms\n- **Learning/evaluation** - Compare approaches to understand trade-offs\n- **Avoiding local optima** - Competitive generation explores solution space better\n\nDo NOT use when:\n\n- Simple, well-defined tasks with obvious solutions\n- Time-sensitive changes\n- Trivial bug fixes or typos\n- Tasks with only one viable approach\n\n#### Quality Enhancement Techniques\n\nTechniques that were used to enhance the quality of the competitive execution pattern.\n\n| Phase         | Technique                       | Benefit                                                                                                              |\n| --------------- | --------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\n| **Phase 1**   | Constitutional AI Self-Critique | Generators review and fix their own solutions before submission, catching 40-60% of issues                           |\n| **Phase 2**   | Chain of Verification           | Judges verify their evaluations with structured questions, improving calibration and reducing bias                   |\n| **Phase 2.5** | Adaptive Strategy Selection     | Orchestrator parses structured judge outputs (VOTE+SCORES) to select optimal strategy, saving 15-20% cost on average |\n| **Phase 3**   | Evidence-Based Synthesis        | Combines proven best elements rather than creating new solutions (only when needed)                                  |\n\n#### Theoretical Foundation\n\nThe competitive execution pattern combines insights from:\n\n**Academic Research:**\n\n- [Multi-Agent Debate](https://arxiv.org/abs/2305.14325) (Du et al., 2023) - Diverse perspectives improve reasoning\n- [Self-Consistency](https://arxiv.org/abs/2203.11171) (Wang et al., 2022) - Multiple reasoning paths improve reliability\n- [Tree of Thoughts](https://arxiv.org/abs/2305.10601) (Yao et al., 2023) - Exploration of solution branches before commitment\n- [Constitutional AI](https://arxiv.org/abs/2212.08073) (Bai et al., 2022) - Self-critique loops catch 40-60% of issues before review\n- [Chain-of-Verification](https://arxiv.org/abs/2309.11495) (Dhuliawala et al., 2023) - Structured verification reduces bias\n- [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) (Zheng et al., 2023) - Structured evaluation rubrics\n\n**Engineering Practices:**\n\n- Design Studio Method - Parallel design, critique, synthesis\n- Spike Solutions (XP/Agile) - Explore approaches, combine best\n- A/B Testing - Compare alternatives with clear metrics\n- Ensemble Methods - Combining multiple models improves performance\n\n### /tree-of-thoughts - Tree of Thoughts with Adaptive Strategy\n\nExecute complex reasoning tasks through systematic exploration of solution space, pruning unpromising branches, expanding viable approaches, and synthesizing the best solution.\n\n- Purpose - Explore multiple solution paths before committing to full implementation\n- Pattern - Tree of Thoughts (ToT) with adaptive strategy selection\n- Output - Superior solution combining systematic exploration with evidence-based synthesis\n- Quality - Enhanced with probability estimates, multi-stage evaluation, and adaptive strategy\n- Efficiency - 15-20% average cost savings through adaptive strategy (polish clear winners, redesign failures)\n\n#### Pattern: Tree of Thoughts (ToT)\n\nThis command implements a six-phase systematic reasoning pattern with adaptive strategy selection:\n\n```\nPhase 1: Exploration (Propose Approaches)\n         ┌─ Agent A → Proposals with probabilities ─┐\nTask ───┼─ Agent B → Proposals with probabilities ─┼─┐\n         └─ Agent C → Proposals with probabilities ─┘ │\n                                                       │\nPhase 2: Pruning (Vote for Best 3)                    │\n         ┌─ Judge 1 → Votes + Rationale ─┐            │\n         ├─ Judge 2 → Votes + Rationale ─┼────────────┤\n         └─ Judge 3 → Votes + Rationale ─┘            │\n                 │                                     │\n                 ├─→ Select Top 3 Proposals            │\n                 │                                     │\nPhase 3: Expansion (Develop Full Solutions)           │\n         ┌─ Agent A → Solution A ─┐                   │\n         ├─ Agent B → Solution B ─┼───────────────────┤\n         └─ Agent C → Solution C ─┘                   │\n                                                       │\nPhase 4: Evaluation (Judge Full Solutions)            │\n         ┌─ Judge 1 → Report 1 ─┐                     │\n         ├─ Judge 2 → Report 2 ─┼─────────────────────┤\n         └─ Judge 3 → Report 3 ─┘                     │\n                                                       │\nPhase 4.5: Adaptive Strategy Selection                │\n         Analyze Consensus ───────────────────────────┤\n                ├─ Clear Winner? → SELECT_AND_POLISH  │\n                ├─ All Flawed (<3.0)? → REDESIGN      │\n                └─ Split Decision? → FULL_SYNTHESIS   │\n                                         │             │\nPhase 5: Synthesis (Only if FULL_SYNTHESIS)           │\n         Synthesizer ────────────────────┴─────────────┴─→ Final Solution\n```\n\n#### Usage\n\n```bash\n# Basic usage\n/tree-of-thoughts <task-description>\n\n# With explicit output specification\n/tree-of-thoughts \"Design authentication middleware\" --output \"specs/auth.md\"\n\n# With specific evaluation criteria\n/tree-of-thoughts \"Design caching strategy\" --criteria \"performance,memory-efficiency,simplicity\"\n```\n\n#### When to Use\n\n✅ **Use ToT when:**\n\n- Solution space is large and poorly understood\n- Wrong approach chosen early would waste significant effort\n- Task has multiple valid approaches with different trade-offs\n- Quality is more important than speed\n- You need to explore before committing\n\n❌ **Don't use ToT when:**\n\n- Solution approach is obvious\n- Task is simple or well-defined\n- Speed matters more than exploration\n- Only one reasonable approach exists\n\n#### Quality Enhancement Techniques\n\n| Phase         | Technique                   | Benefit                                                                         |\n| --------------- | ----------------------------- | --------------------------------------------------------------------------------- |\n| **Phase 1**   | Probabilistic Sampling      | Explorers generate approaches with probability estimates, encouraging diversity |\n| **Phase 2**   | Multi-Judge Pruning         | Independent judges vote on top 3 proposals, reducing groupthink                 |\n| **Phase 3**   | Feedback-Aware Expansion    | Expanders address concerns raised during pruning                                |\n| **Phase 4**   | Chain of Verification       | Judges verify evaluations with structured questions, reducing bias              |\n| **Phase 4.5** | Adaptive Strategy Selection | Orchestrator parses structured outputs to select optimal strategy               |\n| **Phase 5**   | Evidence-Based Synthesis    | Combines proven best elements rather than creating new solutions                |\n\n#### Theoretical Foundation\n\nBased on:\n\n- **[Tree of Thoughts](https://arxiv.org/abs/2305.10601)** (Yao et al., 2023) - Systematic exploration and pruning\n- **[Self-Consistency](https://arxiv.org/abs/2203.11171)** (Wang et al., 2023) - Multiple reasoning paths\n- **[Constitutional AI](https://arxiv.org/abs/2212.08073)** (Bai et al., 2022) - Critique and refinement\n- **[LLM-as-Judge](https://arxiv.org/abs/2306.05685)** (Zheng et al., 2023) - Multi-perspective evaluation\n- **[Chain-of-Verification](https://arxiv.org/abs/2309.11495)** (Dhuliawala et al., 2023) - Structured verification reduces bias\n\n### /judge-with-debate - Multi-Agent Debate Evaluation\n\nEvaluate solutions through iterative multi-judge debate where independent judges analyze, challenge each other's assessments, and refine evaluations until reaching consensus or maximum rounds.\n\n- Purpose - Rigorous evaluation through adversarial critique and evidence-based argumentation\n- Pattern - Independent Analysis → Iterative Debate → Consensus or Disagreement Report\n- Output - Consensus evaluation report with averaged scores and debate summary, or disagreement report flagging unresolved issues\n- Quality - Enhanced through multi-perspective analysis, evidence-based argumentation, and iterative refinement\n- Efficiency - Early termination when consensus reached or judges stop converging\n\n#### Pattern: Debate-Based Evaluation\n\nThis command implements iterative multi-judge debate with filesystem-based communication:\n\n```\nPhase 1: Independent Analysis\n         ┌─ Judge 1 → report.1.md ─┐\nSolution ┼─ Judge 2 → report.2.md ─┼─┐\n         └─ Judge 3 → report.3.md ─┘ │\n                                     │\nPhase 2: Debate Round (iterative)   │\n    Each judge reads others' reports │\n         ↓                           │\n    Argue + Defend + Challenge       │\n         ↓                           │\n    Revise if convinced ─────────────┤\n         ↓                           │\n    Check consensus (≤0.5 overall,   │\n                     ≤1.0 per-criterion)\n         ├─ Yes → Consensus Report   │\n         └─ No → Next Round ─────────┘\n                (max 3 rounds)\n```\n\n#### Usage\n\n```bash\n# Basic usage\n/judge-with-debate --solution \"src/api/users.ts\" --task \"REST API implementation\"\n\n# With specific criteria\n/judge-with-debate \\\n  --solution \"src/api/users.ts\" \\\n  --task \"Implement REST API for user management\" \\\n  --criteria \"correctness:30,design:25,security:20,performance:15,docs:10\" \\\n  --output \"evaluation/\"\n\n# Evaluating design documents\n/judge-with-debate \\\n  --solution \"specs/architecture.md\" \\\n  --task \"System architecture design\" \\\n  --criteria \"completeness:30,feasibility:25,scalability:20,clarity:15,maintainability:10\"\n```\n\n#### When to Use\n\n✅ **Use debate when:**\n\n- High-stakes decisions requiring rigorous evaluation\n- Subjective criteria where perspectives differ legitimately\n- Complex solutions with many evaluation dimensions\n- Quality is more important than speed/cost\n- Initial judge assessments show significant disagreement\n- You need defensible, evidence-based evaluation\n\n❌ **Skip debate when:**\n\n- Objective pass/fail criteria (use simple validation)\n- Trivial solutions (single judge sufficient)\n- Time/cost constraints prohibit multiple rounds\n- Clear rubrics leave little room for interpretation\n- Evaluation criteria are purely mechanical (linting, formatting)\n\n#### Quality Enhancement Techniques\n\n| Phase         | Technique                | Benefit                                                                                            |\n| --------------- | -------------------------- | ---------------------------------------------------------------------------------------------------- |\n| **Phase 1**   | Chain of Verification    | Judges generate verification questions and self-critique before submitting initial assessment      |\n| **Phase 1**   | Evidence Requirement     | All scores must be supported by specific quotes from solution                                      |\n| **Phase 2**   | Filesystem Communication | Judges read each other's reports directly, orchestrator never mediates (prevents context overflow) |\n| **Phase 2**   | Structured Argumentation | Judges must defend positions AND challenge others with counter-evidence                            |\n| **Phase 2**   | Explicit Revision        | Judges must document what changed their mind or why they maintained their position                 |\n| **Consensus** | Adaptive Termination     | Stops early if consensus reached, max rounds hit, or judges stop converging                        |\n\n#### Process Flow\n\n**Step 1: Independent Analysis**\n\n- 3 judges analyze solution in parallel\n- Each writes comprehensive report to `report.[1|2|3].md`\n- Includes per-criterion scores, evidence, overall assessment\n\n**Step 2: Check Consensus**\n\n- Extract all scores from reports\n- Consensus if: overall scores within 0.5 AND all criterion scores within 1.0\n- If achieved → generate consensus report and complete\n\n**Step 3: Debate Round** (if no consensus, max 3 rounds)\n\n- Each judge reads their own report + others' reports from filesystem\n- Identifies disagreements (>1 point gap on any criterion)\n- Defends their ratings with evidence\n- Challenges others' ratings with counter-evidence\n- Revises scores if convinced by others' arguments\n- Appends \"Debate Round N\" section to their own report\n\n**Step 4: Repeat** until consensus, max rounds, or lack of convergence\n\n**Step 5: Final Report**\n\n- If consensus: averaged scores, strengths/weaknesses, debate summary\n- If no consensus: disagreement report with flag for human review\n\n#### Theoretical Foundation\n\nBased on:\n\n- **[Multi-Agent Debate](https://arxiv.org/abs/2305.14325)** (Du et al., 2023) - Adversarial critique improves reasoning accuracy\n- **[LLM-as-a-Judge](https://arxiv.org/abs/2306.05685)** (Zheng et al., 2023) - Pairwise comparison and structured evaluation\n- **[Chain-of-Verification](https://arxiv.org/abs/2309.11495)** (Dhuliawala et al., 2023) - Self-verification reduces bias\n- **Deliberative Democracy** - Argumentation and evidence-based consensus building\n\n**Key Insight**: Debate forces judges to explicitly defend positions with evidence and consider counter-arguments, reducing individual bias and improving calibration.\n\n### /judge - Single-Agent Work Evaluation\n\nEvaluate completed work using LLM-as-Judge with structured rubrics, context isolation, and evidence-based scoring.\n\n- Purpose - Assess quality of work produced earlier in conversation with isolated context\n- Pattern - Context Extraction → Judge Sub-Agent → Validation → Report\n- Output - Evaluation report with weighted scores, evidence citations, and actionable improvements\n- Quality - Enhanced with Chain-of-Thought scoring, self-verification, and bias mitigation\n- Efficiency - Single focused judge for fast evaluation without multi-agent overhead\n\n#### Pattern: LLM-as-Judge with Context Isolation\n\nThis command implements a three-phase evaluation pattern:\n\n```\nPhase 1: Context Extraction\n         Review conversation history\n         Identify work to evaluate\n         Extract: Original task, output, files, constraints\n                     │\nPhase 2: Judge Sub-Agent (Fresh Context)\n         ┌─────────────────────────────────────────┐\n         │ Judge receives ONLY extracted context   │\n         │ (prevents confirmation bias)            │\n         │                                         │\n         │ For each criterion:                     │\n         │   1. Review evidence                    │\n         │   2. Write justification                │\n         │   3. Assign score (1-5)                 │\n         │   4. Self-verify with questions         │\n         │   5. Adjust if needed                   │\n         └─────────────────────────────────────────┘\n                     │\nPhase 3: Validation & Report\n         Verify scores in valid range (1-5)\n         Check justification has evidence\n         Confirm weighted total calculation\n         Present verdict with recommendations\n```\n\n#### Usage\n\n```bash\n> Write new controller for the user model\n\n# Evaluate completed work\n/judge\n\n# Evaluate with specific focus\n/judge code quality and test coverage\n\n# Evaluate security considerations\n/judge security implications\n\n# Evaluate requirements alignment\n/judge requirements fulfillment\n\n# Evaluate documentation completeness\n/judge documentation\n```\n\n#### When to Use\n\n✅ **Use single judge when:**\n\n- Quick quality check needed\n- Work is straightforward with clear criteria\n- Speed/cost matters more than multi-perspective analysis\n- Evaluation is formative (guiding improvements), not summative\n- Low-to-medium stakes decisions\n\n❌ **Use judge-with-debate instead when:**\n\n- High-stakes decisions requiring rigorous evaluation\n- Subjective criteria where perspectives differ legitimately\n- Complex solutions with many evaluation dimensions\n- You need defensible, consensus-based evaluation\n\n#### Default Evaluation Criteria\n\n| Criterion             | Weight | What It Measures                                                  |\n| ----------------------- | -------- | ------------------------------------------------------------------- |\n| Instruction Following | 0.30   | Does output fulfill original request? All requirements addressed? |\n| Output Completeness   | 0.25   | All components covered? Appropriate depth? No gaps?               |\n| Solution Quality      | 0.25   | Sound approach? Best practices? No correctness issues?            |\n| Reasoning Quality     | 0.10   | Clear decision-making? Appropriate methods used?                  |\n| Response Coherence    | 0.10   | Well-structured? Easy to understand? Professional?                |\n\n#### Scoring Interpretation\n\n| Score Range | Verdict           | Recommendation              |\n| ------------- | ------------------- | ----------------------------- |\n| 4.50 - 5.00 | EXCELLENT         | Ready as-is                 |\n| 4.00 - 4.49 | GOOD              | Minor improvements optional |\n| 3.50 - 3.99 | ACCEPTABLE        | Improvements recommended    |\n| 3.00 - 3.49 | NEEDS IMPROVEMENT | Address issues before use   |\n| 1.00 - 2.99 | INSUFFICIENT      | Significant rework needed   |\n\n#### Quality Enhancement Techniques\n\n| Technique                | Benefit                                                                                |\n| -------------------------- | ---------------------------------------------------------------------------------------- |\n| Context Isolation        | Judge receives only extracted context, preventing confirmation bias from session state |\n| Chain-of-Thought Scoring | Justification BEFORE score improves reliability by 15-25%                              |\n| Evidence Requirement     | Every score requires specific citations (file paths, line numbers, quotes)             |\n| Self-Verification        | Judge generates verification questions and documents adjustments                       |\n| Bias Mitigation          | Explicit warnings against length bias, verbosity bias, and authority bias              |\n\n#### Theoretical Foundation\n\nBased on:\n\n- **[LLM-as-a-Judge](https://arxiv.org/abs/2306.05685)** (Zheng et al., 2023) - Structured evaluation rubrics with calibrated scoring\n- **[Chain of Thought Prompting](https://arxiv.org/abs/2201.11903)** (Wei et al., 2022) - Reasoning before conclusion improves accuracy\n- **[Constitutional AI](https://arxiv.org/abs/2212.08073)** (Bai et al., 2022) - Self-critique and verification loops\n- **[Inference-Time Scaling of Verification](https://arxiv.org/abs/2601.15808)** (Wan et al., 2026) - Rubric-guided verification with test-time self-evolution and iterative feedback refinement\n\n## Skills Overview\n\n### subagent-driven-development - Task Execution with Quality Gates\n\nUse when executing implementation plans with independent tasks or facing multiple independent issues that can be investigated without shared state - dispatches fresh subagent for each task with code review between tasks.\n\n- Purpose - Execute plans through coordinated subagents with quality checkpoints\n- Output - Completed implementation with all tasks verified and reviewed\n\n#### When to Use SADD\n\n**Use SADD when:**\n\n- You have an implementation plan with 3+ distinct tasks\n- Tasks can be executed independently (or in clear sequence)\n- You need quality gates between implementation steps\n- Context would accumulate over a long implementation session\n- Multiple unrelated failures need parallel investigation\n- Different subsystems need changes that do not conflict\n\n**Use regular development when:**\n\n- Single task or simple change\n- Tasks are tightly coupled and need shared understanding\n- Exploratory work where scope is undefined\n- You need human-in-the-loop feedback between every step\n\n#### Usage\n\n```bash\n\n# Use the skill when you have an implementation plan\n> I have a plan in specs/feature/plan.md with 5 tasks. Please use subagent-driven development to implement it.\n\n# Or when facing multiple independent issues\n> We have 4 failing test files in different areas. Use subagent-driven development to fix them in parallel.\n```\n\n## How It Works\n\nSADD supports four execution strategies based on task characteristics:\n\n**Sequential Execution**\n\nFor dependent tasks that must be executed in order:\n\n```\nPlan Load → Task 1 → Review → Task 2 → Review → Task 3 → ... → Final Review → Complete\n            ↓        ↓        ↓        ↓        ↓\n         Subagent  Quality  Subagent  Quality  Subagent\n                    Gate              Gate\n```\n\n**Parallel Execution**\n\nFor independent tasks that can run concurrently:\n\n```\n                  ┌─ Task 1 (Subagent) ─┐\nPlan Load → Batch ┼─ Task 2 (Subagent) ─┼─ Batch Review → Next Batch → Final Review → Complete\n                  └─ Task 3 (Subagent) ─┘\n```\n\n**Parallel Investigation**\n\nSpecial case for fixing multiple unrelated failures:\n\n```\n                        ┌─ Domain 1 (Agent) ─┐\nIdentify Domains → Fix ─┼─ Domain 2 (Agent) ─┼─ Review & Integrate → Complete\n                        └─ Domain 3 (Agent) ─┘\n```\n\n### Multi-Agent Analysis Orchestration\n\nCommands often orchestrate multiple agents to provide comprehensive analysis:\n\n**Sequential Analysis:**\n\n```\nCommand → Agent 1 → Agent 2 → Agent 3 → Synthesized Result\n```\n\n**Parallel Analysis:**\n\n```\n         ┌─ Agent 1 ─┐\nCommand ─┼─ Agent 2 ─┼─ Synthesized Result\n         └─ Agent 3 ─┘\n```\n\n**Debate Pattern:**\n\n```\nCommand → Agent 1 ─┐\n       → Agent 2 ─┼─ Debate → Consensus → Result\n       → Agent 3 ─┘\n```\n\n## Processes\n\n### Sequential Execution Process\n\n1. **Load Plan**: Read plan file and create TodoWrite with all tasks\n2. **Execute Task with Subagent**: For each task, dispatch a fresh subagent:\n\n   - Subagent reads the specific task from the plan\n   - Implements exactly what the task specifies\n   - Writes tests following project conventions\n   - Verifies implementation works\n   - Commits the work\n   - Reports back with summary\n3. **Review Subagent's Work**: Dispatch a code-reviewer subagent:\n\n   - Reviews what was implemented against the plan\n   - Returns: Strengths, Issues (Critical/Important/Minor), Assessment\n   - Quality gate: Must pass before proceeding\n4. **Apply Review Feedback**:\n\n   - Fix Critical issues immediately (dispatch fix subagent)\n   - Fix Important issues before next task\n   - Note Minor issues for later\n5. **Mark Complete, Next Task**: Update TodoWrite and proceed to next task\n6. **Final Review**: After all tasks, dispatch final reviewer for overall assessment\n7. **Complete Development**: Use finishing-a-development-branch skill to verify and close\n\n### Parallel Execution Process\n\n1. **Load and Review Plan**: Read plan, identify concerns, create TodoWrite\n2. **Execute Batch**: Execute first 3 tasks (default batch size):\n\n   - Mark each as in_progress\n   - Follow each step exactly\n   - Run verifications as specified\n   - Mark as completed\n3. **Report**: Show what was implemented and verification output\n4. **Continue**: Apply feedback if needed, execute next batch\n5. **Complete Development**: Final verification and close\n\n### Parallel Investigation Process\n\nFor multiple unrelated failures (different files, subsystems, bugs):\n\n1. **Identify Independent Domains**: Group failures by what is broken\n2. **Create Focused Agent Tasks**: Each agent gets specific scope, clear goal, constraints\n3. **Dispatch in Parallel**: All agents run concurrently\n4. **Review and Integrate**: Verify fixes do not conflict, run full suite\n\n#### Quality Gates\n\nQuality gates are enforced at key checkpoints:\n\n| Checkpoint                   | Gate Type            | Action on Failure           |\n| ------------------------------ | ---------------------- | ----------------------------- |\n| After each task (sequential) | Code review          | Fix issues before next task |\n| After batch (parallel)       | Human review         | Apply feedback, continue    |\n| Final review                 | Comprehensive review | Address all findings        |\n| Before merge                 | Full test suite      | All tests must pass         |\n\n**Issue Severity Handling:**\n\n- **Critical**: Fix immediately, do not proceed until resolved\n- **Important**: Fix before next task or batch\n- **Minor**: Note for later, do not block progress\n\n### multi-agent-patterns\n\nUse when single-agent context limits are exceeded, when tasks decompose naturally into subtasks, or when specializing agents improves quality.\n\n**Why Multi-Agent Architectures:**\n\n| Problem                   | Solution                                       |\n| --------------------------- | ------------------------------------------------ |\n| **Context Bottleneck**    | Partition work across multiple context windows |\n| **Sequential Bottleneck** | Parallelize independent subtasks across agents |\n| **Generalist Overhead**   | Specialize agents with lean, focused context   |\n\n**Architecture Patterns:**\n\n| Pattern                     | When to Use                                    | Trade-offs                                    |\n| ----------------------------- | ------------------------------------------------ | ----------------------------------------------- |\n| **Supervisor/Orchestrator** | Clear task decomposition, need human oversight | Central bottleneck, \"telephone game\" risk     |\n| **Peer-to-Peer/Swarm**      | Flexible exploration, emergent requirements    | Coordination complexity, divergence risk      |\n| **Hierarchical**            | Large projects with layered abstraction        | Overhead between layers, alignment challenges |\n\n**Example of Implementation:**\n\n```\nSupervisor Pattern:\nUser Request → Supervisor → [Specialist A, B, C] → Aggregation → Output\n\nKey Insight: Sub-agents exist to isolate context, not to anthropomorphize roles\n```\n\n## Foundation\n\nThe SADD plugin is based on the following foundations:\n\n### Agent Skills for Context Engineering\n\n- [Agent Skills for Context Engineering project](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) by Murat Can Koylan\n\n### Research Papers\n\n**Multi-Agent Patterns:**\n\n- [Multi-Agent Debate](https://arxiv.org/abs/2305.14325) - Du, Y., et al. (2023)\n- [Self-Consistency](https://arxiv.org/abs/2203.11171) - Wang, X., et al. (2022)\n- [Tree of Thoughts](https://arxiv.org/abs/2305.10601) - Yao, S., et al. (2023)\n\n**Evaluation and Critique:**\n\n- [Constitutional AI](https://arxiv.org/abs/2212.08073) - Bai, Y., et al. (2022). Self-critique loops\n- [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) - Zheng, L., et al. (2023). Structured evaluation\n- [Chain-of-Verification](https://arxiv.org/abs/2309.11495) - Dhuliawala, S., et al. (2023). Verification loops\n\n### Engineering Methodologies\n\n- **Design Studio Method** - Parallel design exploration with critique and synthesis\n- **Spike Solutions** (Extreme Programming) - Time-boxed exploration of multiple approaches\n- **Ensemble Methods** (Machine Learning) - Combining multiple models for improved performance\n",
        "plugins/ddd/README.md": "# Domain-Driven Development Plugin\n\nCode quality framework that embeds Clean Architecture, SOLID principles, and Domain-Driven Design patterns into your development workflow through persistent memory updates and contextual skills.\n\nFocused on:\n\n- **Clean Architecture** - Separation of concerns with layered architecture boundaries\n- **Domain-Driven Design** - Ubiquitous language and bounded contexts for complex domains\n- **SOLID Principles** - Single responsibility, open-closed, and dependency inversion patterns\n- **Code Quality Standards** - Consistent formatting, naming conventions, and anti-pattern avoidance\n\n## Overview\n\nThe DDD plugin implements battle-tested software architecture principles that have proven essential for building maintainable, scalable systems. It provides commands to configure AI-assisted development with established best practices, and skills that guide code generation toward high-quality patterns.\n\nThe plugin is based on foundational works including Eric Evans' \"Domain-Driven Design\" (2003), Robert C. Martin's \"Clean Architecture\" (2017), and the SOLID principles that have become industry standards for object-oriented design.\n\nThese principles address the core challenge of software development: **managing complexity**. By establishing clear boundaries between business logic and infrastructure, using domain-specific naming, and following proven design patterns, teams can build systems that remain understandable and modifiable as they grow.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install ddd@NeoLabHQ/context-engineering-kit\n\n# Set up code formatting standards in CLAUDE.md\n/ddd:setup-code-formating\n\n# The software-architecture skill activates automatically when writing code\n# alternatively, you can ask Claude to use DDD directly\n> claude \"Use DDD skill to implement user authentication\"\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands Overview\n\n### /ddd:setup-code-formating - Code Style Configuration\n\nEstablishes consistent code formatting rules and style guidelines by updating your project's CLAUDE.md file with enforced standards.\n\n- Purpose - Configure AI-assisted development with consistent code style\n- Output - Updated CLAUDE.md with formatting rules\n\n```bash\n/ddd:setup-code-formating\n```\n\n#### Arguments\n\nNone required - creates standard formatting configuration.\n\n#### How It Works\n\n1. **Configuration Detection**: Checks for existing CLAUDE.md in the project root\n2. **Standards Application**: Adds or updates the Code Style Rules section with:\n   - Semicolon usage rules\n   - Quote style enforcement\n   - Curly brace conventions\n   - Indentation standards\n   - Import ordering guidelines\n\n3. **Persistent Memory**: Rules are written to CLAUDE.md, ensuring all future AI interactions follow the same standards\n\n**Formatting Rules Applied**\n\nThe command configures the following standards:\n\n| Rule | Setting | Purpose |\n|------|---------|---------|\n| Semicolons | No semicolons | Cleaner, modern JavaScript/TypeScript |\n| Quotes | Single quotes | Consistency across codebase |\n| Curly braces | Minimal (no unnecessary) | Reduced visual noise |\n| Indentation | 2 spaces | Readable, compact code |\n| Import order | External, Internal, Types | Logical organization |\n\n#### Usage Examples\n\n```bash\n# Basic setup - adds formatting rules to CLAUDE.md\n/ddd:setup-code-formating\n\n# Typically used during project initialization\n/sdd:00-setup Use React, TypeScript, Node.js\n/tech-stack:add-typescript-best-practices\n/ddd:setup-code-formating\n```\n\n#### Best Practices\n\n- Run early in project setup - Establish standards before significant code is written\n- Combine with linting tools - Use ESLint/Prettier to enforce rules automatically\n- Team alignment - Ensure all team members use the same CLAUDE.md configuration\n- Review generated rules - Adjust the CLAUDE.md output if your project has different conventions\n\n## Skills Overview\n\n### software-architecture - Quality-Focused Development Guidance\n\nThe software-architecture skill provides comprehensive guidance for writing high-quality, maintainable code. It activates automatically when users engage in code writing, architecture design, or code analysis tasks.\n\n#### What It Provides\n\n**General Principles**\n\n- **Early Return Pattern**: Prefer early returns over nested conditions for improved readability\n- **DRY (Don't Repeat Yourself)**: Create reusable functions and modules to avoid duplication\n- **Function Decomposition**: Break down long functions (>80 lines) into smaller, focused units\n- **File Size Limits**: Keep files under 200 lines; split when necessary\n- **Arrow Functions**: Prefer arrow functions over function declarations\n\n**Library-First Approach**\n\nThe skill emphasizes leveraging existing solutions before writing custom code:\n\n```\nALWAYS search for existing solutions before writing custom code:\n1. Check npm/package registries for existing libraries\n2. Evaluate SaaS solutions and third-party APIs\n3. Consider whether custom code is truly justified\n```\n\nCustom code is justified only when:\n- Implementing specific business logic unique to the domain\n- Performance-critical paths require special optimization\n- External dependencies would be overkill for the use case\n- Security-sensitive code requires full control\n- Existing solutions don't meet requirements after thorough evaluation\n\n**Clean Architecture and DDD Principles**\n\nThe skill enforces architectural boundaries:\n\n- **Domain Layer**: Business entities independent of frameworks\n- **Use Case Layer**: Application-specific business rules\n- **Interface Layer**: Controllers, presenters, gateways\n- **Infrastructure Layer**: Frameworks, databases, external services\n\n**Naming Conventions**\n\n| Avoid | Prefer | Reason |\n|-------|--------|--------|\n| `utils.js` | `OrderCalculator.js` | Domain-specific purpose |\n| `helpers/misc.js` | `UserAuthenticator.js` | Clear responsibility |\n| `common/shared.js` | `InvoiceGenerator.js` | Single bounded context |\n\n**Anti-Patterns to Avoid**\n\nThe skill warns against common architectural mistakes:\n\n- **NIH (Not Invented Here) Syndrome**: Don't build custom auth when Auth0/Supabase exists; don't write custom state management instead of Redux/Zustand\n- **Mixing Concerns**: Business logic in UI components, database queries in controllers\n- **Generic Naming**: `utils.js` with 50 unrelated functions, `helpers/misc.js` as a dumping ground\n\n**Code Quality Standards**\n\n- Proper error handling with typed catch blocks\n- Maximum 3 levels of nesting\n- Functions under 50 lines when possible\n- Files under 200 lines when possible\n\n#### When It Activates\n\nThe skill automatically applies when:\n- Writing new code or features\n- Designing system architecture\n- Analyzing existing code\n- Reviewing code for quality issues\n- Refactoring legacy code\n\n## Foundation\n\nThe DDD plugin is based on foundational software engineering literature that has shaped modern development practices:\n\n### Core Literature\n\n- **[Domain-Driven Design](https://www.domainlanguage.com/ddd/)** (Eric Evans, 2003) - Introduced ubiquitous language, bounded contexts, and strategic design patterns for managing complex domains\n- **[Clean Architecture](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)** (Robert C. Martin, 2012/2017) - Defines dependency rules and layer boundaries for maintainable systems\n- **[SOLID Principles](https://en.wikipedia.org/wiki/SOLID)** (Robert C. Martin, 2000s) - Five principles of object-oriented design that promote maintainability\n\n### Key Concepts Applied\n\n| Concept | Source | Application in Plugin |\n|---------|--------|----------------------|\n| Ubiquitous Language | Evans (DDD) | Domain-specific naming conventions |\n| Bounded Contexts | Evans (DDD) | Module and file organization |\n| Dependency Inversion | Martin (SOLID) | Layer separation rules |\n| Single Responsibility | Martin (SOLID) | Function and file size limits |\n| Separation of Concerns | General | Business logic isolation |\n",
        "plugins/sdd/README.md": "# Spec-Driven Development (SDD) Plugin\n\nComprehensive specification-driven development workflow that transforms vague ideas into production-ready implementations through structured planning, architecture design, and quality-gated execution.\n\nFocused on:\n\n- **Specification-first development** - Define what to build before how to build it\n- **Multi-agent architecture** - Specialized agents for analysis, design, and implementation\n- **Iterative refinement** - Continuous validation and quality gates at each stage\n- **Documentation-driven** - Generate living documentation alongside implementation\n\n## Plugin Target\n\n- Reduce implementation rework - detailed specs catch issues before code is written\n- Improve architecture decisions - structured exploration of alternatives with trade-offs\n- Maintain project consistency - constitution and templates ensure uniform standards\n- Enable complex feature development - break down large features into manageable, testable tasks\n\n## Overview\n\nThe SDD plugin implements a structured software development methodology based on GitHub Spec Kit, OpenSpec, and the BMad Method. It uses specialized AI agents to guide you through the complete development lifecycle: from initial brainstorming through specification, architecture design, task breakdown, implementation, and documentation.\n\nThe workflow ensures that every feature is thoroughly specified, properly architected, and systematically implemented with quality gates at each stage. Each phase produces concrete artifacts (specification files, architecture documents, task lists) that serve as the source of truth for subsequent phases.\n\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install sdd@NeoLabHQ/context-engineering-kit\n\n# Set up project standards (one-time)\n/sdd:00-setup Use TypeScript, follow SOLID principles and Clean Architecture\n\n# Start a new feature\n/sdd:01-specify Add user authentication with OAuth2 providers\n\n# Plan the architecture\n/sdd:02-plan Use Passport.js for OAuth, prioritize security\n\n# Create implementation tasks\n/sdd:03-tasks Use TDD approach, prioritize MVP features\n\n# Execute the implementation\n/sdd:04-implement Focus on test coverage and error handling\n\n# Document the feature\n/sdd:05-document Include API examples and integration guide\n```\n\n[Usage Examples](./usage-examples.md)\n\n\n## Workflow Diagram\n\n```\n┌─────────────────────────────────────────────┐\n│ 1. Setup Project Standards                  │\n│    /sdd:00-setup                            │\n│    (create specs/constitution.md)           │\n└────────────────────┬────────────────────────┘\n                     │\n                     │ project principles established\n                     ▼\n┌─────────────────────────────────────────────┐\n│ 2. Create Specification                     │ ◀─── clarify requirements ───┐\n│    /sdd:01-specify                          │                              │\n│    (create specs/<feature>/spec.md)         │                              │\n└────────────────────┬────────────────────────┘                              │\n                     │                                                       │\n                     │ validated specification                               │\n                     ▼                                                       │\n┌─────────────────────────────────────────────┐                              │\n│ 3. Plan Architecture                        │──────────────────────────────┘\n│    /sdd:02-plan                             │◀─── refine architecture ─────┐\n│    (create plan.md, design.md, research.md) │                              │\n└────────────────────┬────────────────────────┘                              │\n                     │                                                       │\n                     │ approved architecture                                 │\n                     ▼                                                       │\n┌─────────────────────────────────────────────┐                              │\n│ 4. Break Down into Tasks                    │──────────────────────────────┘\n│    /sdd:03-tasks                            │\n│    (create tasks.md)                        │\n└────────────────────┬────────────────────────┘\n                     │\n                     │ executable task list\n                     ▼\n┌─────────────────────────────────────────────┐\n│ 5. Implement Tasks                          │ ◀─── fix issues ─────────────┐\n│    /sdd:04-implement                        │                              │\n│    (write code, run tests)                  │                              │\n└────────────────────┬────────────────────────┘                              │\n                     │                                                       │\n                     │ working implementation                                │\n                     ▼                                                       │\n┌─────────────────────────────────────────────┐                              │\n│ 6. Quality Review                           │──────────────────────────────┘\n│    (automatic in /sdd:04-implement)         │\n└────────────────────┬────────────────────────┘\n                     │\n                     │ approved changes\n                     ▼\n┌─────────────────────────────────────────────┐\n│ 7. Document Changes                         │\n│    /sdd:05-document                         │\n│    (update docs/ directory)                 │\n└─────────────────────────────────────────────┘\n```\n\n\n## Commands Overview\n\n### /sdd:00-setup - Project Constitution Setup\n\nCreate or update the project constitution that establishes development principles, coding standards, and governance rules for all subsequent development.\n\n- Purpose - Establish project-wide development standards and principles\n- Output - `specs/constitution.md` and template files for specs, plans, and tasks\n\n```bash\n/sdd:00-setup [\"principle inputs or constitution parameters\"]\n```\n\n#### Arguments\n\nOptional principle inputs such as technology stack, architectural patterns, or development guidelines. Examples: \"Use NestJS, follow SOLID and Clean Architecture\" or \"Python with FastAPI, prioritize type safety\".\n\n#### How It Works\n\n1. **Template Initialization**: Downloads and creates the constitution template at `specs/constitution.md` along with spec, plan, and tasks templates in `specs/templates/`\n\n2. **Value Collection**: Gathers concrete values for template placeholders from:\n   - User input (conversation)\n   - Existing repo context (README, docs, CLAUDE.md)\n   - Prior constitution versions if present\n\n3. **Constitution Drafting**: Fills the template with:\n   - Project name and description\n   - Development principles (each with name, rules, and rationale)\n   - Governance section with amendment procedures and versioning policy\n   - Compliance review expectations\n\n4. **Consistency Propagation**: Ensures all dependent templates align with updated principles:\n   - `specs/templates/plan-template.md` - Architecture planning template\n   - `specs/templates/spec-template.md` - Feature specification template\n   - `specs/templates/tasks-template.md` - Task breakdown template\n   - `specs/templates/spec-checklist.md` - Specification quality checklist\n\n5. **Sync Impact Report**: Documents version changes, modified principles, and any follow-up TODOs\n\n#### Usage Examples\n\n```bash\n# Initialize with core principles\n/sdd:00-setup Use React with TypeScript, follow atomic design patterns\n\n# Set up for backend project\n/sdd:00-setup NestJS, PostgreSQL, follow hexagonal architecture\n\n# Minimal setup (will prompt for details)\n/sdd:00-setup\n\n# Update existing constitution with new principle\n/sdd:00-setup Add principle: All APIs must be versioned\n```\n\n#### Best practices\n\n- Be specific about tech stack - Clear technology choices improve downstream decisions\n- Include architectural patterns - Patterns like Clean Architecture guide agent decisions\n- Review generated templates - Ensure templates align with your team's workflow\n- Version your constitution - Use semantic versioning for governance changes\n\n---\n\n### /sdd:01-specify - Feature Specification\n\nTransform a natural language feature description into a detailed, validated specification with business requirements, user scenarios, and success criteria.\n\n- Purpose - Create comprehensive feature specification from business requirements\n- Output - `specs/<feature-name>/spec.md` with validated requirements\n\n```bash\n/sdd:01-specify [\"feature description\"]\n```\n\n#### Arguments\n\nNatural language description of the feature to build. Examples: \"Add OAuth authentication with Google and GitHub providers\" or \"Create a dashboard for analytics with real-time data\".\n\n#### How It Works\n\n1. **Feature Naming**: Generates a concise short name (2-4 words) for the feature branch and spec directory\n\n2. **Branch/Directory Management**:\n   - Checks for existing branches to determine the next available feature number\n   - Creates `specs/<number>-<short-name>/` directory (FEATURE_DIR)\n   - Copies spec template to `FEATURE_DIR/spec.md`\n\n3. **Business Analysis**: Launches `business-analyst` agent to:\n   - Perform requirements discovery and stakeholder analysis\n   - Extract key concepts: actors, actions, data, constraints\n   - Write specification following the template structure\n   - Mark unclear aspects with [NEEDS CLARIFICATION] (max 3)\n\n4. **Specification Validation**: Launches second `business-analyst` agent to:\n   - Fill in `spec-checklist.md` with quality criteria\n   - Review spec against each checklist item\n   - Document specific issues with quoted spec sections\n   - Iterate until all items pass (max 3 iterations)\n\n5. **Clarification Resolution**: If [NEEDS CLARIFICATION] markers remain:\n   - Presents max 3 questions with suggested answers in table format\n   - Options include A, B, C choices plus Custom input\n   - Updates spec with user's chosen answers\n   - Re-validates after clarifications\n\n#### Usage Examples\n\n```bash\n# Define a new feature\n/sdd:01-specify Add user authentication with social login support\n\n# Feature with specific scope\n/sdd:01-specify Create invoice generation with PDF export and email delivery\n\n# Complex feature\n/sdd:01-specify Build real-time collaborative document editing with conflict resolution\n\n# Bug fix specification\n/sdd:01-specify Fix payment timeout issues when processing large transactions\n```\n\n#### Best practices\n\n- Focus on WHAT and WHY - Describe the problem and user needs, not implementation\n- Be specific about scope - Clear boundaries prevent scope creep\n- Include success criteria - Measurable outcomes help validation\n- Answer clarification questions - User input improves spec quality\n- Review generated spec - Verify it captures your intent before proceeding\n\n---\n\n### /sdd:02-plan - Architecture Planning\n\nDesign the technical architecture with multiple approaches, research unknowns, and create a comprehensive implementation plan with data models and API contracts.\n\n- Purpose - Create detailed architecture design with trade-off analysis\n- Output - `FEATURE_DIR/plan.md`, `design.md`, `research.md`, `data-model.md`, `contracts.md`\n\n```bash\n/sdd:02-plan [\"plan specifics or preferences\"]\n```\n\n#### Arguments\n\nOptional architecture preferences or constraints. Examples: \"Use libraries instead of direct integration\" or \"Prioritize simplicity over performance\".\n\n#### How It Works\n\n1. **Context Loading**: Reads feature specification and project constitution\n\n2. **Research & Exploration** (Stage 2):\n   - Launches `researcher` agent to investigate unknown technologies and dependencies\n   - Launches 2-3 `code-explorer` agents in parallel to:\n     - Find similar features in the codebase\n     - Map architecture and abstractions\n     - Identify UI patterns and testing approaches\n   - Consolidates findings in `FEATURE_DIR/research.md`\n\n3. **Clarifying Questions** (Stage 3):\n   - Reviews codebase findings and original requirements\n   - Identifies underspecified aspects: edge cases, error handling, integration points\n   - Presents questions and waits for user answers\n\n4. **Architecture Design** (Stage 4):\n   - Launches 2-3 `software-architect` agents with different focuses:\n     - **Minimal changes**: Smallest change, maximum reuse\n     - **Clean architecture**: Maintainability, elegant abstractions\n     - **Pragmatic balance**: Speed + quality trade-off\n   - Each produces a design document with trade-offs\n\n5. **Final Plan** (Stage 5):\n   - User selects preferred approach\n   - Launches `software-architect` agent to create final design\n   - Generates:\n     - `FEATURE_DIR/design.md` - Final architecture document\n     - `FEATURE_DIR/plan.md` - Implementation plan\n     - `FEATURE_DIR/data-model.md` - Entity definitions, relationships, validation rules\n     - `FEATURE_DIR/contracts.md` - API endpoints in OpenAPI/GraphQL format\n\n6. **Plan Review** (Stage 6):\n   - Reviews implementation plan for unclear areas\n   - Resolves high-confidence issues automatically\n   - Presents remaining uncertainties to user for clarification\n\n#### Usage Examples\n\n```bash\n# Start architecture planning\n/sdd:02-plan\n\n# With technology preference\n/sdd:02-plan Use Redis for caching, prefer PostgreSQL transactions\n\n# With architectural constraint\n/sdd:02-plan Must integrate with existing auth system, minimize changes\n\n# Performance focus\n/sdd:02-plan Optimize for high throughput, consider async processing\n```\n\n#### Best practices\n\n- Review research findings - Understand what exists before designing\n- Answer architecture questions - Your input shapes the design direction\n- Compare all approaches - Each has trade-offs worth considering\n- Validate data models early - Entity definitions drive implementation\n- Review API contracts - Contracts become the integration specification\n\n---\n\n### /sdd:03-tasks - Task Generation\n\nGenerate an actionable, dependency-ordered task list organized by user stories with complexity analysis and parallel execution opportunities.\n\n- Purpose - Break down feature into executable tasks with clear dependencies\n- Output - `FEATURE_DIR/tasks.md` with phased task list\n\n```bash\n/sdd:03-tasks [\"task creation guidance\"]\n```\n\n#### Arguments\n\nOptional guidance for task creation. Examples: \"Use TDD approach and prioritize MVP features\" or \"Focus on backend first, then frontend\".\n\n#### How It Works\n\n1. **Context Loading**: Reads from FEATURE_DIR:\n   - **Required**: plan.md (tech stack, architecture), spec.md (user stories with priorities)\n   - **Optional**: data-model.md, contracts.md, research.md\n\n2. **Task Generation**: Launches `tech-lead` agent to create tasks following:\n\n   **Implementation Strategy Selection**:\n   - **Top-to-Bottom**: Workflow-first when process is clear\n   - **Bottom-to-Top**: Building-blocks-first when algorithms are complex\n   - **Mixed**: Combine approaches for different parts\n\n   **Phase Structure**:\n   - Phase 1: Setup (project initialization)\n   - Phase 2: Foundational (blocking prerequisites)\n   - Phase 3+: User Stories in priority order (P1, P2, P3...)\n   - Final Phase: Polish & cross-cutting concerns\n\n3. **Complexity Analysis**: Each task includes:\n   - Clear goal and acceptance criteria\n   - Technical approach and patterns to use\n   - Dependencies and blocking relationships\n   - **Complexity Rating**: Low/Medium/High\n   - **Uncertainty Rating**: Low/Medium/High\n\n4. **Risk Review**: After generation:\n   - Lists all high-complexity or high-uncertainty tasks\n   - Explains what makes each task risky\n   - Asks if user wants further decomposition\n\n#### Usage Examples\n\n```bash\n# Generate tasks with TDD focus\n/sdd:03-tasks Use TDD approach, write tests before implementation\n\n# MVP prioritization\n/sdd:03-tasks Focus on P1 user stories only for initial release\n\n# Parallel-friendly breakdown\n/sdd:03-tasks Maximize parallel execution opportunities\n\n# Sequential approach\n/sdd:03-tasks Prefer sequential tasks for easier debugging\n```\n\n#### Best practices\n\n- Review high-risk tasks - Consider decomposing complex tasks further\n- Validate task dependencies - Ensure parallel tasks are truly independent\n- Check user story coverage - Each story should have complete task set\n- Estimate before starting - Use complexity ratings for planning\n- Keep tasks small - 1-2 day tasks are ideal\n\n---\n\n### /sdd:04-implement - Feature Implementation\n\nExecute the implementation plan by processing all tasks with TDD approach, quality review, and continuous progress tracking.\n\n- Purpose - Implement all tasks following the execution plan\n- Output - Working code with tests passing, updated tasks.md with completion status\n\n```bash\n/sdd:04-implement [\"implementation preferences\"]\n```\n\n#### Arguments\n\nOptional implementation preferences. Examples: \"Focus on test coverage and error handling\" or \"Prioritize performance optimization\".\n\n#### How It Works\n\n1. **Context Loading**: Reads implementation context from FEATURE_DIR:\n   - **Required**: tasks.md, plan.md\n   - **Optional**: data-model.md, contracts.md, research.md\n\n2. **Phase Execution** (Stage 8): For each phase in tasks.md:\n   - Launches `developer` agent to implement the phase\n   - Follows execution rules:\n     - Phase-by-phase: Complete each phase before next\n     - Respect dependencies: Sequential tasks in order, parallel [P] tasks together\n     - TDD approach: Tests before implementation\n     - File coordination: Tasks affecting same files run sequentially\n\n3. **Progress Tracking**:\n   - Reports progress after each completed phase\n   - Marks completed tasks as [X] in tasks.md\n   - Halts on non-parallel task failures\n   - Continues parallel tasks, reports failed ones\n\n4. **Completion Validation**: Launches `developer` agent to verify:\n   - All required tasks completed\n   - Implementation matches specification\n   - Tests pass and coverage meets requirements\n   - Implementation follows technical plan\n\n5. **Quality Review** (Stage 9):\n   - Performs `/code-review:review-local-changes` if available\n   - Otherwise launches 3 `developer` agents focusing on:\n     - Simplicity/DRY/elegance\n     - Bugs/functional correctness\n     - Project conventions/abstractions\n   - Consolidates findings and recommends fixes\n\n6. **User Decision**: Presents findings and asks:\n   - Fix now\n   - Fix later\n   - Proceed as-is\n\n#### Usage Examples\n\n```bash\n# Start implementation\n/sdd:04-implement\n\n# With error handling focus\n/sdd:04-implement Prioritize error handling and edge cases\n\n# Performance-focused\n/sdd:04-implement Optimize for performance, use caching where appropriate\n\n# Test coverage priority\n/sdd:04-implement Achieve 90%+ test coverage\n```\n\n#### Best practices\n\n- Address review findings - Quality issues compound over time\n- Monitor test failures - Fix tests before proceeding\n- Review progress regularly - Check tasks.md for completion status\n- Commit frequently - Save progress after each phase\n\n---\n\n### /sdd:05-document - Feature Documentation\n\nDocument the completed feature implementation with API guides, architecture updates, usage examples, and lessons learned.\n\n- Purpose - Create comprehensive documentation for implemented feature\n- Output - Updated documentation in `docs/` folder\n\n```bash\n/sdd:05-document [\"documentation focus areas\"]\n```\n\n#### Arguments\n\nOptional focus areas for documentation. Examples: \"Include API examples and integration guide\" or \"Focus on troubleshooting common issues\".\n\n#### How It Works\n\n1. **Context Loading**: Reads from FEATURE_DIR:\n   - **Required**: tasks.md (verify completion)\n   - **Optional**: plan.md, spec.md, contracts.md, data-model.md\n\n2. **Implementation Verification** (Stage 10):\n   - Reviews tasks.md to confirm all tasks marked [X]\n   - Identifies incomplete or partially implemented tasks\n   - Reviews codebase for missing functionality\n   - **Presents issues to user**: Fix now or later?\n\n3. **Documentation Update**: Launches `tech-writer` agent following workflow:\n   - Reads all FEATURE_DIR artifacts\n   - Reviews files modified during implementation\n   - Identifies documentation gaps in `docs/`\n\n4. **Documentation Generation**:\n   - API guides and usage examples\n   - Architecture updates reflecting implementation\n   - README.md updates in affected folders\n   - Development specifics for LLM navigation\n   - Troubleshooting guidance for common issues\n\n5. **Output Summary**:\n   - Files updated\n   - Major documentation changes\n   - New best practices documented\n   - Project status after this phase\n\n#### Usage Examples\n\n```bash\n# Generate documentation\n/sdd:05-document\n\n# API-focused documentation\n/sdd:05-document Focus on API documentation with curl examples\n\n# Integration guide\n/sdd:05-document Include step-by-step integration guide\n\n# Troubleshooting emphasis\n/sdd:05-document Document common errors and solutions\n```\n\n#### Best practices\n\n- Complete implementation first - Document working code, not plans\n- Include working examples - Test all code samples\n- Update architecture docs - Reflect actual implementation\n- Document gotchas - Share lessons learned during implementation\n- Cross-reference specs - Link to original requirements\n\n---\n\n### /sdd:create-ideas - Idea Generation\n\nGenerate ideas in one shot using creative sampling. Based on [Verbalized Sampling](https://arxiv.org/abs/2510.01171) - a training-free prompting strategy to mitigate mode collapse in LLMs by requesting responses with probabilities. Achieves 2-3x diversity improvement while maintaining quality.\n\nDifferent from `/sdd:brainstorm`, by much simpler and faster approach, but focused on generating ideas in one shot, don't include refinement, focuses on creativity. Can be used for any other purpose that include creative thinking.\n\n- Purpose - Generate responses which require high diversity and creativity, like brainstorming or creative writing\n- Output - List of ideas with text and probability scores\n\n```bash\n/sdd:create-ideas [topic or problem] [optional: number of ideas]\n```\n\n#### Arguments\n\nTopic or problem to generate ideas for. Optionally specify the number of ideas to generate (defaults to 5).\n\n#### How It Works\n\n1. **Creative Sampling**: Uses verbalized probability sampling to generate diverse responses\n   - Requests responses from the full distribution or distribution tails\n   - Each response includes a probability score (< 0.10 for tail sampling)\n   - Reduces mode collapse common in standard LLM generation\n\n2. **Output Format**: Returns a list where each item contains:\n   - Text: The generated idea or response\n   - Probability: Numeric score indicating sampling position\n\n#### Usage Examples\n\n```bash\n# Generate creative ideas for a feature\n/sdd:create-ideas ways to improve user onboarding\n\n# Brainstorm solutions to a problem\n/sdd:create-ideas reduce API response times\n\n# Creative writing prompts\n/sdd:create-ideas write jokes about cats\n\n# Generate more ideas\n/sdd:create-ideas 10 marketing slogans for a fitness app\n\n# Technical alternatives\n/sdd:create-ideas caching strategies for real-time data\n```\n\n#### When to Use\n\n- **Use `/sdd:create-ideas`** when you need quick, diverse ideas without refinement\n- **Use `/sdd:brainstorm`** when you need thorough exploration with validation and documentation\n\n#### Best practices\n\n- Be specific about the domain - \"API error handling patterns\" vs just \"error handling\"\n- Use for divergent thinking - Generate many options before converging on solutions\n- Review probability scores - Lower probabilities indicate more creative/unusual ideas\n- Combine with brainstorm - Use create-ideas for initial ideation, then brainstorm to refine\n\n---\n\n### /sdd:brainstorm - Idea Refinement\n\nTransform rough ideas into fully-formed designs through collaborative dialogue, incremental validation, and design documentation.\n\n- Purpose - Refine vague ideas into actionable designs\n- Output - Design document in `docs/plans/YYYY-MM-DD-<topic>-design.md`\n\n```bash\n/sdd:brainstorm initial feature concept\n```\n\n#### Arguments\n\nOptional initial concept to explore. Can be vague: \"something to help with user onboarding\" or more specific: \"real-time notification system\".\n\n#### How It Works\n\n1. **Context Understanding**:\n   - Reviews current project state (files, docs, recent commits)\n   - Asks questions one at a time to refine the idea\n   - Prefers multiple choice questions when possible\n   - Focuses on: purpose, constraints, success criteria\n\n2. **Approach Exploration**:\n   - Proposes 2-3 different approaches with trade-offs\n   - Leads with recommended option and reasoning\n   - Presents options conversationally\n\n3. **Design Presentation**:\n   - Breaks design into 200-300 word sections\n   - Asks after each section if it looks right\n   - Covers: architecture, components, data flow, error handling, testing\n   - Ready to clarify if something doesn't make sense\n\n4. **Documentation**:\n   - Writes validated design to `docs/plans/YYYY-MM-DD-<topic>-design.md`\n   - Commits the design document to git\n\n5. **Implementation Handoff** (optional):\n   - Asks if ready to set up for implementation\n   - Can create isolated workspace with git worktrees\n   - Can create detailed implementation plan\n\n#### Key Principles\n\n- **One question at a time** - Don't overwhelm with multiple questions\n- **Multiple choice preferred** - Easier than open-ended when possible\n- **YAGNI ruthlessly** - Remove unnecessary features from designs\n- **Explore alternatives** - Always propose 2-3 approaches before settling\n- **Incremental validation** - Present design in sections, validate each\n\n#### Usage Examples\n\n```bash\n# Start with vague idea\n/sdd:brainstorm Something to improve user onboarding\n\n# More specific concept\n/sdd:brainstorm Real-time collaboration features for document editing\n\n# Technical exploration\n/sdd:brainstorm Caching strategy for our product catalog API\n\n# Process improvement\n/sdd:brainstorm Automated deployment pipeline for our microservices\n```\n\n#### Best practices\n\n- Start with the problem - Describe what you're trying to solve\n- Be open to alternatives - The first idea isn't always best\n- Engage with questions - Your answers shape the design\n- Validate incrementally - Catch issues early in design sections\n- Save the design - Use as input for `/sdd:01-specify`\n\n---\n\n## Available Agents\n\nThe SDD plugin uses specialized agents for different phases of development:\n\n| Agent | Description | Used By |\n|-------|-------------|---------|\n| `business-analyst` | Requirements discovery, stakeholder analysis, specification writing | `/sdd:01-specify` |\n| `researcher` | Technology research, dependency analysis, best practices | `/sdd:02-plan` |\n| `code-explorer` | Codebase analysis, pattern identification, architecture mapping | `/sdd:02-plan` |\n| `software-architect` | Architecture design, component design, implementation planning | `/sdd:02-plan` |\n| `tech-lead` | Task decomposition, dependency mapping, sprint planning | `/sdd:03-tasks` |\n| `developer` | Code implementation, TDD execution, quality review | `/sdd:04-implement` |\n| `tech-writer` | Documentation creation, API guides, architecture docs | `/sdd:05-document` |\n\n## Theoretical Foundation\n\nThe SDD plugin is based on established software engineering methodologies and research:\n\n### Core Methodologies\n\n- **[GitHub Spec Kit](https://github.com/github/spec-kit)** - Specification-driven development templates and workflows\n- **OpenSpec** - Open specification format for software requirements\n- **BMad Method** - Structured approach to breaking down complex features\n\n### Supporting Research\n\n- **[Specification-Driven Development](https://en.wikipedia.org/wiki/Design_by_contract)** - Design by contract and formal specification approaches\n- **[Agile Requirements Engineering](https://www.agilealliance.org/agile101/)** - User stories, acceptance criteria, and iterative refinement\n- **[Test-Driven Development](https://www.agilealliance.org/glossary/tdd/)** - Writing tests before implementation\n- **[Clean Architecture](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)** - Separation of concerns and dependency inversion\n- **[Vertical Slice Architecture](https://jimmybogard.com/vertical-slice-architecture/)** - Feature-based organization for incremental delivery\n- **[Verbalized Sampling](https://arxiv.org/abs/2510.01171)** - Training-free prompting strategy for diverse idea generation. Achieves **2-3x diversity improvement** while maintaining quality. Used for `create-ideas`, `brainstorm` and `plan` commands\n\n",
        "plugins/kaizen/README.md": "# Kaizen Plugin\n\nContinuous improvement framework inspired by the Toyota Production System that brings Lean manufacturing principles to software development through systematic problem analysis, root cause investigation, and iterative improvement cycles.\n\n## Plugin Target\n\n- Find root causes - Stop fixing symptoms; address fundamental issues\n- Prevent recurrence - Understand why problems exist to prevent similar issues\n- Continuous improvement - Small, incremental changes that compound into major gains\n- Reduce waste - Identify and eliminate non-value-adding activities in code and processes\n\n## Overview\n\nThe Kaizen plugin implements proven manufacturing problem-solving techniques adapted for software development. Named after the Japanese word for \"continuous improvement,\" Kaizen philosophy emphasizes that small, ongoing positive changes can lead to major improvements over time.\n\nThe plugin is based on methodologies from the **Toyota Production System (TPS)** and **Lean manufacturing**, which have been validated across industries for over 70 years.\n\nThey are based on the idea that most bugs and quality issues are symptoms of deeper systemic problems. Fixing only the symptom leads to recurring issues; finding and addressing the root cause prevents entire classes of problems.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install kaizen@NeoLabHQ/context-engineering-kit\n\n# Investigate a bug's root cause\n> /kaizen:why \"API returns 500 error on checkout\"\n\n# Analyze code for improvement opportunities\n> /kaizen:analyse src/checkout/\n\n# Document a complex problem comprehensively\n> /kaizen:analyse-problem \"Database connection exhaustion during peak traffic\"\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands Overview\n\n### /kaizen:why - Five Whys Root Cause Analysis\n\nIterative questioning technique that drills from surface symptoms to fundamental root causes by repeatedly asking \"why.\"\n\n- Purpose - Find the true root cause, not just symptoms\n- Output - Chain of causation leading to actionable root cause\n\n```bash\n/kaizen:why [\"issue or symptom description\"]\n```\n\n#### Arguments\n\nOptional description of the issue or symptom to analyze. If not provided, you will be prompted for input.\n\n#### How It Works\n\n1. **State the Problem**: Clearly define the observable symptom or issue\n2. **First Why**: Ask why this problem occurs; document the immediate cause\n3. **Iterate**: For each answer, ask \"why\" again to go deeper\n4. **Branch When Needed**: If multiple causes emerge, explore each branch separately\n5. **Identify Root Cause**: Usually reached after 5 iterations when you hit systemic/process issues\n6. **Validate**: Work backwards from root cause to symptom to verify the chain\n7. **Propose Solutions**: Address root causes, not symptoms\n\n**Depth Guidelines**\n\n- **Stop when**: You reach process, policy, or systemic issues\n- **Keep going if**: \"Human error\" appears (ask why error was possible)\n- **Branch when**: Multiple contributing factors exist\n- **Not always 5**: Stop at true root cause, whether 3 or 7 whys deep\n\n#### Usage Examples\n\n```bash\n# Investigate a production bug\n> /kaizen:why \"Users see 500 error on checkout\"\n\n# Analyze a recurring issue\n> /kaizen:why \"E2E tests fail intermittently\"\n\n# Understand a performance problem\n> /kaizen:why \"Feature deployment takes 2 hours\"\n```\n\n**Example Output**:\n```\nProblem: Users see 500 error on checkout\nWhy 1: Payment service throws exception\nWhy 2: Request timeout after 30 seconds\nWhy 3: Database query takes 45 seconds\nWhy 4: Missing index on transactions table\nWhy 5: Index creation wasn't in migration scripts\n\nRoot Cause: Migration review process doesn't check query performance\nSolution: Add query performance checks to migration PR template\n```\n\n#### Best Practices\n\n- Do not stop at symptoms - Keep asking \"why\" until you reach systemic causes\n- Explore multiple branches - Complex problems often have multiple contributing factors\n- Avoid blame - Focus on process and systems, not individuals\n- Document everything - The chain of causation is valuable for future reference\n- Test solutions - Implement, verify the symptom is resolved, then monitor for recurrence\n\n---\n\n### /kaizen:root-cause-tracing - Bug Tracing Through Call Stack\n\nSystematically traces bugs backward through the call stack to identify where invalid data or incorrect behavior originates.\n\n- Purpose - Find the source of bugs that manifest deep in execution\n- Output - Trace chain from symptom to original trigger with fix recommendation\n\n```bash\n/kaizen:root-cause-tracing\n```\n\n#### Arguments\n\nNone. The command works with the current bug context from your conversation.\n\n#### How It Works\n\n1. **Observe Symptom**: Identify where the error appears (e.g., wrong file created, incorrect output)\n2. **Find Immediate Cause**: Locate the code that directly causes the error\n3. **Trace Upward**: Ask \"what called this?\" and follow the chain\n4. **Track Values**: Note what values were passed at each level\n5. **Find Origin**: Continue until you find where invalid data originated\n6. **Add Instrumentation**: If manual tracing fails, add stack trace logging\n7. **Fix at Source**: Address the root trigger, not the symptom location\n\n**Key Principle**: Never fix just where the error appears. Trace back to find the original trigger.\n\n#### Usage Examples\n\n```bash\n# After encountering a deep stack error\n> /kaizen:root-cause-tracing\n\n# When debugging file creation in wrong location\n> /kaizen:root-cause-tracing\n```\n\n**Example Trace**:\n```\nSymptom: .git created in packages/core/ (source code)\n\nTrace chain:\n1. git init runs in process.cwd() <- empty cwd parameter\n2. WorktreeManager called with empty projectDir\n3. Session.create() passed empty string\n4. Test accessed context.tempDir before beforeEach\n5. setupCoreTest() returns { tempDir: '' } initially\n\nRoot cause: Top-level variable initialization accessing empty value\nFix: Made tempDir a getter that throws if accessed before beforeEach\n\nDefense-in-depth added:\n- Layer 1: Project.create() validates directory\n- Layer 2: WorkspaceManager validates not empty\n- Layer 3: NODE_ENV guard refuses git init outside tmpdir\n- Layer 4: Stack trace logging before git init\n```\n\n#### Best practices\n\n- Use console.error in tests - Loggers may be suppressed in test environments\n- Log before dangerous operations - Capture state before failure, not after\n- Include full context - Directory, cwd, environment variables, timestamps\n- Add defense-in-depth - Fix at source AND add validation at each layer\n- Capture stack traces - Use `new Error().stack` for complete call chains\n\n---\n\n### /kaizen:cause-and-effect - Fishbone Analysis\n\nSystematic exploration of problem causes across six categories using the Ishikawa (Fishbone) diagram approach.\n\n- Purpose - Comprehensive multi-factor root cause exploration\n- Output - Structured analysis across People, Process, Technology, Environment, Methods, and Materials\n\n```bash\n/kaizen:cause-and-effect [\"problem description\"]\n```\n\n#### Arguments\n\nOptional problem description to analyze. If not provided, you will be prompted for input.\n\n#### How It Works\n\n1. **State the Problem**: Define the \"head\" of the fish - the effect you're analyzing\n2. **Explore Each Category**: Brainstorm potential causes in six domains:\n   - **People**: Skills, training, communication, team dynamics\n   - **Process**: Workflows, procedures, standards, reviews\n   - **Technology**: Tools, infrastructure, dependencies, configuration\n   - **Environment**: Workspace, deployment targets, external factors\n   - **Methods**: Approaches, patterns, architectures, practices\n   - **Materials**: Data, dependencies, third-party services, resources\n3. **Dig Deeper**: For each potential cause, ask \"why\" to uncover deeper issues\n4. **Identify Root Causes**: Distinguish contributing factors from fundamental causes\n5. **Prioritize**: Rank causes by impact and likelihood\n6. **Propose Solutions**: Address highest-priority root causes\n\n#### Usage Examples\n\n```bash\n# Analyze performance issues\n> /kaizen:cause-and-effect \"API responses take 3+ seconds\"\n\n# Investigate test reliability\n> /kaizen:cause-and-effect \"15% of test runs fail, passing on retry\"\n\n# Understand delivery delays\n> /kaizen:cause-and-effect \"Feature took 12 weeks instead of 3\"\n```\n\n**Example Output**:\n```\nProblem: API responses take 3+ seconds (target: <500ms)\n\nPEOPLE\n├─ Team unfamiliar with performance optimization\n├─ No one owns performance monitoring\n└─ Frontend team doesn't understand backend constraints\n\nPROCESS\n├─ No performance testing in CI/CD\n├─ No SLA defined for response times\n└─ Performance regression not caught in code review\n\nTECHNOLOGY\n├─ Database queries not optimized\n│  └─ Why: No query analysis tools in place\n├─ N+1 queries in ORM\n│  └─ Why: Eager loading not configured\n└─ No caching layer\n\nROOT CAUSES:\n- No performance requirements defined (Process)\n- Missing performance monitoring tooling (Technology)\n- Architecture doesn't support caching/async (Methods)\n\nSOLUTIONS (Priority Order):\n1. Add database indexes (quick win, high impact)\n2. Implement Redis caching layer (medium effort, high impact)\n3. Define and monitor performance SLAs (low effort, prevents regression)\n```\n\n#### Best practices\n\n- Do not stop at first cause - Explore deeply within each category\n- Look for cross-category connections - Some causes span multiple domains\n- Root causes usually involve process or methods - Not just technology\n- Combine with /kaizen:why - Use Five Whys to dig deeper on specific causes\n- Prioritize by impact x feasibility / effort - Focus on highest-value fixes\n\n---\n\n### /kaizen:analyse-problem - A3 Problem Analysis\n\nComprehensive one-page problem documentation using the A3 format, covering Background, Current Condition, Goal, Root Cause, Countermeasures, Implementation Plan, and Follow-up.\n\n- Purpose - Complete problem documentation for significant issues\n- Output - Structured A3 report with actionable implementation plan\n\n```bash\n/kaizen:analyse-problem [\"problem description\"]\n```\n\n#### Arguments\n\nOptional problem description to document. If not provided, you will be prompted for input.\n\n#### How It Works\n\n1. **Background**: Why this problem matters (context, business impact, urgency)\n2. **Current Condition**: What's happening now (data, metrics, examples - facts, not opinions)\n3. **Goal/Target**: What success looks like (specific, measurable, time-bound)\n4. **Root Cause Analysis**: Why the problem exists (using Five Whys or Fishbone)\n5. **Countermeasures**: Proposed solutions that address root causes (not symptoms)\n6. **Implementation Plan**: Who, what, when, how (timeline, responsibilities, dependencies)\n7. **Follow-up**: How to verify success and prevent recurrence (metrics, monitoring, review dates)\n\n**Named after A3 paper size**, this format forces concise, complete thinking that fits on one page.\n\n#### Usage Examples\n\n```bash\n# Document a production incident\n> /kaizen:analyse-problem \"API downtime due to connection pool exhaustion\"\n\n# Analyze a security vulnerability\n> /kaizen:analyse-problem \"Critical SQL injection vulnerability discovered\"\n\n# Plan a major improvement initiative\n> /kaizen:analyse-problem \"CI/CD pipeline takes 45 minutes\"\n```\n\n**Example Output Structure**:\n```\n═══════════════════════════════════════════════════════════════\n                    A3 PROBLEM ANALYSIS\n═══════════════════════════════════════════════════════════════\n\nTITLE: API Downtime Due to Connection Pool Exhaustion\nOWNER: Backend Team Lead\nDATE: 2024-11-14\n\n1. BACKGROUND\n• API goes down 2-3x per week during peak hours\n• Affects 10,000+ users, average 15min downtime\n• Revenue impact: ~$5K per incident\n\n2. CURRENT CONDITION\n• Connection pool size: 10 (unchanged since launch)\n• Peak concurrent users: 500 (was 300 three weeks ago)\n• Connections leaked: ~2 per hour (never released)\n\n3. GOAL/TARGET\n• Zero downtime due to connection exhaustion\n• Support 1000 concurrent users (2x current peak)\n• Achieve within 1 week\n\n4. ROOT CAUSE ANALYSIS (5 Whys)\nProblem: Connection pool exhausted\nWhy 1: All connections in use, none available\nWhy 2: Connections not released after requests\nWhy 3: Error handling doesn't close connections\nWhy 4: Try-catch blocks missing .finally()\nWhy 5: No code review checklist for resource cleanup\n\n5. COUNTERMEASURES\nImmediate: Audit all DB code, add .finally() for cleanup\nShort-term: Increase pool size, add monitoring\nLong-term: Migrate to connection pool library with auto-release\n\n6. IMPLEMENTATION PLAN\nWeek 1: Fix leaks, increase pool, add monitoring\nWeek 2: Optimize slow queries, create best practices doc\nWeek 3-4: Evaluate and migrate to better pool library\n\n7. FOLLOW-UP\n• Success Metrics: Zero incidents for 4 weeks\n• Monitoring: Daily pool usage dashboard\n• Review Dates: Weekly check-ins until resolved\n═══════════════════════════════════════════════════════════════\n```\n\n#### Best Practices\n\n- Use for significant issues - A3 is overkill for small bugs or one-line fixes\n- Stick to facts - Current Condition should have data, not opinions\n- Countermeasures address root causes - Not just symptoms\n- Clear ownership - Every action item needs an owner and deadline\n- Living document - Update as situation evolves until problem is closed\n- Historical record - A3s become organizational learning artifacts\n\n---\n\n### /kaizen:analyse - Smart Analysis Method Selection\n\nIntelligently selects and applies the most appropriate Kaizen analysis technique based on what you're analyzing: Gemba Walk, Value Stream Mapping, or Muda (Waste) Analysis.\n\n- Purpose - Auto-select best analysis method for your target\n- Output - Detailed analysis using the most appropriate technique\n\n```bash\n/kaizen:analyse [\"target description\"]\n```\n\n#### Arguments\n\nOptional target description (e.g., code area, workflow, or inefficiencies to investigate). You can override auto-selection with METHOD variable.\n\n#### How It Works\n\n**Method Selection Logic**:\n\n| Method | Use When Analyzing |\n|--------|-------------------|\n| **Gemba Walk** | Code implementation, gap between docs and reality, unfamiliar codebase areas |\n| **Value Stream Mapping** | Workflows, CI/CD pipelines, bottlenecks, handoffs between teams |\n| **Muda (Waste)** | Code quality, technical debt, over-engineering, resource utilization |\n\n**Gemba Walk** (\"Go and see\"):\n1. Define scope of code to explore\n2. State assumptions about how it works\n3. Read actual code and observe reality\n4. Document: entry points, data flow, surprises, hidden dependencies\n5. Identify gaps between documentation and implementation\n6. Recommend: update docs, refactor, or accept as-is\n\n**Value Stream Mapping**:\n1. Identify process start and end points\n2. Map all steps including wait/handoff time\n3. Measure processing time vs. waiting time for each step\n4. Calculate efficiency (value-add time / total time)\n5. Identify bottlenecks and waste\n6. Design future state with optimizations\n\n**Muda (Waste) Analysis** - Seven types of waste in software:\n1. **Overproduction**: Features no one uses, premature optimization\n2. **Waiting**: Build time, code review delays, blocked dependencies\n3. **Transportation**: Unnecessary data transformations, API layers with no value\n4. **Over-processing**: Excessive logging, redundant validations\n5. **Inventory**: Unmerged branches, half-finished features, untriaged bugs\n6. **Motion**: Context switching, manual deployments, repetitive tasks\n7. **Defects**: Production bugs, technical debt, flaky tests\n\n#### Usage Examples\n\n```bash\n# Explore unfamiliar code\n> /kaizen:analyse authentication implementation\n\n# Optimize a workflow\n> /kaizen:analyse deployment pipeline\n\n# Find waste in codebase\n> /kaizen:analyse codebase for inefficiencies\n```\n\n#### Best Practices\n\n- Start with Gemba Walk when unfamiliar - Understand reality before optimizing\n- Use VSM for process improvements - CI/CD, deployment, code review workflows\n- Use Muda for efficiency audits - Technical debt, cleanup initiatives\n- Combine methods - Gemba Walk can lead to Muda analysis findings\n- Document findings - Use /kaizen:analyse-problem for comprehensive documentation\n\n---\n\n### /kaizen:plan-do-check-act - PDCA Improvement Cycle\n\nFour-phase iterative cycle for continuous improvement through systematic experimentation: Plan, Do, Check, Act.\n\n- Purpose - Structured approach to measured, sustainable improvements\n- Output - PDCA cycle documentation with baseline, hypothesis, results, and next steps\n\n```bash\n/kaizen:plan-do-check-act [\"improvement goal\"]\n```\n\n#### Arguments\n\nOptional improvement goal or problem to address. If not provided, you will be prompted for input.\n\n#### How It Works\n\n**Phase 1: PLAN**\n1. Define the problem or improvement goal\n2. Analyze current state (baseline metrics)\n3. Identify root causes (use /kaizen:why or /kaizen:cause-and-effect)\n4. Develop hypothesis: \"If we change X, Y will improve\"\n5. Design experiment: what to change, how to measure\n6. Set success criteria (measurable targets)\n\n**Phase 2: DO**\n1. Implement the planned change (small scale first)\n2. Document what was actually done\n3. Record any deviations from plan\n4. Collect data throughout implementation\n5. Note unexpected observations\n\n**Phase 3: CHECK**\n1. Measure results against success criteria\n2. Compare to baseline (before vs. after)\n3. Analyze: did hypothesis hold?\n4. Identify what worked and what did not\n5. Document learnings\n\n**Phase 4: ACT**\n- **If successful**: Standardize the change, update docs, train team, monitor\n- **If unsuccessful**: Learn why, refine hypothesis, start new cycle\n- **If partially successful**: Standardize what worked, plan next cycle for remainder\n\n#### Usage Examples\n\n```bash\n# Reduce build time\n> /kaizen:plan-do-check-act \"Reduce Docker build from 45min to under 10min\"\n\n# Improve code quality\n> /kaizen:plan-do-check-act \"Reduce production bugs from 8 to 4 per month\"\n\n# Speed up code review\n> /kaizen:plan-do-check-act \"Reduce PR merge time from 3 days to 1 day\"\n```\n\n**Example Cycle**:\n```\nCYCLE 1\n───────\nPLAN:\n  Problem: Docker build takes 45 minutes\n  Current State: Full rebuild every time, no layer caching\n  Root Cause: Package manager cache not preserved between builds\n  Hypothesis: Caching dependencies will reduce build to <10 minutes\n  Success Criteria: Build time <10 minutes on unchanged dependencies\n\nDO:\n  - Restructured Dockerfile: COPY package*.json before src files\n  - Added .dockerignore for node_modules\n  - Configured CI cache for Docker layers\n\nCHECK:\n  Results:\n    - Unchanged dependencies: 8 minutes (was 45)\n    - Changed dependencies: 12 minutes (was 45)\n  Analysis: 82% reduction on cached builds, hypothesis confirmed\n\nACT:\n  Standardize:\n    ✓ Merged Dockerfile changes\n    ✓ Updated CI pipeline config\n    ✓ Documented in README\n\n  New Problem: 12 minutes still slow when deps change\n  → Start CYCLE 2\n```\n\n#### Best Practices\n\n- Start small - Make measurable changes, not big overhauls\n- Expect multiple cycles - PDCA is iterative; 2-3 cycles is normal\n- Failed experiments are learning - Document why and adjust hypothesis\n- Success criteria must be measurable - \"Faster\" is not a criteria; \"<10 minutes\" is\n- Standardize successes - Document and train team on what works\n- If stuck after 3 cycles - Revisit root cause analysis\n\n\n## Skills Overview\n\n### kaizen - Continuous Improvement Skill\n\nAutomatically applied skill guiding continuous improvement mindset, error-proofing, standardized work, and just-in-time principles.\n\n#### The Four Pillars of Kaizen\n\nThe Kaizen plugin also includes a skill that applies continuous improvement principles automatically during development:\n\n1. Continuous Improvement - Small, frequent improvements compound into major gains. Always leave code better than you found it.\n2. Poka-Yoke (Error Proofing) - Design systems that prevent errors at compile/design time, not runtime. Make invalid states unrepresentable.\n3. Standardized Work - Follow established patterns. Document what works. Make good practices easy to follow.\n4. Just-In-Time (JIT) - Build what's needed now. No \"just in case\" features. Avoid premature optimization.\n\n---\n\n## Foundation\n\nThe Kaizen plugin is based on methodologies with over 70 years of real-world validation in manufacturing, now adapted for software development:\n\n### Toyota Production System (TPS)\n\nThe foundation of Lean manufacturing, developed at Toyota starting in the 1940s:\n\n- **[The Toyota Way](https://en.wikipedia.org/wiki/The_Toyota_Way)** - 14 principles of continuous improvement and respect for people\n- **[Toyota Kata](https://en.wikipedia.org/wiki/Toyota_Kata)** - Scientific thinking routines for improvement (PDCA)\n- **Proven Results**: Toyota achieved highest quality ratings while reducing production costs by 50%+\n\n### Lean Manufacturing Principles\n\n- **[Kaizen](https://en.wikipedia.org/wiki/Kaizen)** - Philosophy of continuous improvement through small, incremental changes\n- **[Muda (Waste)](https://en.wikipedia.org/wiki/Muda_(Japanese_term))** - Seven types of waste to eliminate\n- **[Value Stream Mapping](https://en.wikipedia.org/wiki/Value-stream_mapping)** - Visualizing process flow to identify improvement opportunities\n- **Industry Impact**: Lean principles have spread to healthcare, software, services, achieving **20-50% efficiency improvements**\n\n### Problem-Solving Techniques\n\n- **[Five Whys](https://en.wikipedia.org/wiki/Five_whys)** - Developed by Sakichi Toyoda, founder of Toyota Industries\n- **[Ishikawa (Fishbone) Diagram](https://en.wikipedia.org/wiki/Ishikawa_diagram)** - Created by Kaoru Ishikawa for quality management\n- **[A3 Problem Solving](https://en.wikipedia.org/wiki/A3_problem_solving)** - Toyota's structured approach to problem documentation\n- **[PDCA Cycle](https://en.wikipedia.org/wiki/PDCA)** - Deming cycle for iterative improvement\n",
        "plugins/customaize-agent/README.md": "# Customaize Agent Plugin\n\nFramework for creating, testing, and optimizing Claude Code extensions including commands, skills, and hooks with built-in prompt engineering best practices.\n\nFocused on:\n\n- **Extension creation** - Interactive assistants for building commands, skills, and hooks with proper structure\n- **TDD for prompts** - RED-GREEN-REFACTOR cycle applied to prompt engineering with subagent testing\n- **Anthropic best practices** - Official guidelines for skill authoring, progressive disclosure, and discoverability\n- **Prompt optimization** - Persuasion principles and token efficiency techniques\n\n## Plugin Target\n\n- Build reusable extensions - Create commands, skills, and hooks that follow established patterns\n- Ensure prompt quality - Test prompts before deployment using isolated subagent scenarios\n- Optimize for discoverability - Apply Claude Search Optimization (CSO) principles\n\n## Overview\n\nThe Customaize Agent plugin provides a complete toolkit for extending Claude Code's capabilities. It applies Test-Driven Development principles to prompt engineering: you write test scenarios first, watch agents fail, create prompts that address those failures, and iterate until bulletproof.\n\nThe plugin is built on Anthropic's official skill authoring best practices and research-backed persuasion principles ([Prompting Science Report 3](https://arxiv.org/abs/2508.00614) - persuasion techniques more than doubled compliance rates from 33% to 72%).\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install customaize-agent@NeoLabHQ/context-engineering-kit\n\n# Create a new agent\n> /customaize-agent:create-agent code-reviewer \"Review code for quality\"\n\n# Create a new command\n> /customaize-agent:create-command validate API documentation\n\n# Create a new skill\n> /customaize-agent:create-skill image-editor\n\n# Test a prompt before deployment\n> /customaize-agent:test-prompt\n\n# Apply Anthropic's best practices to a skill\n> /customaize-agent:apply-anthropic-skill-best-practices\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands Overview\n\n### /customaize-agent:create-command - Command Creation Assistant\n\nInteractive assistant for creating new Claude commands with proper structure, patterns, and MCP tool integration.\n\n- Purpose - Guide through creating well-structured commands\n- Output - Complete command file with frontmatter, sections, and patterns\n\n```bash\n/customaize-agent:create-command [\"command name or description\"]\n```\n\n#### Arguments\n\nOptional command name or description of the command's purpose (e.g., \"validate API documentation\", \"deploy to staging\").\n\n#### Usage Examples\n\n```bash\n# Create an API validation command\n> /customaize-agent:create-command validate API documentation\n\n# Create a deployment command\n> /customaize-agent:create-command deploy feature to staging\n\n# Start without a specific idea\n> /customaize-agent:create-command\n```\n\n#### How It Works\n\n1. **Pattern Research**: Examines existing commands in the target category\n   - Lists commands in project (`.claude/commands/`) or user (`~/.claude/commands/`) directories\n   - Reads similar commands to identify patterns\n   - Notes MCP tool usage, documentation references, and structure\n\n2. **Interactive Interview**: Understands requirements through targeted questions\n   - What problem does this command solve?\n   - Who will use it and when?\n   - Is it interactive or batch?\n   - What's the expected output?\n\n3. **Category Classification**: Determines the command type\n   - Planning (feature ideation, proposals, PRDs)\n   - Implementation (technical execution with modes)\n   - Analysis (review, audit, reports)\n   - Workflow (orchestrate multiple steps)\n   - Utility (simple tools and helpers)\n\n4. **Location Decision**: Chooses where the command should live\n   - Project command (specific to codebase)\n   - User command (available across all projects)\n\n5. **Generation**: Creates the command following established patterns\n   - Proper YAML frontmatter (description, argument-hint)\n   - Task and context sections\n   - MCP tool usage patterns\n   - Human review sections\n   - Documentation references\n\n#### Best Practices\n\n- Research first - Let the assistant examine existing commands before creating new ones\n- Be specific about purpose - Clearly describe what problem the command solves\n- Choose location carefully - Project commands for codebase-specific workflows, user commands for general utilities\n- Include MCP tools - Use MCP tool patterns instead of CLI commands where applicable\n- Add human review sections - Flag decisions that need verification\n\n---\n\n### /customaize-agent:create-workflow-command - Workflow Command Builder\n\nCreate commands that orchestrate multi-step workflows by dispatching sub-agents with task-specific instructions stored in separate files. Solves the **context bloat problem** by keeping orchestrator commands lean.\n\n- Purpose - Build workflow commands that dispatch sub-agents with file-based task prompts\n- Output - Complete workflow structure: orchestrator command, task files, and optional custom agents\n\n```bash\n/customaize-agent:create-workflow-command [workflow-name] [description]\n```\n\n#### Arguments\n\nOptional workflow name (kebab-case) and description of what the workflow accomplishes.\n\n#### Usage Examples\n\n```bash\n# Create a feature implementation workflow\n> /customaize-agent:create-workflow-command feature-implementation \"Research, plan, and implement features\"\n\n# Create a code review workflow\n> /customaize-agent:create-workflow-command code-review \"Multi-phase code analysis and feedback\"\n\n# Start interactive workflow creation\n> /customaize-agent:create-workflow-command\n```\n\n#### How It Works\n\n1. **Gather Requirements**: Collects workflow details\n   - Workflow name and description\n   - List of discrete steps with goals and tools\n   - Execution mode (sequential or parallel)\n   - Agent type preferences\n\n2. **Create Directory Structure**: Sets up the workflow layout\n\n```\nplugins/<plugin-name>/\n├── commands/\n│   └── <workflow>.md          # Lean orchestrator (~50-100 tokens per step)\n├── agents/                     # Optional: reusable executor agents\n│   └── step-executor.md       # Custom agent with specific tools/behavior\n└── tasks/                      # All task instructions directly here\n    ├── step-1-<name>.md       # Full instructions (~500+ tokens each)\n    ├── step-2-<name>.md\n    ├── step-3-<name>.md\n    └── common-context.md      # Shared context across workflows\n```\n\n1. **Create Task Files**: Generates self-contained task instructions\n   - Context and goal for each step\n   - Input/output specifications\n   - Constraints and success criteria\n\n2. **Create Orchestrator Command**: Builds lean dispatch logic\n   - Uses `${CLAUDE_PLUGIN_ROOT}/tasks/` paths for portability\n   - Passes minimal context between steps (summaries, not full data)\n   - Supports sequential, parallel, and stateful (resume) patterns\n\n#### Execution Patterns\n\n| Pattern | Use Case | Description |\n|---------|----------|-------------|\n| **Sequential** | Dependent steps | Each step uses previous step's output |\n| **Parallel** | Independent analysis | Multiple agents run simultaneously |\n| **Stateful (Resume)** | Shared context | Continue same agent across steps |\n\n---\n\n### /customaize-agent:create-agent - Agent Creation Guide\n\nComprehensive guide for creating Claude Code agents with proper structure, triggering conditions, system prompts, and validation. Combines official Anthropic best practices with proven patterns.\n\n- Purpose - Create autonomous agents that handle complex, multi-step tasks independently\n- Output - Complete agent file with frontmatter, triggering examples, and system prompt\n\n```bash\n/customaize-agent:create-agent [agent-name] [optional description]\n```\n\n#### Arguments\n\nOptional agent name (kebab-case) and description of the agent's purpose.\n\n#### Usage Examples\n\n```bash\n# Create a code review agent\n> /customaize-agent:create-agent code-reviewer \"Review code for quality and security\"\n\n# Create a test generation agent\n> /customaize-agent:create-agent test-generator\n\n# Start interactive agent creation\n> /customaize-agent:create-agent\n```\n\n#### How It Works\n\n1. **Gather Requirements**: Collects agent specifications\n   - Agent name (kebab-case, 3-50 characters)\n   - Purpose and core responsibilities\n   - Triggering conditions (when Claude should use this agent)\n   - Required tools (principle of least privilege)\n   - Model requirements (inherit/sonnet/opus/haiku)\n\n2. **Design Triggering**: Creates proper description field\n   - Starts with \"Use this agent when...\"\n   - Includes 2-4 `<example>` blocks with:\n     - Context (situation description)\n     - User request (exact message)\n     - Assistant response (how Claude triggers)\n     - Commentary (reasoning for triggering)\n\n3. **Write System Prompt**: Generates comprehensive prompt\n   - Role statement with specialization\n   - Core responsibilities (numbered list)\n   - Analysis/work process (step-by-step)\n   - Quality standards (measurable criteria)\n   - Output format (specific structure)\n   - Edge cases handling\n\n4. **Validate & Test**: Ensures agent quality\n   - Structural validation (frontmatter, name, description)\n   - Triggering tests with various scenarios\n   - Verification of agent behavior\n\n#### Triggering Patterns\n\n| Pattern | Description | Example |\n|---------|-------------|---------|\n| **Explicit Request** | User directly asks for function | \"Review my code\" |\n| **Implicit Need** | Context suggests agent needed | \"This code is confusing\" |\n| **Proactive Trigger** | After completing relevant work | Code written → review |\n| **Tool Usage Pattern** | Based on prior tool usage | Multiple edits → test analyzer |\n\n#### Frontmatter Fields\n\n| Field | Required | Format | Example |\n|-------|----------|--------|---------|\n| `name` | Yes | lowercase, hyphens, 3-50 chars | `code-reviewer` |\n| `description` | Yes | 10-5000 chars with examples | `Use this agent when...` |\n| `model` | Yes | inherit/sonnet/opus/haiku | `inherit` |\n| `color` | Yes | blue/cyan/green/yellow/magenta/red | `blue` |\n| `tools` | No | Array of tool names | `[\"Read\", \"Grep\"]` |\n\n\n---\n\n### /customaize-agent:create-skill - Skill Development Guide\n\nGuide for creating effective skills using a TDD-based approach. This command treats skill creation as Test-Driven Development applied to process documentation.\n\n- Purpose - Create reusable skills that extend Claude's capabilities\n- Output - Complete skill directory with SKILL.md and optional resources\n\n```bash\n/customaize-agent:create-skill [\"skill name\"]\n```\n\n#### Arguments\n\nOptional skill name (e.g., \"image-editor\", \"pdf-processing\", \"code-review\").\n\n#### Usage Examples\n\n```bash\n# Create an image editing skill\n> /customaize-agent:create-skill image-editor\n\n# Create a database query skill\n> /customaize-agent:create-skill bigquery-analysis\n\n# Start the skill creation workflow\n> /customaize-agent:create-skill\n```\n\n#### How It Works\n\n1. **Understanding with Concrete Examples**: Gathers usage scenarios\n   - What functionality should the skill support?\n   - How would users invoke this skill?\n   - What triggers should activate it?\n\n2. **Planning Reusable Contents**: Analyzes examples to identify resources\n   - Scripts (`scripts/`) - Executable code for deterministic tasks\n   - References (`references/`) - Documentation to load as needed\n   - Assets (`assets/`) - Templates, images, files used in output\n\n3. **Skill Initialization**: Creates proper structure\n   - SKILL.md with YAML frontmatter (name, description)\n   - Resource directories as needed\n   - Proper naming conventions (gerund form: \"Processing PDFs\")\n\n4. **Content Development**: Writes skill documentation\n   - Overview with core principle\n   - When to Use section with triggers and symptoms\n   - Quick Reference for scanning\n   - Implementation details\n   - Common Mistakes section\n\n5. **TDD Testing Cycle**: Applies RED-GREEN-REFACTOR\n   - RED: Run scenarios WITHOUT skill, document failures\n   - GREEN: Write skill addressing those failures\n   - REFACTOR: Close loopholes, iterate until bulletproof\n\n#### Best Practices\n\n- Start with concrete examples - Understand real use cases before writing\n- Apply TDD strictly - No skill without failing tests first\n- Keep SKILL.md lean - Under 500 lines, use separate files for heavy reference\n- Optimize for discovery - Start descriptions with \"Use when...\" and include specific triggers\n- Name by action - Use gerunds like \"Processing PDFs\" not \"PDF Processor\"\n\n---\n\n### /customaize-agent:create-hook - Git Hook Configuration\n\nAnalyze the project, suggest practical hooks, and create them with proper testing. Intelligent project analysis detects tooling and suggests relevant hooks.\n\n- Purpose - Create and configure git hooks with automated testing\n- Output - Working hook script with proper registration\n\n```bash\n/customaize-agent:create-hook [\"hook type or description\"]\n```\n\n#### Arguments\n\nOptional hook type or description of desired behavior (e.g., \"type-check on save\", \"prevent secrets in commits\").\n\n#### Usage Examples\n\n```bash\n# Create a TypeScript type-checking hook\n> /customaize-agent:create-hook type-check TypeScript files\n\n# Create a security scanning hook\n> /customaize-agent:create-hook prevent commits with secrets\n\n# Let the assistant analyze and suggest hooks\n> /customaize-agent:create-hook\n```\n\n#### How It Works\n\n1. **Environment Analysis**: Detects project tooling automatically\n   - TypeScript (`tsconfig.json`) - Suggests type-checking hooks\n   - Prettier (`.prettierrc`) - Suggests formatting hooks\n   - ESLint (`.eslintrc.*`) - Suggests linting hooks\n   - Package scripts - Suggests test/build validation hooks\n   - Git repository - Suggests security scanning hooks\n\n2. **Hook Configuration**: Asks targeted questions\n   - What should this hook do?\n   - When should it run? (PreToolUse, PostToolUse, UserPromptSubmit)\n   - Which tools trigger it? (Write, Edit, Bash, *)\n   - Scope? (global, project, project-local)\n   - Should Claude see and fix issues?\n   - Should successful operations be silent?\n\n3. **Hook Creation**: Generates complete hook setup\n   - Script in `~/.claude/hooks/` or `.claude/hooks/`\n   - Proper executable permissions\n   - Configuration in appropriate `settings.json`\n   - Project-specific commands using detected tooling\n\n4. **Testing & Validation**: Tests both happy and sad paths\n   - Happy path: Create conditions where hook should pass\n   - Sad path: Create conditions where hook should fail/warn\n   - Verification: Check blocking/warning/context behavior\n\n#### Hook Types\n\n| Type | Trigger | Use Case |\n|------|---------|----------|\n| **Code Quality** | PostToolUse | Formatting, linting, type-checking |\n| **Security** | PreToolUse | Block dangerous operations, secrets detection |\n| **Validation** | PreToolUse | Enforce requirements before operations |\n| **Development** | PostToolUse | Automated improvements, documentation |\n\n#### Best Practices\n\n- Test both paths - Always verify both success and failure scenarios\n- Use absolute paths - Avoid relative paths in scripts, use `$CLAUDE_PROJECT_DIR`\n- Read JSON from stdin - Never use argv for hook input\n- Provide specific feedback - Use `additionalContext` for error communication\n- Keep success silent - Use `suppressOutput: true` to avoid context pollution\n\n---\n\n### /customaize-agent:test-skill - Skill Pressure Testing\n\nVerify skills work under pressure and resist rationalization using the RED-GREEN-REFACTOR cycle. Critical for discipline-enforcing skills.\n\n- Purpose - Test skill effectiveness with pressure scenarios\n- Output - Verification report with rationalization table\n\n```bash\n/customaize-agent:test-skill [\"skill path or name\"]\n```\n\n#### Usage Examples\n\n```bash\n# Test a TDD enforcement skill\n> /customaize-agent:test-skill tdd\n\n# Test a custom skill by path\n> /customaize-agent:test-skill ~/.claude/skills/code-review/\n\n# Start testing workflow\n> /customaize-agent:test-skill\n```\n\n#### Arguments\n\nOptional path to skill being tested or skill name.\n\n#### How It Works\n\n1. **RED Phase - Baseline Testing**: Run scenarios WITHOUT the skill\n   - Create pressure scenarios (3+ combined pressures)\n   - Document agent behavior and rationalizations verbatim\n   - Identify patterns in failures\n\n2. **GREEN Phase - Write Minimal Skill**: Address baseline failures\n   - Write skill addressing specific observed rationalizations\n   - Run same scenarios WITH skill\n   - Verify agent now complies\n\n3. **REFACTOR Phase - Close Loopholes**: Iterate until bulletproof\n   - Identify NEW rationalizations from testing\n   - Add explicit counters for each loophole\n   - Build rationalization table\n   - Create red flags list\n   - Re-test until bulletproof\n\n#### Pressure Types\n\n| Pressure | Example |\n|----------|---------|\n| **Time** | Emergency, deadline, deploy window closing |\n| **Sunk cost** | Hours of work, \"waste\" to delete |\n| **Authority** | Senior says skip it, manager overrides |\n| **Economic** | Job, promotion, company survival at stake |\n| **Exhaustion** | End of day, already tired, want to go home |\n| **Social** | Looking dogmatic, seeming inflexible |\n| **Pragmatic** | \"Being pragmatic vs dogmatic\" |\n\n#### Best Practices\n\n- Combine 3+ pressures - Single pressure tests are too weak\n- Document verbatim - Capture exact rationalizations, not summaries\n- Iterate completely - Continue REFACTOR until no new rationalizations\n- Use meta-testing - Ask agents how skill could have been clearer\n- Test all skill types - Discipline-enforcing, technique, pattern, and reference skills need different tests\n\n---\n\n### /customaize-agent:test-prompt - Prompt Testing with Subagents\n\nTest any prompt (commands, hooks, skills, subagent instructions) using the RED-GREEN-REFACTOR cycle with subagents for isolated testing.\n\n- Purpose - Verify prompts produce desired behavior before deployment\n- Output - Test results with improvement recommendations\n\n```bash\n/customaize-agent:test-prompt [\"prompt path or content\"]\n```\n\n#### Usage Examples\n\n```bash\n# Test a command before deployment\n> /customaize-agent:test-prompt .claude/commands/deploy.md\n\n# Test inline prompt content\n> /customaize-agent:test-prompt \"Review this code for security issues\"\n\n# Start interactive testing workflow\n> /customaize-agent:test-prompt\n```\n\n#### Arguments\n\nOptional path to prompt file or inline prompt content to test.\n\n#### How It Works\n\n1. **RED Phase - Baseline Testing**: Run without prompt using subagent\n   - Design test scenarios appropriate for prompt type\n   - Launch subagent WITHOUT prompt\n   - Document agent behavior, actions, and mistakes\n\n2. **GREEN Phase - Write Minimal Prompt**: Make tests pass\n   - Address specific baseline failures\n   - Apply appropriate degrees of freedom\n   - Use persuasion principles if discipline-enforcing\n   - Test WITH prompt using subagent\n\n3. **REFACTOR Phase - Optimize**: Improve while staying green\n   - Close loopholes for discipline violations\n   - Improve clarity using meta-testing\n   - Reduce tokens without losing behavior\n   - Re-test with fresh subagents\n\n#### Why Subagents?\n\n| Benefit | Description |\n|---------|-------------|\n| **Clean slate** | No conversation history affecting behavior |\n| **Isolation** | Test only the prompt, not accumulated context |\n| **Reproducibility** | Same starting conditions every run |\n| **Parallelization** | Test multiple scenarios simultaneously |\n| **Objectivity** | No bias from prior interactions |\n\n#### Prompt Types & Testing Strategies\n\n| Prompt Type | Test Focus | Example |\n|-------------|------------|---------|\n| **Instruction** | Steps followed correctly? | Git workflow command |\n| **Discipline-enforcing** | Resists rationalization? | TDD compliance skill |\n| **Guidance** | Applied appropriately? | Architecture patterns |\n| **Reference** | Accurate and accessible? | API documentation |\n| **Subagent** | Task accomplished reliably? | Code review prompt |\n\n#### Best Practices\n\n- Use fresh subagents - Always via Task tool for isolated testing\n- Design realistic scenarios - Include constraints, pressures, edge cases\n- Document exact failures - \"Agent was wrong\" doesn't tell you what to fix\n- Avoid over-engineering - Only address failures you documented in baseline\n- Iterate on token efficiency - Reduce tokens without losing behavior\n\n---\n\n### /customaize-agent:apply-anthropic-skill-best-practices - Skill Optimization\n\nComprehensive guide for skill development based on Anthropic's official best practices. Use for complex skills requiring detailed structure and optimization.\n\n- Purpose - Apply official guidelines to skill authoring\n- Output - Optimized skill with improved discoverability\n\n```bash\n/customaize-agent:apply-anthropic-skill-best-practices [\"skill path\"]\n```\n\n#### Arguments\n\nOptional skill name or path to skill being reviewed.\n\n#### Usage Examples\n\n```bash\n# Optimize an existing skill\n> /customaize-agent:apply-anthropic-skill-best-practices pdf-processing\n\n# Review a skill by path\n> /customaize-agent:apply-anthropic-skill-best-practices ~/.claude/skills/bigquery/\n\n# Start optimization workflow\n> /customaize-agent:apply-anthropic-skill-best-practices\n```\n\n#### How It Works\n\n1. **Structure Review**: Checks skill organization\n   - YAML frontmatter (name: 64 chars max, description: 1024 chars max)\n   - SKILL.md body under 500 lines\n   - Progressive disclosure with separate files\n   - One-level-deep references\n\n2. **Description Optimization**: Improves discoverability\n   - Third-person writing (injected into system prompt)\n   - \"Use when...\" trigger conditions\n   - Specific keywords and terms\n   - Both what it does AND when to use it\n\n3. **Content Guidelines**: Applies best practices\n   - Avoid time-sensitive information\n   - Consistent terminology throughout\n   - Concrete examples over abstract descriptions\n   - Template patterns and examples patterns\n\n4. **Workflow Enhancement**: Adds feedback loops\n   - Clear sequential steps with checklists\n   - Validation steps for critical operations\n   - Conditional workflow patterns\n\n5. **Token Efficiency**: Optimizes for context window\n   - Remove redundant explanations\n   - Challenge each paragraph's token cost\n   - Use progressive disclosure appropriately\n\n#### Key Principles\n\n| Principle | Description |\n|-----------|-------------|\n| **Progressive Disclosure** | Metadata always loaded, SKILL.md on trigger, resources as needed |\n| **CSO (Claude Search Optimization)** | Rich descriptions with triggers, keywords, and symptoms |\n| **Degrees of Freedom** | Match specificity to task fragility |\n| **Conciseness** | Only add context Claude doesn't already have |\n\n#### Best Practices\n\n- Test with all models - What works for Opus may need more detail for Haiku\n- Iterate with Claude - Use Claude A to design, Claude B to test\n- Observe navigation - Watch how Claude actually uses the skill\n- Build evaluations first - Create test scenarios BEFORE extensive documentation\n- Gather team feedback - Address blind spots from different usage patterns\n\n---\n\n## Skills\n\n### prompt-engineering\n\nAdvanced prompt engineering techniques including Anthropic's official best practices and research-backed persuasion principles.\n\n**Includes:**\n\n- **Few-Shot Learning** - Teach by showing examples\n- **Chain-of-Thought** - Step-by-step reasoning\n- **Prompt Optimization** - Systematic improvement through testing\n- **Template Systems** - Reusable prompt structures\n- **System Prompt Design** - Global behavior and constraints\n\n**Persuasion Principles (from [Prompting Science Report 3](https://arxiv.org/abs/2508.00614)):**\n\n| Principle | Use For | Example |\n|-----------|---------|---------|\n| **Authority** | Discipline enforcement | \"YOU MUST\", \"No exceptions\" |\n| **Commitment** | Accountability | \"Announce skill usage\", \"Choose A, B, or C\" |\n| **Scarcity** | Preventing procrastination | \"IMMEDIATELY\", \"Before proceeding\" |\n| **Social Proof** | Establishing norms | \"Every time\", \"X without Y = failure\" |\n| **Unity** | Collaboration | \"our codebase\", \"we both want quality\" |\n\n**Key Concepts:**\n\n- **Context Window Management** - The window is a shared resource; be concise\n- **Degrees of Freedom** - Match specificity to task fragility\n- **Progressive Disclosure** - Start simple, add complexity when needed\n\n### context-engineering\n\nUse when writing, editing, or optimizing commands, skills, or sub-agent prompts. Provides deep understanding of context mechanics in agent systems.\n\n**The Anatomy of Context:**\n\n| Component | Role | Key Insight |\n|-----------|------|-------------|\n| **System Prompts** | Core identity and constraints | Balance specificity vs flexibility (\"right altitude\") |\n| **Tool Definitions** | Available actions | Poor descriptions force guessing; optimize with examples |\n| **Retrieved Documents** | Domain knowledge | Use just-in-time loading, not pre-loading |\n| **Message History** | Conversation state | Can dominate context in long tasks |\n| **Tool Outputs** | Action results | Up to 83.9% of total context usage |\n\n**Key Principles:**\n\n- **Attention Budget** - Context is finite; every token depletes the budget\n- **Progressive Disclosure** - Load information only when needed\n- **Quality over Quantity** - Smallest high-signal token set wins\n- **Lost-in-Middle Effect** - Critical info at start/end, not middle\n\n**Practical Patterns:**\n\n- File-system based access for progressive disclosure\n- Hybrid strategies (pre-load some, load rest on-demand)\n- Explicit context budgeting with compaction triggers\n\n### agent-evaluation\n\nUse when testing prompt effectiveness, validating context engineering choices, or measuring agent improvement quality.\n\n**Evaluation Approaches:**\n\n- **LLM-as-Judge** - Direct scoring, pairwise comparison, rubric-based\n- **Outcome-Focused** - Judge results, not exact paths (agents may take valid alternative routes)\n- **Multi-Level Testing** - Simple to complex queries, isolated to extended interactions\n- **Bias Mitigation** - Position bias, verbosity bias, self-enhancement bias\n\n**Multi-Dimensional Evaluation Rubric:**\n\n| Dimension | Weight | What It Measures |\n|-----------|--------|------------------|\n| Instruction Following | 0.30 | Task adherence |\n| Output Completeness | 0.25 | Coverage of requirements |\n| Tool Efficiency | 0.20 | Optimal tool selection |\n| Reasoning Quality | 0.15 | Logical soundness |\n| Response Coherence | 0.10 | Structure and clarity |\n\n## Theoretical Foundation\n\nThe Customaize Agent plugin is based on:\n\n### Persuasion Research\n\n- **[Prompting Science Report 3](https://arxiv.org/abs/2508.00614)** - Tested 7 persuasion principles with N=28,000 AI conversations. Persuasion techniques more than doubled compliance rates (33% to 72%, p < .001), based on related SSRN work on persuasion principles.\n\n### Agent Skills for Context Engineering\n\n- [Agent Skills for Context Engineering project](https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering) by Murat Can Koylan.\n",
        "plugins/docs/README.md": "# Docs Plugin\n\nTechnical documentation management plugin that maintains living documentation throughout the development lifecycle, ensuring docs stay accurate, useful, and aligned with code changes.\n\n## Plugin Target\n\n- Reduce documentation debt - Identify and remove outdated or duplicate documentation\n- Improve discoverability - Ensure documentation is findable when users need it\n- Maintain accuracy - Keep docs synchronized with implementation changes\n- Focus effort - Document only what provides real value to users\n\nFocused on:\n\n- **Living documentation** - Documentation that evolves with your codebase\n- **Smart prioritization** - Focus on high-impact documentation that helps users accomplish real tasks\n- **Automation integration** - Leverage generated docs (OpenAPI, JSDoc, GraphQL) where appropriate\n- **Documentation hygiene** - Prevent documentation debt and bloat\n\n## Overview\n\nThe Docs plugin provides a structured approach to documentation management based on the principle that documentation must justify its existence. It implements a documentation philosophy that prioritizes user tasks over comprehensive coverage, preferring automation where possible and manual documentation where it adds unique value.\n\nThe plugin guides you through:\n\n- **Documentation audit** - Assess existing docs for freshness, accuracy, and value\n- **Gap analysis** - Identify high-impact documentation needs\n- **Smart updates** - Create or update documentation with clear purpose\n- **Quality validation** - Verify that examples work and links are valid\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install docs@NeoLabHQ/context-engineering-kit\n\n# Update project documentation after implementing features\n> claude \"implement user profile settings page\"\n> /docs:update-docs\n\n# Focus on specific documentation type\n> /docs:update-docs api\n\n# Target specific directory\n> /docs:update-docs src/payments/\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands Overview\n\n### /docs:update-docs - Documentation Update\n\nComprehensive documentation update command that analyzes your project, identifies documentation needs, and creates or updates documentation following best practices.\n\n- Purpose - Maintain accurate, useful project documentation\n- Output - Updated README files, API docs, JSDoc comments, and guides\n\n```bash\n/docs:update-docs [\"target directory or documentation type\"]\n```\n\n#### Arguments\n\nOptional target specification:\n\n- **Directory path** (e.g., `src/auth/`) - Focus documentation updates on specific module\n- **Documentation type** (e.g., `api`, `guides`, `readme`, `jsdoc`) - Target specific documentation category\n- **No argument** - Full project documentation assessment and update\n\n#### How It Works\n\n1. **Codebase Analysis**: Discovers project structure and existing documentation\n   - Inventories all documentation files (README, docs/, API specs)\n   - Checks for generated documentation (OpenAPI, GraphQL schemas)\n   - Identifies JSDoc/TSDoc coverage\n   - Maps project frameworks and tools in use\n\n2. **User Journey Mapping**: Identifies critical documentation paths\n   - Developer onboarding flow\n   - API consumption journey\n   - Feature usage patterns\n   - Troubleshooting scenarios\n\n3. **Gap Analysis**: Evaluates documentation health\n   - High-impact gaps (missing setup instructions, undocumented APIs)\n   - Quality assessment (freshness, accuracy, discoverability)\n   - Duplication detection\n   - Low-value content identification\n\n4. **Strategic Updates**: Implements prioritized improvements\n   - Fixes critical onboarding blockers first\n   - Updates outdated examples and broken links\n   - Adds missing API examples for common use cases\n   - Creates module navigation READMEs\n\n5. **Validation**: Ensures documentation quality\n   - Tests code examples\n   - Verifies links work\n   - Confirms documentation serves real user needs\n\n#### Documentation Types Updated\n\n**README Files:**\n\n- **Project root README** - Quick start, overview, key links\n- **Module READMEs** - Purpose statement, key exports, minimal usage example\n- **Feature READMEs** - Navigation aid for complex feature directories\n\n**API Documentation:**\n\n- **OpenAPI/Swagger** - REST API specifications from code annotations\n- **GraphQL schemas** - Type definitions and query documentation\n- **Endpoint examples** - Request/response samples with realistic data\n\n**Code Documentation:**\n\n- **JSDoc/TSDoc** - Function contracts for complex business logic\n- **Inline comments** - Non-obvious implementation decisions\n- **Type definitions** - Complex interfaces and type aliases\n\n**Guides and References:**\n\n- **Getting started** - Fastest path to first success\n- **How-to guides** - Task-oriented problem-solving docs\n- **Troubleshooting** - Common problems with proven solutions\n- **Architecture decisions** - When they affect user experience\n\n#### Usage Examples\n\n```bash\n# Full project documentation update\n> /docs:update-docs\n\n# Update API documentation after adding new endpoints\n> claude \"add /api/v2/subscriptions endpoint\"\n> /docs:update-docs api\n\n# Document a specific module after changes\n> /docs:update-docs src/payments/\n\n# Focus on README files only\n> /docs:update-docs readme\n\n# Update JSDoc comments for complex business logic\n> /docs:update-docs jsdoc\n```\n\n## Quality Gates\n\nThe command enforces documentation quality through validation:\n\n**Before Publishing:**\n\n- All code examples tested and working\n- Links verified (no 404s)\n- Document purpose clearly stated\n- Audience and prerequisites identified\n- No duplication of generated docs\n- Maintenance plan established\n\n**Success Metrics:**\n\n- Users complete common tasks without asking questions\n- Issues contain more bug reports, fewer \"how do I...?\" questions\n- Documentation is referenced in code reviews and discussions\n- New contributors can get started independently\n",
        "plugins/tech-stack/README.md": "# Tech Stack Plugin\n\nLanguage and framework-specific best practices plugin that configures your CLAUDE.md with standardized coding standards, ensuring consistent code quality across all AI-assisted development.\n\nFocused on:\n\n- **Standardized Guidelines** - Pre-defined best practices for specific languages and frameworks\n- **Initial context building** - Updates of CLAUDE.md, so it will be loaded during every claude code session\n\n## Overview\n\nThe Tech Stack plugin provides commands for setting up language and framework-specific best practices in your CLAUDE.md file. Instead of manually defining coding standards, this plugin provides curated, production-tested guidelines that can be applied with a single command.\n\nWhen Claude operates with explicit coding standards in CLAUDE.md, it produces more consistent and higher-quality code. The Tech Stack plugin bridges the gap between starting a new project and having well-defined development standards.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install tech-stack@NeoLabHQ/context-engineering-kit\n\n# Add TypeScript best practices to your project\n/tech-stack:add-typescript-best-practices\n\n# Review the updated CLAUDE.md\ncat CLAUDE.md\n```\n\n[Usage Examples](./usage-examples.md)\n\n\n### Why CLAUDE.md Matters\n\nCLAUDE.md is read by Claude at the start of every conversation. By placing coding standards here:\n\n1. **Persistent Context** - Guidelines are always available to Claude\n2. **Project-Specific Rules** - Different projects can have different standards\n3. **Team Synchronization** - All team members share the same AI configuration\n4. **Version Control** - Guidelines are tracked alongside your code\n\n## Commands Overview\n\n### /tech-stack:add-typescript-best-practices - TypeScript Configuration\n\nSets up TypeScript best practices and code style rules in your CLAUDE.md file, providing Claude with explicit guidelines for generating consistent, type-safe code.\n\n- Purpose - Configure TypeScript coding standards\n- Output - Updated CLAUDE.md with TypeScript guidelines\n\n```bash\n/tech-stack:add-typescript-best-practices\n```\n\n#### Arguments\n\nOptional argument which practices to add or avoid.\n\n#### How It Works\n\n1. **File Detection**: Locates or creates CLAUDE.md in your project root\n\n2. **Content Injection**: Adds the following standardized sections:\n   - **Code Style Rules** - General principles for TypeScript development\n   - **Type System Guidelines** - Interface vs type preferences, enum usage\n   - **Library-First Approach** - Recommended libraries for common tasks\n   - **Code Quality Patterns** - Destructuring, time handling, and more\n\n3. **Non-Destructive Update**: Preserves existing CLAUDE.md content while adding new guidelines\n\n",
        "plugins/mcp/README.md": "# MCP Plugin\n\nCommands for integrating Model Context Protocol (MCP) servers with your AI-powered development workflow. Set up well-known MCP servers and create custom servers to extend LLM capabilities.\n\n## Plugin Target\n\nSimplify integration of MCP servers into your development workflow.\n\n## Overview\n\nThe MCP (Model Context Protocol) plugin helps you integrate MCP servers into your development environment. MCP is an open protocol that enables AI assistants to interact with external services, databases, and tools through a standardized interface.\n\nThis plugin provides five key commands:\n\n1. **Context7 MCP Setup** - Access up-to-date documentation for any library or framework\n2. **Serena MCP Setup** - Enable semantic code analysis and symbol-based operations\n3. **Codemap CLI Setup** - Enable intelligent codebase visualization and navigation\n4. **arXiv/Paper Search MCP Setup** - Search and download academic papers from multiple sources\n5. **Build MCP** - Create custom MCP servers for any service or API\n\nEach setup command supports configuration at multiple levels:\n\n- **Project level (shared)** - Configuration tracked in git, shared with team via `./CLAUDE.md`\n- **Project level (personal)** - Local configuration in `./CLAUDE.local.md`, not tracked in git\n- **User level (global)** - Configuration in `~/.claude/CLAUDE.md`, applies to all projects\n\nThe command guides through the MCP setup process and updates the appropriate CLAUDE.md file based on your choice to ensure consistent MCP usage.\n\n## Quick Start\n\nOpen Claude Code in your project directory and run the following commands to setup MCP servers.\n\n```bash\n# Install the plugin\n/plugin install mcp@NeoLabHQ/context-engineering-kit\n\n# Set up documentation access for your project\n> /mcp:setup-context7-mcp react, typescript, prisma\n\n# Enable semantic code analysis\n> /mcp:setup-serena-mcp\n\n# Set up codebase visualization\n> /mcp:setup-codemap-cli\n```\n\n[Usage Examples](./usage-examples.md)\n\n## Commands Overview\n\n### /mcp:setup-context7-mcp - Documentation Access\n\nSet up Context7 MCP server to provide real-time access to library and framework documentation, eliminating hallucinations from outdated training data.\n\n- Purpose - Configure documentation access for your project's technology stack\n- Output - Working Context7 integration with CLAUDE.md configuration\n\n```bash\n/mcp:setup-context7-mcp [technologies]\n```\n\n#### What is Context7?\n\nContext7 is an MCP server that fetches up-to-date documentation with code examples for any library or framework. Instead of relying on potentially outdated training data, the LLM can query actual documentation in real-time.\n\nBenefits:\n- Access latest API references and code examples\n- Eliminate hallucinations about deprecated methods or incorrect signatures\n- Get version-specific documentation for your exact dependencies\n- Reduce back-and-forth when the LLM suggests outdated patterns\n\n#### Arguments\n\nOptional list of languages and frameworks to configure documentation for. If omitted, the command analyzes your project structure to identify relevant technologies.\n\nExamples:\n- `react, typescript, prisma` - Specific technologies\n- `nextjs 14, tailwind` - Version-specific documentation\n- (no arguments) - Auto-detect from project files\n\n#### How It Works\n\n1. **Availability Check**: Verifies if Context7 MCP server is already configured\n2. **Setup Guidance**: If not available, guides you through the installation process for your operating system and development environment\n3. **Technology Analysis**: Parses your input or scans project structure to identify relevant documentation\n4. **Documentation Search**: Queries Context7 to find available documentation IDs for your technologies\n5. **CLAUDE.md Update**: Adds recommended library IDs and usage instructions to your project configuration\n\n#### Usage Examples\n\n```bash\n# Configure for a React/TypeScript project\n> /mcp:setup-context7-mcp react, typescript, @tanstack/react-query\n\n# Let the command detect technologies from your project\n> /mcp:setup-context7-mcp\n\n# Specific framework versions\n> /mcp:setup-context7-mcp nextjs 14, prisma 5, zod\n```\n\nAfter setup, your CLAUDE.md will include:\n\n```markdown\n### Use Context7 MCP for Loading Documentation\n\nContext7 MCP is available to fetch up-to-date documentation with code examples.\n\n**Recommended library IDs**:\n\n- `react` - React core library documentation\n- `typescript` - TypeScript language reference\n- `prisma` - Prisma ORM documentation\n```\n\n#### Best Practices\n\n- Run early in project setup to establish documentation access from the start\n- Include specific versions when working with rapidly evolving libraries\n- Review the generated documentation IDs and remove any that are not relevant\n- Re-run when adding new major dependencies to your project\n\n---\n\n### /mcp:setup-serena-mcp - Semantic Code Analysis\n\nSet up Serena MCP server for semantic code retrieval and symbol-based editing capabilities, enabling precise code manipulation in large codebases.\n\n- Purpose - Enable intelligent code navigation and manipulation\n- Output - Configured Serena integration with indexed project\n\n```bash\n/mcp:setup-serena-mcp [configuration preferences]\n```\n\n#### What is Serena?\n\nSerena is an MCP server that provides semantic understanding of your codebase. Unlike text-based search (grep), Serena understands code structure - functions, classes, types, and their relationships.\n\nBenefits:\n- Find symbols by meaning, not just text matching\n- Navigate complex codebases with symbol-based operations\n- Make precise code changes without breaking references\n- Understand code relationships and dependencies\n- Refactor with confidence using semantic operations\n\n#### Arguments\n\nOptional configuration preferences or client type. The command adapts its setup guidance based on your development environment (Claude Code, Claude Desktop, Cursor, VSCode, etc.).\n\n#### How It Works\n\n1. **Availability Check**: Tests if Serena tools (`find_symbol`, `list_symbols`) are accessible\n2. **Documentation Loading**: Fetches latest Serena documentation for setup guidance\n3. **Prerequisites Verification**: Confirms `uv` is installed (required for running Serena)\n4. **Client Configuration**: Provides setup instructions specific to your MCP client\n5. **Project Setup**: Guides through project initialization and indexing\n6. **Connection Test**: Verifies Serena tools are working correctly\n7. **CLAUDE.md Update**: Adds semantic code analysis guidelines to your project\n\n#### Usage Examples\n\n```bash\n# Standard setup with auto-detection\n> /mcp:setup-serena-mcp\n\n# Specify your client\n> /mcp:setup-serena-mcp cursor\n\n# With specific configuration needs\n> /mcp:setup-serena-mcp claude-desktop\n```\n\nAfter setup, your CLAUDE.md will include:\n\n```markdown\n### Use Serena MCP for Semantic Code Analysis\n\nSerena MCP is available for advanced code retrieval and editing capabilities.\n\n- Use Serena's tools for precise code manipulation in structured codebases\n- Prefer symbol-based operations over file-based grep/sed operations\n\nKey usage points:\n- Use `find_symbol` to locate functions, classes, and types by name\n- Use `list_symbols` to explore available symbols in a file or module\n- Prefer semantic operations for refactoring over text replacement\n```\n\n#### Best Practices\n\n- Set up Serena for large codebases where text search becomes unwieldy\n- Use semantic operations for refactoring to ensure all references are updated\n- Re-index the project after major structural changes\n- Combine with Context7 for documentation + code understanding\n- Prefer symbol-based navigation over grep for code exploration\n\n---\n\n### /mcp:setup-codemap-cli - Codebase Visualization\n\nSet up Codemap CLI for intelligent codebase visualization and navigation, providing tree views, dependency analysis, and change tracking.\n\n- Purpose - Enable comprehensive codebase understanding and navigation\n- Output - Working Codemap installation with CLAUDE.md configuration\n\n```bash\n/mcp:setup-codemap-cli [OS type or configuration preferences]\n```\n\n#### What is Codemap?\n\nCodemap is a CLI tool that provides intelligent codebase visualization and navigation. It generates tree views, tracks changes, analyzes dependencies, and integrates with Claude Code through hooks.\n\nBenefits:\n- Visualize project structure with smart filtering\n- Track changes vs main branch at a glance\n- Analyze file dependencies and import relationships\n- Integrate with Claude Code through session hooks\n- Generate city skyline visualizations of codebase\n\n#### Arguments\n\nOptional OS type or configuration preferences. The command auto-detects your operating system and provides appropriate installation instructions.\n\nExamples:\n- (no arguments) - Auto-detect OS and install\n- `macos` - macOS-specific instructions\n- `windows` - Windows-specific instructions\n\n#### How It Works\n\n1. **Installation Check**: Verifies if Codemap is already installed via `codemap --version`\n2. **Documentation Loading**: Fetches latest Codemap documentation from GitHub\n3. **Installation Guidance**: Provides OS-specific installation commands (Homebrew for macOS/Linux, Scoop for Windows)\n4. **Verification**: Tests installation with basic commands\n5. **CLAUDE.md Update**: Adds Codemap usage instructions and hook configuration\n6. **.gitignore Update**: Adds `.codemap/` directory to ignore list\n\n#### Usage Examples\n\n```bash\n# Standard setup with auto-detection\n> /mcp:setup-codemap-cli\n\n# Specify your OS\n> /mcp:setup-codemap-cli macos\n> /mcp:setup-codemap-cli windows\n```\n\nAfter setup, your CLAUDE.md will include:\n\n```markdown\n## Use Codemap CLI for Codebase Navigation\n\nCodemap CLI is available for intelligent codebase visualization and navigation.\n\n**Required Usage** - You MUST use `codemap --diff --ref master` to research changes different from default branch, and `git diff` + `git status` to research current working state.\n\n### Quick Start\n\ncodemap .                    # Project tree\ncodemap --only md .          # Just Markdown files\ncodemap --diff --ref master  # What changed vs master\ncodemap --deps .             # Dependency flow\n```\n\nThe command also configures Claude Code hooks in `.claude/settings.json` for automatic session context.\n\n#### Best Practices\n\n- Run at project start to establish codebase understanding\n- Use hooks to maintain context during long coding sessions\n- Combine `--diff` with `--ref` to compare against your main branch\n- Use `--deps` to understand module relationships before refactoring\n- Exclude generated files and assets with `--exclude` for cleaner output\n\n---\n\n### /mcp:setup-arxiv-mcp - Academic Paper Search\n\nSet up the Paper Search MCP server via Docker MCP for searching and downloading academic papers from multiple sources including arXiv, PubMed, Semantic Scholar, and more.\n\n- Purpose - Enable academic paper search and retrieval for research workflows\n- Output - Working Paper Search MCP integration with CLAUDE.md configuration\n\n```bash\n/mcp:setup-arxiv-mcp [research topics or configuration]\n```\n\n#### What is Paper Search MCP?\n\nPaper Search MCP is a Docker-based MCP server that provides comprehensive access to academic literature. It aggregates search across multiple academic sources and enables downloading and reading papers directly.\n\nBenefits:\n- Search papers across arXiv, PubMed, bioRxiv, medRxiv, Semantic Scholar, and more\n- Download PDFs and extract text content for analysis\n- Filter by year, author, and other metadata\n- Access cryptography papers via IACR\n- Cross-reference with DOI via CrossRef\n\n#### Arguments\n\nOptional research topics or specific paper sources to configure. The command will guide you through Docker MCP setup if not already available.\n\nExamples:\n- (no arguments) - Standard setup with all paper sources\n- `machine learning, transformers` - Mention specific research areas\n- `cryptography` - Focus on specific domain\n\n#### Prerequisites\n\n- **Docker Desktop** - Required for Docker MCP integration\n- **Docker MCP Toolkit** - For managing MCP servers via Docker\n\n#### How It Works\n\n1. **Docker MCP Check**: Verifies Docker MCP is available\n2. **Server Search**: Finds and adds `paper-search` MCP server from Docker catalog\n3. **Activation**: Enables the server's tools in your session\n4. **Connection Test**: Verifies search functionality works\n5. **CLAUDE.md Update**: Adds paper search usage instructions\n\n#### Available Tools\n\n**Search Tools**:\n- `search_arxiv` - Search arXiv preprints (physics, math, CS, etc.)\n- `search_pubmed` - Search PubMed biomedical literature\n- `search_biorxiv` / `search_medrxiv` - Search biology/medicine preprints\n- `search_semantic` - Search Semantic Scholar with year filters\n- `search_google_scholar` - Broad academic search\n- `search_iacr` - Search cryptography papers (IACR ePrint)\n- `search_crossref` - Search by DOI/citation metadata\n\n**Download and Read Tools**:\n- `download_arxiv` / `read_arxiv_paper` - Download/read arXiv PDFs\n- `download_biorxiv` / `read_biorxiv_paper` - Download/read bioRxiv PDFs\n- `download_semantic` / `read_semantic_paper` - Download/read via Semantic Scholar\n\n#### Usage Examples\n\n```bash\n# Standard setup\n> /mcp:setup-arxiv-mcp\n\n# After setup, search for papers\n> read transformer attention mechanism paper\n\n# Search Semantic Scholar with year filter\n> search large language models papers from 2023\n\n# Download and read a paper\n> read paper 2106.12345\n```\n\nAfter setup, your CLAUDE.md will include:\n\n---\n\n### /mcp:build-mcp - Custom MCP Server Development\n\nComprehensive guide for creating high-quality MCP servers that enable LLMs to interact with external services through well-designed tools.\n\n- Purpose - Build custom MCP servers for any service or API\n- Output - Production-ready MCP server with tools and evaluations\n\n```bash\n/mcp:build-mcp\n```\n\n#### When to Build Custom MCP Servers\n\nBuild an MCP server when you need the LLM to:\n- Interact with internal company APIs or services\n- Access databases or data sources not available via existing MCP servers\n- Integrate with third-party services (CRMs, project management, communication tools)\n- Perform specialized operations unique to your domain\n\n#### How It Works\n\nThe command guides you through a four-phase development process:\n\n**Phase 1: Deep Research and Planning**\n\n1. **Agent-Centric Design Principles**\n   - Build workflow tools, not just API wrappers\n   - Optimize for limited context windows\n   - Design actionable error messages\n   - Follow natural task subdivisions\n\n2. **Protocol Study**: Load MCP specification from `modelcontextprotocol.io`\n\n3. **Framework Selection**:\n   - Python with FastMCP for rapid development\n   - TypeScript with MCP SDK for type safety\n\n4. **API Research**: Exhaustively study the target API documentation\n\n5. **Implementation Planning**:\n   - Tool selection and prioritization\n   - Shared utilities design\n   - Input/output schema design\n   - Error handling strategy\n\n**Phase 2: Implementation**\n\n1. **Project Structure**: Set up according to language-specific best practices\n2. **Core Infrastructure**: Build shared utilities first (API helpers, error handling, formatting)\n3. **Tool Implementation**: Systematically implement each planned tool\n4. **Annotations**: Add proper tool hints (readOnly, destructive, idempotent)\n\n**Phase 3: Review and Refine**\n\n1. **Code Quality Review**: DRY principle, composability, consistency\n2. **Testing**: Verify syntax and imports (note: MCP servers are long-running, use evaluation harness)\n3. **Quality Checklist**: Language-specific verification\n\n**Phase 4: Create Evaluations**\n\n1. **Tool Inspection**: Understand available capabilities\n2. **Content Exploration**: Use read-only operations to explore data\n3. **Question Generation**: Create 10 complex, realistic evaluation questions\n4. **Answer Verification**: Verify each answer is correct and stable\n\n#### Usage Examples\n\n```bash\n# Start building an MCP server\n> /mcp:build-mcp\n\n# The command will guide you through:\n# 1. Understanding your integration requirements\n# 2. Choosing Python or TypeScript\n# 3. Designing tools for your use case\n# 4. Implementing with best practices\n# 5. Testing and evaluation\n```\n",
        "plugins/fpf/README.md": "# First Principles Framework (FPF) Plugin\n\nStructured reasoning plugin that makes AI decision-making transparent and auditable through hypothesis generation, logical verification, and evidence-based validation.\n\nFocused on:\n\n- **Transparent reasoning** - All decisions documented with full audit trails\n- **Hypothesis-driven analysis** - Generate competing alternatives before evaluating\n- **Evidence-based validation** - Computed reliability scores, not estimates\n- **Human-in-the-loop** - AI generates options; humans decide (Transformer Mandate)\n\n## Plugin Target\n\n- Make AI reasoning auditable - full trail from hypothesis to decision\n- Prevent premature conclusions - enforce systematic evaluation of alternatives\n- Build project knowledge over time - decisions become reusable knowledge\n- Enable informed decision-making - trust scores based on evidence quality\n\n## Overview\n\nThe FPF plugin implements structured reasoning using [the First Principles Framework](https://github.com/ailev/FPF) methodology developed by Anatoly Levenchuk a methodology for rigorous, auditable reasoning. The killer feature is turning the black box of AI reasoning into a transparent, evidence-backed audit trail. \n\nThe core cycle follows three modes of inference:\n\n- Abduction — Generate competing hypotheses (don't anchor on the first idea).\n- Deduction — Verify logic and constraints (does the idea make sense?).\n- Induction — Gather evidence through tests or research (does the idea work in reality?).\n\nThen, audit for bias, decide, and document the rationale in a durable record.\n\nThe framework addresses a fundamental challenge in AI-assisted development: making decision-making processes transparent and auditable. Rather than having AI jump to solutions, FPF enforces generating competing hypotheses, checking them logically, testing against evidence, then letting developers choose the path forward.\n\n> **Warning:** This plugin loads the core FPF specification into context, which is large (~600k tokens). As a result it loaded into a subagent with Sonnet[1m] model. But such agent can consume your token limit quickly.\n\nImplementation based on [quint-code](https://github.com/m0n0x41d/quint-code) by m0n0x41d.\n\n## Quick Start\n\n```bash\n# Install the plugin\n/plugin install fpf@NeoLabHQ/context-engineering-kit\n\n# Start a decision process\n/fpf:propose-hypotheses What caching strategy should we use?\n\n# Commad will perform majority of orcestration and launch subagents to perform the work.\n# Additionaly you will be asked to add your own hypotheses and review the results.\n```\n\n## Workflow Diagram\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│ 1. Initialize Context                                           │\n│    /fpf:propose-hypotheses <problem>                            │\n│    (create .fpf/ directory structure)                           │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ problem context captured\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 2. Abduction: Generate Hypotheses                               │ ◀── add your own ───┐\n│    (create L0 hypothesis files)                                 │                     │\n└────────────────────────┬────────────────────────────────────────┘                     │\n                         │                                                              │\n                         │ 3-5 competing hypotheses                                     │\n                         ▼                                                              │\n┌─────────────────────────────────────────────────────────────────┐                     │\n│ 3. User Input                                                   │                     │\n│    (present summary, allow additions)                           │─────────────────────┘\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ all hypotheses collected\n                         ▼ \n┌─────────────────────────────────────────────────────────────────┐\n│ 4. Deduction: Verify Logic (Parallel)                           │\n│    (check constraints, promote to L1 or invalidate)             │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ logically valid hypotheses (L1)\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 5. Induction: Validate Evidence (Parallel)                      │\n│    (gather empirical evidence, promote L1 to L2)                │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ evidence-backed hypotheses (L2)\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 6. Audit Trust (Parallel)                                       │\n│    (compute R_eff using Weakest Link principle)                 │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ trust scores computed\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 7. Decision                                                     │\n│    (present comparison, user selects winner, create DRR)        │\n└────────────────────────┬────────────────────────────────────────┘\n                         │\n                         │ decision recorded\n                         ▼\n┌─────────────────────────────────────────────────────────────────┐\n│ 8. Summary                                                      │\n│    (DRR, winner rationale, next steps)                          │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Commands Overview\n\n### /fpf:propose-hypotheses - Decision Cycle\n\nExecute the complete FPF cycle from hypothesis generation through evidence validation to decision.\n\n- Purpose - Make architectural decisions with full audit trail\n- Output - `.fpf/decisions/DRR-<date>-<topic>.md` with winner and rationale\n\n```bash\n/fpf:propose-hypotheses [problem or decision to make]\n```\n\n#### Arguments\n\nNatural language description of the decision or problem. Examples: \"What caching strategy should we use?\" or \"How should we deploy our application?\"\n\n#### How It Works - ADI Cycle\n\nThe workflow follows three inference modes:\n\n1. **Initialize Context** - Creates `.fpf/` directory structure and captures problem constraints\n\n2. **Abduction: Generate Hypotheses** - FPF agent generates 3-5 generate plausible, diverse, and competing hypotheses in L0 folder.\n   **How it works:**\n   - You pose a problem or question\n   - The AI (as *Abductor* persona) generates 3-5 candidate explanations or solutions\n   - Each hypothesis is stored in `L0/` (unverified observations)\n   - No hypothesis is privileged — anchoring bias is the enemy\n\n   **Output:** Multiple L0 claims, each with:\n   - Clear statement of the hypothesis\n   - Initial reasoning for plausibility\n   - Identified assumptions and constraints\n\n3. **User Input** - Presents hypothesis table, allows user to add alternatives\n\n4. **Deduction: Verify Logic (Parallel)** - Checks each hypothesis against constraints and typing, promotes to L1 or invalidates\n   **How it works:**\n   - The AI (as *Verifier* persona) checks each L0 hypothesis for:\n   - Internal logical consistency\n   - Compatibility with known constraints\n   - Type correctness (does the solution fit the problem shape?)\n   - Hypotheses that pass are promoted to `L1/`\n   - Hypotheses that fail are moved to `invalid/` with explanation\n\n   **Output:** L1 claims (logically sound) or invalidation records.\n\n5. **Induction: Validate Evidence (Parallel)** - Gather empirical evidence through tests or research, promotes L1 hypotheses to L2\n   **How it works:**\n   - For **internal** claims: run tests, measure performance, verify behavior\n   - For **external** claims: research documentation, benchmarks, case studies\n   - Evidence is attached with:\n   - Source and date (for decay tracking)\n   - Congruence rating (how well does external evidence match our context?)\n   - Claims that pass validation are promoted to `L2/`\n\n   **Output:** L2 claims (empirically verified) with evidence chain.\n\n6. **Audit(Parallel)** - Compute trust score R_eff using \n   - **WLNK (Weakest Link):** Assurance = min(evidence levels)\n   - **Congruence Check:** Is external evidence applicable to our context?\n   - **Bias Detection:** Are we anchoring on early hypotheses?\n\n7. **Make Decision**: Presents comparison table, selects winner, creates Design Rationale Record (DRR) which captures:\n   - decision\n   - alternatives considered\n   - evidence\n   - expiry conditions\n\n8. **Present Summary**: Shows DRR, winner rationale, and next steps\n\n#### Usage Examples\n\n```bash\n# Caching strategy decision\n/fpf:propose-hypotheses What caching strategy should we use?\n\n# Deployment approach\n/fpf:propose-hypotheses How should we deploy our application?\n\n# Architecture decision\n/fpf:propose-hypotheses Should we use microservices or monolith?\n\n# Technology selection\n/fpf:propose-hypotheses Which database should we use for high-write workloads?\n```\n\n#### When to Use\n\n**Use it for:**\n\n- Architectural decisions with long-term consequences\n- Multiple viable approaches requiring systematic evaluation\n- Decisions that need an auditable reasoning trail\n- Building up project knowledge over time\nSkip it for:\n\nQuick fixes with obvious solutions\nEasily reversible decisions\nTime-critical situations where the overhead isn't justified\n\n#### Best practices\n\n- Frame as decisions - \"What X should we use?\" or \"How should we Y?\"\n- Be specific about constraints - Include performance, cost, or time requirements\n- Add your own hypotheses - Don't rely only on AI-generated options\n- Review verification failures - Failed hypotheses reveal hidden constraints\n- Document for future reference - DRRs become project knowledge\n\n---\n\n### /fpf:status - Check Progress\n\nShow current FPF phase, hypothesis counts, and any warnings about stale evidence.\n\n- Purpose - Understand current state of reasoning process\n- Output - Status table with phase, counts, and warnings\n\n```bash\n/fpf:status\n```\n\n#### Arguments\n\nNone required.\n\n#### How It Works\n\n1. **Phase Detection**: Identifies current ADI cycle phase (IDLE, ABDUCTION, DEDUCTION, INDUCTION, DECISION)\n\n2. **Hypothesis Count**: Reports counts per knowledge layer (L0, L1, L2, Invalid)\n\n3. **Evidence Status**: Lists evidence files and their freshness\n\n4. **Warning Detection**: Identifies stale evidence, orphaned hypotheses, or incomplete cycles\n\n#### Usage Examples\n\n```bash\n# Check current status\n/fpf:status\n```\n\n**Example Output:**\n\n```markdown\n## FPF Status\n\n### Current Phase: DEDUCTION\n\nYou have 3 hypotheses in L0 awaiting verification.\nNext step: Continue the FPF workflow to process L0 hypotheses.\n\n### Hypothesis Counts\n\n| Layer | Count |\n|-------|-------|\n| L0 | 3 |\n| L1 | 0 |\n| L2 | 0 |\n| Invalid | 0 |\n\n### Evidence Status\n\nNo evidence files yet (hypotheses not validated).\n\n### No Warnings\n\nAll systems nominal.\n```\n\n#### Best practices\n\n- Check before continuing - Know your current phase before proceeding\n- Address warnings - Stale evidence affects trust scores\n- Review invalid hypotheses - Understand why they failed\n\n---\n\n### /fpf:query - Search Knowledge Base\n\nSearch the FPF knowledge base for hypotheses, evidence, or decisions with assurance information.\n\n- Purpose - Find and review stored knowledge with trust scores\n- Output - Search results with layer, R_eff, and evidence counts\n\n```bash\n/fpf:query [keyword or hypothesis name]\n```\n\n#### Arguments\n\nKeyword to search for, specific hypothesis name, or \"DRR\" to list decisions.\n\n#### How It Works\n\n1. **Keyword Search**: Searches hypothesis titles, descriptions, and evidence\n\n2. **Hypothesis Details**: Returns full hypothesis info including layer, kind, scope, and R_eff\n\n3. **DRR Listing**: Shows all Design Rationale Records with winner and rejected alternatives\n\n#### Usage Examples\n\n```bash\n# Search by keyword\n/fpf:query caching\n\n# Query specific hypothesis\n/fpf:query redis-caching\n\n# List all decisions\n/fpf:query DRR\n```\n\n**Example Output (keyword search):**\n\n```markdown\nResults:\n| Hypothesis | Layer | R_eff |\n|------------|-------|-------|\n| redis-caching | L2 | 0.85 |\n| cdn-edge-cache | L2 | 0.72 |\n| lru-cache | invalid | N/A |\n```\n\n**Example Output (specific hypothesis):**\n\n```markdown\n# redis-caching (L2)\n\nTitle: Use Redis for Caching\nKind: system\nScope: High-load systems\nR_eff: 0.85\nEvidence: 2 files\n```\n\n**Example Output (DRR listing):**\n\n```markdown\n# Design Rationale Records\n\n| DRR | Date | Winner | Rejected |\n|-----|------|--------|----------|\n| DRR-2025-01-15-caching | 2025-01-15 | redis-caching | cdn-edge |\n```\n\n#### Best practices\n\n- Search before starting new decisions - Reuse existing knowledge\n- Check R_eff scores - Higher scores indicate more reliable hypotheses\n- Review DRRs - Past decisions inform future choices\n\n---\n\n### /fpf:decay - Manage Evidence Freshness\n\nCheck for stale evidence and choose how to handle it: refresh, deprecate, or waive.\n\n- Purpose - Maintain evidence validity over time\n- Output - Updated evidence status and trust scores\n\nEvidence expires. A benchmark from six months ago might not reflect current performance. `/fpf:decay` shows you what's stale and gives you three options:\n\n- Refresh — Re-run tests to get fresh evidence\n- Deprecate — Downgrade the hypothesis if the decision needs rethinking\n- Waive — Accept the risk temporarily with documented rationale\n\n```bash\n/fpf:decay waive the benchmark until February, we'll re-test after launch\n```\n\n#### Arguments\n\nNone required. Command is interactive.\n\n#### How It Works\n\n1. **Staleness Check**: Identifies evidence files past their freshness threshold\n\n2. **Options Presented**: For each stale evidence:\n   - **Refresh**: Re-run tests for fresh evidence\n   - **Deprecate**: Downgrade hypothesis, flag decision for review\n   - **Waive**: Accept risk temporarily with documented rationale\n\n3. **Trust Recalculation**: Updates R_eff scores based on evidence changes\n\n#### Usage Examples\n\n```bash\n# Check for stale evidence\n/fpf:decay\n\n# Natural language waiver\n# User: Waive the benchmark until February, we'll re-run after migration.\n\n# Agent response:\n# Waiver recorded:\n# - Evidence: ev-benchmark-2024-06-15\n# - Until: 2025-02-01\n# - Rationale: Will re-run after migration\n```\n\n#### Best practices\n\n- Run periodically - Evidence expires; benchmarks from 6 months ago may not reflect current performance\n- Document waivers - Always include rationale and expiration date\n- Refresh critical evidence - High-impact decisions deserve fresh data\n\n---\n\n### /fpf:actualize - Reconcile with Codebase\n\nUpdate the knowledge base to reflect codebase changes that may affect existing hypotheses.\n\n- Purpose - Keep knowledge synchronized with implementation\n- Output - Updated hypothesis validity and evidence relevance\n\nThis command serves as the Observe phase of the FPF's Canonical Evolution Loop (B.4). It reconciles your documented knowledge with the current state of the codebase by:\n\n- Detecting Context Drift: Checks if project files (like package.json) have changed, potentially making your context.md stale.\n- Finding Stale Evidence: Finds evidence whose carrier_ref (the file it points to) has been modified in git.\n- Flagging Outdated Decisions: Identifies decisions whose underlying evidence chain has been impacted by recent code changes.\n\n```bash\n/fpf:actualize\n```\n\n#### How It Works\n\n1. **Change Detection**: Identifies code changes since last actualization\n2. **Impact Analysis**: Determines which hypotheses and evidence are affected\n3. **Validity Update**: Marks affected hypotheses for re-verification if needed\n4. **Report Generation**: Summarizes changes and recommended actions\n\n#### Usage Examples\n\n```bash\n# After major refactoring\n/fpf:actualize\n\n# After dependency updates\n/fpf:actualize\n```\n\n#### Best practices\n\n- Run after major changes - Refactoring may invalidate previous assumptions\n- Review impact report - Some hypotheses may need re-evaluation\n- Update evidence - Changed code may need new benchmarks\n\n---\n\n### /fpf:reset - Start Fresh\n\nArchive the current session and return to IDLE state for a new reasoning cycle.\n\n- Purpose - Clear current state while preserving history\n- Output - Archived session in `.fpf/sessions/`\n\n```bash\n/fpf:reset\n```\n\n#### Arguments\n\nNone required. Command is interactive.\n\n#### How It Works\n\n1. **Reset Type Selection**:\n   - **Soft Reset**: Archive current session, start fresh (recommended)\n   - **Hard Reset**: Delete all FPF data (cannot be undone)\n   - **Decision Reset**: Keep hypotheses, re-evaluate from earlier phase\n\n2. **Session Archive**: Creates timestamped archive in `.fpf/sessions/`\n\n3. **State Clear**: Clears knowledge directories based on reset type\n\n#### Usage Examples\n\n```bash\n/fpf:reset\n\n# Agent: What type of reset would you like?\n# 1. Soft Reset - Archive current session, start fresh\n# 2. Hard Reset - Delete all FPF data (cannot be undone)\n# 3. Decision Reset - Keep hypotheses, re-evaluate from earlier phase\n\n# User: Soft reset please\n\n# Agent: Creating session archive...\n#        [Creates .fpf/sessions/session-2025-01-15-reset.md]\n#        Session archived. Knowledge directories cleared.\n#        Ready for new reasoning cycle.\n```\n\n#### When to Reset\n\n| Scenario | Recommended Action |\n|----------|-------------------|\n| Starting a new problem | Soft reset (archive) |\n| Wrong direction, start over | Soft reset |\n| Testing/learning FPF | Hard reset |\n| Re-evaluate with new info | Decision reset |\n| Context changed significantly | Soft reset + update context |\n\n#### Best practices\n\n- Prefer soft reset - Always preserve history for reference\n- Hard reset only for testing - Production knowledge is valuable\n- Decision reset for pivots - When new information changes the equation\n\n---\n\n## Available Agents\n\n| Agent | Description | Used By |\n|-------|-------------|---------|\n| `fpf-agent` | FPF reasoning specialist for hypothesis generation, verification, validation, and trust calculus using the ADI cycle | All commands |\n\n### fpf-agent\n\n**Purpose**: Executes FPF reasoning tasks with file operations for persisting knowledge state.\n\n**Tools**: Read, Write, Glob, Grep, Bash (mkdir, mv, touch)\n\n**Responsibilities**:\n- Create hypothesis files in knowledge layers\n- Move files between L0/L1/L2/invalid directories\n- Create evidence files and audit reports\n- Generate Design Rationale Records (DRRs)\n\n## Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **ADI Cycle** | Abduction-Deduction-Induction reasoning loop |\n| **Knowledge Layers** | L0 (Conjecture) -> L1 (Substantiated) -> L2 (Corroborated) |\n| **WLNK** | Weakest Link principle: R_eff = min(evidence_scores) |\n| **Holon** | Knowledge unit with identity, layer, kind, and assurance scores |\n| **DRR** | Design Rationale Record documenting decisions |\n| **Transformer Mandate** | AI generates options; humans decide |\n\n### Knowledge Layers (Epistemic Status)\n\n| Layer | Name | Meaning | How to reach |\n|-------|------|---------|--------------|\n| **L0** | Conjecture | Unverified hypothesis | Generate hypotheses |\n| **L1** | Substantiated | Passed logical check | Verify logic |\n| **L2** | Corroborated | Empirically validated | Validate evidence |\n| **Invalid** | Falsified | Failed verification | FAIL verdict |\n\n### Congruence Levels\n\n| Level | Context Match | Penalty |\n|-------|--------------|---------|\n| CL3 | Same (internal test) | None |\n| CL2 | Similar (related project) | Minor |\n| CL1 | Different (external docs) | Significant |\n\n### The Transformer Mandate\n\nA core FPF principle: **A system cannot transform itself.**\n\n- AI generates options with evidence\n- Human decides\n- Making architectural choices autonomously is a PROTOCOL VIOLATION\n\nThis ensures accountability and prevents AI from making unsupervised decisions.\n\n## Directory Structure\n\nThe FPF plugin creates and manages this directory structure:\n\n```\n.fpf/\n├── context.md              # Problem context and constraints\n├── knowledge/\n│   ├── L0/                 # Candidate hypotheses (Conjecture)\n│   ├── L1/                 # Substantiated hypotheses (Passed logic)\n│   ├── L2/                 # Validated hypotheses (Evidence-backed)\n│   └── invalid/            # Rejected hypotheses (Failed verification)\n├── evidence/               # Evidence files and audit reports\n├── decisions/              # DRR files\n└── sessions/               # Archived sessions\n```\n\n## When to Use FPF\n\n**Use it for:**\n- Architectural decisions with long-term consequences\n- Multiple viable approaches requiring systematic evaluation\n- Decisions needing auditable reasoning trails\n- Building project knowledge over time\n\n**Skip it for:**\n- Quick fixes with obvious solutions\n- Easily reversible decisions\n- Time-critical situations\n\n## Theoretical Foundation\n\n### Core Methodology\n\n- **[First Principles Framework (FPF)](https://github.com/ailev/FPF)** - Original methodology by Anatoly Levenchuk for structured epistemic reasoning\n- **[quint-code](https://github.com/m0n0x41d/quint-code)** - Implementation this plugin is based on\n\n### Supporting Concepts\n\n- **Abduction-Deduction-Induction Cycle** - Classical scientific reasoning methodology\n- **Weakest Link Principle** - Trust computation based on minimum evidence quality\n- **Design Rationale** - Documenting not just decisions but the reasoning behind them\n"
      },
      "plugins": [
        {
          "name": "reflexion",
          "description": "Collection of commands that force LLM to reflect on previous response and output. Based on papers like Self-Refine and Reflexion. These techniques improve the output of large language models by introducing feedback and refinement loops.",
          "version": "1.1.4",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/reflexion",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install reflexion@context-engineering-kit"
          ]
        },
        {
          "name": "code-review",
          "description": "Introduce codebase and PR review commands and skills using multiple specialized agents.",
          "version": "1.0.8",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/code-review",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install code-review@context-engineering-kit"
          ]
        },
        {
          "name": "git",
          "description": "Introduces commands for commit and PRs creation, plus skills for git worktrees and notes.",
          "version": "1.2.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/git",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install git@context-engineering-kit"
          ]
        },
        {
          "name": "tdd",
          "description": "Introduces commands for test-driven development, common anti-patterns and skills for testing using subagents.",
          "version": "1.1.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/tdd",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install tdd@context-engineering-kit"
          ]
        },
        {
          "name": "sadd",
          "description": "Introduces skills for subagent-driven development, dispatches fresh subagent for each task with code review between tasks, enabling fast iteration with quality gates.",
          "version": "1.2.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/sadd",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install sadd@context-engineering-kit"
          ]
        },
        {
          "name": "ddd",
          "description": "Introduces command to update CLAUDE.md with best practices for domain-driven development, focused on quality of code, includes Clean Architecture, SOLID principles, and other design patterns.",
          "version": "1.0.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/ddd",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install ddd@context-engineering-kit"
          ]
        },
        {
          "name": "sdd",
          "description": "Specification Driven Development workflow commands and agents, based on Github Spec Kit and OpenSpec. Uses specialized agents for effective context management and quality review.",
          "version": "1.1.5",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/sdd",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install sdd@context-engineering-kit"
          ]
        },
        {
          "name": "kaizen",
          "description": "Inspired by Japanese continuous improvement philosophy, Agile and Lean development practices. Introduces commands for analysis of root cause of issues and problems, including 5 Whys, Cause and Effect Analysis, and other techniques.",
          "version": "1.0.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/kaizen",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install kaizen@context-engineering-kit"
          ]
        },
        {
          "name": "customaize-agent",
          "description": "Commands and skills for writing and refining commands, hooks, skills for Claude Code, includes Anthropic Best Practices and Agent Persuasion Principles that can be useful for sub-agent workflows.",
          "version": "1.3.2",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/customaize-agent",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install customaize-agent@context-engineering-kit"
          ]
        },
        {
          "name": "docs",
          "description": "Commands for analysing project, writing and refining documentation.",
          "version": "1.1.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/docs",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install docs@context-engineering-kit"
          ]
        },
        {
          "name": "tech-stack",
          "description": "Commands for setup or update of CLAUDE.md file with best practices for specific language or framework.",
          "version": "1.0.0",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/tech-stack",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install tech-stack@context-engineering-kit"
          ]
        },
        {
          "name": "mcp",
          "description": "Commands for setup well known MCP server integration if needed and update CLAUDE.md file with requirement to use this MCP server for current project.",
          "version": "1.2.1",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/mcp",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install mcp@context-engineering-kit"
          ]
        },
        {
          "name": "fpf",
          "description": "First Principles Framework (FPF) for structured reasoning. Implements ADI (Abduction-Deduction-Induction) cycle for hypothesis generation, logical verification, empirical validation, and auditable decision-making.",
          "version": "1.1.1",
          "author": {
            "name": "Vlad Goncharov",
            "email": "vlad.goncharov@neolab.finance"
          },
          "source": "./plugins/fpf",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add NeoLabHQ/context-engineering-kit",
            "/plugin install fpf@context-engineering-kit"
          ]
        }
      ]
    }
  ]
}