{
  "author": {
    "id": "redpanda-data",
    "display_name": "redpanda-data",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/49406389?v=4",
    "url": "https://github.com/redpanda-data",
    "bio": "The streaming data platform for developers",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 3,
      "total_skills": 3,
      "total_stars": 8568,
      "total_forks": 913
    }
  },
  "marketplaces": [
    {
      "name": "redpanda-connect-plugins",
      "version": "0.1.0",
      "description": "Plugins for Redpanda Connect",
      "owner_info": {
        "name": "Redpanda Data",
        "url": "https://redpanda.com"
      },
      "keywords": [],
      "repo_full_name": "redpanda-data/connect",
      "repo_url": "https://github.com/redpanda-data/connect",
      "repo_description": "Fancy stream processing made operationally mundane",
      "homepage": "https://docs.redpanda.com/redpanda-connect/about/",
      "signals": {
        "stars": 8568,
        "forks": 913,
        "pushed_at": "2026-01-29T20:48:07Z",
        "created_at": "2016-03-22T01:18:48Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/README.md",
          "type": "blob",
          "size": 3188
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 432
        },
        {
          "path": ".claude-plugin/plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 513
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/commands/blobl.md",
          "type": "blob",
          "size": 679
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/commands/pipeline.md",
          "type": "blob",
          "size": 653
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/commands/search.md",
          "type": "blob",
          "size": 440
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/SETUP.md",
          "type": "blob",
          "size": 810
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/SKILL.md",
          "type": "blob",
          "size": 15131
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources/scripts/format-bloblang.py",
          "type": "blob",
          "size": 5873
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources/scripts/format-bloblang.sh",
          "type": "blob",
          "size": 719
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources/scripts/rpk-version.sh",
          "type": "blob",
          "size": 201
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources/scripts/test-blobl.sh",
          "type": "blob",
          "size": 757
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/component-search",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/component-search/SETUP.md",
          "type": "blob",
          "size": 795
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/component-search/SKILL.md",
          "type": "blob",
          "size": 5169
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/component-search/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/component-search/resources/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/component-search/resources/scripts/format-component-fields.py",
          "type": "blob",
          "size": 6420
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/component-search/resources/scripts/format-component-fields.sh",
          "type": "blob",
          "size": 711
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/component-search/resources/scripts/rpk-version.sh",
          "type": "blob",
          "size": 201
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/SETUP.md",
          "type": "blob",
          "size": 760
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/SKILL.md",
          "type": "blob",
          "size": 12106
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/cdc-replication.md",
          "type": "blob",
          "size": 4210
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/cdc-replication.yaml",
          "type": "blob",
          "size": 2523
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/content-based-router.md",
          "type": "blob",
          "size": 3562
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/content-based-router.yaml",
          "type": "blob",
          "size": 2092
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/custom-metrics.md",
          "type": "blob",
          "size": 5032
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/custom-metrics.yaml",
          "type": "blob",
          "size": 1841
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/dlq-basic.md",
          "type": "blob",
          "size": 4802
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/dlq-basic.yaml",
          "type": "blob",
          "size": 3098
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/kafka-replication.md",
          "type": "blob",
          "size": 2118
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/kafka-replication.yaml",
          "type": "blob",
          "size": 3101
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/multicast.md",
          "type": "blob",
          "size": 3904
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/multicast.yaml",
          "type": "blob",
          "size": 3036
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/rate-limiting.md",
          "type": "blob",
          "size": 633
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/rate-limiting.yaml",
          "type": "blob",
          "size": 470
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-polling.md",
          "type": "blob",
          "size": 569
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-polling.yaml",
          "type": "blob",
          "size": 409
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-sink-basic.md",
          "type": "blob",
          "size": 637
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-sink-basic.yaml",
          "type": "blob",
          "size": 612
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-sink-time-based.md",
          "type": "blob",
          "size": 696
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-sink-time-based.yaml",
          "type": "blob",
          "size": 707
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/stateful-counter.md",
          "type": "blob",
          "size": 3756
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/stateful-counter.yaml",
          "type": "blob",
          "size": 2683
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/validate.sh",
          "type": "blob",
          "size": 267
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/window-aggregation.md",
          "type": "blob",
          "size": 665
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/window-aggregation.yaml",
          "type": "blob",
          "size": 659
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/tests/fixtures",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/tests/fixtures/blobl_transformations.json",
          "type": "blob",
          "size": 13644
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/tests/fixtures/pipeline_descriptions.json",
          "type": "blob",
          "size": 13288
        },
        {
          "path": ".claude-plugin/plugins/redpanda-connect/tests/fixtures/search_queries.json",
          "type": "blob",
          "size": 8366
        }
      ],
      "files": {
        ".claude-plugin/README.md": "# Redpanda Connect Plugin\n\nAI-powered assistant for building Redpanda Connect streaming pipelines with natural language.\n\n**What you get:**\n- Component discovery using natural language\n- Pipeline generation from descriptions\n- Bloblang transformation authoring\n- Configuration validation and fixing\n\n## Use in Claude Code\n\n### Prerequisites\n\n```bash\n# Install Redpanda rpk CLI tool\nbrew install redpanda-data/tap/redpanda\n\n# Install or upgrade Redpanda Connect\nrpk connect install\nrpk connect upgrade\n\n# Install Python and jq (required by plugin)\nbrew install python3 jq\n\n# Verify installation\nrpk version        \npython3 --version  \njq --version\n```\n\n### Plugin Installation\n\n**From GitHub (recommended):**\n\n```bash\n# Add marketplace\n/plugin marketplace add https://github.com/redpanda-data/connect.git\n\n# Install plugin\n/plugin install redpanda-connect\n```\n\n**Local development:**\n\n```bash\n# Add local marketplace\n/plugin marketplace add /path/to/connect\n\n# Install plugin\n/plugin install redpanda-connect\n```\n\nRestart Claude Code after installation.\n\n### Quick Start\n\nThree slash commands provide direct access:\n\n- `/rpcn:search` - Natural language component discovery\n- `/rpcn:blobl` - Bloblang transformation script generation\n- `/rpcn:pipeline` - End-to-end pipeline orchestration\n\nClaude will also automatically assist when you mention Redpanda Connect, streaming pipelines, or Bloblang in conversation.\n\n### Commands Reference\n\n#### `/rpcn:search <query>`\n\nSearch for components using natural language.\n\n**Examples:**\n\n```bash\n/rpcn:search \"kafka consumer\"\n/rpcn:search \"postgres output with connection pooling\"\n/rpcn:search \"rate limiting\"\n```\n\n#### `/rpcn:blobl <description> [sample=<json>]`\n\nGenerate tested Bloblang transformation scripts.\n\n**Examples:**\n\n```bash\n# Basic transformation\n/rpcn:blobl \"parse JSON and extract user.name field\"\n\n# With test data\n/rpcn:blobl \"uppercase name\" sample='{\"name\": \"john\"}'\n```\n\n#### `/rpcn:pipeline <description> [file=<path>]`\n\nCreate new pipelines or fix existing configurations.\n\n**Examples: Create new pipeline:**\n\n```bash\n/rpcn:pipeline \"consume from Kafka, transform with Bloblang, output to S3\"\n/rpcn:pipeline \"HTTP webhook receiver that writes to PostgreSQL\"\n```\n\n**Examples: Fix existing pipeline:**\n\n```bash\n/rpcn:pipeline \"fix connection timeout\" file=config.yaml\n/rpcn:pipeline \"add retry logic\" file=pipeline.yaml\n```\n\n---\n\n## Use in Claude Desktop\n\nIf you're using Claude Desktop (not Claude Code), you can manually install individual skills as standalone tools.\n\n### Skills\n\n- `component-search`: Natural language component discovery\n- `bloblang-authoring`: Bloblang transformation script generation\n- `pipeline-assistant`: End-to-end pipeline orchestration\n\n### Installation\n\nThree skills are available as ZIP files in `./dist/` directory.\nDrag the ZIP files individually into Claude Desktop Settings > Capabilities to install.\n\n### Usage\n\nOnce installed the skills will automatically assist when you mention Redpanda Connect, streaming pipelines, or Bloblang in conversation.\nYou may also trigger them explicitly using keywords like `component-search skill`, `bloblang-authoring skill`, or `pipeline-assistant skill`.\n",
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"redpanda-connect-plugins\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Plugins for Redpanda Connect\",\n  \"owner\": {\n    \"name\": \"Redpanda Data\",\n    \"url\": \"https://redpanda.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"redpanda-connect\",\n      \"description\": \"YAML config and Bloblang authoring for Redpanda Connect\",\n      \"source\": \"./.claude-plugin/plugins/redpanda-connect\",\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugins/redpanda-connect/.claude-plugin/plugin.json": "{\n  \"name\": \"redpanda-connect\",\n  \"description\": \"Interactive YAML config and Bloblang authoring for Redpanda Connect\",\n  \"version\": \"0.2.0\",\n  \"author\": {\n    \"name\": \"Michał Matczuk\",\n    \"email\": \"michal.matczuk@redpanda.com\"\n  },\n  \"license\": \"Apache-2.0\",\n  \"repository\": \"https://github.com/redpanda-data/connect\",\n  \"homepage\": \"https://docs.redpanda.com/redpanda-connect\",\n  \"keywords\": [\n    \"redpanda\",\n    \"connect\",\n    \"kafka\",\n    \"streaming\",\n    \"bloblang\",\n    \"yaml\",\n    \"configuration\"\n  ]\n}\n",
        ".claude-plugin/plugins/redpanda-connect/commands/blobl.md": "---\nname: rpcn:blobl\ndescription: Create and test Bloblang transformation scripts from natural language descriptions\narguments:\n  - name: transformation\n    description: What transformation you want (e.g., \"convert timestamp to ISO format and uppercase name field\")\n    required: true\n  - name: sample\n    description: JSON sample input for testing\n    required: false\nallowed-tools: [\"*\"]\n---\n\n{{#if sample}}\nUse the **bloblang-authoring** skill to create a working, tested Bloblang script for: **{transformation}**\nTest with this sample input: {sample}\n{{else}}\nUse the **bloblang-authoring** skill to create a working, tested Bloblang script for: **{transformation}**\n{{/if}}\n",
        ".claude-plugin/plugins/redpanda-connect/commands/pipeline.md": "---\nname: rpcn:pipeline\ndescription: Create or repair Redpanda Connect configurations with interactive guidance and validation\narguments:\n  - name: context\n    description: What you want to build or fix (e.g., \"read from kafka and write to postgres\", \"fix connection timeout error\")\n    required: true\n  - name: file\n    description: Path to existing config file to fix or modify\n    required: false\nallowed-tools: [\"*\"]\n---\n\n{{#if file}}\nUse the **pipeline-assistant** skill to help fix or modify the configuration at: **{file}**\nContext: {context}\n{{else}}\nUse the **pipeline-assistant** skill to help create a configuration for: **{context}**\n{{/if}}",
        ".claude-plugin/plugins/redpanda-connect/commands/search.md": "---\nname: rpcn:search\ndescription: Search for Redpanda Connect components (inputs, outputs, processors, caches, rate-limits, buffers, metrics, tracers)\narguments:\n  - name: component\n    description: What component you're looking for (e.g., \"kafka consumer\", \"postgres output\", \"http server\")\n    required: true\nallowed-tools: [\"*\"]\n---\n\nUse the **component-search** skill to find the right Redpanda Connect components for: **{component}**\n",
        ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/SETUP.md": "# Setup\n\nThis skill requires: `rpk`, `rpk connect`, `python3`, `jq`\n\n## macOS\n\n```bash\nbrew install redpanda-data/tap/redpanda python3 jq\nrpk connect install\nrpk connect upgrade\n```\n\n## Ubuntu (Intel/AMD64)\n\n```bash\napt-get update && apt-get install -y curl unzip python3 jq\n\ncurl -LO https://github.com/redpanda-data/redpanda/releases/latest/download/rpk-linux-amd64.zip && \\\n  unzip rpk-linux-amd64.zip -d /usr/local/bin/ && \\\n  rm rpk-linux-amd64.zip\n\nrpk connect install\nrpk connect upgrade\n```\n\n## Ubuntu (ARM64)\n\n```bash\napt-get update && apt-get install -y curl unzip python3 jq\n\ncurl -LO https://github.com/redpanda-data/redpanda/releases/latest/download/rpk-linux-arm64.zip && \\\n  unzip rpk-linux-arm64.zip -d /usr/local/bin/ && \\\n  rm rpk-linux-arm64.zip\n\nrpk connect install\nrpk connect upgrade\n```\n",
        ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/SKILL.md": "---\nname: bloblang-authoring\ndescription: This skill should be used when users need to create or debug Bloblang transformation scripts. Trigger when users ask about transforming data, mapping fields, parsing JSON/CSV/XML, converting timestamps, filtering arrays, or mention \"bloblang\", \"blobl\", \"mapping processor\", or describe any data transformation need like \"convert this to that\" or \"transform my JSON\".\n---\n\n# Redpanda Connect Bloblang Script Generator\n\nCreate working, tested Bloblang transformation scripts from natural language descriptions.\n\n## Objective\n\nGenerate a Bloblang (blobl) script that correctly transforms the user's input data according to their requirements.\nThe script MUST be tested before presenting it.\n\n## Setup\n\nThis skill requires `rpk` `rpk connect`, `python3`, and `jq`.\nSee the [SETUP](SETUP.md) for installation instructions.\n\n## Tools\n\n### Script format-bloblang.sh\n\nGenerates category-organized Bloblang reference files in XML format.\n**Run once at the start of each session** before searching for functions/methods.\n\n```bash\n# Usage:\n./resources/scripts/format-bloblang.sh\n```\n- No arguments\n- Generates category files organized by type (e.g., `functions-General.xml`, `methods-String_Manipulation.xml`)\n- Outputs generated files to a versioned directory\n- Outputs the directory path to stdout (capture in `BLOBLREF_DIR` variable for later use)\n- Each XML file contains structured function/method definitions with parameters, descriptions, and examples\n\n#### Functions\n\nGenerated function files have `functions-<Category>.xml` names and contain functions relevant to that category.\n\n- `functions-Encoding.xml` - Schema registry headers\n- `functions-Environment.xml` - Environment vars, files, timestamps, hostname\n- `functions-Fake_Data_Generation.xml` - Fake data generation\n- `functions-General.xml` - Bytes, counter, deleted, ksuid, nanoid, uuid, random, range, snowflake\n- `functions-Message_Info.xml` - Batch index, content, error, metadata, span links, tracing IDs\n- etc.\n\n**The `function` XML tag format:**\n- `name` attribute - function name\n- `params` attribute - comma-separated list of parameters with types, format `<name>:<type>` or empty string if no parameters\n- body - description of function purpose and usage\n- `example` XML subtag\n  - `summary` attribute (optional) - brief description of the example\n  - body - code block demonstrating usage\n\nExample function definition:\n```xml\n<function name=\"random_int\" params=\"seed:query expression, min:integer, max:integer\">\nGenerates a pseudo-random non-negative 64-bit integer.\nUse this for creating random IDs, sampling data, or generating test values.\nProvide a seed for reproducible randomness, or use a dynamic seed like `timestamp_unix_nano()` for unique values per mapping instance.\n\nOptional `min` and `max` parameters constrain the output range (both inclusive).\nFor dynamic ranges based on message data, use the modulo operator instead: `random_int() % dynamic_max + dynamic_min`.\n<example>\nroot.first = random_int()\nroot.second = random_int(1)\nroot.third = random_int(max:20)\nroot.fourth = random_int(min:10, max:20)\nroot.fifth = random_int(timestamp_unix_nano(), 5, 20)\nroot.sixth = random_int(seed:timestamp_unix_nano(), max:20)\n</example>\n<example summary=\"Use a dynamic seed for unique random values per mapping instance.\">\nroot.random_id = random_int(timestamp_unix_nano())\nroot.sample_percent = random_int(seed: timestamp_unix_nano(), min: 0, max: 100)\n</example>\n</function>\n```\n\n#### Methods\n\nGenerated method files have `methods-<Category>.xml` names and contain methods relevant to that category.\n\n- `methods-Encoding_and_Encryption.xml` - Base64, compression, hashing, encryption\n- `methods-General.xml` - Basic operations, type checking\n- `methods-GeoIP.xml` - GeoIP lookups\n- `methods-JSON_Web_Tokens.xml` - JWT operations\n- `methods-Number_Manipulation.xml` - Arithmetic, rounding, formatting\n- `methods-Object___Array_Manipulation.xml` - Filtering, mapping, sorting, merging\n- `methods-Parsing.xml` - JSON, CSV, XML, protocol buffer parsing\n- `methods-Regular_Expressions.xml` - Regex matching and replacement\n- `methods-SQL.xml` - SQL operations\n- `methods-String_Manipulation.xml` - Case, trimming, splitting, formatting\n- `methods-Timestamp_Manipulation.xml` - Parsing, formatting, timezone conversion\n- `methods-Type_Coercion.xml` - Type conversions\n- etc.\n\n**The `method` XML tag format:**\n- `name` attribute - function name\n- `params` attribute - comma-separated list of parameters with types, format `<name>:<type>` or empty string if no parameters\n- body - description of function purpose and usage\n- `example` XML subtag\n  - `summary` attribute (optional) - brief description of the example\n  - body - code block demonstrating usage\n\nExample method definition:\n```xml\n<method name=\"ts_format\" params=\"format:string, tz:string\">\nFormats a timestamp into a string using the specified format layout.\n<example>\nroot.formatted = this.timestamp.ts_format(\"2006-01-02T15:04:05Z07:00\")\n</example>\n</method>\n```\n\n### Grep Search\n\nLists Available functions and methods without loading full files.\n\n```bash\n# List all available functions and methods by name\ngrep -hE '<(function|method) name=' \"$BLOBLREF_DIR\"\n\n# Search by keyword (searches names, descriptions, params, examples)\ngrep -i \"timestamp\" \"$BLOBLREF_DIR\"\n\n# Search by parameter name (e.g., find all with \"format\" parameter)\ngrep 'params=\"[^\"]*format' \"$BLOBLREF_DIR\"\n```\n- Requires `BLOBLREF_DIR` set to the directory output by `format-bloblang.sh`\n\n### Script test-blobl.sh\n\nTests a Bloblang script against input data.\nExecutes the transformation and returns results or errors.\nCan be run repeatedly during iteration.\n\n```bash\n# Usage:\n./resources/scripts/test-blobl.sh <target-directory>\n```\n- Requires `data.json` (input) and `script.blobl` (transformation) in the target directory\n- Returns transformed data or error messages\n\n## Bloblang\n\n**Bloblang** (blobl) is Redpanda Connect's native mapping language for transforming message data.\nIt's designed for readability and safely reshaping documents of any structure.\n\n### Core Concepts\n\n**Assignment**: Create new documents by assigning values to paths.\n- `root` = the new document being created\n- `this` = the input document being read\n\n```bloblang\n# Copy entire input\nroot = this\n\n# Create specific fields\nroot.id = this.thing.id\nroot.type = \"processed\"\n\n# In:  {\"thing\":{\"id\":\"abc123\"}}\n# Out: {\"id\":\"abc123\",\"type\":\"processed\"}\n```\n\n**Field Paths**: Use dot notation for nested fields. Use quotes for special characters:\n```bloblang\nroot.user.name = this.customer.full_name\nroot.\"foo.bar\".baz = this.\"field with spaces\"\n```\n\n**Literals**: Numbers, booleans, strings, null, arrays, and objects:\n```bloblang\nroot = {\n  \"count\": 42,\n  \"active\": true,\n  \"items\": [\"a\", \"b\", \"c\"],\n  \"nested\": {\"key\": \"value\"}\n}\n```\n\n### Functions and Methods\n\n**Functions** generate values (no target needed):\n```bloblang\nroot.id = uuid_v4()\nroot.timestamp = now()\nroot.hostname = hostname()\n```\n\n**Methods** transform values (called on a target with `.`):\n```bloblang\nroot.upper = this.name.uppercase()\nroot.formatted = this.date.ts_parse(\"2006-01-02\").ts_format(\"Mon Jan 2\")\nroot.sorted = this.items.sort()\n```\n\nMethods can be chained:\n```bloblang\nroot.clean = this.text.trim().lowercase().replace_all(\"_\", \"-\")\n```\n\nMethods require a target (called with `.`), while functions do not. \nCheck the XML reference files to determine correct usage:\n\n```bloblang\n# Bad: floor() is a method, not a function\nroot.rounded = floor(this.value)  # Error: floor is not a function\n\n# Good: Call floor() as a method on a value\nroot.rounded = this.value.floor()\n\n# Bad: uuid_v4() is a function, not a method\nroot.id = this.uuid_v4()  # Error: uuid_v4 is not a method\n\n# Good: Call uuid_v4() as a function\nroot.id = uuid_v4()\n```\n\n**Discovering Available Functions & Methods**\n\nBloblang provides hundreds of functions and methods organized into categories.\nStart with these **foundational categories** that cover common use cases:\n- `functions-General.xml` - Core utility functions (uuid_v4, timestamp, random, etc.)\n- `functions-Message_Info.xml` - Message metadata access (hostname, env, content_type, etc.)\n- `methods-General.xml` - Universal transformations (type conversions, existence checks, etc.)\n\nFor specialized needs, consult **domain-specific categories**: strings (uppercase, trim, regexp), timestamps (ts_parse, ts_format), arrays (map_each, filter), objects (keys, values), encoding (base64, json), and more.\n\n**Discovery tools**:\n- Run `format-bloblang.sh` to generate category-organized XML reference files in a versioned directory\n- Use grep patterns to search function/method names, descriptions, parameters, and examples across categories\n- Read specific category XML files for structured definitions with complete function signatures, parameter details, and usage examples\n\n### Control Flow\n\n**Conditionals** (if/else):\n```bloblang\nroot.category = if this.score >= 80 {\n  \"high\"\n} else if this.score >= 50 {\n  \"medium\"\n} else {\n  \"low\"\n}\n```\n\n**Pattern Matching** (match):\n```bloblang\nroot.sound = match this.animal {\n  \"cat\" => \"meow\"\n  \"dog\" => \"woof\"\n  \"cow\" => \"moo\"\n  _ => \"unknown\"  # Catch-all\n}\n```\n\n**Coalescing** (try multiple paths with `|`):\n```bloblang\n# Use first non-null value from alternative fields\nroot.content = this.article.body | this.comment.text | \"no content\"\n\n# Try different nested paths\nroot.id = this.data.(primary_id | secondary_id | backup_id)\n```\n\nNote: Use `|` for alternative field paths (missing fields), use `.catch()` for operation failures (parse errors, type mismatches).\n\n### Common Operations\n\n**Deletion**:\n```bloblang\nroot = this\nroot.password = deleted()  # Remove field\n\n# Or filter entire message\nroot = if this.spam { deleted() }\n```\n\n**Variables** (reuse values without adding to output):\n```bloblang\nlet user_id = this.user.id\nlet enriched = this.user.name + \" (\" + $user_id + \")\"\n\nroot.display_name = $enriched\nroot.user_id = $user_id\n```\n\n**IMPORTANT**: Variables must be declared at the top level, not inside `if`, `match`, or other blocks.\n\n```bloblang\n# Bad: Will cause \"expected }\" parse error\nroot.age = if this.birthdate != null {\n  let parsed = this.birthdate.ts_parse(\"2006-01-02\")  # let not allowed here!\n  $parsed.ts_unix()\n}\n\n# Good: Declare variables at top level\nlet parsed = this.birthdate.ts_parse(\"2006-01-02\").catch(null)\nroot.age = if $parsed != null {\n  $parsed.ts_unix()\n} else {\n  null\n}\n```\n\n**Named mappings**: (reusable scripts)\n```bloblang\nmap extract_user {\n  root.id = this.user_id\n  root.name = this.full_name\n  root.email = this.contact.email\n}\n\nroot.customer = this.customer_data.apply(\"extract_user\")\nroot.vendor = this.vendor_data.apply(\"extract_user\")\n```\n\n**Error Handling** (provide fallback values):\n```bloblang\n# Catch errors from any point in the chain\nroot.count = this.items.length().catch(0)\nroot.parsed = this.data.parse_json().catch({})\n\n# Catch missing/null values\nroot.name = this.user.name.or(\"anonymous\")\n\n# Multi-format parsing with catch chains\n# Store value in variable for reliable access in catch fallbacks\nlet date_str = this.date\nroot.parsed = $date_str.ts_parse(\"2006-01-02\").catch(\n  $date_str.ts_parse(\"2006/01/02\")\n).catch(null)\n```\n\n**IMPORTANT**: When using `.catch()` with fallback expressions that reference `this.field`, store the field in a variable first.\nContext references in catch chains can be unreliable:\n\n```bloblang\n# Risky: Context may not be preserved in catch\nroot.parsed = this.date.ts_parse(\"2006-01-02\").catch(\n  this.date.ts_parse(\"2006/01/02\")  # this.date might not work here\n)\n\n# Safe: Store in variable first\nlet date_str = this.date\nroot.parsed = $date_str.ts_parse(\"2006-01-02\").catch(\n  $date_str.ts_parse(\"2006/01/02\")  # variable reference is reliable\n)\n```\n\n**Metadata**:\n```bloblang\n# Read metadata with @ or metadata()\nroot.topic = @kafka_topic\nroot.partition = @kafka_partition\n\n# Set metadata\nmeta output_key = this.id\nmeta content_type = \"application/json\"\n```\n\n### Common Edge Case Patterns\n\n**Safe field access with fallbacks**\n```bloblang\n# Bad: Will fail if user or name is missing\nroot.name = this.user.name\n\n# Good: Provides fallback chain\nroot.name = this.user.name.or(\"anonymous\")\nroot.name = this.(user.name | profile.display_name | \"unknown\")\n```\n\n**Safe collection operations**\n```bloblang\n# Bad: Will fail on empty array\nroot.first = this.items[0]\n\n# Good: Handles empty arrays\nroot.first = if this.items.length() > 0 { this.items[0] } else { null }\nroot.first = this.items[0].catch(null)\n```\n\n**Safe parsing with error recovery**\n```bloblang\n# Bad: Will fail on invalid JSON\nroot.data = this.payload.parse_json()\n\n# Good: Provides fallback on parse failure\nroot.data = this.payload.parse_json().catch({})\nroot.data = this.payload.parse_json().catch(this.payload)  # Keep original on failure\n```\n\n**Safe type coercion**\n```bloblang\n# Bad: Assumes field is already a string\nroot.id = this.user_id.uppercase()\n\n# Good: Converts to string first\nroot.id = this.user_id.string().uppercase()\nroot.count = this.total.number().catch(0)\n```\n\n**IMPORTANT**: Arithmetic operations on null values fail silently.\nAlways check for null or use `.catch()` to provide fallbacks:\n\n```bloblang\n# Bad: Fails silently if price is null\nroot.total = this.price * this.quantity\n\n# Good: Check for null before operations\nroot.total = if this.price != null && this.quantity != null {\n  this.price * this.quantity\n} else {\n  null\n}\n\n# Also good: Use catch to handle null gracefully\nroot.total = (this.price * this.quantity).catch(null)\n```\n\n## Workflow\n\n1. **Understand** - Analyze input structure, desired output, and required transformations\n     - **Ambiguous requirements**: If transformation goal is unclear, ask clarifying questions before proceeding (e.g., \"Should missing fields be omitted or set to null?\", \"How should arrays with mixed types be handled?\")\n     - **Missing sample data**: If user doesn't provide input example, request it explicitly - never proceed with assumptions\n     - **Complex multistep transformations**: Break down into logical phases (parse → transform → filter → format) and confirm approach with user\n\n2. **Discover** - Generate category files to versioned directory (capture `BLOBLREF_DIR` from script output), identify relevant categories, read specific category XML files to find actual Bloblang functions/methods (NEVER guess)\n\n3. **Develop** - Write valid Bloblang syntax using discovered functions (root for output, this for input, chain methods, handle nulls)\n\n4. **Validate** - Test script with sample input data, verify output matches expectations, iterate on errors until working\n     - **Test edge cases**: Missing fields, null values, invalid formats, empty collections\n     - **Iterate**: Fix syntax errors first (variable placement, method chains), then logic errors\n\n5. **Deliver** - Write the working script and example input to files (`script.blobl`, `data.json`), present the tested output, document any assumptions\n\n**Critical: Never present untested code. All scripts must be validated before showing to user.**\n",
        ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources/scripts/format-bloblang.py": "#!/usr/bin/env python3\n\"\"\"\nFormat bloblang functions or methods metadata from jsonschema output into category files.\n\"\"\"\n\nimport argparse\nimport json\nimport sys\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\n\ndef parse_args():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Format bloblang metadata into category files\"\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=str,\n        required=True,\n        help=\"Directory to write category files to\",\n    )\n    return parser.parse_args()\n\n\ndef get_category_names(category_type: str) -> tuple:\n    \"\"\"Get the tag type and file prefix based on category type.\n\n    Returns:\n        tuple: (tag_type, file_prefix) where tag_type is singular (function/method)\n               and file_prefix is plural (functions/methods)\n    \"\"\"\n    if category_type == \"bloblang-functions\":\n        return (\"function\", \"functions\")\n    else:\n        return (\"method\", \"methods\")\n\n\ndef group_by_category(\n    items: List[Dict[str, Any]], category_type: str\n) -> Dict[str, List[Dict]]:\n    \"\"\"Group items by category (functions) or tags (methods).\"\"\"\n    grouped = defaultdict(list)\n\n    for item in items:\n        if category_type == \"bloblang-functions\":\n            category = item.get(\"category\", \"Uncategorized\")\n        else:  # methods\n            categories = item.get(\"categories\", [])\n            if categories:\n                # Methods can have multiple categories - use first one\n                category = categories[0].get(\"Category\", \"Uncategorized\")\n            else:\n                category = \"Uncategorized\"\n\n        grouped[category].append(item)\n\n    return dict(grouped)\n\n\ndef format_item(item: Dict[str, Any], category_type: str) -> str:\n    \"\"\"Format a single function or method as a tagged section (no category field).\"\"\"\n    name = item[\"name\"]\n\n    # Build params string\n    params = item.get(\"params\", {}).get(\"named\", [])\n    if params:\n        param_strs = [f\"{p['name']}:{p['type']}\" for p in params]\n        params_attr = \", \".join(param_strs)\n    else:\n        params_attr = \"\"\n\n    # Determine tag type (function or method)\n    tag_type, _ = get_category_names(category_type)\n\n    # Opening tag with name and params attributes\n    lines = [f'<{tag_type} name=\"{name}\" params=\"{params_attr}\">']\n\n    # Description, description might be in categories[0].Description instead of top-level\n    desc = item.get(\"description\", \"\")\n    if not desc:\n        categories = item.get(\"categories\", [])\n        if categories and isinstance(categories[0], dict):\n            desc = categories[0].get(\"Description\", \"\")\n\n    if desc:\n        # Split description into sentences (each sentence on its own line)\n        # Split on '. ' to preserve sentence boundaries\n        sentences = desc.split(\". \")\n        for i, sentence in enumerate(sentences):\n            if sentence:  # Skip empty strings\n                # Add period back if not the last sentence\n                if i < len(sentences) - 1 and not sentence.endswith(\".\"):\n                    lines.append(sentence + \".\")\n                else:\n                    lines.append(sentence)\n    else:\n        print(f\"ERROR missing description for {name}\", file=sys.stderr)\n\n    # Examples (print all if present)\n    examples = item.get(\"examples\", [])\n    for idx, example in enumerate(examples):\n        if isinstance(example, dict):\n            summary = example.get(\"summary\", \"\")\n            mapping = example.get(\"mapping\", \"\")\n        else:\n            summary = \"\"\n            mapping = example\n\n        if mapping:  # Only add if not empty\n            # Always use code block format (mapping on new line)\n            if summary:\n                lines.append(f'<example summary=\"{summary}\">')\n            else:\n                lines.append(\"<example>\")\n            lines.append(mapping)\n            lines.append(\"</example>\")\n\n    # Closing tag\n    lines.append(f\"</{tag_type}>\")\n    return \"\\n\".join(lines)\n\n\ndef main():\n    args = parse_args()\n    output_dir = Path(args.output_dir)\n\n    # Ensure output directory exists\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Read JSON from stdin\n    schema = json.load(sys.stdin)\n\n    # Find category type and items\n    category_type = None\n    items = None\n    for key in [\"bloblang-functions\", \"bloblang-methods\"]:\n        if key in schema:\n            category_type = key\n            items = schema[key]\n            break\n\n    if not items:\n        print(\"Error: No bloblang items found in schema\", file=sys.stderr)\n        sys.exit(1)\n\n    # Group by category\n    grouped = group_by_category(items, category_type)\n\n    # Determine file prefix based on type\n    _, file_prefix = get_category_names(category_type)\n\n    # Write each category to separate file\n    for category_name in sorted(grouped.keys()):\n        # Skip empty and deprecated categories\n        if not category_name or category_name == \"Deprecated\":\n            continue\n\n        # Sanitize category name for filename (replace spaces with underscores)\n        safe_category = (\n            category_name.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"&\", \"_\")\n        )\n        filename = f\"{file_prefix}-{safe_category}.xml\"\n        filepath = output_dir / filename\n\n        with open(filepath, \"w\") as f:\n            # Sort items within category by name\n            category_items = sorted(grouped[category_name], key=lambda x: x[\"name\"])\n\n            # Format each item (no category field needed)\n            formatted_items = []\n            for item in category_items:\n                formatted_items.append(format_item(item, category_type))\n\n            f.write(f\"<{file_prefix}>\\n\")\n            f.write(\"\\n\\n\".join(formatted_items))\n            f.write(f\"\\n</{file_prefix}>\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
        ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources/scripts/format-bloblang.sh": "#!/bin/bash\n# Format bloblang functions and methods metadata into category files\n# Usage: ./format-bloblang.sh\n# Automatically uses skill resources cache directory\n\nset -euo pipefail\n\n# Get script directory and skill root\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nSKILL_ROOT=\"$(cd \"$SCRIPT_DIR/../..\" && pwd)\"\n\n# Create output directory in skill resources\nOUTPUT_DIR=\"$SKILL_ROOT/resources/cache/bloblref/$(\"$SCRIPT_DIR/rpk-version.sh\")\"\nmkdir -p \"$OUTPUT_DIR\"\necho \"$OUTPUT_DIR\"\n\n# Process both functions and methods\nfor CATEGORY in bloblang-functions bloblang-methods; do\n    rpk connect list --format jsonschema \"$CATEGORY\" | python3 \"$SCRIPT_DIR/format-bloblang.py\" --output-dir \"$OUTPUT_DIR\"\ndone\n",
        ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources/scripts/rpk-version.sh": "#!/bin/bash\n# Get rpk connect version number\n# Usage: ./rpk-version.sh\n# Output: Version number (e.g., \"4.72.0\")\n\nset -euo pipefail\n\nrpk connect --version | grep -oE '[0-9]+\\.[0-9]+\\.[0-9]+' | head -1\n",
        ".claude-plugin/plugins/redpanda-connect/skills/bloblang-authoring/resources/scripts/test-blobl.sh": "#!/bin/bash\n# Test a Bloblang script with input data\n# Usage: ./test-blobl.sh <directory>\n#\n# Expected files in directory:\n#   - data.json: Input JSON data (one line per message)\n#   - script.blobl: Bloblang transformation script\n\nset -euo pipefail\n\nDIR=\"${1:?Error: DIR argument required}\"\n\n# Validate directory and files exist\nif [[ ! -d \"$DIR\" ]]; then\n    echo \"Error: directory '$DIR' does not exist\" >&2\n    exit 1\nfi\nif [[ ! -f \"$DIR/data.json\" ]]; then\n    echo \"Error: $DIR/data.json not found\" >&2\n    exit 1\nfi\nif [[ ! -f \"$DIR/script.blobl\" ]]; then\n    echo \"Error: $DIR/script.blobl not found\" >&2\n    exit 1\nfi\n\n# Compact JSON with jq and pipe to rpk connect blobl\njq -c < \"$DIR/data.json\" | rpk connect blobl --pretty -f \"$DIR/script.blobl\"\n",
        ".claude-plugin/plugins/redpanda-connect/skills/component-search/SETUP.md": "# Setup\n\nThis skill requires: `rpk`, `rpk connect`, `python3`\n\n## macOS\n\n```bash\nbrew install redpanda-data/tap/redpanda python3\nrpk connect install\nrpk connect upgrade\n```\n\n## Ubuntu (Intel/AMD64)\n\n```bash\napt-get update && apt-get install -y curl unzip python3\n\ncurl -LO https://github.com/redpanda-data/redpanda/releases/latest/download/rpk-linux-amd64.zip && \\\n  unzip rpk-linux-amd64.zip -d /usr/local/bin/ && \\\n  rm rpk-linux-amd64.zip\n\nrpk connect install\nrpk connect upgrade\n```\n\n## Ubuntu (ARM64)\n\n```bash\napt-get update && apt-get install -y curl unzip python3\n\ncurl -LO https://github.com/redpanda-data/redpanda/releases/latest/download/rpk-linux-arm64.zip && \\\n  unzip rpk-linux-arm64.zip -d /usr/local/bin/ && \\\n  rm rpk-linux-arm64.zip\n\nrpk connect install\nrpk connect upgrade\n```\n",
        ".claude-plugin/plugins/redpanda-connect/skills/component-search/SKILL.md": "---\nname: component-search\ndescription: This skill should be used when users need to discover Redpanda Connect components for their streaming pipelines. Trigger when users ask about finding inputs, outputs, processors, or other components, or when they mention specific technologies like \"kafka consumer\", \"postgres output\", \"http server\", or ask \"which component should I use for X\".\n---\n\n# Redpanda Connect Component Search\n\nHelp users discover the right Redpanda Connect components for their streaming pipeline needs.\n\n## Objective\n\nFind and recommend the most relevant components that match the user's natural language query.\nProvide enough information for users to understand what each component does, how to configure it, and why it matches their needs.\n\n## Prerequisites\n\nThis skill requires: `rpk`, `rpk connect`, `python3`.\nSee the [SETUP](SETUP.md) for installation instructions.\n\n## Component Categories\n\nRedpanda Connect has 8 types of components:\n- **inputs** - Read data from sources (Kafka, HTTP, files, databases, etc.)\n- **outputs** - Write data to destinations (Kafka, S3, databases, etc.)\n- **processors** - Transform, filter, or enrich messages (mapping, filtering, etc.)\n- **caches** - Store data for lookups (Redis, in-memory, etc.)\n- **rate-limits** - Control throughput (local, Redis-based, etc.)\n- **buffers** - Queue messages between pipeline stages\n- **metrics** - Export metrics (Prometheus, CloudWatch, etc.)\n- **tracers** - Export traces (Jaeger, OTLP, etc.)\n\n## Tools\n\n### Component Discovery\n\nLists all available components in a category using rpk.\n\n```bash\n# Usage:\nrpk connect list <category>\n\n# Examples:\nrpk connect list inputs\nrpk connect list outputs\nrpk connect list processors\n```\n- Categories: inputs, outputs, processors, caches, rate-limits, buffers, metrics, tracers\n- Returns list of all component names in that category\n- Use this to discover what components exist before searching for specific ones\n\n### Script format-component-fields.sh\n\nRetrieves and formats component configuration schemas.\n\n```bash\n# Usage:\n./resources/scripts/format-component-fields.sh <category> <component>\n\n# Examples:\n./resources/scripts/format-component-fields.sh outputs redis_hash\n./resources/scripts/format-component-fields.sh inputs kafka_franz\n./resources/scripts/format-component-fields.sh processors mapping\n```\n- Requires two arguments:\n  - category (inputs, outputs, processors, caches, rate-limits, buffers, metrics, tracers)\n  - component name (e.g., kafka_franz, redis_hash, postgres)\n- Outputs formatted field information grouped by priority:\n    - `<required_fields>` - Must be configured\n    - `<optional_fields>` - Commonly used settings\n    - `<advanced_fields>` - Less common configuration\n    - `<secret_fields>` - Sensitive credentials\n- Flattens nested fields with dot notation (e.g., `sasl.password`)\n- Shows array element types (e.g., `array[string]`)\n- Automatically filters deprecated fields\n\n### Script rpk-version.sh\n\nReturns the current Redpanda Connect version in rpk.\n\n```bash\n# Usage:\n./resources/scripts/rpk-version.sh\n\n# Output example: 4.70.0\n```\n- No arguments\n- Outputs version as a string (e.g., \"4.70.0\")\n\n### Online Component Documentation\n\nLinks to official documentation for detailed component reference.\n\n```\n# URL pattern:\nhttps://github.com/redpanda-data/connect/blob/v{version}/docs/modules/components/pages/{category}/{component}.adoc\n\n# Examples:\nhttps://github.com/redpanda-data/connect/blob/v4.70.0/docs/modules/components/pages/inputs/kafka_franz.adoc\nhttps://github.com/redpanda-data/connect/blob/v4.70.0/docs/modules/components/pages/outputs/postgres.adoc\n```\n- `{version}` - Connect version from rpk-version.sh (e.g., \"4.70.0\")\n- `{category}` - Component category (inputs, outputs, processors, etc.)\n- `{component}` - Component name with underscores (e.g., \"kafka_franz\")\n\n## Workflow\n\n1. **Understand the query**\n   - Identify what type of component (input/output/processor/etc.), which technology (kafka/postgres/http), and what action (read/write/transform)\n   - If the query is unclear, ask clarifying questions about intent\n\n2. **Find matching components**\n   - Discover components across relevant categories that match the user's needs\n   - If no exact match exists, recommend similar or related components\n\n3. **Retrieve configuration details**\n   - Get schema information for matched components to understand:\n     - What fields are required vs optional\n     - What the component's capabilities are\n     - How complex it is to configure\n\n4. **Rank by relevance**\n   - Prioritize components by:\n     - How well they match the query intent\n     - Their stability status (stable > beta > experimental)\n     - Configuration simplicity (fewer required fields) \n\n5. **Present clearly**\n   - Show the top 1-3 results with:\n     - Component name and category\n     - Brief description of what it does and justification for why it matches the query\n     - Configuration requirements (required fields, common optional fields)\n     - Minimal configuration example\n     - Link to official documentation for more details\n     - If component directly matches the query, ignore similar alternatives\n",
        ".claude-plugin/plugins/redpanda-connect/skills/component-search/resources/scripts/format-component-fields.py": "#!/usr/bin/env python3\n\"\"\"\nFormat component fields from jsonschema output into tagged sections.\n\nUsage: rpk connect list --format jsonschema <category>s <component> | ./format-component-fields.py\nExample: rpk connect list --format jsonschema inputs kafka_franz | ./format-component-fields.py\n\"\"\"\n\nimport sys\nimport json\nfrom typing import Dict, List, Any, Tuple\n\n\ndef format_type(type_str: str, is_array: bool = False) -> str:\n    \"\"\"Format type string with array notation if needed.\"\"\"\n    if is_array:\n        return f\"array[{type_str}]\"\n    return type_str\n\n\ndef extract_fields(properties: Dict[str, Any], parent_name: str = \"\") -> List[Dict[str, Any]]:\n    \"\"\"\n    Extract fields recursively, flattening nested objects with dot notation.\n\n    For arrays of primitives: note as \"array[type]\"\n    For objects: inline child fields with parent.child notation\n    For arrays of objects: inline with parent.child notation and note as array\n    \"\"\"\n    fields = []\n\n    for field_name, field_info in properties.items():\n        full_name = f\"{parent_name}.{field_name}\" if parent_name else field_name\n        field_type = field_info.get(\"type\", \"unknown\")\n        is_advanced = field_info.get(\"is_advanced\", False)\n        is_optional = field_info.get(\"is_optional\", False)\n        is_deprecated = field_info.get(\"is_deprecated\", False)\n        is_secret = field_info.get(\"is_secret\", False)\n\n        # Skip deprecated fields\n        if is_deprecated:\n            continue\n\n        if field_type == \"object\":\n            # Object: inline nested fields with dot notation\n            nested_props = field_info.get(\"properties\", {})\n            if nested_props:\n                # Recursively extract nested fields\n                nested_fields = extract_fields(nested_props, full_name)\n                fields.extend(nested_fields)\n            else:\n                # Empty object or no properties defined\n                fields.append({\n                    \"name\": full_name,\n                    \"type\": \"object\",\n                    \"is_advanced\": is_advanced,\n                    \"is_optional\": is_optional,\n                    \"is_secret\": is_secret,\n                })\n\n        elif field_type == \"array\":\n            # Array: check items type\n            items = field_info.get(\"items\", {})\n            items_type = items.get(\"type\", \"unknown\")\n\n            if items_type == \"object\":\n                # Array of objects: inline nested fields with dot notation\n                nested_props = items.get(\"properties\", {})\n                if nested_props:\n                    nested_fields = extract_fields(nested_props, full_name)\n                    # Mark all nested fields as array types\n                    for nf in nested_fields:\n                        nf[\"type\"] = f\"array[{nf['type']}]\"\n                    fields.extend(nested_fields)\n                else:\n                    fields.append({\n                        \"name\": full_name,\n                        \"type\": \"array[object]\",\n                        \"is_advanced\": is_advanced,\n                        \"is_optional\": is_optional,\n                        \"is_secret\": is_secret,\n                    })\n            else:\n                # Array of primitives\n                fields.append({\n                    \"name\": full_name,\n                    \"type\": format_type(items_type, is_array=True),\n                    \"is_advanced\": is_advanced,\n                    \"is_optional\": is_optional,\n                    \"is_secret\": is_secret,\n                })\n\n        else:\n            # Primitive type\n            fields.append({\n                \"name\": full_name,\n                \"type\": field_type,\n                \"is_advanced\": is_advanced,\n                \"is_optional\": is_optional,\n                \"is_secret\": is_secret,\n            })\n\n    return fields\n\n\ndef group_fields(fields: List[Dict[str, Any]]) -> Tuple[List[Dict], List[Dict], List[Dict], List[Dict]]:\n    \"\"\"Group fields into required, optional, advanced, and secrets.\"\"\"\n    required = []\n    optional = []\n    advanced = []\n    secrets = []\n\n    for field in fields:\n        if field[\"is_secret\"]:\n            secrets.append(field)\n\n        if field[\"is_advanced\"]:\n            advanced.append(field)\n        elif field[\"is_optional\"]:\n            optional.append(field)\n        else:\n            required.append(field)\n\n    return required, optional, advanced, secrets\n\n\ndef format_field(field: Dict[str, Any]) -> str:\n    \"\"\"Format a single field for output.\"\"\"\n    return f\"  - {field['name']} ({field['type']})\"\n\n\ndef main():\n    # Component name passed as command line argument\n    if len(sys.argv) < 2:\n        print(\"Error: Component name required as argument\", file=sys.stderr)\n        sys.exit(1)\n\n    target_component = sys.argv[1]\n\n    # Read JSON from stdin\n    schema = json.load(sys.stdin)\n\n    # Find the target component in the schema\n    component_def = None\n\n    for category_name, category_def in schema.get(\"definitions\", {}).items():\n        for item in category_def.get(\"allOf\", [{}])[0].get(\"anyOf\", []):\n            if target_component in item.get(\"properties\", {}):\n                component_def = item[\"properties\"][target_component]\n                break\n        if component_def:\n            break\n\n    if not component_def:\n        print(f\"Error: Component '{target_component}' not found in schema\", file=sys.stderr)\n        sys.exit(1)\n\n    # Extract and group fields\n    properties = component_def.get(\"properties\", {})\n    fields = extract_fields(properties)\n    required, optional, advanced, secrets = group_fields(fields)\n\n    # Output tagged sections\n    if required:\n        print(\"<required_fields>\")\n        for field in sorted(required, key=lambda f: f[\"name\"]):\n            print(format_field(field))\n        print(\"</required_fields>\")\n\n    if optional:\n        print(\"<optional_fields>\")\n        for field in sorted(optional, key=lambda f: f[\"name\"]):\n            print(format_field(field))\n        print(\"</optional_fields>\")\n\n    if advanced:\n        print(\"<advanced_fields>\")\n        for field in sorted(advanced, key=lambda f: f[\"name\"]):\n            print(format_field(field))\n        print(\"</advanced_fields>\")\n\n    if secrets:\n        print(\"<secret_fields>\")\n        for field in sorted(secrets, key=lambda f: f[\"name\"]):\n            print(format_field(field))\n        print(\"</secret_fields>\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
        ".claude-plugin/plugins/redpanda-connect/skills/component-search/resources/scripts/format-component-fields.sh": "#!/bin/bash\n# Format component fields from jsonschema output into tagged sections\n# Usage: ./format-component-fields.sh <category> <component>\n# Example: ./format-component-fields.sh inputs kafka_franz\n\nset -euo pipefail\n\nCATEGORY=\"$1\"  # e.g., \"inputs\", \"outputs\", \"processors\"\nCOMPONENT=\"$2\"  # e.g., \"kafka_franz\", \"stdout\"\n\n# Get script directory\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n\n# Fetch jsonschema and pipe to Python formatter\n# Note: rpk returns schema for ALL components regardless of component name argument\n# Pass component name to Python script for filtering\nrpk connect list --format jsonschema \"${CATEGORY}\" | python3 \"$SCRIPT_DIR/format-component-fields.py\" \"$COMPONENT\"\n",
        ".claude-plugin/plugins/redpanda-connect/skills/component-search/resources/scripts/rpk-version.sh": "#!/bin/bash\n# Get rpk connect version number\n# Usage: ./rpk-version.sh\n# Output: Version number (e.g., \"4.72.0\")\n\nset -euo pipefail\n\nrpk connect --version | grep -oE '[0-9]+\\.[0-9]+\\.[0-9]+' | head -1\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/SETUP.md": "# Setup\n\nThis skill requires: `rpk`, `rpk connect`\n\n## macOS\n\n```bash\nbrew install redpanda-data/tap/redpanda\nrpk connect install\nrpk connect upgrade\n```\n\n## Ubuntu (Intel/AMD64)\n\n```bash\napt-get update && apt-get install -y curl unzip\n\ncurl -LO https://github.com/redpanda-data/redpanda/releases/latest/download/rpk-linux-amd64.zip && \\\n  unzip rpk-linux-amd64.zip -d /usr/local/bin/ && \\\n  rm rpk-linux-amd64.zip\n\nrpk connect install\nrpk connect upgrade\n```\n\n## Ubuntu (ARM64)\n\n```bash\napt-get update && apt-get install -y curl unzip\n\ncurl -LO https://github.com/redpanda-data/redpanda/releases/latest/download/rpk-linux-arm64.zip && \\\n  unzip rpk-linux-arm64.zip -d /usr/local/bin/ && \\\n  rm rpk-linux-arm64.zip\n\nrpk connect install\nrpk connect upgrade\n```\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/SKILL.md": "---\nname: pipeline-assistant\ndescription: This skill should be used when users need to create or fix Redpanda Connect pipeline configurations. Trigger when users mention \"config\", \"pipeline\", \"YAML\", \"create a config\", \"fix my config\", \"validate my pipeline\", or describe a streaming pipeline need like \"read from Kafka and write to S3\".\n---\n\n# Redpanda Connect Configuration Assistant\n\nCreate working, validated Redpanda Connect configurations from scratch or repair existing configurations that have issues.\n\n**This skill REQUIRES skills: `component-search`, `bloblang-authoring`.**\n\n## Objective\n\nDeliver a complete, valid YAML configuration that passes validation and meets the user's requirements.\nWhether starting from a description or fixing a broken config, the result must be production-ready with properly secured credentials.\n\nHandle Two Scenarios:\n**Creation** - User provides description like \"Read from Kafka on localhost:9092 topic 'events' to stdout\"\n**Repair** - User provides config file path and optional error context\n\nThis skill focuses ONLY on pipeline configuration orchestration and validation.\n\n**Skill Delegation**:\n\nNEVER directly use component-search or bloblang-authoring tools.\n- **Component Discovery** - ALWAYS delegate to `component-search` skill when it is unclear which components to use OR when you need component configuration details\n- **Bloblang Development** - ALWAYS delegate to `bloblang-authoring` skill when creating or fixing Bloblang transformations and NEVER write Bloblang yourself\n\n## Setup\n\nThis skill requires: `rpk`, `rpk connect`.\nSee the [SETUP](SETUP.md) for installation instructions.\n\n## Tools\n\n### Scaffold Pipeline\n\nGenerates YAML configuration template from component expression.\nUseful for quickly creating first pipeline draft.\n\n```bash\n# Usage:\nrpk connect create [--small] <input>,...[/<processor>,...]/<output>,...\n\n# Examples:\nrpk connect create stdin/bloblang,awk/nats\nrpk connect create file,http_server/protobuf/http_client  # Multiple inputs\nrpk connect create kafka_franz/stdout  # Only input and output, no processors\nrpk connect create --small stdin/bloblang/stdout  # Minimal config, omit advanced fields\n```\n- Requires component expression specifying desired inputs, processors, and outputs\n- Expression format: `inputs/processors/outputs` separated by `/`\n- Multiple components of same type separated by `,`\n- Outputs complete YAML configuration with specified components\n- `--small` flag omits advanced fields\n\n### Online Component Documentation\n\nUse the `component-search` skill's `Online Component Documentation` tool to look up detailed configuration information for any Redpanda Connect component containing usage examples, field descriptions, and best practices.\n\n### Lint Pipeline\n\nValidates Redpanda Connect pipeline configurations.\n\n```bash\n# Usage:\nrpk connect lint [--env-file <.env>] <pipeline.yaml>\n\n# Examples:\nrpk connect lint --env-file ./.env ./pipeline.yaml\nrpk connect lint pipeline-without-secrets.yaml\n```\n- Requires pipeline configuration file path (e.g., `pipeline.yaml`)\n- Optional `--env-file` flag provides `.env` file for environment variable substitution\n- Validates YAML syntax, component configurations, and Bloblang expressions\n- Outputs detailed error messages with specific location information\n- Exit code `0` indicates success, non-zero indicates validation failures\n- Can be run repeatedly during pipeline development and iteration\n\n### Run Pipeline\n\nExecutes Redpanda Connect pipeline to test end-to-end functionality.\n\n```bash\n# Usage:\nrpk connect run [--log.level DEBUG] --env-file <.env> <pipeline.yaml>\n\n# Examples:\nrpk connect run pipeline-without-secrets.yaml\nrpk connect run --env-file ./.env ./pipeline.yaml  # With secrets\nrpk connect run --log.level DEBUG --env-file ./.env ./pipeline.yaml  # With debug logging\n```\n- Requires pipeline configuration file path (e.g., `pipeline.yaml`)\n- Optional `--env-file` flag provides dotenv file for environment variable substitution\n- Optional `--log.level DEBUG` enables detailed logging for troubleshooting connection and processing issues\n- Starts pipeline and maintains active connections to inputs and outputs\n- Runs continuously until manually terminated with Ctrl+C (SIGINT)\n- Can be run repeatedly during pipeline development and iteration\n\n### Test with Standard Input/Output\n\nTest pipeline logic with `stdin`/`stdout` before connecting to real systems.\nEspecially useful for validating routing logic, error handling, and transformations.\n\n**Example: Content-based routing**\n\n```yaml\ninput:\n  stdin: {}\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        # Route based on message type\n        if this.type == \"error\" {\n          meta route = \"dlq\"\n        } else if this.priority == \"high\" {\n          meta route = \"urgent\"\n        } else {\n          meta route = \"standard\"\n        }\n\noutput:\n  switch:\n    cases:\n      - check: 'meta(\"route\") == \"dlq\"'\n        output:\n          stdout: {}\n        processors:\n          - mapping: 'root = \"DLQ: \" + content().string()'\n\n      - check: 'meta(\"route\") == \"urgent\"'\n        output:\n          stdout: {}\n        processors:\n          - mapping: 'root = \"URGENT: \" + content().string()'\n\n      - check: 'meta(\"route\") == \"standard\"'\n        output:\n          stdout: {}\n        processors:\n          - mapping: 'root = \"STANDARD: \" + content().string()'\n```\n\n**Test all routes:**\n```bash\necho '{\"type\":\"error\",\"msg\":\"failed\"}' | rpk connect run test.yaml\n# Output: DLQ: {\"type\":\"error\",\"msg\":\"failed\"}\n\necho '{\"priority\":\"high\",\"msg\":\"urgent\"}' | rpk connect run test.yaml\n# Output: URGENT: {\"priority\":\"high\",\"msg\":\"urgent\"}\n\necho '{\"priority\":\"low\",\"msg\":\"normal\"}' | rpk connect run test.yaml\n# Output: STANDARD: {\"priority\":\"low\",\"msg\":\"normal\"}\n```\n\n**Limitations:**\n- Stdin/stdout cannot test batching behavior realistically\n- No connection, retry, or timeout logic validation\n- Cannot test ordering guarantees or parallel processing\n- Real integration testing still required before production deployment\n\n## YAML Configuration Structure\n\nTop-level keys:\n- `input` - Data source (required): kafka_franz, http_server, stdin, aws_s3, etc\n- `output` - Data destination (required): kafka_franz, postgres, stdout, aws_s3, etc\n- `pipeline.processors` - Transformations (optional, execute sequentially)\n- `cache_resources`, `rate_limit_resources` - Reusable components (optional)\n\n**Environment variables (required for secrets):**\n```yaml\n# Basic reference\nbroker: \"${KAFKA_BROKER}\"\n\n# With default value\nbroker: \"${KAFKA_BROKER:localhost:9092}\"\n```\n\n**Field type conventions:**\n- Durations: `\"30s\"`, `\"5m\"`, `\"1h\"`, `\"100ms\"`\n- Sizes: `\"5MB\"`, `\"1GB\"`, `\"512KB\"`\n- Booleans: `true`, `false` (no quotes)\n\n**Minimal example:**\n```yaml\ninput:\n  redpanda:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topics: [\"${TOPIC}\"]\n\npipeline:\n  processors:\n    - mapping:\n        | # Bloblang transformation - use  bloblang-authoring skill to create\n        root = this\n        root.timestamp = now()\n\noutput:\n  stdout: {}\n```\n\nUse `Quick Pipeline Scaffolding` for initial drafts.\n\n### Production Recipes/Patterns\n\nThe `./resources/recipes/` directory contains validated production patterns.\nEach recipe includes:\n- **Markdown documentation** (`.md`) - Pattern explanation, configuration details, testing instructions, and variations\n- **Working YAML configuration** (`.yaml`) - Complete, tested pipeline referenced in the markdown\n\n**Before writing pipelines:**\n1. **Read component documentation** - Use `Online Component Documentation` tool for detailed field info and examples\n2. **Read relevant recipes** - When user describes a pattern matching a recipe (routing, DLQ, replication, etc.), read the markdown file first\n3. **Adapt, don't copy** - Use recipes as reference for patterns and best practices, customize for user's specific requirements\n\n#### Available Recipes\n**Error Handling**\n- `dlq-basic.md` - Dead letter queue for error handling\n\n**Routing**\n- `content-based-router.md` - Route messages by field values\n- `multicast.md` - Fan-out to multiple destinations\n\n**Replication**\n- `kafka-replication.md` - Cross-cluster Kafka streaming\n- `cdc-replication.md` - Database change data capture\n\n**Cloud Storage**\n- `s3-sink-basic.md` - S3 output with batching\n- `s3-sink-time-based.md` - Time-partitioned S3 writes\n- `s3-polling.md` - Poll S3 for new files\n\n**Stateful Processing**\n- `stateful-counter.md` - Stateful counting with cache\n- `window-aggregation.md` - Time-window aggregations\n\n**Performance & Monitoring**\n- `rate-limiting.md` - Throughput control\n- `custom-metrics.md` - Prometheus metrics\n\n## Workflow\n\n### Creating New Configurations\n\n1. **Understand requirements**\n   - Parse description for source, destination, transformations, and special needs (ordering, batching, etc.)\n   - Ask clarifying questions for ambiguous aspects\n   - Check `./resources/recipes/` for relevant patterns\n\n2. **Discover components**\n   - Use `component-search` skill if unclear which components to use\n   - Read component documentation for configuration details\n\n3. **Build configuration**\n   - Generate scaffold with `rpk connect create input/processor/output`\n   - Add all required fields from component schemas\n   - For secrets: ask user for env var names → use `${VAR_NAME}` → document in `.env.example`\n   - Keep configuration minimal and simple\n\n4. **Add transformations** (if needed)\n   - Delegate to `bloblang-authoring` skill for tested scripts\n   - Embed in `pipeline.processors` section\n\n5. **Validate and iterate**\n   - Run `rpk connect lint`\n   - On errors: parse → fix → re-validate until clean\n   - Iterate until validation passes\n\n6. **Test and iterate**\n   - Test with `rpk connect run`\n     - Temporarily use `stdin` and `stdout` for easier testing\n     - Run with `rpk connect run`\n     - Fix any runtime issues\n     - Test all edge cases\n     - Iterate until tests pass\n   - Test connection and authentication to real systems if possible\n\n7. **Deliver**\n   - Deliver final `pipeline.yaml` and `.env.example`\n   - Explain component choices and configuration decisions\n   - Create concise `TESTING.md` with only practical followup testing instructions:\n     - How to set up environment\n     - Command to run the pipeline\n     - Sample curl/test commands with realistic data\n     - How to verify results in the target system\n     - ONLY include new/essential information, avoid verbose explanations\n   - NEVER create README files\n   - Show concise summary in chat response\n\n### Repairing Existing Configurations\n\n1. **Diagnose**\n   - Run `rpk connect lint` to identify errors\n   - Review user-provided context about symptoms\n   - Find root causes (typos, deprecations, type mismatches)\n\n2. **Explain issues**\n   - Translate validation errors to plain language\n   - Explain why current configuration doesn't work\n   - Identify root causes, not just symptoms\n\n3. **Fix minimally**\n   - Get user approval before modifying files\n   - Preserve original structure, comments, and intent\n   - Replace deprecated components if needed\n   - Apply secret handling with environment variables\n\n4. **Verify**\n   - Re-validate after each change\n   - Test modified Bloblang transformations\n   - Confirm no regressions introduced\n\n### Security Requirements (Critical)\n\n**Never store credentials in plain text:**\n- All passwords, secrets, tokens, API keys MUST use `${ENV_VAR}` syntax in YAML\n- Never put actual credentials in YAML or conversation\n\n**Environment variable files:**\n- `.env` - Contains actual secret values, used at runtime with `--env-file .env`, NEVER commit to git\n- `.env.example` - Documents required variables with placeholder values, safe to commit\n- Always remind user to add `.env` to `.gitignore`\n\n**When encountering sensitive fields** (from `<secret_fields>` in component schema):\n1. Ask user for environment variable name (e.g., `KAFKA_PASSWORD`)\n2. Write `${KAFKA_PASSWORD}` in YAML configuration\n3. Document in `.env.example`: `KAFKA_PASSWORD=your_password_here`\n4. User creates actual `.env` with real value: `KAFKA_PASSWORD=actual_secret_123`\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/cdc-replication.md": "# Change Data Capture (CDC) Replication\n\n**Pattern**: Kafka Patterns - Database CDC Replication\n**Difficulty**: Advanced\n**Components**: postgres_cdc, sql_raw, switch, batching\n**Use Case**: Replicate database changes in real-time using Postgres logical replication to keep databases synchronized\n\n## Overview\n\nThis recipe demonstrates Change Data Capture (CDC) for replicating database changes. It streams changes from a Postgres database using logical replication, groups them by transaction, and applies them to a destination database using MERGE (upsert) and DELETE operations. This pattern is essential for building real-time data synchronization pipelines.\n\n## Configuration\n\nSee [`cdc-replication.yaml`](./cdc-replication.yaml) for the complete configuration.\n\n## Key Concepts\n\n### 1. Postgres CDC Input\n\nThe `postgres_cdc` input streams database changes using Postgres logical replication:\n- **Replication Slot**: Named slot for tracking position\n- **Snapshot**: Initial table snapshot before streaming changes\n- **Transaction Markers**: Begin/commit messages for grouping\n- **Operations**: Insert, update, delete with full row data\n\n### 2. Transaction-Based Batching\n\nChanges are grouped by transaction to maintain consistency:\n```yaml\nbatching:\n  check: '@operation == \"commit\"'\n  period: 10s\n```\n\nAll changes in a transaction are batched together before being applied. This preserves foreign key constraints and data consistency.\n\n### 3. Switch Output for Operation Types\n\nDifferent operations require different SQL:\n- **Insert/Update** → SQL MERGE (upsert)\n- **Delete** → SQL DELETE\n\nThe switch routes based on `@operation` metadata.\n\n### 4. SQL MERGE for Upserts\n\nThe MERGE statement handles both inserts and updates atomically:\n```sql\nMERGE INTO dst_table AS old\nUSING (SELECT $1 id, $2 foo, $3 bar) AS new\nON new.id = old.id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT ...\n```\n\nThis ensures idempotency - replaying the same change is safe.\n\n## Important Details\n\n- **Security**: Use environment variables for DSN (`${POSTGRES_DSN}`)\n- **Performance**:\n  - Transaction batching reduces round-trips\n  - Replication slot prevents data loss\n  - Window period (10s) must accommodate largest transaction\n- **Error handling**: `strict_mode: true` ensures all messages match a case\n- **Idempotency**: MERGE operations can be safely retried\n\n## Testing\n\n```bash\n# Set environment variables\nexport SOURCE_DSN=\"postgres://user:pass@source:5432/db?sslmode=disable\"\nexport DEST_DSN=\"postgres://user:pass@dest:5432/db?sslmode=disable\"\n\n# Create replication slot on source database\npsql $SOURCE_DSN -c \"SELECT pg_create_logical_replication_slot('test_slot', 'pgoutput');\"\n\n# Run the pipeline\nrpk connect run cdc-replication.yaml\n\n# In another terminal, make changes to source database\npsql $SOURCE_DSN -c \"INSERT INTO my_src_table (id, foo, bar) VALUES (1, 'test', 'data');\"\npsql $SOURCE_DSN -c \"UPDATE my_src_table SET foo='updated' WHERE id=1;\"\npsql $SOURCE_DSN -c \"DELETE FROM my_src_table WHERE id=1;\"\n\n# Check destination database\npsql $DEST_DSN -c \"SELECT * FROM my_dst_table;\"\n```\n\n## Variations\n\n**Kafka as Destination:**\n```yaml\noutput:\n  switch:\n    cases:\n      - check: '@operation == \"delete\"'\n        output:\n          kafka_franz:\n            topic: deletes\n      - output:\n          kafka_franz:\n            topic: upserts\n```\n\n**Multi-Table Replication:**\n```yaml\ninput:\n  postgres_cdc:\n    tables: [table1, table2, table3]\n\noutput:\n  switch:\n    cases:\n      - check: '@table == \"table1\"'\n        output:\n          sql_raw:\n            query: |\n              MERGE INTO dst_table1 ...\n```\n\n## Related Recipes\n\n- [Content-Based Router](./content-based-router.md) - Similar switch-based routing pattern\n- [Stateful Counter](../stateful/stateful-counter.md) - Track CDC metrics\n\n## References\n\n- [Postgres CDC Input Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/inputs/postgres_cdc.adoc)\n- [SQL Raw Output Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/outputs/sql_raw.adoc)\n- [Postgres Logical Replication](https://www.postgresql.org/docs/current/logical-replication.html)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/cdc-replication.yaml": "# Change Data Capture (CDC) Replication\n# Pattern: Kafka Patterns - Database CDC Replication\n# Difficulty: Advanced\n\n# --- Input Configuration ---\ninput:\n  postgres_cdc:\n    # Source database connection\n    dsn: \"${SOURCE_DSN}\"\n\n    # Include transaction begin/commit markers for grouping\n    include_transaction_markers: true\n\n    # Replication slot name (must be created beforehand)\n    slot_name: test_slot\n\n    # Stream initial snapshot before changes\n    stream_snapshot: true\n\n    # Schema and tables to replicate\n    schema: public\n    tables: [my_src_table]\n\n    # Group changes by transaction\n    # All changes in a transaction are batched together\n    batching:\n      # Batch completes when commit marker is seen\n      check: '@operation == \"commit\"'\n\n      # Window period - must be large enough for full transaction\n      # If a transaction takes longer than this, it may be split\n      period: 10s\n\n      processors:\n        # Remove transaction markers (begin/commit)\n        # Only keep actual data changes\n        - mapping: |\n            root = if @operation == \"begin\" || @operation == \"commit\" {\n              deleted()\n            } else {\n              this\n            }\n\n# --- Output Configuration ---\noutput:\n  # Route based on operation type\n  switch:\n    # Strict mode ensures all messages match a case\n    strict_mode: true\n\n    cases:\n      # Handle INSERT and UPDATE operations\n      - check: '@operation != \"delete\"'\n        output:\n          sql_raw:\n            driver: postgres\n            dsn: \"${DEST_DSN}\"\n\n            # Map message fields to SQL parameters\n            args_mapping: root = [this.id, this.foo, this.bar]\n\n            # MERGE statement for upsert (insert or update)\n            query: |\n              MERGE INTO my_dst_table AS old\n              USING (SELECT\n                $1 id,\n                $2 foo,\n                $3 bar\n              ) AS new\n              ON new.id = old.id\n              WHEN MATCHED THEN\n                UPDATE SET\n                  foo = new.foo,\n                  bar = new.bar\n              WHEN NOT MATCHED THEN\n                INSERT (id, foo, bar)\n                VALUES (new.id, new.foo, new.bar);\n\n      # Handle DELETE operations\n      - check: '@operation == \"delete\"'\n        output:\n          sql_raw:\n            driver: postgres\n            dsn: \"${DEST_DSN}\"\n\n            # Delete by ID\n            query: DELETE FROM my_dst_table WHERE id = $1\n\n            # Only pass the ID field\n            args_mapping: root = [this.id]\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/content-based-router.md": "# Content-Based Router for Kafka\n\n**Pattern**: Kafka Patterns - Content-Based Routing\n**Difficulty**: Basic\n**Components**: kafka_franz (input/output), mapping\n**Use Case**: Route Kafka messages to different topics based on message content fields\n\n## Overview\n\nThe Content-Based Router pattern dynamically routes messages to various destinations based on message content. This recipe shows how to filter Kafka messages by examining payload fields and routing only matching messages to the output topic, while preserving partition keys, timestamps, and headers for ordering guarantees.\n\n## Configuration\n\nSee [`content-based-router.yaml`](./content-based-router.yaml) for the complete configuration.\n\n## Key Concepts\n\n### 1. Content Inspection\n\nMessages are examined using Bloblang to check specific fields:\n```bloblang\nif (this.marketid == \"nyse\") {\n  root = this\n} else {\n  root = deleted()  # Filter out non-matching messages\n}\n```\n\nOnly messages matching the condition are forwarded; others are silently dropped.\n\n### 2. Metadata Preservation\n\nKafka-specific metadata is preserved through the pipeline:\n- Partition key - Maintains message ordering\n- Partition number - Preserves partitioning strategy\n- Timestamp - Keeps original event time\n- Headers - Retains all custom metadata\n\nThis is critical for maintaining ordering guarantees in distributed systems.\n\n### 3. Manual Partitioning\n\nThe output uses `partitioner: \"manual\"` to explicitly control which partition messages go to:\n```yaml\npartitioner: \"manual\"\npartition: \"${!metadata(\\\"kafka_partition\\\")}\"\n```\n\nThis ensures messages maintain their source partition assignment.\n\n## Important Details\n\n- **Security**: Uses environment variables for broker addresses (`${KAFKA_BROKER}`)\n- **Performance**:\n  - `max_in_flight: 256` - High parallelism for throughput\n  - `idempotent_write: true` - Prevents duplicates\n  - `broker_write_max_bytes: 100MiB` - Handles large messages\n- **Error handling**: `auto_replay_nacks: true` retries failed messages\n- **Ordering**: Manual partitioning preserves source partition order\n\n## Testing\n\n```bash\n# Set environment variables\nexport KAFKA_BROKER=localhost:9092\nexport SOURCE_TOPIC=test_in\nexport DEST_TOPIC=topic_a\nexport CONSUMER_GROUP=test_cg\n\n# Run the pipeline\nrpk connect run content-based-router.yaml\n\n# Produce test messages\necho '{\"marketid\":\"nyse\",\"symbol\":\"AAPL\",\"price\":150}' | rpk topic produce $SOURCE_TOPIC\necho '{\"marketid\":\"nasdaq\",\"symbol\":\"MSFT\",\"price\":300}' | rpk topic produce $SOURCE_TOPIC\necho '{\"marketid\":\"nyse\",\"symbol\":\"GOOGL\",\"price\":2800}' | rpk topic produce $SOURCE_TOPIC\n\n# Check output topic (only NYSE messages should appear)\nrpk topic consume $DEST_TOPIC\n```\n\n## Variations\n\n**Multiple Destinations:**\nReplace the filter processor with a `switch` output to route to different topics:\n```yaml\noutput:\n  switch:\n    cases:\n      - check: 'json(\"marketid\") == \"nyse\"'\n        output:\n          kafka_franz:\n            topic: topic_nyse\n      - check: 'json(\"marketid\") == \"nasdaq\"'\n        output:\n          kafka_franz:\n            topic: topic_nasdaq\n```\n\n## Related Recipes\n\n- [DLQ Basic](../error-handling/dlq-basic.md) - Handle messages that fail routing\n- [CDC Replication](./cdc-replication.md) - Advanced switch-based routing\n\n## References\n\n- [Kafka Franz Input Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/inputs/kafka_franz.adoc)\n- [Manual Partitioner](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/outputs/kafka_franz.adoc#partitioner)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/content-based-router.yaml": "# Content-Based Router for Kafka\n# Pattern: Kafka Patterns - Content-Based Routing\n# Difficulty: Basic\n\n# --- Input Configuration ---\ninput:\n  label: consume_from_source\n  kafka_franz:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topics: [\"${SOURCE_TOPIC}\"]\n    regexp_topics: false\n    consumer_group: \"${CONSUMER_GROUP}\"\n    auto_replay_nacks: true  # Retry failed messages\n\n  processors:\n    # Preserve Kafka metadata before processing\n    - label: copy_kafka_metadata\n      mapping: |\n        # Separate Kafka-specific metadata from custom metadata\n        # This allows us to restore partition/key/timestamp in output\n        let kafka_meta = @.filter(kv -> kv.key.has_prefix(\"kafka_\"))\n        meta = @.filter(kv -> !kv.key.has_prefix(\"kafka_\"))\n        meta kafka_metadata = $kafka_meta\n\n    # Filter messages based on content\n    - label: filter_by_marketid\n      mapping: |\n        # Route only NYSE messages\n        if (this.marketid == \"nyse\") {\n          root = this\n        } else {\n          # Filter out non-NYSE messages\n          root = deleted()\n        }\n\n# --- Output Configuration ---\noutput:\n  label: write_to_destination\n  kafka_franz:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topic: \"${DEST_TOPIC}\"\n\n    # Preserve source partition (maintains ordering)\n    partitioner: \"manual\"\n    partition: \"${!metadata(\\\"kafka_metadata\\\").kafka_partition}\"\n\n    # Preserve source message key (maintains co-partitioning)\n    key: \"${!metadata(\\\"kafka_metadata\\\").kafka_key}\"\n\n    # Preserve source timestamp (maintains event time)\n    timestamp: \"${!metadata(\\\"kafka_metadata\\\").kafka_timestamp_unix}\"\n\n    # Preserve all custom headers\n    metadata:\n      include_patterns: [\".*\"]\n\n    # Use idempotent writes to minimize duplicates\n    idempotent_write: true\n\n    # Performance tuning\n    max_message_bytes: 1024          # Batch size before compression\n    broker_write_max_bytes: 100MiB   # Max request size for large messages\n    max_in_flight: 256               # High parallelism for throughput\n\n    # Set client ID for tracing/debugging\n    client_id: \"content_based_router\"\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/custom-metrics.md": "# Custom Prometheus Metrics\n\n**Pattern**: Monitoring - Custom Metrics\n**Difficulty**: Basic\n**Components**: stdin, metric processor, prometheus\n**Use Case**: Emit custom application metrics to Prometheus for monitoring and alerting\n\n## Overview\n\nThis recipe demonstrates how to add custom Prometheus metrics to your Redpanda Connect pipelines. The example tracks JSON validation errors as a counter metric, which can be scraped by Prometheus and used for alerting. This pattern is essential for building observable data pipelines.\n\n## Configuration\n\nSee [`custom-metrics.yaml`](./custom-metrics.yaml) for the complete configuration.\n\n## Key Concepts\n\n### 1. Metric Processor\n\nThe `metric` processor emits metrics during message processing:\n\n```yaml\n- metric:\n    type: counter_by\n    name: json_error_count\n    value: 1\n    labels:\n      pipeline: \"json_validation\"\n      error_type: \"invalid_json\"\n```\n\n- **type**: `counter_by` increments by the specified value\n- **name**: Metric name (appears in Prometheus)\n- **value**: Amount to increment (can use Bloblang expressions)\n- **labels**: Key-value pairs for filtering/grouping\n\n### 2. Prometheus Endpoint\n\nThe `metrics` section configures how metrics are exposed:\n\n```yaml\nmetrics:\n  prometheus: {}  # Default HTTP endpoint on :4195/stats\n  mapping: |\n    # Filter which metrics to expose\n    if this != \"json_error_count\" { deleted() }\n```\n\nThe mapping filters internal metrics, exposing only custom ones.\n\n### 3. Metric Types\n\nRedpanda Connect supports multiple metric types:\n- `counter` - Monotonically increasing (e.g., total messages)\n- `counter_by` - Increment by value\n- `gauge` - Current value (e.g., queue depth)\n- `timing` - Duration tracking\n\n## Important Details\n\n- **Security**: Metrics endpoint is HTTP by default, consider adding auth for production\n- **Performance**: Minimal overhead - metrics are asynchronous\n- **Error handling**: Metrics don't block pipeline - failures are logged\n- **Cardinality**: Be careful with label values - high cardinality can cause issues\n\n## Testing\n\n```bash\n# Run the pipeline\nrpk connect run custom-metrics.yaml\n\n# In another terminal, send test data\necho '{\"valid\":\"json\"}' | nc localhost 8080\necho 'invalid json' | nc localhost 8080\necho '{\"more\":\"data\"}' | nc localhost 8080\n\n# Check metrics endpoint\ncurl -s http://localhost:4195/stats | grep json_error_count\n\n# Expected output (after one error):\n# json_error_count{error_type=\"invalid_json\",label=\"emit_error_metric\",path=\"root.pipeline.processors.1\",pipeline=\"json_validation\"} 1\n```\n\n## Variations\n\n**Gauge Metric (Current Value):**\n```yaml\n- metric:\n    type: gauge\n    name: queue_depth\n    value: ${!json(\"queue_size\")}\n```\n\n**Timing Metric (Duration):**\n```yaml\n- metric:\n    type: timing\n    name: processing_duration_ms\n    value: ${!json(\"duration\")}\n```\n\n**Dynamic Labels:**\n```yaml\n- metric:\n    type: counter_by\n    name: messages_by_topic\n    value: 1\n    labels:\n      topic: ${!metadata(\"kafka_topic\")}\n```\n\n### Multi-Instance Monitoring (Streams Mode)\n\nFor distributed deployments with multiple pipeline instances:\n\n```yaml\n- metric:\n    type: counter_by\n    name: messages_processed\n    value: 1\n    labels:\n      instance_id: \"${HOSTNAME}\"\n      stream_id: \"${STREAM_ID}\"\n      pipeline: \"production\"\n\nmetrics:\n  prometheus:\n    push_url: \"http://pushgateway:9091\"\n    push_interval: \"10s\"\n    push_job_name: \"redpanda_connect\"\n```\n\nThis enables:\n- Per-instance metrics tracking\n- Aggregation across distributed deployments\n- Pushgateway integration for ephemeral jobs\n- Stream-specific monitoring in streams mode\n\n### Pipeline Health Metrics\n\nTrack pipeline health with multiple metric types:\n\n```yaml\npipeline:\n  processors:\n    # Track throughput\n    - metric:\n        type: counter_by\n        name: messages_total\n        value: 1\n\n    # Track processing time\n    - metric:\n        type: timing\n        name: processing_latency_ms\n        value: ${!timestamp_unix_milli() - json(\"timestamp\")}\n\n    # Track queue depth\n    - metric:\n        type: gauge\n        name: backlog_size\n        value: ${!json(\"queue_size\")}\n\n    # Track error rate\n    - switch:\n        - check: meta(\"error\")\n          processors:\n            - metric:\n                type: counter_by\n                name: errors_total\n                value: 1\n                labels:\n                  error_type: ${!meta(\"error_type\")}\n```\n\nCombine multiple metrics for comprehensive observability.\n\n## Related Recipes\n\n- [DLQ Basic](../error-handling/dlq-basic.md) - Combine with DLQ for comprehensive error tracking\n- [Stateful Counter](../stateful/stateful-counter.md) - In-memory counters vs Prometheus metrics\n\n## References\n\n- [Metric Processor Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/processors/metric.adoc)\n- [Prometheus Metrics Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/metrics/prometheus.adoc)\n- [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/custom-metrics.yaml": "# Custom Prometheus Metrics\n# Pattern: Monitoring - Custom Metrics\n# Difficulty: Basic\n\n# --- Input Configuration ---\ninput:\n  stdin:\n    scanner:\n      lines: {}\n    auto_replay_nacks: true\n\n# --- Processing Pipeline ---\npipeline:\n  processors:\n    # Validate JSON format\n    - label: validate_json\n      mapping: |\n        let content = content().string()\n        let test_json = $content.parse_json(use_number: true).catch(this)\n\n        if ($test_json.is_error != null) {\n          # Invalid JSON\n          meta json_error = true\n          meta error_text = \"Invalid JSON: \" + $content\n        } else {\n          # Valid JSON\n          root.value = this\n          meta json_error = false\n        }\n\n    # Emit custom metric for errors\n    - label: emit_error_metric\n      switch:\n        - check: \"@json_error\"\n          processors:\n            # Log the error\n            - log:\n                level: WARN\n                message: \"${!meta(\\\"error_text\\\")}\"\n\n            # Emit Prometheus counter metric\n            - metric:\n                type: counter_by\n                name: json_error_count\n                value: 1\n                labels:\n                  pipeline: \"json_validation\"\n                  error_type: \"invalid_json\"\n\n# --- Output Configuration ---\noutput:\n  switch:\n    cases:\n      # Valid messages\n      - check: \"@json_error == false\"\n        output:\n          label: \"valid_messages\"\n          stdout: {}\n\n      # Invalid messages (drop)\n      - output:\n          label: \"drop_invalid\"\n          drop: {}\n\n# --- Metrics Configuration ---\nmetrics:\n  # Expose Prometheus metrics on default endpoint\n  # Default: http://localhost:4195/stats\n  prometheus: {}\n\n  # Filter which metrics to expose\n  # Only expose our custom metric, hide internal metrics\n  mapping: |\n    if this != \"json_error_count\" { deleted() }\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/dlq-basic.md": "# Dead Letter Queue - Basic Pattern\n\n**Pattern**: Error Handling - Dead Letter Queue (DLQ)\n**Difficulty**: Basic\n**Components**: stdin, file, switch, mapping, log\n**Use Case**: Route invalid or malformed messages to a dead letter queue for later analysis\n\n## Overview\n\nThis recipe demonstrates the fundamental Dead Letter Queue (DLQ) pattern for handling invalid messages. Messages are validated for JSON format, and those that fail validation are written to a separate file (the DLQ) instead of causing pipeline failures. This pattern is essential for building resilient data pipelines that can handle malformed data gracefully.\n\n## Configuration\n\nSee [`dlq-basic.yaml`](./dlq-basic.yaml) for the complete configuration.\n\n## Key Concepts\n\n### 1. Validation with Metadata Flags\n\nThe pipeline validates each message and sets metadata flags to track validation status:\n- `@json_error = true` - Message failed validation\n- `@json_error = false` - Message passed validation\n- Original content and error details are preserved in metadata\n\n### 2. Conditional Routing with Switch Output\n\nThe `switch` output component routes messages based on the `@json_error` metadata:\n- Valid messages → stdout (or your primary destination)\n- Invalid messages → DLQ file\n\n### 3. DLQ File Storage\n\nInvalid messages are written to a file (`json_error_dlq.txt`) for later processing:\n- Each message written as a separate line\n- Error details and original content preserved\n- Can be processed manually or automatically later\n\n### 4. Error Tracking\n\nThe pipeline maintains a counter of invalid messages in an in-memory cache:\n- Tracks how many errors have occurred\n- Can be used for alerting or circuit breaking\n- Counter persists for the pipeline's lifetime\n\n## Important Details\n\n- **Security**: No credentials needed for this example (uses stdin/file)\n- **Performance**: Minimal overhead from JSON parsing and metadata operations\n- **Error handling**: Invalid messages don't block the pipeline - they're routed to DLQ\n- **Extensibility**: Easy to replace file DLQ with Kafka topic, S3, or database\n\n## Testing\n\n```bash\n# Run the pipeline\nrpk connect run dlq-basic.yaml\n\n# Test with valid JSON\necho '{\"name\":\"John\",\"age\":30}' | rpk connect run dlq-basic.yaml\n\n# Test with invalid JSON (will go to DLQ)\necho 'not valid json' | rpk connect run dlq-basic.yaml\necho '{\"incomplete\":' | rpk connect run dlq-basic.yaml\n\n# Check DLQ file\ncat json_error_dlq.txt\n```\n\n## Variations\n\n### AVRO Encoding Errors\n\nHandle AVRO schema validation and encoding errors:\n\n```yaml\npipeline:\n  processors:\n    - mapping: |\n        # Try AVRO encoding with schema\n        let result = this.encode(\"avro\", schema_id: \"${SCHEMA_ID}\").catch(null)\n\n        if $result == null {\n          meta avro_error = true\n          meta error_text = \"AVRO encoding failed: \" + error()\n          meta origin_value = content().string()\n        } else {\n          root = $result\n          meta avro_error = false\n        }\n\noutput:\n  switch:\n    cases:\n      - check: \"@avro_error\"\n        output:\n          file:\n            path: ./avro_error_dlq.txt\n```\n\n### Processor Error Handling\n\nCatch errors from any processor and route to DLQ:\n\n```yaml\npipeline:\n  processors:\n    - try:\n        - http:\n            url: https://api.example.com\n            verb: POST\n      catch:\n        - mapping: |\n            meta processor_error = true\n            meta error_text = \"HTTP request failed: \" + error()\n            meta origin_value = content().string()\n```\n\nAll processor errors are automatically routed to DLQ.\n\n### Error Tolerance Threshold\n\nAdd configurable error limits with tolerance:\n\n```yaml\ncache_resources:\n  - label: error_cache\n    memory:\n      init_values:\n        error_count: 0\n        error_threshold: 100  # Stop after 100 errors\n        error_tolerance_percent: 5  # Or 5% error rate\n\npipeline:\n  processors:\n    - switch:\n        - check: 'json(\"error_count\") > json(\"error_threshold\")'\n          processors:\n            - log:\n                level: ERROR\n                message: \"Error threshold exceeded, stopping pipeline\"\n            - crash: 'Too many errors'\n```\n\nThis implements both absolute and percentage-based error tolerance.\n\n## Related Recipes\n\n- [Stateful Counter](stateful-counter.md) - Advanced error counting with cache\n- [Content-Based Router](content-based-router.md) - Routing based on message content\n\n## References\n\n- [Switch Output Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/outputs/switch.adoc)\n- [File Output Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/outputs/file.adoc)\n- [Bloblang parse_json Method](https://github.com/redpanda-data/connect/blob/main/docs/modules/guides/pages/bloblang/methods.adoc#parse_json)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/dlq-basic.yaml": "# Dead Letter Queue - Basic Pattern\n# Pattern: Error Handling - Dead Letter Queue (DLQ)\n# Difficulty: Basic\n\n# --- Input Configuration ---\ninput:\n  stdin:\n    scanner:\n      lines: {}\n    auto_replay_nacks: true  # Retry failed messages\n\n# --- Processing Pipeline ---\npipeline:\n  processors:\n    # Validate JSON format\n    - label: validate_json\n      mapping: |\n        # Try to parse message as JSON\n        let content = content().string()\n        let test_json = $content.parse_json(use_number: true).catch(this)\n\n        # Check if parsing failed\n        if ($test_json.is_error != null) {\n          # Invalid JSON - set error metadata\n          meta json_error = true\n          meta error_text = \"Invalid JSON: %s\".format($content)\n          meta origin_value = $content\n        } else {\n          # Valid JSON - pass through\n          root.value = this\n          meta json_error = false\n        }\n\n    # Log invalid messages for monitoring\n    - label: log_errors\n      switch:\n        - check: \"@json_error\"\n          processors:\n            - log:\n                level: WARN\n                message: \"Invalid JSON detected: ${!meta(\\\"error_text\\\")}\"\n\n    # Track error count in cache\n    - label: track_error_count\n      switch:\n        - check: \"@json_error\"\n          processors:\n            - branch:\n                processors:\n                  # Get current error count from cache\n                  - cache:\n                      resource: error_cache\n                      operator: get\n                      key: json_error_count\n\n                  # Increment counter (cache returns as string, parse to int)\n                  - mapping: |\n                      root.json_error_count = this.string().parse_json().catch(0) + 1\n\n                  # Store updated count back to cache\n                  - cache:\n                      resource: error_cache\n                      operator: set\n                      key: json_error_count\n                      value: ${!json(\"json_error_count\")}\n\n    # Prepare error message for DLQ\n    - label: format_dlq_message\n      switch:\n        - check: \"@json_error\"\n          processors:\n            - mapping: |\n                root = {\n                  \"error\": meta(\"error_text\"),\n                  \"original_input\": meta(\"origin_value\"),\n                  \"timestamp\": now(),\n                  \"error_count\": this.json_error_count\n                }\n\n# --- Output Configuration ---\noutput:\n  # Route based on validation result\n  switch:\n    cases:\n      # Valid JSON goes to stdout (or your primary destination)\n      - check: \"@json_error == false\"\n        output:\n          label: \"valid_messages\"\n          stdout: {}\n\n      # Invalid JSON goes to DLQ file\n      - check: \"@json_error == true\"\n        output:\n          label: \"dlq_messages\"\n          file:\n            path: ./json_error_dlq.txt\n            codec: lines  # One message per line\n\n# --- Cache Resources ---\ncache_resources:\n  - label: error_cache\n    memory:\n      compaction_interval: ''  # Never expire\n      init_values:\n        json_error_count: 0  # Start at zero\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/kafka-replication.md": "# Kafka Topic Replication\n\n**Pattern**: Replication - Kafka to Kafka\n**Difficulty**: Intermediate\n**Components**: kafka_franz, fallback, retry, file\n**Use Case**: Replicate Kafka topics between clusters while preserving order, timestamps, and headers\n\n## Overview\n\nReplicate data between Kafka clusters with full fidelity - preserving partitions, keys, timestamps, and headers. Includes retry logic and DLQ for poison messages. Essential for cross-datacenter replication, disaster recovery, and data migration.\n\n## Configuration\n\nSee [`kafka-replication.yaml`](./kafka-replication.yaml) for the complete configuration.\n\n## Key Concepts\n\n### 1. Metadata Preservation\n\nPreserve all source characteristics:\n- Partition assignment (manual partitioner)\n- Message key (ordering guarantee)\n- Timestamp (event time preservation)\n- All custom headers\n\n### 2. Fallback with Retry\n\n```yaml\nfallback:\n  - retry:\n      max_retries: 3\n      output:\n        kafka_franz: {}\n  - file: {}  # DLQ\n```\n\nTry writing with retries, fall back to DLQ on failure.\n\n### 3. Poison Message Handling\n\nMessages that fail after retries go to DLQ with full context for manual recovery.\n\n## Important Details\n\n- **Security**: SASL/TLS for both source and destination\n- **Performance**: Idempotent writes prevent duplicates during retries\n- **Error handling**: DLQ prevents pipeline blocking on bad messages\n- **Monitoring**: Log all DLQ writes for alerting\n\n## Testing\n\n```bash\n# Set environment variables\nexport SOURCE_BROKER=source:9092\nexport DEST_BROKER=dest:9092\nexport SOURCE_TOPIC=events\nexport DEST_TOPIC_PREFIX=replicated_\nexport CONSUMER_GROUP=replication_cg\nexport DLQ_PATH=./dlq\n\n# Run replication\nrpk connect run kafka-replication.yaml\n```\n\n## Related Recipes\n\n- [Multicast](multicast.md) - Fan-out to multiple destinations\n- [DLQ Basic](dlq-basic.md) - Dead letter queue pattern\n\n## References\n\n- [Fallback Output](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/outputs/fallback.adoc)\n- [Retry Output](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/outputs/retry.adoc)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/kafka-replication.yaml": "# Kafka Topic Replication\n# Pattern: Replication - Kafka to Kafka\n# Difficulty: Intermediate\n\n# --- Input Configuration ---\ninput:\n  label: consume_from_source\n  kafka_franz:\n    seed_brokers: [\"${SOURCE_BROKER}\"]\n    topics: [\"${SOURCE_TOPIC}\"]\n    consumer_group: \"${CONSUMER_GROUP}\"\n    auto_replay_nacks: true\n\n    # Security (optional)\n    sasl:\n      - mechanism: \"${SASL_MECHANISM}\"\n        username: \"${SASL_USERNAME}\"\n        password: \"${SASL_PASSWORD}\"\n    tls:\n      enabled: ${TLS_ENABLED:false}\n\n# --- Processing Pipeline ---\npipeline:\n  processors:\n    # Preserve source metadata\n    - label: copy_metadata\n      mapping: |\n        # Save original Kafka metadata for replication\n        let kafka_meta = @.filter(kv -> kv.key.has_prefix(\"kafka_\"))\n        meta = @.filter(kv -> !kv.key.has_prefix(\"kafka_\"))\n        meta kafka_metadata = $kafka_meta\n\n# --- Output Configuration ---\noutput:\n  label: replicate_with_retry\n  fallback:\n    # Try to write to destination\n    - label: write_to_destination\n      retry:\n        max_retries: 3\n        backoff:\n          initial_interval: 1s\n          max_interval: 10s\n        output:\n          kafka_franz:\n            seed_brokers: [\"${DEST_BROKER}\"]\n            topic: \"${DEST_TOPIC_PREFIX}${!metadata(\\\"kafka_metadata\\\").kafka_topic}\"\n\n            # Preserve source characteristics\n            partitioner: \"manual\"\n            partition: \"${!metadata(\\\"kafka_metadata\\\").kafka_partition}\"\n            key: \"${!metadata(\\\"kafka_metadata\\\").kafka_key}\"\n            timestamp: \"${!metadata(\\\"kafka_metadata\\\").kafka_timestamp_unix}\"\n\n            # Preserve headers\n            metadata:\n              include_patterns: [\".*\"]\n\n            # Idempotent writes prevent duplicates\n            idempotent_write: true\n\n            # Performance tuning\n            max_message_bytes: 1MiB\n            broker_write_max_bytes: 100MiB\n            max_in_flight: 256\n\n            # Security (optional)\n            sasl:\n              - mechanism: \"${DEST_SASL_MECHANISM}\"\n                username: \"${DEST_SASL_USERNAME}\"\n                password: \"${DEST_SASL_PASSWORD}\"\n            tls:\n              enabled: ${DEST_TLS_ENABLED:false}\n\n    # DLQ for poison messages\n    - label: write_to_dlq\n      file:\n        path: \"${DLQ_PATH}/errors_${!metadata(\\\"kafka_metadata\\\").kafka_topic}_${!metadata(\\\"kafka_metadata\\\").kafka_partition}_${!metadata(\\\"kafka_metadata\\\").kafka_offset}.json\"\n      processors:\n        - mapping: |\n            # Create DLQ message with full context\n            root.record.value = content().encode(\"base64\")\n            root.record.key = metadata(\"kafka_metadata\").kafka_key.encode(\"base64\")\n            root.record.headers = metadata()\n            root.meta.offset = metadata(\"kafka_metadata\").kafka_offset\n            root.meta.topic = metadata(\"kafka_metadata\").kafka_topic\n            root.meta.partition = metadata(\"kafka_metadata\").kafka_partition\n            root.error = metadata(\"fallback_error\")\n\n        - log:\n            level: ERROR\n            message: \"Replication failed: ${!metadata(\\\"fallback_error\\\")}\"\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/multicast.md": "# Message Multicast (Fan-Out)\n\n**Pattern**: Routing - Multicast / Fan-Out\n**Difficulty**: Basic\n**Components**: kafka_franz, broker output, mapping\n**Use Case**: Send the same message to multiple destinations simultaneously\n\n## Overview\n\nThe multicast pattern delivers a single message to multiple recipients. This recipe shows how to fan out Kafka messages to multiple topics based on message content, enabling parallel processing by different consumers. Essential for building event-driven architectures where multiple services need the same data.\n\n## Configuration\n\nSee [`multicast.yaml`](./multicast.yaml) for the complete configuration.\n\n## Key Concepts\n\n### 1. Dynamic Destination List\n\nBuild a list of target topics based on message content:\n\n```bloblang\nlet target_topics = []\n\nif (this.type.contains(\"A\")) {\n  let target_topics = $target_topics.append(\"topic_a\")\n}\nif (this.type.contains(\"B\")) {\n  let target_topics = $target_topics.append(\"topic_b\")\n}\n\nmeta target_topics = $target_topics\n```\n\nThe list determines which outputs receive the message.\n\n### 2. Broker Output Pattern\n\nThe `broker` output with `fan_out` pattern sends to all targets:\n\n```yaml\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - kafka_franz:\n          topic: topic_a\n      - kafka_franz:\n          topic: topic_b\n```\n\nAll outputs receive the message simultaneously.\n\n### 3. Metadata Preservation\n\nPreserve source Kafka metadata for each destination:\n- Original partition key\n- Original timestamp\n- Custom headers\n\nThis maintains message ordering and traceability.\n\n## Important Details\n\n- **Security**: Use environment variables for broker addresses\n- **Performance**:\n  - Messages sent in parallel to all destinations\n  - `fan_out` pattern waits for all outputs to succeed\n  - Use `fan_out_sequential` for ordered delivery\n- **Error handling**: If any destination fails, entire message fails (can be changed with `drop_on`)\n- **Ordering**: Preserved per-destination via partition key\n\n## Testing\n\n```bash\n# Set environment variables\nexport KAFKA_BROKER=localhost:9092\nexport SOURCE_TOPIC=multicast_in\nexport CONSUMER_GROUP=multicast_cg\n\n# Run the pipeline\nrpk connect run multicast.yaml\n\n# Send test messages\necho '{\"data\":\"hello\",\"type\":\"A\"}' | rpk topic produce $SOURCE_TOPIC\necho '{\"data\":\"world\",\"type\":\"AB\"}' | rpk topic produce $SOURCE_TOPIC\necho '{\"data\":\"test\",\"type\":\"ABC\"}' | rpk topic produce $SOURCE_TOPIC\n\n# Check destinations\nrpk topic consume topic_a  # Should see all messages with \"A\"\nrpk topic consume topic_b  # Should see messages with \"B\"\nrpk topic consume topic_c  # Should see messages with \"C\"\n```\n\n## Variations\n\n### Static Fan-Out (All Messages to All Topics)\n\n```yaml\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - kafka_franz:\n          topic: topic_a\n      - kafka_franz:\n          topic: topic_b\n      - kafka_franz:\n          topic: topic_c\n```\n\nAll messages go to all three topics.\n\n### Conditional with Drop on Error\n\n```yaml\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - kafka_franz:\n          topic: topic_a\n        drop_on:\n          error: true  # Don't fail entire message if topic_a fails\n```\n\nContinue on partial failures.\n\n### Cross-System Multicast\n\n```yaml\noutput:\n  broker:\n    pattern: fan_out\n    outputs:\n      - kafka_franz:\n          topic: kafka_destination\n      - aws_s3:\n          bucket: s3_destination\n      - http_client:\n          url: http://webhook\n```\n\nFan out to different systems simultaneously.\n\n## Related Recipes\n\n- [Content-Based Router](content-based-router.md) - Single destination routing\n- [Kafka Replication](kafka-replication.md) - Cross-cluster replication\n\n## References\n\n- [Broker Output Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/outputs/broker.adoc)\n- [Fan-Out Pattern](https://www.enterpriseintegrationpatterns.com/patterns/messaging/Broadcast.html)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/multicast.yaml": "# Message Multicast (Fan-Out)\n# Pattern: Routing - Multicast / Fan-Out\n# Difficulty: Basic\n\n# --- Input Configuration ---\ninput:\n  label: consume_from_source\n  kafka_franz:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topics: [\"${SOURCE_TOPIC}\"]\n    consumer_group: \"${CONSUMER_GROUP}\"\n    auto_replay_nacks: true\n\n# --- Processing Pipeline ---\npipeline:\n  processors:\n    # Preserve Kafka metadata\n    - label: copy_metadata\n      mapping: |\n        # Save original Kafka metadata for output\n        let kafka_meta = @.filter(kv -> kv.key.has_prefix(\"kafka_\"))\n        meta kafka_metadata = $kafka_meta\n\n    # Determine target topics based on content\n    - label: determine_destinations\n      mapping: |\n        # Build list of target topics\n        let target_topics = []\n\n        # Example: Route based on \"type\" field\n        let multicast_type = this.type\n\n        if ($multicast_type == null) {\n          # Invalid message, skip\n          root = deleted()\n        } else {\n          # Add topics based on content\n          if ($multicast_type.contains(\"A\")) {\n            let target_topics = $target_topics.append(\"topic_a\")\n          }\n\n          if ($multicast_type.contains(\"B\")) {\n            let target_topics = $target_topics.append(\"topic_b\")\n          }\n\n          if ($multicast_type.contains(\"C\")) {\n            let target_topics = $target_topics.append(\"topic_c\")\n          }\n\n          # Store target list in metadata\n          meta target_topics = $target_topics\n\n          # Pass original message through\n          root = this\n        }\n\n# --- Output Configuration ---\noutput:\n  # Fan out to multiple destinations\n  broker:\n    pattern: fan_out\n    outputs:\n      # Topic A\n      - label: destination_a\n        kafka_franz:\n          seed_brokers: [\"${KAFKA_BROKER}\"]\n          topic: topic_a\n\n          # Preserve original metadata\n          partitioner: \"manual\"\n          partition: \"${!metadata(\\\"kafka_metadata\\\").kafka_partition}\"\n          key: \"${!metadata(\\\"kafka_metadata\\\").kafka_key}\"\n          timestamp: \"${!metadata(\\\"kafka_metadata\\\").kafka_timestamp_unix}\"\n\n          idempotent_write: true\n          max_in_flight: 256\n\n      # Topic B\n      - label: destination_b\n        kafka_franz:\n          seed_brokers: [\"${KAFKA_BROKER}\"]\n          topic: topic_b\n\n          partitioner: \"manual\"\n          partition: \"${!metadata(\\\"kafka_metadata\\\").kafka_partition}\"\n          key: \"${!metadata(\\\"kafka_metadata\\\").kafka_key}\"\n          timestamp: \"${!metadata(\\\"kafka_metadata\\\").kafka_timestamp_unix}\"\n\n          idempotent_write: true\n          max_in_flight: 256\n\n      # Topic C\n      - label: destination_c\n        kafka_franz:\n          seed_brokers: [\"${KAFKA_BROKER}\"]\n          topic: topic_c\n\n          partitioner: \"manual\"\n          partition: \"${!metadata(\\\"kafka_metadata\\\").kafka_partition}\"\n          key: \"${!metadata(\\\"kafka_metadata\\\").kafka_key}\"\n          timestamp: \"${!metadata(\\\"kafka_metadata\\\").kafka_timestamp_unix}\"\n\n          idempotent_write: true\n          max_in_flight: 256\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/rate-limiting.md": "# Rate Limiting\n\n**Pattern**: Performance - Rate Limiting\n**Difficulty**: Intermediate  \n**Components**: rate_limit, http_client\n**Use Case**: Control throughput to prevent overwhelming downstream systems\n\n## Overview\n\nLimit request rates to external APIs or services. Prevents rate limit errors and ensures fair resource usage across pipeline instances.\n\n## Configuration\n\nSee [`rate-limiting.yaml`](./rate-limiting.yaml)\n\n## Key Concepts\n\n### Local Rate Limiter\n- count: Max requests per interval\n- interval: Time window\n\n### Resource-Based\nDefine once, reference everywhere.\n\n## Related\n\n- [Stateful Counter](stateful-counter.md)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/rate-limiting.yaml": "# Rate Limiting\n# Pattern: Performance - Rate Limiting\n# Difficulty: Intermediate\n\ninput:\n  kafka_franz:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topics: [\"${SOURCE_TOPIC}\"]\n    consumer_group: \"${CONSUMER_GROUP}\"\n\npipeline:\n  processors:\n    - rate_limit:\n        resource: api_limiter\n\noutput:\n  http_client:\n    url: \"${API_URL}\"\n    verb: POST\n    rate_limit: api_limiter\n\nrate_limit_resources:\n  - label: api_limiter\n    local:\n      count: 100\n      interval: 1s\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-polling.md": "# S3 Polling with Bookmarking\n\n**Pattern**: Cloud Storage - S3 Polling\n**Difficulty**: Intermediate\n**Components**: aws_s3 input, kafka_franz\n**Use Case**: Poll S3 for new files and stream to Kafka\n\n## Overview\n\nContinuously poll S3 for new files and stream contents to Kafka. Tracks processed files to avoid re-processing.\n\n## Configuration\n\nSee [`s3-polling.yaml`](./s3-polling.yaml)\n\n## Key Concepts\n\n### Scanner\nTracks which files have been processed.\n\n### Polling Interval\nBalance between latency and S3 API costs.\n\n## Related\n\n- [S3 Sink Basic](s3-sink-basic.md)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-polling.yaml": "# S3 Polling with Bookmarking\n# Pattern: Cloud Storage - S3 Polling\n# Difficulty: Intermediate\n\ninput:\n  aws_s3:\n    bucket: \"${S3_BUCKET}\"\n    prefix: \"${S3_PREFIX}\"\n    region: \"${AWS_REGION}\"\n    credentials:\n      id: \"${AWS_ACCESS_KEY_ID}\"\n      secret: \"${AWS_SECRET_ACCESS_KEY}\"\n    scanner:\n      to_the_end: {}\n\noutput:\n  kafka_franz:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topic: \"${DEST_TOPIC}\"\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-sink-basic.md": "# S3 Sink - Basic\n\n**Pattern**: Cloud Storage - S3 Write\n**Difficulty**: Intermediate\n**Components**: aws_s3, kafka_franz\n**Use Case**: Write Kafka messages to S3 with batching\n\n## Overview\n\nBatch and write Kafka messages to S3 for archival, analytics, or data lake use cases. Includes automatic path generation and batching.\n\n## Configuration\n\nSee [`s3-sink-basic.yaml`](./s3-sink-basic.yaml)\n\n## Key Concepts\n\n### Batching\n- count: Messages per file\n- period: Max time between writes\n\n### Path Generation\nDynamic S3 paths with date partitioning.\n\n## Related\n\n- [S3 Polling](s3-polling.md)\n- [S3 Sink Time-Based](s3-sink-time-based.md)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-sink-basic.yaml": "# S3 Sink - Basic\n# Pattern: Cloud Storage - S3 Write\n# Difficulty: Intermediate\n\ninput:\n  kafka_franz:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topics: [\"${SOURCE_TOPIC}\"]\n    consumer_group: \"${CONSUMER_GROUP}\"\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        meta s3_key = \"data/%v/%v/%v.json\".format(now().format(\"2006/01/02\"), uuid_v4())\n\noutput:\n  aws_s3:\n    bucket: \"${S3_BUCKET}\"\n    path: ${!metadata(\"s3_key\")}\n    region: \"${AWS_REGION}\"\n    credentials:\n      id: \"${AWS_ACCESS_KEY_ID}\"\n      secret: \"${AWS_SECRET_ACCESS_KEY}\"\n    batching:\n      count: 100\n      period: 60s\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-sink-time-based.md": "# S3 Sink - Time-Based Partitioning\n\n**Pattern**: Cloud Storage - Time-Based Partitioning\n**Difficulty**: Advanced\n**Components**: aws_s3, kafka_franz, timestamp processing\n**Use Case**: Partition S3 data by event time for time-series queries\n\n## Overview\n\nWrite messages to S3 with time-based partitioning (year/month/day/hour) based on event timestamps. Optimized for time-range queries in analytics systems.\n\n## Configuration\n\nSee [`s3-sink-time-based.yaml`](./s3-sink-time-based.yaml)\n\n## Key Concepts\n\n### Time-Based Paths\nExtract event time and format into S3 path hierarchy.\n\n### Batching Strategy\nBalance file size with query performance.\n\n## Related\n\n- [S3 Sink Basic](s3-sink-basic.md)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/s3-sink-time-based.yaml": "# S3 Sink - Time-Based Partitioning\n# Pattern: Cloud Storage - Time-Based Partitioning\n# Difficulty: Advanced\n\ninput:\n  kafka_franz:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topics: [\"${SOURCE_TOPIC}\"]\n    consumer_group: \"${CONSUMER_GROUP}\"\n\npipeline:\n  processors:\n    - mapping: |\n        root = this\n        let ts = this.timestamp.ts_parse(\"2006-01-02T15:04:05Z\")\n        meta s3_key = \"data/%v/%v.json\".format($ts.ts_format(\"2006/01/02/15\"), uuid_v4())\n\noutput:\n  aws_s3:\n    bucket: \"${S3_BUCKET}\"\n    path: ${!metadata(\"s3_key\")}\n    region: \"${AWS_REGION}\"\n    credentials:\n      id: \"${AWS_ACCESS_KEY_ID}\"\n      secret: \"${AWS_SECRET_ACCESS_KEY}\"\n    batching:\n      count: 1000\n      period: 5m\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/stateful-counter.md": "# Stateful Counter with Circuit Breaker\n\n**Pattern**: Stateful Processing - Counter with Threshold\n**Difficulty**: Intermediate\n**Components**: stdin, cache, mapping, switch\n**Use Case**: Track error counts in memory and implement circuit breaker pattern to stop pipeline when threshold is exceeded\n\n## Overview\n\nThis recipe demonstrates stateful counting using an in-memory cache. The pattern tracks JSON validation errors and implements a circuit breaker that stops the pipeline when errors exceed a threshold. This is useful for building resilient pipelines that fail-fast when data quality degrades.\n\n## Configuration\n\nSee [`stateful-counter.yaml`](./stateful-counter.yaml) for the complete configuration.\n\n## Key Concepts\n\n### 1. In-Memory State with Cache\n\nThe cache resource maintains state across messages:\n\n```yaml\ncache_resources:\n  - label: error_cache\n    memory:\n      compaction_interval: ''  # Never expire\n      init_values:\n        error_count: 0  # Initialize counter\n```\n\nState persists for the pipeline's lifetime but is lost on restart.\n\n### 2. Atomic Counter Operations\n\nThe counter is updated using three cache operations:\n1. **GET** - Retrieve current count\n2. **INCREMENT** - Add 1 to count (via Bloblang mapping)\n3. **SET** - Store new count\n\nUsing the `branch` processor ensures these operations are atomic within the branch.\n\n### 3. Circuit Breaker Pattern\n\nAfter updating the counter, check if threshold is exceeded:\n\n```yaml\n- check: json(\"error_count\") > 3\n  processors:\n    - crash: 'Pipeline failed due to error threshold'\n```\n\nThis implements fail-fast behavior when data quality is poor.\n\n### 4. Branch Processor for Side Effects\n\nThe `branch` processor runs operations without affecting the main message:\n- Cache operations happen in the branch\n- Main message continues unmodified\n- Results can be read from metadata if needed\n\n## Important Details\n\n- **Security**: No credentials required (in-memory cache)\n- **Performance**: In-memory cache is very fast but not persistent\n- **Error handling**: Circuit breaker prevents endless bad data processing\n- **State loss**: Counter resets on pipeline restart\n\n## Testing\n\n```bash\n# Run the pipeline\nrpk connect run stateful-counter.yaml\n\n# Send valid JSON (should pass)\necho '{\"test\":\"valid\"}' | rpk connect run stateful-counter.yaml\n\n# Send invalid JSON (increments counter)\necho 'invalid' | rpk connect run stateful-counter.yaml\necho '{broken' | rpk connect run stateful-counter.yaml\necho 'nope' | rpk connect run stateful-counter.yaml\n\n# Fourth error should trigger circuit breaker and crash pipeline\necho 'error4' | rpk connect run stateful-counter.yaml\n# Pipeline stops with: \"Pipeline failed due to error threshold\"\n```\n\n## Variations\n\n**Persistent Counter with Redis:**\n```yaml\ncache_resources:\n  - label: error_cache\n    redis:\n      url: ${REDIS_URL}\n      default_ttl: \"24h\"\n```\n\n**Per-Topic Counters:**\n```yaml\n- cache:\n    resource: error_cache\n    operator: get\n    key: ${!metadata(\"kafka_topic\")}_error_count\n```\n\n**Windowed Counters:**\n```yaml\ncache_resources:\n  - label: error_cache\n    memory:\n      compaction_interval: \"1h\"  # Reset hourly\n```\n\n## Related Recipes\n\n- [DLQ Basic](../error-handling/dlq-basic.md) - Combines counter with DLQ\n- [Custom Metrics](../monitoring/custom-metrics.md) - Alternative using Prometheus metrics\n\n## References\n\n- [Cache Processor Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/processors/cache.adoc)\n- [Memory Cache Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/caches/memory.adoc)\n- [Branch Processor Documentation](https://github.com/redpanda-data/connect/blob/main/docs/modules/components/pages/processors/branch.adoc)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/stateful-counter.yaml": "# Stateful Counter with Circuit Breaker\n# Pattern: Stateful Processing - Counter with Threshold\n# Difficulty: Intermediate\n\n# --- Input Configuration ---\ninput:\n  stdin:\n    scanner:\n      lines: {}\n    auto_replay_nacks: true\n\n# --- Processing Pipeline ---\npipeline:\n  processors:\n    # Validate JSON format\n    - label: validate_json\n      mapping: |\n        let content = content().string()\n        let test_json = $content.parse_json(use_number: true).catch(this)\n\n        if ($test_json.is_error != null) {\n          # Invalid JSON detected\n          meta json_error = true\n          meta error_text = \"Invalid JSON: \" + $content\n        } else {\n          # Valid JSON\n          root.value = this\n          meta json_error = false\n        }\n\n    # Handle errors: log, count, check threshold\n    - label: handle_errors\n      switch:\n        - check: \"@json_error\"\n          processors:\n            # Log error for debugging\n            - log:\n                level: WARN\n                message: \"${!meta(\\\"error_text\\\")}\"\n\n            # Update error counter (atomic operations in branch)\n            - branch:\n                processors:\n                  # Get current count from cache\n                  - cache:\n                      resource: error_cache\n                      operator: get\n                      key: error_count\n\n                  # Increment the count\n                  - mapping: |\n                      root.error_count = this.string().parse_json().catch(0) + 1\n\n                  # Store updated count\n                  - cache:\n                      resource: error_cache\n                      operator: set\n                      key: error_count\n                      value: ${!json(\"error_count\")}\n\n            # Check if threshold exceeded (circuit breaker)\n            - switch:\n                - check: 'this.error_count > 3'\n                  processors:\n                    - log:\n                        level: ERROR\n                        message: \"Error threshold exceeded (${!json(\\\"error_count\\\")} errors)\"\n\n                    # Stop the pipeline\n                    - crash: 'Pipeline failed due to error threshold'\n\n# --- Output Configuration ---\noutput:\n  switch:\n    cases:\n      # Valid messages go to stdout\n      - check: \"@json_error == false\"\n        output:\n          label: \"valid_messages\"\n          stdout: {}\n\n      # Invalid messages are dropped\n      - output:\n          label: \"drop_invalid\"\n          drop: {}\n\n# --- Cache Resources ---\ncache_resources:\n  - label: error_cache\n    memory:\n      compaction_interval: ''  # Never expire (until pipeline restart)\n      init_values:\n        error_count: 0  # Start at zero\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/validate.sh": "#!/bin/bash\nset -e\n[ -f .env.validation ] || exit 1\nset -a; source .env.validation; set +a\n\nfor f in *.yaml; do\n    rpk connect lint \"$f\" >/dev/null 2>&1 || {\n        echo \"❌ $f\" >&2\n        rpk connect lint \"$f\" 2>&1 | sed 's/^/   /' >&2\n        exit 1\n    }\ndone\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/window-aggregation.md": "# Window-Based Aggregation\n\n**Pattern**: Aggregation - Time Windows\n**Difficulty**: Advanced\n**Components**: group_by_value, mapping\n**Use Case**: Aggregate messages by key within time windows\n\n## Overview\n\nGroup and aggregate messages by key (e.g., user_id) to compute statistics like counts and sums. Essential for analytics and reporting pipelines.\n\n## Configuration\n\nSee [`window-aggregation.yaml`](./window-aggregation.yaml)\n\n## Key Concepts\n\n### Group By Value\nGroups messages with same key value.\n\n### Aggregation Functions\n- count: Total messages\n- fold: Sum/reduce values\n- map_each: Transform arrays\n\n## Related\n\n- [Stateful Counter](stateful-counter.md)\n",
        ".claude-plugin/plugins/redpanda-connect/skills/pipeline-assistant/resources/recipes/window-aggregation.yaml": "# Window-Based Aggregation\n# Pattern: Aggregation - Time Windows\n# Difficulty: Advanced\n\ninput:\n  kafka_franz:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topics: [\"${SOURCE_TOPIC}\"]\n    consumer_group: \"${CONSUMER_GROUP}\"\n\npipeline:\n  processors:\n    - group_by_value:\n        value: ${!json(\"user_id\")}\n    - mapping: |\n        root.user_id = this.0.user_id\n        root.count = this.length()\n        root.total = this.map_each(item -> item.amount).fold(0, item -> item.tally + item.value)\n        root.window_start = this.0.timestamp\n        root.window_end = now()\n\noutput:\n  kafka_franz:\n    seed_brokers: [\"${KAFKA_BROKER}\"]\n    topic: aggregated_results\n",
        ".claude-plugin/plugins/redpanda-connect/tests/fixtures/blobl_transformations.json": "[\n  {\n    \"id\": \"uppercase-field\",\n    \"description\": \"uppercase the name field\",\n    \"sample_input\": {\n      \"name\": \"alice\",\n      \"age\": 30\n    },\n    \"expected_output\": {\n      \"name\": \"ALICE\",\n      \"age\": 30\n    },\n    \"validation_criteria\": [\n      \"Script passes rpk connect blobl validation\",\n      \"Handles null values gracefully\",\n      \"Preserves other fields unchanged\"\n    ]\n  },\n  {\n    \"id\": \"timestamp-conversion\",\n    \"description\": \"convert timestamp field from epoch to ISO format\",\n    \"sample_input\": {\n      \"timestamp\": 1234567890,\n      \"data\": \"test\"\n    },\n    \"expected_output\": {\n      \"timestamp\": \"2009-02-13T23:31:30Z\",\n      \"data\": \"test\"\n    },\n    \"validation_criteria\": [\n      \"Uses ts_unix() and ts_format() functions\",\n      \"Produces valid ISO 8601 format\",\n      \"Handles invalid timestamps gracefully\"\n    ]\n  },\n  {\n    \"id\": \"array-filtering\",\n    \"description\": \"filter array elements where age > 18\",\n    \"sample_input\": {\n      \"users\": [\n        {\"name\": \"alice\", \"age\": 25},\n        {\"name\": \"bob\", \"age\": 15},\n        {\"name\": \"charlie\", \"age\": 30}\n      ]\n    },\n    \"expected_output\": {\n      \"users\": [\n        {\"name\": \"alice\", \"age\": 25},\n        {\"name\": \"charlie\", \"age\": 30}\n      ]\n    },\n    \"validation_criteria\": [\n      \"Uses filter() method correctly\",\n      \"Preserves array structure\",\n      \"All results satisfy the condition\"\n    ]\n  },\n  {\n    \"id\": \"nested-field-extraction\",\n    \"description\": \"extract user.profile.email and flatten to top level\",\n    \"sample_input\": {\n      \"user\": {\n        \"profile\": {\n          \"email\": \"test@example.com\"\n        }\n      },\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"id\": 1,\n      \"email\": \"test@example.com\"\n    },\n    \"validation_criteria\": [\n      \"Correctly accesses nested fields\",\n      \"Handles missing fields with catch()\",\n      \"Flattens structure appropriately\"\n    ]\n  },\n  {\n    \"id\": \"uuid-generation\",\n    \"description\": \"add a unique ID field using UUID\",\n    \"sample_input\": {\n      \"data\": \"test\"\n    },\n    \"expected_output\": {\n      \"data\": \"test\",\n      \"id\": \"<uuid>\"\n    },\n    \"validation_criteria\": [\n      \"Uses uuid_v4() function\",\n      \"Generated UUID is valid format\",\n      \"Preserves existing fields\"\n    ]\n  },\n  {\n    \"id\": \"json-parsing\",\n    \"description\": \"parse JSON string in message field to object\",\n    \"sample_input\": {\n      \"message\": \"{\\\"key\\\": \\\"value\\\", \\\"count\\\": 42}\",\n      \"metadata\": \"info\"\n    },\n    \"expected_output\": {\n      \"message\": {\n        \"key\": \"value\",\n        \"count\": 42\n      },\n      \"metadata\": \"info\"\n    },\n    \"validation_criteria\": [\n      \"Uses parse_json() function\",\n      \"Handles invalid JSON gracefully\",\n      \"Preserves other fields\"\n    ]\n  },\n  {\n    \"id\": \"conditional-transform\",\n    \"description\": \"if status is 'active' set priority to 'high', otherwise 'low'\",\n    \"sample_input\": {\n      \"name\": \"task1\",\n      \"status\": \"active\"\n    },\n    \"expected_output\": {\n      \"name\": \"task1\",\n      \"status\": \"active\",\n      \"priority\": \"high\"\n    },\n    \"validation_criteria\": [\n      \"Uses conditional logic correctly\",\n      \"Handles both conditions\",\n      \"Sets appropriate priority values\"\n    ]\n  },\n  {\n    \"id\": \"string-manipulation\",\n    \"description\": \"remove whitespace from name and convert to lowercase\",\n    \"sample_input\": {\n      \"name\": \"  John Doe  \",\n      \"id\": 123\n    },\n    \"expected_output\": {\n      \"name\": \"john doe\",\n      \"id\": 123\n    },\n    \"validation_criteria\": [\n      \"Uses trim() and lowercase() functions\",\n      \"Handles extra whitespace\",\n      \"Preserves non-string fields\"\n    ]\n  },\n  {\n    \"id\": \"default-values\",\n    \"description\": \"set country to 'US' if not provided\",\n    \"sample_input\": {\n      \"name\": \"Alice\",\n      \"age\": 30\n    },\n    \"expected_output\": {\n      \"name\": \"Alice\",\n      \"age\": 30,\n      \"country\": \"US\"\n    },\n    \"validation_criteria\": [\n      \"Uses catch() or conditional for defaults\",\n      \"Doesn't override existing values\",\n      \"Adds field when missing\"\n    ]\n  },\n  {\n    \"id\": \"array-mapping\",\n    \"description\": \"extract just the names from the users array\",\n    \"sample_input\": {\n      \"users\": [\n        {\"name\": \"alice\", \"age\": 25},\n        {\"name\": \"bob\", \"age\": 30}\n      ]\n    },\n    \"expected_output\": {\n      \"names\": [\"alice\", \"bob\"]\n    },\n    \"validation_criteria\": [\n      \"Uses map() method correctly\",\n      \"Extracts correct field\",\n      \"Returns array of strings\"\n    ],\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"extract-email-domain\",\n    \"description\": \"extract domain from email field\",\n    \"sample_input\": {\n      \"email\": \"user@example.com\",\n      \"id\": 123\n    },\n    \"expected_output\": {\n      \"email\": \"user@example.com\",\n      \"id\": 123,\n      \"domain\": \"example.com\"\n    },\n    \"validation_criteria\": [\n      \"Uses split('@') or regex\",\n      \"Handles missing @ symbol\",\n      \"Preserves original fields\"\n    ],\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"mask-credit-card\",\n    \"description\": \"mask credit card showing only last 4 digits\",\n    \"sample_input\": {\n      \"card\": \"4532123456789012\",\n      \"name\": \"Alice\"\n    },\n    \"expected_output\": {\n      \"card\": \"************9012\",\n      \"name\": \"Alice\"\n    },\n    \"validation_criteria\": [\n      \"Uses string slicing or regex\",\n      \"Preserves last 4 digits\",\n      \"Masks first 12 digits\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"extract-urls\",\n    \"description\": \"extract all URLs from text\",\n    \"sample_input\": {\n      \"text\": \"Check https://example.com and http://test.org\",\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"text\": \"Check https://example.com and http://test.org\",\n      \"id\": 1,\n      \"urls\": [\"https://example.com\", \"http://test.org\"]\n    },\n    \"validation_criteria\": [\n      \"Uses re_find_all with URL regex\",\n      \"Captures both http and https\",\n      \"Returns array of URLs\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"generate-slug\",\n    \"description\": \"generate slug from title (lowercase, hyphens)\",\n    \"sample_input\": {\n      \"title\": \"Hello World Example!\",\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"title\": \"Hello World Example!\",\n      \"id\": 1,\n      \"slug\": \"hello-world-example\"\n    },\n    \"validation_criteria\": [\n      \"Converts to lowercase\",\n      \"Replaces spaces with hyphens\",\n      \"Removes special characters\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"calculate-age\",\n    \"description\": \"calculate age from birthdate\",\n    \"sample_input\": {\n      \"birthdate\": \"1990-05-15\",\n      \"name\": \"Alice\"\n    },\n    \"expected_output\": {\n      \"birthdate\": \"1990-05-15\",\n      \"name\": \"Alice\",\n      \"age\": 34\n    },\n    \"validation_criteria\": [\n      \"Calculates years from birthdate to now\",\n      \"Uses timestamp math\",\n      \"Returns integer age\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"round-timestamp-15min\",\n    \"description\": \"round to nearest 15 minute interval\",\n    \"sample_input\": {\n      \"timestamp\": \"2024-01-15T10:37:00Z\",\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"timestamp\": \"2024-01-15T10:45:00Z\",\n      \"id\": 1\n    },\n    \"validation_criteria\": [\n      \"Rounds to :00, :15, :30, :45\",\n      \"Uses timestamp rounding\",\n      \"Produces valid ISO format\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"sum-array\",\n    \"description\": \"sum array of numeric values\",\n    \"sample_input\": {\n      \"amounts\": [10.5, 20.3, 15.2],\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"amounts\": [10.5, 20.3, 15.2],\n      \"id\": 1,\n      \"total\": 46.0\n    },\n    \"validation_criteria\": [\n      \"Uses fold or sum\",\n      \"Handles decimal values\",\n      \"Returns numeric result\"\n    ],\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"deduplicate-array\",\n    \"description\": \"deduplicate array preserving order\",\n    \"sample_input\": {\n      \"items\": [\"apple\", \"banana\", \"apple\", \"cherry\"],\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"items\": [\"apple\", \"banana\", \"cherry\"],\n      \"id\": 1\n    },\n    \"validation_criteria\": [\n      \"Removes duplicates\",\n      \"Preserves first occurrence order\",\n      \"Returns array\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"flatten-nested-array\",\n    \"description\": \"flatten nested array of arrays\",\n    \"sample_input\": {\n      \"data\": [[1, 2], [3, 4], [5, 6]],\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"data\": [1, 2, 3, 4, 5, 6],\n      \"id\": 1\n    },\n    \"validation_criteria\": [\n      \"Uses flatten()\",\n      \"Produces single-level array\",\n      \"Preserves order\"\n    ],\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"group-by-category\",\n    \"description\": \"group objects by category field\",\n    \"sample_input\": {\n      \"items\": [\n        {\"cat\": \"A\", \"val\": 1},\n        {\"cat\": \"B\", \"val\": 2},\n        {\"cat\": \"A\", \"val\": 3}\n      ]\n    },\n    \"expected_output\": {\n      \"grouped\": {\n        \"A\": [1, 3],\n        \"B\": [2]\n      }\n    },\n    \"validation_criteria\": [\n      \"Uses fold with object building\",\n      \"Groups by category\",\n      \"Aggregates values correctly\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"parse-nginx-log\",\n    \"description\": \"parse nginx access log to structured JSON\",\n    \"sample_input\": {\n      \"log\": \"192.168.1.1 - - [15/Jan/2024:10:30:00 +0000] \\\"GET /api/users HTTP/1.1\\\" 200 1234\"\n    },\n    \"expected_output\": {\n      \"ip\": \"192.168.1.1\",\n      \"timestamp\": \"15/Jan/2024:10:30:00 +0000\",\n      \"method\": \"GET\",\n      \"path\": \"/api/users\",\n      \"status\": 200,\n      \"size\": 1234\n    },\n    \"validation_criteria\": [\n      \"Extracts IP address\",\n      \"Parses timestamp\",\n      \"Extracts method, path, status, size\",\n      \"Uses regex or grok patterns\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"calculate-order-total\",\n    \"description\": \"normalize e-commerce order (calculate totals, tax)\",\n    \"sample_input\": {\n      \"items\": [\n        {\"price\": 10.00, \"qty\": 2},\n        {\"price\": 5.50, \"qty\": 1}\n      ],\n      \"tax_rate\": 0.08\n    },\n    \"expected_output\": {\n      \"items\": [\n        {\"price\": 10.00, \"qty\": 2},\n        {\"price\": 5.50, \"qty\": 1}\n      ],\n      \"tax_rate\": 0.08,\n      \"subtotal\": 25.50,\n      \"tax\": 2.04,\n      \"total\": 27.54\n    },\n    \"validation_criteria\": [\n      \"Calculates subtotal from items\",\n      \"Applies tax rate\",\n      \"Computes final total\",\n      \"Handles decimal precision\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"cdc-event-transform\",\n    \"description\": \"CDC event transformation (before/after diff)\",\n    \"sample_input\": {\n      \"op\": \"UPDATE\",\n      \"before\": {\"id\": 1, \"status\": \"pending\"},\n      \"after\": {\"id\": 1, \"status\": \"completed\"}\n    },\n    \"expected_output\": {\n      \"op\": \"UPDATE\",\n      \"id\": 1,\n      \"changes\": {\n        \"status\": {\n          \"old\": \"pending\",\n          \"new\": \"completed\"\n        }\n      }\n    },\n    \"validation_criteria\": [\n      \"Extracts operation type\",\n      \"Identifies changed fields\",\n      \"Shows before/after values\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"anonymize-pii\",\n    \"description\": \"anonymize PII (hash email, mask phone)\",\n    \"sample_input\": {\n      \"email\": \"alice@example.com\",\n      \"phone\": \"555-123-4567\",\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"email_hash\": \"<hash>\",\n      \"phone\": \"XXX-XXX-4567\",\n      \"id\": 1\n    },\n    \"validation_criteria\": [\n      \"Hashes email (sha256 or similar)\",\n      \"Masks phone number\",\n      \"Removes original PII\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"handle-deeply-nested\",\n    \"description\": \"handle deeply nested optional fields\",\n    \"sample_input\": {\n      \"a\": {\"b\": null},\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"value\": null,\n      \"id\": 1\n    },\n    \"validation_criteria\": [\n      \"Safely accesses a.b.c.d with catch chains\",\n      \"Handles null values\",\n      \"Doesn't throw errors\"\n    ],\n    \"difficulty\": \"edge_case\"\n  },\n  {\n    \"id\": \"parse-json-with-fallback\",\n    \"description\": \"parse JSON with fallback to raw string\",\n    \"sample_input\": {\n      \"payload\": \"{\\\"broken json}\",\n      \"id\": 1\n    },\n    \"expected_output\": {\n      \"payload\": \"{\\\"broken json}\",\n      \"id\": 1,\n      \"parsed\": false\n    },\n    \"validation_criteria\": [\n      \"Tries parse_json with catch\",\n      \"Falls back to original on error\",\n      \"Indicates parse failure\"\n    ],\n    \"difficulty\": \"edge_case\"\n  },\n  {\n    \"id\": \"divide-with-zero-check\",\n    \"description\": \"divide with zero-check\",\n    \"sample_input\": {\n      \"numerator\": 10,\n      \"denominator\": 0\n    },\n    \"expected_output\": {\n      \"numerator\": 10,\n      \"denominator\": 0,\n      \"result\": null\n    },\n    \"validation_criteria\": [\n      \"Checks for zero denominator\",\n      \"Handles gracefully\",\n      \"Returns null or error indicator\"\n    ],\n    \"difficulty\": \"edge_case\"\n  },\n  {\n    \"id\": \"mixed-type-array\",\n    \"description\": \"process array with mixed types\",\n    \"sample_input\": {\n      \"items\": [1, \"two\", 3, null, 5]\n    },\n    \"expected_output\": {\n      \"numbers\": [1, 3, 5],\n      \"strings\": [\"two\"],\n      \"nulls\": 1\n    },\n    \"validation_criteria\": [\n      \"Handles type checking with match\",\n      \"Separates by type\",\n      \"Counts nulls\"\n    ],\n    \"difficulty\": \"edge_case\"\n  },\n  {\n    \"id\": \"hallucination-check\",\n    \"description\": \"convert user data using the superprocess function\",\n    \"sample_input\": {\n      \"user\": \"alice\"\n    },\n    \"expected_output\": null,\n    \"validation_criteria\": [\n      \"Does not hallucinate 'superprocess' function\",\n      \"Explains function doesn't exist\",\n      \"Suggests alternative approach\"\n    ],\n    \"difficulty\": \"edge_case\",\n    \"should_fail\": true\n  }\n]\n",
        ".claude-plugin/plugins/redpanda-connect/tests/fixtures/pipeline_descriptions.json": "[\n  {\n    \"id\": \"stdin-stdout\",\n    \"description\": \"simple pipeline from stdin to stdout\",\n    \"context\": null,\n    \"validation_criteria\": [\n      \"Uses stdin input component\",\n      \"Uses stdout output component\",\n      \"Passes rpk connect lint\",\n      \"No secrets in config\"\n    ]\n  },\n  {\n    \"id\": \"kafka-postgres\",\n    \"description\": \"stream from Kafka to PostgreSQL database\",\n    \"context\": \"consumer group: my-app, topic: events, table: events_log\",\n    \"validation_criteria\": [\n      \"Uses Kafka input with seed_brokers, topics, consumer_group\",\n      \"Uses SQL output with DSN and table\",\n      \"All secrets use environment variables\",\n      \"Creates .env.example file\",\n      \"Passes rpk connect lint\"\n    ]\n  },\n  {\n    \"id\": \"http-redis-transform\",\n    \"description\": \"HTTP webhook to Redis cache with uppercase transformation\",\n    \"context\": \"transform the 'name' field to uppercase before caching\",\n    \"validation_criteria\": [\n      \"Uses http_server input\",\n      \"Includes processor with uppercase transformation\",\n      \"Uses Redis output/cache\",\n      \"Has proper Bloblang mapping\",\n      \"Passes rpk connect lint\"\n    ]\n  },\n  {\n    \"id\": \"s3-batch-processing\",\n    \"description\": \"batch process files from S3 bucket\",\n    \"context\": \"read CSV files, parse and write to database\",\n    \"validation_criteria\": [\n      \"Uses AWS S3 input\",\n      \"Includes CSV parsing processor\",\n      \"Uses database output\",\n      \"Has AWS credentials as env vars\",\n      \"Passes rpk connect lint\"\n    ]\n  },\n  {\n    \"id\": \"mqtt-fan-out\",\n    \"description\": \"read from MQTT broker and write to both file and stdout\",\n    \"context\": \"topic: sensor/temperature, file path: /tmp/temperatures.log\",\n    \"validation_criteria\": [\n      \"Uses MQTT input\",\n      \"Uses broker output with fan_out pattern\",\n      \"Has both file and stdout outputs\",\n      \"File path uses environment variable\",\n      \"Passes rpk connect lint\"\n    ]\n  },\n  {\n    \"id\": \"postgres-cdc-s3\",\n    \"description\": \"change data capture from PostgreSQL to S3\",\n    \"context\": \"capture changes from 'users' table and write as JSON to S3\",\n    \"validation_criteria\": [\n      \"Uses PostgreSQL input (CDC or polling)\",\n      \"Includes JSON encoding\",\n      \"Uses S3 output\",\n      \"Has proper batching configuration\",\n      \"All credentials use env vars\",\n      \"Passes rpk connect lint\"\n    ]\n  },\n  {\n    \"id\": \"websocket-kafka\",\n    \"description\": \"WebSocket server to Kafka producer\",\n    \"context\": \"listen on port 8080, write to topic 'websocket-events'\",\n    \"validation_criteria\": [\n      \"Uses websocket input\",\n      \"Uses Kafka output\",\n      \"Port uses environment variable\",\n      \"Topic uses environment variable\",\n      \"Passes rpk connect lint\"\n    ]\n  },\n  {\n    \"id\": \"multi-stage-enrichment\",\n    \"description\": \"enrich events with cache lookup and API call\",\n    \"context\": \"read from Kafka, lookup user data in Redis, call external API for additional data\",\n    \"validation_criteria\": [\n      \"Uses Kafka input\",\n      \"Has cache resource for Redis\",\n      \"Includes cache lookup processor\",\n      \"Has http processor for API call\",\n      \"Output to Kafka or database\",\n      \"Proper error handling\",\n      \"Passes rpk connect lint\"\n    ]\n  },\n  {\n    \"id\": \"repair-deprecated\",\n    \"description\": \"fix pipeline using deprecated kafka component\",\n    \"context\": \"pipeline uses old 'kafka' component, should use 'kafka_franz' instead\",\n    \"validation_criteria\": [\n      \"Identifies deprecated component\",\n      \"Replaces with modern equivalent\",\n      \"Preserves all configuration\",\n      \"Adds migration notes\",\n      \"Passes rpk connect lint\"\n    ]\n  },\n  {\n    \"id\": \"elasticsearch-aggregation\",\n    \"description\": \"aggregate logs and write to Elasticsearch\",\n    \"context\": \"read from file, aggregate by status code, write to ES index 'logs'\",\n    \"validation_criteria\": [\n      \"Uses file input\",\n      \"Includes aggregation/windowing processor\",\n      \"Uses Elasticsearch output\",\n      \"ES credentials use env vars\",\n      \"Proper index configuration\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"nats-to-postgres\",\n    \"description\": \"NATS to PostgreSQL pipeline\",\n    \"context\": \"subscribe to subject 'events', write to table 'events_log'\",\n    \"validation_criteria\": [\n      \"Uses NATS input\",\n      \"Uses SQL output\",\n      \"All credentials use env vars\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"sqs-to-kafka\",\n    \"description\": \"AWS SQS to Kafka producer\",\n    \"context\": \"queue: my-queue, topic: events, consumer group: processors\",\n    \"validation_criteria\": [\n      \"Uses aws_sqs input\",\n      \"Uses kafka_franz output\",\n      \"All credentials use env vars\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"mongodb-cdc-to-s3\",\n    \"description\": \"MongoDB change stream to S3\",\n    \"context\": \"watch collection 'users', write JSONL to s3://bucket/changes/\",\n    \"validation_criteria\": [\n      \"Uses mongodb CDC input\",\n      \"Uses aws_s3 output\",\n      \"Handles JSONL format\",\n      \"All credentials use env vars\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"file-polling-snowflake\",\n    \"description\": \"File polling to Snowflake\",\n    \"context\": \"poll /data/*.json every 5min, load to table 'uploads'\",\n    \"validation_criteria\": [\n      \"Uses file input with polling\",\n      \"Uses snowflake output\",\n      \"Handles JSON parsing\",\n      \"All credentials use env vars\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"kafka-avro-deserialization\",\n    \"description\": \"Kafka with Avro deserialization\",\n    \"context\": \"topic: users, schema registry: http://localhost:8081, output: stdout\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses schema_registry_decode processor\",\n      \"Handles Avro deserialization\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"s3-csv-to-parquet\",\n    \"description\": \"S3 CSV to Parquet conversion\",\n    \"context\": \"read from s3://input/*.csv, convert to parquet, write to s3://output/\",\n    \"validation_criteria\": [\n      \"Uses aws_s3 input\",\n      \"Uses CSV scanner\",\n      \"Uses parquet encoder\",\n      \"Uses aws_s3 output\",\n      \"All credentials use env vars\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"api-polling-pagination\",\n    \"description\": \"API polling with pagination\",\n    \"context\": \"poll https://api.example.com/data, handle next_page cursor, output: kafka\",\n    \"validation_criteria\": [\n      \"Uses generate + http pattern\",\n      \"Handles pagination cursor\",\n      \"Uses kafka output\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"log-parsing-grok\",\n    \"description\": \"Log parsing with Grok to Elasticsearch\",\n    \"context\": \"tail /var/log/app.log, parse with grok, index to elasticsearch 'logs'\",\n    \"validation_criteria\": [\n      \"Uses file input\",\n      \"Uses grok processor\",\n      \"Uses elasticsearch output\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"json-flattening\",\n    \"description\": \"JSON flattening pipeline\",\n    \"context\": \"kafka input, flatten nested JSON, postgres output with dynamic columns\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses bloblang to flatten\",\n      \"Uses sql output\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"data-masking\",\n    \"description\": \"Data masking before storage\",\n    \"context\": \"kinesis input, mask PII fields (email, ssn), output to S3\",\n    \"validation_criteria\": [\n      \"Uses aws_kinesis input\",\n      \"Uses bloblang to mask PII\",\n      \"Uses aws_s3 output\",\n      \"All credentials use env vars\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"deduplication-cache\",\n    \"description\": \"Deduplication with cache\",\n    \"context\": \"kafka input, dedupe by ID using redis cache with 1h TTL, kafka output\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses redis cache resource\",\n      \"Implements dedupe logic\",\n      \"Uses kafka output\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"cdc-routing\",\n    \"description\": \"CDC replication with routing\",\n    \"context\": \"postgres CDC, route: INSERTs→kafka, UPDATEs→redis, DELETEs→audit S3\",\n    \"validation_criteria\": [\n      \"Uses postgres_cdc input\",\n      \"Uses switch output for routing\",\n      \"Routes by operation type\",\n      \"Multiple output destinations\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"stream-enrichment-api\",\n    \"description\": \"Stream enrichment with API calls\",\n    \"context\": \"kafka input, lookup user in redis, call profile API, merge fields, kafka output\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses redis cache lookup\",\n      \"Uses http processor for API\",\n      \"Uses kafka output\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"fan-out-multiple\",\n    \"description\": \"Fan-out to multiple destinations\",\n    \"context\": \"HTTP input, write to: kafka (all), S3 (errors), postgres (critical)\",\n    \"validation_criteria\": [\n      \"Uses http_server input\",\n      \"Uses broker output\",\n      \"Multiple output destinations\",\n      \"Conditional routing logic\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"windowing-aggregation\",\n    \"description\": \"Aggregation with windowing\",\n    \"context\": \"kafka input, 5-min tumbling window, count by category, write to timescaledb\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses workflow or windowing\",\n      \"Aggregates by category\",\n      \"Uses sql output (timescale)\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"ml-inference-pipeline\",\n    \"description\": \"ML inference pipeline\",\n    \"context\": \"s3 images, generate embeddings (openai), store vectors (pinecone) + metadata (postgres)\",\n    \"validation_criteria\": [\n      \"Uses aws_s3 input\",\n      \"Uses openai_embeddings processor\",\n      \"Uses pinecone output\",\n      \"Uses postgres for metadata\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"content-routing\",\n    \"description\": \"Content-based routing\",\n    \"context\": \"HTTP input, route by type: orders→kafka, logs→elasticsearch, metrics→prometheus\",\n    \"validation_criteria\": [\n      \"Uses http_server input\",\n      \"Uses switch output\",\n      \"Routes by content type\",\n      \"Multiple destinations\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"retry-exponential-backoff\",\n    \"description\": \"Retry with exponential backoff\",\n    \"context\": \"kafka input, HTTP output with 3 retries (1s, 2s, 4s), DLQ to error topic\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses http processor with retry\",\n      \"Implements exponential backoff\",\n      \"DLQ pattern for failures\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"dlq-pattern\",\n    \"description\": \"Dead letter queue pattern\",\n    \"context\": \"kafka input, transform, on error: send to DLQ topic with error metadata\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses try/catch processors\",\n      \"DLQ output on error\",\n      \"Includes error metadata\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"circuit-breaker\",\n    \"description\": \"Circuit breaker for external API\",\n    \"context\": \"kafka input, call API, circuit breaker: 5 failures → open for 60s\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses http processor\",\n      \"Implements circuit breaker logic\",\n      \"Handles failures gracefully\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"fallback-chain\",\n    \"description\": \"Fallback output chain\",\n    \"context\": \"kafka input, try: primary DB, fallback: secondary DB, final: S3 backup\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses try/fallback pattern\",\n      \"Multiple output attempts\",\n      \"Final fallback to S3\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"poison-pill-handling\",\n    \"description\": \"Poison pill handling\",\n    \"context\": \"kafka input, skip malformed messages, log to errors, continue processing\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Uses try/catch\",\n      \"Logs errors without stopping\",\n      \"Continues processing\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"transaction-batching\",\n    \"description\": \"Transaction batching with rollback\",\n    \"context\": \"kafka input, batch 100 msgs, postgres transaction, rollback batch on any error\",\n    \"validation_criteria\": [\n      \"Uses kafka input\",\n      \"Implements batching\",\n      \"Uses sql with transactions\",\n      \"Rollback on error\",\n      \"Passes rpk connect lint\"\n    ],\n    \"difficulty\": \"advanced\"\n  }\n]\n",
        ".claude-plugin/plugins/redpanda-connect/tests/fixtures/search_queries.json": "[\n  {\n    \"id\": \"kafka-consumer\",\n    \"query\": \"kafka consumer\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"ockam_kafka\", \"redpanda\"],\n    \"description\": \"Basic Kafka consumer search\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"postgres-output\",\n    \"query\": \"postgres output\",\n    \"expected_category\": \"outputs\",\n    \"expected_components\": [\"sql_insert\", \"postgresql\", \"postgres\"],\n    \"description\": \"PostgreSQL database output search\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"http-server\",\n    \"query\": \"http server\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"http_server\"],\n    \"description\": \"HTTP server input search\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"redis-cache\",\n    \"query\": \"redis cache with TTL\",\n    \"expected_category\": \"caches\",\n    \"expected_components\": [\"redis\"],\n    \"description\": \"Redis cache with TTL configuration\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"s3-output\",\n    \"query\": \"write to S3 bucket\",\n    \"expected_category\": \"outputs\",\n    \"expected_components\": [\"aws_s3\"],\n    \"description\": \"AWS S3 output search\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"mqtt-broker\",\n    \"query\": \"mqtt broker\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"mqtt\"],\n    \"description\": \"MQTT broker connection\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"gcp-pubsub\",\n    \"query\": \"google cloud pub/sub\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"gcp_pubsub\"],\n    \"description\": \"GCP Pub/Sub search\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"elasticsearch\",\n    \"query\": \"elasticsearch output\",\n    \"expected_category\": \"outputs\",\n    \"expected_components\": [\"elasticsearch\"],\n    \"description\": \"Elasticsearch output search\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"websocket\",\n    \"query\": \"websocket server\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"websocket\"],\n    \"description\": \"WebSocket server input\"  ,\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"azure-storage\",\n    \"query\": \"azure blob storage\",\n    \"expected_category\": \"outputs\",\n    \"expected_components\": [\"azure_blob_storage\"],\n    \"description\": \"Azure Blob Storage output\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"pulsar-topic\",\n    \"query\": \"consume from Pulsar topic\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"pulsar\"],\n    \"description\": \"Pulsar topic consumer\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"parquet-s3\",\n    \"query\": \"read parquet files from S3\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"aws_s3\"],\n    \"expected_config\": [\"scanner\", \"parquet\"],\n    \"description\": \"S3 with Parquet scanner\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"nats-jetstream\",\n    \"query\": \"subscribe to NATS JetStream\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"nats_jetstream\"],\n    \"description\": \"NATS JetStream subscription\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"mysql-polling\",\n    \"query\": \"poll MySQL database for new records\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"sql_select\", \"mysql_cdc\"],\n    \"description\": \"MySQL polling or CDC\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"snowflake-output\",\n    \"query\": \"write to Snowflake table\",\n    \"expected_category\": \"outputs\",\n    \"expected_components\": [\"snowflake_put\", \"snowflake_streaming\"],\n    \"description\": \"Snowflake data warehouse output\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"sns-output\",\n    \"query\": \"publish to AWS SNS\",\n    \"expected_category\": \"outputs\",\n    \"expected_components\": [\"aws_sns\"],\n    \"description\": \"AWS SNS publish\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"mongodb-output\",\n    \"query\": \"store in MongoDB collection\",\n    \"expected_category\": \"outputs\",\n    \"expected_components\": [\"mongodb\"],\n    \"description\": \"MongoDB collection write\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"clickhouse-output\",\n    \"query\": \"write to ClickHouse database\",\n    \"expected_category\": \"outputs\",\n    \"expected_components\": [\"sql\"],\n    \"expected_config\": [\"driver\", \"clickhouse\"],\n    \"description\": \"ClickHouse database output\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"compress-processor\",\n    \"query\": \"compress messages with gzip\",\n    \"expected_category\": \"processors\",\n    \"expected_components\": [\"compress\"],\n    \"expected_config\": [\"algorithm\", \"gzip\"],\n    \"description\": \"Gzip compression processor\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"avro-schema-registry\",\n    \"query\": \"decode Avro with schema registry\",\n    \"expected_category\": \"processors\",\n    \"expected_components\": [\"avro\", \"schema_registry_decode\"],\n    \"description\": \"Avro schema registry decoding\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"openai-embeddings\",\n    \"query\": \"generate embeddings with OpenAI\",\n    \"expected_category\": \"processors\",\n    \"expected_components\": [\"openai_embeddings\"],\n    \"description\": \"OpenAI embeddings generation\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"javascript-processor\",\n    \"query\": \"run custom JavaScript code\",\n    \"expected_category\": \"processors\",\n    \"expected_components\": [\"javascript\"],\n    \"description\": \"JavaScript processor\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"grok-parser\",\n    \"query\": \"parse logs with Grok patterns\",\n    \"expected_category\": \"processors\",\n    \"expected_components\": [\"grok\"],\n    \"description\": \"Grok log parsing\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"http-processor\",\n    \"query\": \"call external REST API\",\n    \"expected_category\": \"processors\",\n    \"expected_components\": [\"http\"],\n    \"description\": \"HTTP API call processor\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"json-schema-validation\",\n    \"query\": \"validate JSON schema\",\n    \"expected_category\": \"processors\",\n    \"expected_components\": [\"json_schema\"],\n    \"description\": \"JSON schema validation\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"kafka-to-elasticsearch\",\n    \"query\": \"build Kafka to Elasticsearch pipeline\",\n    \"expected_category\": \"multi\",\n    \"expected_components\": [\"kafka\", \"elasticsearch\"],\n    \"description\": \"Kafka to Elasticsearch integration\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"s3-to-bigquery\",\n    \"query\": \"S3 to BigQuery ETL with transformation\",\n    \"expected_category\": \"multi\",\n    \"expected_components\": [\"aws_s3\", \"gcp_bigquery\"],\n    \"description\": \"S3 to BigQuery ETL\",\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"postgres-cdc-snowflake\",\n    \"query\": \"PostgreSQL CDC to Snowflake replication\",\n    \"expected_category\": \"multi\",\n    \"expected_components\": [\"postgres_cdc\", \"snowflake\"],\n    \"description\": \"PostgreSQL CDC to Snowflake\",\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"lru-cache\",\n    \"query\": \"in-memory cache with LRU eviction\",\n    \"expected_category\": \"caches\",\n    \"expected_components\": [\"lru\", \"ristretto\"],\n    \"description\": \"LRU cache\",\n    \"difficulty\": \"basic\"\n  },\n  {\n    \"id\": \"multilevel-cache\",\n    \"query\": \"multi-level caching strategy\",\n    \"expected_category\": \"caches\",\n    \"expected_components\": [\"multilevel\"],\n    \"description\": \"Multi-level cache\",\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"high-throughput-kafka\",\n    \"query\": \"high throughput Kafka consumer\",\n    \"expected_category\": \"inputs\",\n    \"expected_components\": [\"kafka_franz\"],\n    \"expected_config\": [\"batching\", \"parallel\"],\n    \"description\": \"High-performance Kafka setup\",\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"vector-database\",\n    \"query\": \"write to vector database\",\n    \"expected_category\": \"outputs\",\n    \"expected_components\": [\"pinecone\", \"qdrant\"],\n    \"description\": \"Vector database output\",\n    \"difficulty\": \"intermediate\"\n  },\n  {\n    \"id\": \"ai-llm-processing\",\n    \"query\": \"stream processing with AI/LLM\",\n    \"expected_category\": \"processors\",\n    \"expected_components\": [\"openai_chat_completion\", \"aws_bedrock_chat\", \"cohere_chat\"],\n    \"description\": \"AI/LLM processing\",\n    \"difficulty\": \"advanced\"\n  },\n  {\n    \"id\": \"nonexistent-component\",\n    \"query\": \"nonexistent_database_xyz\",\n    \"expected_category\": null,\n    \"expected_components\": [],\n    \"description\": \"Hallucination prevention test - component doesn't exist\",\n    \"difficulty\": \"edge_case\",\n    \"should_not_hallucinate\": true\n  }\n]\n"
      },
      "plugins": [
        {
          "name": "redpanda-connect",
          "description": "YAML config and Bloblang authoring for Redpanda Connect",
          "source": "./.claude-plugin/plugins/redpanda-connect",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add redpanda-data/connect",
            "/plugin install redpanda-connect@redpanda-connect-plugins"
          ]
        }
      ]
    }
  ]
}