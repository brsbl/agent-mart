{
  "author": {
    "id": "atrawog",
    "display_name": "Andreas Trawoeger",
    "avatar_url": "https://avatars.githubusercontent.com/u/927669?u=a11f5491b062a128c80a04084aeed9a936ced0df&v=4"
  },
  "marketplaces": [
    {
      "name": "bazzite-ai-plugins",
      "version": null,
      "description": "Claude Code plugins for Bazzite AI - OS features and development tools",
      "repo_full_name": "atrawog/bazzite-ai-plugins",
      "repo_url": "https://github.com/atrawog/bazzite-ai-plugins",
      "repo_description": "Claude Code plugins for Bazzite AI",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-26T08:15:23Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"bazzite-ai-plugins\",\n  \"owner\": {\n    \"name\": \"atrawog\"\n  },\n  \"metadata\": {\n    \"description\": \"Claude Code plugins for Bazzite AI - OS features and development tools\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"bazzite\",\n      \"source\": \"./bazzite\",\n      \"description\": \"Skills for using Bazzite OS features via ujust commands\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"atrawog\"\n      },\n      \"homepage\": \"https://bazzite.gg/\",\n      \"repository\": \"https://github.com/atrawog/bazzite-ai-plugins\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"bazzite\", \"gaming\", \"immutable-os\", \"ujust\"],\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"bazzite-ai\",\n      \"source\": \"./bazzite-ai\",\n      \"description\": \"Skills for using Bazzite AI OS features via ujust commands\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"atrawog\"\n      },\n      \"homepage\": \"https://bazzite.ai/\",\n      \"repository\": \"https://github.com/atrawog/bazzite-ai-plugins\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"bazzite\", \"ai\", \"immutable-os\", \"ujust\"],\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"bazzite-ai-dev\",\n      \"source\": \"./bazzite-ai-dev\",\n      \"description\": \"Development tools and enforcement agents for Bazzite AI contributors\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"atrawog\"\n      },\n      \"homepage\": \"https://bazzite.ai/\",\n      \"repository\": \"https://github.com/atrawog/bazzite-ai-plugins\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"bazzite\", \"development\", \"testing\", \"enforcement\"],\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"bazzite-ai-jupyter\",\n      \"source\": \"./bazzite-ai-jupyter\",\n      \"description\": \"ML/AI development workflows for JupyterLab - LangChain, RAG, fine-tuning, and model optimization\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"atrawog\"\n      },\n      \"homepage\": \"https://bazzite.ai/\",\n      \"repository\": \"https://github.com/atrawog/bazzite-ai\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"jupyter\", \"langchain\", \"rag\", \"fine-tuning\", \"ml\"],\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"bazzite-ai-ollama\",\n      \"source\": \"./bazzite-ai-ollama\",\n      \"description\": \"Ollama API operations for LLM inference, embeddings, and model management\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"atrawog\"\n      },\n      \"homepage\": \"https://bazzite.ai/\",\n      \"repository\": \"https://github.com/atrawog/bazzite-ai\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"ollama\", \"llm\", \"inference\", \"embeddings\"],\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        "README.md": "# Bazzite AI Plugins\n\nClaude Code plugin marketplace for [Bazzite AI](https://github.com/atrawog/bazzite-ai) - an AI/ML-focused immutable Linux OS.\n\n## Which Plugins Do I Need?\n\n| You are... | Install these plugins |\n|------------|----------------------|\n| **Bazzite OS user** | `bazzite` - Base OS features (gaming, GPU, storage, etc.) |\n| **Bazzite AI user** | `bazzite` + `bazzite-ai` - AI/ML services (Ollama, JupyterLab, etc.) |\n| **Using bazzite-ai-jupyter pod** | `bazzite-ai` + `bazzite-ai-jupyter` - ML workflows in Jupyter notebooks |\n| **Contributing to Bazzite AI** | All plugins including `bazzite-ai-dev` - Development tools & enforcement |\n\n## Available Plugins\n\n| Plugin | Description | Skills | Agents | MCP | Audience |\n|--------|-------------|--------|--------|-----|----------|\n| **bazzite** | Base Bazzite OS features | 12 | - | - | All Bazzite users |\n| **bazzite-ai** | AI/ML service management via ujust | 20 | - | - | Bazzite AI users |\n| **bazzite-ai-jupyter** | ML workflows for Jupyter notebooks | 20 | - | Jupyter | Data scientists using jupyter pod |\n| **bazzite-ai-dev** | Development tools & enforcement | 6 | 14 | GitHub | Contributors |\n\n## Installation\n\n### Method 1: Marketplace (Recommended)\n\n```bash\n# Add the marketplace\n/plugin marketplace add atrawog/bazzite-ai-plugins\n\n# Install plugins (choose what you need)\n/plugin install bazzite@bazzite-ai-plugins           # Base OS features\n/plugin install bazzite-ai@bazzite-ai-plugins        # AI/ML services\n/plugin install bazzite-ai-jupyter@bazzite-ai-plugins # ML workflows\n/plugin install bazzite-ai-dev@bazzite-ai-plugins    # Development tools\n```\n\n### Method 2: Team Configuration\n\nAdd to your Claude Code settings (`.claude/settings.json`):\n\n```json\n{\n  \"extraKnownMarketplaces\": {\n    \"bazzite-ai-plugins\": {\n      \"source\": { \"source\": \"github\", \"repo\": \"atrawog/bazzite-ai-plugins\" }\n    }\n  },\n  \"enabledPlugins\": {\n    \"bazzite@bazzite-ai-plugins\": true,\n    \"bazzite-ai@bazzite-ai-plugins\": true,\n    \"bazzite-ai-jupyter@bazzite-ai-plugins\": true,\n    \"bazzite-ai-dev@bazzite-ai-plugins\": true\n  }\n}\n```\n\n### Method 3: Manual Loading\n\n```bash\n# Clone the repository\ngit clone https://github.com/atrawog/bazzite-ai-plugins.git\n\n# Load plugins\nclaude --plugin-dir ./bazzite-ai-plugins/bazzite\nclaude --plugin-dir ./bazzite-ai-plugins/bazzite-ai\nclaude --plugin-dir ./bazzite-ai-plugins/bazzite-ai-jupyter\nclaude --plugin-dir ./bazzite-ai-plugins/bazzite-ai-dev\n```\n\n## Plugin Details\n\n### bazzite (Base OS)\n\nSkills for base Bazzite OS configuration and management:\n\n| Skill | Description |\n|-------|-------------|\n| **apps** | Third-party apps (CoolerControl, JetBrains, OpenRazer, DisplayLink, scrcpy) |\n| **audio** | Audio configuration (virtual channels, surround sound, PipeWire) |\n| **boot** | Boot configuration (BIOS/UEFI, GRUB, secure boot, dual-boot) |\n| **desktop** | Desktop customization (GTK themes, terminal transparency) |\n| **distrobox** | Container management (manifests, DaVinci Resolve) |\n| **gaming** | Gaming ecosystem (Steam, Proton, EmuDeck, Decky, Sunshine) |\n| **gpu** | GPU drivers (NVIDIA proprietary, Optimus, NVK, Mesa) |\n| **network** | Network configuration (iwd WiFi, Wake-on-LAN, Tailscale) |\n| **security** | Security (LUKS/TPM auto-unlock, secure boot, FIDO2) |\n| **storage** | Storage management (automount, BTRFS deduplication, snapshots) |\n| **system** | System maintenance (updates, cleanup, logs, diagnostics) |\n| **virtualization** | GPU passthrough (KVM, VFIO, Looking Glass, USB hotplug) |\n\n**Usage:**\n\n```bash\n/bazzite:gaming    # Gaming setup help\n/bazzite:gpu       # GPU driver guidance\n/bazzite:storage   # Storage management\n```\n\n### bazzite-ai (AI/ML Services)\n\nSkills for managing AI/ML services via `ujust` commands:\n\n| Skill | Description |\n|-------|-------------|\n| **apptainer** | Apptainer/Singularity HPC container management |\n| **bootc** | Bootable container testing via bcvk |\n| **comfyui** | ComfyUI node-based Stable Diffusion interface |\n| **config** | System configuration dispatcher (services, GPU, desktop) |\n| **deploy** | Helm deployments to k3d (JupyterHub, KubeAI) |\n| **fiftyone** | FiftyOne dataset visualization with MongoDB sidecar |\n| **install** | Development tool installation (Claude Code, pixi, etc.) |\n| **jellyfin** | Jellyfin media server with hardware transcoding |\n| **jupyter** | JupyterLab ML/AI development environment |\n| **k3d** | Lightweight k3s Kubernetes clusters in Podman |\n| **localai** | LocalAI OpenAI-compatible inference API |\n| **ollama** | Ollama LLM inference server with GPU acceleration |\n| **openwebui** | Open WebUI chat interface for Ollama |\n| **pods** | Aggregate pod service management |\n| **portainer** | Portainer CE container management UI |\n| **record** | Terminal recording with asciinema |\n| **runners** | Self-hosted GitHub Actions runners with GPU |\n| **tailscale** | Tailscale Serve for service exposure |\n| **test** | Runtime verification tests (GPU, CUDA, PyTorch) |\n| **vm** | QCOW2 virtual machine management via libvirt |\n\n**Usage:**\n\n```bash\n/bazzite-ai:ollama   # Ollama setup and management\n/bazzite-ai:jupyter  # JupyterLab configuration\n/bazzite-ai:k3d      # Kubernetes cluster help\n```\n\n### bazzite-ai-dev (Development)\n\nDevelopment tools and enforcement agents for contributors.\n\n**Skills (6):**\n\n| Skill | Description |\n|-------|-------------|\n| **build** | Unified build system for OS images, pods, VMs, ISOs |\n| **clean** | Cleanup build artifacts, caches, and temporary files |\n| **lfs** | Git LFS file management (checkout, status, locking) |\n| **overlay** | Overlay session management for live justfile editing |\n| **record** | Batch recording system for documentation |\n| **test** | Runtime verification tests for installations |\n\n**Enforcement Agents (14):**\n\n*BLOCKING (must pass before proceeding):*\n\n| Agent | Trigger | Function |\n|-------|---------|----------|\n| **policy-enforcer** | Before Edit/Write, commits | Verifies all 11 policy rules |\n| **root-cause-analyzer** | Any error in output | Mandatory 8-step error investigation |\n| **testing-validator** | Claiming \"working\" | Confirms LOCAL system testing done |\n| **justfile-validator** | Editing .just files | Validates non-interactive support, <30K |\n| **pre-commit-guardian** | Before git commit | Ensures 100% hook pass rate |\n| **documentation-validator** | Editing docs/*.md | Validates MyST syntax, myst.yml |\n| **config-integrity-enforcer** | Editing ~/.config/* | Blocks - edit source code instead |\n| **pixi-lock-enforcer** | Editing pixi.lock | Blocks - run `pixi install` instead |\n| **sudo-usage-enforcer** | `sudo ujust` detected | Blocks external sudo elevation |\n| **overlay-testing-enforcer** | `just -f` for testing | Blocks - use overlay method |\n\n*ADVISORY (guidance only):*\n\n| Agent | Trigger | Function |\n|-------|---------|----------|\n| **architecture-advisor** | \"Why?\" questions | Explains immutable OS design decisions |\n| **buildcache-validator** | Build file changes | Analyzes build cache performance impact |\n| **code-research** | Architectural \"how\" | Deep codebase analysis |\n| **github-actions** | CI status queries | Reports workflow status and failures |\n\n**MCP Server: GitHub**\n\nProvides 22 tools via `mcp__github__*` for GitHub integration:\n\n| Category | Tools |\n|----------|-------|\n| Issues | `issue_read`, `issue_write`, `add_issue_comment`, `list_issues`, `search_issues`, `list_issue_types`, `sub_issue_write` |\n| Pull Requests | `pull_request_read`, `list_pull_requests`, `search_pull_requests` |\n| Workflows | `list_workflows`, `list_workflow_runs`, `get_workflow_run`, `list_workflow_jobs`, `get_job_logs` |\n| Repository | `get_file_contents`, `list_commits`, `get_commit`, `list_branches`, `get_me` |\n| Labels | `get_label`, `list_label`, `label_write` |\n\n**Usage:**\n\n```bash\n/bazzite-ai-dev:build   # OS image building\n/bazzite-ai-dev:test    # Set up overlay testing\n/bazzite-ai-dev:clean   # Clean build artifacts\n```\n\n### bazzite-ai-jupyter (ML Workflows)\n\nSkills for machine learning workflows in Jupyter notebooks.\n\n**MCP Server: Jupyter**\n\nProvides 14 tools via `mcp__jupyter__*` for direct notebook interaction:\n\n| Tool | Function |\n|------|----------|\n| `list_files` | List files in Jupyter server filesystem |\n| `list_kernels` | List available kernels |\n| `use_notebook` | Activate notebook for operations |\n| `list_notebooks` | List activated notebooks |\n| `read_notebook` | Read notebook cells and structure |\n| `read_cell` | Read specific cell details |\n| `insert_cell` | Insert new cells |\n| `overwrite_cell_source` | Modify cell contents |\n| `delete_cell` | Delete cells |\n| `execute_cell` | Execute notebook cells |\n| `insert_execute_code_cell` | Insert and execute combined |\n| `execute_code` | Execute code directly in kernel |\n| `restart_notebook` | Restart notebook kernel |\n| `unuse_notebook` | Release notebook resources |\n\n**Skills (20):**\n\n*Ollama Integration:*\n\n| Skill | Description |\n|-------|-------------|\n| **chat** | Direct REST API operations using requests library |\n| **ollama** | Official ollama Python library for LLM inference |\n| **openai** | OpenAI compatibility layer for Ollama |\n| **gpu** | GPU monitoring, VRAM usage, inference metrics |\n| **huggingface** | Import GGUF models from HuggingFace |\n\n*ML/AI Development:*\n\n| Skill | Description |\n|-------|-------------|\n| **langchain** | LangChain framework - prompts, chains, model wrappers |\n| **rag** | Retrieval-Augmented Generation with vector stores |\n| **evaluation** | LLM evaluation with Evidently.ai |\n| **transformers** | Transformer architecture concepts |\n| **finetuning** | Fine-tuning with PyTorch and HuggingFace Trainer |\n\n*Training & Optimization:*\n\n| Skill | Description |\n|-------|-------------|\n| **quantization** | Model quantization (GPTQ, AWQ, INT8) |\n| **peft** | Parameter-efficient fine-tuning (LoRA, Unsloth) |\n| **sft** | Supervised Fine-Tuning with SFTTrainer |\n| **qlora** | Advanced QLoRA experiments |\n\n*RLHF:*\n\n| Skill | Description |\n|-------|-------------|\n| **dpo** | Direct Preference Optimization |\n| **grpo** | Group Relative Policy Optimization |\n| **rloo** | Reinforcement Learning with Leave-One-Out |\n| **reward** | Reward model training for RLHF pipelines |\n\n*Inference & Vision:*\n\n| Skill | Description |\n|-------|-------------|\n| **inference** | Fast inference with vLLM, thinking model parsing |\n| **vision** | Vision model fine-tuning with FastVisionModel |\n\n**Usage:**\n\n```bash\n/bazzite-ai-jupyter:sft     # Supervised fine-tuning guide\n/bazzite-ai-jupyter:qlora   # QLoRA training help\n/bazzite-ai-jupyter:rag     # RAG implementation\n```\n\n## Documentation\n\n- **Main Repository:** <https://github.com/atrawog/bazzite-ai>\n- **Full Documentation:** <https://bazzite.ai/>\n\n## Summary\n\n### Component Totals\n\n| Component | Count |\n|-----------|-------|\n| **Plugins** | 4 |\n| **Skills** | 58 |\n| **Agents** | 14 (10 blocking + 4 advisory) |\n| **MCP Servers** | 2 |\n| **MCP Tools** | 36 (GitHub: 22, Jupyter: 14) |\n\n### Skills by Plugin\n\n| Plugin | Skills | Description |\n|--------|--------|-------------|\n| bazzite | 12 | Base OS features |\n| bazzite-ai | 20 | AI/ML service management |\n| bazzite-ai-jupyter | 20 | ML workflows in Jupyter |\n| bazzite-ai-dev | 6 | Development tools |\n| **Total** | **58** | |\n\n### Quick Reference\n\n| Task | Plugin | Skill |\n|------|--------|-------|\n| Gaming setup | bazzite | `/bazzite:gaming` |\n| GPU drivers | bazzite | `/bazzite:gpu` |\n| Ollama server | bazzite-ai | `/bazzite-ai:ollama` |\n| JupyterLab | bazzite-ai | `/bazzite-ai:jupyter` |\n| Fine-tuning | bazzite-ai-jupyter | `/bazzite-ai-jupyter:sft` |\n| Build OS image | bazzite-ai-dev | `/bazzite-ai-dev:build` |\n\n## License\n\nMIT\n",
        "bazzite/README.md": "# Bazzite Plugin\n\nClaude Code skills for default Bazzite OS features via ujust commands.\n\n## Overview\n\nThis plugin provides skills for the standard Bazzite ujust commands - gaming, system management, hardware configuration, and more. For AI/ML-focused features, see the `bazzite-ai` plugin.\n\n## Skills (12)\n\n| Skill | Description |\n|-------|-------------|\n| **system** | Updates, cleanup, logs, diagnostics, benchmarks |\n| **boot** | BIOS/UEFI, GRUB, secure boot, dual-boot Windows |\n| **distrobox** | Container management, DaVinci Resolve |\n| **gaming** | Steam, EmuDeck, Decky, Sunshine, frame generation |\n| **audio** | Virtual channels, surround sound, Bluetooth, PipeWire |\n| **gpu** | NVIDIA drivers, Optimus, NVK, Mesa, Broadcom WiFi |\n| **storage** | Automount, deduplication, snapshots |\n| **network** | iwd WiFi, Wake-on-LAN, Tailscale |\n| **security** | LUKS/TPM unlock, secure boot keys |\n| **virtualization** | VFIO, KVM, Looking Glass, USB hotplug |\n| **desktop** | GTK themes, terminal transparency |\n| **apps** | CoolerControl, OpenRazer, DisplayLink, scrcpy |\n\n## Usage\n\nInvoke skills using the `/bazzite:` prefix:\n\n```bash\n/bazzite:gaming    # Gaming ecosystem help\n/bazzite:gpu       # GPU driver configuration\n/bazzite:system    # System maintenance\n```\n\n## Related Plugins\n\n- **bazzite-ai**: AI/ML-focused features (Jupyter, Ollama, ComfyUI, GPU containers)\n- **bazzite-ai-dev**: Development tools and enforcement agents\n\n## Quick Reference\n\n### System Maintenance\n\n```bash\nujust update              # Update system\nujust changelogs          # View release notes\nujust clean-system        # Cleanup podman/flatpaks\nujust logs-this-boot      # View current boot logs\n```\n\n### Gaming\n\n```bash\nujust setup-sunshine      # Game streaming server\nujust setup-decky         # Decky Loader\nujust install-emudeck     # EmuDeck for emulation\nujust fix-proton-hang     # Kill hung Proton processes\n```\n\n### GPU\n\n```bash\nujust config-nvidia    # NVIDIA driver config\nujust toggle-nvk          # Switch NVIDIA/NVK images\nujust enable-supergfxctl  # GPU switcher for laptops\n```\n\n### Audio\n\n```bash\nujust setup-virtual-channels   # Game/Voice/Browser sinks\nujust setup-virtual-surround   # 7.1 for headphones\nujust restart-pipewire         # Restart audio service\n```\n\n### Virtualization\n\n```bash\nujust setup-virtualization virt-on    # Enable KVM\nujust setup-virtualization vfio-on    # Enable VFIO\nujust setup-virtualization kvmfr      # Looking Glass setup\n```\n\n## License\n\nMIT\n",
        "bazzite-ai/README.md": "# bazzite-ai Plugin\n\nClaude Code plugin for using Bazzite AI OS features via `ujust` commands.\n\n## Purpose\n\nThis plugin provides skills for **OS users** who want to manage and configure Bazzite AI services, containers, and features using the `ujust` command system.\n\n## Available Skills (20)\n\n| Skill | Command | Description |\n|-------|---------|-------------|\n| apptainer | `/bazzite-ai:apptainer` | Apptainer/Singularity HPC container management |\n| bootc | `/bazzite-ai:bootc` | Bootable container testing and management |\n| comfyui | `/bazzite-ai:comfyui` | ComfyUI AI image generation server |\n| config | `/bazzite-ai:config` | System configuration (services, GPU, Docker, etc.) |\n| deploy | `/bazzite-ai:deploy` | Helm deployments to k3d (JupyterHub, KubeAI) |\n| fiftyone | `/bazzite-ai:fiftyone` | FiftyOne dataset visualization and management |\n| install | `/bazzite-ai:install` | System package and Flatpak installation |\n| jellyfin | `/bazzite-ai:jellyfin` | Jellyfin media server management |\n| jupyter | `/bazzite-ai:jupyter` | JupyterLab server management |\n| k3d | `/bazzite-ai:k3d` | Lightweight Kubernetes clusters in Podman |\n| localai | `/bazzite-ai:localai` | LocalAI inference server |\n| ollama | `/bazzite-ai:ollama` | Ollama LLM inference server |\n| openwebui | `/bazzite-ai:openwebui` | Open WebUI chat interface for Ollama |\n| pods | `/bazzite-ai:pods` | Pod container lifecycle management |\n| portainer | `/bazzite-ai:portainer` | Portainer container management UI |\n| record | `/bazzite-ai:record` | Terminal recording with asciinema |\n| runners | `/bazzite-ai:runners` | GitHub Actions self-hosted runners |\n| tailscale | `/bazzite-ai:tailscale` | Tailscale service exposure |\n| test | `/bazzite-ai:test` | Testing and verification commands |\n| vm | `/bazzite-ai:vm` | Virtual machine management |\n\n## Usage Examples\n\n```bash\n# Ask Claude to help with Ollama\n/bazzite-ai:ollama\n# Claude will guide you through Ollama setup, model management, etc.\n\n# Configure JupyterLab\n/bazzite-ai:jupyter\n# Claude will help with JupyterLab installation, configuration, and troubleshooting\n\n# Set up GPU containers and system services\n/bazzite-ai:config\n# Claude will guide you through GPU passthrough and service configuration\n\n# Deploy applications to Kubernetes\n/bazzite-ai:deploy\n# Claude will help deploy JupyterHub or KubeAI to k3d clusters\n```\n\n## Installation\n\n### Manual Loading\n\n```bash\nclaude --plugin-dir /path/to/bazzite-ai-testing/plugins/bazzite-ai\n```\n\n### Permanent Configuration\n\nAdd to your Claude Code settings:\n\n```json\n{\n  \"plugins\": [\n    \"/path/to/bazzite-ai-testing/plugins/bazzite-ai\"\n  ]\n}\n```\n\n## Requirements\n\n- Bazzite AI OS installed\n- `ujust` command available at `/usr/share/ublue-os/justfile`\n\n## Related\n\n- **bazzite-ai-dev**: Development tools for contributors (separate plugin)\n- **Documentation**: <https://bazzite.ai/>\n",
        "bazzite-ai-dev/README.md": "# bazzite-ai-dev Plugin\n\nClaude Code plugin for Bazzite AI development with enforcement agents and development tools.\n\n## Purpose\n\nThis plugin provides:\n\n1. **Development skills** for building, testing, and maintaining Bazzite AI\n2. **Enforcement agents** that ensure code quality and policy compliance\n3. **GitHub MCP integration** for repository operations\n\n## MCP Server\n\nThis plugin includes a GitHub MCP server for repository operations.\n\n**Tools available:**\n\n- Issues: `issue_read`, `issue_write`, `add_issue_comment`, `list_issues`, `search_issues`\n- Pull requests: `pull_request_read`, `list_pull_requests`, `search_pull_requests`\n- Workflows: `list_workflows`, `list_workflow_runs`, `get_workflow_run`, `get_job_logs`\n- Repository: `get_file_contents`, `list_commits`, `get_commit`, `list_branches`\n- Labels: `get_label`, `list_label`, `label_write`\n\n**Prerequisites:**\n\n- `github-mcp-server` available via direnv (installed in project)\n- `GITHUB_TOKEN` environment variable set\n\n## Available Skills (6)\n\n| Skill | Command | Description |\n|-------|---------|-------------|\n| build | `/bazzite-ai-dev:build` | OS image building with Podman (`just build`) |\n| clean | `/bazzite-ai-dev:clean` | Cleanup build artifacts and caches (`just clean`) |\n| lfs | `/bazzite-ai-dev:lfs` | Git LFS file management (`just lfs`) |\n| overlay | `/bazzite-ai-dev:overlay` | Development overlay session management (`just overlay`) |\n| record | `/bazzite-ai-dev:record` | Batch documentation recording (`just record`) |\n| test | `/bazzite-ai-dev:test` | Runtime verification tests (`ujust test`) |\n\n## Enforcement Agents\n\nThese agents are automatically invoked to enforce development policies:\n\n### Blocking Agents (Must Pass)\n\n| Agent | Trigger | Purpose |\n|-------|---------|---------|\n| policy-enforcer | Before Edit/Write, commits | Verifies all policy compliance |\n| root-cause-analyzer | On errors | Mandatory 8-step error analysis |\n| testing-validator | Before claiming \"working\" | Confirms LOCAL testing completed |\n| justfile-validator | Editing .just files | Validates non-interactive support |\n| pre-commit-guardian | Before git commit | Ensures 100% hook pass rate |\n| documentation-validator | Editing docs/*.md | Validates MyST syntax |\n| config-integrity-enforcer | Editing ~/.config/* | Blocks editing output configs |\n| pixi-lock-enforcer | Editing pixi.lock | Blocks manual lock edits |\n| sudo-usage-enforcer | sudo ujust detected | Blocks external sudo elevation |\n| overlay-testing-enforcer | just -f testing | Blocks direct justfile testing |\n\n### Advisory Agents\n\n| Agent | Trigger | Purpose |\n|-------|---------|---------|\n| architecture-advisor | \"Why?\" questions | Explains immutable OS design |\n| buildcache-validator | Build file changes | Analyzes build cache impact |\n| code-research | Architectural questions | Deep codebase analysis |\n| github-actions | CI status queries | Reports workflow status |\n\n## Usage Examples\n\n```bash\n# Build the OS image\n/bazzite-ai-dev:build\n# Claude will help with image building, troubleshooting, etc.\n\n# Enable development overlay mode\n/bazzite-ai-dev:overlay\n# Claude will guide you through overlay setup for live justfile editing\n\n# Run runtime verification tests\n/bazzite-ai-dev:test\n# Claude will help verify GPU, services, and pod functionality\n\n# Clean up after development\n/bazzite-ai-dev:clean\n# Claude will help clean build artifacts and caches\n\n# Manage Git LFS files\n/bazzite-ai-dev:lfs\n# Claude will help with large file checkout and verification\n\n# Generate documentation recordings\n/bazzite-ai-dev:record\n# Claude will help batch-record ujust commands for docs\n```\n\n## Installation\n\n### Manual Loading\n\n```bash\n# Load both plugins for full development experience\nclaude --plugin-dir ./plugins/bazzite-ai --plugin-dir ./plugins/bazzite-ai-dev\n```\n\n### Permanent Configuration\n\nAdd to your Claude Code settings:\n\n```json\n{\n  \"plugins\": [\n    \"/path/to/bazzite-ai-testing/plugins/bazzite-ai\",\n    \"/path/to/bazzite-ai-testing/plugins/bazzite-ai-dev\"\n  ]\n}\n```\n\n## Development Workflow\n\n1. **Enable overlay testing**: `just overlay refresh` (auto-enables if needed)\n2. **Make changes** to justfiles in `just/` directory\n3. **Refresh overlay**: `just overlay refresh`\n4. **Test with ujust**: `ujust <your-command>`\n5. **Verify LOCAL**: Check systemctl status, journalctl logs\n6. **Run pre-commit**: `pre-commit run --all-files`\n7. **Commit** (enforcement agents will verify)\n\n## Policies Enforced\n\n- LOCAL system verification required before claiming \"working\"\n- ~/.config files are outputs - edit source code instead\n- 100% pre-commit hook pass rate required\n- All commands must support non-interactive execution\n- .just files must be under 30K (split proactively at 25K)\n- Never use `sudo ujust` - handle sudo internally\n- Never use `just -f` for testing - use overlay method\n- Never edit pixi.lock manually - regenerate via `pixi install`\n\n## Related\n\n- **bazzite-ai**: OS user skills (separate plugin)\n- **CLAUDE.md**: Full policy documentation\n- **AGENTS.md**: Operational commands and architecture\n",
        "bazzite-ai-jupyter/README.md": "# bazzite-ai-jupyter\n\nML/AI development workflows for JupyterLab - Ollama API, LangChain, RAG, fine-tuning, and model optimization.\n\n## Overview\n\nThis plugin provides skills for **ML/AI workflows** in JupyterLab, including Ollama API operations for LLM inference.\n\n## MCP Server\n\nThis plugin includes a Jupyter MCP server that connects to a running JupyterLab instance.\n\n**Configuration:**\n\n- URL: `http://127.0.0.1:8888/mcp`\n- Type: HTTP-based MCP server\n\n**Prerequisite:** JupyterLab must be running with MCP support enabled (via `ujust jupyter start`).\n\n**Note:** This plugin is designed to work with the `bazzite-ai-pod-jupyter` container or any JupyterLab environment with the required packages.\n\n## Skills\n\n### Ollama API Operations\n\n| Skill | Description |\n|-------|-------------|\n| `chat` | Direct REST API operations using requests library |\n| `ollama` | Official `ollama` Python library usage |\n| `openai` | OpenAI compatibility layer for migration |\n| `gpu` | GPU monitoring, VRAM usage, and inference metrics |\n| `huggingface` | Import GGUF models from HuggingFace |\n\n### ML/AI Development\n\n| Skill | Description |\n|-------|-------------|\n| `langchain` | LangChain framework - prompts, chains, and model wrappers |\n| `rag` | Retrieval-Augmented Generation with vector stores |\n| `evaluation` | LLM evaluation and prompt optimization with Evidently.ai |\n| `transformers` | Transformer architecture concepts (attention, FFN) |\n| `finetuning` | Model fine-tuning with PyTorch and HuggingFace Trainer |\n| `quantization` | Model quantization for efficient inference |\n| `peft` | Parameter-efficient fine-tuning (LoRA, Unsloth) |\n| `sft` | Supervised Fine-Tuning with SFTTrainer and Unsloth |\n| `grpo` | Group Relative Policy Optimization for RLHF |\n| `dpo` | Direct Preference Optimization from preference pairs |\n| `reward` | Reward model training for RLHF pipelines |\n| `rloo` | Reinforcement Learning with Leave-One-Out baseline |\n| `inference` | Fast inference with vLLM and thinking model parsing |\n| `vision` | Vision model fine-tuning with FastVisionModel |\n| `qlora` | Advanced QLoRA experiments (alpha, rank, modules) |\n\n## MCP Server Tools\n\n**Connection:** `http://127.0.0.1:8888/mcp`\n\n| Tool | Description |\n|------|-------------|\n| `mcp__jupyter__list_files` | List files in Jupyter server filesystem |\n| `mcp__jupyter__list_kernels` | List available kernels |\n| `mcp__jupyter__use_notebook` | Activate a notebook for operations |\n| `mcp__jupyter__read_notebook` | Read notebook cells and structure |\n| `mcp__jupyter__insert_cell` | Insert new cells |\n| `mcp__jupyter__execute_cell` | Execute notebook cells |\n| `mcp__jupyter__execute_code` | Execute code directly in kernel |\n\nThe MCP server starts automatically when this plugin is enabled.\n\n## Prerequisites\n\n**JupyterLab Environment:**\n\n- JupyterLab server running at `http://localhost:8888` with MCP enabled\n- GPU access configured if using GPU-accelerated training\n\n**Ollama (for inference):**\n\n- Ollama server running (default: `http://ollama:11434` or `OLLAMA_HOST` env var)\n- Model available (pull via API or Python library)\n\n**Note:** All required Python packages are pre-installed in the `bazzite-ai-pod-jupyter` container.\n\n## Quick Start\n\n### Ollama Python Library\n\n```python\nimport ollama\n\n# Generate text\nresult = ollama.generate(model=\"llama3.2:latest\", prompt=\"Hello!\")\nprint(result[\"response\"])\n\n# Chat completion\nresponse = ollama.chat(\n    model=\"llama3.2:latest\",\n    messages=[{\"role\": \"user\", \"content\": \"What is Python?\"}]\n)\nprint(response[\"message\"][\"content\"])\n```\n\n### Critical Import Order (for Fine-tuning)\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then other imports\nfrom trl import SFTTrainer, SFTConfig\n```\n\n### LangChain with Ollama\n\n```python\nimport os\nfrom langchain_openai import ChatOpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n\nllm = ChatOpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=\"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n)\n\nresponse = llm.invoke(\"What is machine learning?\")\nprint(response.content)\n```\n\n### RAG Pipeline\n\n```python\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\"\n)\n\nvectorstore = Chroma.from_texts(documents, embeddings)\nretriever = vectorstore.as_retriever()\n```\n\n### Fine-tuning with LoRA\n\n```python\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05\n)\n\nmodel = get_peft_model(base_model, lora_config)\n```\n"
      },
      "plugins": [
        {
          "name": "bazzite",
          "source": "./bazzite",
          "description": "Skills for using Bazzite OS features via ujust commands",
          "version": "1.0.0",
          "author": {
            "name": "atrawog"
          },
          "homepage": "https://bazzite.gg/",
          "repository": "https://github.com/atrawog/bazzite-ai-plugins",
          "license": "MIT",
          "keywords": [
            "bazzite",
            "gaming",
            "immutable-os",
            "ujust"
          ],
          "category": "productivity",
          "categories": [
            "bazzite",
            "gaming",
            "immutable-os",
            "productivity",
            "ujust"
          ],
          "install_commands": [
            "/plugin marketplace add atrawog/bazzite-ai-plugins",
            "/plugin install bazzite@bazzite-ai-plugins"
          ]
        },
        {
          "name": "bazzite-ai",
          "source": "./bazzite-ai",
          "description": "Skills for using Bazzite AI OS features via ujust commands",
          "version": "1.0.0",
          "author": {
            "name": "atrawog"
          },
          "homepage": "https://bazzite.ai/",
          "repository": "https://github.com/atrawog/bazzite-ai-plugins",
          "license": "MIT",
          "keywords": [
            "bazzite",
            "ai",
            "immutable-os",
            "ujust"
          ],
          "category": "productivity",
          "categories": [
            "ai",
            "bazzite",
            "immutable-os",
            "productivity",
            "ujust"
          ],
          "install_commands": [
            "/plugin marketplace add atrawog/bazzite-ai-plugins",
            "/plugin install bazzite-ai@bazzite-ai-plugins"
          ]
        },
        {
          "name": "bazzite-ai-dev",
          "source": "./bazzite-ai-dev",
          "description": "Development tools and enforcement agents for Bazzite AI contributors",
          "version": "1.0.0",
          "author": {
            "name": "atrawog"
          },
          "homepage": "https://bazzite.ai/",
          "repository": "https://github.com/atrawog/bazzite-ai-plugins",
          "license": "MIT",
          "keywords": [
            "bazzite",
            "development",
            "testing",
            "enforcement"
          ],
          "category": "development",
          "categories": [
            "bazzite",
            "development",
            "enforcement",
            "testing"
          ],
          "install_commands": [
            "/plugin marketplace add atrawog/bazzite-ai-plugins",
            "/plugin install bazzite-ai-dev@bazzite-ai-plugins"
          ]
        },
        {
          "name": "bazzite-ai-jupyter",
          "source": "./bazzite-ai-jupyter",
          "description": "ML/AI development workflows for JupyterLab - LangChain, RAG, fine-tuning, and model optimization",
          "version": "1.0.0",
          "author": {
            "name": "atrawog"
          },
          "homepage": "https://bazzite.ai/",
          "repository": "https://github.com/atrawog/bazzite-ai",
          "license": "MIT",
          "keywords": [
            "jupyter",
            "langchain",
            "rag",
            "fine-tuning",
            "ml"
          ],
          "category": "development",
          "categories": [
            "development",
            "fine-tuning",
            "jupyter",
            "langchain",
            "ml",
            "rag"
          ],
          "install_commands": [
            "/plugin marketplace add atrawog/bazzite-ai-plugins",
            "/plugin install bazzite-ai-jupyter@bazzite-ai-plugins"
          ]
        },
        {
          "name": "bazzite-ai-ollama",
          "source": "./bazzite-ai-ollama",
          "description": "Ollama API operations for LLM inference, embeddings, and model management",
          "version": "1.0.0",
          "author": {
            "name": "atrawog"
          },
          "homepage": "https://bazzite.ai/",
          "repository": "https://github.com/atrawog/bazzite-ai",
          "license": "MIT",
          "keywords": [
            "ollama",
            "llm",
            "inference",
            "embeddings"
          ],
          "category": "development",
          "categories": [
            "development",
            "embeddings",
            "inference",
            "llm",
            "ollama"
          ],
          "install_commands": [
            "/plugin marketplace add atrawog/bazzite-ai-plugins",
            "/plugin install bazzite-ai-ollama@bazzite-ai-plugins"
          ]
        }
      ]
    },
    {
      "name": "overthink-plugins",
      "version": null,
      "description": "Claude Code plugins for Overthink OS - AI/ML services, development tools, and Jupyter workflows",
      "repo_full_name": "atrawog/overthink-plugins",
      "repo_url": "https://github.com/atrawog/overthink-plugins",
      "repo_description": "Claude Code plugins for Overthink OS - AI/ML service management, development tools, and Jupyter workflows",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-29T17:36:14Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"overthink-plugins\",\n  \"owner\": {\n    \"name\": \"atrawog\"\n  },\n  \"metadata\": {\n    \"description\": \"Claude Code plugins for Overthink OS - AI/ML services, development tools, and Jupyter workflows\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"overthink\",\n      \"source\": \"./overthink\",\n      \"description\": \"Skills for managing AI/ML services on Overthink OS via ujust commands\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"atrawog\"\n      },\n      \"homepage\": \"https://github.com/atrawog/overthink\",\n      \"repository\": \"https://github.com/atrawog/overthink-plugins\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"overthink\", \"ai\", \"ml\", \"immutable-os\", \"ujust\"],\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"overthink-dev\",\n      \"source\": \"./overthink-dev\",\n      \"description\": \"Development tools and enforcement agents for Overthink contributors\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"atrawog\"\n      },\n      \"homepage\": \"https://github.com/atrawog/overthink\",\n      \"repository\": \"https://github.com/atrawog/overthink-plugins\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"overthink\", \"development\", \"testing\", \"enforcement\"],\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"overthink-jupyter\",\n      \"source\": \"./overthink-jupyter\",\n      \"description\": \"ML/AI development workflows for JupyterLab - LangChain, RAG, fine-tuning, and model optimization\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"atrawog\"\n      },\n      \"homepage\": \"https://github.com/atrawog/overthink\",\n      \"repository\": \"https://github.com/atrawog/overthink-plugins\",\n      \"license\": \"MIT\",\n      \"keywords\": [\"jupyter\", \"langchain\", \"rag\", \"fine-tuning\", \"ml\"],\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        "README.md": "# Overthink Plugins\n\nClaude Code plugin marketplace for [Overthink](https://github.com/atrawog/overthink) - an AI/ML-focused immutable Linux OS.\n\n## Which Plugins Do I Need?\n\n| You are... | Install these plugins |\n|------------|----------------------|\n| **Overthink OS user** | `overthink` - AI/ML services (Ollama, JupyterLab, etc.) |\n| **Using overthink-jupyter pod** | `overthink` + `overthink-jupyter` - ML workflows in Jupyter notebooks |\n| **Contributing to Overthink** | All plugins including `overthink-dev` - Development tools & enforcement |\n\n## Available Plugins\n\n| Plugin | Description | Skills | Agents | MCP | Audience |\n|--------|-------------|--------|--------|-----|----------|\n| **overthink** | AI/ML service management via ujust | 20 | - | - | Overthink users |\n| **overthink-jupyter** | ML workflows for Jupyter notebooks | 20 | - | Jupyter | Data scientists using jupyter pod |\n| **overthink-dev** | Development tools & enforcement | 6 | 14 | GitHub | Contributors |\n\n## Installation\n\n### Method 1: Marketplace (Recommended)\n\n```bash\n# Add the marketplace\n/plugin marketplace add atrawog/overthink-plugins\n\n# Install plugins (choose what you need)\n/plugin install overthink@overthink-plugins        # AI/ML services\n/plugin install overthink-jupyter@overthink-plugins # ML workflows\n/plugin install overthink-dev@overthink-plugins    # Development tools\n```\n\n### Method 2: Team Configuration\n\nAdd to your Claude Code settings (`.claude/settings.json`):\n\n```json\n{\n  \"extraKnownMarketplaces\": {\n    \"overthink-plugins\": {\n      \"source\": { \"source\": \"github\", \"repo\": \"atrawog/overthink-plugins\" }\n    }\n  },\n  \"enabledPlugins\": {\n    \"overthink@overthink-plugins\": true,\n    \"overthink-jupyter@overthink-plugins\": true,\n    \"overthink-dev@overthink-plugins\": true\n  }\n}\n```\n\n### Method 3: Manual Loading\n\n```bash\n# Clone the repository\ngit clone https://github.com/atrawog/overthink-plugins.git\n\n# Load plugins\nclaude --plugin-dir ./overthink-plugins/overthink\nclaude --plugin-dir ./overthink-plugins/overthink-jupyter\nclaude --plugin-dir ./overthink-plugins/overthink-dev\n```\n\n## Plugin Details\n\n### overthink (AI/ML Services)\n\nSkills for managing AI/ML services via `ujust` commands:\n\n| Skill | Description |\n|-------|-------------|\n| **apptainer** | Apptainer/Singularity HPC container management |\n| **bootc** | Bootable container testing via bcvk |\n| **comfyui** | ComfyUI node-based Stable Diffusion interface |\n| **config** | System configuration dispatcher (services, GPU, desktop) |\n| **deploy** | Helm deployments to k3d (JupyterHub, KubeAI) |\n| **fiftyone** | FiftyOne dataset visualization with MongoDB sidecar |\n| **install** | Development tool installation (Claude Code, pixi, etc.) |\n| **jellyfin** | Jellyfin media server with hardware transcoding |\n| **jupyter** | JupyterLab ML/AI development environment |\n| **k3d** | Lightweight k3s Kubernetes clusters in Podman |\n| **localai** | LocalAI OpenAI-compatible inference API |\n| **ollama** | Ollama LLM inference server with GPU acceleration |\n| **openwebui** | Open WebUI chat interface for Ollama |\n| **pods** | Aggregate pod service management |\n| **portainer** | Portainer CE container management UI |\n| **record** | Terminal recording with asciinema |\n| **runners** | Self-hosted GitHub Actions runners with GPU |\n| **tailscale** | Tailscale Serve for service exposure |\n| **test** | Runtime verification tests (GPU, CUDA, PyTorch) |\n| **vm** | QCOW2 virtual machine management via libvirt |\n\n**Usage:**\n\n```bash\n/overthink:ollama   # Ollama setup and management\n/overthink:jupyter  # JupyterLab configuration\n/overthink:k3d      # Kubernetes cluster help\n```\n\n### overthink-dev (Development)\n\nDevelopment tools and enforcement agents for contributors.\n\n**Skills (6):**\n\n| Skill | Description |\n|-------|-------------|\n| **build** | Unified build system for OS images, pods, VMs, ISOs |\n| **clean** | Cleanup build artifacts, caches, and temporary files |\n| **lfs** | Git LFS file management (checkout, status, locking) |\n| **overlay** | Overlay session management for live justfile editing |\n| **record** | Batch recording system for documentation |\n| **test** | Runtime verification tests for installations |\n\n**Enforcement Agents (14):**\n\n*BLOCKING (must pass before proceeding):*\n\n| Agent | Trigger | Function |\n|-------|---------|----------|\n| **policy-enforcer** | Before Edit/Write, commits | Verifies all 11 policy rules |\n| **root-cause-analyzer** | Any error in output | Mandatory 8-step error investigation |\n| **testing-validator** | Claiming \"working\" | Confirms LOCAL system testing done |\n| **justfile-validator** | Editing .just files | Validates non-interactive support, <30K |\n| **pre-commit-guardian** | Before git commit | Ensures 100% hook pass rate |\n| **documentation-validator** | Editing docs/*.md | Validates MyST syntax, myst.yml |\n| **config-integrity-enforcer** | Editing ~/.config/* | Blocks - edit source code instead |\n| **pixi-lock-enforcer** | Editing pixi.lock | Blocks - run `pixi install` instead |\n| **sudo-usage-enforcer** | `sudo ujust` detected | Blocks external sudo elevation |\n| **overlay-testing-enforcer** | `just -f` for testing | Blocks - use overlay method |\n\n*ADVISORY (guidance only):*\n\n| Agent | Trigger | Function |\n|-------|---------|----------|\n| **architecture-advisor** | \"Why?\" questions | Explains immutable OS design decisions |\n| **buildcache-validator** | Build file changes | Analyzes build cache performance impact |\n| **code-research** | Architectural \"how\" | Deep codebase analysis |\n| **github-actions** | CI status queries | Reports workflow status and failures |\n\n**MCP Server: GitHub**\n\nProvides 22 tools via `mcp__github__*` for GitHub integration:\n\n| Category | Tools |\n|----------|-------|\n| Issues | `issue_read`, `issue_write`, `add_issue_comment`, `list_issues`, `search_issues`, `list_issue_types`, `sub_issue_write` |\n| Pull Requests | `pull_request_read`, `list_pull_requests`, `search_pull_requests` |\n| Workflows | `list_workflows`, `list_workflow_runs`, `get_workflow_run`, `list_workflow_jobs`, `get_job_logs` |\n| Repository | `get_file_contents`, `list_commits`, `get_commit`, `list_branches`, `get_me` |\n| Labels | `get_label`, `list_label`, `label_write` |\n\n**Usage:**\n\n```bash\n/overthink-dev:build   # OS image building\n/overthink-dev:test    # Set up overlay testing\n/overthink-dev:clean   # Clean build artifacts\n```\n\n### overthink-jupyter (ML Workflows)\n\nSkills for machine learning workflows in Jupyter notebooks.\n\n**MCP Server: Jupyter**\n\nProvides 14 tools via `mcp__jupyter__*` for direct notebook interaction:\n\n| Tool | Function |\n|------|----------|\n| `list_files` | List files in Jupyter server filesystem |\n| `list_kernels` | List available kernels |\n| `use_notebook` | Activate notebook for operations |\n| `list_notebooks` | List activated notebooks |\n| `read_notebook` | Read notebook cells and structure |\n| `read_cell` | Read specific cell details |\n| `insert_cell` | Insert new cells |\n| `overwrite_cell_source` | Modify cell contents |\n| `delete_cell` | Delete cells |\n| `execute_cell` | Execute notebook cells |\n| `insert_execute_code_cell` | Insert and execute combined |\n| `execute_code` | Execute code directly in kernel |\n| `restart_notebook` | Restart notebook kernel |\n| `unuse_notebook` | Release notebook resources |\n\n**Skills (20):**\n\n*Ollama Integration:*\n\n| Skill | Description |\n|-------|-------------|\n| **chat** | Direct REST API operations using requests library |\n| **ollama** | Official ollama Python library for LLM inference |\n| **openai** | OpenAI compatibility layer for Ollama |\n| **gpu** | GPU monitoring, VRAM usage, inference metrics |\n| **huggingface** | Import GGUF models from HuggingFace |\n\n*ML/AI Development:*\n\n| Skill | Description |\n|-------|-------------|\n| **langchain** | LangChain framework - prompts, chains, model wrappers |\n| **rag** | Retrieval-Augmented Generation with vector stores |\n| **evaluation** | LLM evaluation with Evidently.ai |\n| **transformers** | Transformer architecture concepts |\n| **finetuning** | Fine-tuning with PyTorch and HuggingFace Trainer |\n\n*Training & Optimization:*\n\n| Skill | Description |\n|-------|-------------|\n| **quantization** | Model quantization (GPTQ, AWQ, INT8) |\n| **peft** | Parameter-efficient fine-tuning (LoRA, Unsloth) |\n| **sft** | Supervised Fine-Tuning with SFTTrainer |\n| **qlora** | Advanced QLoRA experiments |\n\n*RLHF:*\n\n| Skill | Description |\n|-------|-------------|\n| **dpo** | Direct Preference Optimization |\n| **grpo** | Group Relative Policy Optimization |\n| **rloo** | Reinforcement Learning with Leave-One-Out |\n| **reward** | Reward model training for RLHF pipelines |\n\n*Inference & Vision:*\n\n| Skill | Description |\n|-------|-------------|\n| **inference** | Fast inference with vLLM, thinking model parsing |\n| **vision** | Vision model fine-tuning with FastVisionModel |\n\n**Usage:**\n\n```bash\n/overthink-jupyter:sft     # Supervised fine-tuning guide\n/overthink-jupyter:qlora   # QLoRA training help\n/overthink-jupyter:rag     # RAG implementation\n```\n\n## Documentation\n\n- **Main Repository:** <https://github.com/atrawog/overthink>\n\n## Summary\n\n### Component Totals\n\n| Component | Count |\n|-----------|-------|\n| **Plugins** | 3 |\n| **Skills** | 46 |\n| **Agents** | 14 (10 blocking + 4 advisory) |\n| **MCP Servers** | 2 |\n| **MCP Tools** | 36 (GitHub: 22, Jupyter: 14) |\n\n### Skills by Plugin\n\n| Plugin | Skills | Description |\n|--------|--------|-------------|\n| overthink | 20 | AI/ML service management |\n| overthink-jupyter | 20 | ML workflows in Jupyter |\n| overthink-dev | 6 | Development tools |\n| **Total** | **46** | |\n\n### Quick Reference\n\n| Task | Plugin | Skill |\n|------|--------|-------|\n| Ollama server | overthink | `/overthink:ollama` |\n| JupyterLab | overthink | `/overthink:jupyter` |\n| Fine-tuning | overthink-jupyter | `/overthink-jupyter:sft` |\n| Build OS image | overthink-dev | `/overthink-dev:build` |\n\n## License\n\nMIT\n",
        "overthink/README.md": "# overthink Plugin\n\nClaude Code plugin for using Overthink OS features via `ujust` commands.\n\n## Purpose\n\nThis plugin provides skills for **OS users** who want to manage and configure Overthink services, containers, and features using the `ujust` command system.\n\n## Available Skills (20)\n\n| Skill | Command | Description |\n|-------|---------|-------------|\n| apptainer | `/overthink:apptainer` | Apptainer/Singularity HPC container management |\n| bootc | `/overthink:bootc` | Bootable container testing and management |\n| comfyui | `/overthink:comfyui` | ComfyUI AI image generation server |\n| config | `/overthink:config` | System configuration (services, GPU, Docker, etc.) |\n| deploy | `/overthink:deploy` | Helm deployments to k3d (JupyterHub, KubeAI) |\n| fiftyone | `/overthink:fiftyone` | FiftyOne dataset visualization and management |\n| install | `/overthink:install` | System package and Flatpak installation |\n| jellyfin | `/overthink:jellyfin` | Jellyfin media server management |\n| jupyter | `/overthink:jupyter` | JupyterLab server management |\n| k3d | `/overthink:k3d` | Lightweight Kubernetes clusters in Podman |\n| localai | `/overthink:localai` | LocalAI inference server |\n| ollama | `/overthink:ollama` | Ollama LLM inference server |\n| openwebui | `/overthink:openwebui` | Open WebUI chat interface for Ollama |\n| pods | `/overthink:pods` | Pod container lifecycle management |\n| portainer | `/overthink:portainer` | Portainer container management UI |\n| record | `/overthink:record` | Terminal recording with asciinema |\n| runners | `/overthink:runners` | GitHub Actions self-hosted runners |\n| tailscale | `/overthink:tailscale` | Tailscale service exposure |\n| test | `/overthink:test` | Testing and verification commands |\n| vm | `/overthink:vm` | Virtual machine management |\n\n## Usage Examples\n\n```bash\n# Ask Claude to help with Ollama\n/overthink:ollama\n# Claude will guide you through Ollama setup, model management, etc.\n\n# Configure JupyterLab\n/overthink:jupyter\n# Claude will help with JupyterLab installation, configuration, and troubleshooting\n\n# Set up GPU containers and system services\n/overthink:config\n# Claude will guide you through GPU passthrough and service configuration\n\n# Deploy applications to Kubernetes\n/overthink:deploy\n# Claude will help deploy JupyterHub or KubeAI to k3d clusters\n```\n\n## Installation\n\n### Manual Loading\n\n```bash\nclaude --plugin-dir /path/to/overthink-testing/plugins/overthink\n```\n\n### Permanent Configuration\n\nAdd to your Claude Code settings:\n\n```json\n{\n  \"plugins\": [\n    \"/path/to/overthink-testing/plugins/overthink\"\n  ]\n}\n```\n\n## Requirements\n\n- Overthink OS installed\n- `ujust` command available at `/usr/share/ublue-os/justfile`\n\n## Related\n\n- **overthink-dev**: Development tools for contributors (separate plugin)\n- **Documentation**: <https://bazzite.ai/>\n",
        "overthink-dev/README.md": "# overthink-dev Plugin\n\nClaude Code plugin for Overthink development with enforcement agents and development tools.\n\n## Purpose\n\nThis plugin provides:\n\n1. **Development skills** for building, testing, and maintaining Overthink\n2. **Enforcement agents** that ensure code quality and policy compliance\n3. **GitHub MCP integration** for repository operations\n\n## MCP Server\n\nThis plugin includes a GitHub MCP server for repository operations.\n\n**Tools available:**\n\n- Issues: `issue_read`, `issue_write`, `add_issue_comment`, `list_issues`, `search_issues`\n- Pull requests: `pull_request_read`, `list_pull_requests`, `search_pull_requests`\n- Workflows: `list_workflows`, `list_workflow_runs`, `get_workflow_run`, `get_job_logs`\n- Repository: `get_file_contents`, `list_commits`, `get_commit`, `list_branches`\n- Labels: `get_label`, `list_label`, `label_write`\n\n**Prerequisites:**\n\n- `github-mcp-server` available via direnv (installed in project)\n- `GITHUB_TOKEN` environment variable set\n\n## Available Skills (6)\n\n| Skill | Command | Description |\n|-------|---------|-------------|\n| build | `/overthink-dev:build` | OS image building with Podman (`just build`) |\n| clean | `/overthink-dev:clean` | Cleanup build artifacts and caches (`just clean`) |\n| lfs | `/overthink-dev:lfs` | Git LFS file management (`just lfs`) |\n| overlay | `/overthink-dev:overlay` | Development overlay session management (`just overlay`) |\n| record | `/overthink-dev:record` | Batch documentation recording (`just record`) |\n| test | `/overthink-dev:test` | Runtime verification tests (`ujust test`) |\n\n## Enforcement Agents\n\nThese agents are automatically invoked to enforce development policies:\n\n### Blocking Agents (Must Pass)\n\n| Agent | Trigger | Purpose |\n|-------|---------|---------|\n| policy-enforcer | Before Edit/Write, commits | Verifies all policy compliance |\n| root-cause-analyzer | On errors | Mandatory 8-step error analysis |\n| testing-validator | Before claiming \"working\" | Confirms LOCAL testing completed |\n| justfile-validator | Editing .just files | Validates non-interactive support |\n| pre-commit-guardian | Before git commit | Ensures 100% hook pass rate |\n| documentation-validator | Editing docs/*.md | Validates MyST syntax |\n| config-integrity-enforcer | Editing ~/.config/* | Blocks editing output configs |\n| pixi-lock-enforcer | Editing pixi.lock | Blocks manual lock edits |\n| sudo-usage-enforcer | sudo ujust detected | Blocks external sudo elevation |\n| overlay-testing-enforcer | just -f testing | Blocks direct justfile testing |\n\n### Advisory Agents\n\n| Agent | Trigger | Purpose |\n|-------|---------|---------|\n| architecture-advisor | \"Why?\" questions | Explains immutable OS design |\n| buildcache-validator | Build file changes | Analyzes build cache impact |\n| code-research | Architectural questions | Deep codebase analysis |\n| github-actions | CI status queries | Reports workflow status |\n\n## Usage Examples\n\n```bash\n# Build the OS image\n/overthink-dev:build\n# Claude will help with image building, troubleshooting, etc.\n\n# Enable development overlay mode\n/overthink-dev:overlay\n# Claude will guide you through overlay setup for live justfile editing\n\n# Run runtime verification tests\n/overthink-dev:test\n# Claude will help verify GPU, services, and pod functionality\n\n# Clean up after development\n/overthink-dev:clean\n# Claude will help clean build artifacts and caches\n\n# Manage Git LFS files\n/overthink-dev:lfs\n# Claude will help with large file checkout and verification\n\n# Generate documentation recordings\n/overthink-dev:record\n# Claude will help batch-record ujust commands for docs\n```\n\n## Installation\n\n### Manual Loading\n\n```bash\n# Load both plugins for full development experience\nclaude --plugin-dir ./plugins/overthink --plugin-dir ./plugins/overthink-dev\n```\n\n### Permanent Configuration\n\nAdd to your Claude Code settings:\n\n```json\n{\n  \"plugins\": [\n    \"/path/to/overthink-testing/plugins/overthink\",\n    \"/path/to/overthink-testing/plugins/overthink-dev\"\n  ]\n}\n```\n\n## Development Workflow\n\n1. **Enable overlay testing**: `just overlay refresh` (auto-enables if needed)\n2. **Make changes** to justfiles in `just/` directory\n3. **Refresh overlay**: `just overlay refresh`\n4. **Test with ujust**: `ujust <your-command>`\n5. **Verify LOCAL**: Check systemctl status, journalctl logs\n6. **Run pre-commit**: `pre-commit run --all-files`\n7. **Commit** (enforcement agents will verify)\n\n## Policies Enforced\n\n- LOCAL system verification required before claiming \"working\"\n- ~/.config files are outputs - edit source code instead\n- 100% pre-commit hook pass rate required\n- All commands must support non-interactive execution\n- .just files must be under 30K (split proactively at 25K)\n- Never use `sudo ujust` - handle sudo internally\n- Never use `just -f` for testing - use overlay method\n- Never edit pixi.lock manually - regenerate via `pixi install`\n\n## Related\n\n- **overthink**: OS user skills (separate plugin)\n- **CLAUDE.md**: Full policy documentation\n- **AGENTS.md**: Operational commands and architecture\n",
        "overthink-jupyter/README.md": "# overthink-jupyter\n\nML/AI development workflows for JupyterLab - Ollama API, LangChain, RAG, fine-tuning, and model optimization.\n\n## Overview\n\nThis plugin provides skills for **ML/AI workflows** in JupyterLab, including Ollama API operations for LLM inference.\n\n## MCP Server\n\nThis plugin includes a Jupyter MCP server that connects to a running JupyterLab instance.\n\n**Configuration:**\n\n- URL: `http://127.0.0.1:8888/mcp`\n- Type: HTTP-based MCP server\n\n**Prerequisite:** JupyterLab must be running with MCP support enabled (via `ujust jupyter start`).\n\n**Note:** This plugin is designed to work with the `overthink-pod-jupyter` container or any JupyterLab environment with the required packages.\n\n## Skills\n\n### Ollama API Operations\n\n| Skill | Description |\n|-------|-------------|\n| `chat` | Direct REST API operations using requests library |\n| `ollama` | Official `ollama` Python library usage |\n| `openai` | OpenAI compatibility layer for migration |\n| `gpu` | GPU monitoring, VRAM usage, and inference metrics |\n| `huggingface` | Import GGUF models from HuggingFace |\n\n### ML/AI Development\n\n| Skill | Description |\n|-------|-------------|\n| `langchain` | LangChain framework - prompts, chains, and model wrappers |\n| `rag` | Retrieval-Augmented Generation with vector stores |\n| `evaluation` | LLM evaluation and prompt optimization with Evidently.ai |\n| `transformers` | Transformer architecture concepts (attention, FFN) |\n| `finetuning` | Model fine-tuning with PyTorch and HuggingFace Trainer |\n| `quantization` | Model quantization for efficient inference |\n| `peft` | Parameter-efficient fine-tuning (LoRA, Unsloth) |\n| `sft` | Supervised Fine-Tuning with SFTTrainer and Unsloth |\n| `grpo` | Group Relative Policy Optimization for RLHF |\n| `dpo` | Direct Preference Optimization from preference pairs |\n| `reward` | Reward model training for RLHF pipelines |\n| `rloo` | Reinforcement Learning with Leave-One-Out baseline |\n| `inference` | Fast inference with vLLM and thinking model parsing |\n| `vision` | Vision model fine-tuning with FastVisionModel |\n| `qlora` | Advanced QLoRA experiments (alpha, rank, modules) |\n\n## MCP Server Tools\n\n**Connection:** `http://127.0.0.1:8888/mcp`\n\n| Tool | Description |\n|------|-------------|\n| `mcp__jupyter__list_files` | List files in Jupyter server filesystem |\n| `mcp__jupyter__list_kernels` | List available kernels |\n| `mcp__jupyter__use_notebook` | Activate a notebook for operations |\n| `mcp__jupyter__read_notebook` | Read notebook cells and structure |\n| `mcp__jupyter__insert_cell` | Insert new cells |\n| `mcp__jupyter__execute_cell` | Execute notebook cells |\n| `mcp__jupyter__execute_code` | Execute code directly in kernel |\n\nThe MCP server starts automatically when this plugin is enabled.\n\n## Prerequisites\n\n**JupyterLab Environment:**\n\n- JupyterLab server running at `http://localhost:8888` with MCP enabled\n- GPU access configured if using GPU-accelerated training\n\n**Ollama (for inference):**\n\n- Ollama server running (default: `http://ollama:11434` or `OLLAMA_HOST` env var)\n- Model available (pull via API or Python library)\n\n**Note:** All required Python packages are pre-installed in the `overthink-pod-jupyter` container.\n\n## Quick Start\n\n### Ollama Python Library\n\n```python\nimport ollama\n\n# Generate text\nresult = ollama.generate(model=\"llama3.2:latest\", prompt=\"Hello!\")\nprint(result[\"response\"])\n\n# Chat completion\nresponse = ollama.chat(\n    model=\"llama3.2:latest\",\n    messages=[{\"role\": \"user\", \"content\": \"What is Python?\"}]\n)\nprint(response[\"message\"][\"content\"])\n```\n\n### Critical Import Order (for Fine-tuning)\n\n```python\n# CRITICAL: Import unsloth FIRST for proper TRL patching\nimport unsloth\nfrom unsloth import FastLanguageModel, is_bf16_supported\n\n# Then other imports\nfrom trl import SFTTrainer, SFTConfig\n```\n\n### LangChain with Ollama\n\n```python\nimport os\nfrom langchain_openai import ChatOpenAI\n\nOLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ollama:11434\")\n\nllm = ChatOpenAI(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\",\n    model=\"hf.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF:Q4_K_M\"\n)\n\nresponse = llm.invoke(\"What is machine learning?\")\nprint(response.content)\n```\n\n### RAG Pipeline\n\n```python\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(\n    base_url=f\"{OLLAMA_HOST}/v1\",\n    api_key=\"ollama\"\n)\n\nvectorstore = Chroma.from_texts(documents, embeddings)\nretriever = vectorstore.as_retriever()\n```\n\n### Fine-tuning with LoRA\n\n```python\nfrom peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05\n)\n\nmodel = get_peft_model(base_model, lora_config)\n```\n"
      },
      "plugins": [
        {
          "name": "overthink",
          "source": "./overthink",
          "description": "Skills for managing AI/ML services on Overthink OS via ujust commands",
          "version": "1.0.0",
          "author": {
            "name": "atrawog"
          },
          "homepage": "https://github.com/atrawog/overthink",
          "repository": "https://github.com/atrawog/overthink-plugins",
          "license": "MIT",
          "keywords": [
            "overthink",
            "ai",
            "ml",
            "immutable-os",
            "ujust"
          ],
          "category": "productivity",
          "categories": [
            "ai",
            "immutable-os",
            "ml",
            "overthink",
            "productivity",
            "ujust"
          ],
          "install_commands": [
            "/plugin marketplace add atrawog/overthink-plugins",
            "/plugin install overthink@overthink-plugins"
          ]
        },
        {
          "name": "overthink-dev",
          "source": "./overthink-dev",
          "description": "Development tools and enforcement agents for Overthink contributors",
          "version": "1.0.0",
          "author": {
            "name": "atrawog"
          },
          "homepage": "https://github.com/atrawog/overthink",
          "repository": "https://github.com/atrawog/overthink-plugins",
          "license": "MIT",
          "keywords": [
            "overthink",
            "development",
            "testing",
            "enforcement"
          ],
          "category": "development",
          "categories": [
            "development",
            "enforcement",
            "overthink",
            "testing"
          ],
          "install_commands": [
            "/plugin marketplace add atrawog/overthink-plugins",
            "/plugin install overthink-dev@overthink-plugins"
          ]
        },
        {
          "name": "overthink-jupyter",
          "source": "./overthink-jupyter",
          "description": "ML/AI development workflows for JupyterLab - LangChain, RAG, fine-tuning, and model optimization",
          "version": "1.0.0",
          "author": {
            "name": "atrawog"
          },
          "homepage": "https://github.com/atrawog/overthink",
          "repository": "https://github.com/atrawog/overthink-plugins",
          "license": "MIT",
          "keywords": [
            "jupyter",
            "langchain",
            "rag",
            "fine-tuning",
            "ml"
          ],
          "category": "development",
          "categories": [
            "development",
            "fine-tuning",
            "jupyter",
            "langchain",
            "ml",
            "rag"
          ],
          "install_commands": [
            "/plugin marketplace add atrawog/overthink-plugins",
            "/plugin install overthink-jupyter@overthink-plugins"
          ]
        }
      ]
    }
  ]
}