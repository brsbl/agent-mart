{
  "author": {
    "id": "MakFly",
    "display_name": "Kev",
    "avatar_url": "https://avatars.githubusercontent.com/u/6107225?u=89bffdbb941d488e5f8fb91e124033ef704999d2&v=4"
  },
  "marketplaces": [
    {
      "name": "jarrodwatts-claude-delegator",
      "version": null,
      "description": "GPT expert subagents for Claude Code via Codex CLI. Five specialized experts: Architect, Plan Reviewer, Scope Analyst, Code Reviewer, Security Analyst.",
      "repo_full_name": "MakFly/glm-delegator",
      "repo_url": "https://github.com/MakFly/glm-delegator",
      "repo_description": "GLM-4.7 expert subagents for Claude Code ‚Äî Fork of claude-delegator adapted for Z.AI's GLM models",
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-02-13T15:36:07Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"jarrodwatts-claude-delegator\",\n  \"owner\": {\n    \"name\": \"Jarrod Watts\",\n    \"url\": \"https://github.com/jarrodwatts\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"claude-delegator\",\n      \"source\": \"./\",\n      \"description\": \"GPT expert subagents for Claude Code via Codex CLI. Five specialized experts: Architect, Plan Reviewer, Scope Analyst, Code Reviewer, Security Analyst.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Jarrod Watts\",\n        \"url\": \"https://github.com/jarrodwatts\"\n      },\n      \"homepage\": \"https://github.com/jarrodwatts/claude-delegator\",\n      \"repository\": \"https://github.com/jarrodwatts/claude-delegator\",\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"delegation\",\n        \"mcp\",\n        \"codex\",\n        \"gpt\",\n        \"openai\",\n        \"experts\",\n        \"subagents\",\n        \"orchestration\"\n      ]\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"glm-delegator\",\n  \"description\": \"GLM (4.7/5) expert subagents for Claude Code via Z.AI API. Five specialized experts: Architect, Plan Reviewer, Scope Analyst, Code Reviewer (EN/FR/CN), Security Analyst.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Forked from claude-delegator by Jarrod Watts\",\n    \"url\": \"https://github.com/jarrodwatts/claude-delegator\"\n  },\n  \"homepage\": \"https://github.com/kev/glm-delegator\",\n  \"repository\": \"https://github.com/kev/glm-delegator\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"delegation\", \"mcp\", \"glm\", \"z-ai\", \"anthropic-compatible\", \"experts\", \"subagents\", \"orchestration\", \"chinese\", \"french\", \"multilingual\"]\n}\n",
        "README.md": "# LLM Delegator\n\n> Multi-provider LLM expert subagents for Claude Code ‚Äî A fork of [claude-delegator](https://github.com/jarrodwatts/claude-delegator) supporting **any LLM backend**\n\nLLM expert subagents for Claude Code. Five specialists that can analyze AND implement‚Äîarchitecture, security, code review, and more.\n\n**Supports:** Anthropic Claude, OpenAI GPT, GLM-4.7, GLM-5, Ollama, Groq, DeepInfra, and any OpenAI/Anthropic-compatible API.\n\n[![License](https://img.shields.io/github/license/MakFly/glm-delegator?v=2)](LICENSE)\n[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n\n## What is LLM Delegator?\n\nClaude gains a team of LLM specialists via MCP. Each expert has a distinct specialty and can advise OR implement.\n\n| What You Get | Why It Matters |\n|--------------|----------------|\n| **5 domain experts** | Right specialist for each problem type |\n| **Dual mode** | Experts can analyze (read-only) or implement (write) |\n| **Multi-provider** | Use Claude, GPT-4, GLM (4.7/5), Ollama, or any compatible API |\n| **Auto-routing** | Claude detects when to delegate based on your request |\n| **Synthesized responses** | Claude interprets LLM output, never raw passthrough |\n| **Multilingual** | Code Review supports EN/FR/CN (‰∏≠Êñá) |\n\n### The Experts\n\n| Expert | What They Do | Example Triggers |\n|--------|--------------|------------------|\n| **Architect** | System design, tradeoffs, complex debugging | \"How should I structure this?\" / \"What are the tradeoffs?\" |\n| **Plan Reviewer** | Validate plans before you start | \"Review this migration plan\" / \"Is this approach sound?\" |\n| **Scope Analyst** | Catch ambiguities early | \"What am I missing?\" / \"Clarify the scope\" |\n| **Code Reviewer** | Find bugs, improve quality (EN/FR/CN) | \"Review this PR\" / \"What's wrong with this?\" |\n| **Security Analyst** | Vulnerabilities, threat modeling | \"Is this secure?\" / \"Harden this endpoint\" |\n\n## Differences from claude-delegator\n\n| Feature | claude-delegator | llm-delegator |\n|---------|------------------|---------------|\n| Backend | Codex GPT-5.2 only | **Any LLM provider** |\n| Configuration | CLI args only | CLI args (same model) |\n| Providers | Single | Multi-provider (OpenAI, Anthropic, Ollama, etc.) |\n| Code Review | English only | **EN/FR/CN multilingual** |\n| Security | OWASP | OWASP + Chinese MLPS standards |\n| License | MIT | MIT |\n\n## Install\n\n### Prerequisites\n\n- **Python 3.8+** for the MCP server\n- **API Key** for your chosen provider (Anthropic, OpenAI, Z.AI, etc.)\n- **httpx** library - `pip install -r requirements.txt`\n\n### Step 1: Install Dependencies\n\n```bash\ncd glm-delegator\npip install -r requirements.txt\n```\n\n### Step 2: Configure API Key\n\nSet your API key as an environment variable:\n\n```bash\n# Anthropic Claude\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# GLM via Z.AI\nexport GLM_API_KEY=\"your_z_ai_api_key_here\"\n\n# Groq\nexport GROQ_API_KEY=\"gsk_...\"\n```\n\nFor persistent configuration, add to your `~/.bashrc` or `~/.zshrc`:\n\n```bash\necho 'export ANTHROPIC_API_KEY=\"sk-ant-...\"' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n### Step 3: Register MCP Server\n\nAdd to `~/.claude.json` (or `~/.claude/settings.json`):\n\n#### Using Anthropic Claude\n\n```json\n{\n  \"mcpServers\": {\n    \"claude-experts\": {\n      \"type\": \"stdio\",\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/glm-delegator/glm_mcp_server.py\",\n        \"--provider\", \"anthropic-compatible\",\n        \"--base-url\", \"https://api.anthropic.com/v1\",\n        \"--api-key\", \"$ANTHROPIC_API_KEY\",\n        \"--model\", \"claude-sonnet-4-20250514\"\n      ]\n    }\n  }\n}\n```\n\n#### Using OpenAI GPT\n\n```json\n{\n  \"mcpServers\": {\n    \"openai-experts\": {\n      \"type\": \"stdio\",\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/glm-delegator/glm_mcp_server.py\",\n        \"--provider\", \"openai-compatible\",\n        \"--base-url\", \"https://api.openai.com/v1\",\n        \"--api-key\", \"$OPENAI_API_KEY\",\n        \"--model\", \"gpt-4o\"\n      ]\n    }\n  }\n}\n```\n\n#### Using Ollama (Local)\n\n```json\n{\n  \"mcpServers\": {\n    \"ollama-experts\": {\n      \"type\": \"stdio\",\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/glm-delegator/glm_mcp_server.py\",\n        \"--provider\", \"openai-compatible\",\n        \"--base-url\", \"http://localhost:11434/v1\",\n        \"--model\", \"llama3.1\"\n      ]\n    }\n  }\n}\n```\n\n#### Using GLM via Z.AI\n\n```json\n{\n  \"mcpServers\": {\n    \"glm-experts\": {\n      \"type\": \"stdio\",\n      \"command\": \"python3\",\n      \"args\": [\n        \"/path/to/glm-delegator/glm_mcp_server.py\",\n        \"--provider\", \"anthropic-compatible\",\n        \"--base-url\", \"https://api.z.ai/api/anthropic\",\n        \"--api-key\", \"$GLM_API_KEY\",\n        \"--model\", \"glm-5\"\n      ]\n    }\n  }\n}\n```\n\n> **Note:** Replace `glm-5` with `glm-4.7` if you prefer the previous generation model.\n\n### Step 4: Restart Claude Code\n\nRestart Claude Code to load the MCP server.\n\n## Supported Providers\n\n| Provider | Type | Example Model |\n|----------|------|---------------|\n| **Anthropic** | anthropic-compatible | claude-sonnet-4-20250514 |\n| **OpenAI** | openai-compatible | gpt-4o |\n| **GLM (Z.AI)** | anthropic-compatible | glm-4.7, glm-5 |\n| **Ollama** | openai-compatible | llama3.1 |\n| **Groq** | openai-compatible | llama-3.3-70b-versatile |\n| **DeepInfra** | openai-compatible | deepseek-ai/DeepSeek-V3 |\n| **TogetherAI** | openai-compatible | meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo |\n| **vLLM** | openai-compatible | (custom) |\n| **LM Studio** | openai-compatible | (custom) |\n\n> **Any OpenAI-compatible or Anthropic-compatible API will work!**\n\n## Command-Line Options\n\n```\npython3 glm_mcp_server.py --help\n\nOptions:\n  -p, --provider      Provider type: openai-compatible or anthropic-compatible\n  -u, --base-url      Base URL of the API\n  -k, --api-key       API key (default: GLM_API_KEY or Z_AI_API_KEY env var)\n  -m, --model         Model name\n  --api-version       API version for Anthropic-compatible (default: 2023-06-01)\n  --timeout           Request timeout in seconds (default: 600)\n  --max-tokens        Maximum tokens for responses (default: 8192)\n  --debug             Enable debug logging\n```\n\n## Usage\n\nOnce installed, delegation happens automatically. Claude Code will detect when to delegate based on your request.\n\n### Explicit Delegation\n\nYou can also explicitly request LLM expert help:\n\n```\n\"Ask the architect to review this authentication flow\"\n\"Use the code reviewer to analyze this function\"\n\"Have the security analyst check this code\"\n```\n\n### Manual Tool Call\n\nYou can call the MCP tools directly:\n\n```typescript\n// Advisory mode (analysis only)\nmcp__glm-delegator__glm_architect({\n  task: \"Analyze tradeoffs between Redis and in-memory caching\",\n  mode: \"advisory\",\n  context: \"Building a session store for a Node.js app\"\n})\n\n// Implementation mode (make changes)\nmcp__glm-delegator__glm_code_reviewer({\n  task: \"Fix the SQL injection vulnerability in user.ts\",\n  mode: \"implementation\",\n  files: [\"src/routes/user.ts\"]\n})\n```\n\n## How It Works\n\n```\nYou: \"Is this authentication flow secure?\"\n                    ‚Üì\nClaude: [Detects security question ‚Üí selects Security Analyst]\n                    ‚Üì\n        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n        ‚îÇ  mcp__glm-delegator__glm_   ‚îÇ\n        ‚îÇ  security_analyst           ‚îÇ\n        ‚îÇ  ‚Üí Security Analyst prompt  ‚îÇ\n        ‚îÇ  ‚Üí LLM analyzes your code   ‚îÇ\n        ‚îÇ    via configured API       ‚îÇ\n        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                    ‚Üì\nClaude: \"Based on the analysis, I found 3 issues...\"\n        [Synthesizes response, applies judgment]\n```\n\n## Configuration\n\n### Command-Line Arguments\n\n| Argument | Short | Default | Description |\n|----------|-------|---------|-------------|\n| `--provider` | `-p` | `anthropic-compatible` | Provider type |\n| `--base-url` | `-u` | `https://api.z.ai/api/anthropic` | API base URL |\n| `--api-key` | `-k` | `$GLM_API_KEY` | API key |\n| `--model` | `-m` | `glm-4.7` | Model name (e.g. glm-4.7, glm-5) |\n| `--api-version` | - | `2023-06-01` | Anthropic API version |\n| `--timeout` | - | `600` | Timeout (seconds) |\n| `--max-tokens` | - | `8192` | Max tokens per response |\n| `--debug` | - | `false` | Debug logging |\n\n### Operating Modes\n\nEvery expert supports two modes based on the task:\n\n| Mode | Use When |\n|------|----------|\n| **Advisory** | Analysis, recommendations, reviews |\n| **Implementation** | Making changes, fixing issues |\n\nClaude automatically selects the mode based on your request.\n\n## Multiple Servers\n\nYou can configure multiple servers with different providers:\n\n```json\n{\n  \"mcpServers\": {\n    \"claude-experts\": {\n      \"type\": \"stdio\",\n      \"command\": \"python3\",\n      \"args\": [\"/path/to/glm_mcp_server.py\", \"--provider\", \"anthropic-compatible\", \"--base-url\", \"https://api.anthropic.com/v1\", \"--api-key\", \"$ANTHROPIC_API_KEY\", \"--model\", \"claude-sonnet-4-20250514\"]\n    },\n    \"ollama-local\": {\n      \"type\": \"stdio\",\n      \"command\": \"python3\",\n      \"args\": [\"/path/to/glm_mcp_server.py\", \"--provider\", \"openai-compatible\", \"--base-url\", \"http://localhost:11434/v1\", \"--model\", \"llama3.1\"]\n    }\n  }\n}\n```\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| MCP server not found | Restart Claude Code after configuration |\n| Authentication failed | Verify your API key is correct and active |\n| Tool not appearing | Check `~/.claude.json` has correct entry |\n| Expert not triggered | Try explicit: \"Ask the architect to review this\" |\n| Python not found | Ensure Python 3.8+ is in your PATH |\n\n### Debug Mode\n\nRun with `--debug` to see detailed logs:\n\n```bash\npython3 glm_mcp_server.py --debug --provider anthropic-compatible --api-key $ANTHROPIC_API_KEY\n```\n\n## Development\n\n```bash\ngit clone https://github.com/MakFly/glm-delegator\ncd glm-delegator\n\n# Install dependencies\npip install -r requirements.txt\n\n# Test locally with a specific provider\npython3 glm_mcp_server.py \\\n  --provider openai-compatible \\\n  --base-url http://localhost:11434/v1 \\\n  --model llama3.1 \\\n  --debug\n```\n\n## Architecture\n\n### Components\n\n| Component | Purpose |\n|-----------|---------|\n| `glm_mcp_server.py` | MCP server with argparse-based configuration |\n| `providers.py` | Provider abstraction layer (OpenAI/Anthropic-compatible) |\n| `prompts/*.md` | Expert personality definitions (for reference) |\n| `rules/*.md` | Delegation rules and triggers (for reference) |\n| `.claude-plugin/plugin.json` | Plugin metadata |\n\n### API Flow\n\n```\nClaude Code ‚Üí MCP Request ‚Üí glm_mcp_server.py\n                                ‚Üì\n                        Provider Layer (providers.py)\n                                ‚Üì\n                        Configured API (OpenAI/Anthropic/etc.)\n                                ‚Üì\n                            LLM Response\n                                ‚Üì\n                        Response ‚Üí Claude Code\n```\n\n## Documentation\n\n- **[USAGE.md](USAGE.md)** - üìñ Guide d'utilisation complet avec exemples et sc√©narios\n- **[BACKEND_CONFIG.md](BACKEND_CONFIG.md)** - Configuration multi-provider d√©taill√©e\n\n## Acknowledgments\n\n- Based on [claude-delegator](https://github.com/jarrodwatts/claude-delegator) by [Jarrod Watts](https://github.com/jarrodwatts)\n- Expert prompts adapted from [oh-my-opencode](https://github.com/code-yeongyu/oh-my-opencode) by [@code-yeongyu](https://github.com/code-yeongyu)\n- Uses Z.AI's [Anthropic-compatible API](https://docs.z.ai/devpack/mcp) for GLM support\n\n## License\n\nMIT ‚Äî see [LICENSE](LICENSE)\n"
      },
      "plugins": [
        {
          "name": "claude-delegator",
          "source": "./",
          "description": "GPT expert subagents for Claude Code via Codex CLI. Five specialized experts: Architect, Plan Reviewer, Scope Analyst, Code Reviewer, Security Analyst.",
          "version": "1.0.0",
          "author": {
            "name": "Jarrod Watts",
            "url": "https://github.com/jarrodwatts"
          },
          "homepage": "https://github.com/jarrodwatts/claude-delegator",
          "repository": "https://github.com/jarrodwatts/claude-delegator",
          "license": "MIT",
          "keywords": [
            "delegation",
            "mcp",
            "codex",
            "gpt",
            "openai",
            "experts",
            "subagents",
            "orchestration"
          ],
          "categories": [
            "codex",
            "delegation",
            "experts",
            "gpt",
            "mcp",
            "openai",
            "orchestration",
            "subagents"
          ],
          "install_commands": [
            "/plugin marketplace add MakFly/glm-delegator",
            "/plugin install claude-delegator@jarrodwatts-claude-delegator"
          ]
        }
      ]
    }
  ]
}