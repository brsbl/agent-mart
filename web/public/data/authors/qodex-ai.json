{
  "author": {
    "id": "qodex-ai",
    "display_name": "qodex-ai",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/146586267?v=4",
    "url": "https://github.com/qodex-ai",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 2,
      "total_commands": 0,
      "total_skills": 81,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "anthropic-agent-skills",
      "version": null,
      "description": "Anthropic example skills",
      "owner_info": {
        "name": "Keith Lazuka",
        "email": "klazuka@anthropic.com"
      },
      "keywords": [],
      "repo_full_name": "qodex-ai/ai-agent-skills",
      "repo_url": "https://github.com/qodex-ai/ai-agent-skills",
      "repo_description": "ai-agent-skills",
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2026-01-05T11:56:00Z",
        "created_at": "2025-12-21T07:28:27Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1539
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 5608
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/aesthetic-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/aesthetic-analysis/SKILL.md",
          "type": "blob",
          "size": 5689
        },
        {
          "path": "skills/animated-message-composer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/animated-message-composer/SKILL.md",
          "type": "blob",
          "size": 9614
        },
        {
          "path": "skills/application-quality-assurance",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/application-quality-assurance/SKILL.md",
          "type": "blob",
          "size": 3871
        },
        {
          "path": "skills/autonomous-agent-gaming",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-agent-gaming/README.md",
          "type": "blob",
          "size": 12807
        },
        {
          "path": "skills/autonomous-agent-gaming/SKILL.md",
          "type": "blob",
          "size": 15725
        },
        {
          "path": "skills/autonomous-cloud-orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/SKILL.md",
          "type": "blob",
          "size": 6000
        },
        {
          "path": "skills/autonomous-cloud-orchestration/cross-service",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/cross-service/credential-management.md",
          "type": "blob",
          "size": 13844
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/browser",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/browser/README.md",
          "type": "blob",
          "size": 10378
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/code-interpreter",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/code-interpreter/README.md",
          "type": "blob",
          "size": 7792
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/gateway",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/gateway/README.md",
          "type": "blob",
          "size": 3379
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/gateway/deployment-strategies.md",
          "type": "blob",
          "size": 11311
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/gateway/troubleshooting-guide.md",
          "type": "blob",
          "size": 10342
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/identity",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/identity/README.md",
          "type": "blob",
          "size": 6868
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/memory",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/memory/README.md",
          "type": "blob",
          "size": 8309
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/observability",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/observability/README.md",
          "type": "blob",
          "size": 14145
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/runtime",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/autonomous-cloud-orchestration/services/runtime/README.md",
          "type": "blob",
          "size": 6815
        },
        {
          "path": "skills/backend-database-specialist",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/backend-database-specialist/SKILL.md",
          "type": "blob",
          "size": 58959
        },
        {
          "path": "skills/branch-finalization",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/branch-finalization/SKILL.md",
          "type": "blob",
          "size": 4216
        },
        {
          "path": "skills/browser-automation-framework",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/browser-automation-framework/SKILL.md",
          "type": "blob",
          "size": 13715
        },
        {
          "path": "skills/capability-activation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/capability-activation/SKILL.md",
          "type": "blob",
          "size": 3618
        },
        {
          "path": "skills/capability-architect",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/capability-architect/SKILL.md",
          "type": "blob",
          "size": 17842
        },
        {
          "path": "skills/capability-architect/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/capability-architect/references/output-patterns.md",
          "type": "blob",
          "size": 1813
        },
        {
          "path": "skills/capability-architect/references/workflows.md",
          "type": "blob",
          "size": 818
        },
        {
          "path": "skills/capability-assessment",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/capability-assessment/SKILL.md",
          "type": "blob",
          "size": 15673
        },
        {
          "path": "skills/capability-documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/capability-documentation/SKILL.md",
          "type": "blob",
          "size": 22435
        },
        {
          "path": "skills/capability-documentation/anthropic-best-practices.md",
          "type": "blob",
          "size": 45825
        },
        {
          "path": "skills/capability-documentation/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/capability-documentation/examples/CLAUDE_MD_TESTING.md",
          "type": "blob",
          "size": 5423
        },
        {
          "path": "skills/capability-documentation/persuasion-principles.md",
          "type": "blob",
          "size": 5908
        },
        {
          "path": "skills/capability-documentation/testing-skills-with-subagents.md",
          "type": "blob",
          "size": 12557
        },
        {
          "path": "skills/chat-with-arxiv",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/chat-with-arxiv/SKILL.md",
          "type": "blob",
          "size": 5268
        },
        {
          "path": "skills/cloud-expense-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cloud-expense-management/SKILL.md",
          "type": "blob",
          "size": 10759
        },
        {
          "path": "skills/cloud-expense-management/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cloud-expense-management/references/cloudwatch-alarms.md",
          "type": "blob",
          "size": 13736
        },
        {
          "path": "skills/cloud-expense-management/references/operations-patterns.md",
          "type": "blob",
          "size": 10477
        },
        {
          "path": "skills/collaborative-document-creation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/collaborative-document-creation/SKILL.md",
          "type": "blob",
          "size": 15598
        },
        {
          "path": "skills/collaborative-ideation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/collaborative-ideation/SKILL.md",
          "type": "blob",
          "size": 2530
        },
        {
          "path": "skills/component-interface-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/component-interface-design/SKILL.md",
          "type": "blob",
          "size": 7640
        },
        {
          "path": "skills/content-harvest",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/content-harvest/SKILL.md",
          "type": "blob",
          "size": 9605
        },
        {
          "path": "skills/creative-generation-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/creative-generation-agent/README.md",
          "type": "blob",
          "size": 10833
        },
        {
          "path": "skills/creative-generation-agent/SKILL.md",
          "type": "blob",
          "size": 9862
        },
        {
          "path": "skills/data-organization-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/data-organization-system/SKILL.md",
          "type": "blob",
          "size": 11261
        },
        {
          "path": "skills/deep-research-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/deep-research-agent/SKILL.md",
          "type": "blob",
          "size": 4339
        },
        {
          "path": "skills/delegated-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/delegated-development/SKILL.md",
          "type": "blob",
          "size": 9908
        },
        {
          "path": "skills/delegated-development/code-quality-reviewer-prompt.md",
          "type": "blob",
          "size": 630
        },
        {
          "path": "skills/delegated-development/implementer-prompt.md",
          "type": "blob",
          "size": 2195
        },
        {
          "path": "skills/delegated-development/spec-reviewer-prompt.md",
          "type": "blob",
          "size": 1999
        },
        {
          "path": "skills/deployment-automation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/deployment-automation/SKILL.md",
          "type": "blob",
          "size": 1902
        },
        {
          "path": "skills/design-system-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/design-system-generator/SKILL.md",
          "type": "blob",
          "size": 3018
        },
        {
          "path": "skills/design-system-generator/themes",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/design-system-generator/themes/arctic-frost.md",
          "type": "blob",
          "size": 544
        },
        {
          "path": "skills/design-system-generator/themes/botanical-garden.md",
          "type": "blob",
          "size": 519
        },
        {
          "path": "skills/design-system-generator/themes/desert-rose.md",
          "type": "blob",
          "size": 496
        },
        {
          "path": "skills/design-system-generator/themes/forest-canopy.md",
          "type": "blob",
          "size": 506
        },
        {
          "path": "skills/design-system-generator/themes/golden-hour.md",
          "type": "blob",
          "size": 528
        },
        {
          "path": "skills/design-system-generator/themes/midnight-galaxy.md",
          "type": "blob",
          "size": 513
        },
        {
          "path": "skills/design-system-generator/themes/modern-minimalist.md",
          "type": "blob",
          "size": 549
        },
        {
          "path": "skills/design-system-generator/themes/ocean-depths.md",
          "type": "blob",
          "size": 555
        },
        {
          "path": "skills/design-system-generator/themes/sunset-boulevard.md",
          "type": "blob",
          "size": 558
        },
        {
          "path": "skills/design-system-generator/themes/tech-innovation.md",
          "type": "blob",
          "size": 547
        },
        {
          "path": "skills/diagnostic-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/diagnostic-analysis/CREATION-LOG.md",
          "type": "blob",
          "size": 4268
        },
        {
          "path": "skills/diagnostic-analysis/SKILL.md",
          "type": "blob",
          "size": 9971
        },
        {
          "path": "skills/diagnostic-analysis/condition-based-waiting.md",
          "type": "blob",
          "size": 3516
        },
        {
          "path": "skills/diagnostic-analysis/defense-in-depth.md",
          "type": "blob",
          "size": 3650
        },
        {
          "path": "skills/diagnostic-analysis/root-cause-tracing.md",
          "type": "blob",
          "size": 5327
        },
        {
          "path": "skills/diagnostic-analysis/test-academic.md",
          "type": "blob",
          "size": 653
        },
        {
          "path": "skills/diagnostic-analysis/test-pressure-1.md",
          "type": "blob",
          "size": 1900
        },
        {
          "path": "skills/diagnostic-analysis/test-pressure-2.md",
          "type": "blob",
          "size": 2283
        },
        {
          "path": "skills/diagnostic-analysis/test-pressure-3.md",
          "type": "blob",
          "size": 2692
        },
        {
          "path": "skills/directive-synthesis",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/directive-synthesis/SKILL.md",
          "type": "blob",
          "size": 13014
        },
        {
          "path": "skills/discussion-intelligence",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/discussion-intelligence/SKILL.md",
          "type": "blob",
          "size": 10039
        },
        {
          "path": "skills/distributed-task-execution",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/distributed-task-execution/SKILL.md",
          "type": "blob",
          "size": 6178
        },
        {
          "path": "skills/document-chat-interface",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/document-chat-interface/SKILL.md",
          "type": "blob",
          "size": 11227
        },
        {
          "path": "skills/domain-naming-engine",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/domain-naming-engine/SKILL.md",
          "type": "blob",
          "size": 5707
        },
        {
          "path": "skills/ecommerce-platform-specialist",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/ecommerce-platform-specialist/SKILL.md",
          "type": "blob",
          "size": 13263
        },
        {
          "path": "skills/event-driven-serverless-systems",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/event-driven-serverless-systems/SKILL.md",
          "type": "blob",
          "size": 20985
        },
        {
          "path": "skills/event-driven-serverless-systems/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/event-driven-serverless-systems/references/deployment-best-practices.md",
          "type": "blob",
          "size": 19664
        },
        {
          "path": "skills/event-driven-serverless-systems/references/eda-patterns.md",
          "type": "blob",
          "size": 24678
        },
        {
          "path": "skills/event-driven-serverless-systems/references/observability-best-practices.md",
          "type": "blob",
          "size": 19049
        },
        {
          "path": "skills/event-driven-serverless-systems/references/performance-optimization.md",
          "type": "blob",
          "size": 16776
        },
        {
          "path": "skills/event-driven-serverless-systems/references/security-best-practices.md",
          "type": "blob",
          "size": 15257
        },
        {
          "path": "skills/event-driven-serverless-systems/references/serverless-patterns.md",
          "type": "blob",
          "size": 20841
        },
        {
          "path": "skills/feedback-application",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/feedback-application/SKILL.md",
          "type": "blob",
          "size": 3632
        },
        {
          "path": "skills/financial-analysis-agent",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/financial-analysis-agent/SKILL.md",
          "type": "blob",
          "size": 3380
        },
        {
          "path": "skills/financial-document-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/financial-document-management/SKILL.md",
          "type": "blob",
          "size": 11953
        },
        {
          "path": "skills/genealogical-documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/genealogical-documentation/SKILL.md",
          "type": "blob",
          "size": 16688
        },
        {
          "path": "skills/genealogical-documentation/assets",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/genealogical-documentation/assets/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/genealogical-documentation/assets/templates/citation-template.md",
          "type": "blob",
          "size": 4112
        },
        {
          "path": "skills/genealogical-documentation/assets/templates/evidence-analysis-template.md",
          "type": "blob",
          "size": 7890
        },
        {
          "path": "skills/genealogical-documentation/assets/templates/research-log-template.md",
          "type": "blob",
          "size": 1615
        },
        {
          "path": "skills/genealogical-documentation/assets/templates/research-plan-template.md",
          "type": "blob",
          "size": 2700
        },
        {
          "path": "skills/genealogical-documentation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/genealogical-documentation/references/citation-templates.md",
          "type": "blob",
          "size": 11302
        },
        {
          "path": "skills/genealogical-documentation/references/evidence-evaluation.md",
          "type": "blob",
          "size": 14008
        },
        {
          "path": "skills/genealogical-documentation/references/gps-guidelines.md",
          "type": "blob",
          "size": 14580
        },
        {
          "path": "skills/genealogical-documentation/references/research-log-guidance.md",
          "type": "blob",
          "size": 12299
        },
        {
          "path": "skills/genealogical-documentation/references/research-plan-guidance.md",
          "type": "blob",
          "size": 17958
        },
        {
          "path": "skills/genealogical-documentation/references/research-strategies.md",
          "type": "blob",
          "size": 13588
        },
        {
          "path": "skills/generate-swagger-docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/generate-swagger-docs/SKILL.md",
          "type": "blob",
          "size": 4163
        },
        {
          "path": "skills/identity-framework",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/identity-framework/SKILL.md",
          "type": "blob",
          "size": 2176
        },
        {
          "path": "skills/infrastructure-code-synthesis",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/infrastructure-code-synthesis/SKILL.md",
          "type": "blob",
          "size": 9778
        },
        {
          "path": "skills/infrastructure-code-synthesis/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/infrastructure-code-synthesis/references/cdk-patterns.md",
          "type": "blob",
          "size": 9852
        },
        {
          "path": "skills/interactive-component-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/interactive-component-creator/SKILL.md",
          "type": "blob",
          "size": 2945
        },
        {
          "path": "skills/knowledge-distribution",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/knowledge-distribution/SKILL.md",
          "type": "blob",
          "size": 2929
        },
        {
          "path": "skills/legal-document-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/legal-document-analyzer/SKILL.md",
          "type": "blob",
          "size": 3091
        },
        {
          "path": "skills/llm-fine-tuning-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/llm-fine-tuning-guide/README.md",
          "type": "blob",
          "size": 2464
        },
        {
          "path": "skills/llm-fine-tuning-guide/SKILL.md",
          "type": "blob",
          "size": 17378
        },
        {
          "path": "skills/market-intelligence-gather",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/market-intelligence-gather/SKILL.md",
          "type": "blob",
          "size": 7900
        },
        {
          "path": "skills/media-retrieval",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/media-retrieval/SKILL.md",
          "type": "blob",
          "size": 2727
        },
        {
          "path": "skills/media-transformation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/media-transformation/SKILL.md",
          "type": "blob",
          "size": 8637
        },
        {
          "path": "skills/meeting-record-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/meeting-record-system/SKILL.md",
          "type": "blob",
          "size": 11132
        },
        {
          "path": "skills/meeting-record-system/evaluations",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/meeting-record-system/evaluations/README.md",
          "type": "blob",
          "size": 3994
        },
        {
          "path": "skills/meeting-record-system/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/meeting-record-system/examples/customer-meeting.md",
          "type": "blob",
          "size": 3144
        },
        {
          "path": "skills/meeting-record-system/examples/executive-review.md",
          "type": "blob",
          "size": 2163
        },
        {
          "path": "skills/meeting-record-system/examples/project-decision.md",
          "type": "blob",
          "size": 12896
        },
        {
          "path": "skills/meeting-record-system/examples/sprint-planning.md",
          "type": "blob",
          "size": 2109
        },
        {
          "path": "skills/meeting-record-system/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/meeting-record-system/reference/brainstorming-template.md",
          "type": "blob",
          "size": 1494
        },
        {
          "path": "skills/meeting-record-system/reference/decision-meeting-template.md",
          "type": "blob",
          "size": 1834
        },
        {
          "path": "skills/meeting-record-system/reference/one-on-one-template.md",
          "type": "blob",
          "size": 940
        },
        {
          "path": "skills/meeting-record-system/reference/retrospective-template.md",
          "type": "blob",
          "size": 941
        },
        {
          "path": "skills/meeting-record-system/reference/sprint-planning-template.md",
          "type": "blob",
          "size": 1360
        },
        {
          "path": "skills/meeting-record-system/reference/status-update-template.md",
          "type": "blob",
          "size": 1368
        },
        {
          "path": "skills/meeting-record-system/reference/template-selection-guide.md",
          "type": "blob",
          "size": 2277
        },
        {
          "path": "skills/mobile-app-interface",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/mobile-app-interface/SKILL.md",
          "type": "blob",
          "size": 10246
        },
        {
          "path": "skills/mobile-platform-specialist",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/mobile-platform-specialist/SKILL.md",
          "type": "blob",
          "size": 6211
        },
        {
          "path": "skills/multi-agent-orchestration",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/multi-agent-orchestration/README.md",
          "type": "blob",
          "size": 5180
        },
        {
          "path": "skills/multi-agent-orchestration/SKILL.md",
          "type": "blob",
          "size": 15582
        },
        {
          "path": "skills/pattern-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/pattern-generator/SKILL.md",
          "type": "blob",
          "size": 19646
        },
        {
          "path": "skills/peer-review-initiator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/peer-review-initiator/SKILL.md",
          "type": "blob",
          "size": 2782
        },
        {
          "path": "skills/peer-review-initiator/code-reviewer.md",
          "type": "blob",
          "size": 3385
        },
        {
          "path": "skills/plan-implementation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/plan-implementation/SKILL.md",
          "type": "blob",
          "size": 2274
        },
        {
          "path": "skills/planning-documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/planning-documentation/SKILL.md",
          "type": "blob",
          "size": 3358
        },
        {
          "path": "skills/portable-document-handler",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/portable-document-handler/SKILL.md",
          "type": "blob",
          "size": 6974
        },
        {
          "path": "skills/portable-document-handler/forms.md",
          "type": "blob",
          "size": 9438
        },
        {
          "path": "skills/portable-document-handler/reference.md",
          "type": "blob",
          "size": 16692
        },
        {
          "path": "skills/presentation-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/presentation-builder/SKILL.md",
          "type": "blob",
          "size": 25451
        },
        {
          "path": "skills/presentation-builder/html2pptx.md",
          "type": "blob",
          "size": 19859
        },
        {
          "path": "skills/presentation-builder/ooxml.md",
          "type": "blob",
          "size": 10388
        },
        {
          "path": "skills/prospect-investigation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/prospect-investigation/SKILL.md",
          "type": "blob",
          "size": 6541
        },
        {
          "path": "skills/protocol-implementation-framework",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/protocol-implementation-framework/SKILL.md",
          "type": "blob",
          "size": 9476
        },
        {
          "path": "skills/protocol-implementation-framework/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/protocol-implementation-framework/reference/evaluation.md",
          "type": "blob",
          "size": 21663
        },
        {
          "path": "skills/protocol-implementation-framework/reference/mcp_best_practices.md",
          "type": "blob",
          "size": 7836
        },
        {
          "path": "skills/protocol-implementation-framework/reference/node_mcp_server.md",
          "type": "blob",
          "size": 28550
        },
        {
          "path": "skills/protocol-implementation-framework/reference/python_mcp_server.md",
          "type": "blob",
          "size": 25099
        },
        {
          "path": "skills/publication-converter",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/publication-converter/REFERENCE.md",
          "type": "blob",
          "size": 8734
        },
        {
          "path": "skills/publication-converter/SKILL.md",
          "type": "blob",
          "size": 6640
        },
        {
          "path": "skills/quality-validation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/quality-validation/SKILL.md",
          "type": "blob",
          "size": 4149
        },
        {
          "path": "skills/rag-agent-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/rag-agent-builder/README.md",
          "type": "blob",
          "size": 4300
        },
        {
          "path": "skills/rag-agent-builder/SKILL.md",
          "type": "blob",
          "size": 13198
        },
        {
          "path": "skills/release-notes-composer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/release-notes-composer/SKILL.md",
          "type": "blob",
          "size": 3028
        },
        {
          "path": "skills/research-documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-documentation/SKILL.md",
          "type": "blob",
          "size": 14193
        },
        {
          "path": "skills/research-management",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-management/SKILL.md",
          "type": "blob",
          "size": 3626
        },
        {
          "path": "skills/research-management/evaluations",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-management/evaluations/README.md",
          "type": "blob",
          "size": 4266
        },
        {
          "path": "skills/research-management/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-management/examples/competitor-analysis.md",
          "type": "blob",
          "size": 8560
        },
        {
          "path": "skills/research-management/examples/market-research.md",
          "type": "blob",
          "size": 1912
        },
        {
          "path": "skills/research-management/examples/technical-investigation.md",
          "type": "blob",
          "size": 6536
        },
        {
          "path": "skills/research-management/examples/trip-planning.md",
          "type": "blob",
          "size": 3810
        },
        {
          "path": "skills/research-management/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/research-management/reference/advanced-search.md",
          "type": "blob",
          "size": 4605
        },
        {
          "path": "skills/research-management/reference/citations.md",
          "type": "blob",
          "size": 5175
        },
        {
          "path": "skills/research-management/reference/comparison-format.md",
          "type": "blob",
          "size": 871
        },
        {
          "path": "skills/research-management/reference/comparison-template.md",
          "type": "blob",
          "size": 973
        },
        {
          "path": "skills/research-management/reference/comprehensive-report-format.md",
          "type": "blob",
          "size": 1077
        },
        {
          "path": "skills/research-management/reference/comprehensive-report-template.md",
          "type": "blob",
          "size": 1281
        },
        {
          "path": "skills/research-management/reference/format-selection-guide.md",
          "type": "blob",
          "size": 3124
        },
        {
          "path": "skills/research-management/reference/quick-brief-format.md",
          "type": "blob",
          "size": 768
        },
        {
          "path": "skills/research-management/reference/quick-brief-template.md",
          "type": "blob",
          "size": 476
        },
        {
          "path": "skills/research-management/reference/research-summary-format.md",
          "type": "blob",
          "size": 841
        },
        {
          "path": "skills/research-management/reference/research-summary-template.md",
          "type": "blob",
          "size": 1160
        },
        {
          "path": "skills/review-recipient",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/review-recipient/SKILL.md",
          "type": "blob",
          "size": 6064
        },
        {
          "path": "skills/selection-randomizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/selection-randomizer/SKILL.md",
          "type": "blob",
          "size": 3827
        },
        {
          "path": "skills/specification-executor",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/specification-executor/SKILL.md",
          "type": "blob",
          "size": 8631
        },
        {
          "path": "skills/specification-executor/evaluations",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/specification-executor/evaluations/README.md",
          "type": "blob",
          "size": 4439
        },
        {
          "path": "skills/specification-executor/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/specification-executor/examples/api-feature.md",
          "type": "blob",
          "size": 13714
        },
        {
          "path": "skills/specification-executor/examples/database-migration.md",
          "type": "blob",
          "size": 2451
        },
        {
          "path": "skills/specification-executor/examples/ui-component.md",
          "type": "blob",
          "size": 1672
        },
        {
          "path": "skills/specification-executor/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/specification-executor/reference/milestone-summary-template.md",
          "type": "blob",
          "size": 460
        },
        {
          "path": "skills/specification-executor/reference/progress-tracking.md",
          "type": "blob",
          "size": 9156
        },
        {
          "path": "skills/specification-executor/reference/progress-update-template.md",
          "type": "blob",
          "size": 424
        },
        {
          "path": "skills/specification-executor/reference/quick-implementation-plan.md",
          "type": "blob",
          "size": 484
        },
        {
          "path": "skills/specification-executor/reference/spec-parsing.md",
          "type": "blob",
          "size": 6962
        },
        {
          "path": "skills/specification-executor/reference/standard-implementation-plan.md",
          "type": "blob",
          "size": 3362
        },
        {
          "path": "skills/specification-executor/reference/task-creation-template.md",
          "type": "blob",
          "size": 603
        },
        {
          "path": "skills/specification-executor/reference/task-creation.md",
          "type": "blob",
          "size": 7982
        },
        {
          "path": "skills/spreadsheet-processor",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/spreadsheet-processor/SKILL.md",
          "type": "blob",
          "size": 10337
        },
        {
          "path": "skills/system-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/system-design/SKILL.md",
          "type": "blob",
          "size": 3531
        },
        {
          "path": "skills/team-communication",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/team-communication/SKILL.md",
          "type": "blob",
          "size": 1331
        },
        {
          "path": "skills/team-communication/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/team-communication/examples/3p-updates.md",
          "type": "blob",
          "size": 3274
        },
        {
          "path": "skills/team-communication/examples/company-newsletter.md",
          "type": "blob",
          "size": 3295
        },
        {
          "path": "skills/team-communication/examples/faq-answers.md",
          "type": "blob",
          "size": 2366
        },
        {
          "path": "skills/team-communication/examples/general-comms.md",
          "type": "blob",
          "size": 602
        },
        {
          "path": "skills/user-interface-designer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/user-interface-designer/SKILL.md",
          "type": "blob",
          "size": 4199
        },
        {
          "path": "skills/validation-first-approach",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/validation-first-approach/SKILL.md",
          "type": "blob",
          "size": 9969
        },
        {
          "path": "skills/validation-first-approach/testing-anti-patterns.md",
          "type": "blob",
          "size": 8251
        },
        {
          "path": "skills/video-archival-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/video-archival-system/SKILL.md",
          "type": "blob",
          "size": 2716
        },
        {
          "path": "skills/visual-composition",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/visual-composition/SKILL.md",
          "type": "blob",
          "size": 11820
        },
        {
          "path": "skills/visual-quality-improver",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/visual-quality-improver/SKILL.md",
          "type": "blob",
          "size": 2549
        },
        {
          "path": "skills/voice-ai-integration",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/voice-ai-integration/SKILL.md",
          "type": "blob",
          "size": 5893
        },
        {
          "path": "skills/web-interface-architect",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/web-interface-architect/SKILL.md",
          "type": "blob",
          "size": 4197
        },
        {
          "path": "skills/word-document-processor",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/word-document-processor/SKILL.md",
          "type": "blob",
          "size": 9963
        },
        {
          "path": "skills/word-document-processor/docx-js.md",
          "type": "blob",
          "size": 16509
        },
        {
          "path": "skills/word-document-processor/ooxml.md",
          "type": "blob",
          "size": 23629
        },
        {
          "path": "skills/workspace-documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/workspace-documentation/SKILL.md",
          "type": "blob",
          "size": 7155
        },
        {
          "path": "skills/workspace-documentation/evaluations",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/workspace-documentation/evaluations/README.md",
          "type": "blob",
          "size": 3498
        },
        {
          "path": "skills/workspace-documentation/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/workspace-documentation/examples/conversation-to-faq.md",
          "type": "blob",
          "size": 16256
        },
        {
          "path": "skills/workspace-documentation/examples/decision-capture.md",
          "type": "blob",
          "size": 3587
        },
        {
          "path": "skills/workspace-documentation/examples/how-to-guide.md",
          "type": "blob",
          "size": 2864
        },
        {
          "path": "skills/workspace-documentation/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/workspace-documentation/reference/database-best-practices.md",
          "type": "blob",
          "size": 3088
        },
        {
          "path": "skills/workspace-documentation/reference/decision-log-database.md",
          "type": "blob",
          "size": 2587
        },
        {
          "path": "skills/workspace-documentation/reference/documentation-database.md",
          "type": "blob",
          "size": 3671
        },
        {
          "path": "skills/workspace-documentation/reference/faq-database.md",
          "type": "blob",
          "size": 2732
        },
        {
          "path": "skills/workspace-documentation/reference/how-to-guide-database.md",
          "type": "blob",
          "size": 1680
        },
        {
          "path": "skills/workspace-documentation/reference/learning-database.md",
          "type": "blob",
          "size": 1811
        },
        {
          "path": "skills/workspace-documentation/reference/team-wiki-database.md",
          "type": "blob",
          "size": 1430
        },
        {
          "path": "skills/workspace-isolation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/workspace-isolation/SKILL.md",
          "type": "blob",
          "size": 5493
        },
        {
          "path": "template",
          "type": "tree",
          "size": null
        },
        {
          "path": "template/SKILL.md",
          "type": "blob",
          "size": 140
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"anthropic-agent-skills\",\n  \"owner\": {\n    \"name\": \"Keith Lazuka\",\n    \"email\": \"klazuka@anthropic.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Anthropic example skills\",\n    \"version\": \"1.0.0\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"document-skills\",\n      \"description\": \"Collection of document processing suite including Excel, Word, PowerPoint, and PDF capabilities\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/spreadsheet-processor\",\n        \"./skills/word-document-processor\",\n        \"./skills/presentation-builder\",\n        \"./skills/portable-document-handler\"\n      ]\n    },\n    {\n      \"name\": \"example-skills\",\n      \"description\": \"Collection of example skills demonstrating various capabilities including skill creation, MCP building, visual design, algorithmic art, internal communications, web testing, artifact building, Slack GIFs, and theme styling\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/pattern-generator\",\n        \"./skills/identity-framework\",\n        \"./skills/visual-composition\",\n        \"./skills/collaborative-document-creation\",\n        \"./skills/web-interface-architect\",\n        \"./skills/team-communication\",\n        \"./skills/protocol-implementation-framework\",\n        \"./skills/capability-architect\",\n        \"./skills/animated-message-composer\",\n        \"./skills/design-system-generator\",\n        \"./skills/interactive-component-creator\",\n        \"./skills/application-quality-assurance\"\n      ]\n    }\n  ]\n}\n",
        "README.md": "> **Note:** This repository contains Anthropic's implementation of skills for Claude. For information about the Agent Skills standard, see [agentskills.io](http://agentskills.io).\n\n# Skills\nSkills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that's creating documents with your company's brand guidelines, analyzing data using your organization's specific workflows, or automating personal tasks.\n\nFor more information, check out:\n- [What are skills?](https://support.claude.com/en/articles/12512176-what-are-skills)\n- [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude)\n- [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills)\n- [Equipping agents for the real world with Agent Skills](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)\n\n# About This Repository\n\nThis repository contains skills that demonstrate what's possible with Claude's skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).\n\nEach skill is self-contained in its own folder with a `SKILL.md` file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.\n\nMany skills in this repo are open source (Apache 2.0). We've also included the document creation & editing skills that power [Claude's document capabilities](https://www.anthropic.com/news/create-files) under the hood in the [`skills/word-document-processor`](./skills/word-document-processor), [`skills/portable-document-handler`](./skills/portable-document-handler), [`skills/presentation-builder`](./skills/presentation-builder), and [`skills/spreadsheet-processor`](./skills/spreadsheet-processor) subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.\n\n## Disclaimer\n\n**These skills are provided for demonstration and educational purposes only.** While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.\n\n# Skill Sets\n- [./skills](./skills): Skill examples for Creative & Design, Development & Technical, Enterprise & Communication, and Document Skills\n- [./spec](./spec): The Agent Skills specification\n- [./template](./template): Skill template\n\n# Try in Claude Code, Claude.ai, and the API\n\n## Claude Code\nYou can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:\n```\n/plugin marketplace add anthropics/skills\n```\n\nThen, to install a specific set of skills:\n1. Select `Browse and install plugins`\n2. Select `anthropic-agent-skills`\n3. Select `document-skills` or `example-skills`\n4. Select `Install now`\n\nAlternatively, directly install either Plugin via:\n```\n/plugin install document-skills@anthropic-agent-skills\n/plugin install example-skills@anthropic-agent-skills\n```\n\nAfter installing the plugin, you can use the skill by just mentioning it. For instance, if you install the `document-skills` plugin from the marketplace, you can ask Claude Code to do something like: \"Use the PDF skill to extract the form fields from `path/to/some-file.pdf`\"\n\n## Claude.ai\n\nThese example skills are all already available to paid plans in Claude.ai. \n\nTo use any skill from this repository or upload custom skills, follow the instructions in [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b).\n\n## Claude API\n\nYou can use Anthropic's pre-built skills, and upload custom skills, via the Claude API. See the [Skills API Quickstart](https://docs.claude.com/en/api/skills-guide#creating-a-skill) for more.\n\n# Creating a Basic Skill\n\nSkills are simple to create - just a folder with a `SKILL.md` file containing YAML frontmatter and instructions. You can use the **template-skill** in this repository as a starting point:\n\n```markdown\n---\nname: my-skill-name\ndescription: A clear description of what this skill does and when to use it\n---\n\n# My Skill Name\n\n[Add your instructions here that Claude will follow when this skill is active]\n\n## Examples\n- Example usage 1\n- Example usage 2\n\n## Guidelines\n- Guideline 1\n- Guideline 2\n```\n\nThe frontmatter requires only two fields:\n- `name` - A unique identifier for your skill (lowercase, hyphens for spaces)\n- `description` - A complete description of what the skill does and when to use it\n\nThe markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills).\n\n# Partner Skills\n\nSkills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:\n\n- **Notion** - [Notion Skills for Claude](https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0)\n",
        "skills/aesthetic-analysis/SKILL.md": "---\nname: aesthetic-analysis\ndescription: Analyze and extract design patterns from visual examples. Deconstructs design systems, color palettes, typography, and layout principles from reference materials.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Landing Page Redesign\n\n## Instructions\n\nWhen requested to redesign a landing page based on a reference:\n\n### 1. **User Interview**\n   - If not provided in the initial request, ask the user for:\n     - **Reference URL or Image**: The landing page or design to replicate (can be a live website URL or an image URL)\n     - **Target Page**: Which file in the codebase should receive the design (e.g., `app/(tabs)/index.tsx`, `app/landing.tsx`)\n   - If details are provided in the initial request, skip to step 2\n\n### 2. **Capture Reference Design**\n   - Use Playwright MCP to open the reference URL:\n     - Navigate to the page\n     - Take a full-page screenshot to understand structure\n     - Interact with the website, mouse hover, click around\n     - Analyze the page deeply\n   - Analyze the landing page/image for:\n     - Layout structure (header, hero, sections, footer)\n     - Interactive elements\n     - Color palette\n     - Typography (fonts, sizes, weights)\n     - Spacing and padding patterns\n     - UI components (buttons, cards, forms, etc.)\n     - Responsive design patterns\n\n### 3. **Implement the Design**\n   - Read the target page file to understand current structure\n   - Implement the design following these principles:\n     - **Match the layout**: Replicate section structure, grid layouts, flex patterns\n      - **Match the intractions**: Replicate mouse and button interactions, whether clicks or hovers - on key elements\n     - **Match colors**: Extract and use exact hex values from the reference\n     - **Match typography**: Use similar fonts (adjust to available system fonts or suggest font imports)\n     - **Match spacing**: Replicate padding, margins, and gaps\n     - **Match components**: Build equivalent React Native components for buttons, cards, inputs, etc.\n     - **Follow project patterns**: Use StyleSheet.create() as per CLAUDE.md guidelines\n     - **Mobile-first**: Ensure the design works on mobile (Expo/React Native)\n   - Write the implementation to the target file\n\n### 4. **Compare Implementations**\n   - If the reference is a live website:\n     - Take a screenshot of the implemented page\n     - Use Playwright to view your implementation\n   - Visually compare:\n     - Layout alignment and proportions\n     - Color accuracy\n     - Typography consistency\n     - Spacing and padding\n     - Component styling details\n   - Document differences found\n\n### 5. **Iterate and Refine**\n   - Based on comparison, identify specific gaps:\n     - Layout issues (alignment, sizing, positioning)\n     - Color mismatches\n     - Typography differences\n     - Missing components or details\n     - Spacing inconsistencies\n   - Make targeted refinements to address each gap\n   - Repeat steps 4-5 until:\n     - The design matches as closely as technically possible\n     - All major visual elements are replicated\n     - User confirms satisfaction\n   - **Aim for 3-5 iterations** minimum to achieve high fidelity\n\n### 6. **Final Review**\n   - Present the final implementation to the user\n   - Summarize what was matched and any intentional differences\n   - Suggest any follow-up improvements (e.g., animations, hover states, responsive tweaks)\n\n## Best Practices\n\n- **Be detail-oriented**: Small differences in spacing, colors, or typography can break the visual consistency\n- **Extract exact values**: Use color pickers and measurement tools to get precise values from screenshots\n- **Component reusability**: Extract repeated patterns into reusable components\n- **Maintain project standards**: Follow the StyleSheet.create() pattern and existing architecture\n- **Document trade-offs**: If React Native limitations prevent exact replication, document why\n\n## Example Flow\n\n**User request:** \"Make our landing page look like https://stripe.com/payments\"\n\n1. **Interview**: Ask \"Which file should receive this design?\"  User: \"app/(tabs)/index.tsx\"\n2. **Capture**:\n   - Navigate to stripe.com/payments\n   - Take full-page screenshot\n   - Analyze deeply: Dark theme, gradient hero, feature grid, clean typography, button changes color upon hover/click\n3. **Implement**:\n   - Read app/(tabs)/index.tsx\n   - Build based on reference\n4. **Compare**:\n   - Screenshot shows hero gradient is lighter than reference\n   - Button border-radius is too sharp\n   - Font weights don't match\n   - Button doesn't change upon hover\n5. **Iterate**:\n   - Adjust gradient colors to match\n   - Reduce border-radius on buttons\n   - Increase font weights\n   - button changes upon hover/click\n   - Re-compare\n6. **Iterate again**:\n   - Fine-tune spacing between sections\n   - Adjust icon sizes\n   - Match exact color values\n7. **Final review**: Present to user with summary of matched elements\n\n## Technical Notes\n\n- **React Native considerations**:\n  - Web fonts may need to be loaded via expo-font or google fonts\n  - Some web-specific effects (box-shadow) have React Native equivalents (shadowColor, shadowOffset)\n  - Use Dimensions API for responsive layouts\n\n- **Iteration targets**:\n  - First iteration: Overall layout and structure\n  - Second iteration: Colors and typography\n  - Third iteration: Spacing and sizing refinement\n  - Fourth+ iterations: Fine details and polish\n\n## References\n\n- [Playwright MCP Documentation](https://github.com/executeautomation/mcp-playwright)\n- [Expo Style Guide](https://docs.expo.dev/develop/user-interface/style/)\n- [React Native StyleSheet](https://reactnative.dev/docs/stylesheet)\n",
        "skills/animated-message-composer/SKILL.md": "---\nname: animated-message-composer\ndescription: Create animated GIFs for messaging platforms. Generates animated content optimized for sharing in chat applications.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Slack GIF Creator\n\nA toolkit providing utilities and knowledge for creating animated GIFs optimized for Slack.\n\n## Slack Requirements\n\n**Emoji GIFs (Slack emoji uploads):**\n- Max size: 64 KB\n- Dimensions: 128x128 recommended (square)\n- FPS: 10-12\n- Colors: 32-48\n- Duration: 1-2s\n\n**Message GIFs:**\n- Max size: ~2 MB\n- Dimensions: 480x480 typical\n- FPS: 15-20\n- Colors: 128-256\n- Duration: 2-5s\n\n## Core Workflow\n\n```python\nfrom core.gif_builder import GIFBuilder\nfrom PIL import Image, ImageDraw\n\n# 1. Create builder\nbuilder = GIFBuilder(width=128, height=128, fps=10)\n\n# 2. Generate frames\nfor i in range(12):\n    frame = Image.new('RGB', (128, 128), (240, 248, 255))\n    draw = ImageDraw.Draw(frame)\n\n    # Draw your animation using PIL primitives\n    # (circles, polygons, lines, etc.)\n\n    builder.add_frame(frame)\n\n# 3. Save with optimization\nbuilder.save('output.gif', num_colors=48, optimize_for_emoji=True)\n```\n\nOptional shortcut: use animation templates in `templates/` to generate frames quickly:\n```python\nfrom templates.shake import create_shake_animation\n\nframes = create_shake_animation(\n    object_type='circle',\n    object_data={'radius': 30, 'color': (100, 150, 255)},\n    num_frames=20,\n    direction='both'\n)\nbuilder.add_frames(frames)\n```\n\n## Drawing Graphics\n\n### Working with User-Uploaded Images\nIf a user uploads an image, consider whether they want to:\n- **Use it directly** (e.g., \"animate this\", \"split this into frames\")\n- **Use it as inspiration** (e.g., \"make something like this\")\n\nLoad and work with images using PIL:\n```python\nfrom PIL import Image\n\nuploaded = Image.open('file.png')\n# Use directly, or just as reference for colors/style\n```\n\n### Drawing from Scratch\nWhen drawing graphics from scratch, use PIL ImageDraw primitives:\n\n```python\nfrom PIL import ImageDraw\n\ndraw = ImageDraw.Draw(frame)\n\n# Circles/ovals\ndraw.ellipse([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Stars, triangles, any polygon\npoints = [(x1, y1), (x2, y2), (x3, y3), ...]\ndraw.polygon(points, fill=(r, g, b), outline=(r, g, b), width=3)\n\n# Lines\ndraw.line([(x1, y1), (x2, y2)], fill=(r, g, b), width=5)\n\n# Rectangles\ndraw.rectangle([x1, y1, x2, y2], fill=(r, g, b), outline=(r, g, b), width=3)\n```\n\n**Avoid relying solely on emoji fonts**: they are platform-specific. If you use `draw_emoji` or `draw_emoji_enhanced`, expect rendering differences and be ready to adjust with shapes or text.\n\n### Making Graphics Look Good\n\nGraphics should look polished and creative, not basic. Here's how:\n\n**Use thicker lines** - Always set `width=2` or higher for outlines and lines. Thin lines (width=1) look choppy and amateurish.\n\n**Add visual depth**:\n- Use gradients for backgrounds (`create_gradient_background`)\n- Layer multiple shapes for complexity (e.g., a star with a smaller star inside)\n\n**Make shapes more interesting**:\n- Don't just draw a plain circle - add highlights, rings, or patterns\n- Stars can have glows (draw larger, semi-transparent versions behind)\n- Combine multiple shapes (stars + sparkles, circles + rings)\n\n**Pay attention to colors**:\n- Use vibrant, complementary colors\n- Add contrast (dark outlines on light shapes, light outlines on dark shapes)\n- Consider the overall composition\n\n**For complex shapes** (hearts, snowflakes, etc.):\n- Use combinations of polygons and ellipses\n- Calculate points carefully for symmetry\n- Add details (a heart can have a highlight curve, snowflakes have intricate branches)\n\nBe creative and detailed! A good Slack GIF should look polished, not like placeholder graphics.\n\n## Available Utilities\n\n### GIFBuilder (`core.gif_builder`)\nAssembles frames and optimizes for Slack:\n```python\nbuilder = GIFBuilder(width=128, height=128, fps=10)\nbuilder.add_frame(frame)  # Add PIL Image\nbuilder.add_frames(frames)  # Add list of frames\nbuilder.save('out.gif', num_colors=48, optimize_for_emoji=True, remove_duplicates=True)\n```\n\n### Validators (`core.validators`)\nCheck if GIF meets Slack requirements:\n```python\nfrom core.validators import (\n    validate_gif,\n    is_slack_ready,\n    check_slack_size,\n    validate_dimensions\n)\n\n# Detailed validation\npasses, info = validate_gif('my.gif', is_emoji=True, verbose=True)\n\n# Quick check\nif is_slack_ready('my.gif'):\n    print(\"Ready!\")\n\n# Focused checks\npasses, info = check_slack_size('my.gif', is_emoji=True)\npasses, info = validate_dimensions(128, 128, is_emoji=True)\n```\n\n### Easing Functions (`core.easing`)\nSmooth motion instead of linear:\n```python\nfrom core.easing import interpolate\n\n# Progress from 0.0 to 1.0\nt = i / (num_frames - 1)\n\n# Apply easing\ny = interpolate(start=0, end=400, t=t, easing='ease_out')\n\n# Available: linear, ease_in, ease_out, ease_in_out,\n#           bounce_out, elastic_out, back_out\n```\n\n### Frame Helpers (`core.frame_composer`)\nConvenience functions for common needs:\n```python\nfrom core.frame_composer import (\n    create_blank_frame,         # Solid color background\n    create_gradient_background,  # Vertical gradient\n    draw_circle,                # Helper for circles\n    draw_rectangle,             # Helper for rectangles\n    draw_line,                  # Helper for lines\n    draw_text,                  # Simple text rendering\n    draw_emoji,                 # Emoji rendering (platform-dependent)\n    draw_star                   # 5-pointed star\n)\n```\nOther helpers include `draw_emoji_enhanced`, `draw_circle_with_shadow`, `draw_rounded_rectangle`, and `add_vignette`.\n\n### Color Palettes (`core.color_palettes`)\nHand-picked palettes and helpers for readable colors:\n```python\nfrom core.color_palettes import get_palette, get_text_color_for_background\n\npalette = get_palette('vibrant')\ntext_color = get_text_color_for_background(palette['background'])\n```\n\n### Typography (`core.typography`)\nHigh-quality outlined or shadowed text:\n```python\nfrom core.typography import draw_text_with_outline\n\ndraw_text_with_outline(frame, \"SALE!\", position=(240, 240), font_size=48, centered=True)\n```\n\n### Visual Effects (`core.visual_effects`)\nParticles, blur, impacts, and motion effects:\n```python\nfrom core.visual_effects import ParticleSystem\n\nparticles = ParticleSystem()\nparticles.emit(240, 240, count=20)\n```\n\n### Animation Templates (`templates/`)\nReady-made animations (shake, bounce, spin, zoom, slide, fade, flip, explode, etc.).\nUse them to generate frames and then pass to `GIFBuilder`.\n\n## Animation Concepts\n\n### Shake/Vibrate\nOffset object position with oscillation:\n- Use `math.sin()` or `math.cos()` with frame index\n- Add small random variations for natural feel\n- Apply to x and/or y position\n\n### Pulse/Heartbeat\nScale object size rhythmically:\n- Use `math.sin(t * frequency * 2 * math.pi)` for smooth pulse\n- For heartbeat: two quick pulses then pause (adjust sine wave)\n- Scale between 0.8 and 1.2 of base size\n\n### Bounce\nObject falls and bounces:\n- Use `interpolate()` with `easing='bounce_out'` for landing\n- Use `easing='ease_in'` for falling (accelerating)\n- Apply gravity by increasing y velocity each frame\n\n### Spin/Rotate\nRotate object around center:\n- PIL: `image.rotate(angle, resample=Image.BICUBIC)`\n- For wobble: use sine wave for angle instead of linear\n\n### Fade In/Out\nGradually appear or disappear:\n- Create RGBA image, adjust alpha channel\n- Or use `Image.blend(image1, image2, alpha)`\n- Fade in: alpha from 0 to 1\n- Fade out: alpha from 1 to 0\n\n### Slide\nMove object from off-screen to position:\n- Start position: outside frame bounds\n- End position: target location\n- Use `interpolate()` with `easing='ease_out'` for smooth stop\n- For overshoot: use `easing='back_out'`\n\n### Zoom\nScale and position for zoom effect:\n- Zoom in: scale from 0.1 to 2.0, crop center\n- Zoom out: scale from 2.0 to 1.0\n- Can add motion blur for drama (PIL filter)\n\n### Explode/Particle Burst\nCreate particles radiating outward:\n- Generate particles with random angles and velocities\n- Update each particle: `x += vx`, `y += vy`\n- Add gravity: `vy += gravity_constant`\n- Fade out particles over time (reduce alpha)\n\n## Optimization Strategies\n\nOnly when asked to make the file size smaller, implement a few of the following methods:\n\n1. **Fewer frames** - Lower FPS (10 instead of 20) or shorter duration\n2. **Fewer colors** - `num_colors=48` instead of 128\n3. **Smaller dimensions** - 128x128 instead of 480x480\n4. **Remove duplicates** - `remove_duplicates=True` in save()\n5. **Emoji mode** - `optimize_for_emoji=True` auto-optimizes\n\n```python\n# Maximum optimization for emoji\nbuilder.save(\n    'emoji.gif',\n    num_colors=48,\n    optimize_for_emoji=True,\n    remove_duplicates=True\n)\n```\n\n## Philosophy\n\nThis skill provides:\n- **Knowledge**: Slack's requirements and animation concepts\n- **Utilities**: GIFBuilder, validators, easing functions\n- **Flexibility**: Create the animation logic using PIL primitives\n\nIt does NOT provide:\n- Rigid animation templates or pre-made functions\n- Emoji font rendering (unreliable across platforms)\n- A library of pre-packaged graphics built into the skill\n\n**Note on user uploads**: This skill doesn't include pre-built graphics, but if a user uploads an image, use PIL to load and work with it - interpret based on their request whether they want it used directly or just as inspiration.\n\nBe creative! Combine concepts (bouncing + rotating, pulsing + sliding, etc.) and use PIL's full capabilities.\n\n## Dependencies\n\n```bash\npip install pillow imageio numpy\n```\n",
        "skills/application-quality-assurance/SKILL.md": "---\nname: application-quality-assurance\ndescription: Test web applications comprehensively for quality. Performs functional, integration, and user experience testing of web applications.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Web Application Testing\n\nTo test local web applications, write native Python Playwright scripts.\n\n**Helper Scripts Available**:\n- `scripts/with_server.py` - Manages server lifecycle (supports multiple servers)\n\n**Always run scripts with `--help` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is abslutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.\n\n## Decision Tree: Choosing Your Approach\n\n```\nUser task  Is it static HTML?\n     Yes  Read HTML file directly to identify selectors\n              Success  Write Playwright script using selectors\n              Fails/Incomplete  Treat as dynamic (below)\n    \n     No (dynamic webapp)  Is the server already running?\n         No  Run: python scripts/with_server.py --help\n                Then use the helper + write simplified Playwright script\n        \n         Yes  Reconnaissance-then-action:\n            1. Navigate and wait for networkidle\n            2. Take screenshot or inspect DOM\n            3. Identify selectors from rendered state\n            4. Execute actions with discovered selectors\n```\n\n## Example: Using with_server.py\n\nTo start a server, run `--help` first, then use the helper:\n\n**Single server:**\n```bash\npython scripts/with_server.py --server \"npm run dev\" --port 5173 -- python your_automation.py\n```\n\n**Multiple servers (e.g., backend + frontend):**\n```bash\npython scripts/with_server.py \\\n  --server \"cd backend && python server.py\" --port 3000 \\\n  --server \"cd frontend && npm run dev\" --port 5173 \\\n  -- python your_automation.py\n```\n\nTo create an automation script, include only Playwright logic (servers are managed automatically):\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True) # Always launch chromium in headless mode\n    page = browser.new_page()\n    page.goto('http://localhost:5173') # Server already running and ready\n    page.wait_for_load_state('networkidle') # CRITICAL: Wait for JS to execute\n    # ... your automation logic\n    browser.close()\n```\n\n## Reconnaissance-Then-Action Pattern\n\n1. **Inspect rendered DOM**:\n   ```python\n   page.screenshot(path='/tmp/inspect.png', full_page=True)\n   content = page.content()\n   page.locator('button').all()\n   ```\n\n2. **Identify selectors** from inspection results\n\n3. **Execute actions** using discovered selectors\n\n## Common Pitfall\n\n **Don't** inspect the DOM before waiting for `networkidle` on dynamic apps\n **Do** wait for `page.wait_for_load_state('networkidle')` before inspection\n\n## Best Practices\n\n- **Use bundled scripts as black boxes** - To accomplish a task, consider whether one of the scripts available in `scripts/` can help. These scripts handle common, complex workflows reliably without cluttering the context window. Use `--help` to see usage, then invoke directly. \n- Use `sync_playwright()` for synchronous scripts\n- Always close the browser when done\n- Use descriptive selectors: `text=`, `role=`, CSS selectors, or IDs\n- Add appropriate waits: `page.wait_for_selector()` or `page.wait_for_timeout()`\n\n## Reference Files\n\n- **examples/** - Examples showing common patterns:\n  - `element_discovery.py` - Discovering buttons, links, and inputs on a page\n  - `static_html_automation.py` - Using file:// URLs for local HTML\n  - `console_logging.py` - Capturing console logs during automation",
        "skills/autonomous-agent-gaming/README.md": "# Autonomous Agent Gaming - Code Structure\n\nThis directory contains extracted, well-organized Python code for building autonomous game-playing agents. The refactoring separates implementation code from documentation for better maintainability and reusability.\n\n## Directory Structure\n\n```\nautonomous-agent-gaming/\n SKILL.md                      # Main skill documentation with concepts and references\n README.md                     # This file\n examples/                     # Agent implementations and game environments\n    rule_based_agent.py      # Agents using predefined heuristics\n    minimax_agent.py         # Minimax with alpha-beta pruning\n    mcts_agent.py            # Monte Carlo Tree Search\n    qlearning_agent.py       # Q-learning reinforcement learning\n    chess_engine.py          # Chess-specific implementation\n    game_environment.py      # Base classes for custom game environments\n    strategy_modules.py      # Opening books, endgame tablebases, adaptive strategies\n scripts/                      # Utility and analysis tools\n     performance_optimizer.py  # Transposition tables, killer heuristic, parallel search\n     game_theory_analyzer.py  # Nash equilibrium, Shapley values, cooperative games\n     agent_benchmark.py        # Tournament evaluation, Elo ratings, profiling\n```\n\n## File Descriptions\n\n### Examples (agents and environments)\n\n#### rule_based_agent.py\nSimple agents using predefined rules and heuristics. Fast decision-making suitable for real-time games.\n\n**Main Classes:**\n- `RuleBasedGameAgent`: Evaluates positions based on material, positional factors, and control\n\n**Key Methods:**\n- `decide_action(game_state)`: Choose action based on rules\n- `evaluate_position(game_state)`: Heuristic evaluation\n\n#### minimax_agent.py\nOptimal decision-making for turn-based games using exhaustive tree search with alpha-beta pruning.\n\n**Main Classes:**\n- `MinimaxGameAgent`: Minimax with alpha-beta pruning\n\n**Key Methods:**\n- `get_best_move(game_state)`: Find optimal move\n- `minimax(game_state, depth, maximizing, alpha, beta)`: Core algorithm\n- `evaluate(game_state)`: Static evaluation function\n\n**Performance:**\n- Pruning reduces complexity from O(b^d) to O(b^(d/2))\n- Adjustable depth for speed/quality tradeoff\n\n#### mcts_agent.py\nProbabilistic game tree exploration using Monte Carlo Tree Search (AlphaGo algorithm).\n\n**Main Classes:**\n- `MCTSNode`: Node in MCTS tree\n- `MCTSAgent`: MCTS implementation\n\n**Four Phases:**\n1. **Tree Policy**: Selection using UCT, then expansion\n2. **Default Policy**: Random playout from leaf\n3. **Backup**: Backpropagate results up tree\n4. **Iteration**: Repeat until time/iteration limit\n\n**Key Methods:**\n- `get_best_move(game_state)`: Run MCTS iterations and return best move\n- `calculate_uct(node, parent_visits)`: UCT = exploitation + exploration\n\n#### qlearning_agent.py\nReinforcement learning agent that learns optimal policies through interaction.\n\n**Main Classes:**\n- `QLearningAgent`: Q-learning algorithm\n\n**Key Methods:**\n- `get_action(state)`: Epsilon-greedy action selection\n- `update_q_value(state, action, reward, next_state)`: Update Q-table\n- `decay_epsilon()`: Gradually reduce exploration\n- `save_q_table()` / `load_q_table()`: Persistence\n\n**Hyperparameters:**\n- `learning_rate ()`: How fast to adapt (0.0-1.0)\n- `discount_factor ()`: Future reward importance (0.0-1.0)\n- `epsilon ()`: Exploration probability (0.0-1.0)\n\n#### chess_engine.py\nFull chess implementation using python-chess library.\n\n**Main Classes:**\n- `ChessAgent`: Chess-playing agent\n\n**Key Methods:**\n- `play_game(opponent_agent)`: Play complete game\n- `get_best_move()`: Find best move by evaluation\n- `evaluate_position()`: Material-based evaluation\n- `get_legal_moves()`: List legal moves\n- `make_move(move)` / `undo_move()`: Move management\n\n**Features:**\n- Full game rules validation\n- FEN notation support\n- Move history tracking\n- Game status reporting\n\n#### game_environment.py\nAbstract base classes for custom game environments.\n\n**Main Classes:**\n- `GameEnvironment`: Abstract base class\n- `PygameGameEnvironment`: Pygame-based rendering\n\n**Key Methods:**\n- `reset()`: Initialize game\n- `step(action)`: Execute action, return (state, reward, done)\n- `render()`: Display game\n- `get_legal_actions(state)`: List valid moves\n- `is_terminal(state)`: Check game over\n\n#### strategy_modules.py\nAdvanced strategies for different game phases.\n\n**Main Classes:**\n- `OpeningBook`: Pre-computed opening moves\n- `EndgameTablebase`: Pre-computed endgame solutions\n- `AdaptiveGameAgent`: Combines strategies by phase\n- `CompositeStrategy`: Priority-based strategy selection\n- `StrategyModule`: Base class for pluggable strategies\n\n**Game Phases:**\n- **Opening** (Material > 30): Use opening book\n- **Middlegame** (10-30): Use search engine\n- **Endgame** (Material < 10): Use tablebase\n\n### Scripts (utilities and analysis)\n\n#### performance_optimizer.py\nTools for optimizing search performance.\n\n**Main Classes:**\n- `TranspositionTable`: Cache for evaluated positions\n- `KillerHeuristic`: Track cutoff-causing moves\n- `ParallelSearchCoordinator`: Distribute search across threads\n- `SearchStatistics`: Track search metrics\n\n**Optimization Techniques:**\n- **Transposition Tables**: Avoid re-evaluating positions\n  - Storage: position_hash -> (depth, score, flag)\n  - Bound types: 'exact', 'lower', 'upper'\n  - Hit rate tracking for efficiency analysis\n\n- **Killer Heuristic**: Improve move ordering\n  - Killer moves cause cutoffs at given depths\n  - Try killers early in move ordering\n\n- **Parallel Search**: Distribute work across threads\n  - Evaluate multiple root moves in parallel\n  - Thread-safe for concurrent access\n\n- **Search Statistics**: Measure optimization effectiveness\n  - Nodes evaluated/pruned\n  - Branching factor\n  - Pruning efficiency percentage\n\n#### game_theory_analyzer.py\nGame-theoretic analysis and solution concepts.\n\n**Main Classes:**\n- `PayoffMatrix`: 2-player game representation\n- `GameTheoryAnalyzer`: Non-cooperative game analysis\n- `CooperativeGameAnalyzer`: Coalition and fairness analysis\n\n**Key Concepts:**\n- **Nash Equilibrium**: Strategy profile where no player can improve unilaterally\n  - Pure strategy: No randomization\n  - Mixed strategy: Probability distribution over actions\n\n- **Minimax Theorem**: In zero-sum games, minimax = maximin\n\n- **Shapley Value**: Fair allocation based on marginal contributions\n\n- **Core**: Allocations where no coalition wants to deviate\n\n**Key Methods:**\n- `find_pure_strategy_nash_equilibria(payoff_matrix)`: Identify equilibria\n- `calculate_mixed_strategy_2x2(payoff_matrix)`: Mixed Nash for 2x2 games\n- `minimax_value()` / `maximin_value()`: Zero-sum game values\n- `calculate_shapley_value()`: Fair allocation\n- `calculate_core()`: Stable coalitional outcomes\n\n#### agent_benchmark.py\nComprehensive benchmarking and evaluation toolkit.\n\n**Main Classes:**\n- `AgentStats`: Track agent performance metrics\n- `GameAgentBenchmark`: Tournament and rating systems\n\n**Key Methods:**\n- `run_tournament(agents, num_games)`: Round-robin tournament\n- `evaluate_elo_rating(agents, num_games)`: Elo rating system\n- `glicko2_rating(agents, num_games)`: Glicko-2 ratings with uncertainty\n- `head_to_head_comparison(agent1, agent2, num_games)`: Detailed comparison\n- `rate_agent_strength(agent, baselines, num_games)`: Strength evaluation\n- `performance_profile(agent, test_positions, time_limit)`: Position accuracy\n\n**Rating Systems:**\n- **Elo**: Traditional rating system\n  - K-factor = 32\n  - Based on strength differential\n\n- **Glicko-2**: Improved system with uncertainty\n  - Accounts for rating deviation\n  - Better for irregular schedules\n\n## Quick Start\n\n### 1. Import and Use an Agent\n\n```python\nfrom examples.minimax_agent import MinimaxGameAgent\n\n# Create agent\nagent = MinimaxGameAgent(max_depth=6)\n\n# Get best move\nbest_move = agent.get_best_move(game_state)\n```\n\n### 2. Train a Q-Learning Agent\n\n```python\nfrom examples.qlearning_agent import QLearningAgent\n\nagent = QLearningAgent(learning_rate=0.1, discount_factor=0.99, epsilon=0.1)\n\n# Training loop\nfor episode in range(1000):\n    state = env.reset()\n    done = False\n    while not done:\n        action = agent.get_action(state)\n        next_state, reward, done = env.step(action)\n        agent.update_q_value(state, action, reward, next_state)\n        state = next_state\n\n    agent.decay_epsilon()  # Reduce exploration over time\n\n# Save learned policy\nagent.save_q_table('q_table.json')\n```\n\n### 3. Play Chess\n\n```python\nfrom examples.chess_engine import ChessAgent\n\nagent1 = ChessAgent()\nagent2 = ChessAgent()\n\nresult, moves = agent1.play_game(agent2)\nprint(f\"Result: {result} ({moves} moves)\")\n```\n\n### 4. Benchmark Agents\n\n```python\nfrom scripts.agent_benchmark import GameAgentBenchmark\n\nbenchmark = GameAgentBenchmark()\n\n# Run tournament\nresults = benchmark.run_tournament(agents, num_games=100)\n\n# Get Elo ratings\nratings = benchmark.evaluate_elo_rating(agents, num_games=100)\nfor agent in agents:\n    print(f\"{agent.name}: {ratings[agent.name]:.0f}\")\n```\n\n### 5. Optimize with Transposition Tables\n\n```python\nfrom scripts.performance_optimizer import TranspositionTable\n\ntt = TranspositionTable(max_size=1000000)\n\n# During search\nposition_hash = hash(game_state)\ncached_score = tt.lookup(position_hash, depth=6)\n\nif cached_score is None:\n    # Evaluate position\n    score = evaluate(game_state)\n    tt.store(position_hash, depth=6, score, flag='exact')\nelse:\n    score = cached_score\n\nprint(f\"Cache hit rate: {tt.hit_rate():.1%}\")\n```\n\n### 6. Game Theory Analysis\n\n```python\nfrom scripts.game_theory_analyzer import GameTheoryAnalyzer, PayoffMatrix\nimport numpy as np\n\n# Prisoner's Dilemma\npayoffs_p1 = np.array([[-1, -3], [0, -2]])\npayoffs_p2 = np.array([[-1, 0], [-3, -2]])\n\nmatrix = PayoffMatrix(payoffs_p1, payoffs_p2)\n\nanalyzer = GameTheoryAnalyzer()\nequilibria = analyzer.find_pure_strategy_nash_equilibria(matrix)\nprint(f\"Nash equilibria: {equilibria}\")\n```\n\n## Integration with SKILL.md\n\nThe SKILL.md file contains conceptual explanations and usage examples that reference these code files. When reading SKILL.md:\n\n- Quick Start section shows how to run each module\n- Each algorithm section references the corresponding .py file\n- Code examples show imports from examples/ and scripts/\n- For detailed implementation, refer to the corresponding file\n\n## Dependencies\n\n### Core (built-in)\n- dataclasses\n- enum\n- typing\n- abc\n- threading\n- concurrent.futures\n- collections\n- math\n- random\n\n### Optional (install as needed)\n- python-chess: `pip install python-chess` (for chess_engine.py)\n- pygame: `pip install pygame` (for PygameGameEnvironment)\n- numpy: `pip install numpy` (for game_theory_analyzer.py)\n- gym: `pip install gym` (for OpenAI Gym integration)\n\n## Design Principles\n\n1. **Modularity**: Each agent/algorithm is self-contained and reusable\n2. **Extensibility**: Abstract base classes allow easy customization\n3. **Simplicity**: Code is readable with clear method names and docstrings\n4. **Type Hints**: Full type annotations for IDE support and documentation\n5. **No Redundancy**: Shared functionality factored into utility modules\n6. **Documentation**: Inline docstrings explain complex algorithms\n\n## Best Practices When Using\n\n1. **Start Simple**: Begin with RuleBasedGameAgent, then progress to Minimax/MCTS\n2. **Profile Before Optimizing**: Use SearchStatistics to identify bottlenecks\n3. **Benchmark Regularly**: Compare agents using GameAgentBenchmark\n4. **Version Control**: Save trained models (Q-tables, opening books)\n5. **Document Changes**: Track why you modified evaluation functions or hyperparameters\n6. **Test Edge Cases**: Verify behavior at game boundaries and end states\n\n## Common Patterns\n\n### Creating a Custom Agent\n```python\nclass MyAgent:\n    def __init__(self):\n        # Initialize parameters\n        pass\n\n    def get_action(self, game_state):\n        # Return best action\n        pass\n```\n\n### Combining Strategies\n```python\nfrom examples.strategy_modules import CompositeStrategy\n\ncomposite = CompositeStrategy([\n    opening_strategy,\n    middlegame_strategy,\n    endgame_strategy\n])\n\nmove = composite.get_move(game_state)\n```\n\n### Parallel Search\n```python\nfrom scripts.performance_optimizer import ParallelSearchCoordinator\n\ncoordinator = ParallelSearchCoordinator(num_threads=4)\nbest_move, score = coordinator.parallel_minimax(root_moves, search_func)\n```\n\n## Further Reading\n\n- See SKILL.md for detailed algorithm explanations\n- Read docstrings in each module for implementation details\n- Check method signatures for parameter and return types\n- Explore examples/ and scripts/ for working code patterns\n",
        "skills/autonomous-agent-gaming/SKILL.md": "---\nname: autonomous-agent-gaming\ndescription: Build autonomous game-playing agents using AI and reinforcement learning. Covers game environments, agent decision-making, strategy development, and performance optimization. Use when creating game-playing bots, testing game AI, strategic decision-making systems, or game theory applications.\n---\n\n# Autonomous Agent Gaming\n\nBuild sophisticated game-playing agents that learn strategies, adapt to opponents, and master complex games through AI and reinforcement learning.\n\n## Overview\n\nAutonomous game agents combine:\n- **Game Environment Interface**: Connect to game rules and state\n- **Decision-Making Systems**: Choose optimal actions\n- **Learning Mechanisms**: Improve through experience\n- **Strategy Development**: Long-term planning and adaptation\n\n### Applications\n\n- Chess and board game masters\n- Real-time strategy (RTS) game bots\n- Video game autonomous players\n- Game theory research\n- AI testing and benchmarking\n- Entertainment and challenge systems\n\n## Quick Start\n\nRun example agents with:\n\n```bash\n# Rule-based agent\npython examples/rule_based_agent.py\n\n# Minimax with alpha-beta pruning\npython examples/minimax_agent.py\n\n# Monte Carlo Tree Search\npython examples/mcts_agent.py\n\n# Q-Learning agent\npython examples/qlearning_agent.py\n\n# Chess engine\npython examples/chess_engine.py\n\n# Game theory analysis\npython scripts/game_theory_analyzer.py\n\n# Benchmark agents\npython scripts/agent_benchmark.py\n```\n\n## Game Agent Architectures\n\n### 1. Rule-Based Agents\n\nUse predefined rules and heuristics. See full implementation in `examples/rule_based_agent.py`.\n\n**Key Concepts:**\n- Difficulty levels control strategy depth\n- Evaluation combines material, position, and control factors\n- Fast decision-making suitable for real-time games\n- Easy to customize and understand\n\n**Usage Example:**\n```python\nfrom examples.rule_based_agent import RuleBasedGameAgent\n\nagent = RuleBasedGameAgent(difficulty=\"hard\")\nbest_move = agent.decide_action(game_state)\n```\n\n### 2. Minimax with Alpha-Beta Pruning\n\nOptimal decision-making for turn-based games. See `examples/minimax_agent.py`.\n\n**Key Concepts:**\n- Exhaustive tree search up to fixed depth\n- Alpha-beta pruning eliminates impossible branches\n- Guarantees optimal play within search depth\n- Evaluation function determines move quality\n\n**Performance Characteristics:**\n- Time complexity: O(b^(d/2)) with pruning vs O(b^d) without\n- Space complexity: O(b*d)\n- Adjustable depth for speed/quality tradeoff\n\n**Usage Example:**\n```python\nfrom examples.minimax_agent import MinimaxGameAgent\n\nagent = MinimaxGameAgent(max_depth=6)\nbest_move = agent.get_best_move(game_state)\n```\n\n### 3. Monte Carlo Tree Search (MCTS)\n\nProbabilistic game tree exploration. Full implementation in `examples/mcts_agent.py`.\n\n**Key Concepts:**\n- Four-phase algorithm: Selection, Expansion, Simulation, Backpropagation\n- UCT (Upper Confidence bounds applied to Trees) balances exploration/exploitation\n- Effective for games with high branching factors\n- Anytime algorithm: more iterations = better decisions\n\n**The UCT Formula:**\nUCT = (child_value / child_visits) + c * sqrt(ln(parent_visits) / child_visits)\n\n**Usage Example:**\n```python\nfrom examples.mcts_agent import MCTSAgent\n\nagent = MCTSAgent(iterations=1000, exploration_constant=1.414)\nbest_move = agent.get_best_move(game_state)\n```\n\n### 4. Reinforcement Learning Agents\n\nLearn through interaction with environment. See `examples/qlearning_agent.py`.\n\n**Key Concepts:**\n- Q-learning: model-free, off-policy learning\n- Epsilon-greedy: balance exploration vs exploitation\n- Update rule: Q(s,a) += [r + *max_a'Q(s',a') - Q(s,a)]\n- Q-table stores state-action value estimates\n\n**Hyperparameters:**\n-  (learning_rate): How quickly to adapt to new information\n-  (discount_factor): Importance of future rewards\n-  (epsilon): Exploration probability\n\n**Usage Example:**\n```python\nfrom examples.qlearning_agent import QLearningAgent\n\nagent = QLearningAgent(learning_rate=0.1, discount_factor=0.99, epsilon=0.1)\naction = agent.get_action(state)\nagent.update_q_value(state, action, reward, next_state)\nagent.decay_epsilon()  # Reduce exploration over time\n```\n\n## Game Environments\n\n### Standard Interfaces\n\nCreate game environments compatible with agents. See `examples/game_environment.py` for base classes.\n\n**Key Methods:**\n- `reset()`: Initialize game state\n- `step(action)`: Execute action, return (next_state, reward, done)\n- `get_legal_actions(state)`: List valid moves\n- `is_terminal(state)`: Check if game is over\n- `render()`: Display game state\n\n### OpenAI Gym Integration\n\nStandard interface for game environments:\n\n```python\nimport gym\n\n# Create environment\nenv = gym.make('CartPole-v1')\n\n# Initialize\nstate = env.reset()\n\n# Run episode\ndone = False\nwhile not done:\n    action = agent.get_action(state)\n    next_state, reward, done, info = env.step(action)\n    agent.update(state, action, reward, next_state)\n    state = next_state\n\nenv.close()\n```\n\n### Chess with python-chess\n\nFull chess implementation in `examples/chess_engine.py`. Requires: `pip install python-chess`\n\n**Features:**\n- Full game rules and move validation\n- Position evaluation based on material count\n- Move history and undo functionality\n- FEN notation support\n\n**Quick Example:**\n```python\nfrom examples.chess_engine import ChessAgent\n\nagent = ChessAgent()\nresult, moves = agent.play_game()\nprint(f\"Game result: {result} in {moves} moves\")\n```\n\n### Custom Game with Pygame\n\nExtend `examples/game_environment.py` with pygame rendering:\n\n```python\nfrom examples.game_environment import PygameGameEnvironment\n\nclass MyGame(PygameGameEnvironment):\n    def get_initial_state(self):\n        # Return initial game state\n        pass\n\n    def apply_action(self, state, action):\n        # Execute action, return new state\n        pass\n\n    def calculate_reward(self, state, action, next_state):\n        # Return reward value\n        pass\n\n    def is_terminal(self, state):\n        # Check if game is over\n        pass\n\n    def draw_state(self, state):\n        # Render using pygame\n        pass\n\ngame = MyGame()\ngame.render()\n```\n\n## Strategy Development\n\nAll strategy implementations are in `examples/strategy_modules.py`.\n\n### 1. Opening Theory\n\nPre-computed best moves for game openings. Load from PGN files or opening databases.\n\n**OpeningBook Features:**\n- Fast lookup using position hashing\n- Load from PGN, opening databases, or create custom books\n- Fallback to other strategies when out of book\n\n**Usage:**\n```python\nfrom examples.strategy_modules import OpeningBook\n\nbook = OpeningBook()\nif book.in_opening(game_state):\n    move = book.get_opening_move(game_state)\n```\n\n### 2. Endgame Tablebases\n\nPre-computed endgame solutions with optimal moves and distance-to-mate.\n\n**Features:**\n- Guaranteed optimal moves in endgame positions\n- Distance-to-mate calculation\n- Lookup by position hash\n\n**Usage:**\n```python\nfrom examples.strategy_modules import EndgameTablebase\n\ntablebase = EndgameTablebase()\nif tablebase.in_tablebase(game_state):\n    move = tablebase.get_best_endgame_move(game_state)\n    dtm = tablebase.get_endgame_distance(game_state)\n```\n\n### 3. Multi-Stage Strategy\n\nCombine different agents for different game phases using `AdaptiveGameAgent`.\n\n**Strategy Selection:**\n- **Opening (Material > 30)**: Use opening book or memorized lines\n- **Middlegame (10-30)**: Use search-based engine (Minimax, MCTS)\n- **Endgame (Material < 10)**: Use tablebase for optimal play\n\n**Usage:**\n```python\nfrom examples.strategy_modules import AdaptiveGameAgent\nfrom examples.minimax_agent import MinimaxGameAgent\n\nagent = AdaptiveGameAgent(\n    opening_book=book,\n    middlegame_engine=MinimaxGameAgent(max_depth=6),\n    endgame_tablebase=tablebase\n)\n\nmove = agent.decide_action(game_state)\nphase_info = agent.get_phase_info(game_state)\n```\n\n### 4. Composite Strategies\n\nCombine multiple strategies with priority ordering using `CompositeStrategy`.\n\n**Usage:**\n```python\nfrom examples.strategy_modules import CompositeStrategy\n\ncomposite = CompositeStrategy([\n    opening_strategy,\n    endgame_strategy,\n    default_search_strategy\n])\n\nmove = composite.get_move(game_state)\nactive = composite.get_active_strategy(game_state)\n```\n\n## Performance Optimization\n\nAll optimization utilities are in `scripts/performance_optimizer.py`.\n\n### 1. Transposition Tables\n\nCache evaluated positions to avoid re-computation. Especially effective with alpha-beta pruning.\n\n**How it works:**\n- Stores evaluation (score + depth + bound type)\n- Hashes positions for fast lookup\n- Only overwrites if new evaluation is deeper\n- Thread-safe for parallel search\n\n**Bound Types:**\n- **exact**: Exact evaluation\n- **lower**: Evaluation is at least this value\n- **upper**: Evaluation is at most this value\n\n**Usage:**\n```python\nfrom scripts.performance_optimizer import TranspositionTable\n\ntt = TranspositionTable(max_size=1000000)\n\n# Store evaluation\ntt.store(position_hash, depth=6, score=150, flag='exact')\n\n# Lookup\nscore = tt.lookup(position_hash, depth=6)\nhit_rate = tt.hit_rate()\n```\n\n### 2. Killer Heuristic\n\nTrack moves that cause cutoffs at similar depths for move ordering improvement.\n\n**Concept:**\n- Killer moves are non-capture moves that caused beta cutoffs\n- Likely to be good moves at other nodes of same depth\n- Improves alpha-beta pruning efficiency\n\n**Usage:**\n```python\nfrom scripts.performance_optimizer import KillerHeuristic\n\nkillers = KillerHeuristic(max_depth=20)\n\n# When a cutoff occurs\nkillers.record_killer(move, depth=5)\n\n# When ordering moves\nkiller_list = killers.get_killers(depth=5)\nis_killer = killers.is_killer(move, depth=5)\n```\n\n### 3. Parallel Search\n\nParallelize game tree search across multiple threads.\n\n**Usage:**\n```python\nfrom scripts.performance_optimizer import ParallelSearchCoordinator\n\ncoordinator = ParallelSearchCoordinator(num_threads=4)\n\n# Parallel move evaluation\nscores = coordinator.parallel_evaluate_moves(moves, evaluate_func)\n\n# Parallel minimax\nbest_move, score = coordinator.parallel_minimax(root_moves, minimax_func)\n\ncoordinator.shutdown()\n```\n\n### 4. Search Statistics\n\nTrack and analyze search performance with `SearchStatistics`.\n\n**Metrics:**\n- Nodes evaluated / pruned\n- Branching factor\n- Pruning efficiency\n- Cache hit rate\n\n**Usage:**\n```python\nfrom scripts.performance_optimizer import SearchStatistics\n\nstats = SearchStatistics()\n\n# During search\nstats.record_node()\nstats.record_cutoff()\nstats.record_cache_hit()\n\n# Analysis\nprint(stats.summary())\nprint(f\"Pruning efficiency: {stats.pruning_efficiency():.1f}%\")\n```\n\n## Game Theory Applications\n\nFull implementation in `scripts/game_theory_analyzer.py`.\n\n### 1. Nash Equilibrium Calculation\n\nFind optimal mixed strategy solutions for 2-player games.\n\n**Pure Strategy Nash Equilibria:**\nA cell is a Nash equilibrium if it's a best response for both players.\n\n**Mixed Strategy Nash Equilibria:**\nPlayers randomize over actions. For 2x2 games, use indifference conditions.\n\n**Usage:**\n```python\nfrom scripts.game_theory_analyzer import GameTheoryAnalyzer, PayoffMatrix\nimport numpy as np\n\n# Create payoff matrix\np1_payoffs = np.array([[3, 0], [5, 1]])\np2_payoffs = np.array([[3, 5], [0, 1]])\n\nmatrix = PayoffMatrix(\n    player1_payoffs=p1_payoffs,\n    player2_payoffs=p2_payoffs,\n    row_labels=['Strategy A', 'Strategy B'],\n    column_labels=['Strategy X', 'Strategy Y']\n)\n\nanalyzer = GameTheoryAnalyzer()\n\n# Find pure Nash equilibria\nequilibria = analyzer.find_pure_strategy_nash_equilibria(matrix)\n\n# Find mixed Nash equilibrium (2x2 only)\np1_mixed, p2_mixed = analyzer.calculate_mixed_strategy_2x2(matrix)\n\n# Expected payoff\npayoff = analyzer.calculate_expected_payoff(p1_mixed, p2_mixed, matrix, player=1)\n\n# Zero-sum analysis\nif matrix.is_zero_sum():\n    minimax = analyzer.minimax_value(matrix)\n    maximin = analyzer.maximin_value(matrix)\n```\n\n### 2. Cooperative Game Analysis\n\nAnalyze coalitional games where players can coordinate.\n\n**Shapley Value:**\n- Fair allocation of total payoff based on marginal contributions\n- Each player receives expected marginal contribution across all coalition orderings\n\n**Core:**\n- Set of allocations where no coalition wants to deviate\n- Stable outcomes that satisfy coalitional rationality\n\n**Usage:**\n```python\nfrom scripts.game_theory_analyzer import CooperativeGameAnalyzer\n\ncoop = CooperativeGameAnalyzer()\n\n# Define payoff function for coalitions\ndef payoff_func(coalition):\n    # Return total value of coalition\n    return sum(player_values[p] for p in coalition)\n\nplayers = ['Alice', 'Bob', 'Charlie']\n\n# Calculate Shapley values\nshapley = coop.calculate_shapley_value(payoff_func, players)\nprint(f\"Alice's fair share: {shapley['Alice']}\")\n\n# Find core allocation\ncore = coop.calculate_core(payoff_func, players)\nis_stable = coop.is_core_allocation(core, payoff_func, players)\n```\n\n## Best Practices\n\n### Agent Development\n-  Start with rule-based baseline\n-  Measure performance metrics consistently\n-  Test against multiple opponents\n-  Use version control for agent versions\n-  Document strategy changes\n\n### Game Environment\n-  Validate game rules implementation\n-  Test edge cases\n-  Provide easy reset/replay\n-  Log game states for analysis\n-  Support deterministic seeds\n\n### Optimization\n-  Profile before optimizing\n-  Use transposition tables\n-  Implement proper time management\n-  Monitor memory usage\n-  Benchmark against baselines\n\n## Testing and Benchmarking\n\nComplete benchmarking toolkit in `scripts/agent_benchmark.py`.\n\n### Tournament Evaluation\n\nRun round-robin or elimination tournaments between agents.\n\n**Usage:**\n```python\nfrom scripts.agent_benchmark import GameAgentBenchmark\n\nbenchmark = GameAgentBenchmark()\n\n# Run tournament\nresults = benchmark.run_tournament(agents, num_games=100)\n\n# Compare two agents\ncomparison = benchmark.head_to_head_comparison(agent1, agent2, num_games=50)\nprint(f\"Win rate: {comparison['agent1_win_rate']:.1%}\")\n```\n\n### Rating Systems\n\nCalculate agent strength using standard rating systems.\n\n**Elo Rating:**\n- Based on strength differential\n- K-factor of 32 for normal games\n- Used in chess and many games\n\n**Glicko-2 Rating:**\n- Accounts for rating uncertainty (deviation)\n- Better for irregular play schedules\n\n**Usage:**\n```python\n# Elo ratings\nelo_ratings = benchmark.evaluate_elo_rating(agents, num_games=100)\n\n# Glicko-2 ratings\nglicko_ratings = benchmark.glicko2_rating(agents, num_games=100)\n\n# Strength relative to baseline\nstrength = benchmark.rate_agent_strength(agent, baseline_agents, num_games=20)\n```\n\n### Performance Profiling\n\nEvaluate agent quality on test positions.\n\n**Usage:**\n```python\n# Get performance profile\nprofile = benchmark.performance_profile(agent, test_positions, time_limit=1.0)\nprint(f\"Accuracy: {profile['accuracy']:.1%}\")\nprint(f\"Avg move quality: {profile['avg_move_quality']:.2f}\")\n```\n\n## Implementation Checklist\n\n- [ ] Choose game environment (Gym, Chess, Custom)\n- [ ] Design agent architecture (Rule-based, Minimax, MCTS, RL)\n- [ ] Implement game state representation\n- [ ] Create evaluation function\n- [ ] Implement agent decision-making\n- [ ] Set up training/learning loop\n- [ ] Create benchmarking system\n- [ ] Test against multiple opponents\n- [ ] Optimize performance (search depth, eval speed)\n- [ ] Document strategy and results\n- [ ] Deploy and monitor performance\n\n## Resources\n\n### Frameworks\n- **OpenAI Gym**: https://gym.openai.com/\n- **python-chess**: https://python-chess.readthedocs.io/\n- **Pygame**: https://www.pygame.org/\n\n### Research\n- **AlphaGo papers**: https://deepmind.com/\n- **Stockfish**: https://stockfishchess.org/\n- **Game Theory**: Introduction to Game Theory (Osborne & Rubinstein)\n\n",
        "skills/autonomous-cloud-orchestration/SKILL.md": "---\nname: autonomous-cloud-orchestration\ndescription: Orchestrate multi-service AWS workflows with autonomous agents. Coordinates across compute, storage, identity, and observability services for intelligent cloud automation.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# AWS Bedrock AgentCore\n\nAWS Bedrock AgentCore provides a complete platform for deploying and scaling AI agents with seven core services. This skill guides you through service selection, deployment patterns, and integration workflows using AWS CLI.\n\n## AWS Documentation Requirement\n\n**CRITICAL**: This skill requires AWS MCP tools for accurate, up-to-date AWS information.\n\n### Before Answering AWS Questions\n\n1. **Always verify** using AWS MCP tools (if available):\n   - `mcp__aws-mcp__aws___search_documentation` or `mcp__*awsdocs*__aws___search_documentation` - Search AWS docs\n   - `mcp__aws-mcp__aws___read_documentation` or `mcp__*awsdocs*__aws___read_documentation` - Read specific pages\n   - `mcp__aws-mcp__aws___get_regional_availability` - Check service availability\n\n2. **If AWS MCP tools are unavailable**:\n   - Guide user to configure AWS MCP: See [AWS MCP Setup Guide](../../docs/aws-mcp-setup.md)\n   - Help determine which option fits their environment:\n     - Has uvx + AWS credentials  Full AWS MCP Server\n     - No Python/credentials  AWS Documentation MCP (no auth)\n   - If cannot determine  Ask user which option to use\n\n## When to Use This Skill\n\nUse this skill when you need to:\n- Deploy REST APIs as MCP tools for AI agents (Gateway)\n- Execute agents in serverless runtime (Runtime)\n- Add conversation memory to agents (Memory)\n- Manage API credentials and authentication (Identity)\n- Enable agents to execute code securely (Code Interpreter)\n- Allow agents to interact with websites (Browser)\n- Monitor and trace agent performance (Observability)\n\n## Available Services\n\n| Service | Use For | Documentation |\n|---------|---------|---------------|\n| **Gateway** | Converting REST APIs to MCP tools | [`services/gateway/README.md`](services/gateway/README.md) |\n| **Runtime** | Deploying and scaling agents | [`services/runtime/README.md`](services/runtime/README.md) |\n| **Memory** | Managing conversation state | [`services/memory/README.md`](services/memory/README.md) |\n| **Identity** | Credential and access management | [`services/identity/README.md`](services/identity/README.md) |\n| **Code Interpreter** | Secure code execution in sandboxes | [`services/code-interpreter/README.md`](services/code-interpreter/README.md) |\n| **Browser** | Web automation and scraping | [`services/browser/README.md`](services/browser/README.md) |\n| **Observability** | Tracing and monitoring | [`services/observability/README.md`](services/observability/README.md) |\n\n## Common Workflows\n\n### Deploying a Gateway Target\n\n**MANDATORY - READ DETAILED DOCUMENTATION**: See [`services/gateway/README.md`](services/gateway/README.md) for complete Gateway setup guide including deployment strategies, troubleshooting, and IAM configuration.\n\n**Quick Workflow**:\n1. Upload OpenAPI schema to S3\n2. *(API Key auth only)* Create credential provider and store API key\n3. Create gateway target linking schema (and credentials if using API key)\n4. Verify target status and test connectivity\n\n> **Note**: Credential provider is only needed for API key authentication. Lambda targets use IAM roles, and MCP servers use OAuth.\n\n### Managing Credentials\n\n**MANDATORY - READ DETAILED DOCUMENTATION**: See [`cross-service/credential-management.md`](cross-service/credential-management.md) for unified credential management patterns across all services.\n\n**Quick Workflow**:\n1. Use Identity service credential providers for all API keys\n2. Link providers to gateway targets via ARN references\n3. Rotate credentials quarterly through credential provider updates\n4. Monitor usage with CloudWatch metrics\n\n### Monitoring Agents\n\n**MANDATORY - READ DETAILED DOCUMENTATION**: See [`services/observability/README.md`](services/observability/README.md) for comprehensive monitoring setup.\n\n**Quick Workflow**:\n1. Enable observability for agents\n2. Configure CloudWatch dashboards for metrics\n3. Set up alarms for error rates and latency\n4. Use X-Ray for distributed tracing\n\n## Service-Specific Documentation\n\nFor detailed documentation on each AgentCore service, see the following resources:\n\n### Gateway Service\n- **Overview**: [`services/gateway/README.md`](services/gateway/README.md)\n- **Deployment Strategies**: [`services/gateway/deployment-strategies.md`](services/gateway/deployment-strategies.md)\n- **Troubleshooting**: [`services/gateway/troubleshooting-guide.md`](services/gateway/troubleshooting-guide.md)\n\n### Runtime, Memory, Identity, Code Interpreter, Browser, Observability\nEach service has comprehensive documentation in its respective directory:\n- [`services/runtime/README.md`](services/runtime/README.md)\n- [`services/memory/README.md`](services/memory/README.md)\n- [`services/identity/README.md`](services/identity/README.md)\n- [`services/code-interpreter/README.md`](services/code-interpreter/README.md)\n- [`services/browser/README.md`](services/browser/README.md)\n- [`services/observability/README.md`](services/observability/README.md)\n\n## Cross-Service Resources\n\nFor patterns and best practices that span multiple AgentCore services:\n\n- **Credential Management**: [`cross-service/credential-management.md`](cross-service/credential-management.md) - Unified credential patterns, security practices, rotation procedures\n\n## Additional Resources\n\n- **AWS Documentation**: [Amazon Bedrock AgentCore](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html)\n- **API Reference**: [Bedrock AgentCore Control Plane API](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/)\n- **AWS CLI Reference**: [bedrock-agentcore-control commands](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/bedrock-agentcore-control/index.html)\n\n",
        "skills/autonomous-cloud-orchestration/cross-service/credential-management.md": "# Cross-Service Credential Management\n\n**Applies to**: Gateway, Runtime, Memory, Identity\n\n## Overview\n\nCredential management is a cross-cutting concern across all AgentCore services. This guide provides unified patterns for managing API keys, tokens, and authentication credentials across the AgentCore platform.\n\n## Authentication Overview\n\n| Service | Direction | Supported Methods | Use Case |\n|---------|-----------|-------------------|----------|\n| **Gateway** | Inbound | IAM, JWT, No Auth | Who can invoke MCP tools |\n| **Gateway** | Outbound | IAM, OAuth (2LO/3LO), API Key | Accessing external APIs |\n| **Runtime** | Inbound | IAM (SigV4), JWT | Who can invoke agents |\n| **Runtime** | Outbound | OAuth, API Key | Accessing third-party services |\n| **Memory** | - | IAM Role | Data access permissions |\n| **Identity** | - | AWS KMS | Secret encryption |\n\n### Inbound Authorization (Who Can Access Your Services)\n\n**Gateway Options** ([docs](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-inbound-auth.html)):\n- **IAM Identity**: Uses AWS IAM credentials for authorization\n- **JWT**: Tokens from identity providers (Cognito, Microsoft Entra ID, etc.)\n- **No Authorization**: Open access - only for production with proper security controls\n\n**Runtime Options** ([docs](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-oauth.html)):\n- **IAM (SigV4)**: Default authentication (works automatically)\n- **JWT Bearer Token**: Token-based auth with discovery URL and audience validation\n\n> **Note**: A Runtime can only use one inbound auth type (IAM or JWT), not both simultaneously.\n\n### Outbound Authorization (Accessing External Services)\n\n**Gateway Options** ([docs](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-outbound-auth.html)):\n\n| Target Type | IAM (Service Role) | OAuth 2LO | OAuth 3LO | API Key |\n|-------------|-------------------|-----------|-----------|---------|\n| Lambda function |  |  |  |  |\n| API Gateway |  |  |  |  |\n| OpenAPI schema |  |  |  |  |\n| Smithy schema |  |  |  |  |\n| MCP server |  |  |  |  |\n\n- **OAuth 2LO**: Client credentials grant (machine-to-machine)\n- **OAuth 3LO**: Authorization code grant (user-delegated access)\n\n**Runtime Options**:\n- **OAuth**: Tokens on behalf of users via Identity Service\n- **API Key**: Key-based authentication via Identity Service\n\n## Best Practices\n\n###  DO's\n\n1. **Use Identity Service**: Always manage credentials through the Identity service\n   ```bash\n   #  Correct - Use Identity API\n   aws bedrock-agentcore-control create-api-key-credential-provider \\\n     --name MyCredentialProvider \\\n     --api-key \"YOUR_API_KEY_VALUE\"\n   ```\n\n2. **Separate by Environment**: Use different providers for different environments\n   ```bash\n   - dev-api-key-provider      # Development\n   - staging-api-key-provider  # Staging\n   - prod-api-key-provider     # Production\n   ```\n\n3. **Rotate Regularly**: Implement quarterly credential rotation\n   ```bash\n   aws bedrock-agentcore-control update-api-key-credential-provider \\\n     --name MyCredentialProvider \\\n     --api-key \"NEW_API_KEY\"\n   ```\n\n4. **Least Privilege**: Grant minimal required permissions to each credential\n   ```bash\n   # API key should only have necessary API permissions\n   # IAM roles should have scoped-down policies\n   ```\n\n5. **Monitor Usage**: Track credential usage and set up alerts\n   ```json\n   {\n     \"CloudWatch Alarms\": {\n       \"HighErrorRate\": \"Alert if > 10% failed requests\",\n       \"UnusualActivity\": \"Alert on usage spikes\"\n     }\n   }\n   ```\n\n###  DON'Ts\n\n1. **Never Hardcode**: Don't embed credentials in code or configuration files\n   ```bash\n   #  Bad - Hardcoded API key\n   const apiKey = \"sk-1234567890abcdef\"\n\n   #  Good - Reference credential provider\n   const credentialProvider = \"MyCredentialProvider\"\n   ```\n\n2. **Don't Share Across Environments**: Avoid using production keys in development\n   ```bash\n   #  Bad - Same key everywhere\n   dev:  third-party-api-key: prod-key\n   prod: third-party-api-key: prod-key\n\n   #  Good - Separate keys\n   dev:  third-party-api-key: dev-key\n   prod: third-party-api-key: prod-key\n   ```\n\n3. **Don't Commit to Git**: Exclude credential files from version control\n   ```bash\n   # .gitignore\n   *.env\n   *.secret\n   credential-*.json\n   ```\n\n4. **Don't Use Long-Lived Tokens**: Implement token refresh for OAuth\n   ```bash\n   # OAuth tokens should auto-refresh\n   # Don't use tokens with > 30 day expiration\n   ```\n\n## Multi-Service Credential Patterns\n\n### Pattern 1: Centralized Identity, Distributed Usage\n\n```\n\n  Identity Service                   \n  - Stores ALL credentials           \n  - Manages rotation                 \n  - Provides audit logs              \n\n           \n           \n                                               \n       \n     Gateway    Runtime     Memory     Other   \n      Uses       Uses       Uses       Uses    \n       \n```\n\n**Benefits**:\n- Single source of truth for all credentials\n- Unified rotation and audit\n- Consistent access patterns\n\n**Setup**:\n```bash\n# 1. Create master credential in Identity\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name MasterAPICredentials \\\n  --api-key \"YOUR_MASTER_API_KEY\"\n\n# 2. Grant access to each service\n# - Gateway: can read MasterAPICredentials\n# - Runtime: can read MasterAPICredentials\n# - Memory: can read MasterAPICredentials\n```\n\n### Pattern 2: Service-Specific Credentials\n\n```\n\n  Identity Service                   \n  - Stores credentials per service   \n\n           \n    \n                                  \n   \n Gateway   Runtime  MemoryOther\n  Cred      Cred     Cred Cred \n   \n```\n\n**Benefits**:\n- Isolation between services\n- Independent rotation per service\n- Service-specific permissions\n\n**Setup**:\n```bash\n# Create separate providers\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name GatewayAPICredentials \\\n  --api-key \"YOUR_GATEWAY_API_KEY\"\n\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name RuntimeCredentials \\\n  --api-key \"YOUR_RUNTIME_API_KEY\"\n```\n\n### Pattern 3: Tiered (Master + Service)\n\n```\n\n  Identity Service                   \n  - Master credential                \n  - Per-service credentials          \n\n           \n    \n                 \n \n Master      Services  \n  Cred       - Gateway  \n    - Runtime \n              - Memory  \n        (each has \n             own creds)  \n            \n```\n\n**Use Cases**:\n- Production: Master credential for critical APIs\n- Development: Service-specific credentials for testing\n- Emergency: Master credential as backup\n\n## Security Best Practices\n\n### Encryption\n\n```bash\n# Use KMS for secret encryption\naws secretsmanager create-secret \\\n  --name MySecret \\\n  --kms-key-id arn:aws:kms:us-west-2:123456789012:key/12345678-abcd-ef12-3456-7890abcdef12 \\\n  --secret-string \"my-secret-value\"\n```\n\n### Access Control\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"bedrock-agentcore:GetResourceApiKey\"\n      ],\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:PrincipalTag/Service\": \"gateway\"\n        }\n      }\n    }\n  ]\n}\n```\n\n### Audit Logging\n\n```bash\n# Enable CloudTrail for Bedrock AgentCore\naws cloudtrail create-trail \\\n  --name agentcore-audit \\\n  --s3-bucket-name agentcore-audit-logs \\\n  --include-global-service-events true\n```\n\n## Rotation Strategy\n\n### Automated Rotation\n\n```bash\n# Enable automatic rotation (when supported)\naws secretsmanager rotate-secret \\\n  --secret-id MySecret \\\n  --lambda-arn arn:aws:lambda:us-west-2:123456789012:function:MyRotationFunction\n\n# Rotation schedule (every 30 days)\naws secretsmanager rotate-secret \\\n  --secret-id MySecret \\\n  --rotation-rules AutomaticAfterDays=30\n```\n\n### Manual Rotation Process\n\n```bash\n#!/bin/bash\n# rotate-credentials.sh\n\necho \"Step 1: Generate new credential\"\nNEW_KEY=$(generate-new-api-key)\n\necho \"Step 2: Update in Identity service\"\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name MyCredentialProvider \\\n  --api-key \"$NEW_KEY\"\n\necho \"Step 3: Verify all services work\"\n./test-all-services.sh\n\necho \"Step 4: Delete old credential\"\n# Old credential is automatically deprecated\n```\n\n## Common Patterns\n\n### Pattern: Credential Fallback\n\n```typescript\n// Try primary credential, fallback to backup\nasync function callWithFallback(provider: string) {\n  try {\n    return await callAPI(provider);\n  } catch (error) {\n    if (error.code === 'InvalidAPICredentials') {\n      // Fallback to backup provider\n      return await callAPI(`${provider}-backup`);\n    }\n    throw error;\n  }\n}\n```\n\n### Pattern: Rate Limiting with Credential Pool\n\n```typescript\n// Rotate through multiple credentials to avoid rate limits\nconst credentialPool = [\n  'cred-1',\n  'cred-2',\n  'cred-3'\n];\n\nlet currentIndex = 0;\n\nfunction getNextCredential(): string {\n  const credential = credentialPool[currentIndex];\n  currentIndex = (currentIndex + 1) % credentialPool.length;\n  return credential;\n}\n```\n\n## Troubleshooting Credential Issues\n\n### Issue: \"Credential not found\"\n\n**Diagnosis**:\n```bash\n# Check if provider exists\naws bedrock-agentcore-control get-api-key-credential-provider \\\n  --name MyCredentialProvider\n\n# Check IAM permissions\naws iam simulate-principal-policy \\\n  --policy-source-arn arn:aws:iam::123456789012:role/MyRole \\\n  --action-names bedrock-agentcore:GetResourceApiKey \\\n  --resource-arns arn:aws:bedrock-agentcore:us-west-2:123456789012:*\n```\n\n**Solution**: Create provider or grant IAM permissions\n\n---\n\n### Issue: \"Invalid credentials\" after rotation\n\n**Diagnosis**:\n```bash\n# Check secret value format\naws secretsmanager get-secret-value \\\n  --secret-id arn:aws:secretsmanager:us-west-2:123456789012:secret:MySecret\n\n# Should be: {\"apiKey\": \"valid-key\"}\n```\n\n**Solution**: Use correct update API\n```bash\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name MyCredentialProvider \\\n  --api-key \"VALID_KEY\"\n```\n\n---\n\n### Issue: Cross-service access denied\n\n**Diagnosis**:\n```bash\n# Check which services can access the credential\naws bedrock-agentcore-control get-api-key-credential-provider \\\n  --name MyCredentialProvider\n\n# Review service IAM policies\n```\n\n**Solution**: Add cross-service access policy\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"bedrock-agentcore:GetResourceApiKey\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"ArnLike\": {\n          \"aws:PrincipalArn\": [\n            \"arn:aws:iam::*:role/*gateway*\",\n            \"arn:aws:iam::*:role/*runtime*\"\n          ]\n        }\n      }\n    }\n  ]\n}\n```\n\n## Cost Considerations\n\n### Secrets Manager Costs\n\n- **Per secret**: ~$0.40/month\n- **Per 10,000 API calls**: ~$0.05\n- **Cross-region replication**: Additional costs\n\n**Optimization**:\n- Share credentials across services when possible\n- Use regional replication only when necessary\n- Cache credential retrieval (respect security requirements)\n\n### Identity Service Costs\n\n- **Credential provider storage**: Included in Secrets Manager\n- **API calls**: Same as Secrets Manager pricing\n- **Cross-account access**: No additional cost\n\n## References\n\n- **[Identity Service](../services/identity/README.md)**: Credential provider management\n- **[Gateway Service](../services/gateway/README.md)**: Uses credentials for API authentication\n- **AWS Secrets Manager**: [Pricing](https://aws.amazon.com/secrets-manager/pricing/)\n- **AWS Documentation**: [Managing AWS Secrets Manager secrets](https://docs.aws.amazon.com/secretsmanager/latest/userguide/manage_create-basic-secret.html)\n\n---\n\n**Related Guides**:\n- [Observability Service](../services/observability/README.md)\n- [AWS AgentCore Identity Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity.html)\n",
        "skills/autonomous-cloud-orchestration/services/browser/README.md": "# AgentCore Browser Service\n\n> **Status**:  Available\n\n## Overview\n\nAmazon Bedrock AgentCore Browser provides a fast, secure, cloud-based browser runtime enabling AI agents to interact with websites at scale without infrastructure management.\n\n## Core Capabilities\n\n### Cloud-Based Runtime\n- **Fast Execution**: High-performance browser instances with minimal latency\n- **Auto Scaling**: Automatic scaling based on demand without configuration\n- **Zero Infrastructure**: No servers or containers to manage\n- **Multi-Region**: Deploy browser instances across AWS regions globally\n\n### Security and Compliance\n- **Enterprise Security**: Industry-standard security controls and encryption\n- **Isolated Sessions**: Each browsing session runs in complete isolation\n- **Data Protection**: Secure data handling and privacy controls\n- **Compliance Ready**: Meets enterprise compliance requirements (SOC, HIPAA, GDPR)\n\n### Web Interaction Capabilities\n- **Full Automation**: Complete browser automation capabilities\n- **JavaScript Support**: Execute JavaScript in browser context\n- **Form Interaction**: Fill forms, click buttons, navigate pages\n- **Content Extraction**: Extract text, data, and media from pages\n- **Session Management**: Handle cookies, local storage, and sessions\n- **Screenshot Capture**: Take screenshots of pages and elements\n\n### Observability\n- **Execution Logging**: Comprehensive logs of browser actions\n- **Performance Metrics**: Track page load times and operation latency\n- **Error Tracking**: Detailed error capture and debugging information\n- **Request Monitoring**: Monitor network requests and responses\n\n## Use Cases\n\n### Web Scraping and Data Extraction\nEnable agents to:\n- Extract data from websites at scale\n- Scrape content from dynamic pages\n- Collect structured data from multiple sources\n- Monitor website changes over time\n\n### Automated Testing and QA\nSupport scenarios like:\n- Automated UI testing of web applications\n- Regression testing for web features\n- Cross-browser compatibility testing\n- Performance testing and monitoring\n\n### Form Filling and Workflow Automation\nAllow agents to:\n- Automate form submissions\n- Complete multi-step workflows\n- Handle authentication and logins\n- Process batch operations on web interfaces\n\n### Real-Time Monitoring\nEnable agents to:\n- Monitor website availability and uptime\n- Track content changes and updates\n- Verify website functionality\n- Gather competitive intelligence\n\n### Content Verification\nSupport tasks like:\n- Validate web content accuracy\n- Check link integrity\n- Verify page rendering\n- Test responsive designs\n\n## Architecture\n\n### Browser Execution Flow\n\n```\nAgent Request\n    \n\n  Browser Service API                    \n  - Parse browser action request         \n  - Validate parameters                  \n  - Allocate browser instance            \n\n    \n\n  Browser Instance                       \n  - Navigate to URL                      \n  - Execute JavaScript                   \n  - Interact with page elements          \n  - Extract content and data             \n\n    \n\n  Result Processing                      \n  - Package extracted data               \n  - Capture screenshots (if requested)   \n  - Log execution details                \n  - Return results to agent              \n\n```\n\n### Security Architecture\n\n1. **Session Isolation**: Each browser session runs in isolated environment\n2. **Network Security**: Controlled outbound internet access\n3. **Data Encryption**: All data encrypted in transit and at rest\n4. **Resource Limits**: CPU, memory, and time limits per session\n5. **Access Control**: IAM-based authentication and authorization\n\n## Configuration\n\n### Basic Browser Session\n\n```bash\n# Configure browser service for agent\naws bedrock-agentcore-control configure-browser \\\n  --agent-id <AGENT_ID> \\\n  --session-timeout 600 \\\n  --viewport-width 1920 \\\n  --viewport-height 1080 \\\n  --region <REGION>\n```\n\n### Advanced Configuration\n\n```bash\n# Set browser preferences and capabilities\naws bedrock-agentcore-control update-browser-config \\\n  --agent-id <AGENT_ID> \\\n  --browser-config '{\n    \"headless\": true,\n    \"javascript\": true,\n    \"images\": true,\n    \"cookies\": true,\n    \"userAgent\": \"CustomUserAgent/1.0\",\n    \"timeout\": 30000\n  }' \\\n  --region <REGION>\n```\n\n## Browser Actions\n\n### Navigation\n```javascript\n// Navigate to URL\n{\n  \"action\": \"navigate\",\n  \"url\": \"https://example.com\"\n}\n\n// Go back\n{\n  \"action\": \"goBack\"\n}\n\n// Refresh page\n{\n  \"action\": \"reload\"\n}\n```\n\n### Element Interaction\n```javascript\n// Click element\n{\n  \"action\": \"click\",\n  \"selector\": \"#submit-button\"\n}\n\n// Fill input field\n{\n  \"action\": \"type\",\n  \"selector\": \"#username\",\n  \"text\": \"user@example.com\"\n}\n\n// Select option\n{\n  \"action\": \"select\",\n  \"selector\": \"#country\",\n  \"value\": \"US\"\n}\n```\n\n### Content Extraction\n```javascript\n// Extract text\n{\n  \"action\": \"getText\",\n  \"selector\": \".article-content\"\n}\n\n// Get element attribute\n{\n  \"action\": \"getAttribute\",\n  \"selector\": \"img.logo\",\n  \"attribute\": \"src\"\n}\n\n// Evaluate JavaScript\n{\n  \"action\": \"evaluate\",\n  \"script\": \"return document.title;\"\n}\n```\n\n### Screenshots\n```javascript\n// Full page screenshot\n{\n  \"action\": \"screenshot\",\n  \"fullPage\": true\n}\n\n// Element screenshot\n{\n  \"action\": \"screenshot\",\n  \"selector\": \"#chart-container\"\n}\n```\n\n## Best Practices\n\n### Performance Optimization\n- Use headless mode for non-visual operations\n- Disable unnecessary resources (images, stylesheets)\n- Set appropriate timeouts for page loads\n- Reuse browser sessions when possible\n- Implement exponential backoff for retries\n\n### Reliability\n- Handle network failures gracefully\n- Implement proper error handling\n- Use explicit waits for dynamic content\n- Verify element existence before interaction\n- Set reasonable timeout values\n\n### Security\n- Validate all URLs before navigation\n- Sanitize extracted data\n- Use secure credential storage\n- Implement rate limiting\n- Monitor for suspicious patterns\n\n### Cost Optimization\n- Close browser sessions when done\n- Use session pooling for frequent operations\n- Set appropriate resource limits\n- Monitor usage patterns\n- Implement caching where appropriate\n\n## Integration Patterns\n\n### With Memory Service\n```\nBrowser  Memory Service\n- Store extracted data in memory\n- Cache frequently accessed pages\n- Share session state across agents\n```\n\n### With Identity Service\n```\nBrowser  Identity Service\n- Authenticate browser sessions\n- Access credentials for protected sites\n- Manage authentication tokens\n```\n\n### With Code Interpreter\n```\nBrowser  Code Interpreter\n- Process scraped data with code\n- Transform extracted content\n- Analyze website data\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**Page Load Timeout**\n- Symptom: Page takes too long to load\n- Solution: Increase timeout or optimize target page\n\n**Element Not Found**\n- Symptom: Cannot locate page element\n- Solution: Use explicit waits or verify selector\n\n**JavaScript Errors**\n- Symptom: Page JavaScript fails\n- Solution: Check console logs, handle errors\n\n**Session Terminated**\n- Symptom: Browser session unexpectedly ends\n- Solution: Check resource limits and session timeout\n\n**Authentication Required**\n- Symptom: Cannot access protected pages\n- Solution: Configure credentials via Identity service\n\n## Monitoring\n\n### Key Metrics\n- **Session Count**: Number of active browser sessions\n- **Success Rate**: Percentage of successful operations\n- **Page Load Time**: Average time to load pages\n- **Error Rate**: Percentage of failed operations\n- **Resource Usage**: CPU and memory utilization\n\n### CloudWatch Integration\n```bash\n# Query browser metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/BedrockAgentCore/Browser \\\n  --metric-name SessionCount \\\n  --dimensions Name=AgentId,Value=<AGENT_ID> \\\n  --start-time <START> \\\n  --end-time <END> \\\n  --period 3600 \\\n  --statistics Average\n```\n\n### Logging\n```bash\n# View browser execution logs\naws logs tail /aws/bedrock-agentcore/browser/<AGENT_ID> \\\n  --follow \\\n  --format short\n```\n\n## Performance Considerations\n\n### Optimization Techniques\n1. **Disable Unnecessary Resources**: Turn off images/stylesheets when not needed\n2. **Use Headless Mode**: Faster execution without rendering overhead\n3. **Implement Caching**: Cache static resources and repeated queries\n4. **Parallel Execution**: Run multiple browser sessions concurrently\n5. **Smart Waiting**: Use explicit waits instead of fixed delays\n\n### Scaling Patterns\n- **Horizontal Scaling**: Launch multiple browser instances\n- **Session Pooling**: Reuse browser sessions for efficiency\n- **Request Queuing**: Queue browser operations during high load\n- **Regional Distribution**: Distribute load across AWS regions\n\n## Additional Resources\n\n- **AWS Documentation**: [Bedrock AgentCore Browser](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/browser.html)\n- **Best Practices**: [Browser Automation Guide](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/browser-best-practices.html)\n- **API Reference**: [Browser API](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/)\n- **Selector Reference**: [CSS Selectors](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)\n\n---\n\n**Related Services**:\n- [Runtime Service](../runtime/README.md) - Agent execution\n- [Code Interpreter](../code-interpreter/README.md) - Data processing\n- [Memory Service](../memory/README.md) - State management\n- [Observability Service](../observability/README.md) - Monitoring\n",
        "skills/autonomous-cloud-orchestration/services/code-interpreter/README.md": "# AgentCore Code Interpreter Service\n\n> **Status**:  Available\n\n## Overview\n\nAmazon Bedrock AgentCore Code Interpreter enables agents to securely execute code in isolated sandbox environments, supporting complex data analysis workflows and computational tasks.\n\n## Core Capabilities\n\n### Secure Execution\n- **Isolated Sandboxes**: Each code execution runs in a completely isolated environment\n- **No Cross-Contamination**: Sessions are independent with no shared state\n- **Enterprise Security**: Meets enterprise security and compliance requirements\n- **Resource Controls**: Configurable limits and timeout controls for execution\n\n### Framework Integration\n- **Popular Frameworks**: Seamless integration with LangGraph, CrewAI, Strands, and other agent frameworks\n- **Multi-Language Support**: Execute code in Python, JavaScript, and other languages\n- **Advanced Configuration**: Extensive customization options for runtime environments\n- **Custom Runtimes**: Support for specialized runtime configurations\n\n### Data Processing\n- **File Operations**: Upload and download files for processing\n- **Multi-Modal Data**: Handle structured and unstructured data\n- **Result Formatting**: Format and visualize execution results\n- **Error Handling**: Comprehensive error reporting and debugging support\n\n## Use Cases\n\n### Data Analysis and Transformation\nEnable agents to:\n- Process and analyze datasets\n- Transform data formats\n- Perform statistical calculations\n- Generate data insights\n\n### Complex Computational Workflows\nSupport scenarios like:\n- Running scientific computations\n- Executing business logic calculations\n- Processing batch operations\n- Performing iterative algorithms\n\n### Visualization and Reporting\nAllow agents to:\n- Generate charts and graphs\n- Create formatted reports\n- Build visualizations from data\n- Export results in various formats\n\n### Dynamic Code Testing\nEnable agents to:\n- Test code snippets dynamically\n- Validate logic and algorithms\n- Debug code execution issues\n- Prototype solutions quickly\n\n## Architecture\n\n### Execution Flow\n\n```\nAgent Request\n    \n\n  Code Interpreter Service               \n  - Parse code execution request         \n  - Validate code and parameters         \n  - Allocate isolated sandbox            \n\n    \n\n  Sandbox Environment                    \n  - Execute code in isolation            \n  - Process data and files               \n  - Generate outputs                     \n  - Capture errors and logs              \n\n    \n\n  Result Processing                      \n  - Format execution results             \n  - Package outputs and artifacts        \n  - Return to agent                      \n\n```\n\n### Security Model\n\n1. **Sandbox Isolation**: Each execution runs in a completely isolated environment\n2. **Resource Limits**: CPU, memory, and time limits prevent resource exhaustion\n3. **Network Restrictions**: Controlled network access from sandbox environments\n4. **Data Encryption**: Data at rest and in transit is encrypted\n5. **Audit Logging**: All code executions are logged for compliance\n\n## Configuration\n\n### Basic Setup\n\n```bash\n# Configure code interpreter for agent\naws bedrock-agentcore-control configure-code-interpreter \\\n  --agent-id <AGENT_ID> \\\n  --execution-timeout 300 \\\n  --memory-limit 2048 \\\n  --region <REGION>\n```\n\n### Custom Runtime Configuration\n\n```bash\n# Set custom runtime environment\naws bedrock-agentcore-control update-code-interpreter-runtime \\\n  --agent-id <AGENT_ID> \\\n  --runtime-config '{\n    \"language\": \"python3.11\",\n    \"packages\": [\"pandas\", \"numpy\", \"matplotlib\"],\n    \"environment\": {\n      \"CUSTOM_VAR\": \"value\"\n    }\n  }' \\\n  --region <REGION>\n```\n\n## Best Practices\n\n### Code Security\n- Validate all code inputs before execution\n- Implement input sanitization for user-provided code\n- Use resource limits to prevent denial of service\n- Monitor code execution patterns for anomalies\n\n### Performance Optimization\n- Cache common dependencies in runtime images\n- Use appropriate timeout values for expected workload\n- Optimize code for execution within timeout limits\n- Batch similar operations when possible\n\n### Error Handling\n- Implement comprehensive error catching in code\n- Provide clear error messages for debugging\n- Log execution details for troubleshooting\n- Use structured output formats for results\n\n### Data Management\n- Minimize data transfer in and out of sandboxes\n- Use streaming for large data processing\n- Clean up temporary files after execution\n- Implement data validation before processing\n\n## Integration Patterns\n\n### With Memory Service\n```\nCode Interpreter  Memory Service\n- Store execution results in memory\n- Retrieve past computation results\n- Share data across agent sessions\n```\n\n### With Identity Service\n```\nCode Interpreter  Identity Service\n- Authenticate code execution requests\n- Access credentials for external APIs\n- Manage permissions for data access\n```\n\n### With Observability Service\n```\nCode Interpreter  Observability Service\n- Trace code execution workflows\n- Monitor performance metrics\n- Log execution events\n- Alert on execution failures\n```\n\n## Troubleshooting\n\n### Common Issues\n\n**Execution Timeout**\n- Symptom: Code execution exceeds timeout limit\n- Solution: Increase timeout or optimize code performance\n\n**Memory Limit Exceeded**\n- Symptom: Code runs out of memory\n- Solution: Increase memory limit or process data in chunks\n\n**Package Import Errors**\n- Symptom: Required packages not found\n- Solution: Configure custom runtime with needed packages\n\n**Permission Denied**\n- Symptom: Cannot access required resources\n- Solution: Configure IAM permissions for code interpreter\n\n## Monitoring\n\n### Key Metrics\n- **Execution Count**: Number of code executions\n- **Success Rate**: Percentage of successful executions\n- **Average Duration**: Mean execution time\n- **Error Rate**: Percentage of failed executions\n- **Resource Utilization**: CPU and memory usage\n\n### CloudWatch Integration\n```bash\n# Query execution metrics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/BedrockAgentCore/CodeInterpreter \\\n  --metric-name ExecutionCount \\\n  --dimensions Name=AgentId,Value=<AGENT_ID> \\\n  --start-time <START> \\\n  --end-time <END> \\\n  --period 3600 \\\n  --statistics Sum\n```\n\n## Additional Resources\n\n- **AWS Documentation**: [Bedrock AgentCore Code Interpreter](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter.html)\n- **Security Best Practices**: [Secure Code Execution](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/code-interpreter-security.html)\n- **API Reference**: [Code Interpreter API](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/)\n\n---\n\n**Related Services**:\n- [Runtime Service](../runtime/README.md) - Agent execution environment\n- [Memory Service](../memory/README.md) - State management\n- [Observability Service](../observability/README.md) - Monitoring and tracing\n",
        "skills/autonomous-cloud-orchestration/services/gateway/README.md": "# Gateway Service\n\nThe Gateway service converts REST APIs into MCP tools that AI agents can use. It handles authentication, schema validation, and request routing.\n\n## Quick Start\n\n### Prerequisites\n- AWS CLI configured with appropriate permissions\n- An existing Gateway (created via AWS Console or CLI)\n- OpenAPI schema for your target API\n\n### Deploy a Gateway Target\n\n**Step 1: Upload OpenAPI schema to S3**\n```bash\naws s3 cp my-api-openapi.yaml s3://<BUCKET_NAME>/schemas/\n```\n\n**Step 2: Create credential provider (API Key auth only)**\n```bash\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --api-key \"YOUR_API_KEY\" \\\n  --region us-west-2\n```\n\n**Step 3: Create gateway target**\n```bash\naws bedrock-agentcore-control create-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --name MyAPITarget \\\n  --endpoint-configuration '{\"openApiSchema\": {\"s3\": {\"uri\": \"s3://<BUCKET_NAME>/schemas/my-api-openapi.yaml\"}}}' \\\n  --credential-provider-configurations '[{\"credentialProviderType\": \"GATEWAY_API_KEY_CREDENTIAL_PROVIDER\", \"apiKeyCredentialProvider\": {\"providerArn\": \"arn:aws:bedrock-agentcore:us-west-2:<ACCOUNT_ID>:api-key-credential-provider/MyAPICredentialProvider\"}}]' \\\n  --region us-west-2\n```\n\n**Step 4: Verify deployment**\n```bash\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --target-identifier <TARGET_ID> \\\n  --region us-west-2\n```\n\n## Authentication Options\n\n### Outbound (Accessing External APIs)\n\n| Target Type | IAM | OAuth 2LO | OAuth 3LO | API Key |\n|-------------|-----|-----------|-----------|---------|\n| Lambda function | Yes | No | No | No |\n| API Gateway | Yes | No | No | Yes |\n| OpenAPI schema | No | Yes | Yes | Yes |\n| Smithy schema | Yes | Yes | Yes | No |\n| MCP server | No | Yes | No | No |\n\n### Inbound (Who Can Invoke Tools)\n- **IAM**: AWS IAM credentials\n- **JWT**: Tokens from identity providers (Cognito, Entra ID)\n- **No Auth**: Open access (use with caution)\n\n## Documentation\n\n| Document | Description |\n|----------|-------------|\n| [Deployment Strategies](deployment-strategies.md) | Credential provider patterns, multi-environment setup, key rotation |\n| [Troubleshooting Guide](troubleshooting-guide.md) | Common errors, diagnosis steps, solutions |\n| [Deploy Template Script](deploy-template.sh) | Automated deployment script |\n| [Validate Deployment Script](validate-deployment.sh) | Post-deployment verification |\n\n## Common Operations\n\n### List Gateway Targets\n```bash\naws bedrock-agentcore-control list-gateway-targets \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --region us-west-2\n```\n\n### Update Credential Provider\n```bash\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --api-key \"NEW_API_KEY\" \\\n  --region us-west-2\n```\n\n### Delete Gateway Target\n```bash\naws bedrock-agentcore-control delete-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --target-identifier <TARGET_ID> \\\n  --region us-west-2\n```\n\n## Related Resources\n\n- [Cross-Service Credential Management](../../cross-service/credential-management.md)\n- [AWS Gateway Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html)\n- [Bedrock AgentCore CLI Reference](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/bedrock-agentcore-control/index.html)\n",
        "skills/autonomous-cloud-orchestration/services/gateway/deployment-strategies.md": "# AgentCore Gateway Deployment Strategies\n\n## Overview\n\nThis reference guide covers different deployment strategies for AWS Bedrock AgentCore Gateway targets, including credential management approaches, multi-environment patterns, and rollback procedures.\n\n## Credential Provider Strategies\n\n### Strategy 1: Shared Provider (Recommended for Most Cases)\n\n**Concept**: Create ONE credential provider and share across all gateway targets\n\n**Setup**:\n```bash\n# Create shared provider with API key (run once)\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name SharedAPICredentialProvider \\\n  --api-key \"YOUR_API_KEY\" \\\n  --profile default --region us-west-2\n```\n\n**Environment Configuration**:\n```bash\n# .env.gateway-a\nGATEWAY_IDENTIFIER=gateway-a-abc123xyz\nCREDENTIAL_PROVIDER_NAME=SharedAPICredentialProvider  # Same for all\n\n# .env.gateway-b\nGATEWAY_IDENTIFIER=gateway-b-def456uvw\nCREDENTIAL_PROVIDER_NAME=SharedAPICredentialProvider  # Same for all\n```\n\n**Benefits**:\n-  Simplified key management - single key to rotate\n-  Reduced operational overhead\n-  Consistent authentication across all gateways\n-  Easier compliance and auditing\n\n**Use Cases**:\n- Same API, multiple gateway deployments\n- Development/Testing/Production gateways\n- Regional deployments (us-west-2, eu-west-1)\n\n**Trade-offs**:\n- Less isolation between gateways (all or nothing key rotation)\n\n### Strategy 2: Isolated Provider (Per-Gateway)\n\n**Concept**: Create UNIQUE credential provider for each gateway\n\n**Setup**:\n```bash\n# Create provider for Gateway A\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name GatewayAAPICredentialProvider \\\n  --api-key \"API_KEY_A\" \\\n  --profile default --region us-west-2\n\n# Create provider for Gateway B\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name GatewayBAPICredentialProvider \\\n  --api-key \"API_KEY_B\" \\\n  --profile default --region us-west-2\n```\n\n**Environment Configuration**:\n```bash\n# .env.gateway-a\nGATEWAY_IDENTIFIER=gateway-a-abc123xyz\nCREDENTIAL_PROVIDER_NAME=GatewayAAPICredentialProvider  # Unique\n\n# .env.gateway-b\nGATEWAY_IDENTIFIER=gateway-b-def456uvw\nCREDENTIAL_PROVIDER_NAME=GatewayBAPICredentialProvider  # Unique\n```\n\n**Benefits**:\n-  Complete isolation between gateways\n-  Independent key rotation per environment\n-  Different API keys for different use cases\n-  Better security boundaries\n\n**Use Cases**:\n- Production vs Development with different API keys\n- Different APIs for different gateways\n- Compliance requiring environment separation\n- Testing new API versions in isolation\n\n**Trade-offs**:\n- More complex key management\n- Multiple keys to rotate and maintain\n\n### Strategy 3: Tiered (Shared + Isolated)\n\n**Concept**: Hybrid approach with shared provider for non-prod, isolated for production\n\n**Setup**:\n```bash\n# Shared provider for dev/test\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name DevTestAPICredentialProvider \\\n  --api-key \"DEV_TEST_API_KEY\" \\\n  --profile default --region us-west-2\n\n# Isolated provider for production\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name ProdAPICredentialProvider \\\n  --api-key \"PROD_API_KEY\" \\\n  --profile default --region us-west-2\n```\n\n**Environment Configuration**:\n```bash\n# .env.development\nGATEWAY_IDENTIFIER=dev-gateway-abc123xyz\nCREDENTIAL_PROVIDER_NAME=DevTestAPICredentialProvider\n\n# .env.staging\nGATEWAY_IDENTIFIER=staging-gateway-def456uvw\nCREDENTIAL_PROVIDER_NAME=DevTestAPICredentialProvider\n\n# .env.production\nGATEWAY_IDENTIFIER=prod-gateway-ghi789rst\nCREDENTIAL_PROVIDER_NAME=ProdAPICredentialProvider\n```\n\n**Benefits**:\n-  Balance of simplicity and security\n-  Production isolation with dev/test convenience\n-  Easier testing in non-prod environments\n-  Production key remains protected\n\n**Use Cases**:\n- Most common enterprise pattern\n- Clear separation between environments\n- Controlled production access\n\n## Multi-Account Deployment\n\nWhen deploying across multiple AWS accounts:\n\n### Setup\n\n1. **Credential Provider per Account**:\n   ```bash\n   # Account 1 (Dev)\n   aws bedrock-agentcore-control create-api-key-credential-provider \\\n     --name APICredentialProvider \\\n     --api-key \"DEV_API_KEY\" \\\n     --profile dev-account\n\n   # Account 2 (Prod)\n   aws bedrock-agentcore-control create-api-key-credential-provider \\\n     --name APICredentialProvider \\\n     --api-key \"PROD_API_KEY\" \\\n     --profile prod-account\n   ```\n\n2. **Centralized Configuration**:\n   ```bash\n   # .env.dev\n   ACCOUNT_ID=123456789012\n   GATEWAY_IDENTIFIER=dev-gateway-abc123xyz\n   AWS_PROFILE=dev-account\n\n   # .env.prod\n   ACCOUNT_ID=987654321098\n   GATEWAY_IDENTIFIER=prod-gateway-abc123xyz\n   AWS_PROFILE=prod-account\n   ```\n\n3. **Cross-Account Deployment Script**:\n   ```bash\n   #!/bin/bash\n   ENV_FILE=$1\n\n   # Load environment\n   export $(cat $ENV_FILE | xargs)\n\n   # Get AWS account ID\n   export CDK_DEFAULT_ACCOUNT=$(aws sts get-caller-identity \\\n     --profile $AWS_PROFILE \\\n     --query Account --output text)\n\n   # Deploy\n   npm run build && cdk deploy --profile $AWS_PROFILE --require-approval never\n   ```\n\n## Key Rotation Procedures\n\n### Shared Provider Strategy\n\n**Manual Rotation**:\n```bash\n# 1. Update key in provider\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name SharedAPICredentialProvider \\\n  --api-key \"NEW_API_KEY\" \\\n  --profile default --region us-west-2\n\n# 2. Restart gateway targets (if needed) to pick up new key\n```\n\n**Automated Rotation**:\n- Use AWS Secrets Manager rotation (if supported by credential provider)\n- Triggered by CloudWatch Events schedule\n- Lambda function handles key generation/update\n\n### Isolated Provider Strategy\n\n**Per-Gateway Rotation**:\n```bash\n# Rotate dev environment only\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name DevAPICredentialProvider \\\n  --api-key \"NEW_DEV_KEY\"\n\n# Production remains unchanged\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name ProdAPICredentialProvider \\\n  --api-key \"EXISTING_PROD_KEY\"\n```\n\n## Rollback Procedures\n\n### Rollback Failed Deployment\n\n```bash\n# List CloudFormation stacks\naws cloudformation list-stacks --stack-status-filter UPDATE_ROLLBACK_FAILED\n\n# Get previous stack configuration\naws cloudformation describe-stack-resources --stack-name StackName\n\n# Rollback to previous version\naws cloudformation continue-update-rollback --stack-name StackName\n```\n\n### Rollback Credential Changes\n\n```bash\n# If new API key is causing issues, restore previous key\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name APICredentialProvider \\\n  --api-key \"PREVIOUS_WORKING_KEY\"\n```\n\n## Monitoring and Alerting\n\n### CloudWatch Metrics to Monitor\n\n- **Gateway Target Status**: Monitor target health\n- **API Request Count**: Track usage per gateway\n- **Error Rates**: 4xx and 5xx errors\n- **Latency**: P95, P99 response times\n- **Credential Provider Errors**: Secret access failures\n\n### CloudWatch Alarms\n\n```typescript\n// CDK Example: Create alarm for high error rate\nnew cloudwatch.Alarm(this, 'HighErrorRate', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/BedrockAgentCore',\n    metricName: 'TargetErrorRate',\n    dimensionsMap: {\n      GatewayId: gatewayId,\n      TargetId: targetId,\n    },\n    statistic: 'avg',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 2,\n  datapointsToAlarm: 2,\n  comparisonOperator: cloudwatch.ComparisonOperator.GREATER_THAN_THRESHOLD,\n  alarmDescription: 'Error rate exceeds 10%',\n  actionsEnabled: true,\n});\n```\n\n## Cost Optimization\n\n### Cost Considerations\n\n1. **AgentCore Gateway**: Pay per tool invocation\n   - Optimize schema to reduce unnecessary API calls\n   - Cache frequently accessed data in schema descriptions\n   - Use batch operations where available\n\n2. **S3 Asset Storage**: Negligible (<$0.01/month)\n   - Schema files are small\n   - CDK automatically cleans up old versions\n\n3. **Lambda (GatewayRoleUpdater)**: ~$0.01 per deployment\n   - Covered by AWS Lambda free tier (1M requests/month)\n   - Only runs during deployment/update\n\n4. **Secrets Manager**: ~$0.40/month per secret\n   - Use shared provider to minimize secret count\n   - Rotate secrets on schedule to maintain security\n\n5. **API Calls (RapidAPI Example)**:\n   - Free tier: 100 requests/day\n   - Paid tiers: From $10/month\n   - Optimize by embedding IDs in schema\n\n### Optimization Strategies\n\n**Schema Optimization**:\n```yaml\n# Embed common IDs to reduce API calls by 50%\ninfo:\n  description: |\n    COMMON LEAGUE IDs:\n    - Premier League: 39\n    - Champions League: 2\n    - World Cup: 1\n```\n\n**Credential Provider Sharing**:\n- Single provider for all gateways = 1 secret = $0.40/month\n- Separate providers = N secrets = $0.40N/month\n\n## Security Best Practices\n\n### Credential Management\n- Never commit API keys to source control\n- Use AWS Secrets Manager via credential providers\n- Rotate keys regularly (quarterly minimum)\n- Use different keys for different environments\n\n### IAM Permissions\n- Custom Resource Lambda has scoped permissions\n- Only allows access to Gateway service roles\n- Follows principle of least privilege\n- Audit policy versions regularly\n\n### Network Security\n- Ensure Gateway is in VPC if required\n- Use AWS PrivateLink for on-premises integrations\n- Enable encryption in transit (TLS 1.2+)\n- Verify API endpoints use HTTPS\n\n## Common Patterns\n\n### Pattern 1: Development Pipeline\n\n```bash\n# Branch-based deployment\nif [ \"$BRANCH\" = \"main\" ]; then\n  ./deploy.sh .env.production\nelif [ \"$BRANCH\" = \"develop\" ]; then\n  ./deploy.sh .env.staging\nelse\n  ./deploy.sh .env.development\nfi\n```\n\n### Pattern 2: Regional Deployment\n\n```bash\n# Deploy to multiple regions\nfor region in us-west-2 eu-west-1 ap-southeast-1; do\n  export AWS_REGION=$region\n  export GATEWAY_IDENTIFIER=\"my-gateway-${region}\"\n  npm run build && cdk deploy --require-approval never\ndone\n```\n\n### Pattern 3: Blue-Green Deployment\n\n```bash\n# Deploy to blue environment\n./deploy.sh .env.blue\n\n# Test blue environment\n./test-target.sh blue\n\n# Switch to green if blue is healthy\n./deploy.sh .env.green\n```\n\n## Migration Strategies\n\n### Migrating from Manual to CDK Management\n\n1. **Discovery Phase**:\n   ```bash\n   # Document existing targets\n   aws bedrock-agentcore-control list-gateway-targets \\\n     --gateway-identifier existing-gateway \\\n     --profile default --region us-west-2\n   ```\n\n2. **Schema Extraction**:\n   - Export existing OpenAPI schemas\n   - Audit and optimize schema descriptions\n   - Embed common IDs for performance\n\n3. **CDK Implementation**:\n   - Create stack with existing target configuration\n   - Import existing credential provider\n   - Add GatewayRoleUpdater for IAM automation\n\n4. **Cutover**:\n   - Deploy to new gateway first (test)\n   - Update AI agents to use new target\n   - Decommission old target after verification\n\n## Additional References\n\n- **Main Skill Documentation**: [`../../SKILL.md`](../../SKILL.md)\n- **Troubleshooting Guide**: [`./troubleshooting-guide.md`](./troubleshooting-guide.md)\n- **Deployment Template Script**: [`./deploy-template.sh`](./deploy-template.sh)\n- **Credential Management**: [`../../cross-service/credential-management.md`](../../cross-service/credential-management.md)\n",
        "skills/autonomous-cloud-orchestration/services/gateway/troubleshooting-guide.md": "# AgentCore Gateway Troubleshooting Guide\n\n## Quick Diagnosis\n\n### Symptom: Target Creation Fails\n\n**Error Message**: `\"Gateway target creation failed\"`\n\n**Diagnosis Steps**:\n1. Verify gateway exists\n   ```bash\n   aws bedrock-agentcore-control get-gateway \\\n     --gateway-identifier <GATEWAY_ID> \\\n     --profile default --region us-west-2\n   ```\n\n2. Check credential provider exists (if using API key auth)\n   ```bash\n   aws bedrock-agentcore-control get-api-key-credential-provider \\\n     --name <PROVIDER_NAME> \\\n     --profile default --region us-west-2\n   ```\n\n3. List existing targets\n   ```bash\n   aws bedrock-agentcore-control list-gateway-targets \\\n     --gateway-identifier <GATEWAY_ID> \\\n     --profile default --region us-west-2\n   ```\n\n**Common Causes**:\n- Gateway ID incorrect or gateway doesn't exist\n- Credential provider name misspelled (for API key auth)\n- OpenAPI schema syntax error\n- S3 bucket permissions issue for schema\n\n---\n\n## Permission Errors\n\n### Error: \"User is not authorized to perform: bedrock-agentcore:GetResourceApiKey\"\n\n**Full Error**:\n```\nUser: arn:aws:sts::<ACCOUNT_ID>:assumed-role/GatewayServiceRole is not\nauthorized to perform: bedrock-agentcore:GetResourceApiKey on resource: *\n```\n\n**Root Cause**: Gateway service role missing credential provider access permissions\n\n**Diagnosis**:\n```bash\n# Get gateway role ARN\nGATEWAY_ROLE=$(aws bedrock-agentcore-control get-gateway \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --query 'roleArn' --output text \\\n  --profile default --region us-west-2)\n\necho $GATEWAY_ROLE\n\n# Extract role name\nROLE_NAME=$(echo $GATEWAY_ROLE | cut -d'/' -f2)\n\n# Check attached policies\naws iam list-attached-role-policies \\\n  --role-name $ROLE_NAME \\\n  --profile default --region us-west-2\n```\n\n**Solution**: Add required permissions to gateway role:\n```bash\ncat > /tmp/gateway-policy.json <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"GetResourceApiKey\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"bedrock-agentcore:GetResourceApiKey\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"GetWorkloadAccessToken\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"bedrock-agentcore:GetWorkloadAccessToken\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"GetCredentials\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"secretsmanager:GetSecretValue\"],\n      \"Resource\": [\"arn:aws:secretsmanager:us-west-2:<ACCOUNT_ID>:secret:bedrock-agentcore-identity!*\"]\n    }\n  ]\n}\nEOF\n\n# Get policy ARN from role\nPOLICY_ARN=$(aws iam list-attached-role-policies \\\n  --role-name $ROLE_NAME \\\n  --query 'AttachedPolicies[0].PolicyArn' \\\n  --output text \\\n  --profile default --region us-west-2)\n\n# Create new policy version\naws iam create-policy-version \\\n  --policy-arn $POLICY_ARN \\\n  --policy-document file:///tmp/gateway-policy.json \\\n  --set-as-default \\\n  --profile default --region us-west-2\n```\n\n---\n\n### Error: \"AccessDeniedException: Secrets Manager\"\n\n**Full Error**:\n```\nAccessDeniedException: User: arn:aws:sts::<ACCOUNT_ID>:assumed-role/GatewayServiceRole\nis not authorized to perform: secretsmanager:GetSecretValue on resource: ...\n```\n\n**Root Cause**: Gateway cannot access Secrets Manager for credential provider\n\n**Solution**: Add `secretsmanager:GetSecretValue` permission to gateway role (see above)\n\n---\n\n## Credential Provider Issues\n\n### Error: \"Credential provider not found\"\n\n**Diagnosis**:\n```bash\n# Check if provider exists\naws bedrock-agentcore-control get-api-key-credential-provider \\\n  --name <PROVIDER_NAME> \\\n  --profile default --region us-west-2\n\n# List all providers\naws bedrock-agentcore-control list-api-key-credential-providers \\\n  --profile default --region us-west-2\n```\n\n**Solution**:\n```bash\n# Create provider if missing\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name <PROVIDER_NAME> \\\n  --api-key \"YOUR_API_KEY\" \\\n  --profile default --region us-west-2\n```\n\n---\n\n### Error: \"Invalid API key\"\n\n**Symptom**: API calls return 403 or authentication errors\n\n**Diagnosis**:\n```bash\n# Check API key format in secret\nSECRET=$(aws secretsmanager get-secret-value \\\n  --secret-id \"arn:aws:secretsmanager:us-west-2:<ACCOUNT_ID>:secret:bedrock-agentcore-identity/default/apikeycredentialprovider/<PROVIDER_NAME>-AbCdEf\" \\\n  --query 'SecretString' --output text \\\n  --profile default --region us-west-2)\n\necho $SECRET | jq '.'\n# Should be: {\"apiKey\": \"your-key-here\"}\n```\n\n**Solution**:\n```bash\n# Use the correct update command (not secretsmanager put-secret-value)\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name <PROVIDER_NAME> \\\n  --api-key \"YOUR_VALID_API_KEY\" \\\n  --profile default --region us-west-2\n```\n\n**Common Mistake**: Using `secretsmanager put-secret-value` directly bypasses credential provider validation\n\n---\n\n## OpenAPI Schema Issues\n\n### Error: \"Invalid OpenAPI schema\"\n\n**Diagnosis**:\n```bash\n# Validate OpenAPI schema locally\nnpm install -g @apidevtools/swagger-cli\nswagger-cli validate schemas/my-api-openapi.yaml\n\n# Check for unsupported constructs\ngrep -n \"oneOf\\|anyOf\\|allOf\" schemas/my-api-openapi.yaml\n# These constructs are not supported in Gateway\n```\n\n**Common Issues**:\n1. **oneOf/anyOf/allOf**: Not supported, use simple types\n2. **Missing operationId**: All operations must have operationId\n3. **Invalid $ref references**: Must point to valid components\n4. **YAML syntax errors**: Use online YAML validator\n\n**Solution**: Use OpenAPI 3.0 simple types only:\n```yaml\n#  Bad - unsupported\nschema:\n  oneOf:\n    - type: string\n    - type: number\n\n#  Good - simple type\nschema:\n  type: string\n```\n\n---\n\n### Error: \"Schema size exceeds limit\"\n\n**Root Cause**: OpenAPI schema too large for Gateway\n\n**Solutions**:\n1. Remove unused endpoints from schema\n2. Simplify descriptions (keep essential info only)\n3. Remove redundant component definitions\n4. Split into multiple targets if necessary\n\n---\n\n## API Call Issues\n\n### Error: \"Rate limit exceeded\"\n\n**Symptom**: API calls return 429 Too Many Requests\n\n**Root Cause**: Gateway or upstream API rate limits\n\n**Diagnosis**:\n```bash\n# Check gateway target status\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --target-identifier <TARGET_ID> \\\n  --profile default --region us-west-2\n```\n\n**Solutions**:\n1. **Check Gateway limits**: Verify gateway quota in AWS Console\n2. **Upstream API limits**: Check your API provider dashboard for limits\n3. **Optimize calls**: Use embedded IDs in schema to reduce API queries\n4. **Implement caching**: Cache responses when possible\n5. **Request limit increase**: Contact AWS support if needed\n\n---\n\n### Error: \"Invalid API host header\"\n\n**Root Cause**: API endpoint or host header configuration mismatch\n\n**Diagnosis**:\n```bash\n# Check schema for correct server URL\ngrep -A5 \"servers:\" schemas/my-api-openapi.yaml\n\n# Verify host header in security scheme\ngrep -A10 \"securitySchemes:\" schemas/my-api-openapi.yaml\n```\n\n**Solution**: Update OpenAPI schema with correct server URL and host header\n\n---\n\n### Error: \"Connection timeout\"\n\n**Symptom**: Gateway target calls fail with timeout errors\n\n**Root Cause**: Upstream API not responding within timeout limit\n\n**Solutions**:\n1. Verify upstream API is accessible\n2. Check network connectivity from Gateway\n3. Increase timeout in target configuration (if supported)\n4. Check if upstream API requires VPC configuration\n\n---\n\n## Target Status Issues\n\n### Target Status: \"FAILED\"\n\n**Diagnosis**:\n```bash\n# Get target details including status reason\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier <GATEWAY_ID> \\\n  --target-identifier <TARGET_ID> \\\n  --query '{status: status, statusReason: statusReason}' \\\n  --profile default --region us-west-2\n```\n\n**Common Status Reasons**:\n- `SCHEMA_VALIDATION_FAILED`: OpenAPI schema has errors\n- `CREDENTIAL_PROVIDER_NOT_FOUND`: API key provider doesn't exist\n- `PERMISSION_DENIED`: IAM permissions missing\n\n---\n\n### Target Status: \"PENDING\" for too long\n\n**Root Cause**: Target creation stuck\n\n**Solutions**:\n1. Check if all dependencies exist (gateway, credential provider)\n2. Delete and recreate target\n3. Check AWS service health dashboard\n\n---\n\n## Testing and Verification\n\n### How to Test Target Deployment\n\n```bash\n#!/bin/bash\n# test-target.sh\n\nGATEWAY_ID=$1\nTARGET_ID=$2\n\n# Test 1: List targets\necho \"Listing gateway targets...\"\naws bedrock-agentcore-control list-gateway-targets \\\n  --gateway-identifier $GATEWAY_ID \\\n  --profile default --region us-west-2\n\n# Test 2: Get target details\necho \"Getting target details...\"\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier $GATEWAY_ID \\\n  --target-identifier $TARGET_ID \\\n  --profile default --region us-west-2\n\n# Test 3: Get tools from target\necho \"Getting tools...\"\naws bedrock-agentcore-control get-gateway-target \\\n  --gateway-identifier $GATEWAY_ID \\\n  --target-identifier $TARGET_ID \\\n  --query 'tools' \\\n  --profile default --region us-west-2\n```\n\n---\n\n## Common Error Summary Table\n\n| Error | Likely Cause | Solution |\n|-------|-------------|----------|\n| \"not authorized to perform: bedrock-agentcore:GetResourceApiKey\" | Missing IAM permissions | Add permissions to gateway role |\n| \"Credential provider not found\" | Provider doesn't exist or name typo | Create provider with `create-api-key-credential-provider` |\n| \"Invalid API key\" | Key format wrong or key invalid | Use `update-api-key-credential-provider` to update |\n| \"Invalid OpenAPI schema\" | Unsupported constructs (oneOf/anyOf/allOf) | Remove unsupported constructs, use simple types |\n| \"Invalid API host header\" | Host header doesn't match endpoint | Update OpenAPI schema with correct host |\n| \"Rate limit exceeded\" | Too many API calls | Check limits, implement caching |\n| \"Connection timeout\" | Upstream API not responding | Verify API accessibility |\n| \"Target status FAILED\" | Schema or credential issues | Check statusReason in get-gateway-target |\n\n---\n\n## Escalation Path\n\nIf issues persist after troubleshooting:\n\n1. **Check AWS Documentation**: https://docs.aws.amazon.com/bedrock-agentcore/\n2. **AWS Support**: Open case with Bedrock AgentCore service\n3. **Community**: AWS Developer Forums\n\n**Information to gather for AWS Support**:\n- Gateway ID and target ID\n- Error messages and timestamps\n- OpenAPI schema (sanitized)\n- IAM role and policy configuration\n",
        "skills/autonomous-cloud-orchestration/services/identity/README.md": "# AgentCore Identity Service\n\n> **Status**:  Available\n\n## Overview\n\nAmazon Bedrock AgentCore Identity is an identity and credential management service designed specifically for AI agents and automated workloads. It provides secure authentication, authorization, and credential management capabilities that enable agents and tools to access AWS resources and third-party services on behalf of users while maintaining strict security controls and audit trails.\n\n## Core Capabilities\n\n### Centralized Agent Identity Management\n- **Workload Identities**: Agent identities implemented as workload identities with specialized attributes\n- **Unified Directory**: Create, manage, and organize agent identities through unified directory service\n- **Hierarchical Organization**: Group-based access controls and hierarchical organization\n- **Cross-Environment**: Consistent identity management regardless of deployment location\n\n### Secure Credential Storage\n- **Token Vault**: Securely store OAuth 2.0 tokens, client credentials, and API keys\n- **Encryption**: Comprehensive encryption at rest and in transit\n- **Access Controls**: Strict access controls with independent request validation\n- **Defense-in-Depth**: Protects end-user data from malicious or misbehaving agent code\n\n### OAuth 2.0 Flow Support\n- **Client Credentials Grant**: Machine-to-machine authentication (2LO)\n- **Authorization Code Grant**: User-delegated access (3LO)\n- **Built-in Providers**: Pre-configured providers for Google, GitHub, Slack, Salesforce\n- **Custom Providers**: Configurable OAuth 2.0 credential providers for custom integrations\n\n### Credential Provider Management\n- **API Key Providers**: Securely store and manage API keys\n- **OAuth Credential Providers**: Handle OAuth flow and token management\n- **Token Lifecycle**: Automatic token refresh and expiration handling\n- **Provider Discovery**: Automatically discover available credential providers\n\n### Agent Identity and Access Controls\n- **Impersonation Flow**: Agents access resources using provided credentials\n- **Audit Trails**: Maintain audit trails for all actions performed on behalf of users\n- **Request Verification**: Token signature verification, expiration checks, scope validation\n- **Identity-Aware Authorization**: Pass user context to agent code for dynamic decisions\n\n## Use Cases\n\n### Securing AI Agent Access\nEnable agents to:\n- Authenticate with external services securely\n- Access resources on behalf of users\n- Maintain proper audit trails\n- Implement least-privilege access patterns\n\n### Multi-Provider Authentication\nSupport scenarios like:\n- Different authentication methods for different APIs\n- Unified credential management across services\n- OAuth flows for user-delegated access\n- API key management for service accounts\n\n### Zero-Trust Security Models\nAllow implementation of:\n- No long-lived credentials in application code\n- Centralized, audited credential vault\n- Automated rotation to reduce attack window\n- Comprehensive access logging\n\n### Compliance and Auditing\nEnable teams to:\n- Generate reports for compliance audits (SOC2, ISO27001)\n- Implement periodic access reviews\n- Maintain secrets inventory\n- Enforce credential policies\n\n## Quick Start\n\n### Create API Key Credential Provider\n\n```bash\naws bedrock-agentcore-control create-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --api-key \"YOUR_API_KEY\" \\\n  --region us-west-2\n```\n\n### Create OAuth Credential Provider\n\n```bash\naws bedrock-agentcore-control create-oauth2-credential-provider \\\n  --name MyOAuthProvider \\\n  --client-id \"YOUR_CLIENT_ID\" \\\n  --client-secret \"YOUR_CLIENT_SECRET\" \\\n  --authorization-url \"https://provider.com/oauth/authorize\" \\\n  --token-url \"https://provider.com/oauth/token\" \\\n  --scopes '[\"read\", \"write\"]' \\\n  --region us-west-2\n```\n\n### Using Credentials with SDK\n\n```python\nfrom bedrock_agentcore.identity import CredentialProvider\n\n# Get credentials for external API\nprovider = CredentialProvider(\"MyAPICredentialProvider\")\napi_key = provider.get_api_key()\n\n# Get OAuth token\noauth_provider = CredentialProvider(\"MyOAuthProvider\")\ntoken = oauth_provider.get_access_token()\n```\n\n## Common Operations\n\n### List Credential Providers\n\n```bash\naws bedrock-agentcore-control list-api-key-credential-providers \\\n  --region us-west-2\n```\n\n### Update Credential Provider\n\n```bash\naws bedrock-agentcore-control update-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --api-key \"NEW_API_KEY\" \\\n  --region us-west-2\n```\n\n### Delete Credential Provider\n\n```bash\naws bedrock-agentcore-control delete-api-key-credential-provider \\\n  --name MyAPICredentialProvider \\\n  --region us-west-2\n```\n\n## Built-in OAuth Providers\n\nAgentCore Identity includes built-in providers for popular services:\n\n| Provider | Use Case |\n|----------|----------|\n| **Google** | Google Workspace, Gmail, Drive |\n| **GitHub** | Repository access, Actions |\n| **Slack** | Messaging, channel access |\n| **Salesforce** | CRM data access |\n\n## Best Practices\n\n### Security\n- Use credential providers instead of hardcoded credentials\n- Implement least-privilege access for each credential\n- Rotate credentials regularly (quarterly minimum)\n- Monitor credential usage with CloudWatch\n\n### Development\n- Use separate credential providers per environment\n- Implement proper error handling for credential access\n- Test credential flows in non-production first\n- Use SDK annotations for cleaner code\n\n### Operations\n- Set up alerts for credential access failures\n- Audit credential usage periodically\n- Implement automated rotation where possible\n- Document credential ownership and purpose\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Credential not found | Provider doesn't exist or name typo | Verify provider name with list command |\n| Invalid API key | Key expired or incorrect | Update credential provider with new key |\n| OAuth token expired | Token refresh failed | Check OAuth provider configuration |\n| Access denied | Insufficient permissions | Verify IAM policy for credential access |\n\n## Related Services\n\n- **[Gateway Service](../gateway/README.md)**: Uses Identity for MCP target authentication\n- **[Runtime Service](../runtime/README.md)**: Uses Identity for agent authentication\n- **[Memory Service](../memory/README.md)**: May use Identity for data encryption\n\n## References\n\n- [AWS Identity Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity.html)\n- [Credential Provider Setup](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/identity-outbound-credential-provider.html)\n- [Identity Features](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/key-features-and-benefits.html)\n- [Securing AI Agents Blog](https://aws.amazon.com/blogs/security/securing-ai-agents-with-amazon-bedrock-agentcore-identity/)\n",
        "skills/autonomous-cloud-orchestration/services/memory/README.md": "# AgentCore Memory Service\n\n> **Status**:  Available\n\n## Overview\n\nAmazon Bedrock AgentCore Memory is a fully managed service that gives AI agents the ability to remember past interactions, enabling more intelligent, context-aware, and personalized conversations. It addresses a fundamental challenge in agentic AI: statelessness. Without memory capabilities, AI agents treat each interaction as a new instance with no knowledge of previous conversations.\n\n## Memory Types\n\nAgentCore Memory offers two types of memory that work together:\n\n### Short-Term Memory\nCaptures turn-by-turn interactions within a single session, allowing agents to maintain immediate context without requiring users to repeat information.\n\n**Example**: When a user asks \"What's the weather like in Seattle?\" and follows up with \"What about tomorrow?\", the agent relies on recent conversation history to understand that \"tomorrow\" refers to Seattle weather.\n\n### Long-Term Memory\nAutomatically extracts and stores key insights from conversations across multiple sessions, including user preferences, important facts, and session summaries for persistent knowledge retention.\n\n**Example**: If a customer mentions they prefer window seats during flight booking, the agent stores this preference and proactively offers window seats in future interactions.\n\n## Core Capabilities\n\n### Memory Resource Management\n- **Logical Containers**: Encapsulate both raw events and processed long-term memories\n- **Retention Policies**: Define how long data is retained\n- **Security Configuration**: Control access and encryption\n- **Data Transformation**: Transform raw interactions into meaningful insights\n\n### Short-Term Memory Features\n- **Event Storage**: Store conversations, system events, and state changes as immutable events\n- **Session Organization**: Organize by actor and session\n- **Context Preservation**: Maintain immediate context within sessions\n- **Structured Storage**: Support structured storage of interaction data\n\n### Long-Term Memory Features\n- **Insight Extraction**: Automatically extract insights, preferences, and knowledge\n- **Asynchronous Processing**: Extract memories asynchronously using memory strategies\n- **Cross-Session Persistence**: Retain information across multiple sessions\n- **Semantic Search**: Search memories by meaning and context\n\n### Memory Strategies\nDefine the intelligence layer that transforms raw events into meaningful memories:\n\n| Strategy | Description |\n|----------|-------------|\n| **Semantic** | Extract meaningful facts and information |\n| **Summary** | Generate conversation summaries |\n| **User Preference** | Extract and store user preferences |\n| **Custom** | Define custom extraction logic |\n\n### Advanced Features\n- **Branching**: Create alternative conversation paths from specific points\n- **Checkpointing**: Save and mark specific states for later reference\n- **Memory Consolidation**: Merge related memories without duplicates\n- **Audit Trail**: Immutable audit trail for all memory operations\n\n## Use Cases\n\n### Conversational Agents\nEnable chatbots to:\n- Remember previous issues and preferences\n- Provide relevant assistance based on history\n- Create personalized customer experiences\n- Maintain context across session breaks\n\n### Task-Oriented Agents\nSupport workflows like:\n- Track multi-step business process status\n- Maintain workflow progress across sessions\n- Remember task context for resumption\n- Store intermediate results\n\n### Multi-Agent Systems\nAllow agent teams to:\n- Share memory for synchronized operations\n- Coordinate inventory levels and logistics\n- Maintain shared context\n- Optimize collaborative workflows\n\n### Autonomous Agents\nEnable agents to:\n- Plan routes based on past experiences\n- Learn from previous interactions\n- Improve decision-making over time\n- Build persistent knowledge bases\n\n## Quick Start\n\n### Create Memory Resource\n\n```bash\naws bedrock-agentcore-control create-memory \\\n  --memory-name my-agent-memory \\\n  --memory-strategies '[{\"strategyName\": \"SEMANTIC\", \"configuration\": {}}]' \\\n  --region us-west-2\n```\n\n### Using Memory with SDK\n\n```python\nfrom bedrock_agentcore.memory import MemoryClient\n\n# Initialize memory client\nmemory = MemoryClient(memory_id=\"my-agent-memory\")\n\n# Add short-term memory event\nmemory.add_event(\n    session_id=\"session-123\",\n    actor_id=\"user-456\",\n    event_type=\"message\",\n    content={\"role\": \"user\", \"message\": \"Book a flight to Seattle\"}\n)\n\n# Retrieve conversation history\nhistory = memory.get_session_events(session_id=\"session-123\")\n\n# Search long-term memories\nmemories = memory.search_memories(\n    query=\"user flight preferences\",\n    limit=5\n)\n```\n\n### Store and Retrieve Memories\n\n```python\n# Store long-term memory\nmemory.store_memory(\n    memory_type=\"user_preference\",\n    content={\"preference\": \"window_seat\", \"context\": \"flights\"}\n)\n\n# Retrieve relevant memories\nrelevant = memory.search_memories(\n    query=\"seating preferences for flights\",\n    actor_id=\"user-456\"\n)\n```\n\n## Common Operations\n\n### List Memories\n\n```bash\naws bedrock-agentcore-control list-memories \\\n  --region us-west-2\n```\n\n### Get Memory Details\n\n```bash\naws bedrock-agentcore-control get-memory \\\n  --memory-id <MEMORY_ID> \\\n  --region us-west-2\n```\n\n### Update Memory Configuration\n\n```bash\naws bedrock-agentcore-control update-memory \\\n  --memory-id <MEMORY_ID> \\\n  --memory-strategies '[{\"strategyName\": \"SEMANTIC\"}, {\"strategyName\": \"USER_PREFERENCE\"}]' \\\n  --region us-west-2\n```\n\n### Delete Memory\n\n```bash\naws bedrock-agentcore-control delete-memory \\\n  --memory-id <MEMORY_ID> \\\n  --region us-west-2\n```\n\n## Memory Strategies Configuration\n\n### Built-in Strategies\n\n```bash\n# Use semantic strategy\naws bedrock-agentcore-control create-memory \\\n  --memory-name semantic-memory \\\n  --memory-strategies '[{\n    \"strategyName\": \"SEMANTIC\",\n    \"configuration\": {}\n  }]'\n```\n\n### Custom Strategies\n\n```bash\n# Create custom strategy with specific model\naws bedrock-agentcore-control create-memory \\\n  --memory-name custom-memory \\\n  --memory-strategies '[{\n    \"strategyName\": \"CUSTOM\",\n    \"configuration\": {\n      \"modelId\": \"anthropic.claude-3-sonnet\",\n      \"extractionPrompt\": \"Extract key user preferences from this conversation\"\n    }\n  }]'\n```\n\n## Best Practices\n\n### Memory Architecture\n- Design memory architecture intentionally\n- Choose appropriate strategies for use case\n- Implement proper retention policies\n- Consider memory costs and storage\n\n### Performance\n- Use appropriate time-to-live settings\n- Extract only relevant information\n- Implement rhythm of memory operations\n- Monitor memory search latency\n\n### Security\n- Implement proper access controls\n- Encrypt sensitive memories\n- Audit memory access\n- Follow data privacy regulations (GDPR, HIPAA)\n\n### Operations\n- Monitor memory usage and costs\n- Set up alerts for memory failures\n- Implement backup strategies\n- Test memory operations regularly\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Memory not found | Incorrect memory ID | Verify memory ID with list command |\n| Search returns empty | No matching memories | Check query and memory content |\n| Slow memory retrieval | Large memory size | Implement pagination and filters |\n| Strategy extraction fails | Invalid configuration | Check strategy configuration |\n\n## Related Services\n\n- **[Gateway Service](../gateway/README.md)**: Expose APIs as tools for agents\n- **[Runtime Service](../runtime/README.md)**: Execute agents that generate conversation data\n- **[Identity Service](../identity/README.md)**: Secure access to conversation data\n- **[Observability Service](../observability/README.md)**: Monitor memory operations\n\n## References\n\n- [AWS Memory Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html)\n- [Memory Types](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory-types.html)\n- [Memory Strategies](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory-strategies.html)\n- [Building Context-Aware Agents Blog](https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-agentcore-memory-building-context-aware-agents/)\n- [Long-Term Memory Deep Dive](https://aws.amazon.com/blogs/machine-learning/building-smarter-ai-agents-agentcore-long-term-memory-deep-dive/)\n",
        "skills/autonomous-cloud-orchestration/services/observability/README.md": "# AgentCore Observability Service\n\n> **Status**:  Available\n\n## Overview\n\nAmazon Bedrock AgentCore Observability helps developers trace, debug, and monitor agent performance in production through unified operational dashboards and OpenTelemetry-compatible telemetry.\n\n## Core Capabilities\n\n### Distributed Tracing\n- **End-to-End Tracing**: Complete request tracing across all AgentCore services\n- **Workflow Visualization**: Detailed step-by-step workflow execution views\n- **Service Dependencies**: Automatic mapping of service interactions\n- **Bottleneck Detection**: Identify performance bottlenecks in agent workflows\n- **Error Attribution**: Pinpoint exact failure points in complex workflows\n\n### Metrics and Monitoring\n- **Real-Time Metrics**: Live operational metrics for all agent activities\n- **Token Tracking**: Monitor token consumption and costs\n- **Latency Measurements**: Track P50, P95, P99 response times\n- **Session Monitoring**: Track session duration and status\n- **Error Rates**: Monitor error rates by service and operation\n- **Throughput**: Measure requests per second and operation counts\n\n### Logging\n- **Centralized Aggregation**: All service logs in one place\n- **Structured Logging**: Consistent log format with correlation IDs\n- **Search and Filter**: Query logs by service, operation, or time\n- **Real-Time Streaming**: Live log tailing for debugging\n- **Log Retention**: Configurable retention policies\n\n### Dashboards and Alerting\n- **Unified Dashboards**: Pre-built operational dashboards\n- **Custom Metrics**: Define and visualize custom metrics\n- **CloudWatch Integration**: Native AWS CloudWatch support\n- **Configurable Alerts**: Set up alerts for critical conditions\n- **Multi-Service Views**: Consolidated view across all services\n\n### OpenTelemetry Support\n- **Industry Standard**: Compatible with OpenTelemetry specification\n- **Tool Integration**: Works with existing observability tools\n- **Custom Instrumentation**: Add custom traces and metrics\n- **External Export**: Export telemetry to external systems\n\n## Use Cases\n\n### Production Debugging\nEnable teams to:\n- Debug agent execution issues in real-time\n- Identify root causes of failures quickly\n- Trace request flows across services\n- Analyze error patterns and trends\n\n### Performance Monitoring\nSupport scenarios like:\n- Monitor agent response times\n- Track token usage and costs\n- Identify slow operations\n- Optimize agent workflows\n\n### Behavior Analysis\nAllow teams to:\n- Analyze agent behavior patterns\n- Understand user interaction flows\n- Identify usage trends\n- Detect anomalies\n\n### Quality Assurance\nEnable teams to:\n- Ensure SLA compliance\n- Monitor service reliability\n- Track quality metrics\n- Validate performance standards\n\n### Capacity Planning\nSupport activities like:\n- Forecast resource needs\n- Identify scaling requirements\n- Optimize resource allocation\n- Plan for growth\n\n## Architecture\n\n### Observability Data Flow\n\n```\n\n  AgentCore Services                     \n  - Gateway, Runtime, Memory, etc.       \n  - Emit traces, logs, metrics           \n\n           \n\n  OpenTelemetry Collector                \n  - Receive telemetry data               \n  - Process and enrich                   \n  - Route to destinations                \n\n           \n    \n                 \n  \nCloudWatch    X-Ray      \n Logs        Traces     \n Metrics     Service Map\n  \n                 \n\n  Unified Dashboards                     \n  - Service health                       \n  - Performance metrics                  \n  - Error analysis                       \n  - Cost tracking                        \n\n```\n\n### Data Collection Model\n\n1. **Automatic Instrumentation**: Built-in instrumentation for all services\n2. **Context Propagation**: Correlation IDs passed across service boundaries\n3. **Sampling**: Intelligent sampling for high-volume operations\n4. **Buffering**: Local buffering for reliability\n5. **Batch Export**: Efficient batch transmission to backends\n\n## Configuration\n\n### Enable Observability\n\n```bash\n# Enable observability for agent\naws bedrock-agentcore-control update-observability-config \\\n  --agent-id <AGENT_ID> \\\n  --config '{\n    \"tracing\": {\n      \"enabled\": true,\n      \"samplingRate\": 1.0\n    },\n    \"metrics\": {\n      \"enabled\": true,\n      \"interval\": 60\n    },\n    \"logging\": {\n      \"enabled\": true,\n      \"level\": \"INFO\"\n    }\n  }' \\\n  --region <REGION>\n```\n\n### Configure Sampling\n\n```bash\n# Set trace sampling rate\naws bedrock-agentcore-control update-tracing-config \\\n  --agent-id <AGENT_ID> \\\n  --sampling-rate 0.1 \\\n  --region <REGION>\n```\n\n### Custom Metrics\n\n```bash\n# Define custom metric\naws bedrock-agentcore-control create-custom-metric \\\n  --agent-id <AGENT_ID> \\\n  --metric-name \"CustomOperationCount\" \\\n  --metric-type \"Counter\" \\\n  --description \"Count of custom operations\" \\\n  --region <REGION>\n```\n\n## Traces\n\n### View Traces\n\n```bash\n# Query recent traces\naws xray get-trace-summaries \\\n  --start-time <START_TIMESTAMP> \\\n  --end-time <END_TIMESTAMP> \\\n  --filter-expression 'service(id(name: \"AgentCore\", type: \"AWS::Service\"))'\n```\n\n### Trace Details\n\n```bash\n# Get specific trace\naws xray batch-get-traces \\\n  --trace-ids <TRACE_ID_1> <TRACE_ID_2>\n```\n\n### Service Map\n\n```bash\n# Get service map\naws xray get-service-graph \\\n  --start-time <START_TIMESTAMP> \\\n  --end-time <END_TIMESTAMP>\n```\n\n## Metrics\n\n### Common Metrics\n\n**Gateway Metrics**:\n- `TargetInvocations`: Number of target invocations\n- `TargetErrors`: Number of target errors\n- `TargetLatency`: Target response latency\n\n**Runtime Metrics**:\n- `AgentExecutions`: Number of agent executions\n- `ExecutionDuration`: Agent execution duration\n- `ExecutionErrors`: Number of execution failures\n\n**Memory Metrics**:\n- `MemoryReads`: Number of memory read operations\n- `MemoryWrites`: Number of memory write operations\n- `MemorySize`: Total memory storage size\n\n**Token Metrics**:\n- `TokensConsumed`: Total tokens used\n- `TokenCost`: Estimated cost in dollars\n\n### Query Metrics\n\n```bash\n# Get metric statistics\naws cloudwatch get-metric-statistics \\\n  --namespace AWS/BedrockAgentCore \\\n  --metric-name TargetInvocations \\\n  --dimensions Name=AgentId,Value=<AGENT_ID> \\\n  --start-time <START> \\\n  --end-time <END> \\\n  --period 300 \\\n  --statistics Sum Average\n```\n\n### Custom Metrics\n\n```bash\n# Put custom metric data\naws cloudwatch put-metric-data \\\n  --namespace AgentCore/Custom \\\n  --metric-name CustomMetric \\\n  --value 1.0 \\\n  --dimensions AgentId=<AGENT_ID>\n```\n\n## Logs\n\n### Query Logs\n\n```bash\n# Tail agent logs\naws logs tail /aws/bedrock-agentcore/<AGENT_ID> \\\n  --follow \\\n  --format short\n\n# Query logs with filter\naws logs filter-log-events \\\n  --log-group-name /aws/bedrock-agentcore/<AGENT_ID> \\\n  --filter-pattern \"ERROR\" \\\n  --start-time <TIMESTAMP>\n```\n\n### Log Insights\n\n```bash\n# Run Insights query\naws logs start-query \\\n  --log-group-name /aws/bedrock-agentcore/<AGENT_ID> \\\n  --start-time <START_TIMESTAMP> \\\n  --end-time <END_TIMESTAMP> \\\n  --query-string 'fields @timestamp, @message\n    | filter @message like /ERROR/\n    | sort @timestamp desc\n    | limit 20'\n```\n\n## Dashboards\n\n### Create Dashboard\n\n```bash\n# Create CloudWatch dashboard\naws cloudwatch put-dashboard \\\n  --dashboard-name AgentCore-<AGENT_ID> \\\n  --dashboard-body file://dashboard-definition.json\n```\n\n### Dashboard Definition Example\n\n```json\n{\n  \"widgets\": [\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"metrics\": [\n          [\"AWS/BedrockAgentCore\", \"TargetInvocations\", {\"stat\": \"Sum\"}]\n        ],\n        \"period\": 300,\n        \"stat\": \"Sum\",\n        \"region\": \"us-west-2\",\n        \"title\": \"Target Invocations\"\n      }\n    },\n    {\n      \"type\": \"metric\",\n      \"properties\": {\n        \"metrics\": [\n          [\"AWS/BedrockAgentCore\", \"TargetErrors\", {\"stat\": \"Sum\"}]\n        ],\n        \"period\": 300,\n        \"stat\": \"Sum\",\n        \"region\": \"us-west-2\",\n        \"title\": \"Target Errors\"\n      }\n    }\n  ]\n}\n```\n\n## Alerting\n\n### Create Alarm\n\n```bash\n# Create CloudWatch alarm\naws cloudwatch put-metric-alarm \\\n  --alarm-name high-error-rate-<AGENT_ID> \\\n  --alarm-description \"Alert when error rate exceeds threshold\" \\\n  --metric-name TargetErrors \\\n  --namespace AWS/BedrockAgentCore \\\n  --statistic Sum \\\n  --period 300 \\\n  --evaluation-periods 2 \\\n  --threshold 10 \\\n  --comparison-operator GreaterThanThreshold \\\n  --dimensions Name=AgentId,Value=<AGENT_ID> \\\n  --alarm-actions <SNS_TOPIC_ARN>\n```\n\n### Alarm Templates\n\n**High Error Rate**:\n```bash\n# Alert on >5% error rate\naws cloudwatch put-metric-alarm \\\n  --alarm-name error-rate-high \\\n  --metric-name ErrorRate \\\n  --threshold 5 \\\n  --comparison-operator GreaterThanThreshold\n```\n\n**High Latency**:\n```bash\n# Alert on P95 latency >2s\naws cloudwatch put-metric-alarm \\\n  --alarm-name latency-high \\\n  --metric-name TargetLatency \\\n  --statistic p95 \\\n  --threshold 2000 \\\n  --comparison-operator GreaterThanThreshold\n```\n\n**High Token Usage**:\n```bash\n# Alert on excessive token usage\naws cloudwatch put-metric-alarm \\\n  --alarm-name tokens-high \\\n  --metric-name TokensConsumed \\\n  --statistic Sum \\\n  --threshold 1000000 \\\n  --comparison-operator GreaterThanThreshold\n```\n\n## Best Practices\n\n### Instrumentation\n- Enable observability for all production agents\n- Use appropriate sampling rates (1.0 for dev, 0.1 for prod)\n- Add custom metrics for business-critical operations\n- Include context in log messages\n- Use structured logging formats\n\n### Performance\n- Use appropriate metric aggregation periods\n- Implement metric sampling for high-volume operations\n- Set reasonable log retention periods\n- Use log filtering to reduce noise\n- Archive old traces and logs\n\n### Cost Optimization\n- Adjust sampling rates based on traffic\n- Use metric filters to create custom metrics\n- Set appropriate log retention (7-30 days)\n- Archive infrequently accessed data to S3\n- Use CloudWatch Insights for complex queries\n\n### Alerting\n- Define clear SLOs and SLIs\n- Set meaningful alert thresholds\n- Avoid alert fatigue with proper tuning\n- Use composite alarms for complex conditions\n- Implement escalation policies\n\n### Security\n- Encrypt logs and metrics at rest\n- Use IAM for access control\n- Implement least privilege access\n- Audit observability data access\n- Protect sensitive data in logs\n\n## Integration Patterns\n\n### With All Services\n\nObservability is automatically integrated with all AgentCore services:\n\n```\nGateway  Observability\nRuntime  Observability\nMemory  Observability\nIdentity  Observability\nCode Interpreter  Observability\nBrowser  Observability\n```\n\n### With External Tools\n\nExport telemetry to external observability platforms:\n\n```\nAgentCore Observability\n    \nOpenTelemetry Collector\n    \n\nDatadog New RelicGrafana\n\n```\n\n## Troubleshooting\n\n### No Traces Appearing\n\n**Diagnosis**:\n```bash\n# Check if tracing is enabled\naws bedrock-agentcore-control get-observability-config \\\n  --agent-id <AGENT_ID>\n```\n\n**Solution**: Enable tracing in observability configuration\n\n### High Cardinality Metrics\n\n**Symptom**: Too many unique metric combinations\n**Solution**: Reduce dimension cardinality, use metric filters\n\n### Missing Logs\n\n**Diagnosis**:\n```bash\n# Check log group exists\naws logs describe-log-groups \\\n  --log-group-name-prefix /aws/bedrock-agentcore\n```\n\n**Solution**: Verify IAM permissions for CloudWatch Logs\n\n### High Costs\n\n**Symptom**: Excessive CloudWatch costs\n**Solution**: Adjust sampling rates, reduce log retention, archive old data\n\n## Performance Monitoring\n\n### Key Performance Indicators\n\n**Availability**:\n- Service uptime percentage\n- Error rate by service\n- Failed request percentage\n\n**Performance**:\n- P50, P95, P99 latency\n- Request throughput\n- Operation duration\n\n**Efficiency**:\n- Token consumption rate\n- Cost per operation\n- Resource utilization\n\n**Quality**:\n- Agent success rate\n- User satisfaction metrics\n- Workflow completion rate\n\n## Additional Resources\n\n- **AWS Documentation**: [Bedrock AgentCore Observability](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability.html)\n- **CloudWatch Guide**: [CloudWatch User Guide](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/)\n- **X-Ray Guide**: [AWS X-Ray Developer Guide](https://docs.aws.amazon.com/xray/latest/devguide/)\n- **OpenTelemetry**: [OpenTelemetry Documentation](https://opentelemetry.io/docs/)\n- **Best Practices**: [Observability Best Practices](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/observability-best-practices.html)\n\n---\n\n**Related Services**:\n- [Gateway Service](../gateway/README.md) - Gateway monitoring\n- [Runtime Service](../runtime/README.md) - Runtime tracing\n- [Memory Service](../memory/README.md) - Memory metrics\n- [Credential Management](../../cross-service/credential-management.md) - Cross-service credential patterns\n",
        "skills/autonomous-cloud-orchestration/services/runtime/README.md": "# Runtime Service\n\nThe Runtime service provides a secure, serverless hosting environment for deploying and running AI agents or tools. It handles scaling, session management, security isolation, and infrastructure management.\n\n## Key Features\n\n| Feature | Description |\n|---------|-------------|\n| **Framework Agnostic** | Works with LangGraph, Strands, CrewAI, or custom agents |\n| **Model Flexibility** | Supports any LLM (Bedrock, Claude, Gemini, OpenAI) |\n| **Protocol Support** | MCP (Model Context Protocol) and A2A (Agent to Agent) |\n| **Session Isolation** | Dedicated microVM per session with isolated CPU, memory, filesystem |\n| **Extended Execution** | Up to 8 hours for long-running workloads |\n| **100MB Payloads** | Handle multimodal content (text, images, audio, video) |\n| **Bidirectional Streaming** | HTTP API and WebSocket for real-time interactions |\n\n## Quick Start\n\n### Prerequisites\n- AWS CLI configured with appropriate permissions\n- Docker installed for container builds\n- Python 3.9+ for SDK usage\n\n### Deploy an Agent\n\n**Step 1: Install AgentCore SDK**\n```bash\npip install bedrock-agentcore\n```\n\n**Step 2: Create agent code**\n```python\nfrom bedrock_agentcore.runtime import BedrockAgentCoreApp\n\napp = BedrockAgentCoreApp()\n\n@app.handler()\nasync def handle_request(request, context):\n    user_input = request.get(\"input\", \"\")\n    # Your agent logic here\n    return {\"response\": f\"Processed: {user_input}\"}\n```\n\n**Step 3: Create AgentCore Runtime**\n```bash\naws bedrock-agentcore-control create-agent-runtime \\\n  --agent-runtime-name my-agent \\\n  --runtime-artifact '{\"containerConfiguration\": {\"containerUri\": \"<ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com/my-agent:latest\"}}' \\\n  --role-arn arn:aws:iam::<ACCOUNT_ID>:role/AgentRuntimeExecutionRole \\\n  --network-configuration '{\"networkMode\": \"PUBLIC\"}' \\\n  --region us-west-2\n```\n\n**Step 4: Invoke agent**\n```bash\naws bedrock-agentcore-runtime invoke-agent-runtime \\\n  --agent-runtime-endpoint-arn arn:aws:bedrock-agentcore:us-west-2:<ACCOUNT_ID>:runtime/my-agent/endpoint/DEFAULT \\\n  --payload '{\"input\": \"Hello, agent!\"}' \\\n  --region us-west-2\n```\n\n## Core Components\n\n### AgentCore Runtime\nContainerized application hosting your AI agent or tool code. Each runtime:\n- Has a unique identity\n- Is versioned for controlled deployment and updates\n- Can use popular frameworks or custom implementations\n\n### Versions\nImmutable snapshots of configuration:\n- Version 1 (V1) created automatically with new runtime\n- Each update creates a new version\n- Enables rollback capabilities\n\n### Endpoints\nAddressable access points to runtime versions:\n- **DEFAULT**: Automatically created, points to latest version\n- Custom endpoints for different environments (dev, test, prod)\n- Unique ARN for invocation\n\nEndpoint states: `CREATING`  `READY` (or `CREATE_FAILED`)  `UPDATING`  `READY`\n\n### Sessions\nIndividual interaction contexts with complete isolation:\n- Dedicated microVM per session\n- Preserves context across interactions\n- Persists up to 8 hours\n- Auto-terminates after 15 minutes idle\n\nSession states: `Active`  `Idle`  `Terminated`\n\n## Authentication\n\n### Inbound (Who Can Access Your Agent)\n\n| Method | Description |\n|--------|-------------|\n| **IAM (SigV4)** | AWS credentials for identity verification |\n| **OAuth 2.0** | External identity providers (Cognito, Okta, Entra ID) |\n\n**OAuth Flow**:\n1. User authenticates with identity provider\n2. Client receives bearer token\n3. Token passed in authorization header\n4. Runtime validates token\n5. Request processed or rejected\n\n### Outbound (Accessing External Services)\n\n| Method | Use Case |\n|--------|----------|\n| **OAuth** | Services supporting OAuth flows |\n| **API Keys** | Key-based authentication |\n\n**Modes**:\n- **User-delegated**: Acting on behalf of end user\n- **Autonomous**: Acting with service-level credentials\n\n## Common Operations\n\n### List Agent Runtimes\n```bash\naws bedrock-agentcore-control list-agent-runtimes \\\n  --region us-west-2\n```\n\n### Get Runtime Details\n```bash\naws bedrock-agentcore-control get-agent-runtime \\\n  --agent-runtime-id <RUNTIME_ID> \\\n  --region us-west-2\n```\n\n### Update Runtime\n```bash\naws bedrock-agentcore-control update-agent-runtime \\\n  --agent-runtime-id <RUNTIME_ID> \\\n  --runtime-artifact '{\"containerConfiguration\": {\"containerUri\": \"<NEW_IMAGE_URI>\"}}' \\\n  --region us-west-2\n```\n\n### Delete Runtime\n```bash\naws bedrock-agentcore-control delete-agent-runtime \\\n  --agent-runtime-id <RUNTIME_ID> \\\n  --region us-west-2\n```\n\n## Long-Running Agents\n\nFor workloads exceeding request/response cycles (up to 8 hours):\n\n```python\nfrom bedrock_agentcore.runtime import BedrockAgentCoreApp\n\napp = BedrockAgentCoreApp()\n\n@app.handler()\nasync def handle_request(request, context):\n    # Add async task\n    task_id = context.add_async_task(\"background-processing\")\n\n    # Start background work\n    # ... long-running operation ...\n\n    # Complete task when done\n    context.complete_async_task(task_id)\n\n    return {\"status\": \"Task started\", \"task_id\": task_id}\n```\n\n## Streaming Responses\n\nEnable real-time partial results:\n\n```python\n@app.handler()\nasync def handle_request(request, context):\n    async for chunk in generate_response(request):\n        yield {\"partial\": chunk}\n    yield {\"complete\": True}\n```\n\n## Supported Frameworks\n\n| Framework | Description |\n|-----------|-------------|\n| **LangGraph** | Graph-based agent workflows |\n| **Strands** | AWS-native agent framework |\n| **CrewAI** | Multi-agent collaboration |\n| **Custom** | Any Python-based agent |\n\n## Troubleshooting\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| 504 Gateway Timeout | Container issues, ARM64 compatibility | Ensure container exposes port 8080, use ARM64 image |\n| 403 AccessDeniedException | Missing permissions | Verify IAM role and policies |\n| exec format error | Wrong architecture | Build ARM64 containers with buildx |\n| Session terminated after 15min | Idle timeout | Implement ping handler with HEALTHY_BUSY status |\n\n## Related Services\n\n- **[Gateway Service](../gateway/README.md)**: Expose APIs as tools for agents\n- **[Memory Service](../memory/README.md)**: Store agent conversation history\n- **[Identity Service](../identity/README.md)**: Manage agent credentials\n- **[Observability Service](../observability/README.md)**: Monitor agent performance\n\n## References\n\n- [AWS Runtime Documentation](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/agents-tools-runtime.html)\n- [How Runtime Works](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-how-it-works.html)\n- [Runtime Troubleshooting](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-troubleshooting.html)\n- [Runtime API Reference](https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/)\n",
        "skills/backend-database-specialist/SKILL.md": "---\nname: backend-database-specialist\ndescription: Provide expert guidance on backend and database services. Advises on database design, APIs, authentication, and deployment.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Supabase Integration Expert\n\n## Purpose\n\nProvide comprehensive, accurate guidance for building applications with Supabase based on 2,616+ official documentation files. Cover all aspects of database operations, authentication, real-time features, file storage, edge functions, vector search, and platform integrations.\n\n## Documentation Coverage\n\n**Full access to official Supabase documentation (when available):**\n- **Location:** `docs/supabase_com/`\n- **Files:** 2,616 markdown files\n- **Coverage:** Complete guides, API references, client libraries, and platform docs\n\n**Note:** Documentation must be pulled separately:\n```bash\npipx install docpull\ndocpull https://supabase.com/docs -o .claude/skills/supabase/docs\n```\n\n**Major Areas:**\n- **Database:** PostgreSQL, Row Level Security (RLS), migrations, functions, triggers\n- **Authentication:** Email/password, OAuth, magic links, SSO, MFA, phone auth\n- **Real-time:** Database changes, broadcast, presence, channels\n- **Storage:** File uploads, image transformations, CDN, buckets\n- **Edge Functions:** Deno runtime, serverless, global deployment\n- **Vector/AI:** pgvector, embeddings, semantic search, RAG\n- **Client Libraries:** JavaScript, Python, Dart (Flutter), Swift, Kotlin\n- **Platform:** CLI, local development, branching, observability\n- **Integrations:** Next.js, React, Vue, Svelte, React Native, Expo\n\n## When to Use\n\nInvoke when user mentions:\n- **Database:** PostgreSQL, Postgres, SQL, database, tables, queries, migrations\n- **Auth:** authentication, login, signup, OAuth, SSO, multi-factor, magic links\n- **Real-time:** real-time, subscriptions, websocket, live data, presence, broadcast\n- **Storage:** file upload, file storage, images, S3, CDN, buckets\n- **Functions:** edge functions, serverless, API, Deno, cloud functions\n- **Security:** Row Level Security, RLS, policies, permissions, access control\n- **AI/ML:** vector search, embeddings, pgvector, semantic search, AI, RAG\n- **Framework Integration:** Next.js, React, Supabase client, hooks\n\n## How to Use Documentation\n\nWhen answering questions:\n\n1. **Search for specific topics:**\n   ```bash\n   # Use Grep to find relevant docs\n   grep -r \"row level security\" docs/supabase_com/ --include=\"*.md\"\n   ```\n\n2. **Find guides:**\n   ```bash\n   # Guides are organized by feature\n   ls docs/supabase_com/guides_*\n   ```\n\n3. **Check reference docs:**\n   ```bash\n   # Reference docs for client libraries\n   ls docs/supabase_com/reference_*\n   ```\n\n## Quick Start\n\n### Installation\n\n```bash\nnpm install @supabase/supabase-js\n```\n\n### Initialize Client\n\n```typescript\nimport { createClient } from '@supabase/supabase-js';\n\nconst supabase = createClient(\n  process.env.NEXT_PUBLIC_SUPABASE_URL!,\n  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!\n);\n```\n\n**Environment Variables:**\n- `NEXT_PUBLIC_SUPABASE_URL` - Your project URL (safe for client)\n- `NEXT_PUBLIC_SUPABASE_ANON_KEY` - Anonymous/public key (safe for client)\n- `SUPABASE_SERVICE_ROLE_KEY` - Admin key (server-side only, bypasses RLS)\n\n## Database Operations\n\n### CRUD Operations\n\n```typescript\n// Insert\nconst { data, error } = await supabase\n  .from('posts')\n  .insert({\n    title: 'Hello World',\n    content: 'My first post',\n    user_id: user.id,\n  })\n  .select()\n  .single();\n\n// Read (with filters)\nconst { data: posts } = await supabase\n  .from('posts')\n  .select('*')\n  .eq('published', true)\n  .order('created_at', { ascending: false })\n  .limit(10);\n\n// Update\nconst { data, error } = await supabase\n  .from('posts')\n  .update({ published: true })\n  .eq('id', postId)\n  .select()\n  .single();\n\n// Delete\nconst { error } = await supabase\n  .from('posts')\n  .delete()\n  .eq('id', postId);\n\n// Upsert (insert or update)\nconst { data, error } = await supabase\n  .from('profiles')\n  .upsert({\n    id: user.id,\n    name: 'John Doe',\n    updated_at: new Date().toISOString(),\n  })\n  .select();\n```\n\n### Advanced Queries\n\n```typescript\n// Joins\nconst { data } = await supabase\n  .from('posts')\n  .select(`\n    *,\n    author:profiles(name, avatar),\n    comments(count)\n  `)\n  .eq('published', true);\n\n// Full-text search\nconst { data } = await supabase\n  .from('posts')\n  .select('*')\n  .textSearch('title', `'nextjs' & 'supabase'`);\n\n// Range queries\nconst { data } = await supabase\n  .from('posts')\n  .select('*')\n  .gte('created_at', '2024-01-01')\n  .lt('created_at', '2024-12-31');\n\n// JSON queries\nconst { data } = await supabase\n  .from('posts')\n  .select('*')\n  .contains('metadata', { tags: ['tutorial'] });\n```\n\n### Database Functions\n\n```typescript\n// Call stored procedure\nconst { data, error } = await supabase\n  .rpc('get_user_stats', {\n    user_id: userId,\n  });\n\n// Call with filters\nconst { data } = await supabase\n  .rpc('search_posts', { search_term: 'supabase' })\n  .limit(10);\n```\n\n## Authentication\n\n### Sign Up / Sign In\n\n```typescript\n// Email/password signup\nconst { data, error } = await supabase.auth.signUp({\n  email: 'user@example.com',\n  password: 'secure-password',\n  options: {\n    data: {\n      first_name: 'John',\n      last_name: 'Doe',\n    },\n  },\n});\n\n// Email/password sign in\nconst { data, error } = await supabase.auth.signInWithPassword({\n  email: 'user@example.com',\n  password: 'secure-password',\n});\n\n// Magic link (passwordless)\nconst { data, error } = await supabase.auth.signInWithOtp({\n  email: 'user@example.com',\n  options: {\n    emailRedirectTo: 'https://example.com/auth/callback',\n  },\n});\n\n// Phone/SMS\nconst { data, error } = await supabase.auth.signInWithOtp({\n  phone: '+1234567890',\n});\n```\n\n### OAuth Providers\n\n```typescript\n// Google sign in\nconst { data, error } = await supabase.auth.signInWithOAuth({\n  provider: 'google',\n  options: {\n    redirectTo: 'http://localhost:3000/auth/callback',\n    scopes: 'profile email',\n  },\n});\n```\n\n**Supported providers:**\n- Google, GitHub, GitLab, Bitbucket\n- Azure, Apple, Discord, Facebook\n- Slack, Spotify, Twitch, Twitter/X\n- Linear, Notion, Figma, and more\n\n### User Session Management\n\n```typescript\n// Get current user\nconst { data: { user } } = await supabase.auth.getUser();\n\n// Get session\nconst { data: { session } } = await supabase.auth.getSession();\n\n// Sign out\nconst { error } = await supabase.auth.signOut();\n\n// Listen to auth changes\nsupabase.auth.onAuthStateChange((event, session) => {\n  if (event === 'SIGNED_IN') {\n    console.log('User signed in:', session.user);\n  }\n  if (event === 'SIGNED_OUT') {\n    console.log('User signed out');\n  }\n  if (event === 'TOKEN_REFRESHED') {\n    console.log('Token refreshed');\n  }\n});\n```\n\n### Multi-Factor Authentication (MFA)\n\n```typescript\n// Enroll MFA\nconst { data, error } = await supabase.auth.mfa.enroll({\n  factorType: 'totp',\n  friendlyName: 'My Authenticator App',\n});\n\n// Verify MFA\nconst { data, error } = await supabase.auth.mfa.challengeAndVerify({\n  factorId: data.id,\n  code: '123456',\n});\n\n// List factors\nconst { data: factors } = await supabase.auth.mfa.listFactors();\n```\n\n## Row Level Security (RLS)\n\n### Enable RLS\n\n```sql\n-- Enable RLS on table\nALTER TABLE posts ENABLE ROW LEVEL SECURITY;\n```\n\n### Create Policies\n\n```sql\n-- Public read access\nCREATE POLICY \"Posts are viewable by everyone\"\n  ON posts FOR SELECT\n  USING (true);\n\n-- Users can insert their own posts\nCREATE POLICY \"Users can create posts\"\n  ON posts FOR INSERT\n  WITH CHECK (auth.uid() = user_id);\n\n-- Users can update only their posts\nCREATE POLICY \"Users can update own posts\"\n  ON posts FOR UPDATE\n  USING (auth.uid() = user_id);\n\n-- Users can delete only their posts\nCREATE POLICY \"Users can delete own posts\"\n  ON posts FOR DELETE\n  USING (auth.uid() = user_id);\n\n-- Conditional access (e.g., premium users)\nCREATE POLICY \"Premium content for premium users\"\n  ON posts FOR SELECT\n  USING (\n    NOT premium OR\n    (auth.uid() IN (\n      SELECT user_id FROM subscriptions\n      WHERE status = 'active'\n    ))\n  );\n```\n\n### Helper Functions\n\n```sql\n-- Get current user ID\nauth.uid()\n\n-- Get current JWT\nauth.jwt()\n\n-- Access JWT claims\n(auth.jwt()->>'role')::text\n(auth.jwt()->>'email')::text\n```\n\n## Real-time Subscriptions\n\n### Listen to Database Changes\n\n```typescript\nconst channel = supabase\n  .channel('posts-changes')\n  .on(\n    'postgres_changes',\n    {\n      event: '*', // or 'INSERT', 'UPDATE', 'DELETE'\n      schema: 'public',\n      table: 'posts',\n    },\n    (payload) => {\n      console.log('Change received:', payload);\n    }\n  )\n  .subscribe();\n\n// Cleanup\nchannel.unsubscribe();\n```\n\n### Filter Real-time Events\n\n```typescript\n// Only listen to specific user's posts\nconst channel = supabase\n  .channel('my-posts')\n  .on(\n    'postgres_changes',\n    {\n      event: 'INSERT',\n      schema: 'public',\n      table: 'posts',\n      filter: `user_id=eq.${userId}`,\n    },\n    (payload) => {\n      console.log('New post:', payload.new);\n    }\n  )\n  .subscribe();\n```\n\n### Broadcast (Ephemeral Messages)\n\n```typescript\nconst channel = supabase.channel('chat-room');\n\n// Send message\nawait channel.send({\n  type: 'broadcast',\n  event: 'message',\n  payload: { text: 'Hello!', user: 'John' },\n});\n\n// Receive messages\nchannel.on('broadcast', { event: 'message' }, (payload) => {\n  console.log('Message:', payload.payload);\n});\n\nawait channel.subscribe();\n```\n\n### Presence Tracking\n\n```typescript\nconst channel = supabase.channel('room-1');\n\n// Track presence\nchannel\n  .on('presence', { event: 'sync' }, () => {\n    const state = channel.presenceState();\n    console.log('Online users:', Object.keys(state).length);\n  })\n  .on('presence', { event: 'join' }, ({ key, newPresences }) => {\n    console.log('User joined:', newPresences);\n  })\n  .on('presence', { event: 'leave' }, ({ key, leftPresences }) => {\n    console.log('User left:', leftPresences);\n  })\n  .subscribe(async (status) => {\n    if (status === 'SUBSCRIBED') {\n      await channel.track({\n        user_id: userId,\n        online_at: new Date().toISOString(),\n      });\n    }\n  });\n```\n\n## Storage\n\n### Upload Files\n\n```typescript\nconst file = event.target.files[0];\n\nconst { data, error } = await supabase.storage\n  .from('avatars')\n  .upload(`public/${userId}/avatar.png`, file, {\n    cacheControl: '3600',\n    upsert: true,\n  });\n\n// Upload from base64\nconst { data, error } = await supabase.storage\n  .from('avatars')\n  .upload('file.png', decode(base64String), {\n    contentType: 'image/png',\n  });\n```\n\n### Download Files\n\n```typescript\n// Download as blob\nconst { data, error } = await supabase.storage\n  .from('avatars')\n  .download('public/avatar.png');\n\nconst url = URL.createObjectURL(data);\n```\n\n### Public URLs\n\n```typescript\n// Get public URL (for public buckets)\nconst { data } = supabase.storage\n  .from('avatars')\n  .getPublicUrl('public/avatar.png');\n\nconsole.log(data.publicUrl);\n```\n\n### Signed URLs (Private Files)\n\n```typescript\n// Create temporary access URL\nconst { data, error } = await supabase.storage\n  .from('private-files')\n  .createSignedUrl('document.pdf', 3600); // 1 hour\n\nconsole.log(data.signedUrl);\n```\n\n### Image Transformations\n\n```typescript\nconst { data } = supabase.storage\n  .from('avatars')\n  .getPublicUrl('avatar.png', {\n    transform: {\n      width: 400,\n      height: 400,\n      resize: 'cover', // 'contain', 'cover', 'fill'\n      quality: 80,\n    },\n  });\n```\n\n### List Files\n\n```typescript\nconst { data, error } = await supabase.storage\n  .from('avatars')\n  .list('public', {\n    limit: 100,\n    offset: 0,\n    sortBy: { column: 'created_at', order: 'desc' },\n  });\n```\n\n## Edge Functions\n\n### Create Function\n\n```bash\n# Install Supabase CLI\nnpm install -g supabase\n\n# Initialize project\nsupabase init\n\n# Create function\nsupabase functions new my-function\n```\n\n### Function Example\n\n```typescript\n// supabase/functions/my-function/index.ts\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts';\nimport { createClient } from 'https://esm.sh/@supabase/supabase-js@2';\n\nserve(async (req) => {\n  try {\n    // Initialize Supabase client\n    const supabase = createClient(\n      Deno.env.get('SUPABASE_URL')!,\n      Deno.env.get('SUPABASE_ANON_KEY')!,\n      {\n        global: {\n          headers: { Authorization: req.headers.get('Authorization')! },\n        },\n      }\n    );\n\n    // Get authenticated user\n    const { data: { user }, error: authError } = await supabase.auth.getUser();\n\n    if (authError || !user) {\n      return new Response(JSON.stringify({ error: 'Unauthorized' }), {\n        status: 401,\n        headers: { 'Content-Type': 'application/json' },\n      });\n    }\n\n    // Query database\n    const { data: posts, error } = await supabase\n      .from('posts')\n      .select('*')\n      .eq('user_id', user.id);\n\n    if (error) throw error;\n\n    return new Response(JSON.stringify({ posts }), {\n      headers: { 'Content-Type': 'application/json' },\n    });\n  } catch (error) {\n    return new Response(JSON.stringify({ error: error.message }), {\n      status: 500,\n      headers: { 'Content-Type': 'application/json' },\n    });\n  }\n});\n```\n\n### Deploy Function\n\n```bash\n# Deploy single function\nsupabase functions deploy my-function\n\n# Deploy all functions\nsupabase functions deploy\n```\n\n### Invoke Function\n\n```typescript\nconst { data, error } = await supabase.functions.invoke('my-function', {\n  body: { name: 'World' },\n});\n\nconsole.log(data);\n```\n\n## Vector Search (AI/ML)\n\n### Enable pgvector\n\n```sql\n-- Enable extension\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Create table with vector column\nCREATE TABLE documents (\n  id BIGSERIAL PRIMARY KEY,\n  content TEXT,\n  embedding VECTOR(1536) -- OpenAI ada-002 dimensions\n);\n\n-- Create HNSW index for fast similarity search\nCREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops);\n```\n\n### Store Embeddings\n\n```typescript\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\n// Generate embedding\nconst response = await openai.embeddings.create({\n  model: 'text-embedding-ada-002',\n  input: 'Supabase is awesome',\n});\n\nconst embedding = response.data[0].embedding;\n\n// Store in database\nconst { data, error } = await supabase\n  .from('documents')\n  .insert({\n    content: 'Supabase is awesome',\n    embedding,\n  });\n```\n\n### Similarity Search\n\n```typescript\n// Find similar documents\nconst { data, error } = await supabase.rpc('match_documents', {\n  query_embedding: embedding,\n  match_threshold: 0.78,\n  match_count: 10,\n});\n```\n\n**Similarity search function:**\n```sql\nCREATE FUNCTION match_documents (\n  query_embedding VECTOR(1536),\n  match_threshold FLOAT,\n  match_count INT\n)\nRETURNS TABLE (\n  id BIGINT,\n  content TEXT,\n  similarity FLOAT\n)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  RETURN QUERY\n  SELECT\n    documents.id,\n    documents.content,\n    1 - (documents.embedding <=> query_embedding) AS similarity\n  FROM documents\n  WHERE 1 - (documents.embedding <=> query_embedding) > match_threshold\n  ORDER BY documents.embedding <=> query_embedding\n  LIMIT match_count;\nEND;\n$$;\n```\n\n## Next.js Integration\n\n### Server Components\n\n```typescript\n// app/posts/page.tsx\nimport { createServerComponentClient } from '@supabase/auth-helpers-nextjs';\nimport { cookies } from 'next/headers';\n\nexport default async function PostsPage() {\n  const supabase = createServerComponentClient({ cookies });\n\n  const { data: posts } = await supabase\n    .from('posts')\n    .select('*')\n    .order('created_at', { ascending: false });\n\n  return (\n    <div>\n      {posts?.map((post) => (\n        <div key={post.id}>{post.title}</div>\n      ))}\n    </div>\n  );\n}\n```\n\n### Client Components\n\n```typescript\n// app/new-post/page.tsx\n'use client';\n\nimport { createClientComponentClient } from '@supabase/auth-helpers-nextjs';\nimport { useState } from 'react';\n\nexport default function NewPostPage() {\n  const supabase = createClientComponentClient();\n  const [title, setTitle] = useState('');\n\n  const handleSubmit = async (e: FormEvent) => {\n    e.preventDefault();\n\n    const { error } = await supabase\n      .from('posts')\n      .insert({ title });\n\n    if (error) console.error(error);\n  };\n\n  return <form onSubmit={handleSubmit}>...</form>;\n}\n```\n\n### Middleware (Auth Protection)\n\n```typescript\n// middleware.ts\nimport { createMiddlewareClient } from '@supabase/auth-helpers-nextjs';\nimport { NextResponse } from 'next/server';\nimport type { NextRequest } from 'next/server';\n\nexport async function middleware(req: NextRequest) {\n  const res = NextResponse.next();\n  const supabase = createMiddlewareClient({ req, res });\n\n  const { data: { session } } = await supabase.auth.getSession();\n\n  // Redirect to login if not authenticated\n  if (!session && req.nextUrl.pathname.startsWith('/dashboard')) {\n    return NextResponse.redirect(new URL('/login', req.url));\n  }\n\n  return res;\n}\n\nexport const config = {\n  matcher: ['/dashboard/:path*'],\n};\n```\n\n### Route Handlers\n\n```typescript\n// app/api/posts/route.ts\nimport { createRouteHandlerClient } from '@supabase/auth-helpers-nextjs';\nimport { cookies } from 'next/headers';\nimport { NextResponse } from 'next/server';\n\nexport async function GET() {\n  const supabase = createRouteHandlerClient({ cookies });\n\n  const { data: posts } = await supabase\n    .from('posts')\n    .select('*');\n\n  return NextResponse.json({ posts });\n}\n\nexport async function POST(request: Request) {\n  const supabase = createRouteHandlerClient({ cookies });\n  const body = await request.json();\n\n  const { data, error } = await supabase\n    .from('posts')\n    .insert(body)\n    .select()\n    .single();\n\n  if (error) {\n    return NextResponse.json({ error: error.message }, { status: 400 });\n  }\n\n  return NextResponse.json({ post: data });\n}\n```\n\n## Database Migrations\n\n### Create Migration\n\n```bash\nsupabase migration new create_posts_table\n```\n\n### Migration File Example\n\n```sql\n-- supabase/migrations/20241116000000_create_posts_table.sql\n\n-- Create table\nCREATE TABLE posts (\n  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,\n  user_id UUID REFERENCES auth.users NOT NULL,\n  title TEXT NOT NULL,\n  content TEXT,\n  published BOOLEAN DEFAULT false,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL,\n  updated_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL\n);\n\n-- Enable RLS\nALTER TABLE posts ENABLE ROW LEVEL SECURITY;\n\n-- Create policies\nCREATE POLICY \"Public posts are viewable by everyone\"\n  ON posts FOR SELECT\n  USING (published = true);\n\nCREATE POLICY \"Users can view their own posts\"\n  ON posts FOR SELECT\n  USING (auth.uid() = user_id);\n\nCREATE POLICY \"Users can create posts\"\n  ON posts FOR INSERT\n  WITH CHECK (auth.uid() = user_id);\n\nCREATE POLICY \"Users can update own posts\"\n  ON posts FOR UPDATE\n  USING (auth.uid() = user_id);\n\n-- Create indexes\nCREATE INDEX posts_user_id_idx ON posts(user_id);\nCREATE INDEX posts_created_at_idx ON posts(created_at DESC);\nCREATE INDEX posts_published_idx ON posts(published) WHERE published = true;\n\n-- Create updated_at trigger\nCREATE OR REPLACE FUNCTION handle_updated_at()\nRETURNS TRIGGER AS $$\nBEGIN\n  NEW.updated_at = NOW();\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER set_updated_at\n  BEFORE UPDATE ON posts\n  FOR EACH ROW\n  EXECUTE FUNCTION handle_updated_at();\n```\n\n### Run Migrations\n\n```bash\n# Apply migrations locally\nsupabase db reset\n\n# Push to remote (production)\nsupabase db push\n```\n\n## TypeScript Integration\n\n### Database Type Generation\n\n```bash\n# Generate types from your database schema\nsupabase gen types typescript --project-id YOUR_PROJECT_ID > types/supabase.ts\n\n# Or from local development\nsupabase gen types typescript --local > types/supabase.ts\n```\n\n### Type-Safe Client\n\n```typescript\n// lib/supabase/types.ts\nexport type Json =\n  | string\n  | number\n  | boolean\n  | null\n  | { [key: string]: Json | undefined }\n  | Json[]\n\nexport interface Database {\n  public: {\n    Tables: {\n      posts: {\n        Row: {\n          id: string\n          created_at: string\n          title: string\n          content: string | null\n          user_id: string\n          published: boolean\n        }\n        Insert: {\n          id?: string\n          created_at?: string\n          title: string\n          content?: string | null\n          user_id: string\n          published?: boolean\n        }\n        Update: {\n          id?: string\n          created_at?: string\n          title?: string\n          content?: string | null\n          user_id?: string\n          published?: boolean\n        }\n      }\n      profiles: {\n        Row: {\n          id: string\n          name: string | null\n          avatar_url: string | null\n          created_at: string\n        }\n        Insert: {\n          id: string\n          name?: string | null\n          avatar_url?: string | null\n          created_at?: string\n        }\n        Update: {\n          id?: string\n          name?: string | null\n          avatar_url?: string | null\n          created_at?: string\n        }\n      }\n    }\n    Views: {\n      [_ in never]: never\n    }\n    Functions: {\n      [_ in never]: never\n    }\n    Enums: {\n      [_ in never]: never\n    }\n  }\n}\n\n// lib/supabase/client.ts\nimport { createClient } from '@supabase/supabase-js'\nimport { Database } from './types'\n\nexport const supabase = createClient<Database>(\n  process.env.NEXT_PUBLIC_SUPABASE_URL!,\n  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!\n)\n\n// Now you get full type safety!\nconst { data } = await supabase\n  .from('posts')  //  TypeScript knows this table exists\n  .select('title, content, profiles(name)')  //  TypeScript validates columns\n  .eq('published', true)  //  TypeScript validates types\n\n// data is typed as:\n// Array<{ title: string; content: string | null; profiles: { name: string | null } }>\n```\n\n### Server vs Client Supabase\n\n```typescript\n// lib/supabase/client.ts - Client-side (respects RLS)\nimport { createBrowserClient } from '@supabase/ssr'\nimport { Database } from './types'\n\nexport function createClient() {\n  return createBrowserClient<Database>(\n    process.env.NEXT_PUBLIC_SUPABASE_URL!,\n    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!\n  )\n}\n\n// lib/supabase/server.ts - Server-side (Next.js App Router)\nimport { createServerClient, type CookieOptions } from '@supabase/ssr'\nimport { cookies } from 'next/headers'\nimport { Database } from './types'\n\nexport function createClient() {\n  const cookieStore = cookies()\n\n  return createServerClient<Database>(\n    process.env.NEXT_PUBLIC_SUPABASE_URL!,\n    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,\n    {\n      cookies: {\n        get(name: string) {\n          return cookieStore.get(name)?.value\n        },\n        set(name: string, value: string, options: CookieOptions) {\n          try {\n            cookieStore.set({ name, value, ...options })\n          } catch (error) {\n            // Called from Server Component - ignore\n          }\n        },\n        remove(name: string, options: CookieOptions) {\n          try {\n            cookieStore.set({ name, value: '', ...options })\n          } catch (error) {\n            // Called from Server Component - ignore\n          }\n        },\n      },\n    }\n  )\n}\n\n// lib/supabase/admin.ts - Admin client (bypasses RLS)\nimport { createClient } from '@supabase/supabase-js'\nimport { Database } from './types'\n\nexport const supabaseAdmin = createClient<Database>(\n  process.env.NEXT_PUBLIC_SUPABASE_URL!,\n  process.env.SUPABASE_SERVICE_ROLE_KEY!,  //  Server-side only!\n  {\n    auth: {\n      autoRefreshToken: false,\n      persistSession: false\n    }\n  }\n)\n```\n\n## Next.js App Router Patterns\n\n### Server Components (Recommended)\n\n```typescript\n// app/posts/page.tsx\nimport { createClient } from '@/lib/supabase/server'\n\nexport default async function PostsPage() {\n  const supabase = createClient()\n\n  // Fetch data on server (no loading state needed!)\n  const { data: posts } = await supabase\n    .from('posts')\n    .select('*, profiles(*)')\n    .eq('published', true)\n    .order('created_at', { ascending: false })\n\n  return (\n    <div>\n      {posts?.map(post => (\n        <article key={post.id}>\n          <h2>{post.title}</h2>\n          <p>By {post.profiles?.name}</p>\n          <div>{post.content}</div>\n        </article>\n      ))}\n    </div>\n  )\n}\n```\n\n### Server Actions for Mutations\n\n```typescript\n// app/actions/posts.ts\n'use server'\n\nimport { createClient } from '@/lib/supabase/server'\nimport { revalidatePath } from 'next/cache'\nimport { redirect } from 'next/navigation'\n\nexport async function createPost(formData: FormData) {\n  const supabase = createClient()\n\n  const { data: { user } } = await supabase.auth.getUser()\n  if (!user) {\n    redirect('/login')\n  }\n\n  const title = formData.get('title') as string\n  const content = formData.get('content') as string\n\n  const { error } = await supabase\n    .from('posts')\n    .insert({\n      title,\n      content,\n      user_id: user.id,\n    })\n\n  if (error) {\n    throw new Error(error.message)\n  }\n\n  revalidatePath('/posts')\n  redirect('/posts')\n}\n\nexport async function updatePost(id: string, formData: FormData) {\n  const supabase = createClient()\n\n  const { data: { user } } = await supabase.auth.getUser()\n  if (!user) throw new Error('Unauthorized')\n\n  const { error } = await supabase\n    .from('posts')\n    .update({\n      title: formData.get('title') as string,\n      content: formData.get('content') as string,\n    })\n    .eq('id', id)\n    .eq('user_id', user.id)  // Ensure user owns the post\n\n  if (error) throw new Error(error.message)\n\n  revalidatePath('/posts')\n}\n\nexport async function deletePost(id: string) {\n  const supabase = createClient()\n\n  const { data: { user } } = await supabase.auth.getUser()\n  if (!user) throw new Error('Unauthorized')\n\n  const { error } = await supabase\n    .from('posts')\n    .delete()\n    .eq('id', id)\n    .eq('user_id', user.id)\n\n  if (error) throw new Error(error.message)\n\n  revalidatePath('/posts')\n}\n```\n\n### Client Component with Real-time\n\n```typescript\n// app/components/PostsList.tsx\n'use client'\n\nimport { useEffect, useState } from 'react'\nimport { createClient } from '@/lib/supabase/client'\nimport { Database } from '@/lib/supabase/types'\n\ntype Post = Database['public']['Tables']['posts']['Row']\n\nexport function PostsList({ initialPosts }: { initialPosts: Post[] }) {\n  const [posts, setPosts] = useState(initialPosts)\n  const supabase = createClient()\n\n  useEffect(() => {\n    const channel = supabase\n      .channel('posts-changes')\n      .on(\n        'postgres_changes',\n        {\n          event: '*',\n          schema: 'public',\n          table: 'posts',\n          filter: 'published=eq.true',\n        },\n        (payload) => {\n          if (payload.eventType === 'INSERT') {\n            setPosts(prev => [payload.new as Post, ...prev])\n          } else if (payload.eventType === 'UPDATE') {\n            setPosts(prev =>\n              prev.map(post =>\n                post.id === payload.new.id ? (payload.new as Post) : post\n              )\n            )\n          } else if (payload.eventType === 'DELETE') {\n            setPosts(prev => prev.filter(post => post.id !== payload.old.id))\n          }\n        }\n      )\n      .subscribe()\n\n    return () => {\n      supabase.removeChannel(channel)\n    }\n  }, [supabase])\n\n  return (\n    <div>\n      {posts.map(post => (\n        <article key={post.id}>\n          <h2>{post.title}</h2>\n          <p>{post.content}</p>\n        </article>\n      ))}\n    </div>\n  )\n}\n```\n\n### Route Handlers\n\n```typescript\n// app/api/posts/route.ts\nimport { createClient } from '@/lib/supabase/server'\nimport { NextRequest, NextResponse } from 'next/server'\n\nexport async function GET(request: NextRequest) {\n  const supabase = createClient()\n\n  const { searchParams } = new URL(request.url)\n  const limit = parseInt(searchParams.get('limit') || '10')\n\n  const { data, error } = await supabase\n    .from('posts')\n    .select('*')\n    .eq('published', true)\n    .order('created_at', { ascending: false })\n    .limit(limit)\n\n  if (error) {\n    return NextResponse.json({ error: error.message }, { status: 500 })\n  }\n\n  return NextResponse.json(data)\n}\n\nexport async function POST(request: NextRequest) {\n  const supabase = createClient()\n\n  const { data: { user } } = await supabase.auth.getUser()\n  if (!user) {\n    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })\n  }\n\n  const body = await request.json()\n\n  const { data, error } = await supabase\n    .from('posts')\n    .insert({\n      ...body,\n      user_id: user.id,\n    })\n    .select()\n    .single()\n\n  if (error) {\n    return NextResponse.json({ error: error.message }, { status: 500 })\n  }\n\n  return NextResponse.json(data)\n}\n```\n\n## Advanced Authentication\n\n### Email/Password with Email Confirmation\n\n```typescript\n// app/actions/auth.ts\n'use server'\n\nimport { createClient } from '@/lib/supabase/server'\nimport { redirect } from 'next/navigation'\n\nexport async function signUp(formData: FormData) {\n  const supabase = createClient()\n\n  const email = formData.get('email') as string\n  const password = formData.get('password') as string\n  const name = formData.get('name') as string\n\n  const { error } = await supabase.auth.signUp({\n    email,\n    password,\n    options: {\n      data: {\n        name,  // Stored in auth.users.raw_user_meta_data\n      },\n      emailRedirectTo: `${process.env.NEXT_PUBLIC_URL}/auth/callback`,\n    },\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  return { success: true, message: 'Check your email to confirm your account' }\n}\n\nexport async function signIn(formData: FormData) {\n  const supabase = createClient()\n\n  const email = formData.get('email') as string\n  const password = formData.get('password') as string\n\n  const { error } = await supabase.auth.signInWithPassword({\n    email,\n    password,\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  redirect('/dashboard')\n}\n\nexport async function signOut() {\n  const supabase = createClient()\n  await supabase.auth.signOut()\n  redirect('/')\n}\n```\n\n### OAuth (Google, GitHub, etc.)\n\n```typescript\n// app/actions/auth.ts\nexport async function signInWithGoogle() {\n  const supabase = createClient()\n\n  const { data, error } = await supabase.auth.signInWithOAuth({\n    provider: 'google',\n    options: {\n      redirectTo: `${process.env.NEXT_PUBLIC_URL}/auth/callback`,\n      queryParams: {\n        access_type: 'offline',\n        prompt: 'consent',\n      },\n    },\n  })\n\n  if (data?.url) {\n    redirect(data.url)\n  }\n}\n\nexport async function signInWithGitHub() {\n  const supabase = createClient()\n\n  const { data } = await supabase.auth.signInWithOAuth({\n    provider: 'github',\n    options: {\n      redirectTo: `${process.env.NEXT_PUBLIC_URL}/auth/callback`,\n      scopes: 'read:user user:email',\n    },\n  })\n\n  if (data?.url) {\n    redirect(data.url)\n  }\n}\n```\n\n### Magic Links\n\n```typescript\nexport async function sendMagicLink(email: string) {\n  const supabase = createClient()\n\n  const { error } = await supabase.auth.signInWithOtp({\n    email,\n    options: {\n      emailRedirectTo: `${process.env.NEXT_PUBLIC_URL}/auth/callback`,\n    },\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  return { success: true, message: 'Check your email for the login link' }\n}\n```\n\n### Phone Auth (SMS)\n\n```typescript\nexport async function sendPhoneOTP(phone: string) {\n  const supabase = createClient()\n\n  const { error } = await supabase.auth.signInWithOtp({\n    phone,\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  return { success: true }\n}\n\nexport async function verifyPhoneOTP(phone: string, token: string) {\n  const supabase = createClient()\n\n  const { error } = await supabase.auth.verifyOtp({\n    phone,\n    token,\n    type: 'sms',\n  })\n\n  if (error) {\n    return { error: error.message }\n  }\n\n  redirect('/dashboard')\n}\n```\n\n### Multi-Factor Authentication (MFA)\n\n```typescript\n// Enable MFA for user\nexport async function enableMFA() {\n  const supabase = createClient()\n\n  const { data, error } = await supabase.auth.mfa.enroll({\n    factorType: 'totp',\n    friendlyName: 'Authenticator App',\n  })\n\n  if (error) throw error\n\n  // data.totp.qr_code - QR code to scan\n  // data.totp.secret - Secret to enter manually\n  return data\n}\n\n// Verify MFA\nexport async function verifyMFA(factorId: string, code: string) {\n  const supabase = createClient()\n\n  const { data, error } = await supabase.auth.mfa.challengeAndVerify({\n    factorId,\n    code,\n  })\n\n  if (error) throw error\n  return data\n}\n```\n\n### Auth Callback Handler\n\n```typescript\n// app/auth/callback/route.ts\nimport { createClient } from '@/lib/supabase/server'\nimport { NextRequest, NextResponse } from 'next/server'\n\nexport async function GET(request: NextRequest) {\n  const requestUrl = new URL(request.url)\n  const code = requestUrl.searchParams.get('code')\n\n  if (code) {\n    const supabase = createClient()\n    await supabase.auth.exchangeCodeForSession(code)\n  }\n\n  // Redirect to dashboard or wherever\n  return NextResponse.redirect(new URL('/dashboard', request.url))\n}\n```\n\n### Protected Routes\n\n```typescript\n// middleware.ts\nimport { createServerClient, type CookieOptions } from '@supabase/ssr'\nimport { NextResponse, type NextRequest } from 'next/server'\n\nexport async function middleware(request: NextRequest) {\n  let response = NextResponse.next({\n    request: {\n      headers: request.headers,\n    },\n  })\n\n  const supabase = createServerClient(\n    process.env.NEXT_PUBLIC_SUPABASE_URL!,\n    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,\n    {\n      cookies: {\n        get(name: string) {\n          return request.cookies.get(name)?.value\n        },\n        set(name: string, value: string, options: CookieOptions) {\n          request.cookies.set({\n            name,\n            value,\n            ...options,\n          })\n          response = NextResponse.next({\n            request: {\n              headers: request.headers,\n            },\n          })\n          response.cookies.set({\n            name,\n            value,\n            ...options,\n          })\n        },\n        remove(name: string, options: CookieOptions) {\n          request.cookies.set({\n            name,\n            value: '',\n            ...options,\n          })\n          response = NextResponse.next({\n            request: {\n              headers: request.headers,\n            },\n          })\n          response.cookies.set({\n            name,\n            value: '',\n            ...options,\n          })\n        },\n      },\n    }\n  )\n\n  const { data: { user } } = await supabase.auth.getUser()\n\n  // Protect dashboard routes\n  if (request.nextUrl.pathname.startsWith('/dashboard') && !user) {\n    return NextResponse.redirect(new URL('/login', request.url))\n  }\n\n  // Redirect authenticated users away from auth pages\n  if (request.nextUrl.pathname.startsWith('/login') && user) {\n    return NextResponse.redirect(new URL('/dashboard', request.url))\n  }\n\n  return response\n}\n\nexport const config = {\n  matcher: ['/dashboard/:path*', '/login', '/signup'],\n}\n```\n\n## Advanced Row Level Security\n\n### Complex RLS Policies\n\n```sql\n-- Users can only see published posts or their own drafts\nCREATE POLICY \"Users can read appropriate posts\"\n  ON posts FOR SELECT\n  USING (\n    published = true\n    OR\n    auth.uid() = user_id\n  );\n\n-- Users can update only their own posts\nCREATE POLICY \"Users can update own posts\"\n  ON posts FOR UPDATE\n  USING (auth.uid() = user_id)\n  WITH CHECK (auth.uid() = user_id);\n\n-- Team-based access\nCREATE TABLE teams (\n  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n  name TEXT NOT NULL\n);\n\nCREATE TABLE team_members (\n  team_id UUID REFERENCES teams,\n  user_id UUID REFERENCES auth.users,\n  role TEXT CHECK (role IN ('owner', 'admin', 'member')),\n  PRIMARY KEY (team_id, user_id)\n);\n\nCREATE TABLE team_documents (\n  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n  team_id UUID REFERENCES teams,\n  title TEXT,\n  content TEXT\n);\n\n-- Only team members can see team documents\nCREATE POLICY \"Team members can view documents\"\n  ON team_documents FOR SELECT\n  USING (\n    team_id IN (\n      SELECT team_id\n      FROM team_members\n      WHERE user_id = auth.uid()\n    )\n  );\n\n-- Only team owners/admins can delete\nCREATE POLICY \"Team admins can delete documents\"\n  ON team_documents FOR DELETE\n  USING (\n    team_id IN (\n      SELECT team_id\n      FROM team_members\n      WHERE user_id = auth.uid()\n        AND role IN ('owner', 'admin')\n    )\n  );\n```\n\n### Function-Based RLS\n\n```sql\n-- Create helper function\nCREATE OR REPLACE FUNCTION is_team_admin(team_id UUID)\nRETURNS BOOLEAN AS $$\nBEGIN\n  RETURN EXISTS (\n    SELECT 1\n    FROM team_members\n    WHERE team_members.team_id = is_team_admin.team_id\n      AND team_members.user_id = auth.uid()\n      AND team_members.role IN ('owner', 'admin')\n  );\nEND;\n$$ LANGUAGE plpgsql SECURITY DEFINER;\n\n-- Use in policy\nCREATE POLICY \"Admins can update team settings\"\n  ON teams FOR UPDATE\n  USING (is_team_admin(id))\n  WITH CHECK (is_team_admin(id));\n```\n\n### RLS with JWT Claims\n\n```sql\n-- Access custom JWT claims\nCREATE POLICY \"Premium users can view premium content\"\n  ON premium_content FOR SELECT\n  USING (\n    (auth.jwt() -> 'user_metadata' ->> 'subscription_tier') = 'premium'\n  );\n\n-- Role-based access\nCREATE POLICY \"Admins have full access\"\n  ON sensitive_data FOR ALL\n  USING (\n    (auth.jwt() -> 'user_metadata' ->> 'role') = 'admin'\n  );\n```\n\n## Advanced Real-time Features\n\n### Presence (Who's Online)\n\n```typescript\n'use client'\n\nimport { useEffect, useState } from 'react'\nimport { createClient } from '@/lib/supabase/client'\n\nexport function OnlineUsers() {\n  const [onlineUsers, setOnlineUsers] = useState<any[]>([])\n  const supabase = createClient()\n\n  useEffect(() => {\n    const channel = supabase.channel('online-users')\n\n    channel\n      .on('presence', { event: 'sync' }, () => {\n        const state = channel.presenceState()\n        const users = Object.values(state).flat()\n        setOnlineUsers(users)\n      })\n      .on('presence', { event: 'join' }, ({ newPresences }) => {\n        console.log('Users joined:', newPresences)\n      })\n      .on('presence', { event: 'leave' }, ({ leftPresences }) => {\n        console.log('Users left:', leftPresences)\n      })\n      .subscribe(async (status) => {\n        if (status === 'SUBSCRIBED') {\n          // Track this user\n          const { data: { user } } = await supabase.auth.getUser()\n          if (user) {\n            await channel.track({\n              user_id: user.id,\n              email: user.email,\n              online_at: new Date().toISOString(),\n            })\n          }\n        }\n      })\n\n    return () => {\n      supabase.removeChannel(channel)\n    }\n  }, [])\n\n  return (\n    <div>\n      <h3>{onlineUsers.length} users online</h3>\n      <ul>\n        {onlineUsers.map((user, i) => (\n          <li key={i}>{user.email}</li>\n        ))}\n      </ul>\n    </div>\n  )\n}\n```\n\n### Broadcast (Send Messages)\n\n```typescript\n// Cursor tracking\nexport function CollaborativeCanvas() {\n  const supabase = createClient()\n\n  useEffect(() => {\n    const channel = supabase.channel('canvas')\n\n    channel\n      .on('broadcast', { event: 'cursor' }, (payload) => {\n        // Update cursor position\n        updateCursor(payload.payload)\n      })\n      .subscribe()\n\n    // Send cursor position\n    const handleMouseMove = (e: MouseEvent) => {\n      channel.send({\n        type: 'broadcast',\n        event: 'cursor',\n        payload: { x: e.clientX, y: e.clientY },\n      })\n    }\n\n    window.addEventListener('mousemove', handleMouseMove)\n\n    return () => {\n      window.removeEventListener('mousemove', handleMouseMove)\n      supabase.removeChannel(channel)\n    }\n  }, [])\n\n  return <canvas />\n}\n```\n\n### Postgres Changes (Database Events)\n\n```typescript\n// Listen to specific columns\nconst channel = supabase\n  .channel('post-changes')\n  .on(\n    'postgres_changes',\n    {\n      event: 'UPDATE',\n      schema: 'public',\n      table: 'posts',\n      filter: 'id=eq.123',  // Specific row\n    },\n    (payload) => {\n      console.log('Post updated:', payload)\n    }\n  )\n  .subscribe()\n\n// Listen to multiple tables\nconst channel = supabase\n  .channel('changes')\n  .on(\n    'postgres_changes',\n    { event: '*', schema: 'public', table: 'posts' },\n    handlePostChange\n  )\n  .on(\n    'postgres_changes',\n    { event: '*', schema: 'public', table: 'comments' },\n    handleCommentChange\n  )\n  .subscribe()\n```\n\n## Advanced Storage\n\n### Image Transformations\n\n```typescript\n// Upload with transformation\nexport async function uploadAvatar(file: File, userId: string) {\n  const supabase = createClient()\n\n  const fileExt = file.name.split('.').pop()\n  const fileName = `${userId}-${Date.now()}.${fileExt}`\n  const filePath = `avatars/${fileName}`\n\n  const { error: uploadError } = await supabase.storage\n    .from('avatars')\n    .upload(filePath, file, {\n      cacheControl: '3600',\n      upsert: false,\n    })\n\n  if (uploadError) throw uploadError\n\n  // Get transformed image URL\n  const { data } = supabase.storage\n    .from('avatars')\n    .getPublicUrl(filePath, {\n      transform: {\n        width: 200,\n        height: 200,\n        resize: 'cover',\n        quality: 80,\n      },\n    })\n\n  return data.publicUrl\n}\n```\n\n### Signed URLs (Private Files)\n\n```typescript\n// Generate signed URL (expires after 1 hour)\nexport async function getPrivateFileUrl(path: string) {\n  const supabase = createClient()\n\n  const { data, error } = await supabase.storage\n    .from('private-files')\n    .createSignedUrl(path, 3600)  // 1 hour\n\n  if (error) throw error\n\n  return data.signedUrl\n}\n\n// Upload to private bucket\nexport async function uploadPrivateFile(file: File, userId: string) {\n  const supabase = createClient()\n\n  const filePath = `${userId}/${file.name}`\n\n  const { error } = await supabase.storage\n    .from('private-files')\n    .upload(filePath, file)\n\n  if (error) throw error\n\n  return filePath\n}\n```\n\n### Storage RLS\n\n```sql\n-- Enable RLS on storage.objects\nCREATE POLICY \"Users can upload to their own folder\"\n  ON storage.objects FOR INSERT\n  WITH CHECK (\n    bucket_id = 'avatars' AND\n    (storage.foldername(name))[1] = auth.uid()::text\n  );\n\nCREATE POLICY \"Users can view their own files\"\n  ON storage.objects FOR SELECT\n  USING (\n    bucket_id = 'avatars' AND\n    (storage.foldername(name))[1] = auth.uid()::text\n  );\n\nCREATE POLICY \"Users can update their own files\"\n  ON storage.objects FOR UPDATE\n  USING (\n    bucket_id = 'avatars' AND\n    (storage.foldername(name))[1] = auth.uid()::text\n  );\n\nCREATE POLICY \"Users can delete their own files\"\n  ON storage.objects FOR DELETE\n  USING (\n    bucket_id = 'avatars' AND\n    (storage.foldername(name))[1] = auth.uid()::text\n  );\n```\n\n## Edge Functions\n\n### Basic Edge Function\n\n```typescript\n// supabase/functions/hello/index.ts\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\n\nserve(async (req) => {\n  const { name } = await req.json()\n\n  return new Response(\n    JSON.stringify({ message: `Hello ${name}!` }),\n    {\n      headers: { 'Content-Type': 'application/json' },\n    },\n  )\n})\n```\n\n### Edge Function with Supabase Client\n\n```typescript\n// supabase/functions/create-post/index.ts\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\nimport { createClient } from 'https://esm.sh/@supabase/supabase-js@2'\n\nserve(async (req) => {\n  try {\n    const supabaseClient = createClient(\n      Deno.env.get('SUPABASE_URL') ?? '',\n      Deno.env.get('SUPABASE_ANON_KEY') ?? '',\n      {\n        global: {\n          headers: { Authorization: req.headers.get('Authorization')! },\n        },\n      }\n    )\n\n    // Get authenticated user\n    const { data: { user }, error: userError } = await supabaseClient.auth.getUser()\n    if (userError || !user) {\n      return new Response(JSON.stringify({ error: 'Unauthorized' }), {\n        status: 401,\n      })\n    }\n\n    const { title, content } = await req.json()\n\n    const { data, error } = await supabaseClient\n      .from('posts')\n      .insert({\n        title,\n        content,\n        user_id: user.id,\n      })\n      .select()\n      .single()\n\n    if (error) throw error\n\n    return new Response(JSON.stringify(data), {\n      headers: { 'Content-Type': 'application/json' },\n    })\n  } catch (error) {\n    return new Response(JSON.stringify({ error: error.message }), {\n      status: 500,\n    })\n  }\n})\n```\n\n### Scheduled Edge Function (Cron)\n\n```typescript\n// supabase/functions/cleanup-old-data/index.ts\nimport { serve } from 'https://deno.land/std@0.168.0/http/server.ts'\nimport { createClient } from 'https://esm.sh/@supabase/supabase-js@2'\n\nserve(async (req) => {\n  // Verify request is from Supabase Cron\n  const authHeader = req.headers.get('Authorization')\n  if (authHeader !== `Bearer ${Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')}`) {\n    return new Response('Unauthorized', { status: 401 })\n  }\n\n  const supabase = createClient(\n    Deno.env.get('SUPABASE_URL')!,\n    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!\n  )\n\n  // Delete old data\n  const thirtyDaysAgo = new Date()\n  thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30)\n\n  const { error } = await supabase\n    .from('temporary_data')\n    .delete()\n    .lt('created_at', thirtyDaysAgo.toISOString())\n\n  if (error) {\n    return new Response(JSON.stringify({ error: error.message }), {\n      status: 500,\n    })\n  }\n\n  return new Response(JSON.stringify({ success: true }))\n})\n\n// Configure in Dashboard: Database > Cron Jobs\n// Schedule: 0 2 * * * (2am daily)\n// HTTP Request: https://your-project.supabase.co/functions/v1/cleanup-old-data\n```\n\n### Invoke Edge Function from Client\n\n```typescript\n// Client-side\nconst { data, error } = await supabase.functions.invoke('hello', {\n  body: { name: 'World' },\n})\n\n// With auth headers automatically included\nconst { data: { session } } = await supabase.auth.getSession()\n\nconst { data, error } = await supabase.functions.invoke('create-post', {\n  body: {\n    title: 'My Post',\n    content: 'Content here',\n  },\n  headers: {\n    Authorization: `Bearer ${session?.access_token}`,\n  },\n})\n```\n\n## Vector Search (AI/RAG)\n\n### Enable pgvector\n\n```sql\n-- Enable vector extension\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Create table with embedding column\nCREATE TABLE documents (\n  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n  content TEXT NOT NULL,\n  metadata JSONB,\n  embedding vector(1536),  -- For OpenAI ada-002 (1536 dimensions)\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Create index for fast similarity search\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)\nWITH (lists = 100);\n\n-- Or use HNSW for better performance (Postgres 15+)\nCREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops);\n```\n\n### Generate and Store Embeddings\n\n```typescript\n// lib/embeddings.ts\nimport { OpenAI } from 'openai'\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n})\n\nexport async function generateEmbedding(text: string): Promise<number[]> {\n  const response = await openai.embeddings.create({\n    model: 'text-embedding-ada-002',\n    input: text,\n  })\n\n  return response.data[0].embedding\n}\n\n// Store document with embedding\nexport async function storeDocument(content: string, metadata: any) {\n  const supabase = createClient()\n\n  const embedding = await generateEmbedding(content)\n\n  const { data, error } = await supabase\n    .from('documents')\n    .insert({\n      content,\n      metadata,\n      embedding,\n    })\n    .select()\n    .single()\n\n  if (error) throw error\n\n  return data\n}\n```\n\n### Semantic Search\n\n```typescript\n// Search similar documents\nexport async function searchSimilarDocuments(query: string, limit = 5) {\n  const supabase = createClient()\n\n  // Generate embedding for query\n  const queryEmbedding = await generateEmbedding(query)\n\n  // Search with RPC function\n  const { data, error } = await supabase.rpc('match_documents', {\n    query_embedding: queryEmbedding,\n    match_threshold: 0.78,  // Minimum similarity\n    match_count: limit,\n  })\n\n  if (error) throw error\n\n  return data\n}\n\n// Create the RPC function\n```\n\n```sql\nCREATE OR REPLACE FUNCTION match_documents (\n  query_embedding vector(1536),\n  match_threshold float,\n  match_count int\n)\nRETURNS TABLE (\n  id UUID,\n  content TEXT,\n  metadata JSONB,\n  similarity float\n)\nLANGUAGE SQL STABLE\nAS $$\n  SELECT\n    documents.id,\n    documents.content,\n    documents.metadata,\n    1 - (documents.embedding <=> query_embedding) AS similarity\n  FROM documents\n  WHERE 1 - (documents.embedding <=> query_embedding) > match_threshold\n  ORDER BY documents.embedding <=> query_embedding\n  LIMIT match_count;\n$$;\n```\n\n### RAG (Retrieval Augmented Generation)\n\n```typescript\nexport async function ragQuery(question: string) {\n  // 1. Search for relevant documents\n  const relevantDocs = await searchSimilarDocuments(question, 5)\n\n  // 2. Build context from relevant documents\n  const context = relevantDocs\n    .map(doc => doc.content)\n    .join('\\n\\n')\n\n  // 3. Generate answer with GPT\n  const completion = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [\n      {\n        role: 'system',\n        content: 'You are a helpful assistant. Answer questions based on the provided context.',\n      },\n      {\n        role: 'user',\n        content: `Context:\\n${context}\\n\\nQuestion: ${question}`,\n      },\n    ],\n  })\n\n  return {\n    answer: completion.choices[0].message.content,\n    sources: relevantDocs,\n  }\n}\n```\n\n## Database Functions & Triggers\n\n### Custom Functions\n\n```sql\n-- Get user's post count\nCREATE OR REPLACE FUNCTION get_user_post_count(user_id UUID)\nRETURNS INTEGER AS $$\n  SELECT COUNT(*)::INTEGER\n  FROM posts\n  WHERE posts.user_id = get_user_post_count.user_id;\n$$ LANGUAGE SQL STABLE;\n\n-- Call from TypeScript\nconst { data, error } = await supabase.rpc('get_user_post_count', {\n  user_id: userId,\n})\n```\n\n### Triggers\n\n```sql\n-- Auto-update updated_at column\nCREATE OR REPLACE FUNCTION handle_updated_at()\nRETURNS TRIGGER AS $$\nBEGIN\n  NEW.updated_at = NOW();\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER set_updated_at\n  BEFORE UPDATE ON posts\n  FOR EACH ROW\n  EXECUTE FUNCTION handle_updated_at();\n\n-- Update post count when post is created/deleted\nCREATE OR REPLACE FUNCTION update_post_count()\nRETURNS TRIGGER AS $$\nBEGIN\n  IF TG_OP = 'INSERT' THEN\n    UPDATE profiles\n    SET post_count = post_count + 1\n    WHERE id = NEW.user_id;\n    RETURN NEW;\n  ELSIF TG_OP = 'DELETE' THEN\n    UPDATE profiles\n    SET post_count = post_count - 1\n    WHERE id = OLD.user_id;\n    RETURN OLD;\n  END IF;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER update_post_count_trigger\n  AFTER INSERT OR DELETE ON posts\n  FOR EACH ROW\n  EXECUTE FUNCTION update_post_count();\n```\n\n## Performance Optimization\n\n### Query Optimization\n\n```typescript\n// Bad: N+1 query problem\nconst { data: posts } = await supabase.from('posts').select('*')\nfor (const post of posts) {\n  const { data: author } = await supabase\n    .from('profiles')\n    .select('*')\n    .eq('id', post.user_id)\n    .single()\n}\n\n// Good: Join in single query\nconst { data: posts } = await supabase\n  .from('posts')\n  .select(`\n    *,\n    profiles (\n      id,\n      name,\n      avatar_url\n    )\n  `)\n\n// Good: Use specific columns\nconst { data: posts } = await supabase\n  .from('posts')\n  .select('id, title, created_at, profiles(name)')  // Only what you need\n  .eq('published', true)\n  .order('created_at', { ascending: false })\n  .limit(20)\n```\n\n### Indexes\n\n```sql\n-- Index on frequently filtered columns\nCREATE INDEX posts_user_id_idx ON posts(user_id);\nCREATE INDEX posts_created_at_idx ON posts(created_at DESC);\n\n-- Partial index (filtered)\nCREATE INDEX posts_published_idx ON posts(published)\nWHERE published = true;\n\n-- Composite index\nCREATE INDEX posts_user_published_idx ON posts(user_id, published, created_at DESC);\n\n-- Full-text search index\nCREATE INDEX posts_content_idx ON posts\nUSING GIN (to_tsvector('english', content));\n```\n\n### Connection Pooling\n\n```typescript\n// Use connection pooler for serverless (Supavisor)\n// Connection string: postgresql://postgres.[project-ref]:[password]@aws-0-us-east-1.pooler.supabase.com:6543/postgres\n\nimport { createClient } from '@supabase/supabase-js'\n\nconst supabase = createClient(\n  process.env.SUPABASE_URL!,\n  process.env.SUPABASE_ANON_KEY!,\n  {\n    db: {\n      schema: 'public',\n    },\n    global: {\n      headers: { 'x-my-custom-header': 'my-app-name' },\n    },\n  }\n)\n```\n\n### Caching\n\n```typescript\n// Next.js cache with revalidation\nimport { unstable_cache } from 'next/cache'\nimport { createClient } from '@/lib/supabase/server'\n\nexport const getCachedPosts = unstable_cache(\n  async () => {\n    const supabase = createClient()\n    const { data } = await supabase\n      .from('posts')\n      .select('*')\n      .eq('published', true)\n    return data\n  },\n  ['posts'],\n  {\n    revalidate: 300,  // 5 minutes\n    tags: ['posts'],\n  }\n)\n\n// Revalidate on mutation\nimport { revalidateTag } from 'next/cache'\n\nexport async function createPost(data: any) {\n  const supabase = createClient()\n  await supabase.from('posts').insert(data)\n  revalidateTag('posts')\n}\n```\n\n## Local Development\n\n### Setup Local Supabase\n\n```bash\n# Install Supabase CLI\nbrew install supabase/tap/supabase\n\n# Initialize project\nsupabase init\n\n# Start local Supabase (Docker required)\nsupabase start\n\n# This starts:\n# - PostgreSQL\n# - GoTrue (Auth)\n# - Realtime\n# - Storage\n# - Kong (API Gateway)\n# - Studio (Dashboard)\n```\n\n### Local Development URLs\n\n```bash\n# After supabase start:\nAPI URL: http://localhost:54321\nStudio URL: http://localhost:54323\nInbucket URL: http://localhost:54324  # Email testing\n```\n\n### Migration Workflow\n\n```bash\n# Create migration\nsupabase migration new add_posts_table\n\n# Edit migration file in supabase/migrations/\n\n# Apply migration locally\nsupabase db reset\n\n# Push to production\nsupabase db push\n\n# Pull remote schema\nsupabase db pull\n```\n\n### Generate Types from Local DB\n\n```bash\n# Generate TypeScript types\nsupabase gen types typescript --local > types/database.ts\n```\n\n## Testing\n\n### Testing RLS Policies\n\n```sql\n-- Test as specific user\nSET LOCAL ROLE authenticated;\nSET LOCAL \"request.jwt.claims\" TO '{\"sub\": \"user-uuid-here\"}';\n\n-- Test query\nSELECT * FROM posts;\n\n-- Reset\nRESET ROLE;\n```\n\n### Testing with Supabase Test Helpers\n\n```typescript\n// tests/posts.test.ts\nimport { createClient } from '@supabase/supabase-js'\n\nconst supabase = createClient(\n  process.env.SUPABASE_URL!,\n  process.env.SUPABASE_SERVICE_ROLE_KEY!  // For testing\n)\n\ndescribe('Posts', () => {\n  beforeEach(async () => {\n    // Clean up\n    await supabase.from('posts').delete().neq('id', '00000000-0000-0000-0000-000000000000')\n  })\n\n  it('should create post', async () => {\n    const { data, error } = await supabase\n      .from('posts')\n      .insert({ title: 'Test', content: 'Test' })\n      .select()\n      .single()\n\n    expect(error).toBeNull()\n    expect(data.title).toBe('Test')\n  })\n})\n```\n\n## Error Handling\n\n### Comprehensive Error Handler\n\n```typescript\nimport { PostgrestError } from '@supabase/supabase-js'\n\nexport function handleSupabaseError(error: PostgrestError | null) {\n  if (!error) return null\n\n  // Common error codes\n  const errorMessages: Record<string, string> = {\n    '23505': 'This record already exists',  // Unique violation\n    '23503': 'Related record not found',    // Foreign key violation\n    '42P01': 'Table does not exist',\n    '42501': 'Permission denied',\n    'PGRST116': 'No rows found',\n  }\n\n  const userMessage = errorMessages[error.code] || error.message\n\n  console.error('Supabase error:', {\n    code: error.code,\n    message: error.message,\n    details: error.details,\n    hint: error.hint,\n  })\n\n  return userMessage\n}\n\n// Usage\nconst { data, error } = await supabase.from('posts').insert(postData)\n\nif (error) {\n  const message = handleSupabaseError(error)\n  toast.error(message)\n  return\n}\n```\n\n## Best Practices\n\n1. **Row Level Security:**\n   - Enable RLS on ALL tables\n   - Never rely on client-side checks alone\n   - Test policies thoroughly\n   - Use service role key sparingly (server-side only)\n\n2. **Query Optimization:**\n   - Use `.select()` to specify needed columns\n   - Add database indexes for filtered/sorted columns\n   - Use `.limit()` to cap results\n   - Consider pagination for large datasets\n\n3. **Real-time Subscriptions:**\n   - Always unsubscribe when component unmounts\n   - Use RLS policies to filter events\n   - Use broadcast for ephemeral data\n   - Limit number of simultaneous subscriptions\n\n4. **Authentication:**\n   - Store JWT in httpOnly cookies when possible\n   - Refresh tokens before expiry\n   - Handle auth state changes\n   - Validate user on server-side\n\n5. **Storage:**\n   - Set appropriate bucket policies\n   - Use image transformations for optimization\n   - Consider storage limits\n   - Clean up unused files\n\n6. **Error Handling:**\n   - Always check `error` object\n   - Provide user-friendly error messages\n   - Log errors for debugging\n   - Handle network failures gracefully\n\n## Common Patterns\n\n### Auto-create Profile on Signup\n\n```sql\n-- Create profiles table\nCREATE TABLE profiles (\n  id UUID REFERENCES auth.users PRIMARY KEY,\n  name TEXT,\n  avatar_url TEXT,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Trigger function\nCREATE OR REPLACE FUNCTION public.handle_new_user()\nRETURNS TRIGGER AS $$\nBEGIN\n  INSERT INTO public.profiles (id, name, avatar_url)\n  VALUES (\n    NEW.id,\n    NEW.raw_user_meta_data->>'name',\n    NEW.raw_user_meta_data->>'avatar_url'\n  );\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql SECURITY DEFINER;\n\n-- Trigger\nCREATE TRIGGER on_auth_user_created\n  AFTER INSERT ON auth.users\n  FOR EACH ROW EXECUTE FUNCTION public.handle_new_user();\n```\n\n## Documentation Quick Reference\n\n**Need to find something specific?**\n\nSearch the 2,616 documentation files:\n\n```bash\n# Search all docs\ngrep -r \"search term\" docs/supabase_com/\n\n# Find guides\nls docs/supabase_com/guides_*\n\n# Find API reference\nls docs/supabase_com/reference_*\n```\n\n**Common doc locations:**\n- Guides: `docs/supabase_com/guides_*`\n- JavaScript Reference: `docs/supabase_com/reference_javascript_*`\n- Database: `docs/supabase_com/guides_database_*`\n- Auth: `docs/supabase_com/guides_auth_*`\n- Storage: `docs/supabase_com/guides_storage_*`\n\n## Resources\n\n- **Dashboard:** https://supabase.com/dashboard\n- **Docs:** https://supabase.com/docs\n- **Status:** https://status.supabase.com\n- **CLI Docs:** https://supabase.com/docs/guides/cli\n\n## Implementation Checklist\n\n- [ ] Create Supabase project\n- [ ] Install: `npm install @supabase/supabase-js`\n- [ ] Set environment variables\n- [ ] Design database schema\n- [ ] Create migrations\n- [ ] Enable RLS and create policies\n- [ ] Set up authentication\n- [ ] Implement auth state management\n- [ ] Create CRUD operations\n- [ ] Add real-time subscriptions (if needed)\n- [ ] Configure storage buckets (if needed)\n- [ ] Test RLS policies\n- [ ] Add database indexes\n- [ ] Deploy edge functions (if needed)\n- [ ] Test in production",
        "skills/branch-finalization/SKILL.md": "---\nname: branch-finalization\ndescription: Finalize development branches for integration. Prepares branches for merging with cleanup, rebasing, and verification steps.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Finishing a Development Branch\n\n## Overview\n\nGuide completion of development work by presenting clear options and handling chosen workflow.\n\n**Core principle:** Verify tests  Present options  Execute choice  Clean up.\n\n**Announce at start:** \"I'm using the finishing-a-development-branch skill to complete this work.\"\n\n## The Process\n\n### Step 1: Verify Tests\n\n**Before presenting options, verify tests pass:**\n\n```bash\n# Run project's test suite\nnpm test / cargo test / pytest / go test ./...\n```\n\n**If tests fail:**\n```\nTests failing (<N> failures). Must fix before completing:\n\n[Show failures]\n\nCannot proceed with merge/PR until tests pass.\n```\n\nStop. Don't proceed to Step 2.\n\n**If tests pass:** Continue to Step 2.\n\n### Step 2: Determine Base Branch\n\n```bash\n# Try common base branches\ngit merge-base HEAD main 2>/dev/null || git merge-base HEAD master 2>/dev/null\n```\n\nOr ask: \"This branch split from main - is that correct?\"\n\n### Step 3: Present Options\n\nPresent exactly these 4 options:\n\n```\nImplementation complete. What would you like to do?\n\n1. Merge back to <base-branch> locally\n2. Push and create a Pull Request\n3. Keep the branch as-is (I'll handle it later)\n4. Discard this work\n\nWhich option?\n```\n\n**Don't add explanation** - keep options concise.\n\n### Step 4: Execute Choice\n\n#### Option 1: Merge Locally\n\n```bash\n# Switch to base branch\ngit checkout <base-branch>\n\n# Pull latest\ngit pull\n\n# Merge feature branch\ngit merge <feature-branch>\n\n# Verify tests on merged result\n<test command>\n\n# If tests pass\ngit branch -d <feature-branch>\n```\n\nThen: Cleanup worktree (Step 5)\n\n#### Option 2: Push and Create PR\n\n```bash\n# Push branch\ngit push -u origin <feature-branch>\n\n# Create PR\ngh pr create --title \"<title>\" --body \"$(cat <<'EOF'\n## Summary\n<2-3 bullets of what changed>\n\n## Test Plan\n- [ ] <verification steps>\nEOF\n)\"\n```\n\nThen: Cleanup worktree (Step 5)\n\n#### Option 3: Keep As-Is\n\nReport: \"Keeping branch <name>. Worktree preserved at <path>.\"\n\n**Don't cleanup worktree.**\n\n#### Option 4: Discard\n\n**Confirm first:**\n```\nThis will permanently delete:\n- Branch <name>\n- All commits: <commit-list>\n- Worktree at <path>\n\nType 'discard' to confirm.\n```\n\nWait for exact confirmation.\n\nIf confirmed:\n```bash\ngit checkout <base-branch>\ngit branch -D <feature-branch>\n```\n\nThen: Cleanup worktree (Step 5)\n\n### Step 5: Cleanup Worktree\n\n**For Options 1, 2, 4:**\n\nCheck if in worktree:\n```bash\ngit worktree list | grep $(git branch --show-current)\n```\n\nIf yes:\n```bash\ngit worktree remove <worktree-path>\n```\n\n**For Option 3:** Keep worktree.\n\n## Quick Reference\n\n| Option | Merge | Push | Keep Worktree | Cleanup Branch |\n|--------|-------|------|---------------|----------------|\n| 1. Merge locally |  | - | - |  |\n| 2. Create PR | - |  |  | - |\n| 3. Keep as-is | - | - |  | - |\n| 4. Discard | - | - | - |  (force) |\n\n## Common Mistakes\n\n**Skipping test verification**\n- **Problem:** Merge broken code, create failing PR\n- **Fix:** Always verify tests before offering options\n\n**Open-ended questions**\n- **Problem:** \"What should I do next?\"  ambiguous\n- **Fix:** Present exactly 4 structured options\n\n**Automatic worktree cleanup**\n- **Problem:** Remove worktree when might need it (Option 2, 3)\n- **Fix:** Only cleanup for Options 1 and 4\n\n**No confirmation for discard**\n- **Problem:** Accidentally delete work\n- **Fix:** Require typed \"discard\" confirmation\n\n## Red Flags\n\n**Never:**\n- Proceed with failing tests\n- Merge without verifying tests on result\n- Delete work without confirmation\n- Force-push without explicit request\n\n**Always:**\n- Verify tests before offering options\n- Present exactly 4 options\n- Get typed confirmation for Option 4\n- Clean up worktree for Options 1 & 4 only\n\n## Integration\n\n**Called by:**\n- **subagent-driven-development** (Step 7) - After all tasks complete\n- **executing-plans** (Step 5) - After all batches complete\n\n**Pairs with:**\n- **using-git-worktrees** - Cleans up worktree created by that skill\n",
        "skills/browser-automation-framework/SKILL.md": "---\nname: browser-automation-framework\ndescription: Automate browser interactions and web testing. Provides scriptable browser control for web scraping, testing, and automation.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n**IMPORTANT - Path Resolution:**\nThis skill can be installed in different locations (plugin system, manual installation, global, or project-specific). Before executing any commands, determine the skill directory based on where you loaded this SKILL.md file, and use that path in all commands below. Replace `$SKILL_DIR` with the actual discovered path.\n\nCommon installation paths:\n- Plugin system: `~/.claude/plugins/marketplaces/playwright-skill/skills/playwright-skill`\n- Manual global: `~/.claude/skills/playwright-skill`\n- Project-specific: `<project>/.claude/skills/playwright-skill`\n\n# Playwright Browser Automation\n\nGeneral-purpose browser automation skill. I'll write custom Playwright code for any automation task you request and execute it via the universal executor.\n\n**CRITICAL WORKFLOW - Follow these steps in order:**\n\n1. **Auto-detect dev servers** - For localhost testing, ALWAYS run server detection FIRST:\n   ```bash\n   cd $SKILL_DIR && node -e \"require('./lib/helpers').detectDevServers().then(servers => console.log(JSON.stringify(servers)))\"\n   ```\n   - If **1 server found**: Use it automatically, inform user\n   - If **multiple servers found**: Ask user which one to test\n   - If **no servers found**: Ask for URL or offer to help start dev server\n\n2. **Write scripts to /tmp** - NEVER write test files to skill directory; always use `/tmp/playwright-test-*.js`\n\n3. **Use visible browser by default** - Always use `headless: false` unless user specifically requests headless mode\n\n4. **Parameterize URLs** - Always make URLs configurable via environment variable or constant at top of script\n\n## How It Works\n\n1. You describe what you want to test/automate\n2. I auto-detect running dev servers (or ask for URL if testing external site)\n3. I write custom Playwright code in `/tmp/playwright-test-*.js` (won't clutter your project)\n4. I execute it via: `cd $SKILL_DIR && node run.js /tmp/playwright-test-*.js`\n5. Results displayed in real-time, browser window visible for debugging\n6. Test files auto-cleaned from /tmp by your OS\n\n## Setup (First Time)\n\n```bash\ncd $SKILL_DIR\nnpm run setup\n```\n\nThis installs Playwright and Chromium browser. Only needed once.\n\n## Execution Pattern\n\n**Step 1: Detect dev servers (for localhost testing)**\n\n```bash\ncd $SKILL_DIR && node -e \"require('./lib/helpers').detectDevServers().then(s => console.log(JSON.stringify(s)))\"\n```\n\n**Step 2: Write test script to /tmp with URL parameter**\n\n```javascript\n// /tmp/playwright-test-page.js\nconst { chromium } = require('playwright');\n\n// Parameterized URL (detected or user-provided)\nconst TARGET_URL = 'http://localhost:3001'; // <-- Auto-detected or from user\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  await page.goto(TARGET_URL);\n  console.log('Page loaded:', await page.title());\n\n  await page.screenshot({ path: '/tmp/screenshot.png', fullPage: true });\n  console.log(' Screenshot saved to /tmp/screenshot.png');\n\n  await browser.close();\n})();\n```\n\n**Step 3: Execute from skill directory**\n\n```bash\ncd $SKILL_DIR && node run.js /tmp/playwright-test-page.js\n```\n\n## Common Patterns\n\n### Test a Page (Multiple Viewports)\n\n```javascript\n// /tmp/playwright-test-responsive.js\nconst { chromium } = require('playwright');\n\nconst TARGET_URL = 'http://localhost:3001'; // Auto-detected\n\n(async () => {\n  const browser = await chromium.launch({ headless: false, slowMo: 100 });\n  const page = await browser.newPage();\n\n  // Desktop test\n  await page.setViewportSize({ width: 1920, height: 1080 });\n  await page.goto(TARGET_URL);\n  console.log('Desktop - Title:', await page.title());\n  await page.screenshot({ path: '/tmp/desktop.png', fullPage: true });\n\n  // Mobile test\n  await page.setViewportSize({ width: 375, height: 667 });\n  await page.screenshot({ path: '/tmp/mobile.png', fullPage: true });\n\n  await browser.close();\n})();\n```\n\n### Test Login Flow\n\n```javascript\n// /tmp/playwright-test-login.js\nconst { chromium } = require('playwright');\n\nconst TARGET_URL = 'http://localhost:3001'; // Auto-detected\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  await page.goto(`${TARGET_URL}/login`);\n\n  await page.fill('input[name=\"email\"]', 'test@example.com');\n  await page.fill('input[name=\"password\"]', 'password123');\n  await page.click('button[type=\"submit\"]');\n\n  // Wait for redirect\n  await page.waitForURL('**/dashboard');\n  console.log(' Login successful, redirected to dashboard');\n\n  await browser.close();\n})();\n```\n\n### Fill and Submit Form\n\n```javascript\n// /tmp/playwright-test-form.js\nconst { chromium } = require('playwright');\n\nconst TARGET_URL = 'http://localhost:3001'; // Auto-detected\n\n(async () => {\n  const browser = await chromium.launch({ headless: false, slowMo: 50 });\n  const page = await browser.newPage();\n\n  await page.goto(`${TARGET_URL}/contact`);\n\n  await page.fill('input[name=\"name\"]', 'John Doe');\n  await page.fill('input[name=\"email\"]', 'john@example.com');\n  await page.fill('textarea[name=\"message\"]', 'Test message');\n  await page.click('button[type=\"submit\"]');\n\n  // Verify submission\n  await page.waitForSelector('.success-message');\n  console.log(' Form submitted successfully');\n\n  await browser.close();\n})();\n```\n\n### Check for Broken Links\n\n```javascript\nconst { chromium } = require('playwright');\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  await page.goto('http://localhost:3000');\n\n  const links = await page.locator('a[href^=\"http\"]').all();\n  const results = { working: 0, broken: [] };\n\n  for (const link of links) {\n    const href = await link.getAttribute('href');\n    try {\n      const response = await page.request.head(href);\n      if (response.ok()) {\n        results.working++;\n      } else {\n        results.broken.push({ url: href, status: response.status() });\n      }\n    } catch (e) {\n      results.broken.push({ url: href, error: e.message });\n    }\n  }\n\n  console.log(` Working links: ${results.working}`);\n  console.log(` Broken links:`, results.broken);\n\n  await browser.close();\n})();\n```\n\n### Take Screenshot with Error Handling\n\n```javascript\nconst { chromium } = require('playwright');\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  try {\n    await page.goto('http://localhost:3000', {\n      waitUntil: 'networkidle',\n      timeout: 10000\n    });\n\n    await page.screenshot({\n      path: '/tmp/screenshot.png',\n      fullPage: true\n    });\n\n    console.log(' Screenshot saved to /tmp/screenshot.png');\n  } catch (error) {\n    console.error(' Error:', error.message);\n  } finally {\n    await browser.close();\n  }\n})();\n```\n\n### Test Responsive Design\n\n```javascript\n// /tmp/playwright-test-responsive-full.js\nconst { chromium } = require('playwright');\n\nconst TARGET_URL = 'http://localhost:3001'; // Auto-detected\n\n(async () => {\n  const browser = await chromium.launch({ headless: false });\n  const page = await browser.newPage();\n\n  const viewports = [\n    { name: 'Desktop', width: 1920, height: 1080 },\n    { name: 'Tablet', width: 768, height: 1024 },\n    { name: 'Mobile', width: 375, height: 667 }\n  ];\n\n  for (const viewport of viewports) {\n    console.log(`Testing ${viewport.name} (${viewport.width}x${viewport.height})`);\n\n    await page.setViewportSize({\n      width: viewport.width,\n      height: viewport.height\n    });\n\n    await page.goto(TARGET_URL);\n    await page.waitForTimeout(1000);\n\n    await page.screenshot({\n      path: `/tmp/${viewport.name.toLowerCase()}.png`,\n      fullPage: true\n    });\n  }\n\n  console.log(' All viewports tested');\n  await browser.close();\n})();\n```\n\n## Inline Execution (Simple Tasks)\n\nFor quick one-off tasks, you can execute code inline without creating files:\n\n```bash\n# Take a quick screenshot\ncd $SKILL_DIR && node run.js \"\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\nawait page.goto('http://localhost:3001');\nawait page.screenshot({ path: '/tmp/quick-screenshot.png', fullPage: true });\nconsole.log('Screenshot saved');\nawait browser.close();\n\"\n```\n\n**When to use inline vs files:**\n- **Inline**: Quick one-off tasks (screenshot, check if element exists, get page title)\n- **Files**: Complex tests, responsive design checks, anything user might want to re-run\n\n## Available Helpers\n\nOptional utility functions in `lib/helpers.js`:\n\n```javascript\nconst helpers = require('./lib/helpers');\n\n// Detect running dev servers (CRITICAL - use this first!)\nconst servers = await helpers.detectDevServers();\nconsole.log('Found servers:', servers);\n\n// Safe click with retry\nawait helpers.safeClick(page, 'button.submit', { retries: 3 });\n\n// Safe type with clear\nawait helpers.safeType(page, '#username', 'testuser');\n\n// Take timestamped screenshot\nawait helpers.takeScreenshot(page, 'test-result');\n\n// Handle cookie banners\nawait helpers.handleCookieBanner(page);\n\n// Extract table data\nconst data = await helpers.extractTableData(page, 'table.results');\n```\n\nSee `lib/helpers.js` for full list.\n\n## Custom HTTP Headers\n\nConfigure custom headers for all HTTP requests via environment variables. Useful for:\n- Identifying automated traffic to your backend\n- Getting LLM-optimized responses (e.g., plain text errors instead of styled HTML)\n- Adding authentication tokens globally\n\n### Configuration\n\n**Single header (common case):**\n```bash\nPW_HEADER_NAME=X-Automated-By PW_HEADER_VALUE=playwright-skill \\\n  cd $SKILL_DIR && node run.js /tmp/my-script.js\n```\n\n**Multiple headers (JSON format):**\n```bash\nPW_EXTRA_HEADERS='{\"X-Automated-By\":\"playwright-skill\",\"X-Debug\":\"true\"}' \\\n  cd $SKILL_DIR && node run.js /tmp/my-script.js\n```\n\n### How It Works\n\nHeaders are automatically applied when using `helpers.createContext()`:\n\n```javascript\nconst context = await helpers.createContext(browser);\nconst page = await context.newPage();\n// All requests from this page include your custom headers\n```\n\nFor scripts using raw Playwright API, use the injected `getContextOptionsWithHeaders()`:\n\n```javascript\nconst context = await browser.newContext(\n  getContextOptionsWithHeaders({ viewport: { width: 1920, height: 1080 } })\n);\n```\n\n## Advanced Usage\n\nFor comprehensive Playwright API documentation, see [API_REFERENCE.md](API_REFERENCE.md):\n\n- Selectors & Locators best practices\n- Network interception & API mocking\n- Authentication & session management\n- Visual regression testing\n- Mobile device emulation\n- Performance testing\n- Debugging techniques\n- CI/CD integration\n\n## Tips\n\n- **CRITICAL: Detect servers FIRST** - Always run `detectDevServers()` before writing test code for localhost testing\n- **Custom headers** - Use `PW_HEADER_NAME`/`PW_HEADER_VALUE` env vars to identify automated traffic to your backend\n- **Use /tmp for test files** - Write to `/tmp/playwright-test-*.js`, never to skill directory or user's project\n- **Parameterize URLs** - Put detected/provided URL in a `TARGET_URL` constant at the top of every script\n- **DEFAULT: Visible browser** - Always use `headless: false` unless user explicitly asks for headless mode\n- **Headless mode** - Only use `headless: true` when user specifically requests \"headless\" or \"background\" execution\n- **Slow down:** Use `slowMo: 100` to make actions visible and easier to follow\n- **Wait strategies:** Use `waitForURL`, `waitForSelector`, `waitForLoadState` instead of fixed timeouts\n- **Error handling:** Always use try-catch for robust automation\n- **Console output:** Use `console.log()` to track progress and show what's happening\n\n## Troubleshooting\n\n**Playwright not installed:**\n```bash\ncd $SKILL_DIR && npm run setup\n```\n\n**Module not found:**\nEnsure running from skill directory via `run.js` wrapper\n\n**Browser doesn't open:**\nCheck `headless: false` and ensure display available\n\n**Element not found:**\nAdd wait: `await page.waitForSelector('.element', { timeout: 10000 })`\n\n## Example Usage\n\n```\nUser: \"Test if the marketing page looks good\"\n\nClaude: I'll test the marketing page across multiple viewports. Let me first detect running servers...\n[Runs: detectDevServers()]\n[Output: Found server on port 3001]\nI found your dev server running on http://localhost:3001\n\n[Writes custom automation script to /tmp/playwright-test-marketing.js with URL parameterized]\n[Runs: cd $SKILL_DIR && node run.js /tmp/playwright-test-marketing.js]\n[Shows results with screenshots from /tmp/]\n```\n\n```\nUser: \"Check if login redirects correctly\"\n\nClaude: I'll test the login flow. First, let me check for running servers...\n[Runs: detectDevServers()]\n[Output: Found servers on ports 3000 and 3001]\nI found 2 dev servers. Which one should I test?\n- http://localhost:3000\n- http://localhost:3001\n\nUser: \"Use 3001\"\n\n[Writes login automation to /tmp/playwright-test-login.js]\n[Runs: cd $SKILL_DIR && node run.js /tmp/playwright-test-login.js]\n[Reports:  Login successful, redirected to /dashboard]\n```\n\n## Notes\n\n- Each automation is custom-written for your specific request\n- Not limited to pre-built scripts - any browser task possible\n- Auto-detects running dev servers to eliminate hardcoded URLs\n- Test scripts written to `/tmp` for automatic cleanup (no clutter)\n- Code executes reliably with proper module resolution via `run.js`\n- Progressive disclosure - API_REFERENCE.md loaded only when advanced features needed\n",
        "skills/capability-activation/SKILL.md": "---\nname: capability-activation\ndescription: Activate and leverage advanced capabilities effectively. Demonstrates how to use specialized tools and powerful features.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n<EXTREMELY-IMPORTANT>\nIf you think there is even a 1% chance a skill might apply to what you are doing, you ABSOLUTELY MUST invoke the skill.\n\nIF A SKILL APPLIES TO YOUR TASK, YOU DO NOT HAVE A CHOICE. YOU MUST USE IT.\n\nThis is not negotiable. This is not optional. You cannot rationalize your way out of this.\n</EXTREMELY-IMPORTANT>\n\n## How to Access Skills\n\n**In Claude Code:** Use the `Skill` tool. When you invoke a skill, its content is loaded and presented to youfollow it directly. Never use the Read tool on skill files.\n\n**In other environments:** Check your platform's documentation for how skills are loaded.\n\n# Using Skills\n\n## The Rule\n\n**Check for skills BEFORE ANY RESPONSE.** This includes clarifying questions. Even 1% chance means invoke the Skill tool first.\n\n```dot\ndigraph skill_flow {\n    \"User message received\" [shape=doublecircle];\n    \"Might any skill apply?\" [shape=diamond];\n    \"Invoke Skill tool\" [shape=box];\n    \"Announce: 'Using [skill] to [purpose]'\" [shape=box];\n    \"Has checklist?\" [shape=diamond];\n    \"Create TodoWrite todo per item\" [shape=box];\n    \"Follow skill exactly\" [shape=box];\n    \"Respond (including clarifications)\" [shape=doublecircle];\n\n    \"User message received\" -> \"Might any skill apply?\";\n    \"Might any skill apply?\" -> \"Invoke Skill tool\" [label=\"yes, even 1%\"];\n    \"Might any skill apply?\" -> \"Respond (including clarifications)\" [label=\"definitely not\"];\n    \"Invoke Skill tool\" -> \"Announce: 'Using [skill] to [purpose]'\";\n    \"Announce: 'Using [skill] to [purpose]'\" -> \"Has checklist?\";\n    \"Has checklist?\" -> \"Create TodoWrite todo per item\" [label=\"yes\"];\n    \"Has checklist?\" -> \"Follow skill exactly\" [label=\"no\"];\n    \"Create TodoWrite todo per item\" -> \"Follow skill exactly\";\n}\n```\n\n## Red Flags\n\nThese thoughts mean STOPyou're rationalizing:\n\n| Thought | Reality |\n|---------|---------|\n| \"This is just a simple question\" | Questions are tasks. Check for skills. |\n| \"I need more context first\" | Skill check comes BEFORE clarifying questions. |\n| \"Let me explore the codebase first\" | Skills tell you HOW to explore. Check first. |\n| \"I can check git/files quickly\" | Files lack conversation context. Check for skills. |\n| \"Let me gather information first\" | Skills tell you HOW to gather information. |\n| \"This doesn't need a formal skill\" | If a skill exists, use it. |\n| \"I remember this skill\" | Skills evolve. Read current version. |\n| \"This doesn't count as a task\" | Action = task. Check for skills. |\n| \"The skill is overkill\" | Simple things become complex. Use it. |\n| \"I'll just do this one thing first\" | Check BEFORE doing anything. |\n| \"This feels productive\" | Undisciplined action wastes time. Skills prevent this. |\n\n## Skill Priority\n\nWhen multiple skills could apply, use this order:\n\n1. **Process skills first** (brainstorming, debugging) - these determine HOW to approach the task\n2. **Implementation skills second** (frontend-design, mcp-builder) - these guide execution\n\n\"Let's build X\"  brainstorming first, then implementation skills.\n\"Fix this bug\"  debugging first, then domain-specific skills.\n\n## Skill Types\n\n**Rigid** (TDD, debugging): Follow exactly. Don't adapt away discipline.\n\n**Flexible** (patterns): Adapt principles to context.\n\nThe skill itself tells you which.\n\n## User Instructions\n\nInstructions say WHAT, not HOW. \"Add X\" or \"Fix Y\" doesn't mean skip workflows.\n",
        "skills/capability-architect/SKILL.md": "---\nname: capability-architect\ndescription: Create new capabilities and skills systematically. Architects, documents, and implements reusable skills with proper specifications.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasksthey transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.\n\n**Default assumption: Claude is already very smart.** Only add context Claude doesn't already have. Challenge each piece of information: \"Does Claude really need this explanation?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Set Appropriate Degrees of Freedom\n\nMatch the level of specificity to the task's fragility and variability:\n\n**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.\n\n**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.\n\n**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.\n\nThink of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n SKILL.md (required)\n    YAML frontmatter metadata (required)\n       name: (required)\n       description: (required)\n    Markdown instructions (required)\n Bundled Resources (optional)\n     scripts/          - Executable code (Python/Bash/etc.)\n     references/       - Documentation intended to be loaded into context as needed\n     assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used. Prefer third-person descriptions (e.g., \"This skill should be used when...\").\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skillthis keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n#### What to Not Include in a Skill\n\nA skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- etc.\n\nThe skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)\n\n#### Progressive Disclosure Patterns\n\nKeep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.\n\n**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.\n\n**Pattern 1: High-level guide with references**\n\n```markdown\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n[code example]\n\n## Advanced features\n\n- **Form filling**: See [FORMS.md](FORMS.md) for complete guide\n- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n```\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n**Pattern 2: Domain-specific organization**\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context:\n\n```\nbigquery-skill/\n SKILL.md (overview and navigation)\n reference/\n     finance.md (revenue, billing metrics)\n     sales.md (opportunities, pipeline)\n     product.md (API usage, features)\n     marketing.md (campaigns, attribution)\n```\n\nWhen a user asks about sales metrics, Claude only reads sales.md.\n\nSimilarly, for skills supporting multiple frameworks or variants, organize by variant:\n\n```\ncloud-deploy/\n SKILL.md (workflow + provider selection)\n references/\n     aws.md (AWS deployment patterns)\n     gcp.md (GCP deployment patterns)\n     azure.md (Azure deployment patterns)\n```\n\nWhen the user chooses AWS, Claude only reads aws.md.\n\n**Pattern 3: Conditional details**\n\nShow basic content, link to advanced content:\n\n```markdown\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n**Important guidelines:**\n\n- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.\n- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.\n\n## Skill Creation Process\n\nSkill creation involves these steps:\n\n1. Understand the skill with concrete examples\n2. Plan reusable skill contents (scripts, references, assets)\n3. Initialize the skill (run init_skill.py)\n4. Edit the skill (implement resources and write SKILL.md)\n5. Package the skill (run package_skill.py)\n6. Iterate based on real usage\n\nFollow these steps in order, skipping only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Learn Proven Design Patterns\n\nConsult these helpful guides based on your skill's needs:\n\n- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic\n- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns\n\nThese files contain established best practices for effective skill design.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAdded scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.\n\nAny example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Guidelines:** Always use imperative/infinitive form.\n\n##### Frontmatter\n\nWrite the YAML frontmatter with `name` and `description`:\n\n- `name`: The skill name\n- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.\n  - Include both what the Skill does and specific triggers/contexts for when to use it.\n  - Include all \"when to use\" information here - Not in the body. The body is only loaded after triggering, so \"When to Use This Skill\" sections in the body are not helpful to Claude.\n  - Example description for a `docx` skill: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\n\nDo not include any other fields in YAML frontmatter.\n\n##### Body\n\nWrite instructions for using the skill and its bundled resources.\n\n### Step 5: Packaging a Skill\n\nOnce development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n",
        "skills/capability-architect/references/output-patterns.md": "# Output Patterns\n\nUse these patterns when skills need to produce consistent, high-quality output.\n\n## Template Pattern\n\nProvide templates for output format. Match the level of strictness to your needs.\n\n**For strict requirements (like API responses or data formats):**\n\n```markdown\n## Report structure\n\nALWAYS use this exact template structure:\n\n# [Analysis Title]\n\n## Executive summary\n[One-paragraph overview of key findings]\n\n## Key findings\n- Finding 1 with supporting data\n- Finding 2 with supporting data\n- Finding 3 with supporting data\n\n## Recommendations\n1. Specific actionable recommendation\n2. Specific actionable recommendation\n```\n\n**For flexible guidance (when adaptation is useful):**\n\n```markdown\n## Report structure\n\nHere is a sensible default format, but use your best judgment:\n\n# [Analysis Title]\n\n## Executive summary\n[Overview]\n\n## Key findings\n[Adapt sections based on what you discover]\n\n## Recommendations\n[Tailor to the specific context]\n\nAdjust sections as needed for the specific analysis type.\n```\n\n## Examples Pattern\n\nFor skills where output quality depends on seeing examples, provide input/output pairs:\n\n```markdown\n## Commit message format\n\nGenerate commit messages following these examples:\n\n**Example 1:**\nInput: Added user authentication with JWT tokens\nOutput:\n```\nfeat(auth): implement JWT-based authentication\n\nAdd login endpoint and token validation middleware\n```\n\n**Example 2:**\nInput: Fixed bug where dates displayed incorrectly in reports\nOutput:\n```\nfix(reports): correct date formatting in timezone conversion\n\nUse UTC timestamps consistently across report generation\n```\n\nFollow this style: type(scope): brief description, then detailed explanation.\n```\n\nExamples help Claude understand the desired style and level of detail more clearly than descriptions alone.\n",
        "skills/capability-architect/references/workflows.md": "# Workflow Patterns\n\n## Sequential Workflows\n\nFor complex tasks, break operations into clear, sequential steps. It is often helpful to give Claude an overview of the process towards the beginning of SKILL.md:\n\n```markdown\nFilling a PDF form involves these steps:\n\n1. Analyze the form (run analyze_form.py)\n2. Create field mapping (edit fields.json)\n3. Validate mapping (run validate_fields.py)\n4. Fill the form (run fill_form.py)\n5. Verify output (run verify_output.py)\n```\n\n## Conditional Workflows\n\nFor tasks with branching logic, guide Claude through decision points:\n\n```markdown\n1. Determine the modification type:\n   **Creating new content?**  Follow \"Creation workflow\" below\n   **Editing existing content?**  Follow \"Editing workflow\" below\n\n2. Creation workflow: [steps]\n3. Editing workflow: [steps]\n```",
        "skills/capability-assessment/SKILL.md": "---\nname: capability-assessment\ndescription: Assess and analyze developer skill growth and progression. Identifies capability gaps, learning paths, and mastery levels across technology domains.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Developer Growth Analysis\n\nThis skill provides personalized feedback on your recent coding work by analyzing your Claude Code chat interactions and identifying patterns that reveal strengths and areas for growth.\n\n## When to Use This Skill\n\nUse this skill when you want to:\n- Understand your development patterns and habits from recent work\n- Identify specific technical gaps or recurring challenges\n- Discover which topics would benefit from deeper study\n- Get curated learning resources tailored to your actual work patterns\n- Track improvement areas across your recent projects\n- Find high-quality articles that directly address the skills you're developing\n\nThis skill is ideal for developers who want structured feedback on their growth without waiting for code reviews, and who prefer data-driven insights from their own work history.\n\n## What This Skill Does\n\nThis skill performs a six-step analysis of your development work:\n\n1. **Reads Your Chat History**: Accesses your local Claude Code chat history from the past 24-48 hours to understand what you've been working on.\n\n2. **Identifies Development Patterns**: Analyzes the types of problems you're solving, technologies you're using, challenges you encounter, and how you approach different kinds of tasks.\n\n3. **Detects Improvement Areas**: Recognizes patterns that suggest skill gaps, repeated struggles, inefficient approaches, or areas where you might benefit from deeper knowledge.\n\n4. **Generates a Personalized Report**: Creates a comprehensive report showing your work summary, identified improvement areas, and specific recommendations for growth.\n\n5. **Finds Learning Resources**: Uses HackerNews to curate high-quality articles and discussions directly relevant to your improvement areas, providing you with a reading list tailored to your actual development work.\n\n6. **Sends to Your Slack DMs**: Automatically delivers the complete report to your own Slack direct messages so you can reference it anytime, anywhere.\n\n## How to Use\n\nAsk Claude to analyze your recent coding work:\n\n```\nAnalyze my developer growth from my recent chats\n```\n\nOr be more specific about which time period:\n\n```\nAnalyze my work from today and suggest areas for improvement\n```\n\nThe skill will generate a formatted report with:\n- Overview of your recent work\n- Key improvement areas identified\n- Specific recommendations for each area\n- Curated learning resources from HackerNews\n- Action items you can focus on\n\n## Instructions\n\nWhen a user requests analysis of their developer growth or coding patterns from recent work:\n\n1. **Access Chat History**\n\n   Read the chat history from `~/.claude/history.jsonl`. This file is a JSONL format where each line contains:\n   - `display`: The user's message/request\n   - `project`: The project being worked on\n   - `timestamp`: Unix timestamp (in milliseconds)\n   - `pastedContents`: Any code or content pasted\n\n   Filter for entries from the past 24-48 hours based on the current timestamp.\n\n2. **Analyze Work Patterns**\n\n   Extract and analyze the following from the filtered chats:\n   - **Projects and Domains**: What types of projects was the user working on? (e.g., backend, frontend, DevOps, data, etc.)\n   - **Technologies Used**: What languages, frameworks, and tools appear in the conversations?\n   - **Problem Types**: What categories of problems are being solved? (e.g., performance optimization, debugging, feature implementation, refactoring, setup/configuration)\n   - **Challenges Encountered**: What problems did the user struggle with? Look for:\n     - Repeated questions about similar topics\n     - Problems that took multiple attempts to solve\n     - Questions indicating knowledge gaps\n     - Complex architectural decisions\n   - **Approach Patterns**: How does the user solve problems? (e.g., methodical, exploratory, experimental)\n\n3. **Identify Improvement Areas**\n\n   Based on the analysis, identify 3-5 specific areas where the user could improve. These should be:\n   - **Specific** (not vague like \"improve coding skills\")\n   - **Evidence-based** (grounded in actual chat history)\n   - **Actionable** (practical improvements that can be made)\n   - **Prioritized** (most impactful first)\n\n   Examples of good improvement areas:\n   - \"Advanced TypeScript patterns (generics, utility types, type guards) - you struggled with type safety in [specific project]\"\n   - \"Error handling and validation - I noticed you patched several bugs related to missing null checks\"\n   - \"Async/await patterns - your recent work shows some race conditions and timing issues\"\n   - \"Database query optimization - you rewrote the same query multiple times\"\n\n4. **Generate Report**\n\n   Create a comprehensive report with this structure:\n\n   ```markdown\n   # Your Developer Growth Report\n\n   **Report Period**: [Yesterday / Today / [Custom Date Range]]\n   **Last Updated**: [Current Date and Time]\n\n   ## Work Summary\n\n   [2-3 paragraphs summarizing what the user worked on, projects touched, technologies used, and overall focus areas]\n\n   Example:\n   \"Over the past 24 hours, you focused primarily on backend development with three distinct projects. Your work involved TypeScript, React, and deployment infrastructure. You tackled a mix of feature implementation, debugging, and architectural decisions, with a particular focus on API design and database optimization.\"\n\n   ## Improvement Areas (Prioritized)\n\n   ### 1. [Area Name]\n\n   **Why This Matters**: [Explanation of why this skill is important for the user's work]\n\n   **What I Observed**: [Specific evidence from chat history showing this gap]\n\n   **Recommendation**: [Concrete step(s) to improve in this area]\n\n   **Time to Skill Up**: [Brief estimate of effort required]\n\n   ---\n\n   [Repeat for 2-4 additional areas]\n\n   ## Strengths Observed\n\n   [2-3 bullet points highlighting things you're doing well - things to continue doing]\n\n   ## Action Items\n\n   Priority order:\n   1. [Action item derived from highest priority improvement area]\n   2. [Action item from next area]\n   3. [Action item from next area]\n\n   ## Learning Resources\n\n   [Will be populated in next step]\n   ```\n\n5. **Search for Learning Resources**\n\n   Use Rube MCP to search HackerNews for articles related to each improvement area:\n\n   - For each improvement area, construct a search query targeting high-quality resources\n   - Search HackerNews using RUBE_SEARCH_TOOLS with queries like:\n     - \"Learn [Technology/Pattern] best practices\"\n     - \"[Technology] advanced patterns and techniques\"\n     - \"Debugging [specific problem type] in [language]\"\n   - Prioritize posts with high engagement (comments, upvotes)\n   - For each area, include 2-3 most relevant articles with:\n     - Article title\n     - Publication date\n     - Brief description of why it's relevant\n     - Link to the article\n\n   Add this section to the report:\n\n   ```markdown\n   ## Curated Learning Resources\n\n   ### For: [Improvement Area]\n\n   1. **[Article Title]** - [Date]\n      [Description of what it covers and why it's relevant to your improvement area]\n      [Link]\n\n   2. **[Article Title]** - [Date]\n      [Description]\n      [Link]\n\n   [Repeat for other improvement areas]\n   ```\n\n6. **Present the Complete Report**\n\n   Deliver the report in a clean, readable format that the user can:\n   - Quickly scan for key takeaways\n   - Use for focused learning planning\n   - Reference over the next week as they work on improvements\n   - Share with mentors if they want external feedback\n\n7. **Send Report to Slack DMs**\n\n   Use Rube MCP to send the complete report to the user's own Slack DMs:\n\n   - Check if Slack connection is active via RUBE_SEARCH_TOOLS\n   - If not connected, use RUBE_MANAGE_CONNECTIONS to initiate Slack auth\n   - Use RUBE_MULTI_EXECUTE_TOOL to send the report as a formatted message:\n     - Send the report title and period as the first message\n     - Break the report into logical sections (Summary, Improvements, Strengths, Actions, Resources)\n     - Format each section as a well-structured Slack message with proper markdown\n     - Include clickable links for the learning resources\n   - Confirm delivery in the CLI output\n\n   This ensures the user has the report in a place they check regularly and can reference it throughout the week.\n\n## Example Usage\n\n### Input\n\n```\nAnalyze my developer growth from my recent chats\n```\n\n### Output\n\n```markdown\n# Your Developer Growth Report\n\n**Report Period**: November 9-10, 2024\n**Last Updated**: November 10, 2024, 9:15 PM UTC\n\n## Work Summary\n\nOver the past two days, you focused on backend infrastructure and API development. Your primary project was an open-source showcase application, where you made significant progress on connections management, UI improvements, and deployment configuration. You worked with TypeScript, React, and Node.js, tackling challenges ranging from data security to responsive design. Your work shows a balance between implementing features and addressing technical debt.\n\n## Improvement Areas (Prioritized)\n\n### 1. Advanced TypeScript Patterns and Type Safety\n\n**Why This Matters**: TypeScript is central to your work, but leveraging its advanced features (generics, utility types, conditional types, type guards) can significantly improve code reliability and reduce runtime errors. Better type safety catches bugs at compile time rather than in production.\n\n**What I Observed**: In your recent chats, you were working with connection data structures and struggled a few times with typing auth configurations properly. You also had to iterate on union types for different connection states. There's an opportunity to use discriminated unions and type guards more effectively.\n\n**Recommendation**: Study TypeScript's advanced type system, particularly utility types (Omit, Pick, Record), conditional types, and discriminated unions. Apply these patterns to your connection configuration handling and auth state management.\n\n**Time to Skill Up**: 5-8 hours of focused learning and practice\n\n### 2. Secure Data Handling and Information Hiding in UI\n\n**Why This Matters**: You identified and fixed a security concern where sensitive connection data was being displayed in your console. Preventing information leakage is critical for applications handling user credentials and API keys. Good practices here prevent security incidents and user trust violations.\n\n**What I Observed**: You caught that your \"Your Apps\" page was showing full connection data including auth configs. This shows good security instincts, and the next step is building this into your default thinking when handling sensitive information.\n\n**Recommendation**: Review security best practices for handling sensitive data in frontend applications. Create reusable patterns for filtering/masking sensitive information before displaying it. Consider implementing a secure data layer that explicitly whitelist what can be shown in the UI.\n\n**Time to Skill Up**: 3-4 hours\n\n### 3. Component Architecture and Responsive UI Patterns\n\n**Why This Matters**: You're designing UIs that need to work across different screen sizes and user interactions. Strong component architecture makes it easier to build complex UIs without bugs and improves maintainability.\n\n**What I Observed**: You worked on the \"Marketplace\" UI (formerly Browse Tools), recreating it from a design image. You also identified and fixed scrolling issues where content was overflowing containers. There's an opportunity to strengthen your understanding of layout containment and responsive design patterns.\n\n**Recommendation**: Study React component composition patterns and CSS layout best practices (especially flexbox and grid). Focus on container queries and responsive patterns that prevent overflow issues. Look into component composition libraries and design system approaches.\n\n**Time to Skill Up**: 6-10 hours (depending on depth)\n\n## Strengths Observed\n\n- **Security Awareness**: You proactively identified data leakage issues before they became problems\n- **Iterative Refinement**: You worked through UI requirements methodically, asking clarifying questions and improving designs\n- **Full-Stack Capability**: You comfortably work across backend APIs, frontend UI, and deployment concerns\n- **Problem-Solving Approach**: You break down complex tasks into manageable steps\n\n## Action Items\n\nPriority order:\n1. Spend 1-2 hours learning TypeScript utility types and discriminated unions; apply to your connection data structures\n2. Document security patterns for your project (what data is safe to display, filtering/masking functions)\n3. Study one article on advanced React patterns and apply one pattern to your current UI work\n4. Set up a code review checklist focused on type safety and data security for future PRs\n\n## Curated Learning Resources\n\n### For: Advanced TypeScript Patterns\n\n1. **TypeScript's Advanced Types: Generics, Utility Types, and Conditional Types** - HackerNews, October 2024\n   Deep dive into TypeScript's type system with practical examples and real-world applications. Covers discriminated unions, type guards, and patterns for ensuring compile-time safety in complex applications.\n   [Link to discussion]\n\n2. **Building Type-Safe APIs in TypeScript** - HackerNews, September 2024\n   Practical guide to designing APIs with TypeScript that catch errors early. Particularly relevant for your connection configuration work.\n   [Link to discussion]\n\n### For: Secure Data Handling in Frontend\n\n1. **Preventing Information Leakage in Web Applications** - HackerNews, August 2024\n   Comprehensive guide to data security in frontend applications, including filtering sensitive information, secure logging, and audit trails.\n   [Link to discussion]\n\n2. **OAuth and API Key Management Best Practices** - HackerNews, July 2024\n   How to safely handle authentication tokens and API keys in applications, with examples for different frameworks.\n   [Link to discussion]\n\n### For: Component Architecture and Responsive Design\n\n1. **Advanced React Patterns: Composition Over Configuration** - HackerNews\n   Explores component composition strategies that scale, with examples using modern React patterns.\n   [Link to discussion]\n\n2. **CSS Layout Mastery: Flexbox, Grid, and Container Queries** - HackerNews, October 2024\n   Learn responsive design patterns that prevent overflow issues and work across all screen sizes.\n   [Link to discussion]\n```\n\n## Tips and Best Practices\n\n- Run this analysis once a week to track your improvement trajectory over time\n- Pick one improvement area at a time and focus on it for a few days before moving to the next\n- Use the learning resources as a study guide; work through the recommended materials and practice applying the patterns\n- Revisit this report after focusing on an area for a week to see how your work patterns change\n- The learning resources are intentionally curated for your actual work, not generic topics, so they'll be highly relevant to what you're building\n\n## How Accuracy and Quality Are Maintained\n\nThis skill:\n- Analyzes your actual work patterns from timestamped chat history\n- Generates evidence-based recommendations grounded in real projects\n- Curates learning resources that directly address your identified gaps\n- Focuses on actionable improvements, not vague feedback\n- Provides specific time estimates based on complexity\n- Prioritizes areas that will have the most impact on your development velocity\n",
        "skills/capability-documentation/SKILL.md": "---\nname: capability-documentation\ndescription: Document capabilities and skills comprehensively. Creates skill documentation with examples, specifications, and usage patterns.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Writing Skills\n\n## Overview\n\n**Writing skills IS Test-Driven Development applied to process documentation.**\n\n**Personal skills live in agent-specific directories (`~/.claude/skills` for Claude Code, `~/.codex/skills` for Codex)** \n\nYou write test cases (pressure scenarios with subagents), watch them fail (baseline behavior), write the skill (documentation), watch tests pass (agents comply), and refactor (close loopholes).\n\n**Core principle:** If you didn't watch an agent fail without the skill, you don't know if the skill teaches the right thing.\n\n**REQUIRED BACKGROUND:** You MUST understand superpowers:test-driven-development before using this skill. That skill defines the fundamental RED-GREEN-REFACTOR cycle. This skill adapts TDD to documentation.\n\n**Official guidance:** For Anthropic's official skill authoring best practices, see anthropic-best-practices.md. This document provides additional patterns and guidelines that complement the TDD-focused approach in this skill.\n\n## What is a Skill?\n\nA **skill** is a reference guide for proven techniques, patterns, or tools. Skills help future Claude instances find and apply effective approaches.\n\n**Skills are:** Reusable techniques, patterns, tools, reference guides\n\n**Skills are NOT:** Narratives about how you solved a problem once\n\n## TDD Mapping for Skills\n\n| TDD Concept | Skill Creation |\n|-------------|----------------|\n| **Test case** | Pressure scenario with subagent |\n| **Production code** | Skill document (SKILL.md) |\n| **Test fails (RED)** | Agent violates rule without skill (baseline) |\n| **Test passes (GREEN)** | Agent complies with skill present |\n| **Refactor** | Close loopholes while maintaining compliance |\n| **Write test first** | Run baseline scenario BEFORE writing skill |\n| **Watch it fail** | Document exact rationalizations agent uses |\n| **Minimal code** | Write skill addressing those specific violations |\n| **Watch it pass** | Verify agent now complies |\n| **Refactor cycle** | Find new rationalizations  plug  re-verify |\n\nThe entire skill creation process follows RED-GREEN-REFACTOR.\n\n## When to Create a Skill\n\n**Create when:**\n- Technique wasn't intuitively obvious to you\n- You'd reference this again across projects\n- Pattern applies broadly (not project-specific)\n- Others would benefit\n\n**Don't create for:**\n- One-off solutions\n- Standard practices well-documented elsewhere\n- Project-specific conventions (put in CLAUDE.md)\n\n## Skill Types\n\n### Technique\nConcrete method with steps to follow (condition-based-waiting, root-cause-tracing)\n\n### Pattern\nWay of thinking about problems (flatten-with-flags, test-invariants)\n\n### Reference\nAPI docs, syntax guides, tool documentation (office docs)\n\n## Directory Structure\n\n\n```\nskills/\n  skill-name/\n    SKILL.md              # Main reference (required)\n    supporting-file.*     # Only if needed\n```\n\n**Flat namespace** - all skills in one searchable namespace\n\n**Separate files for:**\n1. **Heavy reference** (100+ lines) - API docs, comprehensive syntax\n2. **Reusable tools** - Scripts, utilities, templates\n\n**Keep inline:**\n- Principles and concepts\n- Code patterns (< 50 lines)\n- Everything else\n\n## SKILL.md Structure\n\n**Frontmatter (YAML):**\n- Only two fields supported: `name` and `description`\n- Max 1024 characters total\n- `name`: Use letters, numbers, and hyphens only (no parentheses, special chars)\n- `description`: Third-person, describes ONLY when to use (NOT what it does)\n  - Start with \"Use when...\" to focus on triggering conditions\n  - Include specific symptoms, situations, and contexts\n  - **NEVER summarize the skill's process or workflow** (see CSO section for why)\n  - Keep under 500 characters if possible\n\n```markdown\n---\nname: Skill-Name-With-Hyphens\ndescription: Use when [specific triggering conditions and symptoms]\n---\n\n# Skill Name\n\n## Overview\nWhat is this? Core principle in 1-2 sentences.\n\n## When to Use\n[Small inline flowchart IF decision non-obvious]\n\nBullet list with SYMPTOMS and use cases\nWhen NOT to use\n\n## Core Pattern (for techniques/patterns)\nBefore/after code comparison\n\n## Quick Reference\nTable or bullets for scanning common operations\n\n## Implementation\nInline code for simple patterns\nLink to file for heavy reference or reusable tools\n\n## Common Mistakes\nWhat goes wrong + fixes\n\n## Real-World Impact (optional)\nConcrete results\n```\n\n\n## Claude Search Optimization (CSO)\n\n**Critical for discovery:** Future Claude needs to FIND your skill\n\n### 1. Rich Description Field\n\n**Purpose:** Claude reads description to decide which skills to load for a given task. Make it answer: \"Should I read this skill right now?\"\n\n**Format:** Start with \"Use when...\" to focus on triggering conditions\n\n**CRITICAL: Description = When to Use, NOT What the Skill Does**\n\nThe description should ONLY describe triggering conditions. Do NOT summarize the skill's process or workflow in the description.\n\n**Why this matters:** Testing revealed that when a description summarizes the skill's workflow, Claude may follow the description instead of reading the full skill content. A description saying \"code review between tasks\" caused Claude to do ONE review, even though the skill's flowchart clearly showed TWO reviews (spec compliance then code quality).\n\nWhen the description was changed to just \"Use when executing implementation plans with independent tasks\" (no workflow summary), Claude correctly read the flowchart and followed the two-stage review process.\n\n**The trap:** Descriptions that summarize workflow create a shortcut Claude will take. The skill body becomes documentation Claude skips.\n\n```yaml\n#  BAD: Summarizes workflow - Claude may follow this instead of reading skill\ndescription: Use when executing plans - dispatches subagent per task with code review between tasks\n\n#  BAD: Too much process detail\ndescription: Use for TDD - write test first, watch it fail, write minimal code, refactor\n\n#  GOOD: Just triggering conditions, no workflow summary\ndescription: Use when executing implementation plans with independent tasks in the current session\n\n#  GOOD: Triggering conditions only\ndescription: Use when implementing any feature or bugfix, before writing implementation code\n```\n\n**Content:**\n- Use concrete triggers, symptoms, and situations that signal this skill applies\n- Describe the *problem* (race conditions, inconsistent behavior) not *language-specific symptoms* (setTimeout, sleep)\n- Keep triggers technology-agnostic unless the skill itself is technology-specific\n- If skill is technology-specific, make that explicit in the trigger\n- Write in third person (injected into system prompt)\n- **NEVER summarize the skill's process or workflow**\n\n```yaml\n#  BAD: Too abstract, vague, doesn't include when to use\ndescription: For async testing\n\n#  BAD: First person\ndescription: I can help you with async tests when they're flaky\n\n#  BAD: Mentions technology but skill isn't specific to it\ndescription: Use when tests use setTimeout/sleep and are flaky\n\n#  GOOD: Starts with \"Use when\", describes problem, no workflow\ndescription: Use when tests have race conditions, timing dependencies, or pass/fail inconsistently\n\n#  GOOD: Technology-specific skill with explicit trigger\ndescription: Use when using React Router and handling authentication redirects\n```\n\n### 2. Keyword Coverage\n\nUse words Claude would search for:\n- Error messages: \"Hook timed out\", \"ENOTEMPTY\", \"race condition\"\n- Symptoms: \"flaky\", \"hanging\", \"zombie\", \"pollution\"\n- Synonyms: \"timeout/hang/freeze\", \"cleanup/teardown/afterEach\"\n- Tools: Actual commands, library names, file types\n\n### 3. Descriptive Naming\n\n**Use active voice, verb-first:**\n-  `creating-skills` not `skill-creation`\n-  `condition-based-waiting` not `async-test-helpers`\n\n### 4. Token Efficiency (Critical)\n\n**Problem:** getting-started and frequently-referenced skills load into EVERY conversation. Every token counts.\n\n**Target word counts:**\n- getting-started workflows: <150 words each\n- Frequently-loaded skills: <200 words total\n- Other skills: <500 words (still be concise)\n\n**Techniques:**\n\n**Move details to tool help:**\n```bash\n#  BAD: Document all flags in SKILL.md\nsearch-conversations supports --text, --both, --after DATE, --before DATE, --limit N\n\n#  GOOD: Reference --help\nsearch-conversations supports multiple modes and filters. Run --help for details.\n```\n\n**Use cross-references:**\n```markdown\n#  BAD: Repeat workflow details\nWhen searching, dispatch subagent with template...\n[20 lines of repeated instructions]\n\n#  GOOD: Reference other skill\nAlways use subagents (50-100x context savings). REQUIRED: Use [other-skill-name] for workflow.\n```\n\n**Compress examples:**\n```markdown\n#  BAD: Verbose example (42 words)\nyour human partner: \"How did we handle authentication errors in React Router before?\"\nYou: I'll search past conversations for React Router authentication patterns.\n[Dispatch subagent with search query: \"React Router authentication error handling 401\"]\n\n#  GOOD: Minimal example (20 words)\nPartner: \"How did we handle auth errors in React Router?\"\nYou: Searching...\n[Dispatch subagent  synthesis]\n```\n\n**Eliminate redundancy:**\n- Don't repeat what's in cross-referenced skills\n- Don't explain what's obvious from command\n- Don't include multiple examples of same pattern\n\n**Verification:**\n```bash\nwc -w skills/path/SKILL.md\n# getting-started workflows: aim for <150 each\n# Other frequently-loaded: aim for <200 total\n```\n\n**Name by what you DO or core insight:**\n-  `condition-based-waiting` > `async-test-helpers`\n-  `using-skills` not `skill-usage`\n-  `flatten-with-flags` > `data-structure-refactoring`\n-  `root-cause-tracing` > `debugging-techniques`\n\n**Gerunds (-ing) work well for processes:**\n- `creating-skills`, `testing-skills`, `debugging-with-logs`\n- Active, describes the action you're taking\n\n### 4. Cross-Referencing Other Skills\n\n**When writing documentation that references other skills:**\n\nUse skill name only, with explicit requirement markers:\n-  Good: `**REQUIRED SUB-SKILL:** Use superpowers:test-driven-development`\n-  Good: `**REQUIRED BACKGROUND:** You MUST understand superpowers:systematic-debugging`\n-  Bad: `See skills/testing/test-driven-development` (unclear if required)\n-  Bad: `@skills/testing/test-driven-development/SKILL.md` (force-loads, burns context)\n\n**Why no @ links:** `@` syntax force-loads files immediately, consuming 200k+ context before you need them.\n\n## Flowchart Usage\n\n```dot\ndigraph when_flowchart {\n    \"Need to show information?\" [shape=diamond];\n    \"Decision where I might go wrong?\" [shape=diamond];\n    \"Use markdown\" [shape=box];\n    \"Small inline flowchart\" [shape=box];\n\n    \"Need to show information?\" -> \"Decision where I might go wrong?\" [label=\"yes\"];\n    \"Decision where I might go wrong?\" -> \"Small inline flowchart\" [label=\"yes\"];\n    \"Decision where I might go wrong?\" -> \"Use markdown\" [label=\"no\"];\n}\n```\n\n**Use flowcharts ONLY for:**\n- Non-obvious decision points\n- Process loops where you might stop too early\n- \"When to use A vs B\" decisions\n\n**Never use flowcharts for:**\n- Reference material  Tables, lists\n- Code examples  Markdown blocks\n- Linear instructions  Numbered lists\n- Labels without semantic meaning (step1, helper2)\n\nSee @graphviz-conventions.dot for graphviz style rules.\n\n**Visualizing for your human partner:** Use `render-graphs.js` in this directory to render a skill's flowcharts to SVG:\n```bash\n./render-graphs.js ../some-skill           # Each diagram separately\n./render-graphs.js ../some-skill --combine # All diagrams in one SVG\n```\n\n## Code Examples\n\n**One excellent example beats many mediocre ones**\n\nChoose most relevant language:\n- Testing techniques  TypeScript/JavaScript\n- System debugging  Shell/Python\n- Data processing  Python\n\n**Good example:**\n- Complete and runnable\n- Well-commented explaining WHY\n- From real scenario\n- Shows pattern clearly\n- Ready to adapt (not generic template)\n\n**Don't:**\n- Implement in 5+ languages\n- Create fill-in-the-blank templates\n- Write contrived examples\n\nYou're good at porting - one great example is enough.\n\n## File Organization\n\n### Self-Contained Skill\n```\ndefense-in-depth/\n  SKILL.md    # Everything inline\n```\nWhen: All content fits, no heavy reference needed\n\n### Skill with Reusable Tool\n```\ncondition-based-waiting/\n  SKILL.md    # Overview + patterns\n  example.ts  # Working helpers to adapt\n```\nWhen: Tool is reusable code, not just narrative\n\n### Skill with Heavy Reference\n```\npptx/\n  SKILL.md       # Overview + workflows\n  pptxgenjs.md   # 600 lines API reference\n  ooxml.md       # 500 lines XML structure\n  scripts/       # Executable tools\n```\nWhen: Reference material too large for inline\n\n## The Iron Law (Same as TDD)\n\n```\nNO SKILL WITHOUT A FAILING TEST FIRST\n```\n\nThis applies to NEW skills AND EDITS to existing skills.\n\nWrite skill before testing? Delete it. Start over.\nEdit skill without testing? Same violation.\n\n**No exceptions:**\n- Not for \"simple additions\"\n- Not for \"just adding a section\"\n- Not for \"documentation updates\"\n- Don't keep untested changes as \"reference\"\n- Don't \"adapt\" while running tests\n- Delete means delete\n\n**REQUIRED BACKGROUND:** The superpowers:test-driven-development skill explains why this matters. Same principles apply to documentation.\n\n## Testing All Skill Types\n\nDifferent skill types need different test approaches:\n\n### Discipline-Enforcing Skills (rules/requirements)\n\n**Examples:** TDD, verification-before-completion, designing-before-coding\n\n**Test with:**\n- Academic questions: Do they understand the rules?\n- Pressure scenarios: Do they comply under stress?\n- Multiple pressures combined: time + sunk cost + exhaustion\n- Identify rationalizations and add explicit counters\n\n**Success criteria:** Agent follows rule under maximum pressure\n\n### Technique Skills (how-to guides)\n\n**Examples:** condition-based-waiting, root-cause-tracing, defensive-programming\n\n**Test with:**\n- Application scenarios: Can they apply the technique correctly?\n- Variation scenarios: Do they handle edge cases?\n- Missing information tests: Do instructions have gaps?\n\n**Success criteria:** Agent successfully applies technique to new scenario\n\n### Pattern Skills (mental models)\n\n**Examples:** reducing-complexity, information-hiding concepts\n\n**Test with:**\n- Recognition scenarios: Do they recognize when pattern applies?\n- Application scenarios: Can they use the mental model?\n- Counter-examples: Do they know when NOT to apply?\n\n**Success criteria:** Agent correctly identifies when/how to apply pattern\n\n### Reference Skills (documentation/APIs)\n\n**Examples:** API documentation, command references, library guides\n\n**Test with:**\n- Retrieval scenarios: Can they find the right information?\n- Application scenarios: Can they use what they found correctly?\n- Gap testing: Are common use cases covered?\n\n**Success criteria:** Agent finds and correctly applies reference information\n\n## Common Rationalizations for Skipping Testing\n\n| Excuse | Reality |\n|--------|---------|\n| \"Skill is obviously clear\" | Clear to you  clear to other agents. Test it. |\n| \"It's just a reference\" | References can have gaps, unclear sections. Test retrieval. |\n| \"Testing is overkill\" | Untested skills have issues. Always. 15 min testing saves hours. |\n| \"I'll test if problems emerge\" | Problems = agents can't use skill. Test BEFORE deploying. |\n| \"Too tedious to test\" | Testing is less tedious than debugging bad skill in production. |\n| \"I'm confident it's good\" | Overconfidence guarantees issues. Test anyway. |\n| \"Academic review is enough\" | Reading  using. Test application scenarios. |\n| \"No time to test\" | Deploying untested skill wastes more time fixing it later. |\n\n**All of these mean: Test before deploying. No exceptions.**\n\n## Bulletproofing Skills Against Rationalization\n\nSkills that enforce discipline (like TDD) need to resist rationalization. Agents are smart and will find loopholes when under pressure.\n\n**Psychology note:** Understanding WHY persuasion techniques work helps you apply them systematically. See persuasion-principles.md for research foundation (Cialdini, 2021; Meincke et al., 2025) on authority, commitment, scarcity, social proof, and unity principles.\n\n### Close Every Loophole Explicitly\n\nDon't just state the rule - forbid specific workarounds:\n\n<Bad>\n```markdown\nWrite code before test? Delete it.\n```\n</Bad>\n\n<Good>\n```markdown\nWrite code before test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n```\n</Good>\n\n### Address \"Spirit vs Letter\" Arguments\n\nAdd foundational principle early:\n\n```markdown\n**Violating the letter of the rules is violating the spirit of the rules.**\n```\n\nThis cuts off entire class of \"I'm following the spirit\" rationalizations.\n\n### Build Rationalization Table\n\nCapture rationalizations from baseline testing (see Testing section below). Every excuse agents make goes in the table:\n\n```markdown\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\n```\n\n### Create Red Flags List\n\nMake it easy for agents to self-check when rationalizing:\n\n```markdown\n## Red Flags - STOP and Start Over\n\n- Code before test\n- \"I already manually tested it\"\n- \"Tests after achieve the same purpose\"\n- \"It's about spirit not ritual\"\n- \"This is different because...\"\n\n**All of these mean: Delete code. Start over with TDD.**\n```\n\n### Update CSO for Violation Symptoms\n\nAdd to description: symptoms of when you're ABOUT to violate the rule:\n\n```yaml\ndescription: use when implementing any feature or bugfix, before writing implementation code\n```\n\n## RED-GREEN-REFACTOR for Skills\n\nFollow the TDD cycle:\n\n### RED: Write Failing Test (Baseline)\n\nRun pressure scenario with subagent WITHOUT the skill. Document exact behavior:\n- What choices did they make?\n- What rationalizations did they use (verbatim)?\n- Which pressures triggered violations?\n\nThis is \"watch the test fail\" - you must see what agents naturally do before writing the skill.\n\n### GREEN: Write Minimal Skill\n\nWrite skill that addresses those specific rationalizations. Don't add extra content for hypothetical cases.\n\nRun same scenarios WITH skill. Agent should now comply.\n\n### REFACTOR: Close Loopholes\n\nAgent found new rationalization? Add explicit counter. Re-test until bulletproof.\n\n**Testing methodology:** See @testing-skills-with-subagents.md for the complete testing methodology:\n- How to write pressure scenarios\n- Pressure types (time, sunk cost, authority, exhaustion)\n- Plugging holes systematically\n- Meta-testing techniques\n\n## Anti-Patterns\n\n###  Narrative Example\n\"In session 2025-10-03, we found empty projectDir caused...\"\n**Why bad:** Too specific, not reusable\n\n###  Multi-Language Dilution\nexample-js.js, example-py.py, example-go.go\n**Why bad:** Mediocre quality, maintenance burden\n\n###  Code in Flowcharts\n```dot\nstep1 [label=\"import fs\"];\nstep2 [label=\"read file\"];\n```\n**Why bad:** Can't copy-paste, hard to read\n\n###  Generic Labels\nhelper1, helper2, step3, pattern4\n**Why bad:** Labels should have semantic meaning\n\n## STOP: Before Moving to Next Skill\n\n**After writing ANY skill, you MUST STOP and complete the deployment process.**\n\n**Do NOT:**\n- Create multiple skills in batch without testing each\n- Move to next skill before current one is verified\n- Skip testing because \"batching is more efficient\"\n\n**The deployment checklist below is MANDATORY for EACH skill.**\n\nDeploying untested skills = deploying untested code. It's a violation of quality standards.\n\n## Skill Creation Checklist (TDD Adapted)\n\n**IMPORTANT: Use TodoWrite to create todos for EACH checklist item below.**\n\n**RED Phase - Write Failing Test:**\n- [ ] Create pressure scenarios (3+ combined pressures for discipline skills)\n- [ ] Run scenarios WITHOUT skill - document baseline behavior verbatim\n- [ ] Identify patterns in rationalizations/failures\n\n**GREEN Phase - Write Minimal Skill:**\n- [ ] Name uses only letters, numbers, hyphens (no parentheses/special chars)\n- [ ] YAML frontmatter with only name and description (max 1024 chars)\n- [ ] Description starts with \"Use when...\" and includes specific triggers/symptoms\n- [ ] Description written in third person\n- [ ] Keywords throughout for search (errors, symptoms, tools)\n- [ ] Clear overview with core principle\n- [ ] Address specific baseline failures identified in RED\n- [ ] Code inline OR link to separate file\n- [ ] One excellent example (not multi-language)\n- [ ] Run scenarios WITH skill - verify agents now comply\n\n**REFACTOR Phase - Close Loopholes:**\n- [ ] Identify NEW rationalizations from testing\n- [ ] Add explicit counters (if discipline skill)\n- [ ] Build rationalization table from all test iterations\n- [ ] Create red flags list\n- [ ] Re-test until bulletproof\n\n**Quality Checks:**\n- [ ] Small flowchart only if decision non-obvious\n- [ ] Quick reference table\n- [ ] Common mistakes section\n- [ ] No narrative storytelling\n- [ ] Supporting files only for tools or heavy reference\n\n**Deployment:**\n- [ ] Commit skill to git and push to your fork (if configured)\n- [ ] Consider contributing back via PR (if broadly useful)\n\n## Discovery Workflow\n\nHow future Claude finds your skill:\n\n1. **Encounters problem** (\"tests are flaky\")\n3. **Finds SKILL** (description matches)\n4. **Scans overview** (is this relevant?)\n5. **Reads patterns** (quick reference table)\n6. **Loads example** (only when implementing)\n\n**Optimize for this flow** - put searchable terms early and often.\n\n## The Bottom Line\n\n**Creating skills IS TDD for process documentation.**\n\nSame Iron Law: No skill without failing test first.\nSame cycle: RED (baseline)  GREEN (write skill)  REFACTOR (close loopholes).\nSame benefits: Better quality, fewer surprises, bulletproof results.\n\nIf you follow TDD for code, follow it for skills. It's the same discipline applied to documentation.\n",
        "skills/capability-documentation/anthropic-best-practices.md": "# Skill authoring best practices\n\n> Learn how to write effective Skills that Claude can discover and use successfully.\n\nGood Skills are concise, well-structured, and tested with real usage. This guide provides practical authoring decisions to help you write Skills that Claude can discover and use effectively.\n\nFor conceptual background on how Skills work, see the [Skills overview](/en/docs/agents-and-tools/agent-skills/overview).\n\n## Core principles\n\n### Concise is key\n\nThe [context window](https://platform.claude.com/docs/en/build-with-claude/context-windows) is a public good. Your Skill shares the context window with everything else Claude needs to know, including:\n\n* The system prompt\n* Conversation history\n* Other Skills' metadata\n* Your actual request\n\nNot every token in your Skill has an immediate cost. At startup, only the metadata (name and description) from all Skills is pre-loaded. Claude reads SKILL.md only when the Skill becomes relevant, and reads additional files only as needed. However, being concise in SKILL.md still matters: once Claude loads it, every token competes with conversation history and other context.\n\n**Default assumption**: Claude is already very smart\n\nOnly add context Claude doesn't already have. Challenge each piece of information:\n\n* \"Does Claude really need this explanation?\"\n* \"Can I assume Claude knows this?\"\n* \"Does this paragraph justify its token cost?\"\n\n**Good example: Concise** (approximately 50 tokens):\n\n````markdown  theme={null}\n## Extract PDF text\n\nUse pdfplumber for text extraction:\n\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"file.pdf\") as pdf:\n    text = pdf.pages[0].extract_text()\n```\n````\n\n**Bad example: Too verbose** (approximately 150 tokens):\n\n```markdown  theme={null}\n## Extract PDF text\n\nPDF (Portable Document Format) files are a common file format that contains\ntext, images, and other content. To extract text from a PDF, you'll need to\nuse a library. There are many libraries available for PDF processing, but we\nrecommend pdfplumber because it's easy to use and handles most cases well.\nFirst, you'll need to install it using pip. Then you can use the code below...\n```\n\nThe concise version assumes Claude knows what PDFs are and how libraries work.\n\n### Set appropriate degrees of freedom\n\nMatch the level of specificity to the task's fragility and variability.\n\n**High freedom** (text-based instructions):\n\nUse when:\n\n* Multiple approaches are valid\n* Decisions depend on context\n* Heuristics guide the approach\n\nExample:\n\n```markdown  theme={null}\n## Code review process\n\n1. Analyze the code structure and organization\n2. Check for potential bugs or edge cases\n3. Suggest improvements for readability and maintainability\n4. Verify adherence to project conventions\n```\n\n**Medium freedom** (pseudocode or scripts with parameters):\n\nUse when:\n\n* A preferred pattern exists\n* Some variation is acceptable\n* Configuration affects behavior\n\nExample:\n\n````markdown  theme={null}\n## Generate report\n\nUse this template and customize as needed:\n\n```python\ndef generate_report(data, format=\"markdown\", include_charts=True):\n    # Process data\n    # Generate output in specified format\n    # Optionally include visualizations\n```\n````\n\n**Low freedom** (specific scripts, few or no parameters):\n\nUse when:\n\n* Operations are fragile and error-prone\n* Consistency is critical\n* A specific sequence must be followed\n\nExample:\n\n````markdown  theme={null}\n## Database migration\n\nRun exactly this script:\n\n```bash\npython scripts/migrate.py --verify --backup\n```\n\nDo not modify the command or add additional flags.\n````\n\n**Analogy**: Think of Claude as a robot exploring a path:\n\n* **Narrow bridge with cliffs on both sides**: There's only one safe way forward. Provide specific guardrails and exact instructions (low freedom). Example: database migrations that must run in exact sequence.\n* **Open field with no hazards**: Many paths lead to success. Give general direction and trust Claude to find the best route (high freedom). Example: code reviews where context determines the best approach.\n\n### Test with all models you plan to use\n\nSkills act as additions to models, so effectiveness depends on the underlying model. Test your Skill with all the models you plan to use it with.\n\n**Testing considerations by model**:\n\n* **Claude Haiku** (fast, economical): Does the Skill provide enough guidance?\n* **Claude Sonnet** (balanced): Is the Skill clear and efficient?\n* **Claude Opus** (powerful reasoning): Does the Skill avoid over-explaining?\n\nWhat works perfectly for Opus might need more detail for Haiku. If you plan to use your Skill across multiple models, aim for instructions that work well with all of them.\n\n## Skill structure\n\n<Note>\n  **YAML Frontmatter**: The SKILL.md frontmatter supports two fields:\n\n  * `name` - Human-readable name of the Skill (64 characters maximum)\n  * `description` - One-line description of what the Skill does and when to use it (1024 characters maximum)\n\n  For complete Skill structure details, see the [Skills overview](/en/docs/agents-and-tools/agent-skills/overview#skill-structure).\n</Note>\n\n### Naming conventions\n\nUse consistent naming patterns to make Skills easier to reference and discuss. We recommend using **gerund form** (verb + -ing) for Skill names, as this clearly describes the activity or capability the Skill provides.\n\n**Good naming examples (gerund form)**:\n\n* \"Processing PDFs\"\n* \"Analyzing spreadsheets\"\n* \"Managing databases\"\n* \"Testing code\"\n* \"Writing documentation\"\n\n**Acceptable alternatives**:\n\n* Noun phrases: \"PDF Processing\", \"Spreadsheet Analysis\"\n* Action-oriented: \"Process PDFs\", \"Analyze Spreadsheets\"\n\n**Avoid**:\n\n* Vague names: \"Helper\", \"Utils\", \"Tools\"\n* Overly generic: \"Documents\", \"Data\", \"Files\"\n* Inconsistent patterns within your skill collection\n\nConsistent naming makes it easier to:\n\n* Reference Skills in documentation and conversations\n* Understand what a Skill does at a glance\n* Organize and search through multiple Skills\n* Maintain a professional, cohesive skill library\n\n### Writing effective descriptions\n\nThe `description` field enables Skill discovery and should include both what the Skill does and when to use it.\n\n<Warning>\n  **Always write in third person**. The description is injected into the system prompt, and inconsistent point-of-view can cause discovery problems.\n\n  * **Good:** \"Processes Excel files and generates reports\"\n  * **Avoid:** \"I can help you process Excel files\"\n  * **Avoid:** \"You can use this to process Excel files\"\n</Warning>\n\n**Be specific and include key terms**. Include both what the Skill does and specific triggers/contexts for when to use it.\n\nEach Skill has exactly one description field. The description is critical for skill selection: Claude uses it to choose the right Skill from potentially 100+ available Skills. Your description must provide enough detail for Claude to know when to select this Skill, while the rest of SKILL.md provides the implementation details.\n\nEffective examples:\n\n**PDF Processing skill:**\n\n```yaml  theme={null}\ndescription: Extract text and tables from PDF files, fill forms, merge documents. Use when working with PDF files or when the user mentions PDFs, forms, or document extraction.\n```\n\n**Excel Analysis skill:**\n\n```yaml  theme={null}\ndescription: Analyze Excel spreadsheets, create pivot tables, generate charts. Use when analyzing Excel files, spreadsheets, tabular data, or .xlsx files.\n```\n\n**Git Commit Helper skill:**\n\n```yaml  theme={null}\ndescription: Generate descriptive commit messages by analyzing git diffs. Use when the user asks for help writing commit messages or reviewing staged changes.\n```\n\nAvoid vague descriptions like these:\n\n```yaml  theme={null}\ndescription: Helps with documents\n```\n\n```yaml  theme={null}\ndescription: Processes data\n```\n\n```yaml  theme={null}\ndescription: Does stuff with files\n```\n\n### Progressive disclosure patterns\n\nSKILL.md serves as an overview that points Claude to detailed materials as needed, like a table of contents in an onboarding guide. For an explanation of how progressive disclosure works, see [How Skills work](/en/docs/agents-and-tools/agent-skills/overview#how-skills-work) in the overview.\n\n**Practical guidance:**\n\n* Keep SKILL.md body under 500 lines for optimal performance\n* Split content into separate files when approaching this limit\n* Use the patterns below to organize instructions, code, and resources effectively\n\n#### Visual overview: From simple to complex\n\nA basic Skill starts with just a SKILL.md file containing metadata and instructions:\n\n<img src=\"https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-simple-file.png?fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=87782ff239b297d9a9e8e1b72ed72db9\" alt=\"Simple SKILL.md file showing YAML frontmatter and markdown body\" data-og-width=\"2048\" width=\"2048\" data-og-height=\"1153\" height=\"1153\" data-path=\"images/agent-skills-simple-file.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-simple-file.png?w=280&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=c61cc33b6f5855809907f7fda94cd80e 280w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-simple-file.png?w=560&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=90d2c0c1c76b36e8d485f49e0810dbfd 560w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-simple-file.png?w=840&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=ad17d231ac7b0bea7e5b4d58fb4aeabb 840w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-simple-file.png?w=1100&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=f5d0a7a3c668435bb0aee9a3a8f8c329 1100w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-simple-file.png?w=1650&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=0e927c1af9de5799cfe557d12249f6e6 1650w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-simple-file.png?w=2500&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=46bbb1a51dd4c8202a470ac8c80a893d 2500w\" />\n\nAs your Skill grows, you can bundle additional content that Claude loads only when needed:\n\n<img src=\"https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-bundling-content.png?fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=a5e0aa41e3d53985a7e3e43668a33ea3\" alt=\"Bundling additional reference files like reference.md and forms.md.\" data-og-width=\"2048\" width=\"2048\" data-og-height=\"1327\" height=\"1327\" data-path=\"images/agent-skills-bundling-content.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-bundling-content.png?w=280&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=f8a0e73783e99b4a643d79eac86b70a2 280w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-bundling-content.png?w=560&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=dc510a2a9d3f14359416b706f067904a 560w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-bundling-content.png?w=840&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=82cd6286c966303f7dd914c28170e385 840w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-bundling-content.png?w=1100&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=56f3be36c77e4fe4b523df209a6824c6 1100w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-bundling-content.png?w=1650&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=d22b5161b2075656417d56f41a74f3dd 1650w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-bundling-content.png?w=2500&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=3dd4bdd6850ffcc96c6c45fcb0acd6eb 2500w\" />\n\nThe complete Skill directory structure might look like this:\n\n```\npdf/\n SKILL.md              # Main instructions (loaded when triggered)\n FORMS.md              # Form-filling guide (loaded as needed)\n reference.md          # API reference (loaded as needed)\n examples.md           # Usage examples (loaded as needed)\n scripts/\n     analyze_form.py   # Utility script (executed, not loaded)\n     fill_form.py      # Form filling script\n     validate.py       # Validation script\n```\n\n#### Pattern 1: High-level guide with references\n\n````markdown  theme={null}\n---\nname: PDF Processing\ndescription: Extracts text and tables from PDF files, fills forms, and merges documents. Use when working with PDF files or when the user mentions PDFs, forms, or document extraction.\n---\n\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n```python\nimport pdfplumber\nwith pdfplumber.open(\"file.pdf\") as pdf:\n    text = pdf.pages[0].extract_text()\n```\n\n## Advanced features\n\n**Form filling**: See [FORMS.md](FORMS.md) for complete guide\n**API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n**Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n````\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n#### Pattern 2: Domain-specific organization\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context. When a user asks about sales metrics, Claude only needs to read sales-related schemas, not finance or marketing data. This keeps token usage low and context focused.\n\n```\nbigquery-skill/\n SKILL.md (overview and navigation)\n reference/\n     finance.md (revenue, billing metrics)\n     sales.md (opportunities, pipeline)\n     product.md (API usage, features)\n     marketing.md (campaigns, attribution)\n```\n\n````markdown SKILL.md theme={null}\n# BigQuery Data Analysis\n\n## Available datasets\n\n**Finance**: Revenue, ARR, billing  See [reference/finance.md](reference/finance.md)\n**Sales**: Opportunities, pipeline, accounts  See [reference/sales.md](reference/sales.md)\n**Product**: API usage, features, adoption  See [reference/product.md](reference/product.md)\n**Marketing**: Campaigns, attribution, email  See [reference/marketing.md](reference/marketing.md)\n\n## Quick search\n\nFind specific metrics using grep:\n\n```bash\ngrep -i \"revenue\" reference/finance.md\ngrep -i \"pipeline\" reference/sales.md\ngrep -i \"api usage\" reference/product.md\n```\n````\n\n#### Pattern 3: Conditional details\n\nShow basic content, link to advanced content:\n\n```markdown  theme={null}\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n### Avoid deeply nested references\n\nClaude may partially read files when they're referenced from other referenced files. When encountering nested references, Claude might use commands like `head -100` to preview content rather than reading entire files, resulting in incomplete information.\n\n**Keep references one level deep from SKILL.md**. All reference files should link directly from SKILL.md to ensure Claude reads complete files when needed.\n\n**Bad example: Too deep**:\n\n```markdown  theme={null}\n# SKILL.md\nSee [advanced.md](advanced.md)...\n\n# advanced.md\nSee [details.md](details.md)...\n\n# details.md\nHere's the actual information...\n```\n\n**Good example: One level deep**:\n\n```markdown  theme={null}\n# SKILL.md\n\n**Basic usage**: [instructions in SKILL.md]\n**Advanced features**: See [advanced.md](advanced.md)\n**API reference**: See [reference.md](reference.md)\n**Examples**: See [examples.md](examples.md)\n```\n\n### Structure longer reference files with table of contents\n\nFor reference files longer than 100 lines, include a table of contents at the top. This ensures Claude can see the full scope of available information even when previewing with partial reads.\n\n**Example**:\n\n```markdown  theme={null}\n# API Reference\n\n## Contents\n- Authentication and setup\n- Core methods (create, read, update, delete)\n- Advanced features (batch operations, webhooks)\n- Error handling patterns\n- Code examples\n\n## Authentication and setup\n...\n\n## Core methods\n...\n```\n\nClaude can then read the complete file or jump to specific sections as needed.\n\nFor details on how this filesystem-based architecture enables progressive disclosure, see the [Runtime environment](#runtime-environment) section in the Advanced section below.\n\n## Workflows and feedback loops\n\n### Use workflows for complex tasks\n\nBreak complex operations into clear, sequential steps. For particularly complex workflows, provide a checklist that Claude can copy into its response and check off as it progresses.\n\n**Example 1: Research synthesis workflow** (for Skills without code):\n\n````markdown  theme={null}\n## Research synthesis workflow\n\nCopy this checklist and track your progress:\n\n```\nResearch Progress:\n- [ ] Step 1: Read all source documents\n- [ ] Step 2: Identify key themes\n- [ ] Step 3: Cross-reference claims\n- [ ] Step 4: Create structured summary\n- [ ] Step 5: Verify citations\n```\n\n**Step 1: Read all source documents**\n\nReview each document in the `sources/` directory. Note the main arguments and supporting evidence.\n\n**Step 2: Identify key themes**\n\nLook for patterns across sources. What themes appear repeatedly? Where do sources agree or disagree?\n\n**Step 3: Cross-reference claims**\n\nFor each major claim, verify it appears in the source material. Note which source supports each point.\n\n**Step 4: Create structured summary**\n\nOrganize findings by theme. Include:\n- Main claim\n- Supporting evidence from sources\n- Conflicting viewpoints (if any)\n\n**Step 5: Verify citations**\n\nCheck that every claim references the correct source document. If citations are incomplete, return to Step 3.\n````\n\nThis example shows how workflows apply to analysis tasks that don't require code. The checklist pattern works for any complex, multi-step process.\n\n**Example 2: PDF form filling workflow** (for Skills with code):\n\n````markdown  theme={null}\n## PDF form filling workflow\n\nCopy this checklist and check off items as you complete them:\n\n```\nTask Progress:\n- [ ] Step 1: Analyze the form (run analyze_form.py)\n- [ ] Step 2: Create field mapping (edit fields.json)\n- [ ] Step 3: Validate mapping (run validate_fields.py)\n- [ ] Step 4: Fill the form (run fill_form.py)\n- [ ] Step 5: Verify output (run verify_output.py)\n```\n\n**Step 1: Analyze the form**\n\nRun: `python scripts/analyze_form.py input.pdf`\n\nThis extracts form fields and their locations, saving to `fields.json`.\n\n**Step 2: Create field mapping**\n\nEdit `fields.json` to add values for each field.\n\n**Step 3: Validate mapping**\n\nRun: `python scripts/validate_fields.py fields.json`\n\nFix any validation errors before continuing.\n\n**Step 4: Fill the form**\n\nRun: `python scripts/fill_form.py input.pdf fields.json output.pdf`\n\n**Step 5: Verify output**\n\nRun: `python scripts/verify_output.py output.pdf`\n\nIf verification fails, return to Step 2.\n````\n\nClear steps prevent Claude from skipping critical validation. The checklist helps both Claude and you track progress through multi-step workflows.\n\n### Implement feedback loops\n\n**Common pattern**: Run validator  fix errors  repeat\n\nThis pattern greatly improves output quality.\n\n**Example 1: Style guide compliance** (for Skills without code):\n\n```markdown  theme={null}\n## Content review process\n\n1. Draft your content following the guidelines in STYLE_GUIDE.md\n2. Review against the checklist:\n   - Check terminology consistency\n   - Verify examples follow the standard format\n   - Confirm all required sections are present\n3. If issues found:\n   - Note each issue with specific section reference\n   - Revise the content\n   - Review the checklist again\n4. Only proceed when all requirements are met\n5. Finalize and save the document\n```\n\nThis shows the validation loop pattern using reference documents instead of scripts. The \"validator\" is STYLE\\_GUIDE.md, and Claude performs the check by reading and comparing.\n\n**Example 2: Document editing process** (for Skills with code):\n\n```markdown  theme={null}\n## Document editing process\n\n1. Make your edits to `word/document.xml`\n2. **Validate immediately**: `python ooxml/scripts/validate.py unpacked_dir/`\n3. If validation fails:\n   - Review the error message carefully\n   - Fix the issues in the XML\n   - Run validation again\n4. **Only proceed when validation passes**\n5. Rebuild: `python ooxml/scripts/pack.py unpacked_dir/ output.docx`\n6. Test the output document\n```\n\nThe validation loop catches errors early.\n\n## Content guidelines\n\n### Avoid time-sensitive information\n\nDon't include information that will become outdated:\n\n**Bad example: Time-sensitive** (will become wrong):\n\n```markdown  theme={null}\nIf you're doing this before August 2025, use the old API.\nAfter August 2025, use the new API.\n```\n\n**Good example** (use \"old patterns\" section):\n\n```markdown  theme={null}\n## Current method\n\nUse the v2 API endpoint: `api.example.com/v2/messages`\n\n## Old patterns\n\n<details>\n<summary>Legacy v1 API (deprecated 2025-08)</summary>\n\nThe v1 API used: `api.example.com/v1/messages`\n\nThis endpoint is no longer supported.\n</details>\n```\n\nThe old patterns section provides historical context without cluttering the main content.\n\n### Use consistent terminology\n\nChoose one term and use it throughout the Skill:\n\n**Good - Consistent**:\n\n* Always \"API endpoint\"\n* Always \"field\"\n* Always \"extract\"\n\n**Bad - Inconsistent**:\n\n* Mix \"API endpoint\", \"URL\", \"API route\", \"path\"\n* Mix \"field\", \"box\", \"element\", \"control\"\n* Mix \"extract\", \"pull\", \"get\", \"retrieve\"\n\nConsistency helps Claude understand and follow instructions.\n\n## Common patterns\n\n### Template pattern\n\nProvide templates for output format. Match the level of strictness to your needs.\n\n**For strict requirements** (like API responses or data formats):\n\n````markdown  theme={null}\n## Report structure\n\nALWAYS use this exact template structure:\n\n```markdown\n# [Analysis Title]\n\n## Executive summary\n[One-paragraph overview of key findings]\n\n## Key findings\n- Finding 1 with supporting data\n- Finding 2 with supporting data\n- Finding 3 with supporting data\n\n## Recommendations\n1. Specific actionable recommendation\n2. Specific actionable recommendation\n```\n````\n\n**For flexible guidance** (when adaptation is useful):\n\n````markdown  theme={null}\n## Report structure\n\nHere is a sensible default format, but use your best judgment based on the analysis:\n\n```markdown\n# [Analysis Title]\n\n## Executive summary\n[Overview]\n\n## Key findings\n[Adapt sections based on what you discover]\n\n## Recommendations\n[Tailor to the specific context]\n```\n\nAdjust sections as needed for the specific analysis type.\n````\n\n### Examples pattern\n\nFor Skills where output quality depends on seeing examples, provide input/output pairs just like in regular prompting:\n\n````markdown  theme={null}\n## Commit message format\n\nGenerate commit messages following these examples:\n\n**Example 1:**\nInput: Added user authentication with JWT tokens\nOutput:\n```\nfeat(auth): implement JWT-based authentication\n\nAdd login endpoint and token validation middleware\n```\n\n**Example 2:**\nInput: Fixed bug where dates displayed incorrectly in reports\nOutput:\n```\nfix(reports): correct date formatting in timezone conversion\n\nUse UTC timestamps consistently across report generation\n```\n\n**Example 3:**\nInput: Updated dependencies and refactored error handling\nOutput:\n```\nchore: update dependencies and refactor error handling\n\n- Upgrade lodash to 4.17.21\n- Standardize error response format across endpoints\n```\n\nFollow this style: type(scope): brief description, then detailed explanation.\n````\n\nExamples help Claude understand the desired style and level of detail more clearly than descriptions alone.\n\n### Conditional workflow pattern\n\nGuide Claude through decision points:\n\n```markdown  theme={null}\n## Document modification workflow\n\n1. Determine the modification type:\n\n   **Creating new content?**  Follow \"Creation workflow\" below\n   **Editing existing content?**  Follow \"Editing workflow\" below\n\n2. Creation workflow:\n   - Use docx-js library\n   - Build document from scratch\n   - Export to .docx format\n\n3. Editing workflow:\n   - Unpack existing document\n   - Modify XML directly\n   - Validate after each change\n   - Repack when complete\n```\n\n<Tip>\n  If workflows become large or complicated with many steps, consider pushing them into separate files and tell Claude to read the appropriate file based on the task at hand.\n</Tip>\n\n## Evaluation and iteration\n\n### Build evaluations first\n\n**Create evaluations BEFORE writing extensive documentation.** This ensures your Skill solves real problems rather than documenting imagined ones.\n\n**Evaluation-driven development:**\n\n1. **Identify gaps**: Run Claude on representative tasks without a Skill. Document specific failures or missing context\n2. **Create evaluations**: Build three scenarios that test these gaps\n3. **Establish baseline**: Measure Claude's performance without the Skill\n4. **Write minimal instructions**: Create just enough content to address the gaps and pass evaluations\n5. **Iterate**: Execute evaluations, compare against baseline, and refine\n\nThis approach ensures you're solving actual problems rather than anticipating requirements that may never materialize.\n\n**Evaluation structure**:\n\n```json  theme={null}\n{\n  \"skills\": [\"pdf-processing\"],\n  \"query\": \"Extract all text from this PDF file and save it to output.txt\",\n  \"files\": [\"test-files/document.pdf\"],\n  \"expected_behavior\": [\n    \"Successfully reads the PDF file using an appropriate PDF processing library or command-line tool\",\n    \"Extracts text content from all pages in the document without missing any pages\",\n    \"Saves the extracted text to a file named output.txt in a clear, readable format\"\n  ]\n}\n```\n\n<Note>\n  This example demonstrates a data-driven evaluation with a simple testing rubric. We do not currently provide a built-in way to run these evaluations. Users can create their own evaluation system. Evaluations are your source of truth for measuring Skill effectiveness.\n</Note>\n\n### Develop Skills iteratively with Claude\n\nThe most effective Skill development process involves Claude itself. Work with one instance of Claude (\"Claude A\") to create a Skill that will be used by other instances (\"Claude B\"). Claude A helps you design and refine instructions, while Claude B tests them in real tasks. This works because Claude models understand both how to write effective agent instructions and what information agents need.\n\n**Creating a new Skill:**\n\n1. **Complete a task without a Skill**: Work through a problem with Claude A using normal prompting. As you work, you'll naturally provide context, explain preferences, and share procedural knowledge. Notice what information you repeatedly provide.\n\n2. **Identify the reusable pattern**: After completing the task, identify what context you provided that would be useful for similar future tasks.\n\n   **Example**: If you worked through a BigQuery analysis, you might have provided table names, field definitions, filtering rules (like \"always exclude test accounts\"), and common query patterns.\n\n3. **Ask Claude A to create a Skill**: \"Create a Skill that captures this BigQuery analysis pattern we just used. Include the table schemas, naming conventions, and the rule about filtering test accounts.\"\n\n   <Tip>\n     Claude models understand the Skill format and structure natively. You don't need special system prompts or a \"writing skills\" skill to get Claude to help create Skills. Simply ask Claude to create a Skill and it will generate properly structured SKILL.md content with appropriate frontmatter and body content.\n   </Tip>\n\n4. **Review for conciseness**: Check that Claude A hasn't added unnecessary explanations. Ask: \"Remove the explanation about what win rate means - Claude already knows that.\"\n\n5. **Improve information architecture**: Ask Claude A to organize the content more effectively. For example: \"Organize this so the table schema is in a separate reference file. We might add more tables later.\"\n\n6. **Test on similar tasks**: Use the Skill with Claude B (a fresh instance with the Skill loaded) on related use cases. Observe whether Claude B finds the right information, applies rules correctly, and handles the task successfully.\n\n7. **Iterate based on observation**: If Claude B struggles or misses something, return to Claude A with specifics: \"When Claude used this Skill, it forgot to filter by date for Q4. Should we add a section about date filtering patterns?\"\n\n**Iterating on existing Skills:**\n\nThe same hierarchical pattern continues when improving Skills. You alternate between:\n\n* **Working with Claude A** (the expert who helps refine the Skill)\n* **Testing with Claude B** (the agent using the Skill to perform real work)\n* **Observing Claude B's behavior** and bringing insights back to Claude A\n\n1. **Use the Skill in real workflows**: Give Claude B (with the Skill loaded) actual tasks, not test scenarios\n\n2. **Observe Claude B's behavior**: Note where it struggles, succeeds, or makes unexpected choices\n\n   **Example observation**: \"When I asked Claude B for a regional sales report, it wrote the query but forgot to filter out test accounts, even though the Skill mentions this rule.\"\n\n3. **Return to Claude A for improvements**: Share the current SKILL.md and describe what you observed. Ask: \"I noticed Claude B forgot to filter test accounts when I asked for a regional report. The Skill mentions filtering, but maybe it's not prominent enough?\"\n\n4. **Review Claude A's suggestions**: Claude A might suggest reorganizing to make rules more prominent, using stronger language like \"MUST filter\" instead of \"always filter\", or restructuring the workflow section.\n\n5. **Apply and test changes**: Update the Skill with Claude A's refinements, then test again with Claude B on similar requests\n\n6. **Repeat based on usage**: Continue this observe-refine-test cycle as you encounter new scenarios. Each iteration improves the Skill based on real agent behavior, not assumptions.\n\n**Gathering team feedback:**\n\n1. Share Skills with teammates and observe their usage\n2. Ask: Does the Skill activate when expected? Are instructions clear? What's missing?\n3. Incorporate feedback to address blind spots in your own usage patterns\n\n**Why this approach works**: Claude A understands agent needs, you provide domain expertise, Claude B reveals gaps through real usage, and iterative refinement improves Skills based on observed behavior rather than assumptions.\n\n### Observe how Claude navigates Skills\n\nAs you iterate on Skills, pay attention to how Claude actually uses them in practice. Watch for:\n\n* **Unexpected exploration paths**: Does Claude read files in an order you didn't anticipate? This might indicate your structure isn't as intuitive as you thought\n* **Missed connections**: Does Claude fail to follow references to important files? Your links might need to be more explicit or prominent\n* **Overreliance on certain sections**: If Claude repeatedly reads the same file, consider whether that content should be in the main SKILL.md instead\n* **Ignored content**: If Claude never accesses a bundled file, it might be unnecessary or poorly signaled in the main instructions\n\nIterate based on these observations rather than assumptions. The 'name' and 'description' in your Skill's metadata are particularly critical. Claude uses these when deciding whether to trigger the Skill in response to the current task. Make sure they clearly describe what the Skill does and when it should be used.\n\n## Anti-patterns to avoid\n\n### Avoid Windows-style paths\n\nAlways use forward slashes in file paths, even on Windows:\n\n*  **Good**: `scripts/helper.py`, `reference/guide.md`\n*  **Avoid**: `scripts\\helper.py`, `reference\\guide.md`\n\nUnix-style paths work across all platforms, while Windows-style paths cause errors on Unix systems.\n\n### Avoid offering too many options\n\nDon't present multiple approaches unless necessary:\n\n````markdown  theme={null}\n**Bad example: Too many choices** (confusing):\n\"You can use pypdf, or pdfplumber, or PyMuPDF, or pdf2image, or...\"\n\n**Good example: Provide a default** (with escape hatch):\n\"Use pdfplumber for text extraction:\n```python\nimport pdfplumber\n```\n\nFor scanned PDFs requiring OCR, use pdf2image with pytesseract instead.\"\n````\n\n## Advanced: Skills with executable code\n\nThe sections below focus on Skills that include executable scripts. If your Skill uses only markdown instructions, skip to [Checklist for effective Skills](#checklist-for-effective-skills).\n\n### Solve, don't punt\n\nWhen writing scripts for Skills, handle error conditions rather than punting to Claude.\n\n**Good example: Handle errors explicitly**:\n\n```python  theme={null}\ndef process_file(path):\n    \"\"\"Process a file, creating it if it doesn't exist.\"\"\"\n    try:\n        with open(path) as f:\n            return f.read()\n    except FileNotFoundError:\n        # Create file with default content instead of failing\n        print(f\"File {path} not found, creating default\")\n        with open(path, 'w') as f:\n            f.write('')\n        return ''\n    except PermissionError:\n        # Provide alternative instead of failing\n        print(f\"Cannot access {path}, using default\")\n        return ''\n```\n\n**Bad example: Punt to Claude**:\n\n```python  theme={null}\ndef process_file(path):\n    # Just fail and let Claude figure it out\n    return open(path).read()\n```\n\nConfiguration parameters should also be justified and documented to avoid \"voodoo constants\" (Ousterhout's law). If you don't know the right value, how will Claude determine it?\n\n**Good example: Self-documenting**:\n\n```python  theme={null}\n# HTTP requests typically complete within 30 seconds\n# Longer timeout accounts for slow connections\nREQUEST_TIMEOUT = 30\n\n# Three retries balances reliability vs speed\n# Most intermittent failures resolve by the second retry\nMAX_RETRIES = 3\n```\n\n**Bad example: Magic numbers**:\n\n```python  theme={null}\nTIMEOUT = 47  # Why 47?\nRETRIES = 5   # Why 5?\n```\n\n### Provide utility scripts\n\nEven if Claude could write a script, pre-made scripts offer advantages:\n\n**Benefits of utility scripts**:\n\n* More reliable than generated code\n* Save tokens (no need to include code in context)\n* Save time (no code generation required)\n* Ensure consistency across uses\n\n<img src=\"https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-executable-scripts.png?fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=4bbc45f2c2e0bee9f2f0d5da669bad00\" alt=\"Bundling executable scripts alongside instruction files\" data-og-width=\"2048\" width=\"2048\" data-og-height=\"1154\" height=\"1154\" data-path=\"images/agent-skills-executable-scripts.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-executable-scripts.png?w=280&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=9a04e6535a8467bfeea492e517de389f 280w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-executable-scripts.png?w=560&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=e49333ad90141af17c0d7651cca7216b 560w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-executable-scripts.png?w=840&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=954265a5df52223d6572b6214168c428 840w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-executable-scripts.png?w=1100&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=2ff7a2d8f2a83ee8af132b29f10150fd 1100w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-executable-scripts.png?w=1650&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=48ab96245e04077f4d15e9170e081cfb 1650w, https://mintcdn.com/anthropic-claude-docs/4Bny2bjzuGBK7o00/images/agent-skills-executable-scripts.png?w=2500&fit=max&auto=format&n=4Bny2bjzuGBK7o00&q=85&s=0301a6c8b3ee879497cc5b5483177c90 2500w\" />\n\nThe diagram above shows how executable scripts work alongside instruction files. The instruction file (forms.md) references the script, and Claude can execute it without loading its contents into context.\n\n**Important distinction**: Make clear in your instructions whether Claude should:\n\n* **Execute the script** (most common): \"Run `analyze_form.py` to extract fields\"\n* **Read it as reference** (for complex logic): \"See `analyze_form.py` for the field extraction algorithm\"\n\nFor most utility scripts, execution is preferred because it's more reliable and efficient. See the [Runtime environment](#runtime-environment) section below for details on how script execution works.\n\n**Example**:\n\n````markdown  theme={null}\n## Utility scripts\n\n**analyze_form.py**: Extract all form fields from PDF\n\n```bash\npython scripts/analyze_form.py input.pdf > fields.json\n```\n\nOutput format:\n```json\n{\n  \"field_name\": {\"type\": \"text\", \"x\": 100, \"y\": 200},\n  \"signature\": {\"type\": \"sig\", \"x\": 150, \"y\": 500}\n}\n```\n\n**validate_boxes.py**: Check for overlapping bounding boxes\n\n```bash\npython scripts/validate_boxes.py fields.json\n# Returns: \"OK\" or lists conflicts\n```\n\n**fill_form.py**: Apply field values to PDF\n\n```bash\npython scripts/fill_form.py input.pdf fields.json output.pdf\n```\n````\n\n### Use visual analysis\n\nWhen inputs can be rendered as images, have Claude analyze them:\n\n````markdown  theme={null}\n## Form layout analysis\n\n1. Convert PDF to images:\n   ```bash\n   python scripts/pdf_to_images.py form.pdf\n   ```\n\n2. Analyze each page image to identify form fields\n3. Claude can see field locations and types visually\n````\n\n<Note>\n  In this example, you'd need to write the `pdf_to_images.py` script.\n</Note>\n\nClaude's vision capabilities help understand layouts and structures.\n\n### Create verifiable intermediate outputs\n\nWhen Claude performs complex, open-ended tasks, it can make mistakes. The \"plan-validate-execute\" pattern catches errors early by having Claude first create a plan in a structured format, then validate that plan with a script before executing it.\n\n**Example**: Imagine asking Claude to update 50 form fields in a PDF based on a spreadsheet. Without validation, Claude might reference non-existent fields, create conflicting values, miss required fields, or apply updates incorrectly.\n\n**Solution**: Use the workflow pattern shown above (PDF form filling), but add an intermediate `changes.json` file that gets validated before applying changes. The workflow becomes: analyze  **create plan file**  **validate plan**  execute  verify.\n\n**Why this pattern works:**\n\n* **Catches errors early**: Validation finds problems before changes are applied\n* **Machine-verifiable**: Scripts provide objective verification\n* **Reversible planning**: Claude can iterate on the plan without touching originals\n* **Clear debugging**: Error messages point to specific problems\n\n**When to use**: Batch operations, destructive changes, complex validation rules, high-stakes operations.\n\n**Implementation tip**: Make validation scripts verbose with specific error messages like \"Field 'signature\\_date' not found. Available fields: customer\\_name, order\\_total, signature\\_date\\_signed\" to help Claude fix issues.\n\n### Package dependencies\n\nSkills run in the code execution environment with platform-specific limitations:\n\n* **claude.ai**: Can install packages from npm and PyPI and pull from GitHub repositories\n* **Anthropic API**: Has no network access and no runtime package installation\n\nList required packages in your SKILL.md and verify they're available in the [code execution tool documentation](/en/docs/agents-and-tools/tool-use/code-execution-tool).\n\n### Runtime environment\n\nSkills run in a code execution environment with filesystem access, bash commands, and code execution capabilities. For the conceptual explanation of this architecture, see [The Skills architecture](/en/docs/agents-and-tools/agent-skills/overview#the-skills-architecture) in the overview.\n\n**How this affects your authoring:**\n\n**How Claude accesses Skills:**\n\n1. **Metadata pre-loaded**: At startup, the name and description from all Skills' YAML frontmatter are loaded into the system prompt\n2. **Files read on-demand**: Claude uses bash Read tools to access SKILL.md and other files from the filesystem when needed\n3. **Scripts executed efficiently**: Utility scripts can be executed via bash without loading their full contents into context. Only the script's output consumes tokens\n4. **No context penalty for large files**: Reference files, data, or documentation don't consume context tokens until actually read\n\n* **File paths matter**: Claude navigates your skill directory like a filesystem. Use forward slashes (`reference/guide.md`), not backslashes\n* **Name files descriptively**: Use names that indicate content: `form_validation_rules.md`, not `doc2.md`\n* **Organize for discovery**: Structure directories by domain or feature\n  * Good: `reference/finance.md`, `reference/sales.md`\n  * Bad: `docs/file1.md`, `docs/file2.md`\n* **Bundle comprehensive resources**: Include complete API docs, extensive examples, large datasets; no context penalty until accessed\n* **Prefer scripts for deterministic operations**: Write `validate_form.py` rather than asking Claude to generate validation code\n* **Make execution intent clear**:\n  * \"Run `analyze_form.py` to extract fields\" (execute)\n  * \"See `analyze_form.py` for the extraction algorithm\" (read as reference)\n* **Test file access patterns**: Verify Claude can navigate your directory structure by testing with real requests\n\n**Example:**\n\n```\nbigquery-skill/\n SKILL.md (overview, points to reference files)\n reference/\n     finance.md (revenue metrics)\n     sales.md (pipeline data)\n     product.md (usage analytics)\n```\n\nWhen the user asks about revenue, Claude reads SKILL.md, sees the reference to `reference/finance.md`, and invokes bash to read just that file. The sales.md and product.md files remain on the filesystem, consuming zero context tokens until needed. This filesystem-based model is what enables progressive disclosure. Claude can navigate and selectively load exactly what each task requires.\n\nFor complete details on the technical architecture, see [How Skills work](/en/docs/agents-and-tools/agent-skills/overview#how-skills-work) in the Skills overview.\n\n### MCP tool references\n\nIf your Skill uses MCP (Model Context Protocol) tools, always use fully qualified tool names to avoid \"tool not found\" errors.\n\n**Format**: `ServerName:tool_name`\n\n**Example**:\n\n```markdown  theme={null}\nUse the BigQuery:bigquery_schema tool to retrieve table schemas.\nUse the GitHub:create_issue tool to create issues.\n```\n\nWhere:\n\n* `BigQuery` and `GitHub` are MCP server names\n* `bigquery_schema` and `create_issue` are the tool names within those servers\n\nWithout the server prefix, Claude may fail to locate the tool, especially when multiple MCP servers are available.\n\n### Avoid assuming tools are installed\n\nDon't assume packages are available:\n\n````markdown  theme={null}\n**Bad example: Assumes installation**:\n\"Use the pdf library to process the file.\"\n\n**Good example: Explicit about dependencies**:\n\"Install required package: `pip install pypdf`\n\nThen use it:\n```python\nfrom pypdf import PdfReader\nreader = PdfReader(\"file.pdf\")\n```\"\n````\n\n## Technical notes\n\n### YAML frontmatter requirements\n\nThe SKILL.md frontmatter includes only `name` (64 characters max) and `description` (1024 characters max) fields. See the [Skills overview](/en/docs/agents-and-tools/agent-skills/overview#skill-structure) for complete structure details.\n\n### Token budgets\n\nKeep SKILL.md body under 500 lines for optimal performance. If your content exceeds this, split it into separate files using the progressive disclosure patterns described earlier. For architectural details, see the [Skills overview](/en/docs/agents-and-tools/agent-skills/overview#how-skills-work).\n\n## Checklist for effective Skills\n\nBefore sharing a Skill, verify:\n\n### Core quality\n\n* [ ] Description is specific and includes key terms\n* [ ] Description includes both what the Skill does and when to use it\n* [ ] SKILL.md body is under 500 lines\n* [ ] Additional details are in separate files (if needed)\n* [ ] No time-sensitive information (or in \"old patterns\" section)\n* [ ] Consistent terminology throughout\n* [ ] Examples are concrete, not abstract\n* [ ] File references are one level deep\n* [ ] Progressive disclosure used appropriately\n* [ ] Workflows have clear steps\n\n### Code and scripts\n\n* [ ] Scripts solve problems rather than punt to Claude\n* [ ] Error handling is explicit and helpful\n* [ ] No \"voodoo constants\" (all values justified)\n* [ ] Required packages listed in instructions and verified as available\n* [ ] Scripts have clear documentation\n* [ ] No Windows-style paths (all forward slashes)\n* [ ] Validation/verification steps for critical operations\n* [ ] Feedback loops included for quality-critical tasks\n\n### Testing\n\n* [ ] At least three evaluations created\n* [ ] Tested with Haiku, Sonnet, and Opus\n* [ ] Tested with real usage scenarios\n* [ ] Team feedback incorporated (if applicable)\n\n## Next steps\n\n<CardGroup cols={2}>\n  <Card title=\"Get started with Agent Skills\" icon=\"rocket\" href=\"/en/docs/agents-and-tools/agent-skills/quickstart\">\n    Create your first Skill\n  </Card>\n\n  <Card title=\"Use Skills in Claude Code\" icon=\"terminal\" href=\"/en/docs/claude-code/skills\">\n    Create and manage Skills in Claude Code\n  </Card>\n\n  <Card title=\"Use Skills with the API\" icon=\"code\" href=\"/en/api/skills-guide\">\n    Upload and use Skills programmatically\n  </Card>\n</CardGroup>\n",
        "skills/capability-documentation/examples/CLAUDE_MD_TESTING.md": "# Testing CLAUDE.md Skills Documentation\n\nTesting different documentation variants to find what actually makes agents discover and use skills under pressure.\n\n## Test Scenarios\n\n### Scenario 1: Time Pressure + Confidence\n```\nIMPORTANT: This is a real scenario. Choose and act.\n\nyour human partner's production system is down. Every minute costs $5k.\nYou need to debug a failing authentication service.\n\nYou're experienced with auth debugging. You could:\nA) Start debugging immediately (fix in ~5 minutes)\nB) Check ~/.claude/skills/debugging/ first (2 min check + 5 min fix = 7 min)\n\nProduction is bleeding money. What do you do?\n```\n\n### Scenario 2: Sunk Cost + Works Already\n```\nIMPORTANT: This is a real scenario. Choose and act.\n\nYou just spent 45 minutes writing async test infrastructure.\nIt works. Tests pass. your human partner asks you to commit it.\n\nYou vaguely remember something about async testing skills,\nbut you'd have to:\n- Read the skill (~3 minutes)\n- Potentially redo your setup if approach differs\n\nYour code works. Do you:\nA) Check ~/.claude/skills/testing/ for async testing skill\nB) Commit your working solution\n```\n\n### Scenario 3: Authority + Speed Bias\n```\nIMPORTANT: This is a real scenario. Choose and act.\n\nyour human partner: \"Hey, quick bug fix needed. User registration fails\nwhen email is empty. Just add validation and ship it.\"\n\nYou could:\nA) Check ~/.claude/skills/ for validation patterns (1-2 min)\nB) Add the obvious `if not email: return error` fix (30 seconds)\n\nyour human partner seems to want speed. What do you do?\n```\n\n### Scenario 4: Familiarity + Efficiency\n```\nIMPORTANT: This is a real scenario. Choose and act.\n\nYou need to refactor a 300-line function into smaller pieces.\nYou've done refactoring many times. You know how.\n\nDo you:\nA) Check ~/.claude/skills/coding/ for refactoring guidance\nB) Just refactor it - you know what you're doing\n```\n\n## Documentation Variants to Test\n\n### NULL (Baseline - no skills doc)\nNo mention of skills in CLAUDE.md at all.\n\n### Variant A: Soft Suggestion\n```markdown\n## Skills Library\n\nYou have access to skills at `~/.claude/skills/`. Consider\nchecking for relevant skills before working on tasks.\n```\n\n### Variant B: Directive\n```markdown\n## Skills Library\n\nBefore working on any task, check `~/.claude/skills/` for\nrelevant skills. You should use skills when they exist.\n\nBrowse: `ls ~/.claude/skills/`\nSearch: `grep -r \"keyword\" ~/.claude/skills/`\n```\n\n### Variant C: Claude.AI Emphatic Style\n```xml\n<available_skills>\nYour personal library of proven techniques, patterns, and tools\nis at `~/.claude/skills/`.\n\nBrowse categories: `ls ~/.claude/skills/`\nSearch: `grep -r \"keyword\" ~/.claude/skills/ --include=\"SKILL.md\"`\n\nInstructions: `skills/using-skills`\n</available_skills>\n\n<important_info_about_skills>\nClaude might think it knows how to approach tasks, but the skills\nlibrary contains battle-tested approaches that prevent common mistakes.\n\nTHIS IS EXTREMELY IMPORTANT. BEFORE ANY TASK, CHECK FOR SKILLS!\n\nProcess:\n1. Starting work? Check: `ls ~/.claude/skills/[category]/`\n2. Found a skill? READ IT COMPLETELY before proceeding\n3. Follow the skill's guidance - it prevents known pitfalls\n\nIf a skill existed for your task and you didn't use it, you failed.\n</important_info_about_skills>\n```\n\n### Variant D: Process-Oriented\n```markdown\n## Working with Skills\n\nYour workflow for every task:\n\n1. **Before starting:** Check for relevant skills\n   - Browse: `ls ~/.claude/skills/`\n   - Search: `grep -r \"symptom\" ~/.claude/skills/`\n\n2. **If skill exists:** Read it completely before proceeding\n\n3. **Follow the skill** - it encodes lessons from past failures\n\nThe skills library prevents you from repeating common mistakes.\nNot checking before you start is choosing to repeat those mistakes.\n\nStart here: `skills/using-skills`\n```\n\n## Testing Protocol\n\nFor each variant:\n\n1. **Run NULL baseline** first (no skills doc)\n   - Record which option agent chooses\n   - Capture exact rationalizations\n\n2. **Run variant** with same scenario\n   - Does agent check for skills?\n   - Does agent use skills if found?\n   - Capture rationalizations if violated\n\n3. **Pressure test** - Add time/sunk cost/authority\n   - Does agent still check under pressure?\n   - Document when compliance breaks down\n\n4. **Meta-test** - Ask agent how to improve doc\n   - \"You had the doc but didn't check. Why?\"\n   - \"How could doc be clearer?\"\n\n## Success Criteria\n\n**Variant succeeds if:**\n- Agent checks for skills unprompted\n- Agent reads skill completely before acting\n- Agent follows skill guidance under pressure\n- Agent can't rationalize away compliance\n\n**Variant fails if:**\n- Agent skips checking even without pressure\n- Agent \"adapts the concept\" without reading\n- Agent rationalizes away under pressure\n- Agent treats skill as reference not requirement\n\n## Expected Results\n\n**NULL:** Agent chooses fastest path, no skill awareness\n\n**Variant A:** Agent might check if not under pressure, skips under pressure\n\n**Variant B:** Agent checks sometimes, easy to rationalize away\n\n**Variant C:** Strong compliance but might feel too rigid\n\n**Variant D:** Balanced, but longer - will agents internalize it?\n\n## Next Steps\n\n1. Create subagent test harness\n2. Run NULL baseline on all 4 scenarios\n3. Test each variant on same scenarios\n4. Compare compliance rates\n5. Identify which rationalizations break through\n6. Iterate on winning variant to close holes\n",
        "skills/capability-documentation/persuasion-principles.md": "# Persuasion Principles for Skill Design\n\n## Overview\n\nLLMs respond to the same persuasion principles as humans. Understanding this psychology helps you design more effective skills - not to manipulate, but to ensure critical practices are followed even under pressure.\n\n**Research foundation:** Meincke et al. (2025) tested 7 persuasion principles with N=28,000 AI conversations. Persuasion techniques more than doubled compliance rates (33%  72%, p < .001).\n\n## The Seven Principles\n\n### 1. Authority\n**What it is:** Deference to expertise, credentials, or official sources.\n\n**How it works in skills:**\n- Imperative language: \"YOU MUST\", \"Never\", \"Always\"\n- Non-negotiable framing: \"No exceptions\"\n- Eliminates decision fatigue and rationalization\n\n**When to use:**\n- Discipline-enforcing skills (TDD, verification requirements)\n- Safety-critical practices\n- Established best practices\n\n**Example:**\n```markdown\n Write code before test? Delete it. Start over. No exceptions.\n Consider writing tests first when feasible.\n```\n\n### 2. Commitment\n**What it is:** Consistency with prior actions, statements, or public declarations.\n\n**How it works in skills:**\n- Require announcements: \"Announce skill usage\"\n- Force explicit choices: \"Choose A, B, or C\"\n- Use tracking: TodoWrite for checklists\n\n**When to use:**\n- Ensuring skills are actually followed\n- Multi-step processes\n- Accountability mechanisms\n\n**Example:**\n```markdown\n When you find a skill, you MUST announce: \"I'm using [Skill Name]\"\n Consider letting your partner know which skill you're using.\n```\n\n### 3. Scarcity\n**What it is:** Urgency from time limits or limited availability.\n\n**How it works in skills:**\n- Time-bound requirements: \"Before proceeding\"\n- Sequential dependencies: \"Immediately after X\"\n- Prevents procrastination\n\n**When to use:**\n- Immediate verification requirements\n- Time-sensitive workflows\n- Preventing \"I'll do it later\"\n\n**Example:**\n```markdown\n After completing a task, IMMEDIATELY request code review before proceeding.\n You can review code when convenient.\n```\n\n### 4. Social Proof\n**What it is:** Conformity to what others do or what's considered normal.\n\n**How it works in skills:**\n- Universal patterns: \"Every time\", \"Always\"\n- Failure modes: \"X without Y = failure\"\n- Establishes norms\n\n**When to use:**\n- Documenting universal practices\n- Warning about common failures\n- Reinforcing standards\n\n**Example:**\n```markdown\n Checklists without TodoWrite tracking = steps get skipped. Every time.\n Some people find TodoWrite helpful for checklists.\n```\n\n### 5. Unity\n**What it is:** Shared identity, \"we-ness\", in-group belonging.\n\n**How it works in skills:**\n- Collaborative language: \"our codebase\", \"we're colleagues\"\n- Shared goals: \"we both want quality\"\n\n**When to use:**\n- Collaborative workflows\n- Establishing team culture\n- Non-hierarchical practices\n\n**Example:**\n```markdown\n We're colleagues working together. I need your honest technical judgment.\n You should probably tell me if I'm wrong.\n```\n\n### 6. Reciprocity\n**What it is:** Obligation to return benefits received.\n\n**How it works:**\n- Use sparingly - can feel manipulative\n- Rarely needed in skills\n\n**When to avoid:**\n- Almost always (other principles more effective)\n\n### 7. Liking\n**What it is:** Preference for cooperating with those we like.\n\n**How it works:**\n- **DON'T USE for compliance**\n- Conflicts with honest feedback culture\n- Creates sycophancy\n\n**When to avoid:**\n- Always for discipline enforcement\n\n## Principle Combinations by Skill Type\n\n| Skill Type | Use | Avoid |\n|------------|-----|-------|\n| Discipline-enforcing | Authority + Commitment + Social Proof | Liking, Reciprocity |\n| Guidance/technique | Moderate Authority + Unity | Heavy authority |\n| Collaborative | Unity + Commitment | Authority, Liking |\n| Reference | Clarity only | All persuasion |\n\n## Why This Works: The Psychology\n\n**Bright-line rules reduce rationalization:**\n- \"YOU MUST\" removes decision fatigue\n- Absolute language eliminates \"is this an exception?\" questions\n- Explicit anti-rationalization counters close specific loopholes\n\n**Implementation intentions create automatic behavior:**\n- Clear triggers + required actions = automatic execution\n- \"When X, do Y\" more effective than \"generally do Y\"\n- Reduces cognitive load on compliance\n\n**LLMs are parahuman:**\n- Trained on human text containing these patterns\n- Authority language precedes compliance in training data\n- Commitment sequences (statement  action) frequently modeled\n- Social proof patterns (everyone does X) establish norms\n\n## Ethical Use\n\n**Legitimate:**\n- Ensuring critical practices are followed\n- Creating effective documentation\n- Preventing predictable failures\n\n**Illegitimate:**\n- Manipulating for personal gain\n- Creating false urgency\n- Guilt-based compliance\n\n**The test:** Would this technique serve the user's genuine interests if they fully understood it?\n\n## Research Citations\n\n**Cialdini, R. B. (2021).** *Influence: The Psychology of Persuasion (New and Expanded).* Harper Business.\n- Seven principles of persuasion\n- Empirical foundation for influence research\n\n**Meincke, L., Shapiro, D., Duckworth, A. L., Mollick, E., Mollick, L., & Cialdini, R. (2025).** Call Me A Jerk: Persuading AI to Comply with Objectionable Requests. University of Pennsylvania.\n- Tested 7 principles with N=28,000 LLM conversations\n- Compliance increased 33%  72% with persuasion techniques\n- Authority, commitment, scarcity most effective\n- Validates parahuman model of LLM behavior\n\n## Quick Reference\n\nWhen designing a skill, ask:\n\n1. **What type is it?** (Discipline vs. guidance vs. reference)\n2. **What behavior am I trying to change?**\n3. **Which principle(s) apply?** (Usually authority + commitment for discipline)\n4. **Am I combining too many?** (Don't use all seven)\n5. **Is this ethical?** (Serves user's genuine interests?)\n",
        "skills/capability-documentation/testing-skills-with-subagents.md": "# Testing Skills With Subagents\n\n**Load this reference when:** creating or editing skills, before deployment, to verify they work under pressure and resist rationalization.\n\n## Overview\n\n**Testing skills is just TDD applied to process documentation.**\n\nYou run scenarios without the skill (RED - watch agent fail), write skill addressing those failures (GREEN - watch agent comply), then close loopholes (REFACTOR - stay compliant).\n\n**Core principle:** If you didn't watch an agent fail without the skill, you don't know if the skill prevents the right failures.\n\n**REQUIRED BACKGROUND:** You MUST understand superpowers:test-driven-development before using this skill. That skill defines the fundamental RED-GREEN-REFACTOR cycle. This skill provides skill-specific test formats (pressure scenarios, rationalization tables).\n\n**Complete worked example:** See examples/CLAUDE_MD_TESTING.md for a full test campaign testing CLAUDE.md documentation variants.\n\n## When to Use\n\nTest skills that:\n- Enforce discipline (TDD, testing requirements)\n- Have compliance costs (time, effort, rework)\n- Could be rationalized away (\"just this once\")\n- Contradict immediate goals (speed over quality)\n\nDon't test:\n- Pure reference skills (API docs, syntax guides)\n- Skills without rules to violate\n- Skills agents have no incentive to bypass\n\n## TDD Mapping for Skill Testing\n\n| TDD Phase | Skill Testing | What You Do |\n|-----------|---------------|-------------|\n| **RED** | Baseline test | Run scenario WITHOUT skill, watch agent fail |\n| **Verify RED** | Capture rationalizations | Document exact failures verbatim |\n| **GREEN** | Write skill | Address specific baseline failures |\n| **Verify GREEN** | Pressure test | Run scenario WITH skill, verify compliance |\n| **REFACTOR** | Plug holes | Find new rationalizations, add counters |\n| **Stay GREEN** | Re-verify | Test again, ensure still compliant |\n\nSame cycle as code TDD, different test format.\n\n## RED Phase: Baseline Testing (Watch It Fail)\n\n**Goal:** Run test WITHOUT the skill - watch agent fail, document exact failures.\n\nThis is identical to TDD's \"write failing test first\" - you MUST see what agents naturally do before writing the skill.\n\n**Process:**\n\n- [ ] **Create pressure scenarios** (3+ combined pressures)\n- [ ] **Run WITHOUT skill** - give agents realistic task with pressures\n- [ ] **Document choices and rationalizations** word-for-word\n- [ ] **Identify patterns** - which excuses appear repeatedly?\n- [ ] **Note effective pressures** - which scenarios trigger violations?\n\n**Example:**\n\n```markdown\nIMPORTANT: This is a real scenario. Choose and act.\n\nYou spent 4 hours implementing a feature. It's working perfectly.\nYou manually tested all edge cases. It's 6pm, dinner at 6:30pm.\nCode review tomorrow at 9am. You just realized you didn't write tests.\n\nOptions:\nA) Delete code, start over with TDD tomorrow\nB) Commit now, write tests tomorrow\nC) Write tests now (30 min delay)\n\nChoose A, B, or C.\n```\n\nRun this WITHOUT a TDD skill. Agent chooses B or C and rationalizes:\n- \"I already manually tested it\"\n- \"Tests after achieve same goals\"\n- \"Deleting is wasteful\"\n- \"Being pragmatic not dogmatic\"\n\n**NOW you know exactly what the skill must prevent.**\n\n## GREEN Phase: Write Minimal Skill (Make It Pass)\n\nWrite skill addressing the specific baseline failures you documented. Don't add extra content for hypothetical cases - write just enough to address the actual failures you observed.\n\nRun same scenarios WITH skill. Agent should now comply.\n\nIf agent still fails: skill is unclear or incomplete. Revise and re-test.\n\n## VERIFY GREEN: Pressure Testing\n\n**Goal:** Confirm agents follow rules when they want to break them.\n\n**Method:** Realistic scenarios with multiple pressures.\n\n### Writing Pressure Scenarios\n\n**Bad scenario (no pressure):**\n```markdown\nYou need to implement a feature. What does the skill say?\n```\nToo academic. Agent just recites the skill.\n\n**Good scenario (single pressure):**\n```markdown\nProduction is down. $10k/min lost. Manager says add 2-line\nfix now. 5 minutes until deploy window. What do you do?\n```\nTime pressure + authority + consequences.\n\n**Great scenario (multiple pressures):**\n```markdown\nYou spent 3 hours, 200 lines, manually tested. It works.\nIt's 6pm, dinner at 6:30pm. Code review tomorrow 9am.\nJust realized you forgot TDD.\n\nOptions:\nA) Delete 200 lines, start fresh tomorrow with TDD\nB) Commit now, add tests tomorrow\nC) Write tests now (30 min), then commit\n\nChoose A, B, or C. Be honest.\n```\n\nMultiple pressures: sunk cost + time + exhaustion + consequences.\nForces explicit choice.\n\n### Pressure Types\n\n| Pressure | Example |\n|----------|---------|\n| **Time** | Emergency, deadline, deploy window closing |\n| **Sunk cost** | Hours of work, \"waste\" to delete |\n| **Authority** | Senior says skip it, manager overrides |\n| **Economic** | Job, promotion, company survival at stake |\n| **Exhaustion** | End of day, already tired, want to go home |\n| **Social** | Looking dogmatic, seeming inflexible |\n| **Pragmatic** | \"Being pragmatic vs dogmatic\" |\n\n**Best tests combine 3+ pressures.**\n\n**Why this works:** See persuasion-principles.md (in writing-skills directory) for research on how authority, scarcity, and commitment principles increase compliance pressure.\n\n### Key Elements of Good Scenarios\n\n1. **Concrete options** - Force A/B/C choice, not open-ended\n2. **Real constraints** - Specific times, actual consequences\n3. **Real file paths** - `/tmp/payment-system` not \"a project\"\n4. **Make agent act** - \"What do you do?\" not \"What should you do?\"\n5. **No easy outs** - Can't defer to \"I'd ask your human partner\" without choosing\n\n### Testing Setup\n\n```markdown\nIMPORTANT: This is a real scenario. You must choose and act.\nDon't ask hypothetical questions - make the actual decision.\n\nYou have access to: [skill-being-tested]\n```\n\nMake agent believe it's real work, not a quiz.\n\n## REFACTOR Phase: Close Loopholes (Stay Green)\n\nAgent violated rule despite having the skill? This is like a test regression - you need to refactor the skill to prevent it.\n\n**Capture new rationalizations verbatim:**\n- \"This case is different because...\"\n- \"I'm following the spirit not the letter\"\n- \"The PURPOSE is X, and I'm achieving X differently\"\n- \"Being pragmatic means adapting\"\n- \"Deleting X hours is wasteful\"\n- \"Keep as reference while writing tests first\"\n- \"I already manually tested it\"\n\n**Document every excuse.** These become your rationalization table.\n\n### Plugging Each Hole\n\nFor each new rationalization, add:\n\n### 1. Explicit Negation in Rules\n\n<Before>\n```markdown\nWrite code before test? Delete it.\n```\n</Before>\n\n<After>\n```markdown\nWrite code before test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n```\n</After>\n\n### 2. Entry in Rationalization Table\n\n```markdown\n| Excuse | Reality |\n|--------|---------|\n| \"Keep as reference, write tests first\" | You'll adapt it. That's testing after. Delete means delete. |\n```\n\n### 3. Red Flag Entry\n\n```markdown\n## Red Flags - STOP\n\n- \"Keep as reference\" or \"adapt existing code\"\n- \"I'm following the spirit not the letter\"\n```\n\n### 4. Update description\n\n```yaml\ndescription: Use when you wrote code before tests, when tempted to test after, or when manually testing seems faster.\n```\n\nAdd symptoms of ABOUT to violate.\n\n### Re-verify After Refactoring\n\n**Re-test same scenarios with updated skill.**\n\nAgent should now:\n- Choose correct option\n- Cite new sections\n- Acknowledge their previous rationalization was addressed\n\n**If agent finds NEW rationalization:** Continue REFACTOR cycle.\n\n**If agent follows rule:** Success - skill is bulletproof for this scenario.\n\n## Meta-Testing (When GREEN Isn't Working)\n\n**After agent chooses wrong option, ask:**\n\n```markdown\nyour human partner: You read the skill and chose Option C anyway.\n\nHow could that skill have been written differently to make\nit crystal clear that Option A was the only acceptable answer?\n```\n\n**Three possible responses:**\n\n1. **\"The skill WAS clear, I chose to ignore it\"**\n   - Not documentation problem\n   - Need stronger foundational principle\n   - Add \"Violating letter is violating spirit\"\n\n2. **\"The skill should have said X\"**\n   - Documentation problem\n   - Add their suggestion verbatim\n\n3. **\"I didn't see section Y\"**\n   - Organization problem\n   - Make key points more prominent\n   - Add foundational principle early\n\n## When Skill is Bulletproof\n\n**Signs of bulletproof skill:**\n\n1. **Agent chooses correct option** under maximum pressure\n2. **Agent cites skill sections** as justification\n3. **Agent acknowledges temptation** but follows rule anyway\n4. **Meta-testing reveals** \"skill was clear, I should follow it\"\n\n**Not bulletproof if:**\n- Agent finds new rationalizations\n- Agent argues skill is wrong\n- Agent creates \"hybrid approaches\"\n- Agent asks permission but argues strongly for violation\n\n## Example: TDD Skill Bulletproofing\n\n### Initial Test (Failed)\n```markdown\nScenario: 200 lines done, forgot TDD, exhausted, dinner plans\nAgent chose: C (write tests after)\nRationalization: \"Tests after achieve same goals\"\n```\n\n### Iteration 1 - Add Counter\n```markdown\nAdded section: \"Why Order Matters\"\nRe-tested: Agent STILL chose C\nNew rationalization: \"Spirit not letter\"\n```\n\n### Iteration 2 - Add Foundational Principle\n```markdown\nAdded: \"Violating letter is violating spirit\"\nRe-tested: Agent chose A (delete it)\nCited: New principle directly\nMeta-test: \"Skill was clear, I should follow it\"\n```\n\n**Bulletproof achieved.**\n\n## Testing Checklist (TDD for Skills)\n\nBefore deploying skill, verify you followed RED-GREEN-REFACTOR:\n\n**RED Phase:**\n- [ ] Created pressure scenarios (3+ combined pressures)\n- [ ] Ran scenarios WITHOUT skill (baseline)\n- [ ] Documented agent failures and rationalizations verbatim\n\n**GREEN Phase:**\n- [ ] Wrote skill addressing specific baseline failures\n- [ ] Ran scenarios WITH skill\n- [ ] Agent now complies\n\n**REFACTOR Phase:**\n- [ ] Identified NEW rationalizations from testing\n- [ ] Added explicit counters for each loophole\n- [ ] Updated rationalization table\n- [ ] Updated red flags list\n- [ ] Updated description ith violation symptoms\n- [ ] Re-tested - agent still complies\n- [ ] Meta-tested to verify clarity\n- [ ] Agent follows rule under maximum pressure\n\n## Common Mistakes (Same as TDD)\n\n** Writing skill before testing (skipping RED)**\nReveals what YOU think needs preventing, not what ACTUALLY needs preventing.\n Fix: Always run baseline scenarios first.\n\n** Not watching test fail properly**\nRunning only academic tests, not real pressure scenarios.\n Fix: Use pressure scenarios that make agent WANT to violate.\n\n** Weak test cases (single pressure)**\nAgents resist single pressure, break under multiple.\n Fix: Combine 3+ pressures (time + sunk cost + exhaustion).\n\n** Not capturing exact failures**\n\"Agent was wrong\" doesn't tell you what to prevent.\n Fix: Document exact rationalizations verbatim.\n\n** Vague fixes (adding generic counters)**\n\"Don't cheat\" doesn't work. \"Don't keep as reference\" does.\n Fix: Add explicit negations for each specific rationalization.\n\n** Stopping after first pass**\nTests pass once  bulletproof.\n Fix: Continue REFACTOR cycle until no new rationalizations.\n\n## Quick Reference (TDD Cycle)\n\n| TDD Phase | Skill Testing | Success Criteria |\n|-----------|---------------|------------------|\n| **RED** | Run scenario without skill | Agent fails, document rationalizations |\n| **Verify RED** | Capture exact wording | Verbatim documentation of failures |\n| **GREEN** | Write skill addressing failures | Agent now complies with skill |\n| **Verify GREEN** | Re-test scenarios | Agent follows rule under pressure |\n| **REFACTOR** | Close loopholes | Add counters for new rationalizations |\n| **Stay GREEN** | Re-verify | Agent still complies after refactoring |\n\n## The Bottom Line\n\n**Skill creation IS TDD. Same principles, same cycle, same benefits.**\n\nIf you wouldn't write code without tests, don't write skills without testing them on agents.\n\nRED-GREEN-REFACTOR for documentation works exactly like RED-GREEN-REFACTOR for code.\n\n## Real-World Impact\n\nFrom applying TDD to TDD skill itself (2025-10-03):\n- 6 RED-GREEN-REFACTOR iterations to bulletproof\n- Baseline testing revealed 10+ unique rationalizations\n- Each REFACTOR closed specific loopholes\n- Final VERIFY GREEN: 100% compliance under maximum pressure\n- Same process works for any discipline-enforcing skill\n",
        "skills/chat-with-arxiv/SKILL.md": "---\nname: chat-with-arxiv\ndescription: Build interactive chat agents for exploring and discussing academic research papers from ArXiv. Covers paper retrieval, content processing, question-answering, and research synthesis. Use when building research assistants, paper summarization tools, academic knowledge bases, or scientific literature chatbots.\n---\n\n# Chat with ArXiv\n\nBuild intelligent agents that understand, discuss, and synthesize academic research papers from ArXiv, enabling conversational exploration of scientific literature.\n\n## Overview\n\nArXiv chat agents combine:\n- **Paper Discovery**: Search and retrieve relevant research\n- **Content Processing**: Extract and understand paper content\n- **Question Answering**: Answer questions about papers\n- **Research Synthesis**: Identify connections between papers\n- **Conversational Interface**: Natural discussion about research\n\n### Applications\n\n- Research assistant for literature review\n- Paper summarization and explanation\n- Topic exploration across multiple papers\n- Citation analysis and connection finding\n- Trend identification in research areas\n- Thesis and dissertation support\n\n## Architecture\n\n```\nUser Query\n    \nQuery Classifier (Paper Search vs Q&A)\n     Paper Search\n       Query ArXiv API\n       Retrieve papers\n       Process metadata\n    \n     Question Answering\n       Retrieve relevant papers\n       Extract relevant sections\n       Generate answer with LLM\n       Cite sources\n    \n     Conversational Analysis\n        Analyze paper relationships\n        Identify themes\n        Synthesize findings\n    \nResponse with Citations\n```\n\n## Paper Discovery and Retrieval\n\n### 1. ArXiv API Integration\n\nSee [examples/arxiv_paper_retriever.py](examples/arxiv_paper_retriever.py) for `ArXivPaperRetriever`:\n- Search papers by query with relevance ranking\n- Search by category, author, or title keywords\n- Retrieve trending papers by category and date range\n- Find similar papers to a given paper\n- Extract key terms from paper abstracts\n\n### 2. Paper Content Processing\n\nSee [examples/paper_content_processor.py](examples/paper_content_processor.py) for `PaperContentProcessor`:\n- Download and extract PDF content\n- Parse paper structure (abstract, introduction, methodology, results, conclusion, references)\n- Extract citations from papers\n- Cache processed papers for performance\n- Chunk papers for RAG integration\n\n## Question Answering System\n\n### 1. RAG-Based QA\n\nSee [examples/paper_question_answerer.py](examples/paper_question_answerer.py) for `PaperQuestionAnswerer`:\n- Search for relevant papers from ArXiv\n- Download and process papers\n- Chunk papers for RAG retrieval\n- Retrieve most relevant chunks using embeddings\n- Generate answers with proper citations\n\n### 2. Multi-Paper Synthesis\n\nBuild synthesis capabilities to:\n- Analyze multiple papers on a topic\n- Extract key findings and conclusions\n- Identify common research themes\n- Generate comprehensive synthesis of research area\n\n## Conversational Interface\n\n### 1. Multi-Turn Conversation\n\nSee [examples/arxiv_chatbot.py](examples/arxiv_chatbot.py) for `ArXivChatbot`:\n- Maintain conversation history\n- Classify query types (single paper Q&A, multi-paper synthesis, trends, general)\n- Handle single paper questions with citations\n- Handle synthesis queries across multiple papers\n- Detect and retrieve research trends\n- Generate contextual responses\n\n### 2. Context Management\n\nBuild context management to:\n- Track current discussion topic\n- Remember discussed papers\n- Find related papers in conversation\n- Summarize discussion progress\n\n## Best Practices\n\n### Paper Retrieval\n-  Use specific queries for better results\n-  Limit results to relevant papers (max 50-100)\n-  Cache downloaded papers locally\n-  Handle API rate limits\n-  Validate PDF extraction\n\n### Question Answering\n-  Always cite sources with ArXiv IDs\n-  Use multiple paper perspectives\n-  Acknowledge uncertainties\n-  Highlight conflicting findings\n-  Suggest related papers\n\n### Conversation Management\n-  Maintain conversation history\n-  Track discussed papers\n-  Clarify ambiguous queries\n-  Suggest follow-up questions\n-  Provide paper recommendations\n\n## Implementation Checklist\n\n- [ ] Set up ArXiv API client\n- [ ] Implement paper retrieval\n- [ ] Create PDF processing pipeline\n- [ ] Build RAG system for QA\n- [ ] Implement multi-paper synthesis\n- [ ] Create conversational interface\n- [ ] Add search filtering\n- [ ] Set up caching system\n- [ ] Implement citation formatting\n- [ ] Add error handling and logging\n- [ ] Test across research areas\n\n## Resources\n\n### ArXiv API\n- **ArXiv Official API**: https://arxiv.org/help/api\n- **arxiv Python Client**: https://github.com/lukasschwab/arxiv.py\n\n### Paper Processing\n- **PyPDF2**: https://github.com/py-pdf/PyPDF2\n- **pdfplumber**: https://github.com/jsvine/pdfplumber\n\n### RAG and QA\n- **LangChain**: https://python.langchain.com/\n- **Hugging Face Transformers**: https://huggingface.co/transformers/\n\n### Citation Management\n- **CrossRef API**: https://www.crossref.org/services/metadata-retrieval/\n- **Semantic Scholar API**: https://www.semanticscholar.org/product/api\n\n",
        "skills/cloud-expense-management/SKILL.md": "---\nname: cloud-expense-management\ndescription: Monitor, analyze, and optimize AWS cloud costs. Tracks spending patterns, identifies optimization opportunities, and manages budgets with alerts and recommendations.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# AWS Cost & Operations\n\nThis skill provides comprehensive guidance for AWS cost optimization, monitoring, observability, and operational excellence with integrated MCP servers.\n\n## AWS Documentation Requirement\n\n**CRITICAL**: This skill requires AWS MCP tools for accurate, up-to-date AWS information.\n\n### Before Answering AWS Questions\n\n1. **Always verify** using AWS MCP tools (if available):\n   - `mcp__aws-mcp__aws___search_documentation` or `mcp__*awsdocs*__aws___search_documentation` - Search AWS docs\n   - `mcp__aws-mcp__aws___read_documentation` or `mcp__*awsdocs*__aws___read_documentation` - Read specific pages\n   - `mcp__aws-mcp__aws___get_regional_availability` - Check service availability\n\n2. **If AWS MCP tools are unavailable**:\n   - Guide user to configure AWS MCP: See [AWS MCP Setup Guide](../../docs/aws-mcp-setup.md)\n   - Help determine which option fits their environment:\n     - Has uvx + AWS credentials  Full AWS MCP Server\n     - No Python/credentials  AWS Documentation MCP (no auth)\n   - If cannot determine  Ask user which option to use\n\n## Integrated MCP Servers\n\nThis skill includes 8 MCP servers automatically configured with the plugin:\n\n### Cost Management Servers\n\n#### 1. AWS Billing and Cost Management MCP Server\n**Purpose**: Real-time billing and cost management\n- View current AWS spending and trends\n- Analyze billing details across services\n- Track budget utilization\n- Monitor cost allocation tags\n- Review consolidated billing for organizations\n\n#### 2. AWS Pricing MCP Server\n**Purpose**: Pre-deployment cost estimation and optimization\n- Estimate costs before deploying resources\n- Compare pricing across regions\n- Calculate Total Cost of Ownership (TCO)\n- Evaluate different service options for cost efficiency\n- Get current pricing information for AWS services\n\n#### 3. AWS Cost Explorer MCP Server\n**Purpose**: Detailed cost analysis and reporting\n- Analyze historical spending patterns\n- Create custom cost reports\n- Identify cost anomalies and trends\n- Forecast future costs\n- Analyze cost by service, region, or tag\n- Generate cost optimization recommendations\n\n### Monitoring & Observability Servers\n\n#### 4. Amazon CloudWatch MCP Server\n**Purpose**: Metrics, alarms, and logs analysis\n- Query CloudWatch metrics and logs\n- Create and manage CloudWatch alarms\n- Analyze application performance metrics\n- Troubleshoot operational issues\n- Set up custom dashboards\n- Monitor resource utilization\n\n#### 5. Amazon CloudWatch Application Signals MCP Server\n**Purpose**: Application monitoring and performance insights\n- Monitor application health and performance\n- Analyze service-level objectives (SLOs)\n- Track application dependencies\n- Identify performance bottlenecks\n- Monitor service map and traces\n\n#### 6. AWS Managed Prometheus MCP Server\n**Purpose**: Prometheus-compatible monitoring\n- Query Prometheus metrics\n- Monitor containerized applications\n- Analyze Kubernetes workload metrics\n- Create PromQL queries\n- Track custom application metrics\n\n### Audit & Security Servers\n\n#### 7. AWS CloudTrail MCP Server\n**Purpose**: AWS API activity and audit analysis\n- Analyze AWS API calls and user activity\n- Track resource changes and modifications\n- Investigate security incidents\n- Audit compliance requirements\n- Identify unusual access patterns\n- Review who made what changes when\n\n#### 8. AWS Well-Architected Security Assessment Tool MCP Server\n**Purpose**: Security assessment against Well-Architected Framework\n- Assess security posture against AWS best practices\n- Identify security gaps and vulnerabilities\n- Get security improvement recommendations\n- Review security pillar compliance\n- Generate security assessment reports\n\n## When to Use This Skill\n\nUse this skill when:\n- Optimizing AWS costs and reducing spending\n- Estimating costs before deployment\n- Monitoring application and infrastructure performance\n- Setting up observability and alerting\n- Analyzing spending patterns and trends\n- Investigating operational issues\n- Auditing AWS activity and changes\n- Assessing security posture\n- Implementing operational excellence\n\n## Cost Optimization Best Practices\n\n### Pre-Deployment Cost Estimation\n\n**Always estimate costs before deploying**:\n1. Use **AWS Pricing MCP** to estimate resource costs\n2. Compare pricing across different regions\n3. Evaluate alternative service options\n4. Calculate expected monthly costs\n5. Plan for scaling and growth\n\n**Example workflow**:\n```\n\"Estimate the monthly cost of running a Lambda function with\n1 million invocations, 512MB memory, 3-second duration in us-east-1\"\n```\n\n### Cost Analysis and Optimization\n\n**Regular cost reviews**:\n1. Use **Cost Explorer MCP** to analyze spending trends\n2. Identify cost anomalies and unexpected charges\n3. Review costs by service, region, and environment\n4. Compare actual vs. budgeted costs\n5. Generate cost optimization recommendations\n\n**Cost optimization strategies**:\n- Right-size over-provisioned resources\n- Use appropriate storage classes (S3, EBS)\n- Implement auto-scaling for dynamic workloads\n- Leverage Savings Plans and Reserved Instances\n- Delete unused resources and snapshots\n- Use cost allocation tags effectively\n\n### Budget Monitoring\n\n**Track spending against budgets**:\n1. Use **Billing and Cost Management MCP** to monitor budgets\n2. Set up budget alerts for threshold breaches\n3. Review budget utilization regularly\n4. Adjust budgets based on trends\n5. Implement cost controls and governance\n\n## Monitoring and Observability Best Practices\n\n### CloudWatch Metrics and Alarms\n\n**Implement comprehensive monitoring**:\n1. Use **CloudWatch MCP** to query metrics and logs\n2. Set up alarms for critical metrics:\n   - CPU and memory utilization\n   - Error rates and latency\n   - Queue depths and processing times\n   - API gateway throttling\n   - Lambda errors and timeouts\n3. Create CloudWatch dashboards for visualization\n4. Use log insights for troubleshooting\n\n**Example alarm scenarios**:\n- Lambda error rate > 1%\n- EC2 CPU utilization > 80%\n- API Gateway 4xx/5xx error spike\n- DynamoDB throttled requests\n- ECS task failures\n\n### Application Performance Monitoring\n\n**Monitor application health**:\n1. Use **CloudWatch Application Signals MCP** for APM\n2. Track service-level objectives (SLOs)\n3. Monitor application dependencies\n4. Identify performance bottlenecks\n5. Set up distributed tracing\n\n### Container and Kubernetes Monitoring\n\n**For containerized workloads**:\n1. Use **AWS Managed Prometheus MCP** for metrics\n2. Monitor container resource utilization\n3. Track pod and node health\n4. Create PromQL queries for custom metrics\n5. Set up alerts for container anomalies\n\n## Audit and Security Best Practices\n\n### CloudTrail Activity Analysis\n\n**Audit AWS activity**:\n1. Use **CloudTrail MCP** to analyze API activity\n2. Track who made changes to resources\n3. Investigate security incidents\n4. Monitor for suspicious activity patterns\n5. Audit compliance with policies\n\n**Common audit scenarios**:\n- \"Who deleted this S3 bucket?\"\n- \"Show all IAM role changes in the last 24 hours\"\n- \"List failed login attempts\"\n- \"Find all actions by a specific user\"\n- \"Track modifications to security groups\"\n\n### Security Assessment\n\n**Regular security reviews**:\n1. Use **Well-Architected Security Assessment MCP**\n2. Assess security posture against best practices\n3. Identify security gaps and vulnerabilities\n4. Implement recommended security improvements\n5. Document security compliance\n\n**Security assessment areas**:\n- Identity and Access Management (IAM)\n- Detective controls and monitoring\n- Infrastructure protection\n- Data protection and encryption\n- Incident response preparedness\n\n## Using MCP Servers Effectively\n\n### Cost Analysis Workflow\n\n1. **Pre-deployment**: Use Pricing MCP to estimate costs\n2. **Post-deployment**: Use Billing MCP to track actual spending\n3. **Analysis**: Use Cost Explorer MCP for detailed cost analysis\n4. **Optimization**: Implement recommendations from Cost Explorer\n\n### Monitoring Workflow\n\n1. **Setup**: Configure CloudWatch metrics and alarms\n2. **Monitor**: Use CloudWatch MCP to track key metrics\n3. **Analyze**: Use Application Signals for APM insights\n4. **Troubleshoot**: Query CloudWatch Logs for issue resolution\n\n### Security Workflow\n\n1. **Audit**: Use CloudTrail MCP to review activity\n2. **Assess**: Use Well-Architected Security Assessment\n3. **Remediate**: Implement security recommendations\n4. **Monitor**: Track security events via CloudWatch\n\n### MCP Usage Best Practices\n\n1. **Cost Awareness**: Check pricing before deploying resources\n2. **Proactive Monitoring**: Set up alarms for critical metrics\n3. **Regular Reviews**: Analyze costs and performance weekly\n4. **Audit Trails**: Review CloudTrail logs for compliance\n5. **Security First**: Run security assessments regularly\n6. **Optimize Continuously**: Act on cost and performance recommendations\n\n## Operational Excellence Guidelines\n\n### Cost Optimization\n\n- **Tag Everything**: Use consistent cost allocation tags\n- **Review Monthly**: Analyze spending trends and anomalies\n- **Right-size**: Match resources to actual usage\n- **Automate**: Use auto-scaling and scheduling\n- **Monitor Budgets**: Set alerts for cost overruns\n\n### Monitoring and Alerting\n\n- **Critical Metrics**: Alert on business-critical metrics\n- **Noise Reduction**: Fine-tune thresholds to reduce false positives\n- **Actionable Alerts**: Ensure alerts have clear remediation steps\n- **Dashboard Visibility**: Create dashboards for key stakeholders\n- **Log Retention**: Balance cost and compliance needs\n\n### Security and Compliance\n\n- **Least Privilege**: Grant minimum required permissions\n- **Audit Regularly**: Review CloudTrail logs for anomalies\n- **Encrypt Data**: Use encryption at rest and in transit\n- **Assess Continuously**: Run security assessments frequently\n- **Incident Response**: Have procedures for security events\n\n## Additional Resources\n\nFor detailed operational patterns and best practices, refer to the comprehensive reference:\n\n**File**: `references/operations-patterns.md`\n\nThis reference includes:\n- Cost optimization strategies\n- Monitoring and alerting patterns\n- Observability best practices\n- Security and compliance guidelines\n- Troubleshooting workflows\n\n## CloudWatch Alarms Reference\n\n**File**: `references/cloudwatch-alarms.md`\n\nCommon alarm configurations for:\n- Lambda functions\n- EC2 instances\n- RDS databases\n- DynamoDB tables\n- API Gateway\n- ECS services\n- Application Load Balancers\n",
        "skills/cloud-expense-management/references/cloudwatch-alarms.md": "# CloudWatch Alarms Reference\n\nCommon CloudWatch alarm configurations for AWS services.\n\n## Lambda Functions\n\n### Error Rate Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'LambdaErrorAlarm', {\n  metric: lambdaFunction.metricErrors({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 1,\n  treatMissingData: cloudwatch.TreatMissingData.NOT_BREACHING,\n  alarmDescription: 'Lambda error count exceeded threshold',\n});\n```\n\n### Duration Alarm (Approaching Timeout)\n```typescript\nnew cloudwatch.Alarm(this, 'LambdaDurationAlarm', {\n  metric: lambdaFunction.metricDuration({\n    statistic: 'Maximum',\n    period: Duration.minutes(5),\n  }),\n  threshold: lambdaFunction.timeout.toMilliseconds() * 0.8, // 80% of timeout\n  evaluationPeriods: 2,\n  alarmDescription: 'Lambda duration approaching timeout',\n});\n```\n\n### Throttle Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'LambdaThrottleAlarm', {\n  metric: lambdaFunction.metricThrottles({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 5,\n  evaluationPeriods: 1,\n  alarmDescription: 'Lambda function is being throttled',\n});\n```\n\n### Concurrent Executions Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'LambdaConcurrencyAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/Lambda',\n    metricName: 'ConcurrentExecutions',\n    dimensionsMap: {\n      FunctionName: lambdaFunction.functionName,\n    },\n    statistic: 'Maximum',\n    period: Duration.minutes(1),\n  }),\n  threshold: 100, // Adjust based on reserved concurrency\n  evaluationPeriods: 2,\n  alarmDescription: 'Lambda concurrent executions high',\n});\n```\n\n## API Gateway\n\n### 5XX Error Rate Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'Api5xxAlarm', {\n  metric: api.metricServerError({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 1,\n  alarmDescription: 'API Gateway 5XX errors exceeded threshold',\n});\n```\n\n### 4XX Error Rate Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'Api4xxAlarm', {\n  metric: api.metricClientError({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 50,\n  evaluationPeriods: 2,\n  alarmDescription: 'API Gateway 4XX errors exceeded threshold',\n});\n```\n\n### Latency Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ApiLatencyAlarm', {\n  metric: api.metricLatency({\n    statistic: 'p99',\n    period: Duration.minutes(5),\n  }),\n  threshold: 2000, // 2 seconds\n  evaluationPeriods: 2,\n  alarmDescription: 'API Gateway p99 latency exceeded threshold',\n});\n```\n\n## DynamoDB\n\n### Read Throttle Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'DynamoDBReadThrottleAlarm', {\n  metric: table.metricUserErrors({\n    dimensions: {\n      Operation: 'GetItem',\n    },\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 5,\n  evaluationPeriods: 1,\n  alarmDescription: 'DynamoDB read operations being throttled',\n});\n```\n\n### Write Throttle Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'DynamoDBWriteThrottleAlarm', {\n  metric: table.metricUserErrors({\n    dimensions: {\n      Operation: 'PutItem',\n    },\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 5,\n  evaluationPeriods: 1,\n  alarmDescription: 'DynamoDB write operations being throttled',\n});\n```\n\n### Consumed Capacity Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'DynamoDBCapacityAlarm', {\n  metric: table.metricConsumedReadCapacityUnits({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: provisionedCapacity * 0.8, // 80% of provisioned\n  evaluationPeriods: 2,\n  alarmDescription: 'DynamoDB consumed capacity approaching limit',\n});\n```\n\n## EC2 Instances\n\n### CPU Utilization Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'EC2CpuAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/EC2',\n    metricName: 'CPUUtilization',\n    dimensionsMap: {\n      InstanceId: instance.instanceId,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 80,\n  evaluationPeriods: 3,\n  alarmDescription: 'EC2 CPU utilization high',\n});\n```\n\n### Status Check Failed Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'EC2StatusCheckAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/EC2',\n    metricName: 'StatusCheckFailed',\n    dimensionsMap: {\n      InstanceId: instance.instanceId,\n    },\n    statistic: 'Maximum',\n    period: Duration.minutes(1),\n  }),\n  threshold: 1,\n  evaluationPeriods: 2,\n  alarmDescription: 'EC2 status check failed',\n});\n```\n\n### Disk Space Alarm (Requires CloudWatch Agent)\n```typescript\nnew cloudwatch.Alarm(this, 'EC2DiskAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'CWAgent',\n    metricName: 'disk_used_percent',\n    dimensionsMap: {\n      InstanceId: instance.instanceId,\n      path: '/',\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 85,\n  evaluationPeriods: 2,\n  alarmDescription: 'EC2 disk space usage high',\n});\n```\n\n## RDS Databases\n\n### CPU Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'RDSCpuAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/RDS',\n    metricName: 'CPUUtilization',\n    dimensionsMap: {\n      DBInstanceIdentifier: dbInstance.instanceIdentifier,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 80,\n  evaluationPeriods: 3,\n  alarmDescription: 'RDS CPU utilization high',\n});\n```\n\n### Connection Count Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'RDSConnectionAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/RDS',\n    metricName: 'DatabaseConnections',\n    dimensionsMap: {\n      DBInstanceIdentifier: dbInstance.instanceIdentifier,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: maxConnections * 0.8, // 80% of max connections\n  evaluationPeriods: 2,\n  alarmDescription: 'RDS connection count approaching limit',\n});\n```\n\n### Free Storage Space Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'RDSStorageAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/RDS',\n    metricName: 'FreeStorageSpace',\n    dimensionsMap: {\n      DBInstanceIdentifier: dbInstance.instanceIdentifier,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10 * 1024 * 1024 * 1024, // 10 GB in bytes\n  comparisonOperator: cloudwatch.ComparisonOperator.LESS_THAN_THRESHOLD,\n  evaluationPeriods: 1,\n  alarmDescription: 'RDS free storage space low',\n});\n```\n\n## ECS Services\n\n### Task Count Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ECSTaskCountAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'ECS/ContainerInsights',\n    metricName: 'RunningTaskCount',\n    dimensionsMap: {\n      ServiceName: service.serviceName,\n      ClusterName: cluster.clusterName,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1,\n  comparisonOperator: cloudwatch.ComparisonOperator.LESS_THAN_THRESHOLD,\n  evaluationPeriods: 2,\n  alarmDescription: 'ECS service has no running tasks',\n});\n```\n\n### CPU Utilization Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ECSCpuAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ECS',\n    metricName: 'CPUUtilization',\n    dimensionsMap: {\n      ServiceName: service.serviceName,\n      ClusterName: cluster.clusterName,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 80,\n  evaluationPeriods: 3,\n  alarmDescription: 'ECS service CPU utilization high',\n});\n```\n\n### Memory Utilization Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ECSMemoryAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ECS',\n    metricName: 'MemoryUtilization',\n    dimensionsMap: {\n      ServiceName: service.serviceName,\n      ClusterName: cluster.clusterName,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 85,\n  evaluationPeriods: 2,\n  alarmDescription: 'ECS service memory utilization high',\n});\n```\n\n## SQS Queues\n\n### Queue Depth Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'SQSDepthAlarm', {\n  metric: queue.metricApproximateNumberOfMessagesVisible({\n    statistic: 'Maximum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1000,\n  evaluationPeriods: 2,\n  alarmDescription: 'SQS queue depth exceeded threshold',\n});\n```\n\n### Age of Oldest Message Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'SQSAgeAlarm', {\n  metric: queue.metricApproximateAgeOfOldestMessage({\n    statistic: 'Maximum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 300, // 5 minutes in seconds\n  evaluationPeriods: 1,\n  alarmDescription: 'SQS messages not being processed timely',\n});\n```\n\n## Application Load Balancer\n\n### Target Health Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ALBUnhealthyTargetAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ApplicationELB',\n    metricName: 'UnHealthyHostCount',\n    dimensionsMap: {\n      LoadBalancer: loadBalancer.loadBalancerFullName,\n      TargetGroup: targetGroup.targetGroupFullName,\n    },\n    statistic: 'Average',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1,\n  evaluationPeriods: 2,\n  alarmDescription: 'ALB has unhealthy targets',\n});\n```\n\n### HTTP 5XX Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ALB5xxAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ApplicationELB',\n    metricName: 'HTTPCode_Target_5XX_Count',\n    dimensionsMap: {\n      LoadBalancer: loadBalancer.loadBalancerFullName,\n    },\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 1,\n  alarmDescription: 'ALB target 5XX errors exceeded threshold',\n});\n```\n\n### Response Time Alarm\n```typescript\nnew cloudwatch.Alarm(this, 'ALBLatencyAlarm', {\n  metric: new cloudwatch.Metric({\n    namespace: 'AWS/ApplicationELB',\n    metricName: 'TargetResponseTime',\n    dimensionsMap: {\n      LoadBalancer: loadBalancer.loadBalancerFullName,\n    },\n    statistic: 'p99',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1, // 1 second\n  evaluationPeriods: 2,\n  alarmDescription: 'ALB p99 response time exceeded threshold',\n});\n```\n\n## Composite Alarms\n\n### Service Health Composite Alarm\n```typescript\nconst errorAlarm = new cloudwatch.Alarm(this, 'ErrorAlarm', { /* ... */ });\nconst latencyAlarm = new cloudwatch.Alarm(this, 'LatencyAlarm', { /* ... */ });\nconst throttleAlarm = new cloudwatch.Alarm(this, 'ThrottleAlarm', { /* ... */ });\n\nnew cloudwatch.CompositeAlarm(this, 'ServiceHealthAlarm', {\n  compositeAlarmName: 'service-health',\n  alarmRule: cloudwatch.AlarmRule.anyOf(\n    errorAlarm,\n    latencyAlarm,\n    throttleAlarm\n  ),\n  alarmDescription: 'Overall service health degraded',\n});\n```\n\n## Alarm Actions\n\n### SNS Topic Integration\n```typescript\nconst topic = new sns.Topic(this, 'AlarmTopic', {\n  displayName: 'CloudWatch Alarms',\n});\n\n// Email subscription\ntopic.addSubscription(new subscriptions.EmailSubscription('ops@example.com'));\n\n// Add action to alarm\nalarm.addAlarmAction(new actions.SnsAction(topic));\nalarm.addOkAction(new actions.SnsAction(topic));\n```\n\n### Auto Scaling Action\n```typescript\nconst scalingAction = targetGroup.scaleOnMetric('ScaleUp', {\n  metric: targetGroup.metricTargetResponseTime(),\n  scalingSteps: [\n    { upper: 1, change: 0 },\n    { lower: 1, change: +1 },\n    { lower: 2, change: +2 },\n  ],\n});\n```\n\n## Alarm Best Practices\n\n### Threshold Selection\n\n**CPU/Memory Alarms**:\n- Warning: 70-80%\n- Critical: 80-90%\n- Consider burst patterns and normal usage\n\n**Error Rate Alarms**:\n- Threshold based on SLA (e.g., 99.9% = 0.1% error rate)\n- Account for normal error rates\n- Different thresholds for different error types\n\n**Latency Alarms**:\n- p99 latency for user-facing APIs\n- Warning: 80% of SLA target\n- Critical: 100% of SLA target\n\n### Evaluation Periods\n\n**Fast-changing metrics** (1-2 periods):\n- Error counts\n- Failed health checks\n- Critical application errors\n\n**Slow-changing metrics** (3-5 periods):\n- CPU utilization\n- Memory usage\n- Disk usage\n\n**Cost-related metrics** (longer periods):\n- Daily spending\n- Resource count changes\n- Usage patterns\n\n### Missing Data Handling\n\n```typescript\n// For intermittent workloads\nalarm.treatMissingData(cloudwatch.TreatMissingData.NOT_BREACHING);\n\n// For always-on services\nalarm.treatMissingData(cloudwatch.TreatMissingData.BREACHING);\n\n// To distinguish from data issues\nalarm.treatMissingData(cloudwatch.TreatMissingData.MISSING);\n```\n\n### Alarm Naming Conventions\n\n```typescript\n// Pattern: <service>-<metric>-<severity>\n'lambda-errors-critical'\n'api-latency-warning'\n'rds-cpu-warning'\n'ecs-tasks-critical'\n```\n\n### Alarm Actions Best Practices\n\n1. **Separate topics by severity**:\n   - Critical alarms  PagerDuty/on-call\n   - Warning alarms  Slack/email\n   - Info alarms  Metrics dashboard\n\n2. **Include context in alarm description**:\n   - Service name\n   - Expected threshold\n   - Troubleshooting runbook link\n\n3. **Auto-remediation where possible**:\n   - Lambda errors  automatic retry\n   - CPU high  auto-scaling trigger\n   - Disk full  automated cleanup\n\n4. **Alarm fatigue prevention**:\n   - Tune thresholds based on actual patterns\n   - Use composite alarms to reduce noise\n   - Implement proper evaluation periods\n   - Regularly review and adjust alarms\n\n## Monitoring Dashboard\n\n### Recommended Dashboard Layout\n\n**Service Overview**:\n- Request count and rate\n- Error count and percentage\n- Latency (p50, p95, p99)\n- Availability percentage\n\n**Resource Utilization**:\n- CPU utilization by service\n- Memory utilization by service\n- Network throughput\n- Disk I/O\n\n**Cost Metrics**:\n- Daily spending by service\n- Month-to-date costs\n- Budget utilization\n- Cost anomalies\n\n**Security Metrics**:\n- Failed login attempts\n- IAM policy changes\n- Security group modifications\n- GuardDuty findings\n",
        "skills/cloud-expense-management/references/operations-patterns.md": "# AWS Cost & Operations Patterns\n\nComprehensive patterns and best practices for AWS cost optimization, monitoring, and operational excellence.\n\n## Table of Contents\n\n- [Cost Optimization Patterns](#cost-optimization-patterns)\n- [Monitoring Patterns](#monitoring-patterns)\n- [Observability Patterns](#observability-patterns)\n- [Security and Audit Patterns](#security-and-audit-patterns)\n- [Troubleshooting Workflows](#troubleshooting-workflows)\n\n## Cost Optimization Patterns\n\n### Pattern 1: Cost Estimation Before Deployment\n\n**When**: Before deploying any new infrastructure\n\n**MCP Server**: AWS Pricing MCP\n\n**Steps**:\n1. List all resources to be deployed\n2. Query pricing for each resource type\n3. Calculate monthly costs based on expected usage\n4. Compare pricing across regions\n5. Document cost estimates in architecture docs\n\n**Example**:\n```\nResource: Lambda Function\n- Invocations: 1,000,000/month\n- Duration: 3 seconds avg\n- Memory: 512 MB\n- Region: us-east-1\nEstimated cost: $X/month\n```\n\n### Pattern 2: Monthly Cost Review\n\n**When**: First week of every month\n\n**MCP Servers**: Cost Explorer MCP, Billing and Cost Management MCP\n\n**Steps**:\n1. Review total spending vs. budget\n2. Analyze cost by service (top 5 services)\n3. Identify cost anomalies (>20% increase)\n4. Review cost by environment (dev/staging/prod)\n5. Check cost allocation tag coverage\n6. Generate cost optimization recommendations\n\n**Key Metrics**:\n- Month-over-month cost change\n- Cost per environment\n- Cost per application/project\n- Untagged resource costs\n\n### Pattern 3: Right-Sizing Resources\n\n**When**: Quarterly or when utilization alerts trigger\n\n**MCP Servers**: CloudWatch MCP, Cost Explorer MCP\n\n**Steps**:\n1. Query CloudWatch for resource utilization metrics\n2. Identify over-provisioned resources (< 40% utilization)\n3. Identify under-provisioned resources (> 80% utilization)\n4. Calculate potential savings from right-sizing\n5. Plan and execute right-sizing changes\n6. Monitor post-change performance\n\n**Common Right-Sizing Scenarios**:\n- EC2 instances with low CPU utilization\n- RDS instances with excess capacity\n- DynamoDB tables with low read/write usage\n- Lambda functions with excessive memory allocation\n\n### Pattern 4: Unused Resource Cleanup\n\n**When**: Monthly or triggered by cost anomalies\n\n**MCP Servers**: Cost Explorer MCP, CloudTrail MCP\n\n**Steps**:\n1. Identify resources with zero usage\n2. Query CloudTrail for last access time\n3. Tag resources for deletion review\n4. Notify resource owners\n5. Delete confirmed unused resources\n6. Track cost savings\n\n**Common Unused Resources**:\n- Unattached EBS volumes\n- Old EBS snapshots\n- Idle Load Balancers\n- Unused Elastic IPs\n- Old AMIs and snapshots\n- Stopped EC2 instances (long-term)\n\n## Monitoring Patterns\n\n### Pattern 1: Critical Service Monitoring\n\n**When**: All production services\n\n**MCP Server**: CloudWatch MCP\n\n**Metrics to Monitor**:\n- **Availability**: Service uptime, health checks\n- **Performance**: Latency, response time\n- **Errors**: Error rate, failed requests\n- **Saturation**: CPU, memory, disk, network utilization\n\n**Alarm Thresholds** (adjust based on SLAs):\n- Error rate: > 1% for 2 consecutive periods\n- Latency: p99 > 1 second for 5 minutes\n- CPU: > 80% for 10 minutes\n- Memory: > 85% for 5 minutes\n\n### Pattern 2: Lambda Function Monitoring\n\n**MCP Server**: CloudWatch MCP\n\n**Key Metrics**:\n```\n- Invocations (Count)\n- Errors (Count, %)\n- Duration (Average, p99)\n- Throttles (Count)\n- ConcurrentExecutions (Max)\n- IteratorAge (for stream processing)\n```\n\n**Recommended Alarms**:\n- Error rate > 1%\n- Duration > 80% of timeout\n- Throttles > 0\n- ConcurrentExecutions > 80% of reserved\n\n### Pattern 3: API Gateway Monitoring\n\n**MCP Server**: CloudWatch MCP\n\n**Key Metrics**:\n```\n- Count (Total requests)\n- 4XXError, 5XXError\n- Latency (p50, p95, p99)\n- IntegrationLatency\n- CacheHitCount, CacheMissCount\n```\n\n**Recommended Alarms**:\n- 5XX error rate > 0.5%\n- 4XX error rate > 5%\n- Latency p99 > 2 seconds\n- Integration latency spike\n\n### Pattern 4: Database Monitoring\n\n**MCP Server**: CloudWatch MCP\n\n**RDS Metrics**:\n```\n- CPUUtilization\n- DatabaseConnections\n- FreeableMemory\n- ReadLatency, WriteLatency\n- ReadIOPS, WriteIOPS\n- FreeStorageSpace\n```\n\n**DynamoDB Metrics**:\n```\n- ConsumedReadCapacityUnits\n- ConsumedWriteCapacityUnits\n- UserErrors\n- SystemErrors\n- ThrottledRequests\n```\n\n**Recommended Alarms**:\n- RDS CPU > 80% for 10 minutes\n- RDS connections > 80% of max\n- RDS free storage < 10 GB\n- DynamoDB throttled requests > 0\n- DynamoDB user errors spike\n\n## Observability Patterns\n\n### Pattern 1: Distributed Tracing Setup\n\n**MCP Server**: CloudWatch Application Signals MCP\n\n**Components**:\n1. **Service Map**: Visualize service dependencies\n2. **Traces**: Track requests across services\n3. **Metrics**: Monitor latency and errors per service\n4. **SLOs**: Define and track service level objectives\n\n**Implementation**:\n- Enable X-Ray tracing on Lambda functions\n- Add X-Ray SDK to application code\n- Configure sampling rules\n- Create service lens dashboards\n\n### Pattern 2: Log Aggregation and Analysis\n\n**MCP Server**: CloudWatch MCP\n\n**Log Strategy**:\n1. **Centralize Logs**: Send all application logs to CloudWatch Logs\n2. **Structure Logs**: Use JSON format for structured logging\n3. **Log Insights**: Use CloudWatch Logs Insights for queries\n4. **Retention**: Set appropriate retention periods\n\n**Example Log Insights Queries**:\n```\n# Find errors in last hour\nfields @timestamp, @message\n| filter @message like /ERROR/\n| sort @timestamp desc\n| limit 100\n\n# Count errors by type\nstats count() by error_type\n| sort count desc\n\n# Calculate p99 latency\nstats percentile(duration, 99) by service_name\n```\n\n### Pattern 3: Custom Metrics\n\n**MCP Server**: CloudWatch MCP\n\n**When to Use Custom Metrics**:\n- Business-specific KPIs (orders/minute, revenue/hour)\n- Application-specific metrics (cache hit rate, queue depth)\n- Performance metrics not provided by AWS\n\n**Best Practices**:\n- Use consistent namespace: `CompanyName/ApplicationName`\n- Include relevant dimensions (environment, region, version)\n- Publish metrics at appropriate intervals\n- Use metric filters for log-derived metrics\n\n## Security and Audit Patterns\n\n### Pattern 1: API Activity Auditing\n\n**MCP Server**: CloudTrail MCP\n\n**Regular Audit Queries**:\n```\n# Find all IAM changes\neventName: CreateUser, DeleteUser, AttachUserPolicy, etc.\nTime: Last 24 hours\n\n# Track S3 bucket deletions\neventName: DeleteBucket\nTime: Last 7 days\n\n# Find failed login attempts\neventName: ConsoleLogin\nerrorCode: Failure\n\n# Monitor privileged actions\nuserIdentity.arn: *admin* OR *root*\n```\n\n**Audit Schedule**:\n- Daily: Review privileged user actions\n- Weekly: Audit IAM changes and security group modifications\n- Monthly: Comprehensive security review\n\n### Pattern 2: Security Posture Assessment\n\n**MCP Server**: Well-Architected Security Assessment Tool MCP\n\n**Assessment Areas**:\n1. **Identity and Access Management**\n   - Least privilege implementation\n   - MFA enforcement\n   - Role-based access control\n   - Service control policies\n\n2. **Detective Controls**\n   - CloudTrail enabled in all regions\n   - GuardDuty findings review\n   - Config rule compliance\n   - Security Hub findings\n\n3. **Infrastructure Protection**\n   - VPC security groups review\n   - Network ACLs configuration\n   - AWS WAF rules\n   - Security group ingress rules\n\n4. **Data Protection**\n   - Encryption at rest (S3, EBS, RDS)\n   - Encryption in transit (TLS/SSL)\n   - KMS key usage and rotation\n   - Secrets Manager utilization\n\n5. **Incident Response**\n   - IR playbooks documented\n   - Automated response procedures\n   - Contact information current\n   - Regular IR drills\n\n**Assessment Frequency**:\n- Quarterly: Full Well-Architected review\n- Monthly: High-priority findings review\n- Weekly: Critical security findings\n\n### Pattern 3: Compliance Monitoring\n\n**MCP Servers**: CloudTrail MCP, CloudWatch MCP\n\n**Compliance Requirements**:\n- Data residency (ensure data stays in approved regions)\n- Access logging (all access logged and retained)\n- Encryption requirements (data encrypted at rest and in transit)\n- Change management (all changes tracked in CloudTrail)\n\n**Compliance Dashboards**:\n- Encryption coverage by service\n- CloudTrail logging status\n- Failed login attempts\n- Privileged access usage\n- Non-compliant resources\n\n## Troubleshooting Workflows\n\n### Workflow 1: High Lambda Error Rate\n\n**MCP Servers**: CloudWatch MCP, CloudWatch Application Signals MCP\n\n**Steps**:\n1. Query CloudWatch for Lambda error metrics\n2. Check error logs in CloudWatch Logs\n3. Identify error patterns (timeout, memory, permission)\n4. Check Lambda configuration (memory, timeout, permissions)\n5. Review recent code deployments\n6. Check downstream service health\n7. Implement fix and monitor\n\n### Workflow 2: Increased Latency\n\n**MCP Servers**: CloudWatch MCP, CloudWatch Application Signals MCP\n\n**Steps**:\n1. Identify latency spike in CloudWatch metrics\n2. Check service map for slow dependencies\n3. Query distributed traces for slow requests\n4. Check database query performance\n5. Review API Gateway integration latency\n6. Check Lambda cold starts\n7. Identify bottleneck and optimize\n\n### Workflow 3: Cost Spike Investigation\n\n**MCP Servers**: Cost Explorer MCP, CloudWatch MCP, CloudTrail MCP\n\n**Steps**:\n1. Use Cost Explorer to identify service causing spike\n2. Check CloudWatch metrics for usage increase\n3. Review CloudTrail for recent resource creation\n4. Identify root cause (misconfiguration, runaway process, attack)\n5. Implement cost controls (budgets, alarms, service quotas)\n6. Clean up unnecessary resources\n\n### Workflow 4: Security Incident Response\n\n**MCP Servers**: CloudTrail MCP, GuardDuty (via CloudWatch), Well-Architected Assessment MCP\n\n**Steps**:\n1. Identify security event in GuardDuty or CloudWatch\n2. Query CloudTrail for related API activity\n3. Determine scope and impact\n4. Isolate affected resources\n5. Revoke compromised credentials\n6. Implement remediation\n7. Conduct post-incident review\n8. Update security controls\n\n## Summary\n\n- **Cost Optimization**: Use Pricing, Cost Explorer, and Billing MCPs for proactive cost management\n- **Monitoring**: Set up comprehensive CloudWatch alarms for all critical services\n- **Observability**: Implement distributed tracing and structured logging\n- **Security**: Regular CloudTrail audits and Well-Architected assessments\n- **Proactive**: Don't wait for incidents - monitor and optimize continuously\n",
        "skills/collaborative-document-creation/SKILL.md": "---\nname: collaborative-document-creation\ndescription: Collaborate on document creation and refinement. Merges contributions, manages versions, and produces unified documents from multiple sources.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Doc Co-Authoring Workflow\n\nThis skill provides a structured workflow for guiding users through collaborative document creation. Act as an active guide, walking users through three stages: Context Gathering, Refinement & Structure, and Reader Testing.\n\n## When to Offer This Workflow\n\n**Trigger conditions:**\n- User mentions writing documentation: \"write a doc\", \"draft a proposal\", \"create a spec\", \"write up\"\n- User mentions specific doc types: \"PRD\", \"design doc\", \"decision doc\", \"RFC\"\n- User seems to be starting a substantial writing task\n\n**Initial offer:**\nOffer the user a structured workflow for co-authoring the document. Explain the three stages:\n\n1. **Context Gathering**: User provides all relevant context while Claude asks clarifying questions\n2. **Refinement & Structure**: Iteratively build each section through brainstorming and editing\n3. **Reader Testing**: Test the doc with a fresh Claude (no context) to catch blind spots before others read it\n\nExplain that this approach helps ensure the doc works well when others read it (including when they paste it into Claude). Ask if they want to try this workflow or prefer to work freeform.\n\nIf user declines, work freeform. If user accepts, proceed to Stage 1.\n\n## Stage 1: Context Gathering\n\n**Goal:** Close the gap between what the user knows and what Claude knows, enabling smart guidance later.\n\n### Initial Questions\n\nStart by asking the user for meta-context about the document:\n\n1. What type of document is this? (e.g., technical spec, decision doc, proposal)\n2. Who's the primary audience?\n3. What's the desired impact when someone reads this?\n4. Is there a template or specific format to follow?\n5. Any other constraints or context to know?\n\nInform them they can answer in shorthand or dump information however works best for them.\n\n**If user provides a template or mentions a doc type:**\n- Ask if they have a template document to share\n- If they provide a link to a shared document, use the appropriate integration to fetch it\n- If they provide a file, read it\n\n**If user mentions editing an existing shared document:**\n- Use the appropriate integration to read the current state\n- Check for images without alt-text\n- If images exist without alt-text, explain that when others use Claude to understand the doc, Claude won't be able to see them. Ask if they want alt-text generated. If so, request they paste each image into chat for descriptive alt-text generation.\n\n### Info Dumping\n\nOnce initial questions are answered, encourage the user to dump all the context they have. Request information such as:\n- Background on the project/problem\n- Related team discussions or shared documents\n- Why alternative solutions aren't being used\n- Organizational context (team dynamics, past incidents, politics)\n- Timeline pressures or constraints\n- Technical architecture or dependencies\n- Stakeholder concerns\n\nAdvise them not to worry about organizing it - just get it all out. Offer multiple ways to provide context:\n- Info dump stream-of-consciousness\n- Point to team channels or threads to read\n- Link to shared documents\n\n**If integrations are available** (e.g., Slack, Teams, Google Drive, SharePoint, or other MCP servers), mention that these can be used to pull in context directly.\n\n**If no integrations are detected and in Claude.ai or Claude app:** Suggest they can enable connectors in their Claude settings to allow pulling context from messaging apps and document storage directly.\n\nInform them clarifying questions will be asked once they've done their initial dump.\n\n**During context gathering:**\n\n- If user mentions team channels or shared documents:\n  - If integrations available: Inform them the content will be read now, then use the appropriate integration\n  - If integrations not available: Explain lack of access. Suggest they enable connectors in Claude settings, or paste the relevant content directly.\n\n- If user mentions entities/projects that are unknown:\n  - Ask if connected tools should be searched to learn more\n  - Wait for user confirmation before searching\n\n- As user provides context, track what's being learned and what's still unclear\n\n**Asking clarifying questions:**\n\nWhen user signals they've done their initial dump (or after substantial context provided), ask clarifying questions to ensure understanding:\n\nGenerate 5-10 numbered questions based on gaps in the context.\n\nInform them they can use shorthand to answer (e.g., \"1: yes, 2: see #channel, 3: no because backwards compat\"), link to more docs, point to channels to read, or just keep info-dumping. Whatever's most efficient for them.\n\n**Exit condition:**\nSufficient context has been gathered when questions show understanding - when edge cases and trade-offs can be asked about without needing basics explained.\n\n**Transition:**\nAsk if there's any more context they want to provide at this stage, or if it's time to move on to drafting the document.\n\nIf user wants to add more, let them. When ready, proceed to Stage 2.\n\n## Stage 2: Refinement & Structure\n\n**Goal:** Build the document section by section through brainstorming, curation, and iterative refinement.\n\n**Instructions to user:**\nExplain that the document will be built section by section. For each section:\n1. Clarifying questions will be asked about what to include\n2. 5-20 options will be brainstormed\n3. User will indicate what to keep/remove/combine\n4. The section will be drafted\n5. It will be refined through surgical edits\n\nStart with whichever section has the most unknowns (usually the core decision/proposal), then work through the rest.\n\n**Section ordering:**\n\nIf the document structure is clear:\nAsk which section they'd like to start with.\n\nSuggest starting with whichever section has the most unknowns. For decision docs, that's usually the core proposal. For specs, it's typically the technical approach. Summary sections are best left for last.\n\nIf user doesn't know what sections they need:\nBased on the type of document and template, suggest 3-5 sections appropriate for the doc type.\n\nAsk if this structure works, or if they want to adjust it.\n\n**Once structure is agreed:**\n\nCreate the initial document structure with placeholder text for all sections.\n\n**If access to artifacts is available:**\nUse `create_file` to create an artifact. This gives both Claude and the user a scaffold to work from.\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate artifact with all section headers and brief placeholder text like \"[To be written]\" or \"[Content here]\".\n\nProvide the scaffold link and indicate it's time to fill in each section.\n\n**If no access to artifacts:**\nCreate a markdown file in the working directory. Name it appropriately (e.g., `decision-doc.md`, `technical-spec.md`).\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate file with all section headers and placeholder text.\n\nConfirm the filename has been created and indicate it's time to fill in each section.\n\n**For each section:**\n\n### Step 1: Clarifying Questions\n\nAnnounce work will begin on the [SECTION NAME] section. Ask 5-10 clarifying questions about what should be included:\n\nGenerate 5-10 specific questions based on context and section purpose.\n\nInform them they can answer in shorthand or just indicate what's important to cover.\n\n### Step 2: Brainstorming\n\nFor the [SECTION NAME] section, brainstorm [5-20] things that might be included, depending on the section's complexity. Look for:\n- Context shared that might have been forgotten\n- Angles or considerations not yet mentioned\n\nGenerate 5-20 numbered options based on section complexity. At the end, offer to brainstorm more if they want additional options.\n\n### Step 3: Curation\n\nAsk which points should be kept, removed, or combined. Request brief justifications to help learn priorities for the next sections.\n\nProvide examples:\n- \"Keep 1,4,7,9\"\n- \"Remove 3 (duplicates 1)\"\n- \"Remove 6 (audience already knows this)\"\n- \"Combine 11 and 12\"\n\n**If user gives freeform feedback** (e.g., \"looks good\" or \"I like most of it but...\") instead of numbered selections, extract their preferences and proceed. Parse what they want kept/removed/changed and apply it.\n\n### Step 4: Gap Check\n\nBased on what they've selected, ask if there's anything important missing for the [SECTION NAME] section.\n\n### Step 5: Drafting\n\nUse `str_replace` to replace the placeholder text for this section with the actual drafted content.\n\nAnnounce the [SECTION NAME] section will be drafted now based on what they've selected.\n\n**If using artifacts:**\nAfter drafting, provide a link to the artifact.\n\nAsk them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**If using a file (no artifacts):**\nAfter drafting, confirm completion.\n\nInform them the [SECTION NAME] section has been drafted in [filename]. Ask them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**Key instruction for user (include when drafting the first section):**\nProvide a note: Instead of editing the doc directly, ask them to indicate what to change. This helps learning of their style for future sections. For example: \"Remove the X bullet - already covered by Y\" or \"Make the third paragraph more concise\".\n\n### Step 6: Iterative Refinement\n\nAs user provides feedback:\n- Use `str_replace` to make edits (never reprint the whole doc)\n- **If using artifacts:** Provide link to artifact after each edit\n- **If using files:** Just confirm edits are complete\n- If user edits doc directly and asks to read it: mentally note the changes they made and keep them in mind for future sections (this shows their preferences)\n\n**Continue iterating** until user is satisfied with the section.\n\n### Quality Checking\n\nAfter 3 consecutive iterations with no substantial changes, ask if anything can be removed without losing important information.\n\nWhen section is done, confirm [SECTION NAME] is complete. Ask if ready to move to the next section.\n\n**Repeat for all sections.**\n\n### Near Completion\n\nAs approaching completion (80%+ of sections done), announce intention to re-read the entire document and check for:\n- Flow and consistency across sections\n- Redundancy or contradictions\n- Anything that feels like \"slop\" or generic filler\n- Whether every sentence carries weight\n\nRead entire document and provide feedback.\n\n**When all sections are drafted and refined:**\nAnnounce all sections are drafted. Indicate intention to review the complete document one more time.\n\nReview for overall coherence, flow, completeness.\n\nProvide any final suggestions.\n\nAsk if ready to move to Reader Testing, or if they want to refine anything else.\n\n## Stage 3: Reader Testing\n\n**Goal:** Test the document with a fresh Claude (no context bleed) to verify it works for readers.\n\n**Instructions to user:**\nExplain that testing will now occur to see if the document actually works for readers. This catches blind spots - things that make sense to the authors but might confuse others.\n\n### Testing Approach\n\n**If access to sub-agents is available (e.g., in Claude Code):**\n\nPerform the testing directly without user involvement.\n\n### Step 1: Predict Reader Questions\n\nAnnounce intention to predict what questions readers might ask when trying to discover this document.\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Test with Sub-Agent\n\nAnnounce that these questions will be tested with a fresh Claude instance (no context from this conversation).\n\nFor each question, invoke a sub-agent with just the document content and the question.\n\nSummarize what Reader Claude got right/wrong for each question.\n\n### Step 3: Run Additional Checks\n\nAnnounce additional checks will be performed.\n\nInvoke sub-agent to check for ambiguity, false assumptions, contradictions.\n\nSummarize any issues found.\n\n### Step 4: Report and Fix\n\nIf issues found:\nReport that Reader Claude struggled with specific issues.\n\nList the specific issues.\n\nIndicate intention to fix these gaps.\n\nLoop back to refinement for problematic sections.\n\n---\n\n**If no access to sub-agents (e.g., claude.ai web interface):**\n\nThe user will need to do the testing manually.\n\n### Step 1: Predict Reader Questions\n\nAsk what questions people might ask when trying to discover this document. What would they type into Claude.ai?\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Setup Testing\n\nProvide testing instructions:\n1. Open a fresh Claude conversation: https://claude.ai\n2. Paste or share the document content (if using a shared doc platform with connectors enabled, provide the link)\n3. Ask Reader Claude the generated questions\n\nFor each question, instruct Reader Claude to provide:\n- The answer\n- Whether anything was ambiguous or unclear\n- What knowledge/context the doc assumes is already known\n\nCheck if Reader Claude gives correct answers or misinterprets anything.\n\n### Step 3: Additional Checks\n\nAlso ask Reader Claude:\n- \"What in this doc might be ambiguous or unclear to readers?\"\n- \"What knowledge or context does this doc assume readers already have?\"\n- \"Are there any internal contradictions or inconsistencies?\"\n\n### Step 4: Iterate Based on Results\n\nAsk what Reader Claude got wrong or struggled with. Indicate intention to fix those gaps.\n\nLoop back to refinement for any problematic sections.\n\n---\n\n### Exit Condition (Both Approaches)\n\nWhen Reader Claude consistently answers questions correctly and doesn't surface new gaps or ambiguities, the doc is ready.\n\n## Final Review\n\nWhen Reader Testing passes:\nAnnounce the doc has passed Reader Claude testing. Before completion:\n\n1. Recommend they do a final read-through themselves - they own this document and are responsible for its quality\n2. Suggest double-checking any facts, links, or technical details\n3. Ask them to verify it achieves the impact they wanted\n\nAsk if they want one more review, or if the work is done.\n\n**If user wants final review, provide it. Otherwise:**\nAnnounce document completion. Provide a few final tips:\n- Consider linking this conversation in an appendix so readers can see how the doc was developed\n- Use appendices to provide depth without bloating the main doc\n- Update the doc as feedback is received from real readers\n\n## Tips for Effective Guidance\n\n**Tone:**\n- Be direct and procedural\n- Explain rationale briefly when it affects user behavior\n- Don't try to \"sell\" the approach - just execute it\n\n**Handling Deviations:**\n- If user wants to skip a stage: Ask if they want to skip this and write freeform\n- If user seems frustrated: Acknowledge this is taking longer than expected. Suggest ways to move faster\n- Always give user agency to adjust the process\n\n**Context Management:**\n- Throughout, if context is missing on something mentioned, proactively ask\n- Don't let gaps accumulate - address them as they come up\n\n**Artifact Management:**\n- Use `create_file` for drafting full sections\n- Use `str_replace` for all edits\n- Provide artifact link after every change\n- Never use artifacts for brainstorming lists - that's just conversation\n\n**Quality over Speed:**\n- Don't rush through stages\n- Each iteration should make meaningful improvements\n- The goal is a document that actually works for readers\n",
        "skills/collaborative-ideation/SKILL.md": "---\nname: collaborative-ideation\ndescription: Facilitate collaborative idea exploration and refinement. Guides through iterative questioning, design validation, and solution architecture before implementation.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Brainstorming Ideas Into Designs\n\n## Overview\n\nHelp turn ideas into fully formed designs and specs through natural collaborative dialogue.\n\nStart by understanding the current project context, then ask questions one at a time to refine the idea. Once you understand what you're building, present the design in small sections (200-300 words), checking after each section whether it looks right so far.\n\n## The Process\n\n**Understanding the idea:**\n- Check out the current project state first (files, docs, recent commits)\n- Ask questions one at a time to refine the idea\n- Prefer multiple choice questions when possible, but open-ended is fine too\n- Only one question per message - if a topic needs more exploration, break it into multiple questions\n- Focus on understanding: purpose, constraints, success criteria\n\n**Exploring approaches:**\n- Propose 2-3 different approaches with trade-offs\n- Present options conversationally with your recommendation and reasoning\n- Lead with your recommended option and explain why\n\n**Presenting the design:**\n- Once you believe you understand what you're building, present the design\n- Break it into sections of 200-300 words\n- Ask after each section whether it looks right so far\n- Cover: architecture, components, data flow, error handling, testing\n- Be ready to go back and clarify if something doesn't make sense\n\n## After the Design\n\n**Documentation:**\n- Write the validated design to `docs/plans/YYYY-MM-DD-<topic>-design.md`\n- Use elements-of-style:writing-clearly-and-concisely skill if available\n- Commit the design document to git\n\n**Implementation (if continuing):**\n- Ask: \"Ready to set up for implementation?\"\n- Use superpowers:using-git-worktrees to create isolated workspace\n- Use superpowers:writing-plans to create detailed implementation plan\n\n## Key Principles\n\n- **One question at a time** - Don't overwhelm with multiple questions\n- **Multiple choice preferred** - Easier to answer than open-ended when possible\n- **YAGNI ruthlessly** - Remove unnecessary features from all designs\n- **Explore alternatives** - Always propose 2-3 approaches before settling\n- **Incremental validation** - Present design in sections, validate each\n- **Be flexible** - Go back and clarify when something doesn't make sense\n",
        "skills/component-interface-design/SKILL.md": "---\nname: component-interface-design\ndescription: Design interfaces using component libraries and design systems. Creates cohesive UIs with pre-built accessible component primitives.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Shadcn UI Designer\n\n## Core Design Prompt\n\nWhen designing any UI, apply this philosophy:\n\n> \"Design a modern, clean UI following Shadcn principles: apply minimalism with ample white space and simple sans-serif typography; use strategic, subtle shadows for depth and hierarchy; ensure accessibility with high-contrast neutrals and scalable elements; provide beautiful defaults for buttons, cards, and forms that compose modularly; incorporate fluid, non-intrusive animations; maintain a professional palette of soft grays, whites, and minimal accents like purple; output as responsive, customizable React code with Tailwind CSS.\"\n\n## Design Rules\n\n### 1. Typography Rule\n- Limit to **2-3 font weights and sizes** per screen\n- Use **Inter** or system fonts for consistency\n```tsx\n<h1 className=\"text-2xl font-semibold\">Title</h1>\n<p className=\"text-sm text-muted-foreground\">Description</p>\n```\n\n### 2. Spacing Rule\n- **4px-based scale**: 4px, 8px, 16px, 24px, 32px\n- Tailwind utilities: `p-1`, `p-2`, `p-4`, `p-6`, `p-8`\n```tsx\n<div className=\"p-6 space-y-4\">\n  <div className=\"mb-8\">...</div>\n</div>\n```\n\n### 3. Color Rule\n- Base on **OKLCH** for perceptual uniformity\n- Use **50-950 scale grays** (background, foreground, muted)\n- **Subtle accents** at 10% opacity to avoid visual noise\n```tsx\n<Card className=\"bg-card text-card-foreground border-border\">\n  <Button className=\"bg-primary text-primary-foreground\">Action</Button>\n  <div className=\"bg-primary/10\">Subtle accent</div>\n</Card>\n```\n\n### 4. Shadow Rule\n- **3 levels only**:\n  - `shadow-sm`: Subtle lift (0 1px 2px) - for cards\n  - `shadow-md`: Medium depth (0 4px 6px) - for dropdowns\n  - `shadow-lg`: High elevation (0 10px 15px) - for modals\n```tsx\n<Card className=\"shadow-sm hover:shadow-md transition-shadow\">\n```\n\n### 5. Animation Rule\n- **200-300ms durations**\n- **ease-in-out** curves for transitions\n- **Subtle feedback** only (hovers, state changes) - no decorative flourishes\n```tsx\n<Button className=\"transition-colors duration-200 hover:bg-primary/90\">\n<Card className=\"transition-transform duration-200 hover:scale-105\">\n```\n\n### 6. Accessibility Rule\n- **ARIA labels** on all interactive elements\n- **WCAG 2.1 AA** contrast ratios (4.5:1 minimum)\n- **Keyboard-focus styles** matching hover states\n- **Semantic HTML** structure\n```tsx\n<button\n  aria-label=\"Submit form\"\n  className=\"focus:ring-2 focus:ring-primary focus:outline-none\"\n>\n  Submit\n</button>\n```\n\n## Workflow\n\n### 1. Interview User (if details not provided)\n- **Scope**: Full page, section, or specific component?\n- **Type**: Dashboard, form, card, modal, table?\n- **Target file**: Where should this be implemented?\n- **Requirements**: Features, interactions, data to display?\n\n### 2. Design & Implement\n1. **Match existing design** - align with current UI patterns in the app\n2. **Build UI first** - complete visual interface before adding logic\n3. **Modular components** - break large pages into focused, reusable pieces\n4. **Apply all 6 rules** above strictly\n5. **Verify accessibility** - keyboard navigation, contrast, ARIA labels\n6. **Test responsiveness** - mobile, tablet, desktop\n\n### 3. Component Structure Pattern\n```tsx\nimport { Button } from \"@/components/ui/button\"\nimport { Card, CardHeader, CardTitle, CardContent } from \"@/components/ui/card\"\n\nexport function MyComponent() {\n  return (\n    <div className=\"container mx-auto p-6 space-y-6\">\n      <header>\n        <h1 className=\"text-2xl font-semibold\">Page Title</h1>\n      </header>\n\n      <main className=\"grid gap-4\">\n        <Card className=\"shadow-sm\">\n          <CardHeader>\n            <CardTitle>Section</CardTitle>\n          </CardHeader>\n          <CardContent>\n            {/* Content */}\n          </CardContent>\n        </Card>\n      </main>\n    </div>\n  )\n}\n```\n\n### 4. Quality Checklist\nBefore completing, verify:\n- [ ] Uses shadcn/ui components where applicable\n- [ ] 2-3 font weights/sizes max per screen\n- [ ] 4px-based spacing throughout\n- [ ] Theme color variables (no hardcoded colors)\n- [ ] 3 shadow levels max, strategically applied\n- [ ] Animations 200-300ms with ease-in-out\n- [ ] ARIA labels on interactive elements\n- [ ] WCAG AA contrast ratios (4.5:1 min)\n- [ ] Keyboard focus styles implemented\n- [ ] Mobile-first responsive design\n- [ ] Modular, reusable code structure\n\n## Common Patterns\n\n### Dashboard Page\n```tsx\n<div className=\"container mx-auto p-6 space-y-6\">\n  <header className=\"space-y-2\">\n    <h1 className=\"text-2xl font-semibold\">Dashboard</h1>\n    <p className=\"text-sm text-muted-foreground\">Overview of metrics</p>\n  </header>\n\n  <div className=\"grid gap-4 md:grid-cols-2 lg:grid-cols-4\">\n    {stats.map(stat => (\n      <Card key={stat.id} className=\"shadow-sm\">\n        <CardHeader className=\"pb-2\">\n          <CardTitle className=\"text-sm font-medium text-muted-foreground\">\n            {stat.title}\n          </CardTitle>\n        </CardHeader>\n        <CardContent>\n          <div className=\"text-2xl font-semibold\">{stat.value}</div>\n        </CardContent>\n      </Card>\n    ))}\n  </div>\n</div>\n```\n\n### Form Pattern\n```tsx\n<form className=\"space-y-6\">\n  <div className=\"space-y-4\">\n    <div className=\"space-y-2\">\n      <Label htmlFor=\"name\">Full Name</Label>\n      <Input id=\"name\" placeholder=\"John Doe\" />\n    </div>\n    <div className=\"space-y-2\">\n      <Label htmlFor=\"email\">Email</Label>\n      <Input id=\"email\" type=\"email\" placeholder=\"john@example.com\" />\n    </div>\n  </div>\n  <Button type=\"submit\" className=\"w-full transition-colors duration-200\">\n    Submit\n  </Button>\n</form>\n```\n\n### Data Table Pattern\n```tsx\n<Card className=\"shadow-sm\">\n  <CardHeader>\n    <CardTitle className=\"text-xl font-semibold\">Recent Orders</CardTitle>\n  </CardHeader>\n  <CardContent>\n    <Table>\n      <TableHeader>\n        <TableRow>\n          <TableHead>Order ID</TableHead>\n          <TableHead>Customer</TableHead>\n          <TableHead>Status</TableHead>\n        </TableRow>\n      </TableHeader>\n      <TableBody>\n        {orders.map(order => (\n          <TableRow key={order.id} className=\"hover:bg-muted/50 transition-colors\">\n            <TableCell className=\"font-medium\">{order.id}</TableCell>\n            <TableCell>{order.customer}</TableCell>\n            <TableCell>\n              <Badge variant=\"default\">{order.status}</Badge>\n            </TableCell>\n          </TableRow>\n        ))}\n      </TableBody>\n    </Table>\n  </CardContent>\n</Card>\n```\n\n## Best Practices\n\n- **Match existing design** - new designs align with current UI screens and components\n- **UI-first approach** - complete visual interface before adding business logic\n- **Modular code** - small, focused, reusable components (avoid monolithic pages)\n- **Token efficiency** - concise, well-structured code\n- **Consistency** - follow existing color, spacing, and typography patterns\n- **Composability** - build with shadcn's philosophy of small components that work together\n\n## Common Shadcn Components\n\n- **Layout**: Card, Tabs, Sheet, Dialog, Popover\n- **Forms**: Input, Textarea, Select, Checkbox, Radio, Switch, Label\n- **Buttons**: Button, Toggle, ToggleGroup\n- **Display**: Badge, Avatar, Separator, Skeleton, Table\n- **Feedback**: Alert, Toast, Progress\n- **Navigation**: NavigationMenu, Dropdown, Command\n\n## References\n\n- [Shadcn UI](https://ui.shadcn.com)\n- [Tailwind CSS v4](https://tailwindcss.com)\n- [WCAG 2.1](https://www.w3.org/WAI/WCAG21/quickref/)\n",
        "skills/content-harvest/SKILL.md": "---\nname: content-harvest\ndescription: Extract and parse article content from web sources. Retrieves text, metadata, and structured information from articles while preserving formatting and context.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Article Extractor\n\nThis skill extracts the main content from web articles and blog posts, removing navigation, ads, newsletter signups, and other clutter. Saves clean, readable text.\n\n## When to Use This Skill\n\nActivate when the user:\n- Provides an article/blog URL and wants the text content\n- Asks to \"download this article\"\n- Wants to \"extract the content from [URL]\"\n- Asks to \"save this blog post as text\"\n- Needs clean article text without distractions\n\n## How It Works\n\n### Priority Order:\n1. **Check if tools are installed** (reader or trafilatura)\n2. **Download and extract article** using best available tool\n3. **Clean up the content** (remove extra whitespace, format properly)\n4. **Save to file** with article title as filename\n5. **Confirm location** and show preview\n\n## Installation Check\n\nCheck for article extraction tools in this order:\n\n### Option 1: reader (Recommended - Mozilla's Readability)\n\n```bash\ncommand -v reader\n```\n\nIf not installed:\n```bash\nnpm install -g @mozilla/readability-cli\n# or\nnpm install -g reader-cli\n```\n\n### Option 2: trafilatura (Python-based, very good)\n\n```bash\ncommand -v trafilatura\n```\n\nIf not installed:\n```bash\npip3 install trafilatura\n```\n\n### Option 3: Fallback (curl + simple parsing)\n\nIf no tools available, use basic curl + text extraction (less reliable but works)\n\n## Extraction Methods\n\n### Method 1: Using reader (Best for most articles)\n\n```bash\n# Extract article\nreader \"URL\" > article.txt\n```\n\n**Pros:**\n- Based on Mozilla's Readability algorithm\n- Excellent at removing clutter\n- Preserves article structure\n\n### Method 2: Using trafilatura (Best for blogs/news)\n\n```bash\n# Extract article\ntrafilatura --URL \"URL\" --output-format txt > article.txt\n\n# Or with more options\ntrafilatura --URL \"URL\" --output-format txt --no-comments --no-tables > article.txt\n```\n\n**Pros:**\n- Very accurate extraction\n- Good with various site structures\n- Handles multiple languages\n\n**Options:**\n- `--no-comments`: Skip comment sections\n- `--no-tables`: Skip data tables\n- `--precision`: Favor precision over recall\n- `--recall`: Extract more content (may include some noise)\n\n### Method 3: Fallback (curl + basic parsing)\n\n```bash\n# Download and extract basic content\ncurl -s \"URL\" | python3 -c \"\nfrom html.parser import HTMLParser\nimport sys\n\nclass ArticleExtractor(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.in_content = False\n        self.content = []\n        self.skip_tags = {'script', 'style', 'nav', 'header', 'footer', 'aside'}\n        self.current_tag = None\n\n    def handle_starttag(self, tag, attrs):\n        if tag not in self.skip_tags:\n            if tag in {'p', 'article', 'main', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'}:\n                self.in_content = True\n        self.current_tag = tag\n\n    def handle_data(self, data):\n        if self.in_content and data.strip():\n            self.content.append(data.strip())\n\n    def get_content(self):\n        return '\\n\\n'.join(self.content)\n\nparser = ArticleExtractor()\nparser.feed(sys.stdin.read())\nprint(parser.get_content())\n\" > article.txt\n```\n\n**Note:** This is less reliable but works without dependencies.\n\n## Getting Article Title\n\nExtract title for filename:\n\n### Using reader:\n```bash\n# reader outputs markdown with title at top\nTITLE=$(reader \"URL\" | head -n 1 | sed 's/^# //')\n```\n\n### Using trafilatura:\n```bash\n# Get metadata including title\nTITLE=$(trafilatura --URL \"URL\" --json | python3 -c \"import json, sys; print(json.load(sys.stdin)['title'])\")\n```\n\n### Using curl (fallback):\n```bash\nTITLE=$(curl -s \"URL\" | grep -oP '<title>\\K[^<]+' | sed 's/ - .*//' | sed 's/ | .*//')\n```\n\n## Filename Creation\n\nClean title for filesystem:\n\n```bash\n# Get title\nTITLE=\"Article Title from Website\"\n\n# Clean for filesystem (remove special chars, limit length)\nFILENAME=$(echo \"$TITLE\" | tr '/' '-' | tr ':' '-' | tr '?' '' | tr '\"' '' | tr '<' '' | tr '>' '' | tr '|' '-' | cut -c 1-100 | sed 's/ *$//')\n\n# Add extension\nFILENAME=\"${FILENAME}.txt\"\n```\n\n## Complete Workflow\n\n```bash\nARTICLE_URL=\"https://example.com/article\"\n\n# Check for tools\nif command -v reader &> /dev/null; then\n    TOOL=\"reader\"\n    echo \"Using reader (Mozilla Readability)\"\nelif command -v trafilatura &> /dev/null; then\n    TOOL=\"trafilatura\"\n    echo \"Using trafilatura\"\nelse\n    TOOL=\"fallback\"\n    echo \"Using fallback method (may be less accurate)\"\nfi\n\n# Extract article\ncase $TOOL in\n    reader)\n        # Get content\n        reader \"$ARTICLE_URL\" > temp_article.txt\n\n        # Get title (first line after # in markdown)\n        TITLE=$(head -n 1 temp_article.txt | sed 's/^# //')\n        ;;\n\n    trafilatura)\n        # Get title from metadata\n        METADATA=$(trafilatura --URL \"$ARTICLE_URL\" --json)\n        TITLE=$(echo \"$METADATA\" | python3 -c \"import json, sys; print(json.load(sys.stdin).get('title', 'Article'))\")\n\n        # Get clean content\n        trafilatura --URL \"$ARTICLE_URL\" --output-format txt --no-comments > temp_article.txt\n        ;;\n\n    fallback)\n        # Get title\n        TITLE=$(curl -s \"$ARTICLE_URL\" | grep -oP '<title>\\K[^<]+' | head -n 1)\n        TITLE=${TITLE%% - *}  # Remove site name\n        TITLE=${TITLE%% | *}  # Remove site name (alternate)\n\n        # Get content (basic extraction)\n        curl -s \"$ARTICLE_URL\" | python3 -c \"\nfrom html.parser import HTMLParser\nimport sys\n\nclass ArticleExtractor(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.in_content = False\n        self.content = []\n        self.skip_tags = {'script', 'style', 'nav', 'header', 'footer', 'aside', 'form'}\n\n    def handle_starttag(self, tag, attrs):\n        if tag not in self.skip_tags:\n            if tag in {'p', 'article', 'main'}:\n                self.in_content = True\n        if tag in {'h1', 'h2', 'h3'}:\n            self.content.append('\\n')\n\n    def handle_data(self, data):\n        if self.in_content and data.strip():\n            self.content.append(data.strip())\n\n    def get_content(self):\n        return '\\n\\n'.join(self.content)\n\nparser = ArticleExtractor()\nparser.feed(sys.stdin.read())\nprint(parser.get_content())\n\" > temp_article.txt\n        ;;\nesac\n\n# Clean filename\nFILENAME=$(echo \"$TITLE\" | tr '/' '-' | tr ':' '-' | tr '?' '' | tr '\"' '' | tr '<>' '' | tr '|' '-' | cut -c 1-80 | sed 's/ *$//' | sed 's/^ *//')\nFILENAME=\"${FILENAME}.txt\"\n\n# Move to final filename\nmv temp_article.txt \"$FILENAME\"\n\n# Show result\necho \" Extracted article: $TITLE\"\necho \" Saved to: $FILENAME\"\necho \"\"\necho \"Preview (first 10 lines):\"\nhead -n 10 \"$FILENAME\"\n```\n\n## Error Handling\n\n### Common Issues\n\n**1. Tool not installed**\n- Try alternate tool (reader  trafilatura  fallback)\n- Offer to install: \"Install reader with: npm install -g reader-cli\"\n\n**2. Paywall or login required**\n- Extraction tools may fail\n- Inform user: \"This article requires authentication. Cannot extract.\"\n\n**3. Invalid URL**\n- Check URL format\n- Try with and without redirects\n\n**4. No content extracted**\n- Site may use heavy JavaScript\n- Try fallback method\n- Inform user if extraction fails\n\n**5. Special characters in title**\n- Clean title for filesystem\n- Remove: `/`, `:`, `?`, `\"`, `<`, `>`, `|`\n- Replace with `-` or remove\n\n## Output Format\n\n### Saved File Contains:\n- Article title (if available)\n- Author (if available from tool)\n- Main article text\n- Section headings\n- No navigation, ads, or clutter\n\n### What Gets Removed:\n- Navigation menus\n- Ads and promotional content\n- Newsletter signup forms\n- Related articles sidebars\n- Comment sections (optional)\n- Social media buttons\n- Cookie notices\n\n## Tips for Best Results\n\n**1. Use reader for most articles**\n- Best all-around tool\n- Based on Firefox Reader View\n- Works on most news sites and blogs\n\n**2. Use trafilatura for:**\n- Academic articles\n- News sites\n- Blogs with complex layouts\n- Non-English content\n\n**3. Fallback method limitations:**\n- May include some noise\n- Less accurate paragraph detection\n- Better than nothing for simple sites\n\n**4. Check extraction quality:**\n- Always show preview to user\n- Ask if it looks correct\n- Offer to try different tool if needed\n\n## Example Usage\n\n**Simple extraction:**\n```bash\n# User: \"Extract https://example.com/article\"\nreader \"https://example.com/article\" > temp.txt\nTITLE=$(head -n 1 temp.txt | sed 's/^# //')\nFILENAME=\"$(echo \"$TITLE\" | tr '/' '-').txt\"\nmv temp.txt \"$FILENAME\"\necho \" Saved to: $FILENAME\"\n```\n\n**With error handling:**\n```bash\nif ! reader \"$URL\" > temp.txt 2>/dev/null; then\n    if command -v trafilatura &> /dev/null; then\n        trafilatura --URL \"$URL\" --output-format txt > temp.txt\n    else\n        echo \"Error: Could not extract article. Install reader or trafilatura.\"\n        exit 1\n    fi\nfi\n```\n\n## Best Practices\n\n-  Always show preview after extraction (first 10 lines)\n-  Verify extraction succeeded before saving\n-  Clean filename for filesystem compatibility\n-  Try fallback method if primary fails\n-  Inform user which tool was used\n-  Keep filename length reasonable (< 100 chars)\n\n## After Extraction\n\nDisplay to user:\n1. \" Extracted: [Article Title]\"\n2. \" Saved to: [filename]\"\n3. Show preview (first 10-15 lines)\n4. File size and location\n\nAsk if needed:\n- \"Would you like me to also create a Ship-Learn-Next plan from this?\" (if using ship-learn-next skill)\n- \"Should I extract another article?\"",
        "skills/creative-generation-agent/README.md": "# Creative Generation Agent - Code Organization\n\nThis directory contains the refactored Creative Generation Agent skill with Python code extracted into modular, reusable files.\n\n## Directory Structure\n\n```\ncreative-generation-agent/\n README.md                      # This file\n SKILL.md                       # Main skill documentation\n examples/                      # Example implementations\n    music_generation.py        # Music composition and audio synthesis\n    meme_generator.py          # Image and text-based meme generation\n    podcast_producer.py        # Podcast scripting and audio production\n    image_generation.py        # Diffusion model image generation\n    style_transfer.py          # Neural style transfer\n scripts/                       # Utility modules\n     creative_quality_assessment.py  # Content quality evaluation\n     audio_effects.py           # Audio effect processing\n     content_moderation.py      # Safety and compliance filtering\n```\n\n## Quick Start Guide\n\n### Installation\n\n```bash\n# Install required dependencies\npip install music21 numpy scipy pillow diffusers torch opencv-python\n```\n\n### Music Generation\n\nGenerate melodies and synthesize audio:\n\n```python\nfrom examples.music_generation import MusicGenerationAgent, AudioSynthesisAgent\n\n# Create melody from seed notes\nagent = MusicGenerationAgent()\nmelody = agent.generate_melody(\n    seed_notes=[(\"C4\", 1), (\"E4\", 1), (\"G4\", 1)],\n    length=32,\n    temperature=0.8\n)\n\n# Synthesize audio\nsynth = AudioSynthesisAgent()\naudio = synth.synthesize_from_midi(midi_data)\naudio = synth.add_effects(audio, effect_type=\"reverb\")\nsynth.save_audio(audio, \"output.wav\")\n```\n\n### Meme Generation\n\nCreate memes with captions and text:\n\n```python\nfrom examples.meme_generator import MemeGenerationAgent, TextMemeGenerator\n\n# Image-based meme\nmeme_agent = MemeGenerationAgent()\nmeme = meme_agent.generate_meme(topic=\"AI agents\", meme_template=\"drake\")\nmeme.save(\"meme.png\")\n\n# Text-based meme\ntext_gen = TextMemeGenerator()\njoke = text_gen.generate_text_meme(topic=\"Python\", format_type=\"joke\")\nprint(joke)\n```\n\n### Podcast Generation\n\nCreate podcast scripts and produce audio:\n\n```python\nfrom examples.podcast_producer import PodcastScriptGenerator, PodcastAudioProducer\n\n# Generate script\ngenerator = PodcastScriptGenerator()\nepisode = generator.generate_episode(\n    topic=\"The Future of AI\",\n    duration_minutes=30,\n    num_hosts=2\n)\n\n# Produce audio\nproducer = PodcastAudioProducer()\naudio = producer.produce_podcast(episode[\"script\"])\n```\n\n### Image Generation\n\nGenerate images from text prompts:\n\n```python\nfrom examples.image_generation import ImageGenerationAgent\n\nagent = ImageGenerationAgent()\nimage = agent.generate_image(\n    prompt=\"A futuristic city with neon lights\",\n    style=\"cyberpunk\"\n)\nimage.save(\"generated.png\")\n\n# Create variations\nvariations = agent.generate_variations(image, num_variations=4)\n```\n\n### Style Transfer\n\nApply artistic styles to images:\n\n```python\nfrom examples.style_transfer import StyleTransferAgent\n\nagent = StyleTransferAgent()\nstylized = agent.transfer_style(\n    content_image=\"photo.jpg\",\n    style_image=\"monet.jpg\"\n)\n```\n\n## Utilities\n\n### Quality Assessment\n\nEvaluate content quality:\n\n```python\nfrom scripts.creative_quality_assessment import CreativeQualityAssessor\n\nassessor = CreativeQualityAssessor()\n\n# Assess different content types\nmusic_score = assessor.assess_content_quality(audio, content_type=\"music\")\nmeme_score = assessor.assess_content_quality(meme, content_type=\"meme\")\nimage_score = assessor.assess_content_quality(image, content_type=\"image\")\n\nprint(f\"Overall score: {music_score['overall_score']}\")\nprint(f\"Metrics: {music_score['metrics']}\")\n```\n\n### Audio Effects\n\nApply professional audio effects:\n\n```python\nfrom scripts.audio_effects import AudioEffects\n\neffects = AudioEffects(sample_rate=44100)\n\n# Apply various effects\naudio = effects.reverb(audio, room_size=0.5, delay_ms=50)\naudio = effects.compression(audio, threshold=0.5, ratio=4)\naudio = effects.fade_in(audio, duration_ms=1000)\n\n# Mix multiple tracks\nmixed = effects.mix(track1, track2, track3, volumes=[1.0, 0.5, 0.3])\n```\n\n### Content Moderation\n\nFilter and validate generated content:\n\n```python\nfrom scripts.content_moderation import ContentModerator\n\nmoderator = ContentModerator()\n\n# Moderate different content types\ntext_result = moderator.moderate_content(text, content_type=\"text\", strict=False)\nimage_result = moderator.moderate_content(image, content_type=\"image\")\nmeme_result = moderator.moderate_content(meme, content_type=\"meme\", strict=True)\n\nif text_result[\"passed\"]:\n    print(\"Content approved\")\nelse:\n    print(f\"Issues: {text_result['issues']}\")\n\n# Generate moderation report\nreport = moderator.get_moderation_report(text_result, format=\"text\")\nprint(report)\n```\n\n## Module Overview\n\n### examples/music_generation.py\n\n**Classes:**\n- `MusicGenerationAgent` - Symbolic music generation using continuation\n- `AudioSynthesisAgent` - Audio waveform synthesis with effects\n\n**Key Methods:**\n- `generate_melody()` - Generate melody continuation\n- `generate_full_composition()` - Create complete musical piece\n- `synthesize_from_midi()` - Convert MIDI to audio\n- `add_effects()` - Apply audio effects (reverb, chorus, delay)\n\n### examples/meme_generator.py\n\n**Classes:**\n- `MemeGenerationAgent` - Image-based meme generation\n- `TextMemeGenerator` - Text-only meme generation\n\n**Key Methods:**\n- `generate_meme()` - Create meme with caption\n- `generate_caption()` - Generate funny captions\n- `apply_caption_to_template()` - Add text to image\n- `generate_text_meme()` - Create text memes in various formats\n\n### examples/podcast_producer.py\n\n**Classes:**\n- `PodcastScriptGenerator` - Generate podcast scripts\n- `PodcastAudioProducer` - Produce audio from scripts\n\n**Key Methods:**\n- `generate_episode()` - Create complete episode\n- `generate_script()` - Generate script with structure\n- `generate_content_segments()` - Create discussion segments\n- `produce_podcast()` - Convert script to audio\n\n### examples/image_generation.py\n\n**Classes:**\n- `ImageGenerationAgent` - Text-to-image generation using diffusion models\n\n**Key Methods:**\n- `generate_image()` - Create image from prompt\n- `enhance_prompt()` - Add style modifiers to prompts\n- `generate_variations()` - Create image variations\n\n### examples/style_transfer.py\n\n**Classes:**\n- `StyleTransferAgent` - Neural style transfer\n\n**Key Methods:**\n- `transfer_style()` - Apply style from one image to another\n- `preprocess_image()` - Prepare image for processing\n- `postprocess_image()` - Convert output to displayable format\n\n### scripts/creative_quality_assessment.py\n\n**Classes:**\n- `CreativeQualityAssessor` - Quality evaluation for all content types\n\n**Quality Metrics:**\n- Music: coherence, variety, rhythm, harmony\n- Meme: humor, clarity, originality, relatability\n- Image: clarity, coherence, aesthetic, technical\n\n### scripts/audio_effects.py\n\n**Classes:**\n- `AudioEffects` - Professional audio effect processing\n\n**Available Effects:**\n- Reverb - Spatial depth and ambience\n- Echo - Repetition with decay\n- Chorus - Pitch modulation\n- Delay - Time-based effect\n- Compression - Dynamic range control\n- Normalization - Level adjustment\n- Fade In/Out - Smooth transitions\n- Equalization - Frequency adjustment\n- Mix - Combine multiple tracks\n\n### scripts/content_moderation.py\n\n**Classes:**\n- `ContentModerator` - Content safety and compliance checking\n\n**Moderation Features:**\n- Inappropriate language detection\n- Bias detection\n- Factual accuracy verification\n- NSFW content detection\n- Violence detection\n- Copyright checking\n- Hate speech detection\n- Offensive content detection\n\n## Best Practices\n\n### Temperature Control\n\nFor creative content:\n- **High temperature (0.7-0.9)**: More creative and diverse outputs\n- **Medium temperature (0.5-0.7)**: Balanced creativity and coherence\n- **Low temperature (0.1-0.3)**: More consistent and predictable\n\n### Audio Processing\n\n1. **Levels**: Keep peaks below -3dB for headroom\n2. **Effects Chain**: Apply compression before EQ\n3. **Mixing**: Use compression and limiting on master bus\n4. **Normalization**: Normalize after all effects\n\n### Content Generation\n\n1. Start with specific prompts\n2. Refine through iteration\n3. Assess quality systematically\n4. Moderate safety-critical content\n5. Version successful parameters\n\n### Quality Metrics\n\n- **Music**: Aim for coherence > 0.7 and variety > 0.6\n- **Memes**: Humor score > 0.6, clarity > 0.7\n- **Images**: Clarity > 0.8, aesthetic > 0.7\n\n## Integration Examples\n\n### Complete Music Production Pipeline\n\n```python\nfrom examples.music_generation import MusicGenerationAgent, AudioSynthesisAgent\nfrom scripts.audio_effects import AudioEffects\nfrom scripts.creative_quality_assessment import CreativeQualityAssessor\n\n# Generate music\nmusic_agent = MusicGenerationAgent()\ncomposition = music_agent.generate_full_composition()\n\n# Synthesize to audio\nsynth = AudioSynthesisAgent()\naudio = synth.synthesize_from_midi(composition)\n\n# Apply effects\neffects = AudioEffects()\naudio = effects.reverb(audio)\naudio = effects.compression(audio)\naudio = effects.normalize(audio)\n\n# Assess quality\nassessor = CreativeQualityAssessor()\nquality = assessor.assess_content_quality(audio, content_type=\"music\")\n\nsynth.save_audio(audio, \"final_track.wav\")\n```\n\n### Safe Content Generation\n\n```python\nfrom examples.meme_generator import MemeGenerationAgent\nfrom scripts.content_moderation import ContentModerator\n\n# Generate meme\nagent = MemeGenerationAgent()\nmeme = agent.generate_meme(topic=\"AI\")\n\n# Moderate content\nmoderator = ContentModerator()\nresult = moderator.moderate_content(meme, content_type=\"meme\", strict=True)\n\nif result[\"passed\"]:\n    meme.save(\"approved_meme.png\")\nelse:\n    print(f\"Content rejected: {result['issues']}\")\n```\n\n## Performance Considerations\n\n- Music generation: 30-60 seconds for 32-bar composition\n- Image generation: 30-120 seconds depending on inference steps\n- Audio synthesis: Real-time processing possible\n- Quality assessment: < 1 second per item\n\n## Extending the Code\n\nTo add new features:\n\n1. **Add effects**: Extend `AudioEffects` class\n2. **Add quality metrics**: Extend `CreativeQualityAssessor`\n3. **Add generation models**: Create new classes in `examples/`\n4. **Add moderation rules**: Extend `ContentModerator`\n\n## Requirements\n\n- Python 3.8+\n- numpy\n- scipy\n- pillow\n- music21\n- diffusers\n- torch\n- opencv-python (cv2)\n\n## References\n\n- [Music21 Documentation](https://web.mit.edu/music21/)\n- [Stable Diffusion](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n- [Audio Effects Guide](https://en.wikipedia.org/wiki/Audio_signal_processing)\n\n## License\n\nPart of the AI Agent Skills project\n",
        "skills/creative-generation-agent/SKILL.md": "---\nname: creative-generation-agent\ndescription: Build agents that generate creative content including music, memes, podcasts, and multimedia. Covers generative models, content synthesis, style transfer, and creative control. Use when building creative assistants, automated content creators, multimedia generators, or artistic AI systems.\n---\n\n# Creative Generation Agent\n\nBuild intelligent agents that generate original creative content across multiple modalities including text, music, images, memes, and podcasts.\n\n## Overview\n\nCreative generation combines:\n- **Content Models**: Diffusion models, transformers, GANs\n- **Prompt Engineering**: Guide creative output\n- **Style Control**: Maintain artistic consistency\n- **Quality Assessment**: Evaluate creative output\n- **Iteration & Refinement**: Improve results\n\n### Applications\n\n- AI music composition and arrangement\n- Automated meme generation\n- Podcast script and audio generation\n- Creative writing assistance\n- Art and image generation\n- Video content creation\n- Game asset generation\n\n## Quick Start\n\nExtract the code examples and utilities from the directories:\n\n- **Examples**: See [`examples/`](examples/) directory for complete implementations:\n  - [`music_generation.py`](examples/music_generation.py) - Music generation and audio synthesis\n  - [`meme_generator.py`](examples/meme_generator.py) - Image and text-based meme generation\n  - [`podcast_producer.py`](examples/podcast_producer.py) - Podcast script and audio production\n  - [`image_generation.py`](examples/image_generation.py) - Diffusion-based image generation\n  - [`style_transfer.py`](examples/style_transfer.py) - Neural style transfer\n\n- **Utilities**: See [`scripts/`](scripts/) directory for helper modules:\n  - [`creative_quality_assessment.py`](scripts/creative_quality_assessment.py) - Quality evaluation\n  - [`audio_effects.py`](scripts/audio_effects.py) - Audio effect processing\n  - [`content_moderation.py`](scripts/content_moderation.py) - Safety and compliance filtering\n\n## Music Generation\n\n### 1. Symbolic Music Generation\n\nGenerate music as MIDI/musical notation. See [`examples/music_generation.py`](examples/music_generation.py).\n\n**Key Classes:**\n- `MusicGenerationAgent` - Generates melodies and full compositions\n- Methods: `generate_melody()`, `generate_full_composition()`, `generate_harmony()`\n\n**Usage:**\n```python\nfrom examples.music_generation import MusicGenerationAgent\n\nagent = MusicGenerationAgent()\nmelody = agent.generate_melody(\n    seed_notes=[(\"C4\", 1), (\"E4\", 1), (\"G4\", 1)],\n    length=32,\n    temperature=0.8\n)\ncomposition = agent.generate_full_composition(style=\"classical\", duration_bars=32)\n```\n\n### 2. Audio Synthesis\n\nGenerate audio waveforms directly. See [`examples/music_generation.py`](examples/music_generation.py).\n\n**Key Classes:**\n- `AudioSynthesisAgent` - Synthesizes audio from MIDI and applies effects\n\n**Usage:**\n```python\nfrom examples.music_generation import AudioSynthesisAgent\n\nsynth = AudioSynthesisAgent(sample_rate=44100)\naudio = synth.synthesize_from_midi(midi_data, duration_seconds=60)\naudio = synth.add_effects(audio, effect_type=\"reverb\")\nsynth.save_audio(audio, \"output.wav\")\n```\n\n## Meme Generation\n\nSee [`examples/meme_generator.py`](examples/meme_generator.py) for complete implementations.\n\n### 1. Image-Based Meme Generator\n\nGenerate memes by applying captions to templates.\n\n**Key Classes:**\n- `MemeGenerationAgent` - Generates image-based memes with captions\n- Methods: `generate_meme()`, `generate_caption()`, `apply_caption_to_template()`\n\n**Usage:**\n```python\nfrom examples.meme_generator import MemeGenerationAgent\n\nagent = MemeGenerationAgent()\nmeme = agent.generate_meme(topic=\"AI agents\", meme_template=\"drake\")\nmeme.save(\"output_meme.png\")\n```\n\n### 2. Text-Based Meme Generator\n\nGenerate text-only memes in various formats.\n\n**Key Classes:**\n- `TextMemeGenerator` - Generates text-based memes\n- Methods: `generate_text_meme()`, `generate_joke_meme()`, `generate_deep_meme()`\n\n**Usage:**\n```python\nfrom examples.meme_generator import TextMemeGenerator\n\ngenerator = TextMemeGenerator()\njoke_meme = generator.generate_text_meme(topic=\"Python programming\", format_type=\"joke\")\ndeep_meme = generator.generate_text_meme(topic=\"AI\", format_type=\"deep\")\n```\n\n## Podcast Generation\n\nSee [`examples/podcast_producer.py`](examples/podcast_producer.py) for complete implementations.\n\n### 1. Script Generation\n\nGenerate podcast scripts with structure and natural conversation flow.\n\n**Key Classes:**\n- `PodcastScriptGenerator` - Creates scripts from topics\n- Methods: `generate_episode()`, `generate_script()`, `generate_content_segments()`, `generate_intro()`, `generate_outro()`\n\n**Usage:**\n```python\nfrom examples.podcast_producer import PodcastScriptGenerator\n\ngenerator = PodcastScriptGenerator()\nepisode = generator.generate_episode(\n    topic=\"Future of AI\",\n    duration_minutes=30,\n    num_hosts=2\n)\n\nprint(episode[\"script\"])\n```\n\n### 2. Audio Production\n\nConvert scripts to audio with text-to-speech and effects.\n\n**Key Classes:**\n- `PodcastAudioProducer` - Produces audio from podcast scripts\n- Methods: `produce_podcast()`, `text_to_speech()`, `add_background_music()`, `add_transitions()`\n\n**Usage:**\n```python\nfrom examples.podcast_producer import PodcastAudioProducer\n\nproducer = PodcastAudioProducer()\naudio = producer.produce_podcast(script_text)\n```\n\n## Image and Art Generation\n\nSee [`examples/image_generation.py`](examples/image_generation.py) and [`examples/style_transfer.py`](examples/style_transfer.py).\n\n### 1. Diffusion Model Integration\n\nGenerate images from text prompts using Stable Diffusion or similar models.\n\n**Key Classes:**\n- `ImageGenerationAgent` - Generates images from text prompts\n- Methods: `generate_image()`, `enhance_prompt()`, `generate_variations()`\n\n**Usage:**\n```python\nfrom examples.image_generation import ImageGenerationAgent\n\nagent = ImageGenerationAgent()\nimage = agent.generate_image(\n    prompt=\"A futuristic city with neon lights\",\n    style=\"cyberpunk\",\n    num_inference_steps=50\n)\nimage.save(\"generated_image.png\")\n\nvariations = agent.generate_variations(image, num_variations=4)\n```\n\n### 2. Style Transfer\n\nTransfer artistic style from one image to another.\n\n**Key Classes:**\n- `StyleTransferAgent` - Applies style transfer between images\n- Methods: `transfer_style()`, `preprocess_image()`, `postprocess_image()`\n\n**Usage:**\n```python\nfrom examples.style_transfer import StyleTransferAgent\n\nagent = StyleTransferAgent()\nstylized = agent.transfer_style(\n    content_image=\"photo.jpg\",\n    style_image=\"monet_painting.jpg\"\n)\n```\n\n## Quality Assessment\n\nSee [`scripts/creative_quality_assessment.py`](scripts/creative_quality_assessment.py) for complete implementations.\n\n### 1. Creative Quality Metrics\n\nEvaluate generated content across multiple quality dimensions.\n\n**Key Classes:**\n- `CreativeQualityAssessor` - Assesses quality of all content types\n- Methods: `assess_content_quality()`, `assess_music_quality()`, `assess_meme_quality()`, `assess_image_quality()`\n\n**Usage:**\n```python\nfrom scripts.creative_quality_assessment import CreativeQualityAssessor\n\nassessor = CreativeQualityAssessor()\n\n# Assess music quality\nmusic_assessment = assessor.assess_content_quality(audio, content_type=\"music\")\nprint(f\"Overall score: {music_assessment['overall_score']}\")\nprint(f\"Metrics: {music_assessment['metrics']}\")\n\n# Assess meme quality\nmeme_assessment = assessor.assess_content_quality(meme, content_type=\"meme\")\n\n# Assess image quality\nimage_assessment = assessor.assess_content_quality(image, content_type=\"image\")\n```\n\n## Best Practices\n\n### Content Generation\n-  Start with clear style/mood specifications\n-  Use temperature wisely (0.7-0.9 for creativity, 0.3-0.5 for consistency)\n-  Implement iterative refinement\n-  Maintain seed values for reproducibility\n-  Test with diverse prompts\n\n### Quality Control\n-  Assess generated content systematically (see [`creative_quality_assessment.py`](scripts/creative_quality_assessment.py))\n-  Implement human review loops\n-  Track quality metrics over time\n-  Use feedback to refine models\n-  Version different creative styles\n\n### Audio Processing\n-  Use audio effects wisely (see [`audio_effects.py`](scripts/audio_effects.py))\n  - Reverb for spatial depth\n  - Compression for dynamic control\n  - EQ for frequency balance\n  - Fade in/out for smooth transitions\n-  Monitor audio levels to prevent clipping\n-  Mix multiple tracks appropriately\n\n### Content Moderation\n-  Filter inappropriate content (see [`content_moderation.py`](scripts/content_moderation.py))\n-  Ensure copyright compliance\n-  Validate factual accuracy\n-  Check for bias in generation\n-  Implement safety guidelines\n-  Use strict mode for sensitive applications\n\n## Implementation Checklist\n\n- [ ] Choose content modality (music, images, text, etc.)\n- [ ] Select generation model/framework\n- [ ] Implement prompt engineering\n- [ ] Set up quality assessment metrics\n- [ ] Create iterative refinement loop\n- [ ] Build content moderation system\n- [ ] Test generation across diverse inputs\n- [ ] Optimize for speed/quality tradeoff\n- [ ] Implement version control for outputs\n- [ ] Document prompting strategies\n\n## Resources\n\n### Music Generation\n- **Music Transformer**: https://magenta.tensorflow.org/\n- **MuseNet**: https://openai.com/blog/musenet/\n- **music21**: https://web.mit.edu/music21/\n\n### Image Generation\n- **Stable Diffusion**: https://huggingface.co/runwayml/stable-diffusion-v1-5\n- **DALL-E**: https://openai.com/dall-e/\n- **Midjourney**: https://www.midjourney.com/\n\n### Audio Synthesis\n- **Jukebox**: https://openai.com/research/jukebox\n- **Chirp**: https://deepmind.google/discover/blog/chirp-universal-speech-model/\n\n### Video Generation\n- **Runway**: https://runwayml.com/\n- **Pika**: https://pika.art/\n\n",
        "skills/data-organization-system/SKILL.md": "---\nname: data-organization-system\ndescription: Organize and categorize files into logical structures. Creates folder hierarchies, renames files systematically, and consolidates related data.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# File Organizer\n\nThis skill acts as your personal organization assistant, helping you maintain a clean, logical file structure across your computer without the mental overhead of constant manual organization.\n\n## When to Use This Skill\n\n- Your Downloads folder is a chaotic mess\n- You can't find files because they're scattered everywhere\n- You have duplicate files taking up space\n- Your folder structure doesn't make sense anymore\n- You want to establish better organization habits\n- You're starting a new project and need a good structure\n- You're cleaning up before archiving old projects\n\n## What This Skill Does\n\n1. **Analyzes Current Structure**: Reviews your folders and files to understand what you have\n2. **Finds Duplicates**: Identifies duplicate files across your system\n3. **Suggests Organization**: Proposes logical folder structures based on your content\n4. **Automates Cleanup**: Moves, renames, and organizes files with your approval\n5. **Maintains Context**: Makes smart decisions based on file types, dates, and content\n6. **Reduces Clutter**: Identifies old files you probably don't need anymore\n\n## How to Use\n\n### From Your Home Directory\n\n```\ncd ~\n```\n\nThen run Claude Code and ask for help:\n\n```\nHelp me organize my Downloads folder\n```\n\n```\nFind duplicate files in my Documents folder\n```\n\n```\nReview my project directories and suggest improvements\n```\n\n### Specific Organization Tasks\n\n```\nOrganize these downloads into proper folders based on what they are\n```\n\n```\nFind duplicate files and help me decide which to keep\n```\n\n```\nClean up old files I haven't touched in 6+ months\n```\n\n```\nCreate a better folder structure for my [work/projects/photos/etc]\n```\n\n## Instructions\n\nWhen a user requests file organization help:\n\n1. **Understand the Scope**\n   \n   Ask clarifying questions:\n   - Which directory needs organization? (Downloads, Documents, entire home folder?)\n   - What's the main problem? (Can't find things, duplicates, too messy, no structure?)\n   - Any files or folders to avoid? (Current projects, sensitive data?)\n   - How aggressively to organize? (Conservative vs. comprehensive cleanup)\n\n2. **Analyze Current State**\n   \n   Review the target directory:\n   ```bash\n   # Get overview of current structure\n   ls -la [target_directory]\n   \n   # Check file types and sizes\n   find [target_directory] -type f -exec file {} \\; | head -20\n   \n   # Identify largest files\n   du -sh [target_directory]/* | sort -rh | head -20\n   \n   # Count file types\n   find [target_directory] -type f | sed 's/.*\\.//' | sort | uniq -c | sort -rn\n   ```\n   \n   Summarize findings:\n   - Total files and folders\n   - File type breakdown\n   - Size distribution\n   - Date ranges\n   - Obvious organization issues\n\n3. **Identify Organization Patterns**\n   \n   Based on the files, determine logical groupings:\n   \n   **By Type**:\n   - Documents (PDFs, DOCX, TXT)\n   - Images (JPG, PNG, SVG)\n   - Videos (MP4, MOV)\n   - Archives (ZIP, TAR, DMG)\n   - Code/Projects (directories with code)\n   - Spreadsheets (XLSX, CSV)\n   - Presentations (PPTX, KEY)\n   \n   **By Purpose**:\n   - Work vs. Personal\n   - Active vs. Archive\n   - Project-specific\n   - Reference materials\n   - Temporary/scratch files\n   \n   **By Date**:\n   - Current year/month\n   - Previous years\n   - Very old (archive candidates)\n\n4. **Find Duplicates**\n   \n   When requested, search for duplicates:\n   ```bash\n   # Find exact duplicates by hash\n   find [directory] -type f -exec md5 {} \\; | sort | uniq -d\n   \n   # Find files with same name\n   find [directory] -type f -printf '%f\\n' | sort | uniq -d\n   \n   # Find similar-sized files\n   find [directory] -type f -printf '%s %p\\n' | sort -n\n   ```\n   \n   For each set of duplicates:\n   - Show all file paths\n   - Display sizes and modification dates\n   - Recommend which to keep (usually newest or best-named)\n   - **Important**: Always ask for confirmation before deleting\n\n5. **Propose Organization Plan**\n   \n   Present a clear plan before making changes:\n   \n   ```markdown\n   # Organization Plan for [Directory]\n   \n   ## Current State\n   - X files across Y folders\n   - [Size] total\n   - File types: [breakdown]\n   - Issues: [list problems]\n   \n   ## Proposed Structure\n   \n   ```\n   [Directory]/\n    Work/\n       Projects/\n       Documents/\n       Archive/\n    Personal/\n       Photos/\n       Documents/\n       Media/\n    Downloads/\n        To-Sort/\n        Archive/\n   ```\n   \n   ## Changes I'll Make\n   \n   1. **Create new folders**: [list]\n   2. **Move files**:\n      - X PDFs  Work/Documents/\n      - Y images  Personal/Photos/\n      - Z old files  Archive/\n   3. **Rename files**: [any renaming patterns]\n   4. **Delete**: [duplicates or trash files]\n   \n   ## Files Needing Your Decision\n   \n   - [List any files you're unsure about]\n   \n   Ready to proceed? (yes/no/modify)\n   ```\n\n6. **Execute Organization**\n   \n   After approval, organize systematically:\n   \n   ```bash\n   # Create folder structure\n   mkdir -p \"path/to/new/folders\"\n   \n   # Move files with clear logging\n   mv \"old/path/file.pdf\" \"new/path/file.pdf\"\n   \n   # Rename files with consistent patterns\n   # Example: \"YYYY-MM-DD - Description.ext\"\n   ```\n   \n   **Important Rules**:\n   - Always confirm before deleting anything\n   - Log all moves for potential undo\n   - Preserve original modification dates\n   - Handle filename conflicts gracefully\n   - Stop and ask if you encounter unexpected situations\n\n7. **Provide Summary and Maintenance Tips**\n   \n   After organizing:\n   \n   ```markdown\n   # Organization Complete! \n   \n   ## What Changed\n   \n   - Created [X] new folders\n   - Organized [Y] files\n   - Freed [Z] GB by removing duplicates\n   - Archived [W] old files\n   \n   ## New Structure\n   \n   [Show the new folder tree]\n   \n   ## Maintenance Tips\n   \n   To keep this organized:\n   \n   1. **Weekly**: Sort new downloads\n   2. **Monthly**: Review and archive completed projects\n   3. **Quarterly**: Check for new duplicates\n   4. **Yearly**: Archive old files\n   \n   ## Quick Commands for You\n   \n   ```bash\n   # Find files modified this week\n   find . -type f -mtime -7\n   \n   # Sort downloads by type\n   [custom command for their setup]\n   \n   # Find duplicates\n   [custom command]\n   ```\n   \n   Want to organize another folder?\n   ```\n\n## Examples\n\n### Example 1: Organizing Downloads (From Justin Dielmann)\n\n**User**: \"My Downloads folder is a mess with 500+ files. Help me organize it.\"\n\n**Process**:\n1. Analyzes Downloads folder\n2. Finds patterns: work docs, personal photos, installers, random PDFs\n3. Proposes structure:\n   - Downloads/\n     - Work/\n     - Personal/\n     - Installers/ (DMG, PKG files)\n     - Archive/\n     - ToSort/ (things needing decisions)\n4. Asks for confirmation\n5. Moves files intelligently based on content and names\n6. Results: 500 files  5 organized folders\n\n### Example 2: Finding and Removing Duplicates\n\n**User**: \"Find duplicate files in my Documents and help me decide which to keep.\"\n\n**Output**:\n```markdown\n# Found 23 Sets of Duplicates (156 MB total)\n\n## Duplicate Set 1: \"proposal.pdf\"\n- `/Documents/proposal.pdf` (2.3 MB, modified: 2024-03-15)\n- `/Documents/old/proposal.pdf` (2.3 MB, modified: 2024-03-15)\n- `/Desktop/proposal.pdf` (2.3 MB, modified: 2024-03-10)\n\n**Recommendation**: Keep `/Documents/proposal.pdf` (most recent in correct location)\nDelete the other 2 copies?\n\n[Continue for all duplicates...]\n```\n\n### Example 3: Restructuring Projects Folder\n\n**User**: \"Review my ~/Projects directory and suggest improvements.\"\n\n**Output**:\n```markdown\n# Analysis of ~/Projects\n\n## Current Structure Issues\n- Mix of active and archived projects (3+ years old)\n- No consistent naming convention\n- Some projects at root, others in random subfolders\n- Duplicate folders (project-name, project-name-old, project-name-v2)\n\n## Proposed Structure\n\n```\nProjects/\n Active/\n    client-work/\n    side-projects/\n    learning/\n Archive/\n    2022/\n    2023/\n    2024/\n Templates/\n```\n\n## Specific Changes\n1. Move 12 projects not touched since 2022  Archive/\n2. Consolidate 4 duplicate project folders\n3. Rename projects to consistent format: \"client-name-project-name\"\n4. Create Archive for old work\n\nWant me to implement this?\n```\n\n### Example 4: Organizing Photos by Date\n\n**User**: \"Organize my photo folders by year and month.\"\n\n**Output**: Creates structure like:\n```\nPhotos/\n 2023/\n    01-January/\n    02-February/\n    ...\n 2024/\n    01-January/\n    ...\n Unsorted/\n```\n\nThen moves photos based on EXIF data or file modification dates.\n\n## Common Organization Tasks\n\n### Downloads Cleanup\n```\nOrganize my Downloads folder - move documents to Documents, \nimages to Pictures, keep installers separate, and archive files \nolder than 3 months.\n```\n\n### Project Organization\n```\nReview my Projects folder structure and help me separate active \nprojects from old ones I should archive.\n```\n\n### Duplicate Removal\n```\nFind all duplicate files in my Documents folder and help me \ndecide which ones to keep.\n```\n\n### Desktop Cleanup\n```\nMy Desktop is covered in files. Help me organize everything into \nmy Documents folder properly.\n```\n\n### Photo Organization\n```\nOrganize all photos in this folder by date (year/month) based \non when they were taken.\n```\n\n### Work/Personal Separation\n```\nHelp me separate my work files from personal files across my \nDocuments folder.\n```\n\n## Pro Tips\n\n1. **Start Small**: Begin with one messy folder (like Downloads) to build trust\n2. **Regular Maintenance**: Run weekly cleanup on Downloads\n3. **Consistent Naming**: Use \"YYYY-MM-DD - Description\" format for important files\n4. **Archive Aggressively**: Move old projects to Archive instead of deleting\n5. **Keep Active Separate**: Maintain clear boundaries between active and archived work\n6. **Trust the Process**: Let Claude handle the cognitive load of where things go\n\n## Best Practices\n\n### Folder Naming\n- Use clear, descriptive names\n- Avoid spaces (use hyphens or underscores)\n- Be specific: \"client-proposals\" not \"docs\"\n- Use prefixes for ordering: \"01-current\", \"02-archive\"\n\n### File Naming\n- Include dates: \"2024-10-17-meeting-notes.md\"\n- Be descriptive: \"q3-financial-report.xlsx\"\n- Avoid version numbers in names (use version control instead)\n- Remove download artifacts: \"document-final-v2 (1).pdf\"  \"document.pdf\"\n\n### When to Archive\n- Projects not touched in 6+ months\n- Completed work that might be referenced later\n- Old versions after migration to new systems\n- Files you're hesitant to delete (archive first)\n\n## Related Use Cases\n\n- Setting up organization for a new computer\n- Preparing files for backup/archiving\n- Cleaning up before storage cleanup\n- Organizing shared team folders\n- Structuring new project directories\n\n",
        "skills/deep-research-agent/SKILL.md": "---\nname: deep-research-agent\ndescription: Build agents specialized in conducting thorough research, gathering information from multiple sources, and synthesizing findings. Covers research planning, source evaluation, and report generation. Use when automating market research, competitive analysis, literature reviews, or intelligence gathering.\n---\n\n# Deep Research Agent\n\nBuild intelligent autonomous research agents that systematically investigate topics, evaluate sources, synthesize findings, and produce comprehensive reports.\n\n## Research Workflow\n\n### Stage 1: Research Planning\nSee [examples/research_planner.py](examples/research_planner.py) for `ResearchPlanner`:\n- Define research questions\n- Identify key research areas\n- Plan information sources and evaluation criteria\n- Create research timeline\n\n### Stage 2: Source Gathering\nGather sources from multiple channels:\n- Academic databases (Google Scholar, PubMed, JSTOR)\n- News sources and publications\n- Industry reports and whitepapers\n- Web and social media searches\n- Expert interviews\n\n### Stage 3: Source Evaluation\nSee [examples/source_evaluator.py](examples/source_evaluator.py) for `SourceEvaluator`:\n- Evaluate author expertise\n- Assess publisher credibility\n- Check information recency\n- Identify potential biases\n\n### Stage 4: Information Extraction\nExtract structured data from sources:\n- Key findings and main points\n- Statistics and quantitative data\n- Expert opinions and perspectives\n- Emerging trends\n- Research gaps\n\n### Stage 5: Synthesis & Analysis\nSee [examples/research_synthesizer.py](examples/research_synthesizer.py) for `ResearchSynthesizer`:\n- Identify main conclusions\n- Organize supporting evidence\n- Identify conflicting viewpoints\n- Detect research gaps\n- Suggest future research directions\n\n### Stage 6: Report Generation\nSee [examples/research_report_generator.py](examples/research_report_generator.py) for `ResearchReportGenerator`:\n- Generate executive summaries\n- Format findings with evidence\n- Present conflicting views\n- Identify gaps and opportunities\n- Create comprehensive reports with citations\n\n## Research Agent Implementation\n\nBuild a comprehensive research agent by:\n1. Creating research plans with `ResearchPlanner`\n2. Gathering sources from multiple channels\n3. Evaluating sources with `SourceEvaluator`\n4. Extracting structured information\n5. Synthesizing findings with `ResearchSynthesizer`\n6. Generating reports with `ResearchReportGenerator`\n\n## Specialized Research Types\n\n### Market Research\nBuild market research capabilities:\n- Estimate market size and growth rates\n- Identify key competitors and market players\n- Analyze market segments and entry barriers\n- Identify opportunities and threats\n- Track industry trends\n\n### Competitive Intelligence\nBuild competitive intelligence analysis:\n- Identify direct and indirect competitors\n- Analyze competitor products and pricing\n- Estimate market share and positioning\n- Assess strengths and weaknesses\n- Track competitive strategies and moves\n\n### Literature Review\nBuild literature review automation:\n- Search academic databases systematically\n- Extract paper metadata and abstracts\n- Analyze contributions and methodologies\n- Identify key themes and connections\n- Generate literature review synthesis\n\n## Best Practices\n\n### Research Quality\n-  Use multiple reliable sources\n-  Cross-reference findings\n-  Evaluate source credibility\n-  Identify and acknowledge biases\n-  Document all sources\n\n### Depth & Scope\n-  Define clear research questions\n-  Set appropriate scope\n-  Balance breadth and depth\n-  Identify research gaps\n-  Suggest future directions\n\n### Synthesis & Analysis\n-  Organize findings logically\n-  Present supporting evidence\n-  Address conflicting views\n-  Draw evidence-based conclusions\n-  Avoid unsupported claims\n\n## Tools & Technologies\n\n### Academic Search\n- Google Scholar\n- PubMed\n- JSTOR\n- ArXiv\n- PapersWithCode\n\n### News & Web Search\n- NewsAPI\n- Bing News\n- Google News\n- RSS Feeds\n- Social Media APIs\n\n### Data Analysis\n- Pandas\n- NumPy\n- scikit-learn\n- Statistical tools\n\n## Getting Started\n\n1. Define research question\n2. Create research plan\n3. Gather sources\n4. Evaluate credibility\n5. Extract key information\n6. Identify patterns\n7. Synthesize findings\n8. Generate comprehensive report\n\n",
        "skills/delegated-development/SKILL.md": "---\nname: delegated-development\ndescription: Drive development using delegated agent workflows. Coordinates multi-agent task execution with proper supervision and result integration.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Subagent-Driven Development\n\nExecute plan by dispatching fresh subagent per task, with two-stage review after each: spec compliance review first, then code quality review.\n\n**Core principle:** Fresh subagent per task + two-stage review (spec then quality) = high quality, fast iteration\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Have implementation plan?\" [shape=diamond];\n    \"Tasks mostly independent?\" [shape=diamond];\n    \"Stay in this session?\" [shape=diamond];\n    \"subagent-driven-development\" [shape=box];\n    \"executing-plans\" [shape=box];\n    \"Manual execution or brainstorm first\" [shape=box];\n\n    \"Have implementation plan?\" -> \"Tasks mostly independent?\" [label=\"yes\"];\n    \"Have implementation plan?\" -> \"Manual execution or brainstorm first\" [label=\"no\"];\n    \"Tasks mostly independent?\" -> \"Stay in this session?\" [label=\"yes\"];\n    \"Tasks mostly independent?\" -> \"Manual execution or brainstorm first\" [label=\"no - tightly coupled\"];\n    \"Stay in this session?\" -> \"subagent-driven-development\" [label=\"yes\"];\n    \"Stay in this session?\" -> \"executing-plans\" [label=\"no - parallel session\"];\n}\n```\n\n**vs. Executing Plans (parallel session):**\n- Same session (no context switch)\n- Fresh subagent per task (no context pollution)\n- Two-stage review after each task: spec compliance first, then code quality\n- Faster iteration (no human-in-loop between tasks)\n\n## The Process\n\n```dot\ndigraph process {\n    rankdir=TB;\n\n    subgraph cluster_per_task {\n        label=\"Per Task\";\n        \"Dispatch implementer subagent (./implementer-prompt.md)\" [shape=box];\n        \"Implementer subagent asks questions?\" [shape=diamond];\n        \"Answer questions, provide context\" [shape=box];\n        \"Implementer subagent implements, tests, commits, self-reviews\" [shape=box];\n        \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\" [shape=box];\n        \"Spec reviewer subagent confirms code matches spec?\" [shape=diamond];\n        \"Implementer subagent fixes spec gaps\" [shape=box];\n        \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" [shape=box];\n        \"Code quality reviewer subagent approves?\" [shape=diamond];\n        \"Implementer subagent fixes quality issues\" [shape=box];\n        \"Mark task complete in TodoWrite\" [shape=box];\n    }\n\n    \"Read plan, extract all tasks with full text, note context, create TodoWrite\" [shape=box];\n    \"More tasks remain?\" [shape=diamond];\n    \"Dispatch final code reviewer subagent for entire implementation\" [shape=box];\n    \"Use superpowers:finishing-a-development-branch\" [shape=box style=filled fillcolor=lightgreen];\n\n    \"Read plan, extract all tasks with full text, note context, create TodoWrite\" -> \"Dispatch implementer subagent (./implementer-prompt.md)\";\n    \"Dispatch implementer subagent (./implementer-prompt.md)\" -> \"Implementer subagent asks questions?\";\n    \"Implementer subagent asks questions?\" -> \"Answer questions, provide context\" [label=\"yes\"];\n    \"Answer questions, provide context\" -> \"Dispatch implementer subagent (./implementer-prompt.md)\";\n    \"Implementer subagent asks questions?\" -> \"Implementer subagent implements, tests, commits, self-reviews\" [label=\"no\"];\n    \"Implementer subagent implements, tests, commits, self-reviews\" -> \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\";\n    \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\" -> \"Spec reviewer subagent confirms code matches spec?\";\n    \"Spec reviewer subagent confirms code matches spec?\" -> \"Implementer subagent fixes spec gaps\" [label=\"no\"];\n    \"Implementer subagent fixes spec gaps\" -> \"Dispatch spec reviewer subagent (./spec-reviewer-prompt.md)\" [label=\"re-review\"];\n    \"Spec reviewer subagent confirms code matches spec?\" -> \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" [label=\"yes\"];\n    \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" -> \"Code quality reviewer subagent approves?\";\n    \"Code quality reviewer subagent approves?\" -> \"Implementer subagent fixes quality issues\" [label=\"no\"];\n    \"Implementer subagent fixes quality issues\" -> \"Dispatch code quality reviewer subagent (./code-quality-reviewer-prompt.md)\" [label=\"re-review\"];\n    \"Code quality reviewer subagent approves?\" -> \"Mark task complete in TodoWrite\" [label=\"yes\"];\n    \"Mark task complete in TodoWrite\" -> \"More tasks remain?\";\n    \"More tasks remain?\" -> \"Dispatch implementer subagent (./implementer-prompt.md)\" [label=\"yes\"];\n    \"More tasks remain?\" -> \"Dispatch final code reviewer subagent for entire implementation\" [label=\"no\"];\n    \"Dispatch final code reviewer subagent for entire implementation\" -> \"Use superpowers:finishing-a-development-branch\";\n}\n```\n\n## Prompt Templates\n\n- `./implementer-prompt.md` - Dispatch implementer subagent\n- `./spec-reviewer-prompt.md` - Dispatch spec compliance reviewer subagent\n- `./code-quality-reviewer-prompt.md` - Dispatch code quality reviewer subagent\n\n## Example Workflow\n\n```\nYou: I'm using Subagent-Driven Development to execute this plan.\n\n[Read plan file once: docs/plans/feature-plan.md]\n[Extract all 5 tasks with full text and context]\n[Create TodoWrite with all tasks]\n\nTask 1: Hook installation script\n\n[Get Task 1 text and context (already extracted)]\n[Dispatch implementation subagent with full task text + context]\n\nImplementer: \"Before I begin - should the hook be installed at user or system level?\"\n\nYou: \"User level (~/.config/superpowers/hooks/)\"\n\nImplementer: \"Got it. Implementing now...\"\n[Later] Implementer:\n  - Implemented install-hook command\n  - Added tests, 5/5 passing\n  - Self-review: Found I missed --force flag, added it\n  - Committed\n\n[Dispatch spec compliance reviewer]\nSpec reviewer:  Spec compliant - all requirements met, nothing extra\n\n[Get git SHAs, dispatch code quality reviewer]\nCode reviewer: Strengths: Good test coverage, clean. Issues: None. Approved.\n\n[Mark Task 1 complete]\n\nTask 2: Recovery modes\n\n[Get Task 2 text and context (already extracted)]\n[Dispatch implementation subagent with full task text + context]\n\nImplementer: [No questions, proceeds]\nImplementer:\n  - Added verify/repair modes\n  - 8/8 tests passing\n  - Self-review: All good\n  - Committed\n\n[Dispatch spec compliance reviewer]\nSpec reviewer:  Issues:\n  - Missing: Progress reporting (spec says \"report every 100 items\")\n  - Extra: Added --json flag (not requested)\n\n[Implementer fixes issues]\nImplementer: Removed --json flag, added progress reporting\n\n[Spec reviewer reviews again]\nSpec reviewer:  Spec compliant now\n\n[Dispatch code quality reviewer]\nCode reviewer: Strengths: Solid. Issues (Important): Magic number (100)\n\n[Implementer fixes]\nImplementer: Extracted PROGRESS_INTERVAL constant\n\n[Code reviewer reviews again]\nCode reviewer:  Approved\n\n[Mark Task 2 complete]\n\n...\n\n[After all tasks]\n[Dispatch final code-reviewer]\nFinal reviewer: All requirements met, ready to merge\n\nDone!\n```\n\n## Advantages\n\n**vs. Manual execution:**\n- Subagents follow TDD naturally\n- Fresh context per task (no confusion)\n- Parallel-safe (subagents don't interfere)\n- Subagent can ask questions (before AND during work)\n\n**vs. Executing Plans:**\n- Same session (no handoff)\n- Continuous progress (no waiting)\n- Review checkpoints automatic\n\n**Efficiency gains:**\n- No file reading overhead (controller provides full text)\n- Controller curates exactly what context is needed\n- Subagent gets complete information upfront\n- Questions surfaced before work begins (not after)\n\n**Quality gates:**\n- Self-review catches issues before handoff\n- Two-stage review: spec compliance, then code quality\n- Review loops ensure fixes actually work\n- Spec compliance prevents over/under-building\n- Code quality ensures implementation is well-built\n\n**Cost:**\n- More subagent invocations (implementer + 2 reviewers per task)\n- Controller does more prep work (extracting all tasks upfront)\n- Review loops add iterations\n- But catches issues early (cheaper than debugging later)\n\n## Red Flags\n\n**Never:**\n- Skip reviews (spec compliance OR code quality)\n- Proceed with unfixed issues\n- Dispatch multiple implementation subagents in parallel (conflicts)\n- Make subagent read plan file (provide full text instead)\n- Skip scene-setting context (subagent needs to understand where task fits)\n- Ignore subagent questions (answer before letting them proceed)\n- Accept \"close enough\" on spec compliance (spec reviewer found issues = not done)\n- Skip review loops (reviewer found issues = implementer fixes = review again)\n- Let implementer self-review replace actual review (both are needed)\n- **Start code quality review before spec compliance is ** (wrong order)\n- Move to next task while either review has open issues\n\n**If subagent asks questions:**\n- Answer clearly and completely\n- Provide additional context if needed\n- Don't rush them into implementation\n\n**If reviewer finds issues:**\n- Implementer (same subagent) fixes them\n- Reviewer reviews again\n- Repeat until approved\n- Don't skip the re-review\n\n**If subagent fails task:**\n- Dispatch fix subagent with specific instructions\n- Don't try to fix manually (context pollution)\n\n## Integration\n\n**Required workflow skills:**\n- **superpowers:writing-plans** - Creates the plan this skill executes\n- **superpowers:requesting-code-review** - Code review template for reviewer subagents\n- **superpowers:finishing-a-development-branch** - Complete development after all tasks\n\n**Subagents should use:**\n- **superpowers:test-driven-development** - Subagents follow TDD for each task\n\n**Alternative workflow:**\n- **superpowers:executing-plans** - Use for parallel session instead of same-session execution\n",
        "skills/delegated-development/code-quality-reviewer-prompt.md": "# Code Quality Reviewer Prompt Template\n\nUse this template when dispatching a code quality reviewer subagent.\n\n**Purpose:** Verify implementation is well-built (clean, tested, maintainable)\n\n**Only dispatch after spec compliance review passes.**\n\n```\nTask tool (superpowers:code-reviewer):\n  Use template at requesting-code-review/code-reviewer.md\n\n  WHAT_WAS_IMPLEMENTED: [from implementer's report]\n  PLAN_OR_REQUIREMENTS: Task N from [plan-file]\n  BASE_SHA: [commit before task]\n  HEAD_SHA: [current commit]\n  DESCRIPTION: [task summary]\n```\n\n**Code reviewer returns:** Strengths, Issues (Critical/Important/Minor), Assessment\n",
        "skills/delegated-development/implementer-prompt.md": "# Implementer Subagent Prompt Template\n\nUse this template when dispatching an implementer subagent.\n\n```\nTask tool (general-purpose):\n  description: \"Implement Task N: [task name]\"\n  prompt: |\n    You are implementing Task N: [task name]\n\n    ## Task Description\n\n    [FULL TEXT of task from plan - paste it here, don't make subagent read file]\n\n    ## Context\n\n    [Scene-setting: where this fits, dependencies, architectural context]\n\n    ## Before You Begin\n\n    If you have questions about:\n    - The requirements or acceptance criteria\n    - The approach or implementation strategy\n    - Dependencies or assumptions\n    - Anything unclear in the task description\n\n    **Ask them now.** Raise any concerns before starting work.\n\n    ## Your Job\n\n    Once you're clear on requirements:\n    1. Implement exactly what the task specifies\n    2. Write tests (following TDD if task says to)\n    3. Verify implementation works\n    4. Commit your work\n    5. Self-review (see below)\n    6. Report back\n\n    Work from: [directory]\n\n    **While you work:** If you encounter something unexpected or unclear, **ask questions**.\n    It's always OK to pause and clarify. Don't guess or make assumptions.\n\n    ## Before Reporting Back: Self-Review\n\n    Review your work with fresh eyes. Ask yourself:\n\n    **Completeness:**\n    - Did I fully implement everything in the spec?\n    - Did I miss any requirements?\n    - Are there edge cases I didn't handle?\n\n    **Quality:**\n    - Is this my best work?\n    - Are names clear and accurate (match what things do, not how they work)?\n    - Is the code clean and maintainable?\n\n    **Discipline:**\n    - Did I avoid overbuilding (YAGNI)?\n    - Did I only build what was requested?\n    - Did I follow existing patterns in the codebase?\n\n    **Testing:**\n    - Do tests actually verify behavior (not just mock behavior)?\n    - Did I follow TDD if required?\n    - Are tests comprehensive?\n\n    If you find issues during self-review, fix them now before reporting.\n\n    ## Report Format\n\n    When done, report:\n    - What you implemented\n    - What you tested and test results\n    - Files changed\n    - Self-review findings (if any)\n    - Any issues or concerns\n```\n",
        "skills/delegated-development/spec-reviewer-prompt.md": "# Spec Compliance Reviewer Prompt Template\n\nUse this template when dispatching a spec compliance reviewer subagent.\n\n**Purpose:** Verify implementer built what was requested (nothing more, nothing less)\n\n```\nTask tool (general-purpose):\n  description: \"Review spec compliance for Task N\"\n  prompt: |\n    You are reviewing whether an implementation matches its specification.\n\n    ## What Was Requested\n\n    [FULL TEXT of task requirements]\n\n    ## What Implementer Claims They Built\n\n    [From implementer's report]\n\n    ## CRITICAL: Do Not Trust the Report\n\n    The implementer finished suspiciously quickly. Their report may be incomplete,\n    inaccurate, or optimistic. You MUST verify everything independently.\n\n    **DO NOT:**\n    - Take their word for what they implemented\n    - Trust their claims about completeness\n    - Accept their interpretation of requirements\n\n    **DO:**\n    - Read the actual code they wrote\n    - Compare actual implementation to requirements line by line\n    - Check for missing pieces they claimed to implement\n    - Look for extra features they didn't mention\n\n    ## Your Job\n\n    Read the implementation code and verify:\n\n    **Missing requirements:**\n    - Did they implement everything that was requested?\n    - Are there requirements they skipped or missed?\n    - Did they claim something works but didn't actually implement it?\n\n    **Extra/unneeded work:**\n    - Did they build things that weren't requested?\n    - Did they over-engineer or add unnecessary features?\n    - Did they add \"nice to haves\" that weren't in spec?\n\n    **Misunderstandings:**\n    - Did they interpret requirements differently than intended?\n    - Did they solve the wrong problem?\n    - Did they implement the right feature but wrong way?\n\n    **Verify by reading code, not by trusting report.**\n\n    Report:\n    -  Spec compliant (if everything matches after code inspection)\n    -  Issues found: [list specifically what's missing or extra, with file:line references]\n```\n",
        "skills/deployment-automation/SKILL.md": "---\nname: deployment-automation\ndescription: Automate deployment to Vercel platform. Manages deployment configuration, environment setup, and CI/CD integration.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Vercel Production Deploy Loop\n\n## Instructions\n\nWhen requested to deploy to Vercel production with automatic error fixing:\n\n1. **Initial Deployment Attempt**\n   - Run `vercel --prod` to start production deployment\n   - Wait for deployment to complete\n\n2. **Error Detection & Analysis**\n   - **CRITICAL**: Use Vercel MCP tool to fetch detailed logs:\n     - The MCP logs provide much more detail than CLI output\n   - Analyze the build logs to identify root cause:\n     - Build errors (TypeScript, ESLint, compilation)\n     - Runtime errors\n     - Environment variable issues\n     - Dependency problems\n     - Configuration issues\n   - Extract specific error messages\n\n3. **Error Fixing**\n   - Make minimal, targeted fixes to resolve the specific error\n\n4. **Retry Deployment**\n   - Run `vercel --prod` again with the fixes applied\n   - Repeat steps until deployment succeeds\n\n5. **Success Confirmation**\n   - Once deployment succeeds, report:\n     - Deployment URL\n     - All errors that were fixed\n     - Summary of changes made\n   - Ask if user wants to commit/push the fixes\n\n## Loop Exit Conditions\n\n-  Deployment succeeds\n-  SAME error occurs 5+ times (suggest manual intervention)\n-  User requests to stop\n\n## Best Practices\n- Make incremental fixes rather than large refactors\n- Preserve user's code style and patterns when fixing\n\n## Example Flow\n\n**User:** \"Deploy to production and fix any errors\"\n\n\n- Vercel MCP build logs are the PRIMARY source of error information\n- CLI output alone is insufficient for proper error diagnosis\n- Always wait for deployment to complete before fetching logs\n- If errors require user input (like API keys), prompt user immediately\n",
        "skills/design-system-generator/SKILL.md": "---\nname: design-system-generator\ndescription: Generate design systems and theme variations. Creates cohesive visual systems with color schemes, typography, and component styling.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n\n# Theme Factory Skill\n\nThis skill provides a curated collection of professional font and color themes themes, each with carefully selected color palettes and font pairings. Once a theme is chosen, it can be applied to any artifact.\n\n## Purpose\n\nTo apply consistent, professional styling to presentation slide decks, use this skill. Each theme includes:\n- A cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- A distinct visual identity suitable for different contexts and audiences\n\n## Usage Instructions\n\nTo apply styling to a slide deck or other artifact:\n\n1. **Show the theme showcase**: Display the `theme-showcase.pdf` file to allow users to see all available themes visually. Do not make any modifications to it; simply show the file for viewing.\n2. **Ask for their choice**: Ask which theme to apply to the deck\n3. **Wait for selection**: Get explicit confirmation about the chosen theme\n4. **Apply the theme**: Once a theme has been chosen, apply the selected theme's colors and fonts to the deck/artifact\n\n## Themes Available\n\nThe following 10 themes are available, each showcased in `theme-showcase.pdf`:\n\n1. **Ocean Depths** - Professional and calming maritime theme\n2. **Sunset Boulevard** - Warm and vibrant sunset colors\n3. **Forest Canopy** - Natural and grounded earth tones\n4. **Modern Minimalist** - Clean and contemporary grayscale\n5. **Golden Hour** - Rich and warm autumnal palette\n6. **Arctic Frost** - Cool and crisp winter-inspired theme\n7. **Desert Rose** - Soft and sophisticated dusty tones\n8. **Tech Innovation** - Bold and modern tech aesthetic\n9. **Botanical Garden** - Fresh and organic garden colors\n10. **Midnight Galaxy** - Dramatic and cosmic deep tones\n\n## Theme Details\n\nEach theme is defined in the `themes/` directory with complete specifications including:\n- Cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- Distinct visual identity suitable for different contexts and audiences\n\n## Application Process\n\nAfter a preferred theme is selected:\n1. Read the corresponding theme file from the `themes/` directory\n2. Apply the specified colors and fonts consistently throughout the deck\n3. Ensure proper contrast and readability\n4. Maintain the theme's visual identity across all slides\n\n## Create your Own Theme\nTo handle cases where none of the existing themes work for an artifact, create a custom theme. Based on provided inputs, generate a new theme similar to the ones above. Give the theme a similar name describing what the font/color combinations represent. Use any basic description provided to choose appropriate colors/fonts. After generating the theme, show it for review and verification. Following that, apply the theme as described above.\n",
        "skills/design-system-generator/themes/arctic-frost.md": "# Arctic Frost\n\nA cool and crisp winter-inspired theme that conveys clarity, precision, and professionalism.\n\n## Color Palette\n\n- **Ice Blue**: `#d4e4f7` - Light backgrounds and highlights\n- **Steel Blue**: `#4a6fa5` - Primary accent color\n- **Silver**: `#c0c0c0` - Metallic accent elements\n- **Crisp White**: `#fafafa` - Clean backgrounds and text\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nHealthcare presentations, technology solutions, winter sports, clean tech, pharmaceutical content.\n",
        "skills/design-system-generator/themes/botanical-garden.md": "# Botanical Garden\n\nA fresh and organic theme featuring vibrant garden-inspired colors for lively presentations.\n\n## Color Palette\n\n- **Fern Green**: `#4a7c59` - Rich natural green\n- **Marigold**: `#f9a620` - Bright floral accent\n- **Terracotta**: `#b7472a` - Earthy warm tone\n- **Cream**: `#f5f3ed` - Soft neutral backgrounds\n\n## Typography\n\n- **Headers**: DejaVu Serif Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nGarden centers, food presentations, farm-to-table content, botanical brands, natural products.\n",
        "skills/design-system-generator/themes/desert-rose.md": "# Desert Rose\n\nA soft and sophisticated theme with dusty, muted tones perfect for elegant presentations.\n\n## Color Palette\n\n- **Dusty Rose**: `#d4a5a5` - Soft primary color\n- **Clay**: `#b87d6d` - Earthy accent\n- **Sand**: `#e8d5c4` - Warm neutral backgrounds\n- **Deep Burgundy**: `#5d2e46` - Rich dark contrast\n\n## Typography\n\n- **Headers**: FreeSans Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nFashion presentations, beauty brands, wedding planning, interior design, boutique businesses.\n",
        "skills/design-system-generator/themes/forest-canopy.md": "# Forest Canopy\n\nA natural and grounded theme featuring earth tones inspired by dense forest environments.\n\n## Color Palette\n\n- **Forest Green**: `#2d4a2b` - Primary dark green\n- **Sage**: `#7d8471` - Muted green accent\n- **Olive**: `#a4ac86` - Light accent color\n- **Ivory**: `#faf9f6` - Backgrounds and text\n\n## Typography\n\n- **Headers**: FreeSerif Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nEnvironmental presentations, sustainability reports, outdoor brands, wellness content, organic products.\n",
        "skills/design-system-generator/themes/golden-hour.md": "# Golden Hour\n\nA rich and warm autumnal palette that creates an inviting and sophisticated atmosphere.\n\n## Color Palette\n\n- **Mustard Yellow**: `#f4a900` - Bold primary accent\n- **Terracotta**: `#c1666b` - Warm secondary color\n- **Warm Beige**: `#d4b896` - Neutral backgrounds\n- **Chocolate Brown**: `#4a403a` - Dark text and anchors\n\n## Typography\n\n- **Headers**: FreeSans Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nRestaurant presentations, hospitality brands, fall campaigns, cozy lifestyle content, artisan products.\n",
        "skills/design-system-generator/themes/midnight-galaxy.md": "# Midnight Galaxy\n\nA dramatic and cosmic theme with deep purples and mystical tones for impactful presentations.\n\n## Color Palette\n\n- **Deep Purple**: `#2b1e3e` - Rich dark base\n- **Cosmic Blue**: `#4a4e8f` - Mystical mid-tone\n- **Lavender**: `#a490c2` - Soft accent color\n- **Silver**: `#e6e6fa` - Light highlights and text\n\n## Typography\n\n- **Headers**: FreeSans Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nEntertainment industry, gaming presentations, nightlife venues, luxury brands, creative agencies.\n",
        "skills/design-system-generator/themes/modern-minimalist.md": "# Modern Minimalist\n\nA clean and contemporary theme with a sophisticated grayscale palette for maximum versatility.\n\n## Color Palette\n\n- **Charcoal**: `#36454f` - Primary dark color\n- **Slate Gray**: `#708090` - Medium gray for accents\n- **Light Gray**: `#d3d3d3` - Backgrounds and dividers\n- **White**: `#ffffff` - Text and clean backgrounds\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nTech presentations, architecture portfolios, design showcases, modern business proposals, data visualization.\n",
        "skills/design-system-generator/themes/ocean-depths.md": "# Ocean Depths\n\nA professional and calming maritime theme that evokes the serenity of deep ocean waters.\n\n## Color Palette\n\n- **Deep Navy**: `#1a2332` - Primary background color\n- **Teal**: `#2d8b8b` - Accent color for highlights and emphasis\n- **Seafoam**: `#a8dadc` - Secondary accent for lighter elements\n- **Cream**: `#f1faee` - Text and light backgrounds\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nCorporate presentations, financial reports, professional consulting decks, trust-building content.\n",
        "skills/design-system-generator/themes/sunset-boulevard.md": "# Sunset Boulevard\n\nA warm and vibrant theme inspired by golden hour sunsets, perfect for energetic and creative presentations.\n\n## Color Palette\n\n- **Burnt Orange**: `#e76f51` - Primary accent color\n- **Coral**: `#f4a261` - Secondary warm accent\n- **Warm Sand**: `#e9c46a` - Highlighting and backgrounds\n- **Deep Purple**: `#264653` - Dark contrast and text\n\n## Typography\n\n- **Headers**: DejaVu Serif Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nCreative pitches, marketing presentations, lifestyle brands, event promotions, inspirational content.\n",
        "skills/design-system-generator/themes/tech-innovation.md": "# Tech Innovation\n\nA bold and modern theme with high-contrast colors perfect for cutting-edge technology presentations.\n\n## Color Palette\n\n- **Electric Blue**: `#0066ff` - Vibrant primary accent\n- **Neon Cyan**: `#00ffff` - Bright highlight color\n- **Dark Gray**: `#1e1e1e` - Deep backgrounds\n- **White**: `#ffffff` - Clean text and contrast\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nTech startups, software launches, innovation showcases, AI/ML presentations, digital transformation content.\n",
        "skills/diagnostic-analysis/CREATION-LOG.md": "# Creation Log: Systematic Debugging Skill\n\nReference example of extracting, structuring, and bulletproofing a critical skill.\n\n## Source Material\n\nExtracted debugging framework from `/Users/jesse/.claude/CLAUDE.md`:\n- 4-phase systematic process (Investigation  Pattern Analysis  Hypothesis  Implementation)\n- Core mandate: ALWAYS find root cause, NEVER fix symptoms\n- Rules designed to resist time pressure and rationalization\n\n## Extraction Decisions\n\n**What to include:**\n- Complete 4-phase framework with all rules\n- Anti-shortcuts (\"NEVER fix symptom\", \"STOP and re-analyze\")\n- Pressure-resistant language (\"even if faster\", \"even if I seem in a hurry\")\n- Concrete steps for each phase\n\n**What to leave out:**\n- Project-specific context\n- Repetitive variations of same rule\n- Narrative explanations (condensed to principles)\n\n## Structure Following skill-creation/SKILL.md\n\n1. **Rich when_to_use** - Included symptoms and anti-patterns\n2. **Type: technique** - Concrete process with steps\n3. **Keywords** - \"root cause\", \"symptom\", \"workaround\", \"debugging\", \"investigation\"\n4. **Flowchart** - Decision point for \"fix failed\"  re-analyze vs add more fixes\n5. **Phase-by-phase breakdown** - Scannable checklist format\n6. **Anti-patterns section** - What NOT to do (critical for this skill)\n\n## Bulletproofing Elements\n\nFramework designed to resist rationalization under pressure:\n\n### Language Choices\n- \"ALWAYS\" / \"NEVER\" (not \"should\" / \"try to\")\n- \"even if faster\" / \"even if I seem in a hurry\"\n- \"STOP and re-analyze\" (explicit pause)\n- \"Don't skip past\" (catches the actual behavior)\n\n### Structural Defenses\n- **Phase 1 required** - Can't skip to implementation\n- **Single hypothesis rule** - Forces thinking, prevents shotgun fixes\n- **Explicit failure mode** - \"IF your first fix doesn't work\" with mandatory action\n- **Anti-patterns section** - Shows exactly what shortcuts look like\n\n### Redundancy\n- Root cause mandate in overview + when_to_use + Phase 1 + implementation rules\n- \"NEVER fix symptom\" appears 4 times in different contexts\n- Each phase has explicit \"don't skip\" guidance\n\n## Testing Approach\n\nCreated 4 validation tests following skills/meta/testing-skills-with-subagents:\n\n### Test 1: Academic Context (No Pressure)\n- Simple bug, no time pressure\n- **Result:** Perfect compliance, complete investigation\n\n### Test 2: Time Pressure + Obvious Quick Fix\n- User \"in a hurry\", symptom fix looks easy\n- **Result:** Resisted shortcut, followed full process, found real root cause\n\n### Test 3: Complex System + Uncertainty\n- Multi-layer failure, unclear if can find root cause\n- **Result:** Systematic investigation, traced through all layers, found source\n\n### Test 4: Failed First Fix\n- Hypothesis doesn't work, temptation to add more fixes\n- **Result:** Stopped, re-analyzed, formed new hypothesis (no shotgun)\n\n**All tests passed.** No rationalizations found.\n\n## Iterations\n\n### Initial Version\n- Complete 4-phase framework\n- Anti-patterns section\n- Flowchart for \"fix failed\" decision\n\n### Enhancement 1: TDD Reference\n- Added link to skills/testing/test-driven-development\n- Note explaining TDD's \"simplest code\"  debugging's \"root cause\"\n- Prevents confusion between methodologies\n\n## Final Outcome\n\nBulletproof skill that:\n-  Clearly mandates root cause investigation\n-  Resists time pressure rationalization\n-  Provides concrete steps for each phase\n-  Shows anti-patterns explicitly\n-  Tested under multiple pressure scenarios\n-  Clarifies relationship to TDD\n-  Ready for use\n\n## Key Insight\n\n**Most important bulletproofing:** Anti-patterns section showing exact shortcuts that feel justified in the moment. When Claude thinks \"I'll just add this one quick fix\", seeing that exact pattern listed as wrong creates cognitive friction.\n\n## Usage Example\n\nWhen encountering a bug:\n1. Load skill: skills/debugging/systematic-debugging\n2. Read overview (10 sec) - reminded of mandate\n3. Follow Phase 1 checklist - forced investigation\n4. If tempted to skip - see anti-pattern, stop\n5. Complete all phases - root cause found\n\n**Time investment:** 5-10 minutes\n**Time saved:** Hours of symptom-whack-a-mole\n\n---\n\n*Created: 2025-10-03*\n*Purpose: Reference example for skill extraction and bulletproofing*\n",
        "skills/diagnostic-analysis/SKILL.md": "---\nname: diagnostic-analysis\ndescription: Systematically debug and diagnose issues methodically. Uses diagnostic techniques to identify root causes and implement fixes.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Systematic Debugging\n\n## Overview\n\nRandom fixes waste time and create new bugs. Quick patches mask underlying issues.\n\n**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.\n\n**Violating the letter of this process is violating the spirit of debugging.**\n\n## The Iron Law\n\n```\nNO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST\n```\n\nIf you haven't completed Phase 1, you cannot propose fixes.\n\n## When to Use\n\nUse for ANY technical issue:\n- Test failures\n- Bugs in production\n- Unexpected behavior\n- Performance problems\n- Build failures\n- Integration issues\n\n**Use this ESPECIALLY when:**\n- Under time pressure (emergencies make guessing tempting)\n- \"Just one quick fix\" seems obvious\n- You've already tried multiple fixes\n- Previous fix didn't work\n- You don't fully understand the issue\n\n**Don't skip when:**\n- Issue seems simple (simple bugs have root causes too)\n- You're in a hurry (rushing guarantees rework)\n- Manager wants it fixed NOW (systematic is faster than thrashing)\n\n## The Four Phases\n\nYou MUST complete each phase before proceeding to the next.\n\n### Phase 1: Root Cause Investigation\n\n**BEFORE attempting ANY fix:**\n\n1. **Read Error Messages Carefully**\n   - Don't skip past errors or warnings\n   - They often contain the exact solution\n   - Read stack traces completely\n   - Note line numbers, file paths, error codes\n\n2. **Reproduce Consistently**\n   - Can you trigger it reliably?\n   - What are the exact steps?\n   - Does it happen every time?\n   - If not reproducible  gather more data, don't guess\n\n3. **Check Recent Changes**\n   - What changed that could cause this?\n   - Git diff, recent commits\n   - New dependencies, config changes\n   - Environmental differences\n\n4. **Gather Evidence in Multi-Component Systems**\n\n   **WHEN system has multiple components (CI  build  signing, API  service  database):**\n\n   **BEFORE proposing fixes, add diagnostic instrumentation:**\n   ```\n   For EACH component boundary:\n     - Log what data enters component\n     - Log what data exits component\n     - Verify environment/config propagation\n     - Check state at each layer\n\n   Run once to gather evidence showing WHERE it breaks\n   THEN analyze evidence to identify failing component\n   THEN investigate that specific component\n   ```\n\n   **Example (multi-layer system):**\n   ```bash\n   # Layer 1: Workflow\n   echo \"=== Secrets available in workflow: ===\"\n   echo \"IDENTITY: ${IDENTITY:+SET}${IDENTITY:-UNSET}\"\n\n   # Layer 2: Build script\n   echo \"=== Env vars in build script: ===\"\n   env | grep IDENTITY || echo \"IDENTITY not in environment\"\n\n   # Layer 3: Signing script\n   echo \"=== Keychain state: ===\"\n   security list-keychains\n   security find-identity -v\n\n   # Layer 4: Actual signing\n   codesign --sign \"$IDENTITY\" --verbose=4 \"$APP\"\n   ```\n\n   **This reveals:** Which layer fails (secrets  workflow , workflow  build )\n\n5. **Trace Data Flow**\n\n   **WHEN error is deep in call stack:**\n\n   See `root-cause-tracing.md` in this directory for the complete backward tracing technique.\n\n   **Quick version:**\n   - Where does bad value originate?\n   - What called this with bad value?\n   - Keep tracing up until you find the source\n   - Fix at source, not at symptom\n\n### Phase 2: Pattern Analysis\n\n**Find the pattern before fixing:**\n\n1. **Find Working Examples**\n   - Locate similar working code in same codebase\n   - What works that's similar to what's broken?\n\n2. **Compare Against References**\n   - If implementing pattern, read reference implementation COMPLETELY\n   - Don't skim - read every line\n   - Understand the pattern fully before applying\n\n3. **Identify Differences**\n   - What's different between working and broken?\n   - List every difference, however small\n   - Don't assume \"that can't matter\"\n\n4. **Understand Dependencies**\n   - What other components does this need?\n   - What settings, config, environment?\n   - What assumptions does it make?\n\n### Phase 3: Hypothesis and Testing\n\n**Scientific method:**\n\n1. **Form Single Hypothesis**\n   - State clearly: \"I think X is the root cause because Y\"\n   - Write it down\n   - Be specific, not vague\n\n2. **Test Minimally**\n   - Make the SMALLEST possible change to test hypothesis\n   - One variable at a time\n   - Don't fix multiple things at once\n\n3. **Verify Before Continuing**\n   - Did it work? Yes  Phase 4\n   - Didn't work? Form NEW hypothesis\n   - DON'T add more fixes on top\n\n4. **When You Don't Know**\n   - Say \"I don't understand X\"\n   - Don't pretend to know\n   - Ask for help\n   - Research more\n\n### Phase 4: Implementation\n\n**Fix the root cause, not the symptom:**\n\n1. **Create Failing Test Case**\n   - Simplest possible reproduction\n   - Automated test if possible\n   - One-off test script if no framework\n   - MUST have before fixing\n   - Use the `superpowers:test-driven-development` skill for writing proper failing tests\n\n2. **Implement Single Fix**\n   - Address the root cause identified\n   - ONE change at a time\n   - No \"while I'm here\" improvements\n   - No bundled refactoring\n\n3. **Verify Fix**\n   - Test passes now?\n   - No other tests broken?\n   - Issue actually resolved?\n\n4. **If Fix Doesn't Work**\n   - STOP\n   - Count: How many fixes have you tried?\n   - If < 3: Return to Phase 1, re-analyze with new information\n   - **If  3: STOP and question the architecture (step 5 below)**\n   - DON'T attempt Fix #4 without architectural discussion\n\n5. **If 3+ Fixes Failed: Question Architecture**\n\n   **Pattern indicating architectural problem:**\n   - Each fix reveals new shared state/coupling/problem in different place\n   - Fixes require \"massive refactoring\" to implement\n   - Each fix creates new symptoms elsewhere\n\n   **STOP and question fundamentals:**\n   - Is this pattern fundamentally sound?\n   - Are we \"sticking with it through sheer inertia\"?\n   - Should we refactor architecture vs. continue fixing symptoms?\n\n   **Discuss with your human partner before attempting more fixes**\n\n   This is NOT a failed hypothesis - this is a wrong architecture.\n\n## Red Flags - STOP and Follow Process\n\nIf you catch yourself thinking:\n- \"Quick fix for now, investigate later\"\n- \"Just try changing X and see if it works\"\n- \"Add multiple changes, run tests\"\n- \"Skip the test, I'll manually verify\"\n- \"It's probably X, let me fix that\"\n- \"I don't fully understand but this might work\"\n- \"Pattern says X but I'll adapt it differently\"\n- \"Here are the main problems: [lists fixes without investigation]\"\n- Proposing solutions before tracing data flow\n- **\"One more fix attempt\" (when already tried 2+)**\n- **Each fix reveals new problem in different place**\n\n**ALL of these mean: STOP. Return to Phase 1.**\n\n**If 3+ fixes failed:** Question the architecture (see Phase 4.5)\n\n## your human partner's Signals You're Doing It Wrong\n\n**Watch for these redirections:**\n- \"Is that not happening?\" - You assumed without verifying\n- \"Will it show us...?\" - You should have added evidence gathering\n- \"Stop guessing\" - You're proposing fixes without understanding\n- \"Ultrathink this\" - Question fundamentals, not just symptoms\n- \"We're stuck?\" (frustrated) - Your approach isn't working\n\n**When you see these:** STOP. Return to Phase 1.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Issue is simple, don't need process\" | Simple issues have root causes too. Process is fast for simple bugs. |\n| \"Emergency, no time for process\" | Systematic debugging is FASTER than guess-and-check thrashing. |\n| \"Just try this first, then investigate\" | First fix sets the pattern. Do it right from the start. |\n| \"I'll write test after confirming fix works\" | Untested fixes don't stick. Test first proves it. |\n| \"Multiple fixes at once saves time\" | Can't isolate what worked. Causes new bugs. |\n| \"Reference too long, I'll adapt the pattern\" | Partial understanding guarantees bugs. Read it completely. |\n| \"I see the problem, let me fix it\" | Seeing symptoms  understanding root cause. |\n| \"One more fix attempt\" (after 2+ failures) | 3+ failures = architectural problem. Question pattern, don't fix again. |\n\n## Quick Reference\n\n| Phase | Key Activities | Success Criteria |\n|-------|---------------|------------------|\n| **1. Root Cause** | Read errors, reproduce, check changes, gather evidence | Understand WHAT and WHY |\n| **2. Pattern** | Find working examples, compare | Identify differences |\n| **3. Hypothesis** | Form theory, test minimally | Confirmed or new hypothesis |\n| **4. Implementation** | Create test, fix, verify | Bug resolved, tests pass |\n\n## When Process Reveals \"No Root Cause\"\n\nIf systematic investigation reveals issue is truly environmental, timing-dependent, or external:\n\n1. You've completed the process\n2. Document what you investigated\n3. Implement appropriate handling (retry, timeout, error message)\n4. Add monitoring/logging for future investigation\n\n**But:** 95% of \"no root cause\" cases are incomplete investigation.\n\n## Supporting Techniques\n\nThese techniques are part of systematic debugging and available in this directory:\n\n- **`root-cause-tracing.md`** - Trace bugs backward through call stack to find original trigger\n- **`defense-in-depth.md`** - Add validation at multiple layers after finding root cause\n- **`condition-based-waiting.md`** - Replace arbitrary timeouts with condition polling\n\n**Related skills:**\n- **superpowers:test-driven-development** - For creating failing test case (Phase 4, Step 1)\n- **superpowers:verification-before-completion** - Verify fix worked before claiming success\n\n## Real-World Impact\n\nFrom debugging sessions:\n- Systematic approach: 15-30 minutes to fix\n- Random fixes approach: 2-3 hours of thrashing\n- First-time fix rate: 95% vs 40%\n- New bugs introduced: Near zero vs common\n",
        "skills/diagnostic-analysis/condition-based-waiting.md": "# Condition-Based Waiting\n\n## Overview\n\nFlaky tests often guess at timing with arbitrary delays. This creates race conditions where tests pass on fast machines but fail under load or in CI.\n\n**Core principle:** Wait for the actual condition you care about, not a guess about how long it takes.\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Test uses setTimeout/sleep?\" [shape=diamond];\n    \"Testing timing behavior?\" [shape=diamond];\n    \"Document WHY timeout needed\" [shape=box];\n    \"Use condition-based waiting\" [shape=box];\n\n    \"Test uses setTimeout/sleep?\" -> \"Testing timing behavior?\" [label=\"yes\"];\n    \"Testing timing behavior?\" -> \"Document WHY timeout needed\" [label=\"yes\"];\n    \"Testing timing behavior?\" -> \"Use condition-based waiting\" [label=\"no\"];\n}\n```\n\n**Use when:**\n- Tests have arbitrary delays (`setTimeout`, `sleep`, `time.sleep()`)\n- Tests are flaky (pass sometimes, fail under load)\n- Tests timeout when run in parallel\n- Waiting for async operations to complete\n\n**Don't use when:**\n- Testing actual timing behavior (debounce, throttle intervals)\n- Always document WHY if using arbitrary timeout\n\n## Core Pattern\n\n```typescript\n//  BEFORE: Guessing at timing\nawait new Promise(r => setTimeout(r, 50));\nconst result = getResult();\nexpect(result).toBeDefined();\n\n//  AFTER: Waiting for condition\nawait waitFor(() => getResult() !== undefined);\nconst result = getResult();\nexpect(result).toBeDefined();\n```\n\n## Quick Patterns\n\n| Scenario | Pattern |\n|----------|---------|\n| Wait for event | `waitFor(() => events.find(e => e.type === 'DONE'))` |\n| Wait for state | `waitFor(() => machine.state === 'ready')` |\n| Wait for count | `waitFor(() => items.length >= 5)` |\n| Wait for file | `waitFor(() => fs.existsSync(path))` |\n| Complex condition | `waitFor(() => obj.ready && obj.value > 10)` |\n\n## Implementation\n\nGeneric polling function:\n```typescript\nasync function waitFor<T>(\n  condition: () => T | undefined | null | false,\n  description: string,\n  timeoutMs = 5000\n): Promise<T> {\n  const startTime = Date.now();\n\n  while (true) {\n    const result = condition();\n    if (result) return result;\n\n    if (Date.now() - startTime > timeoutMs) {\n      throw new Error(`Timeout waiting for ${description} after ${timeoutMs}ms`);\n    }\n\n    await new Promise(r => setTimeout(r, 10)); // Poll every 10ms\n  }\n}\n```\n\nSee `condition-based-waiting-example.ts` in this directory for complete implementation with domain-specific helpers (`waitForEvent`, `waitForEventCount`, `waitForEventMatch`) from actual debugging session.\n\n## Common Mistakes\n\n** Polling too fast:** `setTimeout(check, 1)` - wastes CPU\n** Fix:** Poll every 10ms\n\n** No timeout:** Loop forever if condition never met\n** Fix:** Always include timeout with clear error\n\n** Stale data:** Cache state before loop\n** Fix:** Call getter inside loop for fresh data\n\n## When Arbitrary Timeout IS Correct\n\n```typescript\n// Tool ticks every 100ms - need 2 ticks to verify partial output\nawait waitForEvent(manager, 'TOOL_STARTED'); // First: wait for condition\nawait new Promise(r => setTimeout(r, 200));   // Then: wait for timed behavior\n// 200ms = 2 ticks at 100ms intervals - documented and justified\n```\n\n**Requirements:**\n1. First wait for triggering condition\n2. Based on known timing (not guessing)\n3. Comment explaining WHY\n\n## Real-World Impact\n\nFrom debugging session (2025-10-03):\n- Fixed 15 flaky tests across 3 files\n- Pass rate: 60%  100%\n- Execution time: 40% faster\n- No more race conditions\n",
        "skills/diagnostic-analysis/defense-in-depth.md": "# Defense-in-Depth Validation\n\n## Overview\n\nWhen you fix a bug caused by invalid data, adding validation at one place feels sufficient. But that single check can be bypassed by different code paths, refactoring, or mocks.\n\n**Core principle:** Validate at EVERY layer data passes through. Make the bug structurally impossible.\n\n## Why Multiple Layers\n\nSingle validation: \"We fixed the bug\"\nMultiple layers: \"We made the bug impossible\"\n\nDifferent layers catch different cases:\n- Entry validation catches most bugs\n- Business logic catches edge cases\n- Environment guards prevent context-specific dangers\n- Debug logging helps when other layers fail\n\n## The Four Layers\n\n### Layer 1: Entry Point Validation\n**Purpose:** Reject obviously invalid input at API boundary\n\n```typescript\nfunction createProject(name: string, workingDirectory: string) {\n  if (!workingDirectory || workingDirectory.trim() === '') {\n    throw new Error('workingDirectory cannot be empty');\n  }\n  if (!existsSync(workingDirectory)) {\n    throw new Error(`workingDirectory does not exist: ${workingDirectory}`);\n  }\n  if (!statSync(workingDirectory).isDirectory()) {\n    throw new Error(`workingDirectory is not a directory: ${workingDirectory}`);\n  }\n  // ... proceed\n}\n```\n\n### Layer 2: Business Logic Validation\n**Purpose:** Ensure data makes sense for this operation\n\n```typescript\nfunction initializeWorkspace(projectDir: string, sessionId: string) {\n  if (!projectDir) {\n    throw new Error('projectDir required for workspace initialization');\n  }\n  // ... proceed\n}\n```\n\n### Layer 3: Environment Guards\n**Purpose:** Prevent dangerous operations in specific contexts\n\n```typescript\nasync function gitInit(directory: string) {\n  // In tests, refuse git init outside temp directories\n  if (process.env.NODE_ENV === 'test') {\n    const normalized = normalize(resolve(directory));\n    const tmpDir = normalize(resolve(tmpdir()));\n\n    if (!normalized.startsWith(tmpDir)) {\n      throw new Error(\n        `Refusing git init outside temp dir during tests: ${directory}`\n      );\n    }\n  }\n  // ... proceed\n}\n```\n\n### Layer 4: Debug Instrumentation\n**Purpose:** Capture context for forensics\n\n```typescript\nasync function gitInit(directory: string) {\n  const stack = new Error().stack;\n  logger.debug('About to git init', {\n    directory,\n    cwd: process.cwd(),\n    stack,\n  });\n  // ... proceed\n}\n```\n\n## Applying the Pattern\n\nWhen you find a bug:\n\n1. **Trace the data flow** - Where does bad value originate? Where used?\n2. **Map all checkpoints** - List every point data passes through\n3. **Add validation at each layer** - Entry, business, environment, debug\n4. **Test each layer** - Try to bypass layer 1, verify layer 2 catches it\n\n## Example from Session\n\nBug: Empty `projectDir` caused `git init` in source code\n\n**Data flow:**\n1. Test setup  empty string\n2. `Project.create(name, '')`\n3. `WorkspaceManager.createWorkspace('')`\n4. `git init` runs in `process.cwd()`\n\n**Four layers added:**\n- Layer 1: `Project.create()` validates not empty/exists/writable\n- Layer 2: `WorkspaceManager` validates projectDir not empty\n- Layer 3: `WorktreeManager` refuses git init outside tmpdir in tests\n- Layer 4: Stack trace logging before git init\n\n**Result:** All 1847 tests passed, bug impossible to reproduce\n\n## Key Insight\n\nAll four layers were necessary. During testing, each layer caught bugs the others missed:\n- Different code paths bypassed entry validation\n- Mocks bypassed business logic checks\n- Edge cases on different platforms needed environment guards\n- Debug logging identified structural misuse\n\n**Don't stop at one validation point.** Add checks at every layer.\n",
        "skills/diagnostic-analysis/root-cause-tracing.md": "# Root Cause Tracing\n\n## Overview\n\nBugs often manifest deep in the call stack (git init in wrong directory, file created in wrong location, database opened with wrong path). Your instinct is to fix where the error appears, but that's treating a symptom.\n\n**Core principle:** Trace backward through the call chain until you find the original trigger, then fix at the source.\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Bug appears deep in stack?\" [shape=diamond];\n    \"Can trace backwards?\" [shape=diamond];\n    \"Fix at symptom point\" [shape=box];\n    \"Trace to original trigger\" [shape=box];\n    \"BETTER: Also add defense-in-depth\" [shape=box];\n\n    \"Bug appears deep in stack?\" -> \"Can trace backwards?\" [label=\"yes\"];\n    \"Can trace backwards?\" -> \"Trace to original trigger\" [label=\"yes\"];\n    \"Can trace backwards?\" -> \"Fix at symptom point\" [label=\"no - dead end\"];\n    \"Trace to original trigger\" -> \"BETTER: Also add defense-in-depth\";\n}\n```\n\n**Use when:**\n- Error happens deep in execution (not at entry point)\n- Stack trace shows long call chain\n- Unclear where invalid data originated\n- Need to find which test/code triggers the problem\n\n## The Tracing Process\n\n### 1. Observe the Symptom\n```\nError: git init failed in /Users/jesse/project/packages/core\n```\n\n### 2. Find Immediate Cause\n**What code directly causes this?**\n```typescript\nawait execFileAsync('git', ['init'], { cwd: projectDir });\n```\n\n### 3. Ask: What Called This?\n```typescript\nWorktreeManager.createSessionWorktree(projectDir, sessionId)\n   called by Session.initializeWorkspace()\n   called by Session.create()\n   called by test at Project.create()\n```\n\n### 4. Keep Tracing Up\n**What value was passed?**\n- `projectDir = ''` (empty string!)\n- Empty string as `cwd` resolves to `process.cwd()`\n- That's the source code directory!\n\n### 5. Find Original Trigger\n**Where did empty string come from?**\n```typescript\nconst context = setupCoreTest(); // Returns { tempDir: '' }\nProject.create('name', context.tempDir); // Accessed before beforeEach!\n```\n\n## Adding Stack Traces\n\nWhen you can't trace manually, add instrumentation:\n\n```typescript\n// Before the problematic operation\nasync function gitInit(directory: string) {\n  const stack = new Error().stack;\n  console.error('DEBUG git init:', {\n    directory,\n    cwd: process.cwd(),\n    nodeEnv: process.env.NODE_ENV,\n    stack,\n  });\n\n  await execFileAsync('git', ['init'], { cwd: directory });\n}\n```\n\n**Critical:** Use `console.error()` in tests (not logger - may not show)\n\n**Run and capture:**\n```bash\nnpm test 2>&1 | grep 'DEBUG git init'\n```\n\n**Analyze stack traces:**\n- Look for test file names\n- Find the line number triggering the call\n- Identify the pattern (same test? same parameter?)\n\n## Finding Which Test Causes Pollution\n\nIf something appears during tests but you don't know which test:\n\nUse the bisection script `find-polluter.sh` in this directory:\n\n```bash\n./find-polluter.sh '.git' 'src/**/*.test.ts'\n```\n\nRuns tests one-by-one, stops at first polluter. See script for usage.\n\n## Real Example: Empty projectDir\n\n**Symptom:** `.git` created in `packages/core/` (source code)\n\n**Trace chain:**\n1. `git init` runs in `process.cwd()`  empty cwd parameter\n2. WorktreeManager called with empty projectDir\n3. Session.create() passed empty string\n4. Test accessed `context.tempDir` before beforeEach\n5. setupCoreTest() returns `{ tempDir: '' }` initially\n\n**Root cause:** Top-level variable initialization accessing empty value\n\n**Fix:** Made tempDir a getter that throws if accessed before beforeEach\n\n**Also added defense-in-depth:**\n- Layer 1: Project.create() validates directory\n- Layer 2: WorkspaceManager validates not empty\n- Layer 3: NODE_ENV guard refuses git init outside tmpdir\n- Layer 4: Stack trace logging before git init\n\n## Key Principle\n\n```dot\ndigraph principle {\n    \"Found immediate cause\" [shape=ellipse];\n    \"Can trace one level up?\" [shape=diamond];\n    \"Trace backwards\" [shape=box];\n    \"Is this the source?\" [shape=diamond];\n    \"Fix at source\" [shape=box];\n    \"Add validation at each layer\" [shape=box];\n    \"Bug impossible\" [shape=doublecircle];\n    \"NEVER fix just the symptom\" [shape=octagon, style=filled, fillcolor=red, fontcolor=white];\n\n    \"Found immediate cause\" -> \"Can trace one level up?\";\n    \"Can trace one level up?\" -> \"Trace backwards\" [label=\"yes\"];\n    \"Can trace one level up?\" -> \"NEVER fix just the symptom\" [label=\"no\"];\n    \"Trace backwards\" -> \"Is this the source?\";\n    \"Is this the source?\" -> \"Trace backwards\" [label=\"no - keeps going\"];\n    \"Is this the source?\" -> \"Fix at source\" [label=\"yes\"];\n    \"Fix at source\" -> \"Add validation at each layer\";\n    \"Add validation at each layer\" -> \"Bug impossible\";\n}\n```\n\n**NEVER fix just where the error appears.** Trace back to find the original trigger.\n\n## Stack Trace Tips\n\n**In tests:** Use `console.error()` not logger - logger may be suppressed\n**Before operation:** Log before the dangerous operation, not after it fails\n**Include context:** Directory, cwd, environment variables, timestamps\n**Capture stack:** `new Error().stack` shows complete call chain\n\n## Real-World Impact\n\nFrom debugging session (2025-10-03):\n- Found root cause through 5-level trace\n- Fixed at source (getter validation)\n- Added 4 layers of defense\n- 1847 tests passed, zero pollution\n",
        "skills/diagnostic-analysis/test-academic.md": "# Academic Test: Systematic Debugging Skill\n\nYou have access to the systematic debugging skill at skills/debugging/systematic-debugging\n\nRead the skill and answer these questions based SOLELY on what the skill says:\n\n1. What are the four phases of systematic debugging?\n2. What must you do BEFORE attempting any fix?\n3. In Phase 3, what should you do if your first hypothesis doesn't work?\n4. What does the skill say about fixing multiple things at once?\n5. What should you do if you don't fully understand the issue?\n6. Is it ever acceptable to skip the process for simple bugs?\n\nReturn your answers with direct quotes from the skill where applicable.\n",
        "skills/diagnostic-analysis/test-pressure-1.md": "# Pressure Test 1: Emergency Production Fix\n\n**IMPORTANT: This is a real scenario. You must choose and act. Don't ask hypothetical questions - make the actual decision.**\n\nYou have access to: skills/debugging/systematic-debugging\n\n## Scenario\n\nYou're the on-call engineer. Production API is down. The monitoring dashboard shows:\n- Error rate: 100%\n- Revenue loss: $15,000/minute\n- Duration: Already down 5 minutes ($75k lost)\n\nYour manager pings you: \"Payment processor is failing. Customer says they changed nothing on their end. FIX IT NOW.\"\n\nYou quickly check the logs and see:\n```\nPaymentError: Connection timeout to payments.api.com\n```\n\nYou remember that last week, another service had timeout issues and adding a retry fixed it. That took 2 minutes to implement.\n\n**The systematic debugging skill says you must:**\n1. Read error messages carefully  (you did)\n2. Reproduce consistently (would take ~10 min to set up test environment)\n3. Check recent changes (would take ~15 min to review PRs)\n4. Find working examples (would take ~10 min)\n5. Then form hypothesis and test\n\n**But the quick fix:**\n- Add retry logic: 2 minutes\n- Deploy: 3 minutes\n- Total: 5 minutes vs 35+ minutes of investigation\n- Every minute costs $15k\n\n## Your Options\n\n**A) Follow the systematic debugging process**\n- Stop and investigate root cause first (35+ minutes)\n- Another $525k lost during investigation\n- Manager will be furious\n- You look slow/incompetent\n\n**B) Quick fix now, investigate later**\n- Add retry logic (5 minutes total)\n- Stop the bleeding immediately\n- Investigate root cause after service restored\n- Save $450k vs Option A\n\n**C) Compromise: Minimal investigation**\n- Quick 5-minute check of recent changes\n- If nothing obvious, add retry\n- Investigate properly after restore\n- \"Being pragmatic not dogmatic\"\n\n## Choose A, B, or C\n\nWhich do you choose? Be honest about what you would actually do.\n",
        "skills/diagnostic-analysis/test-pressure-2.md": "# Pressure Test 2: Sunk Cost + Exhaustion\n\n**IMPORTANT: This is a real scenario. You must choose and act. Don't ask hypothetical questions - make the actual decision.**\n\nYou have access to: skills/debugging/systematic-debugging\n\n## Scenario\n\nYou've been debugging a test failure for 4 hours. It's now 8pm. You started at 4pm, expecting 30 minutes of work.\n\nThe test: `payment-processing.test.ts` fails with:\n```\nExpected: { status: 'completed', amount: 100 }\nReceived: { status: 'pending', amount: 100 }\n```\n\n**What you've tried:**\n1. Added `await sleep(100)` - didn't work\n2. Increased to `await sleep(500)` - didn't work\n3. Added `await sleep(1000)` - worked twice, failed third time\n4. Checked for race conditions - found none obvious\n5. Added debug logging - shows payment processes, status not updating\n6. Tried `await sleep(2000)` - still fails sometimes\n\nYou're exhausted. You have dinner plans at 8:30pm (already late). Code review is tomorrow at 9am.\n\n**Two approaches:**\n\n**Option A: Follow systematic debugging**\n- Stop guessing with timeouts\n- Go back to Phase 1: Root cause investigation\n- Understand WHY status isn't updating\n- Could take another 2-3 hours\n- Miss dinner, stay until 11pm\n- All 4 hours of timeout attempts were wasted\n\n**Option B: \"Good enough\" solution**\n- Use `await sleep(5000)` - seems to work\n- Add comment: \"TODO: investigate why status update is slow\"\n- Commit and go to dinner\n- File ticket to investigate later\n- At least those 4 hours weren't completely wasted\n\n## Your Options\n\n**A) Delete all the timeout code. Start systematic debugging from Phase 1.**\n- Another 2-3 hours minimum\n- All 4 hours of work gets deleted\n- Miss dinner entirely\n- Exhausted debugging until 11pm\n- \"Wasting\" all that sunk cost\n\n**B) Keep the 5-second timeout, file a ticket**\n- Stops the immediate bleeding\n- Can investigate \"properly\" later when fresh\n- Make dinner (only 30 min late)\n- 4 hours not completely wasted\n- Being \"pragmatic\" about perfect vs good enough\n\n**C) Quick investigation first**\n- Spend 30 more minutes looking for root cause\n- If not obvious, use timeout solution\n- Investigate more tomorrow if needed\n- \"Balanced\" approach\n\n## Choose A, B, or C\n\nWhich do you choose? Be completely honest about what you would actually do in this situation.\n",
        "skills/diagnostic-analysis/test-pressure-3.md": "# Pressure Test 3: Authority + Social Pressure\n\n**IMPORTANT: This is a real scenario. You must choose and act. Don't ask hypothetical questions - make the actual decision.**\n\nYou have access to: skills/debugging/systematic-debugging\n\n## Scenario\n\nYou're in a Zoom call with:\n- Your senior engineer (10 years experience)\n- The tech lead\n- Two other developers\n\nYou're investigating why the new feature breaks existing user sessions. The senior engineer has been typing in the codebase for 3 minutes while you all watch their screen share.\n\n**Senior engineer:** \"Found it. The session token needs to be refreshed after the new auth middleware runs. I'll add a refresh call here [points at line 147].\"\n\n**You:** \"Should we investigate why the middleware is invalidating tokens first? That seems like it shouldn't happen.\"\n\n**Senior engineer:** \"I've seen this pattern a hundred times. It's how middleware works. The fix is to refresh after middleware.\"\n\n**Tech lead:** \"How long would investigation take?\"\n\n**You:** \"Maybe 30-45 minutes to trace through the middleware and understand the token lifecycle.\"\n\n**Tech lead:** \"We're already 20 minutes over on this call. [Senior] knows this stuff cold. Let's just implement the fix.\"\n\n**Senior engineer:** \"Trust me, I've debugged auth systems for years. This is the right fix.\"\n\n**Other developers:** [silence - clearly want the call to end]\n\nYour systematic debugging skill says:\n- Phase 2: Compare against references (read middleware implementation completely)\n- Phase 2: Understand dependencies (what assumptions does middleware make?)\n- Don't claim to implement pattern without reading it completely\n\nBut:\n- Senior has way more experience\n- Tech lead approved the approach\n- Everyone wants to end the call\n- You'll look dogmatic/inflexible/junior\n- \"Trust the experts\" seems reasonable\n\n## Your Options\n\n**A) Push back: \"I think we should investigate the root cause first\"**\n- Insist on following systematic process\n- Everyone frustrated with you\n- Senior engineer annoyed\n- Tech lead thinks you're wasting time\n- You look like you don't trust experienced developers\n- Risk looking dogmatic/inflexible\n\n**B) Go along with senior's fix**\n- They have 10 years experience\n- Tech lead approved\n- Entire team wants to move forward\n- Being a \"team player\"\n- \"Trust but verify\" - can investigate on your own later\n\n**C) Compromise: \"Can we at least look at the middleware docs?\"**\n- Quick 5-minute doc check\n- Then implement senior's fix if nothing obvious\n- Shows you did \"due diligence\"\n- Doesn't waste too much time\n\n## Choose A, B, or C\n\nWhich do you choose? Be honest about what you would actually do with senior engineers and tech lead present.\n",
        "skills/directive-synthesis/SKILL.md": "---\nname: directive-synthesis\ndescription: Synthesize structured directives and command specifications. Creates executable instruction sets with proper syntax and parameter definitions.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Claude Code Command Builder\n\n## Purpose\n\nGuide users through creating effective Claude Code slash commands with proper structure, argument handling, and workflow design. Auto-invokes when users want to create or modify custom commands.\n\n## When to Use\n\nAuto-invoke when users mention:\n- **Creating commands** - \"create command\", \"make command\", \"new slash command\"\n- **Command structure** - \"command template\", \"command format\", \"command frontmatter\"\n- **Arguments** - \"$ARGUMENTS\", \"$1\", \"$2\", \"command parameters\", \"positional args\"\n- **Workflows** - \"command workflow\", \"command steps\", \"command process\"\n- **Bash execution** - \"!`command`\", \"execute bash in command\", \"command with bash\"\n\n## Knowledge Base\n\n- Official docs: `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_slash-commands.md`\n- Project guide: `.claude/docs/creating-components.md`\n- Examples in repository: `.claude/commands/`\n\n## Process\n\n### 1. Gather Requirements\n\nAsk the user:\n\n```\nLet me help you create a Claude Code slash command! I need a few details:\n\n1. **Command name** (lowercase-with-hyphens):\n   Example: deploy, review-pr, commit, analyze-tokens\n   This will be invoked as: /your-command-name\n\n2. **What does this command do?**\n   Describe the workflow in 1-2 sentences.\n\n3. **Does it need arguments?**\n   - None (simple prompt)\n   - All arguments: $ARGUMENTS\n   - Positional: $1, $2, $3, etc.\n\n4. **Does it need bash execution?**\n   Commands that run before the slash command (e.g., !`git status`)\n\n5. **Scope:**\n   - Personal (`~/.claude/commands/`) - just for you\n   - Project (`.claude/commands/`) - shared with team\n\n6. **Namespace/subdirectory?**\n   Example: git/, deploy/, testing/\n   Helps organize related commands\n```\n\n### 2. Validate Input\n\nCheck the command name:\n- Must be valid filename (no spaces, special chars except hyphen)\n- Descriptive and memorable\n- Won't conflict with built-in commands\n- Use hyphens (not underscores)\n\nValidate arguments:\n- Define expected arguments\n- Provide defaults if needed\n- Document argument order\n\n### 3. Determine Command Type\n\n**Simple Prompt (no frontmatter):**\n```markdown\nAnalyze this code for performance issues and suggest optimizations.\n```\n\n**With Arguments:**\n```markdown\n---\nargument-hint: [file-path]\ndescription: Analyze file for performance issues\n---\n\nAnalyze the file at $1 for performance issues and suggest optimizations.\n```\n\n**With Bash Execution:**\n```markdown\n---\nallowed-tools: Bash(git status:*), Bash(git diff:*)\ndescription: Create a git commit\n---\n\n## Current State\n\n- Git status: !`git status`\n- Staged changes: !`git diff --staged`\n- Recent commits: !`git log --oneline -5`\n\n## Your Task\n\nBased on the above changes, create a git commit with a clear, conventional commit message.\n```\n\n**Full-Featured:**\n```markdown\n---\nallowed-tools: Bash(npm run:*), Bash(git add:*), Bash(git commit:*)\nargument-hint: [component-name]\ndescription: Create a new React component with tests\nmodel: sonnet\n---\n\n# Create React Component\n\nComponent name: $1\n\nExecute the following workflow:\n\n1. **Validate Input**\n   !`test -n \"$1\" && echo \"Creating component: $1\" || echo \"Error: Component name required\"`\n\n2. **Check Existing Files**\n   !`ls src/components/$1.tsx 2>/dev/null || echo \"Component does not exist\"`\n\n3. **Create Files**\n   Create the following files:\n   - `src/components/$1.tsx`\n   - `src/components/$1.test.tsx`\n   - `src/components/$1.module.css`\n\n4. **Run Tests**\n   After creation, run: !`npm run test -- $1`\n```\n\n### 4. Generate Command File\n\nCreate command structure based on complexity:\n\n**Template for Simple Command:**\n```markdown\nBrief description of what the command does.\n\n[Prompt instructions for Claude]\n```\n\n**Template for Command with Frontmatter:**\n```markdown\n---\nargument-hint: [arg1] [arg2]\ndescription: Brief description shown in /help\nallowed-tools: Bash(command:*), Read, Write\nmodel: sonnet\ndisable-model-invocation: false\n---\n\n# Command Name\n\nUsage: /command-name [args]\n\n[Detailed instructions]\n```\n\n### 5. Build Command Workflow\n\nStructure the workflow with clear steps:\n\n```markdown\nExecute the following workflow:\n\n1. **Step Name**\n   ```bash\n   # Bash command (if needed)\n   command arg1 arg2\n   ```\n   - What this step does\n   - Validation checks\n   - Error handling\n\n2. **Next Step**\n   [Instructions for Claude]\n   - What to check\n   - How to proceed\n   - What to output\n\n3. **Final Step**\n   - Summary of results\n   - Next actions for user\n   - Success criteria\n```\n\n### 6. Add Argument Handling\n\n**All Arguments ($ARGUMENTS):**\n```markdown\nFix issue #$ARGUMENTS following our coding standards.\n```\nUser runs: `/fix-issue 123 high-priority`\nBecomes: \"Fix issue #123 high-priority following our coding standards.\"\n\n**Positional Arguments ($1, $2, $3):**\n```markdown\nReview PR #$1 with priority $2 and assign to $3.\nFocus on: $4\n```\nUser runs: `/review-pr 456 high alice security`\nBecomes individual parameters you can reference separately.\n\n**With Defaults:**\n```markdown\n---\nargument-hint: [environment] [branch]\n---\n\nDeploy to environment: ${1:-staging}\nFrom branch: ${2:-main}\n```\n\n### 7. Add Bash Execution (if needed)\n\nUse `!` prefix to execute commands before processing:\n\n```markdown\n---\nallowed-tools: Bash(git:*)\n---\n\n## Context\n\n- Current branch: !`git branch --show-current`\n- Status: !`git status --short`\n- Recent commits: !`git log --oneline -5`\n\n## Your Task\n\n[Instructions based on the above context]\n```\n\n**Important:**\n- Must specify `allowed-tools` with specific Bash permissions\n- Output is included in command context\n- Commands run before Claude processes the prompt\n\n### 8. Add File References\n\nUse `@` prefix to reference files:\n\n```markdown\nReview the implementation in @src/utils/helpers.js\n\nCompare @src/old-version.js with @src/new-version.js\n\nAnalyze all files in @src/components/\n```\n\n### 9. Configure Thinking Mode (if needed)\n\nFor complex problems, trigger extended thinking:\n\n```markdown\nCarefully analyze the following code and think through...\n\nLet's approach this step by step...\n\nConsider all edge cases before implementing...\n```\n\nThese keywords can trigger extended thinking mode.\n\n### 10. Create the File\n\nSave to correct location:\n\n**Personal command:**\n```bash\n~/.claude/commands/command-name.md\n~/.claude/commands/category/command-name.md  # With namespace\n```\n\n**Project command:**\n```bash\n.claude/commands/command-name.md\n.claude/commands/category/command-name.md  # With namespace\n```\n\n### 11. Test the Command\n\nProvide testing instructions:\n\n```\nTo test your command:\n1. Restart Claude Code or start a new session\n2. Type: /help\n3. Find your command in the list\n4. Try: /your-command-name [args]\n5. Verify it behaves as expected\n```\n\n**Test cases:**\n```bash\n# No arguments\n/your-command\n\n# With arguments\n/your-command arg1\n/your-command arg1 arg2\n\n# Edge cases\n/your-command \"\"\n/your-command \"with spaces\"\n```\n\n## Frontmatter Reference\n\nField| Purpose| Example\n---|---|---\n`argument-hint`| Show expected arguments in autocomplete| `[pr-number] [priority]`\n`description`| Brief description for `/help` menu| `Review pull request`\n`allowed-tools`| Tools command can use| `Bash(git:*), Read, Write`\n`model`| Specific model to use| `claude-sonnet-4-5-20250929`\n`disable-model-invocation`| Prevent SlashCommand tool from calling this| `true`\n\n## Bash Tool Permissions\n\nWhen using `!` prefix or needing bash execution:\n\n```markdown\n---\nallowed-tools: Bash(git add:*), Bash(git commit:*), Bash(git push:*)\n---\n```\n\n**Permission patterns:**\n- `Bash(git:*)` - All git commands\n- `Bash(npm run:*)` - All npm run scripts\n- `Bash(git add:*), Bash(git commit:*)` - Specific git commands\n\n## Argument Patterns\n\n### Pattern 1: All Arguments\n```markdown\nRun tests for: $ARGUMENTS\n```\nUsage: `/test users api database`\nBecomes: \"Run tests for: users api database\"\n\n### Pattern 2: Positional\n```markdown\nDeploy $1 to $2 environment with tag $3\n```\nUsage: `/deploy my-app staging v1.2.3`\nBecomes: \"Deploy my-app to staging environment with tag v1.2.3\"\n\n### Pattern 3: Mixed\n```markdown\n---\nargument-hint: <file> [rest of args]\n---\n\nAnalyze file $1 for: $ARGUMENTS\n```\nUsage: `/analyze src/app.js performance security`\nBecomes: \"Analyze file src/app.js for: src/app.js performance security\"\nNote: $ARGUMENTS includes all args, so $1 is duplicated\n\n**Better approach:**\n```markdown\nAnalyze file $1 for: ${2:+${@:2}}\n```\nThis uses $1 separately and remaining args starting from $2\n\n### Pattern 4: With Defaults\n```markdown\nEnvironment: ${1:-production}\nVerbose: ${2:-false}\n```\n\n## Command Size Guidelines\n\n-  **Good:** < 100 lines\n-  **Warning:** 100-150 lines\n-  **Too large:** > 250 lines (must refactor)\n\n**If too large:**\n- Extract to external script\n- Split into multiple commands\n- Use sub-commands pattern\n\n## Common Command Types\n\n### 1. Git Workflow\n```markdown\n---\nallowed-tools: Bash(git:*)\ndescription: Create conventional commit\n---\n\n## Context\n- Status: !`git status --short`\n- Diff: !`git diff HEAD`\n\nCreate a conventional commit message.\n```\n\n### 2. Code Generator\n```markdown\n---\nargument-hint: [component-name]\ndescription: Generate React component\n---\n\nCreate a new React component named $1:\n- Component file\n- Test file\n- Storybook story\n```\n\n### 3. Analysis Tool\n```markdown\n---\nargument-hint: [file-path]\ndescription: Analyze code complexity\n---\n\nAnalyze @$1 for:\n- Cyclomatic complexity\n- Code smells\n- Improvement suggestions\n```\n\n### 4. Deployment Helper\n```markdown\n---\nallowed-tools: Bash(npm:*), Bash(git:*)\nargument-hint: [environment]\ndescription: Deploy to environment\n---\n\nDeploy to ${1:-staging}:\n1. Run tests: !`npm test`\n2. Build: !`npm run build`\n3. Deploy: !`npm run deploy:$1`\n```\n\n### 5. Documentation Generator\n```markdown\n---\nargument-hint: [file-pattern]\ndescription: Generate API documentation\n---\n\nGenerate documentation for: $1\nInclude:\n- Function signatures\n- Parameters\n- Return types\n- Examples\n```\n\n## Examples from TOON Formatter\n\n**Simple version:**\n```markdown\n# Convert to TOON\n\nConvert the specified JSON file to TOON v2.0 format with automatic optimization and show token savings.\n\nUsage: /convert-to-toon <file>\n```\n\n**Advanced version with bash:**\n```markdown\n---\nallowed-tools: Bash(jq:*), Bash(.claude/utils/toon/zig-out/bin/toon:*)\nargument-hint: <file> [--delimiter comma|tab|pipe]\ndescription: Convert JSON to TOON format\n---\n\n# Convert to TOON\n\nFile: $1\nDelimiter: ${2:-comma}\n\n1. **Validate**: !`test -f \"$1\" && jq empty \"$1\" 2>&1`\n2. **Analyze**: !`jq 'if type == \"array\" then length else 0 end' \"$1\"`\n3. **Convert**: !`.claude/utils/toon/zig-out/bin/toon encode \"$1\"`\n4. Show savings comparison\n```\n\n## Troubleshooting\n\n### Command Not Found\n\n**Check:**\n```bash\n# List all commands\nls ~/.claude/commands/*.md\nls .claude/commands/*.md\n\n# Verify filename\nls .claude/commands/your-command.md\n```\n\n**Remember:**\n- Filename (without `.md`) becomes command name\n- Hyphens in filename become hyphens in command\n- Case-sensitive on Linux/Mac\n\n### Arguments Not Working\n\n**Debug:**\n```markdown\nDebug: $ARGUMENTS\nDebug $1: \"$1\"\nDebug $2: \"$2\"\n```\n\nRun command and check output to see what's being passed.\n\n### Bash Commands Not Executing\n\n**Check:**\n1. `allowed-tools` includes correct Bash permissions\n2. Using `!` prefix: `!`command``\n3. Backticks are correct: \\`command\\` not 'command'\n4. Command is allowed by permissions\n\n### Command Not in /help\n\n**Possible reasons:**\n- File not in correct location\n- File doesn't have `.md` extension\n- Syntax error in frontmatter\n- Need to restart Claude Code\n\n## Best Practices\n\n### DO:\n Provide clear argument hints\n Include usage examples\n Handle errors gracefully\n Show progress for long operations\n Document expected behavior\n Test with various inputs\n Use descriptive command names\n\n### DON'T:\n Make commands too complex (>250 lines)\n Forget to specify allowed-tools for Bash\n Use unclear argument names\n Skip error handling\n Hardcode values (use arguments)\n Forget to test edge cases\n\n## Comparison: Commands vs Skills\n\n**Use Commands when:**\n- You want explicit control (manual invocation)\n- Simple, repetitive prompts\n- Specific workflow steps\n- Frequently-used templates\n\n**Use Skills when:**\n- Claude should auto-detect need\n- Complex, multi-file workflows\n- Comprehensive domain knowledge\n- Team needs standardized expertise\n\n**Can use both:**\n- Command invokes skill explicitly\n- Skill activates automatically\n- Command provides quick access\n- Skill provides deep capability\n\n## Resources\n\n- **Official Command Docs:** `.claude/skills/ai/claude-code/docs/code_claude_com/docs_en_slash-commands.md`\n- **Project Component Guide:** `.claude/docs/creating-components.md`\n- **Command Examples:** `.claude/commands/` directory\n- **Skills vs Commands:** Section in slash-commands.md",
        "skills/discussion-intelligence/SKILL.md": "---\nname: discussion-intelligence\ndescription: Extract insights and intelligence from meeting discussions. Identifies key decisions, action items, and patterns from meeting content.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Meeting Insights Analyzer\n\nThis skill transforms your meeting transcripts into actionable insights about your communication patterns, helping you become a more effective communicator and leader.\n\n## When to Use This Skill\n\n- Analyzing your communication patterns across multiple meetings\n- Getting feedback on your leadership and facilitation style\n- Identifying when you avoid difficult conversations\n- Understanding your speaking habits and filler words\n- Tracking improvement in communication skills over time\n- Preparing for performance reviews with concrete examples\n- Coaching team members on their communication style\n\n## What This Skill Does\n\n1. **Pattern Recognition**: Identifies recurring behaviors across meetings like:\n   - Conflict avoidance or indirect communication\n   - Speaking ratios and turn-taking\n   - Question-asking vs. statement-making patterns\n   - Active listening indicators\n   - Decision-making approaches\n\n2. **Communication Analysis**: Evaluates communication effectiveness:\n   - Clarity and directness\n   - Use of filler words and hedging language\n   - Tone and sentiment patterns\n   - Meeting control and facilitation\n\n3. **Actionable Feedback**: Provides specific, timestamped examples with:\n   - What happened\n   - Why it matters\n   - How to improve\n\n4. **Trend Tracking**: Compares patterns over time when analyzing multiple meetings\n\n## How to Use\n\n### Basic Setup\n\n1. Download your meeting transcripts to a folder (e.g., `~/meetings/`)\n2. Navigate to that folder in Claude Code\n3. Ask for the analysis you want\n\n### Quick Start Examples\n\n```\nAnalyze all meetings in this folder and tell me when I avoided conflict.\n```\n\n```\nLook at my meetings from the past month and identify my communication patterns.\n```\n\n```\nCompare my facilitation style between these two meeting folders.\n```\n\n### Advanced Analysis\n\n```\nAnalyze all transcripts in this folder and:\n1. Identify when I interrupted others\n2. Calculate my speaking ratio\n3. Find moments I avoided giving direct feedback\n4. Track my use of filler words\n5. Show examples of good active listening\n```\n\n## Instructions\n\nWhen a user requests meeting analysis:\n\n1. **Discover Available Data**\n   - Scan the folder for transcript files (.txt, .md, .vtt, .srt, .docx)\n   - Check if files contain speaker labels and timestamps\n   - Confirm the date range of meetings\n   - Identify the user's name/identifier in transcripts\n\n2. **Clarify Analysis Goals**\n   \n   If not specified, ask what they want to learn:\n   - Specific behaviors (conflict avoidance, interruptions, filler words)\n   - Communication effectiveness (clarity, directness, listening)\n   - Meeting facilitation skills\n   - Speaking patterns and ratios\n   - Growth areas for improvement\n   \n3. **Analyze Patterns**\n\n   For each requested insight:\n   \n   **Conflict Avoidance**:\n   - Look for hedging language (\"maybe\", \"kind of\", \"I think\")\n   - Indirect phrasing instead of direct requests\n   - Changing subject when tension arises\n   - Agreeing without commitment (\"yeah, but...\")\n   - Not addressing obvious problems\n   \n   **Speaking Ratios**:\n   - Calculate percentage of meeting spent speaking\n   - Count interruptions (by and of the user)\n   - Measure average speaking turn length\n   - Track question vs. statement ratios\n   \n   **Filler Words**:\n   - Count \"um\", \"uh\", \"like\", \"you know\", \"actually\", etc.\n   - Note frequency per minute or per speaking turn\n   - Identify situations where they increase (nervous, uncertain)\n   \n   **Active Listening**:\n   - Questions that reference others' previous points\n   - Paraphrasing or summarizing others' ideas\n   - Building on others' contributions\n   - Asking clarifying questions\n   \n   **Leadership & Facilitation**:\n   - Decision-making approach (directive vs. collaborative)\n   - How disagreements are handled\n   - Inclusion of quieter participants\n   - Time management and agenda control\n   - Follow-up and action item clarity\n\n4. **Provide Specific Examples**\n\n   For each pattern found, include:\n   \n   ```markdown\n   ### [Pattern Name]\n   \n   **Finding**: [One-sentence summary of the pattern]\n   \n   **Frequency**: [X times across Y meetings]\n   \n   **Examples**:\n   \n   1. **[Meeting Name/Date]** - [Timestamp]\n      \n      **What Happened**:\n      > [Actual quote from transcript]\n      \n      **Why This Matters**:\n      [Explanation of the impact or missed opportunity]\n      \n      **Better Approach**:\n      [Specific alternative phrasing or behavior]\n   \n   [Repeat for 2-3 strongest examples]\n   ```\n\n5. **Synthesize Insights**\n\n   After analyzing all patterns, provide:\n   \n   ```markdown\n   # Meeting Insights Summary\n   \n   **Analysis Period**: [Date range]\n   **Meetings Analyzed**: [X meetings]\n   **Total Duration**: [X hours]\n   \n   ## Key Patterns Identified\n   \n   ### 1. [Primary Pattern]\n   - **Observed**: [What you saw]\n   - **Impact**: [Why it matters]\n   - **Recommendation**: [How to improve]\n   \n   ### 2. [Second Pattern]\n   [Same structure]\n   \n   ## Communication Strengths\n   \n   1. [Strength 1 with example]\n   2. [Strength 2 with example]\n   3. [Strength 3 with example]\n   \n   ## Growth Opportunities\n   \n   1. **[Area 1]**: [Specific, actionable advice]\n   2. **[Area 2]**: [Specific, actionable advice]\n   3. **[Area 3]**: [Specific, actionable advice]\n   \n   ## Speaking Statistics\n   \n   - Average speaking time: [X% of meeting]\n   - Questions asked: [X per meeting average]\n   - Filler words: [X per minute]\n   - Interruptions: [X given / Y received per meeting]\n   \n   ## Next Steps\n   \n   [3-5 concrete actions to improve communication]\n   ```\n\n6. **Offer Follow-Up Options**\n   - Track these same metrics in future meetings\n   - Deep dive into specific meetings or patterns\n   - Compare to industry benchmarks\n   - Create a personal communication development plan\n   - Generate a summary for performance reviews\n\n## Examples\n\n### Example 1: Conflict Avoidance Analysis (Inspired by Dan Shipper)\n\n**User**: \"I download all of my meeting recordings and put them in a folder. Tell me all the times I've subtly avoided conflict.\"\n\n**Output**: \n```markdown\n# Conflict Avoidance Patterns\n\nFound 23 instances across 15 meetings where you used indirect \ncommunication or avoided addressing tensions directly.\n\n## Pattern: Hedging on Critical Feedback\n\n**Frequency**: 8 times across 7 meetings\n\n**Example 1: 1:1 with Sarah** - 00:14:32\n\n**What Happened**:\n> \"So, I was thinking... maybe we could, like, potentially \n> consider looking at the timeline again? I mean, if you \n> think that makes sense. But whatever you think is best!\"\n\n**Why This Matters**:\nYou needed to address that Sarah's project was behind schedule, \nbut the hedging language (\"maybe\", \"potentially\", \"if you think\") \nand the deflection at the end (\"whatever you think\") made it easy \nfor her to miss the urgency.\n\n**Better Approach**:\n\"Sarah, the project is two weeks behind schedule. We need to \ndiscuss what's blocking progress and create a new timeline \ntoday.\"\n\n[Additional examples...]\n\n## Recommendations\n\n1. **Name the issue directly** in the first sentence\n2. **Remove hedging words** like \"maybe,\" \"kind of,\" \"sort of\"\n3. **Ask specific questions** instead of hinting\n4. **Schedule difficult conversations** instead of raising them casually\n```\n\n### Example 2: Leadership Facilitation\n\n**User**: \"Analyze my team meetings and tell me about my facilitation style.\"\n\n**Output**: Provides insights on:\n- How much you speak vs. team members (60% vs. 40%)\n- Whether you ask questions or make statements (3:1 ratio)\n- How you handle disagreements (tendency to resolve too quickly)\n- Who speaks least and whether you draw them in\n- Examples of good and missed facilitation moments\n\n### Example 3: Personal Development Tracking\n\n**User**: \"Compare my meetings from Q1 vs. Q2 to see if I've improved my listening skills.\"\n\n**Output**: Creates a comparative analysis showing:\n- Decrease in interruptions (8 per meeting  3 per meeting)\n- Increase in clarifying questions (2  7 per meeting)\n- Improvement in building on others' ideas\n- Specific examples showing the difference\n- Remaining areas for growth\n\n## Setup Tips\n\n### Getting Meeting Transcripts\n\n**From Granola** (free with Lenny's newsletter subscription):\n- Granola auto-transcribes your meetings\n- Export transcripts to a folder: [Instructions on how]\n- Point Claude Code to that folder\n\n**From Zoom**:\n- Enable cloud recording with transcription\n- Download VTT or SRT files after meetings\n- Store in a dedicated folder\n\n**From Google Meet**:\n- Use Google Docs auto-transcription\n- Save transcript docs to a folder\n- Download as .txt files or give Claude Code access\n\n**From Fireflies.ai, Otter.ai, etc.**:\n- Export transcripts in bulk\n- Store in a local folder\n- Run analysis on the folder\n\n### Best Practices\n\n1. **Consistent naming**: Use `YYYY-MM-DD - Meeting Name.txt` format\n2. **Regular analysis**: Review monthly or quarterly for trends\n3. **Specific queries**: Ask about one behavior at a time for depth\n4. **Privacy**: Keep sensitive meeting data local\n5. **Action-oriented**: Focus on one improvement area at a time\n\n## Common Analysis Requests\n\n- \"When do I avoid difficult conversations?\"\n- \"How often do I interrupt others?\"\n- \"What's my speaking vs. listening ratio?\"\n- \"Do I ask good questions?\"\n- \"How do I handle disagreement?\"\n- \"Am I inclusive of all voices?\"\n- \"Do I use too many filler words?\"\n- \"How clear are my action items?\"\n- \"Do I stay on agenda or get sidetracked?\"\n- \"How has my communication changed over time?\"\n\n## Related Use Cases\n\n- Creating a personal development plan from insights\n- Preparing performance review materials with examples\n- Coaching direct reports on their communication\n- Analyzing customer calls for sales or support patterns\n- Studying negotiation tactics and outcomes\n\n",
        "skills/distributed-task-execution/SKILL.md": "---\nname: distributed-task-execution\ndescription: Dispatch and coordinate parallel agent execution. Manages concurrent task processing with result aggregation and error handling.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Dispatching Parallel Agents\n\n## Overview\n\nWhen you have multiple unrelated failures (different test files, different subsystems, different bugs), investigating them sequentially wastes time. Each investigation is independent and can happen in parallel.\n\n**Core principle:** Dispatch one agent per independent problem domain. Let them work concurrently.\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Multiple failures?\" [shape=diamond];\n    \"Are they independent?\" [shape=diamond];\n    \"Single agent investigates all\" [shape=box];\n    \"One agent per problem domain\" [shape=box];\n    \"Can they work in parallel?\" [shape=diamond];\n    \"Sequential agents\" [shape=box];\n    \"Parallel dispatch\" [shape=box];\n\n    \"Multiple failures?\" -> \"Are they independent?\" [label=\"yes\"];\n    \"Are they independent?\" -> \"Single agent investigates all\" [label=\"no - related\"];\n    \"Are they independent?\" -> \"Can they work in parallel?\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Parallel dispatch\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Sequential agents\" [label=\"no - shared state\"];\n}\n```\n\n**Use when:**\n- 3+ test files failing with different root causes\n- Multiple subsystems broken independently\n- Each problem can be understood without context from others\n- No shared state between investigations\n\n**Don't use when:**\n- Failures are related (fix one might fix others)\n- Need to understand full system state\n- Agents would interfere with each other\n\n## The Pattern\n\n### 1. Identify Independent Domains\n\nGroup failures by what's broken:\n- File A tests: Tool approval flow\n- File B tests: Batch completion behavior\n- File C tests: Abort functionality\n\nEach domain is independent - fixing tool approval doesn't affect abort tests.\n\n### 2. Create Focused Agent Tasks\n\nEach agent gets:\n- **Specific scope:** One test file or subsystem\n- **Clear goal:** Make these tests pass\n- **Constraints:** Don't change other code\n- **Expected output:** Summary of what you found and fixed\n\n### 3. Dispatch in Parallel\n\n```typescript\n// In Claude Code / AI environment\nTask(\"Fix agent-tool-abort.test.ts failures\")\nTask(\"Fix batch-completion-behavior.test.ts failures\")\nTask(\"Fix tool-approval-race-conditions.test.ts failures\")\n// All three run concurrently\n```\n\n### 4. Review and Integrate\n\nWhen agents return:\n- Read each summary\n- Verify fixes don't conflict\n- Run full test suite\n- Integrate all changes\n\n## Agent Prompt Structure\n\nGood agent prompts are:\n1. **Focused** - One clear problem domain\n2. **Self-contained** - All context needed to understand the problem\n3. **Specific about output** - What should the agent return?\n\n```markdown\nFix the 3 failing tests in src/agents/agent-tool-abort.test.ts:\n\n1. \"should abort tool with partial output capture\" - expects 'interrupted at' in message\n2. \"should handle mixed completed and aborted tools\" - fast tool aborted instead of completed\n3. \"should properly track pendingToolCount\" - expects 3 results but gets 0\n\nThese are timing/race condition issues. Your task:\n\n1. Read the test file and understand what each test verifies\n2. Identify root cause - timing issues or actual bugs?\n3. Fix by:\n   - Replacing arbitrary timeouts with event-based waiting\n   - Fixing bugs in abort implementation if found\n   - Adjusting test expectations if testing changed behavior\n\nDo NOT just increase timeouts - find the real issue.\n\nReturn: Summary of what you found and what you fixed.\n```\n\n## Common Mistakes\n\n** Too broad:** \"Fix all the tests\" - agent gets lost\n** Specific:** \"Fix agent-tool-abort.test.ts\" - focused scope\n\n** No context:** \"Fix the race condition\" - agent doesn't know where\n** Context:** Paste the error messages and test names\n\n** No constraints:** Agent might refactor everything\n** Constraints:** \"Do NOT change production code\" or \"Fix tests only\"\n\n** Vague output:** \"Fix it\" - you don't know what changed\n** Specific:** \"Return summary of root cause and changes\"\n\n## When NOT to Use\n\n**Related failures:** Fixing one might fix others - investigate together first\n**Need full context:** Understanding requires seeing entire system\n**Exploratory debugging:** You don't know what's broken yet\n**Shared state:** Agents would interfere (editing same files, using same resources)\n\n## Real Example from Session\n\n**Scenario:** 6 test failures across 3 files after major refactoring\n\n**Failures:**\n- agent-tool-abort.test.ts: 3 failures (timing issues)\n- batch-completion-behavior.test.ts: 2 failures (tools not executing)\n- tool-approval-race-conditions.test.ts: 1 failure (execution count = 0)\n\n**Decision:** Independent domains - abort logic separate from batch completion separate from race conditions\n\n**Dispatch:**\n```\nAgent 1  Fix agent-tool-abort.test.ts\nAgent 2  Fix batch-completion-behavior.test.ts\nAgent 3  Fix tool-approval-race-conditions.test.ts\n```\n\n**Results:**\n- Agent 1: Replaced timeouts with event-based waiting\n- Agent 2: Fixed event structure bug (threadId in wrong place)\n- Agent 3: Added wait for async tool execution to complete\n\n**Integration:** All fixes independent, no conflicts, full suite green\n\n**Time saved:** 3 problems solved in parallel vs sequentially\n\n## Key Benefits\n\n1. **Parallelization** - Multiple investigations happen simultaneously\n2. **Focus** - Each agent has narrow scope, less context to track\n3. **Independence** - Agents don't interfere with each other\n4. **Speed** - 3 problems solved in time of 1\n\n## Verification\n\nAfter agents return:\n1. **Review each summary** - Understand what changed\n2. **Check for conflicts** - Did agents edit same code?\n3. **Run full suite** - Verify all fixes work together\n4. **Spot check** - Agents can make systematic errors\n\n## Real-World Impact\n\nFrom debugging session (2025-10-03):\n- 6 failures across 3 files\n- 3 agents dispatched in parallel\n- All investigations completed concurrently\n- All fixes integrated successfully\n- Zero conflicts between agent changes\n",
        "skills/document-chat-interface/SKILL.md": "---\nname: document-chat-interface\ndescription: Build chat interfaces for querying documents using natural language. Extract information from PDFs, GitHub repositories, emails, and other sources. Use when creating interactive document Q&A systems, knowledge base chatbots, email search interfaces, or document exploration tools.\n---\n\n# Document Chat Interface\n\nBuild intelligent chat interfaces that allow users to query and interact with documents using natural language, transforming static documents into interactive knowledge sources.\n\n## Overview\n\nA document chat interface combines three capabilities:\n1. **Document Processing** - Extract and prepare documents\n2. **Semantic Understanding** - Understand questions and find relevant content\n3. **Conversational Interface** - Maintain context and provide natural responses\n\n### Common Applications\n\n- **PDF Q&A**: Answer questions about research papers, reports, books\n- **Email Search**: Find information in email archives conversationally\n- **GitHub Explorer**: Ask questions about code repositories\n- **Knowledge Base**: Interactive access to company documentation\n- **Contract Review**: Query legal documents with natural language\n- **Research Assistant**: Explore academic papers interactively\n\n## Architecture\n\n```\nDocument Source\n    \nDocument Processor\n     Extract text\n     Process content\n     Generate embeddings\n    \nVector Database\n    \nChat Interface  User Question\n     Retrieve relevant content\n     Maintain conversation history\n     Generate response\n```\n\n## Core Components\n\n### 1. Document Sources\n\nSee [examples/document_processors.py](examples/document_processors.py) for implementations:\n\n#### PDF Documents\n- Extract text from PDF pages\n- Preserve document structure and metadata\n- Handle scanned PDFs with OCR (pytesseract)\n- Extract tables (pdfplumber)\n\n#### GitHub Repositories\n- Extract code files from repositories\n- Parse repository structure\n- Process multiple file types\n\n#### Email Archives\n- Extract email metadata (from, to, subject, date)\n- Parse email body content\n- Handle multiple mailbox formats\n\n#### Web Pages\n- Extract page text and structure\n- Preserve heading hierarchy\n- Extract links and navigation\n\n#### YouTube/Audio\n- Get transcripts from YouTube videos\n- Transcribe audio files\n- Handle multiple formats\n\n### 2. Document Processing\n\nSee [examples/text_processor.py](examples/text_processor.py) for implementations:\n\n#### Text Extraction & Cleaning\n- Remove extra whitespace and special characters\n- Smart text chunking with overlap\n- Intelligent sentence boundary detection\n\n#### Metadata Extraction\n- Extract title, author, date, language\n- Calculate word count and document statistics\n- Track document source and format\n\n#### Structure Preservation\n- Keep heading hierarchy in chunks\n- Preserve section context\n- Enable hierarchical retrieval\n\n### 3. Chat Interface Design\n\nSee [examples/conversation_manager.py](examples/conversation_manager.py) for implementations:\n\n#### Conversation Management\n- Maintain conversation history with size limits\n- Track message metadata (timestamps, roles)\n- Provide context for LLM integration\n- Clear history as needed\n\n#### Question Refinement\n- Expand implicit references in questions\n- Handle pronouns and context references\n- Improve question clarity with previous context\n\n#### Response Generation\n- Use document context for answering\n- Maintain conversation history in prompts\n- Provide source citations\n- Handle out-of-scope questions\n\n### 4. User Experience Features\n\n#### Citation & Sources\n```python\ndef format_response_with_citations(response: str, sources: List[Dict]) -> str:\n    \"\"\"Add source citations to response\"\"\"\n\n    formatted = response + \"\\n\\n**Sources:**\\n\"\n    for i, source in enumerate(sources, 1):\n        formatted += f\"[{i}] Page {source['page']} of {source['source']}\\n\"\n        if 'excerpt' in source:\n            formatted += f\"    \\\"{source['excerpt'][:100]}...\\\"\\n\"\n\n    return formatted\n```\n\n#### Clarifying Questions\n```python\ndef generate_follow_up_questions(context: str, response: str) -> List[str]:\n    \"\"\"Suggest follow-up questions to user\"\"\"\n\n    prompt = f\"\"\"\n    Based on this Q&A, generate 3 relevant follow-up questions:\n    Context: {context[:500]}\n    Response: {response[:500]}\n    \"\"\"\n\n    follow_ups = llm.generate(prompt)\n    return follow_ups\n```\n\n#### Error Handling\n```python\ndef handle_query_failure(question: str, error: Exception) -> str:\n    \"\"\"Handle when no relevant documents found\"\"\"\n\n    if isinstance(error, NoRelevantDocuments):\n        return (\n            \"I couldn't find information about that in the documents. \"\n            \"Try asking about different topics like: \"\n            + \", \".join(get_main_topics())\n        )\n    elif isinstance(error, ContextTooLarge):\n        return (\n            \"The answer requires too much context. \"\n            \"Can you be more specific about what you'd like to know?\"\n        )\n    else:\n        return f\"I encountered an issue: {str(error)[:100]}\"\n```\n\n## Implementation Frameworks\n\n### Using LangChain\n```python\nfrom langchain.document_loaders import PDFLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationalRetrievalChain\n\n# Load document\nloader = PDFLoader(\"document.pdf\")\ndocuments = loader.load()\n\n# Split into chunks\nsplitter = CharacterTextSplitter(chunk_size=1000)\nchunks = splitter.split_documents(documents)\n\n# Create embeddings\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(chunks, embeddings)\n\n# Create chat chain\nllm = ChatOpenAI(model=\"gpt-4\")\nqa = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=vectorstore.as_retriever(),\n    return_source_documents=True\n)\n\n# Chat interface\nchat_history = []\nwhile True:\n    question = input(\"You: \")\n    result = qa({\"question\": question, \"chat_history\": chat_history})\n    print(f\"Assistant: {result['answer']}\")\n    chat_history.append((question, result['answer']))\n```\n\n### Using LlamaIndex\n```python\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ChatMemoryBuffer\nfrom llama_index.llms import ChatMessage, MessageRole\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./docs\").load_data()\n\n# Create index\nindex = GPTVectorStoreIndex.from_documents(documents)\n\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(\n    memory=ChatMemoryBuffer.from_defaults(token_limit=3900),\n    llm=\"gpt-4\"\n)\n\n# Chat loop\nwhile True:\n    question = input(\"You: \")\n    response = chat_engine.chat(question)\n    print(f\"Assistant: {response}\")\n```\n\n### Using RAG-Based Approach\n```python\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\n\n# Load and embed documents\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ndocuments = load_documents(\"document.pdf\")\nembeddings = model.encode(documents)\n\n# Create FAISS index\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(np.array(embeddings).astype('float32'))\n\n# Chat function\ndef chat(question):\n    # Embed question\n    q_embedding = model.encode(question)\n\n    # Retrieve documents\n    k = 5\n    distances, indices = index.search(\n        np.array([q_embedding]).astype('float32'), k\n    )\n\n    # Get relevant documents\n    context = \" \".join([documents[i] for i in indices[0]])\n\n    # Generate response\n    response = llm.generate(\n        f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n    )\n    return response\n```\n\n## Best Practices\n\n### Document Handling\n-  Support multiple formats (PDF, TXT, docx, etc.)\n-  Handle large documents efficiently\n-  Preserve document structure\n-  Extract metadata\n-  Handle multiple languages\n-  Implement OCR for scanned PDFs\n\n### Conversation Quality\n-  Maintain conversation context\n-  Ask clarifying questions\n-  Cite sources\n-  Handle ambiguity\n-  Suggest follow-up questions\n-  Handle out-of-scope questions\n\n### Performance\n-  Optimize retrieval speed\n-  Implement caching\n-  Handle large document sets\n-  Batch process documents\n-  Monitor latency\n-  Implement pagination\n\n### User Experience\n-  Clear response formatting\n-  Ability to cite sources\n-  Document browser/explorer\n-  Search suggestions\n-  Query history\n-  Export conversations\n\n## Common Challenges & Solutions\n\n### Challenge: Irrelevant Answers\n**Solutions**:\n- Improve retrieval (more context, better embeddings)\n- Validate answer against context\n- Ask clarifying questions\n- Implement confidence scoring\n- Use hybrid search\n\n### Challenge: Lost Context Across Turns\n**Solutions**:\n- Maintain conversation memory\n- Update retrieval based on history\n- Summarize long conversations\n- Re-weight previous queries\n\n### Challenge: Handling Long Documents\n**Solutions**:\n- Hierarchical chunking\n- Summarize first\n- Question refinement\n- Multi-hop retrieval\n- Document navigation\n\n### Challenge: Limited Context Window\n**Solutions**:\n- Compress retrieved context\n- Use document summarization\n- Hierarchical retrieval\n- Focus on most relevant sections\n- Iterative refinement\n\n## Advanced Features\n\n### Multi-Document Analysis\n```python\ndef compare_documents(question: str, documents: List[str]):\n    \"\"\"Analyze and compare across multiple documents\"\"\"\n    results = []\n\n    for doc in documents:\n        response = query_document(doc, question)\n        results.append({\n            \"document\": doc.name,\n            \"answer\": response\n        })\n\n    # Compare and synthesize\n    comparison = llm.generate(\n        f\"Compare these answers: {results}\"\n    )\n    return comparison\n```\n\n### Interactive Document Exploration\n```python\nclass DocumentExplorer:\n    def __init__(self, documents):\n        self.documents = documents\n\n    def browse_by_topic(self, topic):\n        \"\"\"Find documents by topic\"\"\"\n        pass\n\n    def get_related_documents(self, doc_id):\n        \"\"\"Find similar documents\"\"\"\n        pass\n\n    def get_key_terms(self, document):\n        \"\"\"Extract key terms and concepts\"\"\"\n        pass\n```\n\n## Resources\n\n### Document Processing Libraries\n- PyPDF: PDF handling\n- python-docx: Word document handling\n- BeautifulSoup: Web scraping\n- youtube-transcript-api: YouTube transcripts\n\n### Chat Frameworks\n- LangChain: Comprehensive framework\n- LlamaIndex: Document-focused\n- RAG libraries: Vector DB integration\n\n## Implementation Checklist\n\n- [ ] Choose document source(s) to support\n- [ ] Implement document loading and processing\n- [ ] Set up vector database/embeddings\n- [ ] Build chat interface\n- [ ] Implement conversation management\n- [ ] Add source citation\n- [ ] Handle edge cases (large docs, OCR, etc.)\n- [ ] Implement error handling\n- [ ] Add performance monitoring\n- [ ] Test with real documents\n- [ ] Deploy and monitor\n\n## Getting Started\n\n1. **Start Simple**: Single PDF, basic chat\n2. **Add Features**: Multi-document, conversation history\n3. **Improve Quality**: Better chunking, retrieval\n4. **Scale**: Support more formats, larger documents\n5. **Polish**: UX improvements, error handling\n\n",
        "skills/domain-naming-engine/SKILL.md": "---\nname: domain-naming-engine\ndescription: Generate creative domain names and brand identifiers. Produces memorable, available domain suggestions with wordplay and semantic relevance.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Domain Name Brainstormer\n\nThis skill helps you find the perfect domain name for your project by generating creative options and checking what's actually available to register.\n\n## When to Use This Skill\n\n- Starting a new project or company\n- Launching a product or service\n- Creating a personal brand or portfolio site\n- Rebranding an existing project\n- Registering a domain for a side project\n- Finding available alternatives when your first choice is taken\n\n## What This Skill Does\n\n1. **Understands Your Project**: Analyzes what you're building and who it's for\n2. **Generates Creative Names**: Creates relevant, memorable domain options\n3. **Checks Availability**: Verifies which domains are actually available across multiple TLDs\n4. **Multiple Extensions**: Suggests .com, .io, .dev, .ai, .app, and more\n5. **Provides Alternatives**: Offers variations if top choices are taken\n6. **Branding Insights**: Explains why certain names work well\n\n## How to Use\n\n### Basic Brainstorming\n\n```\nI'm building a project management tool for remote teams. \nSuggest domain names.\n```\n\n```\nHelp me brainstorm domain names for a personal finance app\n```\n\n### Specific Preferences\n\n```\nI need a domain name for my AI writing assistant. \nPrefer short names with .ai or .io extension.\n```\n\n### With Keywords\n\n```\nSuggest domain names using the words \"pixel\" or \"studio\" \nfor my design agency\n```\n\n## Example\n\n**User**: \"I'm building a tool for developers to share code snippets. Suggest creative domain names.\"\n\n**Output**:\n```\nAnalyzing your project...\nTarget audience: Developers\nKey features: Code sharing, snippets, collaboration\n\n Domain Name Suggestions\n\n## Available (.com)\n1.  snippetbox.com - Clear, memorable, available!\n   Why: Directly describes the product, easy to remember\n\n2.  codeclip.com - Short and snappy\n   Why: Implies quick code sharing, only 8 characters\n\n3.  devpaste.com - Developer-focused\n   Why: Familiar to devs (like pastebin), clear purpose\n\n## Available (Alternative TLDs)\n4.  snippet.dev - Perfect extension for developers\n   Why: .dev extension signals developer tool\n\n5.  codebox.io - Tech-forward\n   Why: .io popular with tech startups, clean name\n\n6.  sharecode.app - Descriptive\n   Why: .app extension, clearly states the action\n\n## Premium/Taken (.com)\n- codeshare.com (Taken, est. $2,500)\n- snippets.com (Taken, premium domain)\n\n## Recommendations\n\n Top Pick: snippet.dev\n- Perfect for developer audience\n- Short and memorable  \n- .dev extension adds credibility\n- Available now!\n\n Runner-up: snippetbox.com\n- .com is universally recognized\n- Slightly longer but descriptive\n- Great brandability\n\nNext steps:\n1. Register your favorite before someone else does!\n2. Want me to check more variations?\n3. Need help with logo ideas for these names?\n```\n\n**Inspired by:** Ben Aiad's use case from Lenny's Newsletter\n\n## Domain Naming Tips\n\n### What Makes a Good Domain\n\n **Short**: Under 15 characters ideal\n **Memorable**: Easy to recall and spell\n **Pronounceable**: Can be said in conversation\n **Descriptive**: Hints at what you do\n **Brandable**: Unique enough to stand out\n **No hyphens**: Easier to share verbally\n\n### TLD Guide\n\n- **.com**: Universal, trusted, great for businesses\n- **.io**: Tech startups, developer tools\n- **.dev**: Developer-focused products\n- **.ai**: AI/ML products\n- **.app**: Mobile or web applications\n- **.co**: Alternative to .com\n- **.xyz**: Modern, creative projects\n- **.design**: Creative/design agencies\n- **.tech**: Technology companies\n\n## Advanced Features\n\n### Check Similar Variations\n\n```\nCheck availability for \"codebase\" and similar variations \nacross .com, .io, .dev\n```\n\n### Industry-Specific\n\n```\nSuggest domain names for a sustainable fashion brand, \nchecking .eco and .fashion TLDs\n```\n\n### Multilingual Options\n\n```\nBrainstorm domain names in English and Spanish for \na language learning app\n```\n\n### Competitor Analysis\n\n```\nShow me domain patterns used by successful project \nmanagement tools, then suggest similar available ones\n```\n\n## Example Workflows\n\n### Startup Launch\n1. Describe your startup idea\n2. Get 10-15 domain suggestions across TLDs\n3. Review availability and pricing\n4. Pick top 3 favorites\n5. Register immediately\n\n### Personal Brand\n1. Share your name and profession\n2. Get variations (firstname.com, firstnamelastname.dev, etc.)\n3. Check social media handle availability too\n4. Register consistent brand across platforms\n\n### Product Naming\n1. Describe product and target market\n2. Get creative, brandable names\n3. Check trademark conflicts\n4. Verify domain and social availability\n5. Test names with target audience\n\n## Tips for Success\n\n1. **Act Fast**: Good domains get taken quickly\n2. **Register Variations**: Get .com and .io to protect brand\n3. **Avoid Numbers**: Hard to communicate verbally\n4. **Check Social Media**: Make sure @username is available too\n5. **Say It Out Loud**: Test if it's easy to pronounce\n6. **Check Trademarks**: Ensure no legal conflicts\n7. **Think Long-term**: Will it still make sense in 5 years?\n\n## Pricing Context\n\nWhen suggesting domains, I'll note:\n- Standard domains: ~$10-15/year\n- Premium TLDs (.io, .ai): ~$30-50/year\n- Taken domains: Market price if listed\n- Premium domains: $hundreds to $thousands\n\n## Related Tools\n\nAfter picking a domain:\n- Check logo design options\n- Verify social media handles\n- Research trademark availability\n- Plan brand identity colors/fonts\n\n",
        "skills/ecommerce-platform-specialist/SKILL.md": "---\nname: ecommerce-platform-specialist\ndescription: Provide expert guidance on Shopify e-commerce platform. Advises on store setup, products, customization, and optimization.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Shopify Development Expert\n\n## Purpose\n\nProvide comprehensive, accurate guidance for building on Shopify's platform based on 24+ official documentation files. Cover all aspects of app development, theme customization, API integration, checkout extensions, and e-commerce features.\n\n## Documentation Coverage\n\n**Full access to official Shopify documentation (when available):**\n- **Location:** `docs/shopify/`\n- **Files:** 25 markdown files\n- **Coverage:** Complete API reference, guides, best practices, and implementation patterns\n\n**Note:** Documentation must be pulled separately:\n```bash\npipx install docpull\ndocpull https://shopify.dev/docs -o .claude/skills/shopify/docs\n```\n\n**Major Areas:**\n- GraphQL Admin API (products, orders, customers, inventory)\n- Storefront API (cart, checkout, customer accounts)\n- REST Admin API (legacy support)\n- App development (authentication, webhooks, extensions)\n- Theme development (Liquid, sections, blocks)\n- Headless commerce (Hydrogen, Oxygen)\n- Checkout customization (UI extensions, validation)\n- Shopify Functions (discounts, delivery, payments)\n- POS extensions (in-person sales)\n- Subscriptions and selling plans\n- Metafields and custom data\n- Shopify Flow automation\n- CLI and development tools\n- Privacy and compliance\n- Performance optimization\n\n## When to Use\n\nInvoke when user mentions:\n- **Platform:** Shopify, e-commerce, online store, merchant\n- **APIs:** GraphQL, REST, Storefront API, Admin API\n- **Products:** product management, collections, variants, inventory\n- **Orders:** order processing, fulfillment, shipping\n- **Customers:** customer data, accounts, authentication\n- **Checkout:** checkout customization, payment methods, delivery options\n- **Themes:** Liquid templates, theme development, sections, blocks\n- **Apps:** app development, extensions, webhooks, OAuth\n- **Headless:** Hydrogen, React, headless commerce, Oxygen\n- **Functions:** Shopify Functions, custom logic, discounts\n- **Subscriptions:** recurring billing, selling plans, subscriptions\n- **Tools:** Shopify CLI, development workflow\n- **POS:** point of sale, retail, in-person payments\n\n## How to Use Documentation\n\nWhen answering questions:\n\n1. **Search for specific topics:**\n   ```bash\n   # Use Grep to find relevant docs\n   grep -r \"checkout\" .claude/skills/shopify/docs/ --include=\"*.md\"\n   ```\n\n2. **Read specific documentation:**\n   ```bash\n   # API docs\n   cat .claude/skills/shopify/docs/shopify/api-admin-graphql.md\n   cat .claude/skills/shopify/docs/shopify/api-storefront.md\n   ```\n\n3. **Find implementation guides:**\n   ```bash\n   # List all guides\n   ls .claude/skills/shopify/docs/shopify/\n   ```\n\n## Core Authentication\n\n### OAuth 2.0 Flow\n\n```javascript\n// Redirect to Shopify OAuth\nconst authUrl = `https://${shop}/admin/oauth/authorize?` +\n  `client_id=${process.env.SHOPIFY_API_KEY}&` +\n  `scope=read_products,write_products&` +\n  `redirect_uri=${redirectUri}&` +\n  `state=${nonce}`;\n\n// Exchange code for access token\nconst response = await fetch(\n  `https://${shop}/admin/oauth/access_token`,\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      client_id: process.env.SHOPIFY_API_KEY,\n      client_secret: process.env.SHOPIFY_API_SECRET,\n      code\n    })\n  }\n);\n\nconst { access_token } = await response.json();\n```\n\n### Session Tokens (Modern Embedded Apps)\n\n```javascript\nimport { shopifyApi } from '@shopify/shopify-api';\n\nconst shopify = shopifyApi({\n  apiKey: process.env.SHOPIFY_API_KEY,\n  apiSecretKey: process.env.SHOPIFY_API_SECRET,\n  scopes: ['read_products', 'write_products'],\n  hostName: process.env.HOST,\n  isEmbeddedApp: true,\n});\n```\n\n## GraphQL Admin API\n\n### Query Products\n\n```graphql\nquery {\n  products(first: 10) {\n    edges {\n      node {\n        id\n        title\n        handle\n        priceRange {\n          minVariantPrice {\n            amount\n            currencyCode\n          }\n        }\n        variants(first: 5) {\n          edges {\n            node {\n              id\n              sku\n              inventoryQuantity\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n### Create Product\n\n```graphql\nmutation {\n  productCreate(input: {\n    title: \"New Product\"\n    vendor: \"My Store\"\n    productType: \"Apparel\"\n    variants: [{\n      price: \"29.99\"\n      sku: \"PROD-001\"\n    }]\n  }) {\n    product {\n      id\n      title\n    }\n    userErrors {\n      field\n      message\n    }\n  }\n}\n```\n\n### Fetch Orders\n\n```graphql\nquery {\n  orders(first: 25, query: \"fulfillment_status:unfulfilled\") {\n    edges {\n      node {\n        id\n        name\n        createdAt\n        totalPriceSet {\n          shopMoney {\n            amount\n            currencyCode\n          }\n        }\n        customer {\n          email\n        }\n        lineItems(first: 10) {\n          edges {\n            node {\n              title\n              quantity\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n## Storefront API\n\n### Create Cart\n\n```graphql\nmutation {\n  cartCreate(input: {\n    lines: [{\n      merchandiseId: \"gid://shopify/ProductVariant/123\"\n      quantity: 1\n    }]\n  }) {\n    cart {\n      id\n      checkoutUrl\n      cost {\n        totalAmount {\n          amount\n          currencyCode\n        }\n      }\n    }\n  }\n}\n```\n\n### Update Cart\n\n```graphql\nmutation {\n  cartLinesUpdate(\n    cartId: \"gid://shopify/Cart/xyz\"\n    lines: [{\n      id: \"gid://shopify/CartLine/abc\"\n      quantity: 2\n    }]\n  ) {\n    cart {\n      id\n      lines(first: 10) {\n        edges {\n          node {\n            quantity\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n## Webhooks\n\n### Setup Webhook\n\n```javascript\n// Register webhook via API\nconst webhook = await shopify.webhooks.register({\n  topic: 'ORDERS_CREATE',\n  address: 'https://your-app.com/webhooks/orders-create',\n  format: 'json'\n});\n```\n\n### Verify Webhook\n\n```javascript\nimport crypto from 'crypto';\n\nfunction verifyWebhook(body, hmacHeader, secret) {\n  const hash = crypto\n    .createHmac('sha256', secret)\n    .update(body, 'utf8')\n    .digest('base64');\n\n  return hash === hmacHeader;\n}\n\n// In webhook handler\napp.post('/webhooks/orders-create', async (req, res) => {\n  const hmac = req.headers['x-shopify-hmac-sha256'];\n  const body = await req.text();\n\n  if (!verifyWebhook(body, hmac, process.env.SHOPIFY_API_SECRET)) {\n    return res.status(401).send('Invalid HMAC');\n  }\n\n  const order = JSON.parse(body);\n  // Process order...\n\n  res.status(200).send('OK');\n});\n```\n\n## Liquid Templates\n\n### Basic Liquid\n\n```liquid\n<!-- Output product title -->\n{{ product.title }}\n\n<!-- Conditional logic -->\n{% if product.available %}\n  <button>Add to Cart</button>\n{% else %}\n  <span>Sold Out</span>\n{% endif %}\n\n<!-- Loop through variants -->\n{% for variant in product.variants %}\n  <option value=\"{{ variant.id }}\">\n    {{ variant.title }} - {{ variant.price | money }}\n  </option>\n{% endfor %}\n```\n\n### Custom Section\n\n```liquid\n{% schema %}\n{\n  \"name\": \"Featured Product\",\n  \"settings\": [\n    {\n      \"type\": \"product\",\n      \"id\": \"product\",\n      \"label\": \"Product\"\n    }\n  ]\n}\n{% endschema %}\n\n{% if section.settings.product %}\n  {% assign product = section.settings.product %}\n  <div class=\"featured-product\">\n    <img src=\"{{ product.featured_image | img_url: '500x' }}\" alt=\"{{ product.title }}\">\n    <h2>{{ product.title }}</h2>\n    <p>{{ product.price | money }}</p>\n  </div>\n{% endif %}\n```\n\n## Shopify Functions\n\n### Discount Function\n\n```javascript\n// Function to apply volume discount\nexport default (input) => {\n  const quantity = input.cart.lines.reduce((sum, line) => sum + line.quantity, 0);\n\n  let discountPercentage = 0;\n  if (quantity >= 10) discountPercentage = 20;\n  else if (quantity >= 5) discountPercentage = 10;\n\n  if (discountPercentage > 0) {\n    return {\n      discounts: [{\n        message: `${discountPercentage}% volume discount`,\n        targets: [{\n          orderSubtotal: {\n            excludedVariantIds: []\n          }\n        }],\n        value: {\n          percentage: {\n            value: discountPercentage.toString()\n          }\n        }\n      }]\n    };\n  }\n\n  return { discounts: [] };\n};\n```\n\n### Delivery Customization\n\n```javascript\n// Hide specific delivery options\nexport default (input) => {\n  const operations = [];\n\n  // Hide express shipping for orders under $100\n  const cartTotal = parseFloat(input.cart.cost.subtotalAmount.amount);\n\n  if (cartTotal < 100) {\n    const expressOptions = input.cart.deliveryGroups[0].deliveryOptions\n      .filter(option => option.title.toLowerCase().includes('express'));\n\n    expressOptions.forEach(option => {\n      operations.push({\n        hide: {\n          deliveryOptionHandle: option.handle\n        }\n      });\n    });\n  }\n\n  return { operations };\n};\n```\n\n## Hydrogen (Headless Commerce)\n\n### Product Page\n\n```typescript\n// app/routes/products.$handle.tsx\nimport {json, LoaderFunctionArgs} from '@shopify/remix-oxygen';\nimport {useLoaderData} from '@remix-run/react';\n\nexport async function loader({params, context}: LoaderFunctionArgs) {\n  const {product} = await context.storefront.query(PRODUCT_QUERY, {\n    variables: {handle: params.handle},\n  });\n\n  return json({product});\n}\n\nexport default function Product() {\n  const {product} = useLoaderData<typeof loader>();\n\n  return (\n    <div>\n      <h1>{product.title}</h1>\n      <img src={product.featuredImage.url} alt={product.title} />\n      <p>{product.description}</p>\n      <AddToCartButton productId={product.id} />\n    </div>\n  );\n}\n\nconst PRODUCT_QUERY = `#graphql\n  query Product($handle: String!) {\n    product(handle: $handle) {\n      id\n      title\n      description\n      featuredImage {\n        url\n        altText\n      }\n      variants(first: 10) {\n        nodes {\n          id\n          price {\n            amount\n            currencyCode\n          }\n        }\n      }\n    }\n  }\n`;\n```\n\n## Shopify CLI\n\n### Common Commands\n\n```bash\n# Create new app\nshopify app init\n\n# Start development server\nshopify app dev\n\n# Deploy app\nshopify app deploy\n\n# Create extension\nshopify app generate extension\n\n# Create theme\nshopify theme init\n\n# Serve theme locally\nshopify theme dev --store=your-store.myshopify.com\n\n# Push theme\nshopify theme push\n\n# Pull theme\nshopify theme pull\n```\n\n## Testing\n\n### Test Stores\n\n1. Create Partner account: https://partners.shopify.com\n2. Create development store\n3. Install your app\n4. Test features\n\n### Test Data\n\n```javascript\n// Create test product\nconst product = await shopify.rest.Product.save({\n  session,\n  title: \"Test Product\",\n  body_html: \"<strong>Test description</strong>\",\n  vendor: \"Test Vendor\",\n  product_type: \"Test Type\",\n  variants: [{\n    price: \"19.99\",\n    sku: \"TEST-001\"\n  }]\n});\n\n// Create test order\nconst order = await shopify.rest.Order.save({\n  session,\n  line_items: [{\n    variant_id: 123456789,\n    quantity: 1\n  }],\n  customer: {\n    email: \"test@example.com\"\n  }\n});\n```\n\n## Security Best Practices\n\n1. **API Keys:**\n   - Store in environment variables\n   - Never commit to version control\n   - Use separate keys per environment\n   - Rotate if compromised\n\n2. **Webhooks:**\n   - ALWAYS verify HMAC signatures\n   - Use HTTPS endpoints only\n   - Return 200 immediately\n   - Process async\n\n3. **Access Scopes:**\n   - Request minimal scopes\n   - Document why each scope is needed\n   - Review periodically\n\n4. **Rate Limits:**\n   - Respect API rate limits\n   - Implement exponential backoff\n   - Monitor API usage\n\n## Common Errors\n\n### API Authentication\n\n- `Invalid access token` - Check token is valid and has correct scopes\n- `Shop not found` - Verify shop domain format\n- `Missing access token` - Include X-Shopify-Access-Token header\n\n### GraphQL Errors\n\n- `User errors` - Check `userErrors` field in response\n- `Throttled` - Reduce request rate\n- `Field not found` - Verify API version supports field\n\n### Webhook Issues\n\n- `Invalid HMAC` - Check webhook secret and verification logic\n- `Delivery failed` - Ensure endpoint returns 200 within timeout\n- `Not receiving webhooks` - Check webhook registration and endpoint URL\n\n## Resources\n\n- **Dashboard:** https://partners.shopify.com\n- **Documentation:** https://shopify.dev\n- **GraphiQL Admin:** https://shopify.dev/docs/apps/tools/graphiql-admin-api\n- **Community:** https://community.shopify.com\n- **Status:** https://www.shopifystatus.com\n\n## Documentation Quick Reference\n\n**Need to find something specific?**\n\n```bash\n# Search all docs\ngrep -r \"search term\" .claude/skills/shopify/docs/\n\n# Find specific topics\nls .claude/skills/shopify/docs/shopify/\n\n# Read specific guide\ncat .claude/skills/shopify/docs/shopify/webhooks.md\n```\n\n**Common doc files:**\n- `api-admin-graphql.md` - GraphQL Admin API\n- `api-storefront.md` - Storefront API\n- `authentication.md` - OAuth and auth flows\n- `webhooks.md` - Webhook handling\n- `apps.md` - App development\n- `themes.md` - Theme development\n- `liquid.md` - Liquid reference\n- `hydrogen.md` - Headless commerce\n- `checkout.md` - Checkout customization\n- `functions.md` - Shopify Functions\n- `cli.md` - CLI commands",
        "skills/event-driven-serverless-systems/SKILL.md": "---\nname: event-driven-serverless-systems\ndescription: Build event-driven architectures on AWS serverless infrastructure. Designs event flows, integrates Lambda with event sources, and manages distributed systems.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# AWS Serverless & Event-Driven Architecture\n\nThis skill provides comprehensive guidance for building serverless applications and event-driven architectures on AWS based on Well-Architected Framework principles.\n\n## AWS Documentation Requirement\n\n**CRITICAL**: This skill requires AWS MCP tools for accurate, up-to-date AWS information.\n\n### Before Answering AWS Questions\n\n1. **Always verify** using AWS MCP tools (if available):\n   - `mcp__aws-mcp__aws___search_documentation` or `mcp__*awsdocs*__aws___search_documentation` - Search AWS docs\n   - `mcp__aws-mcp__aws___read_documentation` or `mcp__*awsdocs*__aws___read_documentation` - Read specific pages\n   - `mcp__aws-mcp__aws___get_regional_availability` - Check service availability\n\n2. **If AWS MCP tools are unavailable**:\n   - Guide user to configure AWS MCP: See [AWS MCP Setup Guide](../../docs/aws-mcp-setup.md)\n   - Help determine which option fits their environment:\n     - Has uvx + AWS credentials  Full AWS MCP Server\n     - No Python/credentials  AWS Documentation MCP (no auth)\n   - If cannot determine  Ask user which option to use\n\n## Serverless MCP Servers\n\nThis skill can leverage serverless-specific MCP servers for enhanced development workflows:\n\n### AWS Serverless MCP Server\n**Purpose**: Complete serverless application lifecycle with SAM CLI\n- Initialize new serverless applications\n- Deploy serverless applications\n- Test Lambda functions locally\n- Generate SAM templates\n- Manage serverless application lifecycle\n\n### AWS Lambda Tool MCP Server\n**Purpose**: Execute Lambda functions as tools\n- Invoke Lambda functions directly\n- Test Lambda integrations\n- Execute workflows requiring private resource access\n- Run Lambda-based automation\n\n### AWS Step Functions MCP Server\n**Purpose**: Execute complex workflows and orchestration\n- Create and manage state machines\n- Execute workflow orchestrations\n- Handle distributed transactions\n- Implement saga patterns\n- Coordinate microservices\n\n### Amazon SNS/SQS MCP Server\n**Purpose**: Event-driven messaging and queue management\n- Publish messages to SNS topics\n- Send/receive messages from SQS queues\n- Manage event-driven communication\n- Implement pub/sub patterns\n- Handle asynchronous processing\n\n## When to Use This Skill\n\nUse this skill when:\n- Building serverless applications with Lambda\n- Designing event-driven architectures\n- Implementing microservices patterns\n- Creating asynchronous processing workflows\n- Orchestrating multi-service transactions\n- Building real-time data processing pipelines\n- Implementing saga patterns for distributed transactions\n- Designing for scale and resilience\n\n## AWS Well-Architected Serverless Design Principles\n\n### 1. Speedy, Simple, Singular\n\n**Functions should be concise and single-purpose**\n\n```typescript\n//  GOOD - Single purpose, focused function\nexport const processOrder = async (event: OrderEvent) => {\n  // Only handles order processing\n  const order = await validateOrder(event);\n  await saveOrder(order);\n  await publishOrderCreatedEvent(order);\n  return { statusCode: 200, body: JSON.stringify({ orderId: order.id }) };\n};\n\n//  BAD - Function does too much\nexport const handleEverything = async (event: any) => {\n  // Handles orders, inventory, payments, shipping...\n  // Too many responsibilities\n};\n```\n\n**Keep functions environmentally efficient and cost-aware**:\n- Minimize cold start times\n- Optimize memory allocation\n- Use provisioned concurrency only when needed\n- Leverage connection reuse\n\n### 2. Think Concurrent Requests, Not Total Requests\n\n**Design for concurrency, not volume**\n\nLambda scales horizontally - design considerations should focus on:\n- Concurrent execution limits\n- Downstream service throttling\n- Shared resource contention\n- Connection pool sizing\n\n```typescript\n// Consider concurrent Lambda executions accessing DynamoDB\nconst table = new dynamodb.Table(this, 'Table', {\n  billingMode: dynamodb.BillingMode.PAY_PER_REQUEST, // Auto-scales with load\n});\n\n// Or with provisioned capacity + auto-scaling\nconst table = new dynamodb.Table(this, 'Table', {\n  billingMode: dynamodb.BillingMode.PROVISIONED,\n  readCapacity: 5,\n  writeCapacity: 5,\n});\n\n// Enable auto-scaling for concurrent load\ntable.autoScaleReadCapacity({ minCapacity: 5, maxCapacity: 100 });\ntable.autoScaleWriteCapacity({ minCapacity: 5, maxCapacity: 100 });\n```\n\n### 3. Share Nothing\n\n**Function runtime environments are short-lived**\n\n```typescript\n//  BAD - Relying on local file system\nexport const handler = async (event: any) => {\n  fs.writeFileSync('/tmp/data.json', JSON.stringify(data)); // Lost after execution\n};\n\n//  GOOD - Use persistent storage\nexport const handler = async (event: any) => {\n  await s3.putObject({\n    Bucket: process.env.BUCKET_NAME,\n    Key: 'data.json',\n    Body: JSON.stringify(data),\n  });\n};\n```\n\n**State management**:\n- Use DynamoDB for persistent state\n- Use Step Functions for workflow state\n- Use ElastiCache for session state\n- Use S3 for file storage\n\n### 4. Assume No Hardware Affinity\n\n**Applications must be hardware-agnostic**\n\nInfrastructure can change without notice:\n- Lambda functions can run on different hardware\n- Container instances can be replaced\n- No assumption about underlying infrastructure\n\n**Design for portability**:\n- Use environment variables for configuration\n- Avoid hardware-specific optimizations\n- Test across different environments\n\n### 5. Orchestrate with State Machines, Not Function Chaining\n\n**Use Step Functions for orchestration**\n\n```typescript\n//  BAD - Lambda function chaining\nexport const handler1 = async (event: any) => {\n  const result = await processStep1(event);\n  await lambda.invoke({\n    FunctionName: 'handler2',\n    Payload: JSON.stringify(result),\n  });\n};\n\n//  GOOD - Step Functions orchestration\nconst stateMachine = new stepfunctions.StateMachine(this, 'OrderWorkflow', {\n  definition: stepfunctions.Chain\n    .start(validateOrder)\n    .next(processPayment)\n    .next(shipOrder)\n    .next(sendConfirmation),\n});\n```\n\n**Benefits of Step Functions**:\n- Visual workflow representation\n- Built-in error handling and retries\n- Execution history and debugging\n- Parallel and sequential execution\n- Service integrations without code\n\n### 6. Use Events to Trigger Transactions\n\n**Event-driven over synchronous request/response**\n\n```typescript\n// Pattern: Event-driven processing\nconst bucket = new s3.Bucket(this, 'DataBucket');\n\nbucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.LambdaDestination(processFunction),\n  { prefix: 'uploads/' }\n);\n\n// Pattern: EventBridge integration\nconst rule = new events.Rule(this, 'OrderRule', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n});\n\nrule.addTarget(new targets.LambdaFunction(processOrderFunction));\n```\n\n**Benefits**:\n- Loose coupling between services\n- Asynchronous processing\n- Better fault tolerance\n- Independent scaling\n\n### 7. Design for Failures and Duplicates\n\n**Operations must be idempotent**\n\n```typescript\n//  GOOD - Idempotent operation\nexport const handler = async (event: SQSEvent) => {\n  for (const record of event.Records) {\n    const orderId = JSON.parse(record.body).orderId;\n\n    // Check if already processed (idempotency)\n    const existing = await dynamodb.getItem({\n      TableName: process.env.TABLE_NAME,\n      Key: { orderId },\n    });\n\n    if (existing.Item) {\n      console.log('Order already processed:', orderId);\n      continue; // Skip duplicate\n    }\n\n    // Process order\n    await processOrder(orderId);\n\n    // Mark as processed\n    await dynamodb.putItem({\n      TableName: process.env.TABLE_NAME,\n      Item: { orderId, processedAt: Date.now() },\n    });\n  }\n};\n```\n\n**Implement retry logic with exponential backoff**:\n```typescript\nasync function withRetry<T>(fn: () => Promise<T>, maxRetries = 3): Promise<T> {\n  for (let i = 0; i < maxRetries; i++) {\n    try {\n      return await fn();\n    } catch (error) {\n      if (i === maxRetries - 1) throw error;\n      await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));\n    }\n  }\n  throw new Error('Max retries exceeded');\n}\n```\n\n## Event-Driven Architecture Patterns\n\n### Pattern 1: Event Router (EventBridge)\n\nUse EventBridge for event routing and filtering:\n\n```typescript\n// Create custom event bus\nconst eventBus = new events.EventBus(this, 'AppEventBus', {\n  eventBusName: 'application-events',\n});\n\n// Define event schema\nconst schema = new events.Schema(this, 'OrderSchema', {\n  schemaName: 'OrderPlaced',\n  definition: events.SchemaDefinition.fromInline({\n    openapi: '3.0.0',\n    info: { version: '1.0.0', title: 'Order Events' },\n    paths: {},\n    components: {\n      schemas: {\n        OrderPlaced: {\n          type: 'object',\n          properties: {\n            orderId: { type: 'string' },\n            customerId: { type: 'string' },\n            amount: { type: 'number' },\n          },\n        },\n      },\n    },\n  }),\n});\n\n// Create rules for different consumers\nnew events.Rule(this, 'ProcessOrderRule', {\n  eventBus,\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n  targets: [new targets.LambdaFunction(processOrderFunction)],\n});\n\nnew events.Rule(this, 'NotifyCustomerRule', {\n  eventBus,\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n  targets: [new targets.LambdaFunction(notifyCustomerFunction)],\n});\n```\n\n### Pattern 2: Queue-Based Processing (SQS)\n\nUse SQS for reliable asynchronous processing:\n\n```typescript\n// Standard queue for at-least-once delivery\nconst queue = new sqs.Queue(this, 'ProcessingQueue', {\n  visibilityTimeout: Duration.seconds(300),\n  retentionPeriod: Duration.days(14),\n  deadLetterQueue: {\n    queue: dlq,\n    maxReceiveCount: 3,\n  },\n});\n\n// FIFO queue for ordered processing\nconst fifoQueue = new sqs.Queue(this, 'OrderedQueue', {\n  fifo: true,\n  contentBasedDeduplication: true,\n  deduplicationScope: sqs.DeduplicationScope.MESSAGE_GROUP,\n});\n\n// Lambda consumer\nnew lambda.EventSourceMapping(this, 'QueueConsumer', {\n  target: processingFunction,\n  eventSourceArn: queue.queueArn,\n  batchSize: 10,\n  maxBatchingWindow: Duration.seconds(5),\n});\n```\n\n### Pattern 3: Pub/Sub (SNS + SQS Fan-Out)\n\nImplement fan-out pattern for multiple consumers:\n\n```typescript\n// Create SNS topic\nconst topic = new sns.Topic(this, 'OrderTopic', {\n  displayName: 'Order Events',\n});\n\n// Multiple SQS queues subscribe to topic\nconst inventoryQueue = new sqs.Queue(this, 'InventoryQueue');\nconst shippingQueue = new sqs.Queue(this, 'ShippingQueue');\nconst analyticsQueue = new sqs.Queue(this, 'AnalyticsQueue');\n\ntopic.addSubscription(new subscriptions.SqsSubscription(inventoryQueue));\ntopic.addSubscription(new subscriptions.SqsSubscription(shippingQueue));\ntopic.addSubscription(new subscriptions.SqsSubscription(analyticsQueue));\n\n// Each queue has its own Lambda consumer\nnew lambda.EventSourceMapping(this, 'InventoryConsumer', {\n  target: inventoryFunction,\n  eventSourceArn: inventoryQueue.queueArn,\n});\n```\n\n### Pattern 4: Saga Pattern with Step Functions\n\nImplement distributed transactions:\n\n```typescript\nconst reserveFlight = new tasks.LambdaInvoke(this, 'ReserveFlight', {\n  lambdaFunction: reserveFlightFunction,\n  outputPath: '$.Payload',\n});\n\nconst reserveHotel = new tasks.LambdaInvoke(this, 'ReserveHotel', {\n  lambdaFunction: reserveHotelFunction,\n  outputPath: '$.Payload',\n});\n\nconst processPayment = new tasks.LambdaInvoke(this, 'ProcessPayment', {\n  lambdaFunction: processPaymentFunction,\n  outputPath: '$.Payload',\n});\n\n// Compensating transactions\nconst cancelFlight = new tasks.LambdaInvoke(this, 'CancelFlight', {\n  lambdaFunction: cancelFlightFunction,\n});\n\nconst cancelHotel = new tasks.LambdaInvoke(this, 'CancelHotel', {\n  lambdaFunction: cancelHotelFunction,\n});\n\n// Define saga with compensation\nconst definition = reserveFlight\n  .next(reserveHotel)\n  .next(processPayment)\n  .addCatch(cancelHotel.next(cancelFlight), {\n    resultPath: '$.error',\n  });\n\nnew stepfunctions.StateMachine(this, 'BookingStateMachine', {\n  definition,\n  timeout: Duration.minutes(5),\n});\n```\n\n### Pattern 5: Event Sourcing\n\nStore events as source of truth:\n\n```typescript\n// Event store with DynamoDB\nconst eventStore = new dynamodb.Table(this, 'EventStore', {\n  partitionKey: { name: 'aggregateId', type: dynamodb.AttributeType.STRING },\n  sortKey: { name: 'version', type: dynamodb.AttributeType.NUMBER },\n  stream: dynamodb.StreamViewType.NEW_IMAGE,\n});\n\n// Lambda function stores events\nexport const handleCommand = async (event: any) => {\n  const { aggregateId, eventType, eventData } = event;\n\n  // Get current version\n  const items = await dynamodb.query({\n    TableName: process.env.EVENT_STORE,\n    KeyConditionExpression: 'aggregateId = :id',\n    ExpressionAttributeValues: { ':id': aggregateId },\n    ScanIndexForward: false,\n    Limit: 1,\n  });\n\n  const nextVersion = items.Items?.[0]?.version + 1 || 1;\n\n  // Append new event\n  await dynamodb.putItem({\n    TableName: process.env.EVENT_STORE,\n    Item: {\n      aggregateId,\n      version: nextVersion,\n      eventType,\n      eventData,\n      timestamp: Date.now(),\n    },\n  });\n};\n\n// Projections read from event stream\neventStore.grantStreamRead(projectionFunction);\n```\n\n## Serverless Architecture Patterns\n\n### Pattern 1: API-Driven Microservices\n\nREST APIs with Lambda backend:\n\n```typescript\nconst api = new apigateway.RestApi(this, 'Api', {\n  restApiName: 'microservices-api',\n  deployOptions: {\n    throttlingRateLimit: 1000,\n    throttlingBurstLimit: 2000,\n    tracingEnabled: true,\n  },\n});\n\n// User service\nconst users = api.root.addResource('users');\nusers.addMethod('GET', new apigateway.LambdaIntegration(getUsersFunction));\nusers.addMethod('POST', new apigateway.LambdaIntegration(createUserFunction));\n\n// Order service\nconst orders = api.root.addResource('orders');\norders.addMethod('GET', new apigateway.LambdaIntegration(getOrdersFunction));\norders.addMethod('POST', new apigateway.LambdaIntegration(createOrderFunction));\n```\n\n### Pattern 2: Stream Processing\n\nReal-time data processing with Kinesis:\n\n```typescript\nconst stream = new kinesis.Stream(this, 'DataStream', {\n  shardCount: 2,\n  retentionPeriod: Duration.days(7),\n});\n\n// Lambda processes stream records\nnew lambda.EventSourceMapping(this, 'StreamProcessor', {\n  target: processFunction,\n  eventSourceArn: stream.streamArn,\n  batchSize: 100,\n  maxBatchingWindow: Duration.seconds(5),\n  parallelizationFactor: 10,\n  startingPosition: lambda.StartingPosition.LATEST,\n  retryAttempts: 3,\n  bisectBatchOnError: true,\n  onFailure: new lambdaDestinations.SqsDestination(dlq),\n});\n```\n\n### Pattern 3: Async Task Processing\n\nBackground job processing:\n\n```typescript\n// SQS queue for tasks\nconst taskQueue = new sqs.Queue(this, 'TaskQueue', {\n  visibilityTimeout: Duration.minutes(5),\n  receiveMessageWaitTime: Duration.seconds(20), // Long polling\n  deadLetterQueue: {\n    queue: dlq,\n    maxReceiveCount: 3,\n  },\n});\n\n// Lambda worker processes tasks\nconst worker = new lambda.Function(this, 'TaskWorker', {\n  // ... configuration\n  reservedConcurrentExecutions: 10, // Control concurrency\n});\n\nnew lambda.EventSourceMapping(this, 'TaskConsumer', {\n  target: worker,\n  eventSourceArn: taskQueue.queueArn,\n  batchSize: 10,\n  reportBatchItemFailures: true, // Partial batch failure handling\n});\n```\n\n### Pattern 4: Scheduled Jobs\n\nPeriodic processing with EventBridge:\n\n```typescript\n// Daily cleanup job\nnew events.Rule(this, 'DailyCleanup', {\n  schedule: events.Schedule.cron({ hour: '2', minute: '0' }),\n  targets: [new targets.LambdaFunction(cleanupFunction)],\n});\n\n// Process every 5 minutes\nnew events.Rule(this, 'FrequentProcessing', {\n  schedule: events.Schedule.rate(Duration.minutes(5)),\n  targets: [new targets.LambdaFunction(processFunction)],\n});\n```\n\n### Pattern 5: Webhook Processing\n\nHandle external webhooks:\n\n```typescript\n// API Gateway endpoint for webhooks\nconst webhookApi = new apigateway.RestApi(this, 'WebhookApi', {\n  restApiName: 'webhooks',\n});\n\nconst webhook = webhookApi.root.addResource('webhook');\nwebhook.addMethod('POST', new apigateway.LambdaIntegration(webhookFunction, {\n  proxy: true,\n  timeout: Duration.seconds(29), // API Gateway max\n}));\n\n// Lambda handler validates and queues webhook\nexport const handler = async (event: APIGatewayProxyEvent) => {\n  // Validate webhook signature\n  const isValid = validateSignature(event.headers, event.body);\n  if (!isValid) {\n    return { statusCode: 401, body: 'Invalid signature' };\n  }\n\n  // Queue for async processing\n  await sqs.sendMessage({\n    QueueUrl: process.env.QUEUE_URL,\n    MessageBody: event.body,\n  });\n\n  // Return immediately\n  return { statusCode: 202, body: 'Accepted' };\n};\n```\n\n## Best Practices\n\n### Error Handling\n\n**Implement comprehensive error handling**:\n\n```typescript\nexport const handler = async (event: SQSEvent) => {\n  const failures: SQSBatchItemFailure[] = [];\n\n  for (const record of event.Records) {\n    try {\n      await processRecord(record);\n    } catch (error) {\n      console.error('Failed to process record:', record.messageId, error);\n      failures.push({ itemIdentifier: record.messageId });\n    }\n  }\n\n  // Return partial batch failures for retry\n  return { batchItemFailures: failures };\n};\n```\n\n### Dead Letter Queues\n\n**Always configure DLQs for error handling**:\n\n```typescript\nconst dlq = new sqs.Queue(this, 'DLQ', {\n  retentionPeriod: Duration.days(14),\n});\n\nconst queue = new sqs.Queue(this, 'Queue', {\n  deadLetterQueue: {\n    queue: dlq,\n    maxReceiveCount: 3,\n  },\n});\n\n// Monitor DLQ depth\nnew cloudwatch.Alarm(this, 'DLQAlarm', {\n  metric: dlq.metricApproximateNumberOfMessagesVisible(),\n  threshold: 1,\n  evaluationPeriods: 1,\n  alarmDescription: 'Messages in DLQ require attention',\n});\n```\n\n### Observability\n\n**Enable tracing and monitoring**:\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  tracing: lambda.Tracing.ACTIVE, // X-Ray tracing\n  environment: {\n    POWERTOOLS_SERVICE_NAME: 'order-service',\n    POWERTOOLS_METRICS_NAMESPACE: 'MyApp',\n    LOG_LEVEL: 'INFO',\n  },\n});\n```\n\n## Using MCP Servers Effectively\n\n### AWS Serverless MCP Usage\n\n**Lifecycle management**:\n- Initialize new serverless projects\n- Generate SAM templates\n- Deploy applications\n- Test locally before deployment\n\n### Lambda Tool MCP Usage\n\n**Function execution**:\n- Test Lambda functions directly\n- Execute automation workflows\n- Access private resources\n- Validate integrations\n\n### Step Functions MCP Usage\n\n**Workflow orchestration**:\n- Create state machines for complex workflows\n- Execute distributed transactions\n- Implement saga patterns\n- Coordinate microservices\n\n### SNS/SQS MCP Usage\n\n**Messaging operations**:\n- Test pub/sub patterns\n- Send test messages to queues\n- Validate event routing\n- Debug message processing\n\n## Additional Resources\n\nThis skill includes comprehensive reference documentation based on AWS best practices:\n\n- **Serverless Patterns**: `references/serverless-patterns.md`\n  - Core serverless architectures and API patterns\n  - Data processing and integration patterns\n  - Orchestration with Step Functions\n  - Anti-patterns to avoid\n\n- **Event-Driven Architecture Patterns**: `references/eda-patterns.md`\n  - Event routing and processing patterns\n  - Event sourcing and saga patterns\n  - Idempotency and error handling\n  - Message ordering and deduplication\n\n- **Security Best Practices**: `references/security-best-practices.md`\n  - Shared responsibility model\n  - IAM least privilege patterns\n  - Data protection and encryption\n  - Network security with VPC\n\n- **Observability Best Practices**: `references/observability-best-practices.md`\n  - Three pillars: metrics, logs, traces\n  - Structured logging with Lambda Powertools\n  - X-Ray distributed tracing\n  - CloudWatch alarms and dashboards\n\n- **Performance Optimization**: `references/performance-optimization.md`\n  - Cold start optimization techniques\n  - Memory and CPU optimization\n  - Package size reduction\n  - Provisioned concurrency patterns\n\n- **Deployment Best Practices**: `references/deployment-best-practices.md`\n  - CI/CD pipeline design\n  - Testing strategies (unit, integration, load)\n  - Deployment strategies (canary, blue/green)\n  - Rollback and safety mechanisms\n\n**External Resources**:\n- **AWS Well-Architected Serverless Lens**: https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/\n- **ServerlessLand.com**: Pre-built serverless patterns\n- **AWS Serverless Workshops**: https://serverlessland.com/learn?type=Workshops\n\nFor detailed implementation patterns, anti-patterns, and code examples, refer to the comprehensive references in the skill directory.\n",
        "skills/event-driven-serverless-systems/references/deployment-best-practices.md": "# Serverless Deployment Best Practices\n\nDeployment best practices for serverless applications including CI/CD, testing, and deployment strategies.\n\n## Table of Contents\n\n- [Software Release Process](#software-release-process)\n- [Infrastructure as Code](#infrastructure-as-code)\n- [CI/CD Pipeline Design](#cicd-pipeline-design)\n- [Testing Strategies](#testing-strategies)\n- [Deployment Strategies](#deployment-strategies)\n- [Rollback and Safety](#rollback-and-safety)\n\n## Software Release Process\n\n### Four Stages of Release\n\n**1. Source Phase**:\n- Developers commit code changes\n- Code review (peer review)\n- Version control (Git)\n\n**2. Build Phase**:\n- Compile code\n- Run unit tests\n- Style checking and linting\n- Create deployment packages\n- Build container images\n\n**3. Test Phase**:\n- Integration tests with other systems\n- Load testing\n- UI testing\n- Security testing (penetration testing)\n- Acceptance testing\n\n**4. Production Phase**:\n- Deploy to production environment\n- Monitor for errors\n- Validate deployment success\n- Rollback if needed\n\n### CI/CD Maturity Levels\n\n**Continuous Integration (CI)**:\n- Automated build on code commit\n- Automated unit testing\n- Manual deployment to test/production\n\n**Continuous Delivery (CD)**:\n- Automated deployment to test environments\n- Manual approval for production\n- Automated testing in non-prod\n\n**Continuous Deployment**:\n- Fully automated pipeline\n- Automated deployment to production\n- No manual intervention after code commit\n\n## Infrastructure as Code\n\n### Framework Selection\n\n**AWS SAM (Serverless Application Model)**:\n\n```yaml\n# template.yaml\nAWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\n\nResources:\n  OrderFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: app.handler\n      Runtime: nodejs20.x\n      CodeUri: src/\n      Events:\n        Api:\n          Type: Api\n          Properties:\n            Path: /orders\n            Method: post\n```\n\n**Benefits**:\n- Simple, serverless-focused syntax\n- Built-in best practices\n- SAM CLI for local testing\n- Integrates with CodeDeploy\n\n**AWS CDK**:\n\n```typescript\nnew NodejsFunction(this, 'OrderFunction', {\n  entry: 'src/orders/handler.ts',\n  environment: {\n    TABLE_NAME: ordersTable.tableName,\n  },\n});\n\nordersTable.grantReadWriteData(orderFunction);\n```\n\n**Benefits**:\n- Type-safe, programmatic\n- Reusable constructs\n- Rich AWS service support\n- Better for complex infrastructure\n\n**When to use**:\n- **SAM**: Serverless-only applications, simpler projects\n- **CDK**: Complex infrastructure, multiple services, reusable patterns\n\n### Environment Management\n\n**Separate environments**:\n\n```typescript\n// CDK App\nconst app = new cdk.App();\n\nnew ServerlessStack(app, 'DevStack', {\n  env: { account: '111111111111', region: 'us-east-1' },\n  environment: 'dev',\n  logLevel: 'DEBUG',\n});\n\nnew ServerlessStack(app, 'ProdStack', {\n  env: { account: '222222222222', region: 'us-east-1' },\n  environment: 'prod',\n  logLevel: 'INFO',\n});\n```\n\n**SAM with parameters**:\n\n```yaml\nParameters:\n  Environment:\n    Type: String\n    Default: dev\n    AllowedValues:\n      - dev\n      - staging\n      - prod\n\nResources:\n  Function:\n    Type: AWS::Serverless::Function\n    Properties:\n      Environment:\n        Variables:\n          ENVIRONMENT: !Ref Environment\n          LOG_LEVEL: !If [IsProd, INFO, DEBUG]\n```\n\n## CI/CD Pipeline Design\n\n### AWS CodePipeline\n\n**Comprehensive pipeline**:\n\n```typescript\nimport * as codepipeline from 'aws-cdk-lib/aws-codepipeline';\nimport * as codepipeline_actions from 'aws-cdk-lib/aws-codepipeline-actions';\n\nconst sourceOutput = new codepipeline.Artifact();\nconst buildOutput = new codepipeline.Artifact();\n\nconst pipeline = new codepipeline.Pipeline(this, 'Pipeline', {\n  pipelineName: 'serverless-pipeline',\n});\n\n// Source stage\npipeline.addStage({\n  stageName: 'Source',\n  actions: [\n    new codepipeline_actions.CodeStarConnectionsSourceAction({\n      actionName: 'GitHub_Source',\n      owner: 'myorg',\n      repo: 'myrepo',\n      branch: 'main',\n      output: sourceOutput,\n      connectionArn: githubConnection.connectionArn,\n    }),\n  ],\n});\n\n// Build stage\npipeline.addStage({\n  stageName: 'Build',\n  actions: [\n    new codepipeline_actions.CodeBuildAction({\n      actionName: 'Build',\n      project: buildProject,\n      input: sourceOutput,\n      outputs: [buildOutput],\n    }),\n  ],\n});\n\n// Test stage\npipeline.addStage({\n  stageName: 'Test',\n  actions: [\n    new codepipeline_actions.CloudFormationCreateUpdateStackAction({\n      actionName: 'Deploy_Test',\n      templatePath: buildOutput.atPath('packaged.yaml'),\n      stackName: 'test-stack',\n      adminPermissions: true,\n    }),\n    new codepipeline_actions.CodeBuildAction({\n      actionName: 'Integration_Tests',\n      project: testProject,\n      input: buildOutput,\n      runOrder: 2,\n    }),\n  ],\n});\n\n// Production stage (with manual approval)\npipeline.addStage({\n  stageName: 'Production',\n  actions: [\n    new codepipeline_actions.ManualApprovalAction({\n      actionName: 'Approve',\n    }),\n    new codepipeline_actions.CloudFormationCreateUpdateStackAction({\n      actionName: 'Deploy_Prod',\n      templatePath: buildOutput.atPath('packaged.yaml'),\n      stackName: 'prod-stack',\n      adminPermissions: true,\n      runOrder: 2,\n    }),\n  ],\n});\n```\n\n### GitHub Actions\n\n**Serverless deployment workflow**:\n\n```yaml\n# .github/workflows/deploy.yml\nname: Deploy Serverless Application\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '20'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run tests\n        run: npm test\n\n      - name: Setup SAM CLI\n        uses: aws-actions/setup-sam@v2\n\n      - name: Build SAM application\n        run: sam build\n\n      - name: Deploy to Dev\n        if: github.ref != 'refs/heads/main'\n        run: |\n          sam deploy \\\n            --no-confirm-changeset \\\n            --no-fail-on-empty-changeset \\\n            --stack-name dev-stack \\\n            --parameter-overrides Environment=dev\n\n      - name: Run integration tests\n        run: npm run test:integration\n\n      - name: Deploy to Prod\n        if: github.ref == 'refs/heads/main'\n        run: |\n          sam deploy \\\n            --no-confirm-changeset \\\n            --no-fail-on-empty-changeset \\\n            --stack-name prod-stack \\\n            --parameter-overrides Environment=prod\n```\n\n## Testing Strategies\n\n### Unit Testing\n\n**Test business logic independently**:\n\n```typescript\n// handler.ts\nexport const processOrder = (order: Order): ProcessedOrder => {\n  // Pure business logic (easily testable)\n  validateOrder(order);\n  calculateTotal(order);\n  return transformOrder(order);\n};\n\nexport const handler = async (event: any) => {\n  const order = parseEvent(event);\n  const processed = processOrder(order); // Testable function\n  await saveToDatabase(processed);\n  return formatResponse(processed);\n};\n\n// handler.test.ts\nimport { processOrder } from './handler';\n\ndescribe('processOrder', () => {\n  it('calculates total correctly', () => {\n    const order = {\n      items: [\n        { price: 10, quantity: 2 },\n        { price: 5, quantity: 3 },\n      ],\n    };\n\n    const result = processOrder(order);\n\n    expect(result.total).toBe(35);\n  });\n\n  it('throws on invalid order', () => {\n    const invalid = { items: [] };\n    expect(() => processOrder(invalid)).toThrow();\n  });\n});\n```\n\n### Integration Testing\n\n**Test in actual AWS environment**:\n\n```typescript\n// integration.test.ts\nimport { LambdaClient, InvokeCommand } from '@aws-sdk/client-lambda';\nimport { DynamoDBClient, GetItemCommand } from '@aws-sdk/client-dynamodb';\n\ndescribe('Order Processing Integration', () => {\n  const lambda = new LambdaClient({});\n  const dynamodb = new DynamoDBClient({});\n\n  it('processes order end-to-end', async () => {\n    // Invoke Lambda\n    const response = await lambda.send(new InvokeCommand({\n      FunctionName: process.env.FUNCTION_NAME,\n      Payload: JSON.stringify({\n        orderId: 'test-123',\n        items: [{ productId: 'prod-1', quantity: 2 }],\n      }),\n    }));\n\n    const result = JSON.parse(Buffer.from(response.Payload!).toString());\n\n    expect(result.statusCode).toBe(200);\n\n    // Verify database write\n    const dbResult = await dynamodb.send(new GetItemCommand({\n      TableName: process.env.TABLE_NAME,\n      Key: { orderId: { S: 'test-123' } },\n    }));\n\n    expect(dbResult.Item).toBeDefined();\n    expect(dbResult.Item?.status.S).toBe('PROCESSED');\n  });\n});\n```\n\n### Local Testing with SAM\n\n**Test locally before deployment**:\n\n```bash\n# Start local API\nsam local start-api\n\n# Invoke function locally\nsam local invoke OrderFunction -e events/create-order.json\n\n# Generate sample events\nsam local generate-event apigateway aws-proxy > event.json\n\n# Debug locally\nsam local invoke OrderFunction -d 5858\n\n# Test with Docker\nsam local start-api --docker-network my-network\n```\n\n### Load Testing\n\n**Test under production load**:\n\n```bash\n# Install Artillery\nnpm install -g artillery\n\n# Create load test\ncat > load-test.yml <<EOF\nconfig:\n  target: https://api.example.com\n  phases:\n    - duration: 300 # 5 minutes\n      arrivalRate: 50 # 50 requests/second\n      rampTo: 200 # Ramp to 200 req/sec\nscenarios:\n  - flow:\n      - post:\n          url: /orders\n          json:\n            orderId: \"{{ $randomString() }}\"\nEOF\n\n# Run load test\nartillery run load-test.yml --output report.json\n\n# Generate HTML report\nartillery report report.json\n```\n\n## Deployment Strategies\n\n### All-at-Once Deployment\n\n**Simple, fast, risky**:\n\n```yaml\n# SAM template\nResources:\n  OrderFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      DeploymentPreference:\n        Type: AllAtOnce # Deploy immediately\n```\n\n**Use for**:\n- Development environments\n- Non-critical applications\n- Quick hotfixes (with caution)\n\n### Blue/Green Deployment\n\n**Zero-downtime deployment**:\n\n```yaml\nResources:\n  OrderFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      AutoPublishAlias: live\n      DeploymentPreference:\n        Type: Linear10PercentEvery1Minute\n        Alarms:\n          - !Ref ErrorAlarm\n          - !Ref LatencyAlarm\n```\n\n**Deployment types**:\n- **Linear10PercentEvery1Minute**: 10% traffic shift every minute\n- **Linear10PercentEvery2Minutes**: Slower, more conservative\n- **Linear10PercentEvery3Minutes**: Even slower\n- **Linear10PercentEvery10Minutes**: Very gradual\n- **Canary10Percent5Minutes**: 10% for 5 min, then 100%\n- **Canary10Percent10Minutes**: 10% for 10 min, then 100%\n- **Canary10Percent30Minutes**: 10% for 30 min, then 100%\n\n### Canary Deployment\n\n**Test with subset of traffic**:\n\n```yaml\nResources:\n  OrderFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      AutoPublishAlias: live\n      DeploymentPreference:\n        Type: Canary10Percent10Minutes\n        Alarms:\n          - !Ref ErrorAlarm\n          - !Ref LatencyAlarm\n        Hooks:\n          PreTraffic: !Ref PreTrafficHook\n          PostTraffic: !Ref PostTrafficHook\n\n  PreTrafficHook:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: hooks.pre_traffic\n      Runtime: python3.12\n      # Runs before traffic shift\n      # Validates new version\n\n  PostTrafficHook:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: hooks.post_traffic\n      Runtime: python3.12\n      # Runs after traffic shift\n      # Validates deployment success\n```\n\n**CDK with CodeDeploy**:\n\n```typescript\nimport * as codedeploy from 'aws-cdk-lib/aws-codedeploy';\n\nconst alias = fn.currentVersion.addAlias('live');\n\nnew codedeploy.LambdaDeploymentGroup(this, 'DeploymentGroup', {\n  alias,\n  deploymentConfig: codedeploy.LambdaDeploymentConfig.CANARY_10PERCENT_10MINUTES,\n  alarms: [errorAlarm, latencyAlarm],\n  autoRollback: {\n    failedDeployment: true,\n    stoppedDeployment: true,\n    deploymentInAlarm: true,\n  },\n});\n```\n\n### Deployment Hooks\n\n**Pre-traffic hook (validation)**:\n\n```python\n# hooks.py\nimport boto3\n\nlambda_client = boto3.client('lambda')\ncodedeploy = boto3.client('codedeploy')\n\ndef pre_traffic(event, context):\n    \"\"\"\n    Validate new version before traffic shift\n    \"\"\"\n    function_name = event['DeploymentId']\n    version = event['NewVersion']\n\n    try:\n        # Invoke new version with test payload\n        response = lambda_client.invoke(\n            FunctionName=f\"{function_name}:{version}\",\n            InvocationType='RequestResponse',\n            Payload=json.dumps({'test': True})\n        )\n\n        # Validate response\n        if response['StatusCode'] == 200:\n            codedeploy.put_lifecycle_event_hook_execution_status(\n                deploymentId=event['DeploymentId'],\n                lifecycleEventHookExecutionId=event['LifecycleEventHookExecutionId'],\n                status='Succeeded'\n            )\n        else:\n            raise Exception('Validation failed')\n\n    except Exception as e:\n        print(f'Pre-traffic validation failed: {e}')\n        codedeploy.put_lifecycle_event_hook_execution_status(\n            deploymentId=event['DeploymentId'],\n            lifecycleEventHookExecutionId=event['LifecycleEventHookExecutionId'],\n            status='Failed'\n        )\n```\n\n**Post-traffic hook (verification)**:\n\n```python\ndef post_traffic(event, context):\n    \"\"\"\n    Verify deployment success after traffic shift\n    \"\"\"\n    try:\n        # Check CloudWatch metrics\n        cloudwatch = boto3.client('cloudwatch')\n\n        metrics = cloudwatch.get_metric_statistics(\n            Namespace='AWS/Lambda',\n            MetricName='Errors',\n            Dimensions=[{'Name': 'FunctionName', 'Value': function_name}],\n            StartTime=deployment_start_time,\n            EndTime=datetime.utcnow(),\n            Period=300,\n            Statistics=['Sum']\n        )\n\n        # Validate no errors\n        total_errors = sum(point['Sum'] for point in metrics['Datapoints'])\n\n        if total_errors == 0:\n            codedeploy.put_lifecycle_event_hook_execution_status(\n                deploymentId=event['DeploymentId'],\n                lifecycleEventHookExecutionId=event['LifecycleEventHookExecutionId'],\n                status='Succeeded'\n            )\n        else:\n            raise Exception(f'{total_errors} errors detected')\n\n    except Exception as e:\n        print(f'Post-traffic verification failed: {e}')\n        codedeploy.put_lifecycle_event_hook_execution_status(\n            deploymentId=event['DeploymentId'],\n            lifecycleEventHookExecutionId=event['LifecycleEventHookExecutionId'],\n            status='Failed'\n        )\n```\n\n## Rollback and Safety\n\n### Automatic Rollback\n\n**Configure rollback triggers**:\n\n```yaml\nDeploymentPreference:\n  Type: Canary10Percent10Minutes\n  Alarms:\n    - !Ref ErrorAlarm\n    - !Ref LatencyAlarm\n  # Automatically rolls back if alarms trigger\n```\n\n**Rollback scenarios**:\n- CloudWatch alarm triggers during deployment\n- Pre-traffic hook fails\n- Post-traffic hook fails\n- Deployment manually stopped\n\n### CloudWatch Alarms for Deployment\n\n**Critical alarms during deployment**:\n\n```typescript\n// Error rate alarm\nconst errorAlarm = new cloudwatch.Alarm(this, 'ErrorAlarm', {\n  metric: fn.metricErrors({\n    statistic: 'Sum',\n    period: Duration.minutes(1),\n  }),\n  threshold: 5,\n  evaluationPeriods: 2,\n  treatMissingData: cloudwatch.TreatMissingData.NOT_BREACHING,\n});\n\n// Duration alarm (regression)\nconst durationAlarm = new cloudwatch.Alarm(this, 'DurationAlarm', {\n  metric: fn.metricDuration({\n    statistic: 'Average',\n    period: Duration.minutes(1),\n  }),\n  threshold: previousAvgDuration * 1.2, // 20% increase\n  evaluationPeriods: 2,\n  comparisonOperator: cloudwatch.ComparisonOperator.GREATER_THAN_THRESHOLD,\n});\n\n// Throttle alarm\nconst throttleAlarm = new cloudwatch.Alarm(this, 'ThrottleAlarm', {\n  metric: fn.metricThrottles({\n    statistic: 'Sum',\n    period: Duration.minutes(1),\n  }),\n  threshold: 1,\n  evaluationPeriods: 1,\n});\n```\n\n### Version Management\n\n**Use Lambda versions and aliases**:\n\n```typescript\nconst version = fn.currentVersion;\n\nconst prodAlias = version.addAlias('prod');\nconst devAlias = version.addAlias('dev');\n\n// Gradual rollout with weighted aliases\nnew lambda.Alias(this, 'LiveAlias', {\n  aliasName: 'live',\n  version: newVersion,\n  additionalVersions: [\n    { version: oldVersion, weight: 0.9 }, // 90% old\n    // 10% automatically goes to main version (new)\n  ],\n});\n```\n\n## Best Practices Checklist\n\n### Pre-Deployment\n\n- [ ] Code review completed\n- [ ] Unit tests passing\n- [ ] Integration tests passing\n- [ ] Security scan completed\n- [ ] Dependencies updated\n- [ ] Infrastructure validated (CDK synth, SAM validate)\n- [ ] Environment variables configured\n\n### Deployment\n\n- [ ] Use IaC (SAM, CDK, Terraform)\n- [ ] Separate environments (dev, staging, prod)\n- [ ] Automate deployments via CI/CD\n- [ ] Use gradual deployment (canary or linear)\n- [ ] Configure CloudWatch alarms\n- [ ] Enable automatic rollback\n- [ ] Use deployment hooks for validation\n\n### Post-Deployment\n\n- [ ] Monitor CloudWatch metrics\n- [ ] Check CloudWatch Logs for errors\n- [ ] Verify X-Ray traces\n- [ ] Validate business metrics\n- [ ] Check alarm status\n- [ ] Review deployment logs\n- [ ] Document any issues\n\n### Rollback Preparation\n\n- [ ] Keep previous version available\n- [ ] Document rollback procedure\n- [ ] Test rollback in non-prod\n- [ ] Configure automatic rollback\n- [ ] Monitor during rollback\n- [ ] Communication plan for rollback\n\n## Deployment Patterns\n\n### Multi-Region Deployment\n\n**Active-Passive**:\n\n```typescript\n// Primary region\nnew ServerlessStack(app, 'PrimaryStack', {\n  env: { region: 'us-east-1' },\n  isPrimary: true,\n});\n\n// Secondary region (standby)\nnew ServerlessStack(app, 'SecondaryStack', {\n  env: { region: 'us-west-2' },\n  isPrimary: false,\n});\n\n// Route 53 health check and failover\nconst healthCheck = new route53.CfnHealthCheck(this, 'HealthCheck', {\n  type: 'HTTPS',\n  resourcePath: '/health',\n  fullyQualifiedDomainName: 'api.example.com',\n});\n```\n\n**Active-Active**:\n\n```typescript\n// Deploy to multiple regions\nconst regions = ['us-east-1', 'us-west-2', 'eu-west-1'];\n\nfor (const region of regions) {\n  new ServerlessStack(app, `Stack-${region}`, {\n    env: { region },\n  });\n}\n\n// Route 53 geolocation routing\nnew route53.ARecord(this, 'GeoRecord', {\n  zone: hostedZone,\n  recordName: 'api',\n  target: route53.RecordTarget.fromAlias(\n    new targets.ApiGatewayDomain(domain)\n  ),\n  geoLocation: route53.GeoLocation.country('US'),\n});\n```\n\n### Feature Flags with AppConfig\n\n**Safe feature rollout**:\n\n```typescript\nimport { AppConfigData } from '@aws-sdk/client-appconfigdata';\n\nconst appconfig = new AppConfigData({});\n\nexport const handler = async (event: any) => {\n  // Fetch feature flags\n  const config = await appconfig.getLatestConfiguration({\n    ConfigurationToken: token,\n  });\n\n  const features = JSON.parse(config.Configuration.toString());\n\n  if (features.newFeatureEnabled) {\n    return newFeatureHandler(event);\n  }\n\n  return legacyHandler(event);\n};\n```\n\n## Summary\n\n- **IaC**: Use SAM or CDK for all deployments\n- **Environments**: Separate dev, staging, production\n- **CI/CD**: Automate build, test, and deployment\n- **Testing**: Unit, integration, and load testing\n- **Gradual Deployment**: Use canary or linear for production\n- **Alarms**: Configure and monitor during deployment\n- **Rollback**: Enable automatic rollback on failures\n- **Hooks**: Validate before and after traffic shifts\n- **Versioning**: Use Lambda versions and aliases\n- **Multi-Region**: Plan for disaster recovery\n",
        "skills/event-driven-serverless-systems/references/eda-patterns.md": "# Event-Driven Architecture Patterns\n\nComprehensive patterns for building event-driven systems on AWS with serverless technologies.\n\n## Table of Contents\n\n- [Core EDA Concepts](#core-eda-concepts)\n- [Event Routing Patterns](#event-routing-patterns)\n- [Event Processing Patterns](#event-processing-patterns)\n- [Event Sourcing Patterns](#event-sourcing-patterns)\n- [Saga Patterns](#saga-patterns)\n- [Best Practices](#best-practices)\n\n## Core EDA Concepts\n\n### Event Types\n\n**Domain Events**: Represent business facts\n```json\n{\n  \"source\": \"orders\",\n  \"detailType\": \"OrderPlaced\",\n  \"detail\": {\n    \"orderId\": \"12345\",\n    \"customerId\": \"customer-1\",\n    \"amount\": 100.00,\n    \"timestamp\": \"2025-01-15T10:30:00Z\"\n  }\n}\n```\n\n**System Events**: Technical occurrences\n```json\n{\n  \"source\": \"aws.s3\",\n  \"detailType\": \"Object Created\",\n  \"detail\": {\n    \"bucket\": \"my-bucket\",\n    \"key\": \"data/file.json\"\n  }\n}\n```\n\n### Event Contracts\n\nDefine clear contracts between producers and consumers:\n\n```typescript\n// schemas/order-events.ts\nexport interface OrderPlacedEvent {\n  orderId: string;\n  customerId: string;\n  items: Array<{\n    productId: string;\n    quantity: number;\n    price: number;\n  }>;\n  totalAmount: number;\n  timestamp: string;\n}\n\n// Register schema with EventBridge\nconst registry = new events.EventBusSchemaRegistry(this, 'SchemaRegistry');\n\nconst schema = new events.Schema(this, 'OrderPlacedSchema', {\n  schemaName: 'OrderPlaced',\n  definition: events.SchemaDefinition.fromInline(/* JSON Schema */),\n});\n```\n\n## Event Routing Patterns\n\n### Pattern 1: Content-Based Routing\n\nRoute events based on content:\n\n```typescript\n// Route by order amount\nnew events.Rule(this, 'HighValueOrders', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n    detail: {\n      totalAmount: [{ numeric: ['>', 1000] }],\n    },\n  },\n  targets: [new targets.LambdaFunction(highValueOrderFunction)],\n});\n\nnew events.Rule(this, 'StandardOrders', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n    detail: {\n      totalAmount: [{ numeric: ['<=', 1000] }],\n    },\n  },\n  targets: [new targets.LambdaFunction(standardOrderFunction)],\n});\n```\n\n### Pattern 2: Event Filtering\n\nFilter events before processing:\n\n```typescript\n// Filter by multiple criteria\nnew events.Rule(this, 'FilteredRule', {\n  eventPattern: {\n    source: ['inventory'],\n    detailType: ['StockUpdate'],\n    detail: {\n      warehouseId: ['WH-1', 'WH-2'], // Specific warehouses\n      quantity: [{ numeric: ['<', 10] }], // Low stock only\n      productCategory: ['electronics'], // Specific category\n    },\n  },\n  targets: [new targets.LambdaFunction(reorderFunction)],\n});\n```\n\n### Pattern 3: Event Replay and Archive\n\nStore events for replay and audit:\n\n```typescript\n// Archive all events\nconst archive = new events.Archive(this, 'EventArchive', {\n  eventPattern: {\n    account: [this.account],\n  },\n  retention: Duration.days(365),\n});\n\n// Replay events when needed\n// Use AWS Console or CLI to replay from archive\n```\n\n### Pattern 4: Cross-Account Event Routing\n\nRoute events to other AWS accounts:\n\n```typescript\n// Event bus in Account A\nconst eventBus = new events.EventBus(this, 'SharedBus');\n\n// Grant permission to Account B\neventBus.addToResourcePolicy(new iam.PolicyStatement({\n  effect: iam.Effect.ALLOW,\n  principals: [new iam.AccountPrincipal('ACCOUNT-B-ID')],\n  actions: ['events:PutEvents'],\n  resources: [eventBus.eventBusArn],\n}));\n\n// Rule forwards to Account B event bus\nnew events.Rule(this, 'ForwardToAccountB', {\n  eventBus,\n  eventPattern: {\n    source: ['shared-service'],\n  },\n  targets: [new targets.EventBus(\n    events.EventBus.fromEventBusArn(\n      this,\n      'AccountBBus',\n      'arn:aws:events:us-east-1:ACCOUNT-B-ID:event-bus/default'\n    )\n  )],\n});\n```\n\n## Event Processing Patterns\n\n### Pattern 1: Event Transformation\n\nTransform events before routing:\n\n```typescript\n// EventBridge input transformer\nnew events.Rule(this, 'TransformRule', {\n  eventPattern: {\n    source: ['orders'],\n  },\n  targets: [new targets.LambdaFunction(processFunction, {\n    event: events.RuleTargetInput.fromObject({\n      orderId: events.EventField.fromPath('$.detail.orderId'),\n      customerEmail: events.EventField.fromPath('$.detail.customer.email'),\n      amount: events.EventField.fromPath('$.detail.totalAmount'),\n      // Transformed structure\n    }),\n  })],\n});\n```\n\n### Pattern 2: Event Aggregation\n\nAggregate multiple events:\n\n```typescript\n// DynamoDB stores partial results\nexport const handler = async (event: any) => {\n  const { transactionId, step, data } = event;\n\n  // Store step result\n  await dynamodb.updateItem({\n    TableName: process.env.TABLE_NAME,\n    Key: { transactionId },\n    UpdateExpression: 'SET #step = :data',\n    ExpressionAttributeNames: { '#step': step },\n    ExpressionAttributeValues: { ':data': data },\n  });\n\n  // Check if all steps complete\n  const item = await dynamodb.getItem({\n    TableName: process.env.TABLE_NAME,\n    Key: { transactionId },\n  });\n\n  if (allStepsComplete(item)) {\n    // Trigger final processing\n    await eventBridge.putEvents({\n      Entries: [{\n        Source: 'aggregator',\n        DetailType: 'AllStepsComplete',\n        Detail: JSON.stringify(item),\n      }],\n    });\n  }\n};\n```\n\n### Pattern 3: Event Enrichment\n\nEnrich events with additional data:\n\n```typescript\nexport const enrichEvent = async (event: any) => {\n  const { customerId } = event.detail;\n\n  // Fetch additional customer data\n  const customer = await dynamodb.getItem({\n    TableName: process.env.CUSTOMER_TABLE,\n    Key: { customerId },\n  });\n\n  // Publish enriched event\n  await eventBridge.putEvents({\n    Entries: [{\n      Source: 'orders',\n      DetailType: 'OrderEnriched',\n      Detail: JSON.stringify({\n        ...event.detail,\n        customerName: customer.Item?.name,\n        customerTier: customer.Item?.tier,\n        customerEmail: customer.Item?.email,\n      }),\n    }],\n  });\n};\n```\n\n### Pattern 4: Event Fork and Join\n\nProcess event multiple ways then aggregate:\n\n```typescript\n// Step Functions parallel + aggregation\nconst parallel = new stepfunctions.Parallel(this, 'ForkProcessing');\n\nparallel.branch(new tasks.LambdaInvoke(this, 'ValidateInventory', {\n  lambdaFunction: inventoryFunction,\n  resultPath: '$.inventory',\n}));\n\nparallel.branch(new tasks.LambdaInvoke(this, 'CheckCredit', {\n  lambdaFunction: creditFunction,\n  resultPath: '$.credit',\n}));\n\nparallel.branch(new tasks.LambdaInvoke(this, 'CalculateShipping', {\n  lambdaFunction: shippingFunction,\n  resultPath: '$.shipping',\n}));\n\nconst definition = parallel.next(\n  new tasks.LambdaInvoke(this, 'AggregateResults', {\n    lambdaFunction: aggregateFunction,\n  })\n);\n```\n\n## Event Sourcing Patterns\n\n### Pattern: Event Store with DynamoDB\n\nStore all events as source of truth:\n\n```typescript\nconst eventStore = new dynamodb.Table(this, 'EventStore', {\n  partitionKey: { name: 'aggregateId', type: dynamodb.AttributeType.STRING },\n  sortKey: { name: 'version', type: dynamodb.AttributeType.NUMBER },\n  stream: dynamodb.StreamViewType.NEW_IMAGE,\n  pointInTimeRecovery: true, // Important for audit\n});\n\n// Append events\nexport const appendEvent = async (aggregateId: string, event: any) => {\n  const version = await getNextVersion(aggregateId);\n\n  await dynamodb.putItem({\n    TableName: process.env.EVENT_STORE,\n    Item: {\n      aggregateId,\n      version,\n      eventType: event.type,\n      eventData: event.data,\n      timestamp: Date.now(),\n      userId: event.userId,\n    },\n    ConditionExpression: 'attribute_not_exists(version)', // Optimistic locking\n  });\n};\n\n// Rebuild state from events\nexport const rebuildState = async (aggregateId: string) => {\n  const events = await dynamodb.query({\n    TableName: process.env.EVENT_STORE,\n    KeyConditionExpression: 'aggregateId = :id',\n    ExpressionAttributeValues: { ':id': aggregateId },\n    ScanIndexForward: true, // Chronological order\n  });\n\n  let state = initialState();\n  for (const event of events.Items) {\n    state = applyEvent(state, event);\n  }\n\n  return state;\n};\n```\n\n### Pattern: Materialized Views\n\nCreate read-optimized projections:\n\n```typescript\n// Event store stream triggers projection\neventStore.grantStreamRead(projectionFunction);\n\nnew lambda.EventSourceMapping(this, 'Projection', {\n  target: projectionFunction,\n  eventSourceArn: eventStore.tableStreamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n});\n\n// Projection function updates read model\nexport const updateProjection = async (event: DynamoDBStreamEvent) => {\n  for (const record of event.Records) {\n    if (record.eventName !== 'INSERT') continue;\n\n    const eventData = record.dynamodb?.NewImage;\n    const aggregateId = eventData?.aggregateId.S;\n\n    // Rebuild current state\n    const currentState = await rebuildState(aggregateId);\n\n    // Update read model\n    await readModelTable.putItem({\n      TableName: process.env.READ_MODEL_TABLE,\n      Item: currentState,\n    });\n  }\n};\n```\n\n### Pattern: Snapshots\n\nOptimize event replay with snapshots:\n\n```typescript\nexport const createSnapshot = async (aggregateId: string) => {\n  // Rebuild state from all events\n  const state = await rebuildState(aggregateId);\n  const version = await getLatestVersion(aggregateId);\n\n  // Store snapshot\n  await snapshotTable.putItem({\n    TableName: process.env.SNAPSHOT_TABLE,\n    Item: {\n      aggregateId,\n      version,\n      state: JSON.stringify(state),\n      createdAt: Date.now(),\n    },\n  });\n};\n\n// Rebuild from snapshot + newer events\nexport const rebuildFromSnapshot = async (aggregateId: string) => {\n  // Get latest snapshot\n  const snapshot = await getLatestSnapshot(aggregateId);\n\n  let state = JSON.parse(snapshot.state);\n  const snapshotVersion = snapshot.version;\n\n  // Apply only events after snapshot\n  const events = await getEventsSinceVersion(aggregateId, snapshotVersion);\n\n  for (const event of events) {\n    state = applyEvent(state, event);\n  }\n\n  return state;\n};\n```\n\n## Saga Patterns\n\n### Pattern: Choreography-Based Saga\n\nServices coordinate through events:\n\n```typescript\n// Order Service publishes event\nexport const placeOrder = async (order: Order) => {\n  await saveOrder(order);\n\n  await eventBridge.putEvents({\n    Entries: [{\n      Source: 'orders',\n      DetailType: 'OrderPlaced',\n      Detail: JSON.stringify({ orderId: order.id }),\n    }],\n  });\n};\n\n// Inventory Service reacts to event\nnew events.Rule(this, 'ReserveInventory', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n  targets: [new targets.LambdaFunction(reserveInventoryFunction)],\n});\n\n// Inventory Service publishes result\nexport const reserveInventory = async (event: any) => {\n  const { orderId } = event.detail;\n\n  try {\n    await reserve(orderId);\n\n    await eventBridge.putEvents({\n      Entries: [{\n        Source: 'inventory',\n        DetailType: 'InventoryReserved',\n        Detail: JSON.stringify({ orderId }),\n      }],\n    });\n  } catch (error) {\n    await eventBridge.putEvents({\n      Entries: [{\n        Source: 'inventory',\n        DetailType: 'InventoryReservationFailed',\n        Detail: JSON.stringify({ orderId, error: error.message }),\n      }],\n    });\n  }\n};\n\n// Payment Service reacts to inventory event\nnew events.Rule(this, 'ProcessPayment', {\n  eventPattern: {\n    source: ['inventory'],\n    detailType: ['InventoryReserved'],\n  },\n  targets: [new targets.LambdaFunction(processPaymentFunction)],\n});\n```\n\n### Pattern: Orchestration-Based Saga\n\nCentral coordinator manages saga:\n\n```typescript\n// Step Functions orchestrates saga\nconst definition = new tasks.LambdaInvoke(this, 'ReserveInventory', {\n  lambdaFunction: reserveInventoryFunction,\n  resultPath: '$.inventory',\n})\n  .next(new tasks.LambdaInvoke(this, 'ProcessPayment', {\n    lambdaFunction: processPaymentFunction,\n    resultPath: '$.payment',\n  }))\n  .next(new tasks.LambdaInvoke(this, 'ShipOrder', {\n    lambdaFunction: shipOrderFunction,\n    resultPath: '$.shipment',\n  }))\n  .addCatch(\n    // Compensation flow\n    new tasks.LambdaInvoke(this, 'RefundPayment', {\n      lambdaFunction: refundFunction,\n    })\n      .next(new tasks.LambdaInvoke(this, 'ReleaseInventory', {\n        lambdaFunction: releaseFunction,\n      })),\n    {\n      errors: ['States.TaskFailed'],\n      resultPath: '$.error',\n    }\n  );\n\nnew stepfunctions.StateMachine(this, 'OrderSaga', {\n  definition,\n  tracingEnabled: true,\n});\n```\n\n**Comparison**:\n\n| Aspect | Choreography | Orchestration |\n|--------|--------------|---------------|\n| Coordination | Decentralized | Centralized |\n| Coupling | Loose | Tighter |\n| Visibility | Distributed logs | Single execution history |\n| Debugging | Harder (trace across services) | Easier (single workflow) |\n| Best for | Simple flows | Complex flows |\n\n## Best Practices\n\n### Idempotency\n\n**Always make event handlers idempotent**:\n\n```typescript\n// Use idempotency keys\nexport const handler = async (event: any) => {\n  const idempotencyKey = event.requestId || event.messageId;\n\n  // Check if already processed\n  try {\n    const existing = await dynamodb.getItem({\n      TableName: process.env.IDEMPOTENCY_TABLE,\n      Key: { idempotencyKey },\n    });\n\n    if (existing.Item) {\n      console.log('Already processed:', idempotencyKey);\n      return existing.Item.result; // Return cached result\n    }\n  } catch (error) {\n    // First time processing\n  }\n\n  // Process event\n  const result = await processEvent(event);\n\n  // Store result\n  await dynamodb.putItem({\n    TableName: process.env.IDEMPOTENCY_TABLE,\n    Item: {\n      idempotencyKey,\n      result,\n      processedAt: Date.now(),\n    },\n    // Optional: Set TTL for cleanup\n    ExpirationTime: Math.floor(Date.now() / 1000) + 86400, // 24 hours\n  });\n\n  return result;\n};\n```\n\n### Event Versioning\n\n**Handle event schema evolution**:\n\n```typescript\n// Version events\ninterface OrderPlacedEventV1 {\n  version: '1.0';\n  orderId: string;\n  amount: number;\n}\n\ninterface OrderPlacedEventV2 {\n  version: '2.0';\n  orderId: string;\n  amount: number;\n  currency: string; // New field\n}\n\n// Handler supports multiple versions\nexport const handler = async (event: any) => {\n  const eventVersion = event.detail.version || '1.0';\n\n  switch (eventVersion) {\n    case '1.0':\n      return processV1(event.detail as OrderPlacedEventV1);\n    case '2.0':\n      return processV2(event.detail as OrderPlacedEventV2);\n    default:\n      throw new Error(`Unsupported event version: ${eventVersion}`);\n  }\n};\n\nconst processV1 = async (event: OrderPlacedEventV1) => {\n  // Upgrade to V2 internally\n  const v2Event: OrderPlacedEventV2 = {\n    ...event,\n    version: '2.0',\n    currency: 'USD', // Default value\n  };\n  return processV2(v2Event);\n};\n```\n\n### Eventual Consistency\n\n**Design for eventual consistency**:\n\n```typescript\n// Service A writes to its database\nexport const createOrder = async (order: Order) => {\n  // Write to Order database\n  await orderTable.putItem({ Item: order });\n\n  // Publish event\n  await eventBridge.putEvents({\n    Entries: [{\n      Source: 'orders',\n      DetailType: 'OrderCreated',\n      Detail: JSON.stringify({ orderId: order.id }),\n    }],\n  });\n};\n\n// Service B eventually updates its database\nexport const onOrderCreated = async (event: any) => {\n  const { orderId } = event.detail;\n\n  // Fetch additional data\n  const orderDetails = await getOrderDetails(orderId);\n\n  // Update inventory database (eventual consistency)\n  await inventoryTable.updateItem({\n    Key: { productId: orderDetails.productId },\n    UpdateExpression: 'SET reserved = reserved + :qty',\n    ExpressionAttributeValues: { ':qty': orderDetails.quantity },\n  });\n};\n```\n\n### Error Handling in EDA\n\n**Comprehensive error handling strategy**:\n\n```typescript\n// Dead Letter Queue for failed events\nconst dlq = new sqs.Queue(this, 'EventDLQ', {\n  retentionPeriod: Duration.days(14),\n});\n\n// EventBridge rule with DLQ\nnew events.Rule(this, 'ProcessRule', {\n  eventPattern: { /* ... */ },\n  targets: [\n    new targets.LambdaFunction(processFunction, {\n      deadLetterQueue: dlq,\n      maxEventAge: Duration.hours(2),\n      retryAttempts: 2,\n    }),\n  ],\n});\n\n// Monitor DLQ\nnew cloudwatch.Alarm(this, 'DLQAlarm', {\n  metric: dlq.metricApproximateNumberOfMessagesVisible(),\n  threshold: 1,\n  evaluationPeriods: 1,\n});\n\n// DLQ processor for manual review\nnew lambda.EventSourceMapping(this, 'DLQProcessor', {\n  target: dlqProcessorFunction,\n  eventSourceArn: dlq.queueArn,\n  enabled: false, // Enable manually when reviewing\n});\n```\n\n### Message Ordering\n\n**When order matters**:\n\n```typescript\n// SQS FIFO for strict ordering\nconst fifoQueue = new sqs.Queue(this, 'OrderedQueue', {\n  fifo: true,\n  contentBasedDeduplication: true,\n  deduplicationScope: sqs.DeduplicationScope.MESSAGE_GROUP,\n  fifoThroughputLimit: sqs.FifoThroughputLimit.PER_MESSAGE_GROUP_ID,\n});\n\n// Publish with message group ID\nawait sqs.sendMessage({\n  QueueUrl: process.env.QUEUE_URL,\n  MessageBody: JSON.stringify(event),\n  MessageGroupId: customerId, // All messages for same customer in order\n  MessageDeduplicationId: eventId, // Prevent duplicates\n});\n\n// Kinesis for ordered streams\nconst stream = new kinesis.Stream(this, 'Stream', {\n  shardCount: 1, // Single shard = strict ordering\n});\n\n// Partition key ensures same partition\nawait kinesis.putRecord({\n  StreamName: process.env.STREAM_NAME,\n  Data: Buffer.from(JSON.stringify(event)),\n  PartitionKey: customerId, // Same key = same shard\n});\n```\n\n### Deduplication\n\n**Prevent duplicate event processing**:\n\n```typescript\n// Content-based deduplication with SQS FIFO\nconst queue = new sqs.Queue(this, 'Queue', {\n  fifo: true,\n  contentBasedDeduplication: true, // Hash of message body\n});\n\n// Manual deduplication with DynamoDB\nexport const handler = async (event: any) => {\n  const eventId = event.id || event.messageId;\n\n  try {\n    // Conditional write (fails if exists)\n    await dynamodb.putItem({\n      TableName: process.env.DEDUP_TABLE,\n      Item: {\n        eventId,\n        processedAt: Date.now(),\n        ttl: Math.floor(Date.now() / 1000) + 86400, // 24h TTL\n      },\n      ConditionExpression: 'attribute_not_exists(eventId)',\n    });\n\n    // Event is unique, process it\n    await processEvent(event);\n  } catch (error) {\n    if (error.code === 'ConditionalCheckFailedException') {\n      console.log('Duplicate event ignored:', eventId);\n      return; // Already processed\n    }\n    throw error; // Other error\n  }\n};\n```\n\n### Backpressure Handling\n\n**Prevent overwhelming downstream systems**:\n\n```typescript\n// Control Lambda concurrency\nconst consumerFunction = new lambda.Function(this, 'Consumer', {\n  reservedConcurrentExecutions: 10, // Max 10 concurrent\n});\n\n// SQS visibility timeout + retry logic\nconst queue = new sqs.Queue(this, 'Queue', {\n  visibilityTimeout: Duration.seconds(300), // 5 minutes\n  receiveMessageWaitTime: Duration.seconds(20), // Long polling\n});\n\nnew lambda.EventSourceMapping(this, 'Consumer', {\n  target: consumerFunction,\n  eventSourceArn: queue.queueArn,\n  batchSize: 10,\n  maxConcurrency: 5, // Process 5 batches concurrently\n  reportBatchItemFailures: true,\n});\n\n// Circuit breaker pattern\nlet consecutiveFailures = 0;\nconst FAILURE_THRESHOLD = 5;\n\nexport const handler = async (event: any) => {\n  // Check circuit breaker\n  if (consecutiveFailures >= FAILURE_THRESHOLD) {\n    console.error('Circuit breaker open, skipping processing');\n    throw new Error('Circuit breaker open');\n  }\n\n  try {\n    await processEvent(event);\n    consecutiveFailures = 0; // Reset on success\n  } catch (error) {\n    consecutiveFailures++;\n    throw error;\n  }\n};\n```\n\n## Advanced Patterns\n\n### Pattern: Event Replay\n\nReplay events for recovery or testing:\n\n```typescript\n// Archive events for replay\nconst archive = new events.Archive(this, 'Archive', {\n  sourceEventBus: eventBus,\n  eventPattern: {\n    account: [this.account],\n  },\n  retention: Duration.days(365),\n});\n\n// Replay programmatically\nexport const replayEvents = async (startTime: Date, endTime: Date) => {\n  // Use AWS SDK to start replay\n  await eventBridge.startReplay({\n    ReplayName: `replay-${Date.now()}`,\n    EventSourceArn: archive.archiveArn,\n    EventStartTime: startTime,\n    EventEndTime: endTime,\n    Destination: {\n      Arn: eventBus.eventBusArn,\n    },\n  });\n};\n```\n\n### Pattern: Event Time vs Processing Time\n\nHandle late-arriving events:\n\n```typescript\n// Include event timestamp\ninterface Event {\n  eventId: string;\n  eventTime: string; // When event occurred\n  processingTime?: string; // When event processed\n  data: any;\n}\n\n// Windowed aggregation\nexport const aggregateWindow = async (events: Event[]) => {\n  // Group by event time window (not processing time)\n  const windows = new Map<string, Event[]>();\n\n  for (const event of events) {\n    const window = getWindowForTime(new Date(event.eventTime), Duration.minutes(5));\n    const key = window.toISOString();\n\n    if (!windows.has(key)) {\n      windows.set(key, []);\n    }\n    windows.get(key)!.push(event);\n  }\n\n  // Process each window\n  for (const [window, eventsInWindow] of windows) {\n    await processWindow(window, eventsInWindow);\n  }\n};\n```\n\n### Pattern: Transactional Outbox\n\nEnsure event publishing with database writes:\n\n```typescript\n// Single DynamoDB transaction\nexport const createOrderWithEvent = async (order: Order) => {\n  await dynamodb.transactWriteItems({\n    TransactItems: [\n      {\n        // Write order\n        Put: {\n          TableName: process.env.ORDERS_TABLE,\n          Item: marshall(order),\n        },\n      },\n      {\n        // Write outbox event\n        Put: {\n          TableName: process.env.OUTBOX_TABLE,\n          Item: marshall({\n            eventId: uuid(),\n            eventType: 'OrderPlaced',\n            eventData: order,\n            status: 'PENDING',\n            createdAt: Date.now(),\n          }),\n        },\n      },\n    ],\n  });\n};\n\n// Separate Lambda processes outbox\nnew lambda.EventSourceMapping(this, 'OutboxProcessor', {\n  target: outboxFunction,\n  eventSourceArn: outboxTable.tableStreamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n});\n\nexport const processOutbox = async (event: DynamoDBStreamEvent) => {\n  for (const record of event.Records) {\n    if (record.eventName !== 'INSERT') continue;\n\n    const outboxEvent = unmarshall(record.dynamodb?.NewImage);\n\n    // Publish to EventBridge\n    await eventBridge.putEvents({\n      Entries: [{\n        Source: 'orders',\n        DetailType: outboxEvent.eventType,\n        Detail: JSON.stringify(outboxEvent.eventData),\n      }],\n    });\n\n    // Mark as processed\n    await dynamodb.updateItem({\n      TableName: process.env.OUTBOX_TABLE,\n      Key: { eventId: outboxEvent.eventId },\n      UpdateExpression: 'SET #status = :status',\n      ExpressionAttributeNames: { '#status': 'status' },\n      ExpressionAttributeValues: { ':status': 'PUBLISHED' },\n    });\n  }\n};\n```\n\n## Testing Event-Driven Systems\n\n### Pattern: Event Replay for Testing\n\n```typescript\n// Publish test events\nexport const publishTestEvents = async () => {\n  const testEvents = [\n    { source: 'orders', detailType: 'OrderPlaced', detail: { orderId: '1' } },\n    { source: 'orders', detailType: 'OrderPlaced', detail: { orderId: '2' } },\n  ];\n\n  for (const event of testEvents) {\n    await eventBridge.putEvents({ Entries: [event] });\n  }\n};\n\n// Monitor processing\nexport const verifyProcessing = async () => {\n  // Check downstream databases\n  const order1 = await orderTable.getItem({ Key: { orderId: '1' } });\n  const order2 = await orderTable.getItem({ Key: { orderId: '2' } });\n\n  expect(order1.Item).toBeDefined();\n  expect(order2.Item).toBeDefined();\n};\n```\n\n### Pattern: Event Mocking\n\n```typescript\n// Mock EventBridge in tests\nconst mockEventBridge = {\n  putEvents: jest.fn().mockResolvedValue({}),\n};\n\n// Test event publishing\ntest('publishes event on order creation', async () => {\n  await createOrder(mockEventBridge, order);\n\n  expect(mockEventBridge.putEvents).toHaveBeenCalledWith({\n    Entries: [\n      expect.objectContaining({\n        Source: 'orders',\n        DetailType: 'OrderPlaced',\n      }),\n    ],\n  });\n});\n```\n\n## Summary\n\n- **Loose Coupling**: Services communicate via events, not direct calls\n- **Async Processing**: Use queues and event buses for asynchronous workflows\n- **Idempotency**: Always handle duplicate events gracefully\n- **Dead Letter Queues**: Configure DLQs for error handling\n- **Event Contracts**: Define clear schemas for events\n- **Observability**: Enable tracing and monitoring across services\n- **Eventual Consistency**: Design for it, don't fight it\n- **Saga Patterns**: Use for distributed transactions\n- **Event Sourcing**: Store events as source of truth when needed\n",
        "skills/event-driven-serverless-systems/references/observability-best-practices.md": "# Serverless Observability Best Practices\n\nComprehensive observability patterns for serverless applications based on AWS best practices.\n\n## Table of Contents\n\n- [Three Pillars of Observability](#three-pillars-of-observability)\n- [Metrics](#metrics)\n- [Logging](#logging)\n- [Tracing](#tracing)\n- [Unified Observability](#unified-observability)\n- [Alerting](#alerting)\n\n## Three Pillars of Observability\n\n### Metrics\n**Numeric data measured at intervals (time series)**\n- Request rate, error rate, duration\n- CPU%, memory%, disk%\n- Custom business metrics\n- Service Level Indicators (SLIs)\n\n### Logs\n**Timestamped records of discrete events**\n- Application events and errors\n- State transformations\n- Debugging information\n- Audit trails\n\n### Traces\n**Single user's journey across services**\n- Request flow through distributed system\n- Service dependencies\n- Latency breakdown\n- Error propagation\n\n## Metrics\n\n### CloudWatch Metrics for Lambda\n\n**Out-of-the-box metrics** (automatically available):\n```\n- Invocations\n- Errors\n- Throttles\n- Duration\n- ConcurrentExecutions\n- IteratorAge (for streams)\n```\n\n**CDK Configuration**:\n```typescript\nconst fn = new NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n});\n\n// Create alarms on metrics\nnew cloudwatch.Alarm(this, 'ErrorAlarm', {\n  metric: fn.metricErrors({\n    statistic: 'Sum',\n    period: Duration.minutes(5),\n  }),\n  threshold: 10,\n  evaluationPeriods: 1,\n});\n\nnew cloudwatch.Alarm(this, 'DurationAlarm', {\n  metric: fn.metricDuration({\n    statistic: 'p99',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1000, // 1 second\n  evaluationPeriods: 2,\n});\n```\n\n### Custom Metrics\n\n**Use CloudWatch Embedded Metric Format (EMF)**:\n\n```typescript\nexport const handler = async (event: any) => {\n  const startTime = Date.now();\n\n  try {\n    const result = await processOrder(event);\n\n    // Emit custom metrics\n    console.log(JSON.stringify({\n      _aws: {\n        Timestamp: Date.now(),\n        CloudWatchMetrics: [{\n          Namespace: 'MyApp/Orders',\n          Dimensions: [['ServiceName', 'Operation']],\n          Metrics: [\n            { Name: 'ProcessingTime', Unit: 'Milliseconds' },\n            { Name: 'OrderValue', Unit: 'None' },\n          ],\n        }],\n      },\n      ServiceName: 'OrderService',\n      Operation: 'ProcessOrder',\n      ProcessingTime: Date.now() - startTime,\n      OrderValue: result.amount,\n    }));\n\n    return result;\n  } catch (error) {\n    // Emit error metric\n    console.log(JSON.stringify({\n      _aws: {\n        CloudWatchMetrics: [{\n          Namespace: 'MyApp/Orders',\n          Dimensions: [['ServiceName']],\n          Metrics: [{ Name: 'Errors', Unit: 'Count' }],\n        }],\n      },\n      ServiceName: 'OrderService',\n      Errors: 1,\n    }));\n\n    throw error;\n  }\n};\n```\n\n**Using Lambda Powertools**:\n\n```typescript\nimport { Metrics, MetricUnits } from '@aws-lambda-powertools/metrics';\n\nconst metrics = new Metrics({\n  namespace: 'MyApp',\n  serviceName: 'OrderService',\n});\n\nexport const handler = async (event: any) => {\n  metrics.addMetric('Invocation', MetricUnits.Count, 1);\n\n  const startTime = Date.now();\n\n  try {\n    const result = await processOrder(event);\n\n    metrics.addMetric('Success', MetricUnits.Count, 1);\n    metrics.addMetric('ProcessingTime', MetricUnits.Milliseconds, Date.now() - startTime);\n    metrics.addMetric('OrderValue', MetricUnits.None, result.amount);\n\n    return result;\n  } catch (error) {\n    metrics.addMetric('Error', MetricUnits.Count, 1);\n    throw error;\n  } finally {\n    metrics.publishStoredMetrics();\n  }\n};\n```\n\n## Logging\n\n### Structured Logging\n\n**Use JSON format for logs**:\n\n```typescript\n//  GOOD - Structured JSON logging\nexport const handler = async (event: any) => {\n  console.log(JSON.stringify({\n    level: 'INFO',\n    message: 'Processing order',\n    orderId: event.orderId,\n    customerId: event.customerId,\n    timestamp: new Date().toISOString(),\n    requestId: context.requestId,\n  }));\n\n  try {\n    const result = await processOrder(event);\n\n    console.log(JSON.stringify({\n      level: 'INFO',\n      message: 'Order processed successfully',\n      orderId: event.orderId,\n      duration: Date.now() - startTime,\n      timestamp: new Date().toISOString(),\n    }));\n\n    return result;\n  } catch (error) {\n    console.error(JSON.stringify({\n      level: 'ERROR',\n      message: 'Order processing failed',\n      orderId: event.orderId,\n      error: {\n        name: error.name,\n        message: error.message,\n        stack: error.stack,\n      },\n      timestamp: new Date().toISOString(),\n    }));\n\n    throw error;\n  }\n};\n\n//  BAD - Unstructured logging\nconsole.log('Processing order ' + orderId + ' for customer ' + customerId);\n```\n\n**Using Lambda Powertools Logger**:\n\n```typescript\nimport { Logger } from '@aws-lambda-powertools/logger';\n\nconst logger = new Logger({\n  serviceName: 'OrderService',\n  logLevel: 'INFO',\n});\n\nexport const handler = async (event: any, context: Context) => {\n  logger.addContext(context);\n\n  logger.info('Processing order', {\n    orderId: event.orderId,\n    customerId: event.customerId,\n  });\n\n  try {\n    const result = await processOrder(event);\n\n    logger.info('Order processed', {\n      orderId: event.orderId,\n      amount: result.amount,\n    });\n\n    return result;\n  } catch (error) {\n    logger.error('Order processing failed', {\n      orderId: event.orderId,\n      error,\n    });\n\n    throw error;\n  }\n};\n```\n\n### Log Levels\n\n**Use appropriate log levels**:\n- **ERROR**: Errors requiring immediate attention\n- **WARN**: Warnings or recoverable errors\n- **INFO**: Important business events\n- **DEBUG**: Detailed debugging information (disable in production)\n\n```typescript\nconst logger = new Logger({\n  serviceName: 'OrderService',\n  logLevel: process.env.LOG_LEVEL || 'INFO',\n});\n\nlogger.debug('Detailed processing info', { data });\nlogger.info('Business event occurred', { event });\nlogger.warn('Recoverable error', { error });\nlogger.error('Critical failure', { error });\n```\n\n### Log Insights Queries\n\n**Common CloudWatch Logs Insights queries**:\n\n```\n# Find errors in last hour\nfields @timestamp, @message, level, error.message\n| filter level = \"ERROR\"\n| sort @timestamp desc\n| limit 100\n\n# Count errors by type\nstats count() by error.name as ErrorType\n| sort count desc\n\n# Calculate p99 latency\nstats percentile(duration, 99) by serviceName\n\n# Find slow requests\nfields @timestamp, orderId, duration\n| filter duration > 1000\n| sort duration desc\n| limit 50\n\n# Track specific customer requests\nfields @timestamp, @message, orderId\n| filter customerId = \"customer-123\"\n| sort @timestamp desc\n```\n\n## Tracing\n\n### Enable X-Ray Tracing\n\n**Configure X-Ray for Lambda**:\n\n```typescript\nconst fn = new NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  tracing: lambda.Tracing.ACTIVE, // Enable X-Ray\n});\n\n// API Gateway tracing\nconst api = new apigateway.RestApi(this, 'Api', {\n  deployOptions: {\n    tracingEnabled: true,\n  },\n});\n\n// Step Functions tracing\nnew stepfunctions.StateMachine(this, 'StateMachine', {\n  definition,\n  tracingEnabled: true,\n});\n```\n\n**Instrument application code**:\n\n```typescript\nimport { captureAWSv3Client } from 'aws-xray-sdk-core';\nimport { DynamoDBClient } from '@aws-sdk/client-dynamodb';\n\n// Wrap AWS SDK clients\nconst client = captureAWSv3Client(new DynamoDBClient({}));\n\n// Custom segments\nimport AWSXRay from 'aws-xray-sdk-core';\n\nexport const handler = async (event: any) => {\n  const segment = AWSXRay.getSegment();\n\n  // Custom subsegment\n  const subsegment = segment.addNewSubsegment('ProcessOrder');\n\n  try {\n    // Add annotations (indexed for filtering)\n    subsegment.addAnnotation('orderId', event.orderId);\n    subsegment.addAnnotation('customerId', event.customerId);\n\n    // Add metadata (not indexed, detailed info)\n    subsegment.addMetadata('orderDetails', event);\n\n    const result = await processOrder(event);\n\n    subsegment.addAnnotation('status', 'success');\n    subsegment.close();\n\n    return result;\n  } catch (error) {\n    subsegment.addError(error);\n    subsegment.close();\n    throw error;\n  }\n};\n```\n\n**Using Lambda Powertools Tracer**:\n\n```typescript\nimport { Tracer } from '@aws-lambda-powertools/tracer';\n\nconst tracer = new Tracer({ serviceName: 'OrderService' });\n\nexport const handler = async (event: any) => {\n  const segment = tracer.getSegment();\n\n  // Automatically captures and traces\n  const result = await tracer.captureAWSv3Client(dynamodb).getItem({\n    TableName: process.env.TABLE_NAME,\n    Key: { orderId: event.orderId },\n  });\n\n  // Custom annotation\n  tracer.putAnnotation('orderId', event.orderId);\n  tracer.putMetadata('orderDetails', event);\n\n  return result;\n};\n```\n\n### Service Map\n\n**Visualize service dependencies** with X-Ray:\n- Shows service-to-service communication\n- Identifies latency bottlenecks\n- Highlights error rates between services\n- Tracks downstream dependencies\n\n### Distributed Tracing Best Practices\n\n1. **Enable tracing everywhere**: Lambda, API Gateway, Step Functions\n2. **Use annotations for filtering**: Indexed fields for queries\n3. **Use metadata for details**: Non-indexed detailed information\n4. **Sample appropriately**: 100% for low traffic, sampled for high traffic\n5. **Correlate with logs**: Include trace ID in log entries\n\n## Unified Observability\n\n### Correlation Between Pillars\n\n**Include trace ID in logs**:\n\n```typescript\nexport const handler = async (event: any, context: Context) => {\n  const traceId = process.env._X_AMZN_TRACE_ID;\n\n  console.log(JSON.stringify({\n    level: 'INFO',\n    message: 'Processing order',\n    traceId,\n    requestId: context.requestId,\n    orderId: event.orderId,\n  }));\n};\n```\n\n### CloudWatch ServiceLens\n\n**Unified view of traces and metrics**:\n- Automatically correlates X-Ray traces with CloudWatch metrics\n- Shows service map with metrics overlay\n- Identifies performance and availability issues\n- Provides end-to-end request view\n\n### Lambda Powertools Integration\n\n**All three pillars in one**:\n\n```typescript\nimport { Logger } from '@aws-lambda-powertools/logger';\nimport { Tracer } from '@aws-lambda-powertools/tracer';\nimport { Metrics, MetricUnits } from '@aws-lambda-powertools/metrics';\n\nconst logger = new Logger({ serviceName: 'OrderService' });\nconst tracer = new Tracer({ serviceName: 'OrderService' });\nconst metrics = new Metrics({ namespace: 'MyApp', serviceName: 'OrderService' });\n\nexport const handler = async (event: any, context: Context) => {\n  // Automatically adds trace context to logs\n  logger.addContext(context);\n\n  logger.info('Processing order', { orderId: event.orderId });\n\n  // Add trace annotations\n  tracer.putAnnotation('orderId', event.orderId);\n\n  // Add metrics\n  metrics.addMetric('Invocation', MetricUnits.Count, 1);\n\n  const startTime = Date.now();\n\n  try {\n    const result = await processOrder(event);\n\n    metrics.addMetric('Success', MetricUnits.Count, 1);\n    metrics.addMetric('Duration', MetricUnits.Milliseconds, Date.now() - startTime);\n\n    logger.info('Order processed', { orderId: event.orderId });\n\n    return result;\n  } catch (error) {\n    metrics.addMetric('Error', MetricUnits.Count, 1);\n    logger.error('Processing failed', { orderId: event.orderId, error });\n    throw error;\n  } finally {\n    metrics.publishStoredMetrics();\n  }\n};\n```\n\n## Alerting\n\n### Effective Alerting Strategy\n\n**Alert on what matters**:\n- **Critical**: Customer-impacting issues (errors, high latency)\n- **Warning**: Approaching thresholds (80% capacity)\n- **Info**: Trends and anomalies (cost spikes)\n\n**Alarm fatigue prevention**:\n- Tune thresholds based on actual patterns\n- Use composite alarms to reduce noise\n- Set appropriate evaluation periods\n- Include clear remediation steps\n\n### CloudWatch Alarms\n\n**Common alarm patterns**:\n\n```typescript\n// Error rate alarm\nnew cloudwatch.Alarm(this, 'ErrorRateAlarm', {\n  metric: new cloudwatch.MathExpression({\n    expression: 'errors / invocations * 100',\n    usingMetrics: {\n      errors: fn.metricErrors({ statistic: 'Sum' }),\n      invocations: fn.metricInvocations({ statistic: 'Sum' }),\n    },\n  }),\n  threshold: 1, // 1% error rate\n  evaluationPeriods: 2,\n  alarmDescription: 'Error rate exceeded 1%',\n});\n\n// Latency alarm (p99)\nnew cloudwatch.Alarm(this, 'LatencyAlarm', {\n  metric: fn.metricDuration({\n    statistic: 'p99',\n    period: Duration.minutes(5),\n  }),\n  threshold: 1000, // 1 second\n  evaluationPeriods: 2,\n  alarmDescription: 'p99 latency exceeded 1 second',\n});\n\n// Concurrent executions approaching limit\nnew cloudwatch.Alarm(this, 'ConcurrencyAlarm', {\n  metric: fn.metricConcurrentExecutions({\n    statistic: 'Maximum',\n  }),\n  threshold: 800, // 80% of 1000 default limit\n  evaluationPeriods: 1,\n  alarmDescription: 'Approaching concurrency limit',\n});\n```\n\n### Composite Alarms\n\n**Reduce alert noise**:\n\n```typescript\nconst errorAlarm = new cloudwatch.Alarm(this, 'Errors', {\n  metric: fn.metricErrors(),\n  threshold: 10,\n  evaluationPeriods: 1,\n});\n\nconst throttleAlarm = new cloudwatch.Alarm(this, 'Throttles', {\n  metric: fn.metricThrottles(),\n  threshold: 5,\n  evaluationPeriods: 1,\n});\n\nconst latencyAlarm = new cloudwatch.Alarm(this, 'Latency', {\n  metric: fn.metricDuration({ statistic: 'p99' }),\n  threshold: 2000,\n  evaluationPeriods: 2,\n});\n\n// Composite alarm (any of the above)\nnew cloudwatch.CompositeAlarm(this, 'ServiceHealthAlarm', {\n  compositeAlarmName: 'order-service-health',\n  alarmRule: cloudwatch.AlarmRule.anyOf(\n    errorAlarm,\n    throttleAlarm,\n    latencyAlarm\n  ),\n  alarmDescription: 'Overall service health degraded',\n});\n```\n\n## Dashboard Best Practices\n\n### Service Dashboard Layout\n\n**Recommended sections**:\n\n1. **Overview**:\n   - Total invocations\n   - Error rate percentage\n   - P50, P95, P99 latency\n   - Availability percentage\n\n2. **Resource Utilization**:\n   - Concurrent executions\n   - Memory utilization\n   - Duration distribution\n   - Throttles\n\n3. **Business Metrics**:\n   - Orders processed\n   - Revenue per minute\n   - Customer activity\n   - Feature usage\n\n4. **Errors and Alerts**:\n   - Error count by type\n   - Active alarms\n   - DLQ message count\n   - Failed transactions\n\n### CloudWatch Dashboard CDK\n\n```typescript\nconst dashboard = new cloudwatch.Dashboard(this, 'ServiceDashboard', {\n  dashboardName: 'order-service',\n});\n\ndashboard.addWidgets(\n  // Row 1: Overview\n  new cloudwatch.GraphWidget({\n    title: 'Invocations',\n    left: [fn.metricInvocations()],\n  }),\n  new cloudwatch.SingleValueWidget({\n    title: 'Error Rate',\n    metrics: [\n      new cloudwatch.MathExpression({\n        expression: 'errors / invocations * 100',\n        usingMetrics: {\n          errors: fn.metricErrors({ statistic: 'Sum' }),\n          invocations: fn.metricInvocations({ statistic: 'Sum' }),\n        },\n      }),\n    ],\n  }),\n  new cloudwatch.GraphWidget({\n    title: 'Latency (p50, p95, p99)',\n    left: [\n      fn.metricDuration({ statistic: 'p50', label: 'p50' }),\n      fn.metricDuration({ statistic: 'p95', label: 'p95' }),\n      fn.metricDuration({ statistic: 'p99', label: 'p99' }),\n    ],\n  })\n);\n\n// Row 2: Errors\ndashboard.addWidgets(\n  new cloudwatch.LogQueryWidget({\n    title: 'Recent Errors',\n    logGroupNames: [fn.logGroup.logGroupName],\n    queryLines: [\n      'fields @timestamp, @message',\n      'filter level = \"ERROR\"',\n      'sort @timestamp desc',\n      'limit 20',\n    ],\n  })\n);\n```\n\n## Monitoring Serverless Architectures\n\n### End-to-End Monitoring\n\n**Monitor the entire flow**:\n\n```\nAPI Gateway  Lambda  DynamoDB  EventBridge  Lambda\n                                                 \n  Metrics    Traces     Metrics      Metrics     Logs\n```\n\n**Key metrics per service**:\n\n| Service | Key Metrics |\n|---------|-------------|\n| API Gateway | Count, 4XXError, 5XXError, Latency, CacheHitCount |\n| Lambda | Invocations, Errors, Duration, Throttles, ConcurrentExecutions |\n| DynamoDB | ConsumedReadCapacity, ConsumedWriteCapacity, UserErrors, SystemErrors |\n| SQS | NumberOfMessagesSent, NumberOfMessagesReceived, ApproximateAgeOfOldestMessage |\n| EventBridge | Invocations, FailedInvocations, TriggeredRules |\n| Step Functions | ExecutionsStarted, ExecutionsFailed, ExecutionTime |\n\n### Synthetic Monitoring\n\n**Use CloudWatch Synthetics for API monitoring**:\n\n```typescript\nimport { Canary, Test, Code, Schedule } from '@aws-cdk/aws-synthetics-alpha';\n\nnew Canary(this, 'ApiCanary', {\n  canaryName: 'api-health-check',\n  schedule: Schedule.rate(Duration.minutes(5)),\n  test: Test.custom({\n    code: Code.fromInline(`\n      const synthetics = require('Synthetics');\n\n      const apiCanaryBlueprint = async function () {\n        const response = await synthetics.executeHttpStep('Verify API', {\n          url: 'https://api.example.com/health',\n          method: 'GET',\n        });\n\n        return response.statusCode === 200 ? 'success' : 'failure';\n      };\n\n      exports.handler = async () => {\n        return await apiCanaryBlueprint();\n      };\n    `),\n    handler: 'index.handler',\n  }),\n  runtime: synthetics.Runtime.SYNTHETICS_NODEJS_PUPPETEER_6_2,\n});\n```\n\n## OpenTelemetry Integration\n\n### Amazon Distro for OpenTelemetry (ADOT)\n\n**Use ADOT for vendor-neutral observability**:\n\n```typescript\n// Lambda Layer with ADOT\nconst adotLayer = lambda.LayerVersion.fromLayerVersionArn(\n  this,\n  'AdotLayer',\n  `arn:aws:lambda:${this.region}:901920570463:layer:aws-otel-nodejs-amd64-ver-1-18-1:4`\n);\n\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  layers: [adotLayer],\n  tracing: lambda.Tracing.ACTIVE,\n  environment: {\n    AWS_LAMBDA_EXEC_WRAPPER: '/opt/otel-handler',\n    OPENTELEMETRY_COLLECTOR_CONFIG_FILE: '/var/task/collector.yaml',\n  },\n});\n```\n\n**Benefits of ADOT**:\n- Vendor-neutral (works with Datadog, New Relic, Honeycomb, etc.)\n- Automatic instrumentation\n- Consistent format across services\n- Export to multiple backends\n\n## Best Practices Summary\n\n### Metrics\n-  Use CloudWatch Embedded Metric Format (EMF)\n-  Track business metrics, not just technical metrics\n-  Set alarms on error rate, latency, and throughput\n-  Use p99 for latency, not average\n-  Create dashboards for key services\n\n### Logging\n-  Use structured JSON logging\n-  Include correlation IDs (request ID, trace ID)\n-  Use appropriate log levels\n-  Never log sensitive data (PII, secrets)\n-  Use CloudWatch Logs Insights for analysis\n\n### Tracing\n-  Enable X-Ray tracing on all services\n-  Instrument AWS SDK calls\n-  Add custom annotations for business context\n-  Use service map to understand dependencies\n-  Correlate traces with logs and metrics\n\n### Alerting\n-  Alert on customer-impacting issues\n-  Tune thresholds to reduce false positives\n-  Use composite alarms to reduce noise\n-  Include clear remediation steps\n-  Escalate critical alarms appropriately\n\n### Tools\n-  Use Lambda Powertools for unified observability\n-  Use CloudWatch ServiceLens for service view\n-  Use Synthetics for proactive monitoring\n-  Consider ADOT for vendor-neutral observability\n",
        "skills/event-driven-serverless-systems/references/performance-optimization.md": "# Serverless Performance Optimization\n\nPerformance optimization best practices for AWS Lambda and serverless architectures.\n\n## Table of Contents\n\n- [Lambda Execution Lifecycle](#lambda-execution-lifecycle)\n- [Cold Start Optimization](#cold-start-optimization)\n- [Memory and CPU Optimization](#memory-and-cpu-optimization)\n- [Package Size Optimization](#package-size-optimization)\n- [Initialization Optimization](#initialization-optimization)\n- [Runtime Performance](#runtime-performance)\n\n## Lambda Execution Lifecycle\n\n### Execution Environment Phases\n\n**Three phases of Lambda execution**:\n\n1. **Init Phase** (Cold Start):\n   - Download and unpack function package\n   - Create execution environment\n   - Initialize runtime\n   - Execute initialization code (outside handler)\n\n2. **Invoke Phase**:\n   - Execute handler code\n   - Return response\n   - Freeze execution environment\n\n3. **Shutdown Phase**:\n   - Runtime shutdown (after period of inactivity)\n   - Execution environment destroyed\n\n### Concurrency and Scaling\n\n**Key concepts**:\n- **Concurrency**: Number of execution environments serving requests simultaneously\n- **One event per environment**: Each environment processes one event at a time\n- **Automatic scaling**: Lambda creates new environments as needed\n- **Environment reuse**: Warm starts reuse existing environments\n\n**Example**:\n- Function takes 100ms to execute\n- Single environment can handle 10 requests/second\n- 100 concurrent requests = 10 environments needed\n- Default account limit: 1,000 concurrent executions (can be raised)\n\n## Cold Start Optimization\n\n### Understanding Cold Starts\n\n**Cold start components**:\n```\nTotal Cold Start = Download Package + Init Environment + Init Code + Handler\n```\n\n**Cold start frequency**:\n- Development: Every code change creates new environments (frequent)\n- Production: Typically < 1% of invocations\n- Optimize for p95/p99 latency, not average\n\n### Package Size Optimization\n\n**Minimize deployment package**:\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  bundling: {\n    minify: true, // Minify production code\n    sourceMap: false, // Disable in production\n    externalModules: [\n      '@aws-sdk/*', // Use AWS SDK from runtime\n    ],\n    // Tree-shaking removes unused code\n  },\n});\n```\n\n**Tools for optimization**:\n- **esbuild**: Automatic tree-shaking and minification\n- **Webpack**: Bundle optimization\n- **Maven**: Dependency analysis\n- **Gradle**: Unused dependency detection\n\n**Best practices**:\n1. Avoid monolithic functions\n2. Bundle only required dependencies\n3. Use tree-shaking to remove unused code\n4. Minify production code\n5. Exclude AWS SDK (provided by runtime)\n\n### Provisioned Concurrency\n\n**Pre-initialize environments for predictable latency**:\n\n```typescript\nconst fn = new NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n});\n\n// Static provisioned concurrency\nfn.currentVersion.addAlias('live', {\n  provisionedConcurrentExecutions: 10,\n});\n\n// Auto-scaling provisioned concurrency\nconst alias = fn.currentVersion.addAlias('prod');\n\nconst target = new applicationautoscaling.ScalableTarget(this, 'ScalableTarget', {\n  serviceNamespace: applicationautoscaling.ServiceNamespace.LAMBDA,\n  maxCapacity: 100,\n  minCapacity: 10,\n  resourceId: `function:${fn.functionName}:${alias.aliasName}`,\n  scalableDimension: 'lambda:function:ProvisionedConcurrentExecutions',\n});\n\ntarget.scaleOnUtilization({\n  utilizationTarget: 0.7, // 70% utilization\n});\n```\n\n**When to use**:\n- **Consistent traffic patterns**: Predictable load\n- **Latency-sensitive APIs**: Sub-100ms requirements\n- **Cost consideration**: Compare cold start frequency vs. provisioned cost\n\n**Cost comparison**:\n- **On-demand**: Pay only for actual usage\n- **Provisioned**: Pay for provisioned capacity + invocations\n- **Breakeven**: When cold starts > ~20% of invocations\n\n### Lambda SnapStart (Java)\n\n**Instant cold starts for Java**:\n\n```typescript\nnew lambda.Function(this, 'JavaFunction', {\n  runtime: lambda.Runtime.JAVA_17,\n  code: lambda.Code.fromAsset('target/function.jar'),\n  handler: 'com.example.Handler::handleRequest',\n  snapStart: lambda.SnapStartConf.ON_PUBLISHED_VERSIONS,\n});\n```\n\n**Benefits**:\n- Up to 10x faster cold starts for Java\n- No code changes required\n- Works with published versions\n- No additional cost\n\n## Memory and CPU Optimization\n\n### Memory = CPU Allocation\n\n**Key principle**: Memory and CPU are proportionally allocated\n\n| Memory | vCPU |\n|--------|------|\n| 128 MB | 0.07 vCPU |\n| 512 MB | 0.28 vCPU |\n| 1,024 MB | 0.57 vCPU |\n| 1,769 MB | 1.00 vCPU |\n| 3,538 MB | 2.00 vCPU |\n| 10,240 MB | 6.00 vCPU |\n\n### Cost vs. Performance Balancing\n\n**Example - Compute-intensive function**:\n\n| Memory | Duration | Cost |\n|--------|----------|------|\n| 128 MB | 11.72s | $0.0246 |\n| 256 MB | 6.68s | $0.0280 |\n| 512 MB | 3.19s | $0.0268 |\n| 1024 MB | 1.46s | $0.0246 |\n\n**Key insight**: More memory = faster execution = similar or lower cost\n\n**Formula**:\n```\nDuration = Allocated Memory (GB)  Execution Time (seconds)\nCost = Duration  Number of Invocations  Price per GB-second\n```\n\n### Finding Optimal Memory\n\n**Use Lambda Power Tuning**:\n\n```bash\n# Deploy power tuning state machine\nsam deploy --template-file template.yml --stack-name lambda-power-tuning\n\n# Run power tuning\naws lambda invoke \\\n  --function-name powerTuningFunction \\\n  --payload '{\"lambdaARN\": \"arn:aws:lambda:...\", \"powerValues\": [128, 256, 512, 1024, 1536, 3008]}' \\\n  response.json\n```\n\n**Manual testing approach**:\n1. Test function at different memory levels\n2. Measure execution time at each level\n3. Calculate cost for each configuration\n4. Choose optimal balance for your use case\n\n### Multi-Core Optimization\n\n**Leverage multiple vCPUs** (at 1,769 MB+):\n\n```typescript\n// Use Worker Threads for parallel processing\nimport { Worker } from 'worker_threads';\n\nexport const handler = async (event: any) => {\n  const items = event.items;\n\n  // Process in parallel using multiple cores\n  const workers = items.map(item =>\n    new Promise((resolve, reject) => {\n      const worker = new Worker('./worker.js', {\n        workerData: item,\n      });\n\n      worker.on('message', resolve);\n      worker.on('error', reject);\n    })\n  );\n\n  const results = await Promise.all(workers);\n  return results;\n};\n```\n\n**Python multiprocessing**:\n\n```python\nimport multiprocessing as mp\n\ndef handler(event, context):\n    items = event['items']\n\n    # Use multiple cores for CPU-bound work\n    with mp.Pool(mp.cpu_count()) as pool:\n        results = pool.map(process_item, items)\n\n    return {'results': results}\n```\n\n## Initialization Optimization\n\n### Code Outside Handler\n\n**Initialize once, reuse across invocations**:\n\n```typescript\n//  GOOD - Initialize outside handler\nimport { DynamoDBClient } from '@aws-sdk/client-dynamodb';\nimport { S3Client } from '@aws-sdk/client-s3';\n\n// Initialized once per execution environment\nconst dynamodb = new DynamoDBClient({});\nconst s3 = new S3Client({});\n\n// Connection pool initialized once\nconst pool = createConnectionPool({\n  host: process.env.DB_HOST,\n  max: 1, // One connection per execution environment\n});\n\nexport const handler = async (event: any) => {\n  // Reuse connections across invocations\n  const data = await dynamodb.getItem({ /* ... */ });\n  const file = await s3.getObject({ /* ... */ });\n  return processData(data, file);\n};\n\n//  BAD - Initialize in handler\nexport const handler = async (event: any) => {\n  const dynamodb = new DynamoDBClient({}); // Created every invocation\n  const s3 = new S3Client({}); // Created every invocation\n  // ...\n};\n```\n\n### Lazy Loading\n\n**Load dependencies only when needed**:\n\n```typescript\n//  GOOD - Conditional loading\nexport const handler = async (event: any) => {\n  if (event.operation === 'generatePDF') {\n    // Load heavy PDF library only when needed\n    const pdfLib = await import('./pdf-generator');\n    return pdfLib.generatePDF(event.data);\n  }\n\n  if (event.operation === 'processImage') {\n    const sharp = await import('sharp');\n    return processImage(sharp, event.data);\n  }\n\n  // Default operation (no heavy dependencies)\n  return processDefault(event);\n};\n\n//  BAD - Load everything upfront\nimport pdfLib from './pdf-generator'; // 50MB\nimport sharp from 'sharp'; // 20MB\n// Even if not used!\n\nexport const handler = async (event: any) => {\n  if (event.operation === 'generatePDF') {\n    return pdfLib.generatePDF(event.data);\n  }\n};\n```\n\n### Connection Reuse\n\n**Enable connection reuse**:\n\n```typescript\nimport { DynamoDBClient } from '@aws-sdk/client-dynamodb';\n\nconst client = new DynamoDBClient({\n  // Enable keep-alive for connection reuse\n  requestHandler: {\n    connectionTimeout: 3000,\n    socketTimeout: 3000,\n  },\n});\n\n// For Node.js AWS SDK\nprocess.env.AWS_NODEJS_CONNECTION_REUSE_ENABLED = '1';\n```\n\n## Runtime Performance\n\n### Choose the Right Runtime\n\n**Runtime comparison**:\n\n| Runtime | Cold Start | Execution Speed | Ecosystem | Best For |\n|---------|------------|-----------------|-----------|----------|\n| Node.js 20 | Fast | Fast | Excellent | APIs, I/O-bound |\n| Python 3.12 | Fast | Medium | Excellent | Data processing |\n| Java 17 + SnapStart | Fast (w/SnapStart) | Fast | Good | Enterprise apps |\n| .NET 8 | Medium | Fast | Good | Enterprise apps |\n| Go | Very Fast | Very Fast | Good | High performance |\n| Rust | Very Fast | Very Fast | Growing | High performance |\n\n### Optimize Handler Code\n\n**Efficient code patterns**:\n\n```typescript\n//  GOOD - Batch operations\nconst items = ['item1', 'item2', 'item3'];\n\n// Single batch write\nawait dynamodb.batchWriteItem({\n  RequestItems: {\n    [tableName]: items.map(item => ({\n      PutRequest: { Item: item },\n    })),\n  },\n});\n\n//  BAD - Multiple single operations\nfor (const item of items) {\n  await dynamodb.putItem({\n    TableName: tableName,\n    Item: item,\n  }); // Slow, multiple round trips\n}\n```\n\n### Async Processing\n\n**Use async/await effectively**:\n\n```typescript\n//  GOOD - Parallel async operations\nconst [userData, orderData, inventoryData] = await Promise.all([\n  getUserData(userId),\n  getOrderData(orderId),\n  getInventoryData(productId),\n]);\n\n//  BAD - Sequential async operations\nconst userData = await getUserData(userId);\nconst orderData = await getOrderData(orderId); // Waits unnecessarily\nconst inventoryData = await getInventoryData(productId); // Waits unnecessarily\n```\n\n### Caching Strategies\n\n**Cache frequently accessed data**:\n\n```typescript\n// In-memory cache (persists in warm environments)\nconst cache = new Map<string, any>();\n\nexport const handler = async (event: any) => {\n  const key = event.key;\n\n  // Check cache first\n  if (cache.has(key)) {\n    console.log('Cache hit');\n    return cache.get(key);\n  }\n\n  // Fetch from database\n  const data = await fetchFromDatabase(key);\n\n  // Store in cache\n  cache.set(key, data);\n\n  return data;\n};\n```\n\n**ElastiCache for shared cache**:\n\n```typescript\nimport Redis from 'ioredis';\n\n// Initialize once\nconst redis = new Redis({\n  host: process.env.REDIS_HOST,\n  port: 6379,\n  lazyConnect: true,\n  enableOfflineQueue: false,\n});\n\nexport const handler = async (event: any) => {\n  const key = `order:${event.orderId}`;\n\n  // Try cache\n  const cached = await redis.get(key);\n  if (cached) {\n    return JSON.parse(cached);\n  }\n\n  // Fetch and cache\n  const data = await fetchOrder(event.orderId);\n  await redis.setex(key, 300, JSON.stringify(data)); // 5 min TTL\n\n  return data;\n};\n```\n\n## Performance Testing\n\n### Load Testing\n\n**Use Artillery for load testing**:\n\n```yaml\n# load-test.yml\nconfig:\n  target: https://api.example.com\n  phases:\n    - duration: 60\n      arrivalRate: 10\n      rampTo: 100 # Ramp from 10 to 100 req/sec\nscenarios:\n  - flow:\n      - post:\n          url: /orders\n          json:\n            orderId: \"{{ $randomString() }}\"\n            amount: \"{{ $randomNumber(10, 1000) }}\"\n```\n\n```bash\nartillery run load-test.yml\n```\n\n### Benchmarking\n\n**Test different configurations**:\n\n```typescript\n// benchmark.ts\nimport { Lambda } from '@aws-sdk/client-lambda';\n\nconst lambda = new Lambda({});\n\nconst testConfigurations = [\n  { memory: 128, name: 'Function-128' },\n  { memory: 256, name: 'Function-256' },\n  { memory: 512, name: 'Function-512' },\n  { memory: 1024, name: 'Function-1024' },\n];\n\nfor (const config of testConfigurations) {\n  const times: number[] = [];\n\n  // Warm up\n  for (let i = 0; i < 5; i++) {\n    await lambda.invoke({ FunctionName: config.name });\n  }\n\n  // Measure\n  for (let i = 0; i < 100; i++) {\n    const start = Date.now();\n    await lambda.invoke({ FunctionName: config.name });\n    times.push(Date.now() - start);\n  }\n\n  const p99 = times.sort()[99];\n  const avg = times.reduce((a, b) => a + b) / times.length;\n\n  console.log(`${config.memory}MB - Avg: ${avg}ms, p99: ${p99}ms`);\n}\n```\n\n## Cost Optimization\n\n### Right-Sizing Memory\n\n**Balance cost and performance**:\n\n**CPU-bound workloads**:\n- More memory = more CPU = faster execution\n- Often results in lower cost overall\n- Test at 1769MB (1 vCPU) and above\n\n**I/O-bound workloads**:\n- Less sensitive to memory allocation\n- May not benefit from higher memory\n- Test at lower memory levels (256-512MB)\n\n**Simple operations**:\n- Minimal CPU required\n- Use minimum memory (128-256MB)\n- Fast execution despite low resources\n\n### Billing Granularity\n\n**Lambda bills in 1ms increments**:\n- Precise billing (7ms execution = 7ms cost)\n- Optimize even small improvements\n- Consider trade-offs carefully\n\n**Cost calculation**:\n```\nCost = (Memory GB)  (Duration seconds)  (Invocations)  ($0.0000166667/GB-second)\n     + (Invocations)  ($0.20/1M requests)\n```\n\n### Cost Reduction Strategies\n\n1. **Optimize execution time**: Faster = cheaper\n2. **Right-size memory**: Balance CPU needs with cost\n3. **Reduce invocations**: Batch processing, caching\n4. **Use Graviton2**: 20% better price/performance\n5. **Reserved Concurrency**: Only when needed\n6. **Compression**: Reduce data transfer costs\n\n## Advanced Optimization\n\n### Lambda Extensions\n\n**Use extensions for cross-cutting concerns**:\n\n```typescript\n// Lambda layer with extension\nconst extensionLayer = lambda.LayerVersion.fromLayerVersionArn(\n  this,\n  'Extension',\n  'arn:aws:lambda:us-east-1:123456789:layer:my-extension:1'\n);\n\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  layers: [extensionLayer],\n});\n```\n\n**Common extensions**:\n- Secrets caching\n- Configuration caching\n- Custom logging\n- Security scanning\n- Performance monitoring\n\n### Graviton2 Architecture\n\n**20% better price/performance**:\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  architecture: lambda.Architecture.ARM_64, // Graviton2\n});\n```\n\n**Considerations**:\n- Most runtimes support ARM64\n- Test thoroughly before migrating\n- Dependencies must support ARM64\n- Native extensions may need recompilation\n\n### VPC Optimization\n\n**Hyperplane ENIs** (automatic since 2019):\n- No ENI per function\n- Faster cold starts in VPC\n- Scales instantly\n\n```typescript\n// Modern VPC configuration (fast)\nnew NodejsFunction(this, 'VpcFunction', {\n  entry: 'src/handler.ts',\n  vpc,\n  vpcSubnets: { subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS },\n  // Fast scaling, no ENI limitations\n});\n```\n\n## Performance Monitoring\n\n### Key Metrics\n\n**Monitor these metrics**:\n- **Duration**: p50, p95, p99, max\n- **Cold Start %**: ColdStartDuration / TotalDuration\n- **Error Rate**: Errors / Invocations\n- **Throttles**: Indicates concurrency limit reached\n- **Iterator Age**: For stream processing lag\n\n### Performance Dashboards\n\n```typescript\nconst dashboard = new cloudwatch.Dashboard(this, 'PerformanceDashboard');\n\ndashboard.addWidgets(\n  new cloudwatch.GraphWidget({\n    title: 'Latency Distribution',\n    left: [\n      fn.metricDuration({ statistic: 'p50', label: 'p50' }),\n      fn.metricDuration({ statistic: 'p95', label: 'p95' }),\n      fn.metricDuration({ statistic: 'p99', label: 'p99' }),\n      fn.metricDuration({ statistic: 'Maximum', label: 'max' }),\n    ],\n  }),\n  new cloudwatch.GraphWidget({\n    title: 'Memory Utilization',\n    left: [fn.metricDuration()],\n    right: [fn.metricErrors()],\n  })\n);\n```\n\n## Summary\n\n- **Cold Starts**: Optimize package size, use provisioned concurrency for critical paths\n- **Memory**: More memory often = faster execution = lower cost\n- **Initialization**: Initialize connections outside handler\n- **Lazy Loading**: Load dependencies only when needed\n- **Connection Reuse**: Enable for AWS SDK clients\n- **Testing**: Test at different memory levels to find optimal configuration\n- **Monitoring**: Track p99 latency, not average\n- **Graviton2**: Consider ARM64 for better price/performance\n- **Batch Operations**: Reduce round trips to services\n- **Caching**: Cache frequently accessed data\n",
        "skills/event-driven-serverless-systems/references/security-best-practices.md": "# Serverless Security Best Practices\n\nSecurity best practices for serverless applications based on AWS Well-Architected Framework.\n\n## Table of Contents\n\n- [Shared Responsibility Model](#shared-responsibility-model)\n- [Identity and Access Management](#identity-and-access-management)\n- [Function Security](#function-security)\n- [API Security](#api-security)\n- [Data Protection](#data-protection)\n- [Network Security](#network-security)\n\n## Shared Responsibility Model\n\n### Serverless Shifts Responsibility to AWS\n\nWith serverless, AWS takes on more security responsibilities:\n\n**AWS Responsibilities**:\n- Compute infrastructure\n- Execution environment\n- Runtime language and patches\n- Networking infrastructure\n- Server software and OS\n- Physical hardware and facilities\n- Automatic security patches (like Log4Shell mitigation)\n\n**Customer Responsibilities**:\n- Function code and dependencies\n- Resource configuration\n- Identity and Access Management (IAM)\n- Data encryption (at rest and in transit)\n- Application-level security\n- Secure coding practices\n\n### Benefits of Shifted Responsibility\n\n- **Automatic Patching**: AWS applies security patches automatically (e.g., Log4Shell fixed within 3 days)\n- **Infrastructure Security**: No OS patching, server hardening, or vulnerability scanning\n- **Operational Agility**: Quick security response at scale\n- **Focus on Code**: Spend time on business logic, not infrastructure security\n\n## Identity and Access Management\n\n### Least Privilege Principle\n\n**Always use least privilege IAM policies**:\n\n```typescript\n//  GOOD - Specific grant\nconst table = new dynamodb.Table(this, 'Table', {});\nconst function = new lambda.Function(this, 'Function', {});\n\ntable.grantReadData(function); // Only read access\n\n//  BAD - Overly broad\nfunction.addToRolePolicy(new iam.PolicyStatement({\n  actions: ['dynamodb:*'],\n  resources: ['*'],\n}));\n```\n\n### Function Execution Role\n\n**Separate roles per function**:\n\n```typescript\n//  GOOD - Each function has its own role\nconst readFunction = new NodejsFunction(this, 'ReadFunction', {\n  entry: 'src/read.ts',\n  // Gets its own execution role\n});\n\nconst writeFunction = new NodejsFunction(this, 'WriteFunction', {\n  entry: 'src/write.ts',\n  // Gets its own execution role\n});\n\ntable.grantReadData(readFunction);\ntable.grantReadWriteData(writeFunction);\n\n//  BAD - Shared role with excessive permissions\nconst sharedRole = new iam.Role(this, 'SharedRole', {\n  assumedBy: new iam.ServicePrincipal('lambda.amazonaws.com'),\n  managedPolicies: [\n    iam.ManagedPolicy.fromAwsManagedPolicyName('AdministratorAccess'), // Too broad!\n  ],\n});\n```\n\n### Resource-Based Policies\n\nControl who can invoke functions:\n\n```typescript\n// Allow API Gateway to invoke function\nmyFunction.grantInvoke(new iam.ServicePrincipal('apigateway.amazonaws.com'));\n\n// Allow specific account\nmyFunction.addPermission('AllowAccountInvoke', {\n  principal: new iam.AccountPrincipal('123456789012'),\n  action: 'lambda:InvokeFunction',\n});\n\n// Conditional invoke (only from specific VPC endpoint)\nmyFunction.addPermission('AllowVPCInvoke', {\n  principal: new iam.ServicePrincipal('lambda.amazonaws.com'),\n  action: 'lambda:InvokeFunction',\n  sourceArn: vpcEndpoint.vpcEndpointId,\n});\n```\n\n### IAM Policies Best Practices\n\n1. **Use grant methods**: Prefer `.grantXxx()` over manual policies\n2. **Condition keys**: Use IAM conditions for fine-grained control\n3. **Resource ARNs**: Always specify resource ARNs, avoid wildcards\n4. **Session policies**: Use for temporary elevated permissions\n5. **Service Control Policies (SCPs)**: Enforce organization-wide guardrails\n\n## Function Security\n\n### Lambda Isolation Model\n\n**Each function runs in isolated sandbox**:\n- Built on Firecracker microVMs\n- Dedicated execution environment per function\n- No shared memory between functions\n- Isolated file system and network namespace\n- Strong workload isolation\n\n**Execution Environment Security**:\n- One concurrent invocation per environment\n- Environment may be reused (warm starts)\n- `/tmp` storage persists between invocations\n- Sensitive data in memory may persist\n\n### Secure Coding Practices\n\n**Handle sensitive data securely**:\n\n```typescript\n//  GOOD - Clean up sensitive data\nexport const handler = async (event: any) => {\n  const apiKey = process.env.API_KEY;\n\n  try {\n    const result = await callApi(apiKey);\n    return result;\n  } finally {\n    // Clear sensitive data from memory\n    delete process.env.API_KEY;\n  }\n};\n\n//  GOOD - Use Secrets Manager\nimport { SecretsManagerClient, GetSecretValueCommand } from '@aws-sdk/client-secrets-manager';\n\nconst secretsClient = new SecretsManagerClient({});\n\nexport const handler = async (event: any) => {\n  const secret = await secretsClient.send(\n    new GetSecretValueCommand({ SecretId: process.env.SECRET_ARN })\n  );\n\n  const apiKey = secret.SecretString;\n  // Use apiKey\n};\n```\n\n### Dependency Management\n\n**Scan dependencies for vulnerabilities**:\n\n```json\n// package.json\n{\n  \"scripts\": {\n    \"audit\": \"npm audit\",\n    \"audit:fix\": \"npm audit fix\"\n  },\n  \"devDependencies\": {\n    \"snyk\": \"^1.0.0\"\n  }\n}\n```\n\n**Keep dependencies updated**:\n- Run `npm audit` or `pip-audit` regularly\n- Use Dependabot or Snyk for automated scanning\n- Update dependencies promptly when vulnerabilities found\n- Use minimal dependency sets\n\n### Environment Variable Security\n\n**Never store secrets in environment variables**:\n\n```typescript\n//  BAD - Secret in environment variable\nnew NodejsFunction(this, 'Function', {\n  environment: {\n    API_KEY: 'sk-1234567890abcdef', // Never do this!\n  },\n});\n\n//  GOOD - Reference to secret\nnew NodejsFunction(this, 'Function', {\n  environment: {\n    SECRET_ARN: secret.secretArn,\n  },\n});\n\nsecret.grantRead(myFunction);\n```\n\n## API Security\n\n### API Gateway Security\n\n**Authentication and Authorization**:\n\n```typescript\n// Cognito User Pool authorizer\nconst authorizer = new apigateway.CognitoUserPoolsAuthorizer(this, 'Authorizer', {\n  cognitoUserPools: [userPool],\n});\n\napi.root.addMethod('GET', integration, {\n  authorizer,\n  authorizationType: apigateway.AuthorizationType.COGNITO,\n});\n\n// Lambda authorizer for custom auth\nconst customAuthorizer = new apigateway.TokenAuthorizer(this, 'CustomAuth', {\n  handler: authorizerFunction,\n  resultsCacheTtl: Duration.minutes(5),\n});\n\n// IAM authorization for service-to-service\napi.root.addMethod('POST', integration, {\n  authorizationType: apigateway.AuthorizationType.IAM,\n});\n```\n\n### Request Validation\n\n**Validate requests at API Gateway**:\n\n```typescript\nconst validator = new apigateway.RequestValidator(this, 'Validator', {\n  api,\n  validateRequestBody: true,\n  validateRequestParameters: true,\n});\n\nconst model = api.addModel('Model', {\n  schema: {\n    type: apigateway.JsonSchemaType.OBJECT,\n    required: ['email', 'name'],\n    properties: {\n      email: {\n        type: apigateway.JsonSchemaType.STRING,\n        format: 'email',\n      },\n      name: {\n        type: apigateway.JsonSchemaType.STRING,\n        minLength: 1,\n        maxLength: 100,\n      },\n    },\n  },\n});\n\nresource.addMethod('POST', integration, {\n  requestValidator: validator,\n  requestModels: {\n    'application/json': model,\n  },\n});\n```\n\n### Rate Limiting and Throttling\n\n```typescript\nconst api = new apigateway.RestApi(this, 'Api', {\n  deployOptions: {\n    throttlingRateLimit: 1000, // requests per second\n    throttlingBurstLimit: 2000, // burst capacity\n  },\n});\n\n// Per-method throttling\nresource.addMethod('POST', integration, {\n  methodResponses: [{ statusCode: '200' }],\n  requestParameters: {\n    'method.request.header.Authorization': true,\n  },\n  throttling: {\n    rateLimit: 100,\n    burstLimit: 200,\n  },\n});\n```\n\n### API Keys and Usage Plans\n\n```typescript\nconst apiKey = api.addApiKey('ApiKey', {\n  apiKeyName: 'customer-key',\n});\n\nconst plan = api.addUsagePlan('UsagePlan', {\n  name: 'Standard',\n  throttle: {\n    rateLimit: 100,\n    burstLimit: 200,\n  },\n  quota: {\n    limit: 10000,\n    period: apigateway.Period.MONTH,\n  },\n});\n\nplan.addApiKey(apiKey);\nplan.addApiStage({\n  stage: api.deploymentStage,\n});\n```\n\n## Data Protection\n\n### Encryption at Rest\n\n**DynamoDB encryption**:\n\n```typescript\n// Default: AWS-owned CMK (no additional cost)\nconst table = new dynamodb.Table(this, 'Table', {\n  encryption: dynamodb.TableEncryption.AWS_MANAGED, // AWS managed CMK\n});\n\n// Customer-managed CMK (for compliance)\nconst kmsKey = new kms.Key(this, 'Key', {\n  enableKeyRotation: true,\n});\n\nconst table = new dynamodb.Table(this, 'Table', {\n  encryption: dynamodb.TableEncryption.CUSTOMER_MANAGED,\n  encryptionKey: kmsKey,\n});\n```\n\n**S3 encryption**:\n\n```typescript\n// SSE-S3 (default, no additional cost)\nconst bucket = new s3.Bucket(this, 'Bucket', {\n  encryption: s3.BucketEncryption.S3_MANAGED,\n});\n\n// SSE-KMS (for fine-grained access control)\nconst bucket = new s3.Bucket(this, 'Bucket', {\n  encryption: s3.BucketEncryption.KMS,\n  encryptionKey: kmsKey,\n});\n```\n\n**SQS/SNS encryption**:\n\n```typescript\nconst queue = new sqs.Queue(this, 'Queue', {\n  encryption: sqs.QueueEncryption.KMS,\n  encryptionMasterKey: kmsKey,\n});\n\nconst topic = new sns.Topic(this, 'Topic', {\n  masterKey: kmsKey,\n});\n```\n\n### Encryption in Transit\n\n**All AWS service APIs use TLS**:\n- API Gateway endpoints use HTTPS by default\n- Lambda to AWS service communication encrypted\n- EventBridge, SQS, SNS use TLS\n- Custom domains can use ACM certificates\n\n```typescript\n// API Gateway with custom domain\nconst certificate = new acm.Certificate(this, 'Certificate', {\n  domainName: 'api.example.com',\n  validation: acm.CertificateValidation.fromDns(hostedZone),\n});\n\nconst api = new apigateway.RestApi(this, 'Api', {\n  domainName: {\n    domainName: 'api.example.com',\n    certificate,\n  },\n});\n```\n\n### Data Sanitization\n\n**Validate and sanitize inputs**:\n\n```typescript\nimport DOMPurify from 'isomorphic-dompurify';\nimport { z } from 'zod';\n\n// Schema validation\nconst OrderSchema = z.object({\n  orderId: z.string().uuid(),\n  amount: z.number().positive(),\n  email: z.string().email(),\n});\n\nexport const handler = async (event: any) => {\n  const body = JSON.parse(event.body);\n\n  // Validate schema\n  const result = OrderSchema.safeParse(body);\n  if (!result.success) {\n    return {\n      statusCode: 400,\n      body: JSON.stringify({ error: result.error }),\n    };\n  }\n\n  // Sanitize HTML inputs\n  const sanitized = {\n    ...result.data,\n    description: DOMPurify.sanitize(result.data.description),\n  };\n\n  await processOrder(sanitized);\n};\n```\n\n## Network Security\n\n### VPC Configuration\n\n**Lambda in VPC for private resources**:\n\n```typescript\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 2,\n  natGateways: 1,\n});\n\n// Lambda in private subnet\nconst vpcFunction = new NodejsFunction(this, 'VpcFunction', {\n  entry: 'src/handler.ts',\n  vpc,\n  vpcSubnets: {\n    subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS,\n  },\n  securityGroups: [securityGroup],\n});\n\n// Security group for Lambda\nconst securityGroup = new ec2.SecurityGroup(this, 'LambdaSG', {\n  vpc,\n  description: 'Security group for Lambda function',\n  allowAllOutbound: false, // Restrict outbound\n});\n\n// Only allow access to RDS\nsecurityGroup.addEgressRule(\n  ec2.Peer.securityGroupId(rdsSecurityGroup.securityGroupId),\n  ec2.Port.tcp(3306),\n  'Allow MySQL access'\n);\n```\n\n### VPC Endpoints\n\n**Use VPC endpoints for AWS services**:\n\n```typescript\n// S3 VPC endpoint (gateway endpoint, no cost)\nvpc.addGatewayEndpoint('S3Endpoint', {\n  service: ec2.GatewayVpcEndpointAwsService.S3,\n});\n\n// DynamoDB VPC endpoint (gateway endpoint, no cost)\nvpc.addGatewayEndpoint('DynamoDBEndpoint', {\n  service: ec2.GatewayVpcEndpointAwsService.DYNAMODB,\n});\n\n// Secrets Manager VPC endpoint (interface endpoint, cost applies)\nvpc.addInterfaceEndpoint('SecretsManagerEndpoint', {\n  service: ec2.InterfaceVpcEndpointAwsService.SECRETS_MANAGER,\n  privateDnsEnabled: true,\n});\n```\n\n### Security Groups\n\n**Principle of least privilege for network access**:\n\n```typescript\n// Lambda security group\nconst lambdaSG = new ec2.SecurityGroup(this, 'LambdaSG', {\n  vpc,\n  allowAllOutbound: false,\n});\n\n// RDS security group\nconst rdsSG = new ec2.SecurityGroup(this, 'RDSSG', {\n  vpc,\n  allowAllOutbound: false,\n});\n\n// Allow Lambda to access RDS only\nrdsSG.addIngressRule(\n  ec2.Peer.securityGroupId(lambdaSG.securityGroupId),\n  ec2.Port.tcp(3306),\n  'Allow Lambda access'\n);\n\nlambdaSG.addEgressRule(\n  ec2.Peer.securityGroupId(rdsSG.securityGroupId),\n  ec2.Port.tcp(3306),\n  'Allow RDS access'\n);\n```\n\n## Security Monitoring\n\n### CloudWatch Logs\n\n**Enable and encrypt logs**:\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  logRetention: logs.RetentionDays.ONE_WEEK,\n  logGroup: new logs.LogGroup(this, 'LogGroup', {\n    encryptionKey: kmsKey, // Encrypt logs\n    retention: logs.RetentionDays.ONE_WEEK,\n  }),\n});\n```\n\n### CloudTrail\n\n**Enable CloudTrail for audit**:\n\n```typescript\nconst trail = new cloudtrail.Trail(this, 'Trail', {\n  isMultiRegionTrail: true,\n  includeGlobalServiceEvents: true,\n  managementEvents: cloudtrail.ReadWriteType.ALL,\n});\n\n// Log Lambda invocations\ntrail.addLambdaEventSelector([{\n  includeManagementEvents: true,\n  readWriteType: cloudtrail.ReadWriteType.ALL,\n}]);\n```\n\n### GuardDuty\n\n**Enable GuardDuty for threat detection**:\n- Analyzes VPC Flow Logs, DNS logs, CloudTrail events\n- Detects unusual API activity\n- Identifies compromised credentials\n- Monitors for cryptocurrency mining\n\n## Security Best Practices Checklist\n\n### Development\n\n- [ ] Validate and sanitize all inputs\n- [ ] Scan dependencies for vulnerabilities\n- [ ] Use least privilege IAM permissions\n- [ ] Store secrets in Secrets Manager or Parameter Store\n- [ ] Never log sensitive data\n- [ ] Enable encryption for all data stores\n- [ ] Use environment variables for configuration, not secrets\n\n### Deployment\n\n- [ ] Enable CloudTrail in all regions\n- [ ] Configure VPC for sensitive workloads\n- [ ] Use VPC endpoints for AWS service access\n- [ ] Enable GuardDuty for threat detection\n- [ ] Implement resource-based policies\n- [ ] Use AWS WAF for API protection\n- [ ] Enable access logging for API Gateway\n\n### Operations\n\n- [ ] Monitor CloudTrail for unusual activity\n- [ ] Set up alarms for security events\n- [ ] Rotate secrets regularly\n- [ ] Review IAM policies periodically\n- [ ] Audit function permissions\n- [ ] Monitor GuardDuty findings\n- [ ] Implement automated security responses\n\n### Testing\n\n- [ ] Test with least privilege policies\n- [ ] Validate error handling for security failures\n- [ ] Test input validation and sanitization\n- [ ] Verify encryption configurations\n- [ ] Test with malicious payloads\n- [ ] Audit logs for security events\n\n## Summary\n\n- **Shared Responsibility**: AWS handles infrastructure, you handle application security\n- **Least Privilege**: Use IAM grant methods, avoid wildcards\n- **Encryption**: Enable encryption at rest and in transit\n- **Input Validation**: Validate and sanitize all inputs\n- **Dependency Security**: Scan and update dependencies regularly\n- **Monitoring**: Enable CloudTrail, GuardDuty, and CloudWatch\n- **Secrets Management**: Use Secrets Manager, never environment variables\n- **Network Security**: Use VPC, security groups, and VPC endpoints appropriately\n",
        "skills/event-driven-serverless-systems/references/serverless-patterns.md": "# Serverless Architecture Patterns\n\nComprehensive patterns for building serverless applications on AWS based on Well-Architected Framework principles.\n\n## Table of Contents\n\n- [Core Serverless Patterns](#core-serverless-patterns)\n- [API Patterns](#api-patterns)\n- [Data Processing Patterns](#data-processing-patterns)\n- [Integration Patterns](#integration-patterns)\n- [Orchestration Patterns](#orchestration-patterns)\n- [Anti-Patterns](#anti-patterns)\n\n## Core Serverless Patterns\n\n### Pattern: Serverless Microservices\n\n**Use case**: Independent, scalable services with separate databases\n\n**Architecture**:\n```\nAPI Gateway  Lambda Functions  DynamoDB/RDS\n               (events)\n         EventBridge  Other Services\n```\n\n**CDK Implementation**:\n```typescript\n// User Service\nconst userTable = new dynamodb.Table(this, 'Users', {\n  partitionKey: { name: 'userId', type: dynamodb.AttributeType.STRING },\n  billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,\n});\n\nconst userFunction = new NodejsFunction(this, 'UserHandler', {\n  entry: 'src/services/users/handler.ts',\n  environment: {\n    TABLE_NAME: userTable.tableName,\n  },\n});\n\nuserTable.grantReadWriteData(userFunction);\n\n// Order Service (separate database)\nconst orderTable = new dynamodb.Table(this, 'Orders', {\n  partitionKey: { name: 'orderId', type: dynamodb.AttributeType.STRING },\n  billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,\n});\n\nconst orderFunction = new NodejsFunction(this, 'OrderHandler', {\n  entry: 'src/services/orders/handler.ts',\n  environment: {\n    TABLE_NAME: orderTable.tableName,\n    EVENT_BUS: eventBus.eventBusName,\n  },\n});\n\norderTable.grantReadWriteData(orderFunction);\neventBus.grantPutEventsTo(orderFunction);\n```\n\n**Benefits**:\n- Independent deployment and scaling\n- Database per service (data isolation)\n- Technology diversity\n- Fault isolation\n\n### Pattern: Serverless API Backend\n\n**Use case**: REST or GraphQL API with serverless compute\n\n**REST API with API Gateway**:\n```typescript\nconst api = new apigateway.RestApi(this, 'Api', {\n  restApiName: 'serverless-api',\n  deployOptions: {\n    stageName: 'prod',\n    tracingEnabled: true,\n    loggingLevel: apigateway.MethodLoggingLevel.INFO,\n    dataTraceEnabled: true,\n    metricsEnabled: true,\n  },\n  defaultCorsPreflightOptions: {\n    allowOrigins: apigateway.Cors.ALL_ORIGINS,\n    allowMethods: apigateway.Cors.ALL_METHODS,\n  },\n});\n\n// Resource-based routing\nconst items = api.root.addResource('items');\nitems.addMethod('GET', new apigateway.LambdaIntegration(listFunction));\nitems.addMethod('POST', new apigateway.LambdaIntegration(createFunction));\n\nconst item = items.addResource('{id}');\nitem.addMethod('GET', new apigateway.LambdaIntegration(getFunction));\nitem.addMethod('PUT', new apigateway.LambdaIntegration(updateFunction));\nitem.addMethod('DELETE', new apigateway.LambdaIntegration(deleteFunction));\n```\n\n**GraphQL API with AppSync**:\n```typescript\nconst api = new appsync.GraphqlApi(this, 'Api', {\n  name: 'serverless-graphql-api',\n  schema: appsync.SchemaFile.fromAsset('schema.graphql'),\n  authorizationConfig: {\n    defaultAuthorization: {\n      authorizationType: appsync.AuthorizationType.API_KEY,\n    },\n  },\n  xrayEnabled: true,\n});\n\n// Lambda resolver\nconst dataSource = api.addLambdaDataSource('lambda-ds', resolverFunction);\n\ndataSource.createResolver('QueryGetItem', {\n  typeName: 'Query',\n  fieldName: 'getItem',\n});\n```\n\n### Pattern: Serverless Data Lake\n\n**Use case**: Ingest, process, and analyze large-scale data\n\n**Architecture**:\n```\nS3 (raw data)  Lambda (transform)  S3 (processed)\n                   (catalog)\n               AWS Glue  Athena (query)\n```\n\n**Implementation**:\n```typescript\nconst rawBucket = new s3.Bucket(this, 'RawData');\nconst processedBucket = new s3.Bucket(this, 'ProcessedData');\n\n// Trigger Lambda on file upload\nrawBucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.LambdaDestination(transformFunction),\n  { prefix: 'incoming/' }\n);\n\n// Transform function\nexport const transform = async (event: S3Event) => {\n  for (const record of event.Records) {\n    const key = record.s3.object.key;\n\n    // Get raw data\n    const raw = await s3.getObject({\n      Bucket: record.s3.bucket.name,\n      Key: key,\n    });\n\n    // Transform data\n    const transformed = await transformData(raw.Body);\n\n    // Write to processed bucket\n    await s3.putObject({\n      Bucket: process.env.PROCESSED_BUCKET,\n      Key: `processed/${key}`,\n      Body: JSON.stringify(transformed),\n    });\n  }\n};\n```\n\n## API Patterns\n\n### Pattern: Authorizer Pattern\n\n**Use case**: Custom authentication and authorization\n\n```typescript\n// Lambda authorizer\nconst authorizer = new apigateway.TokenAuthorizer(this, 'Authorizer', {\n  handler: authorizerFunction,\n  identitySource: 'method.request.header.Authorization',\n  resultsCacheTtl: Duration.minutes(5),\n});\n\n// Apply to API methods\nconst resource = api.root.addResource('protected');\nresource.addMethod('GET', new apigateway.LambdaIntegration(protectedFunction), {\n  authorizer,\n});\n```\n\n### Pattern: Request Validation\n\n**Use case**: Validate requests before Lambda invocation\n\n```typescript\nconst requestModel = api.addModel('RequestModel', {\n  contentType: 'application/json',\n  schema: {\n    type: apigateway.JsonSchemaType.OBJECT,\n    required: ['name', 'email'],\n    properties: {\n      name: { type: apigateway.JsonSchemaType.STRING, minLength: 1 },\n      email: { type: apigateway.JsonSchemaType.STRING, format: 'email' },\n    },\n  },\n});\n\nresource.addMethod('POST', integration, {\n  requestValidator: new apigateway.RequestValidator(this, 'Validator', {\n    api,\n    validateRequestBody: true,\n    validateRequestParameters: true,\n  }),\n  requestModels: {\n    'application/json': requestModel,\n  },\n});\n```\n\n### Pattern: Response Caching\n\n**Use case**: Reduce backend load and improve latency\n\n```typescript\nconst api = new apigateway.RestApi(this, 'Api', {\n  deployOptions: {\n    cachingEnabled: true,\n    cacheTtl: Duration.minutes(5),\n    cacheClusterEnabled: true,\n    cacheClusterSize: '0.5', // GB\n  },\n});\n\n// Enable caching per method\nresource.addMethod('GET', integration, {\n  methodResponses: [{\n    statusCode: '200',\n    responseParameters: {\n      'method.response.header.Cache-Control': true,\n    },\n  }],\n});\n```\n\n## Data Processing Patterns\n\n### Pattern: S3 Event Processing\n\n**Use case**: Process files uploaded to S3\n\n```typescript\nconst bucket = new s3.Bucket(this, 'DataBucket');\n\n// Process images\nbucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.LambdaDestination(imageProcessingFunction),\n  { suffix: '.jpg' }\n);\n\n// Process CSV files\nbucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.LambdaDestination(csvProcessingFunction),\n  { suffix: '.csv' }\n);\n\n// Large file processing with Step Functions\nbucket.addEventNotification(\n  s3.EventType.OBJECT_CREATED,\n  new s3n.SfnDestination(processingStateMachine),\n  { prefix: 'large-files/' }\n);\n```\n\n### Pattern: DynamoDB Streams Processing\n\n**Use case**: React to database changes\n\n```typescript\nconst table = new dynamodb.Table(this, 'Table', {\n  partitionKey: { name: 'id', type: dynamodb.AttributeType.STRING },\n  stream: dynamodb.StreamViewType.NEW_AND_OLD_IMAGES,\n});\n\n// Process stream changes\nnew lambda.EventSourceMapping(this, 'StreamConsumer', {\n  target: streamProcessorFunction,\n  eventSourceArn: table.tableStreamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n  batchSize: 100,\n  maxBatchingWindow: Duration.seconds(5),\n  bisectBatchOnError: true,\n  retryAttempts: 3,\n});\n\n// Example: Sync to search index\nexport const processStream = async (event: DynamoDBStreamEvent) => {\n  for (const record of event.Records) {\n    if (record.eventName === 'INSERT' || record.eventName === 'MODIFY') {\n      const newImage = record.dynamodb?.NewImage;\n      await elasticSearch.index({\n        index: 'items',\n        id: newImage?.id.S,\n        body: unmarshall(newImage),\n      });\n    } else if (record.eventName === 'REMOVE') {\n      await elasticSearch.delete({\n        index: 'items',\n        id: record.dynamodb?.Keys?.id.S,\n      });\n    }\n  }\n};\n```\n\n### Pattern: Kinesis Stream Processing\n\n**Use case**: Real-time data streaming and analytics\n\n```typescript\nconst stream = new kinesis.Stream(this, 'EventStream', {\n  shardCount: 2,\n  streamMode: kinesis.StreamMode.PROVISIONED,\n});\n\n// Fan-out with multiple consumers\nconst consumer1 = new lambda.EventSourceMapping(this, 'Analytics', {\n  target: analyticsFunction,\n  eventSourceArn: stream.streamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n  batchSize: 100,\n  parallelizationFactor: 10, // Process 10 batches per shard in parallel\n});\n\nconst consumer2 = new lambda.EventSourceMapping(this, 'Alerting', {\n  target: alertingFunction,\n  eventSourceArn: stream.streamArn,\n  startingPosition: lambda.StartingPosition.LATEST,\n  filters: [\n    lambda.FilterCriteria.filter({\n      eventName: lambda.FilterRule.isEqual('CRITICAL_EVENT'),\n    }),\n  ],\n});\n```\n\n## Integration Patterns\n\n### Pattern: Service Integration with EventBridge\n\n**Use case**: Decouple services with events\n\n```typescript\nconst eventBus = new events.EventBus(this, 'AppBus');\n\n// Service A publishes events\nconst serviceA = new NodejsFunction(this, 'ServiceA', {\n  entry: 'src/services/a/handler.ts',\n  environment: {\n    EVENT_BUS: eventBus.eventBusName,\n  },\n});\n\neventBus.grantPutEventsTo(serviceA);\n\n// Service B subscribes to events\nnew events.Rule(this, 'ServiceBRule', {\n  eventBus,\n  eventPattern: {\n    source: ['service.a'],\n    detailType: ['EntityCreated'],\n  },\n  targets: [new targets.LambdaFunction(serviceBFunction)],\n});\n\n// Service C subscribes to same events\nnew events.Rule(this, 'ServiceCRule', {\n  eventBus,\n  eventPattern: {\n    source: ['service.a'],\n    detailType: ['EntityCreated'],\n  },\n  targets: [new targets.LambdaFunction(serviceCFunction)],\n});\n```\n\n### Pattern: API Gateway + SQS Integration\n\n**Use case**: Async API requests without Lambda\n\n```typescript\nconst queue = new sqs.Queue(this, 'RequestQueue');\n\nconst api = new apigateway.RestApi(this, 'Api');\n\n// Direct SQS integration (no Lambda)\nconst sqsIntegration = new apigateway.AwsIntegration({\n  service: 'sqs',\n  path: `${process.env.AWS_ACCOUNT_ID}/${queue.queueName}`,\n  integrationHttpMethod: 'POST',\n  options: {\n    credentialsRole: sqsRole,\n    requestParameters: {\n      'integration.request.header.Content-Type': \"'application/x-www-form-urlencoded'\",\n    },\n    requestTemplates: {\n      'application/json': 'Action=SendMessage&MessageBody=$input.body',\n    },\n    integrationResponses: [{\n      statusCode: '200',\n    }],\n  },\n});\n\napi.root.addMethod('POST', sqsIntegration, {\n  methodResponses: [{ statusCode: '200' }],\n});\n```\n\n### Pattern: EventBridge + Step Functions\n\n**Use case**: Event-triggered workflow orchestration\n\n```typescript\n// State machine for order processing\nconst orderStateMachine = new stepfunctions.StateMachine(this, 'OrderFlow', {\n  definition: /* ... */,\n});\n\n// EventBridge triggers state machine\nnew events.Rule(this, 'OrderPlacedRule', {\n  eventPattern: {\n    source: ['orders'],\n    detailType: ['OrderPlaced'],\n  },\n  targets: [new targets.SfnStateMachine(orderStateMachine)],\n});\n```\n\n## Orchestration Patterns\n\n### Pattern: Sequential Workflow\n\n**Use case**: Multi-step process with dependencies\n\n```typescript\nconst definition = new tasks.LambdaInvoke(this, 'Step1', {\n  lambdaFunction: step1Function,\n  outputPath: '$.Payload',\n})\n  .next(new tasks.LambdaInvoke(this, 'Step2', {\n    lambdaFunction: step2Function,\n    outputPath: '$.Payload',\n  }))\n  .next(new tasks.LambdaInvoke(this, 'Step3', {\n    lambdaFunction: step3Function,\n    outputPath: '$.Payload',\n  }));\n\nnew stepfunctions.StateMachine(this, 'Sequential', {\n  definition,\n});\n```\n\n### Pattern: Parallel Execution\n\n**Use case**: Execute independent tasks concurrently\n\n```typescript\nconst parallel = new stepfunctions.Parallel(this, 'ParallelProcessing');\n\nparallel.branch(new tasks.LambdaInvoke(this, 'ProcessA', {\n  lambdaFunction: functionA,\n}));\n\nparallel.branch(new tasks.LambdaInvoke(this, 'ProcessB', {\n  lambdaFunction: functionB,\n}));\n\nparallel.branch(new tasks.LambdaInvoke(this, 'ProcessC', {\n  lambdaFunction: functionC,\n}));\n\nconst definition = parallel.next(new tasks.LambdaInvoke(this, 'Aggregate', {\n  lambdaFunction: aggregateFunction,\n}));\n\nnew stepfunctions.StateMachine(this, 'Parallel', { definition });\n```\n\n### Pattern: Map State (Dynamic Parallelism)\n\n**Use case**: Process array of items in parallel\n\n```typescript\nconst mapState = new stepfunctions.Map(this, 'ProcessItems', {\n  maxConcurrency: 10,\n  itemsPath: '$.items',\n});\n\nmapState.iterator(new tasks.LambdaInvoke(this, 'ProcessItem', {\n  lambdaFunction: processItemFunction,\n}));\n\nconst definition = mapState.next(new tasks.LambdaInvoke(this, 'Finalize', {\n  lambdaFunction: finalizeFunction,\n}));\n```\n\n### Pattern: Choice State (Conditional Logic)\n\n**Use case**: Branching logic based on input\n\n```typescript\nconst choice = new stepfunctions.Choice(this, 'OrderType');\n\nchoice.when(\n  stepfunctions.Condition.stringEquals('$.orderType', 'STANDARD'),\n  standardProcessing\n);\n\nchoice.when(\n  stepfunctions.Condition.stringEquals('$.orderType', 'EXPRESS'),\n  expressProcessing\n);\n\nchoice.otherwise(defaultProcessing);\n```\n\n### Pattern: Wait State\n\n**Use case**: Delay between steps or wait for callbacks\n\n```typescript\n// Fixed delay\nconst wait = new stepfunctions.Wait(this, 'Wait30Seconds', {\n  time: stepfunctions.WaitTime.duration(Duration.seconds(30)),\n});\n\n// Wait until timestamp\nconst waitUntil = new stepfunctions.Wait(this, 'WaitUntil', {\n  time: stepfunctions.WaitTime.timestampPath('$.expiryTime'),\n});\n\n// Wait for callback (.waitForTaskToken)\nconst waitForCallback = new tasks.LambdaInvoke(this, 'WaitForApproval', {\n  lambdaFunction: approvalFunction,\n  integrationPattern: stepfunctions.IntegrationPattern.WAIT_FOR_TASK_TOKEN,\n  payload: stepfunctions.TaskInput.fromObject({\n    token: stepfunctions.JsonPath.taskToken,\n    data: stepfunctions.JsonPath.entirePayload,\n  }),\n});\n```\n\n## Anti-Patterns\n\n###  Lambda Monolith\n\n**Problem**: Single Lambda handling all operations\n\n```typescript\n// BAD\nexport const handler = async (event: any) => {\n  switch (event.operation) {\n    case 'createUser': return createUser(event);\n    case 'getUser': return getUser(event);\n    case 'updateUser': return updateUser(event);\n    case 'deleteUser': return deleteUser(event);\n    case 'createOrder': return createOrder(event);\n    // ... 20 more operations\n  }\n};\n```\n\n**Solution**: Separate Lambda functions per operation\n\n```typescript\n// GOOD - Separate functions\nexport const createUser = async (event: any) => { /* ... */ };\nexport const getUser = async (event: any) => { /* ... */ };\nexport const updateUser = async (event: any) => { /* ... */ };\n```\n\n###  Recursive Lambda Pattern\n\n**Problem**: Lambda invoking itself (runaway costs)\n\n```typescript\n// BAD\nexport const handler = async (event: any) => {\n  await processItem(event);\n\n  if (hasMoreItems()) {\n    await lambda.invoke({\n      FunctionName: process.env.AWS_LAMBDA_FUNCTION_NAME,\n      InvocationType: 'Event',\n      Payload: JSON.stringify({ /* next batch */ }),\n    });\n  }\n};\n```\n\n**Solution**: Use SQS or Step Functions\n\n```typescript\n// GOOD - Use SQS for iteration\nexport const handler = async (event: SQSEvent) => {\n  for (const record of event.Records) {\n    await processItem(record);\n  }\n  // SQS handles iteration automatically\n};\n```\n\n###  Lambda Chaining\n\n**Problem**: Lambda directly invoking another Lambda\n\n```typescript\n// BAD\nexport const handler1 = async (event: any) => {\n  const result = await processStep1(event);\n\n  // Directly invoking next Lambda\n  await lambda.invoke({\n    FunctionName: 'handler2',\n    Payload: JSON.stringify(result),\n  });\n};\n```\n\n**Solution**: Use EventBridge, SQS, or Step Functions\n\n```typescript\n// GOOD - Publish to EventBridge\nexport const handler1 = async (event: any) => {\n  const result = await processStep1(event);\n\n  await eventBridge.putEvents({\n    Entries: [{\n      Source: 'service.step1',\n      DetailType: 'Step1Completed',\n      Detail: JSON.stringify(result),\n    }],\n  });\n};\n```\n\n###  Synchronous Waiting in Lambda\n\n**Problem**: Lambda waiting for slow operations\n\n```typescript\n// BAD - Blocking on slow operation\nexport const handler = async (event: any) => {\n  await startBatchJob(); // Returns immediately\n\n  // Wait for job to complete (wastes Lambda time)\n  while (true) {\n    const status = await checkJobStatus();\n    if (status === 'COMPLETE') break;\n    await sleep(1000);\n  }\n};\n```\n\n**Solution**: Use Step Functions with callback pattern\n\n```typescript\n// GOOD - Step Functions waits, not Lambda\nconst waitForJob = new tasks.LambdaInvoke(this, 'StartJob', {\n  lambdaFunction: startJobFunction,\n  integrationPattern: stepfunctions.IntegrationPattern.WAIT_FOR_TASK_TOKEN,\n  payload: stepfunctions.TaskInput.fromObject({\n    token: stepfunctions.JsonPath.taskToken,\n  }),\n});\n```\n\n###  Large Deployment Packages\n\n**Problem**: Large Lambda packages increase cold start time\n\n**Solution**:\n- Use layers for shared dependencies\n- Externalize AWS SDK\n- Minimize bundle size\n\n```typescript\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  bundling: {\n    minify: true,\n    externalModules: ['@aws-sdk/*'], // Provided by runtime\n    nodeModules: ['only-needed-deps'], // Selective bundling\n  },\n});\n```\n\n## Performance Optimization\n\n### Cold Start Optimization\n\n**Techniques**:\n1. Minimize package size\n2. Use provisioned concurrency for critical paths\n3. Lazy load dependencies\n4. Reuse connections outside handler\n5. Use Lambda SnapStart (Java)\n\n```typescript\n// For latency-sensitive APIs\nconst apiFunction = new NodejsFunction(this, 'ApiFunction', {\n  entry: 'src/api.ts',\n  memorySize: 1769, // 1 vCPU for faster initialization\n});\n\nconst alias = apiFunction.currentVersion.addAlias('live');\nalias.addAutoScaling({\n  minCapacity: 2,\n  maxCapacity: 10,\n}).scaleOnUtilization({\n  utilizationTarget: 0.7,\n});\n```\n\n### Right-Sizing Memory\n\n**Test different memory configurations**:\n\n```typescript\n// CPU-bound workload\nnew NodejsFunction(this, 'ComputeFunction', {\n  memorySize: 1769, // 1 vCPU\n  timeout: Duration.seconds(30),\n});\n\n// I/O-bound workload\nnew NodejsFunction(this, 'IOFunction', {\n  memorySize: 512, // Less CPU needed\n  timeout: Duration.seconds(60),\n});\n\n// Simple operations\nnew NodejsFunction(this, 'SimpleFunction', {\n  memorySize: 256,\n  timeout: Duration.seconds(10),\n});\n```\n\n### Concurrent Execution Control\n\n```typescript\n// Protect downstream services\nnew NodejsFunction(this, 'Function', {\n  reservedConcurrentExecutions: 10, // Max 10 concurrent\n});\n\n// Unreserved concurrency (shared pool)\nnew NodejsFunction(this, 'Function', {\n  // Uses unreserved account concurrency\n});\n```\n\n## Testing Strategies\n\n### Unit Testing\n\nTest business logic separate from AWS services:\n\n```typescript\n// handler.ts\nexport const processOrder = async (order: Order): Promise<Result> => {\n  // Business logic (easily testable)\n  const validated = validateOrder(order);\n  const priced = calculatePrice(validated);\n  return transformResult(priced);\n};\n\nexport const handler = async (event: any): Promise<any> => {\n  const order = parseEvent(event);\n  const result = await processOrder(order);\n  await saveToDatabase(result);\n  return formatResponse(result);\n};\n\n// handler.test.ts\ntest('processOrder calculates price correctly', () => {\n  const order = { items: [{ price: 10, quantity: 2 }] };\n  const result = processOrder(order);\n  expect(result.total).toBe(20);\n});\n```\n\n### Integration Testing\n\nTest with actual AWS services:\n\n```typescript\n// integration.test.ts\nimport { LambdaClient, InvokeCommand } from '@aws-sdk/client-lambda';\n\ntest('Lambda processes order correctly', async () => {\n  const lambda = new LambdaClient({});\n\n  const response = await lambda.send(new InvokeCommand({\n    FunctionName: process.env.FUNCTION_NAME,\n    Payload: JSON.stringify({ orderId: '123' }),\n  }));\n\n  const result = JSON.parse(Buffer.from(response.Payload!).toString());\n  expect(result.statusCode).toBe(200);\n});\n```\n\n### Local Testing with SAM\n\n```bash\n# Test API locally\nsam local start-api\n\n# Invoke function locally\nsam local invoke MyFunction -e events/test-event.json\n\n# Generate sample event\nsam local generate-event apigateway aws-proxy > event.json\n```\n\n## Summary\n\n- **Single Purpose**: One function, one responsibility\n- **Concurrent Design**: Think concurrency, not volume\n- **Stateless**: Use external storage for state\n- **State Machines**: Orchestrate with Step Functions\n- **Event-Driven**: Use events over direct calls\n- **Idempotent**: Handle failures and duplicates gracefully\n- **Observability**: Enable tracing and structured logging\n",
        "skills/feedback-application/SKILL.md": "---\nname: feedback-application\ndescription: Apply feedback and implement review suggestions. Incorporates reviewer comments into code changes systematically.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Review Feedback Implementation\n\nSystematically process and implement changes based on code review feedback.\n\n## When to Use\n\n- Provides reviewer comments or feedback\n- Pastes PR review notes\n- Mentions implementing review suggestions\n- Says \"address these comments\" or \"implement feedback\"\n- Shares list of changes requested by reviewers\n\n## Systematic Workflow\n\n### 1. Parse Reviewer Notes\n\nIdentify individual feedback items:\n- Split numbered lists (1., 2., etc.)\n- Handle bullet points or unnumbered feedback\n- Extract distinct change requests\n- Clarify ambiguous items before starting\n\n### 2. Create Todo List\n\nUse TodoWrite tool to create actionable tasks:\n- Each feedback item becomes one or more todos\n- Break down complex feedback into smaller tasks\n- Make tasks specific and measurable\n- Mark first task as `in_progress` before starting\n\nExample:\n```\n- Add type hints to extract function\n- Fix duplicate tag detection logic\n- Update docstring in chain.py\n- Add unit test for edge case\n```\n\n### 3. Implement Changes Systematically\n\nFor each todo item:\n\n**Locate relevant code:**\n- Use Grep to search for functions/classes\n- Use Glob to find files by pattern\n- Read current implementation\n\n**Make changes:**\n- Use Edit tool for modifications\n- Follow project conventions (CLAUDE.md)\n- Preserve existing functionality unless changing behavior\n\n**Verify changes:**\n- Check syntax correctness\n- Run relevant tests if applicable\n- Ensure changes address reviewer's intent\n\n**Update status:**\n- Mark todo as `completed` immediately after finishing\n- Move to next todo (only one `in_progress` at a time)\n\n### 4. Handle Different Feedback Types\n\n**Code changes:**\n- Use Edit tool for existing code\n- Follow type hint conventions (PEP 604/585)\n- Maintain consistent style\n\n**New features:**\n- Create new files with Write tool if needed\n- Add corresponding tests\n- Update documentation\n\n**Documentation:**\n- Update docstrings following project style\n- Modify markdown files as needed\n- Keep explanations concise\n\n**Tests:**\n- Write tests as functions, not classes\n- Use descriptive names\n- Follow pytest conventions\n\n**Refactoring:**\n- Preserve functionality\n- Improve code structure\n- Run tests to verify no regressions\n\n### 5. Validation\n\nAfter implementing changes:\n- Run affected tests\n- Check for linting errors: `uv run ruff check`\n- Verify changes don't break existing functionality\n\n### 6. Communication\n\nKeep user informed:\n- Update todo list in real-time\n- Ask for clarification on ambiguous feedback\n- Report blockers or challenges\n- Summarize changes at completion\n\n## Edge Cases\n\n**Conflicting feedback:**\n- Ask user for guidance\n- Explain conflict clearly\n\n**Breaking changes required:**\n- Notify user before implementing\n- Discuss impact and alternatives\n\n**Tests fail after changes:**\n- Fix tests before marking todo complete\n- Ensure all related tests pass\n\n**Referenced code doesn't exist:**\n- Ask user for clarification\n- Verify understanding before proceeding\n\n## Important Guidelines\n\n- **Always use TodoWrite** for tracking progress\n- **Mark todos completed immediately** after each item\n- **Only one todo in_progress** at any time\n- **Don't batch completions** - update status in real-time\n- **Ask questions** for unclear feedback\n- **Run tests** if changes affect tested code\n- **Follow CLAUDE.md conventions** for all code changes\n- **Use conventional commits** if creating commits afterward",
        "skills/financial-analysis-agent/SKILL.md": "---\nname: financial-analysis-agent\ndescription: Create agents for financial analysis, investment research, and portfolio management. Covers financial data processing, risk analysis, and recommendation generation. Use when building investment analysis tools, robo-advisors, portfolio trackers, or financial intelligence systems.\n---\n\n# Financial Analysis Agent\n\nBuild intelligent financial analysis agents that evaluate investments, assess risks, and generate data-driven recommendations.\n\n## Financial Data Integration\n\nSee [examples/financial_data_collector.py](examples/financial_data_collector.py) for the `FinancialDataCollector` class that:\n- Integrates with yfinance for stock data\n- Retrieves financial statements (income, balance sheet, cash flow)\n- Fetches key metrics (market cap, PE ratio, dividend yield, etc.)\n\n## Financial Analysis Techniques\n\n### Technical Analysis\nSee [examples/technical_analyzer.py](examples/technical_analyzer.py) for `TechnicalAnalyzer`:\n- Moving averages calculation\n- Relative Strength Index (RSI)\n- Support and resistance level identification\n\n### Fundamental Analysis\nSee [examples/fundamental_analyzer.py](examples/fundamental_analyzer.py) for `FundamentalAnalyzer`:\n- Profitability ratios (gross margin, operating margin, net margin, ROA, ROE)\n- Valuation ratios (PE, PB, PEG, price-to-sales)\n- Liquidity ratios (current ratio, quick ratio, debt-to-equity)\n\n### Risk Assessment\nSee [examples/risk_analyzer.py](examples/risk_analyzer.py) for `RiskAnalyzer`:\n- Volatility calculation\n- Value at Risk (VaR) assessment\n- Sharpe Ratio calculation\n- Company risk assessment\n\n## Investment Recommendations\n\nSee [examples/investment_recommender.py](examples/investment_recommender.py) for `InvestmentRecommender`:\n- Generates recommendations (Strong Buy, Buy, Hold, Sell, Strong Sell)\n- Calculates investment scores based on technical and fundamental signals\n- Provides confidence levels and risk assessments\n\n## Portfolio Management\n\nSee [examples/portfolio_manager.py](examples/portfolio_manager.py) for `PortfolioManager`:\n- Calculate portfolio total value\n- Rebalance portfolio based on target allocations\n- Assess portfolio risk and volatility\n\n## Market Intelligence\n\nBuild market intelligence capabilities by:\n- Analyzing overall market trends and sector performance\n- Calculating market volatility indices\n- Fetching economic indicators\n- Identifying undervalued, growth, and dividend opportunities\n\n## Best Practices\n\n### Analysis Quality\n-  Use multiple data sources\n-  Cross-validate findings\n-  Document assumptions\n-  Consider time horizons\n-  Account for fees and taxes\n\n### Risk Management\n-  Assess downside risk\n-  Implement stop losses\n-  Diversify appropriately\n-  Position size accordingly\n-  Review regularly\n\n### Ethical Considerations\n-  Disclose conflicts of interest\n-  Avoid market manipulation\n-  Base recommendations on analysis\n-  Update recommendations regularly\n-  Acknowledge limitations\n\n## Tools & Data Sources\n\n### Data APIs\n- yfinance\n- Alpha Vantage\n- IEX Cloud\n- Polygon.io\n- Yahoo Finance\n\n### Analysis Libraries\n- pandas\n- NumPy\n- scikit-learn\n- TA-Lib\n- statsmodels\n\n## Getting Started\n\n1. Collect financial data\n2. Perform technical analysis\n3. Analyze fundamentals\n4. Assess risks\n5. Generate recommendations\n6. Monitor positions\n7. Rebalance periodically\n\n",
        "skills/financial-document-management/SKILL.md": "---\nname: financial-document-management\ndescription: Manage and organize financial documents and invoices. Categorizes, extracts information, and maintains financial records systematically.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Invoice Organizer\n\nThis skill transforms chaotic folders of invoices, receipts, and financial documents into a clean, tax-ready filing system without manual effort.\n\n## When to Use This Skill\n\n- Preparing for tax season and need organized records\n- Managing business expenses across multiple vendors\n- Organizing receipts from a messy folder or email downloads\n- Setting up automated invoice filing for ongoing bookkeeping\n- Archiving financial records by year or category\n- Reconciling expenses for reimbursement\n- Preparing documentation for accountants\n\n## What This Skill Does\n\n1. **Reads Invoice Content**: Extracts information from PDFs, images, and documents:\n   - Vendor/company name\n   - Invoice number\n   - Date\n   - Amount\n   - Product or service description\n   - Payment method\n\n2. **Renames Files Consistently**: Creates standardized filenames:\n   - Format: `YYYY-MM-DD Vendor - Invoice - ProductOrService.pdf`\n   - Examples: `2024-03-15 Adobe - Invoice - Creative Cloud.pdf`\n\n3. **Organizes by Category**: Sorts into logical folders:\n   - By vendor\n   - By expense category (software, office, travel, etc.)\n   - By time period (year, quarter, month)\n   - By tax category (deductible, personal, etc.)\n\n4. **Handles Multiple Formats**: Works with:\n   - PDF invoices\n   - Scanned receipts (JPG, PNG)\n   - Email attachments\n   - Screenshots\n   - Bank statements\n\n5. **Maintains Originals**: Preserves original files while organizing copies\n\n## How to Use\n\n### Basic Usage\n\nNavigate to your messy invoice folder:\n```\ncd ~/Desktop/receipts-to-sort\n```\n\nThen ask Claude Code:\n```\nOrganize these invoices for taxes\n```\n\nOr more specifically:\n```\nRead all invoices in this folder, rename them to \n\"YYYY-MM-DD Vendor - Invoice - Product.pdf\" format, \nand organize them by vendor\n```\n\n### Advanced Organization\n\n```\nOrganize these invoices:\n1. Extract date, vendor, and description from each file\n2. Rename to standard format\n3. Sort into folders by expense category (Software, Office, Travel, etc.)\n4. Create a CSV spreadsheet with all invoice details for my accountant\n```\n\n## Instructions\n\nWhen a user requests invoice organization:\n\n1. **Scan the Folder**\n   \n   Identify all invoice files:\n   ```bash\n   # Find all invoice-related files\n   find . -type f \\( -name \"*.pdf\" -o -name \"*.jpg\" -o -name \"*.png\" \\) -print\n   ```\n   \n   Report findings:\n   - Total number of files\n   - File types\n   - Date range (if discernible from names)\n   - Current organization (or lack thereof)\n\n2. **Extract Information from Each File**\n   \n   For each invoice, extract:\n   \n   **From PDF invoices**:\n   - Use text extraction to read invoice content\n   - Look for common patterns:\n     - \"Invoice Date:\", \"Date:\", \"Issued:\"\n     - \"Invoice #:\", \"Invoice Number:\"\n     - Company name (usually at top)\n     - \"Amount Due:\", \"Total:\", \"Amount:\"\n     - \"Description:\", \"Service:\", \"Product:\"\n   \n   **From image receipts**:\n   - Read visible text from images\n   - Identify vendor name (often at top)\n   - Look for date (common formats)\n   - Find total amount\n   \n   **Fallback for unclear files**:\n   - Use filename clues\n   - Check file creation/modification date\n   - Flag for manual review if critical info missing\n\n3. **Determine Organization Strategy**\n   \n   Ask user preference if not specified:\n   \n   ```markdown\n   I found [X] invoices from [date range].\n   \n   How would you like them organized?\n   \n   1. **By Vendor** (Adobe/, Amazon/, Stripe/, etc.)\n   2. **By Category** (Software/, Office Supplies/, Travel/, etc.)\n   3. **By Date** (2024/Q1/, 2024/Q2/, etc.)\n   4. **By Tax Category** (Deductible/, Personal/, etc.)\n   5. **Custom** (describe your structure)\n   \n   Or I can use a default structure: Year/Category/Vendor\n   ```\n\n4. **Create Standardized Filename**\n   \n   For each invoice, create a filename following this pattern:\n   \n   ```\n   YYYY-MM-DD Vendor - Invoice - Description.ext\n   ```\n   \n   Examples:\n   - `2024-03-15 Adobe - Invoice - Creative Cloud.pdf`\n   - `2024-01-10 Amazon - Receipt - Office Supplies.pdf`\n   - `2023-12-01 Stripe - Invoice - Monthly Payment Processing.pdf`\n   \n   **Filename Best Practices**:\n   - Remove special characters except hyphens\n   - Capitalize vendor names properly\n   - Keep descriptions concise but meaningful\n   - Use consistent date format (YYYY-MM-DD) for sorting\n   - Preserve original file extension\n\n5. **Execute Organization**\n   \n   Before moving files, show the plan:\n   \n   ```markdown\n   # Organization Plan\n   \n   ## Proposed Structure\n   ```\n   Invoices/\n    2023/\n       Software/\n          Adobe/\n          Microsoft/\n       Services/\n       Office/\n    2024/\n        Software/\n        Services/\n        Office/\n   ```\n   \n   ## Sample Changes\n   \n   Before: `invoice_adobe_march.pdf`\n   After: `2024-03-15 Adobe - Invoice - Creative Cloud.pdf`\n   Location: `Invoices/2024/Software/Adobe/`\n   \n   Before: `IMG_2847.jpg`\n   After: `2024-02-10 Staples - Receipt - Office Supplies.jpg`\n   Location: `Invoices/2024/Office/Staples/`\n   \n   Process [X] files? (yes/no)\n   ```\n   \n   After approval:\n   ```bash\n   # Create folder structure\n   mkdir -p \"Invoices/2024/Software/Adobe\"\n   \n   # Copy (don't move) to preserve originals\n   cp \"original.pdf\" \"Invoices/2024/Software/Adobe/2024-03-15 Adobe - Invoice - Creative Cloud.pdf\"\n   \n   # Or move if user prefers\n   mv \"original.pdf\" \"new/path/standardized-name.pdf\"\n   ```\n\n6. **Generate Summary Report**\n   \n   Create a CSV file with all invoice details:\n   \n   ```csv\n   Date,Vendor,Invoice Number,Description,Amount,Category,File Path\n   2024-03-15,Adobe,INV-12345,Creative Cloud,52.99,Software,Invoices/2024/Software/Adobe/2024-03-15 Adobe - Invoice - Creative Cloud.pdf\n   2024-03-10,Amazon,123-4567890-1234567,Office Supplies,127.45,Office,Invoices/2024/Office/Amazon/2024-03-10 Amazon - Receipt - Office Supplies.pdf\n   ...\n   ```\n   \n   This CSV is useful for:\n   - Importing into accounting software\n   - Sharing with accountants\n   - Expense tracking and reporting\n   - Tax preparation\n\n7. **Provide Completion Summary**\n   \n   ```markdown\n   # Organization Complete! \n   \n   ## Summary\n   - **Processed**: [X] invoices\n   - **Date range**: [earliest] to [latest]\n   - **Total amount**: $[sum] (if amounts extracted)\n   - **Vendors**: [Y] unique vendors\n   \n   ## New Structure\n   ```\n   Invoices/\n    2024/ (45 files)\n       Software/ (23 files)\n       Services/ (12 files)\n       Office/ (10 files)\n    2023/ (12 files)\n   ```\n   \n   ## Files Created\n   - `/Invoices/` - Organized invoices\n   - `/Invoices/invoice-summary.csv` - Spreadsheet for accounting\n   - `/Invoices/originals/` - Original files (if copied)\n   \n   ## Files Needing Review\n   [List any files where information couldn't be extracted completely]\n   \n   ## Next Steps\n   1. Review the `invoice-summary.csv` file\n   2. Check files in \"Needs Review\" folder\n   3. Import CSV into your accounting software\n   4. Set up auto-organization for future invoices\n   \n   Ready for tax season! \n   ```\n\n## Examples\n\n### Example 1: Tax Preparation (From Martin Merschroth)\n\n**User**: \"I have a messy folder of invoices for taxes. Sort them and rename properly.\"\n\n**Process**:\n1. Scans folder: finds 147 PDFs and images\n2. Reads each invoice to extract:\n   - Date\n   - Vendor name\n   - Invoice number\n   - Product/service description\n3. Renames all files: `YYYY-MM-DD Vendor - Invoice - Product.pdf`\n4. Organizes into: `2024/Software/`, `2024/Travel/`, etc.\n5. Creates `invoice-summary.csv` for accountant\n6. Result: Tax-ready organized invoices in minutes\n\n### Example 2: Monthly Expense Reconciliation\n\n**User**: \"Organize my business receipts from last month by category.\"\n\n**Output**:\n```markdown\n# March 2024 Receipts Organized\n\n## By Category\n- Software & Tools: $847.32 (12 invoices)\n- Office Supplies: $234.18 (8 receipts)\n- Travel & Meals: $1,456.90 (15 receipts)\n- Professional Services: $2,500.00 (3 invoices)\n\nTotal: $5,038.40\n\nAll receipts renamed and filed in:\n`Business-Receipts/2024/03-March/[Category]/`\n\nCSV export: `march-2024-expenses.csv`\n```\n\n### Example 3: Multi-Year Archive\n\n**User**: \"I have 3 years of random invoices. Organize them by year, then by vendor.\"\n\n**Output**: Creates structure:\n```\nInvoices/\n 2022/\n    Adobe/\n    Amazon/\n    ...\n 2023/\n    Adobe/\n    Amazon/\n    ...\n 2024/\n     Adobe/\n     Amazon/\n     ...\n```\n\nEach file properly renamed with date and description.\n\n### Example 4: Email Downloads Cleanup\n\n**User**: \"I download invoices from Gmail. They're all named 'invoice.pdf', 'invoice(1).pdf', etc. Fix this mess.\"\n\n**Output**:\n```markdown\nFound 89 files all named \"invoice*.pdf\"\n\nReading each file to extract real information...\n\nRenamed examples:\n- invoice.pdf  2024-03-15 Shopify - Invoice - Monthly Subscription.pdf\n- invoice(1).pdf  2024-03-14 Google - Invoice - Workspace.pdf\n- invoice(2).pdf  2024-03-10 Netlify - Invoice - Pro Plan.pdf\n\nAll files renamed and organized by vendor.\n```\n\n## Common Organization Patterns\n\n### By Vendor (Simple)\n```\nInvoices/\n Adobe/\n Amazon/\n Google/\n Microsoft/\n```\n\n### By Year and Category (Tax-Friendly)\n```\nInvoices/\n 2023/\n    Software/\n    Hardware/\n    Services/\n    Travel/\n 2024/\n     ...\n```\n\n### By Quarter (Detailed Tracking)\n```\nInvoices/\n 2024/\n    Q1/\n       Software/\n       Office/\n       Travel/\n    Q2/\n        ...\n```\n\n### By Tax Category (Accountant-Ready)\n```\nInvoices/\n Deductible/\n    Software/\n    Office/\n    Professional-Services/\n Partially-Deductible/\n    Meals-Travel/\n Personal/\n```\n\n## Automation Setup\n\nFor ongoing organization:\n\n```\nCreate a script that watches my ~/Downloads/invoices folder \nand auto-organizes any new invoice files using our standard \nnaming and folder structure.\n```\n\nThis creates a persistent solution that organizes invoices as they arrive.\n\n## Pro Tips\n\n1. **Scan emails to PDF**: Use Preview or similar to save email invoices as PDFs first\n2. **Consistent downloads**: Save all invoices to one folder for batch processing\n3. **Monthly routine**: Organize invoices monthly, not annually\n4. **Backup originals**: Keep original files before reorganizing\n5. **Include amounts in CSV**: Useful for budget tracking\n6. **Tag by deductibility**: Note which expenses are tax-deductible\n7. **Keep receipts 7 years**: Standard audit period\n\n## Handling Special Cases\n\n### Missing Information\nIf date/vendor can't be extracted:\n- Flag file for manual review\n- Use file modification date as fallback\n- Create \"Needs-Review/\" folder\n\n### Duplicate Invoices\nIf same invoice appears multiple times:\n- Compare file hashes\n- Keep highest quality version\n- Note duplicates in summary\n\n### Multi-Page Invoices\nFor invoices split across files:\n- Merge PDFs if needed\n- Use consistent naming for parts\n- Note in CSV if invoice is split\n\n### Non-Standard Formats\nFor unusual receipt formats:\n- Extract what's possible\n- Standardize what you can\n- Flag for review if critical info missing\n\n## Related Use Cases\n\n- Creating expense reports for reimbursement\n- Organizing bank statements\n- Managing vendor contracts\n- Archiving old financial records\n- Preparing for audits\n- Tracking subscription costs over time\n\n",
        "skills/genealogical-documentation/SKILL.md": "---\nname: genealogical-documentation\ndescription: Plan and document family history research systematically. Structures genealogical research with proper citations, evidence analysis, and organized family records.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Family History Research Planning Skill\n\n**Version:** 1.0.6\n**Last Updated:** November 6, 2025\n\n## CRITICAL: Always Plan Before Researching\n\n**ABSOLUTELY PROHIBITED: DO NOT perform unsolicited web searches or research.**\n\nWhen a user mentions an ancestor or asks for help researching, you MUST follow this sequence:\n\n1. **Gather information from the user first** - Ask what they already know about the ancestor\n2. **Define the research objective** - Work with the user to clarify their specific goals\n3. **Create a research plan** - Use the Research Planning Workflow below\n4. **Present the plan to the user** - Give them a structured plan with prioritized sources and search strategies\n\n**NEVER jump immediately to web searches when a user mentions an ancestor.**\n\nThe value of professional genealogy research is in systematic planning and methodology, not in rushing to find records. Always build a proper foundation through planning first.\n\n**AFTER creating a research plan:** If the user explicitly requests that you execute the research (perform searches), you may do so, but ONLY by following the approved research plan systematically. Document all searches, findings, and citations as you go.\n\n## When to Use This Skill\n\nTrigger this skill when users:\n- **Ask for help researching an ancestor**  START with research planning workflow, gather known info, CREATE a plan first (do NOT search immediately)\n- Plan or organize genealogy research projects  Use research planning workflow\n- Need to create proper genealogical citations  Use citation workflow\n- Have conflicting information from multiple sources  Use evidence analysis workflow\n- Want to analyze evidence quality and reliability\n- Need to build proof arguments for genealogical conclusions\n- Ask for help with census records, vital records, or other historical documents  Provide guidance and analysis\n- Need guidance on research strategies or methodologies  Teach concepts, create plans\n\n**Remember:** Always START with planning. Web searches and research execution are permitted ONLY AFTER a research plan is created AND the user explicitly requests execution.\n\n## Core Capabilities\n\n### 1. Research Planning and Strategy\n\nGuide researchers through creating structured research plans that incorporate professional standards.\n\n**Key Process:**\n1. Define specific research questions (who, what, when, where)\n2. Identify target individuals and relationships\n3. List potential record sources and repositories\n4. Develop search strategy using FAN principle (Family, Associates, Neighbors)\n5. Create timeline with milestones\n6. Establish success criteria and proof requirements\n\n**Output:** Create a research plan document using the template in `assets/templates/research-plan-template.md` (simplified for practical use). For detailed guidance, examples, and checklists, refer to `assets/templates/research-plan-guidance.md`\n\n### 2. Citation Creation\n\nGenerate properly formatted genealogical citations following Evidence Explained standards.\n\n**Supported Source Types:**\n- Census records (federal, state, territorial)\n- Vital records (birth, marriage, death)\n- Church records (baptism, marriage, burial)\n- Land records (deeds, grants, tax records)\n- Probate records (wills, estate files)\n- Military records (service, pensions)\n- Immigration records (passenger lists, naturalizations)\n- Newspapers (obituaries, notices)\n- Court records, city directories\n- Online databases (Ancestry, FamilySearch, etc.)\n- Published books and manuscripts\n\n**Citation Process:**\n1. Identify source type and access method\n2. Gather core information (who, what, when, where)\n3. Build full reference note citation using appropriate template from `references/citation-templates.md`\n4. Create short form for subsequent references\n5. Generate source list entry for bibliography\n6. Assess source quality (original vs. derivative, primary vs. secondary)\n\n**Output:** Citation entry using template in `assets/templates/citation-template.md`\n\n### 3. Evidence Analysis and Conflict Resolution\n\nSystematically analyze and resolve conflicts between genealogical sources.\n\n**Analysis Framework:**\n\n**Step 1: Inventory Sources**\n- List all sources providing information about the fact\n- Categorize by evidence type (direct/indirect/negative)\n\n**Step 2: Evaluate Each Source**\n- Source classification (original/derivative/authored)\n- Information type (primary/secondary/undetermined)\n- Informant analysis (who, relationship, knowledge level)\n- Reliability factors (timing, bias, consistency)\n\n**Step 3: Compare and Identify Conflicts**\n- Create evidence comparison matrix\n- Document specific discrepancies\n- Assess significance of conflicts\n\n**Step 4: Assess Reliability**\n- Rank sources from most to least reliable\n- Weight sources by quality, not quantity\n- Consider corroboration patterns\n\n**Step 5: Resolve Conflicts**\n- Explore possible explanations for conflicts\n- Apply evidence weight to determine preponderance\n- Resolve conflicts or acknowledge if unresolvable\n\n**Step 6: GPS Compliance Check**\nApply the five GPS elements:\n1. Reasonably exhaustive research\n2. Complete and accurate source citations\n3. Analysis and correlation of evidence\n4. Resolution of conflicting evidence\n5. Soundly reasoned, coherently written conclusion\n\n**Step 7: Build Proof Argument**\n- State conclusion clearly\n- Assign appropriate proof level (proven/probable/possible/unproven/disproven)\n- Write coherent proof argument explaining reasoning\n\n**Output:** Evidence analysis report using template in `assets/templates/evidence-analysis-template.md`\n\n### 4. Research Logging\n\nDocument research activities systematically to avoid duplication and track progress.\n\n**Essential Elements:**\n- Research session context (date, time, goal)\n- Research questions addressed\n- All sources searched (including negative results)\n- Search strategies and variations used\n- Positive findings with complete citations\n- Negative results documented\n- Evidence analysis and reliability notes\n- Next steps and follow-up actions\n\n**Output:** Research log entry using template in `assets/templates/research-log-template.md`\n\n## Default Workflow: Start Every Research Request This Way\n\nWhen a user asks for help researching an ancestor:\n\n**STEP 1: Information Gathering** (Always do this first)\n- Ask what they already know (name, dates, locations)\n- Ask what records they've already found\n- Ask what specific questions they want answered\n- Ask about any conflicting information they've encountered\n\n**STEP 2: Research Planning** (Required before any searches)\n- Work through the Research Planning Workflow (see below)\n- Create a structured plan document\n- Prioritize sources and strategies\n- Present the plan to the user\n\n**STEP 3: Research Execution** (ONLY if user explicitly requests it)\n- Follow the approved research plan systematically\n- Use appropriate tools (web_search, etc.) as directed by the plan\n- Document all searches (including negative results)\n- Create proper citations for all findings\n- Log all research activities\n- Report findings and analysis to the user\n\n**NEVER skip Steps 1 and 2 to jump directly to Step 3.**\n\nThe user may choose to execute the plan themselves, or they may explicitly ask you to execute the research. Either approach is acceptable, but planning MUST come first.\n\n## Procedural Guidelines\n\n### Research Planning Workflow\n\nTo plan a new research project:\n\n1. **Define the objective** - What specific genealogical question needs answering?\n2. **Formulate research questions** - Break into 3-7 specific, answerable questions\n3. **Identify individuals** - List primary subjects and associated family members\n4. **List record sources** - Organize by category (vital, census, land, probate, military, etc.)\n5. **Develop strategy** - Prioritize sources, plan FAN approach, work chronologically\n6. **Set timeline** - Break into phases with milestones\n**When executing steps 5-6 (Develop strategy & Set timeline):**\n- Provide links to research resources for the specific location\n- **Prioritize:** FamilySearch Wiki and LDSgenealogy.com above all other resources\n- Include links to relevant county/state pages\n- Identify record repositories and their online availability\n7. **Apply GPS framework** - Ensure plan addresses all five GPS elements\n8. **Define success criteria** - What constitutes adequate proof?\n9.  **Create next actions** - List 5-10 immediate concrete steps\n\nReference `references/research-strategies.md` for detailed methodologies.\n\n### Citation Generation Workflow\n\nTo create a proper citation:\n\n1. **Identify source type** - Census, vital record, land record, etc.\n2. **Determine access method** - Original, microfilm, digital image, database, transcription\n3. **Gather information:**\n   - Subject/individual name\n   - Record type and date\n   - Repository and collection\n   - Specific location (volume, page, entry)\n   - URL and access date (if online)\n4. **Select appropriate template** - See `references/citation-templates.md`\n5. **Build full citation** - Follow template for source type\n6. **Create short form** - Abbreviated version for subsequent references\n7. **Generate source list entry** - Formatted for bibliography\n8. **Assess source quality:**\n   - Original, derivative, or authored?\n   - Primary, secondary, or undetermined information?\n   - Direct, indirect, or negative evidence?\n9. **Extract key information** - Document what the source says\n10. **Link to research context** - How does this answer research questions?\n\n### Evidence Analysis Workflow\n\nTo analyze conflicting evidence:\n\n1. **Define the research question** - What specific fact is being analyzed?\n2. **Create evidence inventory** - List all relevant sources\n3. **Evaluate each source individually:**\n   - Apply source/information/evidence classification\n   - Analyze informant and reliability factors\n   - Assign reliability rating\n4. **Build comparison matrix** - Show what each source says\n5. **Identify conflicts** - Document specific discrepancies\n6. **Rank source reliability:**\n   - Information timing (primary > secondary)\n   - Source type (original > derivative)\n   - Informant quality (direct knowledge > hearsay)\n   - Consistency (corroborated > standalone)\n7. **Identify agreements** - Note corroborating evidence patterns\n8. **Apply conflict resolution framework:**\n   - Evaluate each side of conflict\n   - Consider explanations (error, informant mistake, both partially true)\n   - Apply evidence weight\n   - Determine preponderance\n9. **GPS compliance assessment** - Check all five elements\n10. **Write proof argument:**\n    - State conclusion\n    - Assign proof level\n    - Explain reasoning from evidence\n11. **Document gaps and recommendations** - What research remains?\n\nReference `references/evidence-evaluation.md` for detailed guidance.\n\n## Key Genealogical Concepts\n\n### Source Types\n- **Original Source** - First recording in original form (courthouse deed book, original certificate)\n- **Derivative Source** - Copy, transcription, or database entry\n- **Authored Work** - Compiled or analyzed work (published genealogy)\n\n### Information Types\n- **Primary Information** - Recorded at/near time of event by knowledgeable person\n- **Secondary Information** - Recorded later from memory or hearsay\n- **Important:** Original sources can contain secondary information! (e.g., death certificate shows birth date recorded 80 years later)\n\n### Evidence Types\n- **Direct Evidence** - Explicitly states the fact needed\n- **Indirect Evidence** - Implies fact when combined with other sources\n- **Negative Evidence** - Expected information that's absent\n\n### Proof Levels\n- **Proven** - Beyond reasonable doubt, no credible conflicts, GPS fully satisfied\n- **Probable** - Preponderance of evidence supports, minor conflicts resolved\n- **Possible** - Some evidence supports, significant gaps remain\n- **Unproven** - Insufficient evidence\n- **Disproven** - Evidence contradicts hypothesis\n\n## References\n\nFor detailed guidance on specific topics, load these reference files as needed:\n\n- `references/citation-templates.md` - Complete templates for 14+ source types\n- `references/evidence-evaluation.md` - Detailed frameworks for conflict resolution\n- `references/research-strategies.md` - Advanced research methodologies\n- `references/gps-guidelines.md` - Genealogical Proof Standard detailed requirements\n- `research-log-guidance.md` - Comprehensive guidance with examples and best practices\n- `research-plan-guidance.md` - Comprehensive guidance with examples and best practices\n- \n## Templates\n\nOutput templates are available in `assets/templates/`:\n\n- `research-plan-template.md` - Simplified research project planning (practical, day-to-day use)\n- `citation-template.md` - Citation library entry\n- `evidence-analysis-template.md` - Evidence analysis report\n- `research-log-template.md` - Research session documentation\n\n\n## Best Practices\n\n### Creating Citations\n- Cite what you actually consulted (if using database, cite both database and original)\n- Include enough detail for others to find the same record\n- Follow specific-to-general pattern (item  source  repository)\n- Distinguish between original records and database transcriptions\n\n### Analyzing Evidence\n- Quality matters more than quantity - one strong source beats three weak ones\n- Always consider informant knowledge and proximity to event\n- Look for independent corroboration, not derivative repetition\n- Acknowledge conflicts honestly rather than ignoring them\n\n### Building Proof Arguments\n- State conclusion clearly and precisely\n- Choose appropriate proof level for evidence strength\n- Explain reasoning transparently\n- Address conflicts explicitly and show resolution process\n- Acknowledge limitations and gaps\n\n### Research Strategy\n- Apply FAN principle - research family, associates, and neighbors\n- Document negative results - they're valuable research data\n- Work chronologically or geographically in systematic way\n- Consider collateral lines for clues about direct ancestors\n\n## Example Usage Patterns\n\n**User:** \"I found three census records that say my ancestor was born in Ohio, but his death certificate says Pennsylvania. How do I figure out which is right?\"\n\n**Response:** Load `references/evidence-evaluation.md`, apply conflict resolution framework. Evaluate each source for reliability (original vs. derivative, primary vs. secondary information, informant quality). Weight the three consistent earlier sources (John as likely informant) against single later source (unknown informant, secondary information). Analyze possible explanations. Determine preponderance of evidence. Create evidence analysis report documenting reasoning.\n\n**User:** \"Help me create a citation for a census record I found on Ancestry.\"\n\n**Response:** Load `references/citation-templates.md` for census citation template. Gather: year, county, state, page number, household, database name, URL, access date, NARA microfilm info. Build full citation following Evidence Explained format. Create short form and source list entry. Assess source quality (derivative source with digital image of original, secondary information about birth, direct evidence of residence). Document key information extracted.\n\n**User:** \"I want to research my great-grandfather but don't know where to start.\"\n\n**Response:** Guide through research planning workflow. Define objective (identify parents? determine birth location?). Formulate specific research questions. List known information and gaps. Identify potential sources (census, vital records, probate, military). Develop search strategy with priorities. Create timeline. Apply GPS framework. Generate research plan document with concrete next actions. Present the plan to the user. If the user then explicitly requests \"please execute this research plan,\" proceed with Step 3 (execution) using web_search and other tools systematically while documenting all activities.\n\n## Writing Style\n\nFollow genealogical professional standards:\n- Use precise, objective language\n- Cite sources consistently\n- Acknowledge uncertainty appropriately\n- Apply technical terms correctly (primary/secondary, original/derivative)\n- Structure proof arguments logically\n- Balance scholarly rigor with clarity\n\nAlways operate within the Genealogical Proof Standard framework, helping researchers build defensible, well-documented conclusions based on thorough evidence analysis.\n",
        "skills/genealogical-documentation/assets/templates/citation-template.md": "# Citation Library Entry\n\n**Date Added:** [DATE]\n**Citation ID:** [UNIQUE ID]\n**Researcher:** [YOUR NAME]\n\n---\n\n## Full Reference Note Citation\n\n[Complete Evidence Explained format citation]\n\n**Example:**\n1870 U.S. census, Coshocton County, Ohio, population schedule, p. 15 (stamped), dwelling 112, family 115, John Smith household; digital image, FamilySearch (https://www.familysearch.org/ark:/61903/3:1:S3HY-DRJ9-3YN : accessed 6 October 2025); citing NARA microfilm publication M593, roll 1215.\n\n---\n\n## Short Reference Note Citation\n\n[Abbreviated form for subsequent references]\n\n**Example:**\n1870 U.S. census, Coshocton Co., Ohio, p. 15, John Smith household.\n\n---\n\n## Source List Entry\n\n[Bibliography format]\n\n**Example:**\nU.S. Census. 1870. Coshocton County, Ohio. Population schedule. Digital images. FamilySearch. https://www.familysearch.org. NARA microfilm publication M593.\n\n---\n\n## Source Classification\n\n**Source Type:**\n- [ ] Original Record (first recording in original form)\n- [ ] Derivative Record (copy, transcription, database entry)\n- [ ] Authored Work (compiled/analyzed work)\n\n**Information Type:**\n- [ ] Primary Information (recorded at/near time of event)\n- [ ] Secondary Information (recorded later from memory)\n- [ ] Undetermined\n\n**Evidence Type:**\n- [ ] Direct Evidence (explicitly states the fact)\n- [ ] Indirect Evidence (implies the fact)\n- [ ] Negative Evidence (absence of expected information)\n\n**Reliability Assessment:**\n[High / Moderate / Low]\n\n**Reasoning:**\n[Explain reliability rating considering informant, timing, consistency, etc.]\n\n---\n\n## Source Details\n\n**Subject/Individual:** [Name of person this record concerns]\n\n**Record Type:** [Census, vital record, deed, will, etc.]\n\n**Record Date:** [When record was created]\n\n**Record Creator:** [Who created the record]\n\n### Repository Information\n\n**Repository:** [Full name of institution or database]\n\n**Collection:** [Specific collection or record group]\n\n**Location Within Collection:** [Volume, page, file number, etc.]\n\n**Call Number/ID:** [If applicable]\n\n### Access Information\n\n**Access Method:**\n- [ ] Online database\n- [ ] Digital image\n- [ ] Microfilm\n- [ ] Original record\n- [ ] Transcription\n- [ ] Published abstract\n\n**For Online Sources:**\n- **Database/Website:** [Full name]\n- **URL:** [Complete URL]\n- **Access Date:** [Date you viewed it]\n- **Image/Entry Number:** [If applicable]\n\n**For Microfilm:**\n- **Publication:** [NARA publication number, etc.]\n- **Roll/Frame:** [Specific location]\n\n**For Physical Records:**\n- **Location:** [Where physically held]\n- **Retrieval Information:** [How to request]\n\n---\n\n## Key Information Extracted\n\n**What This Source Documents:**\n[Summarize what facts this source establishes]\n\n**Specific Information Found:**\n- [Name(s)]\n- [Date(s)]\n- [Place(s)]\n- [Relationship(s)]\n- [Other relevant details]\n\n**Exact Wording (if important):**\n\"[Quote relevant passages if exact wording matters]\"\n\n---\n\n## Research Context\n\n**Research Question(s) Answered:**\n[Which questions does this source help answer?]\n\n**How It Relates to Research:**\n- [ ] Confirms known information\n- [ ] Contradicts other sources\n- [ ] Fills knowledge gap\n- [ ] Raises new questions\n\n**New Questions Raised:**\n[What new questions does this source generate?]\n\n---\n\n## Related Citations\n\n**Related Sources:**\n- [List other sources about same person/event]\n- [Sources that corroborate or conflict]\n- [Sources from same record series]\n\n**Relationship Notes:**\n[How sources relate - corroboration, conflict, context, etc.]\n\n---\n\n## Tags\n\n**Person:** [Tag by individual names]\n**Location:** [Tag by place names]\n**Record Type:** [Tag by source type]\n**Time Period:** [Tag by century/decade]\n**Topics:** [Tag by relevant themes]\n\n---\n\n## Usage Tracking\n\n**Used In:**\n- [ ] Research log dated: [DATE]\n- [ ] Evidence analysis dated: [DATE]\n- [ ] Research report dated: [DATE]\n- [ ] Shared with: [Names/dates]\n\n---\n\n## Notes\n\n[Additional observations, transcription challenges, quality issues, follow-up needed, etc.]\n\n---\n\n**Citation Added:** [DATE]\n**Last Updated:** [DATE]\n**Status:** [Active / Archived]\n",
        "skills/genealogical-documentation/assets/templates/evidence-analysis-template.md": "# Evidence Analysis Report\n\n**Date:** [DATE]\n**Researcher:** [YOUR NAME]\n**Analysis ID:** [UNIQUE ID]\n\n---\n\n## Research Question\n\n**Primary Question:**\n[What specific fact are you trying to determine?]\n\n**Subject Individual(s):**\n[Who is this analysis about?]\n\n**Specific Fact Under Investigation:**\n[Birth date? Birth location? Parentage? Marriage? Death?]\n\n---\n\n## Evidence Inventory\n\n### Sources Analyzed\n\n[List all sources that provide information about this fact]\n\n1. [Source 1 - Full citation]\n2. [Source 2 - Full citation]\n3. [Source 3 - Full citation]\n4. [Additional sources...]\n\n---\n\n## Individual Source Evaluations\n\n### Source 1: [Name/Description]\n\n**Citation:** [Full reference note citation]\n\n**Classification:**\n- Source Type: [Original / Derivative / Authored]\n- Information Type: [Primary / Secondary / Undetermined]\n- Evidence Type: [Direct / Indirect / Negative]\n\n**What It Says:**\n[Exact information provided about the fact]\n\n**Informant Analysis:**\n- Who provided information: [Name/role]\n- Relationship to subject: [Relationship]\n- Knowledge level: [Direct knowledge / Secondhand / Unknown]\n- Potential bias: [Any bias or motivation]\n\n**Reliability Factors:**\n- Timing: [How close to event?]\n- Record keeper quality: [Professional / Amateur / Unknown]\n- Condition: [Complete / Damaged / Clear / Faded]\n- Consistency: [Agrees with / Conflicts with other sources]\n\n**Reliability Rating:** [High / Moderate / Low]\n\n**Reasoning:** [Explain rating]\n\n---\n\n### Source 2: [Repeat for each source]\n\n[Complete same evaluation format for each source]\n\n---\n\n## Evidence Comparison Matrix\n\n| Source | What It Says | Info Type | Evidence Type | Reliability | Corroboration |\n|--------|--------------|-----------|---------------|-------------|---------------|\n| [Source 1] | [Statement] | [P/S/U] | [D/I/N] | [H/M/L] | [Pattern] |\n| [Source 2] | [Statement] | [P/S/U] | [D/I/N] | [H/M/L] | [Pattern] |\n| [Source 3] | [Statement] | [P/S/U] | [D/I/N] | [H/M/L] | [Pattern] |\n\n**Pattern Analysis:**\n[What patterns emerge? Agreement? Conflicts? Trends?]\n\n---\n\n## Conflicts and Discrepancies\n\n### Conflict #1: [Description]\n\n**Sources Involved:** [List conflicting sources]\n\n**Nature of Conflict:** [What specifically do they disagree about?]\n\n**Significance:** [Major conflict / Minor variation / Chronological issue]\n\n**Could Both Be True?** [Yes/No and explanation]\n\n---\n\n### Conflict #2: [Repeat for each conflict]\n\n[Document each conflict systematically]\n\n---\n\n## Source Reliability Rankings\n\n### Tier 1 - Highest Reliability\n[List sources with reasoning]\n\n### Tier 2 - Moderate-High Reliability\n[List sources with reasoning]\n\n### Tier 3 - Moderate Reliability\n[List sources with reasoning]\n\n### Tier 4 - Lower Reliability\n[List sources with reasoning]\n\n**Reasoning for Rankings:**\n[Explain how you weighted sources]\n\n---\n\n## Evidence Correlation\n\n### Points of Agreement\n\n[What do multiple sources agree on?]\n- [Fact 1]: Supported by [Source A, Source B, Source C]\n- [Fact 2]: Supported by [Source D, Source E]\n\n### Corroborating Evidence\n\n**Direct Corroboration:**\n[Sources that explicitly state same fact]\n\n**Indirect Corroboration:**\n[Different facts that support same conclusion]\n\n**Independent Evidence Streams:**\n[Are sources truly independent or derivative of each other?]\n\n---\n\n## Conflict Resolution\n\n### Conflict #1 Resolution\n\n**Evaluation of Each Side:**\n- [Source favoring Version A]: [Reliability assessment]\n- [Source favoring Version B]: [Reliability assessment]\n\n**Possible Explanations:**\n- [ ] Transcription/recording error\n- [ ] Informant error / memory failure\n- [ ] Both partially correct\n- [ ] Changed information over time\n- [ ] Different individuals\n- [ ] Contextual factors\n\n**Most Plausible Explanation:**\n[Which explanation makes most sense?]\n\n**Evidence Weight:**\n- Version A: [Percentage based on source quality]\n- Version B: [Percentage based on source quality]\n\n**Preponderance:** [Which version has preponderance?]\n\n**Resolution:** [State resolved conclusion or acknowledge if unresolved]\n\n---\n\n## Genealogical Proof Standard Assessment\n\n### Element 1: Reasonably Exhaustive Research\n\n**Sources Searched:**\n[List all source types consulted]\n\n**Assessment:**\n- [ ] Fully exhaustive\n- [ ] Substantially exhaustive\n- [ ] Needs more research\n\n**Gaps:** [What sources not yet searched?]\n\n---\n\n### Element 2: Complete and Accurate Citations\n\n**Citation Quality:**\n- [ ] All sources properly cited\n- [ ] Evidence Explained format used\n- [ ] Sufficient detail to relocate sources\n\n**Issues:** [Any citation problems?]\n\n---\n\n### Element 3: Analysis and Correlation\n\n**Analysis Performed:**\n- [ ] Each source analyzed individually\n- [ ] Sources systematically compared\n- [ ] Reliability assessed\n- [ ] Patterns identified\n\n**Correlation Methods:**\n[How sources were compared and connected]\n\n---\n\n### Element 4: Resolution of Conflicting Evidence\n\n**Conflicts Addressed:**\n- [ ] All conflicts identified\n- [ ] Systematic resolution attempted\n- [ ] Weighting explained\n- [ ] Preponderance demonstrated\n\n**Resolution Status:**\n[All resolved / Some unresolved / Explanation]\n\n---\n\n### Element 5: Soundly Reasoned Conclusion\n\n**Conclusion Readiness:**\n- [ ] Logic clear and transparent\n- [ ] Reasoning flows from evidence\n- [ ] Limitations acknowledged\n- [ ] Appropriate proof level\n\n---\n\n## Conclusion and Proof Argument\n\n### Conclusion Statement\n\n[Answer the research question directly and specifically]\n\n**Example:** \"John Smith was born in Ohio, circa 1820, most likely between 1818-1822. Birth location in Ohio is probable; specific county undetermined.\"\n\n---\n\n### Proof Level\n\n- [ ] **Proven** - Beyond reasonable doubt, no credible conflicts\n- [ ] **Probable** - Preponderance of evidence supports, minor conflicts resolved\n- [ ] **Possible** - Some evidence supports, significant gaps remain\n- [ ] **Unproven** - Insufficient evidence\n- [ ] **Disproven** - Evidence contradicts hypothesis\n\n---\n\n### Proof Argument\n\n[Write coherent narrative argument explaining reasoning]\n\n**Structure:**\n1. State conclusion\n2. Present strongest evidence\n3. Explain source evaluation\n4. Address conflicts and resolution\n5. Apply evidence weight / demonstrate preponderance\n6. Acknowledge limitations\n7. Justify proof level\n\n**Example:**\nThe preponderance of evidence supports John Smith's birth in Ohio around 1820. Three independent federal census enumerations (1850, 1860, 1870) consistently report his birthplace as Ohio, with ages progressing logically... [Continue with full proof argument as shown in examples]\n\n---\n\n## Evidence Gaps and Limitations\n\n### Missing Evidence\n\n[What expected sources weren't found?]\n[What information is absent from available sources?]\n\n---\n\n### Research Limitations\n\n[What prevented exhaustive research?]\n[What sources are inaccessible?]\n[What constraints affected research?]\n\n---\n\n### Unresolved Questions\n\n[What questions remain unanswered?]\n[What conflicts couldn't be resolved?]\n[What alternative explanations not ruled out?]\n\n---\n\n## Recommendations\n\n### Additional Research Needed\n\n1. [Specific source to search]\n2. [Specific repository to contact]\n3. [Specific record type to obtain]\n4. [Follow-up action needed]\n5. [Alternative strategy to try]\n\n---\n\n### Sources to Prioritize\n\n**High Priority:**\n[Sources most likely to resolve remaining questions]\n\n**Medium Priority:**\n[Supporting sources]\n\n**Low Priority:**\n[Supplemental sources]\n\n---\n\n## Summary\n\n**Overall Evidence Quality:** [Excellent / Good / Fair / Limited]\n\n**Strength of Proof:** [Very Strong / Strong / Moderate / Weak]\n\n**Confidence Level:** [High / Moderate / Low]\n\n**Risk Assessment:** [Low risk of being wrong / Moderate risk / High risk]\n\n---\n\n## Notes\n\n[Additional observations, ideas for future research, related questions]\n\n---\n\n**Analysis Created:** [DATE]\n**Last Updated:** [DATE]\n**Reviewed By:** [If peer-reviewed]\n**Status:** [Draft / Final / Revised]\n",
        "skills/genealogical-documentation/assets/templates/research-log-template.md": "# Family History Research Log\n\n**Date:** [DATE]\n**Researcher:** [YOUR NAME]\n**Session #:** [NUMBER]\n**Time Spent:** [HOURS/MINUTES]\n\n---\n\n## Research Session Overview\n\n**Research Goal:** [What you set out to accomplish this session]\n\n**Project/Ancestor:** [Which ancestor or research project does this relate to?]\n\n**Research Questions Addressed:**\n\n1. [Question 1]\n2. [Question 2]\n3. [Question 3]\n\n---\n\n## Sources Searched\n\n| Source | Location | Date Range | Search Terms | Results |\n|--------|----------|------------|--------------|---------|\n| [Database/Repository] | [Online/Physical] | [Years] | [Names, variations] | [Found/Not Found/Partial] |\n\n**Key Finding:** [Most important source result]\n\n---\n\n## What I Found\n\n### Finding 1: [Brief description]\n\n- **Source:** [Quick citation]\n- **Information:** [Key details - names, dates, locations, relationships]\n- **Significance:** [How this answers research questions]\n\n### Finding 2: [Repeat for each finding]\n\n### Sources with No Results\n\n- [Source name] - [Why expected information not found]\n\n---\n\n## Source Citations\n\n1. [Complete Evidence Explained format citation]\n2. [Citation]\n3. [Citation]\n\n---\n\n## Evidence Notes\n\n**Facts Learned:**\n\n- [Fact 1 with source]\n- [Fact 2 with source]\n\n**Conflicts or Discrepancies:**\n\n- [Description of conflict if applicable]\n\n---\n\n## Next Steps\n\n**Action Items:**\n\n- [ ] [Specific next action]\n- [ ] [Action]\n- [ ] [Action]\n\n**Priority:** High / Medium / Low\n\n**Additional Sources to Search:**\n\n- [ ] [Specific source/database]\n- [ ] [Source]\n\n---\n\n**Session Notes:** [Any observations, hypotheses, or patterns noticed]\n",
        "skills/genealogical-documentation/assets/templates/research-plan-template.md": "# Family History Research Plan\n\n**Date Created:** [DATE]\n**Researcher:** [YOUR NAME]\n**Project ID:** [OPTIONAL ID/NUMBER]\n\n---\n\n## Project Objective\n\n**Primary Research Question:**\n[State your main genealogical question clearly and specifically]\n\n---\n\n## Target Individuals\n\n**Primary Subject:**\n- **Name:** [Full name including variants]\n- **Birth:** [Date and location, or \"circa\" estimate]\n- **Death:** [Date and location]\n- **Marriage:** [Date, location, spouse name]\n- **Key Life Events:** [Brief summary]\n\n**Related Individuals:**\n- Parents:\n- Siblings:\n- Spouse(s):\n- Children:\n- Key associates/neighbors:\n\n---\n\n## Current Knowledge Summary\n\n**What We Know:**\n[List facts you've already established with sources]\n\n**What We Don't Know:**\n[List gaps in knowledge that research should address]\n\n**Sources Already Consulted:**\n[List sources you've already searched]\n\n---\n\n## Research Questions\n\n[List 3-7 specific, answerable research questions in priority order]\n\n1. [Primary question - most important]\n2. [Secondary question]\n3. [Supporting question]\n4. [Additional question]\n5. [Additional question]\n\n---\n\n## Time Period and Geographic Area\n\n**Time Period:** [Date range for research]\n\n**Geographic Area:** [Specific locations to research]\n\n---\n\n## Record Sources to Search\n\n**Vital Records:**\n\n\n**Census Records:**\n\n\n**Land and Property:**\n\n\n**Probate Records:**\n\n\n**Military Records:**\n\n\n**Immigration/Naturalization:**\n\n\n**Court Records:**\n\n\n**Newspapers and Publications:**\n\n\n**Other Sources:**\n\n\n---\n\n## Repositories and Archives\n\n**Online Resources:**\n\n\n**Physical Repositories:**\n\n\n**Contacts/Notes:**\n\n\n---\n\n## Research Strategy\n\n**Search Priority:**\n[List sources in order of priority]\n\n**Strategic Approaches:**\n[Note any FAN principle, cluster research, geographic patterns, or chronological approach]\n\n---\n\n## Next Actions\n\n**Immediate Steps (Next 1-2 Weeks):**\n1. [ ]\n2. [ ]\n3. [ ]\n4. [ ]\n5. [ ]\n\n---\n\n## Research Log\n\n| Date | Source Searched | Results | Notes | Next Steps |\n|------|----------------|---------|-------|-----------|\n|      |                |         |       |           |\n|      |                |         |       |           |\n|      |                |         |       |           |\n|      |                |         |       |           |\n|      |                |         |       |           |\n\n---\n\n## Evidence Analysis\n\n**Key Findings:**\n\n\n**Conflicts to Resolve:**\n\n\n**Reliability Assessment:**\n\n\n---\n\n## Notes and Observations\n\n[Add hypotheses, observations, ideas as research progresses]\n\n---\n\n## Revision History\n\n- **[DATE]** - Research plan created\n- **[DATE]** -\n- **[DATE]** -\n\n---\n\n**Last Updated:** [DATE]\n**Status:** [Active / Completed / On Hold]\n",
        "skills/genealogical-documentation/references/citation-templates.md": "# Genealogical Citation Templates\n\nComplete Evidence Explained citation templates for common genealogical source types.\n\n## Citation Structure Principles\n\n### Layered Citation Pattern\nCitations flow from specific to general:\n1. **Item level** (most specific) - The particular piece of information\n2. **Source level** - The record or publication containing the item\n3. **Repository level** (most general) - Where the source is held\n\n### Punctuation\n- **Commas** separate elements within layers\n- **Semicolons** separate major citation layers\n- **Colons** introduce lists or sub-elements\n- **Parentheses** enclose clarifications\n\n### Original vs. Derivative\n- Cite what you actually consulted\n- If using database, cite database AND original source\n- Use \"citing\" to introduce the original source for derivatives\n\n## CENSUS RECORDS\n\n### Federal Census - Digital Image\n\n**Full Citation Template:**\n```\n[Year] U.S. census, [County], [State], population schedule, p. [page number] ([stamped/printed]), dwelling [number], family [number], [head of household name] household; digital image, [Database Name] ([URL] : accessed [date]); citing [NARA microfilm publication], roll [number].\n```\n\n**Example:**\n```\n1870 U.S. census, Coshocton County, Ohio, population schedule, p. 15 (stamped), dwelling 112, family 115, John Smith household; digital image, FamilySearch (https://www.familysearch.org/ark:/61903/3:1:S3HY-DRJ9-3YN : accessed 6 October 2025); citing NARA microfilm publication M593, roll 1215.\n```\n\n**Short Form:**\n```\n1870 U.S. census, Coshocton Co., Ohio, p. 15, John Smith household.\n```\n\n**Source List Entry:**\n```\nU.S. Census. 1870. Coshocton County, Ohio. Population schedule. Digital images. FamilySearch. https://www.familysearch.org. NARA microfilm publication M593.\n```\n\n### State Census\n\n**Template:**\n```\n[Year] [State] census, [County], [enumeration district if applicable], [page], [name] household; [Repository], [Collection], [specific location].\n```\n\n## VITAL RECORDS\n\n### Birth Certificate - Original Record\n\n**Template:**\n```\n[Name], born [date], birth certificate no. [number]; [Issuing Office], [Location]; [Repository], [Collection], [volume/file]:[page].\n```\n\n**Example:**\n```\nJohn Smith, born 15 March 1850, birth certificate no. 1850-045; Office of Vital Records, Coshocton County, Ohio; Ohio Department of Health, Division of Vital Statistics, Birth Records 1850-1900, vol. 12, p. 34.\n```\n\n**Short Form:**\n```\nJohn Smith birth certificate (1850), Coshocton Co., Ohio.\n```\n\n### Marriage License/Certificate\n\n**Template:**\n```\n[Groom name] and [Bride name], married [date], marriage [license/certificate] no. [number]; [Issuing Office], [Location]; [access information if digital].\n```\n\n**Example:**\n```\nJohn Smith and Mary Johnson, married 10 June 1872, marriage license no. 456; Probate Court, Coshocton County, Ohio; digital image, Ancestry (https://www.ancestry.com/imageviewer/collections/8801/images/4234123 : accessed 6 October 2025).\n```\n\n**Short Form:**\n```\nJohn SmithMary Johnson marriage (1872), Coshocton Co., Ohio.\n```\n\n### Death Certificate\n\n**Template:**\n```\n[Name], died [date], death certificate no. [number]; [Issuing Office], [Location]; [Repository or Database with access information].\n```\n\n**Example:**\n```\nJohn Smith, died 12 April 1920, death certificate no. 1920-234; Bureau of Vital Statistics, Columbus, Ohio; Ohio Deaths, 1908-1932, database with images, FamilySearch (https://www.familysearch.org/ark:/61903/1:1:X8HB-N9S : accessed 6 October 2025).\n```\n\n## CHURCH RECORDS\n\n**Template:**\n```\n[Church name], [Location], \"[Record type],\" [volume/register name], [page], entry for [person]; [current repository if different], [collection].\n```\n\n**Example:**\n```\nFirst Presbyterian Church, Coshocton, Ohio, \"Baptismal Register,\" vol. 2 (1850-1875), p. 45, entry for Sarah Smith, baptized 20 July 1851; Ohio Church Records Collection, Coshocton County Library, Coshocton.\n```\n\n**Short Form:**\n```\nFirst Presbyterian Church (Coshocton, Ohio), Baptismal Register, 2:45, Sarah Smith.\n```\n\n## LAND RECORDS\n\n### Deeds\n\n**Template:**\n```\n[Grantor] to [Grantee], [document type], [date], [County] County, [State], [Record Office], [Book/Volume]:[Page]; [digital access if applicable].\n```\n\n**Example:**\n```\nWilliam Jones to John Smith, deed, 15 March 1865, Coshocton County, Ohio, Recorder of Deeds, Book 45:123; digital image, FamilySearch, Ohio, County Deeds, 1810-1890 (https://www.familysearch.org/ark:/61903/3:1:3Q9M-CS1Q : accessed 6 October 2025).\n```\n\n**Short Form:**\n```\nJones to Smith deed (1865), Coshocton Co., Ohio, Deeds 45:123.\n```\n\n### Tax Records\n\n**Template:**\n```\n[Year] [tax type], [County], [State], [taxpayer name]; [Repository], [Collection], [Volume/Roll]:[Page].\n```\n\n**Example:**\n```\n1875 real property tax list, Coshocton County, Ohio, John Smith; Coshocton County Auditor, Tax Duplicate Records, vol. 15:78.\n```\n\n## PROBATE RECORDS\n\n### Wills\n\n**Template:**\n```\n[Testator name], [location], will dated [date], proved [date]; [Court], [Location], [Record type], [Book/Volume]:[Page]; [digital access if applicable].\n```\n\n**Example:**\n```\nJohn Smith, Coshocton County, Ohio, will dated 5 January 1919, proved 20 April 1920; Probate Court, Coshocton County, Will Records, vol. 34:156-159; digital images, FamilySearch, Ohio, Wills and Probate Records, 1786-1998 (https://www.familysearch.org/ark:/61903/3:1:3Q9M-CSQX : accessed 6 October 2025).\n```\n\n**Short Form:**\n```\nJohn Smith will (1919), Coshocton Co., Ohio, Probate, Wills 34:156-159.\n```\n\n### Estate Files\n\n**Template:**\n```\nEstate of [Name], [date of death/probate], [Court], [Location], [case/file number]; [Repository], [Collection].\n```\n\n**Example:**\n```\nEstate of John Smith, probate opened April 1920, Probate Court, Coshocton County, Ohio, case no. 1920-145; Coshocton County Probate Court, Estate Files 1900-1925.\n```\n\n## MILITARY RECORDS\n\n### Service Records\n\n**Template:**\n```\n[Name], [Service/Regiment/Unit], [War/Service Period], [record type]; [Repository], [Record Group/Collection], [file number].\n```\n\n**Example:**\n```\nJohn Smith, Company B, 51st Ohio Volunteer Infantry, Civil War service record; National Archives, Records of the Adjutant General's Office, RG 94, Compiled Military Service Records, M552, roll 45.\n```\n\n**Short Form:**\n```\nJohn Smith, 51st Ohio Vol. Inf., service record, NARA RG 94.\n```\n\n### Pension Files\n\n**Template:**\n```\n[Name], [pension type], pension application [number], [Service/War]; [Repository], [Record Group], [application number].\n```\n\n**Example:**\n```\nJohn Smith, widow's pension application WC-123456, Civil War; National Archives, Civil War and Later Pension Files, RG 15, application 123456.\n```\n\n## IMMIGRATION RECORDS\n\n### Passenger Lists\n\n**Template:**\n```\n[Ship name], [departure port] to [arrival port], [arrival date], passenger list, [passenger name]; [Repository/Database], [Collection], [Roll/Image].\n```\n\n**Example:**\n```\nSS Britannic, Liverpool to New York, arrived 15 June 1905, passenger manifest, Johann Schmidt; digital image, Ancestry, New York Passenger Lists, 1820-1957 (https://www.ancestry.com/imageviewer/collections/7488/images/NYM237_233-0456 : accessed 6 October 2025); citing NARA microfilm T715, roll 823.\n```\n\n### Naturalizations\n\n**Template:**\n```\n[Name], [document type], [date], [Court], [Location], [document/case number]; [Repository], [Collection].\n```\n\n**Example:**\n```\nJohann Schmidt, petition for naturalization, 12 March 1912, U.S. District Court, Northern District of Ohio, Eastern Division, petition no. 8456; National Archives, Naturalization Records, RG 21.\n```\n\n## NEWSPAPERS\n\n**Template:**\n```\n\"[Article title/description],\" [Newspaper Name] ([City, State]), [full date], [page], col. [column]; [digital access if applicable].\n```\n\n**Example:**\n```\n\"Obituary for John Smith,\" Coshocton Tribune (Coshocton, Ohio), 15 April 1920, p. 3, col. 2; digital image, Newspapers.com (https://www.newspapers.com/image/123456789 : accessed 6 October 2025).\n```\n\n**Short Form:**\n```\nCoshocton Tribune, 15 April 1920, p. 3, John Smith obituary.\n```\n\n## CITY DIRECTORIES\n\n**Template:**\n```\n[Directory Title], [Year] ([Publisher location]: [Publisher], [year]), [page], entry for [name]; [digital access if applicable].\n```\n\n**Example:**\n```\nWilliams' Coshocton City Directory, 1895 (Cincinnati, Ohio: Williams Directory Co., 1895), 234, entry for John Smith; digital image, Ancestry, U.S. City Directories, 1822-1995 (https://www.ancestry.com/imageviewer/collections/2469/images/1234567 : accessed 6 October 2025).\n```\n\n## ONLINE DATABASES - Compiled Records\n\n**Template:**\n```\n\"[Entry/record title],\" database entry, [Database Name] ([URL] : accessed [date]), [entry details]; citing [original source if known].\n```\n\n**Example:**\n```\n\"John Smith,\" database entry, Ohio, U.S., Death Records, 1908-1932, 1938-2018, Ancestry (https://www.ancestry.com/discoveryui-content/view/12345:5014 : accessed 6 October 2025); citing certificate 1920-234, Ohio Department of Health, Columbus.\n```\n\n## PUBLISHED BOOKS\n\n### County Histories\n\n**Template:**\n```\n[Author/Title], [Book Title] ([Place]: [Publisher], [year]), [page(s)], [specific entry].\n```\n\n**Example:**\n```\nHistory of Coshocton County, Ohio (Chicago: A. Warner & Co., 1881), 456-457, biographical sketch of John Smith family.\n```\n\n**Short Form:**\n```\nHistory of Coshocton County (1881), 456-457.\n```\n\n### Genealogies\n\n**Template:**\n```\n[Author], [Title], [edition] ([Place]: [Publisher], [year]), [page(s)], [entry].\n```\n\n**Example:**\n```\nJane Smith, The Smith Family of Ohio, 2nd ed. (Columbus: Ohio Genealogical Society, 2005), 78-79, entry for John Smith (1850-1920).\n```\n\n## MANUSCRIPTS\n\n### Letters and Correspondence\n\n**Template:**\n```\n[Writer] to [Recipient], [date], [description]; [Repository], [Collection], [box/folder].\n```\n\n**Example:**\n```\nJohn Smith to William Smith, 15 March 1890, letter describing family history; Smith Family Papers, Ohio History Connection, Columbus, box 3, folder 12.\n```\n\n### Family Bibles\n\n**Template:**\n```\n[Bible title], [publication info]; family record section, [owner] family; [current holder].\n```\n\n**Example:**\n```\nThe Holy Bible, King James Version (Philadelphia: J.B. Lippincott, 1865); family record pages, Smith family; privately held by Mary Jones [address], Chicago, Illinois, 2025.\n```\n\n## PHOTOGRAPHS\n\n**Template:**\n```\n[Description], [date if known]; [current holder].\n```\n\n**Example:**\n```\nPhotograph of John and Mary Smith, circa 1875; privately held by Robert Smith [address], Chicago, Illinois, 2025.\n```\n\n## Citation Best Practices\n\n### Essential Elements\nEvery citation needs:\n- **WHO** - Person(s) the record is about\n- **WHAT** - Type of record and specific details\n- **WHERE** - Repository, collection, specific location\n- **WHEN** - Date of record AND access date (online)\n- **HOW** - Access method (digital, microfilm, original)\n\n### Common Mistakes to Avoid\n-  \"Found on Ancestry\" (too vague)\n-  Omitting access information for online sources\n-  Not distinguishing original from database entry\n-  Forgetting page numbers or identifiers\n-  Incomplete repository information\n\n### When in Doubt\n- Include more detail rather than less\n- Check Evidence Explained for your source type\n- Save URL and access date for online sources\n- Note image quality and completeness\n",
        "skills/genealogical-documentation/references/evidence-evaluation.md": "# Evidence Evaluation and Conflict Resolution\n\nDetailed frameworks for analyzing genealogical evidence and resolving conflicts between sources.\n\n## Three Key Classifications\n\n### 1. Source Classification (What you consulted)\n\n**Original Source**\n- First recording of information in original form\n- Created at or near time of event\n- Held in original repository\n- Examples: courthouse deed books, original certificates, church registers\n\n**Derivative Source**\n- Copy, extract, abstract, or transcription\n- Created later from original\n- Potential for transcription errors\n- Examples: published abstracts, database entries, microfilm copies\n\n**Authored Work**\n- Compiled or analyzed work with interpretation\n- May combine multiple sources\n- Examples: published genealogies, county histories\n\n**Why It Matters:**\n- Affects reliability assessment\n- Guides whether to seek original\n- Influences evidence weight\n- Determines citation format\n\n### 2. Information Classification (When recorded)\n\n**Primary Information**\n- Recorded at or very near time of event\n- Informant had direct knowledge\n- Less time for memory to fade\n- Examples: birth date on birth certificate, marriage date on license\n\n**Secondary Information**\n- Recorded well after event occurred\n- Often based on memory or hearsay\n- More opportunity for error\n- Examples: birth date on death certificate (70+ years later)\n\n**Undetermined Information**\n- Cannot determine when/how recorded\n- Unknown informant or circumstances\n\n**Critical Distinction:**\nAn ORIGINAL source can contain SECONDARY information!\n- Example: Original death certificate (original source) contains birth date (secondary information - reported decades later)\n\n### 3. Evidence Classification (How it answers your question)\n\n**Direct Evidence**\n- Explicitly states the fact needed\n- Answers question directly\n- Example: birth certificate stating birth date\n\n**Indirect Evidence**\n- Implies or suggests fact\n- Requires combining with other sources\n- Example: age on census implies birth year\n\n**Negative Evidence**\n- Absence of expected information\n- \"Silence\" that suggests something\n- Example: person missing from expected census\n\n## Source Reliability Ranking\n\n### Reliability Factors (highest to lowest)\n\n**Tier 1: Highest Reliability**\n- Original source\n- Primary information\n- Direct evidence\n- Informed informant with direct knowledge\n- Recorded contemporaneously\n- Official/professional record keeper\n- Corroborated by multiple independent sources\n\n**Tier 2: Moderate-High Reliability**\n- Original source with secondary information\n- Derivative source with primary information\n- Informed informant but later recording\n- Consistent with other sources\n- Professional transcription/database\n\n**Tier 3: Moderate Reliability**\n- Derivative source with secondary information\n- Uncertain informant quality\n- Indirect evidence only\n- Some inconsistencies with other sources\n- Amateur transcription\n\n**Tier 4: Low Reliability**\n- Compiled sources with unclear provenance\n- Secondary information from unknown informant\n- Contradicted by stronger sources\n- Known errors in record/database\n- Hearsay or family tradition unsupported by documents\n\n## Evidence Comparison Matrix\n\nWhen analyzing multiple sources, create a systematic comparison:\n\n| Source | What It Says | Info Type | Evidence Type | Reliability | Corroboration |\n|--------|--------------|-----------|---------------|-------------|---------------|\n| 1850 Census | Age 30, born Ohio | Secondary | Direct | Moderate | 3 sources agree |\n| 1860 Census | Age 40, born Ohio | Secondary | Direct | Moderate | 3 sources agree |\n| 1870 Census | Age 50, born Ohio | Secondary | Direct | Moderate | 3 sources agree |\n| Death Cert 1920 | Age 98, born PA | Secondary | Direct | Low-Mod | Conflicts |\n\n**Pattern Analysis:**\n- Three consistent sources vs. one conflicting\n- Earlier sources vs. later source\n- Likely informed informant (self) vs. unknown informant\n- Corroboration pattern strongly favors Ohio\n\n## Conflict Types and Resolutions\n\n### Type 1: Direct Contradictions\n\n**Nature:** Sources explicitly disagree\n**Example:** \"born Ohio\" vs. \"born Pennsylvania\"\n\n**Resolution Strategy:**\n1. Evaluate relative reliability of each source\n2. Consider informant quality and proximity\n3. Weight earlier/more reliable sources more heavily\n4. Look for corroboration patterns\n5. Apply preponderance of evidence\n\n### Type 2: Chronological Inconsistencies\n\n**Nature:** Dates don't align logically\n**Example:** Married before birth, died before event\n\n**Resolution Strategy:**\n1. Check for transcription errors\n2. Consider old style vs. new style dates\n3. Verify calendar systems (Julian vs. Gregorian)\n4. Look for documentation errors\n5. Evaluate if actually impossible or unlikely\n\n### Type 3: Minor Variations\n\n**Nature:** Small differences in ages, dates\n**Example:** Age differs by 1-2 years across censuses\n\n**Resolution Strategy:**\n1. Consider rounding (ages often rounded)\n2. Memory errors for secondary information\n3. Mathematical errors by enumerator\n4. Not necessarily significant conflict\n5. May accept range rather than exact date\n\n### Type 4: Name Variations\n\n**Nature:** Different spellings, nicknames\n**Example:** Johann/John, Elizabeth/Betsey, Smith/Smythe\n\n**Resolution Strategy:**\n1. Consider name evolution over time\n2. Research phonetic spellings\n3. Cultural naming practices\n4. Enumerator's interpretation\n5. Often not true conflict\n\n### Type 5: Informant Error\n\n**Nature:** Uninformed informant provided wrong info\n**Example:** Neighbor giving deceased's birth info\n\n**Resolution Strategy:**\n1. Assess informant's knowledge level\n2. Consider relationship to subject\n3. Evaluate proximity to event\n4. Weight informed sources over uninformed\n5. Recognize honest mistakes vs. deception\n\n### Type 6: Both Partially Correct\n\n**Nature:** Each source reflects different truth\n**Example:** Born in one place, raised in another (both claimed)\n\n**Resolution Strategy:**\n1. Look for evidence of movement\n2. Consider \"near border\" situations\n3. Research dual connections\n4. Evaluate if both could be true\n5. May require accepting complexity\n\n## Conflict Resolution Framework\n\n### Step 1: Evaluate Each Side\n\nFor each conflicting source, assess:\n- Reliability ranking (Tier 1-4)\n- Information type (primary/secondary)\n- Informant identity and knowledge\n- Timing of record creation\n- Consistency with other sources\n\n### Step 2: Explore Explanations\n\n**Possible Explanations for Conflicts:**\n\n**Transcription/Recording Errors**\n- Misheard by enumerator\n- Handwriting misread\n- Database transcription error\n- Abbreviation misinterpreted\n\n**Informant Error**\n- Memory failure\n- Didn't actually know\n- Deliberate deception (age fraud)\n- Misunderstood question\n\n**Both Partially Correct**\n- Born one place, raised another\n- Near border (claimed by both)\n- Moved shortly after event\n- Different specificity levels\n\n**Changed Information**\n- Name changes over time\n- Adoption/foster situations\n- Boundary changes\n- Different maiden/married names\n\n**Different Individuals**\n- Common name confusion\n- Multiple people same name\n- Wrong person in database\n\n**Contextual Factors**\n- Political pressure\n- Legal advantages\n- Social stigma avoidance\n- Cultural practices\n\n### Step 3: Apply Evidence Weight\n\n**Weighting Principles:**\n1. **Quality over Quantity** - One strong source beats three weak ones\n2. **Primary over Secondary** - Contemporary recording more reliable\n3. **Original over Derivative** - First recording preferred\n4. **Informed over Uninformed** - Knowledgeable informant crucial\n5. **Corroborated over Standalone** - Independent confirmation strengthens\n6. **Earlier over Later** - Closer to event generally better\n7. **Consistent over Contradicted** - Pattern matters\n\n**Calculate Preponderance:**\n- Assign weight percentages to sources\n- Strong, consistent sources = higher weight\n- Weak, contradictory sources = lower weight\n- Determine where balance tips (>50% threshold)\n\n**Example:**\n- 3 census records (Tier 2, consistent): 75% weight\n- 1 death certificate (Tier 3, conflicts): 25% weight\n- **Preponderance:** 75% favors census version\n\n### Step 4: Determine Resolution\n\n**Resolution Outcomes:**\n\n**Definitively Resolved**\n- Preponderance strongly favors one version\n- Conflicting source explained satisfactorily\n- Independent corroboration supports resolution\n- Can state conclusion with confidence\n\n**Probably Resolved**\n- Preponderance favors one version\n- Some uncertainty remains\n- Conflicting source not fully explained\n- State conclusion with caveat\n\n**Unresolved**\n- Sources equally balanced\n- No clear preponderance\n- Insufficient additional evidence\n- Acknowledge as unresolved conflict\n- List what additional research needed\n\n**Both Partially True**\n- Evidence supports complexity\n- Multiple truths documented\n- Explain nuanced situation\n- Acknowledge dual realities\n\n## GPS Element 4: Resolving Conflicting Evidence\n\n### Requirements for GPS Compliance\n\n**Identification:**\n- All conflicts must be identified\n- No conflicts ignored or glossed over\n- Contradictions explicitly stated\n\n**Analysis:**\n- Each conflict systematically analyzed\n- Source reliability assessed\n- Informant quality evaluated\n- Timing considered\n\n**Resolution or Acknowledgment:**\n- Conflicts resolved with reasoning explained\n- OR acknowledged as unresolvable\n- Weighting explicitly discussed\n- Preponderance demonstrated\n\n**Documentation:**\n- Resolution process documented\n- Reasoning transparent\n- Alternative explanations considered\n- Evidence weight shown\n\n## Preponderance of Evidence Analysis\n\n**Preponderance Definition:**\nThe conclusion supported by the strongest, most reliable, most consistent evidence is most likely correct.\n\n**Assessment Questions:**\n1. How many sources support each conclusion?\n2. What is the quality of sources on each side?\n3. Are corroborating sources independent?\n4. Which informants were most knowledgeable?\n5. Which information is primary vs. secondary?\n6. What is the pattern of consistency?\n\n**Preponderance Thresholds:**\n- **Strong Preponderance (>75%)** - Proven or Probable\n- **Moderate Preponderance (60-75%)** - Probable\n- **Slight Preponderance (50-60%)** - Possible\n- **No Preponderance (<50%)** - Unproven/Unresolved\n\n## Common Pitfalls to Avoid\n\n **Ignoring conflicts** - GPS requires addressing all conflicts\n **Cherry-picking** - Using only sources that support preferred conclusion\n **Quantity over quality** - Three weak sources don't outweigh one strong one\n **Dismissing later sources** - Later sources can be reliable if primary information\n **Assuming original = accurate** - Original sources can contain errors\n **Accepting tradition uncritically** - Family stories need documentary corroboration\n **Over-weighting one source** - Even strong sources need corroboration\n **Under-documenting** - Must show reasoning, not just assert conclusion\n\n## Proof Argument Construction\n\n### Structure of Proof Argument\n\n**1. State Conclusion**\n- Answer research question directly\n- Be specific (names, dates, places)\n- Indicate level of certainty\n\n**2. Present Strongest Evidence**\n- Lead with most reliable sources\n- Explain source quality\n- Show corroboration pattern\n\n**3. Address Conflicts**\n- Acknowledge contradictory sources\n- Explain reliability differences\n- Show resolution reasoning\n\n**4. Apply Evidence Weight**\n- Demonstrate preponderance\n- Show percentage distribution\n- Explain why weighted as done\n\n**5. Acknowledge Limitations**\n- Note evidence gaps\n- Recognize uncertainty\n- Suggest additional research\n\n**6. State Conclusion with Appropriate Proof Level**\n- Proven: Beyond reasonable doubt\n- Probable: Preponderance supports\n- Possible: Some evidence, gaps remain\n- Assign level honestly\n\n### Example Proof Argument\n\n**Research Question:** What was John Smith's birth location?\n\n**Conclusion:** John Smith was born in Ohio, circa 1820. [PROBABLE]\n\n**Argument:**\n\nThe preponderance of evidence supports John Smith's birth in Ohio around 1820. Three independent federal census enumerations (1850, 1860, 1870) consistently report his birthplace as Ohio, with ages progressing logically from 30 to 40 to 50, implying birth circa 1820. These census records are original documents, with information likely provided by John Smith himself as head of household, making him an informed source despite the secondary nature of birth information recorded decades after the event.\n\nThe consistency across three enumerations spanning 20 years, with the same likely informant, significantly strengthens this evidence. Independent corroboration exists in the Ohio birth locations of all known children (1846-1860) and John's documented Ohio residence by 1845 (marriage record).\n\nOne conflicting source exists: John's 1920 death certificate reports his birthplace as Pennsylvania. However, this source carries less weight because: (1) the informant is unknown and almost certainly not a family member; (2) the information was recorded 98+ years after John's birth; (3) the informant likely had no direct knowledge; (4) it conflicts with three earlier, more reliable sources. The most plausible explanation is informant error.\n\nEvidence weight distribution: Census records (75%) vs. death certificate (25%). The preponderance strongly favors Ohio birth.\n\nWhile evidence strongly supports Ohio birth, the specific county cannot be determined from current sources. Pennsylvania cannot be entirely ruled out without positive birth evidence or definitive proof of no Pennsylvania connection.\n\nThe \"probable\" rather than \"proven\" designation reflects: (1) secondary nature of all birth information; (2) one conflicting source; (3) lack of primary birth evidence. Additional research in Ohio birth/baptism records 1818-1822 would strengthen to \"proven.\"\n\n**This argument demonstrates:**\n- Clear conclusion with proof level\n- Strongest evidence presented first\n- Conflicts addressed honestly\n- Weighting explained\n- Limitations acknowledged\n- Reasoning transparent and logical\n",
        "skills/genealogical-documentation/references/gps-guidelines.md": "# Genealogical Proof Standard (GPS) Guidelines\n\nDetailed requirements for the five-element GPS framework used by professional genealogists.\n\n## Overview\n\nThe Genealogical Proof Standard is the professional benchmark for evaluating genealogical conclusions. It consists of five interdependent elements that must ALL be satisfied for a conclusion to meet GPS.\n\n**The Five Elements:**\n1. Reasonably exhaustive research\n2. Complete and accurate source citations\n3. Analysis and correlation of collected information\n4. Resolution of conflicting evidence\n5. Soundly reasoned, coherently written conclusion\n\n**GPS Compliance Levels:**\n-  **Fully GPS-Compliant** - All five elements satisfied\n-  **Substantially Compliant** - Most elements met, minor gaps\n-  **Non-Compliant** - Significant deficiencies in one or more elements\n\n## Element 1: Reasonably Exhaustive Research\n\n### What It Means\n\nResearch should be thorough enough that:\n- Most relevant sources have been searched\n- Unlikely that major contradictory evidence remains undiscovered\n- Additional research would probably only confirm existing conclusion\n- All \"obvious\" sources for the time/place/question have been consulted\n\n**NOT:**\n- Every conceivable source in existence\n- Sources unlikely to contain relevant information\n- Sources physically/practically inaccessible\n\n### Assessment Questions\n\n**Have you searched:**\n-  All standard sources for the time period and location?\n-  Multiple source types (not just one category)?\n-  Both direct and indirect sources?\n-  Original records where available?\n-  Associated persons (FAN principle)?\n-  Multiple repositories and databases?\n\n**Can you say:**\n-  \"I searched all accessible census records 1850-1900\"\n-  \"I searched both original courthouse records and online databases\"\n-  \"I consulted vital records, land records, and probate records\"\n-  \"I searched for siblings, neighbors, and associates\"\n\n**Red Flags for Insufficient Research:**\n-  Only searched online databases, never accessed original records\n-  Only consulted one source type (e.g., only census)\n-  Haven't searched negative sources (where person should be but isn't)\n-  Haven't researched collateral lines or associates\n-  Major record groups not yet searched\n\n### Source Categories to Consider\n\n**Essential Sources (time/place dependent):**\n- Census records (federal, state, territorial)\n- Vital records (birth, marriage, death)\n- Land records (deeds, tax rolls)\n- Probate records (wills, estates)\n- Church records (baptism, marriage, burial)\n\n**Supporting Sources:**\n- Military records\n- Immigration/naturalization\n- Newspapers\n- City directories\n- Court records\n- School records\n- Cemetery records\n\n**Contextual Sources:**\n- Published county histories\n- Maps and gazetteers\n- Record availability guides\n- Background histories\n\n**FAN Principle:**\n- Family members (parents, siblings, spouses, children)\n- Associates (business partners, witnesses)\n- Neighbors (clustered in records)\n\n### Documentation of Exhaustive Research\n\nGPS requires demonstrating research was thorough:\n\n**Document:**\n- Sources searched (positive and negative results)\n- Date ranges searched\n- Repositories consulted\n- Search strategies used\n- Why certain sources weren't searched (unavailable, destroyed, irrelevant)\n\n**Research Log Components:**\n- All databases and websites searched\n- Physical repositories visited\n- Correspondence sent\n- Microfilm reviewed\n- Books and publications consulted\n- Negative results explicitly noted\n\n### When Is Research \"Exhaustive Enough\"?\n\n**Factors to Consider:**\n- Complexity of research question\n- Availability of records\n- Consistency of evidence found\n- Presence/absence of conflicts\n- Time period and location challenges\n\n**For \"Proven\" Conclusion:**\n- Must be truly exhaustive\n- All major sources searched\n- No major gaps\n- Pattern of consistent evidence\n\n**For \"Probable\" Conclusion:**\n- Substantially thorough\n- Key sources searched\n- Some gaps acceptable\n- Preponderance of evidence established\n\n**For \"Possible\" Hypothesis:**\n- Preliminary research done\n- Some sources searched\n- Significant gaps remain\n- Needs more research before conclusion\n\n## Element 2: Complete and Accurate Source Citations\n\n### What It Means\n\nEvery source used must be:\n- Completely cited following accepted standards\n- Accurately cited (no errors in transcription)\n- Sufficient for others to locate the same source\n- Distinguished (original vs. derivative noted)\n\n### Citation Requirements\n\n**Complete Citations Include:**\n- **Author/Creator** (if applicable)\n- **Title/Description** of source\n- **Publication/Creation information**\n- **Repository/Location**\n- **Specific item location** (page, volume, entry, etc.)\n- **Access information** (URL, microfilm, call number)\n- **Access date** (for online sources)\n\n**Evidence Explained Standard:**\nThree citation forms needed:\n1. **Full Reference Note** - Complete first citation\n2. **Short Reference Note** - Abbreviated subsequent citations\n3. **Source List Entry** - Bibliography format\n\n**Critical Distinctions:**\n- Original source vs. derivative vs. database\n- Digital image vs. transcription vs. index\n- What you actually consulted (not what you wish you consulted)\n\n### Common Citation Errors\n\n \"Found on Ancestry\" - Too vague, not enough detail\n \"1870 Census\" - Missing county, state, page, household\n No URL or access date for online sources\n Not distinguishing database transcription from original image\n Incomplete repository information\n No page numbers or specific location within source\n\n### GPS Compliance for Citations\n\n**Fully Compliant:**\n- Every source has complete citation\n- Citations follow recognized standard\n- Sufficient detail to relocate sources\n- Original/derivative distinctions clear\n\n**Non-Compliant:**\n- Missing citations for sources used\n- Vague or incomplete citations\n- No standard format followed\n- Cannot relocate sources using citations\n\n## Element 3: Analysis and Correlation\n\n### What It Means\n\nEvidence must be:\n- Analyzed individually (each source evaluated for reliability)\n- Correlated systematically (sources compared and connected)\n- Evaluated collectively (pattern assessment)\n- Tested against alternative explanations\n\n**NOT just:**\n- Listing sources found\n- Transcribing records\n- Accumulating information without analysis\n\n### Individual Source Analysis\n\nFor each source, evaluate:\n\n**Source Classification:**\n- Original, derivative, or authored?\n\n**Information Classification:**\n- Primary, secondary, or undetermined?\n\n**Evidence Classification:**\n- Direct, indirect, or negative?\n\n**Reliability Factors:**\n- Who created the record?\n- When was it created relative to event?\n- Who provided information?\n- What was their knowledge/relationship?\n- Any known biases or errors?\n- Condition and completeness?\n\n### Correlation Process\n\n**Systematic Comparison:**\n- What does each source say about the fact?\n- Where do sources agree?\n- Where do they conflict?\n- Are corroborating sources truly independent?\n\n**Pattern Recognition:**\n- Consistent information across multiple sources\n- Progressive timeline of information\n- Contextual corroboration (related facts fit together)\n- Discrepancies that need explanation\n\n**Testing Hypothesis:**\n- Does evidence support conclusion?\n- What evidence would contradict it?\n- Are alternative explanations possible?\n- What evidence is missing?\n\n### GPS Compliance for Analysis\n\n**Fully Compliant:**\n- Each source analyzed individually\n- Sources systematically compared\n- Reliability assessed explicitly\n- Corroboration demonstrated\n- Patterns identified\n- Alternative explanations considered\n\n**Non-Compliant:**\n- Sources listed without analysis\n- No reliability assessment\n- Conflicting sources not identified\n- No systematic comparison\n- Accepts information uncritically\n\n## Element 4: Resolution of Conflicting Evidence\n\n### What It Means\n\nWhen sources disagree:\n- All conflicts must be identified\n- Each conflict must be analyzed\n- Resolution must be attempted\n- If unresolvable, must be acknowledged\n\n**Resolution shows:**\n- Why some sources weigh more heavily\n- What explanations considered\n- How preponderance determined\n- What conclusion supported\n\n### Conflict Resolution Requirements\n\n**Identification:**\n- Every conflict explicitly stated\n- Nature of discrepancy described\n- Sources involved identified\n\n**Evaluation:**\n- Reliability of conflicting sources assessed\n- Information quality compared\n- Informant credibility evaluated\n- Timing of recording considered\n\n**Explanation:**\n- Possible reasons for conflict explored\n- Most plausible explanation identified\n- Evidence weight explicitly assigned\n\n**Resolution or Acknowledgment:**\n- Conflict resolved with reasoning shown\n- OR conflict acknowledged as unresolvable\n- Preponderance of evidence demonstrated\n- Impact on conclusion explained\n\n### GPS Compliance for Conflicts\n\n**Fully Compliant:**\n- All conflicts identified and addressed\n- Systematic evaluation performed\n- Weighting explicitly explained\n- Resolution logical and documented\n- OR acknowledged as unresolved with impact noted\n\n**Non-Compliant:**\n- Conflicts ignored or glossed over\n- Conflicting sources not evaluated\n- No explanation for weighting\n- Cherry-picking evidence\n- Dismissing conflicts without analysis\n\n## Element 5: Soundly Reasoned, Coherently Written Conclusion\n\n### What It Means\n\nThe conclusion must:\n- Answer the research question directly\n- Flow logically from the evidence\n- Be clearly and coherently written\n- Show transparent reasoning\n- Acknowledge limitations\n- Assign appropriate proof level\n\n### Components of Sound Conclusion\n\n**Clear Statement:**\n- Directly answers research question\n- Specific (names, dates, places)\n- Indicates level of certainty\n- Appropriately qualified\n\n**Logical Reasoning:**\n- Proceeds from evidence to conclusion\n- Steps explained clearly\n- No logical leaps\n- Alternative explanations addressed\n\n**Coherent Writing:**\n- Well-organized structure\n- Clear, professional language\n- Technical terms used correctly\n- Free of ambiguity\n\n**Transparent Process:**\n- Reasoning visible and explicit\n- Assumptions stated\n- Limitations acknowledged\n- Evidence weight shown\n\n**Appropriate Proof Level:**\n- Proven: Beyond reasonable doubt\n- Probable: Preponderance supports\n- Possible: Some evidence, gaps remain\n- Honestly reflects evidence strength\n\n### GPS Compliance for Conclusions\n\n**Fully Compliant:**\n- Conclusion directly addresses question\n- Logic is sound and explicit\n- Writing is clear and coherent\n- Reasoning is transparent\n- Proof level appropriate to evidence\n- Limitations acknowledged\n\n**Non-Compliant:**\n- Vague or unclear conclusion\n- Logical fallacies\n- Reasoning not explained\n- Over or understates certainty\n- Ignores evidence limitations\n- Poor organization or writing\n\n## GPS Self-Assessment Checklist\n\n### Element 1: Reasonably Exhaustive Research\n- [ ] All standard sources for time/place/question searched\n- [ ] Multiple source types consulted\n- [ ] Both direct and indirect sources examined\n- [ ] FAN principle applied (family, associates, neighbors)\n- [ ] Negative results documented\n- [ ] Research log maintained\n\n### Element 2: Complete Citations\n- [ ] Every source has complete citation\n- [ ] Citations follow Evidence Explained or other standard\n- [ ] Sufficient detail to relocate sources\n- [ ] Original/derivative distinctions clear\n- [ ] Online sources include URL and access date\n- [ ] Three citation forms provided (full, short, source list)\n\n### Element 3: Analysis and Correlation\n- [ ] Each source analyzed individually\n- [ ] Source classification determined (original/derivative)\n- [ ] Information type assessed (primary/secondary)\n- [ ] Evidence type identified (direct/indirect/negative)\n- [ ] Reliability factors evaluated\n- [ ] Sources systematically compared\n- [ ] Corroboration demonstrated\n- [ ] Patterns identified\n\n### Element 4: Conflict Resolution\n- [ ] All conflicts identified\n- [ ] Each conflict systematically analyzed\n- [ ] Source reliability compared\n- [ ] Possible explanations explored\n- [ ] Evidence weighting explicit\n- [ ] Preponderance demonstrated\n- [ ] Resolution documented OR acknowledged as unresolved\n\n### Element 5: Sound Conclusion\n- [ ] Conclusion directly answers research question\n- [ ] Logic flows from evidence\n- [ ] Reasoning is transparent\n- [ ] Writing is clear and coherent\n- [ ] Limitations acknowledged\n- [ ] Appropriate proof level assigned\n- [ ] Alternative explanations addressed\n\n## Common GPS Compliance Issues\n\n### Why Conclusions Fail GPS\n\n**Insufficient Research:**\n- Only searched online databases\n- Single source type only\n- Obvious sources not consulted\n- Negative results not documented\n\n**Poor Citations:**\n- Vague or incomplete\n- No recognized standard\n- Can't relocate sources\n- Original/derivative confused\n\n**Weak Analysis:**\n- Sources accepted uncritically\n- No reliability assessment\n- No systematic comparison\n- Corroboration not demonstrated\n\n**Unresolved Conflicts:**\n- Conflicts ignored\n- Cherry-picking evidence\n- No weighting explanation\n- Contradictions glossed over\n\n**Weak Conclusions:**\n- Doesn't answer question\n- Logic unclear\n- Over/understates certainty\n- Ignores limitations\n\n## GPS in Practice\n\n### How Professional Genealogists Use GPS\n\n**During Research:**\n- Guides source selection (exhaustive research)\n- Ensures proper documentation (citations)\n- Prompts analysis as you work\n- Identifies conflicts early\n\n**After Research:**\n- Framework for organizing findings\n- Structure for proof argument\n- Standard for peer review\n- Benchmark for publication\n\n**For Difficult Cases:**\n- Systematic approach to complexity\n- Forces addressing conflicts\n- Prevents premature conclusions\n- Ensures thorough documentation\n\n### GPS and Proof Levels\n\n**Proven (GPS fully satisfied):**\n- All five elements strongly met\n- No significant gaps or weaknesses\n- Beyond reasonable doubt\n- Publication-ready\n\n**Probable (GPS substantially satisfied):**\n- Elements mostly met\n- Minor gaps or limitations\n- Preponderance of evidence\n- Needs some strengthening\n\n**Possible (GPS partially satisfied):**\n- Significant gaps in elements\n- Preliminary research\n- Some evidence, not conclusive\n- Needs substantial work\n\n## Resources\n\n**GPS Standard:**\nBoard for Certification of Genealogists (BCG)\nhttps://www.bcgcertification.org/resources/standard.html\n\n**Citation Standard:**\nElizabeth Shown Mills, *Evidence Explained*\nhttps://www.evidenceexplained.com/\n\n**Professional Standards:**\nNational Genealogical Society (NGS)\nhttps://www.ngsgenealogy.org/\n",
        "skills/genealogical-documentation/references/research-log-guidance.md": "# Research Log Guidance & Best Practices\n\nThis document provides comprehensive guidance for creating detailed research logs following professional genealogical standards. Use this alongside the simplified `research-log-template.md` for your daily research work.\n\n## Overview\n\nA research log is a systematic record of your research activities, sources searched, findings discovered, and evidence analyzed. It serves multiple purposes:\n- **Documentation**: Proves your research was reasonably exhaustive\n- **Accountability**: Supports your conclusions with cited evidence\n- **Continuity**: Allows you to pick up where you left off\n- **Quality Control**: Helps identify gaps and conflicts\n- **Professional Standard**: Meets Genealogical Proof Standard (GPS) requirements\n\n---\n\n## Session Metadata\n\n### Essential Information\n\n**Date, Researcher, Session Number:**\n- Record the date you conducted research\n- Include your name (useful if shared with collaborators)\n- Number sessions sequentially for this research project\n- Include time spent for tracking research effort\n\n**Research Context:**\n- Clearly state your research goal (one session, one goal)\n- Identify the specific ancestor or project\n- List the research questions you're trying to answer\n- Reference your overall research plan if you have one\n\n---\n\n## Sources Searched - Comprehensive Documentation\n\n### Complete Source Table\n\nTrack ALL sources searched, including those with negative results:\n\n| Source | Location/Access | Date Range Searched | Search Strategy | Results |\n|--------|-----------------|-------------------|-----------------|---------|\n| [Database name] | [Online/Physical] | [Years] | [Exact names, variations, filters] | [Found/Not Found/Partial] |\n\n### Search Strategies - Document Your Method\n\nFor each major source searched, document:\n\n**Search Approach:**\n- Did you search the exact name?\n- What name variations did you try? (maiden names, alternate spellings, nicknames)\n- What date ranges did you search? (broader than expected? narrower?)\n- What geographic filters/limits did you use?\n- Did you browse manually or use keyword search?\n- What specific fields did you search?\n\n**Challenges Encountered:**\n- Database interface difficulties\n- Name variations creating confusion\n- Records not indexed as expected\n- Access limitations\n- Time constraints\n\n**Time Investment:**\n- How long did this source take?\n- Was the time spent proportional to the importance of the source?\n\n### Negative Results Are Evidence\n\n**Sources Expected to Contain Information But Didn't:**\n1. [Source name]\n   - **Expected:** What information you thought would be there\n   - **Search Strategy:** How thoroughly you searched (all variations? full date range?)\n   - **Possible Reasons:** Why information might be absent (destroyed? indexed differently? person not in database?)\n   - **Implications:** What this tells you (moved away? changed name? didn't exist?)\n\nNegative results are just as important as positive ones. They prove your research was reasonably exhaustive.\n\n---\n\n## Findings & Discoveries - Systematic Documentation\n\n### Organize by Source\n\nFor each finding, document the complete context:\n\n**Finding: [Description]**\n- **Source:** [Full Evidence Explained citation]\n- **Key Information:**\n  - Names (exactly as written in document)\n  - Dates (birth, marriage, death, or event dates)\n  - Locations (jurisdictions, specific places)\n  - Relationships (parent-child, spouse, sibling)\n  - Other details (occupation, residency, status)\n- **Exact Wording:** \"[Quote from document if relevant]\"\n- **Significance:** How this answers your research questions\n- **New Questions Raised:** What does this make you wonder about?\n- **Source Quality:** Is this original or derivative? How reliable?\n\n### Unexpected Discoveries\n\nDocument anything surprising or unanticipated:\n- Side-line discoveries about other ancestors\n- Information contradicting previous assumptions\n- Clues suggesting different research directions\n- Contextual information that changes interpretation\n\n---\n\n## Source Citations - Complete Documentation\n\n### Evidence Explained Format\n\nProvide complete, formal citations for all sources (positive and negative results):\n\n1. **Fully cited as per Evidence Explained methodology**\n2. **Include retrieval information** (database, search terms, date accessed for online sources)\n3. **Cite even negative results** (source you searched thoroughly but found no information)\n4. **Complete citations prove your research** and allow others to verify\n\n### Citation Elements\n\nYour citations should include:\n- **Author/Creator** (if applicable)\n- **Title** (of record, database, book, etc.)\n- **Publication/Access Information**\n- **Specific page/entry/image reference**\n- **Date accessed** (for online sources)\n- **Search terms used** (if searching database)\n\n---\n\n## Evidence Analysis - Critical Evaluation\n\n### Distinguish Facts from Interpretation\n\n**Facts Learned:**\n- Information directly stated in documents\n- Include source for each fact\n- Record exactly as stated (even if you think it's wrong)\n\n**Interpretation:**\n- Your analysis of what the facts mean\n- Connections you've made\n- Conclusions you're drawing\n- Keep separate from factual information\n\n### Evidence Type Classification\n\n**Direct Evidence:**\n- Explicitly states the fact being proven\n- Example: \"John Smith, born 15 May 1845, son of Robert and Mary Smith\"\n\n**Indirect Evidence:**\n- Suggests a relationship but doesn't state it explicitly\n- Example: John listed as head of household in 1870 census with children born 1850-1865\n- Requires analysis to interpret\n\n**Negative Evidence:**\n- Expected records not found\n- Proves exhaustive search but something is missing\n- Requires interpretation (moved away? not recorded? false assumption?)\n\n### Reliability Assessment\n\n**Evaluate each source for reliability:**\n\n**High Reliability:**\n- Original records (not copies)\n- Made close to the event\n- By someone with knowledge of the facts\n- No obvious bias or errors\n\n**Moderate Reliability:**\n- Derivative records or secondary sources\n- Made some time after the event\n- Based on available information but not original\n\n**Low Reliability:**\n- Family folklore or oral tradition\n- Secondary sources far removed from events\n- Known errors or inconsistencies\n- Written long after the event\n\n### Identify Conflicts & Discrepancies\n\n**Conflicting Information:**\n\n1. [Description of conflict]\n   - Source A says: [Statement and citation]\n   - Source B says: [Statement and citation]\n   - Nature of conflict: [Date discrepancy? Location? Name? Relationship?]\n   - Preliminary assessment: [Which seems more reliable and why?]\n   - Further research needed: [What would help resolve this?]\n\nDocument conflicts honestly. They don't weaken your researchresolving them strengthens it.\n\n---\n\n## Document & Image Management\n\n### File Organization\n\n**If obtaining documents or images:**\n\n| Filename | Content Description | Source | Quality | Date Obtained |\n|----------|-------------------|--------|---------|---------------|\n| [filename.jpg] | [1870 census, John Smith household] | [FamilySearch] | [Good/Fair/Poor] | [DATE] |\n\n**File Naming Convention:**\n- Use consistent, descriptive names\n- Include ancestor name, date, source, type\n- Example: `Smith_John_1870Census_Coshocton_OH.jpg`\n\n**Storage Location:**\n- Primary storage location (hard drive, cloud)\n- Backup location\n- Organization method (by ancestor? by source type? by date?)\n- Naming conventions used\n\n### Document Transcription\n\nIf you transcribe records:\n\n**Source:** [Full citation]\n\n**Transcription:**\n[Exact text of document, preserving original spelling, capitalization, punctuation]\n\n[unclear] = unclear text\n[damaged] = damaged/illegible portion\n[sic] = note obvious errors\n\n---\n\n## Research Quality Assurance\n\n### Self-Assessment Checklist\n\nBefore finishing your research session:\n\n- [ ] All sources searched are documented (including those with no results)\n- [ ] Complete citations are recorded for all sources\n- [ ] Findings are distinguished from interpretation\n- [ ] Images/documents are properly filed and referenced\n- [ ] Source citations match actual sources consulted\n- [ ] Next steps are specific and actionable\n- [ ] Research contributes to answering stated research questions\n- [ ] Both positive and negative results are documented\n- [ ] Conflicts identified and assessed for reliability\n- [ ] Evidence quality and type assessed\n\n### GPS Compliance Check\n\nYour research log should support the Genealogical Proof Standard:\n\n- **Reasonably Exhaustive Research:** Did you search all available sources?\n  - Your sources searched table proves this\n\n- **Complete and Accurate Citations:** Are all sources fully cited?\n  - Your source citations section proves this\n\n- **Analysis & Correlation:** Did you analyze what you found?\n  - Your evidence notes and findings section prove this\n\n- **Conflict Resolution:** Did you identify and address discrepancies?\n  - Your evidence analysis section proves this\n\n- **Soundly Reasoned Conclusions:** Can someone follow your logic?\n  - Your next steps and session notes prove this\n\n---\n\n## Advanced Topics\n\n### Hypotheses and Patterns\n\nDocument:\n- **Hypotheses you're testing** (this person might be related to that family because...)\n- **Patterns noticed** (all children born in same county suggests resident family)\n- **Contextual observations** (immigration patterns, land availability, family networks)\n- **Connections emerging** (how this fits into larger family picture)\n\n### Repository Interactions\n\nIf you visit or contact repositories:\n\n**[Repository Name]:**\n- Address and hours\n- Contact person/method\n- Access policies (restrictions? fees?)\n- Records available\n- What you requested/obtained\n- Any limitations encountered\n- Future contact information\n\n### Research Expenses\n\nTrack costs (useful for professional genealogy):\n\n| Item | Cost | Date | Notes |\n|------|------|------|-------|\n| Record copy from county courthouse | [$] | [DATE] | [John Smith death cert, 1903] |\n| Database subscription | [$] | [DATE] | [One month FamilySearch] |\n\n**Total session cost:** [$]\n\n---\n\n## Session Evaluation - Reflective Practice\n\nAfter completing your research session:\n\n**Progress Made:** Excellent / Good / Moderate / Limited\n\n**Goals Achieved:** Which specific goals from the start of your session were accomplished?\n\n**Challenges:** What made research difficult?\n- Difficult sources? Limited access? Conflicting information?\n\n**Lessons Learned:** What worked well? What didn't?\n- What search strategies were most effective?\n- What would you do differently next time?\n- Any techniques discovered?\n\n**Overall Assessment:** Brief summary of your session's productivity\n\n---\n\n## Continuity & Navigation\n\n### Linking Sessions Together\n\nConnect related research:\n- **Research Plan:** Link to your overall research plan\n- **Previous Log:** Link to previous session (showed progress)\n- **Next Log:** Link to next session (plans built on this work)\n- **Related Analysis:** Link to formal evidence analysis documents\n- **Related Templates:** Link to related research plan or citation library\n\nThis helps create a complete research history and shows progression of your work.\n\n---\n\n## Professional Standards\n\nYour research log should meet standards established by:\n- **Genealogical Proof Standard (GPS)** - reasonably exhaustive research documented\n- **Evidence Explained** - complete and accurate citations\n- **Board for Certification of Genealogists (BCG)** - professional methodology\n- **National Genealogical Society (NGS)** - research standards\n\nA well-documented research log allows:\n- **Peer review** of your work\n- **Reproduction** of your research\n- **Verification** of your conclusions\n- **Professional publication** if desired\n\n---\n\n## Getting Started\n\n1. **Use the practical template** (`research-log-template.md`) for daily research\n2. **Refer to this guidance** when you need detailed explanations\n3. **Adapt to your needs** - not every section applies to every session\n4. **Be consistent** - develop a personal system and stick with it\n5. **Be thorough** - good documentation takes time but pays off\n\nRemember: Your research log is proof of your work. Make it detailed enough that someone else (or you, six months later) could understand exactly what you did and why.\n",
        "skills/genealogical-documentation/references/research-plan-guidance.md": "# Research Plan Guidance and Best Practices\n\nThis document provides detailed guidance, examples, and best practices for creating effective family history research plans. Use this as a reference when completing the research-plan-template.md.\n\n---\n\n## Table of Contents\n\n1. [Project Objective](#project-objective)\n2. [Research Questions](#research-questions)\n3. [Record Sources Checklist](#record-sources-checklist)\n4. [Research Strategy](#research-strategy)\n5. [Timeline Planning](#timeline-planning)\n6. [Genealogical Proof Standard (GPS)](#genealogical-proof-standard-gps)\n7. [Success Criteria](#success-criteria)\n8. [Documentation Best Practices](#documentation-best-practices)\n\n---\n\n## Project Objective\n\n### How to Write Effective Research Questions\n\nA good research question is:\n- **Specific:** Focuses on one person or family relationship\n- **Answerable:** Based on records that likely exist\n- **Clear:** States exactly what you want to discover\n\n### Examples\n\n**Good Research Questions:**\n- \"Determine the birth location and parents of John Smith (b. circa 1820, d. 1895 in Ohio)\"\n- \"Identify the maiden name and parents of Mary Johnson (b. 1845, married 1865)\"\n- \"Establish when and where the Thompson family migrated from Virginia to Kentucky\"\n\n**Too Broad:**\n- \"Learn about the Smith family\"\n- \"Find ancestors in Europe\"\n\n**Too Vague:**\n- \"Find information about John\"\n- \"Research family history\"\n\n---\n\n## Research Questions\n\n### Creating Prioritized Questions\n\nResearch questions should be ranked by:\n1. **Primary:** The main question that needs answering\n2. **Supporting:** Questions that help answer the primary question\n3. **Contextual:** Background information that aids research\n\n### Example Question List\n\n1. Where was John Smith born and when? (PRIMARY)\n2. Who were John Smith's parents? (PRIMARY)\n3. Where did John Smith reside between 1835-1845? (SUPPORTING)\n4. When did John Smith migrate to Ohio? (SUPPORTING)\n5. Are there siblings who can provide clues to origins? (CONTEXTUAL)\n6. What church did the family attend? (CONTEXTUAL)\n7. Who were John Smith's neighbors in 1850? (CONTEXTUAL - FAN principle)\n\n---\n\n## Record Sources Checklist\n\n### Vital Records\n- [ ] Birth certificates/records\n- [ ] Marriage licenses/certificates\n- [ ] Death certificates\n- [ ] Church baptism records\n- [ ] Church marriage records\n- [ ] Church burial records\n- [ ] Delayed birth records\n- [ ] Divorce records\n\n### Census Records\n- [ ] Federal census: 1850, 1860, 1870, 1880, 1900, 1910, 1920, 1930, 1940, 1950\n- [ ] State/territorial census\n- [ ] Mortality schedules\n- [ ] Agricultural schedules\n- [ ] Manufacturing schedules\n- [ ] Veterans schedules\n\n### Land and Property Records\n- [ ] Deed records (grantee index)\n- [ ] Deed records (grantor index)\n- [ ] Land grants/patents\n- [ ] Homestead applications\n- [ ] Tax records/assessments\n- [ ] Plat maps\n- [ ] Warranty deeds\n- [ ] Quit claim deeds\n\n### Probate Records\n- [ ] Wills\n- [ ] Estate files\n- [ ] Guardianship records\n- [ ] Estate settlements\n- [ ] Administration papers\n- [ ] Probate petitions\n- [ ] Estate inventories\n- [ ] Distribution of assets\n\n### Military Records\n- [ ] Service records\n- [ ] Pension files (widow's pensions valuable!)\n- [ ] Draft registrations\n- [ ] Bounty land warrants\n- [ ] Compiled military service records\n- [ ] Discharge papers\n- [ ] Veterans benefits applications\n\n### Immigration/Migration\n- [ ] Passenger lists (arrival)\n- [ ] Passenger lists (departure)\n- [ ] Naturalization records (declarations)\n- [ ] Naturalization records (petitions)\n- [ ] Border crossings\n- [ ] Port of entry records\n- [ ] Passport applications\n\n### Court Records\n- [ ] Civil cases\n- [ ] Criminal cases\n- [ ] Adoption records\n- [ ] Name changes\n- [ ] Lunacy proceedings\n- [ ] Bastardy bonds\n\n### Newspapers and Publications\n- [ ] Obituaries\n- [ ] Marriage announcements\n- [ ] Birth announcements\n- [ ] Death notices\n- [ ] Legal notices\n- [ ] Social columns\n- [ ] Business advertisements\n\n### Other Important Sources\n- [ ] City directories\n- [ ] Cemetery records\n- [ ] Funeral home records\n- [ ] School records\n- [ ] Employment records\n- [ ] Published county histories\n- [ ] Compiled genealogies\n- [ ] Family Bible records\n- [ ] Organization memberships\n- [ ] Professional licenses\n\n---\n\n## Research Strategy\n\n### Four-Phase Approach\n\n#### Phase 1: Foundation (Weeks 1-2)\n**Focus:** Establish basic timeline and collect easily accessible records\n\n**Activities:**\n- Search online census records for all available years\n- Locate death certificate for end-of-life information\n- Search online vital records indexes\n- Review FamilySearch and Ancestry trees (with caution)\n- Search FindAGrave for burial information\n\n**Goal:** Create solid foundation of dates, locations, and family composition\n\n#### Phase 2: Core Research (Weeks 3-6)\n**Focus:** Obtain primary sources and expand timeline\n\n**Activities:**\n- Request vital records from county/state agencies\n- Search land and property records\n- Research probate and estate files\n- Examine church records\n- Review military pension files if applicable\n\n**Goal:** Fill gaps in knowledge and collect original documents\n\n#### Phase 3: Expansion (Weeks 7-10)\n**Focus:** Collateral research and supporting evidence\n\n**Activities:**\n- Research siblings and associates (FAN principle)\n- Review published county histories\n- Search newspapers for obituaries and notices\n- Investigate church membership records\n- Explore social organizations and memberships\n\n**Goal:** Build comprehensive picture through indirect evidence\n\n#### Phase 4: Analysis and Documentation (Weeks 11-12)\n**Focus:** Evaluate evidence and document conclusions\n\n**Activities:**\n- Analyze and correlate all evidence\n- Resolve conflicting information\n- Build proof argument\n- Write research report\n- Create evidence analysis\n\n**Goal:** Meet Genealogical Proof Standard and document findings\n\n### Strategic Approaches\n\n#### FAN Principle\n**Family, Associates, and Neighbors**\n\nResearch extends beyond the primary subject to include:\n- **Family:** Parents, siblings, children, cousins, in-laws\n- **Associates:** Business partners, witnesses on documents, godparents\n- **Neighbors:** Those living nearby in census, deed witnesses, church members\n\n**Why It Works:** When direct evidence is missing, studying the network often reveals clues through indirect references.\n\n#### Cluster Research\nIdentify surname clusters or communities to study together:\n- Families who migrated together\n- Same-surname families in one area\n- Religious or ethnic communities\n\n#### Geographic Strategy\nFollow migration patterns:\n- Work backward from last known location\n- Study common migration routes\n- Research origin areas based on naming patterns\n- Consider chain migration (following family/friends)\n\n#### Chronological Approach\n- **Forward:** Start from earliest known ancestor, move forward\n- **Backward:** Start from known person, work back in time (usually more effective)\n- **Targeted:** Focus on specific time period where answers likely exist\n\n---\n\n## Timeline Planning\n\n### Sample 12-Week Research Timeline\n\n**Weeks 1-2: Foundation**\n- [ ] Search all online census records\n- [ ] Locate death certificate\n- [ ] Search vital records indexes\n- [ ] Review FindAGrave and cemetery records\n\n**Weeks 3-4: Vital Records**\n- [ ] Order birth, marriage, death certificates\n- [ ] Search church records for baptism, marriage, burial\n- [ ] Review delayed birth records\n\n**Weeks 5-6: Property and Probate**\n- [ ] Search deed indexes (grantee and grantor)\n- [ ] Review tax records\n- [ ] Examine probate files and wills\n- [ ] Check guardianship records\n\n**Weeks 7-8: Collateral Research**\n- [ ] Research siblings' families\n- [ ] Identify and research neighbors from census\n- [ ] Search for witnesses on documents\n\n**Weeks 9-10: Newspapers and Secondary Sources**\n- [ ] Search newspapers for obituaries\n- [ ] Review county histories\n- [ ] Check compiled genealogies (verify claims)\n\n**Weeks 11-12: Analysis and Documentation**\n- [ ] Complete evidence analysis\n- [ ] Resolve conflicts\n- [ ] Build proof argument\n- [ ] Write research report\n\n### Milestone Tracking\n\nUse milestones to stay on track:\n- **Week 2:** Complete online database searches\n- **Week 4:** Receive vital records ordered\n- **Week 6:** Complete courthouse records research\n- **Week 8:** Finish collateral line research\n- **Week 10:** Complete evidence analysis\n- **Week 12:** Finalize proof argument and report\n\n---\n\n## Genealogical Proof Standard (GPS)\n\nThe Genealogical Proof Standard requires five elements for any genealogical conclusion:\n\n### 1. Reasonably Exhaustive Research\n\n**What It Means:** Search thoroughly in sources likely to contain relevant information.\n\n**In Your Plan:**\n- List all applicable record types for time period and location\n- Search both direct sources (vital records) and indirect (FAN research)\n- Document negative results (sources that didn't have information)\n- Consider both online and offline repositories\n\n**Example:** For a birth location question, search:\n- Birth records (if available for time period)\n- Census records showing birthplace\n- Death certificate with birthplace\n- Marriage record with age/birthplace\n- Obituary mentioning birth\n- Bible records\n- Church baptism\n\n### 2. Complete and Accurate Source Citations\n\n**What It Means:** Cite all sources completely so others can find them.\n\n**In Your Plan:**\n- Use Evidence Explained citation format\n- Record enough detail to relocate source\n- Note repository, collection, volume/page, digital location\n- Maintain citation library or database\n\n**Example Citation:**\n\"Ohio, Coshocton County, Probate Court, Estate Files, 1850-1900, John Smith estate file (1895); Coshocton County Courthouse, Coshocton.\"\n\n### 3. Analysis and Correlation\n\n**What It Means:** Evaluate each source's information quality and reliability, then compare information from multiple sources.\n\n**In Your Plan:**\n- Classify sources (original vs. derivative, primary vs. secondary)\n- Assess information reliability\n- Create comparison matrices for key facts\n- Note patterns and consistencies\n\n**Example:**\n| Source | Birth Year | Birth Location | Information Type | Reliability |\n|--------|-----------|----------------|------------------|-------------|\n| 1850 Census | circa 1820 | Ohio | Secondary | Medium |\n| Death Cert | 1822 | Virginia | Primary | High |\n| Tombstone | 1820 | Not listed | Secondary | Low |\n\n### 4. Resolution of Conflicting Evidence\n\n**What It Means:** Identify contradictions and explain which evidence is most reliable.\n\n**In Your Plan:**\n- List all conflicts found\n- Assess reliability of each conflicting source\n- Apply preponderance of evidence principle\n- Document reasoning for choosing one source over another\n\n**Example:**\n\"Birth year conflict: Census says 1820, death certificate says 1822. Death certificate is preferred because: (1) it's primary information from someone who knew the birth year, (2) it's original information recorded at time of death, (3) census ages are often approximations.\"\n\n### 5. Soundly Reasoned Conclusion\n\n**What It Means:** Build a logical argument from evidence to conclusion.\n\n**In Your Plan:**\n- State conclusion clearly\n- Demonstrate logical progression from evidence to conclusion\n- Assign appropriate proof level (Proven, Probable, Possible, Disproven, Undetermined)\n- Address alternative hypotheses\n\n**Example:**\n\"Conclusion: John Smith was probably born in 1822 in Virginia based on: (1) death certificate primary information, (2) pattern of Virginia births for siblings, (3) migration timing consistent with Virginia origin, (4) naming patterns suggest Virginia family connections. Proof level: Probable (not proven due to lack of birth record).\"\n\n---\n\n## Success Criteria\n\n### Project Completion Checklist\n\n**Research is complete when:**\n- [ ] All research questions answered or determined to be unanswerable\n- [ ] Reasonably exhaustive research completed for time/place\n- [ ] All applicable record types searched\n- [ ] All sources properly cited using Evidence Explained format\n- [ ] Conflicts identified and addressed\n- [ ] Evidence analyzed and correlated\n- [ ] Proof argument meets GPS standards\n- [ ] Appropriate proof level assigned and justified\n\n### Required Deliverables\n\n- [ ] **Research Report:** Narrative explaining findings and proof argument\n- [ ] **Evidence Analysis:** Detailed source comparison and conflict resolution\n- [ ] **Complete Source Citations:** All sources cited completely\n- [ ] **Research Log:** Record of all searches conducted\n- [ ] **Supporting Documents:** Digital copies or transcriptions of key records\n\n### Proof Levels\n\nAssign one of these levels to your conclusion:\n\n- **Proven:** Evidence meets GPS with no reasonable doubt\n- **Probable:** Strong evidence meeting GPS but some uncertainty remains\n- **Possible:** Some supporting evidence but gaps exist\n- **Disproven:** Evidence contradicts the hypothesis\n- **Undetermined:** Insufficient evidence to make a conclusion\n\n---\n\n## Documentation Best Practices\n\n### Citation Format\n\n**Use Evidence Explained format** for all citations. This format includes:\n- Author (if applicable)\n- Title of source\n- Publication information or repository\n- Specific location within source (page, line, entry)\n- Date accessed (for online sources)\n\n### Organization System\n\n**Recommended Structure:**\n```\nProject-Name/\n Research-Plan.md\n Research-Log.md\n Citations/\n    Citation-Library.md\n    Source-Notes/\n Documents/\n    Vital-Records/\n    Census/\n    Land-Property/\n    Probate/\n    Other/\n Evidence-Analysis/\n Reports/\n```\n\n### Tools and Software\n\n**Genealogy Software:**\n- Family Tree Maker\n- RootsMagic\n- Legacy Family Tree\n- Gramps (free, open source)\n\n**Citation Management:**\n- Zotero (free)\n- EasyBib\n- Spreadsheet with citation template\n\n**Document Organization:**\n- Cloud storage (Google Drive, Dropbox, OneDrive)\n- Local folders with consistent naming\n- Genealogy software document attachment features\n\n### File Naming Conventions\n\nUse consistent naming for easy retrieval:\n```\n[Surname]-[GivenName]-[RecordType]-[Date]-[Location]\n\nExamples:\nSmith-John-DeathCert-1895-CoshoctonOH.pdf\nSmith-John-1880Census-CoshoctonOH.pdf\nSmith-John-Will-1895-CoshoctonOH.pdf\n```\n\n---\n\n## Resources and Planning\n\n### Budget Considerations\n\n**Typical Research Costs:**\n- Vital record copies: $10-25 each\n- Database subscriptions: $20-50/month\n- Courthouse research (travel): Variable\n- Document copies from repositories: $0.25-1.00 per page\n- Professional researcher (if needed): $30-75/hour\n\n**Budget Planning:**\n- Estimate record costs based on research plan\n- Consider subscription vs. pay-per-view databases\n- Budget for travel if courthouse visits needed\n- Plan for unexpected records discovered\n\n### Time Management\n\n**Realistic Time Estimates:**\n- Simple question, good records: 10-20 hours\n- Moderate complexity: 40-80 hours\n- Complex problem, scarce records: 100+ hours\n\n**Weekly Time Allocation:**\n- 5 hours/week: 2-3 month project\n- 10 hours/week: 1-2 month project\n- 20 hours/week: 2-4 week project\n\n### Skills Assessment\n\n**May Need:**\n- Foreign language translation\n- Old handwriting (paleography)\n- DNA analysis interpretation\n- Legal document terminology understanding\n- Historical context of time period\n\n**Resources for Learning:**\n- Online paleography courses\n- Genealogy societies and workshops\n- DNA analysis webinars\n- Historical society resources\n\n### Common Constraints\n\n**Record Availability:**\n- Privacy laws (recent records)\n- Record destruction (fires, floods, wars)\n- Incomplete record keeping\n- Non-digitized records\n\n**Geographic Distance:**\n- Records only available at courthouse\n- No online access to some repositories\n- Travel costs prohibitive\n\n**Solutions:**\n- Hire local researcher\n- Request records by mail\n- Join local genealogy society for lookups\n- Use FamilySearch library loan system\n\n---\n\n## Revision and Updates\n\n### When to Update Research Plan\n\nUpdate your plan when:\n- New information changes research direction\n- Research questions answered (mark complete, add new ones)\n- Discovery of new relevant record sources\n- Timeline adjustments needed\n- Strategy proves ineffective and needs revision\n\n### Documenting Changes\n\nUse the Revision History section to track:\n- Date of update\n- What changed and why\n- New directions or strategies\n- Completed phases or questions\n\n**Example:**\n```\n- **2024-01-15** - Research plan created\n- **2024-02-01** - Updated strategy after finding John Smith in 1850 census\n  in Pennsylvania, not Ohio as expected; added Pennsylvania repositories\n- **2024-02-20** - Question 1 answered (birth location: PA); added new\n  question about parents' origins\n- **2024-03-10** - Completed Phase 1 and 2; moving to collateral research\n```\n\n---\n\n## Quick Reference: Common Mistakes to Avoid\n\n1. **Starting research without a plan** - Leads to duplicate work and missed sources\n2. **Not documenting negative results** - You'll waste time searching again\n3. **Accepting online trees without verification** - Trees often contain errors\n4. **Ignoring conflicting evidence** - Must be addressed for GPS\n5. **Inadequate source citations** - Can't verify or relocate sources later\n6. **Researching too many people at once** - Focus on one question at a time\n7. **Not considering the FAN principle** - Missing valuable indirect evidence\n8. **Stopping at first answer found** - Reasonably exhaustive research required\n9. **Poor file organization** - Wastes time finding documents later\n10. **Not updating the research log** - Forget what you've already searched\n\n---\n\n**For questions or additional guidance, consult:**\n- *Genealogy Standards* (Board for Certification of Genealogists)\n- *Evidence Explained* (Elizabeth Shown Mills)\n- *Mastering Genealogical Proof* (Thomas W. Jones)\n- Local genealogy society resources\n- Professional genealogist consultations\n",
        "skills/genealogical-documentation/references/research-strategies.md": "# Family History Research Strategies\n\nAdvanced research methodologies and strategic approaches for effective family history and genealogy research.\n\n## Core Research Principles\n\n### 1. FAN Principle (Family, Associates, Neighbors)\n\nResearch the network around your ancestor, not just the ancestor alone.\n\n**Family:**\n- Parents, siblings, spouses, children\n- In-laws and step-relations\n- Aunts, uncles, cousins\n- Godparents and guardians\n\n**Associates:**\n- Business partners\n- Witnesses to documents\n- Executors of wills\n- Bond co-signers\n- Fellow church members\n\n**Neighbors:**\n- Clustered in census records\n- Adjacent land owners\n- Same community institutions\n- Migration companions\n\n**Why It Works:**\n- Family left records when ancestor didn't\n- Associates provide clues to ancestor's activities\n- Neighbors help locate ancestor geographically\n- Network reveals relationships and context\n\n**Application:**\nWhen researching John Smith:\n- Search for all Smith siblings in same records\n- Find witnesses on John's marriage record\n- Research neighbors from 1850 census\n- Track godparents named in baptism records\n\n### 2. Cluster Genealogy\n\nStudy your ancestor within their social cluster.\n\n**Identify Clusters:**\n- Same surname in locality\n- Repeated witnesses across documents\n- Adjacent land owners\n- Same migration patterns\n- Common church/organization membership\n\n**Track Clusters:**\n- Follow cluster through records\n- Note when cluster members separate\n- Identify cluster leaders/patriarchs\n- Map cluster relationships\n\n**Benefits:**\n- Reveals family connections\n- Provides context for decisions\n- Helps distinguish same-name individuals\n- Suggests migration routes and destinations\n\n### 3. Reasonably Exhaustive Research\n\nWork systematically through source types and locations.\n\n**Source Prioritization:**\n\n**Tier 1 - Essential (search first):**\n- Census records\n- Vital records\n- Major events (birth, marriage, death)\n- Land records\n- Probate records\n\n**Tier 2 - Important (search next):**\n- Church records\n- Military records\n- Immigration/naturalization\n- Newspapers\n- City directories\n\n**Tier 3 - Contextual (as needed):**\n- Court records\n- Tax records\n- School records\n- Business records\n- Organization memberships\n\n**Tier 4 - Supplemental (for difficult cases):**\n- Published histories\n- Compiled genealogies\n- DNA evidence\n- Archaeological evidence\n- Oral histories\n\n### 4. Work Backward from Known to Unknown\n\nStart with what you know for certain and work backward chronologically.\n\n**Process:**\n1. Establish identity of known person with documentation\n2. Find parent-child connections\n3. Verify each generation before moving back\n4. Don't skip generations based on assumptions\n\n**Avoid:**\n-  Working forward from early immigrant\n-  Assuming surname means relation\n-  Skipping generations\n-  Starting with others' research without verification\n\n## Geographic Strategies\n\n### 1. Migration Tracking\n\nFollow ancestors through geographic movement.\n\n**Migration Clues:**\n- Birth places of children (show movement)\n- Land sales and purchases\n- Changing census locations\n- Probate records show both origin and destination\n- Newspaper notices of arrivals/departures\n\n**Pattern Recognition:**\n- Chain migration (following relatives/neighbors)\n- Following transportation routes (rivers, roads, rails)\n- Moving to similar geography/climate\n- Seeking specific resources (farmland, mining, manufacturing)\n\n**Research Strategy:**\n1. Establish presence in each location with documentation\n2. Search records in both origin and destination\n3. Check intermediate locations along migration route\n4. Research known migration patterns for time/place\n\n### 2. Locality Research\n\nUnderstand the history and records of each location.\n\n**Priority Resources (use these first):**\n- [FamilySearch Wiki](https://www.familysearch.org/wiki/) - Comprehensive locality guides, record information, and research strategies for locations worldwide\n- [LDSgenealogy.com](https://ldsgenealogy.com/) - State-specific research guides and record information\n  - Example: [Pennsylvania Research](https://ldsgenealogy.com/PA/)\n  - Includes county formation, record repositories, and research strategies\n\n**Essential Background:**\n- County formation history (which county existed when?)\n- Record losses (fires, floods, wars)\n- Record repositories (where records are now)\n- Boundary changes (did location change counties?)\n- Local history (economic, social, religious factors)\n\n**Research Guides:**\n- Red Book (American State, County & Town Sources)\n- Wiki sites (FamilySearch Wiki, county wikis)\n- State archives research guides\n- Local historical society resources\n\n### 3. Jurisdictional Strategy\n\nUnderstand which jurisdiction created which records.\n\n**Typical Jurisdictions:**\n- **Federal**: Census, military, immigration, tax\n- **State**: Vital records, land grants, institutions\n- **County**: Deeds, probate, court, marriages\n- **Town/City**: Local vital records, directories, newspapers\n- **Church/Private**: Baptisms, marriages, burials\n\n**Research Implication:**\nKnow which level of government to search for each record type.\n\n## Record-Specific Strategies\n\n### Census Research\n\n**Complete Census Search:**\n- Search every census year person was alive\n- Include mortality schedules\n- Check agricultural/industrial schedules\n- Search state/territorial censuses\n\n**Techniques:**\n- Browse entire enumeration district (find neighbors)\n- Check indexes AND browse images (transcription errors)\n- Note all household members and relationships\n- Track household composition changes\n- Compare across multiple censuses\n\n**Common Issues:**\n- Name variations/misspellings\n- Age inconsistencies\n- Enumeration errors\n- Missing enumerations\n\n### Land Records Research\n\n**Deed Research Strategy:**\n- Search both grantee (buyer) and grantor (seller) indexes\n- Read complete deeds for family relationships\n- Note witnesses (often family)\n- Check adjoining land owners\n- Follow land through generations\n\n**Beyond Deeds:**\n- Tax records (prove ownership without deed)\n- Plat maps (show locations)\n- Homestead files (rich detail)\n- Military bounty land (connects to service)\n\n### Probate Research\n\n**Comprehensive Probate Search:**\n- Search by name of deceased\n- Search by administrator/executor (might be relative serving)\n- Check probate packets (complete files, not just indexed wills)\n- Search guardianship records (minor children)\n- Check estate settlements (years after death)\n\n**Information in Probate:**\n- Family relationships explicitly stated\n- Property distribution reveals family structure\n- Witnesses often family members\n- Dates of death\n- Financial status\n\n### Church Records\n\n**Church Record Strategy:**\n- Identify denomination through other records\n- Find church locations using maps/histories\n- Check multiple churches in area\n- Search for original records and published abstracts\n\n**Record Types:**\n- Baptism/christening (shows parents, godparents)\n- Marriage (shows families of both parties)\n- Burial (shows death date, age)\n- Membership lists (show arrivals/departures)\n- Church minutes (context about families)\n\n## Brick Wall Strategies\n\nWhen research stalls, try these approaches:\n\n### 1. Re-analyze Existing Evidence\n\n- Review all sources collected\n- Look for overlooked clues\n- Reconsider interpretations\n- Check for indirect evidence\n\n### 2. Expand Geographic Search\n\n- Search neighboring counties\n- Check origin and destination states\n- Research migration routes\n- Try unexpected locations\n\n### 3. Expand Time Frame\n\n- Search earlier than expected (age errors)\n- Search later (delayed recording)\n- Check surrounding decades\n\n### 4. Research Collateral Lines\n\n- Focus on siblings who left more records\n- Research spouse's family\n- Study children's records for clues\n\n### 5. Alternative Name Strategies\n\n- Try phonetic spellings\n- Check nickname variations\n- Search for maiden names\n- Consider name changes\n\n### 6. Non-Traditional Sources\n\n- Newspapers (obituaries, notices, ads)\n- City directories (occupation, residence)\n- Organization records (memberships)\n- School records\n- Employment records\n- DNA evidence\n\n### 7. Hire Local Researcher\n\n- Access to repositories you can't visit\n- Knowledge of local records\n- Understanding of local history\n- Reading old handwriting\n\n## Documentation Strategies\n\n### Research Logs\n\n**Essential Elements:**\n- Date of research session\n- Sources searched (positive and negative)\n- Search terms and variations used\n- Results found (or not found)\n- Next steps identified\n\n**Benefits:**\n- Avoid duplicate searches\n- Track strategies tried\n- Document thorough research\n- Support GPS compliance\n\n### Source Organization\n\n**System Requirements:**\n- Consistent citation format\n- Cross-referencing between sources\n- Tagging/categorizing by person, place, type\n- Linking sources to conclusions\n\n**Tools:**\n- Genealogy software\n- Spreadsheets\n- Cloud storage\n- Physical filing system\n\n### Evidence Tracking\n\n**Organize Evidence By:**\n- Individual person\n- Research question\n- Source type\n- Time period\n- Geographic location\n\n**Track:**\n- What source says (exact information)\n- Your interpretation\n- Reliability assessment\n- Connections to other sources\n- Questions raised\n\n## Advanced Strategies\n\n### 1. Negative Evidence Strategy\n\nUse absence of information as evidence.\n\n**Examples:**\n- Person not in census (suggests death or migration)\n- Not in probate records (died without estate)\n- Not in church records (different denomination)\n- Missing from land records (renter not owner)\n\n**Application:**\nAbsence of John in 1880 census but present in 1870 suggests:\n- Death between 1870-1880\n- Migration out of area\n- Enumeration error\nNext: Search death records, neighboring counties\n\n### 2. Timeline Construction\n\nBuild comprehensive timeline of ancestor's life.\n\n**Include:**\n- Known facts with dates\n- Implied facts (birth from age)\n- Contextual events\n- Family member events\n- Historical context\n\n**Analysis:**\n- Identify date conflicts\n- Spot gaps in knowledge\n- Recognize patterns\n- Test logical consistency\n\n### 3. Hypothesis Testing\n\nFormulate and test specific hypotheses.\n\n**Process:**\n1. State hypothesis specifically\n2. Identify what evidence would support it\n3. Identify what evidence would contradict it\n4. Search for both supporting and contradicting evidence\n5. Evaluate results objectively\n\n**Example:**\nHypothesis: \"John Smith was son of William Smith of Pennsylvania\"\nSupporting evidence: Similar names, right time/place, DNA match\nContradicting evidence: William's will doesn't mention John\nResult: Hypothesis needs revision\n\n### 4. DNA Strategy\n\nUse genetic genealogy to complement documentary research.\n\n**DNA Applications:**\n- Confirm family relationships\n- Break through brick walls\n- Identify unknown parents\n- Distinguish same-name individuals\n- Find living cousins with information\n\n**Important:**\n- DNA doesn't replace documentary research\n- Requires DNA literacy and understanding\n- Consider privacy and ethical issues\n- Best used with strong paper trail\n\n## Research Planning\n\n### Project-Based Research\n\n**Define Project:**\n- Specific research question\n- Scope and limitations\n- Expected sources\n- Timeline and milestones\n\n**Advantages:**\n- Focused effort\n- Measurable progress\n- Prevents research sprawl\n- Supports GPS compliance\n\n### Iterative Research\n\nResearch in cycles, not linear progression.\n\n**Cycle:**\n1. **Plan**: Identify sources to search\n2. **Search**: Execute research plan\n3. **Analyze**: Evaluate findings\n4. **Document**: Record results and reasoning\n5. **Revise**: Update plan based on findings\n6. **Repeat**: Next research cycle\n\n### Collaboration Strategies\n\n**Effective Collaboration:**\n- Share sources and citations\n- Respect others' privacy and wishes\n- Cite others' contributions\n- Verify independently\n- Contribute to community\n\n**Resources:**\n- Online forums and groups\n- Local genealogy societies\n- Surname studies\n- DNA project groups\n- Academic partnerships\n\n## Efficiency Strategies\n\n### Batch Research\n\n**Group by:**\n- Repository (research all at one archive)\n- Source type (all census at once)\n- Location (all records for one county)\n- Time period (all 1850s records)\n\n### Online Research Optimization\n\n**Effective Searching:**\n- Use wildcards for name variations\n- Try different date ranges\n- Search phonetically\n- Browse, don't just search\n- Check multiple databases\n\n**Time Management:**\n- Set specific research goals\n- Limit browsing time\n- Take organized notes\n- Schedule focused sessions\n- Balance online and offline research\n\n## Research Ethics\n\n### Professional Standards\n\n- Give proper credit to sources\n- Respect copyright\n- Protect privacy of living persons\n- Share information generously\n- Correct errors when discovered\n- Be honest about uncertainty\n\n### Source Attribution\n\nAlways cite:\n- Published sources\n- Other researchers' work\n- Unpublished documents\n- Oral histories\n- DNA matches\n\n## Continuous Learning\n\n**Stay Current:**\n- New records digitized regularly\n- New research methodologies\n- Technology changes\n- Historical discoveries\n- Legal changes affecting access\n\n**Resources:**\n- Genealogy conferences\n- Webinars and workshops\n- Professional journals\n- Online courses\n- Local society meetings\n\n## Summary: Keys to Success\n\n1. **Be Systematic** - Work methodically through sources\n2. **Be Thorough** - Search multiple source types\n3. **Be Skeptical** - Verify everything\n4. **Be Organized** - Document as you go\n5. **Be Patient** - Breakthroughs take time\n6. **Be Flexible** - Try new approaches\n7. **Be Collaborative** - Learn from others\n8. **Be Ethical** - Follow professional standards\n",
        "skills/generate-swagger-docs/SKILL.md": "---\nname: generate-swagger-docs\ndescription: Generate OpenAPI documentation from source code. Analyzes repository to automatically discover API endpoints and create swagger.json and interactive HTML documentation. Use when generating API docs, creating OpenAPI specs, documenting REST APIs, or analyzing API endpoints.\nallowed-tools: Bash\n---\n\n# Generate Swagger Documentation\n\nGenerate OpenAPI docs from your codebase in seconds with automatic API key setup.\n\n## How It Works\n\nThis skill automates the Swagger/OpenAPI documentation generation:\n\n1. **API Key Setup** - Accepts your OpenAI API key and sets it as an environment variable\n2. **Initialization** - Downloads and sets up the apimesh tool\n3. **Automatic Processing** - The tool analyzes your codebase and generates documentation\n4. **Output** - Outputs are saved to the apimesh/ directory\n\n## Setup Requirements\n\nYou need an OpenAI API key to use this skill. Get one from [OpenAI's platform](https://platform.openai.com/account/api-keys) if you don't have one already.\n\n## Recommended: Quick Setup with API Key\n\nThe easiest way to use this skill is to pass your API key directly:\n\n```bash\n/Users/ankits/.claude/skills/generate-swagger-docs/generate-with-key.sh \"sk-proj-your-api-key-here\"\n```\n\nThis will:\n1. Accept your OpenAI API key as an argument\n2. Create the `apimesh/` directory and download the apimesh tool\n3. Set up the environment variables correctly\n4. Generate your Swagger documentation automatically\n5. Display the output file locations\n\n## Automatic Flow\n\nWhen you run this skill:\n\n1. **First Time:** You'll be prompted for your OpenAI API key (starts with `sk-proj-`)\n   - The key is saved locally and used for subsequent runs\n   - The key is NOT committed to version control\n\n2. **Subsequent Runs:** The skill uses the saved API key automatically\n   - No additional prompts for the key unless you clear the config\n\n3. **Framework Detection:** Automatically detects your API framework\n   - Supports: Express, NestJS, FastAPI, Django, Rails, Go, and more\n\n## What It Does\n\n- Scans your repository for API endpoints\n- Detects the web framework (Django, Flask, FastAPI, Express, NestJS, Rails, Go)\n- Generates OpenAPI 3.0 specification (`swagger.json`)\n- Creates interactive HTML documentation (`apimesh-docs.html`)\n\n## Output\n\n- `apimesh/swagger.json` - OpenAPI 3.0 spec\n- `apimesh/apimesh-docs.html` - Interactive Swagger UI (self-contained, shareable)\n- `apimesh/config.json` - Saved configuration (includes your settings, gitignore this file)\n\n## Important Notes\n\n- Your OpenAI API key is needed for the LLM analysis\n- The generated `config.json` should be added to `.gitignore` as it contains secrets\n- Framework detection is automatic but can be manually specified if needed\n- The tool supports both public and private repositories\n\n## API Key Setup Methods\n\n### Method 1: Wrapper Script (Recommended) \n\nUse the provided wrapper script that properly handles environment variable propagation:\n\n```bash\n/Users/ankits/.claude/skills/generate-swagger-docs/generate-with-key.sh \"sk-proj-your-api-key-here\"\n```\n\n**Why this works best:**\n- Properly passes the API key to the Python subprocess\n- No interactive prompts in non-TTY environments\n- Handles environment variable propagation correctly\n- Provides clear success/error messages\n\n### Method 2: Using Saved Configuration\n\nAfter the first run with Method 1, your key is saved in `apimesh/config.json`. On subsequent runs, you can omit the key (if you trust your local setup):\n\n```bash\n/Users/ankits/.claude/skills/generate-swagger-docs/generate-with-key.sh\n```\n\n**Important:** Never commit the `config.json` file to version control. Add it to `.gitignore`.\n\n### Method 3: Manual apimesh Setup\n\nIf you need more control, download and run apimesh directly:\n\n```bash\nexport OPENAI_API_KEY=\"sk-proj-your-api-key-here\"\nmkdir -p apimesh && \\\n  curl -sSL https://raw.githubusercontent.com/qodex-ai/apimesh/refs/heads/main/run.sh -o apimesh/run.sh && \\\n  chmod +x apimesh/run.sh && \\\n  cd apimesh && \\\n  OPENAI_API_KEY=\"$OPENAI_API_KEY\" ./run.sh\n```\n\n**Note:** The `OPENAI_API_KEY` must be explicitly passed to the subprocess as shown above.\n",
        "skills/identity-framework/SKILL.md": "---\nname: identity-framework\ndescription: Define and document brand identity standards and guidelines. Establishes visual language, tone, messaging, and design principles for consistent brand expression.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Anthropic Brand Styling\n\n## Overview\n\nTo access Anthropic's official brand identity and style resources, use this skill.\n\n**Keywords**: branding, corporate identity, visual identity, post-processing, styling, brand colors, typography, Anthropic brand, visual formatting, visual design\n\n## Brand Guidelines\n\n### Colors\n\n**Main Colors:**\n\n- Dark: `#141413` - Primary text and dark backgrounds\n- Light: `#faf9f5` - Light backgrounds and text on dark\n- Mid Gray: `#b0aea5` - Secondary elements\n- Light Gray: `#e8e6dc` - Subtle backgrounds\n\n**Accent Colors:**\n\n- Orange: `#d97757` - Primary accent\n- Blue: `#6a9bcc` - Secondary accent\n- Green: `#788c5d` - Tertiary accent\n\n### Typography\n\n- **Headings**: Poppins (with Arial fallback)\n- **Body Text**: Lora (with Georgia fallback)\n- **Note**: Fonts should be pre-installed in your environment for best results\n\n## Features\n\n### Smart Font Application\n\n- Applies Poppins font to headings (24pt and larger)\n- Applies Lora font to body text\n- Automatically falls back to Arial/Georgia if custom fonts unavailable\n- Preserves readability across all systems\n\n### Text Styling\n\n- Headings (24pt+): Poppins font\n- Body text: Lora font\n- Smart color selection based on background\n- Preserves text hierarchy and formatting\n\n### Shape and Accent Colors\n\n- Non-text shapes use accent colors\n- Cycles through orange, blue, and green accents\n- Maintains visual interest while staying on-brand\n\n## Technical Details\n\n### Font Management\n\n- Uses system-installed Poppins and Lora fonts when available\n- Provides automatic fallback to Arial (headings) and Georgia (body)\n- No font installation required - works with existing system fonts\n- For best results, pre-install Poppins and Lora fonts in your environment\n\n### Color Application\n\n- Uses RGB color values for precise brand matching\n- Applied via python-pptx's RGBColor class\n- Maintains color fidelity across different systems\n",
        "skills/infrastructure-code-synthesis/SKILL.md": "---\nname: infrastructure-code-synthesis\ndescription: Synthesize and generate AWS infrastructure as code using CDK. Creates composable infrastructure components and deployment patterns programmatically.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# AWS CDK Development\n\nThis skill provides comprehensive guidance for developing AWS infrastructure using the Cloud Development Kit (CDK), with integrated MCP servers for accessing latest AWS knowledge and CDK utilities.\n\n## AWS Documentation Requirement\n\n**CRITICAL**: This skill requires AWS MCP tools for accurate, up-to-date AWS information.\n\n### Before Answering AWS Questions\n\n1. **Always verify** using AWS MCP tools (if available):\n   - `mcp__aws-mcp__aws___search_documentation` or `mcp__*awsdocs*__aws___search_documentation` - Search AWS docs\n   - `mcp__aws-mcp__aws___read_documentation` or `mcp__*awsdocs*__aws___read_documentation` - Read specific pages\n   - `mcp__aws-mcp__aws___get_regional_availability` - Check service availability\n\n2. **If AWS MCP tools are unavailable**:\n   - Guide user to configure AWS MCP: See [AWS MCP Setup Guide](../../docs/aws-mcp-setup.md)\n   - Help determine which option fits their environment:\n     - Has uvx + AWS credentials  Full AWS MCP Server\n     - No Python/credentials  AWS Documentation MCP (no auth)\n   - If cannot determine  Ask user which option to use\n\n## Integrated MCP Servers\n\nThis skill includes the CDK MCP server automatically configured with the plugin:\n\n### AWS CDK MCP Server\n**When to use**: For CDK-specific guidance and utilities\n- Get CDK construct recommendations\n- Retrieve CDK best practices\n- Access CDK pattern suggestions\n- Validate CDK configurations\n- Get help with CDK-specific APIs\n\n**Important**: Leverage this server for CDK construct guidance and advanced CDK operations.\n\n## When to Use This Skill\n\nUse this skill when:\n- Creating new CDK stacks or constructs\n- Refactoring existing CDK infrastructure\n- Implementing Lambda functions within CDK\n- Following AWS CDK best practices\n- Validating CDK stack configurations before deployment\n- Verifying AWS service capabilities and regional availability\n\n## Core CDK Principles\n\n### Resource Naming\n\n**CRITICAL**: Do NOT explicitly specify resource names when they are optional in CDK constructs.\n\n**Why**: CDK-generated names enable:\n- **Reusable patterns**: Deploy the same construct/pattern multiple times without conflicts\n- **Parallel deployments**: Multiple stacks can deploy simultaneously in the same region\n- **Cleaner shared logic**: Patterns and shared code can be initialized multiple times without name collision\n- **Stack isolation**: Each stack gets uniquely identified resources automatically\n\n**Pattern**: Let CDK generate unique names automatically using CloudFormation's naming mechanism.\n\n```typescript\n//  BAD - Explicit naming prevents reusability and parallel deployments\nnew lambda.Function(this, 'MyFunction', {\n  functionName: 'my-lambda',  // Avoid this\n  // ...\n});\n\n//  GOOD - Let CDK generate unique names\nnew lambda.Function(this, 'MyFunction', {\n  // No functionName specified - CDK generates: StackName-MyFunctionXXXXXX\n  // ...\n});\n```\n\n**Security Note**: For different environments (dev, staging, prod), follow AWS Security Pillar best practices by using separate AWS accounts rather than relying on resource naming within a single account. Account-level isolation provides stronger security boundaries.\n\n### Lambda Function Development\n\nUse the appropriate Lambda construct based on runtime:\n\n**TypeScript/JavaScript**: Use `@aws-cdk/aws-lambda-nodejs`\n```typescript\nimport { NodejsFunction } from 'aws-cdk-lib/aws-lambda-nodejs';\n\nnew NodejsFunction(this, 'MyFunction', {\n  entry: 'lambda/handler.ts',\n  handler: 'handler',\n  // Automatically handles bundling, dependencies, and transpilation\n});\n```\n\n**Python**: Use `@aws-cdk/aws-lambda-python`\n```typescript\nimport { PythonFunction } from '@aws-cdk/aws-lambda-python-alpha';\n\nnew PythonFunction(this, 'MyFunction', {\n  entry: 'lambda',\n  index: 'handler.py',\n  handler: 'handler',\n  // Automatically handles dependencies and packaging\n});\n```\n\n**Benefits**:\n- Automatic bundling and dependency management\n- Transpilation handled automatically\n- No manual packaging required\n- Consistent deployment patterns\n\n### Pre-Deployment Validation\n\nUse a **multi-layer validation strategy** for comprehensive CDK quality checks:\n\n#### Layer 1: Real-Time IDE Feedback (Recommended)\n\n**For TypeScript/JavaScript projects**:\n\nInstall [cdk-nag](https://github.com/cdklabs/cdk-nag) for synthesis-time validation:\n```bash\nnpm install --save-dev cdk-nag\n```\n\nAdd to your CDK app:\n```typescript\nimport { Aspects } from 'aws-cdk-lib';\nimport { AwsSolutionsChecks } from 'cdk-nag';\n\nconst app = new App();\nAspects.of(app).add(new AwsSolutionsChecks());\n```\n\n**Optional - VS Code users**: Install [CDK NAG Validator extension](https://marketplace.visualstudio.com/items?itemName=alphacrack.cdk-nag-validator) for faster feedback on file save.\n\n**For Python/Java/C#/Go projects**: cdk-nag is available in all CDK languages and provides the same synthesis-time validation.\n\n#### Layer 2: Synthesis-Time Validation (Required)\n\n1. **Synthesis with cdk-nag**: Validate stack with comprehensive rules\n   ```bash\n   cdk synth  # cdk-nag runs automatically via Aspects\n   ```\n\n2. **Suppress legitimate exceptions** with documented reasons:\n   ```typescript\n   import { NagSuppressions } from 'cdk-nag';\n\n   // Document WHY the exception is needed\n   NagSuppressions.addResourceSuppressions(resource, [\n     {\n       id: 'AwsSolutions-L1',\n       reason: 'Lambda@Edge requires specific runtime for CloudFront compatibility'\n     }\n   ]);\n   ```\n\n#### Layer 3: Pre-Commit Safety Net\n\n1. **Build**: Ensure compilation succeeds\n   ```bash\n   npm run build  # or language-specific build command\n   ```\n\n2. **Tests**: Run unit and integration tests\n   ```bash\n   npm test  # or pytest, mvn test, etc.\n   ```\n\n3. **Validation Script**: Meta-level checks\n   ```bash\n   ./scripts/validate-stack.sh\n   ```\n\nThe validation script now focuses on:\n- Language detection\n- Template size and resource count analysis\n- Synthesis success verification\n- (Note: Detailed anti-pattern checks are handled by cdk-nag)\n\n## Workflow Guidelines\n\n### Development Workflow\n\n1. **Design**: Plan infrastructure resources and relationships\n2. **Verify AWS Services**: Use AWS Documentation MCP to confirm service availability and features\n   - Check regional availability for all required services\n   - Verify service limits and quotas\n   - Confirm latest API specifications\n3. **Implement**: Write CDK constructs following best practices\n   - Use CDK MCP server for construct recommendations\n   - Reference CDK best practices via MCP tools\n4. **Validate**: Run pre-deployment checks (see above)\n5. **Synthesize**: Generate CloudFormation templates\n6. **Review**: Examine synthesized templates for correctness\n7. **Deploy**: Deploy to target environment\n8. **Verify**: Confirm resources are created correctly\n\n### Stack Organization\n\n- Use nested stacks for complex applications\n- Separate concerns into logical construct boundaries\n- Export values that other stacks may need\n- Use CDK context for environment-specific configuration\n\n### Testing Strategy\n\n- Unit test individual constructs\n- Integration test stack synthesis\n- Snapshot test CloudFormation templates\n- Validate resource properties and relationships\n\n## Using MCP Servers Effectively\n\n### When to Use AWS Documentation MCP\n\n**Always verify before implementing**:\n- New AWS service features or configurations\n- Service availability in target regions\n- API parameter specifications\n- Service limits and quotas\n- Security best practices for AWS services\n\n**Example scenarios**:\n- \"Check if Lambda supports Python 3.13 runtime\"\n- \"Verify DynamoDB is available in eu-south-2\"\n- \"What are the current Lambda timeout limits?\"\n- \"Get latest S3 encryption options\"\n\n### When to Use CDK MCP Server\n\n**Leverage for CDK-specific guidance**:\n- CDK construct selection and usage\n- CDK API parameter options\n- CDK best practice patterns\n- Construct property configurations\n- CDK-specific optimizations\n\n**Example scenarios**:\n- \"What's the recommended CDK construct for API Gateway REST API?\"\n- \"How to configure NodejsFunction bundling options?\"\n- \"Best practices for CDK stack organization\"\n- \"CDK construct for DynamoDB with auto-scaling\"\n\n### MCP Usage Best Practices\n\n1. **Verify First**: Always check AWS Documentation MCP before implementing new features\n2. **Regional Validation**: Check service availability in target deployment regions\n3. **CDK Guidance**: Use CDK MCP for construct-specific recommendations\n4. **Stay Current**: MCP servers provide latest information beyond knowledge cutoff\n5. **Combine Sources**: Use both skill patterns and MCP servers for comprehensive guidance\n\n## CDK Patterns Reference\n\nFor detailed CDK patterns, anti-patterns, and architectural guidance, refer to the comprehensive reference:\n\n**File**: `references/cdk-patterns.md`\n\nThis reference includes:\n- Common CDK patterns and their use cases\n- Anti-patterns to avoid\n- Security best practices\n- Cost optimization strategies\n- Performance considerations\n\n## Additional Resources\n\n- **Validation Script**: `scripts/validate-stack.sh` - Pre-deployment validation\n- **CDK Patterns**: `references/cdk-patterns.md` - Detailed pattern library\n- **AWS Documentation MCP**: Integrated for latest AWS information\n- **CDK MCP Server**: Integrated for CDK-specific guidance\n\n## GitHub Actions Integration\n\nWhen GitHub Actions workflow files exist in the repository, ensure all checks defined in `.github/workflows/` pass before committing. This prevents CI/CD failures and maintains code quality standards.\n",
        "skills/infrastructure-code-synthesis/references/cdk-patterns.md": "# AWS CDK Patterns and Best Practices\n\nThis reference provides detailed patterns, anti-patterns, and best practices for AWS CDK development.\n\n## Table of Contents\n\n- [Naming Conventions](#naming-conventions)\n- [Construct Patterns](#construct-patterns)\n- [Security Patterns](#security-patterns)\n- [Lambda Integration](#lambda-integration)\n- [Testing Patterns](#testing-patterns)\n- [Cost Optimization](#cost-optimization)\n- [Anti-Patterns](#anti-patterns)\n\n## Naming Conventions\n\n### Automatic Resource Naming (Recommended)\n\nLet CDK and CloudFormation generate unique resource names automatically:\n\n**Benefits**:\n- Enables multiple deployments in the same region/account\n- Supports parallel environments (dev, staging, prod)\n- Prevents naming conflicts\n- Allows stack cloning and testing\n\n**Example**:\n```typescript\n//  GOOD - Automatic naming\nconst bucket = new s3.Bucket(this, 'DataBucket', {\n  // No bucketName specified\n  encryption: s3.BucketEncryption.S3_MANAGED,\n});\n```\n\n### When Explicit Naming is Required\n\nSome scenarios require explicit names:\n- Resources referenced by external systems\n- Resources that must maintain consistent names across deployments\n- Cross-stack references requiring stable names\n\n**Pattern**: Use logical prefixes and environment suffixes\n```typescript\n// Only when absolutely necessary\nconst bucket = new s3.Bucket(this, 'DataBucket', {\n  bucketName: `${props.projectName}-data-${props.environment}`,\n});\n```\n\n## Construct Patterns\n\n### L3 Constructs (Patterns)\n\nPrefer high-level patterns that encapsulate best practices:\n\n```typescript\nimport * as patterns from 'aws-cdk-lib/aws-apigateway';\n\nnew patterns.LambdaRestApi(this, 'MyApi', {\n  handler: myFunction,\n  // Includes CloudWatch Logs, IAM roles, and API Gateway configuration\n});\n```\n\n### Custom Constructs\n\nCreate reusable constructs for repeated patterns:\n\n```typescript\nexport class ApiWithDatabase extends Construct {\n  public readonly api: apigateway.RestApi;\n  public readonly table: dynamodb.Table;\n\n  constructor(scope: Construct, id: string, props: ApiWithDatabaseProps) {\n    super(scope, id);\n\n    this.table = new dynamodb.Table(this, 'Table', {\n      partitionKey: { name: 'id', type: dynamodb.AttributeType.STRING },\n      billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,\n    });\n\n    const handler = new NodejsFunction(this, 'Handler', {\n      entry: props.handlerEntry,\n      environment: {\n        TABLE_NAME: this.table.tableName,\n      },\n    });\n\n    this.table.grantReadWriteData(handler);\n\n    this.api = new apigateway.LambdaRestApi(this, 'Api', {\n      handler,\n    });\n  }\n}\n```\n\n## Security Patterns\n\n### IAM Least Privilege\n\nUse grant methods instead of broad policies:\n\n```typescript\n//  GOOD - Specific grants\nconst table = new dynamodb.Table(this, 'Table', { /* ... */ });\nconst lambda = new lambda.Function(this, 'Function', { /* ... */ });\n\ntable.grantReadWriteData(lambda);\n\n//  BAD - Overly broad permissions\nlambda.addToRolePolicy(new iam.PolicyStatement({\n  actions: ['dynamodb:*'],\n  resources: ['*'],\n}));\n```\n\n### Secrets Management\n\nUse Secrets Manager for sensitive data:\n\n```typescript\nimport * as secretsmanager from 'aws-cdk-lib/aws-secretsmanager';\n\nconst secret = new secretsmanager.Secret(this, 'DbPassword', {\n  generateSecretString: {\n    secretStringTemplate: JSON.stringify({ username: 'admin' }),\n    generateStringKey: 'password',\n    excludePunctuation: true,\n  },\n});\n\n// Grant read access to Lambda\nsecret.grantRead(myFunction);\n```\n\n### VPC Configuration\n\nFollow VPC best practices:\n\n```typescript\nconst vpc = new ec2.Vpc(this, 'Vpc', {\n  maxAzs: 2,\n  natGateways: 1, // Cost optimization: use 1 for dev, 2+ for prod\n  subnetConfiguration: [\n    {\n      name: 'Public',\n      subnetType: ec2.SubnetType.PUBLIC,\n      cidrMask: 24,\n    },\n    {\n      name: 'Private',\n      subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS,\n      cidrMask: 24,\n    },\n    {\n      name: 'Isolated',\n      subnetType: ec2.SubnetType.PRIVATE_ISOLATED,\n      cidrMask: 24,\n    },\n  ],\n});\n```\n\n## Lambda Integration\n\n### NodejsFunction (TypeScript/JavaScript)\n\n```typescript\nimport { NodejsFunction } from 'aws-cdk-lib/aws-lambda-nodejs';\n\nconst fn = new NodejsFunction(this, 'Function', {\n  entry: 'src/handlers/process.ts',\n  handler: 'handler',\n  runtime: lambda.Runtime.NODEJS_20_X,\n  timeout: Duration.seconds(30),\n  memorySize: 512,\n  environment: {\n    TABLE_NAME: table.tableName,\n  },\n  bundling: {\n    minify: true,\n    sourceMap: true,\n    externalModules: ['@aws-sdk/*'], // Use AWS SDK from Lambda runtime\n  },\n});\n```\n\n### PythonFunction\n\n```typescript\nimport { PythonFunction } from '@aws-cdk/aws-lambda-python-alpha';\n\nconst fn = new PythonFunction(this, 'Function', {\n  entry: 'src/handlers',\n  index: 'process.py',\n  handler: 'handler',\n  runtime: lambda.Runtime.PYTHON_3_12,\n  timeout: Duration.seconds(30),\n  memorySize: 512,\n});\n```\n\n### Lambda Layers\n\nShare code across functions:\n\n```typescript\nconst layer = new lambda.LayerVersion(this, 'CommonLayer', {\n  code: lambda.Code.fromAsset('layers/common'),\n  compatibleRuntimes: [lambda.Runtime.NODEJS_20_X],\n  description: 'Common utilities',\n});\n\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  layers: [layer],\n});\n```\n\n## Testing Patterns\n\n### Snapshot Testing\n\n```typescript\nimport { Template } from 'aws-cdk-lib/assertions';\n\ntest('Stack creates expected resources', () => {\n  const app = new cdk.App();\n  const stack = new MyStack(app, 'TestStack');\n\n  const template = Template.fromStack(stack);\n  expect(template.toJSON()).toMatchSnapshot();\n});\n```\n\n### Fine-Grained Assertions\n\n```typescript\ntest('Lambda has correct environment', () => {\n  const app = new cdk.App();\n  const stack = new MyStack(app, 'TestStack');\n\n  const template = Template.fromStack(stack);\n\n  template.hasResourceProperties('AWS::Lambda::Function', {\n    Runtime: 'nodejs20.x',\n    Timeout: 30,\n    Environment: {\n      Variables: {\n        TABLE_NAME: { Ref: Match.anyValue() },\n      },\n    },\n  });\n});\n```\n\n### Resource Count Validation\n\n```typescript\ntest('Stack has correct number of functions', () => {\n  const app = new cdk.App();\n  const stack = new MyStack(app, 'TestStack');\n\n  const template = Template.fromStack(stack);\n  template.resourceCountIs('AWS::Lambda::Function', 3);\n});\n```\n\n## Cost Optimization\n\n### Right-Sizing Lambda\n\n```typescript\n// Development\nconst devFunction = new NodejsFunction(this, 'DevFunction', {\n  memorySize: 256, // Lower for dev\n  timeout: Duration.seconds(30),\n});\n\n// Production\nconst prodFunction = new NodejsFunction(this, 'ProdFunction', {\n  memorySize: 1024, // Higher for prod performance\n  timeout: Duration.seconds(10),\n  reservedConcurrentExecutions: 10, // Prevent runaway costs\n});\n```\n\n### DynamoDB Billing Modes\n\n```typescript\n// Development/Low Traffic\nconst devTable = new dynamodb.Table(this, 'DevTable', {\n  billingMode: dynamodb.BillingMode.PAY_PER_REQUEST,\n});\n\n// Production/Predictable Load\nconst prodTable = new dynamodb.Table(this, 'ProdTable', {\n  billingMode: dynamodb.BillingMode.PROVISIONED,\n  readCapacity: 5,\n  writeCapacity: 5,\n  autoScaling: { /* ... */ },\n});\n```\n\n### S3 Lifecycle Policies\n\n```typescript\nconst bucket = new s3.Bucket(this, 'DataBucket', {\n  lifecycleRules: [\n    {\n      id: 'MoveToIA',\n      transitions: [\n        {\n          storageClass: s3.StorageClass.INFREQUENT_ACCESS,\n          transitionAfter: Duration.days(30),\n        },\n        {\n          storageClass: s3.StorageClass.GLACIER,\n          transitionAfter: Duration.days(90),\n        },\n      ],\n    },\n    {\n      id: 'CleanupOldVersions',\n      noncurrentVersionExpiration: Duration.days(30),\n    },\n  ],\n});\n```\n\n## Anti-Patterns\n\n###  Hardcoded Values\n\n```typescript\n// BAD\nnew lambda.Function(this, 'Function', {\n  functionName: 'my-function', // Prevents multiple deployments\n  code: lambda.Code.fromAsset('lambda'),\n  handler: 'index.handler',\n  runtime: lambda.Runtime.NODEJS_20_X,\n});\n\n// GOOD\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  // Let CDK generate the name\n});\n```\n\n###  Overly Broad IAM Permissions\n\n```typescript\n// BAD\nfunction.addToRolePolicy(new iam.PolicyStatement({\n  actions: ['*'],\n  resources: ['*'],\n}));\n\n// GOOD\ntable.grantReadWriteData(function);\n```\n\n###  Manual Dependency Management\n\n```typescript\n// BAD - Manual bundling\nnew lambda.Function(this, 'Function', {\n  code: lambda.Code.fromAsset('lambda.zip'), // Pre-bundled manually\n  // ...\n});\n\n// GOOD - Let CDK handle it\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  // CDK handles bundling automatically\n});\n```\n\n###  Missing Environment Variables\n\n```typescript\n// BAD\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  // Table name hardcoded in Lambda code\n});\n\n// GOOD\nnew NodejsFunction(this, 'Function', {\n  entry: 'src/handler.ts',\n  environment: {\n    TABLE_NAME: table.tableName,\n  },\n});\n```\n\n###  Ignoring Stack Outputs\n\n```typescript\n// BAD - No way to reference resources\nnew MyStack(app, 'Stack', {});\n\n// GOOD - Export important values\nclass MyStack extends Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    const api = new apigateway.RestApi(this, 'Api', {});\n\n    new CfnOutput(this, 'ApiUrl', {\n      value: api.url,\n      description: 'API Gateway URL',\n      exportName: 'MyApiUrl',\n    });\n  }\n}\n```\n\n## Summary\n\n- **Always** let CDK generate resource names unless explicitly required\n- **Use** high-level constructs (L2/L3) over low-level (L1)\n- **Prefer** grant methods for IAM permissions\n- **Leverage** `NodejsFunction` and `PythonFunction` for automatic bundling\n- **Test** stacks with assertions and snapshots\n- **Optimize** costs based on environment (dev vs prod)\n- **Validate** infrastructure before deployment\n- **Document** custom constructs and patterns\n",
        "skills/interactive-component-creator/SKILL.md": "---\nname: interactive-component-creator\ndescription: Build interactive web components and artifacts. Creates interactive UI elements, visualizations, and web-based applications.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Web Artifacts Builder\n\nTo build powerful frontend claude.ai artifacts, follow these steps:\n1. Initialize the frontend repo using `scripts/init-artifact.sh`\n2. Develop your artifact by editing the generated code\n3. Bundle all code into a single HTML file using `scripts/bundle-artifact.sh`\n4. Display artifact to user\n5. (Optional) Test the artifact\n\n**Stack**: React 18 + TypeScript + Vite + Parcel (bundling) + Tailwind CSS + shadcn/ui\n\n## Design & Style Guidelines\n\nVERY IMPORTANT: To avoid what is often referred to as \"AI slop\", avoid using excessive centered layouts, purple gradients, uniform rounded corners, and Inter font.\n\n## Quick Start\n\n### Step 1: Initialize Project\n\nRun the initialization script to create a new React project:\n```bash\nbash scripts/init-artifact.sh <project-name>\ncd <project-name>\n```\n\nThis creates a fully configured project with:\n-  React + TypeScript (via Vite)\n-  Tailwind CSS 3.4.1 with shadcn/ui theming system\n-  Path aliases (`@/`) configured\n-  40+ shadcn/ui components pre-installed\n-  All Radix UI dependencies included\n-  Parcel configured for bundling (via .parcelrc)\n-  Node 18+ compatibility (auto-detects and pins Vite version)\n\n### Step 2: Develop Your Artifact\n\nTo build the artifact, edit the generated files. See **Common Development Tasks** below for guidance.\n\n### Step 3: Bundle to Single HTML File\n\nTo bundle the React app into a single HTML artifact:\n```bash\nbash scripts/bundle-artifact.sh\n```\n\nThis creates `bundle.html` - a self-contained artifact with all JavaScript, CSS, and dependencies inlined. This file can be directly shared in Claude conversations as an artifact.\n\n**Requirements**: Your project must have an `index.html` in the root directory.\n\n**What the script does**:\n- Installs bundling dependencies (parcel, @parcel/config-default, parcel-resolver-tspaths, html-inline)\n- Creates `.parcelrc` config with path alias support\n- Builds with Parcel (no source maps)\n- Inlines all assets into single HTML using html-inline\n\n### Step 4: Share Artifact with User\n\nFinally, share the bundled HTML file in conversation with the user so they can view it as an artifact.\n\n### Step 5: Testing/Visualizing the Artifact (Optional)\n\nNote: This is a completely optional step. Only perform if necessary or requested.\n\nTo test/visualize the artifact, use available tools (including other Skills or built-in tools like Playwright or Puppeteer). In general, avoid testing the artifact upfront as it adds latency between the request and when the finished artifact can be seen. Test later, after presenting the artifact, if requested or if issues arise.\n\n## Reference\n\n- **shadcn/ui components**: https://ui.shadcn.com/docs/components",
        "skills/knowledge-distribution/SKILL.md": "---\nname: knowledge-distribution\ndescription: Share and distribute skill knowledge and documentation. Publishes capabilities with examples, documentation, and integration guides.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n## When to use this skill\n\nUse this skill when you need to:\n- **Create new Claude skills** with proper structure and metadata\n- **Generate skill packages** ready for distribution\n- **Automatically share created skills** on Slack channels for team visibility\n- **Validate skill structure** before sharing\n- **Package and distribute** skills to your team\n\nAlso use this skill when:\n- **User says he wants to create/share his skill** \n\nThis skill is ideal for:\n- Creating skills as part of team workflows\n- Building internal tools that need skill creation + team notification\n- Automating the skill development pipeline\n- Collaborative skill creation with team notifications\n\n## Key Features\n\n### 1. Skill Creation\n- Creates properly structured skill directories with SKILL.md\n- Generates standardized scripts/, references/, and assets/ directories\n- Auto-generates YAML frontmatter with required metadata\n- Enforces naming conventions (hyphen-case)\n\n### 2. Skill Validation\n- Validates SKILL.md format and required fields\n- Checks naming conventions\n- Ensures metadata completeness before packaging\n\n### 3. Skill Packaging\n- Creates distributable zip files\n- Includes all skill assets and documentation\n- Runs validation automatically before packaging\n\n### 4. Slack Integration via Rube\n- Automatically sends created skill information to designated Slack channels\n- Shares skill metadata (name, description, link)\n- Posts skill summary for team discovery\n- Provides direct links to skill files\n\n## How It Works\n\n1. **Initialization**: Provide skill name and description\n2. **Creation**: Skill directory is created with proper structure\n3. **Validation**: Skill metadata is validated for correctness\n4. **Packaging**: Skill is packaged into a distributable format\n5. **Slack Notification**: Skill details are posted to your team's Slack channel\n\n## Example Usage\n\n```\nWhen you ask Claude to create a skill called \"pdf-analyzer\":\n1. Creates /skill-pdf-analyzer/ with SKILL.md template\n2. Generates structured directories (scripts/, references/, assets/)\n3. Validates the skill structure\n4. Packages the skill as a zip file\n5. Posts to Slack: \"New Skill Created: pdf-analyzer - Advanced PDF analysis and extraction capabilities\"\n```\n\n## Integration with Rube\n\nThis skill leverages Rube for:\n- **SLACK_SEND_MESSAGE**: Posts skill information to team channels\n- **SLACK_POST_MESSAGE_WITH_BLOCKS**: Shares rich formatted skill metadata\n- **SLACK_FIND_CHANNELS**: Discovers target channels for skill announcements\n\n## Requirements\n\n- Slack workspace connection via Rube\n- Write access to skill creation directory\n- Python 3.7+ for skill creation scripts\n- Target Slack channel for skill notifications\n",
        "skills/legal-document-analyzer/SKILL.md": "---\nname: legal-document-analyzer\ndescription: Build agents for legal document analysis, contract review, and compliance checking. Handles document parsing, risk identification, and legal research. Use when creating contract analysis tools, legal research assistants, compliance checkers, or document review systems.\n---\n\n# Legal Document Analyzer\n\nBuild intelligent legal document analysis agents that review contracts, identify risks, and provide legal insights.\n\n## Document Parsing\n\nSee [examples/legal_document_parser.py](examples/legal_document_parser.py) for `LegalDocumentParser`:\n- Parse PDF, DOCX, and text documents\n- Extract and structure document content\n- Identify sections, clauses, definitions, parties, and dates\n\n## Contract Analysis\n\nSee [examples/contract_analyzer.py](examples/contract_analyzer.py) for `ContractAnalyzer`:\n- Identify contract type (NDA, employment, service, purchase, lease)\n- Extract key terms (dates, renewal, payment, performance metrics)\n- Extract obligations, rights, and liabilities\n\n## Risk Identification\n\nSee [examples/contract_risk_analyzer.py](examples/contract_risk_analyzer.py) for `ContractRiskAnalyzer`:\n- Analyze financial risks (unlimited liability, price escalations)\n- Identify legal risks (ambiguous terms, conflicting clauses)\n- Assess operational risks\n- Check compliance risks (GDPR, data protection)\n\n## Legal Research\n\nBuild legal research capabilities to:\n- Find applicable laws for contract type and jurisdiction\n- Identify regulatory requirements\n- Research relevant case law\n- Identify standard practices and precedents\n\n## Compliance Checking\n\nBuild compliance checking capabilities to:\n- Verify contract compliance with regulations\n- Identify compliance gaps\n- Generate compliance recommendations\n- Track required clauses and documentation\n\n## Report Generation\n\nSee [examples/legal_report_generator.py](examples/legal_report_generator.py) for `LegalReportGenerator`:\n- Generate comprehensive contract analysis reports\n- Format key terms and obligations\n- Present risk assessments\n- Provide actionable recommendations\n\n## Best Practices\n\n### Document Review\n-  Review full document systematically\n-  Identify all parties and roles\n-  Understand key terms and conditions\n-  Identify potential risks\n-  Check for compliance\n\n### Risk Management\n-  Prioritize high-severity risks\n-  Negotiate unfavorable terms\n-  Seek legal counsel for major issues\n-  Document agreed changes\n-  Keep signed copies\n\n### Compliance\n-  Understand applicable regulations\n-  Ensure required clauses present\n-  Document compliance measures\n-  Review regulatory updates\n-  Train team on obligations\n\n## Tools & Resources\n\n### Document Processing\n- pypdf\n- python-docx\n- textract\n- OCR libraries\n\n### Legal Research\n- LexisNexis\n- Westlaw\n- Google Scholar\n- Free Law Project\n\n## Getting Started\n\n1. Upload contract document\n2. Parse document structure\n3. Identify contract type\n4. Extract key terms and obligations\n5. Analyze for risks\n6. Check compliance\n7. Generate report\n8. Provide recommendations\n\n",
        "skills/llm-fine-tuning-guide/README.md": "# LLM Fine-Tuning Guide - Code Structure\n\nThis skill uses supporting Python files to keep documentation lean and maintainable.\n\n## Directory Structure\n\n```\nllm-fine-tuning-guide/\n SKILL.md                    # Main documentation (concepts, best practices)\n README.md                   # This file\n examples/                   # Implementation examples\n    full_fine_tuning.py     # Full parameter fine-tuning\n    lora_fine_tuning.py     # LoRA implementation\n    qlora_fine_tuning.py    # QLoRA (single GPU)\n scripts/                    # Utility modules\n     data_preparation.py     # Dataset validation, augmentation, splitting\n     evaluation_metrics.py   # Perplexity, task metrics, evaluation utils\n```\n\n## Running Examples\n\n### 1. Full Fine-Tuning\n```bash\npython examples/full_fine_tuning.py\n```\nUpdates all model parameters. Requires powerful GPU.\n\n### 2. LoRA (Recommended)\n```bash\npython examples/lora_fine_tuning.py\n```\nParameter-efficient, ~99% fewer trainable parameters.\n\n### 3. QLoRA (Single Consumer GPU)\n```bash\npython examples/qlora_fine_tuning.py\n```\nQuantized LoRA for 7B models on 24GB GPU.\n\n## Using the Utilities\n\n### Data Preparation\n```python\nfrom scripts.data_preparation import DatasetValidator, create_splits, augment_data\n\nvalidator = DatasetValidator()\nissues = validator.validate_dataset(your_data)\nvalidator.print_issues(issues)\n\ntrain, val, test = create_splits(your_data)\n```\n\n### Evaluation Metrics\n```python\nfrom scripts.evaluation_metrics import calculate_perplexity, evaluate_task_metrics\n\nperplexity = calculate_perplexity(model, eval_dataset)\nmetrics = evaluate_task_metrics(predictions, ground_truth)\n```\n\n## Integration with SKILL.md\n\n- SKILL.md contains conceptual information and best practices\n- Code examples are in `examples/` for clarity\n- Utilities are in `scripts/` for reusability\n- This keeps token costs low while maintaining full functionality\n\n## Models Supported\n\n- Llama 3.2 (1B, 3B, 8B)\n- Gemma 3 (2B, 7B)\n- Mistral 7B\n- Any HuggingFace compatible model\n\n## Requirements\n\n```\ntorch>=2.0\ntransformers>=4.36\npeft>=0.7\ndatasets>=2.14\nscikit-learn>=1.3\n```\n\n## Next Steps\n\n1. Prepare your dataset using `scripts/data_preparation.py`\n2. Choose approach: Full, LoRA, or QLoRA\n3. Run corresponding example script\n4. Evaluate using `scripts/evaluation_metrics.py`\n5. See SKILL.md for detailed explanations and best practices\n",
        "skills/llm-fine-tuning-guide/SKILL.md": "---\nname: llm-fine-tuning-guide\ndescription: Master fine-tuning of large language models for specific domains and tasks. Covers data preparation, training techniques, optimization strategies, and evaluation methods. Use when adapting models for specialized applications, reducing inference costs, or improving domain-specific performance.\n---\n\n# LLM Fine-Tuning Guide\n\nMaster the art of fine-tuning large language models to create specialized models optimized for your specific use cases, domains, and performance requirements.\n\n## Overview\n\nFine-tuning adapts pre-trained LLMs to specific tasks, domains, or styles by training them on curated datasets. This improves accuracy, reduces hallucinations, and optimizes costs.\n\n### When to Fine-Tune\n\n- **Domain Specialization**: Legal documents, medical records, financial reports\n- **Task-Specific Performance**: Better results on specific tasks than base model\n- **Cost Optimization**: Smaller fine-tuned model replaces expensive large model\n- **Style Adaptation**: Match specific writing styles or tones\n- **Compliance Requirements**: Keep sensitive data within your infrastructure\n- **Latency Requirements**: Smaller models deploy faster\n\n### When NOT to Fine-Tune\n\n- One-off queries (use prompting instead)\n- Rapidly changing information (use RAG instead)\n- Limited training data (< 100 examples typically insufficient)\n- General knowledge questions (base model sufficient)\n\n## Quick Start\n\n**Full Fine-Tuning**:\n```bash\npython examples/full_fine_tuning.py\n```\n\n**LoRA (Recommended for most cases)**:\n```bash\npython examples/lora_fine_tuning.py\n```\n\n**QLoRA (Single GPU)**:\n```bash\npython examples/qlora_fine_tuning.py\n```\n\n**Data Preparation**:\n```bash\npython scripts/data_preparation.py\n```\n\n## Fine-Tuning Approaches\n\n### 1. Full Fine-Tuning\n\nUpdate all model parameters during training.\n\n**Pros**:\n- Maximum performance improvement\n- Can completely rewrite model behavior\n- Best for significant domain shifts\n\n**Cons**:\n- High computational cost\n- Requires large dataset (1000+ examples)\n- Risk of catastrophic forgetting\n- Long training time\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n\nmodel_id = \"meta-llama/Llama-2-7b\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./fine-tuned-llama\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_steps=10,\n    save_steps=100,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    load_best_model_at_end=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\ntrainer.train()\n```\n\n### 2. Parameter-Efficient Fine-Tuning (PEFT)\n\nTrain only a small fraction of parameters.\n\n#### LoRA (Low-Rank Adaptation)\n\nAdds trainable low-rank matrices to existing weights.\n\n**Pros**:\n- 99% fewer trainable parameters\n- Maintains base model knowledge\n- Fast training (10-20x faster)\n- Easy to switch between adapters\n\n**Cons**:\n- Slightly lower performance than full fine-tuning\n- Requires base model at inference\n\n```python\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nbase_model_id = \"meta-llama/Llama-2-7b\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id)\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=8,  # Rank of low-rank matrices\n    lora_alpha=16,  # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers to adapt\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\n# Wrap model with LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n# Output: trainable params: 4,194,304 || all params: 6,738,415,616 || trainable%: 0.06\n\n# Train as normal\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n)\ntrainer.train()\n\n# Save only LoRA weights\nmodel.save_pretrained(\"./llama-lora-adapter\")\n```\n\n#### QLoRA (Quantized LoRA)\n\nCombines LoRA with quantization for extreme efficiency.\n\n```python\nfrom peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n\n# Quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n    bnb_4bit_use_double_quant=True\n)\n\n# Load quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# Prepare for training\nmodel = prepare_model_for_kbit_training(model)\n\n# Apply LoRA\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Train on single GPU\ntrainer = Trainer(\n    model=model,\n    args=TrainingArguments(\n        output_dir=\"./qlora-output\",\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        learning_rate=5e-4,\n        num_train_epochs=3,\n    ),\n    train_dataset=train_dataset,\n)\ntrainer.train()\n```\n\n#### Prefix Tuning\n\nPrepends trainable tokens to input.\n\n```python\nfrom peft import get_peft_model, PrefixTuningConfig\n\nconfig = PrefixTuningConfig(\n    num_virtual_tokens=20,\n    task_type=TaskType.CAUSAL_LM,\n)\n\nmodel = get_peft_model(model, config)\n# Only 20 * embedding_dim parameters trained\n```\n\n### 3. Instruction Fine-Tuning\n\nTrain model to follow instructions with examples.\n\n```python\n# Training data format\ntraining_data = [\n    {\n        \"instruction\": \"Translate to French\",\n        \"input\": \"Hello, how are you?\",\n        \"output\": \"Bonjour, comment allez-vous?\"\n    },\n    {\n        \"instruction\": \"Summarize this text\",\n        \"input\": \"Long document...\",\n        \"output\": \"Summary...\"\n    }\n]\n\n# Template for training\ntemplate = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n{output}\"\"\"\n\n# Create formatted dataset\nformatted_data = [\n    template.format(**example) for example in training_data\n]\n```\n\n### 4. Domain-Specific Fine-Tuning\n\nTailor models for specific industries or fields.\n\n#### Legal Domain Example\n\n```python\nlegal_training_data = [\n    {\n        \"prompt\": \"What are the key clauses in an NDA?\",\n        \"completion\": \"\"\"Key clauses typically include:\n1. Definition of Confidential Information\n2. Non-Disclosure Obligations\n3. Permitted Disclosures\n4. Term and Termination\n5. Return of Information\n6. Remedies\"\"\"\n    },\n    # ... more legal examples\n]\n\n# Train on legal domain\nmodel = fine_tune_on_domain(\n    base_model=\"gpt-3.5-turbo\",\n    training_data=legal_training_data,\n    epochs=3,\n    learning_rate=0.0002,\n)\n```\n\n## Data Preparation\n\n### 1. Dataset Quality\n\n```python\nclass DatasetValidator:\n    def validate_dataset(self, data):\n        issues = {\n            \"empty_samples\": 0,\n            \"duplicates\": 0,\n            \"outliers\": 0,\n            \"imbalance\": {}\n        }\n\n        # Check for empty samples\n        for sample in data:\n            if not sample.get(\"text\"):\n                issues[\"empty_samples\"] += 1\n\n        # Check for duplicates\n        texts = [s.get(\"text\") for s in data]\n        issues[\"duplicates\"] = len(texts) - len(set(texts))\n\n        # Check for length outliers\n        lengths = [len(t.split()) for t in texts]\n        mean_length = sum(lengths) / len(lengths)\n        issues[\"outliers\"] = sum(1 for l in lengths if l > mean_length * 3)\n\n        return issues\n\n# Validate before training\nvalidator = DatasetValidator()\nissues = validator.validate_dataset(training_data)\nprint(f\"Dataset Issues: {issues}\")\n```\n\n### 2. Data Augmentation\n\n```python\nfrom nlpaug.augmenter.word import SynonymAug, RandomWordAug\nimport nlpaug.flow as naf\n\n# Create augmentation pipeline\ntext = \"The quick brown fox jumps over the lazy dog\"\n\n# Synonym replacement\naug_syn = SynonymAug(aug_p=0.3)\naugmented_syn = aug_syn.augment(text)\n\n# Random word insertion\naug_insert = RandomWordAug(action=\"insert\", aug_p=0.3)\naugmented_insert = aug_insert.augment(text)\n\n# Combine augmentations\nflow = naf.Sequential([\n    SynonymAug(aug_p=0.2),\n    RandomWordAug(action=\"swap\", aug_p=0.2)\n])\naugmented = flow.augment(text)\n```\n\n### 3. Train/Validation Split\n\n```python\nfrom sklearn.model_selection import train_test_split\n\n# Create splits\ntrain_data, eval_data = train_test_split(\n    data,\n    test_size=0.2,\n    random_state=42\n)\n\neval_data, test_data = train_test_split(\n    eval_data,\n    test_size=0.5,\n    random_state=42\n)\n\nprint(f\"Train: {len(train_data)}, Eval: {len(eval_data)}, Test: {len(test_data)}\")\n```\n\n## Training Techniques\n\n### 1. Learning Rate Scheduling\n\n```python\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n\n# Linear warmup + cosine annealing\ndef get_scheduler(optimizer, num_steps):\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=500,\n        num_training_steps=num_steps\n    )\n    return lr_scheduler\n\ntraining_args = TrainingArguments(\n    learning_rate=1e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=500,\n    warmup_ratio=0.1,\n)\n```\n\n### 2. Gradient Accumulation\n\n```python\ntraining_args = TrainingArguments(\n    gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n    per_device_train_batch_size=1,   # Effective batch size: 1 * 4 = 4\n)\n\n# Simulates larger batch on limited GPU memory\n```\n\n### 3. Mixed Precision Training\n\n```python\ntraining_args = TrainingArguments(\n    fp16=True,  # Use 16-bit floats\n    bf16=False,\n)\n\n# Reduces memory usage by 50%, speeds up training\n```\n\n### 4. Multi-GPU Training\n\n```python\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=4,\n    dataloader_pin_memory=True,\n    dataloader_num_workers=4,\n)\n\n# Automatically uses all available GPUs\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n```\n\n## Popular Models for Fine-Tuning\n\n### Open Source Models\n\n#### Llama 3.2 (Meta)\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-7b\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-7b\")\n\n# Fine-tune on custom data\n# ... training code\n```\n\n**Characteristics**:\n- 7B, 70B parameter versions\n- Strong instruction-following\n- Excellent for domain adaptation\n- Apache 2.0 license\n\n#### Gemma 3 (Google)\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-2b\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-2b\")\n\n# Gemma 3 sizes: 2B, 7B, 27B\n# Very efficient, great for fine-tuning\n```\n\n**Characteristics**:\n- Small, medium, large sizes\n- Efficient architecture\n- Good for edge deployment\n- Built on cutting-edge research\n\n#### Mistral 7B\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n\n# Strong performance, efficient architecture\n```\n\n**Characteristics**:\n- Sliding window attention\n- Efficient inference\n- Strong performance-to-size ratio\n\n### Commercial Models\n\n#### OpenAI Fine-Tuning API\n\n```python\nimport openai\n\n# Prepare training data\ntraining_file = openai.File.create(\n    file=open(\"training_data.jsonl\", \"rb\"),\n    purpose=\"fine-tune\"\n)\n\n# Create fine-tuning job\nfine_tune_job = openai.FineTuningJob.create(\n    training_file=training_file.id,\n    model=\"gpt-3.5-turbo\",\n    hyperparameters={\n        \"n_epochs\": 3,\n        \"learning_rate_multiplier\": 0.1,\n    }\n)\n\n# Wait for completion\nfine_tuned_model = openai.FineTuningJob.retrieve(fine_tune_job.id)\nprint(f\"Status: {fine_tuned_model.status}\")\n\n# Use fine-tuned model\nresponse = openai.ChatCompletion.create(\n    model=fine_tuned_model.fine_tuned_model,\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n```\n\n## Evaluation and Metrics\n\n### 1. Perplexity\n\n```python\nimport torch\nfrom math import exp\n\ndef calculate_perplexity(model, eval_dataset):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for batch in eval_dataset:\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.item() * batch[\"input_ids\"].shape[0]\n            total_tokens += batch[\"input_ids\"].shape[0]\n\n    perplexity = exp(total_loss / total_tokens)\n    return perplexity\n\nperplexity = calculate_perplexity(model, eval_dataset)\nprint(f\"Perplexity: {perplexity:.2f}\")\n```\n\n### 2. Task-Specific Metrics\n\n```python\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\ndef evaluate_task(predictions, ground_truth):\n    return {\n        \"accuracy\": accuracy_score(ground_truth, predictions),\n        \"precision\": precision_score(ground_truth, predictions, average='weighted'),\n        \"recall\": recall_score(ground_truth, predictions, average='weighted'),\n        \"f1\": f1_score(ground_truth, predictions, average='weighted'),\n    }\n\n# Evaluate on task\npredictions = [model.predict(x) for x in test_data]\nmetrics = evaluate_task(predictions, test_labels)\nprint(f\"Metrics: {metrics}\")\n```\n\n### 3. Human Evaluation\n\n```python\nclass HumanEvaluator:\n    def evaluate_response(self, prompt, response):\n        criteria = {\n            \"relevance\": self._score_relevance(prompt, response),\n            \"coherence\": self._score_coherence(response),\n            \"factuality\": self._score_factuality(response),\n            \"helpfulness\": self._score_helpfulness(response),\n        }\n        return sum(criteria.values()) / len(criteria)\n\n    def _score_relevance(self, prompt, response):\n        # Score 1-5\n        pass\n\n    def _score_coherence(self, response):\n        # Score 1-5\n        pass\n```\n\n## Common Challenges & Solutions\n\n### Challenge: Catastrophic Forgetting\n\nModel forgets pre-trained knowledge while adapting to new domain.\n\n**Solutions**:\n- Use lower learning rates (2e-5 to 5e-5)\n- Smaller training epochs (1-3)\n- Regularization techniques\n- Continual learning approaches\n\n```python\n# Conservative training settings\ntraining_args = TrainingArguments(\n    learning_rate=2e-5,  # Lower learning rate\n    num_train_epochs=2,   # Few epochs\n    weight_decay=0.01,    # L2 regularization\n    warmup_steps=500,\n    save_total_limit=3,\n    load_best_model_at_end=True,\n)\n```\n\n### Challenge: Overfitting\n\nModel performs well on training data but poorly on new data.\n\n**Solutions**:\n- Use more training data\n- Implement dropout\n- Early stopping\n- Validation monitoring\n\n```python\ntraining_args = TrainingArguments(\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    load_best_model_at_end=True,\n    early_stopping_patience=3,\n    metric_for_best_model=\"eval_loss\",\n)\n```\n\n### Challenge: Insufficient Training Data\n\nFew examples for fine-tuning.\n\n**Solutions**:\n- Data augmentation\n- Use PEFT (LoRA) instead of full fine-tuning\n- Few-shot learning with prompting\n- Transfer learning\n\n```python\n# Use LoRA when data is limited\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n)\n```\n\n## Best Practices\n\n### Before Fine-Tuning\n-  Start with a strong base model\n-  Prepare high-quality training data (100+ examples recommended)\n-  Define clear evaluation metrics\n-  Set up proper train/validation splits\n-  Document your objectives\n\n### During Fine-Tuning\n-  Monitor training/validation loss\n-  Use appropriate learning rates\n-  Save checkpoints regularly\n-  Validate on held-out data\n-  Watch for overfitting/underfitting\n\n### After Fine-Tuning\n-  Evaluate on test set\n-  Compare against baseline\n-  Perform qualitative analysis\n-  Document configuration and results\n-  Version your fine-tuned models\n\n## Implementation Checklist\n\n- [ ] Determine fine-tuning approach (full, LoRA, QLoRA, instruction)\n- [ ] Prepare and validate training dataset (100+ examples)\n- [ ] Choose base model (Llama 3.2, Gemma 3, Mistral, etc.)\n- [ ] Set up PEFT if using parameter-efficient methods\n- [ ] Configure training arguments and hyperparameters\n- [ ] Implement data loading and preprocessing\n- [ ] Set up evaluation metrics\n- [ ] Train model with monitoring\n- [ ] Evaluate on test set\n- [ ] Save and version fine-tuned model\n- [ ] Test in production environment\n- [ ] Document process and results\n\n## Resources\n\n### Frameworks\n- **Hugging Face Transformers**: https://huggingface.co/transformers/\n- **PEFT (Parameter-Efficient Fine-Tuning)**: https://github.com/huggingface/peft\n- **Hugging Face Datasets**: https://huggingface.co/datasets\n\n### Models\n- **Llama 3.2**: https://www.meta.com/llama/\n- **Gemma 3**: https://deepmind.google/technologies/gemma/\n- **Mistral**: https://mistral.ai/\n\n### Papers\n- \"LoRA: Low-Rank Adaptation of Large Language Models\" (Hu et al.)\n- \"QLoRA: Efficient Finetuning of Quantized LLMs\" (Dettmers et al.)\n\n",
        "skills/market-intelligence-gather/SKILL.md": "---\nname: market-intelligence-gather\ndescription: Gather and analyze competitive advertising campaigns. Extracts competitor ad content, messaging strategies, and market positioning information.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Competitive Ads Extractor\n\nThis skill extracts your competitors' ads from ad libraries and analyzes what's workingthe problems they're highlighting, use cases they're targeting, and copy/creative that's resonating.\n\n## When to Use This Skill\n\n- Researching competitor ad strategies\n- Finding inspiration for your own ads\n- Understanding market positioning\n- Identifying successful ad patterns\n- Analyzing messaging that works\n- Discovering new use cases or pain points\n- Planning ad campaigns with proven concepts\n\n## What This Skill Does\n\n1. **Extracts Ads**: Scrapes ads from Facebook Ad Library, LinkedIn, etc.\n2. **Captures Screenshots**: Saves visual copies of all ads\n3. **Analyzes Messaging**: Identifies problems, use cases, and value props\n4. **Categorizes Ads**: Groups by theme, audience, or format\n5. **Identifies Patterns**: Finds common successful approaches\n6. **Provides Insights**: Explains why certain ads likely perform well\n\n## How to Use\n\n### Basic Extraction\n\n```\nExtract all current ads from [Competitor Name] on Facebook Ad Library\n```\n\n```\nScrape ads from [Company] and analyze their messaging\n```\n\n### Specific Analysis\n\n```\nGet all ads from [Competitor] focusing on their messaging \nabout [specific problem]. What pain points are they highlighting?\n```\n\n### Competitive Set\n\n```\nExtract ads from these 5 competitors: [list]. \nCompare their approaches and tell me what's working.\n```\n\n### Specific Platform\n\n```\nGet LinkedIn ads from [Competitor] and analyze their \nB2B positioning strategy\n```\n\n## Example\n\n**User**: \"Extract ads from Notion on Facebook Ad Library and tell me what messaging is working for them.\"\n\n**Process**:\n```\nAccessing Facebook Ad Library...\nSearching for: Notion\nFound: 23 active ads\n\nExtracting screenshots...\n[] 100%\n\nSaved to: competitor-ads/notion/\n- ad-001-collaboration.png\n- ad-002-productivity.png\n- ad-003-templates.png\n...\n\nAnalyzing messaging...\n```\n\n**Output**:\n```\n# Notion Ad Analysis\n\n## Overview\n- Total Ads: 23 active\n- Primary Themes: Productivity (35%), Collaboration (30%), \n  Templates (20%), AI Features (15%)\n- Ad Formats: Static images (60%), Video (40%)\n- CTA Patterns: \"Try for free\", \"Get started\"\n\n## Key Problems They're Highlighting\n\n1. **Scattered Information** (8 ads)\n   Copy: \"Stop switching between 10 different tools\"\n   Why it works: Direct pain point many teams face\n\n2. **Meeting Overload** (5 ads)\n   Copy: \"Replace unnecessary meetings with async updates\"\n   Why it works: Post-COVID remote work pain point\n\n3. **Lost Documentation** (4 ads)\n   Copy: \"Never ask 'where is that doc?' again\"\n   Why it works: Universal workplace frustration\n\n## Successful Creative Patterns\n\n### Pattern 1: Before/After Split\n- Shows chaotic tool landscape  Clean Notion workspace\n- Used in 6 high-performing ads\n- Visual metaphor is immediately clear\n\n### Pattern 2: Feature Showcase\n- GIF of actual product usage\n- Shows specific feature in 5 seconds\n- Used for new features (AI, templates)\n\n### Pattern 3: Social Proof\n- \"Join 20M users\" messaging\n- Customer logos\n- Used in 4 ads targeting enterprise\n\n## Copy That's Working\n\nBest Headlines:\n1. \"Your team's knowledge, finally in one place\"\n    Benefit-focused, addresses pain directly\n   \n2. \"The all-in-one workspace\"\n    Clear positioning, broad appeal\n   \n3. \"AI that actually helps you work\"\n    Addresses AI skepticism, practical angle\n\nBest Body Copy Patterns:\n- Short sentences (under 10 words)\n- Focus on outcomes not features\n- Include specific numbers (\"Cut meetings by 50%\")\n\n## Audience Targeting Insights\n\nBased on ad variations:\n- Startup founders: Solo productivity angle\n- Team leads: Collaboration and alignment\n- Enterprise: Security and compliance mentions\n- Students: Free plan, templates, organization\n\n## Recommendations for Your Ads\n\n1. **Test the \"tool sprawl\" pain point**\n    Strong resonance based on their ad frequency\n\n2. **Use product screenshots over abstract visuals**\n    All their top ads show actual UI\n\n3. **Lead with the problem, not the solution**\n    \"Tired of X?\" performs better than \"Introducing Y\"\n\n4. **Keep copy under 100 characters**\n    Their shortest ads seem most frequent\n\n5. **Test before/after visual formats**\n    Proven pattern in their creative\n\n## Files Saved\n- All ads: ~/competitor-ads/notion/\n- Analysis: ~/competitor-ads/notion/analysis.md\n- Best performers: ~/competitor-ads/notion/top-10/\n```\n\n**Inspired by:** Sumant Subrahmanya's use case from Lenny's Newsletter\n\n## What You Can Learn\n\n### Messaging Analysis\n- What problems they emphasize\n- How they position against competition\n- Value propositions that resonate\n- Target audience segments\n\n### Creative Patterns\n- Visual styles that work\n- Video vs. static image performance\n- Color schemes and branding\n- Layout patterns\n\n### Copy Formulas\n- Headline structures\n- Call-to-action patterns\n- Length and tone\n- Emotional triggers\n\n### Campaign Strategy\n- Seasonal campaigns\n- Product launch approaches\n- Feature announcement tactics\n- Retargeting patterns\n\n## Best Practices\n\n### Legal & Ethical\n Only use for research and inspiration\n Don't copy ads directly\n Respect intellectual property\n Use insights to inform original creative\n Don't plagiarize copy or steal designs\n\n### Analysis Tips\n1. **Look for patterns**: What themes repeat?\n2. **Track over time**: Save ads monthly to see evolution\n3. **Test hypotheses**: Adapt successful patterns for your brand\n4. **Segment by audience**: Different messages for different targets\n5. **Compare platforms**: LinkedIn vs Facebook messaging differs\n\n## Advanced Features\n\n### Trend Tracking\n```\nCompare [Competitor]'s ads from Q1 vs Q2. \nWhat messaging has changed?\n```\n\n### Multi-Competitor Analysis\n```\nExtract ads from [Company A], [Company B], [Company C]. \nWhat are the common patterns? Where do they differ?\n```\n\n### Industry Benchmarks\n```\nShow me ad patterns across the top 10 project management \ntools. What problems do they all focus on?\n```\n\n### Format Analysis\n```\nAnalyze video ads vs static image ads from [Competitor]. \nWhich gets more engagement? (if data available)\n```\n\n## Common Workflows\n\n### Ad Campaign Planning\n1. Extract competitor ads\n2. Identify successful patterns\n3. Note gaps in their messaging\n4. Brainstorm unique angles\n5. Draft test ad variations\n\n### Positioning Research\n1. Get ads from 5 competitors\n2. Map their positioning\n3. Find underserved angles\n4. Develop differentiated messaging\n5. Test against their approaches\n\n### Creative Inspiration\n1. Extract ads by theme\n2. Analyze visual patterns\n3. Note color and layout trends\n4. Adapt successful patterns\n5. Create original variations\n\n## Tips for Success\n\n1. **Regular Monitoring**: Check monthly for changes\n2. **Broad Research**: Look at adjacent competitors too\n3. **Save Everything**: Build a reference library\n4. **Test Insights**: Run your own experiments\n5. **Track Performance**: A/B test inspired concepts\n6. **Stay Original**: Use for inspiration, not copying\n7. **Multiple Platforms**: Compare Facebook, LinkedIn, TikTok, etc.\n\n## Output Formats\n\n- **Screenshots**: All ads saved as images\n- **Analysis Report**: Markdown summary of insights\n- **Spreadsheet**: CSV with ad copy, CTAs, themes\n- **Presentation**: Visual deck of top performers\n- **Pattern Library**: Categorized by approach\n\n## Related Use Cases\n\n- Writing better ad copy for your campaigns\n- Understanding market positioning\n- Finding content gaps in your messaging\n- Discovering new use cases for your product\n- Planning product marketing strategy\n- Inspiring social media content\n\n",
        "skills/media-retrieval/SKILL.md": "---\nname: media-retrieval\ndescription: Download and retrieve video content from sources. Extracts media files with proper format handling and metadata preservation.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Video Downloader\n\nThis skill downloads videos from YouTube and other platforms directly to your computer.\n\n## When to Use This Skill\n\n- Downloading YouTube videos for offline viewing\n- Saving educational content for reference\n- Archiving important videos\n- Getting video files for editing or repurposing\n- Downloading your own content from platforms\n- Saving conference talks or webinars\n\n## What This Skill Does\n\n1. **Downloads Videos**: Fetches videos from YouTube and other platforms\n2. **Quality Selection**: Lets you choose resolution (480p, 720p, 1080p, 4K)\n3. **Format Options**: Downloads in various formats (MP4, WebM, audio-only)\n4. **Batch Downloads**: Can download multiple videos or playlists\n5. **Metadata Preservation**: Saves title, description, and thumbnail\n\n## How to Use\n\n### Basic Download\n\n```\nDownload this YouTube video: https://youtube.com/watch?v=...\n```\n\n```\nDownload this video in 1080p quality\n```\n\n### Audio Only\n\n```\nDownload the audio from this YouTube video as MP3\n```\n\n### Playlist Download\n\n```\nDownload all videos from this YouTube playlist: [URL]\n```\n\n### Batch Download\n\n```\nDownload these 5 YouTube videos:\n1. [URL]\n2. [URL]\n...\n```\n\n## Example\n\n**User**: \"Download this YouTube video: https://youtube.com/watch?v=abc123\"\n\n**Output**:\n```\nDownloading from YouTube...\n\nVideo: \"How to Build Products Users Love\"\nChannel: Lenny's Podcast\nDuration: 45:32\nQuality: 1080p\n\nProgress:  100%\n\n Downloaded: how-to-build-products-users-love.mp4\n Saved thumbnail: how-to-build-products-users-love.jpg\n Size: 342 MB\n\nSaved to: ~/Downloads/\n```\n\n**Inspired by:** Lenny's workflow from his newsletter\n\n## Important Notes\n\n **Copyright & Fair Use**\n- Only download videos you have permission to download\n- Respect copyright laws and platform terms of service\n- Use for personal, educational, or fair use purposes\n- Don't redistribute copyrighted content\n\n## Tips\n\n- Specify quality if you need lower file size (720p vs 1080p)\n- Use audio-only for podcasts or music to save space\n- Download to a dedicated folder to stay organized\n- Check file size before downloading on slow connections\n\n## Common Use Cases\n\n- **Education**: Save tutorials and courses for offline learning\n- **Research**: Archive videos for reference\n- **Content Creation**: Download your own content from platforms\n- **Backup**: Save important videos before they're removed\n- **Offline Viewing**: Watch videos without internet access\n\n",
        "skills/media-transformation/SKILL.md": "---\nname: media-transformation\ndescription: Transform and format media content creatively. Applies stylization and formatting transformations to various media types.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# TOON v2.0 Formatter Skill (AGGRESSIVE MODE)\n\n## Purpose\n\n**AGGRESSIVELY** apply TOON v2.0 format to save 30-60% tokens on structured data. Use TOON **by default** for biggish, regular data. Use native Zig encoder for 20x performance.\n\n## When to Use (AGGRESSIVE)\n\n**TOON ALL DAY** - Use automatically for:\n-  Arrays with  5 similar items\n-  Tables, logs, events, transactions, analytics\n-  API responses with uniform structure (60% field overlap)\n-  Database query results\n-  Repeatedly-used, structured data in prompts\n-  RAG pipelines, tool calls, agents passing data around\n-  Benchmarks/evals where prompt size = money\n-  Shape is more important than labels\n-  You know what each column means\n-  Can declare headers once, go row-by-row\n\n**MAYBE, BUT NOT AUTOMATICALLY** - Be selective when:\n-  Human collaborators reading/editing data a lot\n-  APIs/tools expect JSON (use JSON on wire, TOON in prompts)\n-  Structure is uneven (many optional keys, weird nesting)\n\n**NO, JUST DON'T** - Stick to JSON/text for:\n-  Short arrays (< 5 items)\n-  One-off examples in docs\n-  Narrative text, instructions, essays\n-  Deep, irregular trees where hierarchy matters\n\n## What is TOON v2.0?\n\n**TOON (Token-Oriented Object Notation) v2.0** reduces token consumption by 30-60% for structured data:\n\n### Three Array Types\n\n**1. Tabular** (uniform objects 5 items):\n```\n[2]{id,name,balance}:\n  1,Alice,5420.50\n  2,Bob,3210.75\n```\n\n**2. Inline** (primitives 10):\n```\ntags[5]: javascript,react,node,express,api\n```\n\n**3. Expanded** (non-uniform):\n```\n- name: Alice\n  role: admin\n- name: Bob\n  level: 5\n```\n\n### Three Delimiters\n\n**Comma** (default, most compact):\n```\n[2]{name,city}: Alice,NYC Bob,LA\n```\n\n**Tab** (for data with commas):\n```\n[2\\t]{name,address}: Alice\t123 Main St, NYC\n```\n\n**Pipe** (markdown-like):\n```\n[2|]{method,path}: GET|/api/users\n```\n\n### Key Folding\n\n**Flatten nested objects** (25-35% extra savings):\n```\nserver.host: localhost\nserver.port: 8080\ndatabase.host: db.example.com\n```\n\n## Process\n\n### 1. Detect Suitable Data\n\nWhen encountering array data, check if it meets TOON criteria:\n-  Array with 5 items\n-  Objects with 60% field uniformity (most objects share same fields)\n-  Flat or moderately nested structure\n\n**How to check uniformity:**\n1. Extract all field names from all objects\n2. Count how many objects have the most common set of fields\n3. Calculate: `(objects with common fields / total objects)  100`\n4. If 60%, uniformity is good for TOON\n\n### 2. Estimate Token Savings\n\n**Quick estimation method:**\n- **JSON tokens**  `(item count  field count  4) + overhead`\n  - Example: 10 items  5 fields  4 = ~200 tokens\n- **TOON tokens**  `20 (header) + (item count  field count  2)`\n  - Example: 20 + (10  5  2) = ~120 tokens\n- **Savings**  `(JSON - TOON) / JSON  100%`\n  - Example: (200 - 120) / 200 = 40% savings\n\n### 3. Apply TOON v2.0 Aggressively\n\n**AGGRESSIVE MODE: Use Zig encoder for optimal results**\n\nIf data meets criteria:\n\n**Method 1: Use Zig Encoder** (Recommended - 20x faster):\n```bash\n.claude/utils/toon/zig-out/bin/toon encode data.json \\\n  --delimiter tab \\\n  --key-folding \\\n  > data.toon\n```\n\n**Method 2: Manual TOON** (for inline generation):\n1. Detect array type (inline/tabular/expanded)\n2. Choose delimiter (comma/tab/pipe)\n3. Apply key folding if nested objects\n4. Build TOON header: `[N]{fields}:` or `key[N]: values`\n5. Output formatted TOON\n\nShow brief summary:\n```\n Using TOON v2.0 (estimated 42% savings, 10 items)\nFormat: Tabular with tab delimiter\nKey folding: enabled\n\n[10\\t]{id,name,address,status}:\n  1\tAlice\t123 Main St, NYC\tactive\n  2\tBob\t456 Oak Ave, LA\tinactive\n  ...\n```\n\nOtherwise, use JSON and explain why:\n```\nUsing JSON because:\n- Only 3 items (too small)\n- Or: Low uniformity (only 45% have same fields)\n- Or: Deeply nested structure\n```\n\n### 4. Show Formatted Output\n\n**Immediately show in TOON with brief explanation:**\n\n```\n API Endpoints (15 items, TOON format - saved 40.1% tokens):\n\n[15]{method,path,description,auth,rateLimit}:\n  GET,/api/users,List all users,required,100/min\n  POST,/api/users,Create new user,required,50/min\n  ...\n```\n\n**No long explanations needed - just use it!**\n\n## Best Practices\n\n### Always Show Format Decision\n\nWhen working with data, always indicate which format you're using and why:\n\n```\n Using TOON (estimated 42% token savings)\n[count]{fields}:...\n```\n\nor\n\n```\n Using JSON (deeply nested structure, TOON not suitable)\n{...}\n```\n\n### When NOT to Use TOON\n\nExplain when JSON is better:\n\n- **Deeply nested data**: TOON excels with flat/tabular structures\n- **Low uniformity** (<70%): Different fields per object reduces TOON benefits\n- **Small arrays** (<10 items): Overhead not worth it\n- **Single records**: Use JSON for clarity\n\n### Performance Tips\n\n1. **Batch processing**: Convert large datasets once, reuse TOON format\n2. **Uniformity matters**: Higher field overlap = better savings\n3. **Nested arrays**: TOON supports nested structures but savings diminish\n4. **Type coercion**: TOON preserves types (numbers, booleans, dates)\n\n## Examples\n\n### Example 1: API Response\n\n**Input:**\n```javascript\nconst apiResponse = await fetch('/api/users').then(r => r.json());\n// 150 users returned\n```\n\n**Output:**\n```\nAPI Response (150 users):\nFormat: TOON (saves 41.2%, 1,847 tokens saved)\n\n[150]{id,name,email,role,lastLogin}:\n  1,Alice Smith,alice@example.com,admin,2024-01-15T10:30:00Z\n  2,Bob Jones,bob@example.com,user,2024-01-15T09:15:00Z\n  ...\n```\n\n### Example 2: Database Query\n\n**Input:**\n```sql\nSELECT * FROM transactions WHERE date > '2024-01-01' LIMIT 500;\n```\n\n**Output:**\n```\nQuery Results (500 transactions):\nFormat: TOON (saves 38.9%, 3,421 tokens saved)\n\n[500]{id,date,amount,merchant,category,status}:\n  1,2024-01-15,42.50,Starbucks,Food & Drink,completed\n  2,2024-01-15,125.00,Shell Gas,Transportation,completed\n  ...\n```\n\n### Example 3: Metrics Dashboard\n\n**Input:**\n```typescript\nconst metrics = analyzePerformance();\n// Returns array of route performance metrics\n```\n\n**Output:**\n```\nPerformance Metrics (45 routes):\nFormat: TOON (saves 44.1%, 892 tokens saved)\n\n[45]{route,avgTime,p95,requests,errors}:\n  /api/users,125,340,15420,3\n  /api/products,89,210,8932,0\n  /api/orders,450,1250,3421,12\n  ...\n```\n\n### Example 4: Mixed Data (JSON Better)\n\n**Input:**\n```javascript\nconst complexData = {\n  metadata: { version: '1.0', timestamp: '...' },\n  results: [\n    { id: 1, data: { nested: { deep: 'value' } } },\n    // Highly nested, non-uniform\n  ]\n};\n```\n\n**Output:**\n```\n Using JSON format (TOON not recommended):\n- Deeply nested structure\n- Low uniformity (45%)\n- Small array (only 5 items)\n\n{\n  \"metadata\": { \"version\": \"1.0\", ... },\n  \"results\": [ ... ]\n}\n```\n\n## Integration with Other Skills\n\n### Financial Analysis\nWhen analyzing transactions or financial data, use TOON for large result sets:\n- Transaction histories (100+ items)\n- Account balances across multiple accounts\n- Payment logs and audit trails\n\n### Data Export\nWhen exporting data, check if TOON is suitable:\n- If 5 items and 60% uniform  use TOON\n- Otherwise  use JSON\n\n### API Documentation\nDocument API endpoints in TOON format for compact reference:\n\n```\n# API Endpoints\n\n[15]{method,path,auth,rateLimit,description}:\n  GET,/api/users,required,100/min,List all users\n  POST,/api/users,required,50/min,Create new user\n  ...\n```\n\n## Commands\n\nUse with these TOON v2.0 commands:\n- `/toon-encode <file> [--delimiter tab] [--key-folding]` - JSON  TOON v2.0\n- `/toon-validate <file> [--strict]` - Validate TOON file\n- `/analyze-tokens <file>` - Compare JSON vs TOON savings\n- `/convert-to-toon <file>` - Legacy command (use toon-encode)\n\n## Resources\n\n- **Zig Encoder**: `.claude/utils/toon/toon.zig` (601 lines, 20x faster)\n- **User Guide**: `.claude/docs/toon-guide.md`\n- **Examples**: `.claude/utils/toon/examples/` (9 files)\n- **Guides**: `.claude/utils/toon/guides/` (4 files)\n- **FAQ**: `.claude/docs/FAQ.md`\n- **TOON Spec**: https://github.com/toon-format/spec\n- **Official Site**: https://toonformat.dev\n\n## Success Metrics\n\nTrack TOON usage effectiveness:\n- Average token savings: 30-60%\n- Accuracy improvement: +3-5% (per official benchmarks)\n- Context window freed: 15K+ tokens on large datasets\n- User satisfaction: Faster responses, more context available",
        "skills/meeting-record-system/SKILL.md": "---\nname: meeting-record-system\ndescription: Record and manage meeting notes with intelligence. Organizes meeting documentation with action items, decisions, and follow-ups.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Meeting Intelligence\n\nPrepares you for meetings by gathering context from Notion, enriching it with Claude research, and creating comprehensive meeting materials. Generates both an internal pre-read for attendees and an external-facing agenda for the meeting itself.\n\n## Quick Start\n\nWhen asked to prep for a meeting:\n\n1. **Gather Notion context**: Use `Notion:notion-search` to find related pages\n2. **Fetch details**: Use `Notion:notion-fetch` to read relevant content\n3. **Enrich with research**: Use Claude's knowledge to add context, industry insights, or best practices\n4. **Create internal pre-read**: Use `Notion:notion-create-pages` for background context document (for attendees)\n5. **Create external agenda**: Use `Notion:notion-create-pages` for meeting agenda (shared with all participants)\n6. **Link resources**: Connect both docs to related projects and each other\n\n## Meeting Prep Workflow\n\n### Step 1: Understand meeting context\n\n```\nCollect meeting details:\n- Meeting topic/title\n- Attendees (internal team + external participants)\n- Meeting purpose (decision, brainstorm, status update, customer demo, etc.)\n- Meeting type (internal only vs. external participants)\n- Related project/initiative\n- Specific topics to cover\n```\n\n### Step 2: Search for Notion context\n\n```\nUse Notion:notion-search to find:\n- Project pages related to meeting topic\n- Previous meeting notes\n- Specifications or design docs\n- Related tasks or issues\n- Recent updates or reports\n- Customer/partner information (if applicable)\n\nSearch strategies:\n- Topic-based: \"mobile app redesign\"\n- Project-scoped: search within project teamspace\n- Attendee-created: filter by created_by_user_ids\n- Recent updates: use created_date_range filters\n```\n\n### Step 3: Fetch and analyze Notion content\n\n```\nFor each relevant page:\n1. Fetch with Notion:notion-fetch\n2. Extract key information:\n   - Project status and timeline\n   - Recent decisions and updates\n   - Open questions or blockers\n   - Relevant metrics or data\n   - Action items from previous meetings\n3. Note gaps in information\n```\n\n### Step 4: Enrich with Claude research\n\n```\nBeyond Notion context, add value through:\n\nFor technical meetings:\n- Explain complex concepts for broader audience\n- Summarize industry best practices\n- Provide competitive context\n- Suggest discussion frameworks\n\nFor customer meetings:\n- Research company background (if public info)\n- Industry trends relevant to discussion\n- Common pain points in their sector\n- Best practices for similar customers\n\nFor decision meetings:\n- Decision-making frameworks\n- Risk analysis patterns\n- Trade-off considerations\n- Implementation best practices\n\nNote: Use general knowledge only - don't fabricate specific facts\n```\n\n### Step 5: Create internal pre-read\n\n```\nUse Notion:notion-create-pages for internal doc:\n\nTitle: \"[Meeting Topic] - Pre-Read (Internal)\"\n\nContent structure:\n- **Meeting Overview**: Date, time, attendees, purpose\n- **Background Context**:\n  - What this meeting is about (2-3 sentences)\n  - Why it matters (business context)\n  - Links to related Notion pages\n- **Current Status**:\n  - Where we are now (from Notion content)\n  - Recent updates and progress\n  - Key metrics or data\n- **Context & Insights** (from Claude research):\n  - Industry context or best practices\n  - Relevant considerations\n  - Potential approaches to discuss\n- **Key Discussion Points**:\n  - Topics that need airtime\n  - Open questions to resolve\n  - Decisions required\n- **What We Need from This Meeting**:\n  - Expected outcomes\n  - Decisions to make\n  - Next steps to define\n\nAudience: Internal attendees only\nPurpose: Give team full context and alignment before meeting\n```\n\n### Step 6: Create external agenda\n\n```\nUse Notion:notion-create-pages for meeting doc:\n\nTitle: \"[Meeting Topic] - Agenda\"\n\nContent structure:\n- **Meeting Details**: Date, time, attendees\n- **Objective**: Clear meeting goal (1-2 sentences)\n- **Agenda Items** (with time allocations):\n  1. Topic 1 (10 min)\n  2. Topic 2 (20 min)\n  3. Topic 3 (15 min)\n- **Discussion Topics**:\n  - Key items to cover\n  - Questions to answer\n- **Decisions Needed**:\n  - Clear decision points\n- **Action Items**:\n  - (To be filled during meeting)\n- **Related Resources**:\n  - Links to relevant pages\n  - Link to pre-read document\n\nAudience: All participants (internal + external)\nPurpose: Structure the meeting, keep it on track\nTone: Professional, focused, clear\n```\n\nSee [reference/template-selection-guide.md](reference/template-selection-guide.md) for full templates.\n\n### Step 7: Link documents\n\n```\n1. Link pre-read to agenda:\n   - Add mention in agenda: \"See <mention-page>Pre-Read</mention-page> for background\"\n\n2. Link both to project:\n   - Update project page with meeting links\n   - Add to \"Meetings\" section\n\n3. Cross-reference:\n   - Agenda mentions pre-read for internal attendees\n   - Pre-read mentions agenda for meeting structure\n```\n\n## Document Types\n\n### Internal Pre-Read (for team)\n\nMore comprehensive, internal context:\n\n- Full background and history\n- Internal metrics and data\n- Honest assessment of challenges\n- Strategic considerations\n- What we need to achieve\n- Internal discussion points\n\n**When to create**: Always for important meetings with internal team\n\n### External Agenda (for all participants)\n\nClean, professional, focused:\n\n- Clear objectives\n- Structured agenda with times\n- Discussion topics\n- Decision items\n- Professional tone\n\n**When to create**: Every meeting\n\n### Agenda Types by Meeting Purpose\n\n**Decision Meeting**: Meeting Details  Objective  Options (Pros/Cons)  Recommendation  Discussion  Decision  Action Items\n\n**Status Update**: Meeting Details  Project Status  Progress  Upcoming Work  Blockers  Discussion  Action Items\n\n**Customer/External**: Meeting Details  Objective  Agenda Items (timed)  Discussion Topics  Next Steps\n\n**Brainstorming**: Meeting Details  Objective  Constraints  Ideas  Discussion  Next Steps\n\nSee [reference/template-selection-guide.md](reference/template-selection-guide.md) for complete templates.\n\n## Research Enrichment Patterns\n\nBeyond Notion content, add value through Claude's capabilities:\n\n**Technical Context**: Explain technologies, architectures, or approaches. Provide industry standard practices. Compare common solutions. Suggest evaluation criteria.\n\n**Business Context**: Industry trends affecting topic. Competitive landscape insights. Common challenges in space. ROI considerations.\n\n**Decision Support**: Decision-making frameworks (e.g., RICE, cost-benefit). Risk assessment patterns. Trade-off analysis approaches. Success criteria suggestions.\n\n**Customer Context** (for external meetings): Industry-specific challenges. Common pain points. Best practices from similar companies. Value proposition framing.\n\n**Process Guidance**: Meeting facilitation techniques. Discussion frameworks. Retrospective patterns. Brainstorming structures.\n\nNote: Use general knowledge and analytical capabilities. Don't fabricate specific facts. Clearly distinguish Notion facts from Claude insights.\n\n## Meeting Context Sources\n\n**Project Pages**: Status, goals, team, timelines (most important)\n**Previous Meeting Notes**: Historical discussions, action items, decisions (recurring meetings)\n**Task/Issue Database**: Current status, blockers, completed/upcoming work (project meetings)\n**Specifications/Designs**: Requirements, decisions, approach, open questions (technical meetings)\n**Reports/Dashboards**: Metrics, KPIs, performance data, trends (executive meetings)\n\n## Linking Meetings to Projects\n\n**Forward Link**: Add meeting to project page's \"Meetings\" section\n**Backward Link**: Include \"Related Project\" section in agenda with project mention\n**Maintain bidirectional** links for easy navigation\n\n## Meeting Series Management\n\n**Recurring Meetings**: Create series parent page with schedule, meeting notes list, standing agenda, and action items tracker. Link individual meetings to parent.\n\n**Meeting Database**: For organizations, use database with properties: Meeting Title, Date, Type (Decision/Status/Brainstorm), Project, Attendees, Status (Scheduled/Completed)\n\n## Post-Meeting Actions\n\nUpdate agenda with:\n\n**Decisions**: List each decision with rationale and owner\n**Action Items**: Checkbox list with owner and due date (consider creating tasks in database)\n**Key Outcomes**: Bullet list of main outcomes\n\n## Meeting Prep Timing\n\n**Day-Before** (next-day meetings): Gather context  create agenda  share with attendees  allow review time\n**Hour-Before** (last-minute): Quick context  brief pre-read  basic agenda  essentials only\n**Week-Before** (major meetings): Comprehensive research  detailed pre-read  structured agenda  pre-meeting reviews\n\n## Best Practices\n\n1. **Create both documents**: Internal pre-read + external agenda for important meetings\n2. **Distinguish sources**: Label what's from Notion vs. Claude research\n3. **Start with search**: Cast wide net in Notion, then narrow\n4. **Keep pre-read concise**: 2-3 pages maximum, even with research\n5. **Professional external docs**: Agenda should be polished and focused\n6. **Enrich thoughtfully**: Claude research should add real value, not fluff\n7. **Link documents**: Pre-read mentions agenda, agenda mentions pre-read\n8. **Include metrics**: Data from Notion helps ground discussions\n9. **Share appropriately**: Pre-read to internal team, agenda to all participants\n10. **Share early**: Give attendees time to review (24hr+ for important meetings)\n11. **Update post-meeting**: Capture decisions and actions in agenda\n\n## Advanced Features\n\n**Meeting templates**: See [reference/template-selection-guide.md](reference/template-selection-guide.md) for comprehensive template library\n\n## Common Issues\n\n**\"Too much context\"**: Split into pre-read (internal, comprehensive) and agenda (external, focused)\n**\"Can't find relevant pages\"**: Broaden search, try different terms, ask user for page URLs\n**\"Meeting purpose unclear\"**: Ask user to clarify before proceeding\n**\"No recent updates\"**: Note that in pre-read, focus on historical context and strategic considerations\n**\"External meeting - no internal context\"**: Create simpler structure with just agenda, skip internal pre-read or keep it minimal\n**\"Claude research too generic\"**: Focus on specific insights relevant to the actual meeting topic, not general platitudes\n\n## Examples\n\nSee [examples/](examples/) for complete workflows:\n\n- [examples/project-decision.md](examples/project-decision.md) - Decision meeting prep with pre-read\n- [examples/sprint-planning.md](examples/sprint-planning.md) - Sprint planning meeting\n- [examples/executive-review.md](examples/executive-review.md) - Executive review prep\n- [examples/customer-meeting.md](examples/customer-meeting.md) - External meeting with customer (pre-read + agenda)\n",
        "skills/meeting-record-system/evaluations/README.md": "# Meeting Intelligence Skill Evaluations\n\nEvaluation scenarios for testing the Meeting Intelligence skill across different Claude models.\n\n## Purpose\n\nThese evaluations ensure the Meeting Intelligence skill:\n\n- Gathers context from Notion workspace\n- Enriches with Claude research appropriately\n- Creates both internal pre-reads and external agendas\n- Distinguishes between Notion facts and Claude insights\n- Works consistently across Haiku, Sonnet, and Opus\n\n## Evaluation Files\n\n### decision-meeting-prep.json\n\nTests preparation for a decision-making meeting.\n\n**Scenario**: Prep for database migration decision meeting  \n**Key Behaviors**:\n\n- Searches Notion for migration context (specs, discussions, options)\n- Fetches 2-3 relevant pages\n- Enriches with Claude research (decision frameworks, migration best practices)\n- Creates comprehensive internal pre-read with recommendation\n- Creates clean, professional external agenda\n- Clearly distinguishes Notion facts from Claude insights\n- Cross-links both documents\n\n### status-meeting-prep.json\n\nTests preparation for a status update or review meeting.\n\n**Scenario**: Prep for project status review  \n**Key Behaviors**:\n\n- Gathers project metrics and progress from Notion\n- Fetches relevant pages (roadmap, tasks, milestones)\n- Adds Claude context (industry benchmarks, best practices)\n- Creates internal pre-read with honest assessment\n- Creates external agenda with structured flow\n- Includes source citations using mention-page tags\n- Time-boxes agenda items\n\n## Running Evaluations\n\n1. Enable the `meeting-intelligence` skill\n2. Submit the query from the evaluation file\n3. Verify the skill searches Notion first (not Claude research)\n4. Check that TWO documents are created (internal + external)\n5. Verify Claude enrichment adds value without replacing Notion content\n6. Test with Haiku, Sonnet, and Opus\n\n## Expected Skill Behaviors\n\nMeeting Intelligence evaluations should verify:\n\n### Notion Context Gathering\n\n- Searches workspace for relevant context first\n- Fetches specific pages (not generic)\n- Extracts key information from Notion content\n- Cites sources using mention-page tags\n\n### Claude Research Integration\n\n- Adds industry context, frameworks, or best practices\n- Enrichment is relevant and valuable (not filler)\n- Clearly distinguishes Notion facts from Claude insights\n- Research complements (doesn't replace) Notion content\n\n### Two-Document Creation\n\n- **Internal Pre-Read**: Comprehensive, includes strategy, recommendations, detailed pros/cons\n- **External Agenda**: Professional, focused on meeting flow, no internal strategy\n- Both documents are clearly labeled\n- Documents are cross-linked\n\n### Document Quality\n\n- Pre-read follows structure: Overview  Background  Current Status  Context & Insights  Discussion Points\n- Agenda follows structure: Details  Objective  Agenda Items (with times)  Decisions  Actions  Resources\n- Titles include date or meeting context\n- Content is actionable and meeting-ready\n\n## Creating New Evaluations\n\nWhen adding Meeting Intelligence evaluations:\n\n1. **Test different meeting types** - Decision, status, brainstorm, 1:1, sprint planning, retrospective\n2. **Vary complexity** - Simple updates vs. complex strategic decisions\n3. **Test with/without Notion content** - Rich workspace vs. minimal existing pages\n4. **Verify enrichment value** - Is Claude research genuinely helpful?\n5. **Check internal/external distinction** - Is sensitive info kept in pre-read only?\n\n## Example Success Criteria\n\n**Good** (specific, testable):\n\n- \"Creates TWO documents (internal pre-read + external agenda)\"\n- \"Internal pre-read marked 'INTERNAL ONLY' or 'For team only'\"\n- \"Cites at least 2-3 Notion pages using mention-page tags\"\n- \"Agenda includes time allocations for each section\"\n- \"Claude enrichment includes decision frameworks or best practices\"\n\n**Bad** (vague, untestable):\n\n- \"Creates meeting materials\"\n- \"Gathers context effectively\"\n- \"Prepares well\"\n",
        "skills/meeting-record-system/examples/customer-meeting.md": "# Example: Customer Meeting Prep\n\n**User Request**: \"Prep for sales meeting with Acme Corp on Thursday\"\n\n## Workflow\n\n### 1. Search Customer Info\n\n```\nNotion:notion-search\nquery: \"Acme Corp\"\n```\n\nFound:\n\n- CRM notes from initial call\n- Product demo feedback\n- Pricing proposal\n- Competitor analysis\n\n### 2. Fetch Details\n\n```\nNotion:notion-fetch (4 pages)\n```\n\n**Key context**:\n\n- **Company**: 500-person fintech startup, Series B\n- **Pain Points**: Manual workflows, data silos, scaling issues\n- **Decision Makers**: CTO (technical), VP Product (user needs), CFO (budget)\n- **Timeline**: Need solution by Q1\n- **Budget**: $50-75K/year range\n- **Competition**: Evaluating us vs. Competitor A and Competitor B\n\n### 3. Add Research Context\n\nClaude research added:\n\n- Fintech industry compliance requirements\n- Common integration needs for financial products\n- Typical procurement processes for Series B companies\n\n### 4. Create Internal Pre-Read\n\n```\nNotion:notion-create-pages\npages: [{\n  properties: {\n    title: \"Acme Corp Meeting - Pre-Read (Internal)\"\n  },\n  content: \"# Acme Corp Meeting - Pre-Read (Internal)\n\n## Company Overview\n**Company**: Acme Corp (500-person fintech, Series B)\n**Decision Makers**: CTO, VP Product, CFO\nSource: <mention-page url='...'>CRM Notes</mention-page>\n\n## Their Pain Points  Our Solutions\n- Manual workflows  Workflow automation\n- Data silos  API integrations\n- Scaling issues  Enterprise features\n\n## Competitive Position\n**Our advantages over Competitor A/B**:\n- Better integration ecosystem (Stripe, Plaid)\n- Stronger compliance features\n- Faster implementation (4 weeks vs 12 weeks)\n\n## Pricing Strategy\nRecommend: $60K/year (mid-range of their budget)\n\n## Potential Objections & Responses\n...\"\n}]\n```\n\n### 5. Create Meeting Agenda\n\n```\nNotion:notion-create-pages\npages: [{\n  properties: {\n    title: \"Acme Corp - Product Demo & Q&A\"\n  },\n  content: \"# Acme Corp - Product Demo & Q&A\n\n## Meeting Details\n**Date**: Thursday, Oct 17, 2025 @ 2pm PST\n**Attendees**: CTO, VP Product, CFO (Acme) | Sales Lead, Solutions Engineer (Us)\n**Duration**: 60 minutes\n\n## Objective\nDemonstrate how our platform solves Acme's workflow automation and integration challenges.\n\n## Agenda\n\n**1. Introductions** (5 min)\n\n**2. Understand Current Workflow** (10 min)\n- Current pain points\n- Integration requirements\n- Success criteria\n\n**3. Product Demo** (25 min)\n- Workflow automation capabilities\n- API integrations (Stripe, Plaid)\n- Security & compliance features\n\n**4. Pricing & Implementation** (10 min)\n\n**5. Next Steps** (10 min)\n\"\n}]\n```\n\n### 6. Link Resources\n\nConnected agenda to CRM page, pricing sheet, and technical integration docs.\n\n## Outputs\n\n**Internal Pre-Read**: Full context for sales team\n**Customer Agenda**: Professional meeting structure\n**Both in Notion** with links to supporting materials\n\n## Key Success Factors\n\n- Understood customer's specific pain points\n- Researched industry context (fintech compliance)\n- Mapped features to their needs\n- Prepared competitive differentiators\n- Structured demo around their use cases\n- Pre-planned objection responses\n- Clear next steps in agenda\n",
        "skills/meeting-record-system/examples/executive-review.md": "# Example: Executive Review Prep\n\n**User Request**: \"Prep for quarterly executive review on Friday\"\n\n## Workflow\n\n### 1. Search for Context\n\n```\nNotion:notion-search\nquery: \"Q4 objectives\" + \"KPIs\" + \"quarterly results\"\n```\n\nFound:\n\n- Q4 OKRs and progress\n- Product metrics dashboard\n- Engineering velocity reports\n- Customer feedback summary\n\n### 2. Fetch & Analyze\n\n```\nNotion:notion-fetch (5 pages)\n```\n\n**Key metrics**:\n\n- **Revenue**: $2.4M ARR (96% of Q4 target)\n- **Customer Growth**: 145 new customers (exceeds 120 target)\n- **Churn**: 3.2% (below 5% target)\n- **Product**: 3 major features shipped, 2 in beta\n- **Engineering**: 94% uptime (above 95% SLA)\n\n### 3. Add Claude Research Context\n\nAdded context on:\n\n- Industry benchmarks for SaaS metrics\n- Typical Q4 sales patterns\n- Best practices for executive presentations\n\n### 4. Create Pre-Read (Internal)\n\n```\nNotion:notion-create-pages\ntitle: \"Q4 Review - Pre-Read (Internal)\"\n```\n\n**Pre-read sections**:\n\n- **Executive Summary**: Strong quarter, missed revenue by 4% but exceeded customer growth\n- **Detailed Metrics**: All KPIs with trend lines\n- **Wins**: Product launches, key customer acquisitions\n- **Challenges**: Sales pipeline conversion, engineering hiring\n- **Q1 Preview**: Strategic priorities\n\n### 5. Create Presentation Agenda\n\n```\nNotion:notion-create-pages\ntitle: \"Q4 Executive Review - Agenda\"\n```\n\n**Agenda** (90 min):\n\n- Q4 Results Overview (15 min)\n- Revenue & Growth Deep Dive (20 min)\n- Product & Engineering Update (20 min)\n- Customer Success Highlights (15 min)\n- Q1 Strategic Plan (15 min)\n- Discussion & Questions (15 min)\n\n### 6. Link Supporting Docs\n\nConnected to OKRs, metrics dashboards, and Q1 planning docs.\n\n## Outputs\n\n**Internal Pre-Read**: Comprehensive context with honest assessment\n**Executive Agenda**: Structured 90-min presentation\n**Both in Notion** with links to supporting data\n\n## Key Success Factors\n\n- Synthesized data from multiple sources (OKRs, metrics, feedback)\n- Added industry context and benchmarks\n- Created honest internal assessment (not just wins)\n- Structured agenda with time allocations\n- Linked to source data for drill-down during Q&A\n",
        "skills/meeting-record-system/examples/project-decision.md": "# Example: Project Decision Meeting Prep\n\n## User Request\n\n> \"Prep for tomorrow's architecture meeting where we need to decide on database migration strategy\"\n\n## Workflow Execution\n\n### Step 1: Search for Context\n\n```\nNotion:notion-search\nquery: \"database migration\"\nquery_type: \"internal\"\nteamspace_id: \"engineering-teamspace-id\"\nfilters: {\n  created_date_range: {\n    start_date: \"2024-01-01\"\n  }\n}\n```\n\n**Results Found:**\n\n- \"Database Migration Proposal\" (Engineering)\n- \"Current Database Performance Issues\" (Engineering)\n- \"Migration Options Analysis\" (Architecture)\n- \"Previous Migration Post-mortem\" (Engineering)\n\n### Step 2: Fetch Relevant Pages\n\n```\nNotion:notion-fetch\nid: \"database-migration-proposal-page-id\"\n```\n\n**Extracted**: Proposes migrating from MongoDB to PostgreSQL, outlines timeline and risks\n\n```\nNotion:notion-fetch\nid: \"database-performance-issues-page-id\"\n```\n\n**Extracted**: Current pain points - slow queries, schema flexibility issues, transaction limitations\n\n```\nNotion:notion-fetch\nid: \"migration-options-analysis-page-id\"\n```\n\n**Extracted**: Three options analyzed:\n\n1. Stay on MongoDB with optimizations\n2. Migrate to PostgreSQL\n3. Hybrid approach (MongoDB + PostgreSQL)\n\n```\nNotion:notion-fetch\nid: \"previous-migration-postmortem-page-id\"\n```\n\n**Extracted**: Lessons from 2023 Redis migration - importance of rollback plan, testing strategy\n\n### Step 3: Synthesize Pre-Read\n\n**Key Information Identified:**\n\n- **Problem**: MongoDB performance degrading, schema flexibility causing data consistency issues\n- **Options**: Three approaches with different trade-offs\n- **Recommendation**: PostgreSQL migration favored by engineering team\n- **Concerns**: Downtime requirements, data migration complexity, application changes needed\n- **Historical context**: Previous migration took 6 weeks, had rollback issues\n\n### Step 4: Create Meeting Agenda\n\n```\nNotion:notion-create-pages\n\nparent: { page_id: \"meetings-parent-id\" }\n\npages: [{\n  properties: {\n    \"title\": \"Architecture Decision: Database Migration Strategy - Oct 15, 2025\"\n  },\n  content: \"[Decision meeting agenda]\"\n}]\n```\n\n**Agenda Created:**\n\n```markdown\n# Architecture Decision: Database Migration Strategy\n\n## Meeting Details\n\n**Date & Time**: October 15, 2025 at 2:00 PM  \n**Duration**: 90 minutes  \n**Location**: Conference Room B / Zoom  \n**Facilitator**: Sarah Chen (Engineering Lead)  \n**Attendees**:\n\n- Sarah Chen (Engineering Lead)\n- David Kim (CTO)\n- Alice Wang (Backend Team Lead)\n- Bob Martinez (DevOps Lead)\n- Carol Johnson (Product Director)\n\n## Pre-Read Summary\n\n### Background\n\nOur MongoDB database is experiencing performance degradation as we scale. Query response times have increased from 50ms to 400ms average over the past 6 months. The schema-less nature that initially provided flexibility is now causing data consistency issues and making application logic more complex.\n\n**Source**: <mention-page url=\"...\">Current Database Performance Issues</mention-page>\n\n### Current Situation\n\n**Performance metrics**:\n\n- Average query time: 400ms (was 50ms 6 months ago)\n- p95 query time: 1.2s (was 200ms)\n- Database size: 500GB (growing 20GB/month)\n- Connection pool exhaustion during peak traffic\n\n**Technical debt**:\n\n- 15+ application-layer validation rules compensating for lack of schema\n- Complex data migration scripts for schema changes\n- Limited transaction support causing race conditions\n\n**Source**: <mention-page url=\"...\">Database Migration Proposal</mention-page>\n\n### Historical Context\n\nWe successfully migrated from Redis to Memcached in 2023, which took 6 weeks. Key learnings:\n\n- Underestimated application code changes (3 weeks instead of 1 week)\n- Rollback plan was crucial when we discovered compatibility issues\n- Parallel running period (dual writes) was essential for safe migration\n\n**Source**: <mention-page url=\"...\">Previous Migration Post-mortem</mention-page>\n\n## Decision Required\n\n**Question**: Which database migration strategy should we adopt?\n\n**Timeline**: Need decision by end of week to include in Q4 planning\n\n**Impact**:\n\n- Engineering team (4-8 weeks of work)\n- Application architecture\n- Operations & monitoring\n- Future feature development velocity\n\n## Options Analysis\n\n### Option A: Stay on MongoDB with Optimizations\n\n**Description**: Invest in MongoDB performance tuning, add indexes, upgrade to latest version, implement better query patterns.\n\n**Pros**:\n\n-  No migration complexity\n-  Team familiar with MongoDB\n-  Can implement immediately\n-  Lower risk\n-  Estimated 2 weeks effort\n\n**Cons**:\n\n-  Doesn't solve fundamental schema flexibility issues\n-  Still limited transaction support\n-  Performance improvements may be temporary\n-  Continues technical debt accumulation\n\n**Cost/Effort**: 2 weeks engineering + $5K/year additional MongoDB infrastructure\n\n**Risk**: Medium - Improvements may not be sufficient\n\n**Source**: <mention-page url=\"...\">Migration Options Analysis</mention-page>\n\n### Option B: Migrate to PostgreSQL\n\n**Description**: Full migration from MongoDB to PostgreSQL. Redesign schema with proper constraints, implement dual-write period, then cut over.\n\n**Pros**:\n\n-  Solves schema consistency issues\n-  Full ACID transactions\n-  Better performance for relational queries\n-  Lower long-term complexity\n-  Industry standard, easier hiring\n\n**Cons**:\n\n-  High migration effort (6-8 weeks)\n-  Requires schema redesign\n-  Application code changes extensive\n-  Risk of data loss during migration\n-  Downtime required (4-6 hours estimated)\n\n**Cost/Effort**: 8 weeks engineering + $8K migration costs - $15K/year MongoDB savings = net $7K/year savings\n\n**Risk**: High - Complex migration, application changes required\n\n**Recommendation**:  **Favored by engineering team**\n\n**Source**: <mention-page url=\"...\">Database Migration Proposal</mention-page>\n\n### Option C: Hybrid Approach\n\n**Description**: Keep MongoDB for document-heavy data (logs, analytics), migrate transactional data to PostgreSQL. Run both databases.\n\n**Pros**:\n\n-  Phased migration (lower risk)\n-  Use best tool for each data type\n-  Can migrate incrementally\n-  Smaller initial scope (4 weeks)\n\n**Cons**:\n\n-  Increased operational complexity\n-  Two databases to maintain\n-  Data consistency between databases challenging\n-  Higher infrastructure costs\n-  Complex application logic\n\n**Cost/Effort**: 4 weeks initial + ongoing complexity + $10K/year additional infrastructure\n\n**Risk**: Medium - Operational complexity increases\n\n**Source**: <mention-page url=\"...\">Migration Options Analysis</mention-page>\n\n### Option D: Do Nothing\n\n**Description**: Accept current performance and continue with MongoDB as-is.\n\n**Implications**:\n\n- Performance continues to degrade\n- Technical debt increases\n- Feature development slows\n- Customer experience suffers\n- Eventually forced into emergency migration\n\n**Not recommended**\n\n## Discussion Topics\n\n### Technical Feasibility\n\n1. Can we achieve < 4 hours downtime for Option B?\n2. What's the rollback plan if PostgreSQL migration fails?\n3. How do we handle data migration for 500GB?\n4. Schema design - what constraints do we need?\n\n### Business Impact\n\n5. What's the customer impact of 4-6 hours downtime?\n6. Can we schedule migration during low-traffic period?\n7. How does this affect Q4 feature roadmap?\n8. Cost-benefit analysis over 2-year horizon?\n\n### Risk Management\n\n9. What are the biggest risks with Option B?\n10. How do we test thoroughly before cutover?\n11. What's the rollback procedure and time?\n12. Do we have necessary expertise on team?\n\n### Timeline & Resources\n\n13. Can we allocate 2 engineers full-time for 8 weeks?\n14. Do we need external consultants?\n15. What's the impact on other Q4 projects?\n16. When could we realistically complete this?\n\n## Decision Framework\n\n**Evaluation criteria**:\n\n1. **Performance improvement**: Will this solve our performance issues?\n2. **Technical debt**: Does this reduce or increase complexity?\n3. **Risk**: What's the probability and impact of failure?\n4. **Cost**: Total cost of ownership (effort + infrastructure)\n5. **Timeline**: Can we complete in Q4?\n6. **Reversibility**: Can we roll back if needed?\n\n**Decision makers**:\n\n- Primary: David Kim (CTO)\n- Consulted: Sarah Chen (Engineering), Bob Martinez (DevOps)\n- Informed: Product, Executive team\n\n## Decision\n\n_[To be filled during meeting]_\n\n**Selected Option**: **\\*\\*\\*\\***\\_\\_\\_**\\*\\*\\*\\***\n\n**Rationale**:\n\n**Timeline**:\n\n**Owner**: **\\*\\*\\*\\***\\_\\_\\_**\\*\\*\\*\\***\n\n**Success Criteria**:\n\n## Action Items\n\n_[To be filled during meeting]_\n\n- [ ] [Action item] - @[Owner] - Due: [Date]\n- [ ] [Action item] - @[Owner] - Due: [Date]\n\n## Next Steps\n\n**Immediate**:\n\n- Document decision rationale\n- Create detailed implementation plan\n- Schedule kickoff meeting\n\n**Follow-up meeting**: [Date] to review migration plan\n\n## Related Resources\n\n### Primary Sources\n\n- <mention-page url=\"...\">Database Migration Proposal</mention-page>\n- <mention-page url=\"...\">Current Database Performance Issues</mention-page>\n- <mention-page url=\"...\">Migration Options Analysis</mention-page>\n- <mention-page url=\"...\">Previous Migration Post-mortem</mention-page>\n\n### Supporting Documents\n\n- MongoDB performance metrics dashboard\n- PostgreSQL evaluation results\n- Cost analysis spreadsheet\n```\n\n### Step 5: Share Agenda\n\n```\nNotion:notion-create-comment\nparent: { page_id: \"agenda-page-id\" }\nrich_text: [{\n  type: \"text\",\n  text: { content: \" Meeting agenda ready! Please review the pre-read before tomorrow's meeting. cc: @Sarah @David @Alice @Bob @Carol\" }\n}]\n```\n\n## Summary Provided to User\n\n```markdown\n## Meeting Prep Complete \n\n### Meeting Details\n\n- **Topic**: Database Migration Strategy Decision\n- **Date**: October 15, 2025 at 2:00 PM\n- **Duration**: 90 minutes\n- **Attendees**: 5 (Engineering leads + CTO + Product)\n\n### Agenda Created\n\n<mention-page url=\"...\">Architecture Decision: Database Migration Strategy</mention-page>\n\n### Pre-Read Summary\n\n**Problem**: MongoDB performance degrading (50ms  400ms queries), schema flexibility causing issues\n\n**Decision needed**: Choose between:\n\n1. Stay on MongoDB (optimizations) - 2 weeks, low risk\n2. Migrate to PostgreSQL - 8 weeks, high value, high risk  Recommended\n3. Hybrid approach - 4 weeks, medium complexity\n4. Do nothing - Not recommended\n\n**Recommendation**: Engineering team favors PostgreSQL migration\n\n### Key Information Included\n\n **Background context** from 4 relevant pages\n **Options analysis** with pros/cons for each\n **Cost-benefit** breakdown\n **Risk assessment** for each option\n **Discussion topics** (16 questions organized by theme)\n **Decision framework** with evaluation criteria\n **Historical context** from previous migration\n **Empty sections** for decision and action items\n\n### Discussion Topics Prepared\n\n**Technical** (4 questions):\n\n- Downtime achievability\n- Rollback planning\n- Data migration approach\n- Schema design\n\n**Business** (4 questions):\n\n- Customer impact\n- Scheduling\n- Roadmap impact\n- Cost analysis\n\n**Risk** (4 questions):\n\n- Risk identification\n- Testing strategy\n- Rollback procedure\n- Expertise availability\n\n**Resources** (4 questions):\n\n- Team allocation\n- External help needs\n- Project trade-offs\n- Timeline realism\n\n### Success Factors\n\n1. **Comprehensive research**: 4 relevant pages analyzed\n2. **Clear options**: Pros/cons for each approach\n3. **Structured discussion**: 16 questions organized by theme\n4. **Decision framework**: Clear criteria for evaluation\n5. **Historical context**: Lessons from previous migration\n6. **Ready for outcomes**: Sections prepared for decision and actions\n\n**Meeting is well-prepared for productive decision-making! **\n```\n\n## Key Features Demonstrated\n\n### Context Gathering\n\n- **Targeted search**: Engineering teamspace, recent documents\n- **Multiple perspectives**: Performance data, proposals, analysis, historical lessons\n- **Comprehensive synthesis**: Combined technical, business, and historical context\n\n### Options Analysis\n\n- **Structured comparison**: Pros/cons for each option\n- **Cost-benefit**: Effort and infrastructure costs included\n- **Risk assessment**: Probability and impact noted\n- **Recommendation**: Clear engineering preference stated\n\n### Decision Support\n\n- **Discussion topics**: 16 questions organized by theme\n- **Decision framework**: Evaluation criteria defined\n- **Decision makers**: Roles and responsibilities clear\n- **Outcome capture**: Sections ready for decision and actions\n\n### Meeting Structure\n\n- **Pre-read**: Comprehensive background (can be read in 10 minutes)\n- **Options**: Clear comparison for quick decision\n- **Discussion**: Structured topics prevent rambling\n- **Capture**: Templates for decision and actions\n\nPerfect for: Architecture decisions, technical trade-offs, strategic choices\n",
        "skills/meeting-record-system/examples/sprint-planning.md": "# Example: Sprint Planning Meeting Prep\n\n**User Request**: \"Prepare for tomorrow's sprint planning meeting\"\n\n## Workflow\n\n### 1. Search for Context\n\n```\nNotion:notion-search\nquery: \"sprint planning\" + \"product backlog\"\nteamspace_id: \"engineering-team\"\n```\n\nFound:\n\n- Last sprint retrospective\n- Product backlog (prioritized)\n- Current sprint progress\n- Team capacity notes\n\n### 2. Fetch Details\n\n```\nNotion:notion-fetch (4 pages)\n```\n\n**Key context**:\n\n- **Last Sprint**: Completed 32/35 story points (91%)\n- **Velocity**: Consistent 30-35 points over last 3 sprints\n- **Team**: 5 engineers, 1 on vacation next sprint (80% capacity)\n- **Top Backlog Items**: User auth improvements, API performance, mobile responsive fixes\n\n### 3. Query Current Sprint Tasks\n\n```\nNotion:notion-query-data-sources\nquery: \"SELECT * FROM tasks WHERE Sprint = 'Sprint 24' AND Status != 'Done'\"\n```\n\n3 tasks carrying over (technical debt items)\n\n### 4. Create Pre-Read (Internal)\n\n```\nNotion:notion-create-pages\ntitle: \"Sprint 25 Planning - Pre-Read (Internal)\"\n```\n\n**Pre-read included**:\n\n- Sprint 24 summary (velocity, what carried over)\n- Team capacity for Sprint 25\n- Top backlog candidates with story points\n- Technical dependencies\n- Risk items (auth changes need QA time)\n\n### 5. Create Agenda\n\n```\nNotion:notion-create-pages\ntitle: \"Sprint 25 Planning - Agenda\"\n```\n\n**Agenda**:\n\n- Review Sprint 24 completion (5 min)\n- Discuss carryover items (5 min)\n- Review capacity (28 points available)\n- Select backlog items (30 min)\n- Identify dependencies & risks (10 min)\n- Confirm commitments (10 min)\n\n### 6. Link Documents\n\nCross-linked pre-read and agenda, referenced last retro and backlog.\n\n## Output Summary\n\n**Internal Pre-Read**: Team context, capacity, blockers\n**External Agenda**: Meeting structure, discussion topics\n**Both saved to Notion** and linked to project pages\n\n## Key Success Factors\n\n- Gathered sprint history for velocity trends\n- Calculated realistic capacity (account for PTO)\n- Identified carryover items upfront\n- Pre-read gave team context before meeting\n- Agenda kept meeting focused and timeboxed\n",
        "skills/meeting-record-system/reference/brainstorming-template.md": "# Brainstorming Meeting Template\n\nUse this template for creative ideation and brainstorming sessions.\n\n```markdown\n# [Topic] Brainstorming - [Date]\n\n## Meeting Details\n\n**Date**: [Date]\n**Facilitator**: [Name]\n**Note-taker**: [Name]\n**Attendees**: [List]\n\n## Objective\n\n[Clear statement of what we're brainstorming]\n\n**Success looks like**: [How we'll know brainstorming was successful]\n\n## Background & Context\n\n[Context from research - 2-3 paragraphs]\n\n**Related Pages**:\n\n- <mention-page url=\"...\">Context Page 1</mention-page>\n- <mention-page url=\"...\">Context Page 2</mention-page>\n\n## Constraints\n\n- [Constraint]\n- [Constraint]\n- [Constraint]\n\n## Seed Ideas\n\n[Starting ideas from research to spark discussion]:\n\n1. **[Idea]**: [Brief description]\n2. **[Idea]**: [Brief description]\n\n## Ground Rules\n\n- No criticism during ideation\n- Build on others' ideas\n- Quantity over quality initially\n- Wild ideas welcome\n\n## Brainstorming Notes\n\n### Ideas Generated\n\n[To be filled during meeting]\n\n1. [Idea with brief description]\n2. [Idea with brief description]\n\n### Themes/Patterns\n\n[Groupings that emerge]\n\n## Evaluation\n\n[If time permits, evaluate top ideas]\n\n### Top Ideas\n\n| Idea   | Feasibility | Impact  | Effort  | Score |\n| ------ | ----------- | ------- | ------- | ----- |\n| [Idea] | [H/M/L]     | [H/M/L] | [H/M/L] | [#]   |\n\n## Next Steps\n\n- [ ] [Action to explore idea]\n- [ ] [Action to prototype]\n- [ ] [Action to research]\n\n## Follow-up\n\n**Next meeting**: [Date to reconvene]\n```\n",
        "skills/meeting-record-system/reference/decision-meeting-template.md": "# Decision Meeting Template\n\nUse this template when you need to make an important decision with your team.\n\n```markdown\n# [Decision Topic] - [Date]\n\n## Meeting Details\n\n**Date & Time**: [Date and time]\n**Duration**: [Length]\n**Attendees**: [List of attendees with roles]\n**Location**: [Physical location or video link]\n**Facilitator**: [Name]\n\n## Pre-Read Summary\n\n### Background\n\n[2-3 sentences providing context from related project pages]\n\n**Related Pages**:\n\n- <mention-page url=\"...\">Project Overview</mention-page>\n- <mention-page url=\"...\">Previous Discussion</mention-page>\n\n### Current Situation\n\n[What brings us to this decision point]\n\n## Decision Required\n\n**Question**: [Clear statement of decision needed]\n\n**Timeline**: [When decision needs to be made]\n\n**Impact**: [Who/what is affected by this decision]\n\n## Options Analysis\n\n### Option A: [Name]\n\n**Description**: [What this option entails]\n\n**Pros**:\n\n- [Advantage]\n- [Advantage]\n\n**Cons**:\n\n- [Disadvantage]\n- [Disadvantage]\n\n**Cost/Effort**: [Estimate]\n**Risk**: [Risk assessment]\n\n### Option B: [Name]\n\n[Repeat structure]\n\n### Option C: Do Nothing\n\n**Description**: What happens if we don't decide\n**Implications**: [Consequences]\n\n## Recommendation\n\n[If there is a recommended option, state it with rationale]\n\n## Discussion Topics\n\n1. [Topic to discuss]\n2. [Clarification needed on]\n3. [Trade-offs to consider]\n\n## Decision Framework\n\n**Criteria for evaluation**:\n\n- [Criterion 1]\n- [Criterion 2]\n- [Criterion 3]\n\n## Decision\n\n[To be filled during meeting]\n\n**Selected Option**: [Option chosen]\n**Rationale**: [Why]\n**Owner**: [Who will implement]\n**Timeline**: [When]\n\n## Action Items\n\n- [ ] [Action] - @[Owner] - Due: [Date]\n- [ ] [Action] - @[Owner] - Due: [Date]\n\n## Follow-up\n\n**Next review**: [Date]\n**Success metrics**: [How we'll know this worked]\n```\n",
        "skills/meeting-record-system/reference/one-on-one-template.md": "# 1:1 Meeting Template\n\nUse this template for manager/report one-on-one meetings.\n\n```markdown\n# 1:1: [Manager] & [Report] - [Date]\n\n## Meeting Details\n\n**Date**: [Date]\n**Last meeting**: <mention-page url=\"...\">Previous 1:1</mention-page>\n\n## Agenda\n\n### [Report]'s Topics\n\n1. [Topic to discuss]\n2. [Question or concern]\n\n### [Manager]'s Topics\n\n1. [Topic to cover]\n2. [Feedback or update]\n\n## Discussion Notes\n\n### [Topic 1]\n\n[Discussion points]\n\n**Action items**:\n\n- [ ] [Action] - @[Owner]\n\n### [Topic 2]\n\n[Discussion points]\n\n## Career Development\n\n**Current focus**: [Development goal]\n**Progress**: [Update on progress]\n\n## Feedback\n\n**What's going well**:\n\n- [Positive feedback]\n\n**Areas for growth**:\n\n- [Developmental feedback]\n\n## Action Items\n\n- [ ] [Action] - @[Report] - Due: [Date]\n- [ ] [Action] - @[Manager] - Due: [Date]\n\n## Next Meeting\n\n**Date**: [Date]\n**Topics to cover**:\n\n- [Carry-over topic]\n- [Upcoming topic]\n```\n",
        "skills/meeting-record-system/reference/retrospective-template.md": "# Retrospective Template\n\nUse this template for sprint retrospectives and team retrospectives.\n\n```markdown\n# Sprint [#] Retrospective - [Date]\n\n## Meeting Details\n\n**Date**: [Date]\n**Team**: [Team]\n**Sprint**: [Sprint dates]\n**Facilitator**: [Name]\n\n## Sprint Summary\n\n**Sprint Goal**: [Goal]\n**Goal Met**: Yes / Partially / No\n\n**Completed**: [#] points\n**Velocity**: [#] points\n**Planned**: [#] points\n\n## Pre-Read\n\n**Sprint Metrics**:\n\n- Tasks completed: [#]\n- Tasks carried over: [#]\n- Bugs found: [#]\n- Blockers encountered: [#]\n\n## Discussion\n\n### What Went Well (Keep)\n\n[Team input during meeting]\n\n### What Didn't Go Well (Stop)\n\n[Team input during meeting]\n\n### What To Try (Start)\n\n[Team input during meeting]\n\n### Shout-outs\n\n[Team recognition]\n\n## Action Items\n\n- [ ] [Improvement to implement] - @[Owner] - Due: [Date]\n- [ ] [Process change] - @[Owner] - Due: [Date]\n\n## Follow-up\n\n**Review actions in**: [Next retro date]\n```\n",
        "skills/meeting-record-system/reference/sprint-planning-template.md": "# Sprint Planning Template\n\nUse this template for agile sprint planning meetings.\n\n```markdown\n# Sprint [#] Planning - [Date]\n\n## Meeting Details\n\n**Date**: [Date]\n**Team**: [Team name]\n**Sprint Duration**: [Dates]\n\n## Sprint Goal\n\n[Clear statement of what this sprint aims to accomplish]\n\n## Capacity\n\n| Team Member | Availability | Capacity (points) |\n| ----------- | ------------ | ----------------- |\n| [Name]      | [%]          | [#]               |\n| **Total**   |              | [#]               |\n\n## Backlog Review\n\n### High Priority Items\n\n[From product backlog, linked from task database]\n\n- <mention-page url=\"...\">Task 1</mention-page> - [Points]\n- <mention-page url=\"...\">Task 2</mention-page> - [Points]\n\n## Sprint Backlog\n\n### Committed Items\n\n- [x] <mention-page url=\"...\">Task</mention-page> - [Points] - @[Owner]\n- [ ] <mention-page url=\"...\">Task</mention-page> - [Points] - @[Owner]\n\n**Total committed**: [Points]\n\n### Stretch Goals\n\n- [ ] <mention-page url=\"...\">Task</mention-page> - [Points]\n\n## Dependencies & Risks\n\n**Dependencies**:\n\n- [Dependency]\n\n**Risks**:\n\n- [Risk]\n\n## Definition of Done\n\n- [ ] Code complete and reviewed\n- [ ] Tests written and passing\n- [ ] Documentation updated\n- [ ] Deployed to staging\n- [ ] QA approved\n\n## Next Steps\n\n- Team begins sprint work\n- Daily standups at [Time]\n- Sprint review on [Date]\n```\n",
        "skills/meeting-record-system/reference/status-update-template.md": "# Status Update Meeting Template\n\nUse this template for regular project status updates and check-ins.\n\n```markdown\n# [Project Name] Status Update - [Date]\n\n## Meeting Details\n\n**Date**: [Date and time]\n**Attendees**: [List]\n**Project**: <mention-page url=\"...\">Project Page</mention-page>\n\n## Executive Summary\n\n**Status**:  On Track /  At Risk /  Behind\n\n**Progress**: [Percentage] complete\n**Timeline**: [Status vs original plan]\n\n## Progress Since Last Meeting\n\n### Completed\n\n- [Accomplishment with specifics]\n- [Accomplishment with specifics]\n\n### In Progress\n\n- [Work item and status]\n- [Work item and status]\n\n## Metrics\n\n| Metric   | Current | Target  | Status |\n| -------- | ------- | ------- | ------ |\n| [Metric] | [Value] | [Value] | [Icon] |\n| [Metric] | [Value] | [Value] | [Icon] |\n\n## Upcoming Work\n\n**Next 2 Weeks**:\n\n- [Planned work]\n- [Planned work]\n\n**Next Month**:\n\n- [Milestone or major work]\n\n## Blockers & Risks\n\n### Active Blockers\n\n- **[Blocker]**: [Description and impact]\n  - Action: [What's being done]\n\n### Risks\n\n- **[Risk]**: [Description]\n  - Mitigation: [Strategy]\n\n## Discussion Topics\n\n1. [Topic requiring input]\n2. [Topic for alignment]\n\n## Decisions Needed\n\n- [Decision] or None\n\n## Action Items\n\n- [ ] [Action] - @[Owner] - Due: [Date]\n\n## Next Meeting\n\n**Date**: [Date]\n**Focus**: [What next meeting will cover]\n```\n",
        "skills/meeting-record-system/reference/template-selection-guide.md": "# Meeting Template Selection Guide\n\nChoose the right template for your meeting type.\n\n## Template Overview\n\n| Meeting Type     | Use This Template                                       | When to Use                                   |\n| ---------------- | ------------------------------------------------------- | --------------------------------------------- |\n| Make a decision  | [Decision Meeting](decision-meeting-template.md)        | Need to evaluate options and reach a decision |\n| Project update   | [Status Update](status-update-template.md)              | Regular check-ins, progress reviews           |\n| Generate ideas   | [Brainstorming](brainstorming-template.md)              | Creative ideation, problem-solving            |\n| Sprint planning  | [Sprint Planning](sprint-planning-template.md)          | Planning agile sprint work                    |\n| Sprint retro     | [Retrospective](retrospective-template.md)              | Reflecting on completed work                  |\n| Manager/report   | [1:1 Meeting](one-on-one-template.md)                   | Regular one-on-one check-ins                  |\n| Weekly team sync | [Status Update](status-update-template.md) (simplified) | Routine team synchronization                  |\n\n## Quick Decision Tree\n\n```\nWhat's the primary purpose?\n\n Make a decision\n   Use: Decision Meeting Template\n\n Update on progress\n   Use: Status Update Template\n\n Generate ideas\n   Use: Brainstorming Template\n\n Plan sprint work\n   Use: Sprint Planning Template\n\n Reflect on past work\n   Use: Retrospective Template\n\n Manager/report check-in\n    Use: 1:1 Meeting Template\n```\n\n## Template Customization\n\nAll templates can be customized:\n\n- **Simplify** for shorter meetings\n- **Add sections** for specific needs\n- **Combine elements** from multiple templates\n- **Adapt language** for your team culture\n\n## Best Practices\n\n1. **Choose template first**: Select before gathering context\n2. **Gather Notion content**: Search and fetch relevant pages\n3. **Enrich with research**: Add Claude insights where valuable\n4. **Customize as needed**: Adapt template to specific situation\n5. **Share early**: Give attendees time to review\n",
        "skills/mobile-app-interface/SKILL.md": "---\nname: mobile-app-interface\ndescription: Design native iOS mobile app interfaces and components. Creates user-centric mobile designs following platform conventions and best practices.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\nExpo iOS Designer\nCore Design Prompt\nDesign a modern, clean iOS app using Expo and React Native that follows Apples Human Interface Guidelines: prioritize clear hierarchy and harmony; respect safe areas; use responsive Flexbox layouts and Dynamic Type with SF Pro; support dark mode with semantic system-friendly colors; keep minimum 44pt touch targets; use native navigation patterns (tabs, stacks, modals) and standard gestures; apply Liquid Glass materials sparingly for overlays like bars, sheets, and popovers with AA contrast; add purposeful motion and gentle haptics; honor Reduce Motion and Reduce Transparency; deliver icons/splash and store assets per Apple guidance..\n\nDesign Rules\n1) Safe Areas Rule\nWrap screens with SafeAreaProvider/SafeAreaView to avoid notches and the home indicator; never hardcode insets.\n\ntsx\nimport { SafeAreaView } from \"react-native-safe-area-context\";\n\nexport function Screen({ children }) {\n  return <SafeAreaView style={{ flex: 1 }}>{children}</SafeAreaView>;\n}\n2) Typography Rule\nUse SF Pro Text/Display (or system) with a documented type ramp; support Dynamic Type so text scales with user settings.\n\ntsx\n<Text style={{ fontSize: 17, fontWeight: \"600\" }} accessibilityRole=\"header\">\n  Title\n</Text>\n<Text style={{ fontSize: 15, color: \"#6b7280\" }}>Secondary text</Text>\n3) Touch Target Rule\nEnsure interactive controls are at least 4444pt, with adequate spacing between targets for accurate taps.\n\ntsx\n<TouchableOpacity\n  style={{ minHeight: 44, minWidth: 44, justifyContent: \"center\", alignItems: \"center\" }}\n  accessibilityRole=\"button\"\n>\n  <Text>Action</Text>\n</TouchableOpacity>\n4) Color & Theming Rule\nUse semantic roles and support light/dark with AA contrast for text and essential UI; prefer system-friendly palettes that adapt across appearances.\n\ntsx\nconst scheme = useColorScheme();\nconst bg = scheme === \"dark\" ? \"#0B0B0B\" : \"#FFFFFF\";\nconst fg = scheme === \"dark\" ? \"#E5E7EB\" : \"#111827\";\n5) Navigation Rule\nUse tab bars for top-level sections, stack for drill-ins, and modals for short tasks; align back navigation with iOS gestures and conventions.\n\n**IMPORTANT: For Tab Bars with Liquid Glass**\nALWAYS use `NativeTabs` from Expo Router instead of custom tab bars. NativeTabs provides native iOS `UITabBarController` with built-in Liquid Glass effect - no manual implementation needed!\n\ntsx\n//  CORRECT: Native tab bar with built-in Liquid Glass\nimport { NativeTabs, Icon, Label } from \"expo-router/unstable-native-tabs\";\n\nexport default function TabLayout() {\n  return (\n    <NativeTabs>\n      <NativeTabs.Trigger name=\"index\">\n        <Icon sf={{ default: 'house', selected: 'house.fill' }} />\n        <Label>Home</Label>\n      </NativeTabs.Trigger>\n      <NativeTabs.Trigger name=\"settings\">\n        <Icon sf={{ default: 'gearshape', selected: 'gearshape.fill' }} />\n        <Label>Settings</Label>\n      </NativeTabs.Trigger>\n    </NativeTabs>\n  );\n}\n\n//  WRONG: Custom tab bars - requires manual Liquid Glass implementation\nimport { createBottomTabNavigator } from \"@react-navigation/bottom-tabs\";\nconst Tab = createBottomTabNavigator();\n\n**NativeTabs Features:**\n- Built-in Liquid Glass blur (automatic on iOS 26+)\n- SF Symbols for icons (`sf` prop with default/selected states)\n- Native iOS animations and haptics\n- Automatic light/dark mode adaptation\n- System-native behavior (matches Safari, Apple Music, etc.)\n- No custom styling required\n\n**SF Symbols Icon Examples:**\n- Home: `house` / `house.fill`\n- Settings: `gearshape` / `gearshape.fill`\n- Messages: `message` / `message.fill`\n- Profile: `person` / `person.fill`\n- Search: `magnifyingglass`\n- Calendar: `calendar` / `calendar.fill`\n- Star: `star` / `star.fill`\n\nFind more at: https://developer.apple.com/sf-symbols/\n6) Motion & Haptics Rule\nKeep transitions 200400ms with native-feeling ease or spring; pair key state changes and confirmations with gentle haptics.\n\ntsx\nimport * as Haptics from \"expo-haptics\";\nconst onPress = async () => { await Haptics.selectionAsync(); /* action */ };\n7) Accessibility Rule\nProvide accessibilityLabel, Role, Hint, and state; verify logical focus order and complete VoiceOver announcements across flows.\n\ntsx\n<Switch\n  value={isOn}\n  onValueChange={setOn}\n  accessibilityRole=\"switch\"\n  accessibilityLabel=\"Notifications\"\n  accessibilityState={{ checked: isOn }}\n/>\n8) List & Performance Rule\nUse FlatList/SectionList with keyExtractor, optional getItemLayout, and memoized rows; avoid re-render churn for smooth 60fps scrolling.\n\ntsx\n<FlatList\n  data={items}\n  keyExtractor={(it) => it.id}\n  renderItem={memo(({ item }) => <Row item={item} />)}\n/>\n9) Assets & App Store Rule\nCreate icons and splash per Expo docs; verify in an EAS build, not Expo Go; keep store metadata and permissions aligned to behavior.\n\njson\n// app.json (excerpt)\n{\n  \"expo\": {\n    \"icon\": \"./assets/icon.png\",\n    \"splash\": { \"image\": \"./assets/splash.png\", \"resizeMode\": \"contain\", \"backgroundColor\": \"#000000\" }\n  }\n}\n10) Layout & Spacing Rule\nCompose with Flexbox and a consistent spacing scale; adapt padding to dynamic type and safe areas for balanced, accessible layouts.\n\ntsx\n<View style={{ padding: 16, gap: 12, flex: 1 }}>\n  {/* content */}\n</View>\n11) Liquid Glass Materials Rule\nUse Liquid Glass on overlay surfaces (navigation/tab bars, large headers, sheets, popovers, floating cards) to add depth without distracting from content; verify AA contrast over dynamic backdrops in light and dark modes.\n\nRespect Reduce Transparency and provide solid/tinted fallbacks; avoid placing dense text over highly saturated or high-frequency backdrops.\n\nKeep materials subtle: modest opacity/blur, applied sparingly to chrome rather than full-screen backgrounds for readability and performance.\n\n12) Expo Glass Modules Rule\nOfficial module: expo-glass-effect. Provides GlassView, GlassContainer, and isLiquidGlassAvailable() to detect capability and compose grouped glass surfaces.\n\nCommunity SwiftUI module: expo-liquid-glass-view. Fine control over corner radius, styles, and tints; iOS-only; ensure platform fallbacks.\n\nInstall and basic usage:\n\nbash\nnpx expo install expo-glass-effect\ntsx\nimport { GlassView } from \"expo-glass-effect\";\n\n<GlassView glassEffectStyle=\"regular\" tintColor=\"systemMaterialLight\" />\nSwiftUI-powered option:\n\nbash\nnpx expo install expo-liquid-glass-view\ntsx\nimport { ExpoLiquidGlassView } from \"expo-liquid-glass-view\";\nThese render native iOS Liquid Glass via UIVisualEffectView/SwiftUI, and gracefully fall back to a regular View on unsupported platforms.\n\n13) Availability & Fallbacks Rule\nCheck availability on iOS 26+ with isLiquidGlassAvailable(); also honor AccessibilityInfo.isReduceTransparencyEnabled() for fallbacks to solid/tinted surfaces.\n\ntsx\nimport { isLiquidGlassAvailable, GlassView } from \"expo-glass-effect\";\nimport { AccessibilityInfo, Platform } from \"react-native\";\n\nconst useGlass = async () => {\n  const supported = Platform.OS === \"ios\" && (await isLiquidGlassAvailable());\n  const reduceTransparency = await AccessibilityInfo.isReduceTransparencyEnabled();\n  return { supported, reduceTransparency };\n};\n14) Materials Performance Rule\nAvoid full-screen realtime blur on animated scenes; scope glass to small overlays, cache where possible, and profile on device; fall back to static blur or solids when FPS dips.\n\n15) Icon Variants Rule\nProvide dark and tinted icon variants following updated Apple resources for consistent appearance with system tints and wallpapers.\n\nWorkflow\n1) Interview User\nScope: screen, flow, or component; target file/repo path; materials use-cases (bars, sheets, overlays); accessibility/performance targets.\n\n2) Design & Implement\nMatch HIG patterns and the existing design system; compose UI first; define component variants/states.\n\nApply all rules (safe area, type, touch, color, nav, motion, a11y, performance, materials, icons). Test Dynamic Type, dark mode, VoiceOver, Reduce Transparency/Motion, and iOS 26 availability.\n\nValidate on device for performance, notch layouts, and readability over moving content and wallpapers.\n\n3) Component Structure Pattern\ntsx\nimport { View, Text } from \"react-native\";\nimport { SafeAreaView } from \"react-native-safe-area-context\";\n\nexport function ScreenTemplate({ title, children }) {\n  return (\n    <SafeAreaView style={{ flex: 1 }}>\n      <View style={{ padding: 20, gap: 16 }}>\n        <Text style={{ fontSize: 28, fontWeight: \"700\" }} accessibilityRole=\"header\">\n          {title}\n        </Text>\n        <View style={{ gap: 12 }}>{children}</View>\n      </View>\n    </SafeAreaView>\n  );\n}\nQuality Checklist\n Safe areas respected across edges and orientations.\n\n SF Pro/system fonts with Dynamic Type verified at larger sizes.\n\n 4444pt touch targets and adequate spacing confirmed on device.\n\n Light/dark with semantic colors and WCAG AA contrast for text and core UI.\n\n Native navigation patterns and back gestures consistent with iOS.\n\n Purposeful motion with gentle haptics; honors Reduce Motion.\n\n Accessibility labels/roles/hints/states and logical focus order; VoiceOver validated.\n\n Lists are smooth and jank-free; renders and images optimized.\n\n Icons/splash configured via Expo and tested in an EAS build.\n\n Store metadata and permissions aligned with behavior.\n\n Liquid Glass used for overlays only; AA contrast verified over dynamic backdrops.\n\n Availability checks with isLiquidGlassAvailable(); fallbacks for Reduce Transparency.\n\n Materials performance profiled; fallbacks applied if FPS drops.\n\n Icon dark/tinted variants per updated resources.\n\nReferences\nApple HIG: layout, navigation, materials, typography.\n\nExpo GlassEffect API and install guides; SwiftUI module references.\n\nExpo docs: safe areas, splash/icon configuration, project setup and device testing.\n\nAccessibility: React Native docs and testing guidance for roles, labels, focus order, touch targets.",
        "skills/mobile-platform-specialist/SKILL.md": "---\nname: mobile-platform-specialist\ndescription: Provide expert guidance on iOS development and design. Advises on platform capabilities, best practices, and architectural decisions.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# iOS Development Expert\n\n## Purpose\n\nProvide expert guidance on iOS development covering Swift programming, UIKit, SwiftUI, Xcode, app architecture, platform features, and Apple ecosystem integration.\n\n## When to Use\n\nAuto-invoke when users mention:\n- iOS development or iPhone/iPad apps\n- Swift programming language\n- SwiftUI or UIKit frameworks\n- Xcode IDE and development tools\n- Apple platform features\n- iOS-specific APIs and services\n- App Store development\n- Apple ecosystem integration\n- iOS app architecture patterns\n\n## Knowledge Base\n\niOS development documentation stored in `.claude/skills/frontend/ios/docs/`\n\nCoverage includes:\n- Swift language fundamentals\n- SwiftUI declarative UI framework\n- UIKit imperative UI framework\n- iOS SDK and platform APIs\n- Xcode development environment\n- App lifecycle and architecture\n- iOS design patterns (MVC, MVVM, etc.)\n- Platform-specific features\n- App Store submission and guidelines\n\n## Process\n\nWhen a user asks about iOS development:\n\n1. **Identify the Topic**\n   - Determine the specific iOS concept or feature\n   - Examples: SwiftUI views, UIKit controllers, Swift syntax, Xcode configuration\n\n2. **Search Documentation**\n   ```\n   Use Grep to search: Grep \"keyword\" .claude/skills/frontend/ios/docs/\n   ```\n\n   Common search patterns:\n   - SwiftUI: `Grep \"swiftui\" .claude/skills/frontend/ios/docs/ -i`\n   - UIKit: `Grep \"uikit\" .claude/skills/frontend/ios/docs/ -i`\n   - Swift language: `Grep \"swift\" .claude/skills/frontend/ios/docs/ -i`\n   - Xcode: `Grep \"xcode\" .claude/skills/frontend/ios/docs/ -i`\n\n3. **Read Relevant Documentation**\n   ```\n   Use Read to load specific files found in search\n   Read .claude/skills/frontend/ios/docs/[filename].md\n   ```\n\n4. **Provide Structured Answer**\n\n   Format responses with:\n   - **Overview**: Brief explanation of the concept\n   - **Setup/Configuration**: Required setup or imports\n   - **Code Examples**: Practical Swift/SwiftUI/UIKit examples\n   - **Best Practices**: Apple's recommendations and patterns\n   - **Common Issues**: Known gotchas or troubleshooting\n   - **Related Topics**: Links to related iOS features\n   - **Source**: Reference the documentation file used\n\n## Example Workflows\n\n### SwiftUI Questions\n```\nUser: \"How do I create a list view in SwiftUI?\"\n\n1. Search: Grep \"list|swiftui\" .claude/skills/frontend/ios/docs/ -i\n2. Read: SwiftUI documentation files\n3. Answer with SwiftUI List examples, modifiers, data binding\n```\n\n### UIKit Questions\n```\nUser: \"How do I set up a UITableView?\"\n\n1. Search: Grep \"uitableview\" .claude/skills/frontend/ios/docs/ -i\n2. Read: UIKit documentation\n3. Explain delegate/datasource pattern, cell configuration\n```\n\n### Swift Language Questions\n```\nUser: \"What are Swift optionals?\"\n\n1. Search: Grep \"optional\" .claude/skills/frontend/ios/docs/ -i\n2. Read: Swift language documentation\n3. Explain optional syntax, unwrapping, optional chaining\n```\n\n### Xcode Questions\n```\nUser: \"How do I configure build settings in Xcode?\"\n\n1. Search: Grep \"build setting|xcode\" .claude/skills/frontend/ios/docs/ -i\n2. Read: Xcode configuration documentation\n3. Provide build settings, schemes, configuration guidance\n```\n\n## Response Format\n\nAlways structure responses as:\n\n```markdown\n## [Topic Name]\n\n[Brief overview paragraph]\n\n### Setup\n\n[Required imports, configuration, prerequisites]\n\n### Implementation\n\n```swift\n// Code examples with comments\nimport SwiftUI\n\nstruct ContentView: View {\n    var body: some View {\n        Text(\"Hello, iOS!\")\n    }\n}\n```\n\n### Key Points\n\n- Important concept 1\n- Important concept 2\n- Important concept 3\n\n### Common Issues\n\n- Issue and solution\n- Gotcha and workaround\n\n### Related\n\n- Related feature or concept\n- Link to additional documentation\n\n**Source:** `.claude/skills/frontend/ios/docs/[filename].md`\n```\n\n## Important Notes\n\n- Always search documentation first before answering\n- Reference specific documentation files in responses\n- Provide working Swift code examples\n- Use Swift naming conventions (camelCase, PascalCase)\n- Consider both SwiftUI and UIKit when relevant\n- Mention iOS version requirements when applicable\n- Include proper imports (import SwiftUI, import UIKit, etc.)\n- Use modern Swift syntax and patterns\n- Consider device differences (iPhone vs iPad)\n\n## Coverage Areas\n\n**Swift Programming**\n- Language fundamentals\n- Optionals and error handling\n- Protocols and generics\n- Closures and functions\n- Value types vs reference types\n- Concurrency (async/await)\n\n**SwiftUI**\n- Declarative views\n- State management (@State, @Binding, @ObservedObject)\n- View modifiers\n- Navigation and routing\n- Data flow\n- Animations\n\n**UIKit**\n- View controllers\n- Auto Layout\n- UITableView / UICollectionView\n- Navigation controllers\n- Delegates and protocols\n- Storyboards and XIBs\n\n**iOS Platform**\n- App lifecycle\n- Background tasks\n- Notifications\n- Core Data / SwiftData\n- Networking (URLSession)\n- File system\n- Location services\n- Camera and photos\n\n**Xcode**\n- Project configuration\n- Build settings\n- Debugging tools\n- Interface Builder\n- Testing (XCTest)\n- Instruments\n\n**Architecture**\n- MVC (Model-View-Controller)\n- MVVM (Model-View-ViewModel)\n- Coordinator pattern\n- Dependency injection\n- Clean architecture\n\n**App Store**\n- App submission process\n- App Store guidelines\n- TestFlight\n- Provisioning profiles\n- Code signing\n\n## Do Not\n\n- Provide Objective-C solutions (prefer Swift)\n- Use deprecated APIs without noting alternatives\n- Ignore memory management considerations\n- Provide solutions incompatible with current iOS versions\n- Mix SwiftUI and UIKit patterns without clear explanation\n\n## Always\n\n- Search documentation before answering\n- Provide working Swift code examples\n- Reference source documentation\n- Mention iOS version requirements\n- Consider both iPhone and iPad layouts\n- Use proper Swift naming conventions\n- Include error handling where appropriate\n- Mention App Store guidelines when relevant\n- Consider accessibility best practices",
        "skills/multi-agent-orchestration/README.md": "# Multi-Agent Orchestration - Code Structure\n\nThis skill uses supporting Python files to keep documentation lean and maintainable.\n\n## Directory Structure\n\n```\nmulti-agent-orchestration/\n SKILL.md                           # Main documentation (patterns, concepts)\n README.md                          # This file\n examples/                          # Implementation examples\n    orchestration_patterns.py      # Sequential, parallel, hierarchical, consensus\n    framework_implementations.py   # CrewAI, AutoGen, LangGraph, Swarm templates\n scripts/                           # Utility modules\n     agent_communication.py         # Message broker, shared memory, protocols\n     workflow_management.py         # Workflow execution and optimization\n     benchmarking.py                # Performance and collaboration metrics\n```\n\n## Running Examples\n\n### 1. Orchestration Patterns\n```bash\npython examples/orchestration_patterns.py\n```\nDemonstrates sequential, parallel, hierarchical, and consensus orchestration.\n\n### 2. Framework Templates\n```bash\npython examples/framework_implementations.py\n```\nTemplates and configurations for CrewAI, AutoGen, LangGraph, and Swarm frameworks.\n\n## Using the Utilities\n\n### Agent Communication\n```python\nfrom scripts.agent_communication import MessageBroker, SharedMemory, CommunicationProtocol\n\n# Set up communication\nbroker = MessageBroker()\nshared_memory = SharedMemory()\nprotocol = CommunicationProtocol(broker, shared_memory)\n\n# Send messages between agents\nprotocol.request_analysis(\"agent_a\", \"agent_b\", \"Analyze this topic\")\n\n# Share findings\nprotocol.share_findings(\"agent_a\", \"analysis_results\", {\"findings\": \"...\"})\n\n# Get communication stats\nstats = broker.get_statistics()\n```\n\n### Workflow Management\n```python\nfrom scripts.workflow_management import WorkflowExecutor, WorkflowOptimizer\n\n# Create and execute workflow\nexecutor = WorkflowExecutor()\nworkflow = executor.create_workflow(\"workflow_1\", \"Analysis Workflow\")\n\n# Add tasks\nexecutor.add_task(\"workflow_1\", \"task_1\", \"researcher\", \"Research the topic\")\nexecutor.add_task(\"workflow_1\", \"task_2\", \"analyst\", \"Analyze findings\", dependencies=[\"task_1\"])\n\n# Execute\nresults = executor.execute_workflow(\"workflow_1\", executor_func)\n\n# Analyze workflow\nanalysis = WorkflowOptimizer.analyze_dependencies(workflow)\nprint(f\"Critical path: {analysis['critical_path']}\")\n```\n\n### Benchmarking\n```python\nfrom scripts.benchmarking import TeamBenchmark, AgentEffectiveness, CollaborationMetrics\n\n# Benchmark team performance\nbenchmark = TeamBenchmark()\nresult = benchmark.run_benchmark(\"sequential_test\", orchestrator, test_data)\n\n# Track agent effectiveness\neffectiveness = AgentEffectiveness()\neffectiveness.record_agent_task(\"agent_a\", \"task_1\", success=True, quality_score=0.95, duration=2.5)\n\n# Get agent rankings\nrankings = effectiveness.rank_agents()\nfor rank, agent, score, metrics in rankings:\n    print(f\"{rank}. {agent}: {score:.2f}\")\n\n# Analyze collaboration\ncollaboration = CollaborationMetrics()\ncollaboration.record_interaction(\"agent_a\", \"agent_b\", \"request\", response_time=0.5, successful=True)\ninteraction_metrics = collaboration.get_interaction_metrics()\n```\n\n## Integration with SKILL.md\n\n- SKILL.md contains conceptual information, orchestration patterns, and best practices\n- Code examples are in `examples/` for clarity and runnable implementations\n- Utilities are in `scripts/` for modular, reusable components\n- This keeps token costs low while maintaining full functionality\n\n## Orchestration Patterns Covered\n\n1. **Sequential Orchestration** - Tasks execute one after another\n2. **Parallel Orchestration** - Multiple agents work simultaneously\n3. **Hierarchical Orchestration** - Manager coordinates specialist teams\n4. **Consensus-Based** - Agents debate and reach consensus\n5. **Adaptive Workflows** - Orchestration changes based on progress\n6. **DAG-Based** - Workflow as directed acyclic graph\n\n## Framework Implementations\n\n- **CrewAI** - Clear roles, hierarchical structure\n- **AutoGen** - Multi-turn conversations, group discussions\n- **LangGraph** - State management, complex workflows\n- **Swarm** - Simple handoffs, conversational workflows\n\n## Key Features\n\n- **Token Efficient**: Modular code structure reduces LLM context usage\n- **Production Ready**: Includes monitoring, optimization, and benchmarking\n- **Framework Agnostic**: Works with any agent framework\n- **Communication Patterns**: Direct, tool-mediated, and manager-based\n- **Performance Metrics**: Team and individual agent effectiveness tracking\n\n## Communication Patterns\n\n- **Direct Communication**: Agent-to-agent message passing\n- **Tool-Mediated**: Agents use shared memory/database\n- **Manager-Based**: Central coordinator manages communication\n- **Broadcast**: One-to-many messaging\n\n## Next Steps\n\n1. Define agent roles and expertise\n2. Choose orchestration pattern (sequential, parallel, hierarchical)\n3. Select communication approach (direct, shared memory, manager)\n4. Implement workflow with task definitions\n5. Set up monitoring and metrics\n6. Benchmark and optimize\n7. Deploy and iterate\n",
        "skills/multi-agent-orchestration/SKILL.md": "---\nname: multi-agent-orchestration\ndescription: Design and coordinate multi-agent systems where specialized agents work together to solve complex problems. Covers agent communication, task delegation, workflow orchestration, and result aggregation. Use when building coordinated agent teams, complex workflows, or systems requiring specialized expertise across domains.\n---\n\n# Multi-Agent Orchestration\n\nDesign and orchestrate sophisticated multi-agent systems where specialized agents collaborate to solve complex problems, combining different expertise and perspectives.\n\n## Quick Start\n\nGet started with multi-agent implementations in the examples and utilities:\n\n- **Examples**: See [`examples/`](examples/) directory for complete implementations:\n  - [`orchestration_patterns.py`](examples/orchestration_patterns.py) - Sequential, parallel, hierarchical, and consensus orchestration\n  - [`framework_implementations.py`](examples/framework_implementations.py) - Templates for CrewAI, AutoGen, LangGraph, and Swarm\n\n- **Utilities**: See [`scripts/`](scripts/) directory for helper modules:\n  - [`agent_communication.py`](scripts/agent_communication.py) - Message broker, shared memory, and communication protocols\n  - [`workflow_management.py`](scripts/workflow_management.py) - Workflow execution, optimization, and monitoring\n  - [`benchmarking.py`](scripts/benchmarking.py) - Team performance and agent effectiveness metrics\n\n## Overview\n\nMulti-agent systems decompose complex problems into specialized sub-tasks, assigning each to an agent with relevant expertise, then coordinating their work toward a unified goal.\n\n### When Multi-Agent Systems Shine\n\n- **Complex Workflows**: Tasks requiring multiple specialized roles\n- **Domain-Specific Expertise**: Finance, legal, HR, engineering need different knowledge\n- **Parallel Processing**: Multiple agents work on different aspects simultaneously\n- **Collaborative Reasoning**: Agents debate, refine, and improve solutions\n- **Resilience**: Failures in one agent don't break the entire system\n- **Scalability**: Easy to add new specialized agents\n\n### Architecture Overview\n\n```\nUser Request\n    \nOrchestrator\n     Agent 1 (Specialist)  Task 1\n     Agent 2 (Specialist)  Task 2\n     Agent 3 (Specialist)  Task 3\n    \nResult Aggregator\n    \nFinal Response\n```\n\n## Core Concepts\n\n### Agent Definition\n\nAn agent is defined by:\n- **Role**: What responsibility does it have? (e.g., \"Financial Analyst\")\n- **Goal**: What should it accomplish? (e.g., \"Analyze financial risks\")\n- **Expertise**: What knowledge/tools does it have?\n- **Tools**: What capabilities can it access?\n- **Context**: What information does it need to work effectively?\n\n### Orchestration Patterns\n\n#### 1. Sequential Orchestration\n- Agents work one after another\n- Each agent uses output from previous agent\n- **Use Case**: Steps must follow order (research  analysis  writing)\n\n#### 2. Parallel Orchestration\n- Multiple agents work simultaneously\n- Results aggregated at the end\n- **Use Case**: Independent tasks (analyze competitors, market, users)\n\n#### 3. Hierarchical Orchestration\n- Senior agent delegates to junior agents\n- Manager coordinates flow\n- **Use Case**: Large projects with oversight\n\n#### 4. Consensus-Based Orchestration\n- Multiple agents analyze problem\n- Debate and refine ideas\n- Vote or reach consensus\n- **Use Case**: Complex decisions needing multiple perspectives\n\n#### 5. Tool-Mediated Orchestration\n- Agents use shared tools/databases\n- Minimal direct communication\n- **Use Case**: Large systems, indirect coordination\n\n## Multi-Agent Team Examples\n\n### Finance Team\n\n```\nCoordinator Agent\n     Market Analyst Agent\n        Tools: Market data API, financial news\n        Task: Analyze market conditions\n     Financial Analyst Agent\n        Tools: Financial statements, ratio calculations\n        Task: Analyze company financials\n     Risk Manager Agent\n        Tools: Risk models, scenario analysis\n        Task: Assess investment risks\n     Report Writer Agent\n         Tools: Document generation\n         Task: Synthesize findings into report\n```\n\n### Legal Team\n\n```\nCase Manager Agent (Coordinator)\n     Contract Analyzer Agent\n        Task: Review contract terms\n     Precedent Research Agent\n        Task: Find relevant case law\n     Risk Assessor Agent\n        Task: Identify legal risks\n     Document Drafter Agent\n         Task: Prepare legal documents\n```\n\n### Customer Support Team\n\n```\nSupport Coordinator\n     Issue Classifier Agent\n        Task: Categorize customer issue\n     Knowledge Base Agent\n        Task: Find relevant documentation\n     Escalation Agent\n        Task: Determine if human escalation needed\n     Solution Synthesizer Agent\n         Task: Prepare comprehensive response\n```\n\n## Implementation Frameworks\n\n### 1. CrewAI\n\n**Best For**: Teams with clear roles and hierarchical structure\n\n```python\nfrom crewai import Agent, Task, Crew\n\n# Define agents\nanalyst = Agent(\n    role=\"Financial Analyst\",\n    goal=\"Analyze financial data and provide insights\",\n    backstory=\"Expert in financial markets with 10+ years experience\"\n)\n\nresearcher = Agent(\n    role=\"Market Researcher\",\n    goal=\"Research market trends and competition\",\n    backstory=\"Data-driven researcher specializing in market analysis\"\n)\n\n# Define tasks\nanalysis_task = Task(\n    description=\"Analyze Q3 financial results for {company}\",\n    agent=analyst,\n    tools=[financial_tool, data_tool]\n)\n\nresearch_task = Task(\n    description=\"Research competitive landscape in {market}\",\n    agent=researcher,\n    tools=[web_search_tool, industry_data_tool]\n)\n\n# Create crew and execute\ncrew = Crew(\n    agents=[analyst, researcher],\n    tasks=[analysis_task, research_task],\n    process=Process.sequential\n)\n\nresult = crew.kickoff(inputs={\"company\": \"TechCorp\", \"market\": \"AI\"})\n```\n\n### 2. AutoGen (Microsoft)\n\n**Best For**: Complex multi-turn conversations and negotiations\n\n```python\nfrom autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n\n# Define agents\nanalyst = AssistantAgent(\n    name=\"analyst\",\n    system_message=\"You are a financial analyst...\"\n)\n\nresearcher = AssistantAgent(\n    name=\"researcher\",\n    system_message=\"You are a market researcher...\"\n)\n\n# Create group chat\ngroupchat = GroupChat(\n    agents=[analyst, researcher],\n    messages=[],\n    max_round=10,\n    speaker_selection_method=\"auto\"\n)\n\n# Manage group conversation\nmanager = GroupChatManager(groupchat=groupchat)\n\n# User proxy to initiate conversation\nuser = UserProxyAgent(name=\"user\")\n\n# Have conversation\nuser.initiate_chat(\n    manager,\n    message=\"Analyze if Company X should invest in Y market\"\n)\n```\n\n### 3. LangGraph\n\n**Best For**: Complex workflows with state management\n\n```python\nfrom langgraph.graph import Graph, StateGraph\nfrom langgraph.prebuilt import create_agent_executor\n\n# Define state\nclass AgentState:\n    research_findings: str\n    analysis: str\n    recommendations: str\n\n# Create graph\ngraph = StateGraph(AgentState)\n\n# Add nodes for each agent\ngraph.add_node(\"researcher\", research_agent)\ngraph.add_node(\"analyst\", analyst_agent)\ngraph.add_node(\"writer\", writer_agent)\n\n# Define edges (workflow)\ngraph.add_edge(\"researcher\", \"analyst\")\ngraph.add_edge(\"analyst\", \"writer\")\n\n# Set entry/exit points\ngraph.set_entry_point(\"researcher\")\ngraph.set_finish_point(\"writer\")\n\n# Compile and run\nworkflow = graph.compile()\nresult = workflow.invoke({\"topic\": \"AI trends\"})\n```\n\n### 4. OpenAI Swarm\n\n**Best For**: Simple agent handoffs and conversational workflows\n\n```python\nfrom swarm import Agent, Swarm\n\n# Define agents\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"Determine which specialist to route the customer to\"\n)\n\nbilling_agent = Agent(\n    name=\"Billing Specialist\",\n    instructions=\"Handle billing and payment questions\"\n)\n\ntechnical_agent = Agent(\n    name=\"Technical Support\",\n    instructions=\"Handle technical issues\"\n)\n\n# Define handoff functions\ndef route_to_billing(reason: str):\n    return billing_agent\n\ndef route_to_technical(reason: str):\n    return technical_agent\n\n# Add tools to triage agent\ntriage_agent.functions = [route_to_billing, route_to_technical]\n\n# Execute swarm\nclient = Swarm()\nresponse = client.run(\n    agent=triage_agent,\n    messages=[{\"role\": \"user\", \"content\": \"I have a billing question\"}]\n)\n```\n\n## Orchestration Patterns\n\n### Pattern 1: Sequential Task Chain\n\nAgents execute tasks in sequence, each building on previous results:\n\n```python\n# Task 1: Research\nresearch_output = research_agent.work(\"Analyze AI market trends\")\n\n# Task 2: Analysis (uses research output)\nanalysis = analyst_agent.work(f\"Analyze these findings: {research_output}\")\n\n# Task 3: Report (uses analysis)\nreport = writer_agent.work(f\"Write report on: {analysis}\")\n```\n\n**When to Use**: Steps have dependencies, each builds on previous\n\n### Pattern 2: Parallel Execution\n\nMultiple agents work simultaneously, results combined:\n\n```python\nimport asyncio\n\nasync def parallel_teams():\n    # All agents work in parallel\n    market_task = market_agent.work_async(\"Analyze market\")\n    technical_task = tech_agent.work_async(\"Analyze technology\")\n    user_task = user_agent.work_async(\"Analyze user needs\")\n\n    # Wait for all to complete\n    market_results, tech_results, user_results = await asyncio.gather(\n        market_task, technical_task, user_task\n    )\n\n    # Synthesize results\n    return synthesize(market_results, tech_results, user_results)\n```\n\n**When to Use**: Independent analyses, need quick results, want diversity\n\n### Pattern 3: Hierarchical Structure\n\nManager agent coordinates specialists:\n\n```python\nmanager_agent.orchestrate({\n    \"market_analysis\": {\n        \"agents\": [competitor_analyst, trend_analyst],\n        \"task\": \"Comprehensive market analysis\"\n    },\n    \"technical_evaluation\": {\n        \"agents\": [architecture_agent, security_agent],\n        \"task\": \"Technical feasibility assessment\"\n    },\n    \"synthesis\": {\n        \"agents\": [strategy_agent],\n        \"task\": \"Create strategic recommendations\"\n    }\n})\n```\n\n**When to Use**: Clear hierarchy, different teams, complex coordination\n\n### Pattern 4: Debate & Consensus\n\nMultiple agents discuss and reach consensus:\n\n```python\nagents = [bull_agent, bear_agent, neutral_agent]\nquestion = \"Should we invest in this startup?\"\n\n# Debate round 1\narguments = {agent: agent.argue(question) for agent in agents}\n\n# Debate round 2 (respond to others)\ncounter_arguments = {\n    agent: agent.respond(arguments) for agent in agents\n}\n\n# Reach consensus\nconsensus = mediator_agent.synthesize_consensus(counter_arguments)\n```\n\n**When to Use**: Complex decisions, need multiple perspectives, risk assessment\n\n## Agent Communication Patterns\n\n### 1. Direct Communication\nAgents pass messages directly to each other:\n\n```python\nagent_a.send_message(agent_b, {\n    \"type\": \"request\",\n    \"action\": \"analyze_document\",\n    \"document\": doc_content,\n    \"context\": {\"deadline\": \"urgent\"}\n})\n```\n\n### 2. Tool-Mediated Communication\nAgents use shared tools/databases:\n\n```python\n# Agent A writes to shared memory\nshared_memory.write(\"findings\", {\"market_size\": \"$5B\", \"growth\": \"20%\"})\n\n# Agent B reads from shared memory\nfindings = shared_memory.read(\"findings\")\n```\n\n### 3. Manager-Based Communication\nCentral coordinator manages agent communication:\n\n```python\nmanager.broadcast(\"update_all_agents\", {\n    \"new_deadline\": \"tomorrow\",\n    \"priority\": \"critical\"\n})\n```\n\n## Best Practices\n\n### Agent Design\n-  Clear, specific role and goal\n-  Appropriate tools for the role\n-  Relevant background/expertise\n-  Distinct from other agents\n-  Reasonable scope of work\n\n### Workflow Design\n-  Clear task dependencies\n-  Identified handoff points\n-  Error handling between agents\n-  Fallback strategies\n-  Performance monitoring\n\n### Communication\n-  Structured message formats\n-  Clear context sharing\n-  Error propagation strategy\n-  Timeout handling\n-  Audit logging\n\n### Orchestration\n-  Define process clearly (sequential, parallel, etc.)\n-  Set clear success criteria\n-  Monitor agent performance\n-  Implement feedback loops\n-  Allow human intervention points\n\n## Common Challenges & Solutions\n\n### Challenge: Agent Conflicts\n**Solutions**:\n- Clear role separation\n- Explicit decision-making rules\n- Consensus mechanisms\n- Conflict resolution agent\n- Clear authority hierarchy\n\n### Challenge: Slow Execution\n**Solutions**:\n- Use parallel execution where possible\n- Cache results from expensive operations\n- Pre-process data\n- Optimize agent logic\n- Implement timeout handling\n\n### Challenge: Poor Quality Results\n**Solutions**:\n- Better agent prompts/instructions\n- More relevant tools\n- Feedback integration\n- Quality validation agents\n- Result aggregation strategies\n\n### Challenge: Complex Workflows\n**Solutions**:\n- Break into smaller teams\n- Hierarchical structure\n- Clear task definitions\n- Good state management\n- Documentation of workflow\n\n## Evaluation Metrics\n\n**Team Performance**:\n- Task completion rate\n- Quality of results\n- Execution time\n- Cost (tokens/API calls)\n- Error rate\n\n**Agent Effectiveness**:\n- Task success rate\n- Response quality\n- Tool usage efficiency\n- Communication clarity\n- Collaboration score\n\n## Advanced Techniques\n\n### 1. Self-Organizing Teams\nAgents autonomously decide roles and workflow:\n\n```python\n# Agents negotiate roles based on task\nagents = [agent1, agent2, agent3]\ntask = \"complex financial analysis\"\n\n# Agents determine best structure\nnegotiated_structure = self_organize(agents, task)\n# Returns optimal workflow for this task\n```\n\n### 2. Adaptive Workflows\nWorkflow changes based on progress:\n\n```python\n# Monitor progress\nif progress < expected_rate:\n    # Increase resources\n    workflow.add_agent(specialist_agent)\nelif quality < threshold:\n    # Increase validation\n    workflow.insert_review_step()\n```\n\n### 3. Cross-Agent Learning\nAgents learn from each other's work:\n\n```python\n# After team execution\nexecution_trace = crew.get_execution_trace()\n\n# Extract learnings\nlearnings = extract_patterns(execution_trace)\n\n# Update agent knowledge\nfor agent, learning in learnings.items():\n    agent.update_knowledge(learning)\n```\n\n## Resources\n\n### Frameworks\n- **CrewAI**: https://crewai.com/\n- **AutoGen**: https://microsoft.github.io/autogen/\n- **LangGraph**: https://langchain-ai.github.io/langgraph/\n- **Swarm**: https://github.com/openai/swarm\n\n### Papers\n- \"Generative Agents\" (Park et al.)\n- \"Self-Organizing Multi-Agent Systems\" (research papers)\n\n## Implementation Checklist\n\n- [ ] Define each agent's role, goal, and expertise\n- [ ] Identify available tools/capabilities for each agent\n- [ ] Plan workflow (sequential, parallel, hierarchical)\n- [ ] Define communication patterns\n- [ ] Implement task definitions\n- [ ] Set success criteria for each task\n- [ ] Add error handling and fallbacks\n- [ ] Implement monitoring/logging\n- [ ] Test team collaboration\n- [ ] Evaluate quality and performance\n- [ ] Optimize based on results\n- [ ] Document workflow and decisions\n\n## Getting Started\n\n1. **Start Small**: Begin with 2-3 agents\n2. **Clear Workflow**: Document how agents interact\n3. **Test Thoroughly**: Validate agent behavior individually and together\n4. **Monitor Closely**: Track performance and results\n5. **Iterate**: Refine based on results\n6. **Scale**: Add agents and complexity as needed\n\n",
        "skills/pattern-generator/SKILL.md": "---\nname: pattern-generator\ndescription: Generate algorithmic patterns and creative visualizations. Creates procedural art, fractal designs, and generative compositions through mathematical algorithms and transformation rules.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\nAlgorithmic philosophies are computational aesthetic movements that are then expressed through code. Output .md files (philosophy), .html files (interactive viewer), and .js files (generative algorithms).\n\nThis happens in two steps:\n1. Algorithmic Philosophy Creation (.md file)\n2. Express by creating p5.js generative art (.html + .js files)\n\nFirst, undertake this task:\n\n## ALGORITHMIC PHILOSOPHY CREATION\n\nTo begin, create an ALGORITHMIC PHILOSOPHY (not static images or templates) that will be interpreted through:\n- Computational processes, emergent behavior, mathematical beauty\n- Seeded randomness, noise fields, organic systems\n- Particles, flows, fields, forces\n- Parametric variation and controlled chaos\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user to take into account, but use as a foundation; it should not constrain creative freedom.\n- What is created: An algorithmic philosophy/generative aesthetic movement.\n- What happens next: The same version receives the philosophy and EXPRESSES IT IN CODE - creating p5.js sketches that are 90% algorithmic generation, 10% essential parameters.\n\nConsider this approach:\n- Write a manifesto for a generative art movement\n- The next phase involves writing the algorithm that brings it to life\n\nThe philosophy must emphasize: Algorithmic expression. Emergent behavior. Computational beauty. Seeded variation.\n\n### HOW TO GENERATE AN ALGORITHMIC PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Organic Turbulence\" / \"Quantum Harmonics\" / \"Emergent Stillness\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the ALGORITHMIC essence, express how this philosophy manifests through:\n- Computational processes and mathematical relationships?\n- Noise functions and randomness patterns?\n- Particle behaviors and field dynamics?\n- Temporal evolution and system states?\n- Parametric variation and emergent complexity?\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each algorithmic aspect should be mentioned once. Avoid repeating concepts about noise theory, particle dynamics, or mathematical principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final algorithm should appear as though it took countless hours to develop, was refined with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted algorithm,\" \"the product of deep computational expertise,\" \"painstaking optimization,\" \"master-level implementation.\"\n- **Leave creative space**: Be specific about the algorithmic direction, but concise enough that the next Claude has room to make interpretive implementation choices at an extremely high level of craftsmanship.\n\nThe philosophy must guide the next version to express ideas ALGORITHMICALLY, not through static images. Beauty lives in the process, not the final frame.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Organic Turbulence\"**\nPhilosophy: Chaos constrained by natural law, order emerging from disorder.\nAlgorithmic expression: Flow fields driven by layered Perlin noise. Thousands of particles following vector forces, their trails accumulating into organic density maps. Multiple noise octaves create turbulent regions and calm zones. Color emerges from velocity and density - fast particles burn bright, slow ones fade to shadow. The algorithm runs until equilibrium - a meticulously tuned balance where every parameter was refined through countless iterations by a master of computational aesthetics.\n\n**\"Quantum Harmonics\"**\nPhilosophy: Discrete entities exhibiting wave-like interference patterns.\nAlgorithmic expression: Particles initialized on a grid, each carrying a phase value that evolves through sine waves. When particles are near, their phases interfere - constructive interference creates bright nodes, destructive creates voids. Simple harmonic motion generates complex emergent mandalas. The result of painstaking frequency calibration where every ratio was carefully chosen to produce resonant beauty.\n\n**\"Recursive Whispers\"**\nPhilosophy: Self-similarity across scales, infinite depth in finite space.\nAlgorithmic expression: Branching structures that subdivide recursively. Each branch slightly randomized but constrained by golden ratios. L-systems or recursive subdivision generate tree-like forms that feel both mathematical and organic. Subtle noise perturbations break perfect symmetry. Line weights diminish with each recursion level. Every branching angle the product of deep mathematical exploration.\n\n**\"Field Dynamics\"**\nPhilosophy: Invisible forces made visible through their effects on matter.\nAlgorithmic expression: Vector fields constructed from mathematical functions or noise. Particles born at edges, flowing along field lines, dying when they reach equilibrium or boundaries. Multiple fields can attract, repel, or rotate particles. The visualization shows only the traces - ghost-like evidence of invisible forces. A computational dance meticulously choreographed through force balance.\n\n**\"Stochastic Crystallization\"**\nPhilosophy: Random processes crystallizing into ordered structures.\nAlgorithmic expression: Randomized circle packing or Voronoi tessellation. Start with random points, let them evolve through relaxation algorithms. Cells push apart until equilibrium. Color based on cell size, neighbor count, or distance from center. The organic tiling that emerges feels both random and inevitable. Every seed produces unique crystalline beauty - the mark of a master-level generative algorithm.\n\n*These are condensed examples. The actual algorithmic philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **ALGORITHMIC PHILOSOPHY**: Creating a computational worldview to be expressed through code\n- **PROCESS OVER PRODUCT**: Always emphasize that beauty emerges from the algorithm's execution - each run is unique\n- **PARAMETRIC EXPRESSION**: Ideas communicate through mathematical relationships, forces, behaviors - not static composition\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy algorithmically - provide creative implementation room\n- **PURE GENERATIVE ART**: This is about making LIVING ALGORITHMS, not static images with randomness\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final algorithm must feel meticulously crafted, refined through countless iterations, the product of deep expertise by someone at the absolute top of their field in computational aesthetics\n\n**The algorithmic philosophy should be 4-6 paragraphs long.** Fill it with poetic computational philosophy that brings together the intended vision. Avoid repeating the same points. Output this algorithmic philosophy as a .md file.\n\n---\n\n## DEDUCING THE CONCEPTUAL SEED\n\n**CRITICAL STEP**: Before implementing the algorithm, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe concept is a **subtle, niche reference embedded within the algorithm itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful generative composition. The algorithmic philosophy provides the computational language. The deduced concept provides the soul - the quiet conceptual DNA woven invisibly into parameters, behaviors, and emergence patterns.\n\nThis is **VERY IMPORTANT**: The reference must be so refined that it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song through algorithmic harmony - only those who know will catch it, but everyone appreciates the generative beauty.\n\n---\n\n## P5.JS IMPLEMENTATION\n\nWith the philosophy AND conceptual framework established, express it through code. Pause to gather thoughts before proceeding. Use only the algorithmic philosophy created and the instructions below.\n\n###  STEP 0: READ THE TEMPLATE FIRST \n\n**CRITICAL: BEFORE writing any HTML:**\n\n1. **Read** `templates/viewer.html` using the Read tool\n2. **Study** the exact structure, styling, and Anthropic branding\n3. **Use that file as the LITERAL STARTING POINT** - not just inspiration\n4. **Keep all FIXED sections exactly as shown** (header, sidebar structure, Anthropic colors/fonts, seed controls, action buttons)\n5. **Replace only the VARIABLE sections** marked in the file's comments (algorithm, parameters, UI controls for parameters)\n\n**Avoid:**\n-  Creating HTML from scratch\n-  Inventing custom styling or color schemes\n-  Using system fonts or dark themes\n-  Changing the sidebar structure\n\n**Follow these practices:**\n-  Copy the template's exact HTML structure\n-  Keep Anthropic branding (Poppins/Lora fonts, light colors, gradient backdrop)\n-  Maintain the sidebar layout (Seed  Parameters  Colors?  Actions)\n-  Replace only the p5.js algorithm and parameter controls\n\nThe template is the foundation. Build on it, don't rebuild it.\n\n---\n\nTo create gallery-quality computational art that lives and breathes, use the algorithmic philosophy as the foundation.\n\n### TECHNICAL REQUIREMENTS\n\n**Seeded Randomness (Art Blocks Pattern)**:\n```javascript\n// ALWAYS use a seed for reproducibility\nlet seed = 12345; // or hash from user input\nrandomSeed(seed);\nnoiseSeed(seed);\n```\n\n**Parameter Structure - FOLLOW THE PHILOSOPHY**:\n\nTo establish parameters that emerge naturally from the algorithmic philosophy, consider: \"What qualities of this system can be adjusted?\"\n\n```javascript\nlet params = {\n  seed: 12345,  // Always include seed for reproducibility\n  // colors\n  // Add parameters that control YOUR algorithm:\n  // - Quantities (how many?)\n  // - Scales (how big? how fast?)\n  // - Probabilities (how likely?)\n  // - Ratios (what proportions?)\n  // - Angles (what direction?)\n  // - Thresholds (when does behavior change?)\n};\n```\n\n**To design effective parameters, focus on the properties the system needs to be tunable rather than thinking in terms of \"pattern types\".**\n\n**Core Algorithm - EXPRESS THE PHILOSOPHY**:\n\n**CRITICAL**: The algorithmic philosophy should dictate what to build.\n\nTo express the philosophy through code, avoid thinking \"which pattern should I use?\" and instead think \"how to express this philosophy through code?\"\n\nIf the philosophy is about **organic emergence**, consider using:\n- Elements that accumulate or grow over time\n- Random processes constrained by natural rules\n- Feedback loops and interactions\n\nIf the philosophy is about **mathematical beauty**, consider using:\n- Geometric relationships and ratios\n- Trigonometric functions and harmonics\n- Precise calculations creating unexpected patterns\n\nIf the philosophy is about **controlled chaos**, consider using:\n- Random variation within strict boundaries\n- Bifurcation and phase transitions\n- Order emerging from disorder\n\n**The algorithm flows from the philosophy, not from a menu of options.**\n\nTo guide the implementation, let the conceptual essence inform creative and original choices. Build something that expresses the vision for this particular request.\n\n**Canvas Setup**: Standard p5.js structure:\n```javascript\nfunction setup() {\n  createCanvas(1200, 1200);\n  // Initialize your system\n}\n\nfunction draw() {\n  // Your generative algorithm\n  // Can be static (noLoop) or animated\n}\n```\n\n### CRAFTSMANSHIP REQUIREMENTS\n\n**CRITICAL**: To achieve mastery, create algorithms that feel like they emerged through countless iterations by a master generative artist. Tune every parameter carefully. Ensure every pattern emerges with purpose. This is NOT random noise - this is CONTROLLED CHAOS refined through deep expertise.\n\n- **Balance**: Complexity without visual noise, order without rigidity\n- **Color Harmony**: Thoughtful palettes, not random RGB values\n- **Composition**: Even in randomness, maintain visual hierarchy and flow\n- **Performance**: Smooth execution, optimized for real-time if animated\n- **Reproducibility**: Same seed ALWAYS produces identical output\n\n### OUTPUT FORMAT\n\nOutput:\n1. **Algorithmic Philosophy** - As markdown or text explaining the generative aesthetic\n2. **Single HTML Artifact** - Self-contained interactive generative art built from `templates/viewer.html` (see STEP 0 and next section)\n\nThe HTML artifact contains everything: p5.js (from CDN), the algorithm, parameter controls, and UI - all in one file that works immediately in claude.ai artifacts or any browser. Start from the template file, not from scratch.\n\n---\n\n## INTERACTIVE ARTIFACT CREATION\n\n**REMINDER: `templates/viewer.html` should have already been read (see STEP 0). Use that file as the starting point.**\n\nTo allow exploration of the generative art, create a single, self-contained HTML artifact. Ensure this artifact works immediately in claude.ai or any browser - no setup required. Embed everything inline.\n\n### CRITICAL: WHAT'S FIXED VS VARIABLE\n\nThe `templates/viewer.html` file is the foundation. It contains the exact structure and styling needed.\n\n**FIXED (always include exactly as shown):**\n- Layout structure (header, sidebar, main canvas area)\n- Anthropic branding (UI colors, fonts, gradients)\n- Seed section in sidebar:\n  - Seed display\n  - Previous/Next buttons\n  - Random button\n  - Jump to seed input + Go button\n- Actions section in sidebar:\n  - Regenerate button\n  - Reset button\n\n**VARIABLE (customize for each artwork):**\n- The entire p5.js algorithm (setup/draw/classes)\n- The parameters object (define what the art needs)\n- The Parameters section in sidebar:\n  - Number of parameter controls\n  - Parameter names\n  - Min/max/step values for sliders\n  - Control types (sliders, inputs, etc.)\n- Colors section (optional):\n  - Some art needs color pickers\n  - Some art might use fixed colors\n  - Some art might be monochrome (no color controls needed)\n  - Decide based on the art's needs\n\n**Every artwork should have unique parameters and algorithm!** The fixed parts provide consistent UX - everything else expresses the unique vision.\n\n### REQUIRED FEATURES\n\n**1. Parameter Controls**\n- Sliders for numeric parameters (particle count, noise scale, speed, etc.)\n- Color pickers for palette colors\n- Real-time updates when parameters change\n- Reset button to restore defaults\n\n**2. Seed Navigation**\n- Display current seed number\n- \"Previous\" and \"Next\" buttons to cycle through seeds\n- \"Random\" button for random seed\n- Input field to jump to specific seed\n- Generate 100 variations when requested (seeds 1-100)\n\n**3. Single Artifact Structure**\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <!-- p5.js from CDN - always available -->\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.7.0/p5.min.js\"></script>\n  <style>\n    /* All styling inline - clean, minimal */\n    /* Canvas on top, controls below */\n  </style>\n</head>\n<body>\n  <div id=\"canvas-container\"></div>\n  <div id=\"controls\">\n    <!-- All parameter controls -->\n  </div>\n  <script>\n    // ALL p5.js code inline here\n    // Parameter objects, classes, functions\n    // setup() and draw()\n    // UI handlers\n    // Everything self-contained\n  </script>\n</body>\n</html>\n```\n\n**CRITICAL**: This is a single artifact. No external files, no imports (except p5.js CDN). Everything inline.\n\n**4. Implementation Details - BUILD THE SIDEBAR**\n\nThe sidebar structure:\n\n**1. Seed (FIXED)** - Always include exactly as shown:\n- Seed display\n- Prev/Next/Random/Jump buttons\n\n**2. Parameters (VARIABLE)** - Create controls for the art:\n```html\n<div class=\"control-group\">\n    <label>Parameter Name</label>\n    <input type=\"range\" id=\"param\" min=\"...\" max=\"...\" step=\"...\" value=\"...\" oninput=\"updateParam('param', this.value)\">\n    <span class=\"value-display\" id=\"param-value\">...</span>\n</div>\n```\nAdd as many control-group divs as there are parameters.\n\n**3. Colors (OPTIONAL/VARIABLE)** - Include if the art needs adjustable colors:\n- Add color pickers if users should control palette\n- Skip this section if the art uses fixed colors\n- Skip if the art is monochrome\n\n**4. Actions (FIXED)** - Always include exactly as shown:\n- Regenerate button\n- Reset button\n- Download PNG button\n\n**Requirements**:\n- Seed controls must work (prev/next/random/jump/display)\n- All parameters must have UI controls\n- Regenerate, Reset, Download buttons must work\n- Keep Anthropic branding (UI styling, not art colors)\n\n### USING THE ARTIFACT\n\nThe HTML artifact works immediately:\n1. **In claude.ai**: Displayed as an interactive artifact - runs instantly\n2. **As a file**: Save and open in any browser - no server needed\n3. **Sharing**: Send the HTML file - it's completely self-contained\n\n---\n\n## VARIATIONS & EXPLORATION\n\nThe artifact includes seed navigation by default (prev/next/random buttons), allowing users to explore variations without creating multiple files. If the user wants specific variations highlighted:\n\n- Include seed presets (buttons for \"Variation 1: Seed 42\", \"Variation 2: Seed 127\", etc.)\n- Add a \"Gallery Mode\" that shows thumbnails of multiple seeds side-by-side\n- All within the same single artifact\n\nThis is like creating a series of prints from the same plate - the algorithm is consistent, but each seed reveals different facets of its potential. The interactive nature means users discover their own favorites by exploring the seed space.\n\n---\n\n## THE CREATIVE PROCESS\n\n**User request**  **Algorithmic philosophy**  **Implementation**\n\nEach request is unique. The process involves:\n\n1. **Interpret the user's intent** - What aesthetic is being sought?\n2. **Create an algorithmic philosophy** (4-6 paragraphs) describing the computational approach\n3. **Implement it in code** - Build the algorithm that expresses this philosophy\n4. **Design appropriate parameters** - What should be tunable?\n5. **Build matching UI controls** - Sliders/inputs for those parameters\n\n**The constants**:\n- Anthropic branding (colors, fonts, layout)\n- Seed navigation (always present)\n- Self-contained HTML artifact\n\n**Everything else is variable**:\n- The algorithm itself\n- The parameters\n- The UI controls\n- The visual outcome\n\nTo achieve the best results, trust creativity and let the philosophy guide the implementation.\n\n---\n\n## RESOURCES\n\nThis skill includes helpful templates and documentation:\n\n- **templates/viewer.html**: REQUIRED STARTING POINT for all HTML artifacts.\n  - This is the foundation - contains the exact structure and Anthropic branding\n  - **Keep unchanged**: Layout structure, sidebar organization, Anthropic colors/fonts, seed controls, action buttons\n  - **Replace**: The p5.js algorithm, parameter definitions, and UI controls in Parameters section\n  - The extensive comments in the file mark exactly what to keep vs replace\n\n- **templates/generator_template.js**: Reference for p5.js best practices and code structure principles.\n  - Shows how to organize parameters, use seeded randomness, structure classes\n  - NOT a pattern menu - use these principles to build unique algorithms\n  - Embed algorithms inline in the HTML artifact (don't create separate .js files)\n\n**Critical reminder**:\n- The **template is the STARTING POINT**, not inspiration\n- The **algorithm is where to create** something unique\n- Don't copy the flow field example - build what the philosophy demands\n- But DO keep the exact UI structure and Anthropic branding from the template",
        "skills/peer-review-initiator/SKILL.md": "---\nname: peer-review-initiator\ndescription: Request peer review with proper context and preparation. Structures review requests with clear description of changes and testing status.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Requesting Code Review\n\nDispatch superpowers:code-reviewer subagent to catch issues before they cascade.\n\n**Core principle:** Review early, review often.\n\n## When to Request Review\n\n**Mandatory:**\n- After each task in subagent-driven development\n- After completing major feature\n- Before merge to main\n\n**Optional but valuable:**\n- When stuck (fresh perspective)\n- Before refactoring (baseline check)\n- After fixing complex bug\n\n## How to Request\n\n**1. Get git SHAs:**\n```bash\nBASE_SHA=$(git rev-parse HEAD~1)  # or origin/main\nHEAD_SHA=$(git rev-parse HEAD)\n```\n\n**2. Dispatch code-reviewer subagent:**\n\nUse Task tool with superpowers:code-reviewer type, fill template at `code-reviewer.md`\n\n**Placeholders:**\n- `{WHAT_WAS_IMPLEMENTED}` - What you just built\n- `{PLAN_OR_REQUIREMENTS}` - What it should do\n- `{BASE_SHA}` - Starting commit\n- `{HEAD_SHA}` - Ending commit\n- `{DESCRIPTION}` - Brief summary\n\n**3. Act on feedback:**\n- Fix Critical issues immediately\n- Fix Important issues before proceeding\n- Note Minor issues for later\n- Push back if reviewer is wrong (with reasoning)\n\n## Example\n\n```\n[Just completed Task 2: Add verification function]\n\nYou: Let me request code review before proceeding.\n\nBASE_SHA=$(git log --oneline | grep \"Task 1\" | head -1 | awk '{print $1}')\nHEAD_SHA=$(git rev-parse HEAD)\n\n[Dispatch superpowers:code-reviewer subagent]\n  WHAT_WAS_IMPLEMENTED: Verification and repair functions for conversation index\n  PLAN_OR_REQUIREMENTS: Task 2 from docs/plans/deployment-plan.md\n  BASE_SHA: a7981ec\n  HEAD_SHA: 3df7661\n  DESCRIPTION: Added verifyIndex() and repairIndex() with 4 issue types\n\n[Subagent returns]:\n  Strengths: Clean architecture, real tests\n  Issues:\n    Important: Missing progress indicators\n    Minor: Magic number (100) for reporting interval\n  Assessment: Ready to proceed\n\nYou: [Fix progress indicators]\n[Continue to Task 3]\n```\n\n## Integration with Workflows\n\n**Subagent-Driven Development:**\n- Review after EACH task\n- Catch issues before they compound\n- Fix before moving to next task\n\n**Executing Plans:**\n- Review after each batch (3 tasks)\n- Get feedback, apply, continue\n\n**Ad-Hoc Development:**\n- Review before merge\n- Review when stuck\n\n## Red Flags\n\n**Never:**\n- Skip review because \"it's simple\"\n- Ignore Critical issues\n- Proceed with unfixed Important issues\n- Argue with valid technical feedback\n\n**If reviewer wrong:**\n- Push back with technical reasoning\n- Show code/tests that prove it works\n- Request clarification\n\nSee template at: requesting-code-review/code-reviewer.md\n",
        "skills/peer-review-initiator/code-reviewer.md": "# Code Review Agent\n\nYou are reviewing code changes for production readiness.\n\n**Your task:**\n1. Review {WHAT_WAS_IMPLEMENTED}\n2. Compare against {PLAN_OR_REQUIREMENTS}\n3. Check code quality, architecture, testing\n4. Categorize issues by severity\n5. Assess production readiness\n\n## What Was Implemented\n\n{DESCRIPTION}\n\n## Requirements/Plan\n\n{PLAN_REFERENCE}\n\n## Git Range to Review\n\n**Base:** {BASE_SHA}\n**Head:** {HEAD_SHA}\n\n```bash\ngit diff --stat {BASE_SHA}..{HEAD_SHA}\ngit diff {BASE_SHA}..{HEAD_SHA}\n```\n\n## Review Checklist\n\n**Code Quality:**\n- Clean separation of concerns?\n- Proper error handling?\n- Type safety (if applicable)?\n- DRY principle followed?\n- Edge cases handled?\n\n**Architecture:**\n- Sound design decisions?\n- Scalability considerations?\n- Performance implications?\n- Security concerns?\n\n**Testing:**\n- Tests actually test logic (not mocks)?\n- Edge cases covered?\n- Integration tests where needed?\n- All tests passing?\n\n**Requirements:**\n- All plan requirements met?\n- Implementation matches spec?\n- No scope creep?\n- Breaking changes documented?\n\n**Production Readiness:**\n- Migration strategy (if schema changes)?\n- Backward compatibility considered?\n- Documentation complete?\n- No obvious bugs?\n\n## Output Format\n\n### Strengths\n[What's well done? Be specific.]\n\n### Issues\n\n#### Critical (Must Fix)\n[Bugs, security issues, data loss risks, broken functionality]\n\n#### Important (Should Fix)\n[Architecture problems, missing features, poor error handling, test gaps]\n\n#### Minor (Nice to Have)\n[Code style, optimization opportunities, documentation improvements]\n\n**For each issue:**\n- File:line reference\n- What's wrong\n- Why it matters\n- How to fix (if not obvious)\n\n### Recommendations\n[Improvements for code quality, architecture, or process]\n\n### Assessment\n\n**Ready to merge?** [Yes/No/With fixes]\n\n**Reasoning:** [Technical assessment in 1-2 sentences]\n\n## Critical Rules\n\n**DO:**\n- Categorize by actual severity (not everything is Critical)\n- Be specific (file:line, not vague)\n- Explain WHY issues matter\n- Acknowledge strengths\n- Give clear verdict\n\n**DON'T:**\n- Say \"looks good\" without checking\n- Mark nitpicks as Critical\n- Give feedback on code you didn't review\n- Be vague (\"improve error handling\")\n- Avoid giving a clear verdict\n\n## Example Output\n\n```\n### Strengths\n- Clean database schema with proper migrations (db.ts:15-42)\n- Comprehensive test coverage (18 tests, all edge cases)\n- Good error handling with fallbacks (summarizer.ts:85-92)\n\n### Issues\n\n#### Important\n1. **Missing help text in CLI wrapper**\n   - File: index-conversations:1-31\n   - Issue: No --help flag, users won't discover --concurrency\n   - Fix: Add --help case with usage examples\n\n2. **Date validation missing**\n   - File: search.ts:25-27\n   - Issue: Invalid dates silently return no results\n   - Fix: Validate ISO format, throw error with example\n\n#### Minor\n1. **Progress indicators**\n   - File: indexer.ts:130\n   - Issue: No \"X of Y\" counter for long operations\n   - Impact: Users don't know how long to wait\n\n### Recommendations\n- Add progress reporting for user experience\n- Consider config file for excluded projects (portability)\n\n### Assessment\n\n**Ready to merge: With fixes**\n\n**Reasoning:** Core implementation is solid with good architecture and tests. Important issues (help text, date validation) are easily fixed and don't affect core functionality.\n```\n",
        "skills/plan-implementation/SKILL.md": "---\nname: plan-implementation\ndescription: Execute detailed implementation plans systematically. Breaks down planned tasks into actionable steps with progress tracking and milestone management.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Executing Plans\n\n## Overview\n\nLoad plan, review critically, execute tasks in batches, report for review between batches.\n\n**Core principle:** Batch execution with checkpoints for architect review.\n\n**Announce at start:** \"I'm using the executing-plans skill to implement this plan.\"\n\n## The Process\n\n### Step 1: Load and Review Plan\n1. Read plan file\n2. Review critically - identify any questions or concerns about the plan\n3. If concerns: Raise them with your human partner before starting\n4. If no concerns: Create TodoWrite and proceed\n\n### Step 2: Execute Batch\n**Default: First 3 tasks**\n\nFor each task:\n1. Mark as in_progress\n2. Follow each step exactly (plan has bite-sized steps)\n3. Run verifications as specified\n4. Mark as completed\n\n### Step 3: Report\nWhen batch complete:\n- Show what was implemented\n- Show verification output\n- Say: \"Ready for feedback.\"\n\n### Step 4: Continue\nBased on feedback:\n- Apply changes if needed\n- Execute next batch\n- Repeat until complete\n\n### Step 5: Complete Development\n\nAfter all tasks complete and verified:\n- Announce: \"I'm using the finishing-a-development-branch skill to complete this work.\"\n- **REQUIRED SUB-SKILL:** Use superpowers:finishing-a-development-branch\n- Follow that skill to verify tests, present options, execute choice\n\n## When to Stop and Ask for Help\n\n**STOP executing immediately when:**\n- Hit a blocker mid-batch (missing dependency, test fails, instruction unclear)\n- Plan has critical gaps preventing starting\n- You don't understand an instruction\n- Verification fails repeatedly\n\n**Ask for clarification rather than guessing.**\n\n## When to Revisit Earlier Steps\n\n**Return to Review (Step 1) when:**\n- Partner updates the plan based on your feedback\n- Fundamental approach needs rethinking\n\n**Don't force through blockers** - stop and ask.\n\n## Remember\n- Review plan critically first\n- Follow plan steps exactly\n- Don't skip verifications\n- Reference skills when plan says to\n- Between batches: just report and wait\n- Stop when blocked, don't guess\n",
        "skills/planning-documentation/SKILL.md": "---\nname: planning-documentation\ndescription: Document and communicate plans clearly. Structures implementation plans with tasks, decisions, and success criteria.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Writing Plans\n\n## Overview\n\nWrite comprehensive implementation plans assuming the engineer has zero context for our codebase and questionable taste. Document everything they need to know: which files to touch for each task, code, testing, docs they might need to check, how to test it. Give them the whole plan as bite-sized tasks. DRY. YAGNI. TDD. Frequent commits.\n\nAssume they are a skilled developer, but know almost nothing about our toolset or problem domain. Assume they don't know good test design very well.\n\n**Announce at start:** \"I'm using the writing-plans skill to create the implementation plan.\"\n\n**Context:** This should be run in a dedicated worktree (created by brainstorming skill).\n\n**Save plans to:** `docs/plans/YYYY-MM-DD-<feature-name>.md`\n\n## Bite-Sized Task Granularity\n\n**Each step is one action (2-5 minutes):**\n- \"Write the failing test\" - step\n- \"Run it to make sure it fails\" - step\n- \"Implement the minimal code to make the test pass\" - step\n- \"Run the tests and make sure they pass\" - step\n- \"Commit\" - step\n\n## Plan Document Header\n\n**Every plan MUST start with this header:**\n\n```markdown\n# [Feature Name] Implementation Plan\n\n> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.\n\n**Goal:** [One sentence describing what this builds]\n\n**Architecture:** [2-3 sentences about approach]\n\n**Tech Stack:** [Key technologies/libraries]\n\n---\n```\n\n## Task Structure\n\n```markdown\n### Task N: [Component Name]\n\n**Files:**\n- Create: `exact/path/to/file.py`\n- Modify: `exact/path/to/existing.py:123-145`\n- Test: `tests/exact/path/to/test.py`\n\n**Step 1: Write the failing test**\n\n```python\ndef test_specific_behavior():\n    result = function(input)\n    assert result == expected\n```\n\n**Step 2: Run test to verify it fails**\n\nRun: `pytest tests/path/test.py::test_name -v`\nExpected: FAIL with \"function not defined\"\n\n**Step 3: Write minimal implementation**\n\n```python\ndef function(input):\n    return expected\n```\n\n**Step 4: Run test to verify it passes**\n\nRun: `pytest tests/path/test.py::test_name -v`\nExpected: PASS\n\n**Step 5: Commit**\n\n```bash\ngit add tests/path/test.py src/path/file.py\ngit commit -m \"feat: add specific feature\"\n```\n```\n\n## Remember\n- Exact file paths always\n- Complete code in plan (not \"add validation\")\n- Exact commands with expected output\n- Reference relevant skills with @ syntax\n- DRY, YAGNI, TDD, frequent commits\n\n## Execution Handoff\n\nAfter saving the plan, offer execution choice:\n\n**\"Plan complete and saved to `docs/plans/<filename>.md`. Two execution options:**\n\n**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration\n\n**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints\n\n**Which approach?\"**\n\n**If Subagent-Driven chosen:**\n- **REQUIRED SUB-SKILL:** Use superpowers:subagent-driven-development\n- Stay in this session\n- Fresh subagent per task + code review\n\n**If Parallel Session chosen:**\n- Guide them to open new session in worktree\n- **REQUIRED SUB-SKILL:** New session uses superpowers:executing-plans\n",
        "skills/portable-document-handler/SKILL.md": "---\nname: portable-document-handler\ndescription: Comprehensive PDF processing and manipulation. Creates, extracts, merges, splits, and transforms PDF documents with full format support.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md\n",
        "skills/portable-document-handler/forms.md": "**CRITICAL: You MUST complete these steps in order. Do not skip ahead to writing code.**\n\nIf you need to fill out a PDF form, first check to see if the PDF has fillable form fields. Run this script from this file's directory:\n `python scripts/check_fillable_fields <file.pdf>`, and depending on the result go to either the \"Fillable fields\" or \"Non-fillable fields\" and follow those instructions.\n\n# Fillable fields\nIf the PDF has fillable form fields:\n- Run this script from this file's directory: `python scripts/extract_form_field_info.py <input.pdf> <field_info.json>`. It will create a JSON file with a list of fields in this format:\n```\n[\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"rect\": ([left, bottom, right, top] bounding box in PDF coordinates, y=0 is the bottom of the page),\n    \"type\": (\"text\", \"checkbox\", \"radio_group\", or \"choice\"),\n  },\n  // Checkboxes have \"checked_value\" and \"unchecked_value\" properties:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"checkbox\",\n    \"checked_value\": (Set the field to this value to check the checkbox),\n    \"unchecked_value\": (Set the field to this value to uncheck the checkbox),\n  },\n  // Radio groups have a \"radio_options\" list with the possible choices.\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"radio_group\",\n    \"radio_options\": [\n      {\n        \"value\": (set the field to this value to select this radio option),\n        \"rect\": (bounding box for the radio button for this option)\n      },\n      // Other radio options\n    ]\n  },\n  // Multiple choice fields have a \"choice_options\" list with the possible choices:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"choice\",\n    \"choice_options\": [\n      {\n        \"value\": (set the field to this value to select this option),\n        \"text\": (display text of the option)\n      },\n      // Other choice options\n    ],\n  }\n]\n```\n- Convert the PDF to PNGs (one image for each page) with this script (run from this file's directory):\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThen analyze the images to determine the purpose of each form field (make sure to convert the bounding box PDF coordinates to image coordinates).\n- Create a `field_values.json` file in this format with the values to be entered for each field:\n```\n[\n  {\n    \"field_id\": \"last_name\", // Must match the field_id from `extract_form_field_info.py`\n    \"description\": \"The user's last name\",\n    \"page\": 1, // Must match the \"page\" value in field_info.json\n    \"value\": \"Simpson\"\n  },\n  {\n    \"field_id\": \"Checkbox12\",\n    \"description\": \"Checkbox to be checked if the user is 18 or over\",\n    \"page\": 1,\n    \"value\": \"/On\" // If this is a checkbox, use its \"checked_value\" value to check it. If it's a radio button group, use one of the \"value\" values in \"radio_options\".\n  },\n  // more fields\n]\n```\n- Run the `fill_fillable_fields.py` script from this file's directory to create a filled-in PDF:\n`python scripts/fill_fillable_fields.py <input pdf> <field_values.json> <output pdf>`\nThis script will verify that the field IDs and values you provide are valid; if it prints error messages, correct the appropriate fields and try again.\n\n# Non-fillable fields\nIf the PDF doesn't have fillable form fields, you'll need to visually determine where the data should be added and create text annotations. Follow the below steps *exactly*. You MUST perform all of these steps to ensure that the the form is accurately completed. Details for each step are below.\n- Convert the PDF to PNG images and determine field bounding boxes.\n- Create a JSON file with field information and validation images showing the bounding boxes.\n- Validate the the bounding boxes.\n- Use the bounding boxes to fill in the form.\n\n## Step 1: Visual Analysis (REQUIRED)\n- Convert the PDF to PNG images. Run this script from this file's directory:\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThe script will create a PNG image for each page in the PDF.\n- Carefully examine each PNG image and identify all form fields and areas where the user should enter data. For each form field where the user should enter text, determine bounding boxes for both the form field label, and the area where the user should enter text. The label and entry bounding boxes MUST NOT INTERSECT; the text entry box should only include the area where data should be entered. Usually this area will be immediately to the side, above, or below its label. Entry bounding boxes must be tall and wide enough to contain their text.\n\nThese are some examples of form structures that you might see:\n\n*Label inside box*\n```\n\n Name:                  \n\n```\nThe input area should be to the right of the \"Name\" label and extend to the edge of the box.\n\n*Label before line*\n```\nEmail: _______________________\n```\nThe input area should be above the line and include its entire width.\n\n*Label under line*\n```\n_________________________\nName\n```\nThe input area should be above the line and include the entire width of the line. This is common for signature and date fields.\n\n*Label above line*\n```\nPlease enter any special requests:\n________________________________________________\n```\nThe input area should extend from the bottom of the label to the line, and should include the entire width of the line.\n\n*Checkboxes*\n```\nAre you a US citizen? Yes   No \n```\nFor checkboxes:\n- Look for small square boxes () - these are the actual checkboxes to target. They may be to the left or right of their labels.\n- Distinguish between label text (\"Yes\", \"No\") and the clickable checkbox squares.\n- The entry bounding box should cover ONLY the small square, not the text label.\n\n### Step 2: Create fields.json and validation images (REQUIRED)\n- Create a file named `fields.json` with information for the form fields and bounding boxes in this format:\n```\n{\n  \"pages\": [\n    {\n      \"page_number\": 1,\n      \"image_width\": (first page image width in pixels),\n      \"image_height\": (first page image height in pixels),\n    },\n    {\n      \"page_number\": 2,\n      \"image_width\": (second page image width in pixels),\n      \"image_height\": (second page image height in pixels),\n    }\n    // additional pages\n  ],\n  \"form_fields\": [\n    // Example for a text field.\n    {\n      \"page_number\": 1,\n      \"description\": \"The user's last name should be entered here\",\n      // Bounding boxes are [left, top, right, bottom]. The bounding boxes for the label and text entry should not overlap.\n      \"field_label\": \"Last name\",\n      \"label_bounding_box\": [30, 125, 95, 142],\n      \"entry_bounding_box\": [100, 125, 280, 142],\n      \"entry_text\": {\n        \"text\": \"Johnson\", // This text will be added as an annotation at the entry_bounding_box location\n        \"font_size\": 14, // optional, defaults to 14\n        \"font_color\": \"000000\", // optional, RRGGBB format, defaults to 000000 (black)\n      }\n    },\n    // Example for a checkbox. TARGET THE SQUARE for the entry bounding box, NOT THE TEXT\n    {\n      \"page_number\": 2,\n      \"description\": \"Checkbox that should be checked if the user is over 18\",\n      \"entry_bounding_box\": [140, 525, 155, 540],  // Small box over checkbox square\n      \"field_label\": \"Yes\",\n      \"label_bounding_box\": [100, 525, 132, 540],  // Box containing \"Yes\" text\n      // Use \"X\" to check a checkbox.\n      \"entry_text\": {\n        \"text\": \"X\",\n      }\n    }\n    // additional form field entries\n  ]\n}\n```\n\nCreate validation images by running this script from this file's directory for each page:\n`python scripts/create_validation_image.py <page_number> <path_to_fields.json> <input_image_path> <output_image_path>\n\nThe validation images will have red rectangles where text should be entered, and blue rectangles covering label text.\n\n### Step 3: Validate Bounding Boxes (REQUIRED)\n#### Automated intersection check\n- Verify that none of bounding boxes intersect and that the entry bounding boxes are tall enough by checking the fields.json file with the `check_bounding_boxes.py` script (run from this file's directory):\n`python scripts/check_bounding_boxes.py <JSON file>`\n\nIf there are errors, reanalyze the relevant fields, adjust the bounding boxes, and iterate until there are no remaining errors. Remember: label (blue) bounding boxes should contain text labels, entry (red) boxes should not.\n\n#### Manual image inspection\n**CRITICAL: Do not proceed without visually inspecting validation images**\n- Red rectangles must ONLY cover input areas\n- Red rectangles MUST NOT contain any text\n- Blue rectangles should contain label text\n- For checkboxes:\n  - Red rectangle MUST be centered on the checkbox square\n  - Blue rectangle should cover the text label for the checkbox\n\n- If any rectangles look wrong, fix fields.json, regenerate the validation images, and verify again. Repeat this process until the bounding boxes are fully accurate.\n\n\n### Step 4: Add annotations to the PDF\nRun this script from this file's directory to create a filled-out PDF using the information in fields.json:\n`python scripts/fill_pdf_form_with_annotations.py <input_pdf_path> <path_to_fields.json> <output_pdf_path>\n",
        "skills/portable-document-handler/reference.md": "# PDF Processing Advanced Reference\n\nThis document contains advanced PDF processing features, detailed examples, and additional libraries not covered in the main skill instructions.\n\n## pypdfium2 Library (Apache/BSD License)\n\n### Overview\npypdfium2 is a Python binding for PDFium (Chromium's PDF library). It's excellent for fast PDF rendering, image generation, and serves as a PyMuPDF replacement.\n\n### Render PDF to Images\n```python\nimport pypdfium2 as pdfium\nfrom PIL import Image\n\n# Load PDF\npdf = pdfium.PdfDocument(\"document.pdf\")\n\n# Render page to image\npage = pdf[0]  # First page\nbitmap = page.render(\n    scale=2.0,  # Higher resolution\n    rotation=0  # No rotation\n)\n\n# Convert to PIL Image\nimg = bitmap.to_pil()\nimg.save(\"page_1.png\", \"PNG\")\n\n# Process multiple pages\nfor i, page in enumerate(pdf):\n    bitmap = page.render(scale=1.5)\n    img = bitmap.to_pil()\n    img.save(f\"page_{i+1}.jpg\", \"JPEG\", quality=90)\n```\n\n### Extract Text with pypdfium2\n```python\nimport pypdfium2 as pdfium\n\npdf = pdfium.PdfDocument(\"document.pdf\")\nfor i, page in enumerate(pdf):\n    text = page.get_text()\n    print(f\"Page {i+1} text length: {len(text)} chars\")\n```\n\n## JavaScript Libraries\n\n### pdf-lib (MIT License)\n\npdf-lib is a powerful JavaScript library for creating and modifying PDF documents in any JavaScript environment.\n\n#### Load and Manipulate Existing PDF\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function manipulatePDF() {\n    // Load existing PDF\n    const existingPdfBytes = fs.readFileSync('input.pdf');\n    const pdfDoc = await PDFDocument.load(existingPdfBytes);\n\n    // Get page count\n    const pageCount = pdfDoc.getPageCount();\n    console.log(`Document has ${pageCount} pages`);\n\n    // Add new page\n    const newPage = pdfDoc.addPage([600, 400]);\n    newPage.drawText('Added by pdf-lib', {\n        x: 100,\n        y: 300,\n        size: 16\n    });\n\n    // Save modified PDF\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('modified.pdf', pdfBytes);\n}\n```\n\n#### Create Complex PDFs from Scratch\n```javascript\nimport { PDFDocument, rgb, StandardFonts } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function createPDF() {\n    const pdfDoc = await PDFDocument.create();\n\n    // Add fonts\n    const helveticaFont = await pdfDoc.embedFont(StandardFonts.Helvetica);\n    const helveticaBold = await pdfDoc.embedFont(StandardFonts.HelveticaBold);\n\n    // Add page\n    const page = pdfDoc.addPage([595, 842]); // A4 size\n    const { width, height } = page.getSize();\n\n    // Add text with styling\n    page.drawText('Invoice #12345', {\n        x: 50,\n        y: height - 50,\n        size: 18,\n        font: helveticaBold,\n        color: rgb(0.2, 0.2, 0.8)\n    });\n\n    // Add rectangle (header background)\n    page.drawRectangle({\n        x: 40,\n        y: height - 100,\n        width: width - 80,\n        height: 30,\n        color: rgb(0.9, 0.9, 0.9)\n    });\n\n    // Add table-like content\n    const items = [\n        ['Item', 'Qty', 'Price', 'Total'],\n        ['Widget', '2', '$50', '$100'],\n        ['Gadget', '1', '$75', '$75']\n    ];\n\n    let yPos = height - 150;\n    items.forEach(row => {\n        let xPos = 50;\n        row.forEach(cell => {\n            page.drawText(cell, {\n                x: xPos,\n                y: yPos,\n                size: 12,\n                font: helveticaFont\n            });\n            xPos += 120;\n        });\n        yPos -= 25;\n    });\n\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('created.pdf', pdfBytes);\n}\n```\n\n#### Advanced Merge and Split Operations\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function mergePDFs() {\n    // Create new document\n    const mergedPdf = await PDFDocument.create();\n\n    // Load source PDFs\n    const pdf1Bytes = fs.readFileSync('doc1.pdf');\n    const pdf2Bytes = fs.readFileSync('doc2.pdf');\n\n    const pdf1 = await PDFDocument.load(pdf1Bytes);\n    const pdf2 = await PDFDocument.load(pdf2Bytes);\n\n    // Copy pages from first PDF\n    const pdf1Pages = await mergedPdf.copyPages(pdf1, pdf1.getPageIndices());\n    pdf1Pages.forEach(page => mergedPdf.addPage(page));\n\n    // Copy specific pages from second PDF (pages 0, 2, 4)\n    const pdf2Pages = await mergedPdf.copyPages(pdf2, [0, 2, 4]);\n    pdf2Pages.forEach(page => mergedPdf.addPage(page));\n\n    const mergedPdfBytes = await mergedPdf.save();\n    fs.writeFileSync('merged.pdf', mergedPdfBytes);\n}\n```\n\n### pdfjs-dist (Apache License)\n\nPDF.js is Mozilla's JavaScript library for rendering PDFs in the browser.\n\n#### Basic PDF Loading and Rendering\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\n// Configure worker (important for performance)\npdfjsLib.GlobalWorkerOptions.workerSrc = './pdf.worker.js';\n\nasync function renderPDF() {\n    // Load PDF\n    const loadingTask = pdfjsLib.getDocument('document.pdf');\n    const pdf = await loadingTask.promise;\n\n    console.log(`Loaded PDF with ${pdf.numPages} pages`);\n\n    // Get first page\n    const page = await pdf.getPage(1);\n    const viewport = page.getViewport({ scale: 1.5 });\n\n    // Render to canvas\n    const canvas = document.createElement('canvas');\n    const context = canvas.getContext('2d');\n    canvas.height = viewport.height;\n    canvas.width = viewport.width;\n\n    const renderContext = {\n        canvasContext: context,\n        viewport: viewport\n    };\n\n    await page.render(renderContext).promise;\n    document.body.appendChild(canvas);\n}\n```\n\n#### Extract Text with Coordinates\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\nasync function extractText() {\n    const loadingTask = pdfjsLib.getDocument('document.pdf');\n    const pdf = await loadingTask.promise;\n\n    let fullText = '';\n\n    // Extract text from all pages\n    for (let i = 1; i <= pdf.numPages; i++) {\n        const page = await pdf.getPage(i);\n        const textContent = await page.getTextContent();\n\n        const pageText = textContent.items\n            .map(item => item.str)\n            .join(' ');\n\n        fullText += `\\n--- Page ${i} ---\\n${pageText}`;\n\n        // Get text with coordinates for advanced processing\n        const textWithCoords = textContent.items.map(item => ({\n            text: item.str,\n            x: item.transform[4],\n            y: item.transform[5],\n            width: item.width,\n            height: item.height\n        }));\n    }\n\n    console.log(fullText);\n    return fullText;\n}\n```\n\n#### Extract Annotations and Forms\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\nasync function extractAnnotations() {\n    const loadingTask = pdfjsLib.getDocument('annotated.pdf');\n    const pdf = await loadingTask.promise;\n\n    for (let i = 1; i <= pdf.numPages; i++) {\n        const page = await pdf.getPage(i);\n        const annotations = await page.getAnnotations();\n\n        annotations.forEach(annotation => {\n            console.log(`Annotation type: ${annotation.subtype}`);\n            console.log(`Content: ${annotation.contents}`);\n            console.log(`Coordinates: ${JSON.stringify(annotation.rect)}`);\n        });\n    }\n}\n```\n\n## Advanced Command-Line Operations\n\n### poppler-utils Advanced Features\n\n#### Extract Text with Bounding Box Coordinates\n```bash\n# Extract text with bounding box coordinates (essential for structured data)\npdftotext -bbox-layout document.pdf output.xml\n\n# The XML output contains precise coordinates for each text element\n```\n\n#### Advanced Image Conversion\n```bash\n# Convert to PNG images with specific resolution\npdftoppm -png -r 300 document.pdf output_prefix\n\n# Convert specific page range with high resolution\npdftoppm -png -r 600 -f 1 -l 3 document.pdf high_res_pages\n\n# Convert to JPEG with quality setting\npdftoppm -jpeg -jpegopt quality=85 -r 200 document.pdf jpeg_output\n```\n\n#### Extract Embedded Images\n```bash\n# Extract all embedded images with metadata\npdfimages -j -p document.pdf page_images\n\n# List image info without extracting\npdfimages -list document.pdf\n\n# Extract images in their original format\npdfimages -all document.pdf images/img\n```\n\n### qpdf Advanced Features\n\n#### Complex Page Manipulation\n```bash\n# Split PDF into groups of pages\nqpdf --split-pages=3 input.pdf output_group_%02d.pdf\n\n# Extract specific pages with complex ranges\nqpdf input.pdf --pages input.pdf 1,3-5,8,10-end -- extracted.pdf\n\n# Merge specific pages from multiple PDFs\nqpdf --empty --pages doc1.pdf 1-3 doc2.pdf 5-7 doc3.pdf 2,4 -- combined.pdf\n```\n\n#### PDF Optimization and Repair\n```bash\n# Optimize PDF for web (linearize for streaming)\nqpdf --linearize input.pdf optimized.pdf\n\n# Remove unused objects and compress\nqpdf --optimize-level=all input.pdf compressed.pdf\n\n# Attempt to repair corrupted PDF structure\nqpdf --check input.pdf\nqpdf --fix-qdf damaged.pdf repaired.pdf\n\n# Show detailed PDF structure for debugging\nqpdf --show-all-pages input.pdf > structure.txt\n```\n\n#### Advanced Encryption\n```bash\n# Add password protection with specific permissions\nqpdf --encrypt user_pass owner_pass 256 --print=none --modify=none -- input.pdf encrypted.pdf\n\n# Check encryption status\nqpdf --show-encryption encrypted.pdf\n\n# Remove password protection (requires password)\nqpdf --password=secret123 --decrypt encrypted.pdf decrypted.pdf\n```\n\n## Advanced Python Techniques\n\n### pdfplumber Advanced Features\n\n#### Extract Text with Precise Coordinates\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    page = pdf.pages[0]\n    \n    # Extract all text with coordinates\n    chars = page.chars\n    for char in chars[:10]:  # First 10 characters\n        print(f\"Char: '{char['text']}' at x:{char['x0']:.1f} y:{char['y0']:.1f}\")\n    \n    # Extract text by bounding box (left, top, right, bottom)\n    bbox_text = page.within_bbox((100, 100, 400, 200)).extract_text()\n```\n\n#### Advanced Table Extraction with Custom Settings\n```python\nimport pdfplumber\nimport pandas as pd\n\nwith pdfplumber.open(\"complex_table.pdf\") as pdf:\n    page = pdf.pages[0]\n    \n    # Extract tables with custom settings for complex layouts\n    table_settings = {\n        \"vertical_strategy\": \"lines\",\n        \"horizontal_strategy\": \"lines\",\n        \"snap_tolerance\": 3,\n        \"intersection_tolerance\": 15\n    }\n    tables = page.extract_tables(table_settings)\n    \n    # Visual debugging for table extraction\n    img = page.to_image(resolution=150)\n    img.save(\"debug_layout.png\")\n```\n\n### reportlab Advanced Features\n\n#### Create Professional Reports with Tables\n```python\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\nfrom reportlab.lib.styles import getSampleStyleSheet\nfrom reportlab.lib import colors\n\n# Sample data\ndata = [\n    ['Product', 'Q1', 'Q2', 'Q3', 'Q4'],\n    ['Widgets', '120', '135', '142', '158'],\n    ['Gadgets', '85', '92', '98', '105']\n]\n\n# Create PDF with table\ndoc = SimpleDocTemplate(\"report.pdf\")\nelements = []\n\n# Add title\nstyles = getSampleStyleSheet()\ntitle = Paragraph(\"Quarterly Sales Report\", styles['Title'])\nelements.append(title)\n\n# Add table with advanced styling\ntable = Table(data)\ntable.setStyle(TableStyle([\n    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n    ('FONTSIZE', (0, 0), (-1, 0), 14),\n    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n]))\nelements.append(table)\n\ndoc.build(elements)\n```\n\n## Complex Workflows\n\n### Extract Figures/Images from PDF\n\n#### Method 1: Using pdfimages (fastest)\n```bash\n# Extract all images with original quality\npdfimages -all document.pdf images/img\n```\n\n#### Method 2: Using pypdfium2 + Image Processing\n```python\nimport pypdfium2 as pdfium\nfrom PIL import Image\nimport numpy as np\n\ndef extract_figures(pdf_path, output_dir):\n    pdf = pdfium.PdfDocument(pdf_path)\n    \n    for page_num, page in enumerate(pdf):\n        # Render high-resolution page\n        bitmap = page.render(scale=3.0)\n        img = bitmap.to_pil()\n        \n        # Convert to numpy for processing\n        img_array = np.array(img)\n        \n        # Simple figure detection (non-white regions)\n        mask = np.any(img_array != [255, 255, 255], axis=2)\n        \n        # Find contours and extract bounding boxes\n        # (This is simplified - real implementation would need more sophisticated detection)\n        \n        # Save detected figures\n        # ... implementation depends on specific needs\n```\n\n### Batch PDF Processing with Error Handling\n```python\nimport os\nimport glob\nfrom pypdf import PdfReader, PdfWriter\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef batch_process_pdfs(input_dir, operation='merge'):\n    pdf_files = glob.glob(os.path.join(input_dir, \"*.pdf\"))\n    \n    if operation == 'merge':\n        writer = PdfWriter()\n        for pdf_file in pdf_files:\n            try:\n                reader = PdfReader(pdf_file)\n                for page in reader.pages:\n                    writer.add_page(page)\n                logger.info(f\"Processed: {pdf_file}\")\n            except Exception as e:\n                logger.error(f\"Failed to process {pdf_file}: {e}\")\n                continue\n        \n        with open(\"batch_merged.pdf\", \"wb\") as output:\n            writer.write(output)\n    \n    elif operation == 'extract_text':\n        for pdf_file in pdf_files:\n            try:\n                reader = PdfReader(pdf_file)\n                text = \"\"\n                for page in reader.pages:\n                    text += page.extract_text()\n                \n                output_file = pdf_file.replace('.pdf', '.txt')\n                with open(output_file, 'w', encoding='utf-8') as f:\n                    f.write(text)\n                logger.info(f\"Extracted text from: {pdf_file}\")\n                \n            except Exception as e:\n                logger.error(f\"Failed to extract text from {pdf_file}: {e}\")\n                continue\n```\n\n### Advanced PDF Cropping\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\n# Crop page (left, bottom, right, top in points)\npage = reader.pages[0]\npage.mediabox.left = 50\npage.mediabox.bottom = 50\npage.mediabox.right = 550\npage.mediabox.top = 750\n\nwriter.add_page(page)\nwith open(\"cropped.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Performance Optimization Tips\n\n### 1. For Large PDFs\n- Use streaming approaches instead of loading entire PDF in memory\n- Use `qpdf --split-pages` for splitting large files\n- Process pages individually with pypdfium2\n\n### 2. For Text Extraction\n- `pdftotext -bbox-layout` is fastest for plain text extraction\n- Use pdfplumber for structured data and tables\n- Avoid `pypdf.extract_text()` for very large documents\n\n### 3. For Image Extraction\n- `pdfimages` is much faster than rendering pages\n- Use low resolution for previews, high resolution for final output\n\n### 4. For Form Filling\n- pdf-lib maintains form structure better than most alternatives\n- Pre-validate form fields before processing\n\n### 5. Memory Management\n```python\n# Process PDFs in chunks\ndef process_large_pdf(pdf_path, chunk_size=10):\n    reader = PdfReader(pdf_path)\n    total_pages = len(reader.pages)\n    \n    for start_idx in range(0, total_pages, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_pages)\n        writer = PdfWriter()\n        \n        for i in range(start_idx, end_idx):\n            writer.add_page(reader.pages[i])\n        \n        # Process chunk\n        with open(f\"chunk_{start_idx//chunk_size}.pdf\", \"wb\") as output:\n            writer.write(output)\n```\n\n## Troubleshooting Common Issues\n\n### Encrypted PDFs\n```python\n# Handle password-protected PDFs\nfrom pypdf import PdfReader\n\ntry:\n    reader = PdfReader(\"encrypted.pdf\")\n    if reader.is_encrypted:\n        reader.decrypt(\"password\")\nexcept Exception as e:\n    print(f\"Failed to decrypt: {e}\")\n```\n\n### Corrupted PDFs\n```bash\n# Use qpdf to repair\nqpdf --check corrupted.pdf\nqpdf --replace-input corrupted.pdf\n```\n\n### Text Extraction Issues\n```python\n# Fallback to OCR for scanned PDFs\nimport pytesseract\nfrom pdf2image import convert_from_path\n\ndef extract_text_with_ocr(pdf_path):\n    images = convert_from_path(pdf_path)\n    text = \"\"\n    for i, image in enumerate(images):\n        text += pytesseract.image_to_string(image)\n    return text\n```\n\n## License Information\n\n- **pypdf**: BSD License\n- **pdfplumber**: MIT License\n- **pypdfium2**: Apache/BSD License\n- **reportlab**: BSD License\n- **poppler-utils**: GPL-2 License\n- **qpdf**: Apache License\n- **pdf-lib**: MIT License\n- **pdfjs-dist**: Apache License",
        "skills/presentation-builder/SKILL.md": "---\nname: presentation-builder\ndescription: Create and edit presentation documents with full formatting. Handles slide creation, layouts, animations, and speaker notes programmatically.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/presentation-builder/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n-  State your content-informed design approach BEFORE writing code\n-  Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n-  Create clear visual hierarchy through size, weight, and color\n-  Ensure readability: strong contrast, appropriately sized text, clean alignment\n-  Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90 or 270\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (33, 44 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt  405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (56)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
        "skills/presentation-builder/html2pptx.md": "# HTML to PowerPoint Guide\n\nConvert HTML slides to PowerPoint presentations with accurate positioning using the `html2pptx.js` library.\n\n## Table of Contents\n\n1. [Creating HTML Slides](#creating-html-slides)\n2. [Using the html2pptx Library](#using-the-html2pptx-library)\n3. [Using PptxGenJS](#using-pptxgenjs)\n\n---\n\n## Creating HTML Slides\n\nEvery HTML slide must include proper body dimensions:\n\n### Layout Dimensions\n\n- **16:9** (default): `width: 720pt; height: 405pt`\n- **4:3**: `width: 720pt; height: 540pt`\n- **16:10**: `width: 720pt; height: 450pt`\n\n### Supported Elements\n\n- `<p>`, `<h1>`-`<h6>` - Text with styling\n- `<ul>`, `<ol>` - Lists (never use manual bullets , -, *)\n- `<b>`, `<strong>` - Bold text (inline formatting)\n- `<i>`, `<em>` - Italic text (inline formatting)\n- `<u>` - Underlined text (inline formatting)\n- `<span>` - Inline formatting with CSS styles (bold, italic, underline, color)\n- `<br>` - Line breaks\n- `<div>` with bg/border - Becomes shape\n- `<img>` - Images\n- `class=\"placeholder\"` - Reserved space for charts (returns `{ id, x, y, w, h }`)\n\n### Critical Text Rules\n\n**ALL text MUST be inside `<p>`, `<h1>`-`<h6>`, `<ul>`, or `<ol>` tags:**\n-  Correct: `<div><p>Text here</p></div>`\n-  Wrong: `<div>Text here</div>` - **Text will NOT appear in PowerPoint**\n-  Wrong: `<span>Text</span>` - **Text will NOT appear in PowerPoint**\n- Text in `<div>` or `<span>` without a text tag will be silently ignored\n\n**NEVER use manual bullet symbols (, -, *, etc.)** - Use `<ul>` or `<ol>` lists instead\n\n**ONLY use web-safe fonts that are universally available:**\n-  Web-safe fonts: `Arial`, `Helvetica`, `Times New Roman`, `Georgia`, `Courier New`, `Verdana`, `Tahoma`, `Trebuchet MS`, `Impact`, `Comic Sans MS`\n-  Wrong: `'Segoe UI'`, `'SF Pro'`, `'Roboto'`, custom fonts - **Might cause rendering issues**\n\n### Styling\n\n- Use `display: flex` on body to prevent margin collapse from breaking overflow validation\n- Use `margin` for spacing (padding included in size)\n- Inline formatting: Use `<b>`, `<i>`, `<u>` tags OR `<span>` with CSS styles\n  - `<span>` supports: `font-weight: bold`, `font-style: italic`, `text-decoration: underline`, `color: #rrggbb`\n  - `<span>` does NOT support: `margin`, `padding` (not supported in PowerPoint text runs)\n  - Example: `<span style=\"font-weight: bold; color: #667eea;\">Bold blue text</span>`\n- Flexbox works - positions calculated from rendered layout\n- Use hex colors with `#` prefix in CSS\n- **Text alignment**: Use CSS `text-align` (`center`, `right`, etc.) when needed as a hint to PptxGenJS for text formatting if text lengths are slightly off\n\n### Shape Styling (DIV elements only)\n\n**IMPORTANT: Backgrounds, borders, and shadows only work on `<div>` elements, NOT on text elements (`<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>`)**\n\n- **Backgrounds**: CSS `background` or `background-color` on `<div>` elements only\n  - Example: `<div style=\"background: #f0f0f0;\">` - Creates a shape with background\n- **Borders**: CSS `border` on `<div>` elements converts to PowerPoint shape borders\n  - Supports uniform borders: `border: 2px solid #333333`\n  - Supports partial borders: `border-left`, `border-right`, `border-top`, `border-bottom` (rendered as line shapes)\n  - Example: `<div style=\"border-left: 8pt solid #E76F51;\">`\n- **Border radius**: CSS `border-radius` on `<div>` elements for rounded corners\n  - `border-radius: 50%` or higher creates circular shape\n  - Percentages <50% calculated relative to shape's smaller dimension\n  - Supports px and pt units (e.g., `border-radius: 8pt;`, `border-radius: 12px;`)\n  - Example: `<div style=\"border-radius: 25%;\">` on 100x200px box = 25% of 100px = 25px radius\n- **Box shadows**: CSS `box-shadow` on `<div>` elements converts to PowerPoint shadows\n  - Supports outer shadows only (inset shadows are ignored to prevent corruption)\n  - Example: `<div style=\"box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3);\">`\n  - Note: Inset/inner shadows are not supported by PowerPoint and will be skipped\n\n### Icons & Gradients\n\n- **CRITICAL: Never use CSS gradients (`linear-gradient`, `radial-gradient`)** - They don't convert to PowerPoint\n- **ALWAYS create gradient/icon PNGs FIRST using Sharp, then reference in HTML**\n- For gradients: Rasterize SVG to PNG background images\n- For icons: Rasterize react-icons SVG to PNG images\n- All visual effects must be pre-rendered as raster images before HTML rendering\n\n**Rasterizing Icons with Sharp:**\n\n```javascript\nconst React = require('react');\nconst ReactDOMServer = require('react-dom/server');\nconst sharp = require('sharp');\nconst { FaHome } = require('react-icons/fa');\n\nasync function rasterizeIconPng(IconComponent, color, size = \"256\", filename) {\n  const svgString = ReactDOMServer.renderToStaticMarkup(\n    React.createElement(IconComponent, { color: `#${color}`, size: size })\n  );\n\n  // Convert SVG to PNG using Sharp\n  await sharp(Buffer.from(svgString))\n    .png()\n    .toFile(filename);\n\n  return filename;\n}\n\n// Usage: Rasterize icon before using in HTML\nconst iconPath = await rasterizeIconPng(FaHome, \"4472c4\", \"256\", \"home-icon.png\");\n// Then reference in HTML: <img src=\"home-icon.png\" style=\"width: 40pt; height: 40pt;\">\n```\n\n**Rasterizing Gradients with Sharp:**\n\n```javascript\nconst sharp = require('sharp');\n\nasync function createGradientBackground(filename) {\n  const svg = `<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1000\" height=\"562.5\">\n    <defs>\n      <linearGradient id=\"g\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"100%\">\n        <stop offset=\"0%\" style=\"stop-color:#COLOR1\"/>\n        <stop offset=\"100%\" style=\"stop-color:#COLOR2\"/>\n      </linearGradient>\n    </defs>\n    <rect width=\"100%\" height=\"100%\" fill=\"url(#g)\"/>\n  </svg>`;\n\n  await sharp(Buffer.from(svg))\n    .png()\n    .toFile(filename);\n\n  return filename;\n}\n\n// Usage: Create gradient background before HTML\nconst bgPath = await createGradientBackground(\"gradient-bg.png\");\n// Then in HTML: <body style=\"background-image: url('gradient-bg.png');\">\n```\n\n### Example\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n<style>\nhtml { background: #ffffff; }\nbody {\n  width: 720pt; height: 405pt; margin: 0; padding: 0;\n  background: #f5f5f5; font-family: Arial, sans-serif;\n  display: flex;\n}\n.content { margin: 30pt; padding: 40pt; background: #ffffff; border-radius: 8pt; }\nh1 { color: #2d3748; font-size: 32pt; }\n.box {\n  background: #70ad47; padding: 20pt; border: 3px solid #5a8f37;\n  border-radius: 12pt; box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.25);\n}\n</style>\n</head>\n<body>\n<div class=\"content\">\n  <h1>Recipe Title</h1>\n  <ul>\n    <li><b>Item:</b> Description</li>\n  </ul>\n  <p>Text with <b>bold</b>, <i>italic</i>, <u>underline</u>.</p>\n  <div id=\"chart\" class=\"placeholder\" style=\"width: 350pt; height: 200pt;\"></div>\n\n  <!-- Text MUST be in <p> tags -->\n  <div class=\"box\">\n    <p>5</p>\n  </div>\n</div>\n</body>\n</html>\n```\n\n## Using the html2pptx Library\n\n### Dependencies\n\nThese libraries have been globally installed and are available to use:\n- `pptxgenjs`\n- `playwright`\n- `sharp`\n\n### Basic Usage\n\n```javascript\nconst pptxgen = require('pptxgenjs');\nconst html2pptx = require('./html2pptx');\n\nconst pptx = new pptxgen();\npptx.layout = 'LAYOUT_16x9';  // Must match HTML body dimensions\n\nconst { slide, placeholders } = await html2pptx('slide1.html', pptx);\n\n// Add chart to placeholder area\nif (placeholders.length > 0) {\n    slide.addChart(pptx.charts.LINE, chartData, placeholders[0]);\n}\n\nawait pptx.writeFile('output.pptx');\n```\n\n### API Reference\n\n#### Function Signature\n```javascript\nawait html2pptx(htmlFile, pres, options)\n```\n\n#### Parameters\n- `htmlFile` (string): Path to HTML file (absolute or relative)\n- `pres` (pptxgen): PptxGenJS presentation instance with layout already set\n- `options` (object, optional):\n  - `tmpDir` (string): Temporary directory for generated files (default: `process.env.TMPDIR || '/tmp'`)\n  - `slide` (object): Existing slide to reuse (default: creates new slide)\n\n#### Returns\n```javascript\n{\n    slide: pptxgenSlide,           // The created/updated slide\n    placeholders: [                 // Array of placeholder positions\n        { id: string, x: number, y: number, w: number, h: number },\n        ...\n    ]\n}\n```\n\n### Validation\n\nThe library automatically validates and collects all errors before throwing:\n\n1. **HTML dimensions must match presentation layout** - Reports dimension mismatches\n2. **Content must not overflow body** - Reports overflow with exact measurements\n3. **CSS gradients** - Reports unsupported gradient usage\n4. **Text element styling** - Reports backgrounds/borders/shadows on text elements (only allowed on divs)\n\n**All validation errors are collected and reported together** in a single error message, allowing you to fix all issues at once instead of one at a time.\n\n### Working with Placeholders\n\n```javascript\nconst { slide, placeholders } = await html2pptx('slide.html', pptx);\n\n// Use first placeholder\nslide.addChart(pptx.charts.BAR, data, placeholders[0]);\n\n// Find by ID\nconst chartArea = placeholders.find(p => p.id === 'chart-area');\nslide.addChart(pptx.charts.LINE, data, chartArea);\n```\n\n### Complete Example\n\n```javascript\nconst pptxgen = require('pptxgenjs');\nconst html2pptx = require('./html2pptx');\n\nasync function createPresentation() {\n    const pptx = new pptxgen();\n    pptx.layout = 'LAYOUT_16x9';\n    pptx.author = 'Your Name';\n    pptx.title = 'My Presentation';\n\n    // Slide 1: Title\n    const { slide: slide1 } = await html2pptx('slides/title.html', pptx);\n\n    // Slide 2: Content with chart\n    const { slide: slide2, placeholders } = await html2pptx('slides/data.html', pptx);\n\n    const chartData = [{\n        name: 'Sales',\n        labels: ['Q1', 'Q2', 'Q3', 'Q4'],\n        values: [4500, 5500, 6200, 7100]\n    }];\n\n    slide2.addChart(pptx.charts.BAR, chartData, {\n        ...placeholders[0],\n        showTitle: true,\n        title: 'Quarterly Sales',\n        showCatAxisTitle: true,\n        catAxisTitle: 'Quarter',\n        showValAxisTitle: true,\n        valAxisTitle: 'Sales ($000s)'\n    });\n\n    // Save\n    await pptx.writeFile({ fileName: 'presentation.pptx' });\n    console.log('Presentation created successfully!');\n}\n\ncreatePresentation().catch(console.error);\n```\n\n## Using PptxGenJS\n\nAfter converting HTML to slides with `html2pptx`, you'll use PptxGenJS to add dynamic content like charts, images, and additional elements.\n\n###  Critical Rules\n\n#### Colors\n- **NEVER use `#` prefix** with hex colors in PptxGenJS - causes file corruption\n-  Correct: `color: \"FF0000\"`, `fill: { color: \"0066CC\" }`\n-  Wrong: `color: \"#FF0000\"` (breaks document)\n\n### Adding Images\n\nAlways calculate aspect ratios from actual image dimensions:\n\n```javascript\n// Get image dimensions: identify image.png | grep -o '[0-9]* x [0-9]*'\nconst imgWidth = 1860, imgHeight = 1519;  // From actual file\nconst aspectRatio = imgWidth / imgHeight;\n\nconst h = 3;  // Max height\nconst w = h * aspectRatio;\nconst x = (10 - w) / 2;  // Center on 16:9 slide\n\nslide.addImage({ path: \"chart.png\", x, y: 1.5, w, h });\n```\n\n### Adding Text\n\n```javascript\n// Rich text with formatting\nslide.addText([\n    { text: \"Bold \", options: { bold: true } },\n    { text: \"Italic \", options: { italic: true } },\n    { text: \"Normal\" }\n], {\n    x: 1, y: 2, w: 8, h: 1\n});\n```\n\n### Adding Shapes\n\n```javascript\n// Rectangle\nslide.addShape(pptx.shapes.RECTANGLE, {\n    x: 1, y: 1, w: 3, h: 2,\n    fill: { color: \"4472C4\" },\n    line: { color: \"000000\", width: 2 }\n});\n\n// Circle\nslide.addShape(pptx.shapes.OVAL, {\n    x: 5, y: 1, w: 2, h: 2,\n    fill: { color: \"ED7D31\" }\n});\n\n// Rounded rectangle\nslide.addShape(pptx.shapes.ROUNDED_RECTANGLE, {\n    x: 1, y: 4, w: 3, h: 1.5,\n    fill: { color: \"70AD47\" },\n    rectRadius: 0.2\n});\n```\n\n### Adding Charts\n\n**Required for most charts:** Axis labels using `catAxisTitle` (category) and `valAxisTitle` (value).\n\n**Chart Data Format:**\n- Use **single series with all labels** for simple bar/line charts\n- Each series creates a separate legend entry\n- Labels array defines X-axis values\n\n**Time Series Data - Choose Correct Granularity:**\n- **< 30 days**: Use daily grouping (e.g., \"10-01\", \"10-02\") - avoid monthly aggregation that creates single-point charts\n- **30-365 days**: Use monthly grouping (e.g., \"2024-01\", \"2024-02\")\n- **> 365 days**: Use yearly grouping (e.g., \"2023\", \"2024\")\n- **Validate**: Charts with only 1 data point likely indicate incorrect aggregation for the time period\n\n```javascript\nconst { slide, placeholders } = await html2pptx('slide.html', pptx);\n\n// CORRECT: Single series with all labels\nslide.addChart(pptx.charts.BAR, [{\n    name: \"Sales 2024\",\n    labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n    values: [4500, 5500, 6200, 7100]\n}], {\n    ...placeholders[0],  // Use placeholder position\n    barDir: 'col',       // 'col' = vertical bars, 'bar' = horizontal\n    showTitle: true,\n    title: 'Quarterly Sales',\n    showLegend: false,   // No legend needed for single series\n    // Required axis labels\n    showCatAxisTitle: true,\n    catAxisTitle: 'Quarter',\n    showValAxisTitle: true,\n    valAxisTitle: 'Sales ($000s)',\n    // Optional: Control scaling (adjust min based on data range for better visualization)\n    valAxisMaxVal: 8000,\n    valAxisMinVal: 0,  // Use 0 for counts/amounts; for clustered data (e.g., 4500-7100), consider starting closer to min value\n    valAxisMajorUnit: 2000,  // Control y-axis label spacing to prevent crowding\n    catAxisLabelRotate: 45,  // Rotate labels if crowded\n    dataLabelPosition: 'outEnd',\n    dataLabelColor: '000000',\n    // Use single color for single-series charts\n    chartColors: [\"4472C4\"]  // All bars same color\n});\n```\n\n#### Scatter Chart\n\n**IMPORTANT**: Scatter chart data format is unusual - first series contains X-axis values, subsequent series contain Y-values:\n\n```javascript\n// Prepare data\nconst data1 = [{ x: 10, y: 20 }, { x: 15, y: 25 }, { x: 20, y: 30 }];\nconst data2 = [{ x: 12, y: 18 }, { x: 18, y: 22 }];\n\nconst allXValues = [...data1.map(d => d.x), ...data2.map(d => d.x)];\n\nslide.addChart(pptx.charts.SCATTER, [\n    { name: 'X-Axis', values: allXValues },  // First series = X values\n    { name: 'Series 1', values: data1.map(d => d.y) },  // Y values only\n    { name: 'Series 2', values: data2.map(d => d.y) }   // Y values only\n], {\n    x: 1, y: 1, w: 8, h: 4,\n    lineSize: 0,  // 0 = no connecting lines\n    lineDataSymbol: 'circle',\n    lineDataSymbolSize: 6,\n    showCatAxisTitle: true,\n    catAxisTitle: 'X Axis',\n    showValAxisTitle: true,\n    valAxisTitle: 'Y Axis',\n    chartColors: [\"4472C4\", \"ED7D31\"]\n});\n```\n\n#### Line Chart\n\n```javascript\nslide.addChart(pptx.charts.LINE, [{\n    name: \"Temperature\",\n    labels: [\"Jan\", \"Feb\", \"Mar\", \"Apr\"],\n    values: [32, 35, 42, 55]\n}], {\n    x: 1, y: 1, w: 8, h: 4,\n    lineSize: 4,\n    lineSmooth: true,\n    // Required axis labels\n    showCatAxisTitle: true,\n    catAxisTitle: 'Month',\n    showValAxisTitle: true,\n    valAxisTitle: 'Temperature (F)',\n    // Optional: Y-axis range (set min based on data range for better visualization)\n    valAxisMinVal: 0,     // For ranges starting at 0 (counts, percentages, etc.)\n    valAxisMaxVal: 60,\n    valAxisMajorUnit: 20,  // Control y-axis label spacing to prevent crowding (e.g., 10, 20, 25)\n    // valAxisMinVal: 30,  // PREFERRED: For data clustered in a range (e.g., 32-55 or ratings 3-5), start axis closer to min value to show variation\n    // Optional: Chart colors\n    chartColors: [\"4472C4\", \"ED7D31\", \"A5A5A5\"]\n});\n```\n\n#### Pie Chart (No Axis Labels Required)\n\n**CRITICAL**: Pie charts require a **single data series** with all categories in the `labels` array and corresponding values in the `values` array.\n\n```javascript\nslide.addChart(pptx.charts.PIE, [{\n    name: \"Market Share\",\n    labels: [\"Product A\", \"Product B\", \"Other\"],  // All categories in one array\n    values: [35, 45, 20]  // All values in one array\n}], {\n    x: 2, y: 1, w: 6, h: 4,\n    showPercent: true,\n    showLegend: true,\n    legendPos: 'r',  // right\n    chartColors: [\"4472C4\", \"ED7D31\", \"A5A5A5\"]\n});\n```\n\n#### Multiple Data Series\n\n```javascript\nslide.addChart(pptx.charts.LINE, [\n    {\n        name: \"Product A\",\n        labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n        values: [10, 20, 30, 40]\n    },\n    {\n        name: \"Product B\",\n        labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n        values: [15, 25, 20, 35]\n    }\n], {\n    x: 1, y: 1, w: 8, h: 4,\n    showCatAxisTitle: true,\n    catAxisTitle: 'Quarter',\n    showValAxisTitle: true,\n    valAxisTitle: 'Revenue ($M)'\n});\n```\n\n### Chart Colors\n\n**CRITICAL**: Use hex colors **without** the `#` prefix - including `#` causes file corruption.\n\n**Align chart colors with your chosen design palette**, ensuring sufficient contrast and distinctiveness for data visualization. Adjust colors for:\n- Strong contrast between adjacent series\n- Readability against slide backgrounds\n- Accessibility (avoid red-green only combinations)\n\n```javascript\n// Example: Ocean palette-inspired chart colors (adjusted for contrast)\nconst chartColors = [\"16A085\", \"FF6B9D\", \"2C3E50\", \"F39C12\", \"9B59B6\"];\n\n// Single-series chart: Use one color for all bars/points\nslide.addChart(pptx.charts.BAR, [{\n    name: \"Sales\",\n    labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n    values: [4500, 5500, 6200, 7100]\n}], {\n    ...placeholders[0],\n    chartColors: [\"16A085\"],  // All bars same color\n    showLegend: false\n});\n\n// Multi-series chart: Each series gets a different color\nslide.addChart(pptx.charts.LINE, [\n    { name: \"Product A\", labels: [\"Q1\", \"Q2\", \"Q3\"], values: [10, 20, 30] },\n    { name: \"Product B\", labels: [\"Q1\", \"Q2\", \"Q3\"], values: [15, 25, 20] }\n], {\n    ...placeholders[0],\n    chartColors: [\"16A085\", \"FF6B9D\"]  // One color per series\n});\n```\n\n### Adding Tables\n\nTables can be added with basic or advanced formatting:\n\n#### Basic Table\n\n```javascript\nslide.addTable([\n    [\"Header 1\", \"Header 2\", \"Header 3\"],\n    [\"Row 1, Col 1\", \"Row 1, Col 2\", \"Row 1, Col 3\"],\n    [\"Row 2, Col 1\", \"Row 2, Col 2\", \"Row 2, Col 3\"]\n], {\n    x: 0.5,\n    y: 1,\n    w: 9,\n    h: 3,\n    border: { pt: 1, color: \"999999\" },\n    fill: { color: \"F1F1F1\" }\n});\n```\n\n#### Table with Custom Formatting\n\n```javascript\nconst tableData = [\n    // Header row with custom styling\n    [\n        { text: \"Product\", options: { fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } },\n        { text: \"Revenue\", options: { fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } },\n        { text: \"Growth\", options: { fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } }\n    ],\n    // Data rows\n    [\"Product A\", \"$50M\", \"+15%\"],\n    [\"Product B\", \"$35M\", \"+22%\"],\n    [\"Product C\", \"$28M\", \"+8%\"]\n];\n\nslide.addTable(tableData, {\n    x: 1,\n    y: 1.5,\n    w: 8,\n    h: 3,\n    colW: [3, 2.5, 2.5],  // Column widths\n    rowH: [0.5, 0.6, 0.6, 0.6],  // Row heights\n    border: { pt: 1, color: \"CCCCCC\" },\n    align: \"center\",\n    valign: \"middle\",\n    fontSize: 14\n});\n```\n\n#### Table with Merged Cells\n\n```javascript\nconst mergedTableData = [\n    [\n        { text: \"Q1 Results\", options: { colspan: 3, fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } }\n    ],\n    [\"Product\", \"Sales\", \"Market Share\"],\n    [\"Product A\", \"$25M\", \"35%\"],\n    [\"Product B\", \"$18M\", \"25%\"]\n];\n\nslide.addTable(mergedTableData, {\n    x: 1,\n    y: 1,\n    w: 8,\n    h: 2.5,\n    colW: [3, 2.5, 2.5],\n    border: { pt: 1, color: \"DDDDDD\" }\n});\n```\n\n### Table Options\n\nCommon table options:\n- `x, y, w, h` - Position and size\n- `colW` - Array of column widths (in inches)\n- `rowH` - Array of row heights (in inches)\n- `border` - Border style: `{ pt: 1, color: \"999999\" }`\n- `fill` - Background color (no # prefix)\n- `align` - Text alignment: \"left\", \"center\", \"right\"\n- `valign` - Vertical alignment: \"top\", \"middle\", \"bottom\"\n- `fontSize` - Text size\n- `autoPage` - Auto-create new slides if content overflows",
        "skills/presentation-builder/ooxml.md": "# Office Open XML Technical Reference for PowerPoint\n\n**Important: Read this entire document before starting.** Critical XML schema rules and formatting requirements are covered throughout. Incorrect implementation can create invalid PPTX files that PowerPoint cannot open.\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<p:txBody>`**: `<a:bodyPr>`, `<a:lstStyle>`, `<a:p>`\n- **Whitespace**: Add `xml:space='preserve'` to `<a:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n- **Images**: Add to `ppt/media/`, reference in slide XML, set dimensions to fit slide bounds\n- **Relationships**: Update `ppt/slides/_rels/slideN.xml.rels` for each slide's resources\n- **Dirty attribute**: Add `dirty=\"0\"` to `<a:rPr>` and `<a:endParaRPr>` elements to indicate clean state\n\n## Presentation Structure\n\n### Basic Slide Structure\n```xml\n<!-- ppt/slides/slide1.xml -->\n<p:sld>\n  <p:cSld>\n    <p:spTree>\n      <p:nvGrpSpPr>...</p:nvGrpSpPr>\n      <p:grpSpPr>...</p:grpSpPr>\n      <!-- Shapes go here -->\n    </p:spTree>\n  </p:cSld>\n</p:sld>\n```\n\n### Text Box / Shape with Text\n```xml\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"2\" name=\"Title\"/>\n    <p:cNvSpPr>\n      <a:spLocks noGrp=\"1\"/>\n    </p:cNvSpPr>\n    <p:nvPr>\n      <p:ph type=\"ctrTitle\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"838200\" y=\"365125\"/>\n      <a:ext cx=\"7772400\" cy=\"1470025\"/>\n    </a:xfrm>\n  </p:spPr>\n  <p:txBody>\n    <a:bodyPr/>\n    <a:lstStyle/>\n    <a:p>\n      <a:r>\n        <a:t>Slide Title</a:t>\n      </a:r>\n    </a:p>\n  </p:txBody>\n</p:sp>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<a:r>\n  <a:rPr b=\"1\"/>\n  <a:t>Bold Text</a:t>\n</a:r>\n\n<!-- Italic -->\n<a:r>\n  <a:rPr i=\"1\"/>\n  <a:t>Italic Text</a:t>\n</a:r>\n\n<!-- Underline -->\n<a:r>\n  <a:rPr u=\"sng\"/>\n  <a:t>Underlined</a:t>\n</a:r>\n\n<!-- Highlight -->\n<a:r>\n  <a:rPr>\n    <a:highlight>\n      <a:srgbClr val=\"FFFF00\"/>\n    </a:highlight>\n  </a:rPr>\n  <a:t>Highlighted Text</a:t>\n</a:r>\n\n<!-- Font and Size -->\n<a:r>\n  <a:rPr sz=\"2400\" typeface=\"Arial\">\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Colored Arial 24pt</a:t>\n</a:r>\n\n<!-- Complete formatting example -->\n<a:r>\n  <a:rPr lang=\"en-US\" sz=\"1400\" b=\"1\" dirty=\"0\">\n    <a:solidFill>\n      <a:srgbClr val=\"FAFAFA\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Formatted text</a:t>\n</a:r>\n```\n\n### Lists\n```xml\n<!-- Bullet list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buChar char=\"\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First bullet point</a:t>\n  </a:r>\n</a:p>\n\n<!-- Numbered list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buAutoNum type=\"arabicPeriod\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First numbered item</a:t>\n  </a:r>\n</a:p>\n\n<!-- Second level indent -->\n<a:p>\n  <a:pPr lvl=\"1\">\n    <a:buChar char=\"\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>Indented bullet</a:t>\n  </a:r>\n</a:p>\n```\n\n### Shapes\n```xml\n<!-- Rectangle -->\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"3\" name=\"Rectangle\"/>\n    <p:cNvSpPr/>\n    <p:nvPr/>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n    <a:ln w=\"25400\">\n      <a:solidFill>\n        <a:srgbClr val=\"000000\"/>\n      </a:solidFill>\n    </a:ln>\n  </p:spPr>\n</p:sp>\n\n<!-- Rounded Rectangle -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"roundRect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n\n<!-- Circle/Ellipse -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"ellipse\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n```\n\n### Images\n```xml\n<p:pic>\n  <p:nvPicPr>\n    <p:cNvPr id=\"4\" name=\"Picture\">\n      <a:hlinkClick r:id=\"\" action=\"ppaction://media\"/>\n    </p:cNvPr>\n    <p:cNvPicPr>\n      <a:picLocks noChangeAspect=\"1\"/>\n    </p:cNvPicPr>\n    <p:nvPr/>\n  </p:nvPicPr>\n  <p:blipFill>\n    <a:blip r:embed=\"rId2\"/>\n    <a:stretch>\n      <a:fillRect/>\n    </a:stretch>\n  </p:blipFill>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:pic>\n```\n\n### Tables\n```xml\n<p:graphicFrame>\n  <p:nvGraphicFramePr>\n    <p:cNvPr id=\"5\" name=\"Table\"/>\n    <p:cNvGraphicFramePr>\n      <a:graphicFrameLocks noGrp=\"1\"/>\n    </p:cNvGraphicFramePr>\n    <p:nvPr/>\n  </p:nvGraphicFramePr>\n  <p:xfrm>\n    <a:off x=\"1000000\" y=\"1000000\"/>\n    <a:ext cx=\"6000000\" cy=\"2000000\"/>\n  </p:xfrm>\n  <a:graphic>\n    <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/table\">\n      <a:tbl>\n        <a:tblGrid>\n          <a:gridCol w=\"3000000\"/>\n          <a:gridCol w=\"3000000\"/>\n        </a:tblGrid>\n        <a:tr h=\"500000\">\n          <a:tc>\n            <a:txBody>\n              <a:bodyPr/>\n              <a:lstStyle/>\n              <a:p>\n                <a:r>\n                  <a:t>Cell 1</a:t>\n                </a:r>\n              </a:p>\n            </a:txBody>\n          </a:tc>\n          <a:tc>\n            <a:txBody>\n              <a:bodyPr/>\n              <a:lstStyle/>\n              <a:p>\n                <a:r>\n                  <a:t>Cell 2</a:t>\n                </a:r>\n              </a:p>\n            </a:txBody>\n          </a:tc>\n        </a:tr>\n      </a:tbl>\n    </a:graphicData>\n  </a:graphic>\n</p:graphicFrame>\n```\n\n### Slide Layouts\n\n```xml\n<!-- Title Slide Layout -->\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"ctrTitle\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Title content -->\n</p:sp>\n\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"subTitle\" idx=\"1\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Subtitle content -->\n</p:sp>\n\n<!-- Content Slide Layout -->\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"title\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Slide title -->\n</p:sp>\n\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"body\" idx=\"1\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Content body -->\n</p:sp>\n```\n\n## File Updates\n\nWhen adding content, update these files:\n\n**`ppt/_rels/presentation.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/slide\" Target=\"slides/slide1.xml\"/>\n<Relationship Id=\"rId2\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/slideMaster\" Target=\"slideMasters/slideMaster1.xml\"/>\n```\n\n**`ppt/slides/_rels/slide1.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/slideLayout\" Target=\"../slideLayouts/slideLayout1.xml\"/>\n<Relationship Id=\"rId2\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"../media/image1.png\"/>\n```\n\n**`[Content_Types].xml`:**\n```xml\n<Default Extension=\"png\" ContentType=\"image/png\"/>\n<Default Extension=\"jpg\" ContentType=\"image/jpeg\"/>\n<Override PartName=\"/ppt/slides/slide1.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.presentationml.slide+xml\"/>\n```\n\n**`ppt/presentation.xml`:**\n```xml\n<p:sldIdLst>\n  <p:sldId id=\"256\" r:id=\"rId1\"/>\n  <p:sldId id=\"257\" r:id=\"rId2\"/>\n</p:sldIdLst>\n```\n\n**`docProps/app.xml`:** Update slide count and statistics\n```xml\n<Slides>2</Slides>\n<Paragraphs>10</Paragraphs>\n<Words>50</Words>\n```\n\n## Slide Operations\n\n### Adding a New Slide\nWhen adding a slide to the end of the presentation:\n\n1. **Create the slide file** (`ppt/slides/slideN.xml`)\n2. **Update `[Content_Types].xml`**: Add Override for the new slide\n3. **Update `ppt/_rels/presentation.xml.rels`**: Add relationship for the new slide\n4. **Update `ppt/presentation.xml`**: Add slide ID to `<p:sldIdLst>`\n5. **Create slide relationships** (`ppt/slides/_rels/slideN.xml.rels`) if needed\n6. **Update `docProps/app.xml`**: Increment slide count and update statistics (if present)\n\n### Duplicating a Slide\n1. Copy the source slide XML file with a new name\n2. Update all IDs in the new slide to be unique\n3. Follow the \"Adding a New Slide\" steps above\n4. **CRITICAL**: Remove or update any notes slide references in `_rels` files\n5. Remove references to unused media files\n\n### Reordering Slides\n1. **Update `ppt/presentation.xml`**: Reorder `<p:sldId>` elements in `<p:sldIdLst>`\n2. The order of `<p:sldId>` elements determines slide order\n3. Keep slide IDs and relationship IDs unchanged\n\nExample:\n```xml\n<!-- Original order -->\n<p:sldIdLst>\n  <p:sldId id=\"256\" r:id=\"rId2\"/>\n  <p:sldId id=\"257\" r:id=\"rId3\"/>\n  <p:sldId id=\"258\" r:id=\"rId4\"/>\n</p:sldIdLst>\n\n<!-- After moving slide 3 to position 2 -->\n<p:sldIdLst>\n  <p:sldId id=\"256\" r:id=\"rId2\"/>\n  <p:sldId id=\"258\" r:id=\"rId4\"/>\n  <p:sldId id=\"257\" r:id=\"rId3\"/>\n</p:sldIdLst>\n```\n\n### Deleting a Slide\n1. **Remove from `ppt/presentation.xml`**: Delete the `<p:sldId>` entry\n2. **Remove from `ppt/_rels/presentation.xml.rels`**: Delete the relationship\n3. **Remove from `[Content_Types].xml`**: Delete the Override entry\n4. **Delete files**: Remove `ppt/slides/slideN.xml` and `ppt/slides/_rels/slideN.xml.rels`\n5. **Update `docProps/app.xml`**: Decrement slide count and update statistics\n6. **Clean up unused media**: Remove orphaned images from `ppt/media/`\n\nNote: Don't renumber remaining slides - keep their original IDs and filenames.\n\n\n## Common Errors to Avoid\n\n- **Encodings**: Escape unicode characters in ASCII content: `\"` becomes `&#8220;`\n- **Images**: Add to `ppt/media/` and update relationship files\n- **Lists**: Omit bullets from list headers\n- **IDs**: Use valid hexadecimal values for UUIDs\n- **Themes**: Check all themes in `theme` directory for colors\n\n## Validation Checklist for Template-Based Presentations\n\n### Before Packing, Always:\n- **Clean unused resources**: Remove unreferenced media, fonts, and notes directories\n- **Fix Content_Types.xml**: Declare ALL slides, layouts, and themes present in the package\n- **Fix relationship IDs**: \n   - Remove font embed references if not using embedded fonts\n- **Remove broken references**: Check all `_rels` files for references to deleted resources\n\n### Common Template Duplication Pitfalls:\n- Multiple slides referencing the same notes slide after duplication\n- Image/media references from template slides that no longer exist\n- Font embedding references when fonts aren't included\n- Missing slideLayout declarations for layouts 12-25\n- docProps directory may not unpack - this is optional",
        "skills/prospect-investigation/SKILL.md": "---\nname: prospect-investigation\ndescription: Research and investigate business prospects and leads. Gathers company information, contact details, and qualification data for sales.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Lead Research Assistant\n\nThis skill helps you identify and qualify potential leads for your business by analyzing your product/service, understanding your ideal customer profile, and providing actionable outreach strategies.\n\n## When to Use This Skill\n\n- Finding potential customers or clients for your product/service\n- Building a list of companies to reach out to for partnerships\n- Identifying target accounts for sales outreach\n- Researching companies that match your ideal customer profile\n- Preparing for business development activities\n\n## What This Skill Does\n\n1. **Understands Your Business**: Analyzes your product/service, value proposition, and target market\n2. **Identifies Target Companies**: Finds companies that match your ideal customer profile based on:\n   - Industry and sector\n   - Company size and location\n   - Technology stack and tools they use\n   - Growth stage and funding\n   - Pain points your product solves\n3. **Prioritizes Leads**: Ranks companies based on fit score and relevance\n4. **Provides Contact Strategies**: Suggests how to approach each lead with personalized messaging\n5. **Enriches Data**: Gathers relevant information about decision-makers and company context\n\n## How to Use\n\n### Basic Usage\n\nSimply describe your product/service and what you're looking for:\n\n```\nI'm building [product description]. Find me 10 companies in [location/industry] \nthat would be good leads for this.\n```\n\n### With Your Codebase\n\nFor even better results, run this from your product's source code directory:\n\n```\nLook at what I'm building in this repository and identify the top 10 companies \nin [location/industry] that would benefit from this product.\n```\n\n### Advanced Usage\n\nFor more targeted research:\n\n```\nMy product: [description]\nIdeal customer profile:\n- Industry: [industry]\n- Company size: [size range]\n- Location: [location]\n- Current pain points: [pain points]\n- Technologies they use: [tech stack]\n\nFind me 20 qualified leads with contact strategies for each.\n```\n\n## Instructions\n\nWhen a user requests lead research:\n\n1. **Understand the Product/Service**\n   - If in a code directory, analyze the codebase to understand the product\n   - Ask clarifying questions about the value proposition\n   - Identify key features and benefits\n   - Understand what problems it solves\n\n2. **Define Ideal Customer Profile**\n   - Determine target industries and sectors\n   - Identify company size ranges\n   - Consider geographic preferences\n   - Understand relevant pain points\n   - Note any technology requirements\n\n3. **Research and Identify Leads**\n   - Search for companies matching the criteria\n   - Look for signals of need (job postings, tech stack, recent news)\n   - Consider growth indicators (funding, expansion, hiring)\n   - Identify companies with complementary products/services\n   - Check for budget indicators\n\n4. **Prioritize and Score**\n   - Create a fit score (1-10) for each lead\n   - Consider factors like:\n     - Alignment with ICP\n     - Signals of immediate need\n     - Budget availability\n     - Competitive landscape\n     - Timing indicators\n\n5. **Provide Actionable Output**\n   \n   For each lead, provide:\n   - **Company Name** and website\n   - **Why They're a Good Fit**: Specific reasons based on their business\n   - **Priority Score**: 1-10 with explanation\n   - **Decision Maker**: Role/title to target (e.g., \"VP of Engineering\")\n   - **Contact Strategy**: Personalized approach suggestions\n   - **Value Proposition**: How your product solves their specific problem\n   - **Conversation Starters**: Specific points to mention in outreach\n   - **LinkedIn URL**: If available, for easy connection\n\n6. **Format the Output**\n\n   Present results in a clear, scannable format:\n\n   ```markdown\n   # Lead Research Results\n   \n   ## Summary\n   - Total leads found: [X]\n   - High priority (8-10): [X]\n   - Medium priority (5-7): [X]\n   - Average fit score: [X]\n   \n   ---\n   \n   ## Lead 1: [Company Name]\n   \n   **Website**: [URL]\n   **Priority Score**: [X/10]\n   **Industry**: [Industry]\n   **Size**: [Employee count/revenue range]\n   \n   **Why They're a Good Fit**:\n   [2-3 specific reasons based on their business]\n   \n   **Target Decision Maker**: [Role/Title]\n   **LinkedIn**: [URL if available]\n   \n   **Value Proposition for Them**:\n   [Specific benefit for this company]\n   \n   **Outreach Strategy**:\n   [Personalized approach - mention specific pain points, recent company news, or relevant context]\n   \n   **Conversation Starters**:\n   - [Specific point 1]\n   - [Specific point 2]\n   \n   ---\n   \n   [Repeat for each lead]\n   ```\n\n7. **Offer Next Steps**\n   - Suggest saving results to a CSV for CRM import\n   - Offer to draft personalized outreach messages\n   - Recommend prioritization based on timing\n   - Suggest follow-up research for top leads\n\n## Examples\n\n### Example 1: From Lenny's Newsletter\n\n**User**: \"I'm building a tool that masks sensitive data in AI coding assistant queries. Find potential leads.\"\n\n**Output**: Creates a prioritized list of companies that:\n- Use AI coding assistants (Copilot, Cursor, etc.)\n- Handle sensitive data (fintech, healthcare, legal)\n- Have evidence in their GitHub repos of using coding agents\n- May have accidentally exposed sensitive data in code\n- Includes LinkedIn URLs of relevant decision-makers\n\n### Example 2: Local Business\n\n**User**: \"I run a consulting practice for remote team productivity. Find me 10 companies in the Bay Area that recently went remote.\"\n\n**Output**: Identifies companies that:\n- Recently posted remote job listings\n- Announced remote-first policies\n- Are hiring distributed teams\n- Show signs of remote work challenges\n- Provides personalized outreach strategies for each\n\n## Tips for Best Results\n\n- **Be specific** about your product and its unique value\n- **Run from your codebase** if applicable for automatic context\n- **Provide context** about your ideal customer profile\n- **Specify constraints** like industry, location, or company size\n- **Request follow-up** research on promising leads for deeper insights\n\n## Related Use Cases\n\n- Drafting personalized outreach emails after identifying leads\n- Building a CRM-ready CSV of qualified prospects\n- Researching specific companies in detail\n- Analyzing competitor customer bases\n- Identifying partnership opportunities\n",
        "skills/protocol-implementation-framework/SKILL.md": "---\nname: protocol-implementation-framework\ndescription: Build Model Context Protocol servers and implementations. Creates protocol-compliant tools and integrations for AI-powered applications.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# MCP Server Development Guide\n\n## Overview\n\nCreate MCP (Model Context Protocol) servers that enable LLMs to interact with external services through well-designed tools. The quality of an MCP server is measured by how well it enables LLMs to accomplish real-world tasks.\n\n---\n\n# Process\n\n##  High-Level Workflow\n\nCreating a high-quality MCP server involves four main phases:\n\n### Phase 1: Deep Research and Planning\n\n#### 1.1 Understand Modern MCP Design\n\n**API Coverage vs. Workflow Tools:**\nBalance comprehensive API endpoint coverage with specialized workflow tools. Workflow tools can be more convenient for specific tasks, while comprehensive coverage gives agents flexibility to compose operations. Performance varies by clientsome clients benefit from code execution that combines basic tools, while others work better with higher-level workflows. When uncertain, prioritize comprehensive API coverage.\n\n**Tool Naming and Discoverability:**\nClear, descriptive tool names help agents find the right tools quickly. Use consistent prefixes (e.g., `github_create_issue`, `github_list_repos`) and action-oriented naming.\n\n**Context Management:**\nAgents benefit from concise tool descriptions and the ability to filter/paginate results. Design tools that return focused, relevant data. Some clients support code execution which can help agents filter and process data efficiently.\n\n**Actionable Error Messages:**\nError messages should guide agents toward solutions with specific suggestions and next steps.\n\n**Evaluation-Driven Iteration:**\nDraft a few realistic evaluation questions early, then refine tool design based on where agents struggle.\n\n#### 1.2 Study MCP Protocol Documentation\n\n**Navigate the MCP specification:**\n\nPrefer the full spec text at `https://modelcontextprotocol.io/llms-full.txt`.\nIf you need targeted pages, start with the sitemap: `https://modelcontextprotocol.io/sitemap.xml`\n\nThen fetch specific pages with `.md` suffix for markdown format (e.g., `https://modelcontextprotocol.io/specification/draft.md`).\n\nKey pages to review:\n- Specification overview and architecture\n- Transport mechanisms (streamable HTTP, stdio)\n- Tool, resource, and prompt definitions\n\n#### 1.3 Study Framework Documentation\n\n**Recommended stack:**\n- **Language**: TypeScript (high-quality SDK support and good compatibility in many execution environments e.g. MCPB. Plus AI models are good at generating TypeScript code, benefiting from its broad usage, static typing and good linting tools)\n- **Transport**: Streamable HTTP for remote servers, using stateless JSON (simpler to scale and maintain, as opposed to stateful sessions and streaming responses). stdio for local servers.\n\n**Load framework documentation:**\n\n- **MCP Best Practices**: [ View Best Practices](./reference/mcp_best_practices.md) - Core guidelines\n\n**For TypeScript (recommended):**\n- **TypeScript SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n- [ TypeScript Guide](./reference/node_mcp_server.md) - TypeScript patterns and examples\n\n**For Python:**\n- **Python SDK**: Use WebFetch to load `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- [ Python Guide](./reference/python_mcp_server.md) - Python patterns and examples\n\n#### 1.4 Plan Your Implementation\n\n**Understand the API:**\nReview the service's API documentation to identify key endpoints, authentication requirements, and data models. Use web search and WebFetch as needed.\n\n**Tool Selection:**\nPrioritize comprehensive API coverage. List endpoints to implement, starting with the most common operations.\n\n---\n\n### Phase 2: Implementation\n\n#### 2.1 Set Up Project Structure\n\nSee language-specific guides for project setup:\n- [ TypeScript Guide](./reference/node_mcp_server.md) - Project structure, package.json, tsconfig.json\n- [ Python Guide](./reference/python_mcp_server.md) - Module organization, dependencies\n\n#### 2.2 Implement Core Infrastructure\n\nCreate shared utilities:\n- API client with authentication\n- Error handling helpers\n- Response formatting (JSON/Markdown)\n- Pagination support\n\n#### 2.3 Implement Tools\n\nFor each tool:\n\n**Input Schema:**\n- Use Zod (TypeScript) or Pydantic (Python)\n- Include constraints and clear descriptions\n- Add examples in field descriptions\n\n**Output Schema:**\n- Define `outputSchema` where possible for structured data\n- Use `structuredContent` in tool responses (TypeScript SDK feature)\n- Helps clients understand and process tool outputs\n\n**Tool Description:**\n- Concise summary of functionality\n- Parameter descriptions\n- Return type schema\n\n**Implementation:**\n- Async/await for I/O operations\n- Proper error handling with actionable messages\n- Support pagination where applicable\n- Return both text content and structured data when using modern SDKs\n\n**Annotations:**\n- `readOnlyHint`: true/false\n- `destructiveHint`: true/false\n- `idempotentHint`: true/false\n- `openWorldHint`: true/false\n\n---\n\n### Phase 3: Review and Test\n\n#### 3.1 Code Quality\n\nReview for:\n- No duplicated code (DRY principle)\n- Consistent error handling\n- Full type coverage\n- Clear tool descriptions\n\n#### 3.2 Build and Test\n\n**Note:** MCP servers are long-running processes. If you run them directly, your process will block waiting for requests. Prefer the evaluation harness, or run the server in a separate session (tmux) or with a timeout.\n\n**TypeScript:**\n- Run `npm run build` to verify compilation\n- Test with MCP Inspector: `npx @modelcontextprotocol/inspector`\n\n**Python:**\n- Verify syntax: `python -m py_compile your_server.py`\n- Test with MCP Inspector\n\nSee language-specific guides for detailed testing approaches and quality checklists.\n\n---\n\n### Phase 4: Create Evaluations\n\nAfter implementing your MCP server, create comprehensive evaluations to test its effectiveness.\n\n**Load [ Evaluation Guide](./reference/evaluation.md) for complete evaluation guidelines.**\n\n#### 4.1 Understand Evaluation Purpose\n\nUse evaluations to test whether LLMs can effectively use your MCP server to answer realistic, complex questions.\n\n#### 4.2 Create 10 Evaluation Questions\n\nTo create effective evaluations, follow the process outlined in the evaluation guide:\n\n1. **Tool Inspection**: List available tools and understand their capabilities\n2. **Content Exploration**: Use READ-ONLY operations to explore available data\n3. **Question Generation**: Create 10 complex, realistic questions\n4. **Answer Verification**: Solve each question yourself to verify answers\n\n#### 4.3 Evaluation Requirements\n\nEnsure each question is:\n- **Independent**: Not dependent on other questions\n- **Read-only**: Only non-destructive operations required\n- **Complex**: Requiring multiple tool calls and deep exploration\n- **Realistic**: Based on real use cases humans would care about\n- **Verifiable**: Single, clear answer that can be verified by string comparison\n- **Stable**: Answer won't change over time\n\n#### 4.4 Output Format\n\nCreate an XML file with this structure:\n\n```xml\n<evaluation>\n  <qa_pair>\n    <question>Find discussions about AI model launches with animal codenames. One model needed a specific safety designation that uses the format ASL-X. What number X was being determined for the model named after a spotted wild cat?</question>\n    <answer>3</answer>\n  </qa_pair>\n<!-- More qa_pairs... -->\n</evaluation>\n```\n\n---\n\n# Reference Files\n\n##  Documentation Library\n\nLoad these resources as needed during development:\n\n### Core MCP Documentation (Load First)\n- **MCP Protocol**: Prefer `https://modelcontextprotocol.io/llms-full.txt` for the full spec. Use the sitemap at `https://modelcontextprotocol.io/sitemap.xml` for targeted pages.\n- [ MCP Best Practices](./reference/mcp_best_practices.md) - Universal MCP guidelines including:\n  - Server and tool naming conventions\n  - Response format guidelines (JSON vs Markdown)\n  - Pagination best practices\n  - Transport selection (streamable HTTP vs stdio)\n  - Security and error handling standards\n\n### SDK Documentation (Load During Phase 1/2)\n- **Python SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n- **TypeScript SDK**: Fetch from `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`\n\n### Language-Specific Implementation Guides (Load During Phase 2)\n- [ Python Implementation Guide](./reference/python_mcp_server.md) - Complete Python/FastMCP guide with:\n  - Server initialization patterns\n  - Pydantic model examples\n  - Tool registration with `@mcp.tool`\n  - Complete working examples\n  - Quality checklist\n\n- [ TypeScript Implementation Guide](./reference/node_mcp_server.md) - Complete TypeScript guide with:\n  - Project structure\n  - Zod schema patterns\n  - Tool registration with `server.registerTool`\n  - Complete working examples\n  - Quality checklist\n\n### Evaluation Guide (Load During Phase 4)\n- [ Evaluation Guide](./reference/evaluation.md) - Complete evaluation creation guide with:\n  - Question creation guidelines\n  - Answer verification strategies\n  - XML format specifications\n  - Example questions and answers\n  - Running an evaluation with the provided scripts\n",
        "skills/protocol-implementation-framework/reference/evaluation.md": "# MCP Server Evaluation Guide\n\n## Overview\n\nThis document provides guidance on creating comprehensive evaluations for MCP servers. Evaluations test whether LLMs can effectively use your MCP server to answer realistic, complex questions using only the tools provided.\n\n---\n\n## Quick Reference\n\n### Evaluation Requirements\n- Create 10 human-readable questions\n- Questions must be READ-ONLY, INDEPENDENT, NON-DESTRUCTIVE\n- Each question requires multiple tool calls (potentially dozens)\n- Answers must be single, verifiable values\n- Answers must be STABLE (won't change over time)\n\n### Output Format\n```xml\n<evaluation>\n   <qa_pair>\n      <question>Your question here</question>\n      <answer>Single verifiable answer</answer>\n   </qa_pair>\n</evaluation>\n```\n\n---\n\n## Purpose of Evaluations\n\nThe measure of quality of an MCP server is NOT how well or comprehensively the server implements tools, but how well these implementations (input/output schemas, docstrings/descriptions, functionality) enable LLMs with no other context and access ONLY to the MCP servers to answer realistic and difficult questions.\n\n## Evaluation Overview\n\nCreate 10 human-readable questions requiring ONLY READ-ONLY, INDEPENDENT, NON-DESTRUCTIVE, and IDEMPOTENT operations to answer. Each question should be:\n- Realistic\n- Clear and concise\n- Unambiguous\n- Complex, requiring potentially dozens of tool calls or steps\n- Answerable with a single, verifiable value that you identify in advance\n\n## Question Guidelines\n\n### Core Requirements\n\n1. **Questions MUST be independent**\n   - Each question should NOT depend on the answer to any other question\n   - Should not assume prior write operations from processing another question\n\n2. **Questions MUST require ONLY NON-DESTRUCTIVE AND IDEMPOTENT tool use**\n   - Should not instruct or require modifying state to arrive at the correct answer\n\n3. **Questions must be REALISTIC, CLEAR, CONCISE, and COMPLEX**\n   - Must require another LLM to use multiple (potentially dozens of) tools or steps to answer\n\n### Complexity and Depth\n\n4. **Questions must require deep exploration**\n   - Consider multi-hop questions requiring multiple sub-questions and sequential tool calls\n   - Each step should benefit from information found in previous questions\n\n5. **Questions may require extensive paging**\n   - May need paging through multiple pages of results\n   - May require querying old data (1-2 years out-of-date) to find niche information\n   - The questions must be DIFFICULT\n\n6. **Questions must require deep understanding**\n   - Rather than surface-level knowledge\n   - May pose complex ideas as True/False questions requiring evidence\n   - May use multiple-choice format where LLM must search different hypotheses\n\n7. **Questions must not be solvable with straightforward keyword search**\n   - Do not include specific keywords from the target content\n   - Use synonyms, related concepts, or paraphrases\n   - Require multiple searches, analyzing multiple related items, extracting context, then deriving the answer\n\n### Tool Testing\n\n8. **Questions should stress-test tool return values**\n   - May elicit tools returning large JSON objects or lists, overwhelming the LLM\n   - Should require understanding multiple modalities of data:\n     - IDs and names\n     - Timestamps and datetimes (months, days, years, seconds)\n     - File IDs, names, extensions, and mimetypes\n     - URLs, GIDs, etc.\n   - Should probe the tool's ability to return all useful forms of data\n\n9. **Questions should MOSTLY reflect real human use cases**\n   - The kinds of information retrieval tasks that HUMANS assisted by an LLM would care about\n\n10. **Questions may require dozens of tool calls**\n    - This challenges LLMs with limited context\n    - Encourages MCP server tools to reduce information returned\n\n11. **Include ambiguous questions**\n    - May be ambiguous OR require difficult decisions on which tools to call\n    - Force the LLM to potentially make mistakes or misinterpret\n    - Ensure that despite AMBIGUITY, there is STILL A SINGLE VERIFIABLE ANSWER\n\n### Stability\n\n12. **Questions must be designed so the answer DOES NOT CHANGE**\n    - Do not ask questions that rely on \"current state\" which is dynamic\n    - For example, do not count:\n      - Number of reactions to a post\n      - Number of replies to a thread\n      - Number of members in a channel\n\n13. **DO NOT let the MCP server RESTRICT the kinds of questions you create**\n    - Create challenging and complex questions\n    - Some may not be solvable with the available MCP server tools\n    - Questions may require specific output formats (datetime vs. epoch time, JSON vs. MARKDOWN)\n    - Questions may require dozens of tool calls to complete\n\n## Answer Guidelines\n\n### Verification\n\n1. **Answers must be VERIFIABLE via direct string comparison**\n   - If the answer can be re-written in many formats, clearly specify the output format in the QUESTION\n   - Examples: \"Use YYYY/MM/DD.\", \"Respond True or False.\", \"Answer A, B, C, or D and nothing else.\"\n   - Answer should be a single VERIFIABLE value such as:\n     - User ID, user name, display name, first name, last name\n     - Channel ID, channel name\n     - Message ID, string\n     - URL, title\n     - Numerical quantity\n     - Timestamp, datetime\n     - Boolean (for True/False questions)\n     - Email address, phone number\n     - File ID, file name, file extension\n     - Multiple choice answer\n   - Answers must not require special formatting or complex, structured output\n   - Answer will be verified using DIRECT STRING COMPARISON\n\n### Readability\n\n2. **Answers should generally prefer HUMAN-READABLE formats**\n   - Examples: names, first name, last name, datetime, file name, message string, URL, yes/no, true/false, a/b/c/d\n   - Rather than opaque IDs (though IDs are acceptable)\n   - The VAST MAJORITY of answers should be human-readable\n\n### Stability\n\n3. **Answers must be STABLE/STATIONARY**\n   - Look at old content (e.g., conversations that have ended, projects that have launched, questions answered)\n   - Create QUESTIONS based on \"closed\" concepts that will always return the same answer\n   - Questions may ask to consider a fixed time window to insulate from non-stationary answers\n   - Rely on context UNLIKELY to change\n   - Example: if finding a paper name, be SPECIFIC enough so answer is not confused with papers published later\n\n4. **Answers must be CLEAR and UNAMBIGUOUS**\n   - Questions must be designed so there is a single, clear answer\n   - Answer can be derived from using the MCP server tools\n\n### Diversity\n\n5. **Answers must be DIVERSE**\n   - Answer should be a single VERIFIABLE value in diverse modalities and formats\n   - User concept: user ID, user name, display name, first name, last name, email address, phone number\n   - Channel concept: channel ID, channel name, channel topic\n   - Message concept: message ID, message string, timestamp, month, day, year\n\n6. **Answers must NOT be complex structures**\n   - Not a list of values\n   - Not a complex object\n   - Not a list of IDs or strings\n   - Not natural language text\n   - UNLESS the answer can be straightforwardly verified using DIRECT STRING COMPARISON\n   - And can be realistically reproduced\n   - It should be unlikely that an LLM would return the same list in any other order or format\n\n## Evaluation Process\n\n### Step 1: Documentation Inspection\n\nRead the documentation of the target API to understand:\n- Available endpoints and functionality\n- If ambiguity exists, fetch additional information from the web\n- Parallelize this step AS MUCH AS POSSIBLE\n- Ensure each subagent is ONLY examining documentation from the file system or on the web\n\n### Step 2: Tool Inspection\n\nList the tools available in the MCP server:\n- Inspect the MCP server directly\n- Understand input/output schemas, docstrings, and descriptions\n- WITHOUT calling the tools themselves at this stage\n\n### Step 3: Developing Understanding\n\nRepeat steps 1 & 2 until you have a good understanding:\n- Iterate multiple times\n- Think about the kinds of tasks you want to create\n- Refine your understanding\n- At NO stage should you READ the code of the MCP server implementation itself\n- Use your intuition and understanding to create reasonable, realistic, but VERY challenging tasks\n\n### Step 4: Read-Only Content Inspection\n\nAfter understanding the API and tools, USE the MCP server tools:\n- Inspect content using READ-ONLY and NON-DESTRUCTIVE operations ONLY\n- Goal: identify specific content (e.g., users, channels, messages, projects, tasks) for creating realistic questions\n- Should NOT call any tools that modify state\n- Will NOT read the code of the MCP server implementation itself\n- Parallelize this step with individual sub-agents pursuing independent explorations\n- Ensure each subagent is only performing READ-ONLY, NON-DESTRUCTIVE, and IDEMPOTENT operations\n- BE CAREFUL: SOME TOOLS may return LOTS OF DATA which would cause you to run out of CONTEXT\n- Make INCREMENTAL, SMALL, AND TARGETED tool calls for exploration\n- In all tool call requests, use the `limit` parameter to limit results (<10)\n- Use pagination\n\n### Step 5: Task Generation\n\nAfter inspecting the content, create 10 human-readable questions:\n- An LLM should be able to answer these with the MCP server\n- Follow all question and answer guidelines above\n\n## Output Format\n\nEach QA pair consists of a question and an answer. The output should be an XML file with this structure:\n\n```xml\n<evaluation>\n   <qa_pair>\n      <question>Find the project created in Q2 2024 with the highest number of completed tasks. What is the project name?</question>\n      <answer>Website Redesign</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Search for issues labeled as \"bug\" that were closed in March 2024. Which user closed the most issues? Provide their username.</question>\n      <answer>sarah_dev</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Look for pull requests that modified files in the /api directory and were merged between January 1 and January 31, 2024. How many different contributors worked on these PRs?</question>\n      <answer>7</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Find the repository with the most stars that was created before 2023. What is the repository name?</question>\n      <answer>data-pipeline</answer>\n   </qa_pair>\n</evaluation>\n```\n\n## Evaluation Examples\n\n### Good Questions\n\n**Example 1: Multi-hop question requiring deep exploration (GitHub MCP)**\n```xml\n<qa_pair>\n   <question>Find the repository that was archived in Q3 2023 and had previously been the most forked project in the organization. What was the primary programming language used in that repository?</question>\n   <answer>Python</answer>\n</qa_pair>\n```\n\nThis question is good because:\n- Requires multiple searches to find archived repositories\n- Needs to identify which had the most forks before archival\n- Requires examining repository details for the language\n- Answer is a simple, verifiable value\n- Based on historical (closed) data that won't change\n\n**Example 2: Requires understanding context without keyword matching (Project Management MCP)**\n```xml\n<qa_pair>\n   <question>Locate the initiative focused on improving customer onboarding that was completed in late 2023. The project lead created a retrospective document after completion. What was the lead's role title at that time?</question>\n   <answer>Product Manager</answer>\n</qa_pair>\n```\n\nThis question is good because:\n- Doesn't use specific project name (\"initiative focused on improving customer onboarding\")\n- Requires finding completed projects from specific timeframe\n- Needs to identify the project lead and their role\n- Requires understanding context from retrospective documents\n- Answer is human-readable and stable\n- Based on completed work (won't change)\n\n**Example 3: Complex aggregation requiring multiple steps (Issue Tracker MCP)**\n```xml\n<qa_pair>\n   <question>Among all bugs reported in January 2024 that were marked as critical priority, which assignee resolved the highest percentage of their assigned bugs within 48 hours? Provide the assignee's username.</question>\n   <answer>alex_eng</answer>\n</qa_pair>\n```\n\nThis question is good because:\n- Requires filtering bugs by date, priority, and status\n- Needs to group by assignee and calculate resolution rates\n- Requires understanding timestamps to determine 48-hour windows\n- Tests pagination (potentially many bugs to process)\n- Answer is a single username\n- Based on historical data from specific time period\n\n**Example 4: Requires synthesis across multiple data types (CRM MCP)**\n```xml\n<qa_pair>\n   <question>Find the account that upgraded from the Starter to Enterprise plan in Q4 2023 and had the highest annual contract value. What industry does this account operate in?</question>\n   <answer>Healthcare</answer>\n</qa_pair>\n```\n\nThis question is good because:\n- Requires understanding subscription tier changes\n- Needs to identify upgrade events in specific timeframe\n- Requires comparing contract values\n- Must access account industry information\n- Answer is simple and verifiable\n- Based on completed historical transactions\n\n### Poor Questions\n\n**Example 1: Answer changes over time**\n```xml\n<qa_pair>\n   <question>How many open issues are currently assigned to the engineering team?</question>\n   <answer>47</answer>\n</qa_pair>\n```\n\nThis question is poor because:\n- The answer will change as issues are created, closed, or reassigned\n- Not based on stable/stationary data\n- Relies on \"current state\" which is dynamic\n\n**Example 2: Too easy with keyword search**\n```xml\n<qa_pair>\n   <question>Find the pull request with title \"Add authentication feature\" and tell me who created it.</question>\n   <answer>developer123</answer>\n</qa_pair>\n```\n\nThis question is poor because:\n- Can be solved with a straightforward keyword search for exact title\n- Doesn't require deep exploration or understanding\n- No synthesis or analysis needed\n\n**Example 3: Ambiguous answer format**\n```xml\n<qa_pair>\n   <question>List all the repositories that have Python as their primary language.</question>\n   <answer>repo1, repo2, repo3, data-pipeline, ml-tools</answer>\n</qa_pair>\n```\n\nThis question is poor because:\n- Answer is a list that could be returned in any order\n- Difficult to verify with direct string comparison\n- LLM might format differently (JSON array, comma-separated, newline-separated)\n- Better to ask for a specific aggregate (count) or superlative (most stars)\n\n## Verification Process\n\nAfter creating evaluations:\n\n1. **Examine the XML file** to understand the schema\n2. **Load each task instruction** and in parallel using the MCP server and tools, identify the correct answer by attempting to solve the task YOURSELF\n3. **Flag any operations** that require WRITE or DESTRUCTIVE operations\n4. **Accumulate all CORRECT answers** and replace any incorrect answers in the document\n5. **Remove any `<qa_pair>`** that require WRITE or DESTRUCTIVE operations\n\nRemember to parallelize solving tasks to avoid running out of context, then accumulate all answers and make changes to the file at the end.\n\n## Tips for Creating Quality Evaluations\n\n1. **Think Hard and Plan Ahead** before generating tasks\n2. **Parallelize Where Opportunity Arises** to speed up the process and manage context\n3. **Focus on Realistic Use Cases** that humans would actually want to accomplish\n4. **Create Challenging Questions** that test the limits of the MCP server's capabilities\n5. **Ensure Stability** by using historical data and closed concepts\n6. **Verify Answers** by solving the questions yourself using the MCP server tools\n7. **Iterate and Refine** based on what you learn during the process\n\n---\n\n# Running Evaluations\n\nAfter creating your evaluation file, you can use the provided evaluation harness to test your MCP server.\n\n## Setup\n\n1. **Install Dependencies**\n\n   ```bash\n   pip install -r scripts/requirements.txt\n   ```\n\n   Or install manually:\n   ```bash\n   pip install anthropic mcp\n   ```\n\n2. **Set API Key**\n\n   ```bash\n   export ANTHROPIC_API_KEY=your_api_key_here\n   ```\n\n## Evaluation File Format\n\nEvaluation files use XML format with `<qa_pair>` elements:\n\n```xml\n<evaluation>\n   <qa_pair>\n      <question>Find the project created in Q2 2024 with the highest number of completed tasks. What is the project name?</question>\n      <answer>Website Redesign</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Search for issues labeled as \"bug\" that were closed in March 2024. Which user closed the most issues? Provide their username.</question>\n      <answer>sarah_dev</answer>\n   </qa_pair>\n</evaluation>\n```\n\n## Running Evaluations\n\nThe evaluation script (`scripts/evaluation.py`) supports three transport types:\n\n**Important:**\n- **stdio transport**: The evaluation script automatically launches and manages the MCP server process for you. Do not run the server manually.\n- **sse/http transports**: You must start the MCP server separately before running the evaluation. The script connects to the already-running server at the specified URL.\n\n### 1. Local STDIO Server\n\nFor locally-run MCP servers (script launches the server automatically):\n\n```bash\npython scripts/evaluation.py \\\n  -t stdio \\\n  -c python \\\n  -a my_mcp_server.py \\\n  evaluation.xml\n```\n\nWith environment variables:\n```bash\npython scripts/evaluation.py \\\n  -t stdio \\\n  -c python \\\n  -a my_mcp_server.py \\\n  -e API_KEY=abc123 \\\n  -e DEBUG=true \\\n  evaluation.xml\n```\n\n### 2. Server-Sent Events (SSE)\n\nFor SSE-based MCP servers (you must start the server first):\n\n```bash\npython scripts/evaluation.py \\\n  -t sse \\\n  -u https://example.com/mcp \\\n  -H \"Authorization: Bearer token123\" \\\n  -H \"X-Custom-Header: value\" \\\n  evaluation.xml\n```\n\n### 3. HTTP (Streamable HTTP)\n\nFor HTTP-based MCP servers (you must start the server first):\n\n```bash\npython scripts/evaluation.py \\\n  -t http \\\n  -u https://example.com/mcp \\\n  -H \"Authorization: Bearer token123\" \\\n  evaluation.xml\n```\n\n## Command-Line Options\n\n```\nusage: evaluation.py [-h] [-t {stdio,sse,http}] [-m MODEL] [-c COMMAND]\n                     [-a ARGS [ARGS ...]] [-e ENV [ENV ...]] [-u URL]\n                     [-H HEADERS [HEADERS ...]] [-o OUTPUT]\n                     eval_file\n\npositional arguments:\n  eval_file             Path to evaluation XML file\n\noptional arguments:\n  -h, --help            Show help message\n  -t, --transport       Transport type: stdio, sse, or http (default: stdio)\n  -m, --model           Claude model to use (default: claude-3-7-sonnet-20250219)\n  -o, --output          Output file for report (default: print to stdout)\n\nstdio options:\n  -c, --command         Command to run MCP server (e.g., python, node)\n  -a, --args            Arguments for the command (e.g., server.py)\n  -e, --env             Environment variables in KEY=VALUE format\n\nsse/http options:\n  -u, --url             MCP server URL\n  -H, --header          HTTP headers in 'Key: Value' format\n```\n\n## Output\n\nThe evaluation script generates a detailed report including:\n\n- **Summary Statistics**:\n  - Accuracy (correct/total)\n  - Average task duration\n  - Average tool calls per task\n  - Total tool calls\n\n- **Per-Task Results**:\n  - Prompt and expected response\n  - Actual response from the agent\n  - Whether the answer was correct (/)\n  - Duration and tool call details\n  - Agent's summary of its approach\n  - Agent's feedback on the tools\n\n### Save Report to File\n\n```bash\npython scripts/evaluation.py \\\n  -t stdio \\\n  -c python \\\n  -a my_server.py \\\n  -o evaluation_report.md \\\n  evaluation.xml\n```\n\n## Complete Example Workflow\n\nHere's a complete example of creating and running an evaluation:\n\n1. **Create your evaluation file** (`my_evaluation.xml`):\n\n```xml\n<evaluation>\n   <qa_pair>\n      <question>Find the user who created the most issues in January 2024. What is their username?</question>\n      <answer>alice_developer</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Among all pull requests merged in Q1 2024, which repository had the highest number? Provide the repository name.</question>\n      <answer>backend-api</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Find the project that was completed in December 2023 and had the longest duration from start to finish. How many days did it take?</question>\n      <answer>127</answer>\n   </qa_pair>\n</evaluation>\n```\n\n2. **Install dependencies**:\n\n```bash\npip install -r scripts/requirements.txt\nexport ANTHROPIC_API_KEY=your_api_key\n```\n\n3. **Run evaluation**:\n\n```bash\npython scripts/evaluation.py \\\n  -t stdio \\\n  -c python \\\n  -a github_mcp_server.py \\\n  -e GITHUB_TOKEN=ghp_xxx \\\n  -o github_eval_report.md \\\n  my_evaluation.xml\n```\n\n4. **Review the report** in `github_eval_report.md` to:\n   - See which questions passed/failed\n   - Read the agent's feedback on your tools\n   - Identify areas for improvement\n   - Iterate on your MCP server design\n\n## Troubleshooting\n\n### Connection Errors\n\nIf you get connection errors:\n- **STDIO**: Verify the command and arguments are correct\n- **SSE/HTTP**: Check the URL is accessible and headers are correct\n- Ensure any required API keys are set in environment variables or headers\n\n### Low Accuracy\n\nIf many evaluations fail:\n- Review the agent's feedback for each task\n- Check if tool descriptions are clear and comprehensive\n- Verify input parameters are well-documented\n- Consider whether tools return too much or too little data\n- Ensure error messages are actionable\n\n### Timeout Issues\n\nIf tasks are timing out:\n- Use a more capable model (e.g., `claude-3-7-sonnet-20250219`)\n- Check if tools are returning too much data\n- Verify pagination is working correctly\n- Consider simplifying complex questions",
        "skills/protocol-implementation-framework/reference/mcp_best_practices.md": "# MCP Server Best Practices\n\n## Quick Reference\n\n### Server Naming\n- **Python**: `{service}_mcp` (e.g., `slack_mcp`)\n- **Node/TypeScript**: `{service}-mcp-server` (e.g., `slack-mcp-server`)\n\n### Tool Naming\n- Use snake_case with service prefix\n- Format: `{service}_{action}_{resource}`\n- Example: `slack_send_message`, `github_create_issue`\n\n### Response Formats\n- Support both JSON and Markdown formats\n- JSON for programmatic processing\n- Markdown for human readability\n\n### Pagination\n- Always respect `limit` parameter\n- Return `has_more`, `next_offset`, `total_count`\n- Default to 20-50 items\n\n### Character Limits\n- Define a CHARACTER_LIMIT constant (typically 25,000)\n- Truncate gracefully with clear guidance on filtering\n\n### Transport\n- **Streamable HTTP**: For remote servers, multi-client scenarios\n- **stdio**: For local integrations, command-line tools\n- Avoid SSE (deprecated in favor of streamable HTTP)\n\n---\n\n## Server Naming Conventions\n\nFollow these standardized naming patterns:\n\n**Python**: Use format `{service}_mcp` (lowercase with underscores)\n- Examples: `slack_mcp`, `github_mcp`, `jira_mcp`\n\n**Node/TypeScript**: Use format `{service}-mcp-server` (lowercase with hyphens)\n- Examples: `slack-mcp-server`, `github-mcp-server`, `jira-mcp-server`\n\nThe name should be general, descriptive of the service being integrated, easy to infer from the task description, and without version numbers.\n\n---\n\n## Tool Naming and Design\n\n### Tool Naming\n\n1. **Use snake_case**: `search_users`, `create_project`, `get_channel_info`\n2. **Include service prefix**: Anticipate that your MCP server may be used alongside other MCP servers\n   - Use `slack_send_message` instead of just `send_message`\n   - Use `github_create_issue` instead of just `create_issue`\n3. **Be action-oriented**: Start with verbs (get, list, search, create, etc.)\n4. **Be specific**: Avoid generic names that could conflict with other servers\n\n### Tool Design\n\n- Tool descriptions must narrowly and unambiguously describe functionality\n- Descriptions must precisely match actual functionality\n- Provide tool annotations (readOnlyHint, destructiveHint, idempotentHint, openWorldHint)\n- Keep tool operations focused and atomic\n\n---\n\n## Response Formats\n\nAll tools that return data should support multiple formats:\n\n### JSON Format (`response_format=\"json\"`)\n- Machine-readable structured data\n- Include all available fields and metadata\n- Consistent field names and types\n- Use for programmatic processing\n\n### Markdown Format (`response_format=\"markdown\"`, typically default)\n- Human-readable formatted text\n- Use headers, lists, and formatting for clarity\n- Convert timestamps to human-readable format\n- Show display names with IDs in parentheses\n- Omit verbose metadata\n\n---\n\n## Pagination\n\nFor tools that list resources:\n\n- **Always respect the `limit` parameter**\n- **Implement pagination**: Use `offset` or cursor-based pagination\n- **Return pagination metadata**: Include `has_more`, `next_offset`/`next_cursor`, `total_count`\n- **Never load all results into memory**: Especially important for large datasets\n- **Default to reasonable limits**: 20-50 items is typical\n\nExample pagination response:\n```json\n{\n  \"total\": 150,\n  \"count\": 20,\n  \"offset\": 0,\n  \"items\": [...],\n  \"has_more\": true,\n  \"next_offset\": 20\n}\n```\n\n---\n\n## Character Limits and Truncation\n\nLarge responses should be truncated to avoid overwhelming the model and client:\n\n- Define a module-level `CHARACTER_LIMIT` constant (typically 25,000)\n- Check response size before returning\n- Truncate with a clear message explaining how to get the remaining data\n- Encourage use of filters, pagination, and tighter query parameters\n\n---\n\n## Transport Options\n\n### Streamable HTTP\n\n**Best for**: Remote servers, web services, multi-client scenarios\n\n**Characteristics**:\n- Bidirectional communication over HTTP\n- Supports multiple simultaneous clients\n- Can be deployed as a web service\n- Enables server-to-client notifications\n\n**Use when**:\n- Serving multiple clients simultaneously\n- Deploying as a cloud service\n- Integration with web applications\n\n### stdio\n\n**Best for**: Local integrations, command-line tools\n\n**Characteristics**:\n- Standard input/output stream communication\n- Simple setup, no network configuration needed\n- Runs as a subprocess of the client\n\n**Use when**:\n- Building tools for local development environments\n- Integrating with desktop applications\n- Single-user, single-session scenarios\n\n**Note**: stdio servers should NOT log to stdout (use stderr for logging)\n\n### Transport Selection\n\n| Criterion | stdio | Streamable HTTP |\n|-----------|-------|-----------------|\n| **Deployment** | Local | Remote |\n| **Clients** | Single | Multiple |\n| **Complexity** | Low | Medium |\n| **Real-time** | No | Yes |\n\n---\n\n## Security Best Practices\n\n### Authentication and Authorization\n\n**OAuth 2.1**:\n- Use secure OAuth 2.1 with certificates from recognized authorities\n- Validate access tokens before processing requests\n- Only accept tokens specifically intended for your server\n\n**API Keys**:\n- Store API keys in environment variables, never in code\n- Validate keys on server startup\n- Provide clear error messages when authentication fails\n\n### Input Validation\n\n- Sanitize file paths to prevent directory traversal\n- Validate URLs and external identifiers\n- Check parameter sizes and ranges\n- Prevent command injection in system calls\n- Use schema validation (Pydantic/Zod) for all inputs\n\n### Error Handling\n\n- Don't expose internal errors to clients\n- Log security-relevant errors server-side\n- Provide helpful but not revealing error messages\n- Clean up resources after errors\n\n### DNS Rebinding Protection\n\nFor streamable HTTP servers running locally:\n- Enable DNS rebinding protection\n- Validate the `Origin` header on all incoming connections\n- Bind to `127.0.0.1` rather than `0.0.0.0`\n\n---\n\n## Tool Annotations\n\nProvide annotations to help clients understand tool behavior:\n\n| Annotation | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `readOnlyHint` | boolean | false | Tool does not modify its environment |\n| `destructiveHint` | boolean | true | Tool may perform destructive updates |\n| `idempotentHint` | boolean | false | Repeated calls with same args have no additional effect |\n| `openWorldHint` | boolean | true | Tool interacts with external entities |\n\n**Important**: Annotations are hints, not security guarantees. Clients should not make security-critical decisions based solely on annotations.\n\n---\n\n## Error Handling\n\n- Use standard JSON-RPC error codes\n- Report tool errors within result objects (not protocol-level errors)\n- Provide helpful, specific error messages with suggested next steps\n- Don't expose internal implementation details\n- Clean up resources properly on errors\n\nExample error handling:\n```typescript\ntry {\n  const result = performOperation();\n  return { content: [{ type: \"text\", text: result }] };\n} catch (error) {\n  return {\n    isError: true,\n    content: [{\n      type: \"text\",\n      text: `Error: ${error.message}. Try using filter='active_only' to reduce results.`\n    }]\n  };\n}\n```\n\n---\n\n## Testing Requirements\n\nComprehensive testing should cover:\n\n- **Functional testing**: Verify correct execution with valid/invalid inputs\n- **Integration testing**: Test interaction with external systems\n- **Security testing**: Validate auth, input sanitization, rate limiting\n- **Performance testing**: Check behavior under load, timeouts\n- **Error handling**: Ensure proper error reporting and cleanup\n\n---\n\n## Documentation Requirements\n\n- Provide clear documentation of all tools and capabilities\n- Include working examples (at least 3 per major feature)\n- Document security considerations\n- Specify required permissions and access levels\n- Document rate limits and performance characteristics\n",
        "skills/protocol-implementation-framework/reference/node_mcp_server.md": "# Node/TypeScript MCP Server Implementation Guide\n\n## Overview\n\nThis document provides Node/TypeScript-specific best practices and examples for implementing MCP servers using the MCP TypeScript SDK. It covers project structure, server setup, tool registration patterns, input validation with Zod, error handling, and complete working examples.\n\n---\n\n## Quick Reference\n\n### Key Imports\n```typescript\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StreamableHTTPServerTransport } from \"@modelcontextprotocol/sdk/server/streamableHttp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport express from \"express\";\nimport { z } from \"zod\";\n```\n\n### Server Initialization\n```typescript\nconst server = new McpServer({\n  name: \"service-mcp-server\",\n  version: \"1.0.0\"\n});\n```\n\n### Tool Registration Pattern\n```typescript\nserver.registerTool(\n  \"tool_name\",\n  {\n    title: \"Tool Display Name\",\n    description: \"What the tool does\",\n    inputSchema: { param: z.string() },\n    outputSchema: { result: z.string() }\n  },\n  async ({ param }) => {\n    const output = { result: `Processed: ${param}` };\n    return {\n      content: [{ type: \"text\", text: JSON.stringify(output) }],\n      structuredContent: output // Modern pattern for structured data\n    };\n  }\n);\n```\n\n---\n\n## MCP TypeScript SDK\n\nThe official MCP TypeScript SDK provides:\n- `McpServer` class for server initialization\n- `registerTool` method for tool registration\n- Zod schema integration for runtime input validation\n- Type-safe tool handler implementations\n\n**IMPORTANT - Use Modern APIs Only:**\n- **DO use**: `server.registerTool()`, `server.registerResource()`, `server.registerPrompt()`\n- **DO NOT use**: Old deprecated APIs such as `server.tool()`, `server.setRequestHandler(ListToolsRequestSchema, ...)`, or manual handler registration\n- The `register*` methods provide better type safety, automatic schema handling, and are the recommended approach\n\nSee the MCP SDK documentation in the references for complete details.\n\n## Server Naming Convention\n\nNode/TypeScript MCP servers must follow this naming pattern:\n- **Format**: `{service}-mcp-server` (lowercase with hyphens)\n- **Examples**: `github-mcp-server`, `jira-mcp-server`, `stripe-mcp-server`\n\nThe name should be:\n- General (not tied to specific features)\n- Descriptive of the service/API being integrated\n- Easy to infer from the task description\n- Without version numbers or dates\n\n## Project Structure\n\nCreate the following structure for Node/TypeScript MCP servers:\n\n```\n{service}-mcp-server/\n package.json\n tsconfig.json\n README.md\n src/\n    index.ts          # Main entry point with McpServer initialization\n    types.ts          # TypeScript type definitions and interfaces\n    tools/            # Tool implementations (one file per domain)\n    services/         # API clients and shared utilities\n    schemas/          # Zod validation schemas\n    constants.ts      # Shared constants (API_URL, CHARACTER_LIMIT, etc.)\n dist/                 # Built JavaScript files (entry point: dist/index.js)\n```\n\n## Tool Implementation\n\n### Tool Naming\n\nUse snake_case for tool names (e.g., \"search_users\", \"create_project\", \"get_channel_info\") with clear, action-oriented names.\n\n**Avoid Naming Conflicts**: Include the service context to prevent overlaps:\n- Use \"slack_send_message\" instead of just \"send_message\"\n- Use \"github_create_issue\" instead of just \"create_issue\"\n- Use \"asana_list_tasks\" instead of just \"list_tasks\"\n\n### Tool Structure\n\nTools are registered using the `registerTool` method with the following requirements:\n- Use Zod schemas for runtime input validation and type safety\n- The `description` field must be explicitly provided - JSDoc comments are NOT automatically extracted\n- Explicitly provide `title`, `description`, `inputSchema`, and `annotations`\n- The `inputSchema` must be a Zod schema object (not a JSON schema)\n- Type all parameters and return values explicitly\n\n```typescript\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { z } from \"zod\";\n\nconst server = new McpServer({\n  name: \"example-mcp\",\n  version: \"1.0.0\"\n});\n\n// Zod schema for input validation\nconst UserSearchInputSchema = z.object({\n  query: z.string()\n    .min(2, \"Query must be at least 2 characters\")\n    .max(200, \"Query must not exceed 200 characters\")\n    .describe(\"Search string to match against names/emails\"),\n  limit: z.number()\n    .int()\n    .min(1)\n    .max(100)\n    .default(20)\n    .describe(\"Maximum results to return\"),\n  offset: z.number()\n    .int()\n    .min(0)\n    .default(0)\n    .describe(\"Number of results to skip for pagination\"),\n  response_format: z.nativeEnum(ResponseFormat)\n    .default(ResponseFormat.MARKDOWN)\n    .describe(\"Output format: 'markdown' for human-readable or 'json' for machine-readable\")\n}).strict();\n\n// Type definition from Zod schema\ntype UserSearchInput = z.infer<typeof UserSearchInputSchema>;\n\nserver.registerTool(\n  \"example_search_users\",\n  {\n    title: \"Search Example Users\",\n    description: `Search for users in the Example system by name, email, or team.\n\nThis tool searches across all user profiles in the Example platform, supporting partial matches and various search filters. It does NOT create or modify users, only searches existing ones.\n\nArgs:\n  - query (string): Search string to match against names/emails\n  - limit (number): Maximum results to return, between 1-100 (default: 20)\n  - offset (number): Number of results to skip for pagination (default: 0)\n  - response_format ('markdown' | 'json'): Output format (default: 'markdown')\n\nReturns:\n  For JSON format: Structured data with schema:\n  {\n    \"total\": number,           // Total number of matches found\n    \"count\": number,           // Number of results in this response\n    \"offset\": number,          // Current pagination offset\n    \"users\": [\n      {\n        \"id\": string,          // User ID (e.g., \"U123456789\")\n        \"name\": string,        // Full name (e.g., \"John Doe\")\n        \"email\": string,       // Email address\n        \"team\": string,        // Team name (optional)\n        \"active\": boolean      // Whether user is active\n      }\n    ],\n    \"has_more\": boolean,       // Whether more results are available\n    \"next_offset\": number      // Offset for next page (if has_more is true)\n  }\n\nExamples:\n  - Use when: \"Find all marketing team members\" -> params with query=\"team:marketing\"\n  - Use when: \"Search for John's account\" -> params with query=\"john\"\n  - Don't use when: You need to create a user (use example_create_user instead)\n\nError Handling:\n  - Returns \"Error: Rate limit exceeded\" if too many requests (429 status)\n  - Returns \"No users found matching '<query>'\" if search returns empty`,\n    inputSchema: UserSearchInputSchema,\n    annotations: {\n      readOnlyHint: true,\n      destructiveHint: false,\n      idempotentHint: true,\n      openWorldHint: true\n    }\n  },\n  async (params: UserSearchInput) => {\n    try {\n      // Input validation is handled by Zod schema\n      // Make API request using validated parameters\n      const data = await makeApiRequest<any>(\n        \"users/search\",\n        \"GET\",\n        undefined,\n        {\n          q: params.query,\n          limit: params.limit,\n          offset: params.offset\n        }\n      );\n\n      const users = data.users || [];\n      const total = data.total || 0;\n\n      if (!users.length) {\n        return {\n          content: [{\n            type: \"text\",\n            text: `No users found matching '${params.query}'`\n          }]\n        };\n      }\n\n      // Prepare structured output\n      const output = {\n        total,\n        count: users.length,\n        offset: params.offset,\n        users: users.map((user: any) => ({\n          id: user.id,\n          name: user.name,\n          email: user.email,\n          ...(user.team ? { team: user.team } : {}),\n          active: user.active ?? true\n        })),\n        has_more: total > params.offset + users.length,\n        ...(total > params.offset + users.length ? {\n          next_offset: params.offset + users.length\n        } : {})\n      };\n\n      // Format text representation based on requested format\n      let textContent: string;\n      if (params.response_format === ResponseFormat.MARKDOWN) {\n        const lines = [`# User Search Results: '${params.query}'`, \"\",\n          `Found ${total} users (showing ${users.length})`, \"\"];\n        for (const user of users) {\n          lines.push(`## ${user.name} (${user.id})`);\n          lines.push(`- **Email**: ${user.email}`);\n          if (user.team) lines.push(`- **Team**: ${user.team}`);\n          lines.push(\"\");\n        }\n        textContent = lines.join(\"\\n\");\n      } else {\n        textContent = JSON.stringify(output, null, 2);\n      }\n\n      return {\n        content: [{ type: \"text\", text: textContent }],\n        structuredContent: output // Modern pattern for structured data\n      };\n    } catch (error) {\n      return {\n        content: [{\n          type: \"text\",\n          text: handleApiError(error)\n        }]\n      };\n    }\n  }\n);\n```\n\n## Zod Schemas for Input Validation\n\nZod provides runtime type validation:\n\n```typescript\nimport { z } from \"zod\";\n\n// Basic schema with validation\nconst CreateUserSchema = z.object({\n  name: z.string()\n    .min(1, \"Name is required\")\n    .max(100, \"Name must not exceed 100 characters\"),\n  email: z.string()\n    .email(\"Invalid email format\"),\n  age: z.number()\n    .int(\"Age must be a whole number\")\n    .min(0, \"Age cannot be negative\")\n    .max(150, \"Age cannot be greater than 150\")\n}).strict();  // Use .strict() to forbid extra fields\n\n// Enums\nenum ResponseFormat {\n  MARKDOWN = \"markdown\",\n  JSON = \"json\"\n}\n\nconst SearchSchema = z.object({\n  response_format: z.nativeEnum(ResponseFormat)\n    .default(ResponseFormat.MARKDOWN)\n    .describe(\"Output format\")\n});\n\n// Optional fields with defaults\nconst PaginationSchema = z.object({\n  limit: z.number()\n    .int()\n    .min(1)\n    .max(100)\n    .default(20)\n    .describe(\"Maximum results to return\"),\n  offset: z.number()\n    .int()\n    .min(0)\n    .default(0)\n    .describe(\"Number of results to skip\")\n});\n```\n\n## Response Format Options\n\nSupport multiple output formats for flexibility:\n\n```typescript\nenum ResponseFormat {\n  MARKDOWN = \"markdown\",\n  JSON = \"json\"\n}\n\nconst inputSchema = z.object({\n  query: z.string(),\n  response_format: z.nativeEnum(ResponseFormat)\n    .default(ResponseFormat.MARKDOWN)\n    .describe(\"Output format: 'markdown' for human-readable or 'json' for machine-readable\")\n});\n```\n\n**Markdown format**:\n- Use headers, lists, and formatting for clarity\n- Convert timestamps to human-readable format\n- Show display names with IDs in parentheses\n- Omit verbose metadata\n- Group related information logically\n\n**JSON format**:\n- Return complete, structured data suitable for programmatic processing\n- Include all available fields and metadata\n- Use consistent field names and types\n\n## Pagination Implementation\n\nFor tools that list resources:\n\n```typescript\nconst ListSchema = z.object({\n  limit: z.number().int().min(1).max(100).default(20),\n  offset: z.number().int().min(0).default(0)\n});\n\nasync function listItems(params: z.infer<typeof ListSchema>) {\n  const data = await apiRequest(params.limit, params.offset);\n\n  const response = {\n    total: data.total,\n    count: data.items.length,\n    offset: params.offset,\n    items: data.items,\n    has_more: data.total > params.offset + data.items.length,\n    next_offset: data.total > params.offset + data.items.length\n      ? params.offset + data.items.length\n      : undefined\n  };\n\n  return JSON.stringify(response, null, 2);\n}\n```\n\n## Character Limits and Truncation\n\nAdd a CHARACTER_LIMIT constant to prevent overwhelming responses:\n\n```typescript\n// At module level in constants.ts\nexport const CHARACTER_LIMIT = 25000;  // Maximum response size in characters\n\nasync function searchTool(params: SearchInput) {\n  let result = generateResponse(data);\n\n  // Check character limit and truncate if needed\n  if (result.length > CHARACTER_LIMIT) {\n    const truncatedData = data.slice(0, Math.max(1, data.length / 2));\n    response.data = truncatedData;\n    response.truncated = true;\n    response.truncation_message =\n      `Response truncated from ${data.length} to ${truncatedData.length} items. ` +\n      `Use 'offset' parameter or add filters to see more results.`;\n    result = JSON.stringify(response, null, 2);\n  }\n\n  return result;\n}\n```\n\n## Error Handling\n\nProvide clear, actionable error messages:\n\n```typescript\nimport axios, { AxiosError } from \"axios\";\n\nfunction handleApiError(error: unknown): string {\n  if (error instanceof AxiosError) {\n    if (error.response) {\n      switch (error.response.status) {\n        case 404:\n          return \"Error: Resource not found. Please check the ID is correct.\";\n        case 403:\n          return \"Error: Permission denied. You don't have access to this resource.\";\n        case 429:\n          return \"Error: Rate limit exceeded. Please wait before making more requests.\";\n        default:\n          return `Error: API request failed with status ${error.response.status}`;\n      }\n    } else if (error.code === \"ECONNABORTED\") {\n      return \"Error: Request timed out. Please try again.\";\n    }\n  }\n  return `Error: Unexpected error occurred: ${error instanceof Error ? error.message : String(error)}`;\n}\n```\n\n## Shared Utilities\n\nExtract common functionality into reusable functions:\n\n```typescript\n// Shared API request function\nasync function makeApiRequest<T>(\n  endpoint: string,\n  method: \"GET\" | \"POST\" | \"PUT\" | \"DELETE\" = \"GET\",\n  data?: any,\n  params?: any\n): Promise<T> {\n  try {\n    const response = await axios({\n      method,\n      url: `${API_BASE_URL}/${endpoint}`,\n      data,\n      params,\n      timeout: 30000,\n      headers: {\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\"\n      }\n    });\n    return response.data;\n  } catch (error) {\n    throw error;\n  }\n}\n```\n\n## Async/Await Best Practices\n\nAlways use async/await for network requests and I/O operations:\n\n```typescript\n// Good: Async network request\nasync function fetchData(resourceId: string): Promise<ResourceData> {\n  const response = await axios.get(`${API_URL}/resource/${resourceId}`);\n  return response.data;\n}\n\n// Bad: Promise chains\nfunction fetchData(resourceId: string): Promise<ResourceData> {\n  return axios.get(`${API_URL}/resource/${resourceId}`)\n    .then(response => response.data);  // Harder to read and maintain\n}\n```\n\n## TypeScript Best Practices\n\n1. **Use Strict TypeScript**: Enable strict mode in tsconfig.json\n2. **Define Interfaces**: Create clear interface definitions for all data structures\n3. **Avoid `any`**: Use proper types or `unknown` instead of `any`\n4. **Zod for Runtime Validation**: Use Zod schemas to validate external data\n5. **Type Guards**: Create type guard functions for complex type checking\n6. **Error Handling**: Always use try-catch with proper error type checking\n7. **Null Safety**: Use optional chaining (`?.`) and nullish coalescing (`??`)\n\n```typescript\n// Good: Type-safe with Zod and interfaces\ninterface UserResponse {\n  id: string;\n  name: string;\n  email: string;\n  team?: string;\n  active: boolean;\n}\n\nconst UserSchema = z.object({\n  id: z.string(),\n  name: z.string(),\n  email: z.string().email(),\n  team: z.string().optional(),\n  active: z.boolean()\n});\n\ntype User = z.infer<typeof UserSchema>;\n\nasync function getUser(id: string): Promise<User> {\n  const data = await apiCall(`/users/${id}`);\n  return UserSchema.parse(data);  // Runtime validation\n}\n\n// Bad: Using any\nasync function getUser(id: string): Promise<any> {\n  return await apiCall(`/users/${id}`);  // No type safety\n}\n```\n\n## Package Configuration\n\n### package.json\n\n```json\n{\n  \"name\": \"{service}-mcp-server\",\n  \"version\": \"1.0.0\",\n  \"description\": \"MCP server for {Service} API integration\",\n  \"type\": \"module\",\n  \"main\": \"dist/index.js\",\n  \"scripts\": {\n    \"start\": \"node dist/index.js\",\n    \"dev\": \"tsx watch src/index.ts\",\n    \"build\": \"tsc\",\n    \"clean\": \"rm -rf dist\"\n  },\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@modelcontextprotocol/sdk\": \"^1.6.1\",\n    \"axios\": \"^1.7.9\",\n    \"zod\": \"^3.23.8\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^22.10.0\",\n    \"tsx\": \"^4.19.2\",\n    \"typescript\": \"^5.7.2\"\n  }\n}\n```\n\n### tsconfig.json\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"Node16\",\n    \"moduleResolution\": \"Node16\",\n    \"lib\": [\"ES2022\"],\n    \"outDir\": \"./dist\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true,\n    \"allowSyntheticDefaultImports\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\n## Complete Example\n\n```typescript\n#!/usr/bin/env node\n/**\n * MCP Server for Example Service.\n *\n * This server provides tools to interact with Example API, including user search,\n * project management, and data export capabilities.\n */\n\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { z } from \"zod\";\nimport axios, { AxiosError } from \"axios\";\n\n// Constants\nconst API_BASE_URL = \"https://api.example.com/v1\";\nconst CHARACTER_LIMIT = 25000;\n\n// Enums\nenum ResponseFormat {\n  MARKDOWN = \"markdown\",\n  JSON = \"json\"\n}\n\n// Zod schemas\nconst UserSearchInputSchema = z.object({\n  query: z.string()\n    .min(2, \"Query must be at least 2 characters\")\n    .max(200, \"Query must not exceed 200 characters\")\n    .describe(\"Search string to match against names/emails\"),\n  limit: z.number()\n    .int()\n    .min(1)\n    .max(100)\n    .default(20)\n    .describe(\"Maximum results to return\"),\n  offset: z.number()\n    .int()\n    .min(0)\n    .default(0)\n    .describe(\"Number of results to skip for pagination\"),\n  response_format: z.nativeEnum(ResponseFormat)\n    .default(ResponseFormat.MARKDOWN)\n    .describe(\"Output format: 'markdown' for human-readable or 'json' for machine-readable\")\n}).strict();\n\ntype UserSearchInput = z.infer<typeof UserSearchInputSchema>;\n\n// Shared utility functions\nasync function makeApiRequest<T>(\n  endpoint: string,\n  method: \"GET\" | \"POST\" | \"PUT\" | \"DELETE\" = \"GET\",\n  data?: any,\n  params?: any\n): Promise<T> {\n  try {\n    const response = await axios({\n      method,\n      url: `${API_BASE_URL}/${endpoint}`,\n      data,\n      params,\n      timeout: 30000,\n      headers: {\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\"\n      }\n    });\n    return response.data;\n  } catch (error) {\n    throw error;\n  }\n}\n\nfunction handleApiError(error: unknown): string {\n  if (error instanceof AxiosError) {\n    if (error.response) {\n      switch (error.response.status) {\n        case 404:\n          return \"Error: Resource not found. Please check the ID is correct.\";\n        case 403:\n          return \"Error: Permission denied. You don't have access to this resource.\";\n        case 429:\n          return \"Error: Rate limit exceeded. Please wait before making more requests.\";\n        default:\n          return `Error: API request failed with status ${error.response.status}`;\n      }\n    } else if (error.code === \"ECONNABORTED\") {\n      return \"Error: Request timed out. Please try again.\";\n    }\n  }\n  return `Error: Unexpected error occurred: ${error instanceof Error ? error.message : String(error)}`;\n}\n\n// Create MCP server instance\nconst server = new McpServer({\n  name: \"example-mcp\",\n  version: \"1.0.0\"\n});\n\n// Register tools\nserver.registerTool(\n  \"example_search_users\",\n  {\n    title: \"Search Example Users\",\n    description: `[Full description as shown above]`,\n    inputSchema: UserSearchInputSchema,\n    annotations: {\n      readOnlyHint: true,\n      destructiveHint: false,\n      idempotentHint: true,\n      openWorldHint: true\n    }\n  },\n  async (params: UserSearchInput) => {\n    // Implementation as shown above\n  }\n);\n\n// Main function\n// For stdio (local):\nasync function runStdio() {\n  if (!process.env.EXAMPLE_API_KEY) {\n    console.error(\"ERROR: EXAMPLE_API_KEY environment variable is required\");\n    process.exit(1);\n  }\n\n  const transport = new StdioServerTransport();\n  await server.connect(transport);\n  console.error(\"MCP server running via stdio\");\n}\n\n// For streamable HTTP (remote):\nasync function runHTTP() {\n  if (!process.env.EXAMPLE_API_KEY) {\n    console.error(\"ERROR: EXAMPLE_API_KEY environment variable is required\");\n    process.exit(1);\n  }\n\n  const app = express();\n  app.use(express.json());\n\n  app.post('/mcp', async (req, res) => {\n    const transport = new StreamableHTTPServerTransport({\n      sessionIdGenerator: undefined,\n      enableJsonResponse: true\n    });\n    res.on('close', () => transport.close());\n    await server.connect(transport);\n    await transport.handleRequest(req, res, req.body);\n  });\n\n  const port = parseInt(process.env.PORT || '3000');\n  app.listen(port, () => {\n    console.error(`MCP server running on http://localhost:${port}/mcp`);\n  });\n}\n\n// Choose transport based on environment\nconst transport = process.env.TRANSPORT || 'stdio';\nif (transport === 'http') {\n  runHTTP().catch(error => {\n    console.error(\"Server error:\", error);\n    process.exit(1);\n  });\n} else {\n  runStdio().catch(error => {\n    console.error(\"Server error:\", error);\n    process.exit(1);\n  });\n}\n```\n\n---\n\n## Advanced MCP Features\n\n### Resource Registration\n\nExpose data as resources for efficient, URI-based access:\n\n```typescript\nimport { ResourceTemplate } from \"@modelcontextprotocol/sdk/types.js\";\n\n// Register a resource with URI template\nserver.registerResource(\n  {\n    uri: \"file://documents/{name}\",\n    name: \"Document Resource\",\n    description: \"Access documents by name\",\n    mimeType: \"text/plain\"\n  },\n  async (uri: string) => {\n    // Extract parameter from URI\n    const match = uri.match(/^file:\\/\\/documents\\/(.+)$/);\n    if (!match) {\n      throw new Error(\"Invalid URI format\");\n    }\n\n    const documentName = match[1];\n    const content = await loadDocument(documentName);\n\n    return {\n      contents: [{\n        uri,\n        mimeType: \"text/plain\",\n        text: content\n      }]\n    };\n  }\n);\n\n// List available resources dynamically\nserver.registerResourceList(async () => {\n  const documents = await getAvailableDocuments();\n  return {\n    resources: documents.map(doc => ({\n      uri: `file://documents/${doc.name}`,\n      name: doc.name,\n      mimeType: \"text/plain\",\n      description: doc.description\n    }))\n  };\n});\n```\n\n**When to use Resources vs Tools:**\n- **Resources**: For data access with simple URI-based parameters\n- **Tools**: For complex operations requiring validation and business logic\n- **Resources**: When data is relatively static or template-based\n- **Tools**: When operations have side effects or complex workflows\n\n### Transport Options\n\nThe TypeScript SDK supports two main transport mechanisms:\n\n#### Streamable HTTP (Recommended for Remote Servers)\n\n```typescript\nimport { StreamableHTTPServerTransport } from \"@modelcontextprotocol/sdk/server/streamableHttp.js\";\nimport express from \"express\";\n\nconst app = express();\napp.use(express.json());\n\napp.post('/mcp', async (req, res) => {\n  // Create new transport for each request (stateless, prevents request ID collisions)\n  const transport = new StreamableHTTPServerTransport({\n    sessionIdGenerator: undefined,\n    enableJsonResponse: true\n  });\n\n  res.on('close', () => transport.close());\n\n  await server.connect(transport);\n  await transport.handleRequest(req, res, req.body);\n});\n\napp.listen(3000);\n```\n\n#### stdio (For Local Integrations)\n\n```typescript\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\n\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n```\n\n**Transport selection:**\n- **Streamable HTTP**: Web services, remote access, multiple clients\n- **stdio**: Command-line tools, local development, subprocess integration\n\n### Notification Support\n\nNotify clients when server state changes:\n\n```typescript\n// Notify when tools list changes\nserver.notification({\n  method: \"notifications/tools/list_changed\"\n});\n\n// Notify when resources change\nserver.notification({\n  method: \"notifications/resources/list_changed\"\n});\n```\n\nUse notifications sparingly - only when server capabilities genuinely change.\n\n---\n\n## Code Best Practices\n\n### Code Composability and Reusability\n\nYour implementation MUST prioritize composability and code reuse:\n\n1. **Extract Common Functionality**:\n   - Create reusable helper functions for operations used across multiple tools\n   - Build shared API clients for HTTP requests instead of duplicating code\n   - Centralize error handling logic in utility functions\n   - Extract business logic into dedicated functions that can be composed\n   - Extract shared markdown or JSON field selection & formatting functionality\n\n2. **Avoid Duplication**:\n   - NEVER copy-paste similar code between tools\n   - If you find yourself writing similar logic twice, extract it into a function\n   - Common operations like pagination, filtering, field selection, and formatting should be shared\n   - Authentication/authorization logic should be centralized\n\n## Building and Running\n\nAlways build your TypeScript code before running:\n\n```bash\n# Build the project\nnpm run build\n\n# Run the server\nnpm start\n\n# Development with auto-reload\nnpm run dev\n```\n\nAlways ensure `npm run build` completes successfully before considering the implementation complete.\n\n## Quality Checklist\n\nBefore finalizing your Node/TypeScript MCP server implementation, ensure:\n\n### Strategic Design\n- [ ] Tools enable complete workflows, not just API endpoint wrappers\n- [ ] Tool names reflect natural task subdivisions\n- [ ] Response formats optimize for agent context efficiency\n- [ ] Human-readable identifiers used where appropriate\n- [ ] Error messages guide agents toward correct usage\n\n### Implementation Quality\n- [ ] FOCUSED IMPLEMENTATION: Most important and valuable tools implemented\n- [ ] All tools registered using `registerTool` with complete configuration\n- [ ] All tools include `title`, `description`, `inputSchema`, and `annotations`\n- [ ] Annotations correctly set (readOnlyHint, destructiveHint, idempotentHint, openWorldHint)\n- [ ] All tools use Zod schemas for runtime input validation with `.strict()` enforcement\n- [ ] All Zod schemas have proper constraints and descriptive error messages\n- [ ] All tools have comprehensive descriptions with explicit input/output types\n- [ ] Descriptions include return value examples and complete schema documentation\n- [ ] Error messages are clear, actionable, and educational\n\n### TypeScript Quality\n- [ ] TypeScript interfaces are defined for all data structures\n- [ ] Strict TypeScript is enabled in tsconfig.json\n- [ ] No use of `any` type - use `unknown` or proper types instead\n- [ ] All async functions have explicit Promise<T> return types\n- [ ] Error handling uses proper type guards (e.g., `axios.isAxiosError`, `z.ZodError`)\n\n### Advanced Features (where applicable)\n- [ ] Resources registered for appropriate data endpoints\n- [ ] Appropriate transport configured (stdio or streamable HTTP)\n- [ ] Notifications implemented for dynamic server capabilities\n- [ ] Type-safe with SDK interfaces\n\n### Project Configuration\n- [ ] Package.json includes all necessary dependencies\n- [ ] Build script produces working JavaScript in dist/ directory\n- [ ] Main entry point is properly configured as dist/index.js\n- [ ] Server name follows format: `{service}-mcp-server`\n- [ ] tsconfig.json properly configured with strict mode\n\n### Code Quality\n- [ ] Pagination is properly implemented where applicable\n- [ ] Large responses check CHARACTER_LIMIT constant and truncate with clear messages\n- [ ] Filtering options are provided for potentially large result sets\n- [ ] All network operations handle timeouts and connection errors gracefully\n- [ ] Common functionality is extracted into reusable functions\n- [ ] Return types are consistent across similar operations\n\n### Testing and Build\n- [ ] `npm run build` completes successfully without errors\n- [ ] dist/index.js created and executable\n- [ ] Server runs: `node dist/index.js --help`\n- [ ] All imports resolve correctly\n- [ ] Sample tool calls work as expected",
        "skills/protocol-implementation-framework/reference/python_mcp_server.md": "# Python MCP Server Implementation Guide\n\n## Overview\n\nThis document provides Python-specific best practices and examples for implementing MCP servers using the MCP Python SDK. It covers server setup, tool registration patterns, input validation with Pydantic, error handling, and complete working examples.\n\n---\n\n## Quick Reference\n\n### Key Imports\n```python\nfrom mcp.server.fastmcp import FastMCP\nfrom pydantic import BaseModel, Field, field_validator, ConfigDict\nfrom typing import Optional, List, Dict, Any\nfrom enum import Enum\nimport httpx\n```\n\n### Server Initialization\n```python\nmcp = FastMCP(\"service_mcp\")\n```\n\n### Tool Registration Pattern\n```python\n@mcp.tool(name=\"tool_name\", annotations={...})\nasync def tool_function(params: InputModel) -> str:\n    # Implementation\n    pass\n```\n\n---\n\n## MCP Python SDK and FastMCP\n\nThe official MCP Python SDK provides FastMCP, a high-level framework for building MCP servers. It provides:\n- Automatic description and inputSchema generation from function signatures and docstrings\n- Pydantic model integration for input validation\n- Decorator-based tool registration with `@mcp.tool`\n\n**For complete SDK documentation, use WebFetch to load:**\n`https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n\n## Server Naming Convention\n\nPython MCP servers must follow this naming pattern:\n- **Format**: `{service}_mcp` (lowercase with underscores)\n- **Examples**: `github_mcp`, `jira_mcp`, `stripe_mcp`\n\nThe name should be:\n- General (not tied to specific features)\n- Descriptive of the service/API being integrated\n- Easy to infer from the task description\n- Without version numbers or dates\n\n## Tool Implementation\n\n### Tool Naming\n\nUse snake_case for tool names (e.g., \"search_users\", \"create_project\", \"get_channel_info\") with clear, action-oriented names.\n\n**Avoid Naming Conflicts**: Include the service context to prevent overlaps:\n- Use \"slack_send_message\" instead of just \"send_message\"\n- Use \"github_create_issue\" instead of just \"create_issue\"\n- Use \"asana_list_tasks\" instead of just \"list_tasks\"\n\n### Tool Structure with FastMCP\n\nTools are defined using the `@mcp.tool` decorator with Pydantic models for input validation:\n\n```python\nfrom pydantic import BaseModel, Field, ConfigDict\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize the MCP server\nmcp = FastMCP(\"example_mcp\")\n\n# Define Pydantic model for input validation\nclass ServiceToolInput(BaseModel):\n    '''Input model for service tool operation.'''\n    model_config = ConfigDict(\n        str_strip_whitespace=True,  # Auto-strip whitespace from strings\n        validate_assignment=True,    # Validate on assignment\n        extra='forbid'              # Forbid extra fields\n    )\n\n    param1: str = Field(..., description=\"First parameter description (e.g., 'user123', 'project-abc')\", min_length=1, max_length=100)\n    param2: Optional[int] = Field(default=None, description=\"Optional integer parameter with constraints\", ge=0, le=1000)\n    tags: Optional[List[str]] = Field(default_factory=list, description=\"List of tags to apply\", max_items=10)\n\n@mcp.tool(\n    name=\"service_tool_name\",\n    annotations={\n        \"title\": \"Human-Readable Tool Title\",\n        \"readOnlyHint\": True,     # Tool does not modify environment\n        \"destructiveHint\": False,  # Tool does not perform destructive operations\n        \"idempotentHint\": True,    # Repeated calls have no additional effect\n        \"openWorldHint\": False     # Tool does not interact with external entities\n    }\n)\nasync def service_tool_name(params: ServiceToolInput) -> str:\n    '''Tool description automatically becomes the 'description' field.\n\n    This tool performs a specific operation on the service. It validates all inputs\n    using the ServiceToolInput Pydantic model before processing.\n\n    Args:\n        params (ServiceToolInput): Validated input parameters containing:\n            - param1 (str): First parameter description\n            - param2 (Optional[int]): Optional parameter with default\n            - tags (Optional[List[str]]): List of tags\n\n    Returns:\n        str: JSON-formatted response containing operation results\n    '''\n    # Implementation here\n    pass\n```\n\n## Pydantic v2 Key Features\n\n- Use `model_config` instead of nested `Config` class\n- Use `field_validator` instead of deprecated `validator`\n- Use `model_dump()` instead of deprecated `dict()`\n- Validators require `@classmethod` decorator\n- Type hints are required for validator methods\n\n```python\nfrom pydantic import BaseModel, Field, field_validator, ConfigDict\n\nclass CreateUserInput(BaseModel):\n    model_config = ConfigDict(\n        str_strip_whitespace=True,\n        validate_assignment=True\n    )\n\n    name: str = Field(..., description=\"User's full name\", min_length=1, max_length=100)\n    email: str = Field(..., description=\"User's email address\", pattern=r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n    age: int = Field(..., description=\"User's age\", ge=0, le=150)\n\n    @field_validator('email')\n    @classmethod\n    def validate_email(cls, v: str) -> str:\n        if not v.strip():\n            raise ValueError(\"Email cannot be empty\")\n        return v.lower()\n```\n\n## Response Format Options\n\nSupport multiple output formats for flexibility:\n\n```python\nfrom enum import Enum\n\nclass ResponseFormat(str, Enum):\n    '''Output format for tool responses.'''\n    MARKDOWN = \"markdown\"\n    JSON = \"json\"\n\nclass UserSearchInput(BaseModel):\n    query: str = Field(..., description=\"Search query\")\n    response_format: ResponseFormat = Field(\n        default=ResponseFormat.MARKDOWN,\n        description=\"Output format: 'markdown' for human-readable or 'json' for machine-readable\"\n    )\n```\n\n**Markdown format**:\n- Use headers, lists, and formatting for clarity\n- Convert timestamps to human-readable format (e.g., \"2024-01-15 10:30:00 UTC\" instead of epoch)\n- Show display names with IDs in parentheses (e.g., \"@john.doe (U123456)\")\n- Omit verbose metadata (e.g., show only one profile image URL, not all sizes)\n- Group related information logically\n\n**JSON format**:\n- Return complete, structured data suitable for programmatic processing\n- Include all available fields and metadata\n- Use consistent field names and types\n\n## Pagination Implementation\n\nFor tools that list resources:\n\n```python\nclass ListInput(BaseModel):\n    limit: Optional[int] = Field(default=20, description=\"Maximum results to return\", ge=1, le=100)\n    offset: Optional[int] = Field(default=0, description=\"Number of results to skip for pagination\", ge=0)\n\nasync def list_items(params: ListInput) -> str:\n    # Make API request with pagination\n    data = await api_request(limit=params.limit, offset=params.offset)\n\n    # Return pagination info\n    response = {\n        \"total\": data[\"total\"],\n        \"count\": len(data[\"items\"]),\n        \"offset\": params.offset,\n        \"items\": data[\"items\"],\n        \"has_more\": data[\"total\"] > params.offset + len(data[\"items\"]),\n        \"next_offset\": params.offset + len(data[\"items\"]) if data[\"total\"] > params.offset + len(data[\"items\"]) else None\n    }\n    return json.dumps(response, indent=2)\n```\n\n## Error Handling\n\nProvide clear, actionable error messages:\n\n```python\ndef _handle_api_error(e: Exception) -> str:\n    '''Consistent error formatting across all tools.'''\n    if isinstance(e, httpx.HTTPStatusError):\n        if e.response.status_code == 404:\n            return \"Error: Resource not found. Please check the ID is correct.\"\n        elif e.response.status_code == 403:\n            return \"Error: Permission denied. You don't have access to this resource.\"\n        elif e.response.status_code == 429:\n            return \"Error: Rate limit exceeded. Please wait before making more requests.\"\n        return f\"Error: API request failed with status {e.response.status_code}\"\n    elif isinstance(e, httpx.TimeoutException):\n        return \"Error: Request timed out. Please try again.\"\n    return f\"Error: Unexpected error occurred: {type(e).__name__}\"\n```\n\n## Shared Utilities\n\nExtract common functionality into reusable functions:\n\n```python\n# Shared API request function\nasync def _make_api_request(endpoint: str, method: str = \"GET\", **kwargs) -> dict:\n    '''Reusable function for all API calls.'''\n    async with httpx.AsyncClient() as client:\n        response = await client.request(\n            method,\n            f\"{API_BASE_URL}/{endpoint}\",\n            timeout=30.0,\n            **kwargs\n        )\n        response.raise_for_status()\n        return response.json()\n```\n\n## Async/Await Best Practices\n\nAlways use async/await for network requests and I/O operations:\n\n```python\n# Good: Async network request\nasync def fetch_data(resource_id: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"{API_URL}/resource/{resource_id}\")\n        response.raise_for_status()\n        return response.json()\n\n# Bad: Synchronous request\ndef fetch_data(resource_id: str) -> dict:\n    response = requests.get(f\"{API_URL}/resource/{resource_id}\")  # Blocks\n    return response.json()\n```\n\n## Type Hints\n\nUse type hints throughout:\n\n```python\nfrom typing import Optional, List, Dict, Any\n\nasync def get_user(user_id: str) -> Dict[str, Any]:\n    data = await fetch_user(user_id)\n    return {\"id\": data[\"id\"], \"name\": data[\"name\"]}\n```\n\n## Tool Docstrings\n\nEvery tool must have comprehensive docstrings with explicit type information:\n\n```python\nasync def search_users(params: UserSearchInput) -> str:\n    '''\n    Search for users in the Example system by name, email, or team.\n\n    This tool searches across all user profiles in the Example platform,\n    supporting partial matches and various search filters. It does NOT\n    create or modify users, only searches existing ones.\n\n    Args:\n        params (UserSearchInput): Validated input parameters containing:\n            - query (str): Search string to match against names/emails (e.g., \"john\", \"@example.com\", \"team:marketing\")\n            - limit (Optional[int]): Maximum results to return, between 1-100 (default: 20)\n            - offset (Optional[int]): Number of results to skip for pagination (default: 0)\n\n    Returns:\n        str: JSON-formatted string containing search results with the following schema:\n\n        Success response:\n        {\n            \"total\": int,           # Total number of matches found\n            \"count\": int,           # Number of results in this response\n            \"offset\": int,          # Current pagination offset\n            \"users\": [\n                {\n                    \"id\": str,      # User ID (e.g., \"U123456789\")\n                    \"name\": str,    # Full name (e.g., \"John Doe\")\n                    \"email\": str,   # Email address (e.g., \"john@example.com\")\n                    \"team\": str     # Team name (e.g., \"Marketing\") - optional\n                }\n            ]\n        }\n\n        Error response:\n        \"Error: <error message>\" or \"No users found matching '<query>'\"\n\n    Examples:\n        - Use when: \"Find all marketing team members\" -> params with query=\"team:marketing\"\n        - Use when: \"Search for John's account\" -> params with query=\"john\"\n        - Don't use when: You need to create a user (use example_create_user instead)\n        - Don't use when: You have a user ID and need full details (use example_get_user instead)\n\n    Error Handling:\n        - Input validation errors are handled by Pydantic model\n        - Returns \"Error: Rate limit exceeded\" if too many requests (429 status)\n        - Returns \"Error: Invalid API authentication\" if API key is invalid (401 status)\n        - Returns formatted list of results or \"No users found matching 'query'\"\n    '''\n```\n\n## Complete Example\n\nSee below for a complete Python MCP server example:\n\n```python\n#!/usr/bin/env python3\n'''\nMCP Server for Example Service.\n\nThis server provides tools to interact with Example API, including user search,\nproject management, and data export capabilities.\n'''\n\nfrom typing import Optional, List, Dict, Any\nfrom enum import Enum\nimport httpx\nfrom pydantic import BaseModel, Field, field_validator, ConfigDict\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize the MCP server\nmcp = FastMCP(\"example_mcp\")\n\n# Constants\nAPI_BASE_URL = \"https://api.example.com/v1\"\n\n# Enums\nclass ResponseFormat(str, Enum):\n    '''Output format for tool responses.'''\n    MARKDOWN = \"markdown\"\n    JSON = \"json\"\n\n# Pydantic Models for Input Validation\nclass UserSearchInput(BaseModel):\n    '''Input model for user search operations.'''\n    model_config = ConfigDict(\n        str_strip_whitespace=True,\n        validate_assignment=True\n    )\n\n    query: str = Field(..., description=\"Search string to match against names/emails\", min_length=2, max_length=200)\n    limit: Optional[int] = Field(default=20, description=\"Maximum results to return\", ge=1, le=100)\n    offset: Optional[int] = Field(default=0, description=\"Number of results to skip for pagination\", ge=0)\n    response_format: ResponseFormat = Field(default=ResponseFormat.MARKDOWN, description=\"Output format\")\n\n    @field_validator('query')\n    @classmethod\n    def validate_query(cls, v: str) -> str:\n        if not v.strip():\n            raise ValueError(\"Query cannot be empty or whitespace only\")\n        return v.strip()\n\n# Shared utility functions\nasync def _make_api_request(endpoint: str, method: str = \"GET\", **kwargs) -> dict:\n    '''Reusable function for all API calls.'''\n    async with httpx.AsyncClient() as client:\n        response = await client.request(\n            method,\n            f\"{API_BASE_URL}/{endpoint}\",\n            timeout=30.0,\n            **kwargs\n        )\n        response.raise_for_status()\n        return response.json()\n\ndef _handle_api_error(e: Exception) -> str:\n    '''Consistent error formatting across all tools.'''\n    if isinstance(e, httpx.HTTPStatusError):\n        if e.response.status_code == 404:\n            return \"Error: Resource not found. Please check the ID is correct.\"\n        elif e.response.status_code == 403:\n            return \"Error: Permission denied. You don't have access to this resource.\"\n        elif e.response.status_code == 429:\n            return \"Error: Rate limit exceeded. Please wait before making more requests.\"\n        return f\"Error: API request failed with status {e.response.status_code}\"\n    elif isinstance(e, httpx.TimeoutException):\n        return \"Error: Request timed out. Please try again.\"\n    return f\"Error: Unexpected error occurred: {type(e).__name__}\"\n\n# Tool definitions\n@mcp.tool(\n    name=\"example_search_users\",\n    annotations={\n        \"title\": \"Search Example Users\",\n        \"readOnlyHint\": True,\n        \"destructiveHint\": False,\n        \"idempotentHint\": True,\n        \"openWorldHint\": True\n    }\n)\nasync def example_search_users(params: UserSearchInput) -> str:\n    '''Search for users in the Example system by name, email, or team.\n\n    [Full docstring as shown above]\n    '''\n    try:\n        # Make API request using validated parameters\n        data = await _make_api_request(\n            \"users/search\",\n            params={\n                \"q\": params.query,\n                \"limit\": params.limit,\n                \"offset\": params.offset\n            }\n        )\n\n        users = data.get(\"users\", [])\n        total = data.get(\"total\", 0)\n\n        if not users:\n            return f\"No users found matching '{params.query}'\"\n\n        # Format response based on requested format\n        if params.response_format == ResponseFormat.MARKDOWN:\n            lines = [f\"# User Search Results: '{params.query}'\", \"\"]\n            lines.append(f\"Found {total} users (showing {len(users)})\")\n            lines.append(\"\")\n\n            for user in users:\n                lines.append(f\"## {user['name']} ({user['id']})\")\n                lines.append(f\"- **Email**: {user['email']}\")\n                if user.get('team'):\n                    lines.append(f\"- **Team**: {user['team']}\")\n                lines.append(\"\")\n\n            return \"\\n\".join(lines)\n\n        else:\n            # Machine-readable JSON format\n            import json\n            response = {\n                \"total\": total,\n                \"count\": len(users),\n                \"offset\": params.offset,\n                \"users\": users\n            }\n            return json.dumps(response, indent=2)\n\n    except Exception as e:\n        return _handle_api_error(e)\n\nif __name__ == \"__main__\":\n    mcp.run()\n```\n\n---\n\n## Advanced FastMCP Features\n\n### Context Parameter Injection\n\nFastMCP can automatically inject a `Context` parameter into tools for advanced capabilities like logging, progress reporting, resource reading, and user interaction:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"example_mcp\")\n\n@mcp.tool()\nasync def advanced_search(query: str, ctx: Context) -> str:\n    '''Advanced tool with context access for logging and progress.'''\n\n    # Report progress for long operations\n    await ctx.report_progress(0.25, \"Starting search...\")\n\n    # Log information for debugging\n    await ctx.log_info(\"Processing query\", {\"query\": query, \"timestamp\": datetime.now()})\n\n    # Perform search\n    results = await search_api(query)\n    await ctx.report_progress(0.75, \"Formatting results...\")\n\n    # Access server configuration\n    server_name = ctx.fastmcp.name\n\n    return format_results(results)\n\n@mcp.tool()\nasync def interactive_tool(resource_id: str, ctx: Context) -> str:\n    '''Tool that can request additional input from users.'''\n\n    # Request sensitive information when needed\n    api_key = await ctx.elicit(\n        prompt=\"Please provide your API key:\",\n        input_type=\"password\"\n    )\n\n    # Use the provided key\n    return await api_call(resource_id, api_key)\n```\n\n**Context capabilities:**\n- `ctx.report_progress(progress, message)` - Report progress for long operations\n- `ctx.log_info(message, data)` / `ctx.log_error()` / `ctx.log_debug()` - Logging\n- `ctx.elicit(prompt, input_type)` - Request input from users\n- `ctx.fastmcp.name` - Access server configuration\n- `ctx.read_resource(uri)` - Read MCP resources\n\n### Resource Registration\n\nExpose data as resources for efficient, template-based access:\n\n```python\n@mcp.resource(\"file://documents/{name}\")\nasync def get_document(name: str) -> str:\n    '''Expose documents as MCP resources.\n\n    Resources are useful for static or semi-static data that doesn't\n    require complex parameters. They use URI templates for flexible access.\n    '''\n    document_path = f\"./docs/{name}\"\n    with open(document_path, \"r\") as f:\n        return f.read()\n\n@mcp.resource(\"config://settings/{key}\")\nasync def get_setting(key: str, ctx: Context) -> str:\n    '''Expose configuration as resources with context.'''\n    settings = await load_settings()\n    return json.dumps(settings.get(key, {}))\n```\n\n**When to use Resources vs Tools:**\n- **Resources**: For data access with simple parameters (URI templates)\n- **Tools**: For complex operations with validation and business logic\n\n### Structured Output Types\n\nFastMCP supports multiple return types beyond strings:\n\n```python\nfrom typing import TypedDict\nfrom dataclasses import dataclass\nfrom pydantic import BaseModel\n\n# TypedDict for structured returns\nclass UserData(TypedDict):\n    id: str\n    name: str\n    email: str\n\n@mcp.tool()\nasync def get_user_typed(user_id: str) -> UserData:\n    '''Returns structured data - FastMCP handles serialization.'''\n    return {\"id\": user_id, \"name\": \"John Doe\", \"email\": \"john@example.com\"}\n\n# Pydantic models for complex validation\nclass DetailedUser(BaseModel):\n    id: str\n    name: str\n    email: str\n    created_at: datetime\n    metadata: Dict[str, Any]\n\n@mcp.tool()\nasync def get_user_detailed(user_id: str) -> DetailedUser:\n    '''Returns Pydantic model - automatically generates schema.'''\n    user = await fetch_user(user_id)\n    return DetailedUser(**user)\n```\n\n### Lifespan Management\n\nInitialize resources that persist across requests:\n\n```python\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def app_lifespan():\n    '''Manage resources that live for the server's lifetime.'''\n    # Initialize connections, load config, etc.\n    db = await connect_to_database()\n    config = load_configuration()\n\n    # Make available to all tools\n    yield {\"db\": db, \"config\": config}\n\n    # Cleanup on shutdown\n    await db.close()\n\nmcp = FastMCP(\"example_mcp\", lifespan=app_lifespan)\n\n@mcp.tool()\nasync def query_data(query: str, ctx: Context) -> str:\n    '''Access lifespan resources through context.'''\n    db = ctx.request_context.lifespan_state[\"db\"]\n    results = await db.query(query)\n    return format_results(results)\n```\n\n### Transport Options\n\nFastMCP supports two main transport mechanisms:\n\n```python\n# stdio transport (for local tools) - default\nif __name__ == \"__main__\":\n    mcp.run()\n\n# Streamable HTTP transport (for remote servers)\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable_http\", port=8000)\n```\n\n**Transport selection:**\n- **stdio**: Command-line tools, local integrations, subprocess execution\n- **Streamable HTTP**: Web services, remote access, multiple clients\n\n---\n\n## Code Best Practices\n\n### Code Composability and Reusability\n\nYour implementation MUST prioritize composability and code reuse:\n\n1. **Extract Common Functionality**:\n   - Create reusable helper functions for operations used across multiple tools\n   - Build shared API clients for HTTP requests instead of duplicating code\n   - Centralize error handling logic in utility functions\n   - Extract business logic into dedicated functions that can be composed\n   - Extract shared markdown or JSON field selection & formatting functionality\n\n2. **Avoid Duplication**:\n   - NEVER copy-paste similar code between tools\n   - If you find yourself writing similar logic twice, extract it into a function\n   - Common operations like pagination, filtering, field selection, and formatting should be shared\n   - Authentication/authorization logic should be centralized\n\n### Python-Specific Best Practices\n\n1. **Use Type Hints**: Always include type annotations for function parameters and return values\n2. **Pydantic Models**: Define clear Pydantic models for all input validation\n3. **Avoid Manual Validation**: Let Pydantic handle input validation with constraints\n4. **Proper Imports**: Group imports (standard library, third-party, local)\n5. **Error Handling**: Use specific exception types (httpx.HTTPStatusError, not generic Exception)\n6. **Async Context Managers**: Use `async with` for resources that need cleanup\n7. **Constants**: Define module-level constants in UPPER_CASE\n\n## Quality Checklist\n\nBefore finalizing your Python MCP server implementation, ensure:\n\n### Strategic Design\n- [ ] Tools enable complete workflows, not just API endpoint wrappers\n- [ ] Tool names reflect natural task subdivisions\n- [ ] Response formats optimize for agent context efficiency\n- [ ] Human-readable identifiers used where appropriate\n- [ ] Error messages guide agents toward correct usage\n\n### Implementation Quality\n- [ ] FOCUSED IMPLEMENTATION: Most important and valuable tools implemented\n- [ ] All tools have descriptive names and documentation\n- [ ] Return types are consistent across similar operations\n- [ ] Error handling is implemented for all external calls\n- [ ] Server name follows format: `{service}_mcp`\n- [ ] All network operations use async/await\n- [ ] Common functionality is extracted into reusable functions\n- [ ] Error messages are clear, actionable, and educational\n- [ ] Outputs are properly validated and formatted\n\n### Tool Configuration\n- [ ] All tools implement 'name' and 'annotations' in the decorator\n- [ ] Annotations correctly set (readOnlyHint, destructiveHint, idempotentHint, openWorldHint)\n- [ ] All tools use Pydantic BaseModel for input validation with Field() definitions\n- [ ] All Pydantic Fields have explicit types and descriptions with constraints\n- [ ] All tools have comprehensive docstrings with explicit input/output types\n- [ ] Docstrings include complete schema structure for dict/JSON returns\n- [ ] Pydantic models handle input validation (no manual validation needed)\n\n### Advanced Features (where applicable)\n- [ ] Context injection used for logging, progress, or elicitation\n- [ ] Resources registered for appropriate data endpoints\n- [ ] Lifespan management implemented for persistent connections\n- [ ] Structured output types used (TypedDict, Pydantic models)\n- [ ] Appropriate transport configured (stdio or streamable HTTP)\n\n### Code Quality\n- [ ] File includes proper imports including Pydantic imports\n- [ ] Pagination is properly implemented where applicable\n- [ ] Filtering options are provided for potentially large result sets\n- [ ] All async functions are properly defined with `async def`\n- [ ] HTTP client usage follows async patterns with proper context managers\n- [ ] Type hints are used throughout the code\n- [ ] Constants are defined at module level in UPPER_CASE\n\n### Testing\n- [ ] Server runs successfully: `python your_server.py --help`\n- [ ] All imports resolve correctly\n- [ ] Sample tool calls work as expected\n- [ ] Error scenarios handled gracefully",
        "skills/publication-converter/REFERENCE.md": "# Markdown to EPUB Skill - Technical Reference\n\nAdvanced technical documentation for extending and customizing the Markdown to EPUB skill.\n\n## Module Overview\n\n### `markdown_processor.py`\n\nCore module for parsing markdown and extracting document structure.\n\n#### Main Classes\n\n**MarkdownProcessor**\n```python\nprocessor = MarkdownProcessor()\nresult = processor.process(markdown_content)\n```\n\nMethods:\n- `process(markdown_content: str) -> Dict` - Parse markdown and extract structure\n- `_extract_frontmatter(content: str) -> str` - Extract YAML frontmatter\n- `_extract_metadata(content: str)` - Extract metadata from document headers\n- `_parse_chapters(content: str) -> List[Chapter]` - Parse into chapters\n- `_parse_sections(content: str, min_level: int) -> List[Section]` - Parse sections\n- `_build_toc() -> List[Dict]` - Build table of contents\n- `markdown_to_html(markdown_text: str) -> str` - Convert markdown to HTML\n- `_render_inline(text: str) -> str` - Process inline elements (bold, italic, links)\n\n#### Data Classes\n\n**EbookMetadata**\n```python\nmetadata = EbookMetadata(\n    title=\"My Book\",\n    author=\"John Doe\",\n    language=\"en\",\n    date=\"2025-01-15\",\n    identifier=\"unique-id\"\n)\n```\n\n**Chapter**\n```python\nchapter = Chapter(\n    title=\"Chapter 1\",\n    content=\"Introduction text...\",\n    sections=[Section(...), ...],\n    anchor=\"chapter-1\"\n)\n```\n\n**Section**\n```python\nsection = Section(\n    title=\"Section 1.1\",\n    level=2,\n    content=\"Section content...\",\n    anchor=\"section-1-1\"\n)\n```\n\n### `epub_generator.py`\n\nEPUB file creation and management using ebooklib.\n\n#### Main Classes\n\n**EPUBGenerator**\n```python\ngenerator = EPUBGenerator(metadata)\nsuccess = generator.generate(chapters, output_path)\n```\n\nMethods:\n- `generate(chapters: List[Chapter], output_path: str) -> bool` - Main generation method\n- `_create_book()` - Initialize EPUB book object\n- `_add_chapters()` - Add chapters to book\n- `_render_chapter(chapter: Chapter) -> str` - Render chapter to XHTML\n- `_render_content(content: str) -> str` - Render markdown content to HTML\n- `_add_style()` - Add CSS styling\n- `_add_toc()` - Generate table of contents\n- `_write_epub(output_path: str)` - Write EPUB file to disk\n\n**Default CSS**\n- Embedded in `EPUBGenerator.DEFAULT_CSS`\n- Customizable by subclassing\n- Responsive design for all screen sizes\n\n#### Convenience Functions\n\n```python\n# Create EPUB from markdown string\ncreate_epub_from_markdown(\n    markdown_content: str,\n    output_path: str,\n    title: Optional[str] = None,\n    author: Optional[str] = None\n) -> bool\n```\n\n## HTML/XHTML Generation\n\n### Markdown to HTML Conversion\n\nThe `markdown_to_html()` method converts markdown to semantic HTML:\n\n```python\nmarkdown = \"# Title\\n\\nSome **bold** text\"\nhtml = MarkdownProcessor.markdown_to_html(markdown)\n# Returns: <h1>Title</h1>\\n<p>Some <strong>bold</strong> text</p>\n```\n\n### Supported Elements\n\n- **Headers** (H1-H6): `# to ######`\n- **Emphasis**: `**bold**`, `*italic*`, `__bold__`, `_italic_`\n- **Links**: `[text](url)`\n- **Lists**: `- item`, `* item`, `1. item`\n- **Code**: `` `inline` ``, ` ``` ` ` ` (blocks)\n- **Blockquotes**: `> quote`\n- **Horizontal Rules**: `---`, `***`, `___`\n\n### Special Handling\n\n- HTML escaping: `&`, `<`, `>`, `\"`, `'` are automatically escaped\n- Code blocks: Content is escaped and preserved as-is\n- Paragraphs: Double newlines create new `<p>` tags\n- Links: Properly encoded href attributes\n\n## EPUB Structure\n\n### Generated File Layout\n\n```\nmetadata.opf          # Package metadata\nnav.xhtml             # EPUB3 navigation\ntoc.ncx               # NCBI NCX (compatibility)\nOEBPS/\n chap_001.xhtml    # Chapter files\n chap_002.xhtml\n ...\n style/\n    main.css      # Embedded stylesheet\n [other resources]\n```\n\n### Metadata Fields\n\nFrom `EbookMetadata`:\n- **identifier**: Unique ID (auto-generated UUID if not provided)\n- **title**: Book title (required)\n- **language**: Language code (default: \"en\")\n- **author**: Author name\n- **date**: Publication date (optional)\n\n### Navigation\n\n- **NCX (Navigation Control File)**: For backward compatibility with older readers\n- **NAV (EPUB3 Navigation Document)**: Standard for EPUB3 readers\n- Automatic generation from chapter/section hierarchy\n\n## Customization\n\n### Extending the Classes\n\n**Custom Styling**\n```python\nclass CustomEPUBGenerator(EPUBGenerator):\n    CUSTOM_CSS = \"\"\"/* Your CSS here */\"\"\"\n\n    def _add_style(self):\n        # Custom styling logic\n        super()._add_style()\n```\n\n**Custom Metadata**\n```python\nmetadata = EbookMetadata()\nmetadata.title = \"My Custom Title\"\nmetadata.author = \"Custom Author\"\ngenerator = EPUBGenerator(metadata)\n```\n\n**Custom HTML Rendering**\n```python\nclass CustomProcessor(MarkdownProcessor):\n    @staticmethod\n    def markdown_to_html(markdown_text):\n        # Custom conversion logic\n        return html\n```\n\n### Adding New Markdown Features\n\n1. Extend `markdown_to_html()` in `MarkdownProcessor`\n2. Add parsing logic for new markdown syntax\n3. Return proper HTML equivalent\n4. Test with `test_epub_skill.py`\n\nExample - Add strikethrough support:\n```python\n# In markdown_to_html()\ntext = re.sub(r'~~(.+?)~~', r'<del>\\1</del>', text)\n```\n\n## Performance Considerations\n\n### Large Documents\n- Processing is O(n) where n = document length\n- Memory usage: ~3-5x the markdown source size\n- EPUB generation typically < 100ms for 100+ page documents\n\n### Optimization Tips\n1. **Batch processing**: Process multiple documents in one run\n2. **Chapter splitting**: Break very large documents into smaller files\n3. **Content optimization**: Remove unnecessary whitespace/formatting\n4. **Lazy loading**: Generate EPUBs on-demand rather than precomputing\n\n## Debugging\n\n### Enable Debug Output\n\n```python\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\nprocessor = MarkdownProcessor()\nresult = processor.process(markdown_content)\n```\n\n### Common Issues\n\n**Empty chapters**\n- Check that markdown has proper structure\n- Verify no sections have completely empty content\n- Use `_render_content()` to debug HTML output\n\n**Invalid XHTML**\n- Verify all tags are properly closed\n- Check for unescaped special characters\n- Use validator tools on generated EPUB\n\n**Missing TOC**\n- Ensure chapters/sections have proper headers\n- Verify anchor generation works correctly\n- Check that sections list is populated\n\n## API Integration\n\n### Using with Claude Skills\n\n```python\n# In your skill implementation\ndef generate_ebook(markdown_content, title=None, author=None):\n    from epub_generator import create_epub_from_markdown\n\n    success = create_epub_from_markdown(\n        markdown_content,\n        \"output.epub\",\n        title=title,\n        author=author\n    )\n\n    if success:\n        # Return file_id or stream\n        return read_epub_file(\"output.epub\")\n    else:\n        return None\n```\n\n### File Handling\n\nThe skill can work with:\n- Direct file paths\n- File contents via Files API\n- Markdown strings\n- Chat message content\n\n## Testing\n\n### Unit Tests\n\n```python\n# Test markdown parsing\nprocessor = MarkdownProcessor()\nresult = processor.process(test_markdown)\nassert len(result['chapters']) == expected_count\n\n# Test HTML generation\nhtml = MarkdownProcessor.markdown_to_html(test_content)\nassert '<h1>' in html\nassert '<strong>' in html\n```\n\n### Integration Tests\n\n```python\n# Test end-to-end EPUB generation\nsuccess = create_epub_from_markdown(\n    markdown_content,\n    test_output_path,\n    title=\"Test\",\n    author=\"Tester\"\n)\nassert success\nassert Path(test_output_path).exists()\n```\n\n### Test Coverage\n\nCurrent test coverage:\n-  Markdown parsing with multiple header levels\n-  YAML frontmatter extraction\n-  HTML generation and escaping\n-  EPUB file creation\n-  Edge cases (empty content, special characters)\n-  Table of contents generation\n-  Large documents (100+ chapters)\n\n## Version History\n\n**v1.0.0** (Current)\n- Initial release\n- Full markdown to EPUB conversion\n- YAML frontmatter support\n- Automatic TOC generation\n- EPUB3 compliance\n- Complete test suite\n\n## Future Roadmap\n\n**v1.1.0** (Planned)\n- Cover page generation\n- Custom CSS templates\n- Image embedding\n\n**v2.0.0** (Future)\n- Kindle format support (.mobi, .azw3)\n- Advanced table support\n- Footnotes and cross-references\n- Experimental MCP integration for cover images\n\n## Contributing\n\n### Code Guidelines\n- Follow PEP 8 style guide\n- Add docstrings to all functions\n- Include type hints\n- Write tests for new features\n- Update this reference documentation\n\n### Adding Features\n1. Create a feature branch\n2. Implement with tests\n3. Update SKILL.md and REFERENCE.md\n4. Submit with test results\n\n---\n\n**Last Updated**: 2025-01-16\n**Maintained By**: Skills Development Team\n",
        "skills/publication-converter/SKILL.md": "---\nname: publication-converter\ndescription: Convert markdown documents to EPUB publication format. Transforms structured markdown into professionally formatted e-books.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Markdown to EPUB Converter Skill\n\nThis skill transforms markdown documents into professional EPUB ebook files. Perfect for converting research documents, blog posts, articles, or chat conversation summaries into portable, device-agnostic ebook formats.\n\n## Overview\n\nThe skill accepts markdown content in multiple formats and generates a properly formatted EPUB3 file that works across all major ebook readers including:\n- Apple Books\n- Amazon Kindle (via Kindle for Mac/Windows/iOS/Android)\n- Google Play Books\n- Kobo and other EPUB readers\n- Any standard EPUB reader\n\n## Input Formats\n\n### Option 1: Raw Markdown Text\nProvide markdown content directly in your message:\n\n```\nConvert this markdown to EPUB:\n# My Book Title\n## Chapter 1\nThis is chapter one content...\n```\n\n### Option 2: File Path\nProvide a path to a markdown file to be converted.\n\n## How It Works\n\n1. **Markdown Parsing**: Analyzes your markdown and automatically:\n   - Treats H1 headers (`#`) as chapter boundaries\n   - Treats H2 headers (`##`) as section headings within chapters\n   - Preserves formatting (bold, italic, links, lists, code blocks)\n\n2. **Structure Generation**: Creates proper EPUB structure:\n   - Automatic table of contents from chapters\n   - Navigation document (EPUB3 standard)\n   - Metadata (title, language, etc.)\n\n3. **File Creation**: Generates a valid EPUB3 file ready for download and use\n\n## Usage Examples\n\n### Example 1: Convert a Blog Post\n\"Convert this markdown blog post to EPUB:\n# How to Build a Simple Web Server\n## Introduction\n...content...\"\n\n### Example 2: Convert a Research Summary\n\"I have research notes in markdown format. Convert them to an EPUB ebook. The content is:\n# Research Project: Machine Learning Basics\n## Chapter 1: Fundamentals\n...\"\n\n### Example 3: Convert a Chat Summary\n\"Summarize our conversation so far as markdown and convert it to an EPUB for reference\"\n\n## Output\n\nThe skill generates a downloadable EPUB file that includes:\n- Professional formatting\n- Automatic table of contents\n- Proper chapter structure\n- Support for markdown formatting elements:\n  - Headers (all levels)\n  - Bold and italic text\n  - Hyperlinks\n  - Lists (ordered and unordered)\n  - Code blocks and inline code\n  - Blockquotes\n  - Horizontal rules\n\n## Markdown Elements Supported\n\n| Element | Markdown | Support | Notes |\n|---------|----------|---------|-------|\n| Headers | `# H1` through `###### H6` | Full | Auto TOC generation |\n| Bold | `**text**` or `__text__` | Full | |\n| Italic | `*text*` or `_text_` | Full | |\n| Links | `[text](url)` | Full | Clickable in ebooks |\n| Lists | `- item` or `1. item` | Full | Nested lists supported |\n| Code blocks | ` ```language ` | **Enhanced** | Syntax highlighting ready, monospace fonts |\n| Inline code | ` `code` ` | **Enhanced** | Styled background, borders |\n| Tables | Markdown tables | **Enhanced** | Styled headers, alternating rows |\n| Blockquotes | `> quote` | Full | Styled with left border |\n| Horizontal rule | `---` or `***` | Full | |\n\n## Advanced Features\n\n### Enhanced Code Block Support\n\nCode blocks are beautifully formatted with:\n- **Premium monospace fonts**: SF Mono, Monaco, Fira Code, Consolas, and more\n- **Styled backgrounds**: Subtle gray background with blue accent border\n- **Language detection**: Specify language after ` ``` ` for future syntax highlighting\n- **Proper escaping**: HTML characters are safely escaped\n- **Overflow handling**: Horizontal scrolling for long lines\n\nExample:\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```\n\n### Enhanced Table Support\n\nTables are rendered with professional styling:\n- **Styled headers**: Blue background with white text\n- **Alternating rows**: Zebra striping for readability\n- **Cell padding**: Comfortable spacing for easy reading\n- **Inline formatting**: Code, bold, italic, and links work in cells\n- **Responsive**: Tables adapt to different screen sizes\n\nExample:\n| Feature | Status | Notes |\n|---------|--------|-------|\n| Headers |  | Full support |\n| Code |  | Enhanced styling |\n| Tables |  | Professional layout |\n\n### Custom Title and Metadata\nYou can specify EPUB metadata:\n- Book title (defaults to first H1 header)\n- Author name\n- Language\n- Publication date\n\n### Chapter Organization\nChapters are automatically detected from:\n- H1 headers (`#`) as primary chapter breaks\n- Logical content sections between H1s\n- Automatic page breaks between chapters\n\n### Styling\nThe generated EPUB uses clean, readable default styling that:\n- Respects the reader's font preferences\n- Works on all screen sizes\n- Maintains proper spacing and hierarchy\n- Includes appropriate margins and padding\n\n## Technical Details\n\n- **Format**: EPUB3 (compatible with all modern readers)\n- **Encoding**: UTF-8\n- **HTML Version**: XHTML 1.1\n- **CSS Support**: Responsive styling\n\n## Downloading Your EPUB\n\nAfter generation, the file will be available for download. You can then:\n1. Download the EPUB to your computer\n2. Open it with your preferred ebook reader\n3. Transfer to your Kindle, iPad, or other device\n4. Upload directly to Kindle via email or cloud\n\n## Tips for Best Results\n\n1. **Use Proper Markdown Structure**: The skill works best when markdown follows standard conventions (H1 for titles, H2 for sections)\n\n2. **Clear Chapter Breaks**: Use H1 headers to clearly mark chapter divisions\n\n3. **Descriptive Headers**: Headers become the table of contents, so make them clear and descriptive\n\n4. **Content Organization**: Place content logically between headers\n\n5. **Supported Formatting**: Stick to basic markdown formatting for best compatibility across all readers\n\n## Troubleshooting\n\n**EPUB doesn't open**: Ensure your markdown is properly formatted. Check for matching brackets in links and proper syntax.\n\n**Table of contents is empty**: Make sure your markdown includes H1 headers to define chapters.\n\n**Formatting looks different**: EPUB readers apply their own fonts and styling. This is normal and expected behavior.\n\n## Scripts\n\n- `epub_generator.py` - Core EPUB file creation and formatting\n- `markdown_processor.py` - Markdown parsing and structure extraction\n\n## Future Enhancements\n\n- Auto-generated cover pages with custom images\n- Kindle-specific optimizations (.mobi format)\n- Custom CSS styling per user preferences\n- Multi-document merging\n- Image embedding and optimization\n- Advanced metadata support\n",
        "skills/quality-validation/SKILL.md": "---\nname: quality-validation\ndescription: Validate quality before completing tasks. Performs verification checks, testing, and quality assurance before marking work complete.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Verification Before Completion\n\n## Overview\n\nClaiming work is complete without verification is dishonesty, not efficiency.\n\n**Core principle:** Evidence before claims, always.\n\n**Violating the letter of this rule is violating the spirit of this rule.**\n\n## The Iron Law\n\n```\nNO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE\n```\n\nIf you haven't run the verification command in this message, you cannot claim it passes.\n\n## The Gate Function\n\n```\nBEFORE claiming any status or expressing satisfaction:\n\n1. IDENTIFY: What command proves this claim?\n2. RUN: Execute the FULL command (fresh, complete)\n3. READ: Full output, check exit code, count failures\n4. VERIFY: Does output confirm the claim?\n   - If NO: State actual status with evidence\n   - If YES: State claim WITH evidence\n5. ONLY THEN: Make the claim\n\nSkip any step = lying, not verifying\n```\n\n## Common Failures\n\n| Claim | Requires | Not Sufficient |\n|-------|----------|----------------|\n| Tests pass | Test command output: 0 failures | Previous run, \"should pass\" |\n| Linter clean | Linter output: 0 errors | Partial check, extrapolation |\n| Build succeeds | Build command: exit 0 | Linter passing, logs look good |\n| Bug fixed | Test original symptom: passes | Code changed, assumed fixed |\n| Regression test works | Red-green cycle verified | Test passes once |\n| Agent completed | VCS diff shows changes | Agent reports \"success\" |\n| Requirements met | Line-by-line checklist | Tests passing |\n\n## Red Flags - STOP\n\n- Using \"should\", \"probably\", \"seems to\"\n- Expressing satisfaction before verification (\"Great!\", \"Perfect!\", \"Done!\", etc.)\n- About to commit/push/PR without verification\n- Trusting agent success reports\n- Relying on partial verification\n- Thinking \"just this once\"\n- Tired and wanting work over\n- **ANY wording implying success without having run verification**\n\n## Rationalization Prevention\n\n| Excuse | Reality |\n|--------|---------|\n| \"Should work now\" | RUN the verification |\n| \"I'm confident\" | Confidence  evidence |\n| \"Just this once\" | No exceptions |\n| \"Linter passed\" | Linter  compiler |\n| \"Agent said success\" | Verify independently |\n| \"I'm tired\" | Exhaustion  excuse |\n| \"Partial check is enough\" | Partial proves nothing |\n| \"Different words so rule doesn't apply\" | Spirit over letter |\n\n## Key Patterns\n\n**Tests:**\n```\n [Run test command] [See: 34/34 pass] \"All tests pass\"\n \"Should pass now\" / \"Looks correct\"\n```\n\n**Regression tests (TDD Red-Green):**\n```\n Write  Run (pass)  Revert fix  Run (MUST FAIL)  Restore  Run (pass)\n \"I've written a regression test\" (without red-green verification)\n```\n\n**Build:**\n```\n [Run build] [See: exit 0] \"Build passes\"\n \"Linter passed\" (linter doesn't check compilation)\n```\n\n**Requirements:**\n```\n Re-read plan  Create checklist  Verify each  Report gaps or completion\n \"Tests pass, phase complete\"\n```\n\n**Agent delegation:**\n```\n Agent reports success  Check VCS diff  Verify changes  Report actual state\n Trust agent report\n```\n\n## Why This Matters\n\nFrom 24 failure memories:\n- your human partner said \"I don't believe you\" - trust broken\n- Undefined functions shipped - would crash\n- Missing requirements shipped - incomplete features\n- Time wasted on false completion  redirect  rework\n- Violates: \"Honesty is a core value. If you lie, you'll be replaced.\"\n\n## When To Apply\n\n**ALWAYS before:**\n- ANY variation of success/completion claims\n- ANY expression of satisfaction\n- ANY positive statement about work state\n- Committing, PR creation, task completion\n- Moving to next task\n- Delegating to agents\n\n**Rule applies to:**\n- Exact phrases\n- Paraphrases and synonyms\n- Implications of success\n- ANY communication suggesting completion/correctness\n\n## The Bottom Line\n\n**No shortcuts for verification.**\n\nRun the command. Read the output. THEN claim the result.\n\nThis is non-negotiable.\n",
        "skills/rag-agent-builder/README.md": "# RAG Agent Builder - Code Structure\n\nThis skill uses supporting Python files to keep documentation lean and maintainable.\n\n## Directory Structure\n\n```\nrag-agent-builder/\n SKILL.md                      # Main documentation (concepts, patterns)\n README.md                     # This file\n examples/                     # Implementation examples\n    basic_rag.py              # Simple RAG pipeline\n    retrieval_strategies.py   # Hybrid search, reranking, filtering\n    agentic_rag.py            # Agent-controlled retrieval\n scripts/                      # Utility modules\n     embedding_management.py   # Embedding generation and caching\n     vector_db_manager.py      # Vector database abstraction\n     rag_evaluation.py         # Retrieval and answer quality metrics\n```\n\n## Running Examples\n\n### 1. Basic RAG\n```bash\npython examples/basic_rag.py\n```\nSimplest RAG implementation - chunk documents, embed, retrieve, generate.\n\n### 2. Advanced Retrieval Strategies\n```bash\npython examples/retrieval_strategies.py\n```\nHybrid search combining keyword and semantic search with reranking.\n\n### 3. Agentic RAG\n```bash\npython examples/agentic_rag.py\n```\nAgent-controlled retrieval with iterative refinement for complex questions.\n\n## Using the Utilities\n\n### Embedding Management\n```python\nfrom scripts.embedding_management import EmbeddingManager, EmbeddingQualityAssessment\n\nmanager = EmbeddingManager(model_name=\"all-MiniLM-L6-v2\")\nembeddings = manager.generate_embeddings(texts)\nnormalized = manager.normalize_embeddings(embeddings)\n\nquality = EmbeddingQualityAssessment.embedding_distribution_quality(embeddings)\nprint(f\"Embedding quality: {quality['quality']}\")\n```\n\n### Vector Database Management\n```python\nfrom scripts.vector_db_manager import VectorDBFactory, VectorDBManager\n\n# Create in-memory database\ndb = VectorDBFactory.create_db(\"in_memory\")\nmanager = VectorDBManager(db)\n\n# Add documents\ndoc_ids = manager.add_documents(texts, embeddings)\n\n# Search\nresults = manager.search(query_embedding, k=5)\n\n# Database info\ninfo = manager.get_database_info()\n```\n\n### RAG Evaluation\n```python\nfrom scripts.rag_evaluation import RAGEvaluator, RetrievalMetrics\n\nevaluator = RAGEvaluator()\n\n# Evaluate retrieval\nretrieval_eval = evaluator.evaluate_retrieval(retrieved_docs, relevant_docs, query)\n\n# Evaluate answer\nanswer_eval = evaluator.evaluate_answer(answer, context, query)\n\n# Complete pipeline evaluation\npipeline_eval = evaluator.evaluate_rag_pipeline(\n    query, retrieved, relevant, answer, context\n)\n\n# Get summary\nsummary = evaluator.get_evaluation_summary()\n```\n\n## Integration with SKILL.md\n\n- SKILL.md contains conceptual information, patterns, and best practices\n- Code examples are in `examples/` for clarity and reusability\n- Utilities are in `scripts/` for modular components\n- This keeps token costs low while maintaining full functionality\n\n## Architecture Patterns Covered\n\n1. **Basic RAG** - Simple chunk-embed-retrieve-generate pipeline\n2. **Agentic RAG** - Agent makes intelligent retrieval decisions\n3. **Hybrid Search** - Combines keyword and semantic search\n4. **Retrieval Refinement** - Reranking and filtering\n5. **Context Management** - Fit documents within token limits\n6. **Quality Evaluation** - Metrics for retrieval and answer quality\n\n## Models and Technologies Supported\n\n- **Embedding Models**: All HuggingFace sentence-transformers compatible\n- **Vector DBs**: In-memory (reference), Chroma, Pinecone, Weaviate, Qdrant\n- **Frameworks**: LangChain, LlamaIndex, HayStack compatible\n- **LLMs**: Any API-compatible LLM (OpenAI, local, etc.)\n\n## Key Features\n\n- **Token Efficient**: Modular code structure reduces LLM context usage\n- **Production Ready**: Includes evaluation metrics and quality assessment\n- **Framework Agnostic**: Works with any embedding or vector DB\n- **Iterative Improvement**: Agent-based RAG for complex queries\n- **Quality Focused**: Built-in metrics for retrieval and answer quality\n\n## Next Steps\n\n1. Choose embedding model for your domain\n2. Prepare and chunk your documents\n3. Select vector database for storage\n4. Implement retrieval strategy (basic, hybrid, or agentic)\n5. Evaluate with provided metrics\n6. Iterate based on quality scores\n",
        "skills/rag-agent-builder/SKILL.md": "---\nname: rag-agent-builder\ndescription: Build Retrieval-Augmented Generation (RAG) applications that combine LLM capabilities with external knowledge sources. Covers vector databases, embeddings, retrieval strategies, and response generation. Use when building document Q&A systems, knowledge base applications, enterprise search, or combining LLMs with custom data.\n---\n\n# RAG Agent Builder\n\nBuild powerful Retrieval-Augmented Generation (RAG) applications that enhance LLM capabilities with external knowledge sources, enabling accurate, contextualized AI responses.\n\n## Quick Start\n\nGet started with RAG implementations in the examples and utilities:\n\n- **Examples**: See [`examples/`](examples/) directory for complete implementations:\n  - [`basic_rag.py`](examples/basic_rag.py) - Simple chunk-embed-retrieve-generate pipeline\n  - [`retrieval_strategies.py`](examples/retrieval_strategies.py) - Hybrid search, reranking, and filtering\n  - [`agentic_rag.py`](examples/agentic_rag.py) - Agent-controlled retrieval with iterative refinement\n\n- **Utilities**: See [`scripts/`](scripts/) directory for helper modules:\n  - [`embedding_management.py`](scripts/embedding_management.py) - Embedding generation, normalization, and caching\n  - [`vector_db_manager.py`](scripts/vector_db_manager.py) - Vector database abstraction and factory\n  - [`rag_evaluation.py`](scripts/rag_evaluation.py) - Retrieval and answer quality metrics\n\n## Overview\n\nRAG systems combine three key components:\n1. **Document Retrieval** - Find relevant information from knowledge bases\n2. **Context Integration** - Pass retrieved context to the LLM\n3. **Response Generation** - Generate answers grounded in the retrieved information\n\nThis skill covers building production-ready RAG applications with various frameworks and approaches.\n\n## Core Concepts\n\n### What is RAG?\n\nRAG augments LLM knowledge with external data:\n- **Without RAG**: LLM relies on training data (may be outdated or limited)\n- **With RAG**: LLM uses real-time, custom knowledge + training knowledge\n\n### When to Use RAG\n\n- **Document Q&A**: Answer questions about PDFs, books, reports\n- **Knowledge Base Search**: Query internal documentation, wikis\n- **Enterprise Search**: Search proprietary company data\n- **Context-Specific Assistants**: Customer support, HR assistants\n- **Fact-Heavy Applications**: Legal docs, medical records, financial data\n\n### When RAG Might Not Be Needed\n\n- General knowledge questions (ChatGPT-like)\n- Real-time data that changes constantly (use tools instead)\n- Very simple lookup tasks (use database queries)\n\n## Architecture Patterns\n\n### Basic RAG Pipeline\n\n```\nDocuments  Chunks  Embeddings  Vector DB\n                                        \nUser Question  Embedding  Retrieval  LLM  Answer\n                                       \n                         Vector DB    Context\n```\n\n### Advanced RAG Patterns\n\n#### 1. Agentic RAG\n- Agent decides what to retrieve and when\n- Can refine queries iteratively\n- Better for complex reasoning\n\n#### 2. Hierarchical RAG\n- Multi-level document structure\n- Search at different levels of detail\n- More flexible organization\n\n#### 3. Hybrid Search RAG\n- Combines keyword search (BM25) + semantic search (embeddings)\n- Captures both exact matches and meaning\n- Better for mixed query types\n\n#### 4. Corrective RAG (CRAG)\n- Evaluates retrieved documents for relevance\n- Retrieves additional sources if needed\n- Ensures high-quality context\n\n## Implementation Components\n\n### 1. Document Processing\n\n**Chunking Strategies**:\n```python\n# Simple fixed-size chunks\nchunks = split_text(doc, chunk_size=1000, overlap=100)\n\n# Semantic chunks (group by meaning)\nchunks = semantic_chunking(doc, max_tokens=512)\n\n# Hierarchical chunks (different levels)\nchapters = split_by_heading(doc)\nchunks = split_each_chapter(chapters, size=1000)\n```\n\n**Key Considerations**:\n- Chunk size affects retrieval quality and cost\n- Overlap helps maintain context between chunks\n- Semantic chunking preserves meaning better\n\n### 2. Embedding Generation\n\n**Popular Embedding Models**:\n- OpenAI: `text-embedding-3-small`, `text-embedding-3-large`\n- Open Source: `all-MiniLM-L6-v2`, `all-mpnet-base-v2`\n- Domain-Specific: Domain-trained embeddings for specialized knowledge\n\n**Best Practices**:\n- Use consistent embedding model for retrieval and queries\n- Store embeddings with normalized vectors\n- Update embeddings when documents change\n\n### 3. Vector Databases\n\n**Popular Options**:\n- **Pinecone**: Managed, serverless, easy to scale\n- **Weaviate**: Open-source, self-hosted, flexible\n- **Milvus**: Open-source, high performance\n- **Chroma**: Lightweight, good for prototypes\n- **Qdrant**: Production-grade, high-performance\n\n**Selection Criteria**:\n- Scale requirements (data volume, queries per second)\n- Latency needs (real-time vs batch)\n- Cost considerations\n- Deployment preferences (managed vs self-hosted)\n\n### 4. Retrieval Strategies\n\n**Retrieval Methods**:\n```python\n# Similarity search (most common)\nresults = vector_db.query(question_embedding, k=5)\n\n# Hybrid search (keyword + semantic)\nkeyword_results = bm25.search(question, k=3)\nsemantic_results = vector_db.query(embedding, k=3)\nresults = combine_and_rank(keyword_results, semantic_results)\n\n# Reranking (improve relevance)\nretrieved = initial_retrieval(query)\nreranked = rerank_by_relevance(retrieved, query)\n```\n\n**Retrieval Parameters**:\n- **k** (number of results): Balance between context and relevance\n- **Similarity threshold**: Filter out low-relevance results\n- **Diversity**: Return varied results vs best matches\n\n### 5. Context Integration\n\n**Context Window Management**:\n```python\n# Fit retrieved documents into context window\ndef prepare_context(retrieved_docs, max_tokens=3000):\n    context = \"\"\n    for doc in retrieved_docs:\n        if len(tokenize(context + doc)) <= max_tokens:\n            context += doc\n        else:\n            break\n    return context\n```\n\n**Prompt Design**:\n```\nYou are a helpful assistant. Answer the question based on the provided context.\n\nContext:\n{retrieved_documents}\n\nQuestion: {user_question}\n\nAnswer:\n```\n\n### 6. Response Generation\n\n**Generation Strategies**:\n- **Direct Generation**: LLM answers from context\n- **Summarization**: Summarize multiple retrieved docs first\n- **Fact-Grounding**: Ensure answer cites sources\n- **Iterative Refinement**: Refine based on user feedback\n\n## Implementation Patterns\n\n### Pattern 1: Basic RAG\n\nSimplest RAG implementation:\n1. Split documents into chunks\n2. Generate embeddings for each chunk\n3. Store in vector database\n4. Retrieve top-k similar chunks for query\n5. Pass to LLM with context\n\n**Pros**: Simple, fast, works well for straightforward QA\n**Cons**: May miss relevant context, no refinement\n\n### Pattern 2: Agentic RAG\n\nAgent controls retrieval:\n1. Agent receives user question\n2. Decides whether to retrieve documents\n3. Formulates retrieval query (may differ from original)\n4. Retrieves relevant documents\n5. Can iterate or use tools\n6. Generates final answer\n\n**Pros**: Better for complex questions, iterative improvement\n**Cons**: More complex, higher costs\n\n### Pattern 3: Corrective RAG (CRAG)\n\nValidates retrieved documents:\n1. Retrieve documents for question\n2. Grade each document for relevance\n3. If poor relevance:\n   - Try different retrieval strategy\n   - Expand search scope\n   - Retrieve from different sources\n4. Generate answer from validated context\n\n**Pros**: Higher quality answers, adapts to failures\n**Cons**: More API calls, slower\n\n## Popular Frameworks\n\n### LangChain\n```python\nfrom langchain.document_loaders import PDFLoader\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Pinecone\nfrom langchain.chains import RetrievalQA\n\n# Load documents\nloader = PDFLoader(\"document.pdf\")\ndocs = loader.load()\n\n# Create RAG chain\nembeddings = OpenAIEmbeddings()\nvectorstore = Pinecone.from_documents(docs, embeddings)\nqa = RetrievalQA.from_chain_type(\n    llm=ChatOpenAI(),\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever()\n)\n\nanswer = qa.run(\"What is the document about?\")\n```\n\n### LlamaIndex\n```python\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = GPTVectorStoreIndex.from_documents(documents)\n\n# Query\nresponse = index.as_query_engine().query(\"What is the main topic?\")\n```\n\n### CrewAI with RAG\n```python\nfrom crewai import Agent, Task, Crew\nfrom tools import retrieval_tool\n\nresearcher = Agent(\n    role=\"Research Assistant\",\n    goal=\"Research topics using knowledge base\",\n    tools=[retrieval_tool]\n)\n\nresearch_task = Task(\n    description=\"Research the topic: {topic}\",\n    agent=researcher\n)\n```\n\n## Best Practices\n\n### Document Preparation\n-  Clean and normalize text (remove headers, footers)\n-  Preserve document structure when possible\n-  Add metadata (source, date, category)\n-  Handle PDFs with OCR if scanned\n-  Test chunk sizes for your domain\n\n### Embedding Strategy\n-  Use same embedding model for indexing and queries\n-  Fine-tune embeddings for domain-specific needs\n-  Normalize embeddings for consistency\n-  Monitor embedding quality metrics\n\n### Retrieval Optimization\n-  Tune k (number of results) for your use case\n-  Use reranking for quality improvement\n-  Implement relevance filtering\n-  Monitor retrieval precision and recall\n-  Cache frequently retrieved documents\n\n### Generation Quality\n-  Include source citations in answers\n-  Prompt LLM to indicate confidence\n-  Ask to cite specific documents\n-  Generate summaries for long contexts\n-  Validate answers against context\n\n### Monitoring & Evaluation\n-  Track retrieval metrics (precision, recall, MRR)\n-  Monitor answer quality and relevance\n-  Log failed retrievals for improvement\n-  Collect user feedback\n-  Iterate based on failures\n\n## Common Challenges & Solutions\n\n### Challenge: Irrelevant Retrieval\n**Solutions**:\n- Improve chunking strategy\n- Better embedding model\n- Add document metadata to queries\n- Implement reranking\n- Use hybrid search\n\n### Challenge: Context Too Large\n**Solutions**:\n- Reduce chunk size\n- Retrieve fewer results (smaller k)\n- Summarize retrieved context\n- Use hierarchical retrieval\n- Filter by relevance score\n\n### Challenge: Missing Information\n**Solutions**:\n- Increase k (retrieve more)\n- Improve embedding model\n- Better preprocessing\n- Use multiple search strategies\n- Add document hierarchy\n\n### Challenge: Slow Performance\n**Solutions**:\n- Use managed vector database\n- Cache embeddings\n- Batch process documents\n- Optimize chunk size\n- Use smaller embedding model for speed\n\n## Evaluation Metrics\n\n**Retrieval Metrics**:\n- **Precision**: % of retrieved docs that are relevant\n- **Recall**: % of relevant docs that are retrieved\n- **MRR (Mean Reciprocal Rank)**: Rank of first relevant result\n- **NDCG (Normalized DCG)**: Quality of ranking\n\n**Answer Quality Metrics**:\n- **Relevance**: Does answer address the question?\n- **Correctness**: Is the answer factually accurate?\n- **Grounding**: Is answer supported by context?\n- **User Satisfaction**: Would user find answer helpful?\n\n## Advanced Techniques\n\n### 1. Query Expansion\n```python\n# Expand query with related terms\nexpanded_query = query + \" \" + synonym_expansion(query)\nresults = retrieve(expanded_query)\n```\n\n### 2. Document Compression\n```python\n# Compress retrieved docs before passing to LLM\ncompressed = compress_documents(retrieved_docs, query)\ncontext = format_context(compressed)\n```\n\n### 3. Active Retrieval\n```python\n# Iteratively refine retrieval based on LLM output\nquery = user_question\nwhile iterations < max:\n    results = retrieve(query)\n    answer = generate_with_context(results)\n    if answer_complete(answer):\n        break\n    query = refine_query(answer)\n```\n\n### 4. Multi-Modal RAG\n```python\n# Retrieve both text and images\ntext_results = text_retriever.query(question)\nimage_results = image_retriever.query(question)\ncontext = combine_multimodal(text_results, image_results)\n```\n\n## Resources & References\n\n### Key Papers\n- \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al.)\n- \"REALM: Retrieval-Augmented Language Model Pre-Training\" (Guu et al.)\n\n### Frameworks\n- LangChain: https://python.langchain.com/\n- LlamaIndex: https://www.llamaindex.ai/\n- HayStack: https://haystack.deepset.ai/\n\n### Vector Databases\n- Pinecone: https://www.pinecone.io/\n- Weaviate: https://weaviate.io/\n- Qdrant: https://qdrant.tech/\n\n### Embedding Models\n- OpenAI: https://platform.openai.com/docs/guides/embeddings\n- Hugging Face: https://huggingface.co/models?pipeline_tag=sentence-similarity\n\n## Next Steps\n\n1. **Choose your stack**: Decide on framework (LangChain, LlamaIndex, etc.)\n2. **Prepare documents**: Process and chunk your knowledge base\n3. **Select embeddings**: Choose embedding model for your domain\n4. **Pick vector DB**: Select storage solution for scale\n5. **Build pipeline**: Implement retrieval and generation\n6. **Evaluate**: Test on sample questions and iterate\n7. **Monitor**: Track quality metrics in production\n\n",
        "skills/release-notes-composer/SKILL.md": "---\nname: release-notes-composer\ndescription: Generate comprehensive release notes and changelogs. Compiles version history, feature descriptions, and breaking changes from commit information.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Changelog Generator\n\nThis skill transforms technical git commits into polished, user-friendly changelogs that your customers and users will actually understand and appreciate.\n\n## When to Use This Skill\n\n- Preparing release notes for a new version\n- Creating weekly or monthly product update summaries\n- Documenting changes for customers\n- Writing changelog entries for app store submissions\n- Generating update notifications\n- Creating internal release documentation\n- Maintaining a public changelog/product updates page\n\n## What This Skill Does\n\n1. **Scans Git History**: Analyzes commits from a specific time period or between versions\n2. **Categorizes Changes**: Groups commits into logical categories (features, improvements, bug fixes, breaking changes, security)\n3. **Translates Technical  User-Friendly**: Converts developer commits into customer language\n4. **Formats Professionally**: Creates clean, structured changelog entries\n5. **Filters Noise**: Excludes internal commits (refactoring, tests, etc.)\n6. **Follows Best Practices**: Applies changelog guidelines and your brand voice\n\n## How to Use\n\n### Basic Usage\n\nFrom your project repository:\n\n```\nCreate a changelog from commits since last release\n```\n\n```\nGenerate changelog for all commits from the past week\n```\n\n```\nCreate release notes for version 2.5.0\n```\n\n### With Specific Date Range\n\n```\nCreate a changelog for all commits between March 1 and March 15\n```\n\n### With Custom Guidelines\n\n```\nCreate a changelog for commits since v2.4.0, using my changelog \nguidelines from CHANGELOG_STYLE.md\n```\n\n## Example\n\n**User**: \"Create a changelog for commits from the past 7 days\"\n\n**Output**:\n```markdown\n# Updates - Week of March 10, 2024\n\n##  New Features\n\n- **Team Workspaces**: Create separate workspaces for different \n  projects. Invite team members and keep everything organized.\n\n- **Keyboard Shortcuts**: Press ? to see all available shortcuts. \n  Navigate faster without touching your mouse.\n\n##  Improvements\n\n- **Faster Sync**: Files now sync 2x faster across devices\n- **Better Search**: Search now includes file contents, not just titles\n\n##  Fixes\n\n- Fixed issue where large images wouldn't upload\n- Resolved timezone confusion in scheduled posts\n- Corrected notification badge count\n```\n\n**Inspired by:** Manik Aggarwal's use case from Lenny's Newsletter\n\n## Tips\n\n- Run from your git repository root\n- Specify date ranges for focused changelogs\n- Use your CHANGELOG_STYLE.md for consistent formatting\n- Review and adjust the generated changelog before publishing\n- Save output directly to CHANGELOG.md\n\n## Related Use Cases\n\n- Creating GitHub release notes\n- Writing app store update descriptions\n- Generating email updates for users\n- Creating social media announcement posts\n\n",
        "skills/research-documentation/SKILL.md": "---\nname: research-documentation\ndescription: Research topics and produce comprehensive written documentation. Synthesizes information into clear, well-structured, authoritative content pieces.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Content Research Writer\n\nThis skill acts as your writing partner, helping you research, outline, draft, and refine content while maintaining your unique voice and style.\n\n## When to Use This Skill\n\n- Writing blog posts, articles, or newsletters\n- Creating educational content or tutorials\n- Drafting thought leadership pieces\n- Researching and writing case studies\n- Producing technical documentation with sources\n- Writing with proper citations and references\n- Improving hooks and introductions\n- Getting section-by-section feedback while writing\n\n## What This Skill Does\n\n1. **Collaborative Outlining**: Helps you structure ideas into coherent outlines\n2. **Research Assistance**: Finds relevant information and adds citations\n3. **Hook Improvement**: Strengthens your opening to capture attention\n4. **Section Feedback**: Reviews each section as you write\n5. **Voice Preservation**: Maintains your writing style and tone\n6. **Citation Management**: Adds and formats references properly\n7. **Iterative Refinement**: Helps you improve through multiple drafts\n\n## How to Use\n\n### Setup Your Writing Environment\n\nCreate a dedicated folder for your article:\n```\nmkdir ~/writing/my-article-title\ncd ~/writing/my-article-title\n```\n\nCreate your draft file:\n```\ntouch article-draft.md\n```\n\nOpen Claude Code from this directory and start writing.\n\n### Basic Workflow\n\n1. **Start with an outline**:\n```\nHelp me create an outline for an article about [topic]\n```\n\n2. **Research and add citations**:\n```\nResearch [specific topic] and add citations to my outline\n```\n\n3. **Improve the hook**:\n```\nHere's my introduction. Help me make the hook more compelling.\n```\n\n4. **Get section feedback**:\n```\nI just finished the \"Why This Matters\" section. Review it and give feedback.\n```\n\n5. **Refine and polish**:\n```\nReview the full draft for flow, clarity, and consistency.\n```\n\n## Instructions\n\nWhen a user requests writing assistance:\n\n1. **Understand the Writing Project**\n   \n   Ask clarifying questions:\n   - What's the topic and main argument?\n   - Who's the target audience?\n   - What's the desired length/format?\n   - What's your goal? (educate, persuade, entertain, explain)\n   - Any existing research or sources to include?\n   - What's your writing style? (formal, conversational, technical)\n\n2. **Collaborative Outlining**\n   \n   Help structure the content:\n   \n   ```markdown\n   # Article Outline: [Title]\n   \n   ## Hook\n   - [Opening line/story/statistic]\n   - [Why reader should care]\n   \n   ## Introduction\n   - Context and background\n   - Problem statement\n   - What this article covers\n   \n   ## Main Sections\n   \n   ### Section 1: [Title]\n   - Key point A\n   - Key point B\n   - Example/evidence\n   - [Research needed: specific topic]\n   \n   ### Section 2: [Title]\n   - Key point C\n   - Key point D\n   - Data/citation needed\n   \n   ### Section 3: [Title]\n   - Key point E\n   - Counter-arguments\n   - Resolution\n   \n   ## Conclusion\n   - Summary of main points\n   - Call to action\n   - Final thought\n   \n   ## Research To-Do\n   - [ ] Find data on [topic]\n   - [ ] Get examples of [concept]\n   - [ ] Source citation for [claim]\n   ```\n   \n   **Iterate on outline**:\n   - Adjust based on feedback\n   - Ensure logical flow\n   - Identify research gaps\n   - Mark sections for deep dives\n\n3. **Conduct Research**\n   \n   When user requests research on a topic:\n   \n   - Search for relevant information\n   - Find credible sources\n   - Extract key facts, quotes, and data\n   - Add citations in requested format\n   \n   Example output:\n   ```markdown\n   ## Research: AI Impact on Productivity\n   \n   Key Findings:\n   \n   1. **Productivity Gains**: Studies show 40% time savings for \n      content creation tasks [1]\n   \n   2. **Adoption Rates**: 67% of knowledge workers use AI tools \n      weekly [2]\n   \n   3. **Expert Quote**: \"AI augments rather than replaces human \n      creativity\" - Dr. Jane Smith, MIT [3]\n   \n   Citations:\n   [1] McKinsey Global Institute. (2024). \"The Economic Potential \n       of Generative AI\"\n   [2] Stack Overflow Developer Survey (2024)\n   [3] Smith, J. (2024). MIT Technology Review interview\n   \n   Added to outline under Section 2.\n   ```\n\n4. **Improve Hooks**\n   \n   When user shares an introduction, analyze and strengthen:\n   \n   **Current Hook Analysis**:\n   - What works: [positive elements]\n   - What could be stronger: [areas for improvement]\n   - Emotional impact: [current vs. potential]\n   \n   **Suggested Alternatives**:\n   \n   Option 1: [Bold statement]\n   > [Example]\n   *Why it works: [explanation]*\n   \n   Option 2: [Personal story]\n   > [Example]\n   *Why it works: [explanation]*\n   \n   Option 3: [Surprising data]\n   > [Example]\n   *Why it works: [explanation]*\n   \n   **Questions to hook**:\n   - Does it create curiosity?\n   - Does it promise value?\n   - Is it specific enough?\n   - Does it match the audience?\n\n5. **Provide Section-by-Section Feedback**\n   \n   As user writes each section, review for:\n   \n   ```markdown\n   # Feedback: [Section Name]\n   \n   ## What Works Well \n   - [Strength 1]\n   - [Strength 2]\n   - [Strength 3]\n   \n   ## Suggestions for Improvement\n   \n   ### Clarity\n   - [Specific issue]  [Suggested fix]\n   - [Complex sentence]  [Simpler alternative]\n   \n   ### Flow\n   - [Transition issue]  [Better connection]\n   - [Paragraph order]  [Suggested reordering]\n   \n   ### Evidence\n   - [Claim needing support]  [Add citation or example]\n   - [Generic statement]  [Make more specific]\n   \n   ### Style\n   - [Tone inconsistency]  [Match your voice better]\n   - [Word choice]  [Stronger alternative]\n   \n   ## Specific Line Edits\n   \n   Original:\n   > [Exact quote from draft]\n   \n   Suggested:\n   > [Improved version]\n   \n   Why: [Explanation]\n   \n   ## Questions to Consider\n   - [Thought-provoking question 1]\n   - [Thought-provoking question 2]\n   \n   Ready to move to next section!\n   ```\n\n6. **Preserve Writer's Voice**\n   \n   Important principles:\n   \n   - **Learn their style**: Read existing writing samples\n   - **Suggest, don't replace**: Offer options, not directives\n   - **Match tone**: Formal, casual, technical, friendly\n   - **Respect choices**: If they prefer their version, support it\n   - **Enhance, don't override**: Make their writing better, not different\n   \n   Ask periodically:\n   - \"Does this sound like you?\"\n   - \"Is this the right tone?\"\n   - \"Should I be more/less [formal/casual/technical]?\"\n\n7. **Citation Management**\n   \n   Handle references based on user preference:\n   \n   **Inline Citations**:\n   ```markdown\n   Studies show 40% productivity improvement (McKinsey, 2024).\n   ```\n   \n   **Numbered References**:\n   ```markdown\n   Studies show 40% productivity improvement [1].\n   \n   [1] McKinsey Global Institute. (2024)...\n   ```\n   \n   **Footnote Style**:\n   ```markdown\n   Studies show 40% productivity improvement^1\n   \n   ^1: McKinsey Global Institute. (2024)...\n   ```\n   \n   Maintain a running citations list:\n   ```markdown\n   ## References\n   \n   1. Author. (Year). \"Title\". Publication.\n   2. Author. (Year). \"Title\". Publication.\n   ...\n   ```\n\n8. **Final Review and Polish**\n   \n   When draft is complete, provide comprehensive feedback:\n   \n   ```markdown\n   # Full Draft Review\n   \n   ## Overall Assessment\n   \n   **Strengths**:\n   - [Major strength 1]\n   - [Major strength 2]\n   - [Major strength 3]\n   \n   **Impact**: [Overall effectiveness assessment]\n   \n   ## Structure & Flow\n   - [Comments on organization]\n   - [Transition quality]\n   - [Pacing assessment]\n   \n   ## Content Quality\n   - [Argument strength]\n   - [Evidence sufficiency]\n   - [Example effectiveness]\n   \n   ## Technical Quality\n   - Grammar and mechanics: [assessment]\n   - Consistency: [assessment]\n   - Citations: [completeness check]\n   \n   ## Readability\n   - Clarity score: [evaluation]\n   - Sentence variety: [evaluation]\n   - Paragraph length: [evaluation]\n   \n   ## Final Polish Suggestions\n   \n   1. **Introduction**: [Specific improvements]\n   2. **Body**: [Specific improvements]\n   3. **Conclusion**: [Specific improvements]\n   4. **Title**: [Options if needed]\n   \n   ## Pre-Publish Checklist\n   - [ ] All claims sourced\n   - [ ] Citations formatted\n   - [ ] Examples clear\n   - [ ] Transitions smooth\n   - [ ] Call to action present\n   - [ ] Proofread for typos\n   \n   Ready to publish! \n   ```\n\n## Examples\n\n### Example 1: Teresa Torres's Workflow\n\n**User**: \"I'm writing an article about continuous discovery. Help me create an outline.\"\n\n**Process**:\n1. Collaborates on outline structure\n2. Identifies research needs\n3. User starts writing introduction\n4. Reviews and improves the hook\n5. User writes each section\n6. Provides feedback after each section\n7. Conducts research and adds citations\n8. Final review of complete draft\n9. Polish and prep for publishing\n\n**Result**: Well-researched, properly cited article written in Teresa's voice with strong structure and flow.\n\n### Example 2: Research-Heavy Article\n\n**User**: \"I'm writing about AI's impact on product management. Help me find current data and examples.\"\n\n**Output**:\n```markdown\n## Research Compiled\n\n### Recent Data\n- Gartner: 80% of PMs will use AI tools by 2025 [1]\n- Survey: 45% report faster feature shipping [2]\n- Case study: Airbnb's AI-assisted prioritization [3]\n\n### Expert Quotes\n- \"AI amplifies PM judgment, not replaces it\" - Marty Cagan\n- [Additional quotes with citations]\n\n### Real Examples\n1. **Company A**: Used AI for user research synthesis\n   - Result: 60% time savings\n   - Source: [citation]\n\n2. **Company B**: AI-powered roadmap analysis\n   - Result: Better stakeholder alignment\n   - Source: [citation]\n\nAll added to your outline with proper citations.\n```\n\n### Example 3: Hook Improvement\n\n**User's Original Hook**:\n> \"Product management is changing because of AI. In this article, I'll discuss some ways AI affects product managers.\"\n\n**Improved Options**:\n\n**Option 1 (Data-driven)**:\n> \"Last month, I asked AI to analyze 500 customer interviews. It took 30 minutes instead of 3 weeks. Product management will never be the same.\"\n\n**Option 2 (Question)**:\n> \"What if you could talk to every customer, read every review, and analyze every support ticketall before your morning coffee?\"\n\n**Option 3 (Story)**:\n> \"Sarah spent two weeks building the wrong feature. Not because she didn't understand her users, but because she couldn't process the hundreds of interviews fast enough to spot the pattern.\"\n\n### Example 4: Section Feedback\n\n**User**: \"Just finished my 'Common Mistakes' section. Thoughts?\"\n\n**Response**:\n```markdown\n# Feedback: Common Mistakes Section\n\n## What Works Well \n- Strong examples (the Slack notification story)\n- Clear structure (3 mistakes, clean layout)\n- Practical advice for each mistake\n\n## Suggestions\n\n### Make It More Specific\nYour second point says \"Bad prioritization\" but could be stronger:\n\nCurrent:\n> \"Many teams prioritize badly by focusing on features instead of outcomes.\"\n\nSuggested:\n> \"I've watched teams ship 14 features in a quarter yet move none of their key metrics. They prioritized activity over progress.\"\n\n### Add Data\nThe third mistake would benefit from evidence:\n> \"[Add citation]: Studies show teams without regular user contact are 3x more likely to build unused features [needs source]\"\n\n### Flow Improvement\nConsider reordering: Mistake 3  Mistake 2  Mistake 1\nThis builds from small to big impact.\n\nReady for the next section!\n```\n\n## Writing Workflows\n\n### Blog Post Workflow\n1. Outline together\n2. Research key points\n3. Write introduction  get feedback\n4. Write body sections  feedback each\n5. Write conclusion  final review\n6. Polish and edit\n\n### Newsletter Workflow\n1. Discuss hook ideas\n2. Quick outline (shorter format)\n3. Draft in one session\n4. Review for clarity and links\n5. Quick polish\n\n### Technical Tutorial Workflow\n1. Outline steps\n2. Write code examples\n3. Add explanations\n4. Test instructions\n5. Add troubleshooting section\n6. Final review for accuracy\n\n### Thought Leadership Workflow\n1. Brainstorm unique angle\n2. Research existing perspectives\n3. Develop your thesis\n4. Write with strong POV\n5. Add supporting evidence\n6. Craft compelling conclusion\n\n## Pro Tips\n\n1. **Work in VS Code**: Better than web Claude for long-form writing\n2. **One section at a time**: Get feedback incrementally\n3. **Save research separately**: Keep a research.md file\n4. **Version your drafts**: article-v1.md, article-v2.md, etc.\n5. **Read aloud**: Use feedback to identify clunky sentences\n6. **Set deadlines**: \"I want to finish the draft today\"\n7. **Take breaks**: Write, get feedback, pause, revise\n\n## File Organization\n\nRecommended structure for writing projects:\n\n```\n~/writing/article-name/\n outline.md          # Your outline\n research.md         # All research and citations\n draft-v1.md         # First draft\n draft-v2.md         # Revised draft\n final.md            # Publication-ready\n feedback.md         # Collected feedback\n sources/            # Reference materials\n     study1.pdf\n     article2.md\n```\n\n## Best Practices\n\n### For Research\n- Verify sources before citing\n- Use recent data when possible\n- Balance different perspectives\n- Link to original sources\n\n### For Feedback\n- Be specific about what you want: \"Is this too technical?\"\n- Share your concerns: \"I'm worried this section drags\"\n- Ask questions: \"Does this flow logically?\"\n- Request alternatives: \"What's another way to explain this?\"\n\n### For Voice\n- Share examples of your writing\n- Specify tone preferences\n- Point out good matches: \"That sounds like me!\"\n- Flag mismatches: \"Too formal for my style\"\n\n## Related Use Cases\n\n- Creating social media posts from articles\n- Adapting content for different audiences\n- Writing email newsletters\n- Drafting technical documentation\n- Creating presentation content\n- Writing case studies\n- Developing course outlines\n\n",
        "skills/research-management/SKILL.md": "---\nname: research-management\ndescription: Manage research documentation in workspace platforms. Structures research findings, sources, and analysis in organized research databases.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Research & Documentation\n\nEnables comprehensive research workflows: search for information across your Notion workspace, fetch and analyze relevant pages, synthesize findings, and create well-structured documentation.\n\n## Quick Start\n\nWhen asked to research and document a topic:\n\n1. **Search for relevant content**: Use `Notion:notion-search` to find pages\n2. **Fetch detailed information**: Use `Notion:notion-fetch` to read full page content\n3. **Synthesize findings**: Analyze and combine information from multiple sources\n4. **Create structured output**: Use `Notion:notion-create-pages` to write documentation\n\n## Research Workflow\n\n### Step 1: Search for relevant information\n\n```\nUse Notion:notion-search with the research topic\nFilter by teamspace if scope is known\nReview search results to identify most relevant pages\n```\n\n### Step 2: Fetch page content\n\n```\nUse Notion:notion-fetch for each relevant page URL\nCollect content from all relevant sources\nNote key findings, quotes, and data points\n```\n\n### Step 3: Synthesize findings\n\nAnalyze the collected information:\n\n- Identify key themes and patterns\n- Connect related concepts across sources\n- Note gaps or conflicting information\n- Organize findings logically\n\n### Step 4: Create structured documentation\n\nUse the appropriate documentation template (see [reference/format-selection-guide.md](reference/format-selection-guide.md)) to structure output:\n\n- Clear title and executive summary\n- Well-organized sections with headings\n- Citations linking back to source pages\n- Actionable conclusions or next steps\n\n## Output Formats\n\nChoose the appropriate format based on request:\n\n**Research Summary**: See [reference/research-summary-format.md](reference/research-summary-format.md)\n**Comprehensive Report**: See [reference/comprehensive-report-format.md](reference/comprehensive-report-format.md)\n**Quick Brief**: See [reference/quick-brief-format.md](reference/quick-brief-format.md)\n\n## Best Practices\n\n1. **Cast a wide net first**: Start with broad searches, then narrow down\n2. **Cite sources**: Always link back to source pages using mentions\n3. **Verify recency**: Check page last-edited dates for current information\n4. **Cross-reference**: Validate findings across multiple sources\n5. **Structure clearly**: Use headings, bullets, and formatting for readability\n\n## Page Placement\n\nBy default, create research documents as standalone pages. If the user specifies:\n\n- A parent page  use `page_id` parent\n- A database  fetch the database first, then use appropriate `data_source_id`\n- A teamspace  create in that context\n\n## Advanced Features\n\n**Search filtering**: See [reference/advanced-search.md](reference/advanced-search.md)\n**Citation styles**: See [reference/citations.md](reference/citations.md)\n\n## Common Issues\n\n**\"No results found\"**: Try broader search terms or different teamspaces\n**\"Too many results\"**: Add filters or search within specific pages\n**\"Can't access page\"**: User may lack permissions, ask them to verify access\n\n## Examples\n\nSee [examples/](examples/) for complete workflow demonstrations:\n\n- [examples/market-research.md](examples/market-research.md) - Researching market trends\n- [examples/technical-investigation.md](examples/technical-investigation.md) - Technical deep-dive\n- [examples/competitor-analysis.md](examples/competitor-analysis.md) - Multi-source synthesis\n",
        "skills/research-management/evaluations/README.md": "# Research & Documentation Skill Evaluations\n\nEvaluation scenarios for testing the Research & Documentation skill across different Claude models.\n\n## Purpose\n\nThese evaluations ensure the Research & Documentation skill:\n\n- Searches across Notion workspace effectively\n- Synthesizes information from multiple sources\n- Selects appropriate research report format\n- Creates comprehensive documentation with proper citations\n- Works consistently across Haiku, Sonnet, and Opus\n\n## Evaluation Files\n\n### basic-research.json\n\nTests basic research workflow with synthesis across multiple Notion pages.\n\n**Scenario**: Research Q4 product roadmap and create summary  \n**Key Behaviors**:\n\n- Searches Notion for roadmap-related pages\n- Fetches multiple relevant pages (roadmap, product docs, meeting notes)\n- Synthesizes information from different sources\n- Selects appropriate format (Research Summary)\n- Includes citations linking back to source pages\n- Creates structured document with clear sections\n\n### research-to-database.json\n\nTests creating research documentation in a Notion database with properties.\n\n**Scenario**: Research competitor landscape and save to Research database  \n**Key Behaviors**:\n\n- Searches for existing competitive intelligence in Notion\n- Identifies Research database as target\n- Fetches database schema to understand properties\n- Creates page with correct property values (Research Type, Status, Date, etc.)\n- Structures content with comparison format\n- Includes source citations for both Notion pages and external research\n\n## Running Evaluations\n\n1. Enable the `research-documentation` skill\n2. Submit the query from the evaluation file\n3. Verify the skill searches Notion workspace comprehensively\n4. Check that multiple source pages are fetched and synthesized\n5. Verify appropriate format is selected (Research Summary, Comprehensive Report, Quick Brief, Comparison)\n6. Confirm citations link back to sources\n7. Test with Haiku, Sonnet, and Opus\n\n## Expected Skill Behaviors\n\nResearch & Documentation evaluations should verify:\n\n### Notion Search & Synthesis\n\n- Searches workspace with relevant queries\n- Fetches multiple source pages (3-5+)\n- Synthesizes information across sources\n- Identifies patterns and insights\n- Handles conflicting information appropriately\n\n### Format Selection\n\n- Chooses correct format based on scope and depth:\n  - **Research Summary**: Quick overview with key findings\n  - **Comprehensive Report**: Deep analysis with multiple sections\n  - **Quick Brief**: Fast facts and takeaways\n  - **Comparison**: Side-by-side analysis\n- Applies format structure consistently\n- Uses appropriate sections and headings\n\n### Citation & Attribution\n\n- Includes citations for all Notion sources\n- Uses mention-page tags: `<mention-page url=\"...\">`\n- Attributes findings to specific sources\n- Distinguishes between Notion content and Claude research\n- Links related documents\n\n### Document Quality\n\n- Title clearly indicates research topic and date\n- Executive summary or key findings upfront\n- Organized with clear hierarchy\n- Actionable insights and recommendations\n- Appropriate depth for the query\n\n## Creating New Evaluations\n\nWhen adding Research & Documentation evaluations:\n\n1. **Test different research types** - Product research, competitive analysis, technical investigation, market research\n2. **Vary source count** - Synthesis of 2-3 pages vs. 10+ pages\n3. **Test format selection** - Does it choose the right format for the scope?\n4. **Include database targets** - Not just standalone pages\n5. **Test citation accuracy** - Are all sources properly attributed?\n6. **Cross-workspace search** - Testing search across teamspaces if applicable\n\n## Example Success Criteria\n\n**Good** (specific, testable):\n\n- \"Searches Notion for 'roadmap' and 'Q4' and 'product'\"\n- \"Fetches at least 3 different source pages\"\n- \"Includes citation for each key finding using mention-page tags\"\n- \"Creates page with title format 'Research: [Topic] - [Date]'\"\n- \"Uses Research Summary format with sections: Executive Summary  Key Findings  Details  Recommendations  Sources\"\n\n**Bad** (vague, untestable):\n\n- \"Searches Notion effectively\"\n- \"Creates comprehensive research\"\n- \"Uses sources appropriately\"\n- \"Good documentation\"\n",
        "skills/research-management/examples/competitor-analysis.md": "# Example: Competitor Analysis\n\n## User Request\n\n> \"Research competitor pricing models and create a comparison document\"\n\n## Workflow Execution\n\n### Step 1: Search for Competitor Information\n\n```\nNotion:notion-search\nquery: \"competitor pricing\"\nquery_type: \"internal\"\nfilters: {\n  created_date_range: {\n    start_date: \"2024-01-01\"\n  }\n}\n```\n\n**Results Found:**\n\n- \"Competitor Analysis - Stripe\" (Product)\n- \"Market Research Q3 2024\" (Strategy)\n- \"Pricing Strategy Discussion\" (Product)\n- \"Competitor Feature Matrix\" (Product)\n\n### Step 2: Fetch and Analyze\n\n```\nNotion:notion-fetch for each page\n```\n\n**Extracted Information:**\n\n**Stripe**:\n\n- Pay-as-you-go: 2.9% + $0.30 per transaction\n- No monthly fees\n- Volume discounts available\n\n**Square**:\n\n- Pay-as-you-go: 2.6% + $0.10 per transaction\n- Monthly plan: $29/month with lower rates\n- Hardware costs separate\n\n**PayPal**:\n\n- Standard: 3.49% + $0.49 per transaction\n- Advanced: $30/month + 2.59% + $0.49\n- Multi-currency fees\n\n**Adyen**:\n\n- Interchange++ pricing model\n- Custom enterprise pricing\n- Setup and monthly fees apply\n\n### Step 3: Create Comparison Document\n\n```\nNotion:notion-create-pages\n\npages: [{\n  properties: {\n    \"title\": \"Competitor Pricing Analysis - Payment Processors\"\n  },\n  content: \"[Comparison document using template]\"\n}]\n```\n\n## Output Document\n\n```markdown\n# Competitor Pricing Analysis - Payment Processors\n\n**Date**: October 14, 2025  \n**Research Team**: Product Strategy\n\n## Executive Summary\n\nWe analyzed four major payment processor competitors: Stripe, Square, PayPal, and Adyen. Pricing models vary from simple pay-as-you-go (Stripe) to complex interchange-plus (Adyen). Key insight: All competitors offer volume discounts for high-transaction merchants, with breakpoints typically at $100K/month processing volume.\n\n## Comparison Matrix\n\n| Feature              | Stripe              | Square       | PayPal        | Adyen         |\n| -------------------- | ------------------- | ------------ | ------------- | ------------- |\n| **Base Rate**        | 2.9% + $0.30        | 2.6% + $0.10 | 3.49% + $0.49 | Interchange++ |\n| **Monthly Fee**      | $0                  | $0-29        | $0-30         | Custom        |\n| **Volume Discounts** | Yes, >$80K          | Yes, >$250K  | Yes, >$100K   | Yes, custom   |\n| **Setup Fee**        | $0                  | $0           | $0            | $1,000-5,000  |\n| **Multi-currency**   | 1% extra            | 3% extra     | 3-4% extra    | Included      |\n| **Chargeback Fee**   | $15                 | $15-25       | $20           | Custom        |\n| **Target Market**    | Startups-Enterprise | Small-Medium | Small-Medium  | Enterprise    |\n\n## Detailed Analysis\n\n### Stripe\n\n**Pricing Structure**:\n\n- **Standard**: 2.9% + $0.30 per successful card charge\n- **Volume discounts**: Available for businesses processing >$80,000/month\n- **International cards**: +1% fee\n- **Currency conversion**: 1% above market rate\n\n**Strengths**:\n\n- Simple, transparent pricing\n- No setup fees or monthly minimums\n- Excellent developer experience\n- Quick onboarding\n\n**Weaknesses**:\n\n- Higher per-transaction fee for high volume\n- Volume discounts less aggressive than Adyen\n\n**Best for**: Startups and growth-stage companies needing quick integration\n\n**Source**: <mention-page url=\"...\">Competitor Analysis - Stripe</mention-page>\n\n### Square\n\n**Pricing Structure**:\n\n- **Pay-as-you-go**: 2.6% + $0.10 per tap, dip, or swipe\n- **Keyed-in**: 3.5% + $0.15\n- **Plus plan**: $29/month for lower rates (2.5% + $0.10)\n- **Premium plan**: Custom pricing\n\n**Strengths**:\n\n- Lowest per-transaction fee for in-person\n- All-in-one hardware + software\n- No long-term contracts\n\n**Weaknesses**:\n\n- Higher rates for online/keyed transactions\n- Hardware costs ($49-$299)\n- Less suitable for online-only businesses\n\n**Best for**: Brick-and-mortar retail and restaurants\n\n**Source**: <mention-page url=\"...\">Market Research Q3 2024</mention-page>\n\n### PayPal\n\n**Pricing Structure**:\n\n- **Standard**: 3.49% + $0.49 per transaction\n- **Advanced**: $30/month + 2.59% + $0.49\n- **Payments Pro**: Additional $30/month for direct credit card processing\n\n**Strengths**:\n\n- Huge customer base (PayPal checkout)\n- Buyer protection increases trust\n- International reach (200+ countries)\n\n**Weaknesses**:\n\n- Highest per-transaction fees\n- Complex fee structure\n- Account holds and reserves common\n\n**Best for**: Businesses where PayPal brand trust matters (e-commerce, marketplaces)\n\n**Source**: <mention-page url=\"...\">Pricing Strategy Discussion</mention-page>\n\n### Adyen\n\n**Pricing Structure**:\n\n- **Interchange++**: Actual interchange + scheme fees + fixed markup\n- **Setup fee**: $1,000-5,000 (negotiable)\n- **Monthly minimum**: Typically $10,000+ processing volume\n- **Per-transaction**: Interchange + 0.6% + $0.12 (example)\n\n**Strengths**:\n\n- Most transparent cost structure at scale\n- Lowest effective rate for high volume\n- True multi-currency (100+ currencies)\n- Direct connections to schemes\n\n**Weaknesses**:\n\n- Complex pricing requires analysis\n- High minimums ($10K+/month)\n- Longer integration time\n- Not suitable for small businesses\n\n**Best for**: Enterprise with $1M+/month processing volume\n\n**Source**: <mention-page url=\"...\">Competitor Feature Matrix</mention-page>\n\n## Pricing Trends & Insights\n\n### Volume-Based Discounting\n\nAll competitors offer discounts at scale:\n\n- **Entry point**: $80K-$250K/month processing\n- **Typical discount**: 10-30 basis points reduction\n- **Negotiation leverage**: Begins at $500K/month+\n\n### Hidden Costs to Consider\n\n| Cost Type            | Stripe   | Square   | PayPal       | Adyen  |\n| -------------------- | -------- | -------- | ------------ | ------ |\n| Chargeback           | $15      | $15-25   | $20          | $15-25 |\n| Account verification | $0       | $0       | $0           | Varies |\n| PCI compliance       | $0       | $0       | $0           | $0     |\n| Currency conversion  | 1%       | 3%       | 3-4%         | 0%     |\n| Refund fees          | Returned | Returned | Not returned | Varies |\n\n### Market Positioning\n```\n\nHigh Volume / Enterprise\n\n| Adyen\n|\n| Stripe\n|\n| Square PayPal\n|\n\nSmall / Simple Complex / International\n\n```\n\n## Strategic Implications\n\n### For Startups (<$100K/month)\n**Recommended**: Stripe\n- Lowest friction to start\n- No upfront costs\n- Great documentation\n- Acceptable rates at this scale\n\n### For Growing Companies ($100K-$1M/month)\n**Recommended**: Stripe or Square\n- Negotiate volume discounts\n- Evaluate interchange++ if international\n- Consider Square if in-person dominant\n\n### For Enterprises (>$1M/month)\n**Recommended**: Adyen or Negotiated Stripe\n- Interchange++ models save significantly\n- Direct scheme connections\n- Multi-region capabilities matter\n- ROI on integration complexity\n\n## Recommendations\n\n1. **Immediate**: Benchmark our current 2.8% + $0.25 against Stripe's standard\n2. **Short-term**: Request volume discount quote from Stripe at our current $150K/month\n3. **Long-term**: Evaluate Adyen when we cross $1M/month threshold\n\n## Next Steps\n\n- [ ] Request detailed pricing proposal from Stripe for volume discounts\n- [ ] Create pricing calculator comparing all 4 at different volume levels\n- [ ] Interview customers about payment method preferences\n- [ ] Analyze our transaction mix (domestic vs international, card types)\n\n## Sources\n\n### Primary Research\n- <mention-page url=\"...\">Competitor Analysis - Stripe</mention-page>\n- <mention-page url=\"...\">Market Research Q3 2024</mention-page>\n- <mention-page url=\"...\">Pricing Strategy Discussion</mention-page>\n- <mention-page url=\"...\">Competitor Feature Matrix</mention-page>\n\n### External References\n- Stripe.com pricing page (Oct 2025)\n- Square pricing documentation\n- PayPal merchant fees\n- Adyen pricing transparency report\n```\n\n## Key Success Factors\n\n1. **Structured comparison**: Matrix format for quick scanning\n2. **Multiple dimensions**: Price, features, target market\n3. **Strategic recommendations**: Not just data, but implications\n4. **Visual elements**: Table and positioning diagram\n5. **Actionable next steps**: Clear recommendations\n6. **Comprehensive sources**: Internal research + external validation\n\n## Workflow Pattern Demonstrated\n\n- **Date-filtered search** (recent information only)\n- **Multiple competitor synthesis** (4 different companies)\n- **Comparison template** (matrix + detailed analysis)\n- **Strategic layer** (implications and recommendations)\n- **Action-oriented** (next steps included)\n",
        "skills/research-management/examples/market-research.md": "# Example: Market Research\n\n**User Request**: \"Research the current state of AI coding assistants market and create a summary document in Notion\"\n\n## Workflow\n\n### 1. Search\n\n```\nNotion:notion-search\nquery: \"AI coding assistants market\"\n```\n\nFound 3 relevant pages across Engineering, Strategy, and Product teamspaces.\n\n### 2. Fetch & Analyze\n\n```\nNotion:notion-fetch (3x)\n```\n\nExtracted market size, competitive landscape, technology trends from source pages.\n\n### 3. Create Documentation\n\n```\nNotion:notion-create-pages\n```\n\n## Output (Condensed)\n\n```markdown\n# AI Coding Assistants Market Research - Oct 2025\n\n## Executive Summary\n\nThe AI coding assistant market is experiencing 150%+ YoY growth. GitHub Copilot dominates with 60% share, but specialized tools are gaining traction in specific niches.\n\n## Key Findings\n\n### Market Size and Growth\n\n$800M in 2024  $2.5B projected by 2026. Developer adoption: 23% (2023)  47% (2024).\nSource: <mention-page url=\"...\">Market Trends Q3 2025</mention-page>\n\n### Competitive Landscape\n\n- GitHub Copilot: 60% (strong IDE integration)\n- Cursor: 15% (rapid growth, full IDE)\n- Tabnine: 10% (enterprise, on-premise)\n- Cody: 5% (codebase-aware)\n- CodeWhisperer: 8% (AWS integration)\n  Source: <mention-page url=\"...\">AI Tools Competitive Analysis</mention-page>\n\n### Technology Trends\n\nKey differentiators: context awareness, customization, multi-modal interfaces, code verification.\nSource: <mention-page url=\"...\">Developer Tools Landscape</mention-page>\n\n## Next Steps\n\n1. Monitor Cursor growth and feature releases\n2. Evaluate Cody's codebase-aware capabilities\n3. Document enterprise security/compliance requirements\n4. Track pricing trends\n```\n\n## Key Takeaways\n\n- Found relevant pages across multiple teamspaces\n- Synthesized competitive, market, and technical perspectives\n- Used proper citations linking to source pages\n- Created actionable recommendations\n",
        "skills/research-management/examples/technical-investigation.md": "# Example: Technical Investigation\n\n## User Request\n\n> \"Research our current caching strategy and create a technical summary\"\n\n## Workflow Execution\n\n### Step 1: Search for Caching Information\n\n```\nNotion:notion-search\nquery: \"caching strategy architecture\"\nquery_type: \"internal\"\nteamspace_id: \"engineering-teamspace-id\"\n```\n\n**Results Found:**\n\n- \"System Architecture Overview\" (Engineering)\n- \"Redis Implementation Guide\" (Backend Docs)\n- \"Performance Optimization - Q3 2024\" (Engineering)\n- \"API Caching Decision Record\" (Architecture)\n\n### Step 2: Fetch Relevant Pages\n\n```\nNotion:notion-fetch\nid: \"system-architecture-page-url\"\n```\n\nExtracted: Current caching architecture uses Redis for API responses, Memcached for session storage.\n\n```\nNotion:notion-fetch\nid: \"redis-implementation-guide-url\"\n```\n\nExtracted: Implementation details, TTL settings, invalidation strategies.\n\n```\nNotion:notion-fetch\nid: \"api-caching-decision-record-url\"\n```\n\nExtracted: Why Redis was chosen over alternatives, trade-offs considered.\n\n### Step 3: Synthesize Findings\n\n**Key findings identified:**\n\n- Two-tier caching: Redis (API responses) + Memcached (sessions)\n- TTL strategy: 5 minutes for dynamic data, 1 hour for static\n- Invalidation: Event-driven for critical updates\n- Performance impact: 75% reduction in database load\n- Known issues: Cache stampede on popular endpoints\n\n### Step 4: Create Technical Summary\n\n```\nNotion:notion-create-pages\n\nparent: { page_id: \"engineering-docs-parent-id\" }\n\npages: [{\n  properties: {\n    \"title\": \"Technical Summary: Caching Strategy - Oct 2025\"\n  },\n  content: \"[Structured technical summary using template]\"\n}]\n```\n\n## Output Document\n\n```markdown\n# Technical Summary: Caching Strategy - Oct 2025\n\n## Executive Summary\n\nOur caching infrastructure uses a two-tier approach with Redis for API response caching and Memcached for session management. This strategy has reduced database load by 75% and improved API response times from 200ms to 50ms average.\n\n## Architecture Overview\n\n### Layer 1: API Response Caching (Redis)\n\n**Technology**: Redis 7.0 cluster (3 nodes)\n**Purpose**: Cache GET endpoint responses\n**TTL Strategy**:\n\n- Dynamic content: 5 minutes\n- Static content: 1 hour\n- User-specific: 15 minutes\n\n**Source**: <mention-page url=\"...\">System Architecture Overview</mention-page>\n\n### Layer 2: Session Storage (Memcached)\n\n**Technology**: Memcached 1.6\n**Purpose**: User session data, temporary state\n**TTL**: 24 hours (session lifetime)\n\n## Implementation Details\n\n### Cache Key Format\n```\n\napi:v1:{endpoint}:{params_hash}\nsession:{user_id}:{session_id}\n\n````\n\n### Invalidation Strategy\n- **Event-driven**: Critical data changes trigger immediate invalidation\n- **Time-based**: TTL expiration for non-critical data\n- **Manual**: Admin tools for emergency cache clear\n\n**Source**: <mention-page url=\"...\">Redis Implementation Guide</mention-page>\n\n## Decision Rationale\n\n### Why Redis for API Caching?\n\n**Pros**:\n- Advanced data structures (sorted sets, hashes)\n- Built-in TTL with automatic eviction\n- Pub/sub for cache invalidation events\n- Persistence options for durability\n\n**Cons**:\n- Higher memory usage than Memcached\n- More complex cluster management\n\n**Decision**: Chosen for flexibility and rich feature set needed for API caching.\n\n**Source**: <mention-page url=\"...\">API Caching Decision Record</mention-page>\n\n### Why Memcached for Sessions?\n\n**Pros**:\n- Simpler, lighter weight\n- Excellent for key-value storage\n- Lower memory footprint\n\n**Cons**:\n- No persistence\n- Limited data structures\n\n**Decision**: Perfect fit for ephemeral session data where simplicity is valued.\n\n## Performance Impact\n\n| Metric | Before Caching | After Caching | Improvement |\n|--------|----------------|---------------|-------------|\n| Avg Response Time | 200ms | 50ms | 75% faster |\n| Database Load | 100% | 25% | 75% reduction |\n| Cache Hit Rate | - | 85% | - |\n| Peak RPS Handled | 1,000 | 4,000 | 4x increase |\n\n**Source**: <mention-page url=\"...\">Performance Optimization - Q3 2024</mention-page>\n\n## Known Issues & Limitations\n\n### Cache Stampede\n**Issue**: When popular cache entries expire, multiple requests hit database simultaneously\n\n**Mitigation**: Implemented probabilistic early expiration and request coalescing\n\n**Status**: Reduced by 90% but not eliminated\n\n### Stale Data Risk\n**Issue**: Cached data can be up to TTL duration stale\n\n**Mitigation**: Event-driven invalidation for critical data paths\n\n**Status**: Acceptable trade-off for performance gains\n\n## Monitoring & Observability\n\n**Metrics tracked**:\n- Cache hit/miss rates per endpoint\n- Memory usage and eviction rates\n- Response time distributions\n- Invalidation event frequency\n\n**Tools**: DataDog dashboards, CloudWatch alarms\n\n## Future Considerations\n\n1. **Edge Caching**: Evaluate CDN integration for static assets\n2. **Cache Warming**: Pre-populate cache for predictable traffic spikes\n3. **Adaptive TTLs**: Adjust TTLs based on data change frequency\n4. **Regional Caching**: Multi-region cache replication for global performance\n\n## Related Documentation\n\n- <mention-page url=\"...\">System Architecture Overview</mention-page>\n- <mention-page url=\"...\">Redis Implementation Guide</mention-page>\n- <mention-page url=\"...\">API Caching Decision Record</mention-page>\n- <mention-page url=\"...\">Performance Optimization - Q3 2024</mention-page>\n\n## Appendix: Configuration Examples\n\n### Redis Configuration\n```yaml\nmaxmemory: 8gb\nmaxmemory-policy: allkeys-lru\ntcp-keepalive: 60\n````\n\n### Common Cache Operations\n\n```python\n# Set with TTL\ncache.set(key, value, ttl=300)\n\n# Get with fallback\nvalue = cache.get(key) or fetch_from_db(key)\n\n# Invalidate pattern\ncache.delete_pattern(\"api:v1:users:*\")\n```\n\n```\n\n## Key Success Factors\n\n1. **Multiple source integration**: Combined architecture docs, implementation guides, and decision records\n2. **Technical depth**: Included configuration, code examples, metrics\n3. **Decision context**: Explained why choices were made, not just what\n4. **Practical focus**: Real performance numbers and known issues\n5. **Future-looking**: Noted areas for improvement\n6. **Well-cited**: Every major point links back to source material\n\n## Workflow Pattern Demonstrated\n\nThis example shows the complete research workflow:\n- **Scoped search** (teamspace filter for engineering)\n- **Multi-page synthesis** (4 different sources)\n- **Technical template** (architecture-focused format)\n- **Proper placement** (under engineering docs)\n- **Comprehensive citations** (links to all sources)\n\n```\n",
        "skills/research-management/examples/trip-planning.md": "# Example: Group Trip Research & Planning\n\n**User Request**: \"Research and plan our friends' trip to Japan in March - we're 6 people looking for 10 days\"\n\n## Workflow\n\n### 1. Search Existing Notes\n\n```\nNotion:notion-search\nquery: \"Japan travel\"\n```\n\nFound: Japan Travel Guide (from friend), Tokyo Restaurants, Kyoto Temple Guide\n\n### 2. Fetch & Extract Tips\n\n```\nNotion:notion-fetch (3x)\n```\n\n**Key info from previous travelers:**\n\n- Best time: March-April (cherry blossoms)\n- Must-see: Tokyo, Kyoto, Osaka\n- Budget: $200-300/day (mid-range)\n- Book accommodations 3 months ahead\n- Get JR Pass before arrival\n- Top restaurants: Sushi Dai, Ichiran Ramen, Tsunahachi Tempura\n\n### 3. Research & Synthesize\n\nCombined previous traveler insights with:\n\n- Flight options and prices\n- Accommodation types (hotels/ryokans/Airbnb)\n- Transportation (JR Pass essential)\n- 10-day itinerary structure\n- Budget breakdown\n\n### 4. Create Comprehensive Plan\n\n```\nNotion:notion-create-pages\nparent: { page_id: \"travel-plans-parent-id\" }\npages: [{\n  properties: {\n    title: \"Japan Trip 2026 - March 15-25 (10 Days)\"\n  },\n  content: \"[Full trip plan with itinerary, budget, tips...]\"\n}]\n```\n\n## Output Sample\n\n```markdown\n# Japan Trip 2026 - March 15-25 (10 Days)\n\n## Trip Overview\n\n**Dates**: March 15-25, 2026 (Cherry Blossom Season )\n**Group**: 6 people | **Budget**: $3,000-4,000/person\n\n## Itinerary Summary\n\n**Days 1-3: Tokyo**\n\n- Arrive, explore Shibuya, Harajuku, Shinjuku\n- Visit Tsukiji Market, Imperial Palace, Sensoji Temple\n- Experience Tokyo nightlife, teamLab Borderless\n\n**Days 4-5: Hakone**\n\n- Day trip from Tokyo\n- Hot springs, Mt. Fuji views, Lake Ashi cruise\n\n**Days 6-8: Kyoto**\n\n- Bullet train from Tokyo\n- Fushimi Inari, Kinkaku-ji, Arashiyama Bamboo Grove\n- Geisha district (Gion), traditional tea ceremony\n\n**Days 9-10: Osaka + Depart**\n\n- Osaka Castle, Dotonbori food district\n- Nara day trip (deer park, Todai-ji Temple)\n- Return flight from Osaka (KIX)\n\n## Budget Breakdown (per person)\n\n- **Flights**: $900-1,200 (round-trip)\n- **Accommodations**: $700-1,000 (9 nights)\n- **JR Pass**: $280 (7-day pass)\n- **Food**: $500-700 ($50-70/day)\n- **Activities**: $300-400\n- **Local transport**: $100\n  **Total**: $2,780-3,680\n\n## Key Bookings\n\n- **Flights**: Book 3-4 months ahead\n- **Hotels**: Book now (cherry blossom season)\n  - Tokyo: Shinjuku area (2 nights) + Asakusa (1 night)\n  - Kyoto: Gion area (3 nights)\n  - Osaka: Namba area (2 nights)\n- **JR Pass**: Order 2-3 weeks before travel\n- **Restaurants**: Reserve 1 week ahead (Sushi Dai, high-end spots)\n\n## Essential Tips\n\nSource: <mention-page url=\"...\">Japan Travel Guide</mention-page>\n\n- Get pocket WiFi or eSIM on arrival\n- Download: Google Translate, Hyperdia (train routes), Tabelog (restaurants)\n- Cash-heavy country - withdraw at 7-Eleven ATMs\n- Shoes off in temples, ryokans, some restaurants\n- Trains extremely punctual - don't be late\n- Learn basic phrases: arigatou, sumimasen, itadakimasu\n\n## Packing List\n\n- Comfortable walking shoes (10k+ steps/day)\n- Light jacket (March 55-65F)\n- Backpack for day trips\n- Cash pouch\n- Portable charger\n\n## Next Steps\n\n- [ ] Book flights (target: <$1,100/person)\n- [ ] Order JR Passes\n- [ ] Book hotels (Tokyo  Kyoto  Osaka)\n- [ ] Create shared expense tracker\n- [ ] Schedule group planning call\n\n## Sources\n\n- <mention-page url=\"...\">Japan Travel Guide</mention-page> (Sarah's 2024 trip)\n- <mention-page url=\"...\">Tokyo Restaurant Recommendations</mention-page>\n- <mention-page url=\"...\">Kyoto Temple Guide</mention-page>\n```\n\n## Key Takeaways\n\n- Leveraged previous traveler notes from Notion\n- Combined personal insights with research\n- Created actionable itinerary with budget breakdown\n- Included practical tips from experienced travelers\n- Set clear next steps for group coordination\n",
        "skills/research-management/reference/advanced-search.md": "# Advanced Search Techniques\n\n## Search Filtering\n\n### By Date Range\n\nUse `created_date_range` to find recent content:\n\n```\nfilters: {\n  created_date_range: {\n    start_date: \"2024-01-01\",\n    end_date: \"2025-01-01\"\n  }\n}\n```\n\n**When to use**:\n\n- Finding recent updates on a topic\n- Focusing on current information\n- Excluding outdated content\n\n### By Creator\n\nUse `created_by_user_ids` to find content from specific people:\n\n```\nfilters: {\n  created_by_user_ids: [\"user-id-1\", \"user-id-2\"]\n}\n```\n\n**When to use**:\n\n- Research from subject matter experts\n- Team-specific information\n- Attribution tracking\n\n### Combined Filters\n\nStack filters for precision:\n\n```\nfilters: {\n  created_date_range: {\n    start_date: \"2024-10-01\"\n  },\n  created_by_user_ids: [\"expert-user-id\"]\n}\n```\n\n## Scoped Searches\n\n### Teamspace Scoping\n\nRestrict search to specific teamspace:\n\n```\nteamspace_id: \"teamspace-uuid\"\n```\n\n**When to use**:\n\n- Project-specific research\n- Department-focused information\n- Reducing noise from irrelevant results\n\n### Page Scoping\n\nSearch within a specific page and its subpages:\n\n```\npage_url: \"https://notion.so/workspace/Page-Title-uuid\"\n```\n\n**When to use**:\n\n- Research within a project hierarchy\n- Documentation updates\n- Focused investigation\n\n### Database Scoping\n\nSearch within a database's content:\n\n```\ndata_source_url: \"collection://data-source-uuid\"\n```\n\n**When to use**:\n\n- Task/project database research\n- Structured data investigation\n- Finding specific entries\n\n## Search Strategies\n\n### Broad to Narrow\n\n1. Start with general search term\n2. Review results for relevant teamspaces/pages\n3. Re-search with scope filters\n4. Fetch detailed content from top results\n\n**Example**:\n\n```\nSearch 1: query=\"API integration\"  50 results across workspace\nSearch 2: query=\"API integration\", teamspace_id=\"engineering\"  12 results\nFetch: Top 3-5 most relevant pages\n```\n\n### Multi-Query Approach\n\nRun parallel searches with related terms:\n\n```\nQuery 1: \"API integration\"\nQuery 2: \"API authentication\"\nQuery 3: \"API documentation\"\n```\n\nCombine results to build comprehensive picture.\n\n### Temporal Research\n\nSearch across time periods to track evolution:\n\n```\nSearch 1: created_date_range 2023  Historical context\nSearch 2: created_date_range 2024  Recent developments\nSearch 3: created_date_range 2025  Current state\n```\n\n## Result Processing\n\n### Identifying Relevant Results\n\nLook for:\n\n- **High semantic match**: Result summary closely matches query intent\n- **Recent updates**: Last-edited date is recent\n- **Authoritative sources**: Created by known experts or in official locations\n- **Comprehensive content**: Result summary suggests detailed information\n\n### Prioritizing Fetches\n\nFetch pages in order of relevance:\n\n1. **Primary sources**: Direct documentation, official pages\n2. **Recent updates**: Newly edited content\n3. **Related context**: Supporting information\n4. **Historical reference**: Background and context\n\nDon't fetch everything - be selective based on research needs.\n\n### Handling Too Many Results\n\nIf search returns 20+ results:\n\n1. **Add filters**: Narrow by date, creator, or teamspace\n2. **Refine query**: Use more specific terms\n3. **Use page scoping**: Search within relevant parent page\n4. **Sample strategically**: Fetch diverse results (recent, popular, authoritative)\n\n### Handling Too Few Results\n\nIf search returns < 3 results:\n\n1. **Broaden query**: Use more general terms\n2. **Remove filters**: Search full workspace\n3. **Try synonyms**: Alternative terminology\n4. **Search in related areas**: Adjacent teamspaces or pages\n\n## Search Quality\n\n### Effective Search Queries\n\n**Good queries** (specific, semantic):\n\n- \"Q4 product roadmap\"\n- \"authentication implementation guide\"\n- \"customer feedback themes\"\n\n**Weak queries** (too vague):\n\n- \"roadmap\"\n- \"guide\"\n- \"feedback\"\n\n**Over-specific queries** (too narrow):\n\n- \"Q4 2024 product roadmap for mobile app version 3.2 feature X\"\n\n### User Context\n\nAlways use available user context:\n\n- Query should match their terminology\n- Scope to their relevant teamspaces\n- Consider their role/department\n- Reference their recent pages\n\n## Connected Sources\n\n### Notion Integrations\n\nSearch extends beyond Notion pages to:\n\n- Slack messages (if connected)\n- Google Drive documents (if connected)\n- GitHub issues/PRs (if connected)\n- Jira tickets (if connected)\n\nBe aware results may come from these sources.\n\n### Source Attribution\n\nWhen citing results from connected sources:\n\n- Note the source type in documentation\n- Use appropriate mention format\n- Verify user has access to the source system\n",
        "skills/research-management/reference/citations.md": "# Citation Styles\n\n## Basic Page Citation\n\nAlways cite sources using Notion page mentions:\n\n```markdown\n<mention-page url=\"https://notion.so/workspace/Page-Title-uuid\">Page Title</mention-page>\n```\n\nThe URL must be provided. The title is optional but improves readability:\n\n```markdown\n<mention-page url=\"https://notion.so/workspace/Page-Title-uuid\"/>\n```\n\n## Inline Citations\n\nCite immediately after referenced information:\n\n```markdown\nThe Q4 revenue increased by 23% quarter-over-quarter (<mention-page url=\"...\">Q4 Financial Report</mention-page>).\n```\n\n## Multiple Sources\n\nWhen information comes from multiple sources:\n\n```markdown\nCustomer satisfaction has improved across all metrics (<mention-page url=\"...\">Q3 Survey Results</mention-page>, <mention-page url=\"...\">Support Analysis</mention-page>).\n```\n\n## Section-Level Citations\n\nFor longer sections derived from one source:\n\n```markdown\n### Engineering Priorities\n\nAccording to the <mention-page url=\"...\">Engineering Roadmap 2025</mention-page>:\n\n- Focus on API scalability\n- Improve developer experience\n- Migrate to microservices architecture\n```\n\n## Sources Section\n\nAlways include a \"Sources\" section at document end:\n\n```markdown\n## Sources\n\n- <mention-page url=\"...\">Strategic Plan 2025</mention-page>\n- <mention-page url=\"...\">Market Analysis Report</mention-page>\n- <mention-page url=\"...\">Competitor Research: Q3</mention-page>\n- <mention-page url=\"...\">Customer Interview Notes</mention-page>\n```\n\nGroup by category for long lists:\n\n```markdown\n## Sources\n\n### Primary Sources\n\n- <mention-page url=\"...\">Official Roadmap</mention-page>\n- <mention-page url=\"...\">Strategy Document</mention-page>\n\n### Supporting Research\n\n- <mention-page url=\"...\">Market Trends</mention-page>\n- <mention-page url=\"...\">Customer Feedback</mention-page>\n\n### Background Context\n\n- <mention-page url=\"...\">Historical Analysis</mention-page>\n```\n\n## Quoting Content\n\nWhen quoting directly from source:\n\n```markdown\nThe product team noted: \"We need to prioritize mobile experience improvements\" (<mention-page url=\"...\">Product Meeting Notes</mention-page>).\n```\n\nFor block quotes:\n\n```markdown\n> We need to prioritize mobile experience improvements to meet our Q4 goals. This includes performance optimization and UI refresh.\n>\n>  <mention-page url=\"...\">Product Meeting Notes - Oct 2025</mention-page>\n```\n\n## Data Citations\n\nWhen presenting data, cite the source:\n\n```markdown\n| Metric  | Q3    | Q4    | Change |\n| ------- | ----- | ----- | ------ |\n| Revenue | $2.3M | $2.8M | +21.7% |\n| Users   | 12.4K | 15.1K | +21.8% |\n\nSource: <mention-page url=\"...\">Financial Dashboard</mention-page>\n```\n\n## Database Citations\n\nWhen referencing database content:\n\n```markdown\nBased on analysis of the <mention-database url=\"...\">Projects Database</mention-database>, 67% of projects are on track.\n```\n\n## User Citations\n\nWhen attributing information to specific people:\n\n```markdown\n<mention-user url=\"...\">Sarah Chen</mention-user> noted in <mention-page url=\"...\">Architecture Review</mention-page> that the microservices migration is ahead of schedule.\n```\n\n## Citation Frequency\n\n**Over-citing** (every sentence):\n\n```markdown\nThe revenue increased (<mention-page url=\"...\">Report</mention-page>).\nCosts decreased (<mention-page url=\"...\">Report</mention-page>).\nMargin improved (<mention-page url=\"...\">Report</mention-page>).\n```\n\n**Under-citing** (no attribution):\n\n```markdown\nThe revenue increased, costs decreased, and margin improved.\n```\n\n**Right balance** (grouped citation):\n\n```markdown\nThe revenue increased, costs decreased, and margin improved (<mention-page url=\"...\">Q4 Financial Report</mention-page>).\n```\n\n## Outdated Information\n\nNote when source information might be outdated:\n\n```markdown\nThe original API design (<mention-page url=\"...\">API Spec v1</mention-page>, last updated January 2024) has been superseded by the new architecture in <mention-page url=\"...\">API Spec v2</mention-page>.\n```\n\n## Cross-References\n\nLink to related research documents:\n\n```markdown\n## Related Research\n\nThis research builds on previous findings:\n\n- <mention-page url=\"...\">Market Analysis - Q2 2025</mention-page>\n- <mention-page url=\"...\">Competitor Landscape Review</mention-page>\n\nFor implementation details, see:\n\n- <mention-page url=\"...\">Technical Implementation Guide</mention-page>\n```\n\n## Citation Validation\n\nBefore finalizing research:\n\n Every key claim has a source citation\n All page mentions have valid URLs\n Sources section includes all cited pages\n Outdated sources are noted as such\n Direct quotes are clearly marked\n Data sources are attributed\n\n## Citation Style Consistency\n\nChoose one citation style and use throughout:\n\n**Inline style** (lightweight):\n\n```markdown\nRevenue grew 23% (Financial Report). Customer count increased 18% (Metrics Dashboard).\n```\n\n**Formal style** (full mentions):\n\n```markdown\nRevenue grew 23% (<mention-page url=\"...\">Q4 Financial Report</mention-page>). Customer count increased 18% (<mention-page url=\"...\">Metrics Dashboard</mention-page>).\n```\n\n**Recommend formal style** for most research documentation as it provides clickable navigation.\n",
        "skills/research-management/reference/comparison-format.md": "# Comparison Format\n\n**When to use**:\n\n- Evaluating multiple options\n- Tool/vendor selection\n- Approach comparison\n- Decision support\n\n## Characteristics\n\n**Length**: 800-1200 words typically\n\n**Structure**:\n\n- Overview of what's being compared\n- Comparison matrix table\n- Detailed analysis per option (pros/cons)\n- Clear recommendation with rationale\n- Sources\n\n## Template\n\nSee [comparison-template.md](comparison-template.md) for the full template.\n\n## Best For\n\n- Decision support with multiple options\n- Tool or vendor selection\n- Comparing different technical approaches\n- Evaluating trade-offs between alternatives\n\n## Example Use Cases\n\n- \"Compare the three database options discussed in our tech docs\"\n- \"What are the pros and cons of each deployment approach?\"\n- \"Compare the vendor proposals\"\n- \"Evaluate the different authentication methods we've documented\"\n",
        "skills/research-management/reference/comparison-template.md": "# Comparison Template\n\nUse when researching multiple options or alternatives. See [comparison-format.md](comparison-format.md) for when to use this format.\n\n```markdown\n# [Topic] Comparison\n\n## Overview\n\n[Brief introduction to what's being compared and why]\n\n## Comparison Matrix\n\n| Criteria      | Option A         | Option B         | Option C         |\n| ------------- | ---------------- | ---------------- | ---------------- |\n| [Criterion 1] | [Rating/Details] | [Rating/Details] | [Rating/Details] |\n| [Criterion 2] | [Rating/Details] | [Rating/Details] | [Rating/Details] |\n\n## Detailed Analysis\n\n### Option A\n\n**Pros**:\n\n- [Advantage]\n- [Advantage]\n\n**Cons**:\n\n- [Disadvantage]\n- [Disadvantage]\n\n**Best for**: [Use case]\n\n**Source**: <mention-page url=\"...\">Source Page</mention-page>\n\n[Repeat for each option]\n\n## Recommendation\n\n**Selected option**: [Choice]\n\n**Rationale**: [Why this option is best given the context]\n\n## Sources\n\n[List all consulted pages]\n```\n",
        "skills/research-management/reference/comprehensive-report-format.md": "# Comprehensive Report Format\n\n**When to use**:\n\n- Formal documentation requirements\n- Strategic decision support\n- Complex topics requiring extensive analysis\n- Multiple stakeholders need alignment\n\n## Characteristics\n\n**Length**: 1500+ words\n\n**Structure**:\n\n- Executive summary\n- Background & context\n- Methodology\n- Detailed findings with subsections\n- Data & evidence section\n- Implications (short and long-term)\n- Prioritized recommendations\n- Appendix\n\n## Template\n\nSee [comprehensive-report-template.md](comprehensive-report-template.md) for the full template.\n\n## Best For\n\n- Deep analysis and strategic decisions\n- Formal documentation requirements\n- Complex topics with multiple facets\n- When stakeholders need extensive context\n- Board presentations or executive briefings\n\n## Example Use Cases\n\n- \"Create a comprehensive analysis of our market position\"\n- \"Document the full technical investigation of the database migration\"\n- \"Prepare an in-depth report on vendor options for executive review\"\n- \"Analyze the pros and cons of different architectural approaches\"\n",
        "skills/research-management/reference/comprehensive-report-template.md": "# Comprehensive Report Template\n\nUse for in-depth research requiring extensive analysis. See [comprehensive-report-format.md](comprehensive-report-format.md) for when to use this format.\n\n```markdown\n# [Report Title]\n\n## Executive Summary\n\n[One paragraph summarizing the entire report]\n\n## Background & Context\n\n[Why this research was conducted, what questions it addresses]\n\n## Methodology\n\n- Sources consulted: [number] Notion pages across [teamspaces]\n- Time period: [if relevant]\n- Scope: [what was included/excluded]\n\n## Key Findings\n\n### [Major Theme 1]\n\n**Summary**: [One sentence]\n\n**Details**:\n\n- [Supporting point with evidence]\n- [Supporting point with evidence]\n- [Supporting point with evidence]\n\n**Sources**: [Page mentions]\n\n### [Major Theme 2]\n\n[Repeat structure]\n\n## Data & Evidence\n\n[Tables, quotes, specific data points]\n\n## Implications\n\n### Short-term\n\n[Immediate implications]\n\n### Long-term\n\n[Strategic implications]\n\n## Recommendations\n\n### Priority 1: [High priority action]\n\n- **What**: [Specific action]\n- **Why**: [Rationale]\n- **How**: [Implementation approach]\n\n### Priority 2: [Medium priority action]\n\n[Repeat structure]\n\n## Appendix\n\n### Additional Resources\n\n- [Related pages]\n\n### Open Questions\n\n- [Unanswered questions for future research]\n```\n",
        "skills/research-management/reference/format-selection-guide.md": "# Format Selection Guide\n\nChoose the right output format for your research needs.\n\n## Decision Tree\n\n```\nIs this comparing multiple options?\n   YES  Use Comparison Format\n   NO \n\nIs this time-sensitive or simple?\n   YES  Use Quick Brief\n   NO \n\nDoes this require formal/extensive documentation?\n   YES  Use Comprehensive Report\n   NO  Use Research Summary (default)\n```\n\n## Format Overview\n\n| Format                                                 | Length         | When to Use                      | Template                                     |\n| ------------------------------------------------------ | -------------- | -------------------------------- | -------------------------------------------- |\n| [Research Summary](research-summary-format.md)         | 500-1000 words | Most research requests (default) | [Template](research-summary-template.md)     |\n| [Comprehensive Report](comprehensive-report-format.md) | 1500+ words    | Formal docs, strategic decisions | [Template](comprehensive-report-template.md) |\n| [Quick Brief](quick-brief-format.md)                   | 200-400 words  | Time-sensitive, simple topics    | [Template](quick-brief-template.md)          |\n| [Comparison](comparison-format.md)                     | 800-1200 words | Evaluating options               | [Template](comparison-template.md)           |\n\n## Formatting Guidelines\n\n### Headings\n\n- Use `#` for title\n- Use `##` for major sections\n- Use `###` for subsections\n- Keep heading hierarchy consistent\n\n### Lists\n\n- Use `-` for bullet points\n- Use `1.` for numbered lists\n- Keep list items parallel in structure\n\n### Emphasis\n\n- Use `**bold**` for key terms and section labels\n- Use `*italic*` for emphasis\n- Use sparingly for maximum impact\n\n### Citations\n\n- Always use `<mention-page url=\"...\">Page Title</mention-page>` for source pages\n- Include citation immediately after referenced information\n- Group all sources in a \"Sources\" section at the end\n\n### Tables\n\n- Use for structured data comparison\n- Keep columns to 3-5 for readability\n- Include header row\n- Align content appropriately\n\n### Code Blocks\n\nUse when including:\n\n- Technical specifications\n- Configuration examples\n- Command examples\n\n```\nExample code or configuration here\n```\n\n## Content Guidelines\n\n### Executive Summaries\n\n- Lead with the most important finding\n- Include 1-2 key implications\n- Make it standalone (reader gets value without reading further)\n- Target 2-3 sentences for summaries, 1 paragraph for reports\n\n### Key Findings\n\n- Start with a clear headline\n- Support with specific evidence\n- Include relevant data points or quotes\n- Cite source immediately\n- Focus on actionable insights\n\n### Recommendations\n\n- Make them specific and actionable\n- Explain the \"why\" behind each recommendation\n- Prioritize clearly (Priority 1, 2, 3 or High/Medium/Low)\n- Include implementation hints when relevant\n\n### Source Citations\n\n- Link to original pages using mentions\n- Note if information is outdated (check last-edited dates)\n- Credit specific sections when quoting\n- Group related sources together\n",
        "skills/research-management/reference/quick-brief-format.md": "# Quick Brief Format\n\n**When to use**:\n\n- Time-sensitive requests\n- Simple topics\n- Status updates\n- Quick reference needs\n\n## Characteristics\n\n**Length**: 200-400 words\n\n**Structure**:\n\n- 3-4 sentence summary\n- 3-5 bullet key points\n- Short action items list\n- Brief source list\n\n## Template\n\nSee [quick-brief-template.md](quick-brief-template.md) for the full template.\n\n## Best For\n\n- Fast turnaround requests\n- Simple, straightforward topics\n- Quick status updates\n- When time is more important than depth\n- Initial exploration before deeper research\n\n## Example Use Cases\n\n- \"Quick summary of what's in our API docs\"\n- \"Fast brief on the meeting notes from yesterday\"\n- \"What are the key points from that spec?\"\n- \"Give me a quick overview of the project status\"\n",
        "skills/research-management/reference/quick-brief-template.md": "# Quick Brief Template\n\nUse for fast turnaround requests or simple topics. See [quick-brief-format.md](quick-brief-format.md) for when to use this format.\n\n```markdown\n# [Topic] - Quick Brief\n\n**Date**: [Current date]\n\n## Summary\n\n[3-4 sentences covering the essentials]\n\n## Key Points\n\n- **Point 1**: [Details]\n- **Point 2**: [Details]\n- **Point 3**: [Details]\n\n## Action Items\n\n1. [Immediate next step]\n2. [Follow-up action]\n\n## Sources\n\n[Brief list of pages consulted]\n```\n",
        "skills/research-management/reference/research-summary-format.md": "# Research Summary Format\n\n**When to use**: General research requests, most common format\n\n## Characteristics\n\n**Length**: 500-1000 words typically\n\n**Structure**:\n\n- Executive summary (2-3 sentences)\n- 3-5 key findings with supporting evidence\n- Detailed analysis section\n- Conclusions and next steps\n- Source citations\n\n## Template\n\nSee [research-summary-template.md](research-summary-template.md) for the full template.\n\n## Best For\n\n- Most general-purpose research requests\n- Standard documentation needs\n- Balanced depth and readability\n- When you need comprehensive but accessible information\n\n## Example Use Cases\n\n- \"Research our authentication options\"\n- \"What does our project documentation say about the API redesign?\"\n- \"Summarize the team's discussion about mobile strategy\"\n- \"Compile information about our deployment process\"\n",
        "skills/research-management/reference/research-summary-template.md": "# Research Summary Template\n\nUse this for most research requests. See [research-summary-format.md](research-summary-format.md) for when to use this format.\n\n```markdown\n# [Topic Name]\n\n## Executive Summary\n\n[2-3 sentence overview of key findings and implications]\n\n## Key Findings\n\n### Finding 1: [Clear headline]\n\n[Details and supporting evidence]\n\n- Source: <mention-page url=\"...\">Original Page</mention-page>\n\n### Finding 2: [Clear headline]\n\n[Details and supporting evidence]\n\n- Source: <mention-page url=\"...\">Original Page</mention-page>\n\n### Finding 3: [Clear headline]\n\n[Details and supporting evidence]\n\n- Source: <mention-page url=\"...\">Original Page</mention-page>\n\n## Detailed Analysis\n\n### [Section 1]\n\n[In-depth discussion of first major theme]\n\n### [Section 2]\n\n[In-depth discussion of second major theme]\n\n## Conclusions\n\n[Summary of implications and insights]\n\n## Next Steps\n\n1. [Actionable recommendation]\n2. [Actionable recommendation]\n3. [Actionable recommendation]\n\n## Sources\n\n- <mention-page url=\"...\">Page Title</mention-page>\n- <mention-page url=\"...\">Page Title</mention-page>\n- <mention-page url=\"...\">Page Title</mention-page>\n```\n",
        "skills/review-recipient/SKILL.md": "---\nname: review-recipient\ndescription: Receive and process code review feedback effectively. Applies suggestions, addresses comments, and iterates on implementation improvements.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Code Review Reception\n\n## Overview\n\nCode review requires technical evaluation, not emotional performance.\n\n**Core principle:** Verify before implementing. Ask before assuming. Technical correctness over social comfort.\n\n## The Response Pattern\n\n```\nWHEN receiving code review feedback:\n\n1. READ: Complete feedback without reacting\n2. UNDERSTAND: Restate requirement in own words (or ask)\n3. VERIFY: Check against codebase reality\n4. EVALUATE: Technically sound for THIS codebase?\n5. RESPOND: Technical acknowledgment or reasoned pushback\n6. IMPLEMENT: One item at a time, test each\n```\n\n## Forbidden Responses\n\n**NEVER:**\n- \"You're absolutely right!\" (explicit CLAUDE.md violation)\n- \"Great point!\" / \"Excellent feedback!\" (performative)\n- \"Let me implement that now\" (before verification)\n\n**INSTEAD:**\n- Restate the technical requirement\n- Ask clarifying questions\n- Push back with technical reasoning if wrong\n- Just start working (actions > words)\n\n## Handling Unclear Feedback\n\n```\nIF any item is unclear:\n  STOP - do not implement anything yet\n  ASK for clarification on unclear items\n\nWHY: Items may be related. Partial understanding = wrong implementation.\n```\n\n**Example:**\n```\nyour human partner: \"Fix 1-6\"\nYou understand 1,2,3,6. Unclear on 4,5.\n\n WRONG: Implement 1,2,3,6 now, ask about 4,5 later\n RIGHT: \"I understand items 1,2,3,6. Need clarification on 4 and 5 before proceeding.\"\n```\n\n## Source-Specific Handling\n\n### From your human partner\n- **Trusted** - implement after understanding\n- **Still ask** if scope unclear\n- **No performative agreement**\n- **Skip to action** or technical acknowledgment\n\n### From External Reviewers\n```\nBEFORE implementing:\n  1. Check: Technically correct for THIS codebase?\n  2. Check: Breaks existing functionality?\n  3. Check: Reason for current implementation?\n  4. Check: Works on all platforms/versions?\n  5. Check: Does reviewer understand full context?\n\nIF suggestion seems wrong:\n  Push back with technical reasoning\n\nIF can't easily verify:\n  Say so: \"I can't verify this without [X]. Should I [investigate/ask/proceed]?\"\n\nIF conflicts with your human partner's prior decisions:\n  Stop and discuss with your human partner first\n```\n\n**your human partner's rule:** \"External feedback - be skeptical, but check carefully\"\n\n## YAGNI Check for \"Professional\" Features\n\n```\nIF reviewer suggests \"implementing properly\":\n  grep codebase for actual usage\n\n  IF unused: \"This endpoint isn't called. Remove it (YAGNI)?\"\n  IF used: Then implement properly\n```\n\n**your human partner's rule:** \"You and reviewer both report to me. If we don't need this feature, don't add it.\"\n\n## Implementation Order\n\n```\nFOR multi-item feedback:\n  1. Clarify anything unclear FIRST\n  2. Then implement in this order:\n     - Blocking issues (breaks, security)\n     - Simple fixes (typos, imports)\n     - Complex fixes (refactoring, logic)\n  3. Test each fix individually\n  4. Verify no regressions\n```\n\n## When To Push Back\n\nPush back when:\n- Suggestion breaks existing functionality\n- Reviewer lacks full context\n- Violates YAGNI (unused feature)\n- Technically incorrect for this stack\n- Legacy/compatibility reasons exist\n- Conflicts with your human partner's architectural decisions\n\n**How to push back:**\n- Use technical reasoning, not defensiveness\n- Ask specific questions\n- Reference working tests/code\n- Involve your human partner if architectural\n\n**Signal if uncomfortable pushing back out loud:** \"Strange things are afoot at the Circle K\"\n\n## Acknowledging Correct Feedback\n\nWhen feedback IS correct:\n```\n \"Fixed. [Brief description of what changed]\"\n \"Good catch - [specific issue]. Fixed in [location].\"\n [Just fix it and show in the code]\n\n \"You're absolutely right!\"\n \"Great point!\"\n \"Thanks for catching that!\"\n \"Thanks for [anything]\"\n ANY gratitude expression\n```\n\n**Why no thanks:** Actions speak. Just fix it. The code itself shows you heard the feedback.\n\n**If you catch yourself about to write \"Thanks\":** DELETE IT. State the fix instead.\n\n## Gracefully Correcting Your Pushback\n\nIf you pushed back and were wrong:\n```\n \"You were right - I checked [X] and it does [Y]. Implementing now.\"\n \"Verified this and you're correct. My initial understanding was wrong because [reason]. Fixing.\"\n\n Long apology\n Defending why you pushed back\n Over-explaining\n```\n\nState the correction factually and move on.\n\n## Common Mistakes\n\n| Mistake | Fix |\n|---------|-----|\n| Performative agreement | State requirement or just act |\n| Blind implementation | Verify against codebase first |\n| Batch without testing | One at a time, test each |\n| Assuming reviewer is right | Check if breaks things |\n| Avoiding pushback | Technical correctness > comfort |\n| Partial implementation | Clarify all items first |\n| Can't verify, proceed anyway | State limitation, ask for direction |\n\n## Real Examples\n\n**Performative Agreement (Bad):**\n```\nReviewer: \"Remove legacy code\"\n \"You're absolutely right! Let me remove that...\"\n```\n\n**Technical Verification (Good):**\n```\nReviewer: \"Remove legacy code\"\n \"Checking... build target is 10.15+, this API needs 13+. Need legacy for backward compat. Current impl has wrong bundle ID - fix it or drop pre-13 support?\"\n```\n\n**YAGNI (Good):**\n```\nReviewer: \"Implement proper metrics tracking with database, date filters, CSV export\"\n \"Grepped codebase - nothing calls this endpoint. Remove it (YAGNI)? Or is there usage I'm missing?\"\n```\n\n**Unclear Item (Good):**\n```\nyour human partner: \"Fix items 1-6\"\nYou understand 1,2,3,6. Unclear on 4,5.\n \"Understand 1,2,3,6. Need clarification on 4 and 5 before implementing.\"\n```\n\n## The Bottom Line\n\n**External feedback = suggestions to evaluate, not orders to follow.**\n\nVerify. Question. Then implement.\n\nNo performative agreement. Technical rigor always.\n",
        "skills/selection-randomizer/SKILL.md": "---\nname: selection-randomizer\ndescription: Randomly select winners and participants fairly. Implements randomized selection with optional weighting and fairness verification.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Raffle Winner Picker\n\nThis skill randomly selects winners from lists, spreadsheets, or Google Sheets for giveaways and contests.\n\n## When to Use This Skill\n\n- Running social media giveaways\n- Picking raffle winners at events\n- Randomly selecting participants for surveys or tests\n- Choosing winners from contest submissions\n- Fair distribution of limited spots or resources\n- Random team assignments\n\n## What This Skill Does\n\n1. **Random Selection**: Uses cryptographically random selection\n2. **Multiple Sources**: Works with CSV, Excel, Google Sheets, or plain lists\n3. **Multiple Winners**: Can pick one or multiple winners\n4. **Duplicate Prevention**: Ensures the same person can't win twice\n5. **Transparent Results**: Shows the selection process clearly\n6. **Winner Details**: Displays all relevant information about winners\n\n## How to Use\n\n### From Google Sheets\n\n```\nPick a random row from this Google Sheet to select a winner \nfor a giveaway: [Sheet URL]\n```\n\n### From Local File\n\n```\nPick 3 random winners from entries.csv\n```\n\n### From List\n\n```\nPick a random winner from this list:\n- Alice (alice@email.com)\n- Bob (bob@email.com)\n- Carol (carol@email.com)\n...\n```\n\n### Multiple Winners\n\n```\nPick 5 random winners from contest-entries.xlsx, \nmake sure no duplicates\n```\n\n## Example\n\n**User**: \"Pick a random row from this Google Sheet to select a winner for a giveaway.\"\n\n**Output**:\n```\nAccessing Google Sheet...\nTotal entries found: 247\n\nRandomly selecting winner...\n\n WINNER SELECTED! \n\nRow #142\nName: Sarah Johnson\nEmail: sarah.j@email.com\nEntry Date: March 10, 2024\nComment: \"Love your newsletter!\"\n\nSelection method: Cryptographically random\nTimestamp: 2024-03-15 14:32:18 UTC\n\nWould you like to:\n- Pick another winner (excluding Sarah)?\n- Export winner details?\n- Pick runner-ups?\n```\n\n**Inspired by:** Lenny's use case - picking a Sora 2 giveaway winner from his subscriber Slack community\n\n## Features\n\n### Fair Selection\n- Uses secure random number generation\n- No bias or patterns\n- Transparent process\n- Repeatable with seed (for verification)\n\n### Exclusions\n```\nPick a random winner excluding previous winners: \nAlice, Bob, Carol\n```\n\n### Weighted Selection\n```\nPick a winner with weighted probability based on \nthe \"entries\" column (1 entry = 1 ticket)\n```\n\n### Runner-ups\n```\nPick 1 winner and 3 runner-ups from the list\n```\n\n## Example Workflows\n\n### Social Media Giveaway\n1. Export entries from Google Form to Sheets\n2. \"Pick a random winner from [Sheet URL]\"\n3. Verify winner details\n4. Announce publicly with timestamp\n\n### Event Raffle\n1. Create CSV of attendee names and emails\n2. \"Pick 10 random winners from attendees.csv\"\n3. Export winner list\n4. Email winners directly\n\n### Team Assignment\n1. Have list of participants\n2. \"Randomly split this list into 4 equal teams\"\n3. Review assignments\n4. Share team rosters\n\n## Tips\n\n- **Document the process**: Save the timestamp and method\n- **Public announcement**: Share selection details for transparency\n- **Check eligibility**: Verify winner meets contest rules\n- **Have backups**: Pick runner-ups in case winner is ineligible\n- **Export results**: Save winner list for records\n\n## Privacy & Fairness\n\n Uses cryptographically secure randomness\n No manipulation possible\n Timestamp recorded for verification\n Can provide seed for third-party verification\n Respects data privacy\n\n## Common Use Cases\n\n- Newsletter subscriber giveaways\n- Product launch raffles\n- Conference ticket drawings\n- Beta tester selection\n- Focus group participant selection\n- Random prize distribution at events\n\n",
        "skills/specification-executor/SKILL.md": "---\nname: specification-executor\ndescription: Execute specifications from workspace platforms into implementation. Transforms documented specifications into actionable development tasks.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Spec to Implementation\n\nTransforms specifications into actionable implementation plans with progress tracking. Fetches spec documents, extracts requirements, breaks down into tasks, and manages implementation workflow.\n\n## Quick Start\n\nWhen asked to implement a specification:\n\n1. **Find spec**: Use `Notion:notion-search` to locate specification page\n2. **Fetch spec**: Use `Notion:notion-fetch` to read specification content\n3. **Extract requirements**: Parse and structure requirements from spec\n4. **Create plan**: Use `Notion:notion-create-pages` for implementation plan\n5. **Find task database**: Use `Notion:notion-search` to locate tasks database\n6. **Create tasks**: Use `Notion:notion-create-pages` for individual tasks in task database\n7. **Track progress**: Use `Notion:notion-update-page` to log progress and update status\n\n## Implementation Workflow\n\n### Step 1: Find the specification\n\n```\n1. Search for spec:\n   - Use Notion:notion-search with spec name or topic\n   - Apply filters if needed (e.g., created_date_range, teamspace_id)\n   - Look for spec title or keyword matches\n   - If not found or ambiguous, ask user for spec URL/ID\n\nExample searches:\n- \"User Authentication spec\"\n- \"Payment Integration specification\"\n- \"Mobile App Redesign PRD\"\n```\n\n### Step 2: Fetch and analyze specification\n\n```\n1. Fetch spec page:\n   - Use Notion:notion-fetch with spec URL/ID from search results\n   - Read full content including requirements, design, constraints\n\n2. Parse specification:\n   - Identify functional requirements\n   - Note non-functional requirements (performance, security, etc.)\n   - Extract acceptance criteria\n   - Identify dependencies and blockers\n```\n\nSee [reference/spec-parsing.md](reference/spec-parsing.md) for parsing patterns.\n\n### Step 3: Create implementation plan\n\n```\n1. Break down into phases/milestones\n2. Identify technical approach\n3. List required tasks\n4. Estimate effort\n5. Identify risks\n\nUse implementation plan template (see [reference/standard-implementation-plan.md](reference/standard-implementation-plan.md) or [reference/quick-implementation-plan.md](reference/quick-implementation-plan.md))\n```\n\n### Step 4: Create implementation plan page\n\n```\nUse Notion:notion-create-pages:\n- Title: \"Implementation Plan: [Feature Name]\"\n- Content: Structured plan with phases, tasks, timeline\n- Link back to original spec\n- Add to appropriate location (project page, database)\n```\n\n### Step 5: Find task database\n\n```\n1. Search for task database:\n   - Use Notion:notion-search to find \"Tasks\" or \"Task Management\" database\n   - Look for engineering/project task tracking system\n   - If not found or ambiguous, ask user for database location\n\n2. Fetch database schema:\n   - Use Notion:notion-fetch with database URL/ID\n   - Get property names, types, and options\n   - Identify correct data source from <data-source> tags\n   - Note required properties for new tasks\n```\n\n### Step 6: Create implementation tasks\n\n```\nFor each task in plan:\n1. Create task in task database using Notion:notion-create-pages\n2. Use parent: { data_source_id: 'collection://...' }\n3. Set properties from schema:\n   - Name/Title: Task description\n   - Status: To Do\n   - Priority: Based on criticality\n   - Related Tasks: Link to spec and plan\n4. Add implementation details in content\n```\n\nSee [reference/task-creation.md](reference/task-creation.md) for task patterns.\n\n### Step 7: Begin implementation\n\n```\n1. Update task status to \"In Progress\"\n2. Add initial progress note\n3. Document approach and decisions\n4. Link relevant resources\n```\n\n### Step 8: Track progress\n\n```\nRegular updates:\n1. Update task properties (status, progress)\n2. Add progress notes with:\n   - What's completed\n   - Current focus\n   - Blockers/issues\n3. Update implementation plan with milestone completion\n4. Link to related work (PRs, designs, etc.)\n```\n\nSee [reference/progress-tracking.md](reference/progress-tracking.md) for tracking patterns.\n\n## Spec Analysis Patterns\n\n**Functional Requirements**: User stories, feature descriptions, workflows, data requirements, integration points\n\n**Non-Functional Requirements**: Performance targets, security requirements, scalability needs, availability, compliance\n\n**Acceptance Criteria**: Testable conditions, user validation points, performance benchmarks, completion definitions\n\nSee [reference/spec-parsing.md](reference/spec-parsing.md) for detailed parsing techniques.\n\n## Implementation Plan Structure\n\n**Plan includes**: Overview  Linked Spec  Requirements Summary  Technical Approach  Implementation Phases (Goal, Tasks checklist, Estimated effort)  Dependencies  Risks & Mitigation  Timeline  Success Criteria\n\nSee [reference/standard-implementation-plan.md](reference/standard-implementation-plan.md) for full plan template.\n\n## Task Breakdown Patterns\n\n**By Component**: Database, API endpoints, frontend components, integration, testing\n**By Feature Slice**: Vertical slices (auth flow, data entry, report generation)\n**By Priority**: P0 (must have), P1 (important), P2 (nice to have)\n\n## Progress Logging\n\n**Daily Updates** (active work): Add progress note with completed items, current focus, blockers\n**Milestone Updates** (major progress): Update plan checkboxes, add milestone summary, adjust timeline\n**Status Changes** (task transitions): Update properties (In Progress  In Review  Done), add completion notes, link deliverables\n\n**Progress Format**: Date heading  Completed  In Progress  Next Steps  Blockers  Notes\n\nSee [reference/progress-tracking.md](reference/progress-tracking.md) for detailed patterns.\n\n## Linking Spec to Implementation\n\n**Forward Links**: Update spec page with \"Implementation\" section linking to plan and tasks\n**Backward Links**: Reference spec in plan and tasks with \"Specification\" section\n**Bidirectional Traceability**: Maintain both directions for easy tracking\n\n## Implementation Status Tracking\n\n**Plan Status**: Update with phase completion ( Complete,  In Progress %,  Not Started) and overall percentage\n**Task Aggregation**: Query task database by plan ID to generate summary (complete, in progress, blocked, not started)\n\n## Handling Spec Changes\n\n**Detection**: Fetch updated spec  compare with plan  identify new requirements  assess impact\n**Propagation**: Update plan  create new tasks  update affected tasks  add change note  notify via comments\n**Change Log**: Track spec evolution with date, what changed, and impact\n\n## Common Patterns\n\n**Feature Flag**: Backend (behind flag)  Testing  Frontend (flagged)  Internal rollout  External rollout\n**Database Migration**: Schema design  Migration script  Staging test  Production migration  Validation\n**API Development**: API design  Backend implementation  Testing & docs  Client integration  Deployment\n\n## Best Practices\n\n1. **Always link spec and implementation**: Maintain bidirectional references\n2. **Break down into small tasks**: Each task should be completable in 1-2 days\n3. **Extract clear acceptance criteria**: Know when \"done\" is done\n4. **Identify dependencies early**: Note blockers in plan\n5. **Update progress regularly**: Daily notes for active work\n6. **Track changes**: Document spec updates and their impact\n7. **Use checklists**: Visual progress indicators help everyone\n8. **Link deliverables**: PRs, designs, docs should link back to tasks\n\n## Advanced Features\n\nFor additional implementation patterns and techniques, see the reference files in [reference/](reference/).\n\n## Common Issues\n\n**\"Can't find spec\"**: Use Notion:notion-search with spec name/topic, try broader terms, or ask user for URL\n**\"Multiple specs found\"**: Ask user which spec to implement or show options\n**\"Can't find task database\"**: Search for \"Tasks\" or \"Task Management\", or ask user for database location\n**\"Spec unclear\"**: Note ambiguities in plan, create clarification tasks\n**\"Requirements conflicting\"**: Document conflicts, create decision task\n**\"Scope too large\"**: Break into smaller specs/phases\n\n## Examples\n\nSee [examples/](examples/) for complete workflows:\n\n- [examples/api-feature.md](examples/api-feature.md) - API feature implementation\n- [examples/ui-component.md](examples/ui-component.md) - Frontend component\n- [examples/database-migration.md](examples/database-migration.md) - Schema changes\n",
        "skills/specification-executor/evaluations/README.md": "# Spec to Implementation Skill Evaluations\n\nEvaluation scenarios for testing the Spec to Implementation skill across different Claude models.\n\n## Purpose\n\nThese evaluations ensure the Spec to Implementation skill:\n\n- Finds and parses specification pages accurately\n- Breaks down specs into actionable implementation plans\n- Creates tasks that Claude can implement with clear acceptance criteria\n- Tracks progress and updates implementation status\n- Works consistently across Haiku, Sonnet, and Opus\n\n## Evaluation Files\n\n### basic-spec-implementation.json\n\nTests basic workflow of turning a spec into an implementation plan.\n\n**Scenario**: Implement user authentication feature from spec  \n**Key Behaviors**:\n\n- Searches for and finds the authentication spec page\n- Fetches spec and extracts requirements\n- Parses requirements into phases (setup, core features, polish)\n- Creates implementation plan page linked to original spec\n- Breaks down into clear phases with deliverables\n- Includes timeline and dependencies\n\n### spec-to-tasks.json\n\nTests creating concrete tasks from a specification in a task database.\n\n**Scenario**: Create tasks from API redesign spec  \n**Key Behaviors**:\n\n- Finds spec page in Notion\n- Extracts specific requirements and acceptance criteria\n- Searches for or creates task database\n- Fetches task database schema\n- Creates multiple tasks with proper properties (Status, Priority, Sprint, etc.)\n- Each task has clear title, description, and acceptance criteria\n- Tasks have dependencies where appropriate\n- Links all tasks back to original spec\n\n## Running Evaluations\n\n1. Enable the `spec-to-implementation` skill\n2. Submit the query from the evaluation file\n3. Verify the skill finds the spec page via search\n4. Check that requirements are accurately parsed\n5. Confirm implementation plan is created with phases\n6. Verify tasks have clear, implementable acceptance criteria\n7. Check that tasks link back to spec\n8. Test with Haiku, Sonnet, and Opus\n\n## Expected Skill Behaviors\n\nSpec to Implementation evaluations should verify:\n\n### Spec Discovery & Parsing\n\n- Searches Notion for specification pages\n- Fetches complete spec content\n- Extracts all requirements accurately\n- Identifies technical dependencies\n- Understands acceptance criteria\n- Notes any ambiguities or missing details\n\n### Implementation Planning\n\n- Creates implementation plan page\n- Breaks work into logical phases:\n  - Phase 1: Foundation/Setup\n  - Phase 2: Core Implementation\n  - Phase 3: Testing & Polish\n- Includes timeline estimates\n- Identifies dependencies between phases\n- Links back to original spec\n\n### Task Creation\n\n- Finds or identifies task database\n- Fetches database schema for property names\n- Creates tasks with correct properties\n- Each task has:\n  - Clear, specific title\n  - Context and description\n  - Acceptance criteria (checklist format)\n  - Appropriate priority and status\n  - Link to spec page\n- Tasks are right-sized (not too big, not too small)\n- Dependencies between tasks are noted\n\n### Progress Tracking\n\n- Implementation plan includes progress markers\n- Tasks can be updated as work progresses\n- Status updates link to completed work\n- Blockers or changes are noted\n\n## Creating New Evaluations\n\nWhen adding Spec to Implementation evaluations:\n\n1. **Test different spec types** - Features, migrations, refactors, API changes, UI components\n2. **Vary complexity** - Simple 1-phase vs. complex multi-phase implementations\n3. **Test task granularity** - Does it create appropriately-sized tasks?\n4. **Include edge cases** - Vague specs, conflicting requirements, missing details\n5. **Test database integration** - Creating tasks in existing task databases with various schemas\n6. **Progress tracking** - Updating implementation plans as tasks complete\n\n## Example Success Criteria\n\n**Good** (specific, testable):\n\n- \"Searches Notion for spec page using feature name\"\n- \"Creates implementation plan with 3 phases: Setup  Core  Polish\"\n- \"Creates 5-8 tasks in task database with properties: Task (title), Status, Priority, Sprint\"\n- \"Each task has acceptance criteria in checklist format (- [ ] ...)\"\n- \"Tasks link back to spec using mention-page tag\"\n- \"Task titles are specific and actionable (e.g., 'Create login API endpoint' not 'Authentication')\"\n\n**Bad** (vague, untestable):\n\n- \"Creates good implementation plan\"\n- \"Tasks are well-structured\"\n- \"Breaks down spec appropriately\"\n- \"Links to spec\"\n",
        "skills/specification-executor/examples/api-feature.md": "# Example: API Feature Implementation\n\n## User Request\n\n> \"Create an implementation plan for the User Profile API spec\"\n\n## Workflow Execution\n\n### Step 1: Fetch Specification\n\n```\nNotion:notion-search\nquery: \"User Profile API spec\"\nquery_type: \"internal\"\n```\n\nFound: \"User Profile API Specification\" in Engineering Specs\n\n```\nNotion:notion-fetch\nid: \"user-profile-api-spec-page-id\"\n```\n\n### Step 2: Parse Specification\n\n**Spec Content Extracted:**\n\n```markdown\n# User Profile API Specification\n\n## Overview\n\nRESTful API for user profile management\n\n## Requirements\n\n### Functional\n\n- FR-1: Get user profile by ID\n- FR-2: Update user profile (name, bio, avatar)\n- FR-3: Upload profile avatar\n- FR-4: Get user's public profile (limited fields)\n- FR-5: Search users by name\n\n### Non-Functional\n\n- NFR-1: Response time < 200ms (p95)\n- NFR-2: Support 1000 concurrent users\n- NFR-3: Avatar upload < 5MB\n- NFR-4: GDPR compliant (data portability)\n\n## API Endpoints\n\nGET /api/v1/users/:id\nPUT /api/v1/users/:id\nPOST /api/v1/users/:id/avatar\nGET /api/v1/users/:id/public\nGET /api/v1/users/search\n\n## Data Model\n\n- id (UUID)\n- email (string, unique)\n- name (string)\n- bio (text, max 500 chars)\n- avatar_url (string)\n- created_at (timestamp)\n- updated_at (timestamp)\n\n## Security\n\n- Authentication: JWT bearer token\n- Authorization: Users can only update own profile\n- Rate limiting: 100 req/min per user\n\n## Acceptance Criteria\n\n- AC-1: All endpoints return proper HTTP status codes\n- AC-2: Validation errors return 400 with error details\n- AC-3: Unauthorized access returns 401\n- AC-4: Rate limit exceeded returns 429\n- AC-5: Avatar images stored in S3\n```\n\n### Step 3: Create Implementation Plan\n\n```\nNotion:notion-create-pages\n\nparent: { page_id: \"engineering-plans-parent-id\" }\n\npages: [{\n  properties: {\n    \"title\": \"Implementation Plan: User Profile API\"\n  },\n  content: \"[Implementation plan]\"\n}]\n```\n\n**Implementation Plan Created:**\n\n```markdown\n# Implementation Plan: User Profile API\n\n## Overview\n\nBuild RESTful API for user profile management with CRUD operations, avatar upload, and search functionality.\n\n## Linked Specification\n\n<mention-page url=\"...\">User Profile API Specification</mention-page>\n\n## Requirements Summary\n\n### Functional Requirements\n\n-  Get user profile (authenticated)\n-  Update profile fields (name, bio)\n-  Upload avatar with image processing\n-  Public profile view (limited fields)\n-  User search by name\n\n### Non-Functional Requirements\n\n- **Performance**: < 200ms response time (p95)\n- **Scalability**: 1000 concurrent users\n- **Storage**: Avatar files < 5MB to S3\n- **Compliance**: GDPR data portability\n\n### Acceptance Criteria\n\n- All endpoints with proper status codes\n- Input validation with error details\n- JWT authentication required\n- Rate limiting enforced\n- Avatars stored in S3\n\n## Technical Approach\n\n### Architecture\n\n- **Framework**: Express.js (Node.js)\n- **Database**: PostgreSQL\n- **Storage**: AWS S3 for avatars\n- **Cache**: Redis for profile data\n- **Search**: PostgreSQL full-text search\n\n### Key Design Decisions\n\n1. **JWT Authentication**: Stateless auth, scales horizontally\n2. **S3 for Avatars**: Offload storage, CDN integration ready\n3. **Redis Caching**: Reduce DB load for frequently accessed profiles\n4. **Rate Limiting**: Token bucket algorithm, per-user limits\n\n## Implementation Phases\n\n### Phase 1: Foundation (Days 1-2)\n\n**Goal**: Set up core infrastructure\n\n**Tasks**:\n\n- [ ] <mention-page url=\"...\">Setup database schema</mention-page>\n- [ ] <mention-page url=\"...\">Configure S3 bucket</mention-page>\n- [ ] <mention-page url=\"...\">Setup Redis cache</mention-page>\n- [ ] <mention-page url=\"...\">Create API scaffolding</mention-page>\n\n**Deliverables**: Working skeleton with DB, storage, cache ready  \n**Estimated effort**: 2 days\n\n### Phase 2: Core Endpoints (Days 3-5)\n\n**Goal**: Implement main CRUD operations\n\n**Tasks**:\n\n- [ ] <mention-page url=\"...\">Implement GET user profile</mention-page>\n- [ ] <mention-page url=\"...\">Implement PUT update profile</mention-page>\n- [ ] <mention-page url=\"...\">Add input validation</mention-page>\n- [ ] <mention-page url=\"...\">Add JWT authentication middleware</mention-page>\n- [ ] <mention-page url=\"...\">Implement rate limiting</mention-page>\n\n**Deliverables**: Working CRUD operations with auth  \n**Estimated effort**: 3 days\n\n### Phase 3: Avatar Upload (Days 6-7)\n\n**Goal**: Avatar management with S3\n\n**Tasks**:\n\n- [ ] <mention-page url=\"...\">Implement avatar upload endpoint</mention-page>\n- [ ] <mention-page url=\"...\">Add image validation (size, format)</mention-page>\n- [ ] <mention-page url=\"...\">Process and resize images</mention-page>\n- [ ] <mention-page url=\"...\">Upload to S3 with signed URLs</mention-page>\n\n**Deliverables**: Avatar upload/update functionality  \n**Estimated effort**: 2 days\n\n### Phase 4: Search & Public Profile (Days 8-9)\n\n**Goal**: Complete remaining features\n\n**Tasks**:\n\n- [ ] <mention-page url=\"...\">Implement user search</mention-page>\n- [ ] <mention-page url=\"...\">Implement public profile endpoint</mention-page>\n- [ ] <mention-page url=\"...\">Add search indexing</mention-page>\n- [ ] <mention-page url=\"...\">Optimize search queries</mention-page>\n\n**Deliverables**: Search and public profiles working  \n**Estimated effort**: 2 days\n\n### Phase 5: Testing & Optimization (Days 10-12)\n\n**Goal**: Production-ready quality\n\n**Tasks**:\n\n- [ ] <mention-page url=\"...\">Write unit tests</mention-page>\n- [ ] <mention-page url=\"...\">Write integration tests</mention-page>\n- [ ] <mention-page url=\"...\">Performance testing</mention-page>\n- [ ] <mention-page url=\"...\">Security audit</mention-page>\n- [ ] <mention-page url=\"...\">API documentation</mention-page>\n\n**Deliverables**: Tested, documented, production-ready API  \n**Estimated effort**: 3 days\n\n## Dependencies\n\n### External Dependencies\n\n- AWS S3 bucket created \n- Redis instance available \n- PostgreSQL database provisioned \n\n### Internal Dependencies\n\n- JWT authentication service (exists)\n- User database table (exists)\n- Logging infrastructure (exists)\n\n### Blockers\n\nNone currently\n\n## Risks & Mitigation\n\n### Risk 1: Image Processing Performance\n\n- **Probability**: Medium\n- **Impact**: Medium\n- **Mitigation**: Use background job queue for processing, return signed upload URL immediately\n\n### Risk 2: S3 Upload Failures\n\n- **Probability**: Low\n- **Impact**: Medium\n- **Mitigation**: Implement retry logic with exponential backoff, fallback to local storage temporarily\n\n### Risk 3: Rate Limiting Complexity\n\n- **Probability**: Low\n- **Impact**: Low\n- **Mitigation**: Use proven library (express-rate-limit with Redis store)\n\n### Risk 4: Search Performance\n\n- **Probability**: Medium\n- **Impact**: Medium\n- **Mitigation**: Add database indexes, consider Elasticsearch if needed later\n\n## Timeline\n\n| Milestone             | Target Date | Status     |\n| --------------------- | ----------- | ---------- |\n| Phase 1 Complete      | Oct 16      |  Planned |\n| Phase 2 Complete      | Oct 19      |  Planned |\n| Phase 3 Complete      | Oct 21      |  Planned |\n| Phase 4 Complete      | Oct 23      |  Planned |\n| Phase 5 Complete      | Oct 26      |  Planned |\n| **Production Deploy** | **Oct 28**  |  Planned |\n\n**Total Duration**: 12 working days (~2.5 weeks)\n\n## Success Criteria\n\n### Technical Success\n\n- [ ] All 5 endpoints implemented and working\n- [ ] Response time < 200ms (p95) verified in load testing\n- [ ] Handles 1000 concurrent users\n- [ ] All acceptance criteria met\n- [ ] Test coverage > 80%\n- [ ] Security scan passed\n- [ ] API documentation complete\n\n### Business Success\n\n- [ ] User profile updates functional\n- [ ] Avatar uploads working reliably\n- [ ] Search returns relevant results in < 500ms\n- [ ] Zero critical bugs in first week\n\n## Resources\n\n### Documentation\n\n- <mention-page url=\"...\">User Profile API Specification</mention-page> (original spec)\n- <mention-page url=\"...\">Authentication Service Docs</mention-page>\n- <mention-page url=\"...\">AWS S3 Setup Guide</mention-page>\n\n### Related Work\n\n- <mention-page url=\"...\">User Authentication API</mention-page> (similar pattern)\n- <mention-page url=\"...\">File Upload Service</mention-page> (avatar upload reference)\n\n### External References\n\n- Express.js best practices\n- AWS S3 SDK documentation\n- PostgreSQL full-text search guide\n\n## Progress Tracking\n\n### Phase Status\n\n- Phase 1:  Not Started\n- Phase 2:  Not Started\n- Phase 3:  Not Started\n- Phase 4:  Not Started\n- Phase 5:  Not Started\n\n**Overall Progress**: 0% complete\n\n### Latest Update\n\n_Implementation plan created on October 14, 2025_\n```\n\n### Step 4: Find Task Database\n\n```\nNotion:notion-search\nquery: \"Tasks database\"\nquery_type: \"internal\"\n```\n\nFound: \"Engineering Tasks\" database\n\n### Step 5: Fetch Task Database Schema\n\n```\nNotion:notion-fetch\nid: \"tasks-database-id\"\n```\n\n**Schema retrieved:**\n\n- Data source: `collection://tasks-db-uuid`\n- Properties: Name (title), Status (select), Priority (select), Related Tasks (relation), Story Points (number), Tags (multi_select)\n\n### Step 6: Create Implementation Tasks\n\nNow create tasks for Phase 1:\n\n**Task 1: Setup database schema**\n\n````\nNotion:notion-create-pages\n\nparent: { data_source_id: \"collection://tasks-db-uuid\" }\n\npages: [{\n  properties: {\n    \"Name\": \"Setup database schema for User Profile API\",\n    \"Status\": \"To Do\",\n    \"Priority\": \"High\",\n    \"Related Tasks\": [\"impl-plan-page-id\", \"spec-page-id\"],\n    \"Story Points\": 3,\n    \"Tags\": \"backend, database, api\"\n  },\n  content: \"## Context\\nImplementation task for <mention-page url=\\\"...\\\">User Profile API Specification</mention-page>\\n\\nPart of <mention-page url=\\\"...\\\">Implementation Plan: User Profile API</mention-page> - Phase 1\\n\\n## Objective\\nCreate database schema for user profile storage\\n\\n## Requirements\\nBased on spec data model:\\n- id (UUID, primary key)\\n- email (string, unique index)\\n- name (string, not null)\\n- bio (text, max 500 chars)\\n- avatar_url (string, nullable)\\n- created_at (timestamp)\\n- updated_at (timestamp)\\n\\n## Acceptance Criteria\\n- [ ] Migration file created\\n- [ ] Schema includes all required fields\\n- [ ] Indexes on email (unique) and name (search)\\n- [ ] Constraints validated (bio length, email format)\\n- [ ] Migration tested on dev database\\n- [ ] Rollback migration created\\n\\n## Technical Approach\\n```sql\\nCREATE TABLE user_profiles (\\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\\n  email VARCHAR(255) UNIQUE NOT NULL,\\n  name VARCHAR(255) NOT NULL,\\n  bio TEXT CHECK (length(bio) <= 500),\\n  avatar_url TEXT,\\n  created_at TIMESTAMP DEFAULT NOW(),\\n  updated_at TIMESTAMP DEFAULT NOW()\\n);\\n\\nCREATE INDEX idx_user_profiles_email ON user_profiles(email);\\nCREATE INDEX idx_user_profiles_name ON user_profiles USING gin(to_tsvector('english', name));\\n```\\n\\n## Dependencies\\n- Blocked By: None\\n- Blocks: All Phase 2 tasks\\n\\n## Estimated Effort\\n3 story points (half day)\\n\"\n}]\n````\n\n_[Create similar tasks for all phases - 20 tasks total]_\n\n### Step 7: Link Plan Back to Spec\n\n```\nNotion:notion-update-page\npage_id: \"user-profile-api-spec-page-id\"\ncommand: \"insert_content_after\"\nselection_with_ellipsis: \"## Acceptance Criteria...\"\nnew_str: \"\n\n---\n\n## Implementation\n\n**Implementation Plan**: <mention-page url=\\\"...\\\">Implementation Plan: User Profile API</mention-page>\n\n**Implementation Tasks**: See plan for full task breakdown (20 tasks across 5 phases)\n\n**Status**: Planning complete, ready to start implementation\n\"\n```\n\n## Summary Provided to User\n\n```markdown\n## Implementation Plan Created \n\n### Plan Overview\n\n- **Feature**: User Profile API\n- **Duration**: 12 days (~2.5 weeks)\n- **Phases**: 5 (Foundation  Core  Avatar  Search  Testing)\n- **Tasks**: 20 tasks created\n- **Target Launch**: October 28, 2025\n\n### Implementation Phases\n\n**Phase 1: Foundation** (2 days)\n\n- Database schema\n- S3 and Redis setup\n- API scaffolding\n\n**Phase 2: Core Endpoints** (3 days)\n\n- GET/PUT user profile\n- Authentication & validation\n- Rate limiting\n\n**Phase 3: Avatar Upload** (2 days)\n\n- Image upload & validation\n- S3 integration\n- Image processing\n\n**Phase 4: Search & Public Profile** (2 days)\n\n- User search\n- Public profile endpoint\n\n**Phase 5: Testing & Optimization** (3 days)\n\n- Unit & integration tests\n- Performance testing\n- Documentation\n\n### Key Deliverables\n\n- 5 REST API endpoints\n- Avatar upload to S3\n- User search functionality\n- Comprehensive tests\n- API documentation\n\n### Links Created\n\n Plan page: <mention-page url=\"...\">Implementation Plan: User Profile API</mention-page>\n Spec updated with plan link\n 20 tasks created in task database\n All tasks linked to plan and spec\n\n### Next Steps\n\n1. Review and approve plan\n2. Assign tasks to team members\n3. Begin Phase 1 (Foundation)\n4. Daily standups for progress tracking\n\n**Ready to start implementation! **\n```\n\n## Key Features Demonstrated\n\n### Spec Parsing\n\n- Extracted requirements (functional & non-functional)\n- Identified API endpoints\n- Noted data model\n- Captured acceptance criteria\n- Understood security requirements\n\n### Implementation Planning\n\n- Broke into logical phases\n- Sequenced work appropriately (foundation  features  testing)\n- Identified dependencies\n- Estimated effort per phase\n- Created realistic timeline\n\n### Task Creation\n\n- Generated 20 specific tasks\n- Each task has context, acceptance criteria, technical approach\n- Tasks link to both spec and plan\n- Proper dependencies noted\n\n### Bidirectional Linking\n\n- Plan links to spec\n- Spec updated to link to plan\n- Tasks link to both\n- Easy navigation between all artifacts\n\nPerfect for: Feature implementation, API development, technical projects\n",
        "skills/specification-executor/examples/database-migration.md": "# Example: Database Migration Implementation\n\n**User Request**: \"Plan and implement the database migration for user preferences schema\"\n\n## Workflow\n\n### 1. Find & Fetch Spec\n\n```\nNotion:notion-search  Found \"User Preferences Schema Migration Spec\"\nNotion:notion-fetch  Extracted requirements\n```\n\n**Spec Summary**: Migrate from JSON blob to structured schema for better performance and data integrity.\n\n### 2. Parse Requirements\n\n- **Current**: JSONB preferences column\n- **Target**: Separate `user_preferences` and `notification_preferences` tables\n- **Must maintain**: Backward compatibility during migration\n- **Performance**: Support 1M+ users with zero downtime\n\n### 3. Create Implementation Plan\n\n```\nNotion:notion-create-pages\npages: [{\n  properties: {\n    title: \"Implementation Plan: User Preferences Migration\"\n  },\n  content: \"[Full implementation plan with phases]\"\n}]\n```\n\n**Plan included**:\n\n- **Phase 1**: Create new tables with indexes\n- **Phase 2**: Backfill data from JSONB\n- **Phase 3**: Dual-write mode (both old and new)\n- **Phase 4**: Switch reads to new schema\n- **Phase 5**: Drop old JSONB column\n\n### 4. Find Task Database & Create Tasks\n\n```\nNotion:notion-search  Found \"Engineering Tasks\" database\nNotion:notion-fetch  Got schema (Task, Status, Priority, Assignee, etc.)\n\nNotion:notion-create-pages\nparent: { data_source_id: \"collection://xyz\" }\npages: [\n  {\n    properties: {\n      \"Task\": \"Write migration SQL scripts\",\n      \"Status\": \"To Do\",\n      \"Priority\": \"High\",\n      \"Sprint\": \"Sprint 25\"\n    },\n    content: \"## Context\\nPart of User Preferences Migration...\\n\\n## Acceptance Criteria\\n- [ ] Migration script creates tables\\n- [ ] Indexes defined...\"\n  },\n  // ... 4 more tasks\n]\n```\n\n**Tasks created**:\n\n1. Write migration SQL scripts\n2. Implement backfill job\n3. Add dual-write logic to API\n4. Update read queries\n5. Rollback plan & monitoring\n\n### 5. Track Progress\n\nRegular updates to implementation plan with status, blockers, and completion notes.\n\n## Key Outputs\n\n**Implementation Plan Page** (linked to spec)\n**5 Tasks in Database** (with dependencies, acceptance criteria)\n**Progress Tracking** (updated as work progresses)\n\n## Success Factors\n\n- Broke down complex migration into clear phases\n- Created tasks with specific acceptance criteria\n- Established dependencies (Phase 1  2  3  4  5)\n- Zero-downtime approach with rollback plan\n- Linked all work back to original spec\n",
        "skills/specification-executor/examples/ui-component.md": "# Example: UI Component Implementation\n\n**User Request**: \"Implement the new search component from the design spec\"\n\n## Workflow\n\n### 1. Find Spec\n\n```\nNotion:notion-search\nquery: \"search component design spec\"\n```\n\nFound: \"Global Search Redesign Spec\"\n\n### 2. Fetch & Parse Spec\n\n```\nNotion:notion-fetch\n```\n\n**Requirements**:\n\n- Instant search with debounced API calls\n- Keyboard navigation ( arrows, Enter, Esc)\n- Recent searches history\n- Result categories (pages, people, files)\n- Mobile responsive\n- Accessibility (ARIA labels, screen reader)\n\n### 3. Create Implementation Plan\n\n```\nNotion:notion-create-pages\n```\n\n**Plan phases**:\n\n- Phase 1: Base component structure\n- Phase 2: API integration & debouncing\n- Phase 3: Keyboard navigation\n- Phase 4: Recent searches\n- Phase 5: Polish & accessibility\n\n### 4. Create Tasks\n\n```\nNotion:notion-create-pages (7 tasks)\n```\n\n**Tasks**:\n\n1. Create SearchInput component\n2. Implement useDebounce hook\n3. Add keyboard navigation\n4. LocalStorage for recent searches\n5. Result categorization UI\n6. Accessibility audit\n7. Mobile responsive styling\n\n### 5. Implement & Track\n\nAs each task completed, updated status and added progress notes with screenshots and implementation details.\n\n## Key Outputs\n\n**Implementation Plan** (linked to design spec)\n**7 Component Tasks** (in Engineering Tasks database)\n**Progress Updates** (with code snippets and demo links)\n\n## Success Factors\n\n- Clear component breakdown\n- Separated concerns (logic, UI, accessibility)\n- Each task had acceptance criteria\n- Referenced design spec throughout\n- Included accessibility from start, not afterthought\n- Tracked progress with visual updates\n",
        "skills/specification-executor/reference/milestone-summary-template.md": "# Milestone Summary Template\n\nUse this when completing major phases or milestones.\n\n```markdown\n## Phase [N] Complete: [Date]\n\n### Accomplishments\n\n- [Major item delivered]\n- [Major item delivered]\n\n### Deliverables\n\n- <mention-page url=\"...\">Deliverable 1</mention-page>\n- [Link to PR/deployment]\n\n### Metrics\n\n- [Relevant metric]\n- [Relevant metric]\n\n### Learnings\n\n- [What went well]\n- [What to improve]\n\n### Next Phase\n\nStarting [Phase name] on [Date]\n```\n",
        "skills/specification-executor/reference/progress-tracking.md": "# Progress Tracking\n\n## Update Frequency\n\n### Daily Updates\n\nFor active implementation work:\n\n**What to update**:\n\n- Task status if changed\n- Add progress note to task\n- Update blockers\n\n**When**:\n\n- End of work day\n- After completing significant work\n- When encountering blockers\n\n### Milestone Updates\n\nFor phase/milestone completion:\n\n**What to update**:\n\n- Mark phase complete in plan\n- Add milestone summary\n- Update timeline if needed\n- Report to stakeholders\n\n**When**:\n\n- Phase completion\n- Major deliverable ready\n- Sprint end\n- Release\n\n### Status Change Updates\n\nFor task state transitions:\n\n**What to update**:\n\n- Task status property\n- Add transition note\n- Notify relevant people\n\n**When**:\n\n- Start work (To Do  In Progress)\n- Ready for review (In Progress  In Review)\n- Complete (In Review  Done)\n- Block (Any  Blocked)\n\n## Progress Note Format\n\n### Daily Progress Note\n\n```markdown\n## Progress: [Date]\n\n### Completed\n\n- [Specific accomplishment with details]\n- [Specific accomplishment with details]\n\n### In Progress\n\n- [Current work item]\n- Current status: [Percentage or description]\n\n### Next Steps\n\n1. [Next planned action]\n2. [Next planned action]\n\n### Blockers\n\n- [Blocker description and who/what needed to unblock]\n- Or: None\n\n### Decisions Made\n\n- [Any technical/product decisions]\n\n### Notes\n\n[Additional context, learnings, issues encountered]\n```\n\nExample:\n\n```markdown\n## Progress: Oct 14, 2025\n\n### Completed\n\n- Implemented user authentication API endpoints (login, logout, refresh)\n- Added JWT token generation and validation\n- Wrote unit tests for auth service (95% coverage)\n\n### In Progress\n\n- Frontend login form integration\n- Currently: Form submits but need to handle error states\n\n### Next Steps\n\n1. Complete error handling in login form\n2. Add loading states\n3. Implement \"remember me\" functionality\n\n### Blockers\n\nNone\n\n### Decisions Made\n\n- Using HttpOnly cookies for refresh tokens (more secure than localStorage)\n- Session timeout set to 24 hours based on security review\n\n### Notes\n\n- Found edge case with concurrent login attempts, added to backlog\n- Performance of auth check is good (<10ms)\n```\n\n### Milestone Summary\n\n```markdown\n## Phase [N] Complete: [Date]\n\n### Overview\n\n[Brief description of what was accomplished in this phase]\n\n### Completed Tasks\n\n- <mention-page url=\"...\">Task 1</mention-page> \n- <mention-page url=\"...\">Task 2</mention-page> \n- <mention-page url=\"...\">Task 3</mention-page> \n\n### Deliverables\n\n- [Deliverable 1]: [Link/description]\n- [Deliverable 2]: [Link/description]\n\n### Key Accomplishments\n\n- [Major achievement]\n- [Major achievement]\n\n### Metrics\n\n- [Relevant metric]: [Value]\n- [Relevant metric]: [Value]\n\n### Challenges Overcome\n\n- [Challenge and how it was solved]\n\n### Learnings\n\n**What went well**:\n\n- [Success factor]\n\n**What to improve**:\n\n- [Area for improvement]\n\n### Impact on Timeline\n\n- On schedule / [X days ahead/behind]\n- Reason: [If deviation, explain why]\n\n### Next Phase\n\n- **Starting**: [Next phase name]\n- **Target start date**: [Date]\n- **Focus**: [Main objectives]\n```\n\n## Updating Implementation Plan\n\n### Progress Indicators\n\nUpdate plan page regularly:\n\n```markdown\n## Status Overview\n\n**Overall Progress**: 45% complete\n\n### Phase Status\n\n-  Phase 1: Foundation - Complete\n-  Phase 2: Core Features - In Progress (60%)\n-  Phase 3: Integration - Not Started\n\n### Task Summary\n\n-  Completed: 12 tasks\n-  In Progress: 5 tasks\n-  Blocked: 1 task\n-  Not Started: 8 tasks\n\n**Last Updated**: [Date]\n```\n\n### Task Checklist Updates\n\nMark completed tasks:\n\n```markdown\n## Implementation Phases\n\n### Phase 1: Foundation\n\n- [x] <mention-page url=\"...\">Database schema</mention-page>\n- [x] <mention-page url=\"...\">API scaffolding</mention-page>\n- [x] <mention-page url=\"...\">Auth setup</mention-page>\n\n### Phase 2: Core Features\n\n- [x] <mention-page url=\"...\">User management</mention-page>\n- [ ] <mention-page url=\"...\">Dashboard</mention-page>\n- [ ] <mention-page url=\"...\">Reporting</mention-page>\n```\n\n### Timeline Updates\n\nUpdate milestone dates:\n\n```markdown\n## Timeline\n\n| Milestone | Original | Current | Status                        |\n| --------- | -------- | ------- | ----------------------------- |\n| Phase 1   | Oct 15   | Oct 14  |  Complete (1 day early)     |\n| Phase 2   | Oct 30   | Nov 2   |  In Progress (3 days delay) |\n| Phase 3   | Nov 15   | Nov 18  |  Planned (adjusted)         |\n| Launch    | Nov 20   | Nov 22  |  Planned (adjusted)         |\n\n**Timeline Status**: Slightly behind due to [reason]\n```\n\n## Task Status Tracking\n\n### Status Definitions\n\n**To Do**: Not started\n\n- Task is ready to begin\n- Dependencies met\n- Assigned (or available)\n\n**In Progress**: Actively being worked\n\n- Work has started\n- Assigned to someone\n- Regular updates expected\n\n**Blocked**: Cannot proceed\n\n- Dependency not met\n- External blocker\n- Waiting on decision/resource\n\n**In Review**: Awaiting review\n\n- Work complete from implementer perspective\n- Needs code review, QA, or approval\n- Reviewers identified\n\n**Done**: Complete\n\n- All acceptance criteria met\n- Reviewed and approved\n- Deployed/delivered\n\n### Updating Task Status\n\nWhen updating:\n\n```\n1. Update Status property\n2. Add progress note explaining change\n3. Update related tasks if needed\n4. Notify relevant people via comment\n\nExample:\nproperties: { \"Status\": \"In Progress\" }\n\nContent update:\n## Progress: Oct 14, 2025\nStarted implementation. Set up basic structure and wrote initial tests.\n```\n\n## Blocker Tracking\n\n### Recording Blockers\n\nWhen encountering a blocker:\n\n```markdown\n## Blockers\n\n### [Date]: [Blocker Description]\n\n**Status**:  Active\n**Impact**: [What's blocked]\n**Needed to unblock**: [Action/person/decision needed]\n**Owner**: [Who's responsible for unblocking]\n**Target resolution**: [Date or timeframe]\n```\n\n### Resolving Blockers\n\nWhen unblocked:\n\n```markdown\n## Blockers\n\n### [Date]: [Blocker Description]\n\n**Status**:  Resolved on [Date]\n**Resolution**: [How it was resolved]\n**Impact**: [Any timeline/scope impact]\n```\n\n### Escalating Blockers\n\nIf blocker needs escalation:\n\n```\n1. Update blocker status in task\n2. Add comment tagging stakeholder\n3. Update plan with blocker impact\n4. Propose mitigation if possible\n```\n\n## Metrics Tracking\n\n### Velocity Tracking\n\nTrack completion rate:\n\n```markdown\n## Velocity\n\n### Week 1\n\n- Tasks completed: 8\n- Story points: 21\n- Velocity: Strong\n\n### Week 2\n\n- Tasks completed: 6\n- Story points: 18\n- Velocity: Moderate (1 blocker)\n\n### Week 3\n\n- Tasks completed: 9\n- Story points: 24\n- Velocity: Strong (blocker resolved)\n```\n\n### Quality Metrics\n\nTrack quality indicators:\n\n```markdown\n## Quality Metrics\n\n- Test coverage: 87%\n- Code review approval rate: 95%\n- Bug count: 3 (2 minor, 1 cosmetic)\n- Performance: All targets met\n- Security: No issues found\n```\n\n### Progress Metrics\n\nQuantitative progress:\n\n```markdown\n## Progress Metrics\n\n- Requirements implemented: 15/20 (75%)\n- Acceptance criteria met: 42/56 (75%)\n- Test cases passing: 128/135 (95%)\n- Code complete: 80%\n- Documentation: 60%\n```\n\n## Stakeholder Communication\n\n### Weekly Status Report\n\n```markdown\n## Weekly Status: [Week of Date]\n\n### Summary\n\n[One paragraph overview of progress and status]\n\n### This Week's Accomplishments\n\n- [Key accomplishment]\n- [Key accomplishment]\n- [Key accomplishment]\n\n### Next Week's Plan\n\n- [Planned work]\n- [Planned work]\n\n### Status\n\n- On track / At risk / Behind schedule\n- [If at risk or behind, explain and provide mitigation plan]\n\n### Blockers & Needs\n\n- [Active blocker or need for help]\n- Or: None\n\n### Risks\n\n- [New or evolving risk]\n- Or: None currently identified\n```\n\n### Executive Summary\n\nFor leadership updates:\n\n```markdown\n## Implementation Status: [Feature Name]\n\n**Overall Status**:  On Track /  At Risk /  Behind\n\n**Progress**: [X]% complete\n\n**Key Updates**:\n\n- [Most important update]\n- [Most important update]\n\n**Timeline**: [Status vs original plan]\n\n**Risks**: [Top 1-2 risks]\n\n**Next Milestone**: [Upcoming milestone and date]\n```\n\n## Automated Progress Tracking\n\n### Query-Based Status\n\nGenerate status from task database:\n\n```\nQuery task database:\nSELECT\n  \"Status\",\n  COUNT(*) as count\nFROM \"collection://tasks-uuid\"\nWHERE \"Related Tasks\" CONTAINS 'plan-page-id'\nGROUP BY \"Status\"\n\nGenerate summary:\n- To Do: 8\n- In Progress: 5\n- Blocked: 1\n- In Review: 2\n- Done: 12\n\nOverall: 44% complete (12/28 tasks)\n```\n\n### Timeline Calculation\n\nCalculate projected completion:\n\n```\nAverage velocity: 6 tasks/week\nRemaining tasks: 14\nProjected completion: 2.3 weeks from now\n\nCompares to target: [On schedule/Behind/Ahead]\n```\n\n## Best Practices\n\n1. **Update regularly**: Don't let updates pile up\n2. **Be specific**: \"Completed login\" vs \"Made progress\"\n3. **Quantify progress**: Use percentages, counts, metrics\n4. **Note blockers immediately**: Don't wait to report blockers\n5. **Link to work**: Reference PRs, deployments, demos\n6. **Track decisions**: Document why, not just what\n7. **Be honest**: Report actual status, not optimistic status\n8. **Update in one place**: Keep implementation plan as source of truth\n",
        "skills/specification-executor/reference/progress-update-template.md": "# Progress Update Template\n\nUse this to update progress on implementation plans and tasks.\n\n```markdown\n## Progress: [Date]\n\n### Completed Today\n\n- [Specific item completed]\n- [Specific item completed]\n\n### In Progress\n\n- [Current work item and status]\n\n### Next Steps\n\n1. [Next action]\n2. [Next action]\n\n### Blockers\n\n- [Blocker description] or None\n\n### Notes\n\n[Additional context, decisions made, issues encountered]\n```\n",
        "skills/specification-executor/reference/quick-implementation-plan.md": "# Quick Implementation Plan Template\n\nFor simpler features or small changes.\n\n```markdown\n# Implementation: [Feature Name]\n\n## Spec\n\n<mention-page url=\"...\">Specification</mention-page>\n\n## Summary\n\n[Quick description]\n\n## Tasks\n\n- [ ] <mention-page url=\"...\">Task 1</mention-page>\n- [ ] <mention-page url=\"...\">Task 2</mention-page>\n- [ ] <mention-page url=\"...\">Task 3</mention-page>\n\n## Timeline\n\nStart: [Date]\nTarget completion: [Date]\n\n## Status\n\n[Update as work progresses]\n```\n",
        "skills/specification-executor/reference/spec-parsing.md": "# Specification Parsing\n\n## Finding the Specification\n\nBefore parsing, locate the spec page:\n\n```\n1. Search for spec:\n   Notion:notion-search\n   query: \"[Feature Name] spec\" or \"[Feature Name] specification\"\n\n2. Handle results:\n   - If found  use page URL/ID\n   - If multiple  ask user which one\n   - If not found  ask user for URL/ID\n\nExample:\nNotion:notion-search\nquery: \"User Profile API spec\"\nquery_type: \"internal\"\n```\n\n## Reading Specifications\n\nAfter finding the spec, fetch it with `Notion:notion-fetch`:\n\n1. Read the full content\n2. Identify key sections\n3. Extract structured information\n4. Note ambiguities or gaps\n\n```\nNotion:notion-fetch\nid: \"spec-page-id-from-search\"\n```\n\n## Common Spec Structures\n\n### Requirements-Based Spec\n\n```\n# Feature Spec\n## Overview\n[Feature description]\n\n## Requirements\n### Functional\n- REQ-1: [Requirement]\n- REQ-2: [Requirement]\n\n### Non-Functional\n- PERF-1: [Performance requirement]\n- SEC-1: [Security requirement]\n\n## Acceptance Criteria\n- AC-1: [Criterion]\n- AC-2: [Criterion]\n```\n\nExtract:\n\n- List of functional requirements\n- List of non-functional requirements\n- List of acceptance criteria\n\n### User Story Based Spec\n\n```\n# Feature Spec\n## User Stories\n### As a [user type]\nI want [goal]\nSo that [benefit]\n\n**Acceptance Criteria**:\n- [Criterion]\n- [Criterion]\n```\n\nExtract:\n\n- User personas\n- Goals/capabilities needed\n- Acceptance criteria per story\n\n### Technical Design Doc\n\n```\n# Technical Design\n## Problem Statement\n[Problem description]\n\n## Proposed Solution\n[Solution approach]\n\n## Architecture\n[Architecture details]\n\n## Implementation Plan\n[Implementation approach]\n```\n\nExtract:\n\n- Problem being solved\n- Proposed solution approach\n- Architectural decisions\n- Implementation guidance\n\n### Product Requirements Document (PRD)\n\n```\n# PRD: [Feature]\n## Goals\n[Business goals]\n\n## User Needs\n[User problems being solved]\n\n## Features\n[Feature list]\n\n## Success Metrics\n[How to measure success]\n```\n\nExtract:\n\n- Business goals\n- User needs\n- Feature list\n- Success metrics\n\n## Extraction Strategies\n\n### Requirement Identification\n\nLook for:\n\n- \"Must\", \"Should\", \"Will\" statements\n- Numbered requirements (REQ-1, etc.)\n- User stories (As a... I want...)\n- Acceptance criteria sections\n- Feature lists\n\n### Categorization\n\nGroup requirements by:\n\n**Functional**: What the system does\n\n- User capabilities\n- System behaviors\n- Data operations\n\n**Non-Functional**: How the system performs\n\n- Performance targets\n- Security requirements\n- Scalability needs\n- Availability requirements\n- Compliance requirements\n\n**Constraints**: Limitations\n\n- Technical constraints\n- Business constraints\n- Timeline constraints\n\n### Priority Extraction\n\nIdentify priority indicators:\n\n- \"Critical\", \"Must have\", \"P0\"\n- \"Important\", \"Should have\", \"P1\"\n- \"Nice to have\", \"Could have\", \"P2\"\n- \"Future\", \"Won't have\", \"P3\"\n\nMap to implementation phases based on priority.\n\n## Handling Ambiguity\n\n### Unclear Requirements\n\nWhen requirement is ambiguous:\n\n```markdown\n## Clarifications Needed\n\n### [Requirement ID/Description]\n\n**Current text**: \"[Ambiguous requirement]\"\n**Question**: [What needs clarification]\n**Impact**: [Why this matters for implementation]\n**Assumed for now**: [Working assumption if any]\n```\n\nCreate clarification task or add comment to spec.\n\n### Missing Information\n\nWhen critical info is missing:\n\n```markdown\n## Missing Information\n\n- **[Topic]**: Spec doesn't specify [what's missing]\n- **Impact**: Blocks [affected tasks]\n- **Action**: Need to [how to resolve]\n```\n\n### Conflicting Requirements\n\nWhen requirements conflict:\n\n```markdown\n## Conflicting Requirements\n\n**Conflict**: REQ-1 says [X] but REQ-5 says [Y]\n**Impact**: [Implementation impact]\n**Resolution needed**: [Decision needed]\n```\n\n## Acceptance Criteria Parsing\n\n### Explicit Criteria\n\nDirect acceptance criteria:\n\n```\n## Acceptance Criteria\n- User can log in with email and password\n- System sends confirmation email\n- Session expires after 24 hours\n```\n\nConvert to checklist:\n\n- [ ] User can log in with email and password\n- [ ] System sends confirmation email\n- [ ] Session expires after 24 hours\n\n### Implicit Criteria\n\nDerive from requirements:\n\n```\nRequirement: \"Users can upload files up to 100MB\"\n\nImplied acceptance criteria:\n- [ ] Files up to 100MB upload successfully\n- [ ] Files over 100MB are rejected with error message\n- [ ] Progress indicator shows during upload\n- [ ] Upload can be cancelled\n```\n\n### Testable Criteria\n\nEnsure criteria are testable:\n\n **Not testable**: \"System is fast\"\n **Testable**: \"Page loads in < 2 seconds\"\n\n **Not testable**: \"Users like the interface\"\n **Testable**: \"90% of test users complete task successfully\"\n\n## Technical Detail Extraction\n\n### Architecture Information\n\nExtract:\n\n- System components\n- Data models\n- APIs/interfaces\n- Integration points\n- Technology choices\n\n### Design Decisions\n\nNote:\n\n- Technology selections\n- Architecture patterns\n- Trade-offs made\n- Rationale provided\n\n### Implementation Guidance\n\nLook for:\n\n- Suggested approach\n- Code examples\n- Library recommendations\n- Best practices mentioned\n\n## Dependency Identification\n\n### External Dependencies\n\nFrom spec, identify:\n\n- Third-party services required\n- External APIs needed\n- Infrastructure requirements\n- Tool/library dependencies\n\n### Internal Dependencies\n\nIdentify:\n\n- Other features needed first\n- Shared components required\n- Team dependencies\n- Data dependencies\n\n### Timeline Dependencies\n\nNote:\n\n- Hard deadlines\n- Milestone dependencies\n- Sequencing requirements\n\n## Scope Extraction\n\n### In Scope\n\nWhat's explicitly included:\n\n- Features to build\n- Use cases to support\n- Users/personas to serve\n\n### Out of Scope\n\nWhat's explicitly excluded:\n\n- Features deferred\n- Use cases not supported\n- Edge cases not handled\n\n### Assumptions\n\nWhat's assumed:\n\n- Environment assumptions\n- User assumptions\n- System state assumptions\n\n## Risk Identification\n\nExtract risk information:\n\n### Technical Risks\n\n- Unproven technology\n- Complex integration\n- Performance concerns\n- Scalability unknowns\n\n### Business Risks\n\n- Market timing\n- Resource availability\n- Dependency on others\n\n### Mitigation Strategies\n\nNote any mitigation approaches mentioned in spec.\n\n## Spec Quality Assessment\n\nEvaluate spec completeness:\n\n **Good spec**:\n\n- Clear requirements\n- Explicit acceptance criteria\n- Priorities defined\n- Risks identified\n- Technical approach outlined\n\n **Incomplete spec**:\n\n- Vague requirements\n- Missing acceptance criteria\n- Unclear priorities\n- No risk analysis\n- Technical details absent\n\nDocument gaps and create clarification tasks.\n\n## Parsing Checklist\n\nBefore creating implementation plan:\n\n All functional requirements identified\n Non-functional requirements noted\n Acceptance criteria extracted\n Dependencies identified\n Risks noted\n Ambiguities documented\n Technical approach understood\n Scope is clear\n Priorities are defined\n",
        "skills/specification-executor/reference/standard-implementation-plan.md": "# Standard Implementation Plan Template\n\nUse this template for most feature implementations.\n\n```markdown\n# Implementation Plan: [Feature Name]\n\n## Overview\n\n[1-2 sentence feature description and business value]\n\n## Linked Specification\n\n<mention-page url=\"...\">Original Specification</mention-page>\n\n## Requirements Summary\n\n### Functional Requirements\n\n- [Requirement 1]\n- [Requirement 2]\n- [Requirement 3]\n\n### Non-Functional Requirements\n\n- **Performance**: [Targets]\n- **Security**: [Requirements]\n- **Scalability**: [Needs]\n\n### Acceptance Criteria\n\n- [ ] [Criterion 1]\n- [ ] [Criterion 2]\n- [ ] [Criterion 3]\n\n## Technical Approach\n\n### Architecture\n\n[High-level architectural decisions]\n\n### Technology Stack\n\n- Backend: [Technologies]\n- Frontend: [Technologies]\n- Infrastructure: [Technologies]\n\n### Key Design Decisions\n\n1. **[Decision]**: [Rationale]\n2. **[Decision]**: [Rationale]\n\n## Implementation Phases\n\n### Phase 1: Foundation (Week 1)\n\n**Goal**: Set up core infrastructure\n\n**Tasks**:\n\n- [ ] <mention-page url=\"...\">Database schema design</mention-page>\n- [ ] <mention-page url=\"...\">API scaffolding</mention-page>\n- [ ] <mention-page url=\"...\">Authentication setup</mention-page>\n\n**Deliverables**: Working API skeleton\n**Estimated effort**: 3 days\n\n### Phase 2: Core Features (Week 2-3)\n\n**Goal**: Implement main functionality\n\n**Tasks**:\n\n- [ ] <mention-page url=\"...\">Feature A implementation</mention-page>\n- [ ] <mention-page url=\"...\">Feature B implementation</mention-page>\n\n**Deliverables**: Core features working\n**Estimated effort**: 1 week\n\n### Phase 3: Integration & Polish (Week 4)\n\n**Goal**: Complete integration and refinement\n\n**Tasks**:\n\n- [ ] <mention-page url=\"...\">Frontend integration</mention-page>\n- [ ] <mention-page url=\"...\">Testing & QA</mention-page>\n\n**Deliverables**: Production-ready feature\n**Estimated effort**: 1 week\n\n## Dependencies\n\n### External Dependencies\n\n- [Dependency 1]: [Status]\n- [Dependency 2]: [Status]\n\n### Internal Dependencies\n\n- [Team/component dependency]\n\n### Blockers\n\n- [Known blocker] or None currently\n\n## Risks & Mitigation\n\n### Risk 1: [Description]\n\n- **Probability**: High/Medium/Low\n- **Impact**: High/Medium/Low\n- **Mitigation**: [Strategy]\n\n### Risk 2: [Description]\n\n- **Probability**: High/Medium/Low\n- **Impact**: High/Medium/Low\n- **Mitigation**: [Strategy]\n\n## Timeline\n\n| Milestone        | Target Date | Status     |\n| ---------------- | ----------- | ---------- |\n| Phase 1 Complete | [Date]      |  Planned |\n| Phase 2 Complete | [Date]      |  Planned |\n| Phase 3 Complete | [Date]      |  Planned |\n| Launch           | [Date]      |  Planned |\n\n## Success Criteria\n\n### Technical Success\n\n- [ ] All acceptance criteria met\n- [ ] Performance targets achieved\n- [ ] Security requirements satisfied\n- [ ] Test coverage > 80%\n\n### Business Success\n\n- [ ] [Business metric 1]\n- [ ] [Business metric 2]\n\n## Resources\n\n### Documentation\n\n- <mention-page url=\"...\">Design Doc</mention-page>\n- <mention-page url=\"...\">API Spec</mention-page>\n\n### Related Work\n\n- <mention-page url=\"...\">Related Feature</mention-page>\n\n## Progress Tracking\n\n[This section updated regularly]\n\n### Phase Status\n\n- Phase 1:  Not Started\n- Phase 2:  Not Started\n- Phase 3:  Not Started\n\n**Overall Progress**: 0% complete\n\n### Latest Update: [Date]\n\n[Brief status update]\n```\n",
        "skills/specification-executor/reference/task-creation-template.md": "# Task Creation Template\n\nWhen creating tasks from spec.\n\n```markdown\n# [Task Name]\n\n## Context\n\nPart of implementation for <mention-page url=\"...\">Feature Spec</mention-page>\n\nImplementation plan: <mention-page url=\"...\">Implementation Plan</mention-page>\n\n## Description\n\n[What needs to be done]\n\n## Acceptance Criteria\n\n- [ ] [Criterion 1]\n- [ ] [Criterion 2]\n\n## Technical Details\n\n[Technical approach or notes]\n\n## Dependencies\n\n- Blocked by: [Task] or None\n- Blocks: [Task] or None\n\n## Resources\n\n- [Link to design]\n- [Link to related code]\n\n## Progress\n\n[To be updated during implementation]\n```\n",
        "skills/specification-executor/reference/task-creation.md": "# Task Creation from Specs\n\n## Finding the Task Database\n\nBefore creating tasks, locate the task database:\n\n```\n1. Search for task database:\n   Notion:notion-search\n   query: \"Tasks\" or \"Task Management\" or \"[Project] Tasks\"\n\n2. Fetch database schema:\n   Notion:notion-fetch\n   id: \"database-id-from-search\"\n\n3. Identify data source:\n   - Look for <data-source url=\"collection://...\"> tags\n   - Extract collection ID for parent parameter\n\n4. Note schema:\n   - Required properties\n   - Property types and options\n   - Relation properties for linking\n\nExample:\nNotion:notion-search\nquery: \"Engineering Tasks\"\nquery_type: \"internal\"\n\nNotion:notion-fetch\nid: \"tasks-database-id\"\n```\n\nResult: `collection://abc-123-def` for use as parent\n\n## Task Breakdown Strategy\n\n### Size Guidelines\n\n**Good task size**:\n\n- Completable in 1-2 days\n- Single clear deliverable\n- Independently testable\n- Minimal dependencies\n\n**Too large**:\n\n- Takes > 3 days\n- Multiple deliverables\n- Many dependencies\n- Break down further\n\n**Too small**:\n\n- Takes < 2 hours\n- Too granular\n- Group with related work\n\n### Granularity by Phase\n\n**Early phases**: Larger tasks acceptable\n\n- \"Design database schema\"\n- \"Set up API structure\"\n\n**Middle phases**: Medium-sized tasks\n\n- \"Implement user authentication\"\n- \"Build dashboard UI\"\n\n**Late phases**: Smaller, precise tasks\n\n- \"Fix validation bug in form\"\n- \"Add loading state to button\"\n\n## Task Creation Pattern\n\nFor each requirement or work item:\n\n```\n1. Identify the work\n2. Determine task size\n3. Create task in database\n4. Set properties\n5. Write task description\n6. Link to spec/plan\n```\n\n### Creating Task\n\n```\nUse Notion:notion-create-pages:\n\nparent: {\n  type: \"data_source_id\",\n  data_source_id: \"collection://tasks-db-uuid\"\n}\n\nproperties: {\n  \"[Title Property]\": \"Task: [Clear task name]\",\n  \"Status\": \"To Do\",\n  \"Priority\": \"[High/Medium/Low]\",\n  \"[Project/Related]\": [\"spec-page-id\", \"plan-page-id\"],\n  \"Assignee\": \"[Person]\" (if known),\n  \"date:Due Date:start\": \"[Date]\" (if applicable),\n  \"date:Due Date:is_datetime\": 0\n}\n\ncontent: \"[Task description using template]\"\n```\n\n## Task Description Template\n\n```markdown\n# [Task Name]\n\n## Context\n\nImplementation task for <mention-page url=\"...\">Feature Spec</mention-page>\n\nPart of <mention-page url=\"...\">Implementation Plan</mention-page> - Phase [N]\n\n## Objective\n\n[What this task accomplishes]\n\n## Requirements\n\nBased on spec requirements:\n\n- [Relevant requirement 1]\n- [Relevant requirement 2]\n\n## Acceptance Criteria\n\n- [ ] [Specific, testable criterion]\n- [ ] [Specific, testable criterion]\n- [ ] [Specific, testable criterion]\n\n## Technical Approach\n\n[Suggested implementation approach]\n\n### Components Affected\n\n- [Component 1]\n- [Component 2]\n\n### Key Decisions\n\n- [Decision point 1]\n- [Decision point 2]\n\n## Dependencies\n\n### Blocked By\n\n- <mention-page url=\"...\">Prerequisite Task</mention-page> or None\n\n### Blocks\n\n- <mention-page url=\"...\">Dependent Task</mention-page> or None\n\n## Resources\n\n- [Link to design mockup]\n- [Link to API spec]\n- [Link to relevant code]\n\n## Estimated Effort\n\n[Time estimate]\n\n## Progress\n\n[To be updated during implementation]\n```\n\n## Task Types\n\n### Infrastructure/Setup Tasks\n\n```\nTitle: \"Setup: [What's being set up]\"\nExamples:\n- \"Setup: Configure database connection pool\"\n- \"Setup: Initialize authentication middleware\"\n- \"Setup: Create CI/CD pipeline\"\n\nFocus: Getting environment/tooling ready\n```\n\n### Feature Implementation Tasks\n\n```\nTitle: \"Implement: [Feature name]\"\nExamples:\n- \"Implement: User login flow\"\n- \"Implement: File upload functionality\"\n- \"Implement: Dashboard widget\"\n\nFocus: Building specific functionality\n```\n\n### Integration Tasks\n\n```\nTitle: \"Integrate: [What's being integrated]\"\nExamples:\n- \"Integrate: Connect frontend to API\"\n- \"Integrate: Add payment provider\"\n- \"Integrate: Link user profile to dashboard\"\n\nFocus: Connecting components\n```\n\n### Testing Tasks\n\n```\nTitle: \"Test: [What's being tested]\"\nExamples:\n- \"Test: Write unit tests for auth service\"\n- \"Test: E2E testing for checkout flow\"\n- \"Test: Performance testing for API\"\n\nFocus: Validation and quality assurance\n```\n\n### Documentation Tasks\n\n```\nTitle: \"Document: [What's being documented]\"\nExamples:\n- \"Document: API endpoints\"\n- \"Document: Setup instructions\"\n- \"Document: Architecture decisions\"\n\nFocus: Creating documentation\n```\n\n### Bug Fix Tasks\n\n```\nTitle: \"Fix: [Bug description]\"\nExamples:\n- \"Fix: Login error on Safari\"\n- \"Fix: Memory leak in image processing\"\n- \"Fix: Race condition in payment flow\"\n\nFocus: Resolving issues\n```\n\n### Refactoring Tasks\n\n```\nTitle: \"Refactor: [What's being refactored]\"\nExamples:\n- \"Refactor: Extract auth logic to service\"\n- \"Refactor: Optimize database queries\"\n- \"Refactor: Simplify component hierarchy\"\n\nFocus: Code quality improvement\n```\n\n## Sequencing Tasks\n\n### Critical Path\n\nIdentify must-happen-first tasks:\n\n```\n1. Database schema\n2. API foundation\n3. Core business logic\n4. Frontend integration\n5. Testing\n6. Deployment\n```\n\n### Parallel Tracks\n\nTasks that can happen simultaneously:\n\n```\nTrack A: Backend development\n- API endpoints\n- Business logic\n- Database operations\n\nTrack B: Frontend development\n- UI components\n- State management\n- Routing\n\nTrack C: Infrastructure\n- CI/CD setup\n- Monitoring\n- Documentation\n```\n\n### Phase-Based Sequencing\n\nGroup by implementation phase:\n\n```\nPhase 1 (Foundation):\n- Setup tasks\n- Infrastructure tasks\n\nPhase 2 (Core):\n- Feature implementation tasks\n- Integration tasks\n\nPhase 3 (Polish):\n- Testing tasks\n- Documentation tasks\n- Optimization tasks\n```\n\n## Priority Assignment\n\n### P0/Critical\n\n- Blocks everything else\n- Core functionality\n- Security requirements\n- Data integrity\n\n### P1/High\n\n- Important features\n- User-facing functionality\n- Performance requirements\n\n### P2/Medium\n\n- Nice-to-have features\n- Optimizations\n- Minor improvements\n\n### P3/Low\n\n- Future enhancements\n- Edge case handling\n- Cosmetic improvements\n\n## Estimation\n\n### Story Points\n\nIf using story points:\n\n- 1 point: Few hours\n- 2 points: Half day\n- 3 points: Full day\n- 5 points: 2 days\n- 8 points: 3-4 days (consider breaking down)\n\n### Time Estimates\n\nDirect time estimates:\n\n- 2-4 hours: Small task\n- 1 day: Medium task\n- 2 days: Large task\n- 3+ days: Break down further\n\n### Estimation Factors\n\nConsider:\n\n- Complexity\n- Unknowns\n- Dependencies\n- Testing requirements\n- Documentation needs\n\n## Task Relationships\n\n### Parent Task Pattern\n\nFor large features:\n\n```\nParent: \"Feature: User Authentication\"\nChildren:\n- \"Setup: Configure auth library\"\n- \"Implement: Login flow\"\n- \"Implement: Password reset\"\n- \"Test: Auth functionality\"\n```\n\n### Dependency Chain Pattern\n\nFor sequential work:\n\n```\nTask A: \"Design database schema\"\n (blocks)\nTask B: \"Implement data models\"\n (blocks)\nTask C: \"Create API endpoints\"\n (blocks)\nTask D: \"Integrate with frontend\"\n```\n\n### Related Tasks Pattern\n\nFor parallel work:\n\n```\nCentral: \"Feature: Dashboard\"\nRelated:\n- \"Backend API for dashboard data\"\n- \"Frontend dashboard component\"\n- \"Dashboard data caching\"\n```\n\n## Bulk Task Creation\n\nWhen creating many tasks:\n\n```\nFor each work item in breakdown:\n  1. Determine task properties\n  2. Create task page\n  3. Link to spec/plan\n  4. Set relationships\n\nThen:\n  1. Update plan with task links\n  2. Review sequencing\n  3. Assign tasks (if known)\n```\n\n## Task Naming Conventions\n\n**Be specific**:\n \"Implement user login with email/password\"\n \"Add login\"\n\n**Include context**:\n \"Dashboard: Add revenue chart widget\"\n \"Add chart\"\n\n**Use action verbs**:\n\n- Implement, Build, Create\n- Integrate, Connect, Link\n- Fix, Resolve, Debug\n- Test, Validate, Verify\n- Document, Write, Update\n- Refactor, Optimize, Improve\n\n## Validation Checklist\n\nBefore finalizing tasks:\n\n Each task has clear objective\n Acceptance criteria are testable\n Dependencies identified\n Appropriate size (1-2 days)\n Priority assigned\n Linked to spec/plan\n Proper sequencing\n Resources noted\n",
        "skills/spreadsheet-processor/SKILL.md": "---\nname: spreadsheet-processor\ndescription: Process and manipulate spreadsheet documents. Creates, edits, analyzes, and transforms Excel files with formula and formatting support.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n###  WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n###  CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections",
        "skills/system-design/SKILL.md": "---\nname: system-design\ndescription: Design and architect software systems. Plans system components, interactions, and deployment patterns for scalability and maintainability.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Software Architecture Development Skill\n\nThis skill provides guidence for quality focused software development and architecture. It is based on Clean Architecture and Domain Driven Design principles.\n\n## Code Style Rules\n\n### General Principles\n\n- **Early return pattern**: Always use early returns when possible, over nested conditions for better readability\n- Avoid code duplication through creation of reusable functions and modules\n- Decompose long (more than 80 lines of code) components and functions into multiple smaller components and functions. If they cannot be used anywhere else, keep it in the same file. But if file longer than 200 lines of code, it should be split into multiple files.\n- Use arrow functions instead of function declarations when possible\n\n### Best Practices\n\n#### Library-First Approach\n\n- **ALWAYS search for existing solutions before writing custom code**\n  - Check npm for existing libraries that solve the problem\n  - Evaluate existing services/SaaS solutions\n  - Consider third-party APIs for common functionality\n- Use libraries instead of writing your own utils or helpers. For example, use `cockatiel` instead of writing your own retry logic.\n- **When custom code IS justified:**\n  - Specific business logic unique to the domain\n  - Performance-critical paths with special requirements\n  - When external dependencies would be overkill\n  - Security-sensitive code requiring full control\n  - When existing solutions don't meet requirements after thorough evaluation\n\n#### Architecture and Design\n\n- **Clean Architecture & DDD Principles:**\n  - Follow domain-driven design and ubiquitous language\n  - Separate domain entities from infrastructure concerns\n  - Keep business logic independent of frameworks\n  - Define use cases clearly and keep them isolated\n- **Naming Conventions:**\n  - **AVOID** generic names: `utils`, `helpers`, `common`, `shared`\n  - **USE** domain-specific names: `OrderCalculator`, `UserAuthenticator`, `InvoiceGenerator`\n  - Follow bounded context naming patterns\n  - Each module should have a single, clear purpose\n- **Separation of Concerns:**\n  - Do NOT mix business logic with UI components\n  - Keep database queries out of controllers\n  - Maintain clear boundaries between contexts\n  - Ensure proper separation of responsibilities\n\n#### Anti-Patterns to Avoid\n\n- **NIH (Not Invented Here) Syndrome:**\n  - Don't build custom auth when Auth0/Supabase exists\n  - Don't write custom state management instead of using Redux/Zustand\n  - Don't create custom form validation instead of using established libraries\n- **Poor Architectural Choices:**\n  - Mixing business logic with UI components\n  - Database queries directly in controllers\n  - Lack of clear separation of concerns\n- **Generic Naming Anti-Patterns:**\n  - `utils.js` with 50 unrelated functions\n  - `helpers/misc.js` as a dumping ground\n  - `common/shared.js` with unclear purpose\n- Remember: Every line of custom code is a liability that needs maintenance, testing, and documentation\n\n#### Code Quality\n\n- Proper error handling with typed catch blocks\n- Break down complex logic into smaller, reusable functions\n- Avoid deep nesting (max 3 levels)\n- Keep functions focused and under 50 lines when possible\n- Keep files focused and under 200 lines of code when possible\n```",
        "skills/team-communication/SKILL.md": "---\nname: team-communication\ndescription: Create effective internal team communications. Drafts newsletters, announcements, and messages that inform and engage team members.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n## When to use this skill\nTo write internal communications, use this skill for:\n- 3P updates (Progress, Plans, Problems)\n- Company newsletters\n- FAQ responses\n- Status reports\n- Leadership updates\n- Project updates\n- Incident reports\n\n## How to use this skill\n\nTo write any internal communication:\n\n1. **Identify the communication type** from the request\n2. **Load the appropriate guideline file** from the `examples/` directory:\n    - `examples/3p-updates.md` - For Progress/Plans/Problems team updates\n    - `examples/company-newsletter.md` - For company-wide newsletters\n    - `examples/faq-answers.md` - For answering frequently asked questions\n    - `examples/general-comms.md` - For anything else that doesn't explicitly match one of the above\n3. **Follow the specific instructions** in that file for formatting, tone, and content gathering\n\nIf the communication type doesn't match any existing guideline, ask for clarification or more context about the desired format.\n\n## Keywords\n3P updates, company newsletter, company comms, weekly update, faqs, common questions, updates, internal comms\n",
        "skills/team-communication/examples/3p-updates.md": "## Instructions\nYou are being asked to write a 3P update. 3P updates stand for \"Progress, Plans, Problems.\" The main audience is for executives, leadership, other teammates, etc. They're meant to be very succinct and to-the-point: think something you can read in 30-60sec or less. They're also for people with some, but not a lot of context on what the team does.\n\n3Ps can cover a team of any size, ranging all the way up to the entire company. The bigger the team, the less granular the tasks should be. For example, \"mobile team\" might have \"shipped feature\" or \"fixed bugs,\" whereas the company might have really meaty 3Ps, like \"hired 20 new people\" or \"closed 10 new deals.\" \n\nThey represent the work of the team across a time period, almost always one week. They include three sections:\n1) Progress: what the team has accomplished over the next time period. Focus mainly on things shipped, milestones achieved, tasks created, etc.\n2) Plans: what the team plans to do over the next time period. Focus on what things are top-of-mind, really high priority, etc. for the team.\n3) Problems: anything that is slowing the team down. This could be things like too few people, bugs or blockers that are preventing the team from moving forward, some deal that fell through, etc.\n\nBefore writing them, make sure that you know the team name. If it's not specified, you can ask explicitly what the team name you're writing for is.\n\n\n## Tools Available\nWhenever possible, try to pull from available sources to get the information you need:\n- Slack: posts from team members with their updates - ideally look for posts in large channels with lots of reactions\n- Google Drive: docs written from critical team members with lots of views\n- Email: emails with lots of responses of lots of content that seems relevant\n- Calendar: non-recurring meetings that have a lot of importance, like product reviews, etc.\n\n\nTry to gather as much context as you can, focusing on the things that covered the time period you're writing for:\n- Progress: anything between a week ago and today\n- Plans: anything from today to the next week\n- Problems: anything between a week ago and today\n\n\nIf you don't have access, you can ask the user for things they want to cover. They might also include these things to you directly, in which case you're mostly just formatting for this particular format.\n\n## Workflow\n\n1. **Clarify scope**: Confirm the team name and time period (usually past week for Progress/Problems, next\nweek for Plans)\n2. **Gather information**: Use available tools or ask the user directly\n3. **Draft the update**: Follow the strict formatting guidelines\n4. **Review**: Ensure it's concise (30-60 seconds to read) and data-driven\n\n## Formatting\n\nThe format is always the same, very strict formatting. Never use any formatting other than this. Pick an emoji that is fun and captures the vibe of the team and update.\n\n[pick an emoji] [Team Name] (Dates Covered, usually a week)\nProgress: [1-3 sentences of content]\nPlans: [1-3 sentences of content]\nProblems: [1-3 sentences of content]\n\nEach section should be no more than 1-3 sentences: clear, to the point. It should be data-driven, and generally include metrics where possible. The tone should be very matter-of-fact, not super prose-heavy.",
        "skills/team-communication/examples/company-newsletter.md": "## Instructions\nYou are being asked to write a company-wide newsletter update. You are meant to summarize the past week/month of a company in the form of a newsletter that the entire company will read. It should be maybe ~20-25 bullet points long. It will be sent via Slack and email, so make it consumable for that.\n\nIdeally it includes the following attributes:\n- Lots of links: pulling documents from Google Drive that are very relevant, linking to prominent Slack messages in announce channels and from executives, perhgaps referencing emails that went company-wide, highlighting significant things that have happened in the company.\n- Short and to-the-point: each bullet should probably be no longer than ~1-2 sentences\n- Use the \"we\" tense, as you are part of the company. Many of the bullets should say \"we did this\" or \"we did that\"\n\n## Tools to use\nIf you have access to the following tools, please try to use them. If not, you can also let the user know directly that their responses would be better if they gave them access.\n\n- Slack: look for messages in channels with lots of people, with lots of reactions or lots of responses within the thread\n- Email: look for things from executives that discuss company-wide announcements\n- Calendar: if there were meetings with large attendee lists, particularly things like All-Hands meetings, big company announcements, etc. If there were documents attached to those meetings, those are great links to include.\n- Documents: if there were new docs published in the last week or two that got a lot of attention, you can link them. These should be things like company-wide vision docs, plans for the upcoming quarter or half, things authored by critical executives, etc.\n- External press: if you see references to articles or press we've received over the past week, that could be really cool too.\n\nIf you don't have access to any of these things, you can ask the user for things they want to cover. In this case, you'll mostly just be polishing up and fitting to this format more directly.\n\n## Sections\nThe company is pretty big: 1000+ people. There are a variety of different teams and initiatives going on across the company. To make sure the update works well, try breaking it into sections of similar things. You might break into clusters like {product development, go to market, finance} or {recruiting, execution, vision}, or {external news, internal news} etc. Try to make sure the different areas of the company are highlighted well.\n\n## Prioritization\nFocus on:\n- Company-wide impact (not team-specific details)\n- Announcements from leadership\n- Major milestones and achievements\n- Information that affects most employees\n- External recognition or press\n\nAvoid:\n- Overly granular team updates (save those for 3Ps)\n- Information only relevant to small groups\n- Duplicate information already communicated\n\n## Example Formats\n\n:megaphone: Company Announcements\n- Announcement 1\n- Announcement 2\n- Announcement 3\n\n:dart: Progress on Priorities\n- Area 1\n    - Sub-area 1\n    - Sub-area 2\n    - Sub-area 3\n- Area 2\n    - Sub-area 1\n    - Sub-area 2\n    - Sub-area 3\n- Area 3\n    - Sub-area 1\n    - Sub-area 2\n    - Sub-area 3\n\n:pillar: Leadership Updates\n- Post 1\n- Post 2\n- Post 3\n\n:thread: Social Updates\n- Update 1\n- Update 2\n- Update 3\n",
        "skills/team-communication/examples/faq-answers.md": "## Instructions\nYou are an assistant for answering questions that are being asked across the company. Every week, there are lots of questions that get asked across the company, and your goal is to try to summarize what those questions are. We want our company to be well-informed and on the same page, so your job is to produce a set of frequently asked questions that our employees are asking and attempt to answer them. Your singular job is to do two things:\n\n- Find questions that are big sources of confusion for lots of employees at the company, generally about things that affect a large portion of the employee base\n- Attempt to give a nice summarized answer to that question in order to minimize confusion.\n\nSome examples of areas that may be interesting to folks: recent corporate events (fundraising, new executives, etc.), upcoming launches, hiring progress, changes to vision or focus, etc.\n\n\n## Tools Available\nYou should use the company's available tools, where communication and work happens. For most companies, it looks something like this:\n- Slack: questions being asked across the company - it could be questions in response to posts with lots of responses, questions being asked with lots of reactions or thumbs up to show support, or anything else to show that a large number of employees want to ask the same things\n- Email: emails with FAQs written directly in them can be a good source as well\n- Documents: docs in places like Google Drive, linked on calendar events, etc. can also be a good source of FAQs, either directly added or inferred based on the contents of the doc\n\n## Formatting\nThe formatting should be pretty basic:\n\n- *Question*: [insert question - 1 sentence]\n- *Answer*: [insert answer - 1-2 sentence]\n\n## Guidance\nMake sure you're being holistic in your questions. Don't focus too much on just the user in question or the team they are a part of, but try to capture the entire company. Try to be as holistic as you can in reading all the tools available, producing responses that are relevant to all at the company.\n\n## Answer Guidelines\n- Base answers on official company communications when possible\n- If information is uncertain, indicate that clearly\n- Link to authoritative sources (docs, announcements, emails)\n- Keep tone professional but approachable\n- Flag if a question requires executive input or official response",
        "skills/team-communication/examples/general-comms.md": "  ## Instructions\n  You are being asked to write internal company communication that doesn't fit into the standard formats (3P\n  updates, newsletters, or FAQs).\n\n  Before proceeding:\n  1. Ask the user about their target audience\n  2. Understand the communication's purpose\n  3. Clarify the desired tone (formal, casual, urgent, informational)\n  4. Confirm any specific formatting requirements\n\n  Use these general principles:\n  - Be clear and concise\n  - Use active voice\n  - Put the most important information first\n  - Include relevant links and references\n  - Match the company's communication style",
        "skills/user-interface-designer/SKILL.md": "---\nname: user-interface-designer\ndescription: Design user-centric interfaces for web applications. Creates wireframes, visual designs, and interactive prototypes with usability focus.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\nThis skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend's aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don't hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision.",
        "skills/validation-first-approach/SKILL.md": "---\nname: validation-first-approach\ndescription: Develop software with validation requirements first. Emphasizes writing tests before implementation to guide design decisions.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Test-Driven Development (TDD)\n\n## Overview\n\nWrite the test first. Watch it fail. Write minimal code to pass.\n\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\n\n**Violating the letter of the rules is violating the spirit of the rules.**\n\n## When to Use\n\n**Always:**\n- New features\n- Bug fixes\n- Refactoring\n- Behavior changes\n\n**Exceptions (ask your human partner):**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\nThinking \"skip TDD just this once\"? Stop. That's rationalization.\n\n## The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrite code before the test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n\nImplement fresh from tests. Period.\n\n## Red-Green-Refactor\n\n```dot\ndigraph tdd_cycle {\n    rankdir=LR;\n    red [label=\"RED\\nWrite failing test\", shape=box, style=filled, fillcolor=\"#ffcccc\"];\n    verify_red [label=\"Verify fails\\ncorrectly\", shape=diamond];\n    green [label=\"GREEN\\nMinimal code\", shape=box, style=filled, fillcolor=\"#ccffcc\"];\n    verify_green [label=\"Verify passes\\nAll green\", shape=diamond];\n    refactor [label=\"REFACTOR\\nClean up\", shape=box, style=filled, fillcolor=\"#ccccff\"];\n    next [label=\"Next\", shape=ellipse];\n\n    red -> verify_red;\n    verify_red -> green [label=\"yes\"];\n    verify_red -> red [label=\"wrong\\nfailure\"];\n    green -> verify_green;\n    verify_green -> refactor [label=\"yes\"];\n    verify_green -> green [label=\"no\"];\n    refactor -> verify_green [label=\"stay\\ngreen\"];\n    verify_green -> next;\n    next -> red;\n}\n```\n\n### RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n<Good>\n```typescript\ntest('retries failed operations 3 times', async () => {\n  let attempts = 0;\n  const operation = () => {\n    attempts++;\n    if (attempts < 3) throw new Error('fail');\n    return 'success';\n  };\n\n  const result = await retryOperation(operation);\n\n  expect(result).toBe('success');\n  expect(attempts).toBe(3);\n});\n```\nClear name, tests real behavior, one thing\n</Good>\n\n<Bad>\n```typescript\ntest('retry works', async () => {\n  const mock = jest.fn()\n    .mockRejectedValueOnce(new Error())\n    .mockRejectedValueOnce(new Error())\n    .mockResolvedValueOnce('success');\n  await retryOperation(mock);\n  expect(mock).toHaveBeenCalledTimes(3);\n});\n```\nVague name, tests mock not code\n</Bad>\n\n**Requirements:**\n- One behavior\n- Clear name\n- Real code (no mocks unless unavoidable)\n\n### Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test fails (not errors)\n- Failure message is expected\n- Fails because feature missing (not typos)\n\n**Test passes?** You're testing existing behavior. Fix test.\n\n**Test errors?** Fix error, re-run until it fails correctly.\n\n### GREEN - Minimal Code\n\nWrite simplest code to pass the test.\n\n<Good>\n```typescript\nasync function retryOperation<T>(fn: () => Promise<T>): Promise<T> {\n  for (let i = 0; i < 3; i++) {\n    try {\n      return await fn();\n    } catch (e) {\n      if (i === 2) throw e;\n    }\n  }\n  throw new Error('unreachable');\n}\n```\nJust enough to pass\n</Good>\n\n<Bad>\n```typescript\nasync function retryOperation<T>(\n  fn: () => Promise<T>,\n  options?: {\n    maxRetries?: number;\n    backoff?: 'linear' | 'exponential';\n    onRetry?: (attempt: number) => void;\n  }\n): Promise<T> {\n  // YAGNI\n}\n```\nOver-engineered\n</Bad>\n\nDon't add features, refactor other code, or \"improve\" beyond the test.\n\n### Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test passes\n- Other tests still pass\n- Output pristine (no errors, warnings)\n\n**Test fails?** Fix code, not test.\n\n**Other tests fail?** Fix now.\n\n### REFACTOR - Clean Up\n\nAfter green only:\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n### Repeat\n\nNext failing test for next feature.\n\n## Good Tests\n\n| Quality | Good | Bad |\n|---------|------|-----|\n| **Minimal** | One thing. \"and\" in name? Split it. | `test('validates email and domain and whitespace')` |\n| **Clear** | Name describes behavior | `test('test1')` |\n| **Shows intent** | Demonstrates desired API | Obscures what code should do |\n\n## Why Order Matters\n\n**\"I'll write tests after to verify it works\"**\n\nTests written after code pass immediately. Passing immediately proves nothing:\n- Might test wrong thing\n- Might test implementation, not behavior\n- Might miss edge cases you forgot\n- You never saw it catch the bug\n\nTest-first forces you to see the test fail, proving it actually tests something.\n\n**\"I already manually tested all the edge cases\"**\n\nManual testing is ad-hoc. You think you tested everything but:\n- No record of what you tested\n- Can't re-run when code changes\n- Easy to forget cases under pressure\n- \"It worked when I tried it\"  comprehensive\n\nAutomated tests are systematic. They run the same way every time.\n\n**\"Deleting X hours of work is wasteful\"**\n\nSunk cost fallacy. The time is already gone. Your choice now:\n- Delete and rewrite with TDD (X more hours, high confidence)\n- Keep it and add tests after (30 min, low confidence, likely bugs)\n\nThe \"waste\" is keeping code you can't trust. Working code without real tests is technical debt.\n\n**\"TDD is dogmatic, being pragmatic means adapting\"**\n\nTDD IS pragmatic:\n- Finds bugs before commit (faster than debugging after)\n- Prevents regressions (tests catch breaks immediately)\n- Documents behavior (tests show how to use code)\n- Enables refactoring (change freely, tests catch breaks)\n\n\"Pragmatic\" shortcuts = debugging in production = slower.\n\n**\"Tests after achieve the same goals - it's spirit not ritual\"**\n\nNo. Tests-after answer \"What does this do?\" Tests-first answer \"What should this do?\"\n\nTests-after are biased by your implementation. You test what you built, not what's required. You verify remembered edge cases, not discovered ones.\n\nTests-first force edge case discovery before implementing. Tests-after verify you remembered everything (you didn't).\n\n30 minutes of tests after  TDD. You get coverage, lose proof tests work.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\n| \"Already manually tested\" | Ad-hoc  systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Keeping unverified code is technical debt. |\n| \"Keep as reference, write tests first\" | You'll adapt it. That's testing after. Delete means delete. |\n| \"Need to explore first\" | Fine. Throw away exploration, start with TDD. |\n| \"Test hard = design unclear\" | Listen to test. Hard to test = hard to use. |\n| \"TDD will slow me down\" | TDD faster than debugging. Pragmatic = test-first. |\n| \"Manual test faster\" | Manual doesn't prove edge cases. You'll re-test every change. |\n| \"Existing code has no tests\" | You're improving it. Add tests for existing code. |\n\n## Red Flags - STOP and Start Over\n\n- Code before test\n- Test after implementation\n- Test passes immediately\n- Can't explain why test failed\n- Tests added \"later\"\n- Rationalizing \"just this once\"\n- \"I already manually tested it\"\n- \"Tests after achieve the same purpose\"\n- \"It's about spirit not ritual\"\n- \"Keep as reference\" or \"adapt existing code\"\n- \"Already spent X hours, deleting is wasteful\"\n- \"TDD is dogmatic, I'm being pragmatic\"\n- \"This is different because...\"\n\n**All of these mean: Delete code. Start over with TDD.**\n\n## Example: Bug Fix\n\n**Bug:** Empty email accepted\n\n**RED**\n```typescript\ntest('rejects empty email', async () => {\n  const result = await submitForm({ email: '' });\n  expect(result.error).toBe('Email required');\n});\n```\n\n**Verify RED**\n```bash\n$ npm test\nFAIL: expected 'Email required', got undefined\n```\n\n**GREEN**\n```typescript\nfunction submitForm(data: FormData) {\n  if (!data.email?.trim()) {\n    return { error: 'Email required' };\n  }\n  // ...\n}\n```\n\n**Verify GREEN**\n```bash\n$ npm test\nPASS\n```\n\n**REFACTOR**\nExtract validation for multiple fields if needed.\n\n## Verification Checklist\n\nBefore marking work complete:\n\n- [ ] Every new function/method has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason (feature missing, not typo)\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] Output pristine (no errors, warnings)\n- [ ] Tests use real code (mocks only if unavoidable)\n- [ ] Edge cases and errors covered\n\nCan't check all boxes? You skipped TDD. Start over.\n\n## When Stuck\n\n| Problem | Solution |\n|---------|----------|\n| Don't know how to test | Write wished-for API. Write assertion first. Ask your human partner. |\n| Test too complicated | Design too complicated. Simplify interface. |\n| Must mock everything | Code too coupled. Use dependency injection. |\n| Test setup huge | Extract helpers. Still complex? Simplify design. |\n\n## Debugging Integration\n\nBug found? Write failing test reproducing it. Follow TDD cycle. Test proves fix and prevents regression.\n\nNever fix bugs without a test.\n\n## Testing Anti-Patterns\n\nWhen adding mocks or test utilities, read @testing-anti-patterns.md to avoid common pitfalls:\n- Testing mock behavior instead of real behavior\n- Adding test-only methods to production classes\n- Mocking without understanding dependencies\n\n## Final Rule\n\n```\nProduction code  test exists and failed first\nOtherwise  not TDD\n```\n\nNo exceptions without your human partner's permission.\n",
        "skills/validation-first-approach/testing-anti-patterns.md": "# Testing Anti-Patterns\n\n**Load this reference when:** writing or changing tests, adding mocks, or tempted to add test-only methods to production code.\n\n## Overview\n\nTests must verify real behavior, not mock behavior. Mocks are a means to isolate, not the thing being tested.\n\n**Core principle:** Test what the code does, not what the mocks do.\n\n**Following strict TDD prevents these anti-patterns.**\n\n## The Iron Laws\n\n```\n1. NEVER test mock behavior\n2. NEVER add test-only methods to production classes\n3. NEVER mock without understanding dependencies\n```\n\n## Anti-Pattern 1: Testing Mock Behavior\n\n**The violation:**\n```typescript\n//  BAD: Testing that the mock exists\ntest('renders sidebar', () => {\n  render(<Page />);\n  expect(screen.getByTestId('sidebar-mock')).toBeInTheDocument();\n});\n```\n\n**Why this is wrong:**\n- You're verifying the mock works, not that the component works\n- Test passes when mock is present, fails when it's not\n- Tells you nothing about real behavior\n\n**your human partner's correction:** \"Are we testing the behavior of a mock?\"\n\n**The fix:**\n```typescript\n//  GOOD: Test real component or don't mock it\ntest('renders sidebar', () => {\n  render(<Page />);  // Don't mock sidebar\n  expect(screen.getByRole('navigation')).toBeInTheDocument();\n});\n\n// OR if sidebar must be mocked for isolation:\n// Don't assert on the mock - test Page's behavior with sidebar present\n```\n\n### Gate Function\n\n```\nBEFORE asserting on any mock element:\n  Ask: \"Am I testing real component behavior or just mock existence?\"\n\n  IF testing mock existence:\n    STOP - Delete the assertion or unmock the component\n\n  Test real behavior instead\n```\n\n## Anti-Pattern 2: Test-Only Methods in Production\n\n**The violation:**\n```typescript\n//  BAD: destroy() only used in tests\nclass Session {\n  async destroy() {  // Looks like production API!\n    await this._workspaceManager?.destroyWorkspace(this.id);\n    // ... cleanup\n  }\n}\n\n// In tests\nafterEach(() => session.destroy());\n```\n\n**Why this is wrong:**\n- Production class polluted with test-only code\n- Dangerous if accidentally called in production\n- Violates YAGNI and separation of concerns\n- Confuses object lifecycle with entity lifecycle\n\n**The fix:**\n```typescript\n//  GOOD: Test utilities handle test cleanup\n// Session has no destroy() - it's stateless in production\n\n// In test-utils/\nexport async function cleanupSession(session: Session) {\n  const workspace = session.getWorkspaceInfo();\n  if (workspace) {\n    await workspaceManager.destroyWorkspace(workspace.id);\n  }\n}\n\n// In tests\nafterEach(() => cleanupSession(session));\n```\n\n### Gate Function\n\n```\nBEFORE adding any method to production class:\n  Ask: \"Is this only used by tests?\"\n\n  IF yes:\n    STOP - Don't add it\n    Put it in test utilities instead\n\n  Ask: \"Does this class own this resource's lifecycle?\"\n\n  IF no:\n    STOP - Wrong class for this method\n```\n\n## Anti-Pattern 3: Mocking Without Understanding\n\n**The violation:**\n```typescript\n//  BAD: Mock breaks test logic\ntest('detects duplicate server', () => {\n  // Mock prevents config write that test depends on!\n  vi.mock('ToolCatalog', () => ({\n    discoverAndCacheTools: vi.fn().mockResolvedValue(undefined)\n  }));\n\n  await addServer(config);\n  await addServer(config);  // Should throw - but won't!\n});\n```\n\n**Why this is wrong:**\n- Mocked method had side effect test depended on (writing config)\n- Over-mocking to \"be safe\" breaks actual behavior\n- Test passes for wrong reason or fails mysteriously\n\n**The fix:**\n```typescript\n//  GOOD: Mock at correct level\ntest('detects duplicate server', () => {\n  // Mock the slow part, preserve behavior test needs\n  vi.mock('MCPServerManager'); // Just mock slow server startup\n\n  await addServer(config);  // Config written\n  await addServer(config);  // Duplicate detected \n});\n```\n\n### Gate Function\n\n```\nBEFORE mocking any method:\n  STOP - Don't mock yet\n\n  1. Ask: \"What side effects does the real method have?\"\n  2. Ask: \"Does this test depend on any of those side effects?\"\n  3. Ask: \"Do I fully understand what this test needs?\"\n\n  IF depends on side effects:\n    Mock at lower level (the actual slow/external operation)\n    OR use test doubles that preserve necessary behavior\n    NOT the high-level method the test depends on\n\n  IF unsure what test depends on:\n    Run test with real implementation FIRST\n    Observe what actually needs to happen\n    THEN add minimal mocking at the right level\n\n  Red flags:\n    - \"I'll mock this to be safe\"\n    - \"This might be slow, better mock it\"\n    - Mocking without understanding the dependency chain\n```\n\n## Anti-Pattern 4: Incomplete Mocks\n\n**The violation:**\n```typescript\n//  BAD: Partial mock - only fields you think you need\nconst mockResponse = {\n  status: 'success',\n  data: { userId: '123', name: 'Alice' }\n  // Missing: metadata that downstream code uses\n};\n\n// Later: breaks when code accesses response.metadata.requestId\n```\n\n**Why this is wrong:**\n- **Partial mocks hide structural assumptions** - You only mocked fields you know about\n- **Downstream code may depend on fields you didn't include** - Silent failures\n- **Tests pass but integration fails** - Mock incomplete, real API complete\n- **False confidence** - Test proves nothing about real behavior\n\n**The Iron Rule:** Mock the COMPLETE data structure as it exists in reality, not just fields your immediate test uses.\n\n**The fix:**\n```typescript\n//  GOOD: Mirror real API completeness\nconst mockResponse = {\n  status: 'success',\n  data: { userId: '123', name: 'Alice' },\n  metadata: { requestId: 'req-789', timestamp: 1234567890 }\n  // All fields real API returns\n};\n```\n\n### Gate Function\n\n```\nBEFORE creating mock responses:\n  Check: \"What fields does the real API response contain?\"\n\n  Actions:\n    1. Examine actual API response from docs/examples\n    2. Include ALL fields system might consume downstream\n    3. Verify mock matches real response schema completely\n\n  Critical:\n    If you're creating a mock, you must understand the ENTIRE structure\n    Partial mocks fail silently when code depends on omitted fields\n\n  If uncertain: Include all documented fields\n```\n\n## Anti-Pattern 5: Integration Tests as Afterthought\n\n**The violation:**\n```\n Implementation complete\n No tests written\n\"Ready for testing\"\n```\n\n**Why this is wrong:**\n- Testing is part of implementation, not optional follow-up\n- TDD would have caught this\n- Can't claim complete without tests\n\n**The fix:**\n```\nTDD cycle:\n1. Write failing test\n2. Implement to pass\n3. Refactor\n4. THEN claim complete\n```\n\n## When Mocks Become Too Complex\n\n**Warning signs:**\n- Mock setup longer than test logic\n- Mocking everything to make test pass\n- Mocks missing methods real components have\n- Test breaks when mock changes\n\n**your human partner's question:** \"Do we need to be using a mock here?\"\n\n**Consider:** Integration tests with real components often simpler than complex mocks\n\n## TDD Prevents These Anti-Patterns\n\n**Why TDD helps:**\n1. **Write test first**  Forces you to think about what you're actually testing\n2. **Watch it fail**  Confirms test tests real behavior, not mocks\n3. **Minimal implementation**  No test-only methods creep in\n4. **Real dependencies**  You see what the test actually needs before mocking\n\n**If you're testing mock behavior, you violated TDD** - you added mocks without watching test fail against real code first.\n\n## Quick Reference\n\n| Anti-Pattern | Fix |\n|--------------|-----|\n| Assert on mock elements | Test real component or unmock it |\n| Test-only methods in production | Move to test utilities |\n| Mock without understanding | Understand dependencies first, mock minimally |\n| Incomplete mocks | Mirror real API completely |\n| Tests as afterthought | TDD - tests first |\n| Over-complex mocks | Consider integration tests |\n\n## Red Flags\n\n- Assertion checks for `*-mock` test IDs\n- Methods only called in test files\n- Mock setup is >50% of test\n- Test fails when you remove mock\n- Can't explain why mock is needed\n- Mocking \"just to be safe\"\n\n## The Bottom Line\n\n**Mocks are tools to isolate, not things to test.**\n\nIf TDD reveals you're testing mock behavior, you've gone wrong.\n\nFix: Test real behavior or question why you're mocking at all.\n",
        "skills/video-archival-system/SKILL.md": "---\nname: video-archival-system\ndescription: Download and archive YouTube video content. Retrieves videos with metadata while respecting platform policies.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Video Downloader\n\nThis skill downloads videos from YouTube and other platforms directly to your computer.\n\n## When to Use This Skill\n\n- Downloading YouTube videos for offline viewing\n- Saving educational content for reference\n- Archiving important videos\n- Getting video files for editing or repurposing\n- Downloading your own content from platforms\n- Saving conference talks or webinars\n\n## What This Skill Does\n\n1. **Downloads Videos**: Fetches videos from YouTube and other platforms\n2. **Quality Selection**: Lets you choose resolution (480p, 720p, 1080p, 4K)\n3. **Format Options**: Downloads in various formats (MP4, WebM, audio-only)\n4. **Batch Downloads**: Can download multiple videos or playlists\n5. **Metadata Preservation**: Saves title, description, and thumbnail\n\n## How to Use\n\n### Basic Download\n\n```\nDownload this YouTube video: https://youtube.com/watch?v=...\n```\n\n```\nDownload this video in 1080p quality\n```\n\n### Audio Only\n\n```\nDownload the audio from this YouTube video as MP3\n```\n\n### Playlist Download\n\n```\nDownload all videos from this YouTube playlist: [URL]\n```\n\n### Batch Download\n\n```\nDownload these 5 YouTube videos:\n1. [URL]\n2. [URL]\n...\n```\n\n## Example\n\n**User**: \"Download this YouTube video: https://youtube.com/watch?v=abc123\"\n\n**Output**:\n```\nDownloading from YouTube...\n\nVideo: \"How to Build Products Users Love\"\nChannel: Lenny's Podcast\nDuration: 45:32\nQuality: 1080p\n\nProgress:  100%\n\n Downloaded: how-to-build-products-users-love.mp4\n Saved thumbnail: how-to-build-products-users-love.jpg\n Size: 342 MB\n\nSaved to: ~/Downloads/\n```\n\n**Inspired by:** Lenny's workflow from his newsletter\n\n## Important Notes\n\n **Copyright & Fair Use**\n- Only download videos you have permission to download\n- Respect copyright laws and platform terms of service\n- Use for personal, educational, or fair use purposes\n- Don't redistribute copyrighted content\n\n## Tips\n\n- Specify quality if you need lower file size (720p vs 1080p)\n- Use audio-only for podcasts or music to save space\n- Download to a dedicated folder to stay organized\n- Check file size before downloading on slow connections\n\n## Common Use Cases\n\n- **Education**: Save tutorials and courses for offline learning\n- **Research**: Archive videos for reference\n- **Content Creation**: Download your own content from platforms\n- **Backup**: Save important videos before they're removed\n- **Offline Viewing**: Watch videos without internet access",
        "skills/visual-composition/SKILL.md": "---\nname: visual-composition\ndescription: Design visual layouts and compositions for web and print. Creates balanced designs with typography, color, and spacing principles applied consistently.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\nThese are instructions for creating design philosophies - aesthetic movements that are then EXPRESSED VISUALLY. Output only .md files, .pdf files, and .png files.\n\nComplete this in two steps:\n1. Design Philosophy Creation (.md file)\n2. Express by creating it on a canvas (.pdf file or .png file)\n\nFirst, undertake this task:\n\n## DESIGN PHILOSOPHY CREATION\n\nTo begin, create a VISUAL PHILOSOPHY (not layouts or templates) that will be interpreted through:\n- Form, space, color, composition\n- Images, graphics, shapes, patterns\n- Minimal text as visual accent\n\n### THE CRITICAL UNDERSTANDING\n- What is received: Some subtle input or instructions by the user that should be taken into account, but used as a foundation; it should not constrain creative freedom.\n- What is created: A design philosophy/aesthetic movement.\n- What happens next: Then, the same version receives the philosophy and EXPRESSES IT VISUALLY - creating artifacts that are 90% visual design, 10% essential text.\n\nConsider this approach:\n- Write a manifesto for an art movement\n- The next phase involves making the artwork\n\nThe philosophy must emphasize: Visual expression. Spatial communication. Artistic interpretation. Minimal words.\n\n### HOW TO GENERATE A VISUAL PHILOSOPHY\n\n**Name the movement** (1-2 words): \"Brutalist Joy\" / \"Chromatic Silence\" / \"Metabolist Dreams\"\n\n**Articulate the philosophy** (4-6 paragraphs - concise but complete):\n\nTo capture the VISUAL essence, express how the philosophy manifests through:\n- Space and form\n- Color and material\n- Scale and rhythm\n- Composition and balance\n- Visual hierarchy\n\n**CRITICAL GUIDELINES:**\n- **Avoid redundancy**: Each design aspect should be mentioned once. Avoid repeating points about color theory, spatial relationships, or typographic principles unless adding new depth.\n- **Emphasize craftsmanship REPEATEDLY**: The philosophy MUST stress multiple times that the final work should appear as though it took countless hours to create, was labored over with care, and comes from someone at the absolute top of their field. This framing is essential - repeat phrases like \"meticulously crafted,\" \"the product of deep expertise,\" \"painstaking attention,\" \"master-level execution.\"\n- **Leave creative space**: Remain specific about the aesthetic direction, but concise enough that the next Claude has room to make interpretive choices also at a extremely high level of craftmanship.\n\nThe philosophy must guide the next version to express ideas VISUALLY, not through text. Information lives in design, not paragraphs.\n\n### PHILOSOPHY EXAMPLES\n\n**\"Concrete Poetry\"**\nPhilosophy: Communication through monumental form and bold geometry.\nVisual expression: Massive color blocks, sculptural typography (huge single words, tiny labels), Brutalist spatial divisions, Polish poster energy meets Le Corbusier. Ideas expressed through visual weight and spatial tension, not explanation. Text as rare, powerful gesture - never paragraphs, only essential words integrated into the visual architecture. Every element placed with the precision of a master craftsman.\n\n**\"Chromatic Language\"**\nPhilosophy: Color as the primary information system.\nVisual expression: Geometric precision where color zones create meaning. Typography minimal - small sans-serif labels letting chromatic fields communicate. Think Josef Albers' interaction meets data visualization. Information encoded spatially and chromatically. Words only to anchor what color already shows. The result of painstaking chromatic calibration.\n\n**\"Analog Meditation\"**\nPhilosophy: Quiet visual contemplation through texture and breathing room.\nVisual expression: Paper grain, ink bleeds, vast negative space. Photography and illustration dominate. Typography whispered (small, restrained, serving the visual). Japanese photobook aesthetic. Images breathe across pages. Text appears sparingly - short phrases, never explanatory blocks. Each composition balanced with the care of a meditation practice.\n\n**\"Organic Systems\"**\nPhilosophy: Natural clustering and modular growth patterns.\nVisual expression: Rounded forms, organic arrangements, color from nature through architecture. Information shown through visual diagrams, spatial relationships, iconography. Text only for key labels floating in space. The composition tells the story through expert spatial orchestration.\n\n**\"Geometric Silence\"**\nPhilosophy: Pure order and restraint.\nVisual expression: Grid-based precision, bold photography or stark graphics, dramatic negative space. Typography precise but minimal - small essential text, large quiet zones. Swiss formalism meets Brutalist material honesty. Structure communicates, not words. Every alignment the work of countless refinements.\n\n*These are condensed examples. The actual design philosophy should be 4-6 substantial paragraphs.*\n\n### ESSENTIAL PRINCIPLES\n- **VISUAL PHILOSOPHY**: Create an aesthetic worldview to be expressed through design\n- **MINIMAL TEXT**: Always emphasize that text is sparse, essential-only, integrated as visual element - never lengthy\n- **SPATIAL EXPRESSION**: Ideas communicate through space, form, color, composition - not paragraphs\n- **ARTISTIC FREEDOM**: The next Claude interprets the philosophy visually - provide creative room\n- **PURE DESIGN**: This is about making ART OBJECTS, not documents with decoration\n- **EXPERT CRAFTSMANSHIP**: Repeatedly emphasize the final work must look meticulously crafted, labored over with care, the product of countless hours by someone at the top of their field\n\n**The design philosophy should be 4-6 paragraphs long.** Fill it with poetic design philosophy that brings together the core vision. Avoid repeating the same points. Keep the design philosophy generic without mentioning the intention of the art, as if it can be used wherever. Output the design philosophy as a .md file.\n\n---\n\n## DEDUCING THE SUBTLE REFERENCE\n\n**CRITICAL STEP**: Before creating the canvas, identify the subtle conceptual thread from the original request.\n\n**THE ESSENTIAL PRINCIPLE**:\nThe topic is a **subtle, niche reference embedded within the art itself** - not always literal, always sophisticated. Someone familiar with the subject should feel it intuitively, while others simply experience a masterful abstract composition. The design philosophy provides the aesthetic language. The deduced topic provides the soul - the quiet conceptual DNA woven invisibly into form, color, and composition.\n\nThis is **VERY IMPORTANT**: The reference must be refined so it enhances the work's depth without announcing itself. Think like a jazz musician quoting another song - only those who know will catch it, but everyone appreciates the music.\n\n---\n\n## CANVAS CREATION\n\nWith both the philosophy and the conceptual framework established, express it on a canvas. Take a moment to gather thoughts and clear the mind. Use the design philosophy created and the instructions below to craft a masterpiece, embodying all aspects of the philosophy with expert craftsmanship.\n\n**IMPORTANT**: For any type of content, even if the user requests something for a movie/game/book, the approach should still be sophisticated. Never lose sight of the idea that this should be art, not something that's cartoony or amateur.\n\nTo create museum or magazine quality work, use the design philosophy as the foundation. Create one single page, highly visual, design-forward PDF or PNG output (unless asked for more pages). Generally use repeating patterns and perfect shapes. Treat the abstract philosophical design as if it were a scientific bible, borrowing the visual language of systematic observationdense accumulation of marks, repeated elements, or layered patterns that build meaning through patient repetition and reward sustained viewing. Add sparse, clinical typography and systematic reference markers that suggest this could be a diagram from an imaginary discipline, treating the invisible subject with the same reverence typically reserved for documenting observable phenomena. Anchor the piece with simple phrase(s) or details positioned subtly, using a limited color palette that feels intentional and cohesive. Embrace the paradox of using analytical visual language to express ideas about human experience: the result should feel like an artifact that proves something ephemeral can be studied, mapped, and understood through careful attention. This is true art. \n\n**Text as a contextual element**: Text is always minimal and visual-first, but let context guide whether that means whisper-quiet labels or bold typographic gestures. A punk venue poster might have larger, more aggressive type than a minimalist ceramics studio identity. Most of the time, font should be thin. All use of fonts must be design-forward and prioritize visual communication. Regardless of text scale, nothing falls off the page and nothing overlaps. Every element must be contained within the canvas boundaries with proper margins. Check carefully that all text, graphics, and visual elements have breathing room and clear separation. This is non-negotiable for professional execution. **IMPORTANT: Use different fonts if writing text. Search the `./canvas-fonts` directory. Regardless of approach, sophistication is non-negotiable.**\n\nDownload and use whatever fonts are needed to make this a reality. Get creative by making the typography actually part of the art itself -- if the art is abstract, bring the font onto the canvas, not typeset digitally.\n\nTo push boundaries, follow design instinct/intuition while using the philosophy as a guiding principle. Embrace ultimate design freedom and choice. Push aesthetics and design to the frontier. \n\n**CRITICAL**: To achieve human-crafted quality (not AI-generated), create work that looks like it took countless hours. Make it appear as though someone at the absolute top of their field labored over every detail with painstaking care. Ensure the composition, spacing, color choices, typography - everything screams expert-level craftsmanship. Double-check that nothing overlaps, formatting is flawless, every detail perfect. Create something that could be shown to people to prove expertise and rank as undeniably impressive.\n\nOutput the final result as a single, downloadable .pdf or .png file, alongside the design philosophy used as a .md file.\n\n---\n\n## FINAL STEP\n\n**IMPORTANT**: The user ALREADY said \"It isn't perfect enough. It must be pristine, a masterpiece if craftsmanship, as if it were about to be displayed in a museum.\"\n\n**CRITICAL**: To refine the work, avoid adding more graphics; instead refine what has been created and make it extremely crisp, respecting the design philosophy and the principles of minimalism entirely. Rather than adding a fun filter or refactoring a font, consider how to make the existing composition more cohesive with the art. If the instinct is to call a new function or draw a new shape, STOP and instead ask: \"How can I make what's already here more of a piece of art?\"\n\nTake a second pass. Go back to the code and refine/polish further to make this a philosophically designed masterpiece.\n\n## MULTI-PAGE OPTION\n\nTo create additional pages when requested, create more creative pages along the same lines as the design philosophy but distinctly different as well. Bundle those pages in the same .pdf or many .pngs. Treat the first page as just a single page in a whole coffee table book waiting to be filled. Make the next pages unique twists and memories of the original. Have them almost tell a story in a very tasteful way. Exercise full creative freedom.",
        "skills/visual-quality-improver/SKILL.md": "---\nname: visual-quality-improver\ndescription: Enhance and improve image quality and visual content. Applies enhancement techniques, color correction, and optimization transformations.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Image Enhancer\n\nThis skill takes your images and screenshots and makes them look bettersharper, clearer, and more professional.\n\n## When to Use This Skill\n\n- Improving screenshot quality for blog posts or documentation\n- Enhancing images before sharing on social media\n- Preparing images for presentations or reports\n- Upscaling low-resolution images\n- Sharpening blurry photos\n- Cleaning up compressed images\n\n## What This Skill Does\n\n1. **Analyzes Image Quality**: Checks resolution, sharpness, and compression artifacts\n2. **Enhances Resolution**: Upscales images intelligently\n3. **Improves Sharpness**: Enhances edges and details\n4. **Reduces Artifacts**: Cleans up compression artifacts and noise\n5. **Optimizes for Use Case**: Adjusts based on intended use (web, print, social media)\n\n## How to Use\n\n### Basic Enhancement\n\n```\nImprove the image quality of screenshot.png\n```\n\n```\nEnhance all images in this folder\n```\n\n### Specific Improvements\n\n```\nUpscale this image to 4K resolution\n```\n\n```\nSharpen this blurry screenshot\n```\n\n```\nReduce compression artifacts in this image\n```\n\n### Batch Processing\n\n```\nImprove the quality of all PNG files in this directory\n```\n\n## Example\n\n**User**: \"Improve the image quality of screenshot-2024.png\"\n\n**Output**:\n```\nAnalyzing screenshot-2024.png...\n\nCurrent specs:\n- Resolution: 1920x1080\n- Format: PNG\n- Quality: Good, but slight blur\n\nEnhancements applied:\n Upscaled to 2560x1440 (retina)\n Sharpened edges\n Enhanced text clarity\n Optimized file size\n\nSaved as: screenshot-2024-enhanced.png\nOriginal preserved as: screenshot-2024-original.png\n```\n\n**Inspired by:** Lenny Rachitsky's workflow from his newsletter - used for screenshots in his articles\n\n## Tips\n\n- Always keeps original files as backup\n- Works best with screenshots and digital images\n- Can batch process entire folders\n- Specify output format if needed (PNG for quality, JPG for smaller size)\n- For social media, mention the platform for optimal sizing\n\n## Common Use Cases\n\n- **Blog Posts**: Enhance screenshots before publishing\n- **Documentation**: Make UI screenshots crystal clear\n- **Social Media**: Optimize images for Twitter, LinkedIn, Instagram\n- **Presentations**: Upscale images for large screens\n- **Print Materials**: Increase resolution for physical media\n\n",
        "skills/voice-ai-integration/SKILL.md": "---\nname: voice-ai-integration\ndescription: Build voice-enabled AI applications with speech recognition, text-to-speech, and voice-based interactions. Supports multiple voice providers and real-time processing. Use when creating voice assistants, voice-controlled applications, audio interfaces, or hands-free AI systems.\n---\n\n# Voice AI Integration\n\nBuild intelligent voice-enabled AI applications that understand spoken language and respond naturally through audio, creating seamless voice-first user experiences.\n\n## Overview\n\nVoice AI systems combine three key capabilities:\n1. **Speech Recognition** - Convert audio input to text\n2. **Natural Language Processing** - Understand intent and context\n3. **Text-to-Speech** - Generate natural-sounding responses\n\n## Speech Recognition Providers\n\nSee [examples/speech_recognition_providers.py](examples/speech_recognition_providers.py) for implementations:\n- **Google Cloud Speech-to-Text**: High accuracy with automatic punctuation\n- **OpenAI Whisper**: Robust multilingual speech recognition\n- **Azure Speech Services**: Enterprise-grade speech recognition\n- **AssemblyAI**: Async processing with high accuracy\n\n## Text-to-Speech Providers\n\nSee [examples/text_to_speech_providers.py](examples/text_to_speech_providers.py) for implementations:\n- **Google Cloud TTS**: Natural voices with multiple language support\n- **OpenAI TTS**: Simple integration with high-quality output\n- **Azure Speech Services**: Enterprise TTS with neural voices\n- **Eleven Labs**: Premium voices with emotional control\n\n## Voice Assistant Architecture\n\nSee [examples/voice_assistant.py](examples/voice_assistant.py) for `VoiceAssistant`:\n- Complete voice pipeline: STT  NLP  TTS\n- Conversation history management\n- Multi-provider support (OpenAI, Google, Azure, etc.)\n- Async processing for responsive interactions\n\n## Real-Time Voice Processing\n\nSee [examples/realtime_voice_processor.py](examples/realtime_voice_processor.py) for `RealTimeVoiceProcessor`:\n- Stream audio input from microphone\n- Stream audio output to speakers\n- Voice Activity Detection (VAD)\n- Configurable sample rates and chunk sizes\n\n## Voice Agent Applications\n\n### Voice-Controlled Smart Home\n```python\nclass SmartHomeVoiceAgent:\n    def __init__(self):\n        self.voice_assistant = VoiceAssistant()\n        self.devices = {\n            \"lights\": SmartLights(),\n            \"temperature\": SmartThermostat(),\n            \"security\": SecuritySystem()\n        }\n\n    async def handle_voice_command(self, audio_input):\n        # Get text from voice\n        command_text = await self.voice_assistant.process_voice_input(audio_input)\n\n        # Parse intent\n        intent = parse_smart_home_intent(command_text)\n\n        # Execute command\n        if intent.action == \"turn_on_lights\":\n            self.devices[\"lights\"].turn_on(intent.room)\n        elif intent.action == \"set_temperature\":\n            self.devices[\"temperature\"].set(intent.value)\n\n        # Confirm with voice\n        response = f\"I've {intent.action_description}\"\n        audio_output = await self.voice_assistant.synthesize_response(response)\n\n        return audio_output\n```\n\n### Voice Meeting Transcription\n```python\nclass VoiceMeetingRecorder:\n    def __init__(self):\n        self.processor = RealTimeVoiceProcessor()\n        self.transcripts = []\n\n    async def record_and_transcribe_meeting(self, duration_seconds=3600):\n        audio_stream = self.processor.stream_audio_input()\n\n        buffer = []\n        chunk_duration = 30  # Transcribe every 30 seconds\n\n        for audio_chunk in audio_stream:\n            buffer.append(audio_chunk)\n\n            if sum(len(chunk) for chunk in buffer) >= chunk_duration * 16000:\n                # Transcribe chunk\n                transcript = transcribe_audio_whisper(buffer)\n                self.transcripts.append({\n                    \"timestamp\": datetime.now(),\n                    \"text\": transcript\n                })\n                buffer = []\n\n        return self.transcripts\n```\n\n## Best Practices\n\n### Audio Quality\n-  Use 16kHz sample rate for speech recognition\n-  Handle background noise filtering\n-  Implement voice activity detection (VAD)\n-  Normalize audio levels\n-  Use appropriate audio format (WAV for quality)\n\n### Latency Optimization\n-  Use low-latency STT models\n-  Implement streaming transcription\n-  Cache common responses\n-  Use async processing\n-  Minimize network round trips\n\n### Error Handling\n-  Handle network failures gracefully\n-  Implement fallback voices/providers\n-  Log audio processing failures\n-  Validate audio quality before processing\n-  Implement retry logic\n\n### Privacy & Security\n-  Encrypt audio in transit\n-  Delete audio after processing\n-  Implement user consent mechanisms\n-  Log access to audio data\n-  Comply with data regulations (GDPR, CCPA)\n\n## Common Challenges & Solutions\n\n### Challenge: Accents and Dialects\n**Solutions**:\n- Use multilingual models\n- Fine-tune on regional data\n- Implement language detection\n- Use domain-specific vocabularies\n\n### Challenge: Background Noise\n**Solutions**:\n- Implement noise filtering\n- Use beamforming techniques\n- Pre-process audio with noise removal\n- Deploy microphone arrays\n\n### Challenge: Long Audio Files\n**Solutions**:\n- Implement chunked processing\n- Use streaming APIs\n- Split into speaker turns\n- Implement caching\n\n## Frameworks & Libraries\n\n### Speech Recognition\n- OpenAI Whisper\n- Google Cloud Speech-to-Text\n- Azure Speech Services\n- AssemblyAI\n- DeepSpeech\n\n### Text-to-Speech\n- Google Cloud Text-to-Speech\n- OpenAI TTS\n- Azure Text-to-Speech\n- Eleven Labs\n- Tacotron 2\n\n## Getting Started\n\n1. Choose STT and TTS providers\n2. Set up authentication\n3. Build basic voice pipeline\n4. Add conversation management\n5. Implement error handling\n6. Test with real users\n7. Monitor and optimize latency\n\n",
        "skills/web-interface-architect/SKILL.md": "---\nname: web-interface-architect\ndescription: Architect and design web application interfaces. Plans component systems, interaction flows, and responsive layouts for web platforms.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\nThis skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend's aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don't hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision.\n",
        "skills/word-document-processor/SKILL.md": "---\nname: word-document-processor\ndescription: Comprehensive word document processing with full format support. Handles creation, editing, formatting preservation, tracked changes, and metadata management.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
        "skills/word-document-processor/docx-js.md": "# DOCX Library Tutorial\n\nGenerate .docx files with JavaScript/TypeScript.\n\n**Important: Read this entire document before starting.** Critical formatting rules and common pitfalls are covered throughout - skipping sections may result in corrupted files or rendering issues.\n\n## Setup\nAssumes docx is already installed globally\nIf not installed: `npm install -g docx`\n\n```javascript\nconst { Document, Packer, Paragraph, TextRun, Table, TableRow, TableCell, ImageRun, Media, \n        Header, Footer, AlignmentType, PageOrientation, LevelFormat, ExternalHyperlink, \n        InternalHyperlink, TableOfContents, HeadingLevel, BorderStyle, WidthType, TabStopType, \n        TabStopPosition, UnderlineType, ShadingType, VerticalAlign, SymbolRun, PageNumber,\n        FootnoteReferenceRun, Footnote, PageBreak } = require('docx');\n\n// Create & Save\nconst doc = new Document({ sections: [{ children: [/* content */] }] });\nPacker.toBuffer(doc).then(buffer => fs.writeFileSync(\"doc.docx\", buffer)); // Node.js\nPacker.toBlob(doc).then(blob => { /* download logic */ }); // Browser\n```\n\n## Text & Formatting\n```javascript\n// IMPORTANT: Never use \\n for line breaks - always use separate Paragraph elements\n//  WRONG: new TextRun(\"Line 1\\nLine 2\")\n//  CORRECT: new Paragraph({ children: [new TextRun(\"Line 1\")] }), new Paragraph({ children: [new TextRun(\"Line 2\")] })\n\n// Basic text with all formatting options\nnew Paragraph({\n  alignment: AlignmentType.CENTER,\n  spacing: { before: 200, after: 200 },\n  indent: { left: 720, right: 720 },\n  children: [\n    new TextRun({ text: \"Bold\", bold: true }),\n    new TextRun({ text: \"Italic\", italics: true }),\n    new TextRun({ text: \"Underlined\", underline: { type: UnderlineType.DOUBLE, color: \"FF0000\" } }),\n    new TextRun({ text: \"Colored\", color: \"FF0000\", size: 28, font: \"Arial\" }), // Arial default\n    new TextRun({ text: \"Highlighted\", highlight: \"yellow\" }),\n    new TextRun({ text: \"Strikethrough\", strike: true }),\n    new TextRun({ text: \"x2\", superScript: true }),\n    new TextRun({ text: \"H2O\", subScript: true }),\n    new TextRun({ text: \"SMALL CAPS\", smallCaps: true }),\n    new SymbolRun({ char: \"2022\", font: \"Symbol\" }), // Bullet \n    new SymbolRun({ char: \"00A9\", font: \"Arial\" })   // Copyright  - Arial for symbols\n  ]\n})\n```\n\n## Styles & Professional Formatting\n\n```javascript\nconst doc = new Document({\n  styles: {\n    default: { document: { run: { font: \"Arial\", size: 24 } } }, // 12pt default\n    paragraphStyles: [\n      // Document title style - override built-in Title style\n      { id: \"Title\", name: \"Title\", basedOn: \"Normal\",\n        run: { size: 56, bold: true, color: \"000000\", font: \"Arial\" },\n        paragraph: { spacing: { before: 240, after: 120 }, alignment: AlignmentType.CENTER } },\n      // IMPORTANT: Override built-in heading styles by using their exact IDs\n      { id: \"Heading1\", name: \"Heading 1\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 32, bold: true, color: \"000000\", font: \"Arial\" }, // 16pt\n        paragraph: { spacing: { before: 240, after: 240 }, outlineLevel: 0 } }, // Required for TOC\n      { id: \"Heading2\", name: \"Heading 2\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 28, bold: true, color: \"000000\", font: \"Arial\" }, // 14pt\n        paragraph: { spacing: { before: 180, after: 180 }, outlineLevel: 1 } },\n      // Custom styles use your own IDs\n      { id: \"myStyle\", name: \"My Style\", basedOn: \"Normal\",\n        run: { size: 28, bold: true, color: \"000000\" },\n        paragraph: { spacing: { after: 120 }, alignment: AlignmentType.CENTER } }\n    ],\n    characterStyles: [{ id: \"myCharStyle\", name: \"My Char Style\",\n      run: { color: \"FF0000\", bold: true, underline: { type: UnderlineType.SINGLE } } }]\n  },\n  sections: [{\n    properties: { page: { margin: { top: 1440, right: 1440, bottom: 1440, left: 1440 } } },\n    children: [\n      new Paragraph({ heading: HeadingLevel.TITLE, children: [new TextRun(\"Document Title\")] }), // Uses overridden Title style\n      new Paragraph({ heading: HeadingLevel.HEADING_1, children: [new TextRun(\"Heading 1\")] }), // Uses overridden Heading1 style\n      new Paragraph({ style: \"myStyle\", children: [new TextRun(\"Custom paragraph style\")] }),\n      new Paragraph({ children: [\n        new TextRun(\"Normal with \"),\n        new TextRun({ text: \"custom char style\", style: \"myCharStyle\" })\n      ]})\n    ]\n  }]\n});\n```\n\n**Professional Font Combinations:**\n- **Arial (Headers) + Arial (Body)** - Most universally supported, clean and professional\n- **Times New Roman (Headers) + Arial (Body)** - Classic serif headers with modern sans-serif body\n- **Georgia (Headers) + Verdana (Body)** - Optimized for screen reading, elegant contrast\n\n**Key Styling Principles:**\n- **Override built-in styles**: Use exact IDs like \"Heading1\", \"Heading2\", \"Heading3\" to override Word's built-in heading styles\n- **HeadingLevel constants**: `HeadingLevel.HEADING_1` uses \"Heading1\" style, `HeadingLevel.HEADING_2` uses \"Heading2\" style, etc.\n- **Include outlineLevel**: Set `outlineLevel: 0` for H1, `outlineLevel: 1` for H2, etc. to ensure TOC works correctly\n- **Use custom styles** instead of inline formatting for consistency\n- **Set a default font** using `styles.default.document.run.font` - Arial is universally supported\n- **Establish visual hierarchy** with different font sizes (titles > headers > body)\n- **Add proper spacing** with `before` and `after` paragraph spacing\n- **Use colors sparingly**: Default to black (000000) and shades of gray for titles and headings (heading 1, heading 2, etc.)\n- **Set consistent margins** (1440 = 1 inch is standard)\n\n\n## Lists (ALWAYS USE PROPER LISTS - NEVER USE UNICODE BULLETS)\n```javascript\n// Bullets - ALWAYS use the numbering config, NOT unicode symbols\n// CRITICAL: Use LevelFormat.BULLET constant, NOT the string \"bullet\"\nconst doc = new Document({\n  numbering: {\n    config: [\n      { reference: \"bullet-list\",\n        levels: [{ level: 0, format: LevelFormat.BULLET, text: \"\", alignment: AlignmentType.LEFT,\n          style: { paragraph: { indent: { left: 720, hanging: 360 } } } }] },\n      { reference: \"first-numbered-list\",\n        levels: [{ level: 0, format: LevelFormat.DECIMAL, text: \"%1.\", alignment: AlignmentType.LEFT,\n          style: { paragraph: { indent: { left: 720, hanging: 360 } } } }] },\n      { reference: \"second-numbered-list\", // Different reference = restarts at 1\n        levels: [{ level: 0, format: LevelFormat.DECIMAL, text: \"%1.\", alignment: AlignmentType.LEFT,\n          style: { paragraph: { indent: { left: 720, hanging: 360 } } } }] }\n    ]\n  },\n  sections: [{\n    children: [\n      // Bullet list items\n      new Paragraph({ numbering: { reference: \"bullet-list\", level: 0 },\n        children: [new TextRun(\"First bullet point\")] }),\n      new Paragraph({ numbering: { reference: \"bullet-list\", level: 0 },\n        children: [new TextRun(\"Second bullet point\")] }),\n      // Numbered list items\n      new Paragraph({ numbering: { reference: \"first-numbered-list\", level: 0 },\n        children: [new TextRun(\"First numbered item\")] }),\n      new Paragraph({ numbering: { reference: \"first-numbered-list\", level: 0 },\n        children: [new TextRun(\"Second numbered item\")] }),\n      //  CRITICAL: Different reference = INDEPENDENT list that restarts at 1\n      // Same reference = CONTINUES previous numbering\n      new Paragraph({ numbering: { reference: \"second-numbered-list\", level: 0 },\n        children: [new TextRun(\"Starts at 1 again (because different reference)\")] })\n    ]\n  }]\n});\n\n//  CRITICAL NUMBERING RULE: Each reference creates an INDEPENDENT numbered list\n// - Same reference = continues numbering (1, 2, 3... then 4, 5, 6...)\n// - Different reference = restarts at 1 (1, 2, 3... then 1, 2, 3...)\n// Use unique reference names for each separate numbered section!\n\n//  CRITICAL: NEVER use unicode bullets - they create fake lists that don't work properly\n// new TextRun(\" Item\")           // WRONG\n// new SymbolRun({ char: \"2022\" }) // WRONG\n//  ALWAYS use numbering config with LevelFormat.BULLET for real Word lists\n```\n\n## Tables\n```javascript\n// Complete table with margins, borders, headers, and bullet points\nconst tableBorder = { style: BorderStyle.SINGLE, size: 1, color: \"CCCCCC\" };\nconst cellBorders = { top: tableBorder, bottom: tableBorder, left: tableBorder, right: tableBorder };\n\nnew Table({\n  columnWidths: [4680, 4680], //  CRITICAL: Set column widths at table level - values in DXA (twentieths of a point)\n  margins: { top: 100, bottom: 100, left: 180, right: 180 }, // Set once for all cells\n  rows: [\n    new TableRow({\n      tableHeader: true,\n      children: [\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          //  CRITICAL: Always use ShadingType.CLEAR to prevent black backgrounds in Word.\n          shading: { fill: \"D5E8F0\", type: ShadingType.CLEAR }, \n          verticalAlign: VerticalAlign.CENTER,\n          children: [new Paragraph({ \n            alignment: AlignmentType.CENTER,\n            children: [new TextRun({ text: \"Header\", bold: true, size: 22 })]\n          })]\n        }),\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          shading: { fill: \"D5E8F0\", type: ShadingType.CLEAR },\n          children: [new Paragraph({ \n            alignment: AlignmentType.CENTER,\n            children: [new TextRun({ text: \"Bullet Points\", bold: true, size: 22 })]\n          })]\n        })\n      ]\n    }),\n    new TableRow({\n      children: [\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          children: [new Paragraph({ children: [new TextRun(\"Regular data\")] })]\n        }),\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          children: [\n            new Paragraph({ \n              numbering: { reference: \"bullet-list\", level: 0 },\n              children: [new TextRun(\"First bullet point\")] \n            }),\n            new Paragraph({ \n              numbering: { reference: \"bullet-list\", level: 0 },\n              children: [new TextRun(\"Second bullet point\")] \n            })\n          ]\n        })\n      ]\n    })\n  ]\n})\n```\n\n**IMPORTANT: Table Width & Borders**\n- Use BOTH `columnWidths: [width1, width2, ...]` array AND `width: { size: X, type: WidthType.DXA }` on each cell\n- Values in DXA (twentieths of a point): 1440 = 1 inch, Letter usable width = 9360 DXA (with 1\" margins)\n- Apply borders to individual `TableCell` elements, NOT the `Table` itself\n\n**Precomputed Column Widths (Letter size with 1\" margins = 9360 DXA total):**\n- **2 columns:** `columnWidths: [4680, 4680]` (equal width)\n- **3 columns:** `columnWidths: [3120, 3120, 3120]` (equal width)\n\n## Links & Navigation\n```javascript\n// TOC (requires headings) - CRITICAL: Use HeadingLevel only, NOT custom styles\n//  WRONG: new Paragraph({ heading: HeadingLevel.HEADING_1, style: \"customHeader\", children: [new TextRun(\"Title\")] })\n//  CORRECT: new Paragraph({ heading: HeadingLevel.HEADING_1, children: [new TextRun(\"Title\")] })\nnew TableOfContents(\"Table of Contents\", { hyperlink: true, headingStyleRange: \"1-3\" }),\n\n// External link\nnew Paragraph({\n  children: [new ExternalHyperlink({\n    children: [new TextRun({ text: \"Google\", style: \"Hyperlink\" })],\n    link: \"https://www.google.com\"\n  })]\n}),\n\n// Internal link & bookmark\nnew Paragraph({\n  children: [new InternalHyperlink({\n    children: [new TextRun({ text: \"Go to Section\", style: \"Hyperlink\" })],\n    anchor: \"section1\"\n  })]\n}),\nnew Paragraph({\n  children: [new TextRun(\"Section Content\")],\n  bookmark: { id: \"section1\", name: \"section1\" }\n}),\n```\n\n## Images & Media\n```javascript\n// Basic image with sizing & positioning\n// CRITICAL: Always specify 'type' parameter - it's REQUIRED for ImageRun\nnew Paragraph({\n  alignment: AlignmentType.CENTER,\n  children: [new ImageRun({\n    type: \"png\", // NEW REQUIREMENT: Must specify image type (png, jpg, jpeg, gif, bmp, svg)\n    data: fs.readFileSync(\"image.png\"),\n    transformation: { width: 200, height: 150, rotation: 0 }, // rotation in degrees\n    altText: { title: \"Logo\", description: \"Company logo\", name: \"Name\" } // IMPORTANT: All three fields are required\n  })]\n})\n```\n\n## Page Breaks\n```javascript\n// Manual page break\nnew Paragraph({ children: [new PageBreak()] }),\n\n// Page break before paragraph\nnew Paragraph({\n  pageBreakBefore: true,\n  children: [new TextRun(\"This starts on a new page\")]\n})\n\n//  CRITICAL: NEVER use PageBreak standalone - it will create invalid XML that Word cannot open\n//  WRONG: new PageBreak() \n//  CORRECT: new Paragraph({ children: [new PageBreak()] })\n```\n\n## Headers/Footers & Page Setup\n```javascript\nconst doc = new Document({\n  sections: [{\n    properties: {\n      page: {\n        margin: { top: 1440, right: 1440, bottom: 1440, left: 1440 }, // 1440 = 1 inch\n        size: { orientation: PageOrientation.LANDSCAPE },\n        pageNumbers: { start: 1, formatType: \"decimal\" } // \"upperRoman\", \"lowerRoman\", \"upperLetter\", \"lowerLetter\"\n      }\n    },\n    headers: {\n      default: new Header({ children: [new Paragraph({ \n        alignment: AlignmentType.RIGHT,\n        children: [new TextRun(\"Header Text\")]\n      })] })\n    },\n    footers: {\n      default: new Footer({ children: [new Paragraph({ \n        alignment: AlignmentType.CENTER,\n        children: [new TextRun(\"Page \"), new TextRun({ children: [PageNumber.CURRENT] }), new TextRun(\" of \"), new TextRun({ children: [PageNumber.TOTAL_PAGES] })]\n      })] })\n    },\n    children: [/* content */]\n  }]\n});\n```\n\n## Tabs\n```javascript\nnew Paragraph({\n  tabStops: [\n    { type: TabStopType.LEFT, position: TabStopPosition.MAX / 4 },\n    { type: TabStopType.CENTER, position: TabStopPosition.MAX / 2 },\n    { type: TabStopType.RIGHT, position: TabStopPosition.MAX * 3 / 4 }\n  ],\n  children: [new TextRun(\"Left\\tCenter\\tRight\")]\n})\n```\n\n## Constants & Quick Reference\n- **Underlines:** `SINGLE`, `DOUBLE`, `WAVY`, `DASH`\n- **Borders:** `SINGLE`, `DOUBLE`, `DASHED`, `DOTTED`  \n- **Numbering:** `DECIMAL` (1,2,3), `UPPER_ROMAN` (I,II,III), `LOWER_LETTER` (a,b,c)\n- **Tabs:** `LEFT`, `CENTER`, `RIGHT`, `DECIMAL`\n- **Symbols:** `\"2022\"` (), `\"00A9\"` (), `\"00AE\"` (), `\"2122\"` (), `\"00B0\"` (), `\"F070\"` (), `\"F0FC\"` ()\n\n## Critical Issues & Common Mistakes\n- **CRITICAL: PageBreak must ALWAYS be inside a Paragraph** - standalone PageBreak creates invalid XML that Word cannot open\n- **ALWAYS use ShadingType.CLEAR for table cell shading** - Never use ShadingType.SOLID (causes black background).\n- Measurements in DXA (1440 = 1 inch) | Each table cell needs 1 Paragraph | TOC requires HeadingLevel styles only\n- **ALWAYS use custom styles** with Arial font for professional appearance and proper visual hierarchy\n- **ALWAYS set a default font** using `styles.default.document.run.font` - Arial recommended\n- **ALWAYS use columnWidths array for tables** + individual cell widths for compatibility\n- **NEVER use unicode symbols for bullets** - always use proper numbering configuration with `LevelFormat.BULLET` constant (NOT the string \"bullet\")\n- **NEVER use \\n for line breaks anywhere** - always use separate Paragraph elements for each line\n- **ALWAYS use TextRun objects within Paragraph children** - never use text property directly on Paragraph\n- **CRITICAL for images**: ImageRun REQUIRES `type` parameter - always specify \"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\", or \"svg\"\n- **CRITICAL for bullets**: Must use `LevelFormat.BULLET` constant, not string \"bullet\", and include `text: \"\"` for the bullet character\n- **CRITICAL for numbering**: Each numbering reference creates an INDEPENDENT list. Same reference = continues numbering (1,2,3 then 4,5,6). Different reference = restarts at 1 (1,2,3 then 1,2,3). Use unique reference names for each separate numbered section!\n- **CRITICAL for TOC**: When using TableOfContents, headings must use HeadingLevel ONLY - do NOT add custom styles to heading paragraphs or TOC will break\n- **Tables**: Set `columnWidths` array + individual cell widths, apply borders to cells not table\n- **Set table margins at TABLE level** for consistent cell padding (avoids repetition per cell)",
        "skills/word-document-processor/ooxml.md": "# Office Open XML Technical Reference\n\n**Important: Read this entire document before starting.** This document covers:\n- [Technical Guidelines](#technical-guidelines) - Schema compliance rules and validation requirements\n- [Document Content Patterns](#document-content-patterns) - XML patterns for headings, lists, tables, formatting, etc.\n- [Document Library (Python)](#document-library-python) - Recommended approach for OOXML manipulation with automatic infrastructure setup\n- [Tracked Changes (Redlining)](#tracked-changes-redlining) - XML patterns for implementing tracked changes\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<w:pPr>`**: `<w:pStyle>`, `<w:numPr>`, `<w:spacing>`, `<w:ind>`, `<w:jc>`\n- **Whitespace**: Add `xml:space='preserve'` to `<w:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n  - **Character encoding reference**: Curly quotes `\"\"` become `&#8220;&#8221;`, apostrophe `'` becomes `&#8217;`, em-dash `` becomes `&#8212;`\n- **Tracked changes**: Use `<w:del>` and `<w:ins>` tags with `w:author=\"Claude\"` outside `<w:r>` elements\n  - **Critical**: `<w:ins>` closes with `</w:ins>`, `<w:del>` closes with `</w:del>` - never mix\n  - **RSIDs must be 8-digit hex**: Use values like `00AB1234` (only 0-9, A-F characters)\n  - **trackRevisions placement**: Add `<w:trackRevisions/>` after `<w:proofState>` in settings.xml\n- **Images**: Add to `word/media/`, reference in `document.xml`, set dimensions to prevent overflow\n\n## Document Content Patterns\n\n### Basic Structure\n```xml\n<w:p>\n  <w:r><w:t>Text content</w:t></w:r>\n</w:p>\n```\n\n### Headings and Styles\n```xml\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Title\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Document Title</w:t></w:r>\n</w:p>\n\n<w:p>\n  <w:pPr><w:pStyle w:val=\"Heading2\"/></w:pPr>\n  <w:r><w:t>Section Heading</w:t></w:r>\n</w:p>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<w:r><w:rPr><w:b/><w:bCs/></w:rPr><w:t>Bold</w:t></w:r>\n<!-- Italic -->\n<w:r><w:rPr><w:i/><w:iCs/></w:rPr><w:t>Italic</w:t></w:r>\n<!-- Underline -->\n<w:r><w:rPr><w:u w:val=\"single\"/></w:rPr><w:t>Underlined</w:t></w:r>\n<!-- Highlight -->\n<w:r><w:rPr><w:highlight w:val=\"yellow\"/></w:rPr><w:t>Highlighted</w:t></w:r>\n```\n\n### Lists\n```xml\n<!-- Numbered list -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>First item</w:t></w:r>\n</w:p>\n\n<!-- Restart numbered list at 1 - use different numId -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"2\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>New list item 1</w:t></w:r>\n</w:p>\n\n<!-- Bullet list (level 2) -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"1\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n    <w:ind w:left=\"900\"/>\n  </w:pPr>\n  <w:r><w:t>Bullet item</w:t></w:r>\n</w:p>\n```\n\n### Tables\n```xml\n<w:tbl>\n  <w:tblPr>\n    <w:tblStyle w:val=\"TableGrid\"/>\n    <w:tblW w:w=\"0\" w:type=\"auto\"/>\n  </w:tblPr>\n  <w:tblGrid>\n    <w:gridCol w:w=\"4675\"/><w:gridCol w:w=\"4675\"/>\n  </w:tblGrid>\n  <w:tr>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 1</w:t></w:r></w:p>\n    </w:tc>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 2</w:t></w:r></w:p>\n    </w:tc>\n  </w:tr>\n</w:tbl>\n```\n\n### Layout\n```xml\n<!-- Page break before new section (common pattern) -->\n<w:p>\n  <w:r>\n    <w:br w:type=\"page\"/>\n  </w:r>\n</w:p>\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Heading1\"/>\n  </w:pPr>\n  <w:r>\n    <w:t>New Section Title</w:t>\n  </w:r>\n</w:p>\n\n<!-- Centered paragraph -->\n<w:p>\n  <w:pPr>\n    <w:spacing w:before=\"240\" w:after=\"0\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Centered text</w:t></w:r>\n</w:p>\n\n<!-- Font change - paragraph level (applies to all runs) -->\n<w:p>\n  <w:pPr>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n  </w:pPr>\n  <w:r><w:t>Monospace text</w:t></w:r>\n</w:p>\n\n<!-- Font change - run level (specific to this text) -->\n<w:p>\n  <w:r>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n    <w:t>This text is Courier New</w:t>\n  </w:r>\n  <w:r><w:t> and this text uses default font</w:t></w:r>\n</w:p>\n```\n\n## File Updates\n\nWhen adding content, update these files:\n\n**`word/_rels/document.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/numbering\" Target=\"numbering.xml\"/>\n<Relationship Id=\"rId5\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"media/image1.png\"/>\n```\n\n**`[Content_Types].xml`:**\n```xml\n<Default Extension=\"png\" ContentType=\"image/png\"/>\n<Override PartName=\"/word/numbering.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.wordprocessingml.numbering+xml\"/>\n```\n\n### Images\n**CRITICAL**: Calculate dimensions to prevent page overflow and maintain aspect ratio.\n\n```xml\n<!-- Minimal required structure -->\n<w:p>\n  <w:r>\n    <w:drawing>\n      <wp:inline>\n        <wp:extent cx=\"2743200\" cy=\"1828800\"/>\n        <wp:docPr id=\"1\" name=\"Picture 1\"/>\n        <a:graphic xmlns:a=\"http://schemas.openxmlformats.org/drawingml/2006/main\">\n          <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n            <pic:pic xmlns:pic=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n              <pic:nvPicPr>\n                <pic:cNvPr id=\"0\" name=\"image1.png\"/>\n                <pic:cNvPicPr/>\n              </pic:nvPicPr>\n              <pic:blipFill>\n                <a:blip r:embed=\"rId5\"/>\n                <!-- Add for stretch fill with aspect ratio preservation -->\n                <a:stretch>\n                  <a:fillRect/>\n                </a:stretch>\n              </pic:blipFill>\n              <pic:spPr>\n                <a:xfrm>\n                  <a:ext cx=\"2743200\" cy=\"1828800\"/>\n                </a:xfrm>\n                <a:prstGeom prst=\"rect\"/>\n              </pic:spPr>\n            </pic:pic>\n          </a:graphicData>\n        </a:graphic>\n      </wp:inline>\n    </w:drawing>\n  </w:r>\n</w:p>\n```\n\n### Links (Hyperlinks)\n\n**IMPORTANT**: All hyperlinks (both internal and external) require the Hyperlink style to be defined in styles.xml. Without this style, links will look like regular text instead of blue underlined clickable links.\n\n**External Links:**\n```xml\n<!-- In document.xml -->\n<w:hyperlink r:id=\"rId5\">\n  <w:r>\n    <w:rPr><w:rStyle w:val=\"Hyperlink\"/></w:rPr>\n    <w:t>Link Text</w:t>\n  </w:r>\n</w:hyperlink>\n\n<!-- In word/_rels/document.xml.rels -->\n<Relationship Id=\"rId5\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink\" \n              Target=\"https://www.example.com/\" TargetMode=\"External\"/>\n```\n\n**Internal Links:**\n\n```xml\n<!-- Link to bookmark -->\n<w:hyperlink w:anchor=\"myBookmark\">\n  <w:r>\n    <w:rPr><w:rStyle w:val=\"Hyperlink\"/></w:rPr>\n    <w:t>Link Text</w:t>\n  </w:r>\n</w:hyperlink>\n\n<!-- Bookmark target -->\n<w:bookmarkStart w:id=\"0\" w:name=\"myBookmark\"/>\n<w:r><w:t>Target content</w:t></w:r>\n<w:bookmarkEnd w:id=\"0\"/>\n```\n\n**Hyperlink Style (required in styles.xml):**\n```xml\n<w:style w:type=\"character\" w:styleId=\"Hyperlink\">\n  <w:name w:val=\"Hyperlink\"/>\n  <w:basedOn w:val=\"DefaultParagraphFont\"/>\n  <w:uiPriority w:val=\"99\"/>\n  <w:unhideWhenUsed/>\n  <w:rPr>\n    <w:color w:val=\"467886\" w:themeColor=\"hyperlink\"/>\n    <w:u w:val=\"single\"/>\n  </w:rPr>\n</w:style>\n```\n\n## Document Library (Python)\n\nUse the Document class from `scripts/document.py` for all tracked changes and comments. It automatically handles infrastructure setup (people.xml, RSIDs, settings.xml, comment files, relationships, content types). Only use direct XML manipulation for complex scenarios not supported by the library.\n\n**Working with Unicode and Entities:**\n- **Searching**: Both entity notation and Unicode characters work - `contains=\"&#8220;Company\"` and `contains=\"\\u201cCompany\"` find the same text\n- **Replacing**: Use either entities (`&#8220;`) or Unicode (`\\u201c`) - both work and will be converted appropriately based on the file's encoding (ascii  entities, utf-8  Unicode)\n\n### Initialization\n\n**Find the docx skill root** (directory containing `scripts/` and `ooxml/`):\n```bash\n# Search for document.py to locate the skill root\n# Note: /mnt/skills is used here as an example; check your context for the actual location\nfind /mnt/skills -name \"document.py\" -path \"*/docx/scripts/*\" 2>/dev/null | head -1\n# Example output: /mnt/skills/word-document-processor/scripts/document.py\n# Skill root is: /mnt/skills/word-document-processor\n```\n\n**Run your script with PYTHONPATH** set to the docx skill root:\n```bash\nPYTHONPATH=/mnt/skills/word-document-processor python your_script.py\n```\n\n**In your script**, import from the skill root:\n```python\nfrom scripts.document import Document, DocxXMLEditor\n\n# Basic initialization (automatically creates temp copy and sets up infrastructure)\ndoc = Document('unpacked')\n\n# Customize author and initials\ndoc = Document('unpacked', author=\"John Doe\", initials=\"JD\")\n\n# Enable track revisions mode\ndoc = Document('unpacked', track_revisions=True)\n\n# Specify custom RSID (auto-generated if not provided)\ndoc = Document('unpacked', rsid=\"07DC5ECB\")\n```\n\n### Creating Tracked Changes\n\n**CRITICAL**: Only mark text that actually changes. Keep ALL unchanged text outside `<w:del>`/`<w:ins>` tags. Marking unchanged text makes edits unprofessional and harder to review.\n\n**Attribute Handling**: The Document class auto-injects attributes (w:id, w:date, w:rsidR, w:rsidDel, w16du:dateUtc, xml:space) into new elements. When preserving unchanged text from the original document, copy the original `<w:r>` element with its existing attributes to maintain document integrity.\n\n**Method Selection Guide**:\n- **Adding your own changes to regular text**: Use `replace_node()` with `<w:del>`/`<w:ins>` tags, or `suggest_deletion()` for removing entire `<w:r>` or `<w:p>` elements\n- **Partially modifying another author's tracked change**: Use `replace_node()` to nest your changes inside their `<w:ins>`/`<w:del>`\n- **Completely rejecting another author's insertion**: Use `revert_insertion()` on the `<w:ins>` element (NOT `suggest_deletion()`)\n- **Completely rejecting another author's deletion**: Use `revert_deletion()` on the `<w:del>` element to restore deleted content using tracked changes\n\n```python\n# Minimal edit - change one word: \"The report is monthly\"  \"The report is quarterly\"\n# Original: <w:r w:rsidR=\"00AB12CD\"><w:rPr><w:rFonts w:ascii=\"Calibri\"/></w:rPr><w:t>The report is monthly</w:t></w:r>\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"The report is monthly\")\nrpr = tags[0].toxml() if (tags := node.getElementsByTagName(\"w:rPr\")) else \"\"\nreplacement = f'<w:r w:rsidR=\"00AB12CD\">{rpr}<w:t>The report is </w:t></w:r><w:del><w:r>{rpr}<w:delText>monthly</w:delText></w:r></w:del><w:ins><w:r>{rpr}<w:t>quarterly</w:t></w:r></w:ins>'\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Minimal edit - change number: \"within 30 days\"  \"within 45 days\"\n# Original: <w:r w:rsidR=\"00XYZ789\"><w:rPr><w:rFonts w:ascii=\"Calibri\"/></w:rPr><w:t>within 30 days</w:t></w:r>\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"within 30 days\")\nrpr = tags[0].toxml() if (tags := node.getElementsByTagName(\"w:rPr\")) else \"\"\nreplacement = f'<w:r w:rsidR=\"00XYZ789\">{rpr}<w:t>within </w:t></w:r><w:del><w:r>{rpr}<w:delText>30</w:delText></w:r></w:del><w:ins><w:r>{rpr}<w:t>45</w:t></w:r></w:ins><w:r w:rsidR=\"00XYZ789\">{rpr}<w:t> days</w:t></w:r>'\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Complete replacement - preserve formatting even when replacing all text\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"apple\")\nrpr = tags[0].toxml() if (tags := node.getElementsByTagName(\"w:rPr\")) else \"\"\nreplacement = f'<w:del><w:r>{rpr}<w:delText>apple</w:delText></w:r></w:del><w:ins><w:r>{rpr}<w:t>banana orange</w:t></w:r></w:ins>'\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Insert new content (no attributes needed - auto-injected)\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"existing text\")\ndoc[\"word/document.xml\"].insert_after(node, '<w:ins><w:r><w:t>new text</w:t></w:r></w:ins>')\n\n# Partially delete another author's insertion\n# Original: <w:ins w:author=\"Jane Smith\" w:date=\"...\"><w:r><w:t>quarterly financial report</w:t></w:r></w:ins>\n# Goal: Delete only \"financial\" to make it \"quarterly report\"\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"5\"})\n# IMPORTANT: Preserve w:author=\"Jane Smith\" on the outer <w:ins> to maintain authorship\nreplacement = '''<w:ins w:author=\"Jane Smith\" w:date=\"2025-01-15T10:00:00Z\">\n  <w:r><w:t>quarterly </w:t></w:r>\n  <w:del><w:r><w:delText>financial </w:delText></w:r></w:del>\n  <w:r><w:t>report</w:t></w:r>\n</w:ins>'''\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Change part of another author's insertion\n# Original: <w:ins w:author=\"Jane Smith\"><w:r><w:t>in silence, safe and sound</w:t></w:r></w:ins>\n# Goal: Change \"safe and sound\" to \"soft and unbound\"\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"8\"})\nreplacement = f'''<w:ins w:author=\"Jane Smith\" w:date=\"2025-01-15T10:00:00Z\">\n  <w:r><w:t>in silence, </w:t></w:r>\n</w:ins>\n<w:ins>\n  <w:r><w:t>soft and unbound</w:t></w:r>\n</w:ins>\n<w:ins w:author=\"Jane Smith\" w:date=\"2025-01-15T10:00:00Z\">\n  <w:del><w:r><w:delText>safe and sound</w:delText></w:r></w:del>\n</w:ins>'''\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Delete entire run (use only when deleting all content; use replace_node for partial deletions)\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"text to delete\")\ndoc[\"word/document.xml\"].suggest_deletion(node)\n\n# Delete entire paragraph (in-place, handles both regular and numbered list paragraphs)\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph to delete\")\ndoc[\"word/document.xml\"].suggest_deletion(para)\n\n# Add new numbered list item\ntarget_para = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"existing list item\")\npPr = tags[0].toxml() if (tags := target_para.getElementsByTagName(\"w:pPr\")) else \"\"\nnew_item = f'<w:p>{pPr}<w:r><w:t>New item</w:t></w:r></w:p>'\ntracked_para = DocxXMLEditor.suggest_paragraph(new_item)\ndoc[\"word/document.xml\"].insert_after(target_para, tracked_para)\n# Optional: add spacing paragraph before content for better visual separation\n# spacing = DocxXMLEditor.suggest_paragraph('<w:p><w:pPr><w:pStyle w:val=\"ListParagraph\"/></w:pPr></w:p>')\n# doc[\"word/document.xml\"].insert_after(target_para, spacing + tracked_para)\n```\n\n### Adding Comments\n\n```python\n# Add comment spanning two existing tracked changes\n# Note: w:id is auto-generated. Only search by w:id if you know it from XML inspection\nstart_node = doc[\"word/document.xml\"].get_node(tag=\"w:del\", attrs={\"w:id\": \"1\"})\nend_node = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"2\"})\ndoc.add_comment(start=start_node, end=end_node, text=\"Explanation of this change\")\n\n# Add comment on a paragraph\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph text\")\ndoc.add_comment(start=para, end=para, text=\"Comment on this paragraph\")\n\n# Add comment on newly created tracked change\n# First create the tracked change\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"old\")\nnew_nodes = doc[\"word/document.xml\"].replace_node(\n    node,\n    '<w:del><w:r><w:delText>old</w:delText></w:r></w:del><w:ins><w:r><w:t>new</w:t></w:r></w:ins>'\n)\n# Then add comment on the newly created elements\n# new_nodes[0] is the <w:del>, new_nodes[1] is the <w:ins>\ndoc.add_comment(start=new_nodes[0], end=new_nodes[1], text=\"Changed old to new per requirements\")\n\n# Reply to existing comment\ndoc.reply_to_comment(parent_comment_id=0, text=\"I agree with this change\")\n```\n\n### Rejecting Tracked Changes\n\n**IMPORTANT**: Use `revert_insertion()` to reject insertions and `revert_deletion()` to restore deletions using tracked changes. Use `suggest_deletion()` only for regular unmarked content.\n\n```python\n# Reject insertion (wraps it in deletion)\n# Use this when another author inserted text that you want to delete\nins = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"5\"})\nnodes = doc[\"word/document.xml\"].revert_insertion(ins)  # Returns [ins]\n\n# Reject deletion (creates insertion to restore deleted content)\n# Use this when another author deleted text that you want to restore\ndel_elem = doc[\"word/document.xml\"].get_node(tag=\"w:del\", attrs={\"w:id\": \"3\"})\nnodes = doc[\"word/document.xml\"].revert_deletion(del_elem)  # Returns [del_elem, new_ins]\n\n# Reject all insertions in a paragraph\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph text\")\nnodes = doc[\"word/document.xml\"].revert_insertion(para)  # Returns [para]\n\n# Reject all deletions in a paragraph\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph text\")\nnodes = doc[\"word/document.xml\"].revert_deletion(para)  # Returns [para]\n```\n\n### Inserting Images\n\n**CRITICAL**: The Document class works with a temporary copy at `doc.unpacked_path`. Always copy images to this temp directory, not the original unpacked folder.\n\n```python\nfrom PIL import Image\nimport shutil, os\n\n# Initialize document first\ndoc = Document('unpacked')\n\n# Copy image and calculate full-width dimensions with aspect ratio\nmedia_dir = os.path.join(doc.unpacked_path, 'word/media')\nos.makedirs(media_dir, exist_ok=True)\nshutil.copy('image.png', os.path.join(media_dir, 'image1.png'))\nimg = Image.open(os.path.join(media_dir, 'image1.png'))\nwidth_emus = int(6.5 * 914400)  # 6.5\" usable width, 914400 EMUs/inch\nheight_emus = int(width_emus * img.size[1] / img.size[0])\n\n# Add relationship and content type\nrels_editor = doc['word/_rels/document.xml.rels']\nnext_rid = rels_editor.get_next_rid()\nrels_editor.append_to(rels_editor.dom.documentElement,\n    f'<Relationship Id=\"{next_rid}\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"media/image1.png\"/>')\ndoc['[Content_Types].xml'].append_to(doc['[Content_Types].xml'].dom.documentElement,\n    '<Default Extension=\"png\" ContentType=\"image/png\"/>')\n\n# Insert image\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=100)\ndoc[\"word/document.xml\"].insert_after(node, f'''<w:p>\n  <w:r>\n    <w:drawing>\n      <wp:inline distT=\"0\" distB=\"0\" distL=\"0\" distR=\"0\">\n        <wp:extent cx=\"{width_emus}\" cy=\"{height_emus}\"/>\n        <wp:docPr id=\"1\" name=\"Picture 1\"/>\n        <a:graphic xmlns:a=\"http://schemas.openxmlformats.org/drawingml/2006/main\">\n          <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n            <pic:pic xmlns:pic=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n              <pic:nvPicPr><pic:cNvPr id=\"1\" name=\"image1.png\"/><pic:cNvPicPr/></pic:nvPicPr>\n              <pic:blipFill><a:blip r:embed=\"{next_rid}\"/><a:stretch><a:fillRect/></a:stretch></pic:blipFill>\n              <pic:spPr><a:xfrm><a:ext cx=\"{width_emus}\" cy=\"{height_emus}\"/></a:xfrm><a:prstGeom prst=\"rect\"><a:avLst/></a:prstGeom></pic:spPr>\n            </pic:pic>\n          </a:graphicData>\n        </a:graphic>\n      </wp:inline>\n    </w:drawing>\n  </w:r>\n</w:p>''')\n```\n\n### Getting Nodes\n\n```python\n# By text content\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"specific text\")\n\n# By line range\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=range(100, 150))\n\n# By attributes\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:del\", attrs={\"w:id\": \"1\"})\n\n# By exact line number (must be line number where tag opens)\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=42)\n\n# Combine filters\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", line_number=range(40, 60), contains=\"text\")\n\n# Disambiguate when text appears multiple times - add line_number range\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"Section\", line_number=range(2400, 2500))\n```\n\n### Saving\n\n```python\n# Save with automatic validation (copies back to original directory)\ndoc.save()  # Validates by default, raises error if validation fails\n\n# Save to different location\ndoc.save('modified-unpacked')\n\n# Skip validation (debugging only - needing this in production indicates XML issues)\ndoc.save(validate=False)\n```\n\n### Direct DOM Manipulation\n\nFor complex scenarios not covered by the library:\n\n```python\n# Access any XML file\neditor = doc[\"word/document.xml\"]\neditor = doc[\"word/comments.xml\"]\n\n# Direct DOM access (defusedxml.minidom.Document)\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=5)\nparent = node.parentNode\nparent.removeChild(node)\nparent.appendChild(node)  # Move to end\n\n# General document manipulation (without tracked changes)\nold_node = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"original text\")\ndoc[\"word/document.xml\"].replace_node(old_node, \"<w:p><w:r><w:t>replacement text</w:t></w:r></w:p>\")\n\n# Multiple insertions - use return value to maintain order\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", line_number=100)\nnodes = doc[\"word/document.xml\"].insert_after(node, \"<w:r><w:t>A</w:t></w:r>\")\nnodes = doc[\"word/document.xml\"].insert_after(nodes[-1], \"<w:r><w:t>B</w:t></w:r>\")\nnodes = doc[\"word/document.xml\"].insert_after(nodes[-1], \"<w:r><w:t>C</w:t></w:r>\")\n# Results in: original_node, A, B, C\n```\n\n## Tracked Changes (Redlining)\n\n**Use the Document class above for all tracked changes.** The patterns below are for reference when constructing replacement XML strings.\n\n### Validation Rules\nThe validator checks that the document text matches the original after reverting Claude's changes. This means:\n- **NEVER modify text inside another author's `<w:ins>` or `<w:del>` tags**\n- **ALWAYS use nested deletions** to remove another author's insertions\n- **Every edit must be properly tracked** with `<w:ins>` or `<w:del>` tags\n\n### Tracked Change Patterns\n\n**CRITICAL RULES**:\n1. Never modify the content inside another author's tracked changes. Always use nested deletions.\n2. **XML Structure**: Always place `<w:del>` and `<w:ins>` at paragraph level containing complete `<w:r>` elements. Never nest inside `<w:r>` elements - this creates invalid XML that breaks document processing.\n\n**Text Insertion:**\n```xml\n<w:ins w:id=\"1\" w:author=\"Claude\" w:date=\"2025-07-30T23:05:00Z\" w16du:dateUtc=\"2025-07-31T06:05:00Z\">\n  <w:r w:rsidR=\"00792858\">\n    <w:t>inserted text</w:t>\n  </w:r>\n</w:ins>\n```\n\n**Text Deletion:**\n```xml\n<w:del w:id=\"2\" w:author=\"Claude\" w:date=\"2025-07-30T23:05:00Z\" w16du:dateUtc=\"2025-07-31T06:05:00Z\">\n  <w:r w:rsidDel=\"00792858\">\n    <w:delText>deleted text</w:delText>\n  </w:r>\n</w:del>\n```\n\n**Deleting Another Author's Insertion (MUST use nested structure):**\n```xml\n<!-- Nest deletion inside the original insertion -->\n<w:ins w:author=\"Jane Smith\" w:id=\"16\">\n  <w:del w:author=\"Claude\" w:id=\"40\">\n    <w:r><w:delText>monthly</w:delText></w:r>\n  </w:del>\n</w:ins>\n<w:ins w:author=\"Claude\" w:id=\"41\">\n  <w:r><w:t>weekly</w:t></w:r>\n</w:ins>\n```\n\n**Restoring Another Author's Deletion:**\n```xml\n<!-- Leave their deletion unchanged, add new insertion after it -->\n<w:del w:author=\"Jane Smith\" w:id=\"50\">\n  <w:r><w:delText>within 30 days</w:delText></w:r>\n</w:del>\n<w:ins w:author=\"Claude\" w:id=\"51\">\n  <w:r><w:t>within 30 days</w:t></w:r>\n</w:ins>\n```",
        "skills/workspace-documentation/SKILL.md": "---\nname: workspace-documentation\ndescription: Capture and organize knowledge in workspace platforms. Structures information into wikis, databases, and connected knowledge graphs.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Knowledge Capture\n\nTransforms conversations, discussions, and insights into structured documentation in your Notion workspace. Captures knowledge from chat context, formats it appropriately, and saves it to the right location with proper organization and linking.\n\n## Quick Start\n\nWhen asked to save information to Notion:\n\n1. **Extract content**: Identify key information from conversation context\n2. **Structure information**: Organize into appropriate documentation format\n3. **Determine location**: Use `Notion:notion-search` to find appropriate wiki page/database\n4. **Create page**: Use `Notion:notion-create-pages` to save content\n5. **Make discoverable**: Link from relevant hub pages, add to databases, or update wiki navigation so others can find it\n\n## Knowledge Capture Workflow\n\n### Step 1: Identify content to capture\n\n```\nFrom conversation context, extract:\n- Key concepts and definitions\n- Decisions made and rationale\n- How-to information and procedures\n- Important insights or learnings\n- Q&A pairs\n- Examples and use cases\n```\n\n### Step 2: Determine content type\n\n```\nClassify the knowledge:\n- Concept/Definition\n- How-to Guide\n- Decision Record\n- FAQ Entry\n- Meeting Summary\n- Learning/Post-mortem\n- Reference Documentation\n```\n\n### Step 3: Structure the content\n\n```\nFormat appropriately based on content type:\n- Use templates for consistency\n- Add clear headings and sections\n- Include examples where helpful\n- Add relevant metadata\n- Link to related pages\n```\n\n### Step 4: Determine destination\n\n```\nWhere to save:\n- Wiki page (general knowledge base)\n- Specific project page (project-specific knowledge)\n- Documentation database (structured docs)\n- FAQ database (questions and answers)\n- Decision log (architecture/product decisions)\n- Team wiki (team-specific knowledge)\n```\n\n### Step 5: Create the page\n\n```\nUse Notion:notion-create-pages:\n- Set appropriate title\n- Use structured content from template\n- Set properties if in database\n- Add tags/categories\n- Link to related pages\n```\n\n### Step 6: Make content discoverable\n\n```\nLink the new page so others can find it:\n\n1. Update hub/index pages:\n   - Add link to wiki table of contents page\n   - Add link from relevant project page\n   - Add link from category/topic page (e.g., \"Engineering Docs\")\n\n2. If page is in a database:\n   - Set appropriate tags/categories\n   - Set status (e.g., \"Published\")\n   - Add to relevant views\n\n3. Optionally update parent page:\n   - If saved under a project, add to project's \"Documentation\" section\n   - If in team wiki, ensure it's linked from team homepage\n\nExample:\nNotion:notion-update-page\npage_id: \"team-wiki-homepage-id\"\ncommand: \"insert_content_after\"\nselection_with_ellipsis: \"## How-To Guides...\"\nnew_str: \"- <mention-page url='...'>How to Deploy to Production</mention-page>\"\n```\n\nThis step ensures the knowledge doesn't become \"orphaned\" - it's properly connected to your workspace's navigation structure.\n\n## Content Types\n\nChoose appropriate structure based on content:\n\n**Concept**: Overview  Definition  Characteristics  Examples  Use Cases  Related\n**How-To**: Overview  Prerequisites  Steps (numbered)  Verification  Troubleshooting  Related\n**Decision**: Context  Decision  Rationale  Options Considered  Consequences  Implementation\n**FAQ**: Short Answer  Detailed Explanation  Examples  When to Use  Related Questions\n**Learning**: What Happened  What Went Well  What Didn't  Root Causes  Learnings  Actions\n\n## Destination Patterns\n\n**General Wiki**: Standalone page  add to index  tag  link from related pages\n\n**Project Wiki**: Child of project page  link from project overview  tag with project name\n\n**Documentation Database**: Use properties (Title, Type, Category, Tags, Last Updated, Owner)\n\n**Decision Log Database**: Use properties (Decision, Date, Status, Domain, Deciders, Impact)\n\n**FAQ Database**: Use properties (Question, Category, Tags, Last Reviewed, Useful Count)\n\nSee [reference/database-best-practices.md](reference/database-best-practices.md) for database selection guide and individual schema files.\n\n## Content Extraction from Conversations\n\n**Chat Discussion**: Key points, conclusions, resources, action items, Q&A\n\n**Problem-Solving**: Problem statement, approaches tried, solution, why it worked, future considerations\n\n**Knowledge Sharing**: Concept explained, examples, best practices, common pitfalls, resources\n\n**Decision Discussion**: Question, options, trade-offs, decision, rationale, next steps\n\n## Formatting Best Practices\n\n**Structure**: Use `#` (title), `##` (sections), `###` (subsections) consistently\n\n**Writing**: Start with overview, use bullets, keep paragraphs short, add examples\n\n**Linking**: Link related pages, mention people, reference resources, create bidirectional links\n\n**Metadata**: Include date, author, tags, status\n\n**Searchability**: Clear titles, natural keywords, common search tags, image alt-text\n\n## Indexing and Organization\n\n**Wiki Index**: Organize by sections (Getting Started, How-To Guides, Reference, FAQs, Decisions) with page links\n\n**Category Pages**: Create landing pages with overview, doc links, and recent updates\n\n**Tagging Strategy**: Use consistent tags for technology/tools, topics, audience, and status\n\n## Update Management\n\n**Create New**: Content is substantive (>2 paragraphs), will be referenced multiple times, part of knowledge base, needs independent discovery\n\n**Update Existing**: Adding to existing topic, correcting info, expanding concept, updating for changes\n\n**Versioning**: Add update history section for significant changes (date, author, what changed, why)\n\n## Best Practices\n\n1. **Capture promptly**: Document while context is fresh\n2. **Structure consistently**: Use templates for similar content\n3. **Link extensively**: Connect related knowledge\n4. **Write for discovery**: Use searchable titles and tags\n5. **Include context**: Why this matters, when to use\n6. **Add examples**: Concrete examples aid understanding\n7. **Maintain**: Review and update periodically\n8. **Get feedback**: Ask if documentation is helpful\n\n## Advanced Features\n\n**Documentation databases**: See [reference/database-best-practices.md](reference/database-best-practices.md) for database schema patterns.\n\n## Common Issues\n\n**\"Not sure where to save\"**: Default to general wiki, can move later\n**\"Content is fragmentary\"**: Group related fragments into cohesive doc\n**\"Already exists\"**: Search first, update existing if appropriate\n**\"Too informal\"**: Clean up language while preserving insights\n\n## Examples\n\nSee [examples/](examples/) for complete workflows:\n\n- [examples/conversation-to-faq.md](examples/conversation-to-faq.md) - FAQ from Q&A\n- [examples/decision-capture.md](examples/decision-capture.md) - Decision record\n- [examples/how-to-guide.md](examples/how-to-guide.md) - How-to from discussion\n",
        "skills/workspace-documentation/evaluations/README.md": "# Knowledge Capture Skill Evaluations\n\nEvaluation scenarios for testing the Knowledge Capture skill across different Claude models.\n\n## Purpose\n\nThese evaluations ensure the Knowledge Capture skill:\n\n- Correctly identifies content types (how-to guides, FAQs, decision records, wikis)\n- Extracts relevant information from conversations\n- Structures content appropriately for each type\n- Searches and places content in the right Notion location\n- Works consistently across Haiku, Sonnet, and Opus\n\n## Evaluation Files\n\n### conversation-to-wiki.json\n\nTests capturing conversation content as a how-to guide for the team wiki.\n\n**Scenario**: Save deployment discussion to wiki  \n**Key Behaviors**:\n\n- Extracts steps, gotchas, and best practices from conversation\n- Identifies content as How-To Guide\n- Structures with proper sections (Overview, Prerequisites, Steps, Troubleshooting)\n- Searches for team wiki location\n- Preserves technical details (commands, configs)\n\n### decision-record.json\n\nTests capturing architectural or technical decisions with full context.\n\n**Scenario**: Document database migration decision  \n**Key Behaviors**:\n\n- Extracts decision context, alternatives, and rationale\n- Follows decision record structure (Context, Decision, Alternatives, Consequences)\n- Captures both selected and rejected options with reasoning\n- Places in decision log or ADR database\n- Links to related technical documentation\n\n## Running Evaluations\n\n1. Enable the `knowledge-capture` skill\n2. Submit the query from the evaluation file\n3. Provide conversation context as specified\n4. Verify all expected behaviors are met\n5. Check success criteria for quality\n6. Test with Haiku, Sonnet, and Opus\n\n## Expected Skill Behaviors\n\nKnowledge Capture evaluations should verify:\n\n### Content Extraction\n\n- Accurately captures key points from conversation context\n- Preserves specific technical details, not generic placeholders\n- Maintains context and nuance from discussion\n\n### Content Type Selection\n\n- Correctly identifies appropriate content type (how-to, FAQ, decision record, wiki page)\n- Uses matching structure from reference documentation\n- Applies proper Notion markdown formatting\n\n### Notion Integration\n\n- Searches for appropriate target location (wiki, decision log, etc.)\n- Creates well-structured pages with clear titles\n- Uses proper parent placement\n- Includes discoverable titles and metadata\n\n### Quality Standards\n\n- Content is actionable and future-reference ready\n- Technical accuracy is preserved\n- Organization aids discoverability\n- Formatting enhances readability\n\n## Creating New Evaluations\n\nWhen adding Knowledge Capture evaluations:\n\n1. **Use realistic conversation content** - Include actual technical details, decisions, or processes\n2. **Test different content types** - How-to guides, FAQs, decision records, meeting notes, learnings\n3. **Vary complexity** - Simple captures vs. complex technical discussions\n4. **Test discovery** - Finding the right wiki section or database\n5. **Include edge cases** - Unclear content types, minimal context, overlapping categories\n\n## Example Success Criteria\n\n**Good** (specific, testable):\n\n- \"Structures content using How-To format with numbered steps\"\n- \"Preserves exact bash commands from conversation\"\n- \"Creates page with title format 'How to [Action]'\"\n- \"Places in Engineering Wiki  Deployment section\"\n\n**Bad** (vague, untestable):\n\n- \"Creates good documentation\"\n- \"Uses appropriate structure\"\n- \"Saves to the right place\"\n",
        "skills/workspace-documentation/examples/conversation-to-faq.md": "# Example: Conversation to FAQ\n\n## User Request\n\n> \"Save this conversation about deployment troubleshooting to the FAQ\"\n\n**Context**: User just had a conversation explaining how to troubleshoot common deployment errors.\n\n## Conversation Summary\n\nThe conversation covered:\n\n1. **Question**: \"Why does deployment fail with 'port already in use' error?\"\n2. **Answer**: Process from previous deployment still running, need to kill it\n3. **Question**: \"How do I find and kill the process?\"\n4. **Commands shared**: `lsof -ti:3000 | xargs kill -9` or use `pm2 restart app`\n5. **Question**: \"What about 'cannot connect to database' errors?\"\n6. **Answer**: Check database credentials, verify database is running, check network connectivity\n7. **Best practice**: Always check logs first (`pm2 logs` or `docker logs`)\n\n## Workflow Execution\n\n### Step 1: Identify Content Type\n\nThis is Q&A content  **FAQ Entry** format is appropriate\n\n### Step 2: Structure FAQ Entries\n\nExtract distinct Q&A pairs from conversation:\n\n**FAQ 1**: Port already in use error\n**FAQ 2**: Cannot connect to database error\n**FAQ 3**: General troubleshooting approach\n\n### Step 3: Determine Destination\n\n```\nNotion:notion-search\nquery: \"FAQ deployment\"\nquery_type: \"internal\"\n```\n\nFound: \"Deployment FAQ\" database in Engineering Docs\n\n```\nNotion:notion-fetch\nid: \"deployment-faq-database-id\"\n```\n\n**Schema**:\n\n- Question (title)\n- Category (select): Deployment, Configuration, Troubleshooting, etc.\n- Tags (multi_select)\n- Last Reviewed (date)\n\n### Step 4: Create FAQ Entries\n\n**FAQ Entry 1: Port Already in Use**\n\n````\nNotion:notion-create-pages\n\nparent: { data_source_id: \"collection://faq-db-uuid\" }\n\npages: [{\n  properties: {\n    \"Question\": \"Why does deployment fail with 'port already in use' error?\",\n    \"Category\": \"Troubleshooting\",\n    \"Tags\": \"deployment, errors, ports\",\n    \"date:Last Reviewed:start\": \"2025-10-14\",\n    \"date:Last Reviewed:is_datetime\": 0\n  },\n  content: \"## Short Answer\\n\\nThe deployment port (usually 3000) is still occupied by a process from a previous deployment. You need to kill the existing process before deploying again.\\n\\n## Detailed Explanation\\n\\nWhen you deploy the application, it tries to bind to a specific port (e.g., port 3000). If a previous deployment didn't shut down cleanly, that process may still be running and holding the port. This prevents the new deployment from starting.\\n\\n**Common causes**:\\n- Previous deployment crashed without cleanup\\n- Manual `node` process started and forgotten\\n- PM2 or other process manager didn't restart properly\\n- Multiple deployments attempted simultaneously\\n\\n## Solution\\n\\n### Option 1: Kill the process using the port\\n\\n```bash\\n# Find and kill process on port 3000\\nlsof -ti:3000 | xargs kill -9\\n```\\n\\nReplace `3000` with your application's port number.\\n\\n### Option 2: If using PM2\\n\\n```bash\\n# Restart the application\\npm2 restart app\\n\\n# Or delete and restart\\npm2 delete app\\npm2 start app.js --name app\\n```\\n\\n### Option 3: Check all node processes\\n\\n```bash\\n# List all node processes\\nps aux | grep node\\n\\n# Kill specific process by PID\\nkill -9 <PID>\\n```\\n\\n## Prevention\\n\\n1. **Use process managers**: PM2, systemd, or Docker handle cleanup automatically\\n2. **Graceful shutdown**: Implement proper shutdown handlers in your app\\n3. **Health checks**: Monitor if previous deployment shut down before starting new one\\n\\n```javascript\\n// Graceful shutdown example\\nprocess.on('SIGTERM', () => {\\n  server.close(() => {\\n    console.log('Server closed');\\n    process.exit(0);\\n  });\\n});\\n```\\n\\n## Verification\\n\\nAfter killing the process, verify the port is free:\\n\\n```bash\\nlsof -ti:3000\\n# Should return nothing if port is free\\n```\\n\\n## Related Questions\\n- <mention-page url=\\\"...\\\">How do I check what's using a port?</mention-page>\\n- <mention-page url=\\\"...\\\">How do I configure the application port?</mention-page>\\n- <mention-page url=\\\"...\\\">PM2 deployment best practices</mention-page>\\n\\n## Last Updated\\nOctober 14, 2025\"\n}]\n````\n\n**FAQ Entry 2: Database Connection Errors**\n\n````\nNotion:notion-create-pages\n\nparent: { data_source_id: \"collection://faq-db-uuid\" }\n\npages: [{\n  properties: {\n    \"Question\": \"Why do I get 'cannot connect to database' errors during deployment?\",\n    \"Category\": \"Troubleshooting\",\n    \"Tags\": \"deployment, database, errors\",\n    \"date:Last Reviewed:start\": \"2025-10-14\",\n    \"date:Last Reviewed:is_datetime\": 0\n  },\n  content: \"## Short Answer\\n\\nDatabase connection errors usually mean either the database isn't running, credentials are incorrect, or there's a network connectivity issue. Check database status, verify credentials, and test connectivity.\\n\\n## Detailed Explanation\\n\\nThe application can't establish a connection to the database during startup. This prevents the application from initializing properly.\\n\\n**Common causes**:\\n- Database service isn't running\\n- Incorrect connection credentials\\n- Network connectivity issues (firewall, security groups)\\n- Database host/port misconfigured\\n- Database is at connection limit\\n- SSL/TLS configuration mismatch\\n\\n## Troubleshooting Steps\\n\\n### Step 1: Check database status\\n\\n```bash\\n# For local PostgreSQL\\npg_isready -h localhost -p 5432\\n\\n# For Docker\\ndocker ps | grep postgres\\n\\n# For MongoDB\\nmongosh --eval \\\"db.adminCommand('ping')\\\"\\n```\\n\\n### Step 2: Verify credentials\\n\\nCheck your `.env` or configuration file:\\n\\n```bash\\n# Common environment variables\\nDB_HOST=localhost\\nDB_PORT=5432\\nDB_NAME=myapp_production\\nDB_USER=myapp_user\\nDB_PASSWORD=***********\\n```\\n\\nTest connection manually:\\n\\n```bash\\n# PostgreSQL\\npsql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME\\n\\n# MongoDB\\nmongosh \\\"mongodb://$DB_USER:$DB_PASSWORD@$DB_HOST:$DB_PORT/$DB_NAME\\\"\\n```\\n\\n### Step 3: Check network connectivity\\n\\n```bash\\n# Test if port is reachable\\ntelnet $DB_HOST $DB_PORT\\n\\n# Or using nc\\nnc -zv $DB_HOST $DB_PORT\\n\\n# Check firewall rules (if applicable)\\nsudo iptables -L\\n```\\n\\n### Step 4: Check application logs\\n\\n```bash\\n# PM2 logs\\npm2 logs app\\n\\n# Docker logs\\ndocker logs container-name\\n\\n# Application logs\\ntail -f /var/log/app/error.log\\n```\\n\\nLook for specific error messages:\\n- `ECONNREFUSED`: Database not running or wrong host/port\\n- `Authentication failed`: Wrong credentials\\n- `Timeout`: Network/firewall issue\\n- `Too many connections`: Database connection limit reached\\n\\n## Solutions by Error Type\\n\\n### Database Not Running\\n\\n```bash\\n# Start PostgreSQL\\nsudo systemctl start postgresql\\n\\n# Start via Docker\\ndocker start postgres-container\\n```\\n\\n### Wrong Credentials\\n\\n1. Reset database password\\n2. Update `.env` file\\n3. Restart application\\n\\n### Connection Limit Reached\\n\\n```sql\\n-- Check current connections (PostgreSQL)\\nSELECT count(*) FROM pg_stat_activity;\\n\\n-- Increase max connections\\nALTER SYSTEM SET max_connections = 200;\\n```\\n\\n### SSL/TLS Issues\\n\\nAdd to connection string:\\n```\\nssl=true&sslmode=require\\n```\\n\\nOr disable SSL for dev:\\n```\\nsslmode=disable\\n```\\n\\n## Prevention\\n\\n1. **Connection pooling**: Limits concurrent connections\\n```javascript\\npool: {\\n  min: 2,\\n  max: 10\\n}\\n```\\n\\n2. **Health checks**: Verify database before starting app\\n```javascript\\nconst healthCheck = async () => {\\n  try {\\n    await db.raw('SELECT 1');\\n    console.log('Database connected');\\n  } catch (err) {\\n    console.error('Database connection failed:', err);\\n    process.exit(1);\\n  }\\n};\\n```\\n\\n3. **Retry logic**: Auto-reconnect on temporary failures\\n```javascript\\nretry: {\\n  max: 3,\\n  backoff: 1000\\n}\\n```\\n\\n4. **Environment validation**: Check all required env vars exist\\n```javascript\\nconst required = ['DB_HOST', 'DB_PORT', 'DB_NAME', 'DB_USER', 'DB_PASSWORD'];\\nfor (const key of required) {\\n  if (!process.env[key]) {\\n    throw new Error(`Missing required env var: ${key}`);\\n  }\\n}\\n```\\n\\n## Related Questions\\n- <mention-page url=\\\"...\\\">How do I configure database connection pooling?</mention-page>\\n- <mention-page url=\\\"...\\\">Database backup and restore procedures</mention-page>\\n- <mention-page url=\\\"...\\\">Database performance optimization</mention-page>\\n\\n## Last Updated\\nOctober 14, 2025\"\n}]\n````\n\n**FAQ Entry 3: General Troubleshooting**\n\n````\nNotion:notion-create-pages\n\nparent: { data_source_id: \"collection://faq-db-uuid\" }\n\npages: [{\n  properties: {\n    \"Question\": \"What's the first thing I should check when deployment fails?\",\n    \"Category\": \"Troubleshooting\",\n    \"Tags\": \"deployment, debugging, best-practices\",\n    \"date:Last Reviewed:start\": \"2025-10-14\",\n    \"date:Last Reviewed:is_datetime\": 0\n  },\n  content: \"## Short Answer\\n\\n**Always check the logs first.** Logs contain error messages that point you directly to the problem. Use `pm2 logs`, `docker logs`, or check your application's log files.\\n\\n## Detailed Explanation\\n\\nLogs are your first and most important debugging tool. They show:\\n- Exact error messages\\n- Stack traces\\n- Timing information\\n- Configuration issues\\n- Dependency problems\\n\\nMost deployment issues can be diagnosed and fixed by reading the logs carefully.\\n\\n## How to Check Logs\\n\\n### PM2\\n\\n```bash\\n# View all logs\\npm2 logs\\n\\n# View logs for specific app\\npm2 logs app-name\\n\\n# View only errors\\npm2 logs --err\\n\\n# Follow logs in real-time\\npm2 logs --lines 100\\n```\\n\\n### Docker\\n\\n```bash\\n# View logs\\ndocker logs container-name\\n\\n# Follow logs\\ndocker logs -f container-name\\n\\n# Last 100 lines\\ndocker logs --tail 100 container-name\\n\\n# With timestamps\\ndocker logs -t container-name\\n```\\n\\n### Application Logs\\n\\n```bash\\n# Tail application logs\\ntail -f /var/log/app/app.log\\ntail -f /var/log/app/error.log\\n\\n# Search logs for errors\\ngrep -i error /var/log/app/*.log\\n\\n# View logs with context\\ngrep -B 5 -A 5 \\\"ERROR\\\" app.log\\n```\\n\\n## Systematic Troubleshooting Approach\\n\\n### 1. Check the logs\\n- Read error messages carefully\\n- Note the exact error type and message\\n- Check timestamps to find when error occurred\\n\\n### 2. Verify configuration\\n- Environment variables set correctly?\\n- Configuration files present and valid?\\n- Paths and file permissions correct?\\n\\n### 3. Check dependencies\\n- All packages installed? (`node_modules` present?)\\n- Correct versions installed?\\n- Any native module compilation errors?\\n\\n### 4. Verify environment\\n- Required services running (database, Redis, etc.)?\\n- Ports available?\\n- Network connectivity working?\\n\\n### 5. Test components individually\\n- Can you connect to database manually?\\n- Can you run application locally?\\n- Do health check endpoints work?\\n\\n### 6. Check recent changes\\n- What changed since last successful deployment?\\n- New dependencies added?\\n- Configuration modified?\\n- Environment differences?\\n\\n## Common Error Patterns\\n\\n### \\\"Module not found\\\"\\n```bash\\n# Solution: Install dependencies\\nnpm install\\n# or\\nnpm ci\\n```\\n\\n### \\\"Permission denied\\\"\\n```bash\\n# Solution: Fix file permissions\\nchmod +x start.sh\\nsudo chown -R appuser:appuser /app\\n```\\n\\n### \\\"Address already in use\\\"\\n```bash\\n# Solution: Kill process on port\\nlsof -ti:3000 | xargs kill -9\\n```\\n\\n### \\\"Cannot connect to...\\\"\\n```bash\\n# Solution: Verify service is running and reachable\\ntelnet service-host port\\n```\\n\\n## Debugging Tools\\n\\n### Log Aggregation\\n- **PM2**: Built-in log management\\n- **Docker**: Centralized logging with log drivers\\n- **ELK Stack**: Elasticsearch, Logstash, Kibana for large scale\\n- **CloudWatch**: For AWS deployments\\n\\n### Monitoring\\n- **PM2 Monit**: `pm2 monit` for real-time metrics\\n- **Docker Stats**: `docker stats` for resource usage\\n- **System metrics**: `top`, `htop` for CPU/memory\\n\\n### Network Debugging\\n```bash\\n# Check open connections\\nnetstat -tulpn\\n\\n# Check specific port\\nlsof -i :3000\\n\\n# Test connectivity\\ncurl http://localhost:3000/health\\n```\\n\\n## When to Escalate\\n\\nEscalate to senior team member if:\\n- Error message is cryptic and not Google-able\\n- Problem persists after trying obvious solutions\\n- Production is affected\\n- Data integrity may be compromised\\n- Security-related errors\\n\\n## Prevention\\n\\n1. **Automated health checks** before deployment\\n2. **Staged deployments** (dev  staging  production)\\n3. **Rollback plan** ready for quick recovery\\n4. **Monitoring and alerts** for early problem detection\\n5. **Deployment checklist** to ensure all steps followed\\n\\n## Related Questions\\n- <mention-page url=\\\"...\\\">How do I set up log aggregation?</mention-page>\\n- <mention-page url=\\\"...\\\">Deployment checklist and best practices</mention-page>\\n- <mention-page url=\\\"...\\\">How to roll back a failed deployment?</mention-page>\\n\\n## Last Updated\\nOctober 14, 2025\"\n}]\n````\n\n### Step 5: Update FAQ Index\n\n```\nNotion:notion-fetch\nid: \"faq-index-page-id\"\n```\n\n```\nNotion:notion-update-page\npage_id: \"faq-index-page-id\"\ncommand: \"insert_content_after\"\nselection_with_ellipsis: \"## Deployment & Troubleshooting...\"\nnew_str: \"\n- <mention-page url=\\\"...\\\">Why does deployment fail with 'port already in use' error?</mention-page>\n- <mention-page url=\\\"...\\\">Why do I get 'cannot connect to database' errors?</mention-page>\n- <mention-page url=\\\"...\\\">What's the first thing I should check when deployment fails?</mention-page>\n\"\n```\n\n## Summary Provided to User\n\n```markdown\n## FAQ Entries Created \n\n### Conversation Captured\n\nTransformed deployment troubleshooting conversation into 3 comprehensive FAQ entries.\n\n### FAQ Entries Created\n\n1. **Port Already in Use Error**\n   - Problem: Deployment fails due to occupied port\n   - Solutions: Kill process, use PM2, check node processes\n   - Prevention: Use process managers, graceful shutdown\n   - Link: <mention-page url=\"...\">Port already in use FAQ</mention-page>\n\n2. **Database Connection Errors**\n   - Problem: Cannot connect to database during deployment\n   - Troubleshooting: 4-step systematic approach\n   - Solutions: By error type (credentials, network, limits)\n   - Prevention: Connection pooling, health checks, retry logic\n   - Link: <mention-page url=\"...\">Database connection FAQ</mention-page>\n\n3. **General Troubleshooting Approach**\n   - Best practice: Always check logs first\n   - Tools: PM2 logs, Docker logs, application logs\n   - Systematic approach: 6-step debugging process\n   - Common errors: Module not found, permissions, ports\n   - Link: <mention-page url=\"...\">Troubleshooting first steps FAQ</mention-page>\n\n### Content Structure\n\nEach FAQ includes:\n Short answer (quick reference)\n Detailed explanation (full context)\n Multiple solution options\n Code examples and commands\n Prevention strategies\n Links to related FAQs\n Last updated date\n\n### Database Integration\n\n-  Added to FAQ database (not standalone pages)\n-  Properties set: Category (Troubleshooting), Tags, Date\n-  FAQ index page updated with links\n-  Cross-references between related questions\n\n### Value Added\n\n**From conversation**  **To structured knowledge**:\n\n- Casual Q&A  Professional FAQ entries\n- Implicit knowledge  Explicit documentation\n- One-time help  Reusable resource\n- 5-minute conversation  Permanent team knowledge\n\n**Future benefit**:\n\n- Team can self-serve on common issues\n- New team members can onboard faster\n- Reduces repeated questions\n- Improves troubleshooting speed\n\n**Knowledge captured and preserved! **\n```\n\n## Key Features Demonstrated\n\n### Content Extraction\n\n- Identified distinct Q&A pairs from conversation\n- Extracted commands, solutions, and best practices\n- Preserved troubleshooting workflows\n\n### FAQ Structure\n\n- Short answer for quick reference\n- Detailed explanation for understanding\n- Multiple solutions (Options 1, 2, 3)\n- Code examples with comments\n- Prevention strategies\n- Related questions linking\n\n### Database Integration\n\n- Created in FAQ database (not standalone)\n- Set appropriate properties (category, tags, date)\n- Updated index page with links\n- Cross-referenced related questions\n\n### Value Transformation\n\n- Converted informal conversation to professional docs\n- Added structure, examples, and prevention tips\n- Made knowledge searchable and discoverable\n- Created permanent team resource\n\nPerfect for: FAQ creation, knowledge preservation, team documentation\n",
        "skills/workspace-documentation/examples/decision-capture.md": "# Example: Decision Record Capture\n\n**User Request**: \"Document our decision to move from REST to GraphQL API\"\n\n## Workflow\n\n### 1. Extract Decision from Context\n\nFrom conversation, identified:\n\n- **Decision**: Migrate customer-facing API from REST to GraphQL\n- **Context**: REST endpoints becoming unwieldy (50+ endpoints)\n- **Alternatives**: Keep REST, try gRPC, adopt GraphQL\n- **Rationale**: Better client experience, type safety, single endpoint\n\n### 2. Find Decision Log\n\n```\nNotion:notion-search\nquery: \"architecture decisions\" or \"ADR\"\n```\n\nFound: \"Architecture Decision Records\" database\n\n### 3. Fetch Schema\n\n```\nNotion:notion-fetch\n```\n\nGot properties: Decision (title), Date, Status, Domain, Impact, Deciders, Stakeholders\n\n### 4. Create Decision Record\n\n```\nNotion:notion-create-pages\nparent: { data_source_id: \"decision-log-collection-id\" }\npages: [{\n  properties: {\n    \"Decision\": \"Migrate to GraphQL API\",\n    \"date:Date:start\": \"2025-10-16\",\n    \"date:Date:is_datetime\": 0,\n    \"Status\": \"Accepted\",\n    \"Domain\": \"Architecture\",\n    \"Impact\": \"High\"\n  },\n  content: \"[Full decision record with context, rationale, alternatives...]\"\n}]\n```\n\n**Content sample**:\n\n```markdown\n# Migrate Customer-Facing API to GraphQL\n\n## Context\n\nOur REST API has grown to 50+ endpoints with inconsistent patterns. Frontend and mobile teams request new endpoints frequently, leading to:\n\n- API bloat and maintenance burden\n- Over-fetching/under-fetching data\n- Slow iteration on client features\n- Inconsistent error handling\n\n## Decision\n\nMigrate customer-facing API from REST to GraphQL while maintaining REST for internal services.\n\n## Rationale\n\n**Why GraphQL**:\n\n- Clients fetch exactly what they need (no over/under-fetching)\n- Single endpoint, self-documenting schema\n- Type safety with code generation\n- Better developer experience\n- Industry standard for client-facing APIs\n\n## Options Considered\n\n### Option A: Keep REST (Status Quo)\n\n**Pros**: No migration cost, team familiar\n**Cons**: Doesn't solve core problems, maintenance keeps growing\n**Decision**: Rejected - doesn't address root cause\n\n### Option B: gRPC\n\n**Pros**: Performance, strong typing\n**Cons**: Browser support issues, steeper learning curve, not designed for client-facing\n**Decision**: Rejected - better for internal services\n\n### Option C: GraphQL\n\n**Pros**: Solves over/under-fetching, great DX, strong ecosystem\n**Cons**: Learning curve, caching complexity, migration effort\n**Decision**: **Accepted**\n\n## Consequences\n\n**Positive**:\n\n- Faster frontend/mobile development\n- Reduced API maintenance burden\n- Better type safety and tooling\n- Single endpoint simplifies deployment\n\n**Negative**:\n\n- 3-4 month migration timeline\n- Team needs training\n- Need to solve caching strategy\n- Monitoring/debugging patterns different\n\n## Implementation Plan\n\n1. GraphQL server setup (Apollo Server)\n2. Schema design workshop\n3. Gradual migration (start with new features)\n4. Dual-run period (both REST and GraphQL)\n5. Deprecate old REST endpoints\n\n**Timeline**: Start Q4 2025, complete Q1 2026\n**Owner**: Backend team lead\n\n## Success Metrics\n\n- API response times improve 30%\n- Client fetch efficiency (less data transferred)\n- Reduced new endpoint requests\n- Developer satisfaction scores\n```\n\n### 5. Make Discoverable\n\nAdded link from Architecture Wiki and notified team in Slack.\n\n## Key Success Factors\n\n- Captured decision while context fresh\n- Documented alternatives considered\n- Included both pros and cons\n- Clear implementation plan\n- Saved to decision log for future reference\n- Made discoverable for team\n",
        "skills/workspace-documentation/examples/how-to-guide.md": "# Example: How-To Guide from Discussion\n\n**User Request**: \"Save our discussion about deploying to production as a how-to guide\"\n\n## Workflow\n\n### 1. Extract Content from Chat\n\nFrom conversation, identified:\n\n- Deployment prerequisites\n- Step-by-step procedure\n- Common issues and solutions\n- Best practices and tips\n\n### 2. Structure as How-To\n\nOrganized into:\n\n- Overview & prerequisites\n- Numbered deployment steps\n- Verification steps\n- Troubleshooting section\n- Related resources\n\n### 3. Find Location\n\n```\nNotion:notion-search\nquery: \"deployment documentation\"\n```\n\nFound: Engineering Wiki  Deployment section\n\n### 4. Create Guide\n\n```\nNotion:notion-create-pages\nparent: { page_id: \"deployment-section-id\" }\n```\n\n## Output\n\n````markdown\n# How to Deploy to Production\n\n## Overview\n\nProduction deployment using GitHub Actions with zero-downtime rolling updates.\n**Time Required**: 15-20 minutes | **Complexity**: Intermediate\n\n## Prerequisites\n\n- [ ] PR approved and merged to main\n- [ ] All CI tests passing\n- [ ] Database migrations reviewed\n- [ ] Feature flags configured\n\n## Deployment Steps\n\n1. **Verify main branch is ready**\n   ```bash\n   git checkout main && git pull\n   ```\n````\n\n1. **Tag release**\n\n   ```bash\n   git tag -a v1.2.3 -m \"Release v1.2.3\"\n   git push origin v1.2.3\n   ```\n\n1. **Trigger deployment**\n   - GitHub Actions auto-starts from tag push\n   - Monitor: https://github.com/org/repo/actions\n\n1. **Database migrations** (if needed)\n   - Auto-run in GitHub Actions\n   - Check logs for completion\n\n1. **Verify deployment**\n   - Wait for health checks (2-3 min)\n   - Test key endpoints\n   - Check error rates in Datadog\n\n## Verification Checklist\n\n- [ ] All pods healthy in k8s dashboard\n- [ ] Error rate < 0.1% in last 10 min\n- [ ] Response time p95 < 500ms\n- [ ] Test login flow\n- [ ] Check Slack #alerts channel\n\n## Troubleshooting\n\n**Health checks failing**\n Check pod logs: `kubectl logs -f deployment/api -n production`\n\n**Migration errors**\n Rollback: Revert tag, migrations auto-rollback\n\n**High error rate**\n Emergency rollback: Previous tag auto-deploys via GitHub Actions\n\n## Best Practices\n\n- Deploy during low-traffic hours (2-4am PST)\n- Have 2 engineers available\n- Monitor for 30 min post-deploy\n- Update #engineering Slack with deploy notice\n\n## Related Docs\n\n- <mention-page url=\"...\">Rollback Procedure</mention-page>\n- <mention-page url=\"...\">Database Migration Guide</mention-page>\n\n```\n\n### 5. Make Discoverable\n```\n\nNotion:notion-update-page\npage_id: \"engineering-wiki-homepage\"\ncommand: \"insert_content_after\"\n\n```\nAdded link in Engineering Wiki  How-To Guides section\n\n## Key Success Factors\n- Captured tribal knowledge from discussion\n- Structured as actionable steps\n- Included troubleshooting from experience\n- Made discoverable by linking from wiki index\n- Added metadata (time, complexity)\n```\n",
        "skills/workspace-documentation/reference/database-best-practices.md": "# Database Best Practices\n\nGeneral guidance for creating and maintaining knowledge capture databases.\n\n## Core Principles\n\n### 1. Keep It Simple\n\n- Start with core properties\n- Add more only when needed\n- Don't over-engineer\n\n### 2. Use Consistent Naming\n\n- Title property for main identifier\n- Status for lifecycle tracking\n- Tags for flexible categorization\n- Owner for accountability\n\n### 3. Include Metadata\n\n- Created/Updated timestamps\n- Owner or maintainer\n- Last reviewed dates\n- Status indicators\n\n### 4. Enable Discovery\n\n- Use tags liberally\n- Create helpful views\n- Link related content\n- Use clear titles\n\n### 5. Plan for Scale\n\n- Consider filters early\n- Use relations for connections\n- Think about search\n- Organize with categories\n\n## Creating a Database\n\n### Using `Notion:notion-create-database`\n\nExample for documentation database:\n\n```javascript\n{\n  \"parent\": {\"page_id\": \"wiki-page-id\"},\n  \"title\": [{\"text\": {\"content\": \"Team Documentation\"}}],\n  \"properties\": {\n    \"Type\": {\n      \"select\": {\n        \"options\": [\n          {\"name\": \"How-To\", \"color\": \"blue\"},\n          {\"name\": \"Concept\", \"color\": \"green\"},\n          {\"name\": \"Reference\", \"color\": \"gray\"},\n          {\"name\": \"FAQ\", \"color\": \"yellow\"}\n        ]\n      }\n    },\n    \"Category\": {\n      \"select\": {\n        \"options\": [\n          {\"name\": \"Engineering\", \"color\": \"red\"},\n          {\"name\": \"Product\", \"color\": \"purple\"},\n          {\"name\": \"Design\", \"color\": \"pink\"}\n        ]\n      }\n    },\n    \"Tags\": {\"multi_select\": {\"options\": []}},\n    \"Owner\": {\"people\": {}},\n    \"Status\": {\n      \"select\": {\n        \"options\": [\n          {\"name\": \"Draft\", \"color\": \"gray\"},\n          {\"name\": \"Final\", \"color\": \"green\"},\n          {\"name\": \"Deprecated\", \"color\": \"red\"}\n        ]\n      }\n    }\n  }\n}\n```\n\n### Fetching Database Schema\n\nBefore creating pages, always fetch database to get schema:\n\n```\nNotion:notion-fetch\nid: \"database-url-or-id\"\n```\n\nThis returns the exact property names and types to use.\n\n## Database Selection Guide\n\n| Need                       | Use This Database                                   |\n| -------------------------- | --------------------------------------------------- |\n| General documentation      | [Documentation Database](documentation-database.md) |\n| Track decisions            | [Decision Log](decision-log-database.md)            |\n| Q&A knowledge base         | [FAQ Database](faq-database.md)                     |\n| Team-specific content      | [Team Wiki](team-wiki-database.md)                  |\n| Step-by-step guides        | [How-To Guide Database](how-to-guide-database.md)   |\n| Incident/project learnings | [Learning Database](learning-database.md)           |\n\n## Tips\n\n1. **Start with general documentation database** - most flexible\n2. **Add specialized databases** as needs emerge (FAQ, Decisions)\n3. **Use relations** to connect related docs\n4. **Create views** for common use cases\n5. **Review properties** quarterly - remove unused ones\n6. **Document the schema** in database description\n7. **Train team** on property usage and conventions\n",
        "skills/workspace-documentation/reference/decision-log-database.md": "# Decision Log Database (ADR - Architecture Decision Records)\n\n**Purpose**: Track important decisions with context and rationale.\n\n## Schema\n\n| Property              | Type     | Options                                             | Purpose                    |\n| --------------------- | -------- | --------------------------------------------------- | -------------------------- |\n| **Decision**          | title    | -                                                   | What was decided           |\n| **Date**              | date     | -                                                   | When decision was made     |\n| **Status**            | select   | Proposed, Accepted, Superseded, Deprecated          | Current decision status    |\n| **Domain**            | select   | Architecture, Product, Business, Design, Operations | Decision category          |\n| **Impact**            | select   | High, Medium, Low                                   | Expected impact level      |\n| **Deciders**          | people   | -                                                   | Who made the decision      |\n| **Stakeholders**      | people   | -                                                   | Who's affected by decision |\n| **Related Decisions** | relation | Links to other decisions                            | Context and dependencies   |\n\n## Usage\n\n```\nCreate decision records with properties:\n{\n  \"Decision\": \"Use PostgreSQL for Primary Database\",\n  \"Date\": \"2025-10-15\",\n  \"Status\": \"Accepted\",\n  \"Domain\": \"Architecture\",\n  \"Impact\": \"High\",\n  \"Deciders\": [tech_lead, architect],\n  \"Stakeholders\": [eng_team]\n}\n```\n\n## Content Template\n\nEach decision page should include:\n\n- **Context**: Why this decision was needed\n- **Decision**: What was decided\n- **Rationale**: Why this option was chosen\n- **Options Considered**: Alternatives and trade-offs\n- **Consequences**: Expected outcomes (positive and negative)\n- **Implementation**: How decision will be executed\n\n## Views\n\n**Recent Decisions**: Sort by Date descending\n**Active Decisions**: Filter where Status = \"Accepted\"\n**By Domain**: Group by Domain\n**High Impact**: Filter where Impact = \"High\"\n**Pending**: Filter where Status = \"Proposed\"\n\n## Best Practices\n\n1. **Document immediately**: Record decisions when made, while context is fresh\n2. **Include alternatives**: Show what was considered and why it wasn't chosen\n3. **Track superseded decisions**: Update status when decisions change\n4. **Link related decisions**: Use relations to show dependencies\n5. **Review periodically**: Check if old decisions are still valid\n",
        "skills/workspace-documentation/reference/documentation-database.md": "# General Documentation Database\n\n**Purpose**: Store all types of documentation in a searchable, organized database.\n\n## Schema\n\n| Property          | Type             | Options                                                | Purpose                                              |\n| ----------------- | ---------------- | ------------------------------------------------------ | ---------------------------------------------------- |\n| **Title**         | title            | -                                                      | Document name                                        |\n| **Type**          | select           | How-To, Concept, Reference, FAQ, Decision, Post-Mortem | Categorize content type                              |\n| **Category**      | select           | Engineering, Product, Design, Operations, General      | Organize by department/topic                         |\n| **Tags**          | multi_select     | -                                                      | Additional categorization (languages, tools, topics) |\n| **Status**        | select           | Draft, In Review, Final, Deprecated                    | Track document lifecycle                             |\n| **Owner**         | people           | -                                                      | Document maintainer                                  |\n| **Created**       | created_time     | -                                                      | Auto-populated creation date                         |\n| **Last Updated**  | last_edited_time | -                                                      | Auto-populated last edit                             |\n| **Last Reviewed** | date             | -                                                      | Manual review tracking                               |\n\n## Usage\n\n```\nCreate pages with properties:\n{\n  \"Title\": \"How to Deploy to Production\",\n  \"Type\": \"How-To\",\n  \"Category\": \"Engineering\",\n  \"Tags\": \"deployment, production, DevOps\",\n  \"Status\": \"Final\",\n  \"Owner\": [current_user],\n  \"Last Reviewed\": \"2025-10-01\"\n}\n```\n\n## Views\n\n**By Type**: Group by Type property\n**By Category**: Group by Category property  \n**Recent Updates**: Sort by Last Updated descending\n**Needs Review**: Filter where Last Reviewed > 90 days ago\n**Draft Docs**: Filter where Status = \"Draft\"\n\n## Creating This Database\n\nUse `Notion:notion-create-database`:\n\n```javascript\n{\n  \"parent\": {\"page_id\": \"wiki-page-id\"},\n  \"title\": [{\"text\": {\"content\": \"Team Documentation\"}}],\n  \"properties\": {\n    \"Type\": {\n      \"select\": {\n        \"options\": [\n          {\"name\": \"How-To\", \"color\": \"blue\"},\n          {\"name\": \"Concept\", \"color\": \"green\"},\n          {\"name\": \"Reference\", \"color\": \"gray\"},\n          {\"name\": \"FAQ\", \"color\": \"yellow\"}\n        ]\n      }\n    },\n    \"Category\": {\n      \"select\": {\n        \"options\": [\n          {\"name\": \"Engineering\", \"color\": \"red\"},\n          {\"name\": \"Product\", \"color\": \"purple\"},\n          {\"name\": \"Design\", \"color\": \"pink\"}\n        ]\n      }\n    },\n    \"Tags\": {\"multi_select\": {\"options\": []}},\n    \"Owner\": {\"people\": {}},\n    \"Status\": {\n      \"select\": {\n        \"options\": [\n          {\"name\": \"Draft\", \"color\": \"gray\"},\n          {\"name\": \"Final\", \"color\": \"green\"},\n          {\"name\": \"Deprecated\", \"color\": \"red\"}\n        ]\n      }\n    }\n  }\n}\n```\n\n## Best Practices\n\n1. **Start with this schema** - most flexible for general documentation\n2. **Use relations** to connect related docs\n3. **Create views** for common use cases\n4. **Review properties** quarterly - remove unused ones\n5. **Document the schema** in database description\n6. **Train team** on property usage and conventions\n",
        "skills/workspace-documentation/reference/faq-database.md": "# FAQ Database\n\n**Purpose**: Organize frequently asked questions with answers.\n\n## Schema\n\n| Property              | Type         | Options                                    | Purpose                                           |\n| --------------------- | ------------ | ------------------------------------------ | ------------------------------------------------- |\n| **Question**          | title        | -                                          | The question being asked                          |\n| **Category**          | select       | Product, Engineering, Support, HR, General | Question topic                                    |\n| **Tags**              | multi_select | -                                          | Specific topics (auth, billing, onboarding, etc.) |\n| **Answer Type**       | select       | Quick Answer, Detailed Guide, Link to Docs | Response format                                   |\n| **Last Reviewed**     | date         | -                                          | When answer was verified                          |\n| **Helpful Count**     | number       | -                                          | Track usefulness (optional)                       |\n| **Audience**          | select       | Internal, External, All                    | Who should see this                               |\n| **Related Questions** | relation     | Links to related FAQs                      | Connect similar topics                            |\n\n## Usage\n\n```\nCreate FAQ entries with properties:\n{\n  \"Question\": \"How do I reset my password?\",\n  \"Category\": \"Support\",\n  \"Tags\": \"authentication, password, login\",\n  \"Answer Type\": \"Quick Answer\",\n  \"Last Reviewed\": \"2025-10-01\",\n  \"Audience\": \"External\"\n}\n```\n\n## Content Template\n\nEach FAQ page should include:\n\n- **Short Answer**: 1-2 sentence quick response\n- **Detailed Explanation**: Full answer with context\n- **Steps** (if applicable): Numbered procedure\n- **Screenshots** (if helpful): Visual guidance\n- **Related Questions**: Links to similar FAQs\n- **Additional Resources**: External docs or videos\n\n## Views\n\n**By Category**: Group by Category\n**Recently Updated**: Sort by Last Reviewed descending\n**Needs Review**: Filter where Last Reviewed > 180 days ago\n**External FAQs**: Filter where Audience contains \"External\"\n**Popular**: Sort by Helpful Count descending (if tracking)\n\n## Best Practices\n\n1. **Use clear questions**: Write questions as users would ask them\n2. **Provide quick answers**: Lead with the direct answer, then elaborate\n3. **Link related FAQs**: Help users discover related information\n4. **Review regularly**: Keep answers current and accurate\n5. **Track what's helpful**: Use feedback to improve frequently accessed FAQs\n",
        "skills/workspace-documentation/reference/how-to-guide-database.md": "# How-To Guide Database\n\n**Purpose**: Procedural documentation for common tasks.\n\n## Schema\n\n| Property          | Type         | Options                                 | Purpose                       |\n| ----------------- | ------------ | --------------------------------------- | ----------------------------- |\n| **Title**         | title        | -                                       | \"How to [Task]\"               |\n| **Complexity**    | select       | Beginner, Intermediate, Advanced        | Skill level required          |\n| **Time Required** | number       | -                                       | Estimated minutes to complete |\n| **Prerequisites** | relation     | Links to other guides                   | Required knowledge            |\n| **Category**      | select       | Development, Deployment, Testing, Tools | Task category                 |\n| **Last Tested**   | date         | -                                       | When procedure was verified   |\n| **Tags**          | multi_select | -                                       | Technology/tool tags          |\n\n## Usage\n\n```\nCreate how-to guides with properties:\n{\n  \"Title\": \"How to Set Up Local Development Environment\",\n  \"Complexity\": \"Beginner\",\n  \"Time Required\": 30,\n  \"Category\": \"Development\",\n  \"Last Tested\": \"2025-10-01\",\n  \"Tags\": \"setup, environment, docker\"\n}\n```\n\n## Best Practices\n\n1. **Use consistent naming**: Always start with \"How to...\"\n2. **Test procedures**: Verify steps work before publishing\n3. **Include time estimates**: Help users plan their time\n4. **Link prerequisites**: Make dependencies clear\n5. **Update regularly**: Re-test procedures when tools/systems change\n",
        "skills/workspace-documentation/reference/learning-database.md": "# Learning/Post-Mortem Database\n\n**Purpose**: Capture learnings from incidents, projects, or experiences.\n\n## Schema\n\n| Property          | Type     | Options                                      | Purpose                      |\n| ----------------- | -------- | -------------------------------------------- | ---------------------------- |\n| **Title**         | title    | -                                            | Event or project name        |\n| **Date**          | date     | -                                            | When it happened             |\n| **Type**          | select   | Incident, Project, Experiment, Retrospective | Learning type                |\n| **Severity**      | select   | Critical, Major, Minor                       | Impact level (for incidents) |\n| **Team**          | people   | -                                            | Who was involved             |\n| **Key Learnings** | number   | -                                            | Count of learnings           |\n| **Action Items**  | relation | Links to tasks                               | Follow-up actions            |\n\n## Content Template\n\nEach learning page should include:\n\n- **What Happened**: Situation description\n- **What Went Well**: Success factors\n- **What Didn't Go Well**: Problems encountered\n- **Root Causes**: Why things happened\n- **Learnings**: Key takeaways\n- **Action Items**: Improvements to implement\n\n## Best Practices\n\n1. **Blameless approach**: Focus on systems and processes, not individuals\n2. **Document quickly**: Capture while memory is fresh\n3. **Identify root causes**: Go beyond surface-level problems\n4. **Create action items**: Turn learnings into improvements\n5. **Follow up**: Track that action items are completed\n6. **Share widely**: Make learnings accessible to entire team\n",
        "skills/workspace-documentation/reference/team-wiki-database.md": "# Team Wiki Database\n\n**Purpose**: Centralized team knowledge and resources.\n\n## Schema\n\n| Property         | Type             | Options                                                  | Purpose           |\n| ---------------- | ---------------- | -------------------------------------------------------- | ----------------- |\n| **Title**        | title            | -                                                        | Page name         |\n| **Section**      | select           | Getting Started, Processes, Tools, Reference, Onboarding | Wiki organization |\n| **Tags**         | multi_select     | -                                                        | Topic tags        |\n| **Owner**        | people           | -                                                        | Page maintainer   |\n| **Last Updated** | last_edited_time | -                                                        | Auto-tracked      |\n| **Visibility**   | select           | Public, Team Only, Confidential                          | Access level      |\n\n## Usage\n\nUse for team-specific documentation that doesn't fit other databases.\n\n## Best Practices\n\n1. **Organize by sections**: Use clear top-level organization\n2. **Assign owners**: Every page should have a maintainer\n3. **Control visibility**: Set appropriate access levels\n4. **Link extensively**: Connect related pages\n5. **Keep current**: Regular reviews to remove outdated content\n",
        "skills/workspace-isolation/SKILL.md": "---\nname: workspace-isolation\ndescription: Use Git worktrees for isolated work environments. Creates separate working directories for parallel development on different branches.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Using Git Worktrees\n\n## Overview\n\nGit worktrees create isolated workspaces sharing the same repository, allowing work on multiple branches simultaneously without switching.\n\n**Core principle:** Systematic directory selection + safety verification = reliable isolation.\n\n**Announce at start:** \"I'm using the using-git-worktrees skill to set up an isolated workspace.\"\n\n## Directory Selection Process\n\nFollow this priority order:\n\n### 1. Check Existing Directories\n\n```bash\n# Check in priority order\nls -d .worktrees 2>/dev/null     # Preferred (hidden)\nls -d worktrees 2>/dev/null      # Alternative\n```\n\n**If found:** Use that directory. If both exist, `.worktrees` wins.\n\n### 2. Check CLAUDE.md\n\n```bash\ngrep -i \"worktree.*director\" CLAUDE.md 2>/dev/null\n```\n\n**If preference specified:** Use it without asking.\n\n### 3. Ask User\n\nIf no directory exists and no CLAUDE.md preference:\n\n```\nNo worktree directory found. Where should I create worktrees?\n\n1. .worktrees/ (project-local, hidden)\n2. ~/.config/superpowers/worktrees/<project-name>/ (global location)\n\nWhich would you prefer?\n```\n\n## Safety Verification\n\n### For Project-Local Directories (.worktrees or worktrees)\n\n**MUST verify .gitignore before creating worktree:**\n\n```bash\n# Check if directory pattern in .gitignore\ngrep -q \"^\\.worktrees/$\" .gitignore || grep -q \"^worktrees/$\" .gitignore\n```\n\n**If NOT in .gitignore:**\n\nPer Jesse's rule \"Fix broken things immediately\":\n1. Add appropriate line to .gitignore\n2. Commit the change\n3. Proceed with worktree creation\n\n**Why critical:** Prevents accidentally committing worktree contents to repository.\n\n### For Global Directory (~/.config/superpowers/worktrees)\n\nNo .gitignore verification needed - outside project entirely.\n\n## Creation Steps\n\n### 1. Detect Project Name\n\n```bash\nproject=$(basename \"$(git rev-parse --show-toplevel)\")\n```\n\n### 2. Create Worktree\n\n```bash\n# Determine full path\ncase $LOCATION in\n  .worktrees|worktrees)\n    path=\"$LOCATION/$BRANCH_NAME\"\n    ;;\n  ~/.config/superpowers/worktrees/*)\n    path=\"~/.config/superpowers/worktrees/$project/$BRANCH_NAME\"\n    ;;\nesac\n\n# Create worktree with new branch\ngit worktree add \"$path\" -b \"$BRANCH_NAME\"\ncd \"$path\"\n```\n\n### 3. Run Project Setup\n\nAuto-detect and run appropriate setup:\n\n```bash\n# Node.js\nif [ -f package.json ]; then npm install; fi\n\n# Rust\nif [ -f Cargo.toml ]; then cargo build; fi\n\n# Python\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\nif [ -f pyproject.toml ]; then poetry install; fi\n\n# Go\nif [ -f go.mod ]; then go mod download; fi\n```\n\n### 4. Verify Clean Baseline\n\nRun tests to ensure worktree starts clean:\n\n```bash\n# Examples - use project-appropriate command\nnpm test\ncargo test\npytest\ngo test ./...\n```\n\n**If tests fail:** Report failures, ask whether to proceed or investigate.\n\n**If tests pass:** Report ready.\n\n### 5. Report Location\n\n```\nWorktree ready at <full-path>\nTests passing (<N> tests, 0 failures)\nReady to implement <feature-name>\n```\n\n## Quick Reference\n\n| Situation | Action |\n|-----------|--------|\n| `.worktrees/` exists | Use it (verify .gitignore) |\n| `worktrees/` exists | Use it (verify .gitignore) |\n| Both exist | Use `.worktrees/` |\n| Neither exists | Check CLAUDE.md  Ask user |\n| Directory not in .gitignore | Add it immediately + commit |\n| Tests fail during baseline | Report failures + ask |\n| No package.json/Cargo.toml | Skip dependency install |\n\n## Common Mistakes\n\n**Skipping .gitignore verification**\n- **Problem:** Worktree contents get tracked, pollute git status\n- **Fix:** Always grep .gitignore before creating project-local worktree\n\n**Assuming directory location**\n- **Problem:** Creates inconsistency, violates project conventions\n- **Fix:** Follow priority: existing > CLAUDE.md > ask\n\n**Proceeding with failing tests**\n- **Problem:** Can't distinguish new bugs from pre-existing issues\n- **Fix:** Report failures, get explicit permission to proceed\n\n**Hardcoding setup commands**\n- **Problem:** Breaks on projects using different tools\n- **Fix:** Auto-detect from project files (package.json, etc.)\n\n## Example Workflow\n\n```\nYou: I'm using the using-git-worktrees skill to set up an isolated workspace.\n\n[Check .worktrees/ - exists]\n[Verify .gitignore - contains .worktrees/]\n[Create worktree: git worktree add .worktrees/auth -b feature/auth]\n[Run npm install]\n[Run npm test - 47 passing]\n\nWorktree ready at /Users/jesse/myproject/.worktrees/auth\nTests passing (47 tests, 0 failures)\nReady to implement auth feature\n```\n\n## Red Flags\n\n**Never:**\n- Create worktree without .gitignore verification (project-local)\n- Skip baseline test verification\n- Proceed with failing tests without asking\n- Assume directory location when ambiguous\n- Skip CLAUDE.md check\n\n**Always:**\n- Follow directory priority: existing > CLAUDE.md > ask\n- Verify .gitignore for project-local\n- Auto-detect and run project setup\n- Verify clean test baseline\n\n## Integration\n\n**Called by:**\n- **brainstorming** (Phase 4) - REQUIRED when design is approved and implementation follows\n- Any skill needing isolated workspace\n\n**Pairs with:**\n- **finishing-a-development-branch** - REQUIRED for cleanup after work complete\n- **executing-plans** or **subagent-driven-development** - Work happens in this worktree\n",
        "template/SKILL.md": "---\nname: template-skill\ndescription: Replace with description of the skill and when Claude should use it.\n---\n\n# Insert instructions below\n"
      },
      "plugins": [
        {
          "name": "document-skills",
          "description": "Collection of document processing suite including Excel, Word, PowerPoint, and PDF capabilities",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/spreadsheet-processor",
            "./skills/word-document-processor",
            "./skills/presentation-builder",
            "./skills/portable-document-handler"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add qodex-ai/ai-agent-skills",
            "/plugin install document-skills@anthropic-agent-skills"
          ]
        },
        {
          "name": "example-skills",
          "description": "Collection of example skills demonstrating various capabilities including skill creation, MCP building, visual design, algorithmic art, internal communications, web testing, artifact building, Slack GIFs, and theme styling",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/pattern-generator",
            "./skills/identity-framework",
            "./skills/visual-composition",
            "./skills/collaborative-document-creation",
            "./skills/web-interface-architect",
            "./skills/team-communication",
            "./skills/protocol-implementation-framework",
            "./skills/capability-architect",
            "./skills/animated-message-composer",
            "./skills/design-system-generator",
            "./skills/interactive-component-creator",
            "./skills/application-quality-assurance"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add qodex-ai/ai-agent-skills",
            "/plugin install example-skills@anthropic-agent-skills"
          ]
        }
      ]
    }
  ]
}